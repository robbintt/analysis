---
ver: rpa2
title: 'AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation
  using Large Language Models'
arxiv_id: '2310.18331'
source_url: https://arxiv.org/abs/2310.18331
tags:
- html
- alltogether
- task
- arxiv
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces AllTogether, a standardized prompt template
  designed to improve large language models' (LLMs) performance in HTML-based web
  navigation tasks. The template integrates task objectives, action histories, and
  HTML snippets into a single prompt, enabling LLMs to directly output defined actions.
---

# AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models

## Quick Facts
- arXiv ID: 2310.18331
- Source URL: https://arxiv.org/abs/2310.18331
- Reference count: 0
- Key outcome: GPT-4 achieves 64.73% element accuracy and 33.33% positive recall using AllTogether prompt template for HTML web navigation

## Executive Summary
AllTogether introduces a standardized prompt template that integrates task objectives, action histories, and HTML snippets into a single prompt for web navigation using large language models. The approach evaluates open-source Llama-2 models and API-accessible GPT models through prompt learning and instruction finetuning. Results demonstrate that GPT-4 significantly outperforms smaller models, with performance strongly influenced by HTML snippet length and action history trajectory. The study shows that real-time environmental feedback outperforms prior step-by-step instructions, offering valuable insights for developing LLM-driven web agents.

## Method Summary
The study evaluates AllTogether prompt template on the Mind2Web dataset using Llama-2-7B/13B-chat models with LoRA finetuning (α=16, rank=64, lr=2×10⁻⁴) and GPT models with in-context learning (temperature=0.1). The prompt concatenates task objective, sequential action history, and HTML snippet (≤2000 tokens) to constrain model output to specific action formats. Performance is measured using Element Accuracy, Positive Recall, and Non-matching Recall metrics. Ablations test history length (3/7 steps) and HTML window size (3/10/25/40) impacts on model performance.

## Key Results
- GPT-4 achieves 64.73% element accuracy and 33.33% positive recall using AllTogether prompt
- Longer HTML snippets and extended action histories significantly improve model accuracy
- Real-time environmental feedback outperforms prior step-by-step instructions
- Llama-2 models show limited performance compared to GPT series, with predominant None action outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AllTogether prompt structure improves LLM performance by providing comprehensive task context in a single prompt
- Mechanism: The prompt concatenates task objective, action history trajectory, and HTML snippet, allowing the model to leverage its reasoning capabilities across all relevant information simultaneously
- Core assumption: Large language models can effectively process and reason over concatenated long-form context containing task descriptions, historical actions, and HTML content
- Evidence anchors:
  - [abstract] "AllTogether, a standardized prompt template that enhances task context representation, thereby improving LLM's performance in HTML-based web navigation"
  - [section] "AllTogether comprises three primary sections, namely the user task objective, the sequential history of actions, and the HTML snippet of the current web page"
  - [corpus] Weak - the corpus papers focus on different prompt architectures but don't directly validate the concatenated context approach
- Break condition: If the model cannot process the concatenated context due to length limitations or if the context integration degrades rather than enhances reasoning

### Mechanism 2
- Claim: Larger parameter models (GPT-4) significantly outperform smaller models when using AllTogether prompting
- Mechanism: Larger models have better capacity to understand complex HTML structures, maintain longer context coherence, and generate syntactically correct actions from comprehensive prompts
- Core assumption: Model scale correlates with improved HTML understanding and action generation capabilities in complex web navigation tasks
- Evidence anchors:
  - [abstract] "Our results reveal that models like GPT-4 outperform smaller models in web navigation tasks"
  - [section] "The AllTogether prompt achieved its best accuracy of 64.73% on GPT-4, with a relatively high action recall rate of 33.3%"
  - [corpus] Weak - corpus contains papers on web agents but doesn't provide direct comparative performance data across model scales
- Break condition: If model performance plateaus or degrades with increased parameter count, or if the improvement is due to factors other than scale (e.g., training data differences)

### Mechanism 3
- Claim: Action history trajectory length significantly influences model performance, with longer histories improving accuracy
- Mechanism: Extended action histories provide valuable environmental feedback and context for the model to reason about the current state and plan appropriate actions
- Core assumption: Historical action sequences contain relevant information that the model can leverage to improve decision-making in web navigation tasks
- Evidence anchors:
  - [section] "For gpt-4 based AllTogether, the more historical information the better" and "the performance of the AllTogether prompt exhibits a declining trend as the length of the history sequence diminishes"
  - [section] "extended action histories furnish more valuable information for agent inference"
  - [corpus] Weak - corpus papers don't specifically analyze the impact of action history length on performance
- Break condition: If action histories become too long and introduce noise or if the model's attention mechanism cannot effectively utilize extended histories

## Foundational Learning

- Concept: HTML DOM structure understanding
  - Why needed here: The model must parse and reason about HTML elements to identify target elements and generate appropriate actions
  - Quick check question: Can you explain how an HTML DOM tree represents web page structure and how elements are typically selected for interaction?

- Concept: Prompt engineering for structured output
  - Why needed here: The AllTogether prompt constrains model output to specific action formats (CLICK, TYPE, SELECT, NONE), requiring careful prompt design
  - Quick check question: How would you design a prompt template to ensure a model outputs actions in a specific structured format?

- Concept: Context window limitations and chunking
  - Why needed here: Long HTML snippets must be chunked to fit within model context limits while preserving task-relevant information
  - Quick check question: What strategies would you use to chunk long HTML content while maintaining the ability to locate and interact with target elements?

## Architecture Onboarding

- Component map: Prompt template (Task Objective + Action History + HTML Snippet) -> Model (Llama-2 or GPT series) -> Action Output (CLICK, TYPE, SELECT, NONE) -> Environment Interaction
- Critical path: HTML snippet parsing -> Target element identification -> Action generation -> Syntax validation
- Design tradeoffs: Comprehensive context vs. prompt length limitations; simple prompt structure vs. specialized HTML processing modules
- Failure signatures: Incorrect element selection (wrong NODE_ID), malformed action output, inability to follow task instructions, performance degradation with longer HTML
- First 3 experiments:
  1. Validate basic prompt structure: Test AllTogether prompt with a small HTML snippet and simple task, verify correct action generation
  2. Window size impact: Vary HTML snippet length around the threshold identified in the paper to confirm the relationship between window size and accuracy
  3. History length impact: Test different action history lengths to verify the claim that longer histories improve performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal window size for HTML snippets in the AllTogether prompt to maximize model performance?
- Basis in paper: [explicit] The paper discusses the impact of HTML snippet length on model performance, noting a threshold window size beyond which the prompt's capability is fully realized
- Why unresolved: The paper indicates a threshold but does not specify the exact optimal window size, as it varies based on the model and task complexity
- What evidence would resolve it: Systematic experiments varying the window size across different models and tasks to identify the point of maximum performance

### Open Question 2
- Question: How does the inclusion of multimodal information (e.g., images) alongside HTML snippets affect the performance of LLM-driven web navigation agents?
- Basis in paper: [inferred] The paper focuses on HTML-based web navigation tasks but acknowledges that previous studies have addressed multimodal information, suggesting potential benefits
- Why unresolved: The paper does not explore the integration of multimodal information, leaving its impact on performance unexplored
- What evidence would resolve it: Comparative studies evaluating LLM performance with and without multimodal inputs in web navigation tasks

### Open Question 3
- Question: What is the impact of prior instruction generation on the performance of web navigation agents, and how can it be optimized?
- Basis in paper: [explicit] The paper discusses the use of prior instructions generated by an instructor model, noting limited effectiveness compared to real-time environmental feedback
- Why unresolved: The paper suggests limited effectiveness but does not explore optimization strategies for prior instruction generation
- What evidence would resolve it: Experiments testing different methods of prior instruction generation and their impact on agent performance in various web navigation scenarios

## Limitations
- Performance heavily dependent on HTML snippet length and action history length, with diminishing returns beyond certain thresholds
- Results based on filtered dataset (removing negative candidates) and specific HTML chunking strategy (2000 tokens)
- Limited model comparison (only Llama-2 and GPT variants) prevents broader generalizability
- Effectiveness of prior instruction generation found to be limited compared to real-time feedback

## Confidence

- **High confidence**: HTML snippet length impact on performance, basic AllTogether prompt structure effectiveness
- **Medium confidence**: Action history trajectory importance, GPT-4 vs smaller models performance gap
- **Low confidence**: Generalization to other web navigation datasets, long-term effectiveness in dynamic web environments

## Next Checks

1. Test AllTogether prompt structure with a different web navigation dataset (e.g., WebShop or MiniWoB) to assess generalizability beyond Mind2Web
2. Evaluate model performance with varying HTML chunking strategies (different token limits and semantic preservation methods) to identify optimal context window sizes
3. Conduct user studies comparing AllTogether-generated actions against human web navigation patterns to validate practical utility and identify potential usability gaps