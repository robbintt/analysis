---
ver: rpa2
title: Benchmarking and In-depth Performance Study of Large Language Models on Habana
  Gaudi Processors
arxiv_id: '2309.16976'
source_url: https://arxiv.org/abs/2309.16976
tags:
- gaudi
- performance
- attention
- habana
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks and studies the performance of large language
  models (LLMs) on Habana Gaudi processors. The authors identify key challenges in
  accelerating Transformer-based models on Gaudi, including unbalanced workloads between
  the Matrix Multiplication Engine (MME) and Tensor Processing Cores (TPC), performance
  bottlenecks when handling long sequences, and lack of end-to-end LLM performance
  evaluation.
---

# Benchmarking and In-depth Performance Study of Large Language Models on Habana Gaudi Processors

## Quick Facts
- arXiv ID: 2309.16976
- Source URL: https://arxiv.org/abs/2309.16976
- Reference count: 22
- Key outcome: Performance study of LLMs on Habana Gaudi processors identifying MME-TPC workload imbalance and softmax bottleneck, proposing linearized attention for optimization.

## Executive Summary
This paper presents a comprehensive benchmarking and performance analysis of large language models on Habana Gaudi processors, identifying key architectural bottlenecks in Transformer-based model training. Through systematic profiling experiments, the authors discover that the softmax attention operation creates a major performance bottleneck when mapped to Tensor Processing Cores (TPC) due to its lower computational power compared to the Matrix Multiplication Engine (MME). The study proposes linearized attention as an effective alternative that better utilizes MME resources and provides 2-6× speedup improvements. The findings offer practical insights for optimizing Transformer training on Gaudi hardware.

## Method Summary
The authors conduct profiling experiments using PyTorch-based SynapseAI software on Habana Gaudi processors to analyze performance characteristics of Transformer-based models. They compare execution times between MME and TPC for various operations including matrix multiplications of different sizes, and evaluate Transformer layer performance with both softmax attention and linearized attention implementations. The study benchmarks BERT and GPT models on a book corpus dataset, measuring end-to-end performance and analyzing hardware traces to identify workload distribution patterns and bottlenecks. Different activation functions are also tested to assess their impact on TPC performance.

## Key Results
- Softmax operations in Transformers consume over 80% of TPC runtime, creating a major bottleneck on Gaudi processors
- Linearized attention provides 2-6× speedup compared to softmax attention by better utilizing MME computational resources
- MME achieves 6.6-6.7× speedup over TPC for matrix multiplication operations, creating significant workload imbalance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The Matrix Multiplication Engine (MME) is significantly faster than the Tensor Processing Cores (TPC) for large matrix operations, creating an imbalance in workload distribution.
- **Mechanism**: MME is specifically optimized for matrix multiplication tasks like convolutions and GEMM, while TPC handles non-matrix operations. Profiling shows MME achieves 6.6-6.7x speedup over TPC for matrix multiplications of various sizes (128-2048).
- **Core assumption**: The speedup measurements from Table 2 accurately represent the performance gap between MME and TPC for the types of operations typically found in Transformers.
- **Evidence anchors**:
  - [section]: "Table 2 shows the execution time between MME and TPC for matrix multiplications of different sizes. We can conclude that the computational performance of TPC is up to 7× lower than that of MME."
  - [corpus]: No direct corpus evidence found for this specific performance gap claim.
- **Break condition**: If the workload distribution changes such that TPC handles more computationally intensive operations, or if MME becomes the bottleneck instead.

### Mechanism 2
- **Claim**: Softmax attention operations create a significant performance bottleneck on TPC due to their computational complexity and TPC's lower computational power.
- **Mechanism**: Softmax attention has O(N²) complexity and involves exponential and reduction operations that are not well-suited for TPC's SIMD architecture. This results in TPC becoming the bottleneck, with softmax operations exceeding 80% of total running time in Transformer layers.
- **Core assumption**: The softmax operation's computational characteristics make it particularly problematic for TPC's architecture.
- **Evidence anchors**:
  - [section]: "From this result, we have two observations. (1). There are many blank areas in the MME operating area... (2). In the running region of TPC, it is very clearly shown that the running time of softmax exceeds 80% of the total running time."
  - [abstract]: "Through profiling experiments, they find that softmax operations in Transformers become a major bottleneck when mapped to TPC due to its lower computational power compared to MME."
- **Break condition**: If alternative attention mechanisms like linearized attention become the standard, reducing the computational burden on TPC.

### Mechanism 3
- **Claim**: Linearized attention provides significant performance improvements by better utilizing MME and reducing the computational burden on TPC.
- **Mechanism**: Linearized attention transforms the softmax-based attention from O(N²) to O(N) complexity by using feature maps, allowing most computations to be mapped to MME. This results in 2-6× speedup compared to softmax attention.
- **Core assumption**: The feature map approach maintains adequate attention quality while providing computational benefits.
- **Evidence anchors**:
  - [section]: "Compared to original softmax-based attention, linear Transformers and Performer achieve 6 ×, 2 × speedup."
  - [abstract]: "They propose using linearized attention as an alternative to softmax attention, which improves performance by better utilizing MME."
- **Break condition**: If the feature map approximation significantly degrades model accuracy, or if MME becomes saturated with the increased workload.

## Foundational Learning

- **Concept**: Heterogeneous compute architecture (MME vs TPC)
  - Why needed here: Understanding the different capabilities and performance characteristics of MME and TPC is crucial for optimizing Transformer performance on Gaudi processors.
  - Quick check question: What types of operations are best suited for MME versus TPC based on their architectural characteristics?

- **Concept**: Self-attention mechanism and its computational complexity
  - Why needed here: The quadratic complexity of self-attention (O(N²)) is a fundamental bottleneck that the paper addresses through alternative attention mechanisms.
  - Quick check question: Why does the standard softmax attention become problematic for long sequences on Gaudi processors?

- **Concept**: Activation functions and their impact on hardware performance
  - Why needed here: Different activation functions have varying performance characteristics on TPC, affecting overall model training efficiency.
  - Quick check question: How do element-wise operations like activation functions interact with TPC's SIMD architecture?

## Architecture Onboarding

- **Component map**: Input -> MME (matrix operations) -> TPC (non-matrix operations) -> DMA (data transfer management) -> Network units (communication)
- **Critical path**: Data flows from input through MME for matrix operations, TPC for non-matrix operations, with DMA managing the transfer between them. The slowest component in this path determines overall performance.
- **Design tradeoffs**: MME offers higher performance but is limited to matrix operations, while TPC is more flexible but slower. Optimizing workload distribution between them is crucial.
- **Failure signatures**: Unbalanced workload (one component idle while another is busy), poor overlap between MME and TPC operations, softmax operations dominating TPC runtime.
- **First 3 experiments**:
  1. Profile a simple Transformer layer to identify the distribution of operations between MME and TPC and measure their respective execution times.
  2. Implement and benchmark linearized attention (e.g., Performer) to compare performance against standard softmax attention.
  3. Test different activation functions (ReLU, GELU, GLU) to identify their impact on TPC performance and overall training speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can GAUDI's performance for Transformer-based models be further optimized beyond the current linearization and activation function improvements?
- Basis in paper: [explicit] The paper identifies unbalanced workloads between MME and TPC as a bottleneck and suggests better code optimization and scheduling as potential solutions.
- Why unresolved: The paper only touches on basic profiling and some initial optimizations like linearized attention and activation functions. It does not explore advanced techniques such as kernel fusion, memory layout optimization, or hardware-aware model architectures.
- What evidence would resolve it: Experimental results showing performance improvements using advanced optimization techniques like kernel fusion, memory layout tuning, and hardware-aware neural architecture search on GAUDI.

### Open Question 2
- Question: How does GAUDI's performance scale with larger models and longer sequences compared to other specialized AI accelerators?
- Basis in paper: [inferred] The paper focuses on GAUDI's performance for Transformer-based models but does not provide comparative analysis with other accelerators like NVIDIA's Hopper or Google's TPUv4.
- Why unresolved: The paper does not include benchmarking against other hardware platforms, making it difficult to assess GAUDI's relative performance in the broader context of AI accelerators.
- What evidence would resolve it: Comprehensive benchmarking results comparing GAUDI's performance with other accelerators on identical Transformer-based workloads, including scaling studies with increasing model size and sequence length.

### Open Question 3
- Question: What are the energy efficiency implications of using GAUDI for training Transformer-based models compared to traditional GPU-based approaches?
- Basis in paper: [inferred] The paper mentions concerns about the energy efficiency of Transformers but does not provide specific power consumption or energy efficiency measurements for GAUDI.
- Why unresolved: The paper focuses on performance metrics like execution time and TFLOPS but does not address power consumption or energy efficiency, which are crucial for real-world deployment.
- What evidence would resolve it: Power consumption measurements and energy efficiency calculations (e.g., TFLOPS per watt) for GAUDI compared to GPU-based systems during Transformer training, including total energy usage for full model training runs.

## Limitations
- Hardware-specific nature: Findings are specific to Habana Gaudi processors and may not generalize to other AI accelerators.
- Implementation-specific results: Performance measurements depend on specific PyTorch-based SynapseAI implementation and custom kernels used.
- Model architecture dependencies: Study focuses primarily on standard Transformer architectures, results may differ for newer architectures.

## Confidence
- **High confidence**: Softmax attention creates bottleneck on TPC due to O(N²) complexity and TPC's architectural limitations
- **Medium confidence**: Specific performance gap measurements between MME and TPC (6.6-6.7× speedup)
- **Medium confidence**: Performance improvements from linearized attention (2-6× speedup)

## Next Checks
1. **Cross-hardware validation**: Replicate profiling experiments on GPU-based systems (NVIDIA A100, AMD Instinct) to determine whether softmax bottleneck is specific to Gaudi's architecture.
2. **Alternative attention mechanism comparison**: Implement and benchmark additional linearized attention variants (Performers with different kernel functions, Nyströmformer) to assess generality of performance improvements.
3. **Memory-bandwidth sensitivity analysis**: Measure how performance characteristics change with different batch sizes, sequence lengths, and hidden dimensions to identify computational vs memory-bandwidth constraints.