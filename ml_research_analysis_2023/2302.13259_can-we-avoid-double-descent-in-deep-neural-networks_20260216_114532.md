---
ver: rpa2
title: Can we avoid Double Descent in Deep Neural Networks?
arxiv_id: '2302.13259'
source_url: https://arxiv.org/abs/2302.13259
tags:
- double
- descent
- learning
- regularization
- phenomenon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates whether the double descent phenomenon in
  deep neural networks can be avoided through regularization. The double descent phenomenon
  causes model performance to initially worsen as the model size increases beyond
  a certain point, before improving again.
---

# Can we avoid Double Descent in Deep Neural Networks?

## Quick Facts
- arXiv ID: 2302.13259
- Source URL: https://arxiv.org/abs/2302.13259
- Reference count: 0
- This work investigates whether the double descent phenomenon in deep neural networks can be avoided through regularization

## Executive Summary
This work investigates whether the double descent phenomenon in deep neural networks can be avoided through regularization. The double descent phenomenon causes model performance to initially worsen as the model size increases beyond a certain point, before improving again. This makes it difficult to find the optimal model size. The authors propose using ℓ2 regularization to avoid double descent by driving excess parameters to zero, so they have no effect on the model output. They test this on LeNet-300-100 on MNIST and ResNet-18 on CIFAR-10/100 with noisy labels. On the simpler MNIST task, ℓ2 regularization successfully avoids double descent. However, on the more complex CIFAR datasets, double descent still occurs even with high ℓ2 regularization, indicating more work is needed to fully avoid it in complex scenarios.

## Method Summary
The authors investigate double descent avoidance through ℓ2 regularization on two model-dataset pairs: LeNet-300-100 on MNIST and ResNet-18 on CIFAR-10/100 with symmetric label noise. They employ iterative magnitude-based pruning with lottery ticket rewinding, training models with varying ℓ2 regularization strengths, pruning 20% of parameters at each iteration, rewinding surviving parameters to their initial values, and retraining. They measure test accuracy and loss across different sparsity levels to detect double descent patterns. For CIFAR experiments, they use 10%, 20%, and 50% label noise levels.

## Key Results
- ℓ2 regularization successfully avoids double descent on the simpler MNIST task
- Double descent still occurs on complex CIFAR datasets even with high ℓ2 regularization
- Excessive ℓ2 regularization (λ=1×10⁻³) harms performance on CIFAR-10, suggesting a narrow effective range
- The effectiveness of regularization appears to depend on task complexity, with classification tasks requiring more sophisticated approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Excess parameters can be driven to zero through ℓ2 regularization, effectively removing their impact on model output.
- Mechanism: ℓ2 regularization penalizes large parameter values, pushing them toward zero during optimization. When parameters approach zero, their contribution to the forward pass becomes negligible due to the activation function property φ(z) ≈ z when z → 0.
- Core assumption: The optimization process will drive parameters to zero when ℓ2 regularization is sufficiently strong, and this will eliminate the contribution of excess parameters to the model output.
- Evidence anchors:
  - [abstract] "They test this on LeNet-300-100 on MNIST and ResNet-18 on CIFAR-10/100 with noisy labels. On the simpler MNIST task, ℓ2 regularization successfully avoids double descent."
  - [section] "Let us focus on the second case. For numerical reasons, this scenario is unrealistic during continuous optimization; hence we can achieve a similar outcome by satisfying two conditions..."
  - [corpus] Weak evidence - no direct mention of ℓ2 regularization effectiveness in avoiding double descent in the corpus neighbors.
- Break condition: If the regularization strength is insufficient, parameters will not approach zero and will continue to contribute noise to the output, maintaining the double descent phenomenon.

### Mechanism 2
- Claim: Double descent occurs when excess parameters create a perturbation in the model output that varies with model size.
- Mechanism: As model size increases beyond a certain point, excess parameters contribute to output noise that follows the pattern described in Equation 1. This perturbation creates the characteristic U-shaped curve in test error, where performance initially worsens before improving again.
- Core assumption: The contribution of excess parameters to the output can be modeled as a sum of individual parameter contributions that create a net perturbation.
- Evidence anchors:
  - [abstract] "The double descent phenomenon causes model performance to initially worsen as the model size increases beyond a certain point, before improving again."
  - [section] "Let us say, the target, optimal output for the model is yopt... For instance, there is some subset wexc ∈ w of parameters belonging to the model which are in excess..."
  - [corpus] No direct evidence in corpus neighbors about the mechanism of double descent perturbation.
- Break condition: If the model architecture or regularization strategy changes the relationship between parameter magnitude and output contribution, the double descent pattern may not emerge.

### Mechanism 3
- Claim: Magnitude pruning combined with lottery ticket rewinding can reveal whether double descent has been avoided by testing performance at varying sparsity levels.
- Mechanism: The algorithm prunes parameters based on magnitude, then rewinds surviving parameters to their initialization and retrains. This process tests whether a smaller model can achieve similar performance to the original, indicating that excess parameters were not contributing essential information.
- Core assumption: If double descent is avoided, the test accuracy will remain relatively stable or improve as sparsity increases, rather than following the characteristic dip and recovery pattern.
- Evidence anchors:
  - [abstract] "They test this on LeNet-300-100 on MNIST and ResNet-18 on CIFAR-10/100 with noisy labels."
  - [section] "After training for the first time the model on the learning task Ξ... a magnitude pruning stage is set up..."
  - [corpus] No direct mention in corpus neighbors of the specific pruning and rewinding approach used to detect double descent.
- Break condition: If the pruning threshold or rewinding strategy is not appropriate for the specific model architecture, the test may not accurately reveal whether double descent has been avoided.

## Foundational Learning

- Concept: Bias-variance tradeoff and its relationship to model capacity
  - Why needed here: Understanding how model complexity affects generalization error is fundamental to interpreting double descent, which appears to contradict classical bias-variance theory
  - Quick check question: If you increase model capacity beyond the interpolation threshold, what happens to the bias and variance components of generalization error according to classical theory?

- Concept: Regularization techniques and their effect on parameter optimization
  - Why needed here: The proposed solution relies on ℓ2 regularization to drive excess parameters to zero, so understanding how different regularization methods affect optimization is crucial
  - Quick check question: How does ℓ2 regularization influence the gradient update for a parameter during training compared to unregularized gradient descent?

- Concept: Neural network pruning and the lottery ticket hypothesis
  - Why needed here: The experimental approach uses magnitude pruning and lottery ticket rewinding to test for double descent, so understanding these techniques is essential for implementing the evaluation
  - Quick check question: What is the key insight of the lottery ticket hypothesis regarding the relationship between initialization and trainability of pruned subnetworks?

## Architecture Onboarding

- Component map: Training pipeline → ℓ2 regularization → Magnitude pruning → Lottery ticket rewinding → Retraining → Accuracy measurement
- Critical path: Model training → pruning → rewinding → retraining → accuracy measurement. This sequence must be followed precisely to ensure valid results.
- Design tradeoffs: Higher ℓ2 regularization values may avoid double descent but could also prevent the model from learning the training data effectively, as seen in the ablation study. The pruning percentage must balance between revealing excess parameters and maintaining sufficient model capacity.
- Failure signatures: If double descent persists despite high ℓ2 regularization, the failure may indicate that the regularization strength is insufficient or that the model architecture requires a different regularization strategy. Inconsistent accuracy patterns across sparsity levels may indicate issues with the pruning or rewinding implementation.
- First 3 experiments:
  1. Train LeNet-300-100 on MNIST with varying ℓ2 regularization strengths (λ = 1e-5, 1e-4, 1e-3) and plot test accuracy vs. sparsity to verify the double descent avoidance on the simple dataset.
  2. Train ResNet-18 on CIFAR-10 with 10% label noise using the optimal λ from experiment 1, then vary the pruning percentage to observe the double descent pattern.
  3. Conduct an ablation study on λ for CIFAR-10 by testing values spanning several orders of magnitude (5e-5, 1e-4, 5e-4, 1e-3) to identify the threshold where regularization becomes counterproductive.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can double descent be completely avoided in classification tasks with complex datasets through regularization techniques?
- Basis in paper: [explicit] The authors conclude that standard regularization approaches like ℓ2 have limitations in avoiding double descent in complex classification scenarios, suggesting a need for a custom regularization approach.
- Why unresolved: The paper shows that while ℓ2 regularization can mitigate double descent in simpler tasks, it fails to do so in complex classification datasets like CIFAR-10 and CIFAR-100. This indicates that a different or more sophisticated regularization technique is needed, but the paper does not identify what that might be.
- What evidence would resolve it: Successful demonstration of a regularization technique that prevents double descent in complex classification tasks across multiple datasets and model architectures.

### Open Question 2
- Question: What specific properties of the activation functions and weight magnitudes are required to avoid double descent in neural networks?
- Basis in paper: [explicit] The authors discuss conditions where excess parameters should have minimal effect on output, including that activation functions should not amplify signals and parameters should have low magnitudes. They suggest ℓ∞ regularization might be ideal but propose ℓ2 as a compromise.
- Why unresolved: The paper identifies theoretical conditions for avoiding double descent but doesn't provide concrete specifications for activation functions or precise regularization strategies that guarantee these conditions.
- What evidence would resolve it: Mathematical proof and empirical validation showing specific activation function properties and regularization schemes that ensure minimal impact from excess parameters across various network architectures.

### Open Question 3
- Question: How does the amount of label noise affect the threshold at which double descent occurs, and can this relationship be modeled predictively?
- Basis in paper: [explicit] The authors test different percentages of symmetric label noise (10%, 20%, 50%) and observe how double descent manifests differently across these conditions, noting that the phenomenon persists regardless of noise level.
- Why unresolved: While the paper shows that double descent occurs at various noise levels, it doesn't establish a predictive model for how noise levels influence the specific point or severity of double descent.
- What evidence would resolve it: A mathematical framework that predicts the relationship between label noise percentage and the characteristics of double descent (timing, severity, recoverability) validated across multiple datasets and noise distributions.

## Limitations
- ℓ2 regularization successfully avoids double descent only on simpler MNIST task, failing on complex CIFAR datasets
- The paper does not explore alternative regularization strategies beyond ℓ2
- Excessive ℓ2 regularization harms performance, indicating a narrow effective range requiring careful tuning

## Confidence
- **High confidence**: The observation that double descent occurs on complex datasets like CIFAR-10/100 even with ℓ2 regularization, as this is directly supported by experimental results.
- **Medium confidence**: The theoretical mechanism by which ℓ2 regularization should drive excess parameters to zero, as the mathematical framework is sound but its practical implementation shows limitations.
- **Medium confidence**: The conclusion that avoiding double descent in complex scenarios requires more sophisticated approaches, given the empirical evidence presented.

## Next Checks
1. Test whether adaptive regularization schemes (e.g., LARS or per-layer regularization) can avoid double descent on CIFAR datasets more effectively than uniform ℓ2 regularization.
2. Investigate the relationship between initialization strategy and double descent persistence by comparing standard initialization with techniques like Fixup or NTK parameterization.
3. Conduct experiments on additional complex datasets (e.g., ImageNet-32 or TinyImageNet) to determine whether the double descent phenomenon scales predictably with dataset complexity and size.