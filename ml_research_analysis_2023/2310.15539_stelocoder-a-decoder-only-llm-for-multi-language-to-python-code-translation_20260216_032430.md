---
ver: rpa2
title: 'SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation'
arxiv_id: '2310.15539'
source_url: https://arxiv.org/abs/2310.15539
tags:
- code
- language
- translation
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SteloCoder is a decoder-only LLM for multi-language to Python code
  translation. It modifies StarCoder with Mixture-of-Experts (MoE) and LoRA to create
  language-specific experts, enabling translation from C++, C, JavaScript, Java, or
  PHP to Python without specifying input language.
---

# SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation

## Quick Facts
- arXiv ID: 2310.15539
- Source URL: https://arxiv.org/abs/2310.15539
- Reference count: 10
- Key outcome: 73.76 CodeBLEU score on XLCoST datasets, surpassing top performance by 3.5+

## Executive Summary
SteloCoder is a decoder-only language model designed for multi-language to Python code translation. Built on StarCoder, it uses a Mixture-of-Experts (MoE) architecture with language-specific LoRA experts to translate C++, C#, JavaScript, Java, and PHP into Python. The model employs curriculum learning and self-instruct data to achieve high translation quality with only 45M extra parameters and 32 hours of training on a single 80GB A100 GPU.

## Method Summary
SteloCoder modifies StarCoder with Mixture-of-Experts (MoE) and LoRA to create language-specific experts for translating between five programming languages and Python. Each source language has a dedicated LoRA expert that is selected by a gating network based on the input code. The model is trained using curriculum learning, starting with snippet-level data and progressing to program-level data. During inference, the model automatically identifies the input language and routes to the appropriate expert without requiring explicit language specification.

## Key Results
- Achieves 73.76 CodeBLEU score on XLCoST datasets
- Outperforms previous state-of-the-art by at least 3.5 CodeBLEU points
- Maintains high translation quality while adding only 45M parameters beyond the 15.5B parameter StarCoder base
- Routes to correct language experts with ~95% accuracy

## Why This Works (Mechanism)

### Mechanism 1
Language-specific LoRA experts enable precise code translation for each source language while keeping parameter count small. LoRA fine-tunes StarCoder separately for each language-to-Python task, storing low-rank adaptation matrices as frozen experts. The gating network selects the correct expert during inference without merging parameters.

### Mechanism 2
The MoE gating network learns to classify input programming language and route to the corresponding LoRA expert. A linear layer after embedding computes language probabilities, with soft routing during training and hard routing during inference. This allows the model to automatically identify source language without explicit specification.

### Mechanism 3
Curriculum learning accelerates convergence and improves translation quality by first training on short code snippets for 2 epochs, then on full programs for 2 epochs. This gradually increases complexity, allowing the model to leverage easier examples to find better local minima.

## Foundational Learning

- **Concept**: Low-Rank Adaptation (LoRA)
  - Why needed: Reduces trainable parameters while preserving backbone weights for efficient multi-expert training
  - Quick check: How does LoRA represent weight updates using two small matrices A and B instead of updating the full weight matrix?

- **Concept**: Mixture-of-Experts (MoE) routing
  - Why needed: Enables conditional execution of task-specific experts, applying correct LoRA set per input language without merging parameters
  - Quick check: What is the difference between soft routing during training and hard routing during inference in MoE?

- **Concept**: Curriculum Learning (CL)
  - Why needed: Improves convergence speed and accuracy by training on easier snippet-level data before harder program-level data
  - Quick check: Why might training on only program-level data from the start hurt model performance?

## Architecture Onboarding

- **Component map**: Input code -> Embedding layer -> MoE gating network -> Selected LoRA expert set -> StarCoder backbone -> Python output
- **Critical path**: Embed input → gating network classifies language → select LoRA expert set → pass through StarCoder with selected LoRA applied in parallel → generate Python code output
- **Design tradeoffs**: Memory overhead from unmerged LoRA matrices requiring 3× matrix multiplications per expert layer; flexibility to add new languages by training new LoRA sets and gating network; expert specialization improves accuracy but limits to code translation tasks
- **Failure signatures**: High MoE gating confusion matrix off-diagonal entries indicate wrong expert selection; large variance in expert accuracy suggests some languages poorly handled; training instability may indicate learning rate or data shuffling issues
- **First 3 experiments**: 1) Train single LoRA expert on C++→Python with StarCoder base; 2) Add MoE gating trained on balanced language dataset; 3) Apply curriculum learning schedule and compare against non-CL baseline

## Open Questions the Paper Calls Out

- **Open Question 1**: How does SteloCoder's performance compare to encoder-decoder models on code translation tasks, especially for languages not included in the current five-language set? The paper focuses on five specific languages without exploring generalization to other programming languages or comparisons with encoder-decoder architectures.

- **Open Question 2**: What is the impact of different LoRA rank values on SteloCoder's performance and training efficiency? The paper mentions using LoRA rank of 4 but does not explore how varying this hyperparameter affects effectiveness or efficiency.

- **Open Question 3**: How does SteloCoder's code generation capability compare to its code translation performance, and what are the limitations of its generation abilities? The paper mentions generation capabilities but notes limitations and lack of broader knowledge compared to general-purpose LLMs without detailed comparison.

## Limitations

- The claimed parameter efficiency of 45M extra parameters may be understated due to parallel computation overhead of multiple LoRA matrices during inference
- ~95% language identification accuracy may degrade in real-world scenarios with syntactically similar languages like C# vs Java
- Curriculum learning effectiveness demonstrated only on XLCoST datasets; optimal schedule may vary with different dataset characteristics

## Confidence

- **High Confidence**: Core LoRA fine-tuning and MoE routing mechanisms are well-established; basic translation quality improvement over baselines is supported
- **Medium Confidence**: Specific architectural choices (rank=4 LoRA, linear gating network, 2+2 curriculum schedule) are claimed to work well but optimality is not rigorously established
- **Low Confidence**: "No need to specify input language" claim relies entirely on gating network reliability; no error analysis provided for cases where wrong expert is selected

## Next Checks

1. **Stress Test Language Identification**: Create adversarial test cases with syntactically similar languages (C# vs Java, JavaScript vs PHP) and measure gating network accuracy on edge cases compared to baseline requiring explicit language specification

2. **Ablation Study on Curriculum Schedule**: Systematically vary curriculum learning schedule (different epoch splits, reverse order, random order) and measure impact on final CodeBLEU scores and training time to validate optimal schedule

3. **Memory and Latency Profiling**: Measure actual inference time and memory usage of unmerged LoRA approach versus merged parameter version to quantify parameter efficiency against practical deployment costs including GPU memory requirements