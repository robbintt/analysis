---
ver: rpa2
title: 'PCNN: Probable-Class Nearest-Neighbor Explanations Improve Fine-Grained Image
  Classification Accuracy for AIs and Humans'
arxiv_id: '2308.13651'
source_url: https://arxiv.org/abs/2308.13651
tags:
- advisingnets
- image
- class
- top-1
- advisingnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Probable-Class Nearest-Neighbor (PCNN) explanations,
  which leverage nearest-neighbor comparisons within the top-K predicted classes to
  improve both fine-grained image classification accuracy and human-AI team performance.
  The method uses a novel binary classifier, AdvisingNet, that compares an input image
  with nearest neighbors from the top-Q predicted classes of a frozen, pretrained
  model.
---

# PCNN: Probable-Class Nearest-Neighbor Explanations Improve Fine-Grained Image Classification Accuracy for AIs and Humans

## Quick Facts
- arXiv ID: 2308.13651
- Source URL: https://arxiv.org/abs/2308.13651
- Reference count: 21
- Primary result: Improves fine-grained classification accuracy on CUB-200 from 85.83% to 88.00% and Cars-196 from 89.73% to 90.37%

## Executive Summary
This paper introduces Probable-Class Nearest-Neighbor (PCNN) explanations that leverage nearest-neighbor comparisons within top-Q predicted classes to improve both fine-grained image classification accuracy and human-AI team performance. The method uses a novel binary classifier, AdvisingNet, that compares input images with nearest neighbors from the top-Q predicted classes of a frozen, pretrained model. This comparison yields re-ranked predictions that improve classification accuracy while simultaneously reducing human over-reliance on AI by showing users the most probable class examples.

## Method Summary
The method trains an AdvisingNet (a hybrid CNN-Transformer binary classifier) to compare input images with nearest neighbors from the top-Q predicted classes of a frozen, pretrained classifier. During training, the model generates positive/negative pairs based on whether ground-truth labels match candidate classes, then uses these pairs to learn similarity distinctions. During inference, the model retrieves nearest neighbors for each top-Q class, computes similarity scores via AdvisingNet, and re-ranks the classes based on these scores to produce the final prediction.

## Key Results
- Classification accuracy improvements: CUB-200 (85.83% → 88.00%), Cars-196 (89.73% → 90.37%), Dogs-120 (85.26% → 86.00%)
- Human-AI team performance: Reduces over-reliance on AI predictions by showing probable-class examples rather than just top-1 predictions
- AdvisingNet accuracy: Achieves 86.8% accuracy on CUB-200 and 84.2% on Cars-196 for binary classification task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model improves classification by re-ranking top-Q predicted classes based on similarity scores between the input image and nearest neighbor representatives.
- Mechanism: After a pretrained classifier C produces its top-Q predictions, an AdvisingNet compares the input image to the nearest neighbor of each candidate class. The AdvisingNet outputs a similarity score for each comparison. The classes are then re-ranked according to these scores, and the highest-scoring class becomes the new top-1 prediction.
- Core assumption: The ground-truth class is likely to appear within the top-Q predictions of C, so re-ranking only those Q classes suffices.
- Evidence anchors:
  - [abstract]: "use scores from S to weight the confidence scores of C to refine predictions."
  - [section]: "Top-class Reranking (TCR) that re-ranks the top-predicted classes... By focusing on the top-Q classes... this strategy mitigates issues associated with the long-tail distribution."
- Break condition: If the true class is not in the top-Q predictions, the re-ranking will not improve accuracy and may degrade it.

### Mechanism 2
- Claim: The binary AdvisingNet classifier learns to distinguish correct from incorrect predictions by comparing input images with nearest neighbors from both the ground-truth and other top-Q classes.
- Mechanism: During training, for each input image x, the model generates Q+K-1 pairs: K pairs from the top-1 predicted class (positive if it matches the ground truth, negative otherwise) and one pair per other top-Q class (positive if from ground truth, negative otherwise). The AdvisingNet learns to output high scores for same-class pairs and low scores for different-class pairs.
- Core assumption: The top-1 predicted class often matches the ground truth in training, providing a strong source of positive pairs.
- Evidence anchors:
  - [section]: "we consider only the top-Q classes... NNs taken from classes outside the top-Q are often clearly different... we sample K neighbors from the top-1 class to derive K positive pairs."
  - [section]: "For each image in the top-Q classes, we mark the (x, xnn) pairs as positive (+) or negative (–) accordingly, depending on whether the ground-truth label of x matches these classes."
- Break condition: If the top-1 class rarely matches the ground truth, the model will have insufficient positive pairs and will struggle to learn meaningful similarity distinctions.

### Mechanism 3
- Claim: Hybrid architecture combining conv layers from pretrained C and cross-attention Transformer layers allows effective cross-image comparison.
- Mechanism: The AdvisingNet uses conv layers (initialized from C) to extract patch tokens for both the input image and a nearest neighbor. These tokens are fed into cross-attention Transformer layers that compute similarity features, which are then passed through MLPs to produce a similarity score.
- Core assumption: The conv features already encode discriminative visual information, and cross-attention can effectively compare two such encodings.
- Evidence anchors:
  - [section]: "We initialize the convolutional layers with pretrained weights from C to encode input images... we adopt the CrossViT backbone... Our two branches handle the same image scale and share the weights."
- Break condition: If conv features are not discriminative enough or cross-attention fails to align features meaningfully, similarity scores will be noisy and classification will not improve.

## Foundational Learning

- Concept: Nearest neighbor retrieval and distance computation
  - Why needed here: The model relies on retrieving nearest neighbors from the training set for each candidate class during both training and inference.
  - Quick check question: How does the model retrieve nearest neighbors for each class, and what distance metric is used?

- Concept: Softmax-based classification and top-K ranking
  - Why needed here: The pretrained classifier C provides the initial ranking of classes, which the re-ranking algorithm modifies.
  - Quick check question: What is the difference between using top-1 vs top-Q predictions, and how does Q affect the model's coverage of possible classes?

- Concept: Cross-attention mechanisms in Vision Transformers
  - Why needed here: The AdvisingNet uses cross-attention to compare two images (input and neighbor) effectively.
  - Quick check question: How does cross-attention differ from self-attention, and why is it suitable for comparing two separate images?

## Architecture Onboarding

- Component map:
  - Pretrained classifier C -> Nearest neighbor database -> AdvisingNet (conv + cross-attention Transformer + MLPs) -> TCR algorithm

- Critical path:
  1. Input image → C → top-Q class probabilities
  2. For each class in top-Q, retrieve its nearest neighbor
  3. For each (input, neighbor) pair, feed into AdvisingNet → similarity score
  4. Rerank classes by AdvisingNet scores → final prediction

- Design tradeoffs:
  - Using only top-Q classes limits computation but risks missing the true class if it falls outside Q
  - Sharing conv weights between branches reduces model size but may limit specialization
  - K controls number of positive pairs; larger K increases training data but may introduce noise

- Failure signatures:
  - Accuracy drops when Q is too small and true class is frequently outside top-Q
  - AdvisingNet fails to generalize if conv features from C are not discriminative enough
  - Re-ranking produces worse results if AdvisingNet scores are uncorrelated with class correctness

- First 3 experiments:
  1. Vary Q from 3 to 20 on CUB-200 and measure accuracy and runtime to find sweet spot
  2. Compare AdvisingNet accuracy when conv layers are frozen vs. trainable to assess need for fine-tuning
  3. Test generalization by training AdvisingNet on one backbone (e.g., RN50) and applying TCR to unseen backbones (e.g., NTS-Net)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the performance characteristics of AdvisingNets vary when applied to large-scale datasets like ImageNet compared to fine-grained datasets like CUB-200 and Cars-196?
- Basis in paper: [inferred] The paper mentions that due to computational constraints, scaling AdvisingNets for large-scale classification datasets such as ImageNet has not been explored. It suggests a potential connection between the performance of AdvisingNets and that of the pretrained models, which has not been investigated yet.
- Why unresolved: The study primarily focuses on fine-grained datasets, and the computational demands of AdvisingNets increase with the size of the dataset. The relationship between AdvisingNets' performance and the capabilities of pretrained models on larger datasets remains unexplored.
- What evidence would resolve it: Conducting experiments with AdvisingNets on large-scale datasets like ImageNet and comparing the results with those from fine-grained datasets would provide insights into the scalability and performance characteristics of AdvisingNets.

### Open Question 2
- Question: What is the impact of using different pretrained models as the backbone for AdvisingNets on their classification performance?
- Basis in paper: [explicit] The paper investigates the effect of retraining conv layers of classifiers C and finds that freezing these layers significantly slows down the learning process of AdvisingNet. It also notes that the selection of C for AdvisingNets significantly influences their binary classification performance.
- Why unresolved: While the paper explores the impact of retraining conv layers and using different backbones, it does not extensively analyze how different pretrained models as backbones affect the overall classification performance of AdvisingNets.
- What evidence would resolve it: Training AdvisingNets with various pretrained models as backbones and evaluating their performance on classification tasks would clarify the impact of the choice of pretrained model on AdvisingNets' effectiveness.

### Open Question 3
- Question: How does the choice of hyperparameters Q and K affect the performance of AdvisingNets, and what is the optimal balance between accuracy improvement and computational efficiency?
- Basis in paper: [explicit] The paper mentions that Q and K are crucial hyperparameters for training AdvisingNets and that their values significantly influence the performance. It notes that while larger values can introduce noise during training, there is a balance to strike between test accuracy and computational demand.
- Why unresolved: The paper provides insights into the effects of Q and K but does not determine the optimal values for these hyperparameters or establish a clear balance between accuracy and computational efficiency.
- What evidence would resolve it: Conducting a comprehensive study with varying values of Q and K, analyzing their effects on AdvisingNets' performance, and determining the optimal balance would provide clarity on the best practices for hyperparameter selection.

## Limitations

- Limited dataset scope: All experiments conducted on CUB-200 and Cars-196 fine-grained datasets; effectiveness on broader image classification tasks remains unproven
- Confidence in training methodology: Some implementation details (OneCycleLR parameters, data augmentation specifics) are not fully specified
- Human study limitations: User study based on Mechanical Turk participants with unspecified domain expertise; generalizability to experts or real-world deployment unclear

## Confidence

- **High confidence**: Technical feasibility of AdvisingNet architecture, correctness of TCR algorithm, basic accuracy improvements on tested datasets
- **Medium confidence**: Claim that PCNN explanations reduce over-reliance on AI predictions; requires more rigorous testing across diverse user groups
- **Low confidence**: Assertion that model generalizes well to non-fine-grained datasets; nearest neighbor approach may not scale effectively

## Next Checks

1. **Dataset Generalization Test**: Evaluate PCNN on non-fine-grained datasets like CIFAR-100 or ImageNet to verify if accuracy improvements persist outside the targeted domain.

2. **Ablation Study on Q and K**: Systematically vary Q (3-20) and K (1-20) parameters to identify optimal settings and test model sensitivity to these hyperparameters across different datasets.

3. **Real-world User Study**: Conduct a user study with domain experts (ornithologists for bird classification, automotive experts for car classification) to validate whether PCNN explanations genuinely improve human decision-making in practical scenarios compared to Mechanical Turk results.