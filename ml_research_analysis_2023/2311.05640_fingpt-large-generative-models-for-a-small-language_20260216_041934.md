---
ver: rpa2
title: 'FinGPT: Large Generative Models for a Small Language'
arxiv_id: '2311.05640'
source_url: https://arxiv.org/abs/2311.05640
tags:
- urkunlp
- small
- hatanp
- finnishnlp
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of creating large generative
  language models for Finnish, a language with limited resources. The authors compile
  a comprehensive dataset from various sources and train monolingual models ranging
  from 186M to 13B parameters, as well as a 176B parameter multilingual model called
  BLUUMI by continuing BLOOM's pretraining.
---

# FinGPT: Large Generative Models for a Small Language

## Quick Facts
- arXiv ID: 2311.05640
- Source URL: https://arxiv.org/abs/2311.05640
- Reference count: 40
- One-line primary result: Monolingual Finnish models (186M-13B params) outperform previous Finnish models, with best achieving 48.7% accuracy on FIN-bench

## Executive Summary
This study addresses the challenge of creating large generative language models for Finnish, a language with limited resources. The authors compile a comprehensive dataset from various sources and train monolingual models ranging from 186M to 13B parameters, as well as a 176B parameter multilingual model called BLUUMI by continuing BLOOM's pretraining. They introduce FIN-bench, a Finnish evaluation dataset, and assess model performance, alignment, bias, and toxicity. Results show that the monolingual models outperform previous Finnish models, with the best achieving 48.7% accuracy on FIN-bench. The BLUUMI model demonstrates significantly improved Finnish capabilities compared to BLOOM while maintaining English performance.

## Method Summary
The authors compiled an extensive Finnish dataset from web crawls, news, social media, and eBooks, totaling over 200 billion tokens. They trained seven monolingual models from scratch (186M to 13B parameters) using GPT architecture and BLOOM's pretraining approach, along with a 176B parameter multilingual model BLUUMI by continuing BLOOM pretraining on Finnish data. Models were trained using the Megatron-DeepSpeed framework on LUMI supercomputer with AMD MI250X GPUs. The dataset underwent preprocessing including deduplication, heuristic filtering, N-gram model filtering, toxicity filtering, and personal data masking. Models were evaluated on FIN-bench (a Finnish version of BIG-bench) in zero- to three-shot settings, and assessed for alignment, bias, and toxicity.

## Key Results
- Monolingual models outperform previous Finnish models, with best achieving 48.7% accuracy on FIN-bench
- BLUUMI model shows significantly improved Finnish capabilities compared to BLOOM while maintaining English performance
- Models demonstrate limitations in alignment and reflect gender stereotypes present in training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FinGPT models achieve strong performance by leveraging large-scale monolingual Finnish data combined with BLOOM's multilingual architecture
- Mechanism: The FinGPT models are trained from scratch on a diverse Finnish dataset (over 200 billion tokens), while BLUUMI extends BLOOM's multilingual capabilities by continuing pretraining with Finnish data
- Core assumption: The Finnish data is sufficiently representative and diverse to enable effective pretraining of monolingual models
- Evidence anchors:
  - [abstract] The study compiles an extensive dataset of Finnish from web crawls, news, social media, and eBooks, totaling over 200 billion tokens
  - [section 3] The dataset includes sources like Parsebank, mC4-Fi, CC-Fi, Fiwiki, and others, ensuring broad coverage of Finnish text
  - [corpus] The dataset is preprocessed to remove duplicates, filter out non-Finnish texts, and mask personal data, ensuring high quality

### Mechanism 2
- Claim: The FinGPT models outperform previous Finnish models due to their larger size and more extensive training data
- Mechanism: By training models up to 13 billion parameters on 300 billion tokens, the FinGPT models can capture more complex linguistic patterns and nuances in Finnish
- Core assumption: Larger models with more training data can better capture the intricacies of the Finnish language
- Evidence anchors:
  - [abstract] The best FinGPT model achieves 48.7% accuracy on FIN-bench, outperforming previous Finnish models
  - [section 5.2] The 13B parameter model shows improved performance compared to smaller models, though overfitting is a concern

### Mechanism 3
- Claim: The FinGPT models address alignment, bias, and toxicity issues through careful data preprocessing and evaluation
- Mechanism: The dataset is filtered for toxicity, and the models are evaluated for alignment, bias, and toxicity using specific tasks and metrics
- Core assumption: Proper evaluation and mitigation of alignment, bias, and toxicity issues are crucial for the responsible deployment of language models
- Evidence anchors:
  - [abstract] The study assesses model alignment, bias, and toxicity, demonstrating limitations and areas for improvement
  - [section 5.3] The models perform poorly on HHH alignment tasks, indicating the need for further alignment efforts
  - [section 5.4] The models reflect gender stereotypes present in the training data, highlighting the importance of bias mitigation

## Foundational Learning

- Concept: Data preprocessing and filtering
  - Why needed here: To ensure the quality and representativeness of the training data, which directly impacts model performance
  - Quick check question: How does the dataset handle duplicates and non-Finnish texts?

- Concept: Model architecture and pretraining
  - Why needed here: To understand how the FinGPT models are designed and trained to achieve strong performance in Finnish language tasks
  - Quick check question: What are the key differences between the FinGPT models and BLOOM?

- Concept: Evaluation and benchmarking
  - Why needed here: To assess the models' capabilities and limitations, ensuring they meet the desired performance standards
  - Quick check question: How is the FIN-bench dataset used to evaluate the models?

## Architecture Onboarding

- Component map: Data collection → Preprocessing → Model training (monolingual and multilingual) → Evaluation (FIN-bench, alignment, bias, toxicity)
- Critical path: Data collection and preprocessing → Model pretraining → Evaluation and benchmarking → Assessment of alignment, bias, and toxicity
- Design tradeoffs: The choice between monolingual and multilingual models, as well as the decision to train on a large dataset versus focusing on data quality
- Failure signatures: Overfitting due to excessive parameters, poor performance on specific tasks, and issues with alignment, bias, or toxicity
- First 3 experiments:
  1. Evaluate the performance of the FinGPT models on the FIN-bench dataset to assess their capabilities
  2. Compare the FinGPT models to previous Finnish models to demonstrate improvements
  3. Assess the alignment, bias, and toxicity of the FinGPT models to identify areas for improvement

## Open Questions the Paper Calls Out

# Open Question 1
- Question: What is the optimal number of pretraining epochs for monolingual Finnish models of different sizes?
- Basis in paper: [inferred] The paper notes that larger models (8B vs 13B) may overfit due to excessive parameters and training steps compared to the limited amount of non-repeated Finnish text available
- Why unresolved: The paper trains models for 300 billion tokens (approximately 8 epochs) but doesn't systematically explore the impact of different epoch counts on model performance
- What evidence would resolve it: A controlled experiment varying the number of epochs for different model sizes and measuring performance on FIN-bench and downstream tasks would clarify the optimal training duration

# Open Question 2
- Question: How does the performance of Finnish language models scale with the amount of Finnish text available in pretraining data?
- Basis in paper: [explicit] The paper discusses the challenge of creating LLMs for Finnish due to limited data availability (less than 1% of Common Crawl texts are Finnish)
- Why unresolved: The paper trains on a fixed dataset size and doesn't explore how performance would change with more or less Finnish data
- What evidence would resolve it: Training models on systematically varied amounts of Finnish data (while controlling for quality) and measuring performance on FIN-bench and other benchmarks would establish the relationship between data quantity and model capability

# Open Question 3
- Question: What is the impact of continued pretraining on the computational efficiency and environmental cost of multilingual models?
- Basis in paper: [explicit] The paper introduces BLUUMI by continuing BLOOM's pretraining with Finnish data, showing improved Finnish capabilities without compromising English performance
- Why unresolved: The paper demonstrates the effectiveness of continued pretraining for language coverage but doesn't address the resource efficiency or environmental implications
- What evidence would resolve it: A comprehensive analysis comparing the computational resources, energy consumption, and carbon emissions of continued pretraining versus monolingual training for similar performance levels

## Limitations

- Evaluation scope limited to English-language benchmarks and a relatively small Finnish-specific test set (37 tasks)
- Toxicity filtering approach uses a binary classifier with limited validation on Finnish content
- Comparison to previous Finnish models constrained by limited availability of published results for earlier models

## Confidence

**High Confidence**: The data compilation methodology and dataset characteristics are well-documented with specific token counts, source descriptions, and preprocessing steps clearly outlined. The technical approach of training monolingual models and continuing BLOOM pretraining is straightforward and reproducible.

**Medium Confidence**: The model performance results are well-supported by the data, but the interpretation of alignment, bias, and toxicity assessments requires careful consideration. The models show poor alignment scores and gender bias, but the practical implications for real-world deployment need further investigation.

**Low Confidence**: The generalizability of the models beyond the FIN-bench evaluation to broader Finnish language applications remains uncertain. The study does not extensively test the models on diverse real-world Finnish language tasks or compare performance across different Finnish language domains.

## Next Checks

1. **Cross-domain performance validation**: Evaluate the FinGPT models on additional Finnish language datasets from different domains (legal, medical, technical) not included in the training corpus to assess generalization capabilities and identify potential domain-specific weaknesses.

2. **Toxicity filtering effectiveness audit**: Conduct a detailed analysis of the toxicity filtering process by sampling filtered and unfiltered data to calculate precision, recall, and F1-score of the Detoxify classifier on Finnish content. Compare the impact of different filtering thresholds on model performance and safety.

3. **Morphological complexity benchmark**: Develop and implement a Finnish-specific benchmark focusing on morphological richness and compound word processing, as these linguistic features are particularly challenging for Finnish and may reveal limitations not captured by current evaluations.