---
ver: rpa2
title: Evaluating Cognitive Maps and Planning in Large Language Models with CogEval
arxiv_id: '2309.15129'
source_url: https://arxiv.org/abs/2309.15129
tags:
- graph
- cognitive
- planning
- llms
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CogEval, a cognitive science-inspired protocol
  for systematically evaluating cognitive abilities in LLMs. The authors apply CogEval
  to evaluate cognitive maps and planning abilities across eight LLMs using task prompts
  adapted from human experiments.
---

# Evaluating Cognitive Maps and Planning in Large Language Models with CogEval

## Quick Facts
- arXiv ID: 2309.15129
- Source URL: https://arxiv.org/abs/2309.15129
- Reference count: 40
- One-line primary result: LLMs show apparent competence on simpler tasks but systematically fail at complex planning tasks requiring understanding of latent relational structures (cognitive maps), suggesting they lack genuine planning abilities.

## Executive Summary
This paper introduces CogEval, a cognitive science-inspired protocol for systematically evaluating cognitive abilities in large language models (LLMs). The authors apply CogEval to assess cognitive maps and planning abilities across eight LLMs using task prompts adapted from human experiments. While LLMs display apparent competence on simpler tasks with explicit route memorization, systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and getting trapped in loops. These findings challenge the notion of emergent out-of-the-box planning ability in LLMs and suggest they lack understanding of the latent relational structures underlying planning problems.

## Method Summary
The CogEval protocol evaluates LLM performance on cognitive map and planning tasks using prompts adapted from human cognitive experiments. The method involves generating prompts for different graph structures (line, tree, community), domains (spatial, social, object relations), and task conditions (value-based planning, reward/transition revaluation, shortcuts, detours). LLMs generate responses to these prompts, with OpenAI models tested at multiple temperature settings and other models tested once. Results are analyzed using logistic regression with factors including graph structure, domain, condition, LLM model, and temperature to identify performance patterns and failure modes.

## Key Results
- LLMs perform well on simple line graphs with fully specified trajectories but fail on complex community graphs
- Systematic evaluation reveals specific failure modes: hallucinations of non-existent edges, getting trapped in loops, and failure to find shortest paths
- Model size and temperature show non-significant effects on planning performance, while graph structure and domain are significant predictors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail at multi-step planning when tasks require understanding latent relational structures (cognitive maps) because they lack an internalized model of the environment's graph structure.
- Mechanism: LLMs rely on pattern matching and memorization of explicit trajectories provided in the prompt, but struggle to infer and use the underlying graph topology to generate new paths or adapt to structural changes.
- Core assumption: Planning competence in humans and RL agents depends on constructing and using a cognitive map, not just memorizing routes.
- Evidence anchors:
  - [abstract] "These findings do not support the idea of emergent out-of-the-box planning ability in LLMs. This could be because LLMs do not understand the latent relational structures underlying planning problems, known as cognitive maps, and fail at unrolling goal-directed trajectories based on the underlying structure."
  - [section] "Our systematic evaluation reveals that while LLMs display apparent competence on some tasks in smaller graphs, they do not have out-of-the-box emergent cognitive map comprehension or planning competence."
- Break condition: If LLMs can be augmented with explicit graph traversal algorithms (e.g., BFS/DFS instructions) and their performance on complex community graphs improves, the mechanism is partially supported but suggests the issue is architectural, not inherent.

### Mechanism 2
- Claim: LLMs show apparent competence on simpler tasks because they can memorize and replay complete trajectories laid out in the prompt, rather than planning from scratch.
- Mechanism: In tasks with small graphs and fully specified paths, LLMs retrieve and piece together memorized sequences, but this strategy fails when paths are not fully provided or when the graph structure is complex.
- Core assumption: Apparent success in small, fully-specified tasks does not imply genuine planning ability.
- Evidence anchors:
  - [section] "We observe that LLMs do better in problems where the entire trajectories are explicitly available in the text prompts, and they only need to retrieve and piece together partial changes."
- Break condition: If performance on complex community graphs improves with CoT prompts (e.g., BFS/DFS instructions), the mechanism is weakened, suggesting that LLMs can plan if given the right algorithmic guidance.

### Mechanism 3
- Claim: Temperature and model size do not significantly affect LLM planning performance, but the underlying graph structure and task domain do.
- Mechanism: The ability to plan is not a function of model scale or randomness in generation, but rather of the complexity of the underlying graph and the domain (spatial vs. social vs. object relations).
- Core assumption: Graph structure and domain are more important predictors of planning success than model size or temperature.
- Evidence anchors:
  - [section] "Conversely, the temperature showed a non-significant chi-squared statistic (χ2(11) = 1.28, p = .53) and the interaction between the LLM and temperature was also non-significant (χ2(11) = 10.69, p = .71)."
- Break condition: If increasing temperature or model size leads to improved planning performance on complex graphs, the mechanism is invalid.

## Foundational Learning

- Concept: Cognitive maps and planning in humans and RL agents
  - Why needed here: Understanding the theoretical foundation of planning helps interpret why LLMs may fail and what architectural features are missing.
  - Quick check question: What is a cognitive map, and how does it enable planning in humans and RL agents?

- Concept: Graph theory and graph traversal algorithms (BFS, DFS)
  - Why needed here: These are the tools LLMs would need to use to plan effectively; their absence explains LLM failure modes.
  - Quick check question: How do BFS and DFS differ, and why is BFS useful for finding shortest paths?

- Concept: Statistical analysis of experimental results (logistic regression, chi-squared tests)
  - Why needed here: Interpreting the results requires understanding how statistical significance is determined and what it implies about model performance.
  - Quick check question: What does a significant chi-squared statistic for graph structure tell us about LLM performance?

## Architecture Onboarding

- Component map: Prompt generation → LLM inference → Response aggregation → Statistical analysis → Interpretation
- Critical path: Prompt generation → LLM inference → Response aggregation → Statistical analysis → Interpretation
- Design tradeoffs: Using novel prompts avoids contamination but limits comparison to existing benchmarks; statistical analysis is robust but requires sufficient data
- Failure signatures: Hallucinations of non-existent edges, getting trapped in loops, failure to find shortest paths, and inconsistent performance across similar tasks
- First 3 experiments:
  1. Evaluate LLM performance on simple line graphs with fully specified trajectories
  2. Test LLM adaptation to reward revaluation (changing goal location)
  3. Assess LLM performance on community graphs with sparse connectivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs' planning failures stem from fundamental architectural limitations or could architectural modifications enable emergent planning abilities?
- Basis in paper: [explicit] The paper discusses LLMs as "programmable machines with natural language as their programming language" rather than emergent intelligence, and suggests that "smaller models with well-thought-out architecture, augmentation, and energy constraints could potentially achieve the same skills."
- Why unresolved: The paper demonstrates LLMs' planning failures but doesn't conclusively determine whether these are inherent architectural limitations or could be overcome with different architectures.
- What evidence would resolve it: Systematic evaluation of planning capabilities in LLMs with different architectures (beyond transformers) or with specific architectural modifications designed to enhance planning abilities.

### Open Question 2
- Question: How do different Chain of Thought (CoT) prompt structures and complexity levels affect LLM planning performance across various graph types and conditions?
- Basis in paper: [explicit] The paper tested simple BFS/DFS instructions and found mixed results, noting that "the effects are not consistent nor monotonic" and that this "needs further investigation to be better understood."
- Why unresolved: The paper only tested basic graph traversal instructions and found inconsistent effects, suggesting more nuanced CoT prompt design could be valuable.
- What evidence would resolve it: Systematic testing of various CoT prompt structures (from simple to complex) across different graph types, measuring performance improvements and identifying which prompt features are most effective.

### Open Question 3
- Question: What are the specific neural representational differences between LLMs that succeed and fail at planning tasks, and how do these relate to human cognitive map representations?
- Basis in paper: [inferred] The paper mentions this as a future direction, noting that "analyzing representational similarities in the embeddings and test hypotheses about representations underlying success and failure modes" could mirror neuroscience approaches.
- Why unresolved: The paper identifies failure modes but doesn't analyze the underlying neural representations that distinguish successful from unsuccessful planning attempts.
- What evidence would resolve it: Comparative analysis of LLM embeddings during planning tasks, identifying patterns that correlate with success/failure, and comparison to human neural data from cognitive map studies.

## Limitations

- Novel prompt methodology limits comparison to existing LLM evaluation benchmarks
- Unclear distinction between inference difficulties and genuine planning deficits
- Asymmetric experimental design (multiple temperature settings for some models, single evaluation for others)

## Confidence

- High confidence: Empirical observation that LLMs struggle with complex community graph planning tasks and exhibit specific failure modes
- Medium confidence: Interpretation that these failures indicate LLMs lack cognitive maps or genuine planning ability
- Low confidence: Generalizability of findings to all LLMs given limited sample and novel evaluation protocol

## Next Checks

1. Systematically test whether different prompt formulations (e.g., explicit graph notation vs. natural language descriptions, inclusion of traversal algorithms) affect LLM planning performance

2. Evaluate whether providing LLMs with explicit graph traversal instructions (BFS/DFS algorithms) or tools for pathfinding improves performance on complex community graphs

3. Apply the CogEval protocol to a broader range of LLM architectures and different graph types to assess generalizability across model families and problem domains