---
ver: rpa2
title: 'Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey'
arxiv_id: '2305.05403'
source_url: https://arxiv.org/abs/2305.05403
tags:
- recall
- knowledge
- entities
- statements
- completeness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey explores the challenge of assessing and representing
  incompleteness, recall, and negation in large-scale knowledge bases. It highlights
  the critical need for understanding where KBs are incomplete, and presents methods
  to automatically estimate recall, extract cardinality information, and identify
  salient negative statements.
---

# Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey

## Quick Facts
- arXiv ID: 2305.05403
- Source URL: https://arxiv.org/abs/2305.05403
- Reference count: 40
- One-line primary result: Comprehensive survey of methods for assessing and representing incompleteness, recall, and negation in large-scale knowledge bases.

## Executive Summary
This survey provides a comprehensive overview of techniques for understanding and measuring incompleteness, recall, and negation in large-scale knowledge bases. It addresses the fundamental challenge of knowing where KBs are incomplete and how to automatically estimate recall without exhaustive enumeration. The survey covers logical foundations, statistical estimation methods, cardinality extraction, negative statement identification, and relative recall approaches. Key outcomes include improved methods for predicting missing entities, automatic extraction of cardinality assertions, and identification of salient negative statements, all of which are essential for enhancing KB quality and enabling quality-aware applications.

## Method Summary
The survey synthesizes research on KB incompleteness assessment through multiple complementary approaches. Statistical estimation methods leverage Benford's Law, mark-and-recapture, and Good-Turing estimators to predict missing entities from observed patterns. Cardinality extraction involves detecting counting predicates, aligning them with enumerating predicates, and extracting numeric assertions from both KBs and text. Relative recall approaches compare KBs to external resources, related entities, or use cases through class-based, graph-based, or embedding-based similarity measures. The methods range from proof-of-concept implementations to practical techniques, with varying degrees of validation across different domains.

## Key Results
- Statistical estimation methods can predict KB incompleteness using patterns like Benford's Law and mark-and-recapture without requiring exhaustive real-world enumeration
- Cardinality assertions from KBs and text provide a direct numeric basis for assessing recall by comparing expected versus actual counts
- Relative recall assessment offers a pragmatic alternative to absolute recall by comparing entities/resources/use cases based on relatedness measures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Statistical estimation methods can predict KB incompleteness without exhaustive real-world enumeration.
- **Mechanism:** Leverages Benford's Law, sampling distributions, and statistical invariants (e.g., mark-and-recapture adapted for collaborative KB edits) to infer missing entities and statements from observed data patterns.
- **Core assumption:** Real-world distributions follow predictable statistical patterns (e.g., leading digits in quantities, entity edit frequencies).
- **Evidence anchors:**
  - [abstract]: "estimation of this information via statistical patterns"
  - [section 3]: Describes Benford's Law, mark-and-recapture, Good-Turing estimators for collaborative KBs
  - [corpus]: No direct corpus evidence, but mentions "average neighbor FMR=0.256" implying weak statistical patterns
- **Break condition:** If real-world data does not follow expected statistical distributions, or if the KB's sampling process is biased.

### Mechanism 2
- **Claim:** Cardinality assertions from KBs and text provide a direct numeric basis for assessing KB recall.
- **Mechanism:** Identifies counting predicates (e.g., numberOfAwards) and aligns them with enumerating predicates (e.g., wonAward) to compare expected vs. actual counts; extracts cardinality information from text using pattern-based and sequence labeling approaches.
- **Core assumption:** KBs and text contain explicit or implicit cardinality information that can be reliably extracted and aligned.
- **Evidence anchors:**
  - [abstract]: "automatic extraction of cardinality assertions"
  - [section 4]: Details methods for detecting counting/enumerating predicates, aligning them, and extracting cardinalities from text
  - [corpus]: No direct corpus evidence for cardinality alignment, but weak statistical patterns suggest some feasibility
- **Break condition:** If cardinality assertions are sparse, misaligned, or extraction from text is unreliable due to variability.

### Mechanism 3
- **Claim:** Relative recall assessment provides a pragmatic alternative to absolute recall by comparing entities/resources/use cases.
- **Mechanism:** Defines relatedness via class-based, graph-based, or embedding-based similarity; quantifies recall via frequency aggregation, embedding similarity, or usage profiles; compares KB entities to related entities, external resources, or query logs.
- **Core assumption:** Related entities/resources/use cases provide a meaningful reference point for assessing KB completeness.
- **Evidence anchors:**
  - [abstract]: "relaxed notions of relative recall"
  - [section 6]: Describes Recoin, ORES, KB-to-text, KB-to-human associations, and QA-driven recall
  - [corpus]: Weak statistical evidence (FMR=0.256) but no direct corpus evidence for relative recall mechanisms
- **Break condition:** If relatedness measures are noisy, reference resources are unavailable, or use case profiles are not representative.

## Foundational Learning

- **Concept: Open-World Assumption (OWA)**
  - Why needed here: KBs under OWA treat absent statements as unknown, not false, which is fundamental to understanding incompleteness and the need for recall assessment.
  - Quick check question: If a KB under OWA does not contain a statement about Marie Curie winning the Nobel Prize in Chemistry, is it necessarily false? (Answer: No, it is unknown.)

- **Concept: Partial Completeness Assumption (PCA)**
  - Why needed here: PCA is a key heuristic used in many recall estimation methods, assuming that if a subject has at least one object for a predicate, then the list of objects is complete.
  - Quick check question: If Marie Curie has one award listed in the KB, does PCA imply she has no other awards? (Answer: Yes, under PCA.)

- **Concept: Cardinality Assertions**
  - Why needed here: Cardinality assertions provide a direct way to quantify KB recall by specifying the expected number of objects for a subject-predicate pair.
  - Quick check question: If a KB asserts that Marie Curie has 37 awards, and the KB lists only 2, what is the recall? (Answer: 2/37 â‰ˆ 5.4%.)

## Architecture Onboarding

- **Component map:**
  - Data Ingestion -> Statistical Estimation -> Cardinality Detection -> Relative Recall -> Validation
  - Collects KB data, text corpora, query logs, and external resources
  - Applies Benford's Law, mark-and-recapture, and other statistical methods
  - Identifies counting/enumerating predicates and extracts cardinality information
  - Computes entity similarity and generates recall metrics
  - Uses ground truth, crowd-sourcing, or logs to validate estimates

- **Critical path:**
  1. Data collection (KB, text, logs)
  2. Statistical estimation (optional, if no cardinality data)
  3. Cardinality detection and alignment
  4. Relative recall assessment
  5. Validation and refinement

- **Design tradeoffs:**
  - Precision vs. recall in statistical estimation: More complex models may improve accuracy but increase computational cost.
  - Completeness vs. efficiency in cardinality detection: Exhaustive alignment may be more accurate but slower.
  - Granularity vs. interpretability in relative recall: Fine-grained metrics may be harder to interpret but more actionable.

- **Failure signatures:**
  - Statistical estimation fails: Large discrepancies between estimated and actual recall, especially for non-statistical domains.
  - Cardinality detection fails: Misalignment of predicates, sparse cardinality data, or unreliable text extraction.
  - Relative recall fails: Noisy similarity measures, unavailable reference resources, or unrepresentative use cases.

- **First 3 experiments:**
  1. Apply Benford's Law to a KB's numerical predicates to estimate missing entities.
  2. Extract cardinality information from Wikipedia articles and align with KB predicates.
  3. Compare KB recall to text recall using a sample of open information extraction statements.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we build high-accuracy recall estimators that combine multiple complementary techniques?
- **Basis in paper:** [explicit] The paper states that "most of the estimators presented in this survey are proofs-of-concept, tested only in limited domains, or under very specific assumptions. Building practically usable high-accuracy estimators, possibly by combining several complementary estimation techniques, remains a major open challenge."
- **Why unresolved:** Current recall estimation techniques are often limited in scope or rely on specific assumptions, making them less practical for real-world applications.
- **What evidence would resolve it:** Development and validation of a recall estimation method that consistently achieves high accuracy across diverse knowledge bases and domains, possibly through a combination of techniques like statistical properties, sampling, and text analysis.

### Open Question 2
- **Question:** How can we estimate the recall of pre-trained language models for multi-valued, optional, and long-tail predicates?
- **Basis in paper:** [explicit] The paper mentions that "knowledge extraction from pre-trained language models has recently received much attention, yet it remains unclear to which degree this approach can yield knowledge for multi-valued, optional, and long-tail predicates. Systematically measuring the recall of language models, and comparing it with structured KBs, is an open challenge."
- **Why unresolved:** Pre-trained language models show promise for knowledge extraction, but their effectiveness for specific types of predicates is not well understood, and comparing their recall to structured knowledge bases is challenging.
- **What evidence would resolve it:** A comprehensive study comparing the recall of pre-trained language models and structured knowledge bases for various types of predicates, including multi-valued, optional, and long-tail predicates, across multiple domains.

### Open Question 3
- **Question:** How can we combine recall estimation with KB completion to optimize update frequencies for decaying information?
- **Basis in paper:** [explicit] The paper suggests that "despite their obvious connection, research on recall estimation and KB completion has so far evolved largely independently. Quantifying the value of knowledge, and defining use-case-driven priorization strategies for recall improvement based on insights from recall assessment, are great research opportunities."
- **Why unresolved:** Current approaches to KB completion and recall estimation are often siloed, and there is a lack of integration between the two to optimize the update process for knowledge that may become outdated over time.
- **What evidence would resolve it:** A framework that integrates recall estimation with KB completion, allowing for the prioritization of updates based on the expected value of new information and the likelihood of existing knowledge becoming outdated.

## Limitations

- Statistical estimation methods assume predictable real-world data distributions that may not hold across all domains or KB types.
- Cardinality extraction from text faces challenges due to variability in linguistic expressions and potential misalignment with KB predicates.
- Absolute recall estimation remains fundamentally hard without ground truth enumeration, necessitating reliance on relative recall approaches with quality-dependent relatedness measures.

## Confidence

- **High Confidence:** The logical foundations of KB incompleteness under the Open-World Assumption and the Partial Completeness Assumption are well-established in the literature.
- **Medium Confidence:** The statistical estimation methods (Benford's Law, mark-and-recapture) have been validated in specific contexts but may not generalize well to all KBs or domains.
- **Low Confidence:** The survey's discussion of negative statement identification and relative recall assessment is less detailed, and the effectiveness of these approaches may vary significantly depending on the specific KB and use case.

## Next Checks

1. **Statistical Estimation Validation:** Apply Benford's Law and mark-and-recapture estimators to a KB with known ground truth to quantify prediction accuracy and identify failure modes.
2. **Cardinality Extraction Evaluation:** Extract cardinality information from a sample of Wikipedia articles and align with KB predicates to measure extraction accuracy and alignment quality.
3. **Relative Recall Benchmarking:** Compare KB recall to text recall using a sample of open information extraction statements to assess the feasibility and limitations of relative recall approaches.