---
ver: rpa2
title: 'CycleAlign: Iterative Distillation from Black-box LLM to White-box Models
  for Better Human Alignment'
arxiv_id: '2310.16271'
source_url: https://arxiv.org/abs/2310.16271
tags:
- human
- responses
- alignment
- ranking
- black-box
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CycleAlign, a framework that iteratively aligns
  a parameter-visible white-box language model with a parameter-invisible black-box
  LLM (like ChatGPT) using in-context learning and ranking-based fine-tuning. The
  method addresses the challenge of aligning language models with human preferences,
  which is crucial for mitigating harmful and toxic content generation.
---

# CycleAlign: Iterative Distillation from Black-box LLM to White-box Models for Better Human Alignment

## Quick Facts
- arXiv ID: 2310.16271
- Source URL: https://arxiv.org/abs/2310.16271
- Authors: Multiple
- Reference count: 11
- Key outcome: CycleAlign significantly improves reward scores by 7.03 on Harmlessbase and 5.31 in total for RRHF when using LLaMA as the backbone

## Executive Summary
CycleAlign introduces an iterative framework for aligning white-box language models with human preferences by leveraging black-box LLMs like ChatGPT as ranking oracles. The method addresses the challenge of parameter-invisible black-box models by using in-context learning and ranking-based fine-tuning to progressively align a white-box model. Through cyclical collaboration between models, CycleAlign achieves state-of-the-art performance on the HH-RLHF dataset, outperforming existing methods like PPO, RRHF, and PRO in generating harmless and helpful responses.

## Method Summary
CycleAlign employs a cyclical collaborative process where a white-box model generates multiple responses to an instruction, which are then ranked by a black-box model using in-context learning with static and dynamically updated demonstrations. The ranking is used to optimize the white-box model through pairwise ranking objectives like RRHF or PRO. Dynamic demonstrations are created by extracting agreement rankings between the two models using longest common subsequence, which are fed back to improve the black-box model's ranking accuracy over iterations. This iterative distillation process transfers alignment capabilities from the black-box LLM to the white-box model while maintaining parameter visibility for efficient fine-tuning.

## Key Results
- Improves reward scores by 7.03 on Harmlessbase subset compared to RRHF baseline
- Achieves 5.31 total improvement in reward scores for RRHF objective with LLaMA backbone
- Outperforms PPO, RRHF, and PRO methods on HH-RLHF dataset for both harmless and helpful response generation

## Why This Works (Mechanism)

### Mechanism 1
The iterative collaboration between black-box and white-box models enables progressive alignment through mutual feedback loops. White-box model generates multiple responses, black-box model ranks them using in-context learning with dynamic demonstrations, then white-box model is fine-tuned on these rankings. Agreement rankings are fed back to enhance black-box model's demonstrations, creating a virtuous cycle. Core assumption: Both models' rankings contain useful signals that can be combined to improve alignment performance.

### Mechanism 2
Dynamic in-context demonstrations improve black-box model ranking accuracy by providing more relevant training examples. After each iteration, the longest common subsequence of white-box and black-box rankings is extracted and added as new demonstrations to the black-box model's prompt, progressively improving its ranking capability. Core assumption: The white-box model's ranking becomes more accurate as it is fine-tuned, making its agreement with the black-box model more valuable as training data.

### Mechanism 3
Ranking-based supervised fine-tuning with PRO or RRHF objectives effectively aligns the white-box model with human preferences. The black-box model's rankings are used to optimize the white-box model using pairwise ranking loss functions, which are more stable and effective than RL methods. Core assumption: The black-box model's rankings accurately reflect human preferences and can serve as effective training signals.

## Foundational Learning

- Concept: In-context learning (ICL) capabilities of LLMs
  - Why needed here: ICL is the core mechanism that allows the black-box model to rank responses effectively using demonstrations
  - Quick check question: How does increasing the number of demonstrations in ICL typically affect LLM performance on ranking tasks?

- Concept: Ranking-based supervised fine-tuning
  - Why needed here: Provides the optimization objective for aligning the white-box model based on black-box rankings
  - Quick check question: What is the key difference between ranking-based SFT and traditional RLHF approaches?

- Concept: Knowledge distillation from black-box to white-box models
  - Why needed here: The entire framework is built on transferring alignment capabilities from a parameter-invisible model to a parameter-visible one
  - Quick check question: What are the limitations of unidirectional distillation that CycleAlign specifically addresses?

## Architecture Onboarding

- Component map: Instruction → White-box response generation → Black-box ranking → Agreement extraction → White-box fine-tuning → Updated demonstrations

- Critical path: Instruction → White-box response generation → Black-box ranking → Agreement extraction → White-box fine-tuning → Updated demonstrations

- Design tradeoffs:
  - Black-box vs white-box: Using ChatGPT provides better alignment but adds API cost and dependency
  - Number of responses: More responses provide better ranking signal but increase computational cost
  - Iterative vs one-shot: Multiple iterations improve alignment but increase training time

- Failure signatures:
  - Plateauing performance after few iterations
  - Degradation in black-box ranking accuracy
  - Inconsistent agreement rankings across different instructions
  - Reward scores not improving despite multiple iterations

- First 3 experiments:
  1. Single iteration CycleAlign with static demonstrations only to establish baseline performance
  2. Multi-iteration CycleAlign with dynamic demonstrations to verify improvement over baseline
  3. Ablation study removing ICL or dynamic demonstrations to identify critical components

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CycleAlign vary with different types of black-box LLMs, beyond just ChatGPT? The paper only uses ChatGPT as the black-box model for experiments. Exploring other well-aligned models like GPT-4 or Claude could provide insights into the generalizability of CycleAlign.

### Open Question 2
What is the optimal number of candidate responses (n) to generate from the white-box model for each instruction in CycleAlign? The paper fixes l to 3 without exploring how different values of l might affect the alignment performance or computational efficiency.

### Open Question 3
How does the performance of CycleAlign compare to other methods when the black-box LLM's ranking ability is intentionally degraded? Understanding CycleAlign's robustness to varying qualities of black-box LLM ranking could provide insights into its practical applicability.

## Limitations

- Heavy dependence on black-box LLM APIs introduces computational costs and potential variability due to API rate limits and model updates
- Assumes consistent behavior from black-box model across iterations, but OpenAI's models can exhibit non-deterministic outputs
- Results only tested on single dataset (HH-RLHF) and specific model architectures (LLaMA-7B, Alpaca-7B)

## Confidence

- **High confidence**: The core mechanism of using black-box model rankings for white-box model fine-tuning is well-established and technically sound
- **Medium confidence**: The specific implementation of dynamic demonstrations and their effectiveness in improving black-box ranking accuracy
- **Low confidence**: Generalizability of results across different datasets, model sizes, and black-box model providers

## Next Checks

1. **Cross-dataset validation**: Test CycleAlign on additional alignment datasets beyond HH-RLHF to verify that performance gains are not dataset-specific and that the dynamic demonstration mechanism remains effective with different data distributions.

2. **Black-box model ablation**: Replace ChatGPT with other black-box models (Claude, Gemini, or open-source alternatives like Vicuna) to assess whether the framework's performance is robust to changes in the black-box model provider and whether certain models provide more consistent ranking signals for the iterative process.

3. **Scaling experiment**: Evaluate CycleAlign on larger white-box models (LLaMA-13B, LLaMA-30B) to determine if the iterative distillation approach scales effectively with model size and whether the dynamic demonstration mechanism becomes more or less important as the white-box model capacity increases.