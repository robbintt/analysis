---
ver: rpa2
title: 'FPTQ: Fine-grained Post-Training Quantization for Large Language Models'
arxiv_id: '2308.15987'
source_url: https://arxiv.org/abs/2308.15987
tags:
- activation
- quantization
- values
- w4a8
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes FPTQ, a fine-grained post-training quantization
  method for large language models (LLMs) that achieves high performance W4A8 compression
  (4-bit weights, 8-bit activations) without requiring additional fine-tuning. The
  key innovation is a layerwise activation quantization strategy combined with fine-grained
  weight quantization.
---

# FPTQ: Fine-grained Post-Training Quantization for Large Language Models

## Quick Facts
- arXiv ID: 2308.15987
- Source URL: https://arxiv.org/abs/2308.15987
- Reference count: 37
- One-line primary result: Achieves state-of-the-art W4A8 performance on BLOOM and LLaMA series models without additional fine-tuning

## Executive Summary
This paper introduces FPTQ, a fine-grained post-training quantization method that achieves high-performance 4-bit weight and 8-bit activation (W4A8) compression for large language models without requiring additional fine-tuning. The key innovation is a layerwise activation quantization strategy combined with fine-grained weight quantization that adapts to the varying characteristics of different layers. By using logarithmic activation equalization for medium-range layers and employing different quantization strategies based on activation range analysis, FPTQ achieves accuracy comparable to full-precision models while enabling efficient deployment through 8-bit matrix computation and 4-bit weight storage.

## Method Summary
FPTQ employs a layerwise activation quantization strategy that classifies layers based on their activation range characteristics. For layers with small ranges (≤15), it uses per-tensor static quantization; for medium ranges (15-150), it applies logarithmic activation equalization followed by per-tensor static quantization; and for large ranges (≥150), it uses per-token dynamic quantization. The method also incorporates fine-grained groupwise weight quantization to address quantization difficulty in smaller LLMs. The approach is calibrated offline using activation statistics from a subset of the Pile dataset and does not require additional fine-tuning of the pre-trained models.

## Key Results
- Achieves state-of-the-art W4A8 performance on BLOOM and LLaMA series models
- Matches or exceeds accuracy of original models on LAMBADA, MMLU, and Common Sense QA benchmarks
- Enables efficient deployment through 8-bit matrix computation and 4-bit weight storage
- Works without requiring additional fine-tuning of pre-trained models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Logarithmic activation equalization (LAE) reduces outlier dominance in activation distributions, improving quantization accuracy for intermediate-range layers.
- Mechanism: LAE applies a non-linear mapping (division by log₂(2 + max(|x|))) that preserves inlier information while compressing outliers, reducing quantization error in per-tensor static quantization.
- Core assumption: Outliers occupy fixed channels and dominate the quantization scale, compressing effective bits for inliers.
- Evidence anchors:
  - [abstract] states LAE is used for "most intractable layers" and combines with fine-grained weight quantization
  - [section 3.3.1] explains that larger outliers dominate distribution and LAE "squashes various distributions comparably"
  - [corpus] has no direct evidence for LAE effectiveness

### Mechanism 2
- Claim: Layerwise quantization strategy matches quantization granularity to activation range characteristics, balancing accuracy and hardware efficiency.
- Mechanism: Uses per-tensor static for small ranges (v ≤ v0), LAE + per-tensor for medium ranges (v0 < v < v1), and per-token dynamic for large ranges (v ≥ v1), optimizing both accuracy and acceleration.
- Core assumption: Different activation ranges require different quantization strategies to balance quantization error and hardware acceleration benefits.
- Evidence anchors:
  - [section 3.2] shows activation ranges differ dramatically across layers, motivating layerwise strategy
  - [section 3.3.1] describes the three-range policy with specific thresholds (v0=15, v1=150)
  - [corpus] has no direct evidence for effectiveness of this specific policy

### Mechanism 3
- Claim: Fine-grained groupwise weight quantization addresses quantization difficulty in smaller LLMs without engineering overhead.
- Mechanism: Divides weight matrices into groups and computes quantization scales per group, handling the quantization difficulty that per-channel quantization cannot address alone.
- Core assumption: Per-channel quantization is insufficient for LLM weights, requiring groupwise approach for better performance.
- Evidence anchors:
  - [section 3.3.2] states that "per-channel strategy only" is "not tractable" and adopts fine-grained groupwise quantization
  - [section 4.2] mentions groupwise weight quantization is "identically costly from engineering perspective"
  - [corpus] has no direct evidence for this specific claim

## Foundational Learning

- Concept: Quantization step size computation and its relationship to bit-width
  - Why needed here: Understanding how scale = max(|x|)/(2^(b-1)-1) affects quantization error is crucial for grasping why different strategies are needed for different activation ranges
  - Quick check question: If an activation tensor has range [0, 1000] and we use 8-bit quantization, what is the quantization step size?

- Concept: Trade-offs between static vs dynamic quantization and per-tensor vs per-token quantization
  - Why needed here: The paper's layerwise strategy depends on understanding when each approach is appropriate based on activation characteristics and hardware constraints
  - Quick check question: Why would per-token dynamic quantization be used for layers with large activation ranges despite sacrificing some hardware acceleration?

- Concept: Activation distribution analysis and outlier detection
  - Why needed here: The paper's core innovation relies on analyzing activation distributions to identify which layers need LAE and which can use simpler quantization methods
  - Quick check question: How would you determine if a layer's activation distribution is "intractable" based on its range characteristics?

## Architecture Onboarding

- Component map: FPTQ consists of three main components: (1) Layerwise Activation Quantization Strategy with LAE for medium-range layers, (2) Fine-grained Groupwise Weight Quantization, and (3) Policy Selection Logic that classifies layers into quantization categories based on activation range analysis.
- Critical path: The most critical path is the layerwise quantization strategy, as it directly impacts both accuracy (through appropriate quantization method selection) and performance (through hardware acceleration balance).
- Design tradeoffs: The main tradeoff is between quantization accuracy (favoring per-token dynamic for large ranges) and hardware efficiency (favoring per-tensor static for smaller ranges). LAE attempts to bridge this gap for medium-range layers.
- Failure signatures: Poor accuracy suggests incorrect layer classification or inadequate LAE parameters; slow inference suggests overuse of per-token dynamic quantization; quantization artifacts suggest inappropriate group size in weight quantization.
- First 3 experiments:
  1. Replicate the activation distribution analysis on a simple LLM to verify the three-range policy thresholds (v0=15, v1=150) are appropriate
  2. Implement LAE on a single medium-range layer and compare quantization error against baseline per-tensor static quantization
  3. Test the complete layerwise strategy on a small LLM (like LLaMA-7B) to verify accuracy retention on LAMBADA dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the parameter alpha in the generalized form of logarithmic activation equalization (LAE) affect the performance of the quantization method?
- Basis in paper: [explicit] The paper mentions a hyper-parameter alpha to control the level of suppression in the generalized form of LAE.
- Why unresolved: The paper only mentions the existence of alpha but does not explore its impact on the quantization performance.
- What evidence would resolve it: Experiments varying the alpha parameter and measuring the impact on the quantization accuracy and performance.

### Open Question 2
- Question: Can the logarithmic activation equalization (LAE) method be applied to other types of neural networks beyond large language models?
- Basis in paper: [inferred] The paper focuses on applying LAE to large language models, but the method could potentially be applied to other types of neural networks.
- Why unresolved: The paper does not explore the applicability of LAE to other types of neural networks.
- What evidence would resolve it: Experiments applying LAE to different types of neural networks and measuring the impact on their quantization performance.

### Open Question 3
- Question: How does the choice of the activation value ranges (v0 and v1) affect the performance of the layer-wise activation quantization strategy?
- Basis in paper: [explicit] The paper sets v0 as 15 and v1 as 150 for the investigated LLMs, but it does not explore the impact of different choices of these ranges.
- Why unresolved: The paper does not explore the impact of different choices of v0 and v1 on the quantization performance.
- What evidence would resolve it: Experiments varying the values of v0 and v1 and measuring the impact on the quantization accuracy and performance.

## Limitations

- Effectiveness of logarithmic activation equalization relies heavily on the assumption that outliers dominate specific activation channels, but this distribution characteristic is not universally verified across different LLM architectures
- The layerwise quantization strategy thresholds (v0=15, v1=150) appear empirically chosen without systematic sensitivity analysis, raising questions about their generalizability to other models
- Experimental validation is limited to BLOOM and LLaMA series models, with no testing on encoder-decoder architectures or multimodal models

## Confidence

**High Confidence**: The fundamental observation that activation ranges vary dramatically across LLM layers is well-supported by the presented data. The claim that W4A8 quantization enables efficient deployment through 8-bit matrix computation and 4-bit weight storage is technically sound and directly implementable.

**Medium Confidence**: The effectiveness of logarithmic activation equalization for medium-range layers has theoretical justification but lacks direct empirical validation against alternative approaches. The assertion that groupwise weight quantization addresses quantization difficulty without engineering overhead is plausible but not independently verified.

**Low Confidence**: The specific threshold values (v0=15, v1=150) for layer classification are presented without sensitivity analysis or justification for why these particular values were chosen. The claim that FPTQ achieves state-of-the-art W4A8 performance relative to all competing methods is difficult to verify without comprehensive comparison across the entire quantization literature.

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically vary the classification thresholds (v0 and v1) across a wider range of values (e.g., 10-20 for v0, 100-200 for v1) and measure the impact on both accuracy retention and hardware acceleration benefits. This would validate whether the chosen thresholds are optimal or merely reasonable.

2. **Alternative Outlier Handling Comparison**: Implement and compare LAE against alternative outlier handling strategies such as winsorization, outlier-clipping, or per-channel dynamic quantization on the same medium-range layers. This would provide direct evidence for LAE's effectiveness beyond theoretical justification.

3. **Architecture Generalization Test**: Apply FPTQ to encoder-decoder models (like T5 or BART) and multimodal architectures (like LLaVA) to verify that the layerwise quantization strategy and LAE technique generalize beyond the decoder-only transformer architectures tested in the paper.