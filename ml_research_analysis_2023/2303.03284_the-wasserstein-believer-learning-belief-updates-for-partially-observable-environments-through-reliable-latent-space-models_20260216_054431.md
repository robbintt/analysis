---
ver: rpa2
title: 'The Wasserstein Believer: Learning Belief Updates for Partially Observable
  Environments through Reliable Latent Space Models'
arxiv_id: '2303.03284'
source_url: https://arxiv.org/abs/2303.03284
tags:
- belief
- latent
- state
- learning
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Wasserstein Believer, an RL algorithm
  for POMDPs that learns a latent model and approximates the belief update function
  using the Wasserstein distance. The method comes with theoretical guarantees on
  the quality of the approximation, ensuring the learned beliefs allow for optimal
  value function learning.
---

# The Wasserstein Believer: Learning Belief Updates for Partially Observable Environments through Reliable Latent Space Models

## Quick Facts
- **arXiv ID:** 2303.03284
- **Source URL:** https://arxiv.org/abs/2303.03284
- **Reference count:** 40
- **Primary result:** Introduces an RL algorithm for POMDPs that learns a latent model and approximates belief updates using Wasserstein distance with theoretical guarantees

## Executive Summary
The Wasserstein Believer addresses the challenge of decision-making in partially observable environments by learning a latent model of the POMDP and approximating the belief update function. Unlike traditional approaches that use recurrent networks to maintain beliefs, this method employs feed-forward networks with theoretical guarantees on the quality of belief approximations. The algorithm learns a latent MDP using a Wasserstein Auto-encoder framework and optimizes a belief encoder to approximate the true belief update, ensuring the learned beliefs allow for optimal value function learning.

## Method Summary
The Wasserstein Believer learns a latent MDP model of the POMDP through a Wasserstein Auto-encoded MDP (WAE-MDP) framework. It then trains a belief encoder network to approximate the belief update function by minimizing the Wasserstein distance between the true belief update and the learned approximation. The method uses KL divergence as a computationally tractable proxy for Wasserstein distance during optimization. The algorithm employs a round-robin learning procedure where the latent model, belief encoder, and policy are trained alternately, with the policy learning based on latent beliefs rather than raw observations.

## Key Results
- Learns a latent MDP model of the POMDP and approximates belief updates with theoretical guarantees
- Uses feed-forward networks instead of RNNs for belief updates, avoiding backpropagation through time
- Experiments on small POMDP environments show effectiveness in tasks requiring long-term and short-term memory
- Outperforms baseline methods on simple memory-based POMDP tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The belief update function can be approximated in a latent space while preserving the ability to learn optimal policies.
- Mechanism: By learning a latent MDP model of the POMDP and minimizing the Wasserstein distance between the true belief update and the learned belief encoder, the algorithm creates a latent belief space that is behaviorally equivalent to the original POMDP.
- Core assumption: The latent POMDP learned through WAE-MDP is close enough to the original POMDP that policies optimized in the latent belief space will perform well in the original.
- Evidence anchors:
  - [abstract]: "learns a latent model of the POMDP and an approximation of the belief update. Our approach comes with theoretical guarantees on the quality of our approximation ensuring that our outputted beliefs allow for learning the optimal value function."
  - [section 3.2]: "We provide theoretical guarantees ensuring that our latent belief learner... outputs a belief encoding adapted to learn the value function."
  - [corpus]: Found 25 related papers, but none specifically discuss theoretical guarantees on belief approximation quality. Weak evidence.
- Break condition: If the latent model learned by WAE-MDP diverges significantly from the true POMDP dynamics, the belief approximation will fail to preserve optimality guarantees.

### Mechanism 2
- Claim: Using Normalizing Flows allows for complex belief distributions that can accurately represent the state uncertainty in POMDPs.
- Mechanism: Masked Auto-Regressive Flows (MAF) transform sub-beliefs (vector representations of belief parameters) into full belief distributions, enabling the representation of multi-modal and complex belief distributions necessary for POMDPs.
- Core assumption: The belief distribution in the POMDP is sufficiently complex that simple parametric forms (like Gaussian) are inadequate.
- Evidence anchors:
  - [section 4]: "To accommodate complex belief distributions, we use Masked Auto-Regressive Flows (MAF)... Unlike the current state-of-the-art algorithms the beliefs are only optimized towards accurately representing the current state distribution and following the belief-update rule."
  - [corpus]: Found 25 related papers, but none specifically discuss using Normalizing Flows for belief representation in POMDPs. Weak evidence.
- Break condition: If the belief distribution in the target POMDP is simple (e.g., unimodal Gaussian), the complexity of MAF may be unnecessary and could hurt performance.

### Mechanism 3
- Claim: The combination of on-policy KL divergence and belief regularizers ensures the learned beliefs follow the true belief update rule while being suitable for policy learning.
- Mechanism: The algorithm minimizes KL divergence between the learned belief encoder and the true belief update, along with reward and transition regularizers that align the latent belief space with the original POMDP dynamics.
- Core assumption: KL divergence is a good proxy for Wasserstein distance when the learned model is accurate enough.
- Evidence anchors:
  - [section 4]: "Setting ð· as the Wassertein between the true latent belief update and our belief encoder leads to the following loss... As an alternative to the Wasserstein optimization, we minimize the KL divergence between the two distributions."
  - [section 3.2]: "Setting ð· as the Wassertein between the true latent belief update and our belief encoder leads to the following loss... we argue that the following reward and transition regularizers are required to bound the gap..."
  - [corpus]: Found 25 related papers, but none specifically discuss using KL divergence as a proxy for Wasserstein in belief approximation. Weak evidence.
- Break condition: If the latent model is inaccurate, KL divergence may not properly approximate Wasserstein distance, leading to poor belief updates.

## Foundational Learning

- Concept: Belief states as sufficient statistics for history in POMDPs
  - Why needed here: The entire algorithm is built on maintaining and updating beliefs instead of raw histories, which would be intractable due to exponential growth.
  - Quick check question: Can you explain why beliefs are sufficient statistics for histories in POMDPs and how they enable tractable decision-making?

- Concept: Wasserstein distance and its properties
  - Why needed here: The algorithm uses Wasserstein distance (and its proxy KL divergence) to measure the quality of belief approximations and provide theoretical guarantees.
  - Quick check question: What is the Kantorovich-Rubinstein duality for Wasserstein distance and how does it relate to Lipschitz functions?

- Concept: Normalizing Flows and Masked Auto-Regressive Flows
  - Why needed here: These are used to represent complex belief distributions that cannot be captured by simple parametric forms.
  - Quick check question: How do Masked Auto-Regressive Flows transform simple distributions into complex ones, and why is this useful for belief representation?

## Architecture Onboarding

- Component map: WAE-MDP (state embedding, latent transitions, rewards, observation function) -> Belief Encoder -> MAF (belief distributions) -> Policy Network
- Critical path: WAE-MDP training â†’ Belief Encoder training â†’ Policy learning
- Design tradeoffs:
  - Using feed-forward networks instead of RNNs for belief updates avoids backpropagating through time but requires careful handling of temporal dependencies
  - KL divergence as proxy for Wasserstein is computationally easier but only provides guarantees when divergence approaches zero
  - Assuming state access during training enables learning the latent model but may not be realistic in all scenarios
- Failure signatures:
  - Belief loss not decreasing: Indicates issues with belief encoder or latent model accuracy
  - Policy performance poor despite low belief loss: Suggests misalignment between latent and true POMDP dynamics
  - Training instability: May indicate problems with the KL divergence approximation or learning rate issues
- First 3 experiments:
  1. Implement the Repeat Previous environment and verify the belief encoder can memorize and retrieve observations correctly
  2. Test the stateless Cart-Pole environment to validate short-term memory for control tasks
  3. Compare performance against R-A2C and DVRL baselines on the POPGym suite to establish baseline effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of discrepancy measure (e.g., Wasserstein distance vs. KL divergence) affect the quality of the belief update approximation and the resulting policy performance?
- Basis in paper: [explicit] The paper discusses using Wasserstein distance as the theoretical loss for belief update, but uses KL divergence as a proxy due to optimization challenges.
- Why unresolved: The paper does not provide experimental results comparing the two discrepancy measures.
- What evidence would resolve it: Experiments comparing policy performance and belief update quality using Wasserstein distance versus KL divergence.

### Open Question 2
- Question: What is the impact of the temperature parameter Î» in the WAE-MDP framework on the quality of the learned latent model and the resulting policy?
- Basis in paper: [explicit] The paper mentions that the theoretical guarantees hold as the temperature parameter Î» goes to zero, but does not explore the practical implications of different Î» values.
- Why unresolved: The paper does not provide experimental results or analysis on the effect of Î» on model quality or policy performance.
- What evidence would resolve it: Experiments varying the temperature parameter Î» and evaluating its impact on model quality and policy performance.

### Open Question 3
- Question: How does the belief updater's performance scale with the complexity and size of the POMDP environment?
- Basis in paper: [inferred] The paper presents experiments on small POMDP environments, but does not address scalability to larger, more complex environments.
- Why unresolved: The paper does not provide experimental results or analysis on the performance of the belief updater in larger, more complex environments.
- What evidence would resolve it: Experiments evaluating the belief updater's performance in a range of POMDP environments with varying complexity and size.

## Limitations

- Theoretical guarantees on belief approximation quality rely on assumptions about latent model accuracy that are difficult to verify in practice
- Empirical validation is limited to small-scale environments, raising questions about scalability
- Using KL divergence as a proxy for Wasserstein distance introduces uncertainty as the equivalence only holds when divergence approaches zero

## Confidence

- Theoretical guarantees on belief approximation: Medium
- Effectiveness of KL divergence as Wasserstein proxy: Low-Medium
- Normalizing Flows necessity: Low-Medium

## Next Checks

1. Validate belief approximation quality by comparing learned beliefs against ground truth beliefs in simple POMDPs with known belief dynamics
2. Test algorithm performance when the true belief distribution is unimodal to assess if Normalizing Flows complexity is justified
3. Evaluate scalability by testing on larger POMDP environments with more complex state and observation spaces