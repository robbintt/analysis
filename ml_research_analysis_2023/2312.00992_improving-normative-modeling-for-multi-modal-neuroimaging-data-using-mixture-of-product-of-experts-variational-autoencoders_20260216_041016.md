---
ver: rpa2
title: Improving Normative Modeling for Multi-modal Neuroimaging Data using mixture-of-product-of-experts
  variational autoencoders
arxiv_id: '2312.00992'
source_url: https://arxiv.org/abs/2312.00992
tags:
- latent
- deviations
- normative
- brain
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses limitations in existing multimodal variational
  autoencoder (VAE)-based normative models for neuroimaging, which can lead to uninformative
  joint latent distributions when aggregating information across multiple imaging
  modalities. The authors propose a Mixture-of-Product-of-Experts (MoPoE) approach
  that combines the strengths of Product-of-Experts (PoE) and Mixture-of-Experts (MoE)
  to learn a more informative joint latent posterior.
---

# Improving Normative Modeling for Multi-modal Neuroimaging Data using mixture-of-product-of-experts variational autoencoders

## Quick Facts
- arXiv ID: 2312.00992
- Source URL: https://arxiv.org/abs/2312.00992
- Reference count: 0
- Key outcome: MoPoE normative model achieved highest likelihood ratio for outlier detection compared to baselines when identifying AD patients from healthy controls using multimodal neuroimaging data.

## Executive Summary
This paper addresses limitations in existing multimodal variational autoencoder (VAE)-based normative models for neuroimaging, which can produce uninformative joint latent distributions when aggregating information across multiple imaging modalities. The authors propose a Mixture-of-Product-of-Experts (MoPoE) approach that combines the strengths of Product-of-Experts (PoE) and Mixture-of-Experts (MoE) to learn a more informative joint latent posterior. Their MoPoE normative model is trained on multimodal neuroimaging data (T1-weighted MRI and AV45 PET) from healthy controls and quantifies deviations of Alzheimer's disease (AD) patients from the normative distribution in the joint latent space. Clinical validation showed that the latent deviations were sensitive to AD stages and significantly associated with cognition scores.

## Method Summary
The method involves training a multimodal VAE with modality-specific encoders and decoders, where latent posteriors are aggregated using the proposed MoPoE approach. The model is trained on 248 cognitively unimpaired subjects and 726 AD patients from ADNI, using regional gray matter volumes from T1 MRI and SUVR values from AV45 PET across 90 brain regions. Age and sex are included as covariates. Deviations are quantified using Mahalanobis distance in the joint latent space, and results are compared against baseline methods including MoE, PoE, gPoE, MRI-only, amyloid-only, concatenated, mmJSD, JMVAE, and MVTCAE approaches.

## Key Results
- MoPoE achieved the highest likelihood ratio for detecting outliers compared to baseline methods, indicating better performance in identifying disease subjects with abnormal deviations
- Latent deviations showed greater sensitivity to AD stages compared to feature-space deviations
- Latent deviations were significantly associated with cognition scores, demonstrating clinical relevance
- Interpretability analysis identified specific latent dimensions and brain regions (frontal, temporal, hippocampal, and amygdala) with abnormal deviations associated with AD pathology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MoPoE aggregates unimodal posteriors by first taking PoE within subsets then MoE across subsets, producing a joint posterior sharper than either PoE or MoE alone.
- Mechanism: Subsets of unimodal experts are combined with PoE to form sharp within-subset estimates; these are then mixed via MoE to preserve multimodality without domination by overconfident experts.
- Core assumption: The true joint posterior can be well-approximated by a mixture of products of subsets of unimodal posteriors.
- Evidence anchors:
  - [abstract] "MoPoE combines the strengths of both MoE and PoE, leading to more informative joint latent distribution."
  - [section 2.2] "This combines the strengths of both MoE and PoE while addressing their weaknesses."
- Break condition: If the subsets are too small, PoE step yields overconfident estimates; if too large, MoE step flattens the distribution.

### Mechanism 2
- Claim: Mahalanobis distance in the latent space captures correlated deviations more accurately than Euclidean distance in feature space.
- Mechanism: By using the covariance structure of healthy controls' latent vectors, the Mahalanobis distance normalizes for inter-latent correlations and yields a scale-invariant measure of abnormality.
- Core assumption: The latent space covariance estimated from healthy controls is representative of the normative distribution and stable across test subjects.
- Evidence anchors:
  - [section 2.4] "Mahalanobis distance... which accounts for correlations between latent vectors."
  - [section 3.3] "Latent deviations Dml achieved greater likelihood ratios compared to feature-space deviations Dmf."
- Break condition: If the covariance estimate is ill-conditioned (e.g., small sample size), the distance metric becomes unstable.

### Mechanism 3
- Claim: Mapping statistically significant latent dimensions to feature-space deviations isolates brain regions most affected by AD pathology.
- Mechanism: Only latent dimensions with significant Zml scores are used to reconstruct features, ensuring that resulting Zmf maps reflect pathology-related variation rather than noise.
- Core assumption: Significant latent deviations correspond to meaningful pathology and their reconstruction captures interpretable feature changes.
- Evidence anchors:
  - [section 3.5.1] "We passed these selected latent vectors through the decoders setting the remaining latent dimensions and covariates to be 0..."
  - [section 3.5.2] "Region-level group differences for MRI atrophy were most evident within the frontal, temporal and hippocampal and amygdala regions..."
- Break condition: If latent dimensions are not truly pathology-specific, mapping introduces confounding variance.

## Foundational Learning

- Concept: Variational Autoencoder ELBO formulation
  - Why needed here: The normative model is trained by maximizing the ELBO; understanding the trade-off between reconstruction and KL divergence is critical for tuning.
  - Quick check question: What happens to the ELBO if the KL term dominates during training?

- Concept: Product-of-Experts vs Mixture-of-Experts
  - Why needed here: MoPoE is built on both PoE and MoE; knowing their individual limitations explains why MoPoE improves performance.
  - Quick check question: In PoE, which expert dominates the joint posterior if one has much lower variance?

- Concept: Mahalanobis distance and statistical significance testing
  - Why needed here: Outlier detection relies on Mahalanobis distance; understanding p-value thresholds and FDR correction is essential for interpreting results.
  - Quick check question: Why is Mahalanobis distance preferred over Euclidean distance when latent dimensions are correlated?

## Architecture Onboarding

- Component map: Two modality-specific encoders → MoPoE aggregator → shared latent space → two modality-specific decoders → reconstruction + deviation metrics
- Critical path: Encoder → MoPoE → latent space → decoders → reconstructions; deviations computed after full forward pass
- Design tradeoffs: MoPoE adds computation but yields sharper latent posteriors; simpler PoE/MoE are faster but less informative
- Failure signatures: If latent deviations are uniform across subjects, likely covariance estimation failure; if reconstructions are poor, encoder/decoder capacity may be insufficient
- First 3 experiments:
  1. Train MoPoE with d=5 latent dimensions and verify ELBO convergence; compare to MoE baseline
  2. Compute Mahalanobis distances on held-out healthy controls to ensure within-normative distribution
  3. Map latent deviations to feature space and visualize Zmf maps for AD vs control groups

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MoPoE compare to other advanced multimodal VAE approaches (like mmJSD, JMVAE, MVTCAE) when using different numbers of latent dimensions?
- Basis in paper: [explicit] The paper compares MoPoE to baselines including mmJSD, JMVAE, and MVTCAE, but only reports results for d=5,10,15,20. It would be valuable to understand performance across a wider range of latent dimensions.
- Why unresolved: The paper only shows results for 4 specific latent dimension values (5,10,15,20), limiting our understanding of how MoPoE scales with different dimensionalities compared to other methods.
- What evidence would resolve it: Comprehensive comparison of all models across a wider range of latent dimensions (e.g., 2,5,10,15,20,25,30) showing likelihood ratios for each configuration.

### Open Question 2
- Question: What is the impact of different subset sizes in the MoPoE formulation on outlier detection performance?
- Basis in paper: [inferred] The paper uses MoPoE but doesn't explore how varying the subset sizes Xk in Equation 2 affects performance. The choice of subset sizes could significantly impact the aggregation of information across modalities.
- Why unresolved: The paper uses the full MoPoE formulation but doesn't investigate how different subset sizes or specific subsets affect the model's ability to detect outliers or the quality of the joint latent distribution.
- What evidence would resolve it: Systematic experiments varying subset sizes in MoPoE and comparing outlier detection performance to understand the optimal subset configuration.

### Open Question 3
- Question: How does the proposed MoPoE normative model perform on other neurodegenerative diseases beyond Alzheimer's Disease?
- Basis in paper: [explicit] The paper focuses exclusively on ADNI data for Alzheimer's Disease. While the method is presented as generalizable, its performance on other diseases is not evaluated.
- Why unresolved: The clinical validation and outlier detection performance are only demonstrated on AD data. The generalizability of the approach to other neurodegenerative conditions remains untested.
- What evidence would resolve it: Application of the MoPoE normative model to other neurodegenerative disease datasets (e.g., Parkinson's disease, multiple sclerosis) with comparative performance metrics.

## Limitations
- The exact network architecture details beyond stated layer sizes remain unspecified, including activation functions and computational handling of the power set of modalities in MoPoE
- Results are based on ADNI data with specific preprocessing and regional parcellation, limiting generalization to other datasets or parcellation schemes
- While age and sex are included as covariates, the conditioning mechanism and whether additional relevant covariates should be included is unclear

## Confidence
- High Confidence: The mechanism by which MoPoE improves upon PoE and MoE alone; the superiority of Mahalanobis distance over Euclidean distance for correlated latent spaces; clinical validation showing latent deviations correlate with AD stages and cognition
- Medium Confidence: The interpretability of mapped feature-space deviations to specific brain regions; the stability of covariance estimates in the Mahalanobis calculation; the generalizability to other neurodegenerative conditions
- Low Confidence: The optimal number of latent dimensions for different applications; the sensitivity of results to specific preprocessing choices; the performance of MoPoE with more than two modalities

## Next Checks
1. **Robustness Testing**: Evaluate MoPoE performance across multiple train/test splits and with different regional parcellation schemes to assess stability of outlier detection and interpretability results.
2. **Covariate Sensitivity Analysis**: Test the model with varying numbers and types of covariates to determine their impact on latent deviations and clinical correlations.
3. **Cross-Dataset Validation**: Apply the trained MoPoE model to an independent dataset (e.g., AIBL, OASIS) to verify generalizability of AD detection and brain region identification beyond ADNI.