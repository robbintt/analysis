---
ver: rpa2
title: Active Preference Inference using Language Models and Probabilistic Reasoning
arxiv_id: '2312.12009'
source_url: https://arxiv.org/abs/2312.12009
tags:
- product
- questions
- question
- reward
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an inference-time algorithm that helps LLMs
  efficiently infer user preferences by using more informative questions. The algorithm
  uses a probabilistic model whose conditional distributions are defined by prompting
  an LLM, and returns questions that optimize expected entropy and expected model
  change.
---

# Active Preference Inference using Language Models and Probabilistic Reasoning

## Quick Facts
- arXiv ID: 2312.12009
- Source URL: https://arxiv.org/abs/2312.12009
- Reference count: 40
- Key outcome: LLM-based algorithm outperforms baselines using fewer user interactions in web shopping preference inference

## Executive Summary
This paper proposes an inference-time algorithm that helps large language models efficiently infer user preferences by selecting more informative questions. The approach uses a probabilistic model whose conditional distributions are defined by prompting an LLM, then selects questions that optimize expected entropy reduction or expected model change. In a simplified interactive web shopping setting with real product items, the LLM equipped with the entropy reduction algorithm outperforms both vanilla instruction-tuned LLM and ReAct baselines while using fewer user interactions.

## Method Summary
The method uses an LLM to define a probabilistic model over products, questions, and answers, then selects questions that maximize expected information gain. Specifically, it samples a finite set of questions from a proposal distribution generated by the LLM, computes expected entropy reduction for each using the probabilistic model, and selects the question with highest expected information gain. The approach assumes a binary target reward function and leverages the mathematical equivalence between expected entropy minimization and expected model change maximization in this setting.

## Key Results
- The entropy reduction algorithm achieves higher average expected binary and soft rewards than baselines at all numbers of questions
- Performance improves with more questions, demonstrating the effectiveness of the information-theoretic approach
- The method requires fewer user interactions compared to vanilla instruction-tuned LLM and ReAct baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using LLM-defined probabilistic models to select questions improves preference inference efficiency.
- Mechanism: The LLM defines conditional distributions for how likely a user is to answer a question in certain ways, given a product. This allows calculation of expected entropy reduction or model change for each candidate question.
- Core assumption: The LLM's generated scores for answers are meaningful estimates of the true conditional probabilities.
- Evidence anchors:
  - [abstract] "Our algorithm uses a probabilistic model whose conditional distributions are defined by prompting an LLM, and returns questions that optimize expected entropy and expected model change."
  - [section 3.1] "p(a|x, q) = a binary 0/1 score output by prompting an LLM whether the answer a to the question q is consistent with the product description of x"
- Break condition: If the LLM's conditional probability estimates are inaccurate, the selected questions may not actually maximize information gain.

### Mechanism 2
- Claim: Entropy reduction and model change maximization objectives are equivalent in this setting.
- Mechanism: For binary reward functions, minimizing expected entropy of the posterior distribution over products is mathematically equivalent to maximizing the expected KL divergence between posterior and prior distributions.
- Core assumption: The target reward function is binary (1 for the target product, 0 elsewhere).
- Evidence anchors:
  - [section 3.2] "In our specific setting, it can be shown that expected entropy minimization and expected model change maximization are in fact equivalent."
  - [section 3.1] "To develop our model, we make an assumption that the target reward function R* is a binary function that is 1 only at the target decision x* âˆˆ X and 0 everywhere else."
- Break condition: If the reward function is not binary (e.g., multi-valued or continuous), this equivalence breaks down.

### Mechanism 3
- Claim: Sampling a finite set of questions from the LLM's proposal distribution is sufficient for good performance.
- Mechanism: Instead of evaluating all possible questions, the algorithm samples a finite set (e.g., 8 questions) from the LLM's proposal distribution r(q|c) and selects the best among them based on the information gain objective.
- Core assumption: The proposal distribution r(q|c) is a good approximation of the true distribution over informative questions.
- Evidence anchors:
  - [section 3.1] "We define a question proposal distribution r(q) by asking an LLM to generate a finite number of possible questions given all product information X."
  - [section 4] "Our entropy reduction method samples 8 questions from the proposal distribution r(q|c)"
- Break condition: If the proposal distribution is too narrow or misses important question types, the sampled questions may not include truly optimal ones.

## Foundational Learning

- Concept: Probabilistic reasoning and Bayesian inference
  - Why needed here: The algorithm uses Bayesian updating to refine beliefs about the target product based on user answers, requiring understanding of how to compute posteriors from priors and likelihoods.
  - Quick check question: If p(x) is uniform over 10 products and p(a|x,q) is 1 for the true product and 0 otherwise, what is p(x|q,a) for a "yes" answer to a question true for only product 3?

- Concept: Information theory and entropy
  - Why needed here: The algorithm selects questions based on expected entropy reduction, requiring understanding of how entropy measures uncertainty and how conditional entropy relates to information gain.
  - Quick check question: If p(x) is uniform over 4 products and a question splits them evenly, what is the expected entropy of p(x|q,a)?

- Concept: Active learning and experimental design
  - Why needed here: The algorithm is an instance of active learning, where the learner chooses queries to maximize information gain, requiring understanding of how to design informative experiments.
  - Quick check question: If you have a uniform prior over 8 products and can ask one yes/no question, what question maximizes expected information gain?

## Architecture Onboarding

- Component map:
  LLM prompts for conditional distributions -> Probabilistic model computation -> Question selection algorithm -> User interaction loop

- Critical path:
  1. Sample questions from r(q|c)
  2. For each question, compute expected entropy reduction using the probabilistic model
  3. Select question with highest expected information gain
  4. Get user answer and update conversation context
  5. Repeat until confident about target product or question limit reached

- Design tradeoffs:
  - Sampling more questions increases computational cost but may find better ones
  - Using more informative questions reduces user effort but increases LLM inference cost
  - Assuming binary rewards simplifies computation but may not capture all preference structures

- Failure signatures:
  - Poor question selection despite high LLM scores: LLM's conditional probability estimates may be inaccurate
  - Diminishing returns after few questions: Information gain may be too low for remaining uncertainty
  - Baseline outperforming algorithm: Proposal distribution r(q|c) may be too narrow or misaligned

- First 3 experiments:
  1. Compare performance with different numbers of sampled questions (e.g., 4 vs 8 vs 16) to find optimal tradeoff
  2. Test with non-binary reward functions to see if algorithm still provides gains
  3. Evaluate with open-ended questions (requiring approximation of expected information gain) to test algorithm's generality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed inference-time algorithm compare to traditional interactive NLP systems that require task-specific data collection?
- Basis in paper: [explicit] The paper mentions that traditional interactive NLP systems require task-specific data collection, while their LLM-based approach bypasses this process with zero-shot learning capability.
- Why unresolved: The paper does not provide a direct comparison between the proposed method and traditional systems that require task-specific data collection. It only mentions that their approach avoids the need for such data collection.
- What evidence would resolve it: A comparison of the proposed method's performance against traditional interactive NLP systems that require task-specific data collection, using the same web shopping setting or similar tasks.

### Open Question 2
- Question: How sensitive is the proposed method to the number of questions sampled from the proposal distribution r(q|c)?
- Basis in paper: [inferred] The paper mentions that the entropy reduction method samples 8 questions from the proposal distribution r(q|c), but it is not clear how the choice of this number affects the performance of the algorithm.
- Why unresolved: The paper does not provide an analysis of how the number of sampled questions impacts the performance of the entropy reduction algorithm. It only mentions that the method samples 8 questions without discussing the sensitivity to this choice.
- What evidence would resolve it: An ablation study showing the performance of the entropy reduction algorithm with different numbers of sampled questions, to determine the optimal number of questions to sample from the proposal distribution.

### Open Question 3
- Question: How does the proposed method perform in a setting where open-ended questions are allowed?
- Basis in paper: [explicit] The paper mentions that one future direction is to allow open-ended questions, but notes that this would require an approximation to the objective calculation due to the potentially infinite number of possible answers.
- Why unresolved: The paper only evaluates the proposed method in a setting where yes/no questions are allowed, and does not explore the performance of the method when open-ended questions are permitted.
- What evidence would resolve it: An experimental evaluation of the proposed method in a setting where open-ended questions are allowed, comparing its performance to the yes/no question setting and discussing the challenges and benefits of using open-ended questions.

### Open Question 4
- Question: How does the proposed method handle tasks that involve a full prediction of the reward function, rather than assuming a binary target reward function?
- Basis in paper: [explicit] The paper mentions that the proposed method assumes a binary target reward function, and notes that this assumption may not work well for tasks that involve a full prediction of the reward function.
- Why unresolved: The paper does not provide an evaluation of the proposed method on tasks that involve a full prediction of the reward function, and does not discuss how the method could be extended to handle such tasks.
- What evidence would resolve it: An experimental evaluation of the proposed method on tasks that involve a full prediction of the reward function, and a discussion of how the method could be adapted to handle such tasks effectively.

## Limitations

- The method assumes binary reward functions, which may not generalize to tasks requiring full reward function prediction
- Performance depends critically on the LLM's ability to accurately estimate conditional probabilities, which is not validated in the paper
- The proposal distribution's coverage of informative questions is not analyzed, leaving uncertainty about question selection quality

## Confidence

- High confidence: The mathematical formulation of expected entropy reduction and its relationship to KL divergence is well-established
- Medium confidence: The empirical results showing improved performance over baselines are convincing, but limited to a specific domain (web shopping)
- Low confidence: The generalizability of the approach to other preference inference tasks and non-binary reward functions

## Next Checks

1. Test the algorithm with multi-valued reward functions (e.g., rating scales) to verify if the entropy reduction objective still provides meaningful improvements
2. Conduct ablation studies varying the number of sampled questions (e.g., 4, 8, 16) to quantify the tradeoff between computational cost and performance gains
3. Evaluate the quality of the LLM-generated proposal distribution r(q|c) by measuring its coverage of questions that human evaluators would consider informative