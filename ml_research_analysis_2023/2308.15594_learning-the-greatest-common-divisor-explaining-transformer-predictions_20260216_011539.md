---
ver: rpa2
title: 'Learning the greatest common divisor: explaining transformer predictions'
arxiv_id: '2308.15594'
source_url: https://arxiv.org/abs/2308.15594
tags:
- base
- training
- accuracy
- epoch
- predicted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the ability of small transformers to compute
  the greatest common divisor (GCD) of two positive integers. Models are trained on
  pairs of integers uniformly sampled between 1 and 10^6, encoded in different bases,
  and tasked to predict their GCD.
---

# Learning the greatest common divisor: explaining transformer predictions

## Quick Facts
- arXiv ID: 2308.15594
- Source URL: https://arxiv.org/abs/2308.15594
- Reference count: 40
- Small transformers can learn GCD computation with up to 98% accuracy using appropriate base choices and training distributions

## Executive Summary
This paper investigates whether small transformers can compute the greatest common divisor (GCD) of two positive integers. Models are trained on pairs of integers uniformly sampled between 1 and 10^6, encoded in different bases, and tasked to predict their GCD. The study reveals that transformers can achieve high accuracy (up to 98%) when the base B used to represent integers is carefully chosen (e.g. B=30), but performance can drop to 61% in other bases. The key finding is that model predictions are deterministic and fully interpretable - for any two integers with GCD k, the model always predicts the largest product of primes dividing B that also divides k. The paper demonstrates that training distribution dramatically impacts performance, with log-uniform sampling of operands and balanced GCD outcomes significantly improving results.

## Method Summary
The study uses sequence-to-sequence transformers with 4 layers, 512 dimensions, and 8 attention heads, trained on pairs of integers (1-10^6) encoded in bases ranging from 2 to 1024. The models are trained using Adam optimizer with constant learning rate of 10^-5 on batches of 256 examples. Three training distributions are compared: uniform operands/outcomes, log-uniform operands with uniform outcomes, and log-uniform operands with log-uniform outcomes. Performance is evaluated on both natural test sets (uniformly distributed input pairs) and stratified test sets (uniformly distributed GCDs from 1-100).

## Key Results
- Transformers achieve up to 98% accuracy on GCD prediction when using carefully chosen base B (e.g. B=30)
- Model predictions are deterministic, always predicting the largest product of primes dividing B that also divides the true GCD
- Training from log-uniform operands significantly improves performance (73/100 GCD correct vs 27/100 for uniform)
- Balancing training set distribution of GCD from inverse square to log-uniform brings additional boost (91/100 GCD correct)
- Training from uniform distribution of GCD breaks deterministic model behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The transformer learns deterministic mapping rules based on divisibility by factors of the encoding base B.
- Mechanism: For any pair (a, b) with GCD k, the model predicts the largest product of primes dividing B that also divides k.
- Core assumption: Divisibility rules are reflected in the rightmost digits of numbers in base B.
- Evidence anchors:
  - [abstract]: "Model predictions are deterministic and fully interpretable. For any two integers with GCD k, the model always predicts the largest product of primes dividing B that also divides k."
  - [section]: "For instance, in base 2, the model will correctly predict the GCD of 8 = 1000₂ and 12 = 1100₂ (3 and 2 rightmost zeros), as 2² = 4."
  - [corpus]: Weak evidence - corpus mentions transformers and GCD but not base-specific divisibility rules.
- Break condition: If the base B has no small prime divisors, the model cannot leverage this shortcut and performance drops significantly.

### Mechanism 2
- Claim: Training distribution dramatically impacts the number of GCD values the model can correctly predict.
- Mechanism: Log-uniform sampling of operands provides more examples with small operands, enabling memorization of simple cases. Log-uniform outcomes balance the training set by making large GCD more common.
- Core assumption: Models need sufficient examples of simple cases to generalize to complex ones.
- Evidence anchors:
  - [abstract]: "Training from log-uniform operands significantly improves performance, with 73 out of 100 GCD correctly predicted."
  - [section]: "Training models from log-uniform operands signiﬁcantly improves performance, by providing the model with many simple instances."
  - [corpus]: Weak evidence - corpus mentions training and accuracy but not the specific impact of log-uniform distributions.
- Break condition: If outcomes are uniformly distributed, the model stops making deterministic predictions and performance degrades.

### Mechanism 3
- Claim: Large composite bases enable "grokking" of small prime GCD values not dividing B.
- Mechanism: After extensive training, models learn to predict multiples of small primes that don't divide B, through a sudden improvement in accuracy.
- Core assumption: The model can discover general divisibility rules beyond the base-specific shortcuts.
- Evidence anchors:
  - [abstract]: "Large composite bases sometimes exhibit a phenomenon related to grokking, which allows them to learn multiples of small primes not dividing B."
  - [section]: "For the next 100 epochs, training and test losses are flat, and it seems that the model is not learning any more. Yet, at epoch 188, the model begins to predict GCD 3."
  - [corpus]: Weak evidence - corpus mentions grokking but not in the context of prime divisors.
- Break condition: Grokking requires extensive training and specific base properties; it may not occur with all bases or training regimes.

## Foundational Learning

- Concept: Divisibility rules in different bases
  - Why needed here: Understanding how divisibility by factors of B is reflected in the representation of numbers is crucial for interpreting model behavior.
  - Quick check question: In base 10, which numbers are divisible by 2, 5, and 10 based on their rightmost digit?

- Concept: Training distribution effects on learning
  - Why needed here: The paper shows that log-uniform sampling of operands and outcomes dramatically improves performance compared to uniform sampling.
  - Quick check question: How does log-uniform sampling of operands affect the proportion of small vs large GCD values in the training set?

- Concept: Grokking phenomenon in neural networks
  - Why needed here: The paper describes a grokking-like behavior where models suddenly improve after long periods of stagnation, learning prime GCD values not dividing the base.
  - Quick check question: What distinguishes grokking from overfitting, and why might it occur in this GCD learning task?

## Architecture Onboarding

- Component map: Input encoding -> divisibility rule extraction -> class partitioning -> deterministic prediction based on base factors
- Critical path: Input encoding → divisibility rule extraction → class partitioning → deterministic prediction based on base factors
- Design tradeoffs: Small bases have shorter sequences but larger vocabularies; composite bases provide divisibility shortcuts but may require more parameters
- Failure signatures: High accuracy on natural test set but low accuracy on stratified test set indicates the model learned base-specific shortcuts rather than general GCD computation
- First 3 experiments:
  1. Train a model with base 10 and uniform operands/outcomes; observe ~85% accuracy but only 13/100 GCD correct
  2. Train a model with base 30 and log-uniform operands/outcomes; observe ~98% accuracy and 91/100 GCD correct
  3. Train a model with base 1000 and uniform operands/outcomes; observe eventual learning of 95/100 GCD through grokking after ~800 epochs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the three rules with grokking (G1-G3) be generalized to other number-theoretic tasks beyond GCD?
- Basis in paper: [explicit] The paper demonstrates that for GCD, models leverage representation shortcuts to learn divisibility rules and classify pairs according to their GCD, with deterministic predictions following predictable patterns. The paper explicitly asks whether similar principles apply to factorization, modular arithmetic, or other number-theoretic problems.
- Why unresolved: While the paper provides a detailed characterization of GCD learning, it does not explore whether these principles extend to related tasks. Number-theoretic problems like factorization or modular arithmetic may involve different computational structures, requiring new analysis.
- What evidence would resolve it: Experimental results showing whether transformers trained on factorization or modular arithmetic exhibit similar deterministic patterns and whether those patterns can be explained by analogous rules. Additionally, formal proofs connecting the learned representations to specific algorithms would be valuable.

### Open Question 2
- Question: How does the choice of integer representation (base B) interact with the underlying algorithmic structure of the task?
- Basis in paper: [explicit] The paper shows that base choice dramatically affects performance (e.g., 96.8% accuracy for base 420 vs 61.3% for base 31), and that models leverage divisibility shortcuts specific to the base. It also notes that large composite bases enable grokking of small primes.
- Why unresolved: While the paper characterizes the impact of base choice on GCD learning, it does not explain why certain bases are more effective or how this relates to the computational structure of the task. For example, why do composite bases with many small prime factors perform better?
- What evidence would resolve it: Theoretical analysis linking the prime factorization of the base to the ease of learning specific GCD values. Experimental results comparing performance across bases with similar prime factorizations but different structures.

### Open Question 3
- Question: What is the relationship between the training distribution of outcomes and the interpretability of model predictions?
- Basis in paper: [explicit] The paper shows that uniform distributions of outcomes lead to deterministic but changing predictions, while unbalanced distributions lead to stable predictions of the most common value. It also notes that uniform outcomes eventually break determinism entirely.
- Why unresolved: The paper characterizes the behavior under different outcome distributions but does not explain why this occurs or what it reveals about the learning process. For example, why do uniform outcomes lead to chaotic but stable prediction classes?
- What evidence would resolve it: Analysis of the learned representations to understand how outcome distributions shape the partitioning of input pairs. Experiments testing whether alternative training objectives (e.g., entropy regularization) can maintain interpretability while using uniform outcomes.

## Limitations

- The deterministic prediction mechanism only applies when training distribution and base properties align
- Grokking phenomenon requires extensive training (800+ epochs) and specific base properties
- Evaluation relies on a limited test set of 100 GCD values, which may not capture full complexity

## Confidence

- **High confidence**: Transformers can achieve high accuracy on GCD tasks when trained with appropriate distributions and specific base choices
- **Medium confidence**: The deterministic prediction mechanism based on divisibility rules is well-demonstrated, but its applicability to other arithmetic tasks remains uncertain
- **Medium confidence**: The grokking-like behavior for large composite bases is plausible but requires extensive training and specific conditions

## Next Checks

1. Replicate grokking phenomenon: Train transformers with base 1000 and uniform operand distributions for 1000+ epochs, monitoring accuracy on both natural and stratified test sets every 10 epochs to verify the sudden improvement pattern described in the paper.

2. Evaluate generalization to other bases: Test models trained on base 30 with log-uniform distributions on a comprehensive stratified test set (all GCD values from 1-1000), measuring the percentage of correctly predicted GCDs to assess whether the deterministic mechanism extends beyond the small test set.

3. Investigate architecture scaling: Train larger transformers (8 layers, 1024 dimensions) with the same training protocol to determine if the deterministic behavior and high accuracy can be maintained with increased model capacity, providing insights into the scalability of these findings.