---
ver: rpa2
title: Learning without Forgetting for Vision-Language Models
arxiv_id: '2305.19270'
source_url: https://arxiv.org/abs/2305.19270
tags:
- uni00000013
- uni00000026
- uni00000044
- uni00000032
- uni00000033
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in vision-language
  models (VLMs) when learning new classes incrementally. The proposed method, PROOF,
  trains task-specific projections on frozen image/text encoders and uses a cross-modal
  fusion module to adjust embeddings.
---

# Learning without Forgetting for Vision-Language Models

## Quick Facts
- arXiv ID: 2305.19270
- Source URL: https://arxiv.org/abs/2305.19270
- Reference count: 40
- Outperforms state-of-the-art by a substantial margin on nine benchmark datasets in class-incremental learning

## Executive Summary
This paper addresses catastrophic forgetting in vision-language models (VLMs) when learning new classes incrementally. The proposed method, PROOF, trains task-specific projections on frozen image/text encoders and uses a cross-modal fusion module to adjust embeddings. This enables the model to retain former knowledge while incorporating new concepts. Extensive experiments on nine benchmark datasets show that PROOF achieves state-of-the-art performance, outperforming existing methods by a substantial margin in class-incremental learning tasks.

## Method Summary
PROOF addresses catastrophic forgetting in VLMs through three key mechanisms: (1) task-specific projections that expand incrementally while freezing previous ones to preserve old knowledge, (2) a cross-modal fusion module using self-attention to contextualize visual and textual embeddings, and (3) expandable context prompts that capture task-specific information. The method trains on frozen CLIP or BEiT-3 encoders, adding new projection layers for each task while keeping previous ones fixed. During inference, it combines visual and textual matching through the fusion module to produce final classification logits. The approach maintains an exemplar set for rehearsal and achieves state-of-the-art performance on multiple vision-language benchmarks.

## Key Results
- Achieves state-of-the-art performance on nine benchmark datasets in class-incremental learning
- Outperforms existing methods by a substantial margin in accuracy on both seen and unseen classes
- Demonstrates effective preservation of zero-shot performance while adapting to new classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific projections prevent overwriting of old concepts by freezing former projections and expanding new ones.
- Mechanism: When a new task arrives, PROOF initializes new projection layers while keeping projections from previous tasks frozen. This creates a residual learning structure where new concepts are learned without altering the feature space of old classes.
- Core assumption: Pre-trained image/text encoders provide generalizable features that can be effectively mapped to downstream tasks via linear projections without further fine-tuning.
- Evidence anchors:
  - [abstract] "To handle the first challenge, we propose training task-specific projections based on the frozen image/text encoders. When facing new tasks, new projections are expanded and former projections are fixed, alleviating the forgetting of old concepts."
  - [section] "By freezing the image and text encoders, it aligns the downstream features in the projected space, allowing the model to encode the relevant downstream information into projection layers."
  - [corpus] Weak - the corpus papers discuss similar approaches but don't directly confirm this specific mechanism.
- Break condition: If the pre-trained features are not sufficiently generalizable, or if the linear projection cannot adequately map to downstream tasks, catastrophic forgetting will occur.

### Mechanism 2
- Claim: Cross-modal fusion via self-attention contextualizes embeddings by incorporating information from both visual prototypes and textual classifiers.
- Mechanism: The fusion module uses self-attention to jointly adapt query embeddings with context information (visual prototypes and textual features), creating instance-specific embeddings that capture cross-modal relationships.
- Core assumption: Visual and textual embeddings contain complementary information that can be jointly processed to improve classification accuracy beyond simple matching.
- Evidence anchors:
  - [abstract] "For the second challenge, we propose the fusion module to better utilize the cross-modality information. By jointly adjusting visual and textual features, the model can capture semantic information with a stronger representation ability."
  - [section] "We leverage visual prototype features [51] as a useful tool for capturing the common characteristics of each class... Combining prototypes from multiple modalities help the model adapt and fuse information in a cross-modal manner."
  - [corpus] Weak - the corpus doesn't provide direct evidence for this specific self-attention based cross-modal fusion mechanism.
- Break condition: If the self-attention mechanism fails to capture meaningful cross-modal relationships, or if the context information is insufficient, the fusion will not improve performance.

### Mechanism 3
- Claim: Expandable context prompts capture task-specific information incrementally without interfering with previous tasks.
- Mechanism: Each new task gets a newly initialized context prompt that is trained while previous prompts remain frozen, allowing the model to adapt to new tasks while preserving knowledge of old ones.
- Core assumption: Task-specific prompts can effectively encode incremental task characteristics without disrupting the semantic space established by previous tasks.
- Evidence anchors:
  - [abstract] "we also introduce a set of learnable context prompts {c1, · · · , cb}, ci ∈ Rc×d to be optimized as data evolves... Similar to projection layers, we make the context prompts expandable to catch the new characteristics of new tasks."
  - [section] "The context prompts serve as adaptable context information, enhancing the co-adaption."
  - [corpus] Weak - no direct evidence in corpus about context prompt effectiveness.
- Break condition: If context prompts interfere with each other or fail to capture task-specific characteristics, performance will degrade.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why standard fine-tuning fails in incremental learning scenarios is crucial for appreciating PROOF's design.
  - Quick check question: What happens to the weights of a neural network when it is sequentially trained on new tasks without any regularization or rehearsal?

- Concept: Vision-Language Models (VLMs) and contrastive learning
  - Why needed here: PROOF builds on CLIP's architecture, so understanding how visual and textual embeddings are mapped to a shared space is essential.
  - Quick check question: How does CLIP train visual and textual encoders to map them into a shared embedding space?

- Concept: Class-Incremental Learning (CIL) setting
  - Why needed here: The specific constraints of CIL (no access to previous data, need to maintain performance on old classes) drive the design of PROOF.
  - Quick check question: In CIL, what data is available during training for task b, and what is the evaluation goal?

## Architecture Onboarding

- Component map:
  - Frozen image encoder (gi) and text encoder (gt)
  - Task-specific projection layers (Pi, Pt) - one per task
  - Cross-modal fusion module (self-attention)
  - Context prompts (expandable)
  - Exemplar set for rehearsal

- Critical path:
  1. Extract visual prototypes for new classes
  2. Freeze previous projections and prompts, initialize new ones
  3. For each training instance, compute projected features
  4. Construct context from visual prototypes, textual classifiers, and context prompts
  5. Apply cross-modal fusion via self-attention
  6. Compute logits using both visual and textual matching
  7. Update current projections and context prompts with cross-entropy loss

- Design tradeoffs:
  - Using frozen pre-trained encoders trades adaptation capability for preservation of generalizability
  - Linear projections are parameter-efficient but may limit expressiveness
  - Self-attention fusion adds computation but captures cross-modal relationships
  - Exemplar rehearsal maintains performance but requires storage

- Failure signatures:
  - Performance degradation on old classes indicates forgetting
  - Failure to improve on new classes indicates insufficient adaptation
  - Large performance gap between visual and textual matching suggests imbalance in fusion
  - Sensitivity to context prompt length suggests instability in prompt learning

- First 3 experiments:
  1. Test baseline: Sequential fine-tuning of CLIP on a single incremental task to observe forgetting
  2. Test projection-only variant: PROOF without cross-modal fusion to isolate projection benefits
  3. Test fusion-only variant: PROOF with fixed projections to isolate fusion benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does PROOF's performance change when applied to exemplar-free continual learning scenarios?
- Basis in paper: [inferred] The paper acknowledges that using exemplars is a limitation and suggests extending PROOF to exemplar-free scenarios as future work.
- Why unresolved: The current implementation relies on a fixed memory budget for exemplars, and the paper does not provide experimental results for exemplar-free settings.
- What evidence would resolve it: Empirical results comparing PROOF's performance with and without exemplars on benchmark datasets would clarify its effectiveness in exemplar-free scenarios.

### Open Question 2
- Question: What is the impact of different projection layer aggregation methods on PROOF's performance?
- Basis in paper: [inferred] The paper mentions that summation is used for aggregating projection layers but does not explore other methods like concatenation or weighted summation.
- Why unresolved: The paper does not provide experimental comparisons of different aggregation strategies for projection layers.
- What evidence would resolve it: Results from experiments using different aggregation methods (e.g., concatenation, weighted summation) for projection layers would determine the optimal approach.

### Open Question 3
- Question: How does PROOF maintain zero-shot performance while adapting to new classes, and what trade-offs exist?
- Basis in paper: [explicit] The paper introduces PROOF†, a variation that preserves zero-shot performance by using residual projections, but acknowledges a trade-off between adaptivity and generalizability.
- Why unresolved: The paper does not quantify the trade-off between seen and unseen class performance or explore other methods to balance them.
- What evidence would resolve it: Detailed analysis of the trade-off between adaptivity on seen classes and zero-shot performance on unseen classes, including comparisons with other methods, would clarify the balance.

## Limitations
- Reliance on exemplar rehearsal requires additional memory storage
- Performance depends on the generalizability of frozen pre-trained encoders
- Limited evaluation on long task sequences (maximum 10 tasks)

## Confidence

- High confidence in projection mechanism effectiveness (extensive ablation studies provided)
- Medium confidence in cross-modal fusion benefits (fewer ablation studies, but strong performance gains)
- Medium confidence in scalability claims (evaluated on 9 datasets but limited to 10 tasks maximum)

## Next Checks

1. Test PROOF on VLMs with different pre-training objectives (e.g., ALIGN, FLAVA) to verify architecture-agnostic performance
2. Conduct stress tests with longer task sequences (>10 tasks) to evaluate long-term scalability
3. Implement a variant without exemplar rehearsal to assess true incremental learning capability and compare against regularization-based approaches