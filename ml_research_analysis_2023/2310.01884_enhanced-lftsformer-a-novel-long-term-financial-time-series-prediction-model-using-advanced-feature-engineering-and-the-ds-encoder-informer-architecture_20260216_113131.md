---
ver: rpa2
title: 'Enhanced LFTSformer: A Novel Long-Term Financial Time Series Prediction Model
  Using Advanced Feature Engineering and the DS Encoder Informer Architecture'
arxiv_id: '2310.01884'
source_url: https://arxiv.org/abs/2310.01884
tags:
- stock
- forecasting
- data
- time
- informer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces the Enhanced LFTSformer, a hybrid model for
  long-term financial time series forecasting. It combines advanced feature engineering
  with the DS Encoder Informer architecture.
---

# Enhanced LFTSformer: A Novel Long-Term Financial Time Series Prediction Model Using Advanced Feature Engineering and the DS Encoder Informer Architecture

## Quick Facts
- **arXiv ID**: 2310.01884
- **Source URL**: https://arxiv.org/abs/2310.01884
- **Reference count**: 40
- **Primary result**: Hybrid model combining VMD-MIC+FE feature engineering with stacked Informer architecture achieves MAE of 0.049, RMSE of 0.063, and R2 of 0.960 on financial time series prediction

## Executive Summary
The Enhanced LFTSformer is a hybrid deep learning model designed for long-term financial time series forecasting that addresses the limitations of traditional models in capturing complex, non-linear patterns in stock market data. The model integrates advanced feature engineering through Variational Mode Decomposition with Maximal Information Coefficient (VMD-MIC) and Fuzzy Entropy-based complexity assessment, with a modified stacked Informer architecture incorporating multi-scale temporal processing. This approach enables comprehensive perception and extraction of deep-level features from complex financial datasets, resulting in superior prediction accuracy, adaptability, and generality compared to traditional models and other Informer-based architectures.

## Method Summary
The Enhanced LFTSformer combines VMD-MIC+FE feature engineering with a stacked Informer architecture. The feature engineering pipeline decomposes financial time series into quasi-orthogonal intrinsic mode functions using VMD, selects optimal decomposition parameters through MIC analysis, and groups modes by complexity using Fuzzy Entropy. The stacked Informer processes input sequences through three parallel temporal scales (L, L/2, L/4), each with different attention block configurations, before fusing features for decoding. Training employs a dynamic loss function with adaptive robustness parameters and a GC-enhanced Adam optimizer with gradient centralization. The model was evaluated on historical stock data from CNPC (sh.601857) on the Shanghai Stock Exchange, using 29 features including technical indicators and fundamental metrics.

## Key Results
- Achieved MAE of 0.049, RMSE of 0.063, and R2 of 0.960 on test set for long-term financial time series prediction
- Outperformed traditional models and other Informer-based architectures in terms of prediction accuracy, adaptability, and generality
- Demonstrated superior feature extraction capabilities through VMD-MIC+FE pipeline, enabling better capture of complex temporal patterns in financial data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The VMD-MIC+FE feature engineering pipeline enhances model performance by decomposing complex financial time series into interpretable modes and selectively reconstructing them based on complexity and information relevance.
- Mechanism: VMD decomposes the original signal into K Intrinsic Mode Functions (IMFs) which are frequency-specific components. MIC is then used to determine the optimal K by measuring the mutual information between the original signal and the reconstructed signal. FE (Fuzzy Entropy) quantifies the complexity of each IMF, and IMFs are grouped by complexity to create composite features. This process removes noise and redundancy while preserving significant patterns.
- Core assumption: The financial time series can be effectively decomposed into modes with distinct frequency characteristics, and the combination of MIC for parameter selection and FE for complexity-based grouping yields superior features compared to raw data or single decomposition methods.
- Evidence anchors:
  - [abstract]: "VMD-MIC+FE Feature Engineering: The incorporation of sophisticated feature engineering techniques, specifically through the integration of Variational Mode Decomposition (VMD), Maximal Information Coefficient (MIC), and feature engineering (FE) methods, enables comprehensive perception and extraction of deep-level features from complex and variable financial datasets."
  - [section 3.2.1]: "The VMD-MIC method enhances the conventional VMD approach by utilizing the computation of the MICyy0 metric... A MICyy0 value closer to one indicates a higher fidelity in the VMD decomposition, translating to minimal information loss and an optimal decomposition result."
  - [corpus]: No direct evidence in corpus for this specific VMD-MIC combination; this is a novel methodological claim.
- Break condition: If the financial time series is not sufficiently smooth or lacks clear frequency separation, VMD decomposition may fail to produce meaningful IMFs, rendering MIC parameter selection ineffective.

### Mechanism 2
- Claim: The Stacked Informer structure improves long-term forecasting by processing the input sequence at multiple temporal scales and fusing features from different scales before decoding.
- Mechanism: The input sequence L is processed through three parallel paths with decreasing sequence lengths (L, L/2, L/4), each passing through a different number of attention blocks and convolution layers. This multi-scale processing allows the model to capture both fine-grained and coarse-grained temporal patterns. The features from all three paths are then concatenated and fed into the decoder, providing a richer representation than a single-scale approach.
- Core assumption: Long-term financial time series contain patterns at multiple temporal scales, and processing these scales separately before fusion allows the model to capture dependencies that would be missed by a single-scale architecture.
- Evidence anchors:
  - [section 3.2.3]: "The Stacked Informer Structure incorporates a multi-scale input representation, where distinct temporal scales are processed through varied pathways... By integrating features from varied scales into a cohesive feature map, the model effectively considers interactions and dependencies across scales."
  - [abstract]: "The architecture of the original Informer has been modified by adopting a Stacked Informer structure in the encoder... This modification has led to a reduction in the number of attention blocks, thereby enhancing both the training accuracy and speed."
  - [corpus]: No direct evidence in corpus for this specific stacked multi-scale architecture; this appears to be a novel architectural contribution.
- Break condition: If the temporal dependencies in the data are not scale-dependent or if the scale reduction causes loss of critical information, the multi-scale approach may not provide benefits over a single-scale model.

### Mechanism 3
- Claim: The GC-enhanced Adam optimizer with dynamic loss function improves training stability and prediction accuracy by centralizing gradients and adapting the loss function to handle outliers and varying noise levels.
- Mechanism: Gradient Centralization (GC) modifies the gradient update rule by subtracting the mean of the gradient from each element, effectively projecting gradients onto a hyperplane through the origin. This regularization technique stabilizes training. The dynamic loss function incorporates a robustness parameter β that adapts during training, allowing the model to be less sensitive to outliers when β is small and more sensitive when β is large.
- Core assumption: Centralizing gradients improves the conditioning of the optimization problem, and an adaptive loss function that adjusts robustness based on training dynamics can better handle the non-Gaussian noise and outliers common in financial time series.
- Evidence anchors:
  - [section 3.2.4]: "The adaptive loss function... integrates a continuous parameter signifying robustness into the traditional loss function. Throughout the optimization phase of model training, this dynamic loss function refines the robustness parameters in tandem with the loss minimization procedure, enhancing prediction precision."
  - [section 3.2.5]: "The GC technique was pioneered by Yong et al. in 2020... This centralization is believed to foster a more stable and efficient training process."
  - [corpus]: No direct evidence in corpus for combining GC with Adam or for using this specific dynamic loss function in financial forecasting; these appear to be novel contributions.
- Break condition: If the data is already well-conditioned or if the noise characteristics are stable and predictable, the additional complexity of GC and dynamic loss may not provide significant benefits over standard Adam with MSE loss.

## Foundational Learning

- Concept: Variational Mode Decomposition (VMD)
  - Why needed here: VMD is used to decompose the complex, non-stationary financial time series into a set of quasi-orthogonal Intrinsic Mode Functions (IMFs) with specific frequency characteristics, which is essential for extracting meaningful temporal patterns that are not apparent in the raw data.
  - Quick check question: What is the key difference between VMD and Empirical Mode Decomposition (EMD) in terms of how they extract modes from a signal?

- Concept: Maximal Information Coefficient (MIC)
  - Why needed here: MIC is used to determine the optimal number of decompositions (K) in the VMD process by measuring the mutual information between the original signal and the reconstructed signal, ensuring that the decomposition preserves the maximum information content.
  - Quick check question: How does MIC differ from traditional correlation measures when evaluating the relationship between two variables?

- Concept: Transformer-based attention mechanisms
  - Why needed here: The Informer model, which is based on the Transformer architecture, uses self-attention mechanisms to capture long-range dependencies in the time series data, which is crucial for making accurate long-term predictions in financial markets where past events can influence future outcomes over extended periods.
  - Quick check question: What is the main computational bottleneck of standard self-attention in Transformers, and how does the Informer address this issue?

## Architecture Onboarding

- Component map: Raw financial data → VMD-MIC decomposition → Fuzzy Entropy complexity assessment → Feature selection via MIC → Normalized feature vectors → Multi-scale Stacked Informer encoder (three parallel paths) → Feature fusion → Decoder with masked multi-head attention → Prediction head
- Critical path: Data preprocessing (VMD-MIC+FE) → Model forward pass (Stacked Informer) → Loss computation (Dynamic loss) → Gradient computation and centralization → Parameter update (GC-Adam) → Prediction
- Design tradeoffs:
  - VMD vs. EMD: VMD provides better frequency separation but requires parameter selection (K), which MIC addresses; EMD is parameter-free but may suffer from mode mixing.
  - Stacked vs. single-scale Informer: Stacked provides multi-scale feature extraction but increases model complexity and memory usage; single-scale is simpler but may miss cross-scale dependencies.
  - GC vs. standard Adam: GC provides regularization and stability but adds computational overhead; standard Adam is faster but may be less stable on volatile data.
- Failure signatures:
  - Poor VMD decomposition: If MIC cannot find a good K value or if IMFs are not well-separated, the feature engineering step will produce noisy or redundant features, leading to degraded model performance.
  - Attention saturation: If the attention weights become too concentrated or too diffuse, the model may fail to capture meaningful dependencies, resulting in poor predictions.
  - Optimizer instability: If GC is not properly implemented or if the dynamic loss function's β parameter does not adapt correctly, training may diverge or converge slowly.
- First 3 experiments:
  1. Ablation study on VMD-MIC+FE: Train the model with raw features, with VMD-only, with VMD-MIC, and with VMD-MIC+FE to quantify the contribution of each component to prediction accuracy.
  2. Scale sensitivity analysis: Train the Stacked Informer with different numbers of scales (e.g., 1, 2, 3) and different reduction factors to determine the optimal multi-scale configuration for the financial dataset.
  3. Optimizer comparison: Train the model with standard Adam, Adam+GC, and GC-Adam with static vs. dynamic loss functions to evaluate the individual and combined effects of gradient centralization and adaptive loss on training stability and final performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would the Enhanced LFTSformer model be in predicting stock prices for small and medium-sized enterprises (SMEs) compared to large corporations, especially during periods of market volatility or unexpected events?
- Basis in paper: [explicit] The paper identifies the need for future research to focus on predictive modeling for small enterprises, noting that stock prices of smaller firms may be more sensitive and vulnerable to unforeseen events.
- Why unresolved: The current study primarily focuses on large-scale enterprise stocks, and the model's performance on smaller firms, particularly during volatile periods, has not been evaluated.
- What evidence would resolve it: Comparative analysis of the model's performance on both large corporations and SMEs, with a focus on periods of market volatility or unexpected events, would provide insights into its effectiveness across different enterprise sizes.

### Open Question 2
- Question: How would incorporating non-linear optimization techniques in the feature engineering design, such as activation functions in neural networks or dynamically learned weight coefficients, impact the performance and adaptability of the Enhanced LFTSformer model?
- Basis in paper: [inferred] The paper mentions the need for further optimizations in feature engineering, suggesting that the current linear approach could be enhanced by non-linear methods to improve model performance and adaptability.
- Why unresolved: The current study employs a linear approach in feature engineering, and the potential benefits of non-linear optimization techniques have not been explored.
- What evidence would resolve it: Comparative analysis of the model's performance with and without the incorporation of non-linear optimization techniques in feature engineering would provide insights into the impact on performance and adaptability.

### Open Question 3
- Question: How would the use of alternative optimization algorithms, such as F-score and sensitivity analysis, for feature selection impact the performance of the Enhanced LFTSformer model, particularly in terms of reducing data redundancy and improving prediction accuracy?
- Basis in paper: [explicit] The paper identifies the need for further refinements in feature selection, suggesting that the current approach, which only considers correlation factors, could be improved by incorporating metrics like F-score and sensitivity.
- Why unresolved: The current study uses a feature selection approach based solely on correlation factors, and the potential benefits of alternative optimization algorithms have not been evaluated.
- What evidence would resolve it: Comparative analysis of the model's performance with and without the use of alternative optimization algorithms for feature selection would provide insights into the impact on data redundancy and prediction accuracy.

## Limitations
- Single-stock evaluation (CNPC) limits generalizability to broader financial markets and different market microstructures
- Absence of real-time or out-of-distribution testing conditions restricts assessment of practical deployment capabilities
- Computational complexity of the multi-stage pipeline may limit practical deployment in real-time trading scenarios

## Confidence
- **High confidence**: Overall hybrid architecture design and theoretical foundations (VMD, Transformer-based models, gradient centralization have well-established theoretical support)
- **Medium confidence**: Specific implementation details of VMD-MIC+FE pipeline (sufficient algorithmic description but lacks some implementation specifics)
- **Low confidence**: Generalizability of results (single stock from one market limits broader market applicability claims)

## Next Checks
1. **Cross-market validation**: Test the Enhanced LFTSformer on multiple stocks across different exchanges (e.g., NYSE, NASDAQ, LSE) to assess generalization performance and robustness to different market microstructures and regulatory environments.

2. **Benchmark comparison**: Conduct head-to-head comparisons against leading financial forecasting models including LSTM-based approaches, traditional econometric models (ARIMA, GARCH), and other Transformer variants specifically designed for financial time series (Finformer, FEDformer).

3. **Ablation study with alternative decompositions**: Replace VMD with alternative decomposition methods (EMD, EEMD, SSA) while keeping the Informer architecture constant to isolate the contribution of the specific feature engineering pipeline to overall performance improvements.