---
ver: rpa2
title: A Definition of Open-Ended Learning Problems for Goal-Conditioned Agents
arxiv_id: '2311.00344'
source_url: https://arxiv.org/abs/2311.00344
tags:
- learning
- open-ended
- agent
- agents
- goals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper defines open-ended learning (OEL) as a process that
  generates novel elements from time to time over an infinite horizon. The authors
  formalize this elementary property of OEL and apply it to goal-conditioned reinforcement
  learning (GCRL) problems, where goals are generated by an open-ended process.
---

# A Definition of Open-Ended Learning Problems for Goal-Conditioned Agents

## Quick Facts
- arXiv ID: 2311.00344
- Source URL: https://arxiv.org/abs/2311.00344
- Authors: 
- Reference count: 25
- One-line primary result: This paper defines open-ended learning (OEL) as a process that generates novel elements from time to time over an infinite horizon.

## Executive Summary
This paper proposes a foundational definition of open-ended learning (OEL) by isolating novelty generation as the elementary property of open-ended processes. The authors formalize this property for goal-conditioned reinforcement learning (GCRL) problems, distinguishing between first-order and second-order OEL based on whether goals come from a single or multiple goal spaces. The framework aims to provide a clear, minimal definition that can be combined with other properties like lifelong learning and autotelic/teachable agents. However, the paper acknowledges that this definition does not imply performance progress and further work is needed to characterize goal discovery processes and define performance measures.

## Method Summary
The paper presents a theoretical framework for defining open-ended learning problems in the context of goal-conditioned reinforcement learning. The authors isolate novelty generation over infinite horizon as the core property of open-ended processes and formalize this for GCRL problems. They distinguish between first-order and second-order OEL based on goal space complexity, and discuss how to combine OEL with other properties like lifelong learning and autotelic/teachable agents. The approach relies on observer-relative novelty judgments and does not prescribe specific implementation details or evaluation metrics.

## Key Results
- Defines open-endedness as novelty generation over infinite horizon, isolating it from other composite properties
- Distinguishes first-order (single goal space) and second-order (multiple goal spaces) OEL problems
- Proposes combining OEL with lifelong learning and autotelic/teachable properties for a complete developmental AI framework

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The definition of open-endedness as novelty generation over infinite horizon resolves the ambiguity of previous composite definitions.
- **Mechanism:** By isolating novelty as the elementary property, the framework sidesteps the need to enumerate all possible features (autonomy, intrinsic motivation, abstraction, etc.) that previous works bundled together.
- **Core assumption:** Observers can consistently judge novelty, and novelty over infinite horizon implies an unbounded number of distinct tokens.
- **Evidence anchors:**
  - [abstract] "we propose to isolate a key elementary property of open-ended processes, which is to always produce novel elements from time to time over an infinite horizon."
  - [section 3.1] Formal definition: "An observer considers a process as open-ended if, for any time t, there exists a time t′ > t at which the process generates a token that is new according to this observer's perspective."
  - [corpus] No direct corpus evidence; the claim relies entirely on the paper's formal definition.
- **Break condition:** If the observer's novelty criterion is inconsistent or if the process is bounded in time, the open-endedness property fails.

### Mechanism 2
- **Claim:** First-order and second-order OEL problems provide a hierarchy that distinguishes trivial from developmentally interesting novelty.
- **Mechanism:** First-order requires novelty within a single goal space; second-order requires novelty across multiple goal spaces, which better matches developmental AI intuitions about progressive complexity.
- **Core assumption:** Multiple goal spaces reflect richer developmental trajectories than a single infinite goal space.
- **Evidence anchors:**
  - [section 3.3] "A first-order open-ended GCRL problem is a GCRL problem where all goals gi are sampled from the same goal space G."
  - [section 3.3] "A second-order open-ended GCRL problem is a GCRL problem where some open-ended process generates an open-ended sequence of goal representation spaces Ωj ⊂ Ω, and another open-ended process generates goals gi,j within such goal spaces Ωj."
  - [corpus] No direct corpus evidence; the claim is derived from the paper's own classification.
- **Break condition:** If goal spaces do not provide meaningfully different challenges, the distinction collapses into triviality.

### Mechanism 3
- **Claim:** Combining open-endedness with lifelong learning and autotelic/teachable properties yields a complete framework for developmental AI agents.
- **Mechanism:** OEL ensures novelty, lifelong learning ensures no forgetting, and autotelic/teachable properties define where goals come from and how the agent's curriculum evolves.
- **Core assumption:** These properties are complementary and can be composed without conflict.
- **Evidence anchors:**
  - [section 4.2] "A lifelong open-ended GCRL agent is an open-ended GCRL agent which does not forget how to achieve a goal when it is learning how to achieve other goals."
  - [section 4.3] "An open-ended GCRL agent is autotelic if its goal generation process is fully internal to the agent (autonomous)."
  - [section 4.3] "An autotelic GCRL agent is 'teachable' if its internal goal generation process can be influenced by other agents."
  - [corpus] No direct corpus evidence; the claim is a synthesis of the paper's definitions.
- **Break condition:** If the agent cannot balance novelty generation with skill retention, or if goal generation conflicts with social teachability, the framework fails.

## Foundational Learning

- **Concept: Novelty as observer-relative property**
  - Why needed here: The paper's core definition of open-endedness depends on an observer judging novelty, so understanding observer-relative novelty is essential.
  - Quick check question: Can two different observers agree that the same process is open-ended, or might they disagree? Why?

- **Concept: Goal-conditioned reinforcement learning (GCRL)**
  - Why needed here: The framework focuses on OEL within GCRL, so familiarity with goal spaces, reward conditioning, and policy conditioning is required.
  - Quick check question: In GCRL, how does the goal-conditioned reward function differ from the standard reward function?

- **Concept: Lifelong learning vs. continual learning**
  - Why needed here: The paper distinguishes lifelong learning (no forgetting) from continual learning (never stop learning), so understanding both is critical for applying the framework.
  - Quick check question: What is the main practical difference between a lifelong learner and a continual learner in the context of skill retention?

## Architecture Onboarding

- **Component map:**
  - Observer module -> Novelty checker -> Goal generator -> Policy learner -> Memory module -> Social interface (optional)

- **Critical path:**
  1. Generate a new goal.
  2. Check novelty via observer.
  3. Condition policy on goal and history.
  4. Execute policy, receive reward.
  5. Update policy and memory.
  6. Repeat indefinitely.

- **Design tradeoffs:**
  - Novelty detection can be strict (risk missing real novelty) or lenient (risk trivial novelty).
  - Second-order OEL requires managing multiple goal spaces, increasing complexity.
  - Lifelong learning trades off memory capacity for skill retention.
  - Autotelic agents must self-generate curricula; teachable agents depend on social input.

- **Failure signatures:**
  - Novelty detector never flags new goals → agent stalls.
  - Memory module grows without bound → resource exhaustion.
  - Goal generator cycles within a finite set → violates open-endedness.
  - Policy updates overwrite old skills → catastrophic forgetting.

- **First 3 experiments:**
  1. Implement a simple novelty detector (e.g., distance threshold in goal space) and test on a discrete counting goal space.
  2. Extend to second-order OEL: generate two nested goal spaces (e.g., maze size and target location) and verify novelty across both.
  3. Add lifelong learning: train on a sequence of goals, then test zero-shot performance on earlier goals to confirm no forgetting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms or criteria should be used to determine if a learning agent is truly open-ended, given the difficulty of evaluating this property in practice?
- Basis in paper: [explicit] The paper discusses the challenge of evaluating open-ended learning (OEL) agents and mentions the need for methods to determine if an agent is truly OEL.
- Why unresolved: The paper acknowledges that it is difficult to evaluate if an agent is showing open-ended learning because it should address new goals forever, which is not feasible in practice. It suggests measuring the rate of novel goals discovered but does not provide a concrete mechanism.
- What evidence would resolve it: A concrete method or set of criteria that can reliably determine if an agent is exhibiting open-ended learning properties, possibly through a combination of goal discovery rates, novelty measures, and performance metrics.

### Open Question 2
- Question: How can the open-ended learning property be effectively combined with other properties like lifelong learning and autotelic learning to create a comprehensive framework for developmental AI?
- Basis in paper: [explicit] The paper discusses the need to combine the core open-ended learning property with other properties such as lifelong learning and autotelic learning to capture the idea of a growing repertoire of capabilities.
- Why unresolved: While the paper suggests the need for combining these properties, it does not provide a detailed framework or methodology for integrating them into a cohesive system.
- What evidence would resolve it: A detailed framework or model that successfully integrates open-ended learning with lifelong learning and autotelic learning, demonstrating how these properties interact and enhance each other in a developmental AI system.

### Open Question 3
- Question: What are the necessary conditions for an environment to foster open-ended goal generation and space creation by autonomous agents?
- Basis in paper: [inferred] The paper discusses the importance of the environment in open-ended learning, particularly in second-order open-ended GCRL problems where agents generate goal spaces from their own experience.
- Why unresolved: The paper highlights the need for a rich environment but does not specify the exact conditions or features that would support open-ended goal generation and space creation.
- What evidence would resolve it: Empirical studies or theoretical models that identify and validate the specific environmental features and conditions that enable autonomous agents to generate new goals and goal spaces effectively.

## Limitations
- The definition does not imply performance progress, focusing only on novelty generation
- Observer-relative novelty judgments lack empirical validation and may be inconsistent
- The framework remains largely theoretical without concrete implementation or evaluation metrics

## Confidence
- **Medium**: The core novelty definition is logically consistent but relies on observer-relative judgments without empirical validation
- **Low**: The claim that combining OEL with lifelong learning and autotelic/teachable properties yields a complete framework, as this remains largely theoretical
- **Medium**: The distinction between first-order and second-order OEL problems, as the conceptual framework is clear but lacks empirical support

## Next Checks
1. Implement a simple novelty detection mechanism (e.g., distance threshold in goal space) and test on a discrete counting goal space to verify that the open-endedness property holds over time.

2. Extend the novelty detection to a second-order OEL setup with nested goal spaces (e.g., maze size and target location) and empirically verify that novelty is maintained across both spaces.

3. Train an OEL agent on a sequence of goals and test zero-shot performance on earlier goals to confirm that the lifelong learning property (no forgetting) is maintained.