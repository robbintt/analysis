---
ver: rpa2
title: 'Let''s Synthesize Step by Step: Iterative Dataset Synthesis with Large Language
  Models by Extrapolating Errors from Small Models'
arxiv_id: '2310.13671'
source_url: https://arxiv.org/abs/2310.13671
tags:
- data
- dataset
- small
- synthesis
- synthesized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces S3, a novel data synthesis framework designed
  to reduce the distribution gap between synthetic and real-world data in NLP tasks.
  S3 achieves this by iteratively extrapolating the errors made by a small model trained
  on synthesized data using a large language model.
---

# Let's Synthesize Step by Step: Iterative Dataset Synthesis with Large Language Models by Extrapolating Errors from Small Models

## Quick Facts
- arXiv ID: 2310.13671
- Source URL: https://arxiv.org/abs/2310.13671
- Reference count: 16
- Key outcome: S3 achieves performance comparable to models trained on full gold data using only 30.43% of the data on average

## Executive Summary
This paper introduces S3, a novel data synthesis framework that iteratively improves synthetic datasets by extrapolating errors made by small models using a large language model. The method addresses the distribution gap between synthetic and real-world data by training a small model on initial synthesized data, evaluating it on gold validation data, and using the LLM to generate new data that fills the identified gaps. Experiments across multiple NLP tasks show S3 significantly outperforms baseline methods, achieving comparable performance to models trained on full gold datasets while using substantially less data.

## Method Summary
S3 synthesizes training data through an iterative process that begins with generating seed data using an LLM with rationale explanations. A small model is trained on this data and evaluated on a gold validation set. The LLM then extrapolates from the small model's errors to generate additional synthetic data, which is combined with the existing dataset. This process repeats iteratively until performance converges, with each round aiming to reduce the distribution gap between synthetic and real data.

## Key Results
- S3 outperforms ZeroGen and GoldGen baselines by 9.48% and 2.73% on average
- Achieves performance comparable to full gold data models using only 30.43% of the data on average
- Shows consistent improvements across multiple GLUE tasks and sentiment analysis datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: S3 reduces the distribution gap by extrapolating small model errors on a validation set
- Mechanism: Small model trained on synthesized data is evaluated on gold validation data; misclassified examples are fed back to LLM to generate new synthetic data that fills gaps
- Core assumption: LLM can effectively extrapolate from misclassified examples to synthesize data that reduces the distribution gap
- Evidence anchors:
  - [abstract] "S3 achieves this by iteratively extrapolating the errors made by a small model trained on synthesized data using a large language model."
  - [section 2.3] "EES process extrapolates errors made by small models on the real-world validation datasets to synthesize some additional data to fix the error."
- Break condition: If LLM cannot effectively extrapolate from errors, distribution gap will persist

### Mechanism 2
- Claim: Using rationales in seed data synthesis improves synthetic data quality
- Mechanism: LLM generates rationales for each label before synthesizing examples, leading to more diverse and informative synthetic data
- Core assumption: Including rationales in synthesis prompt guides LLM to generate more realistic and task-relevant examples
- Evidence anchors:
  - [abstract] "S3 first synthesizes a seed dataset with an explain-then-generate method that first prompts LLMs to generate rationales for each label"
  - [section 2.2] "This 'explain-then-generate' approach enables us to generate more diverse, informative, and realistic examples."
- Break condition: If LLM fails to generate useful rationales, quality of synthetic data may not improve

### Mechanism 3
- Claim: Iterative refinement through multiple EES rounds gradually reduces the distribution gap
- Mechanism: After each EES round, small model is retrained on expanded dataset, process repeated until performance converges
- Core assumption: Each EES round provides new information that further reduces distribution gap, repeated iterations lead to convergence
- Evidence anchors:
  - [section 2.3] "S3 repeats this process iteratively to gradually reduce the distribution gap and optimize the mixed dataset until convergence."
  - [section 3.3] "S3 even outperforms gold data performance on IMDB and RTE."
- Break condition: If distribution gap cannot be further reduced after few rounds, additional iterations won't improve performance

## Foundational Learning

- Concept: Distributional discrepancy in synthetic data
  - Why needed here: Understanding the gap between synthetic and real data distributions is crucial for designing methods to reduce it
  - Quick check question: What is the main challenge in dataset synthesis that S3 aims to address?

- Concept: Error extrapolation using LLMs
  - Why needed here: The core mechanism of S3 relies on using an LLM to generate new data based on errors made by a small model
  - Quick check question: How does S3 use the LLM to generate new synthetic data after each iteration?

- Concept: Iterative optimization
  - Why needed here: S3's effectiveness depends on repeatedly refining the synthetic dataset through multiple rounds of error extrapolation and retraining
  - Quick check question: Why does S3 perform multiple rounds of error extrapolation instead of just one?

## Architecture Onboarding

- Component map: LLM -> Small Model -> Gold Validation Set -> EES Module -> LLM
- Critical path: 1. LLM generates seed data with rationales 2. Small model trained on seed data 3. Small model evaluated on gold validation set 4. Misclassified examples used for error extrapolation 5. LLM generates new synthetic data based on errors 6. Repeat steps 2-5 until convergence
- Design tradeoffs:
  - Using LLM for error extrapolation vs. direct sampling from gold data
  - Number of EES rounds (computational cost vs. performance)
  - Size of gold validation set (coverage vs. cost)
- Failure signatures:
  - Performance plateaus after initial rounds
  - Generated data is too similar to existing synthetic data
  - LLM fails to generate meaningful rationales
- First 3 experiments:
  1. Implement seed data synthesis with rationales and evaluate diversity
  2. Run one round of EES and measure distribution gap reduction
  3. Compare performance of S3 vs. ZeroGen on a single dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which the iterative error extrapolation process improves the distribution alignment between the synthetic and real data?
- Basis in paper: [explicit] The paper states that S3 iteratively reduces the distribution gap by extrapolating errors made by the small model on a small validation set, but does not provide a detailed mathematical explanation of how this extrapolation specifically improves distribution alignment.
- Why unresolved: While the paper mentions the process of error extrapolation, it does not delve into the specific mathematical or statistical mechanisms that ensure the distribution gap is effectively reduced. Understanding these mechanisms would provide deeper insights into the efficacy of the approach.
- What evidence would resolve it: A detailed mathematical analysis or simulation showing the step-by-step improvement in distribution alignment during each iteration of the error extrapolation process would clarify the mechanism.

### Open Question 2
- Question: How does the quality and diversity of the synthesized data compare to human-annotated data in terms of semantic richness and task-specific nuances?
- Basis in paper: [explicit] The paper discusses the use of rationales and the iterative refinement process, but does not provide a detailed comparison of the semantic richness and task-specific nuances between synthesized and human-annotated data.
- Why unresolved: The paper focuses on quantitative performance metrics but does not address the qualitative aspects of the data, such as semantic richness and task-specific nuances, which are crucial for understanding the true effectiveness of the synthesized data.
- What evidence would resolve it: A qualitative analysis involving human evaluation or advanced NLP techniques to assess the semantic richness and task-specific nuances of the synthesized data compared to human-annotated data would provide insights into the quality of the synthesized data.

### Open Question 3
- Question: How does the performance of S3 vary with different sizes of the initial seed dataset and validation sets?
- Basis in paper: [explicit] The paper mentions the use of seed data synthesis and a small validation set but does not explore how varying the sizes of these datasets impacts the performance of S3.
- Why unresolved: The paper does not investigate the sensitivity of S3 to the size of the seed and validation datasets, which is important for understanding the scalability and robustness of the approach.
- What evidence would resolve it: Experiments varying the sizes of the seed and validation datasets and analyzing the corresponding performance changes would provide insights into the optimal dataset sizes for S3 and its robustness to dataset size variations.

## Limitations
- Limited dataset diversity with primary evaluation on GLUE benchmark
- Weaker improvements on multi-domain sentiment analysis tasks (SCD and OA)
- Lack of ablation studies for rationale generation component

## Confidence
- Error extrapolation mechanism: High
- Distribution gap reduction claims: Medium
- Generalizability to other domains: Low

## Next Checks
1. Perform t-SNE visualization comparing synthetic vs. gold data distributions at each EES iteration to verify the claimed distribution gap reduction
2. Analyze the types of errors corrected in each EES round to confirm that the LLM is effectively extrapolating from small model mistakes rather than simply generating more diverse data
3. Evaluate S3 on additional datasets from different domains (e.g., biomedical or legal text) to test the generalizability of the iterative error extrapolation approach beyond the GLUE benchmark