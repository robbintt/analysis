---
ver: rpa2
title: Uncertainty quantification and out-of-distribution detection using surjective
  normalizing flows
arxiv_id: '2311.00377'
source_url: https://arxiv.org/abs/2311.00377
tags:
- data
- intervention
- shift
- uncertainty
- normalizing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses uncertainty quantification and out-of-distribution
  (OoD) detection in deep learning models, particularly for applications where models
  are trained in one environment but applied to multiple different environments, such
  as in climate science or mobility analysis. The authors propose a simple approach
  using surjective normalizing flows (SNFs) to identify OoD data sets that can be
  computed in a single forward pass.
---

# Uncertainty quantification and out-of-distribution detection using surjective normalizing flows

## Quick Facts
- arXiv ID: 2311.00377
- Source URL: https://arxiv.org/abs/2311.00377
- Authors: 
- Reference count: 40
- Key outcome: SNF achieves significantly lower negative log-likelihoods on both train and test sets compared to Dirichlet process mixture model and bijective flow, and more reliably detects OoD data using simple graphical checks or statistical tests between pairs of distributions of likelihood estimates.

## Executive Summary
This paper addresses uncertainty quantification and out-of-distribution (OoD) detection in deep learning models trained in one environment but applied to multiple different environments. The authors propose using surjective normalizing flows (SNFs) to identify OoD data sets that can be computed in a single forward pass. They apply their method to a synthetic mobility dataset and demonstrate that their approach can reliably discern OoD data from in-distribution data, outperforming Dirichlet process mixture models and bijective flows.

## Method Summary
The method uses surjective normalizing flows (SNFs) that combine dimensionality-reducing surjections with conditional normalizing flows to estimate densities in feature space. The approach builds on recent developments in deep uncertainty quantification and generative modeling with normalizing flows. The authors train a transformer-based next location prediction model to extract features, then fit SNF, bijective normalizing flow, and Dirichlet process mixture models on these features. OoD detection is performed by comparing feature-space likelihood distributions between training and test data using statistical tests (t-tests and Wasserstein distances).

## Key Results
- SNF achieves significantly lower negative log-likelihoods on both train and test sets compared to Dirichlet process mixture model and bijective flow
- SNF more reliably detects OoD data using simple graphical checks or statistical tests between pairs of distributions of likelihood estimates
- The surjections are a crucial component to reliably distinguish in-distribution from OoD data

## Why This Works (Mechanism)

### Mechanism 1
Surjective normalizing flows detect OoD samples by projecting data into a lower-dimensional manifold and computing likelihoods, where low density indicates OoD. SNFs combine dimensionality-reducing surjections with conditional normalizing flows, projecting data into a lower-dimensional space while preserving density information through Jacobian determinants. Low density estimates in this manifold indicate OoD samples. Core assumption: Low density in the projected manifold correlates with being out-of-distribution.

### Mechanism 2
Feature-space regularization ensures the penultimate layer's features are both smooth and sensitive, preventing feature collapse that could obscure OoD detection. Bi-Lipschitz constraint via spectral normalization and residual connections maintains sensitivity to input variations while ensuring smoothness. This prevents the feature space from collapsing to regions that map all data to similar density estimates. Core assumption: Without regularization, neural network features would collapse in a way that obscures OoD detection.

### Mechanism 3
Comparing likelihood distributions between train/test and OoD samples using statistical tests provides a principled OoD detection criterion. After computing feature-space likelihoods for all samples, statistical tests (t-tests, Wasserstein distances) compare the distributions. Significant differences indicate OoD samples. Core assumption: OoD samples produce systematically different likelihood distributions than in-distribution samples.

## Foundational Learning

- Concept: Normalizing flows and their invertibility properties
  - Why needed here: Understanding how normalizing flows transform data and compute densities is essential for implementing SNFs
  - Quick check question: What property of normalizing flows allows them to compute exact likelihoods?

- Concept: Surjections and dimensionality reduction in neural networks
  - Why needed here: SNFs use surjections to project data to lower dimensions while preserving density information
  - Quick check question: How does a surjective layer differ from a bijective layer in a normalizing flow?

- Concept: Bi-Lipschitz constraints and feature space regularization
  - Why needed here: Ensures features are both smooth and sensitive for reliable OoD detection
  - Quick check question: What is the purpose of enforcing a bi-Lipschitz constraint on feature extractors?

## Architecture Onboarding

- Component map: Transformer encoder (from Hong et al.) → Feature extraction → Surjective Normalizing Flow (SNF) → Density estimation in feature space → Statistical test module → OoD detection decision → Spectral normalization layer → Feature space regularization

- Critical path: Input data → Transformer encoder → Feature vector → SNF → Density estimate → Density estimates from train/test vs OoD → Statistical test → OoD decision

- Design tradeoffs:
  - Surjective vs bijective layers: Surjections reduce dimensionality for better OoD detection but add complexity
  - Number of flow layers: More layers increase flexibility but computational cost
  - Dimensionality reduction ratio: Affects sensitivity to OoD patterns

- Failure signatures:
  - Poor OoD detection: Feature space may be collapsing or surjections not preserving density information
  - High computational cost: Too many flow layers or complex conditioners
  - Unstable training: Learning rate too high or spectral normalization too restrictive

- First 3 experiments:
  1. Train SNF on synthetic mobility data and visualize feature-space likelihood distributions for train vs OoD samples
  2. Compare SNF vs bijective flow on same data to verify surjections improve OoD detection
  3. Test sensitivity by varying the dimensionality reduction ratio in surjective layers and measuring OoD detection performance

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the SNF method compare to other state-of-the-art uncertainty quantification methods, such as Monte Carlo dropout or deep ensembles, on the same mobility dataset? The paper compares the SNF to a Dirichlet process mixture model and a bijective flow, but does not mention other popular uncertainty quantification methods.

### Open Question 2
How sensitive is the SNF method to the choice of hyperparameters, such as the number of flow layers, the number of bins in the RQ-NSFs, and the learning rate? The paper mentions that the architecture of the SNF was chosen somewhat arbitrarily and that no hyperparameterization optimization was conducted.

### Open Question 3
How well does the SNF method generalize to other types of data distributions, such as those with different levels of complexity or dimensionality? The paper only tests the SNF method on a synthetic mobility dataset.

## Limitations
- The synthetic nature of the data limits generalizability of the findings to real-world applications
- The paper does not explore the impact of different hyperparameter choices on the performance of the SNF method
- The statistical tests for OoD detection (t-tests, Wasserstein distances) are presented as simple solutions, but their robustness across different data distributions and sample sizes is not thoroughly evaluated

## Confidence

- High: The theoretical framework connecting surjections to density estimation and the implementation of spectral normalization for feature space regularization are well-established techniques.
- Medium: The empirical demonstration on synthetic data convincingly shows the SNF outperforms baselines, but the synthetic nature of the data limits generalizability.
- Low: The statistical tests for OoD detection (t-tests, Wasserstein distances) are presented as simple solutions, but their robustness across different data distributions and sample sizes is not thoroughly evaluated.

## Next Checks

1. **Cross-dataset validation**: Test the SNF approach on established OoD benchmarks (e.g., CIFAR-10 vs CIFAR-100, MNIST vs Fashion-MNIST) to verify performance generalizes beyond synthetic mobility data.

2. **Statistical test robustness**: Systematically vary sample sizes and data distributions to evaluate how robust the t-test and Wasserstein distance approaches are to different OoD detection scenarios.

3. **Surjection sensitivity analysis**: Conduct ablation studies varying the number and dimensionality of surjections in the SNF to identify optimal configurations for different types of OoD data.