---
ver: rpa2
title: 'Social Bias Probing: Fairness Benchmarking for Language Models'
arxiv_id: '2311.09090'
source_url: https://arxiv.org/abs/2311.09090
tags:
- language
- identities
- fairness
- stereotypes
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel framework for probing language models
  for societal biases by assessing disparate treatment across different demographic
  groups. The proposed method uses a perplexity-based fairness score to measure language
  models' associations with various identities.
---

# Social Bias Probing: Fairness Benchmarking for Language Models

## Quick Facts
- **arXiv ID**: 2311.09090
- **Source URL**: https://arxiv.org/abs/2311.09090
- **Reference count**: 10
- **Primary result**: Introduces a novel perplexity-based framework for probing language models for societal biases across diverse demographic groups, revealing larger models exhibit higher bias

## Executive Summary
This paper presents a novel framework for probing language models for societal biases by measuring disparate treatment across demographic groups using perplexity-based fairness scores. The authors introduce the SOFA (Social Fairness) benchmark dataset, which expands beyond binary stereotypical vs. anti-stereotypical associations to capture a full spectrum of identity-stereotype interactions. Through three-dimensional analysis across societal categories, identities, and stereotypes, the framework reveals that larger model variants exhibit higher degrees of bias, with religious identities showing the most pronounced disparate treatment.

## Method Summary
The framework uses perplexity as a proxy for bias measurement, where lower perplexity indicates higher probability of generating stereotypical content. The SOFA dataset is created by combining stereotypes from SBIC with identities from a lexicon, then generating probes through combinatorial pairing. Language models are evaluated by calculating normalized perplexity for each probe, followed by computation of Delta Disparity Scores to measure bias. The framework conducts analysis across intra-identities, intra-stereotypes, intra-categories, and global fairness scores, enabling comprehensive bias detection beyond simplified binary approaches.

## Key Results
- Larger model variants (XLNET-large) exhibit the highest variance in bias, with religions causing the most pronounced disparate treatments
- The framework expands bias evaluation beyond binary stereotypical vs. anti-stereotypical associations, providing more nuanced understanding
- Perplexity-based fairness scores successfully identify bias patterns across multiple societal categories including gender, religion, disability, and nationality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perplexity scores can serve as a proxy for social bias measurement in language models.
- Mechanism: When a language model generates text containing stereotypical content, it assigns lower perplexity (higher likelihood) to the text, indicating higher probability of generating such content. The paper proposes normalizing perplexity by the identity's individual perplexity to account for identity-specific likelihoods.
- Core assumption: Lower perplexity indicates higher probability of generation, and thus higher bias towards that stereotype.
- Evidence anchors:
  - [abstract] "We propose perplexity (Jelinek et al., 1977), a measure of a language model's uncertainty, as a proxy for bias."
  - [section 3.2] "We identify bias manifestations when a model exhibits low PPL values for statements that contain stereotypes, thus indicating a higher probability of their generation."
- Break condition: If perplexity does not correlate with likelihood in a meaningful way for the specific language model architecture, or if the normalization by identity perplexity does not adequately control for identity-specific variations.

### Mechanism 2
- Claim: Expanding bias evaluation beyond binary stereotypical vs anti-stereotypical associations provides a more nuanced understanding of bias.
- Mechanism: By creating probes that combine each identity with each stereotype, the framework captures the full spectrum of how different identities interact with different stereotypes, revealing nuanced patterns that binary tests miss.
- Core assumption: Social biases are not simply binary but exist on a spectrum across multiple identities and stereotypes.
- Evidence anchors:
  - [abstract] "In response to these limitations, we introduce a novel probing framework... expands beyond this oversimplified binary categorization."
  - [section 3.1] "Our approach requires a set of identities from diverse social and demographic groups, alongside an inventory of stereotypes."
- Break condition: If the combinatorial explosion of probes makes the framework impractical, or if the additional complexity does not yield meaningful insights beyond what binary tests provide.

### Mechanism 3
- Claim: Larger model variants exhibit a higher degree of bias.
- Mechanism: As model size increases, the capacity to encode and perpetuate societal biases from training data also increases, leading to stronger associations between identities and stereotypes.
- Core assumption: Model size correlates with bias severity due to increased capacity to encode training data patterns.
- Evidence anchors:
  - [abstract] "In agreement with recent findings, we find that larger model variants exhibit a higher degree of bias."
  - [section 4.2] "XLNET-large emerges as the model with the highest variance. Indeed, prior work identified XLNET to be highly biased compared to other language model architectures."
- Break condition: If bias mitigation techniques are applied during training that specifically target larger models, or if the relationship between size and bias varies significantly across different model architectures.

## Foundational Learning

- Concept: Perplexity as a language modeling metric
  - Why needed here: The paper uses perplexity as the core metric for measuring bias, so understanding what perplexity measures is essential.
  - Quick check question: What does it mean when a language model assigns low perplexity to a sequence of text?

- Concept: Bias evaluation frameworks in NLP
  - Why needed here: Understanding existing approaches (like CrowS-Pairs and StereoSet) is crucial to appreciate the novelty of this framework.
  - Quick check question: How does the binary stereotypical vs anti-stereotypical approach differ from the multi-identity approach proposed in this paper?

- Concept: Data preprocessing and normalization
  - Why needed here: The paper involves extensive preprocessing of stereotypes and identities, including normalization and filtering based on perplexity.
  - Quick check question: Why might the authors filter stereotypes with high perplexity scores, and what could be the implications of this filtering?

## Architecture Onboarding

- Component map:
  - Stereotypes from SBIC -> Identities from lexicon -> Probe generation -> Perplexity calculation -> Normalization -> Delta Disparity Score computation -> Three-dimensional analysis

- Critical path:
  1. Data collection and preprocessing
  2. Probe generation
  3. Perplexity calculation for each model
  4. Normalization and Delta Disparity Score computation
  5. Analysis and ranking

- Design tradeoffs:
  - Using synthetic probes allows for controlled testing but may lack natural language plausibility
  - The combinatorial approach provides comprehensive coverage but results in a large number of probes
  - Perplexity as a metric is model-agnostic but may not capture all aspects of bias

- Failure signatures:
  - High perplexity for all probes may indicate model limitations or data quality issues
  - Extremely low variance across categories may suggest the model is "too fair" to be realistic
  - Disproportionate scores for certain categories may indicate dataset imbalances

- First 3 experiments:
  1. Run perplexity calculation on a small subset of probes to validate the basic pipeline works
  2. Compare perplexity-based scores with a binary benchmark (like StereoSet) on overlapping probes
  3. Test the framework on a smaller language model to establish baseline behavior before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do intersectional probes combining identities across different categories affect bias detection in language models?
- Basis in paper: [explicit] The authors explicitly mention future plans to "explore and evaluate intersectional probes that combine identities across different categories."
- Why unresolved: The current framework and dataset only consider individual categories (religion, gender, disability, nationality) separately, without examining how biases might compound when multiple identities intersect.
- What evidence would resolve it: Experiments showing how bias scores change when probes combine multiple identities (e.g., "Muslim women" or "disabled elderly people") compared to single-category probes.

### Open Question 2
- Question: To what extent do fairness measures at the pre-training level correlate with downstream application harms?
- Basis in paper: [explicit] The authors note that "fairness measures investigated at the pre-training level may not necessarily align with the harms manifested in downstream applications" and suggest extrinsic evaluation.
- Why unresolved: The study focuses on intrinsic evaluation using perplexity scores, but doesn't examine how these pre-training biases translate to actual harms in downstream tasks like sentiment analysis or text generation.
- What evidence would resolve it: Comparative studies measuring biases in pre-trained models versus their performance on downstream tasks involving sensitive demographic groups.

### Open Question 3
- Question: How do language models' bias scores vary across different languages and cultural contexts?
- Basis in paper: [explicit] The authors acknowledge that their experiments focus on English "due to the limited availability of datasets for other languages having stereotypes annotated" and encourage development of multilingual datasets.
- Why unresolved: The entire study is conducted on English language models, limiting generalizability to other languages and cultures where different stereotypes and biases may exist.
- What evidence would resolve it: Replicating the SOFA framework in multiple languages with culturally appropriate stereotypes and identities, then comparing bias scores across languages.

## Limitations

- The normalization by identity-specific perplexity may not fully account for inherent biases in how different identities are treated in language models
- Reliance on synthetic probes generated through combinatorial pairing may not capture the full complexity of natural language contexts where biases manifest
- The framework's sensitivity to preprocessing choices, particularly in filtering stereotypes based on perplexity thresholds, introduces potential bias in the evaluation dataset itself

## Confidence

**High Confidence**: The observation that larger model variants exhibit higher bias aligns with established findings in the literature and is supported by consistent results across multiple model architectures (GPT2, XLNET, BART).

**Medium Confidence**: The three-dimensional analysis approach provides novel insights into bias patterns across categories, identities, and stereotypes. However, the practical significance of these nuanced patterns versus simpler binary evaluations remains to be fully established.

**Low Confidence**: The claim that identities expressing different religions lead to the most pronounced disparate treatments is based on the specific SOFA dataset composition and may not generalize across all cultural contexts or different dataset compositions.

## Next Checks

1. **Dataset Generalization Test**: Evaluate the framework's consistency using alternative stereotype sources beyond SBIC, such as stereotypes from different cultural contexts or domains, to assess robustness across dataset variations.

2. **Human Evaluation Validation**: Conduct human judgments on a subset of the generated probes to validate whether perplexity-based bias scores align with human perceptions of bias, particularly for edge cases where low perplexity might reflect common usage rather than bias.

3. **Cross-Modal Application**: Apply the perplexity-based fairness framework to vision-language models to test whether the same principles hold across different model architectures and modalities, examining whether the normalization approach remains effective.