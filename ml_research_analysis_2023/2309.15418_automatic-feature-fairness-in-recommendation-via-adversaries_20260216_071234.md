---
ver: rpa2
title: Automatic Feature Fairness in Recommendation via Adversaries
arxiv_id: '2309.15418'
source_url: https://arxiv.org/abs/2309.15418
tags:
- feature
- fairness
- adversarial
- training
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses fairness in recommender systems by proposing
  feature fairness as a core objective, aiming to provide equitable treatment across
  diverse groups defined by feature combinations. The authors introduce Adaptive Adversarial
  Factorization Machines (AAFM), which leverages adversarial training with feature-specific
  perturbations to enhance generalization for underrepresented features.
---

# Automatic Feature Fairness in Recommendation via Adversaries

## Quick Facts
- arXiv ID: 2309.15418
- Source URL: https://arxiv.org/abs/2309.15418
- Authors: Not specified
- Reference count: 40
- Primary result: Improves accuracy by 1.9% in AUC and fairness (reducing group standard deviation by up to 10%) on three datasets

## Executive Summary
This paper addresses fairness in recommender systems by proposing feature fairness as a core objective, focusing on equitable treatment across diverse groups defined by feature combinations. The authors introduce Adaptive Adversarial Factorization Machines (AAFM), which leverages adversarial training with feature-specific perturbations to enhance generalization for underrepresented features. By automatically adjusting perturbation strength based on feature frequency and combination variety, AAFM improves both accuracy and fairness metrics across multiple datasets. The approach demonstrates that adversarial training can simultaneously address accuracy and fairness objectives when properly calibrated.

## Method Summary
AAFM builds on Factorization Machines (FM) and introduces adaptive adversarial training that targets feature-level biases. The method automatically adjusts perturbation strength based on two metrics: feature frequency (how often a feature value appears) and combination variety (how many different feature combinations a value participates in). Features with low combination variety receive stronger perturbations, while low-frequency features receive higher training weights. The perturbation strength and training weights decay during training, balancing fairness improvements with accuracy maintenance. The model treats fairness as a feature property rather than user/item property, allowing it to address systematic biases in the feature space.

## Key Results
- Improves accuracy by 1.9% in AUC across three datasets (MovieLens-100K, Pinterest, Yelp)
- Reduces group standard deviation in performance by up to 10%, indicating better fairness
- Outperforms strong baselines including AdvFM and traditional FM models
- Shows particular robustness improvements for small, underrepresented groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature fairness improves overall accuracy by balancing feature generalizability.
- Mechanism: The model treats fairness as a feature-level property, not tied to users or items. By applying adversarial perturbations that target underrepresented feature combinations, it improves the model's ability to generalize across all feature values.
- Core assumption: Feature frequency and combination variety are independent biases that can be separately measured and addressed.
- Evidence anchors:
  - [abstract] "This improves overall accuracy through balanced feature generalizability."
  - [section] "Feature values in the data distribution have the following statistical properties. To aid understanding, we show an example of feature value ð‘£ in the feature domain ð‘¥ð‘–."
  - [corpus] Weak evidence; no directly comparable mechanism found.
- Break condition: If frequency and combination variety are strongly correlated, the independent adjustment strategy may overcompensate.

### Mechanism 2
- Claim: Adaptive adversarial perturbations based on combination variety improve robustness for underrepresented features.
- Mechanism: Features with fewer co-occurring combinations receive stronger adversarial perturbations, forcing the model to learn more robust representations that generalize better to rare feature interactions.
- Core assumption: Low combination variety indicates higher sensitivity to perturbations, requiring stronger regularization.
- Evidence anchors:
  - [abstract] "Stronger perturbations are applied to feature values with fewer combination varieties to improve generalization."
  - [section] "Considering each feature domain ð‘¥ð‘– with the corresponding value ð‘£ð‘–, a smaller combination variety ð›½ð‘£ð‘– indicates a higher degree of sensitivity representation."
  - [corpus] No direct evidence; this is a novel contribution.
- Break condition: If combination variety is not a good proxy for sensitivity, the perturbation scaling may be ineffective.

### Mechanism 3
- Claim: Re-weighting adversarial training by feature frequency balances the learning impact of rare features.
- Mechanism: Samples with low overall feature frequency receive higher adversarial training weights, ensuring underrepresented features get adequate learning attention during training.
- Core assumption: Low-frequency features need proportionally higher training emphasis to overcome their scarcity in the data.
- Evidence anchors:
  - [abstract] "Higher weights for low-frequency features address training imbalances."
  - [section] "We conduct sample-specific ones. Specifically, given a sample x(ð‘˜ ), the sample-specific adversary weight ðœ†ð‘˜ is defined as: ðœ†ð‘˜ = Î¦(âˆ’ÃŽð‘¥âˆˆx(ð‘˜ ) ð›¼ð‘¥ , ð‘¡)"
  - [corpus] Weak evidence; related work mentions re-weighting but not specifically for adversarial training.
- Break condition: If frequency re-weighting creates too much emphasis on noise in rare samples, overall accuracy may degrade.

## Foundational Learning

- Concept: Adversarial training and gradient-based perturbations
  - Why needed here: The core mechanism relies on perturbing feature embeddings to improve robustness and fairness
  - Quick check question: What is the mathematical form of the adversarial perturbation applied to feature embeddings in this work?

- Concept: Factorization Machines and second-order feature interactions
  - Why needed here: The backbone model uses FM to capture pairwise feature interactions, which the adversarial perturbations modify
  - Quick check question: How does the FM model compute the prediction from feature embeddings and pairwise interaction weights?

- Concept: Statistical bias in feature distributions (frequency and combination variety)
  - Why needed here: These two metrics drive the adaptive perturbation strength and training weight adjustments
  - Quick check question: How are feature frequency and combination variety calculated for a specific feature value in the dataset?

## Architecture Onboarding

- Component map: Embedding layer -> FM backbone -> Adversarial perturbation module -> Updated embedding -> Final prediction
- Critical path: Data â†’ Embedding â†’ FM prediction â†’ Adversarial perturbation â†’ Updated embedding â†’ Final prediction
- Design tradeoffs:
  - Fixed vs. adaptive perturbation strength: Adaptive allows finer control but adds complexity
  - Normal training vs. adversarial training weight: Higher weights improve fairness but may hurt accuracy if overdone
  - Combination variety vs. frequency as bias metric: Using both captures different aspects of feature imbalance
- Failure signatures:
  - Accuracy drops significantly with adversarial training: Perturbations too strong or training weights imbalanced
  - Fairness metrics don't improve: Adaptive scaling not effective or biases not well captured
  - Training instability: Learning rate or perturbation strength decaying too slowly
- First 3 experiments:
  1. Run FM baseline on MovieLens with AUC and fairness metrics to establish reference
  2. Implement basic adversarial training (AdvFM) without adaptation to verify perturbation works
  3. Add adaptive perturbation strength based on combination variety only to isolate its effect

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively balance the impact of biased negative sampling in different feature groups?
- Basis in paper: [inferred] The paper mentions that AAFM defaults to using random negative sampling, which biases toward the majority of users/items features, and suggests this as a direction for future study.
- Why unresolved: The paper does not provide a concrete solution or methodology for addressing this bias in negative sampling across different feature groups.
- What evidence would resolve it: A proposed method or framework that demonstrates improved fairness metrics when applied to datasets with varying feature group distributions.

### Open Question 2
- Question: How do different adversarial attack methods (e.g., PGD, C&W) perform compared to FGSM when applied to complex neural recommendation backbones?
- Basis in paper: [explicit] The paper suggests investigating the effectiveness of different adversaries (e.g., PGD, C&W) on more complex neural recommendation backbones as future work.
- Why unresolved: The paper only uses FGSM for adversarial perturbation and does not compare its performance against other attack methods on more sophisticated models.
- What evidence would resolve it: Comparative experimental results showing the relative performance of different adversarial attack methods on both accuracy and fairness metrics across multiple recommendation models.

### Open Question 3
- Question: What is the optimal way to dynamically adjust perturbation strength and adversarial training weights during training to maximize both fairness and accuracy?
- Basis in paper: [explicit] The paper introduces adaptive mechanisms for adjusting perturbation strength and adversarial training weights, but notes that finding the right balance is crucial and that persistent perturbations can impact model accuracy.
- Why unresolved: The paper presents an adaptive approach but does not provide a definitive answer on how to optimally tune these parameters for different datasets and scenarios.
- What evidence would resolve it: A comprehensive study showing the relationship between perturbation strength, adversarial weights, and model performance across various datasets, along with guidelines for parameter selection.

## Limitations

- The adaptive scaling functions for perturbation strength and training weights rely on specific parameter choices (t, Î±, Îµ) that aren't fully specified and likely require dataset-specific tuning
- The paper assumes feature frequency and combination variety are independent bias factors, but doesn't explore scenarios where these metrics are highly correlated
- The claim that the method improves robustness against adversarial noise is only briefly mentioned without detailed experimental validation

## Confidence

**High Confidence**: The core claim that adversarial training can improve both accuracy and fairness is well-supported by the experimental results across three diverse datasets.

**Medium Confidence**: The adaptive scaling approach based on feature frequency and combination variety is novel and shows promising results, but lacks extensive ablation studies to isolate individual contributions.

**Low Confidence**: The paper's claim about improved robustness against adversarial noise is only briefly mentioned without detailed validation.

## Next Checks

1. **Ablation Study on Adaptive Components**: Run experiments with only frequency-based re-weighting, only combination variety-based perturbation scaling, and the full adaptive system to quantify the individual contributions of each mechanism.

2. **Parameter Sensitivity Analysis**: Systematically vary the key hyperparameters (t, Î±, Îµ initial value) across a range of values for each dataset to understand their impact on the fairness-accuracy tradeoff.

3. **Correlation Analysis**: Measure the correlation between feature frequency and combination variety in each dataset to test the assumption that these metrics capture independent forms of bias.