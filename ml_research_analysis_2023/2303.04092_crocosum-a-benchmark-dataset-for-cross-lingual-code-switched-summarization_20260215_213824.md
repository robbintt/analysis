---
ver: rpa2
title: 'CroCoSum: A Benchmark Dataset for Cross-Lingual Code-Switched Summarization'
arxiv_id: '2303.04092'
source_url: https://arxiv.org/abs/2303.04092
tags:
- language
- code-switching
- summarization
- crocosum
- code-switched
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CroCoSum, a new dataset for cross-lingual
  code-switched summarization. The dataset contains over 24,000 English source articles
  and 18,000 human-written Chinese summaries with code-switching, collected from a
  Chinese technology news platform.
---

# CroCoSum: A Benchmark Dataset for Cross-Lingual Code-Switched Summarization

## Quick Facts
- arXiv ID: 2303.04092
- Source URL: https://arxiv.org/abs/2303.04092
- Authors: 
- Reference count: 16
- Primary result: New dataset of 24K English articles with 18K Chinese code-switched summaries shows existing CLS pretraining ineffective

## Executive Summary
This paper introduces CroCoSum, a new benchmark dataset for cross-lingual code-switched summarization. The dataset contains over 24,000 English source articles and 18,000 human-written Chinese summaries with code-switching, collected from a Chinese technology news platform. The authors evaluate various baseline approaches including pipeline, end-to-end, and zero-shot methods on CroCoSum. They find that leveraging existing cross-lingual summarization resources as pretraining does not improve performance on CroCoSum, indicating the limited generalizability of current datasets. The authors also perform a qualitative analysis revealing common error types in code-switched generation.

## Method Summary
The authors collected CroCoSum from solidot.org, a Chinese technology news platform where users write summaries in Chinese but retain English technical terms. The dataset contains 24,171 English source articles and 18,557 human-written Chinese-English code-switched summaries. They evaluate pipeline methods (translate-then-summarize and summarize-then-translate using Google Translate API), end-to-end methods (fine-tuning mT5, mBART, and mBART-50), and zero-shot GPT-3 prompting approaches. The models are trained on 70% of the data, validated on 15%, and tested on 15%. Evaluation uses ROUGE-{1,2,L} and BERTScore metrics, along with code-switching metrics (CMI and SP).

## Key Results
- End-to-end models outperform pipeline approaches on CroCoSum, contradicting prior work showing pipeline methods are superior
- Pretraining on existing cross-lingual summarization datasets (WikiLingua, CrossSum) does not improve performance on CroCoSum
- Over 92% of human-written summaries contain code-switched phrases, validating the dataset's focus on authentic code-switching

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset captures code-switching patterns that are naturally occurring in human-written summaries rather than artificially induced through translation.
- Mechanism: By collecting summaries directly from solidot.org where users write in Chinese but retain English technical terms, the dataset preserves authentic code-switching that reflects real multilingual communication needs.
- Core assumption: Human-written summaries on technology news platforms naturally code-switch when English technical terms lack formal Chinese translations or when international readers prefer original terminology.
- Evidence anchors:
  - [abstract] "However, given the rareness of naturally occurring CLS resources, the majority of datasets are forced to rely on translation which can contain overly literal artifacts."
  - [section 1] "To study the phenomenon of code-switching in CLS, we introduce CroCoSum, a dataset of cross-lingual code-switched summarization of technology news."
- Break condition: If the code-switching patterns are primarily driven by editorial requirements rather than natural user preferences, the authenticity of the dataset could be compromised.

### Mechanism 2
- Claim: The limited generalizability of current CLS datasets is revealed when existing resources fail to improve performance on CroCoSum through pretraining.
- Mechanism: The unique code-switching patterns and longer text lengths in CroCoSum create a distribution shift that existing CLS datasets cannot bridge, exposing the limitations of current resources.
- Core assumption: The code-switching patterns and text characteristics in CroCoSum are sufficiently different from existing datasets that pretraining on those resources provides no benefit.
- Evidence anchors:
  - [abstract] "We show that leveraging existing CLS resources as a pretraining step does not improve performance on CroCoSum, indicating the limited generalizability of current datasets."
  - [section 5] "To explore whether we can further boost our best-performing baseline, we leverage English-Chinese CLS pairs in WikiLingua and CrossSum in an additional pretraining step for mBART50 before finetuning it on CroCoSum."
- Break condition: If the pretraining experiments used insufficient data or suboptimal hyperparameters, the negative results might not reflect true limitations.

### Mechanism 3
- Claim: The end-to-end approach performs better than pipeline methods on CroCoSum due to error propagation in translation modules.
- Mechanism: Pipeline methods that first translate then summarize accumulate translation errors that compound during summarization, while end-to-end methods learn to handle both tasks jointly.
- Core assumption: The quality of machine translation for technical content with code-switched terms is insufficient to serve as a reliable input for summarization models.
- Evidence anchors:
  - [section 4.1] "Pipeline methods can be further broken down into translate-then-summarize and summarize-then-translate approaches. We use the Google Translate API as the translation module in the pipeline methods."
  - [section 5] "Our results seem to contradict what Ladhak et al. (2020) and Wang et al. (2022a) have reported, namely that end-to-end finetuning performs worse than the strongest pipeline methods."
- Break condition: If the Google Translate API performs exceptionally well on the technical domain of CroCoSum, the pipeline approach might become competitive.

## Foundational Learning

- Concept: Code-switching in multilingual communication
  - Why needed here: Understanding why humans naturally mix languages in technical contexts helps interpret the dataset's patterns and design appropriate evaluation metrics.
  - Quick check question: Why might a Chinese summary of technology news retain English terms instead of translating them?

- Concept: Cross-lingual summarization task formulation
  - Why needed here: The task requires models to both translate and summarize, making it essential to understand how these objectives interact and compete.
  - Quick check question: What are the key differences between within-language summarization and cross-lingual summarization?

- Concept: Multilingual language model pretraining objectives
  - Why needed here: Different pretraining strategies (unsupervised vs. translation-based) lead to different capabilities in handling code-switched text.
  - Quick check question: How does pretraining on translation tasks differ from unsupervised pretraining in terms of cross-lingual alignment?

## Architecture Onboarding

- Component map: Data collection pipeline -> preprocessing -> baseline models (pipeline, end-to-end, zero-shot) -> evaluation metrics -> qualitative analysis
- Critical path: Data collection -> baseline model training -> automatic evaluation -> qualitative error analysis
- Design tradeoffs: Longer summaries provide more context but increase computational costs; authentic code-switching improves realism but complicates evaluation
- Failure signatures: Poor performance on code-switched phrases, high variance in evaluation metrics, mismatch between automatic metrics and qualitative assessment
- First 3 experiments:
  1. Train mT5-base end-to-end on CroCoSum training set and evaluate on test set using ROUGE and BERTScore
  2. Implement translate-then-summarize pipeline using Google Translate API and compare performance to end-to-end approach
  3. Fine-tune mBART-50 on WikiLingua and CrossSum, then finetune on CroCoSum to test pretraining effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively evaluate cross-lingual code-switched summaries given the discrepancy between automatic metrics and human judgment?
- Basis in paper: [explicit] The authors note that their qualitative analysis revealed limitations in current automatic evaluation methods and call for a more comprehensive evaluation framework that considers naturalness and relevance.
- Why unresolved: Current metrics like ROUGE and BERTScore focus on lexical and semantic similarity but don't account for the naturalness of code-switching patterns or whether the code-switched elements are contextually appropriate.
- What evidence would resolve it: Development and validation of new evaluation metrics that incorporate human judgment of code-switching naturalness and relevance, along with correlation studies showing these metrics better align with human preferences than current methods.

### Open Question 2
- Question: Why does pretraining on existing cross-lingual summarization datasets not improve performance on CroCoSum?
- Basis in paper: [explicit] The authors found that leveraging English-Chinese CLS pairs from WikiLingua and CrossSum as additional pretraining for mBART50 actually decreased performance on CroCoSum.
- Why unresolved: The paper suggests limited generalizability of current CLS resources, but doesn't investigate whether this is due to domain differences (technology news vs. other domains), stylistic differences, or fundamental differences in code-switching patterns.
- What evidence would resolve it: Systematic experiments varying pretraining data domains and analyzing code-switching patterns in pretraining vs. target data to identify specific factors causing negative transfer.

### Open Question 3
- Question: What are the optimal architectural approaches for generating natural-sounding code-switched summaries?
- Basis in paper: [inferred] The authors observe that end-to-end models achieve better automatic metrics than pipeline approaches, but qualitative analysis shows generated summaries often contain over-switched, under-switched, or erroneous code-switching.
- Why unresolved: Current models optimize for standard summarization metrics rather than code-switching naturalness, and the paper's qualitative analysis reveals systematic errors in how models handle code-switching decisions.
- What evidence would resolve it: Development of specialized architectures or training objectives that explicitly model code-switching decisions, validated through improved qualitative scores on naturalness and reduced error types identified in the analysis.

## Limitations
- Dataset size is relatively small compared to large-scale within-language summarization benchmarks, potentially limiting model training effectiveness
- Heavy reliance on automatic evaluation metrics without extensive human evaluation introduces uncertainty about true summary quality
- Negative pretraining results may stem from suboptimal experimental design rather than fundamental dataset incompatibility

## Confidence

**High Confidence:** The core contribution of introducing CroCoSum as a benchmark dataset for cross-lingual code-switched summarization is well-established. The data collection methodology from solidot.org is clearly described, and the dataset statistics are transparently reported. The observation that the majority of summaries (>92%) contain code-switched phrases is directly verifiable from the dataset.

**Medium Confidence:** The comparative analysis showing end-to-end methods outperforming pipeline approaches is reasonably supported, though the exact magnitude of performance differences may vary depending on implementation details and random initialization. The finding that existing CLS datasets do not improve performance through pretraining is supported by the experiments but could benefit from additional ablations and alternative training strategies.

**Low Confidence:** The qualitative analysis of error types in code-switched generation provides useful insights but is based on a limited sample size (50 random samples). The specific patterns identified may not represent the full spectrum of errors that could emerge with different model architectures or training regimes.

## Next Checks

1. **Human Evaluation Study:** Conduct comprehensive human evaluation of generated summaries using multiple annotators to assess both summary quality and code-switching naturalness. Compare human judgments with automatic metrics to identify systematic discrepancies and validate which evaluation approach better captures true performance.

2. **Alternative Pretraining Experiments:** Design a systematic study exploring different pretraining strategies including: (a) mixed fine-tuning on multiple CLS datasets with varying proportions, (b) curriculum learning starting from simpler code-switching patterns, (c) domain-adaptive pretraining on additional technology news corpora, and (d) parameter-efficient finetuning methods like LoRA to isolate whether model capacity or training strategy is limiting.

3. **Domain Generalization Test:** Evaluate models trained on CroCoSum on a held-out test set of technology news articles from different sources (e.g., tech blogs, research publications) to assess whether the dataset captures general code-switching patterns or is overly specific to solidot.org's editorial style. Additionally, test cross-domain generalization by evaluating on summaries from non-technology domains that still contain technical terminology.