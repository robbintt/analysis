---
ver: rpa2
title: Adversarial Robustness on Image Classification with $k$-means
arxiv_id: '2312.09533'
source_url: https://arxiv.org/abs/2312.09533
tags:
- adversarial
- training
- learning
- clustering
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of k-means clustering
  to adversarial attacks and proposes an adversarial training method to enhance its
  robustness. The method leverages transferability of adversarial examples from a
  supervised model (ResNet-18 trained on MNIST) to the unsupervised k-means model.
---

# Adversarial Robustness on Image Classification with $k$-means

## Quick Facts
- arXiv ID: 2312.09533
- Source URL: https://arxiv.org/abs/2312.09533
- Reference count: 40
- Primary result: Adversarial training improves k-means robustness from 43% to 80% on MNIST and 38% to 74% on Fashion-MNIST

## Executive Summary
This paper investigates the vulnerability of k-means clustering to adversarial attacks and proposes an adversarial training method to enhance its robustness. The method leverages transferability of adversarial examples from a supervised ResNet-18 model (trained on MNIST) to the unsupervised k-means model. By incrementally increasing attack strength during training and initializing centroids from previous steps, the method significantly improves adversarial accuracy while maintaining high clean accuracy on both MNIST and Fashion-MNIST datasets.

## Method Summary
The method uses a pre-trained ResNet-18 on MNIST as a surrogate model to generate adversarial examples via I-FGSM. These adversarial examples are then used to train k-means clustering through incremental adversarial training. The training process involves combining clean and adversarial examples based on proportion η, with centroids initialized from previous steps to enable continuous learning. The attack strength is gradually increased over β steps to stabilize the clustering process and improve robustness.

## Key Results
- Improved adversarial accuracy from 43% to 80% on MNIST and 38% to 74% on Fashion-MNIST
- Maintained clean accuracy above 89% on MNIST and 77% on Fashion-MNIST
- Demonstrated effectiveness of transferability from supervised to unsupervised models across different datasets
- Showed importance of incremental training with centroid initialization for stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transferability of adversarial examples from supervised to unsupervised models improves robustness
- Core assumption: Adversarial examples exhibit cross-model transferability
- Evidence anchors: Abstract highlights effectiveness of transferability using supervised model to target unsupervised model
- Break condition: Transferability fails if models learn fundamentally different decision boundaries

### Mechanism 2
- Claim: Incremental adversarial training with continuous centroid initialization stabilizes k-means
- Core assumption: Continuous learning prevents catastrophic forgetting
- Evidence anchors: Abstract discusses incremental attack strength and centroid initialization importance
- Break condition: Method fails if initialization becomes poor due to distribution shifts

### Mechanism 3
- Claim: Balancing clean and adversarial examples optimizes both accuracies
- Core assumption: Optimal proportion maximizes both clean and adversarial performance
- Evidence anchors: Abstract mentions importance of various parameters including proportion η
- Break condition: Optimal proportion is dataset-dependent and not generalizable

## Foundational Learning

- Concept: Adversarial examples and their transferability
  - Why needed here: Crucial for understanding how attacks on one model affect another
  - Quick check question: Can you explain why an adversarial example that fools ResNet-18 might also fool k-means clustering?

- Concept: Unsupervised learning and clustering algorithms
  - Why needed here: Paper focuses on enhancing k-means clustering robustness
  - Quick check question: What is the main objective of clustering algorithms like k-means?

- Concept: Adversarial training and its application to unsupervised models
  - Why needed here: Proposed method adapts adversarial training for unsupervised k-means
  - Quick check question: How does the proposed method differ from traditional adversarial training?

## Architecture Onboarding

- Component map:
  - Surrogate model: ResNet-18 trained on MNIST
  - Target model: k-means clustering
  - Adversarial attack: Iterative Fast Gradient Sign Method (I-FGSM)
  - Training data manipulation: Proportion of clean and adversarial examples (η)
  - Training process: Incremental attack strength and continuous centroid initialization

- Critical path:
  1. Train ResNet-18 on MNIST
  2. Generate adversarial examples using I-FGSM
  3. Combine clean and adversarial examples based on proportion η
  4. Initialize k-means centroids
  5. Train k-means on combined dataset
  6. Incrementally increase attack strength and repeat
  7. Evaluate clean and adversarial accuracy

- Design tradeoffs:
  - Robustness vs. clean accuracy: Increasing attack strength improves robustness but may decrease clean accuracy
  - Proportion of adversarial examples: Higher proportion improves robustness but requires more resources
  - Incremental training vs. full perturbation: Incremental training is more stable but requires more steps

- Failure signatures:
  - Low adversarial accuracy despite high clean accuracy: Insufficient adversarial examples during training
  - Low clean accuracy: Overfitting to adversarial examples or unstable clustering
  - High variance in results: Sensitivity to initialization or need for stable training

- First 3 experiments:
  1. Evaluate transferability by testing MNIST attacks on both MNIST and Fashion-MNIST k-means
  2. Test impact of different proportions (η) of clean and adversarial examples
  3. Compare k-means performance with and without incremental training and centroid initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does transferability effectiveness vary with different surrogate models or training datasets?
- Basis in paper: Paper tested transferability with ResNet-18 on MNIST to k-means on Fashion-MNIST
- Why unresolved: Only tested one architecture and dataset combination
- What evidence would resolve it: Systematic experiments across different architectures and datasets

### Open Question 2
- Question: What are theoretical limits of robustness in unsupervised clustering vs supervised learning?
- Basis in paper: Paper demonstrates empirical improvements but lacks theoretical analysis
- Why unresolved: Study focuses on empirical results without theoretical bounds
- What evidence would resolve it: Theoretical analysis establishing robustness bounds and comparisons

### Open Question 3
- Question: How does sample distribution sensitivity affect robustness in real-world applications?
- Basis in paper: Paper highlights sensitivity to sample distributions in controlled experiments
- Why unresolved: Does not address real-world applications with complex or changing distributions
- What evidence would resolve it: Case studies or simulations on real-world datasets with non-uniform distributions

## Limitations
- Transferability mechanism relies on empirical observation rather than theoretical guarantees
- Effectiveness across different attack types remains untested
- k=856 cluster selection appears dataset-specific and may not generalize
- Incremental training requires more ablation studies to isolate component contributions

## Confidence

- High: Empirical improvements in adversarial accuracy using transferability are well-supported
- Medium: Incremental training mechanism is supported but requires further validation
- Low: Claims about general sensitivity of unsupervised models need broader validation

## Next Checks

1. Test transferability effectiveness with alternative supervised models (VGG, EfficientNet) and attack methods (PGD, CW)
2. Conduct ablation studies isolating impact of incremental training, centroid initialization, and proportion optimization
3. Evaluate performance on additional datasets (CIFAR-10, SVHN) to assess generalizability of cluster selection and η optimization