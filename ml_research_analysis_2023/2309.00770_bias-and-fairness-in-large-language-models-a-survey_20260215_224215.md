---
ver: rpa2
title: 'Bias and Fairness in Large Language Models: A Survey'
arxiv_id: '2309.00770'
source_url: https://arxiv.org/abs/2309.00770
tags:
- bias
- language
- association
- linguistics
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of bias evaluation
  and mitigation techniques for large language models (LLMs). The authors first formalize
  social bias and fairness definitions for NLP, then propose three intuitive taxonomies:
  one for bias evaluation metrics, one for bias evaluation datasets, and one for bias
  mitigation techniques.'
---

# Bias and Fairness in Large Language Models: A Survey

## Quick Facts
- arXiv ID: 2309.00770
- Source URL: https://arxiv.org/abs/2309.00770
- Reference count: 40
- Key outcome: Comprehensive survey of bias evaluation and mitigation techniques for LLMs with proposed taxonomies

## Executive Summary
This paper provides a comprehensive survey of bias evaluation and mitigation techniques for large language models (LLMs). The authors formalize social bias and fairness definitions for NLP, then propose three intuitive taxonomies: one for bias evaluation metrics, one for bias evaluation datasets, and one for bias mitigation techniques. For each taxonomy, they discuss a wide range of existing methods, formalize them mathematically, provide examples, and discuss limitations. The survey also outlines open problems and challenges for future research, emphasizing the need to address power imbalances, develop more robust fairness desiderata, improve evaluation standards, and better understand bias mechanisms in LLMs.

## Method Summary
The paper consolidates, formalizes, and expands notions of social bias and fairness in NLP through a comprehensive literature review of 40 references. The authors propose three taxonomies for bias evaluation (metrics and datasets) and mitigation techniques, identify open problems and challenges, and organize the survey around these structured frameworks. The method involves categorizing existing approaches, providing mathematical formalizations, and discussing their applications and limitations.

## Key Results
- Proposed three taxonomies: bias evaluation metrics, bias evaluation datasets, and bias mitigation techniques
- Formalized fairness desiderata for NLP including unawareness, invariance, and equal social group associations
- Identified key open problems including need for more robust evaluation standards and understanding bias mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Taxonomizing bias metrics by what they use (embeddings, probabilities, generated text) clarifies which metrics are appropriate for a given dataset structure and access level.
- Mechanism: The survey introduces three data structures—counterfactual inputs (masked tokens, unmasked sentences) and prompts—and matches them with metrics operating at embedding, probability, or text generation levels. This alignment ensures metrics are only used where the underlying data assumptions hold.
- Core assumption: Each metric's required input (embeddings, probabilities, or generated text) is consistently available from the model and dataset structure.
- Evidence anchors:
  - [abstract] "We propose three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text."
  - [section] "The subset of appropriate metrics largely depends on the structure of the dataset and task. We illustrate this relationship in Figure 2."
- Break condition: If a metric is applied to incompatible data (e.g., embedding-based metric on a prompt dataset without sentence embeddings), the evaluation becomes invalid.

### Mechanism 2
- Claim: Classifying bias mitigation techniques by intervention stage (pre-processing, in-training, intra-processing, post-processing) guides practitioners to select methods appropriate to their access level and constraints.
- Mechanism: The survey organizes mitigation into four stages based on when and how the technique modifies the model or data. Pre-processing changes inputs before training; in-training modifies training dynamics; intra-processing alters inference behavior; post-processing rewrites outputs. Each stage matches different resource and access scenarios.
- Core assumption: Practitioners have identifiable constraints on model access (trainable parameters, outputs only) and computational resources that align with one intervention stage.
- Evidence anchors:
  - [abstract] "Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing..."
  - [section] "We organize bias mitigation techniques by the stage at which they operate in the LLM workflow: pre-processing, in-training, intra-processing, and post-processing."
- Break condition: If the selected stage does not match actual constraints (e.g., applying post-processing to a black-box model with no output access), the mitigation fails.

### Mechanism 3
- Claim: Defining explicit fairness desiderata for NLP tasks operationalizes abstract fairness concepts into measurable constraints, enabling consistent evaluation and mitigation.
- Mechanism: The survey introduces fairness through unawareness, invariance, equal social group associations, equal neutral associations, and replicated distributions as concrete mathematical criteria that can be measured via metrics and targeted by mitigations.
- Core assumption: These desiderata capture relevant fairness concerns for NLP tasks and are measurable with available metrics.
- Evidence anchors:
  - [abstract] "We shift fairness frameworks typically applied to machine learning classification problems towards NLP and introduce several fairness desiderata that begin to operationalize various fairness notions for LLMs."
  - [section] "Instead of suggesting a single fairness constraint, we provide a number of possible fairness desiderata for LLMs."
- Break condition: If desiderata are too narrow or miss relevant harms, mitigation may be ineffective or misaligned with actual fairness needs.

## Foundational Learning

- Concept: Social group definitions and power asymmetries
  - Why needed here: Bias evaluation and mitigation require understanding how social groups are constructed and how historical power imbalances affect language model behavior.
  - Quick check question: Can you list three examples of social groups that may be differentially impacted by language model bias?

- Concept: Evaluation metric vs. evaluation dataset distinction
  - Why needed here: The survey explicitly separates these to avoid conflating the data structure with the method of measurement, which is critical for selecting appropriate bias metrics.
  - Quick check question: What is the difference between an evaluation metric and an evaluation dataset in the context of bias measurement?

- Concept: Token probability modeling in LLMs
  - Why needed here: Many bias metrics rely on comparing token probabilities or pseudo-log-likelihoods, requiring understanding of how models assign likelihoods to words.
  - Quick check question: How does a masked language model estimate the probability of a masked token given the context?

## Architecture Onboarding

- Component map: The bias evaluation and mitigation pipeline consists of (1) dataset selection and structure, (2) metric selection aligned with dataset and model access, (3) intervention stage choice for mitigation, and (4) fairness desiderata definition.
- Critical path: Identify dataset structure → select compatible metrics → apply appropriate mitigation stage → measure with fairness desiderata.
- Design tradeoffs: Granular metrics (e.g., embedding-based) may be more precise but require more access; broader metrics (e.g., generated text-based) are more general but less specific.
- Failure signatures: Mismatched metric-dataset pairs produce invalid results; mitigation applied at wrong stage fails due to access constraints; fairness criteria too narrow miss key harms.
- First 3 experiments:
  1. Take a counterfactual input dataset (e.g., WinoBias) and apply a masked token probability-based metric (e.g., Log-Probability Bias Score) to measure gender bias.
  2. Use a prompt dataset (e.g., BOLD) with a generated text-based metric (e.g., Counterfactual Sentiment Bias) to assess sentiment disparities.
  3. Apply a pre-processing mitigation (e.g., Counterfactual Data Augmentation) to a training set and evaluate bias reduction using an embedding-based metric (e.g., Sentence Encoder Association Test).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are different bias mitigation techniques at various intervention stages (pre-processing, in-training, intra-processing, post-processing) when applied in combination?
- Basis in paper: [explicit] The paper discusses that most bias mitigation techniques target only a single intervention stage, and that hybrid techniques reducing bias at multiple or all intervention stages could be more effective.
- Why unresolved: While the paper provides an extensive taxonomy of mitigation techniques at each stage, there is a lack of empirical studies comparing the effectiveness of hybrid techniques that combine methods across multiple stages.
- What evidence would resolve it: Controlled experiments comparing the effectiveness of hybrid mitigation techniques that combine methods from multiple intervention stages against single-stage techniques on various bias evaluation metrics and downstream tasks.

### Open Question 2
- Question: How can evaluation metrics and datasets be improved to better capture the nuanced and context-dependent nature of bias in language models?
- Basis in paper: [explicit] The paper highlights that current bias evaluation metrics and datasets often have reliability and validity issues, may not fully represent real-world harms, and may not capture the context-dependent nature of bias.
- Why unresolved: While the paper discusses these limitations, it does not provide specific solutions for improving evaluation metrics and datasets to address these issues.
- What evidence would resolve it: Development and validation of new evaluation metrics and datasets that are more reliable, valid, and context-aware, along with empirical studies demonstrating their effectiveness in capturing real-world bias.

### Open Question 3
- Question: How can bias mitigation techniques be designed to be more inclusive and avoid further marginalizing underrepresented and minoritized voices?
- Basis in paper: [explicit] The paper discusses the importance of centering marginalized communities and avoiding further marginalization, but does not provide specific solutions for achieving this in bias mitigation techniques.
- Why unresolved: While the paper highlights the importance of this issue, it does not provide concrete guidance on how to design bias mitigation techniques that are inclusive and avoid further marginalization.
- What evidence would resolve it: Development and evaluation of bias mitigation techniques that incorporate participatory approaches, value diverse linguistic styles, and are designed with input from marginalized communities.

## Limitations

- The effectiveness of taxonomy-guided selection in real-world scenarios remains to be empirically validated across diverse datasets and applications.
- The survey's broad scope means some techniques receive less detailed treatment than others, potentially limiting practical implementation guidance.
- The proposed fairness desiderata may not capture all relevant fairness concerns across diverse NLP applications, particularly nuanced or context-dependent harms.

## Confidence

- Mechanism 1: Medium - Well-supported framework but limited empirical validation across diverse datasets
- Mechanism 2: Medium - Practical guidance but real-world constraints may not always align neatly with four-stage model
- Mechanism 3: Low - Concrete criteria provided but may not capture all relevant fairness concerns

## Next Checks

1. Test the dataset-metric alignment framework by applying multiple metrics to a single counterfactual dataset and comparing results to identify mismatches.
2. Implement a pre-processing mitigation technique on a training corpus and evaluate its effectiveness using both embedding-based and generated text-based metrics.
3. Compare the survey's fairness desiderata against a case study of bias harms in a specific NLP application to identify potential gaps.