---
ver: rpa2
title: Are Vision Transformers More Data Hungry Than Newborn Visual Systems?
arxiv_id: '2312.02843'
source_url: https://arxiv.org/abs/2312.02843
tags:
- object
- learning
- chicks
- vits
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared the data efficiency of vision transformers
  (ViTs) and newborn chicks on a view-invariant object recognition task. Chicks were
  raised in impoverished environments containing a single object, and their visual
  experiences were digitally simulated in a video game engine.
---

# Are Vision Transformers More Data Hungry Than Newborn Visual Systems?

## Quick Facts
- arXiv ID: 2312.02843
- Source URL: https://arxiv.org/abs/2312.02843
- Reference count: 40
- Primary result: ViTs match newborn chick performance on view-invariant object recognition when trained on identical embodied visual data streams

## Executive Summary
This study investigates whether vision transformers require more training data than newborn visual systems by comparing their performance on a view-invariant object recognition task. Newborn chicks were raised in impoverished environments with a single rotating object, and their visual experiences were digitally simulated in a video game engine. When vision transformers (ViT-CoT and VideoMAE) were trained on these same simulated experiences using temporal contrastive learning, they performed on par with or better than the chicks across various architecture sizes. The results demonstrate that ViTs are not inherently more data-hungry than newborn visual systems when provided with appropriate embodied data streams and learning objectives.

## Method Summary
The study used a controlled-rearing paradigm where newborn chicks were raised in chambers with a single object rotating through a 60° viewpoint range. These visual experiences were digitally simulated using Unity 3D, generating 80,000 first-person images at 64x64 resolution. Two self-supervised ViT architectures (ViT-CoT and VideoMAE) were trained using contrastive learning through time (CLTT) with a 300ms temporal window. Performance was evaluated using linear classifiers on frozen representations, comparing accuracy across novel viewpoints with the same evaluation protocol used for chicks.

## Key Results
- ViTs trained on embodied visual data streams matched or exceeded newborn chick performance on view-invariant object recognition
- Larger ViT architectures were not more data-hungry than smaller ones when trained with temporal contrastive learning
- Performance improved systematically with more training images across all architecture sizes
- ViTs showed view-invariance despite lacking explicit spatial inductive bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision Transformers can learn view-invariant object recognition without explicit spatial inductive bias when trained on embodied visual data streams.
- Mechanism: The temporal contrastive learning objective leverages the inherent temporal continuity in first-person visual experiences to bind object features across time, allowing the model to learn stable object representations despite viewpoint changes.
- Core assumption: Embodied visual data contains sufficient temporal structure to enable self-supervised learning of invariant object features.
- Evidence anchors:
  - [abstract] "When ViTs were trained 'through the eyes' of newborn chicks, the ViTs solved the same view-invariant object recognition tasks as the chicks."
  - [section] "The algorithm (Figure 2C) contrasts temporally adjacent instances (positive examples) against non-adjacent instances (negative examples), thereby learning representations that capture the underlying dynamics, context, and patterns across time."
  - [corpus] Weak evidence - no direct mention of temporal learning in related papers.
- Break condition: If the visual environment lacks temporal structure or contains excessive noise that disrupts temporal continuity.

### Mechanism 2
- Claim: Self-supervised ViTs with temporal learning can match or exceed newborn chick performance on view-invariant object recognition using the same training data.
- Mechanism: The contrastive learning through time (CLTT) objective function trains the model to make temporally adjacent frames more similar in embedding space while pushing apart non-adjacent frames, effectively learning temporal consistency as a proxy for object identity.
- Core assumption: Temporal proximity in first-person visual streams correlates with object identity stability.
- Evidence anchors:
  - [abstract] "When ViTs were trained 'through the eyes' of newborn chicks, the ViTs solved the same view-invariant object recognition tasks as the chicks."
  - [section] "losszt = −logexp(sim(zt, zt+1)/τ ) + exp(sim(zt, zt+2)/τ ) / 2NP exp(sim(zt, zk)/τ )" describes the CLTT loss function explicitly.
  - [corpus] Weak evidence - no direct mention of CLTT or temporal learning in related papers.
- Break condition: If the temporal window is too short to capture object consistency or too long to maintain temporal coherence.

### Mechanism 3
- Claim: Larger ViT architectures are not necessarily more data-hungry than smaller ones when trained on embodied visual data with appropriate temporal learning.
- Mechanism: Performance improves systematically with more training images across all architecture sizes, suggesting that the temporal learning objective provides sufficient inductive bias for learning invariant features regardless of model size.
- Core assumption: The temporal contrastive learning objective provides adequate regularization to prevent overfitting in larger architectures.
- Evidence anchors:
  - [section] "We observed nearly identical patterns of improvement across the small, medium, and large architecture sizes, indicating that larger ViT-CoTs were not more data hungry than smaller ViT-CoTs."
  - [section] "Performance was largely the same across the different learning windows" suggests robustness to architectural variations.
  - [corpus] No direct evidence - this is a novel finding not mentioned in related papers.
- Break condition: If the temporal learning objective fails to provide sufficient regularization for larger models, leading to overfitting on small datasets.

## Foundational Learning

- Concept: Contrastive Learning
  - Why needed here: Enables self-supervised learning without labels by contrasting positive (temporally adjacent) and negative (non-adjacent) examples
  - Quick check question: How does the CLTT loss function distinguish between temporally adjacent and non-adjacent frames?

- Concept: Temporal Continuity
  - Why needed here: Leverages the natural temporal structure in embodied visual experiences to learn stable object representations
  - Quick check question: Why is a 300ms temporal window used, and what biological evidence supports this choice?

- Concept: Embodied Data Streams
  - Why needed here: Provides rich, naturally augmented visual experiences through first-person perspective and self-motion
  - Quick check question: How does the virtual agent's head rotation provide data augmentation similar to newborn chicks?

## Architecture Onboarding

- Component map: Input patches → Positional embeddings + class token → Transformer blocks with self-attention → Output embedding → (Optional) Projection head for CLTT → Linear classifier for evaluation
- Critical path: The temporal contrastive learning objective is the critical component - without it, the model cannot learn from unlabeled data
- Design tradeoffs: CLTT trades explicit spatial inductive bias for learned temporal consistency; higher resolution images provide more detail but increase computational cost
- Failure signatures: Poor performance on novel viewpoints, overfitting to training viewpoint, failure to generalize across different architecture sizes
- First 3 experiments:
  1. Train ViT-CoT with 1, 3, 6, and 9 attention heads on the same 80k image dataset, evaluate on Ntrain=11, Ntest=1
  2. Vary the number of training images (0, 1.25k, 10k, 80k) for each architecture size, evaluate performance
  3. Compare ViT-CoT performance with VideoMAE and CNN baselines using the same training data and evaluation protocol

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the architectural inductive biases of CNNs and ViTs contribute to their different performance levels on view-invariant object recognition tasks?
- Basis in paper: [explicit] The paper notes that CNNs outperformed ViTs by 7.3% and 19.6% on the view-invariant object recognition task, potentially due to CNNs' strong architectural inductive bias.
- Why unresolved: While the paper acknowledges the performance gap, it doesn't explore the specific mechanisms by which CNNs' architectural inductive bias contributes to their superior performance.
- What evidence would resolve it: Detailed comparative analyses of feature representations and attention mechanisms in CNNs and ViTs during object recognition tasks.

### Open Question 2
- Question: Can ViTs achieve comparable performance to CNNs on view-invariant object recognition tasks if trained with additional architectural modifications or larger datasets?
- Basis in paper: [inferred] The paper suggests that ViTs' flexibility allows them to learn more abstract and generalizable features than CNNs, but they are often regarded as more data-hungry.
- Why unresolved: The study doesn't explore whether architectural modifications or increased training data could improve ViTs' performance to match or exceed that of CNNs.
- What evidence would resolve it: Experiments comparing ViTs with different architectural modifications and varying amounts of training data against CNNs on the same object recognition tasks.

### Open Question 3
- Question: How does the embodiment of ViTs in artificial agents that collect their own training data compare to passive training in terms of learning efficiency and performance?
- Basis in paper: [explicit] The paper mentions that models were trained passively, learning from pre-specified batches of images, contrasting with the active learning of newborn animals who interact with their environment.
- Why unresolved: The study doesn't investigate the potential benefits of active data collection and embodied learning for ViTs.
- What evidence would resolve it: Comparative studies of ViTs trained through passive data ingestion versus those trained through active exploration and data collection in virtual environments.

## Limitations

- Virtual rearing environments were highly simplified compared to real-world visual experiences
- Study only tested one specific type of temporal learning objective (CLTT)
- Results may not generalize to other object categories or more complex visual scenes
- Biological comparison is limited to one species (chicks) and one specific developmental stage

## Confidence

- High confidence: ViTs can match chick performance on view-invariant object recognition when trained on identical data
- Medium confidence: Generalizability to more complex visual environments
- Medium confidence: Scalability claims beyond 80k training images

## Next Checks

1. Test ViT-CoT performance on more complex visual environments with multiple objects, occlusions, and naturalistic backgrounds to assess robustness beyond controlled conditions
2. Evaluate whether other temporal learning objectives (beyond CLTT) can achieve similar data efficiency when applied to embodied visual streams
3. Conduct ablation studies systematically varying the temporal learning window, resolution, and data augmentation parameters to identify optimal conditions for data-efficient learning