---
ver: rpa2
title: Self-Supervised Detection of Perfect and Partial Input-Dependent Symmetries
arxiv_id: '2312.12223'
source_url: https://arxiv.org/abs/2312.12223
tags:
- symmetry
- group
- symmetries
- distribution
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of detecting input-dependent
  symmetries in datasets where different classes exhibit varying symmetry levels,
  such as in images of cars and planes. The authors propose a self-supervised method
  to learn these input-specific symmetry levels without requiring labels.
---

# Self-Supervised Detection of Perfect and Partial Input-Dependent Symmetries

## Quick Facts
- arXiv ID: 2312.12223
- Source URL: https://arxiv.org/abs/2312.12223
- Reference count: 40
- The paper proposes a self-supervised method to learn input-specific symmetry levels without labels, validated on synthetic datasets with varying per-class symmetry levels.

## Executive Summary
This paper addresses the challenge of detecting input-dependent symmetries in datasets where different classes exhibit varying symmetry levels, such as in images of cars and planes. The authors propose a self-supervised method to learn these input-specific symmetry levels without requiring labels. Their approach involves training a modified Invariant-Equivariant Autoencoder (IE-AE) to capture the distribution of symmetries, then using the learned distribution to generate pseudo-labels for self-supervised training of a symmetry boundary prediction network. This network predicts the symmetry level for each input. The method is validated on synthetic datasets with different per-class symmetry levels, demonstrating accurate predictions of symmetry boundaries. Additionally, the approach enables practical applications like detecting out-of-distribution symmetries during inference and generating standardized datasets by removing symmetries.

## Method Summary
The method learns input-dependent symmetry boundaries by constraining the group action estimator to collapse canonical representations to symmetry centers. After pre-training the constrained IE-AE, the approach finds k-nearest neighbors in G-invariant space for each input, estimates their symmetry boundary using the group action estimator, and trains a boundary prediction network to match these pseudo-labels. The symmetry standardization process removes rotational symmetries by reorienting inputs to their canonical positions using the inverse transformation predicted by the group action estimator.

## Key Results
- Accurate prediction of symmetry boundaries across various symmetry distributions including uniform, Gaussian, and discrete cyclic groups
- Mean Absolute Error (MAE) between predictions and true boundary angles consistently below 2° on synthetic datasets
- Successful detection of out-of-distribution symmetries during inference and generation of standardized datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method learns input-dependent symmetry boundaries by constraining the group action estimator to collapse canonical representations to symmetry centers.
- Mechanism: The constrained IE-AE enforces ψ(c[x])=e for each equivalence class, ensuring consistent canonical representations. This allows the pseudo-label generation process to accurately estimate symmetry boundaries per input.
- Core assumption: Inputs with similar G-invariant embeddings share the same symmetry distribution.
- Evidence anchors:
  - [abstract] "We derive a sufficient and necessary condition to learn the distribution of symmetries in the data"
  - [section 3.1] "Consider the equivalence relation ∼G in X defined by x ∼G y if and only if ∃g ∈G such that x=ρX(g)y"
  - [corpus] Weak - no direct mention of symmetry center collapsing, but partial symmetry detection methods exist
- Break condition: If inputs with similar embeddings have different symmetry distributions, the pseudo-labels become unreliable.

### Mechanism 2
- Claim: The self-supervised training uses pseudo-labels from nearest neighbors in G-invariant space to predict symmetry boundaries.
- Mechanism: After pre-training the constrained IE-AE, the method finds k-nearest neighbors in Zinv for each input, estimates their symmetry boundary using the group action estimator, and trains a boundary prediction network to match these pseudo-labels.
- Core assumption: The k-nearest neighbors in G-invariant space share the same symmetry distribution as the query input.
- Evidence anchors:
  - [section 3.4] "Let Nk,d=Nk ∶X /∫hortrightarrowP(X) be a function that maps each input x to the set Nk,d(x) ⊂X of k-neighbors"
  - [section 3.3] "we base our approach on the convenient assumption that similar objects share the same distribution of symmetries"
  - [corpus] Weak - no direct mention of neighbor-based pseudo-labeling, but contrastive learning methods exist
- Break condition: If nearest neighbors have heterogeneous symmetry distributions, the pseudo-labels become noisy.

### Mechanism 3
- Claim: The symmetry standardization process removes rotational symmetries by reorienting inputs to their canonical positions.
- Mechanism: Using the constrained group action estimator, the method applies the inverse transformation ψ(x)−1 to each input, collapsing it to the center of symmetry of its equivalence class.
- Core assumption: The constrained group action estimator correctly identifies the transformation needed to reach the symmetry center.
- Evidence anchors:
  - [section 3.5] "the inverse of the group actions predicted by ψ can be used to reorient the input towards the center of symmetry"
  - [section 2.1] "ρX(ψ(x)) δ(η(x)) = x, ∀x∈X" defines the suitable group action estimator property
  - [corpus] Weak - no direct mention of symmetry standardization, but data augmentation methods exist
- Break condition: If the group action estimator doesn't correctly identify symmetry centers, the standardization produces incorrect orientations.

## Foundational Learning

- Concept: Group equivariance and invariance
  - Why needed here: The entire framework relies on understanding how transformations affect inputs and outputs in neural networks
  - Quick check question: What's the difference between a G-equivariant and G-invariant function?

- Concept: Equivalence classes and quotient spaces
  - Why needed here: The method groups inputs into equivalence classes based on shared symmetries, which is fundamental to the approach
  - Quick check question: How does the equivalence relation ∼G define which inputs belong to the same class?

- Concept: Probability distributions on groups
  - Why needed here: The method learns probability distributions over symmetry transformations to characterize partial symmetries
  - Quick check question: Why is learning a uniform distribution U[-θ,θ] sufficient for characterizing partial rotational symmetries?

## Architecture Onboarding

- Component map:
  - Constrained IE-AE (pre-training): G-invariant encoder η, G-equivariant group action estimator ψ, decoder δ
  - Boundary prediction network Θ (self-supervised): G-invariant network ϕ + fully connected network ω
  - Symmetry standardization module (inference): Uses trained ψ to reorient inputs

- Critical path:
  1. Pre-train constrained IE-AE using loss L = L1 + L2
  2. Generate pseudo-labels using k-nearest neighbors and method of moments estimator
  3. Train boundary prediction network Θ to minimize L3
  4. Use trained ψ for symmetry standardization or Θ for symmetry prediction

- Design tradeoffs:
  - More neighbors k → more accurate pseudo-labels but higher computational cost and risk of heterogeneous distributions
  - Stronger L2 constraint weight → better canonical representations but potentially harder optimization
  - Higher dimensional Zinv → better separation of symmetry distributions but more neighbors needed

- Failure signatures:
  - Poor symmetry boundary predictions → check neighbor selection and pseudo-label quality
  - Inconsistent canonical representations → check L2 constraint strength and IE-AE training
  - High variance in predictions → check number of neighbors and outlier detection method

- First 3 experiments:
  1. Train on MNIST with full 60° rotation symmetry - should predict θ=60° consistently
  2. Train on MNIST with class-dependent symmetries (0-4: 60°, 5-9: 90°) - should predict different θ per class
  3. Train on MNIST with increasing symmetries per class (0: 0°, 1: 18°, 2: 36°, etc.) - should predict increasing θ per class

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the method perform on real-world datasets like CIFAR-10 where different classes have varying symmetry levels?
- Basis in paper: [explicit] The paper mentions that the method could be applied to CIFAR-10 but does not provide experimental results or analysis on this dataset.
- Why unresolved: The paper focuses on synthetic MNIST datasets and does not test the method on more complex, real-world datasets.
- What evidence would resolve it: Experiments applying the method to CIFAR-10 or similar real-world datasets, comparing performance to other methods, and analyzing results for different classes.

### Open Question 2
- Question: What is the impact of the choice of hyperparameters like the number of neighbors k and the learning rate on the method's performance?
- Basis in paper: [explicit] The paper mentions these hyperparameters but does not provide a systematic analysis of their impact on the method's performance.
- Why unresolved: The paper only reports results using specific hyperparameter values without exploring the sensitivity of the method to these choices.
- What evidence would resolve it: Experiments varying the hyperparameters and analyzing their impact on the method's accuracy and stability.

### Open Question 3
- Question: How does the method handle multimodal symmetry distributions where a single input might have multiple symmetry centers?
- Basis in paper: [inferred] The paper focuses on unimodal symmetry distributions and mentions this as a limitation.
- Why unresolved: The method is designed for unimodal distributions and may not generalize to more complex, multimodal cases.
- What evidence would resolve it: Experiments testing the method on datasets with multimodal symmetry distributions and analyzing its ability to correctly identify multiple symmetry centers.

## Limitations
- The method relies heavily on the assumption that nearest neighbors in G-invariant space share the same symmetry distribution, which may not hold for heterogeneous datasets
- Performance depends on careful tuning of hyperparameters including L2 constraint weight (0.03125) and number of neighbors k
- The approach is currently limited to discrete symmetry groups and may not generalize to continuous symmetry detection

## Confidence
- Mechanism 1 (symmetry center collapsing): Low confidence - while the theoretical framework supports this, direct experimental validation of the collapsing behavior is limited
- Mechanism 2 (neighbor-based pseudo-labeling): Medium confidence - the method shows consistent results across synthetic datasets, but the assumption of homogeneous neighbor distributions needs further testing
- Mechanism 3 (symmetry standardization): Medium confidence - the theoretical foundation is sound, but practical effectiveness on real-world data remains to be demonstrated

## Next Checks
1. Test the method on real-world datasets (e.g., CIFAR-10) to evaluate performance on naturally occurring symmetry variations
2. Conduct ablation studies to quantify the impact of different k values and L2 constraint strengths on prediction accuracy
3. Validate the symmetry standardization process by measuring the reduction in variance of symmetry predictions after reorientation