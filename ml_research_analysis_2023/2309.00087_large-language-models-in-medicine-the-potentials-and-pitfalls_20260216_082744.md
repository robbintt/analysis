---
ver: rpa2
title: 'Large language models in medicine: the potentials and pitfalls'
arxiv_id: '2309.00087'
source_url: https://arxiv.org/abs/2309.00087
tags:
- llms
- medical
- language
- medicine
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews large language models (LLMs) and their applications
  in medicine, covering their development, current uses, limitations, and future directions.
  It provides a tutorial section with practical examples of using ChatGPT in medical
  tasks like writing authorization letters, creating patient handouts, and generating
  exam questions.
---

# Large language models in medicine: the potentials and pitfalls

## Quick Facts
- arXiv ID: 2309.00087
- Source URL: https://arxiv.org/abs/2309.00087
- Reference count: 0
- Primary result: Reviews LLMs in medicine covering development, applications, limitations, and future directions

## Executive Summary
This paper provides a comprehensive review of large language models in medical applications, examining their development from transformer architectures through current implementations like GPT-4 and specialized medical models. The authors present practical tutorials demonstrating ChatGPT applications in medical tasks including documentation, patient communication, and education. Key challenges identified include accuracy limitations, bias concerns, privacy issues, and the need for regulatory frameworks. The paper emphasizes that while LLMs show promise for medical applications, careful human oversight and domain-specific fine-tuning are essential for safe deployment.

## Method Summary
The paper synthesizes existing literature on medical LLMs through a comprehensive review approach, organizing findings around development history, current applications, limitations, and future directions. A tutorial section provides practical examples using publicly available ChatGPT, demonstrating zero-shot and few-shot prompting techniques for common medical tasks. The methodology relies on literature surveys rather than original empirical studies, with evidence drawn from published papers and technical documentation. No training or model development was performed as part of this review.

## Key Results
- Domain-specific fine-tuning on medical corpora improves biomedical task performance compared to general LLMs
- Multi-modal LLMs that process both text and images show promise for handling the multifaceted nature of medical data
- Current publicly available LLMs lack HIPAA compliance and raise significant privacy concerns for clinical use

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning large language models on domain-specific medical corpora produces models that outperform general-purpose LLMs on biomedical tasks.
- Mechanism: Domain-specific pre-training on medical literature allows the model to learn specialized vocabulary, clinical concepts, and medical reasoning patterns that are not present in general text corpora.
- Core assumption: Medical texts contain unique linguistic patterns and knowledge structures that require specialized training to capture effectively.
- Evidence anchors:
  - [section]: "Domain-specific pre-training on medical corpora produces models that excel on biomedical tasks compared to generalist LLMs (with some exceptions like GPT-4)."
  - [corpus]: Weak evidence - only general surveys of medical LLMs found, no direct comparison studies cited in corpus.
- Break condition: If the domain-specific dataset is too small or contains biased/incorrect medical information, the fine-tuned model may perform worse than general models or perpetuate errors.

### Mechanism 2
- Claim: Multi-modal LLMs that process both text and images achieve better performance on medical tasks than text-only models.
- Mechanism: Medical data inherently combines multiple modalities (text reports, imaging, patient records), and models that can process this multi-modal input can capture richer contextual information and relationships.
- Core assumption: Medical information is inherently multi-modal and requires integrated processing of different data types.
- Evidence anchors:
  - [section]: "Recent studies such as LLaVa-Med, SkinGPT-4, and MiniGPT-4 provide compelling evidence for the effectiveness of multi-modal LLMs, which are poised to gain prevalence in healthcare due to the multi-faceted nature of medical data that spans text, images, audio, and genetics."
  - [corpus]: No direct evidence in corpus - only general surveys found.
- Break condition: If image processing is not properly aligned with text understanding, or if the model lacks sufficient training data for the combined modalities, performance may degrade.

### Mechanism 3
- Claim: Instruction tuning and few-shot prompting techniques can effectively adapt general LLMs to medical tasks without extensive retraining.
- Mechanism: By providing examples or instructions during inference, the model can leverage its general language understanding to perform specialized medical tasks without domain-specific pre-training.
- Core assumption: General LLMs possess sufficient underlying language understanding to adapt to new domains with appropriate prompting.
- Evidence anchors:
  - [section]: "This approach offers significant benefits over the few-shot prompt approaches, particularly for clinical applications as demonstrated by Singhal et al."
  - [corpus]: No direct evidence in corpus - only general surveys found.
- Break condition: If the task requires deep medical knowledge that wasn't captured in the general training, instruction tuning may produce inaccurate or nonsensical outputs.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how LLMs process language is crucial for understanding their capabilities and limitations in medical applications
  - Quick check question: What is the key innovation that allows transformers to handle long-range dependencies in text better than previous architectures?

- Concept: Fine-tuning vs pre-training
  - Why needed here: Different training approaches (fine-tuning vs instruction tuning vs few-shot) have different implications for medical LLM development and deployment
  - Quick check question: What is the main difference between pre-training and fine-tuning in terms of data requirements and computational cost?

- Concept: Bias and fairness in AI systems
  - Why needed here: Medical applications have direct impact on patient care, making bias and fairness critical considerations
  - Quick check question: Why might medical LLMs perpetuate or amplify existing healthcare disparities?

## Architecture Onboarding

- Component map: Base LLM (GPT, BERT, PaLM variants) -> Pre-training corpus (general web data, medical literature) -> Fine-tuning datasets (clinical notes, medical exams, patient interactions) -> Evaluation benchmarks (MedMCQA, PubMedQA, clinical accuracy metrics) -> Safety and bias detection systems -> Human oversight and feedback mechanisms

- Critical path: Data → Pre-training → Fine-tuning → Evaluation → Deployment → Monitoring
  - Each stage must maintain data quality and address bias concerns
  - Human oversight is critical at multiple stages

- Design tradeoffs:
  - General vs specialized models: General models are more flexible but may lack domain accuracy; specialized models are more accurate but less adaptable
  - Size vs efficiency: Larger models perform better but require more computational resources
  - Privacy vs performance: Using real clinical data improves performance but raises privacy concerns

- Failure signatures:
  - Hallucinations: Fabricated medical information presented confidently
  - Bias amplification: Systematic errors affecting certain patient populations
  - Out-of-distribution errors: Poor performance on rare conditions or edge cases
  - Privacy violations: Disclosure of sensitive patient information

- First 3 experiments:
  1. Compare general LLM vs medical fine-tuned model on standard medical QA benchmarks
  2. Test few-shot prompting effectiveness for common medical tasks (patient communication, documentation)
  3. Evaluate bias patterns across different demographic groups using medical scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can domain-specific fine-tuning of large language models improve their accuracy and reduce bias in medical applications?
- Basis in paper: [explicit] The paper discusses the limitations of current LLMs in medicine, including accuracy issues and inherent biases, and suggests that training these models on domain-specific datasets could mitigate these problems.
- Why unresolved: While the paper proposes domain-specific fine-tuning as a potential solution, it does not provide concrete evidence or case studies demonstrating the effectiveness of this approach in improving LLM performance in medical tasks.
- What evidence would resolve it: Comparative studies evaluating the performance of domain-specific fine-tuned LLMs against general-purpose models on medical benchmarks, measuring accuracy, bias reduction, and clinical relevance.

### Open Question 2
- Question: What are the long-term effects of using large language models in medical education and clinical decision-making?
- Basis in paper: [inferred] The paper mentions the potential use of LLMs in medical education and clinical workflow support, but does not explore the long-term implications of relying on these models for training and decision-making.
- Why unresolved: The novelty of LLM technology in medicine means that its long-term effects on medical education quality, clinical decision-making processes, and patient outcomes have not been thoroughly studied.
- What evidence would resolve it: Longitudinal studies tracking the performance and career development of medical professionals trained with LLM assistance, as well as patient outcome analyses in clinical settings utilizing LLM support over extended periods.

### Open Question 3
- Question: How can large language models be effectively integrated into electronic health record systems while maintaining HIPAA compliance and data privacy?
- Basis in paper: [explicit] The paper highlights privacy concerns and the lack of HIPAA compliance in current publicly available LLM models, suggesting integration within EHR systems as a potential mitigation strategy.
- Why unresolved: The paper does not provide specific technical or regulatory frameworks for integrating LLMs into EHR systems while ensuring compliance with healthcare privacy laws and protecting patient data.
- What evidence would resolve it: Case studies of successful LLM integration into EHR systems, demonstrating HIPAA compliance, data security measures, and improved clinical workflows without compromising patient privacy.

## Limitations

- Limited empirical evidence: Many claims rely on literature surveys rather than controlled experiments or direct comparisons
- Rapid technological evolution: Findings may become outdated quickly due to fast-paced LLM development
- Privacy and compliance gaps: No clear framework for HIPAA-compliant LLM integration in clinical settings

## Confidence

- **High Confidence**: The general observation that LLMs can perform various medical writing and documentation tasks. The tutorial section provides reproducible examples that work with current ChatGPT implementations.
- **Medium Confidence**: Claims about domain-specific fine-tuning advantages and multi-modal model benefits. While theoretically sound and supported by some literature, direct comparative evidence from controlled studies is limited in the corpus.
- **Low Confidence**: Predictions about future regulatory frameworks and the timeline for widespread clinical adoption. These depend on complex sociotechnical factors beyond current technical capabilities.

## Next Checks

1. **Direct Comparative Study**: Conduct controlled experiments comparing general LLMs (GPT-3.5, GPT-4) against domain-specific fine-tuned models on standardized medical benchmarks like MedMCQA and PubMedQA to quantify performance differences.

2. **Bias Impact Assessment**: Design and execute a systematic evaluation of bias patterns across demographic groups using medical scenarios, measuring both accuracy disparities and potential harm to patient care.

3. **Multi-Modal Validation**: Test multi-modal LLMs (e.g., LLaVa-Med, SkinGPT-4) on integrated medical tasks combining text and imaging data to verify claimed performance improvements over text-only approaches.