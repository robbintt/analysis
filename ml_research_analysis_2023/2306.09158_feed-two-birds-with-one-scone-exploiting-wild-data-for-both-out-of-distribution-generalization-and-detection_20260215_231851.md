---
ver: rpa2
title: 'Feed Two Birds with One Scone: Exploiting Wild Data for Both Out-of-Distribution
  Generalization and Detection'
arxiv_id: '2306.09158'
source_url: https://arxiv.org/abs/2306.09158
tags:
- data
- learning
- generalization
- detection
- wild
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a unified framework SCONE that jointly tackles
  out-of-distribution (OOD) generalization and OOD detection by exploiting freely
  available unlabeled wild data that naturally captures test-time OOD distributions
  under both covariate and semantic shifts. The key idea is to enforce an energy margin
  constraint on in-distribution data while optimizing a binary OOD detector based
  on the energy function, which provably and empirically leads to improved OOD generalization
  without sacrificing OOD detection performance.
---

# Feed Two Birds with One Scone: Exploiting Wild Data for Both Out-of-Distribution Generalization and Detection

## Quick Facts
- arXiv ID: 2306.09158
- Source URL: https://arxiv.org/abs/2306.09158
- Reference count: 40
- Primary result: Unified framework SCONE achieves both OOD generalization and detection by leveraging wild data with energy margin constraints

## Executive Summary
This paper addresses the challenge of achieving both out-of-distribution (OOD) generalization and OOD detection using unlabeled wild data that naturally captures test-time OOD distributions. The proposed SCONE framework exploits freely available wild data containing covariate and semantic shifts to train a model that can both correctly classify covariate-shifted inputs and detect semantic-shifted OOD samples. By enforcing an energy margin constraint on in-distribution data while optimizing a binary OOD detector based on the energy function, SCONE achieves state-of-the-art performance on CIFAR-10 and ImageNet datasets, notably improving OOD accuracy from 52.76% to 84.69% on covariate-shifted CIFAR-10-C data.

## Method Summary
SCONE is a unified framework that jointly tackles OOD generalization and detection by exploiting wild data containing three components: in-distribution data, covariate-shifted OOD, and semantic-shifted OOD. The key innovation is a margin-based energy constraint that enforces a separation between in-distribution data and the OOD detector, enabling correct classification of covariate-shifted inputs while maintaining detection capability for semantic OOD. The method uses an energy-based OOD detector with a margin parameter η that is optimized through a constrained optimization problem solved via the Augmented Lagrangian method. The framework trains on a mixture of wild data (1−πs−πc)Pin + πcPcovariate_out + πsPsemantic_out using a Wide ResNet architecture with standard SGD optimization.

## Key Results
- Improves OOD accuracy from 52.76% to 84.69% on CIFAR-10-C covariate-shifted data
- Achieves better OOD detection (lower FPR, higher AUROC) than WOODS baseline across 19 corruption types
- Outperforms competitive baselines on CIFAR-10 and ImageNet-100 datasets
- Demonstrates robustness to varying wild data compositions and corruption levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The margin constraint enforces energy separation between ID and covariate-OOD data, enabling correct classification of covariate-shifted inputs.
- Mechanism: By constraining ID data to have energy less than a negative margin η, the model is forced to increase the logits for covariate-OOD points that are semantically related to ID data, pushing them toward the correct class decision boundary.
- Core assumption: Covariate-shifted data points are close to their corresponding ID data points in feature space.
- Evidence anchors:
  - [abstract] "the proposed margin constraint is the key to achieving both OOD generalization and detection"
  - [section 3.3] "If η < − log 2 − 1/2 Lδ then each covariate-shifted point is classified correctly and is detected as semantic IN"
  - [corpus] Weak - no direct evidence found

### Mechanism 2
- Claim: The energy margin creates a buffer zone that prevents covariate-OOD data from being misclassified as semantic OUT.
- Mechanism: Without margin (η = 0), the OOD detector tries to classify as many wild samples as OUT, pushing the decision boundary too close to ID data. With margin, the boundary moves away from ID data, correctly placing covariate-OOD data on the IN side.
- Core assumption: The wild data mixture contains both covariate and semantic OOD components that need different treatment.
- Evidence anchors:
  - [section 3.2] "the OOD detection boundary undesirably places the covariate-shifted data...on the wrong side—labeling it as semantic OUT"
  - [section 3.2] "our key idea is to enforce a sufficient margin between the OOD detector and the ID data"
  - [corpus] Weak - no direct evidence found

### Mechanism 3
- Claim: The margin-based optimization jointly improves both OOD generalization and OOD detection by leveraging the structure of wild data.
- Mechanism: The optimization problem simultaneously learns a robust multi-class classifier (via cross-entropy loss) and a binary OOD detector (via energy margin), exploiting the natural structure of wild data that contains all three types of distributions.
- Core assumption: The wild data mixture Pwild := (1 − πs − πc)Pin + πcPcovariate_out + πsPsemantic_out is available and representative.
- Evidence anchors:
  - [abstract] "exploits freely available unlabeled data in the wild that naturally captures the environmental test-time OOD distributions"
  - [section 3.2] "Our framework offers substantial advantages over the counterpart approaches that rely only on the ID data"
  - [corpus] Weak - no direct evidence found

## Foundational Learning

- Concept: Out-of-Distribution (OOD) Detection
  - Why needed here: The paper aims to detect semantic-shifted OOD data while generalizing to covariate-shifted data.
  - Quick check question: What is the difference between covariate shift and semantic shift in the context of OOD detection?

- Concept: Energy-Based Models
  - Why needed here: The method uses energy functions as OOD scores and enforces margin constraints on energy values.
  - Quick check question: How does the energy function Eθ(x) = -log(Σj exp(f(j)θ(x))) relate to the classification logits?

- Concept: Domain Generalization
  - Why needed here: The paper addresses OOD generalization, which is a form of domain generalization without access to target domain data.
  - Quick check question: What is the key difference between domain adaptation and domain generalization?

## Architecture Onboarding

- Component map: Input -> Feature extractor -> Multi-class classifier, Input -> Feature extractor -> Energy function -> OOD detector, Margin constraint module, Optimization module (Augmented Lagrangian)

- Critical path: 1. Input → Feature extractor → Multi-class classifier, 2. Input → Feature extractor → Energy function → OOD detector, 3. Compute margin constraint violation, 4. Update parameters using constrained optimization

- Design tradeoffs:
  - Margin value η: Larger margins improve OOD generalization but may slightly degrade OOD detection
  - Wild data composition: Balance between covariate and semantic OOD components
  - Feature representation: Quality of feature space affects the effectiveness of margin enforcement

- Failure signatures:
  - OOD generalization degrades when covariate-OOD data is too far from ID data in feature space
  - OOD detection performance drops when semantic-OOD data is too close to ID data
  - Training instability when α and τ constraints are too tight

- First 3 experiments:
  1. Train with η = 0 (no margin) vs. η = -10 to observe impact on OOD generalization
  2. Vary πc (covariate-OOD fraction) to test robustness to different wild data compositions
  3. Compare feature embeddings (t-SNE) with and without margin to visualize decision boundary effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the optimal margin parameter η vary across different covariate shift types and corruption levels in the wild data?
- Basis in paper: The paper mentions that η is selected based on a validation heuristic measuring the fraction of wild samples predicted as OUT, and that optimal values were found experimentally for CIFAR-10/CIFAR-10-C/SVHN settings.
- Why unresolved: The ablation in Table 10 shows SCONE outperforms WOODS for all 19 corruption types, but the paper does not report how η was tuned for each corruption type or whether a single η value works universally.
- What evidence would resolve it: A systematic ablation showing OOD accuracy/FPR/AUROC for different η values for each of the 19 corruption types would reveal whether η needs per-corruption tuning or if a universal value exists.

### Open Question 2
- Question: Can the margin-based framework be extended to handle continuous semantic shifts where unknown classes gradually morph into known classes, rather than binary IN/OUT decisions?
- Basis in paper: The current framework assumes semantic OOD data is completely outside the known label space Y and should be rejected, while covariate OOD is semantically related to ID data. The paper does not explore gradual semantic similarity.
- Why unresolved: Real-world deployments may encounter samples that are partially related to known classes (e.g., hybrid objects or fine-grained subclasses). The binary energy margin may not optimally handle such cases.
- What evidence would resolve it: Experiments on datasets with hierarchical or continuous semantic relationships (e.g., CIFAR-100 with coarse/fine labels, or synthetic morphing datasets) showing whether margin-based training can be adapted to score such intermediate cases.

### Open Question 3
- Question: What is the impact of the wild data mixing ratio πc on the theoretical generalization bounds for OOD detection, and can tighter bounds be derived?
- Basis in paper: The paper provides a theoretical insight (Proposition 3.1) showing that with an appropriate η, covariate-shifted points near ID points are correctly classified, but this is under idealized assumptions. The empirical ablation in Table 4 shows performance degrades with high πc for WOODS but SCONE is more robust.
- Why unresolved: The theory does not quantify how πc affects the margin's effectiveness or provide explicit generalization bounds that depend on πc. The empirical results suggest robustness but lack theoretical justification.
- What evidence would resolve it: A formal analysis deriving generalization bounds for the energy margin approach that explicitly depend on πc, distance δ between covariate and ID features, and margin η, validated against empirical performance across πc settings.

## Limitations
- The theoretical analysis relies on assumptions about Lipschitz continuity and feature space proximity that may not hold for complex real-world data
- The method's performance on real-world datasets with complex distribution shifts remains underexplored
- The sensitivity to hyperparameters like η and wild data composition ratios needs more systematic investigation

## Confidence
- **High confidence**: The core algorithmic framework and experimental methodology are well-defined
- **Medium confidence**: The empirical improvements on benchmark datasets are significant but may not generalize
- **Low confidence**: The theoretical bounds and assumptions in the analysis

## Next Checks
1. **Ablation on wild data composition**: Systematically vary πc and πs ratios to quantify their impact on OOD generalization vs. detection tradeoff
2. **Feature space analysis**: Visualize and quantify the distance distributions between ID, covariate-OOD, and semantic-OOD points in the learned feature space
3. **Robustness to dataset shifts**: Test SCONE on datasets with non-Gaussian corruption patterns and more complex semantic shifts beyond the current evaluation scope