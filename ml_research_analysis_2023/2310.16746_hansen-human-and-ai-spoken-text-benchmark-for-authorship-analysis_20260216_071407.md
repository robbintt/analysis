---
ver: rpa2
title: 'HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis'
arxiv_id: '2310.16746'
source_url: https://arxiv.org/abs/2310.16746
tags:
- text
- spoken
- datasets
- texts
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HANSEN, a benchmark dataset for spoken text
  authorship analysis. The dataset contains 17 human-spoken datasets and AI-generated
  spoken text created using ChatGPT, PaLM2, and Vicuna13B.
---

# HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis

## Quick Facts
- arXiv ID: 2310.16746
- Source URL: https://arxiv.org/abs/2310.16746
- Reference count: 40
- Primary result: Introduces HANSEN benchmark for spoken text authorship analysis, showing character n-grams outperform other features for AA and highlighting need for better AI-generated text detection methods.

## Executive Summary
This paper introduces HANSEN, a benchmark dataset for spoken text authorship analysis containing 17 human-spoken datasets and AI-generated spoken text from ChatGPT, PaLM2, and Vicuna13B. The authors evaluate state-of-the-art methods for Authorship Attribution (AA), Author Verification (AV), and Human vs. AI spoken text detection. While existing methods perform comparably on human-spoken text to written text, there is significant room for improvement in detecting AI-generated spoken text. The benchmark reveals that character n-grams are particularly effective for AA on spoken text, and that written and spoken text exhibit distinct stylistic patterns requiring separate analysis approaches.

## Method Summary
The HANSEN benchmark provides a comprehensive framework for evaluating authorship analysis on spoken text. The authors implement multiple methods across three tasks: N-gram, Stylometry, FastText+LSTM, BERT-AA, and Finetuned BERT for AA; N-gram similarity, PPM, Stylometry differences, Adhominem, and Finetuned BERT for AV; OpenAI detector, Roberta-Large, DetectGPT, and GPT-Zero for Human vs. AI detection. Experiments are conducted five times with average metrics reported. The benchmark reveals modality-specific challenges and performance differences across methods.

## Key Results
- Character n-grams outperform word n-grams by 5-10% for AA on spoken text
- Finetuned BERT models substantially improve performance compared to BERT-AA
- Speaker verification performance drops significantly when training on written text and testing on spoken text (and vice versa)
- Current methods show much room for improvement in AI-generated spoken text detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Character n-gram features outperform other features for AA on spoken text.
- Mechanism: Character n-grams capture fine-grained stylistic patterns in informal, repetitive spoken language better than word n-grams or embeddings.
- Core assumption: Spoken text has unique surface-level patterns that character n-grams can exploit.
- Evidence anchors:
  - Character n-grams outperform word n-grams by a notable margin (5%-10% in general), emphasizing the potential enhancement of AA performance through the inclusion of informal words (e.g., 'eh,' 'err,' 'uhh').
  - Character n-gram performs best in most scenarios, with BERT-ft being a close contender.
  - Weak - no direct corpus comparison provided.

### Mechanism 2
- Claim: Finetuned BERT models improve performance on spoken text classification tasks.
- Mechanism: Pre-training on general text followed by task-specific finetuning allows BERT to adapt to spoken text characteristics.
- Core assumption: Spoken text has enough shared structure with written text for pre-training to be useful, but requires adaptation.
- Evidence anchors:
  - BERT-AA is subpar when directly applied to spoken text datasets, but finetuning them improves performance substantially.
  - Finetuned BERT shows strong performance in both AA and AV tasks.
  - Weak - no direct corpus comparison of pre-trained vs finetuned performance.

### Mechanism 3
- Claim: Speaker verification performance drops when training on written text and testing on spoken text (and vice versa).
- Mechanism: Written and spoken text have distinct stylistic features that are not transferable between modalities.
- Core assumption: Individual stylistic patterns are modality-dependent.
- Evidence anchors:
  - Our results affirm the distinctions between individuals' written and spoken texts, consistent with prior corpus-based research (Biber et al., 2000; Farahani et al., 2020).
  - The results in Table 9 show a substantial decrease in the macro f1 score for both character n-gram and BERT-ft.
  - Weak - limited to SEC and PAN datasets with parallel written/spoken samples.

## Foundational Learning

- Concept: N-gram features (character and word level)
  - Why needed here: N-grams are fundamental features for stylometric analysis and serve as baselines for comparing more complex models.
  - Quick check question: What's the difference between character n-grams and word n-grams in capturing linguistic style?

- Concept: Finetuning pre-trained language models
  - Why needed here: Understanding how to adapt general-purpose models to domain-specific tasks like spoken text classification.
  - Quick check question: What's the difference between BERT-AA (no finetuning) and Finetuned BERT in this context?

- Concept: Speaker verification vs. attribution
  - Why needed here: These are distinct tasks requiring different approaches - verification is binary classification while attribution is multi-class.
  - Quick check question: Why might Adhominem perform better for verification than character n-grams?

## Architecture Onboarding

- Component map: Data loading -> Pre-processing -> Feature extraction -> Model training -> Evaluation
  - HANSEN dataset module handles data loading
  - Pre-processing includes speaker alignment and text length normalization
  - Feature extraction varies by method (n-grams, stylometry, embeddings, or transformer-based)
  - Model training uses different architectures per method
  - Evaluation uses AA/AV/TT specific metrics

- Critical path: For AA on spoken text, the critical path is:
  Text preprocessing -> Character n-gram extraction -> Logistic Regression -> F1 evaluation
  This is fastest and most reliable baseline to validate pipeline.

- Design tradeoffs:
  - N-gram methods: Fast, interpretable but may miss semantic patterns
  - Stylometry features: Rich but require feature engineering
  - FastText+LSTM: Captures semantics but slower
  - BERT-based: Powerful but computationally expensive and may overfit on small datasets

- Failure signatures:
  - Low F1 scores across all methods: Dataset quality issues or speaker overlap
  - N-grams outperform transformers: Dataset too small for DL methods
  - Finetuned BERT underperforms: Insufficient training data or poor finetuning

- First 3 experiments:
  1. Run character n-gram baseline on TED dataset with N=10 speakers to establish baseline performance
  2. Compare character vs word n-grams on CEO interview dataset to validate n-gram choice
  3. Test finetuned BERT vs BERT-AA on PAN dataset to measure finetuning impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do spoken text characteristics differ across languages for authorship analysis tasks?
- Basis in paper: While the HANSEN benchmark encompasses multiple human and AI-generated spoken text datasets, it is essential to note that they are currently limited to English. Variations in spoken text structures and norms among humans differ significantly across languages (Crystal, 2007). Consequently, the performance of authorship analysis techniques may vary when applied to other languages (Halvani et al., 2016).
- Why unresolved: The authors explicitly state that the HANSEN benchmark is limited to English datasets, preventing analysis of cross-linguistic differences in spoken text characteristics for authorship analysis.
- What evidence would resolve it: Creating and evaluating spoken text authorship analysis benchmarks across multiple languages would provide empirical evidence of how spoken text characteristics and authorship analysis performance vary by language.

### Open Question 2
- Question: What are the optimal methods for detecting AI-generated spoken text versus human-spoken text?
- Basis in paper: While SOTA methods, such as, character n-gram or Transformer-based model, exhibit similar AA & AV performance in human-spoken datasets compared to written ones, there is much room for improvement in AI-generated spoken text detection.
- Why unresolved: The authors note that current methods show much room for improvement in detecting AI-generated spoken text, indicating that optimal detection methods have not yet been developed.
- What evidence would resolve it: Comparative evaluation of a wide range of detection methods on large, diverse spoken text datasets would identify which methods perform best for AI vs. human spoken text detection.

### Open Question 3
- Question: To what extent do individuals' written and spoken texts exhibit similar stylistic characteristics?
- Basis in paper: Our results affirm the distinctions between individuals' written and spoken texts, consistent with prior corpus-based research (Biber et al., 2000; Farahani et al., 2020). We also conduct an ablation study on SEC and PAN datasets, training on written texts only and testing on spoken texts (and vice versa). The results in Table 9 show a substantial decrease in the macro f1 score for both character n-gram and BERT-ft. Also, stylometry features exhibit poor performance, underscoring the stylistic differences between the two text forms and thus emphasizing the significance of separate authorship analysis for spoken text.
- Why unresolved: While the authors show that written and spoken texts from the same individuals differ stylistically, they acknowledge that more parallel datasets are needed to conclusively determine the extent of similarity in individual stylistic characteristics across written and spoken forms.
- What evidence would resolve it: Creating large-scale parallel datasets of written and spoken texts from the same individuals, followed by comprehensive stylometric analysis, would provide empirical evidence of the degree of similarity in individual stylistic characteristics across written and spoken forms.

## Limitations
- Benchmark limited to English language datasets, preventing cross-linguistic analysis
- Evaluation primarily based on cross-validation within datasets rather than independent test sets
- Limited to three LLM sources for AI-generated text detection evaluation
- Small number of parallel written/spoken datasets (SEC and PAN only) for modality comparison

## Confidence
- Character n-grams outperforming word n-grams: **High**
- Written vs. spoken text modality differences: **High**
- AI-generated text detection performance gaps: **Medium**
- Generalizability across diverse speaking styles: **Low**

## Next Checks
1. Evaluate the same methods on an entirely independent spoken text corpus not included in HANSEN to test generalizability.
2. Conduct ablation studies removing informal markers (e.g., "uh," "um," "eh") to quantify their specific contribution to character n-gram performance.
3. Test cross-lingual transfer by training models on English spoken text and evaluating on non-English datasets to assess language dependency of the observed patterns.