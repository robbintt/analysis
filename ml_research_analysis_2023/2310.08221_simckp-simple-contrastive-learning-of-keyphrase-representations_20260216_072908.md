---
ver: rpa2
title: 'SimCKP: Simple Contrastive Learning of Keyphrase Representations'
arxiv_id: '2310.08221'
source_url: https://arxiv.org/abs/2310.08221
tags:
- keyphrase
- keyphrases
- document
- learning
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SimCKP, a simple contrastive learning framework
  for keyphrase prediction that jointly learns to extract present keyphrases and generate
  absent ones by building context-aware phrase-level representations. The method consists
  of two stages: 1) an extractor-generator that extracts present keyphrases by learning
  phrase-level representations in a contrastive manner while generating absent keyphrases,
  and 2) a reranker that adapts scores for each generated phrase by aligning their
  representations with the corresponding document.'
---

# SimCKP: Simple Contrastive Learning of Keyphrase Representations

## Quick Facts
- arXiv ID: 2310.08221
- Source URL: https://arxiv.org/abs/2310.08221
- Reference count: 20
- Key outcome: Introduces a two-stage contrastive learning framework that jointly extracts present keyphrases and generates absent ones, achieving state-of-the-art performance on multiple benchmark datasets

## Executive Summary
This paper presents SimCKP, a simple contrastive learning framework for keyphrase prediction that addresses the limitations of existing methods that rely on token-level representations or separate extraction/generation models. The framework consists of two stages: an extractor-generator that learns context-aware phrase-level representations through contrastive learning while simultaneously extracting present and generating absent keyphrases, and a reranker that filters noisy candidates using another round of contrastive learning. Experimental results demonstrate that SimCKP significantly outperforms state-of-the-art models on benchmark datasets including KP20k, Inspec, Krapivin, NUS, and SemEval, achieving new SOTA results in both present and absent keyphrase prediction.

## Method Summary
SimCKP is a two-stage contrastive learning framework for keyphrase prediction. The first stage uses a BART-based encoder-decoder with a contrastive loss that pulls document embeddings closer to present keyphrase embeddings while pushing away other candidate phrases mined from the document using POS tagging and chunking rules. This stage simultaneously extracts present keyphrases and generates absent ones via beam search. The second stage employs a dual-encoder architecture (BERT-based) that re-ranks the generated candidates through another contrastive learning phase, aligning document and absent keyphrase representations. The final predictions are selected using a threshold determined from validation F1@M scores. The method leverages hard negative phrase mining and temperature-scaled cosine similarity to learn high-quality phrase-level representations.

## Key Results
- SimCKP achieves new state-of-the-art results on KP20k, Inspec, Krapivin, NUS, and SemEval datasets for both present and absent keyphrase prediction
- The two-stage contrastive learning approach outperforms previous methods by significant margins in both F1@5 and F1@M metrics
- Hard negative phrase mining proves crucial, outperforming random and in-batch document negatives in the contrastive learning objective
- The dual-encoder reranker stage successfully improves absent keyphrase generation precision while maintaining high recall

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning enables the model to learn context-aware phrase-level representations rather than token-level predictions.
- Mechanism: The framework maximizes agreement between document embeddings and positive phrase embeddings (present keyphrases) while minimizing agreement with negative phrase embeddings (other candidates). This pulls semantically relevant phrase representations closer to the document representation in the embedding space.
- Core assumption: Phrase-level representations capture more semantic information than token-level predictions, leading to better keyphrase extraction and generation.
- Evidence anchors:
  - [abstract]: "This paper introduces SimCKP, a simple contrastive learning framework for keyphrase prediction that jointly learns to extract present keyphrases and generate absent ones by building context-aware phrase-level representations."
  - [section 4.2]: "The model pulls keyphrase embeddings to the document embedding and pushes away the rest of the candidates in a contrastive manner."

### Mechanism 2
- Claim: The reranker stage improves absent keyphrase generation by filtering out noisy candidates through semantic alignment.
- Mechanism: After the extractor-generator stage generates candidate phrases with beam search, the reranker uses a separate dual-encoder architecture to score each candidate based on its semantic similarity to the document. This second contrastive learning stage aligns absent keyphrase representations with the document.
- Core assumption: The extractor-generator stage generates a high-recall set of candidates that includes most target absent keyphrases, making reranking effective at improving precision.
- Evidence anchors:
  - [section 4.3]: "To reduce the noise introduced by beam search, we train a reranker that allocates new scores for the generated phrases via another round of contrastive learning, where this time the agreement between the document and absent keyphrase representations is maximized."
  - [section 6.6]: "The high recall demonstrates the potential for reranking to increase precision, and we observe that there is room for improvement by better reranking."

### Mechanism 3
- Claim: Hard negative phrase mining creates effective training signals by providing challenging negative examples.
- Mechanism: The phrase mining algorithm extracts candidate phrases from the document using POS tagging and chunking rules, then filters out nongrammatical phrases. These candidates serve as negative examples during contrastive learning, creating a more challenging optimization problem.
- Core assumption: Including phrases that are semantically similar to keyphrases but not actual keyphrases creates stronger learning signals than random negative examples.
- Evidence anchors:
  - [section 4.1]: "As noted by Gillick et al. (2019), hard negatives are important for learning a high-quality encoder, and we claim that our mining accomplishes this objective."
  - [section 6.4]: "The outcomes of these two baselines demonstrate that our approach successfully mines hard negatives, enabling our encoder to acquire high-quality representations of keyphrases."

## Foundational Learning

- Concept: Contrastive learning objectives
  - Why needed here: The framework relies on maximizing similarity between positive pairs (document + keyphrase) and minimizing similarity between negative pairs (document + non-keyphrase phrases) to learn effective phrase representations.
  - Quick check question: What is the difference between the InfoNCE loss and the cross-entropy loss used in this framework?

- Concept: Phrase structure parsing and POS tagging
  - Why needed here: The hard negative mining algorithm requires accurate identification of noun, verb, adjective, and adverb phrases to generate candidate phrases for contrastive learning.
  - Quick check question: How does the NLTK RegexpParser work with POS-tagged text to extract phrase chunks?

- Concept: Encoder-decoder architectures and attention mechanisms
  - Why needed here: The extractor-generator stage uses a BART encoder-decoder model to simultaneously learn extraction and generation tasks through attention mechanisms.
  - Quick check question: How does the BART model's cross-attention layer help in generating absent keyphrases while maintaining context from the document?

## Architecture Onboarding

- Component map: Document text → Phrase mining (POS tagging + chunking) → Extractor-Generator (BART encoder-decoder with contrastive learning) → Beam search candidate generation → Reranker (dual BERT encoders with contrastive learning) → Cosine similarity threshold → Final top-k keyphrases

- Critical path: Document → Phrase mining → Contrastive learning (Stage 1) → Beam search generation → Reranking (Stage 2) → Final predictions

- Design tradeoffs:
  - Using two separate contrastive learning stages vs. end-to-end training
  - Fixed global similarity threshold vs. adaptive per-document thresholds
  - Hard negative mining vs. random or in-batch negatives
  - Separate BERT models for reranking vs. sharing weights with extractor

- Failure signatures:
  - Stage 1 fails: Low F1@5 scores, poor performance on present keyphrases
  - Stage 2 fails: High recall but low precision in absent keyphrase generation
  - Phrase mining fails: Noisy or incomplete candidate phrases affecting contrastive learning
  - Threshold selection fails: Either too few predictions (threshold too high) or too many noisy predictions (threshold too low)

- First 3 experiments:
  1. Verify phrase mining produces expected candidate phrases by running on sample documents and checking POS tagging and chunking output
  2. Test contrastive learning stage by training on a small dataset and visualizing document-keyphrase embeddings with t-SNE
  3. Validate reranking stage by generating candidates with beam search and checking if the reranker improves precision on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SimCKP compare when using different negative mining strategies (e.g., in-batch document negatives, random negatives) versus the proposed hard negative phrase mining approach?
- Basis in paper: [explicit] The paper compares the proposed hard negative phrase mining method with in-batch document negatives and random negatives, showing that the proposed method significantly outperforms the other two.
- Why unresolved: The comparison is based on a specific dataset and evaluation metrics. It is unclear how the performance would vary with different datasets, domains, or evaluation metrics.
- What evidence would resolve it: Additional experiments on different datasets and domains, as well as evaluation using different metrics, would provide a more comprehensive understanding of the effectiveness of the hard negative phrase mining approach.

### Open Question 2
- Question: What is the impact of using different threshold values for selecting keyphrases during inference, and how can an adaptive threshold selection mechanism be incorporated into SimCKP?
- Basis in paper: [inferred] The paper mentions that using a fixed global threshold has limitations, and suggests that adaptively selecting a threshold value via an auxiliary module might overcome this challenge.
- Why unresolved: The paper does not provide any experimental results or insights on the impact of different threshold values or the potential benefits of an adaptive threshold selection mechanism.
- What evidence would resolve it: Experiments comparing the performance of SimCKP with different threshold values, as well as incorporating and evaluating an adaptive threshold selection mechanism, would provide insights into the impact of thresholds and potential improvements.

### Open Question 3
- Question: How does the performance of SimCKP change when using different pre-trained language models (e.g., RoBERTa, T5) as the encoder and decoder, and what are the trade-offs in terms of computational resources and performance?
- Basis in paper: [inferred] The paper uses BART and BERT as the encoder-decoder and reranker models, respectively. However, it is unclear how the performance would change if different pre-trained language models were used.
- Why unresolved: The paper does not provide any experiments or insights on the impact of using different pre-trained language models on the performance of SimCKP.
- What evidence would resolve it: Experiments comparing the performance of SimCKP with different pre-trained language models, along with an analysis of the trade-offs in terms of computational resources and performance, would provide insights into the impact of the choice of language models.

## Limitations

- The framework's effectiveness depends heavily on the quality of phrase mining, which may not generalize well to non-scientific domains or documents with different linguistic structures
- The use of a fixed global threshold for final predictions may not adapt well to documents with varying keyphrase densities or distributions
- The method requires substantial computational resources for training two separate contrastive learning stages with large pre-trained models (BART and BERT)

## Confidence

- **High confidence**: The overall framework design and the empirical superiority over state-of-the-art models (as measured by F1@5 and F1@M on benchmark datasets). The use of contrastive learning to align document and keyphrase representations is a well-motivated approach with clear implementation details.
- **Medium confidence**: The effectiveness of hard negative mining in improving representation quality. While the paper argues this creates stronger learning signals, the exact quality and diversity of the mined negatives are not fully characterized, and their impact could vary with different datasets or domains.
- **Medium confidence**: The dual-encoder reranker's ability to significantly improve absent keyphrase generation. The paper shows improved precision, but the recall at the beam search stage and the trade-off between recall and precision are not thoroughly analyzed.

## Next Checks

1. **Validate Phrase Mining Quality**: Run the phrase mining algorithm on a diverse set of sample documents from different domains (not just scientific articles) and manually inspect the generated candidate phrases. Check for grammatical correctness, relevance, and diversity. This will confirm whether the hard negative mining is effective and whether the approach generalizes beyond the training domain.

2. **Analyze Threshold Sensitivity**: Instead of using a fixed global threshold, implement an adaptive threshold per document (e.g., based on document length or keyphrase density). Compare the F1@5 and F1@M scores with the fixed threshold approach on both in-domain and out-of-domain datasets. This will test whether the current threshold selection is optimal or if adaptive methods could yield better results.

3. **Test Cross-Domain Generalization**: Train SimCKP on KP20k and evaluate on a dataset from a completely different domain (e.g., news articles or social media posts). Measure the drop in F1@5 and F1@M compared to in-domain performance. Analyze the phrase embeddings using t-SNE to visualize domain shift and identify whether the contrastive learning representations are domain-specific or transferable.