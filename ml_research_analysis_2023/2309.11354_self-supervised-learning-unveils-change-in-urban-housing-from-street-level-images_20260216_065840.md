---
ver: rpa2
title: Self-supervised learning unveils change in urban housing from street-level
  images
arxiv_id: '2309.11354'
source_url: https://arxiv.org/abs/2309.11354
tags:
- change
- images
- urban
- street-level
- housing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces Street2Vec, a self-supervised learning method
  to detect urban change in housing from temporal street-level imagery without requiring
  manual annotations. By adapting Barlow Twins to spatial-temporal street images,
  Street2Vec learns embeddings invariant to irrelevant variations (lighting, seasonality,
  occlusion) while capturing structural urban change.
---

# Self-supervised learning unveils change in urban housing from street-level images

## Quick Facts
- arXiv ID: 2309.11354
- Source URL: https://arxiv.org/abs/2309.11354
- Authors: 
- Reference count: 40
- Key outcome: Street2Vec, a self-supervised learning method, detects urban housing change from temporal street-level imagery without manual annotations, outperforming ImageNet baselines on 15.3 million London images (2008–2021).

## Executive Summary
This study introduces Street2Vec, a self-supervised learning method to detect urban change in housing from temporal street-level imagery without requiring manual annotations. By adapting Barlow Twins to spatial-temporal street images, Street2Vec learns embeddings invariant to irrelevant variations (lighting, seasonality, occlusion) while capturing structural urban change. Applied to 15.3 million images from London (2008–2021), Street2Vec detected change in Opportunity Areas and distinguished minor from major change, outperforming ImageNet-pretrained baselines. Manual labeling of 1,449 image pairs validated its ability to identify subtle (e.g., facade renovations) and major (e.g., new buildings) transformations in housing stock, providing a scalable, label-free tool for urban planning and tracking progress toward sustainable development goals.

## Method Summary
Street2Vec adapts the Barlow Twins self-supervised learning framework to learn representations from pairs of street-level images captured at the same location but different times. The method uses a ResNet-50 backbone with a 3-layer MLP projector to generate 1024-dimensional embeddings. During training, images from the same location at different years serve as positive pairs, while the loss function encourages invariance to irrelevant variations (lighting, weather, occlusion) while preserving structural differences. Change detection is performed by computing cosine distances between embeddings of image pairs from the same location at different times, with larger distances indicating greater urban structural change.

## Key Results
- Street2Vec embeddings outperformed ImageNet-pretrained ResNet-50 baselines for detecting urban change
- The method successfully distinguished between five levels of change (minimal irrelevant, noticeable irrelevant, minor urban, major urban, anomalous) on manually labeled validation pairs
- Street2Vec detected meaningful change patterns in London's Opportunity Areas, demonstrating practical utility for urban planning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Street2Vec embeddings capture urban structural change while being invariant to irrelevant visual variations such as lighting, seasonality, and occlusion.
- Mechanism: By adapting Barlow Twins to temporal street-level images, Street2Vec learns representations where cross-correlated embedding dimensions are maximally correlated across images from the same location but different years, while decorrelating feature dimensions to encode distinct urban properties.
- Core assumption: Changes in built environment structure will be minimal on average between images from the same location at different times, so the model will learn to ignore irrelevant variations and focus on structural differences.
- Evidence anchors:
  - [abstract] "Street2Vec learns embeddings invariant to irrelevant variations (lighting, seasonality, occlusion) while capturing structural urban change."
  - [section] "Our assumption is that on average, street-level images taken at two different time instants will have strong visual appearance variations representing changes that are not the focus of this study such as lighting conditions, seasonality, people, or cars, but no or only minimal change in urban structural elements."
  - [corpus] Weak evidence; no directly related studies in the corpus address this specific mechanism of invariance learning.
- Break condition: If structural changes in housing stock are not visually detectable in street-level imagery (e.g., interior renovations without external facade changes), the invariance assumption fails and embeddings will not capture true change.

### Mechanism 2
- Claim: Cosine distance between Street2Vec embeddings effectively quantifies urban change between image pairs from the same location at different times.
- Mechanism: After learning embeddings, cosine distance is computed between pairs of images from the same location at different years; larger distances indicate greater structural change, while small distances indicate minimal or irrelevant change.
- Core assumption: The learned embedding space preserves relative distances that correspond to the magnitude of urban structural change, enabling quantitative comparison.
- Evidence anchors:
  - [abstract] "We computed the degree of change based on the cosine distance between the embeddings of two images captured a decade apart."
  - [section] "To perform change detection, we compare single image embeddings at all locations of interest and extract a single summary statistic representing a notion of deviation or distance. The farther the embeddings are, the more likely the location represented by the pair of images is to have undergone structural urban changes."
  - [corpus] Weak evidence; no directly related studies in the corpus validate this specific application of cosine distance for urban change quantification.
- Break condition: If the embedding space becomes too high-dimensional or the model overfits to noise, cosine distances may not reliably reflect true urban change magnitude.

### Mechanism 3
- Claim: Street2Vec outperforms ImageNet-pretrained baselines for detecting urban change because it learns domain-specific representations tailored to urban structures rather than generic visual features.
- Mechanism: Street2Vec is trained specifically on street-level imagery with temporal pairs, learning urban-relevant features, while ImageNet models learn generic features optimized for object classification, which may not capture subtle urban structural changes.
- Core assumption: Urban change detection requires features specific to urban environments (building facades, street layouts) that are not emphasized in generic image classification tasks.
- Evidence anchors:
  - [abstract] "Street2Vec... outperformed generic embeddings, successfully identified point-level change in London's housing supply from street-level images, and distinguished between major and minor change."
  - [section] "We found that the mean cosine distances from Street2Vec followed the expected order, successfully distinguishing between minimal, irrelevant, minor, and major change... Street2Vec embeddings also outperformed generic features learned from the baseline CNN model."
  - [corpus] Weak evidence; no directly related studies in the corpus compare self-supervised urban embeddings against ImageNet baselines for change detection.
- Break condition: If the dataset size is too small or the temporal variation is too limited, the self-supervised training may not learn sufficiently discriminative urban features, reducing the advantage over generic embeddings.

## Foundational Learning

- Concept: Self-supervised representation learning
  - Why needed here: Labels for urban change are scarce and expensive to obtain, so self-supervised learning allows learning meaningful embeddings without manual annotations by using temporal image pairs as surrogate supervision.
  - Quick check question: How does Barlow Twins use image pairs to learn invariant representations without explicit labels?

- Concept: Temporal image pair sampling
  - Why needed here: The method relies on sampling pairs of images from the same location at different times to create the "positive pairs" needed for Barlow Twins training; without sufficient temporal coverage, the model cannot learn change-invariant features.
  - Quick check question: What happens if most locations only have images from a single year in the dataset?

- Concept: Cosine distance as similarity metric in high-dimensional space
  - Why needed here: Embeddings are high-dimensional, and cosine distance is scale-invariant and less sensitive to large deviations in single dimensions, making it suitable for comparing learned representations.
  - Quick check question: Why is cosine distance preferred over Euclidean distance for comparing high-dimensional embeddings?

## Architecture Onboarding

- Component map: Image preprocessing (resize, concatenate 4 orientations) -> Barlow Twins backbone (ResNet-50 + MLP projector) -> Embedding space (1024D) -> Cosine distance computation -> Change detection output
- Critical path: Temporal image pair sampling -> Barlow Twins training -> Embedding generation -> Distance computation -> Change classification
- Design tradeoffs: Using panoramic concatenation increases context but also memory usage; smaller batch size (48) limits gradient stability but fits GPU constraints; λ=0.005 balances decorrelation vs correlation preservation
- Failure signatures: High variance in distance distributions across classes suggests poor invariance learning; failure to distinguish major vs minor change indicates insufficient feature discrimination; poor performance on labeled validation pairs indicates overfitting or poor embedding quality
- First 3 experiments:
  1. Train Street2Vec on a small subset (e.g., 10% of data) with 2 years of coverage, evaluate embedding variance and distance distributions on a labeled validation set
  2. Compare Street2Vec distances against ImageNet baseline on the same labeled pairs to confirm improved change discrimination
  3. Visualize UMAP projections of embeddings from different change classes to verify clustering by change magnitude

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does Street2Vec generalize to cities with significantly different urban characteristics (e.g., developing cities with slums vs. developed cities with high-rise buildings)?
- Basis in paper: [inferred] The paper discusses that the method is "generalizable to most cities around the world" but also notes that "access restrictions" and "coverage may be problematic in some countries where there is high public scrutiny of data privacy (e.g., Germany) and restricted access in developing areas such as slums."
- Why unresolved: The paper primarily validates the method on London data and only speculates about performance in other urban contexts without empirical testing.
- What evidence would resolve it: Systematic testing of Street2Vec across diverse urban environments (developed vs. developing, dense vs. sparse, formal vs. informal settlements) with quantitative performance comparisons.

### Open Question 2
- Question: Can Street2Vec effectively detect changes in urban housing that don't produce visible exterior changes, such as internal renovations or changes in housing use?
- Basis in paper: [explicit] The paper states: "The assumption that all changes in the housing stock will have visual signals captured by street images may not always hold because renewal projects such as energy efficiency improvements or increasing housing capacity within existing buildings, or differences in uses may not always result in visible changes in external views."
- Why unresolved: The study focuses on visually detectable changes and acknowledges this limitation without proposing solutions or quantifying the extent of undetected changes.
- What evidence would resolve it: Validation studies comparing Street2Vec results with ground truth data on housing changes that include both visible and non-visible modifications, potentially through integration with administrative housing records.

### Open Question 3
- Question: What is the optimal frequency for capturing street-level imagery to detect meaningful urban change without excessive data redundancy?
- Basis in paper: [inferred] The paper uses a 10-year interval (2008-2018) for change detection but doesn't explore how detection performance varies with different time intervals or how temporal resolution affects the method's effectiveness.
- Why unresolved: The paper demonstrates success with a specific 10-year timeframe but doesn't investigate the temporal sensitivity of the method or provide guidance on optimal update frequencies for different types of urban changes.
- What evidence would resolve it: Systematic experiments varying the time intervals between image captures to determine detection accuracy, false positive rates, and the minimum interval required to reliably detect different categories of urban change (minor vs. major).

## Limitations
- The method may not detect changes that don't produce visible exterior modifications (e.g., interior renovations, changes in use)
- Generalization to cities with different urban morphologies and architectural styles remains untested
- Reliance on Google Street View's temporal coverage introduces potential sampling bias

## Confidence
- **High confidence**: The method successfully learns embeddings invariant to irrelevant visual variations and outperforms generic baselines on labeled validation data
- **Medium confidence**: The approach can distinguish between minor and major change categories, though some overlap exists in distance distributions
- **Low confidence**: Generalization to cities with different architectural styles and urban planning patterns remains untested

## Next Checks
1. Apply Street2Vec to street-level imagery from a geographically and architecturally distinct city (e.g., Tokyo or New York) to test cross-city generalization
2. Compare Street2Vec change detection against ground-truth construction permits or planning application data to validate real-world accuracy
3. Test the method's sensitivity to different temporal intervals (e.g., 5-year vs 10-year differences) to establish optimal change detection windows