---
ver: rpa2
title: Improving Compositional Generalization Using Iterated Learning and Simplicial
  Embeddings
arxiv_id: '2310.18777'
source_url: https://arxiv.org/abs/2310.18777
tags:
- learning
- training
- which
- compositional
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles compositional generalization, the ability to
  generalize to unseen combinations of latent factors, a challenge for deep neural
  networks but easy for humans. Inspired by cognitive science's iterated learning
  hypothesis, the authors propose a method combining iterated learning with simplicial
  embeddings (SEM) to enhance compositional generalization.
---

# Improving Compositional Generalization Using Iterated Learning and Simplicial Embeddings

## Quick Facts
- **arXiv ID:** 2310.18777
- **Source URL:** https://arxiv.org/abs/2310.18777
- **Reference count:** 40
- **Key outcome:** SEM-IL improves compositional generalization on vision and molecular datasets by combining iterated learning with simplicial embeddings

## Executive Summary
This paper addresses the challenge of compositional generalization—the ability to generalize to unseen combinations of latent factors—which deep neural networks struggle with despite being easy for humans. The authors propose a method combining iterated learning (inspired by cognitive science) with simplicial embeddings (SEM) to enhance this capability. By discretizing continuous representations using SEM and iteratively resetting and relearning models, the approach amplifies compressibility pressure while maintaining expressivity. Experiments on controlled vision datasets (3dShapes, dSprites, MPI3D-real) and molecular graph property prediction tasks (ogbg-molhiv, ogbg-molpcba, PCQM4Mv2) demonstrate significant improvements in compositional generalization and downstream performance compared to baselines.

## Method Summary
The method combines iterated learning with simplicial embeddings to improve compositional generalization. The network is split into a backbone and task head, with the backbone's output discretized using SEM layers that create approximately sparse representations. The learning process alternates between imitation phases (where a student network learns from a teacher's discretized representations) and interaction phases (standard supervised training). This iterative process amplifies compressibility pressure, favoring simpler, more compositional mappings. The approach is evaluated on both controlled vision datasets with known ground-truth generating factors and molecular datasets with scaffold splits designed to test compositional generalization.

## Key Results
- SEM-IL consistently improves topological similarity between learned representations and ground-truth factors across vision datasets
- Significant performance gains on molecular graph property prediction tasks compared to standard training and information bottleneck approaches
- The method demonstrates better compositional generalization particularly on challenging scaffold splits
- Visualization shows representations gradually aligning with ground-truth factors across generations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterated learning amplifies compressibility pressure through repeated imitation phases
- Mechanism: Each generation's student network learns from a teacher whose representations have been compressed through prior training. Confident predictions are learned faster and remembered better, while uncertain ones are quickly forgotten, leading to representations that reuse common rules
- Core assumption: Neural networks have a bias toward learning simpler mappings faster and more robustly
- Evidence anchors:
  - [abstract] "iteratively resetting and relearning models, amplifying compressibility pressure while maintaining expressivity"
  - [section] "We thus expect that iteratively resetting and relearning can amplify the compressibility pressure"
  - [corpus] Weak - corpus contains related work on iterated learning but not specific evidence for this mechanism
- Break condition: If the student cannot effectively learn from the teacher due to architectural mismatch or if the imitation phase is too short to establish stable mappings

### Mechanism 2
- Claim: Simplicial embeddings (SEM) discretize continuous representations into approximately sparse vectors
- Mechanism: Dense representations are transformed into m vectors of length v, each passed through softmax with temperature τ. This creates approximately one-hot vectors that can be sampled from, creating discrete pseudo-labels for the imitation phase
- Core assumption: Discretized representations increase compressibility pressure compared to continuous representations during imitation
- Evidence anchors:
  - [abstract] "discretizing representations using SEM and iteratively resetting and relearning models"
  - [section] "we propose to split the network into a backbone and a task head, and discretize the representation at the end of the backbone using simplicial embeddings (SEM, [45])"
  - [corpus] Moderate - corpus includes related work on simplicial embeddings improving sample efficiency
- Break condition: If temperature τ is too high (distributions become uniform) or too low (gradients vanish), or if the discretization loses critical information for the downstream task

### Mechanism 3
- Claim: The combination of SEM and iterated learning creates representations that align with ground-truth generating factors
- Mechanism: SEM provides discrete representations that can be compared to generating factors using topological similarity measures. Iterated learning with these discrete representations amplifies compressibility pressure, making the learned mappings more compositional and systematic
- Core assumption: Ground-truth generating factors can be meaningfully compared to learned representations using distance correlations
- Evidence anchors:
  - [abstract] "we propose to improve the compositional generalization of deep networks by using iterated learning on models with simplicial embeddings, which can approximately discretize representations"
  - [section] "we can directly observe how z gradually becomes more similar to G, and how the compressibility and expressivity pressures affect the training process"
  - [corpus] Weak - corpus lacks direct evidence linking SEM-IL to alignment with ground-truth factors
- Break condition: If the ground-truth generating factors are not discrete or if the topological similarity measure does not capture meaningful structure

## Foundational Learning

- Concept: Kolmogorov complexity and its relationship to compositionality
  - Why needed here: The paper argues that compositional mappings have lower Kolmogorov complexity than non-compositional ones, which justifies why iterated learning (which favors simpler mappings) helps compositional generalization
  - Quick check question: If a mapping from z to G can be described with fewer rules when it's compositional versus when it's holistic, what does this imply about its Kolmogorov complexity?

- Concept: Information bottleneck and its limitations for compositional generalization
  - Why needed here: The paper shows that standard information bottleneck approaches (minimizing I(Z;X) while maximizing I(Z;Y)) cannot reliably reach the highest rung of the compositionality ladder
  - Quick check question: Why does a representation z with H(G|z)=0 and H(z|G)>0 (Stage II) still struggle with compositional generalization even though it has perfect training performance?

- Concept: Topological similarity as a measure of compositionality
- Why needed here: The paper uses distance correlation between representations and ground-truth factors to quantify how compositional learned mappings are
- Quick check question: If two representations z1 and z2 both achieve H(G|z)=0, but z1 has higher topological similarity with G than z2, which one is more likely to generalize compositionally and why?

## Architecture Onboarding

- Component map:
  Backbone network h(x) -> SEM layer -> Task head g(z) -> Prediction y
  Teacher network (frozen) <- Student network (current generation)

- Critical path:
  1. Input x → Backbone h(x) → SEM layer → Discretized z
  2. z → Task head g(z) → Prediction y
  3. Imitation phase: Student learns to reconstruct teacher's z
  4. Interaction phase: Standard supervised training on downstream task

- Design tradeoffs:
  - SEM temperature τ: Higher values create smoother distributions but less discrete representations; lower values create more discrete representations but risk gradient vanishing
  - Number of factors m and vocabulary size v: Larger values increase representational capacity but also computational cost and risk of overfitting
  - Number of generations: More generations increase compressibility pressure but also training time

- Failure signatures:
  - Training loss plateaus early: SEM temperature may be too low or backbone capacity insufficient
  - Test performance worse than baseline: Iterated learning may be over-regularizing or SEM discretization losing critical information
  - No improvement across generations: Teacher-student gap too small or imitation phase too short

- First 3 experiments:
  1. Train baseline model on 3dShapes with regression task, measure topological similarity ρ(z,G)
  2. Add SEM layer only, train with standard supervised learning, compare ρ(z,G) and test performance
  3. Implement SEM-IL with 3 generations, compare ρ(z,G), training/test curves, and downstream performance across all methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of generations (Tgen) in the iterated learning algorithm for achieving the best compositional generalization performance?
- Basis in paper: [explicit] The paper mentions "we can choose to randomly initialize the speaker (usually when the model is small), copy the pretrained checkpoint (when the model is large), or copy the parameters of the teacher in previous generations (the seed iterated learning variant mentioned in [50])." It also discusses how the number of active bases decreases as the number of generations increases, but does not specify an optimal number.
- Why unresolved: The paper does not provide a clear guideline on how to determine the optimal number of generations. The choice might depend on factors like model size, dataset complexity, and the specific task.
- What evidence would resolve it: Conducting experiments with varying numbers of generations and analyzing the trade-off between performance improvement and computational cost would provide insights into the optimal number of generations.

### Open Question 2
- Question: How does the choice of temperature (τ) in the softmax operation of the simplicial embedding layer affect the performance of the proposed method?
- Basis in paper: [explicit] The paper mentions "The overall process is ¯zi = Softmaxτ(zi) = [ezij /τ PV k=1 ezik/τ ]j ∈ Rv z = [ ¯z⊤ 1 . . . ¯z⊤ m]⊤ ∈ Rmv." It also states that "By using an encoder with a final SEM layer, we obtain an approximately-sparse z."
- Why unresolved: The paper does not discuss the impact of different temperature values on the sparsity of the output and the subsequent performance of the model.
- What evidence would resolve it: Conducting experiments with different temperature values and analyzing their impact on the sparsity of the output and the performance of the model would provide insights into the optimal temperature setting.

### Open Question 3
- Question: How does the proposed method perform on datasets with continuous latent factors instead of discrete ones?
- Basis in paper: [explicit] The paper states "However, as nature might not be purely discrete, incorporating the continuous latent space is crucial to enlarge the scope of our study. We would leave this in our future work."
- Why unresolved: The paper only demonstrates the effectiveness of the proposed method on datasets with discrete latent factors. The performance on datasets with continuous latent factors remains unexplored.
- What evidence would resolve it: Applying the proposed method to datasets with continuous latent factors and comparing its performance with other methods would provide insights into its effectiveness in handling continuous latent factors.

## Limitations

- The method requires multiple training iterations, significantly increasing computational cost
- The optimal hyperparameters (temperature τ, number of generations, SEM dimensions) appear dataset-dependent and require extensive tuning
- The topological similarity measure may not generalize well to datasets where ground-truth factors are not clearly defined or are continuous

## Confidence

- **High confidence**: The core experimental results showing SEM-IL outperforming baselines on both synthetic and molecular datasets are well-documented and reproducible
- **Medium confidence**: The theoretical arguments linking iterated learning to improved compositional generalization through compressibility pressure are sound but rely on assumptions about neural network learning dynamics that are not fully proven
- **Low confidence**: The claim that SEM-IL can reliably reach Stage III compositionality for arbitrary datasets is not yet established, as the current experiments focus on controlled settings

## Next Checks

1. **Cross-dataset hyperparameter transfer**: Train SEM-IL on one dataset and evaluate on another with minimal hyperparameter tuning to test robustness
2. **Scaling analysis**: Systematically vary dataset size and model capacity to identify when SEM-IL provides the greatest benefit relative to standard training
3. **Ablation on iteration count**: Measure the marginal benefit of each generation to determine the optimal stopping point and computational trade-offs