---
ver: rpa2
title: Is Pre-training Truly Better Than Meta-Learning?
arxiv_id: '2306.13841'
source_url: https://arxiv.org/abs/2306.13841
tags:
- maml
- diversity
- meta-learning
- size
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper re-examines the claim that pre-training outperforms
  meta-learning for few-shot image classification. Through a rigorous empirical study,
  the authors compare pre-trained (PT) models with Model Agnostic Meta-Learning (MAML)
  across 21 diverse few-shot learning benchmarks, including large-scale datasets.
---

# Is Pre-training Truly Better Than Meta-Learning?

## Quick Facts
- **arXiv ID**: 2306.13841
- **Source URL**: https://arxiv.org/abs/2306.13841
- **Reference count**: 40
- **Primary result**: Pre-training does not always beat meta-learning; dataset diversity determines which approach performs better, with effect sizes considered small.

## Executive Summary
This paper challenges the prevailing narrative that pre-training is superior to meta-learning for few-shot image classification. Through a rigorous empirical study comparing pre-trained models with Model Agnostic Meta-Learning (MAML) across 21 diverse benchmarks, the authors find that dataset diversity is the key factor determining which approach performs better. Using effect size analysis (Cohen's d) and Task2Vec diversity coefficient, they show that pre-training slightly outperforms MAML on low-diversity datasets (effect size 0.0909), while MAML marginally outperforms pre-training on high-diversity datasets (effect size -0.105). Both effect sizes are considered small according to classical statistical thresholds.

## Method Summary
The authors conduct a fair comparison by using the same architecture (ResNet12), optimizer (Adam), and training all models to convergence. They employ 300-500 tasks for meta-batch size and use the Task2Vec diversity coefficient to quantify dataset diversity. The effect size (Cohen's d) is calculated to assess practical significance rather than relying solely on statistical significance. This approach allows them to compare pre-trained models against MAML across diverse few-shot learning benchmarks including Meta-Dataset, MiniImagenet, Cifar-fs, and Omniglot.

## Key Results
- For low-diversity datasets, pre-training beats MAML with effect size 0.0909
- For high-diversity datasets, MAML beats pre-training with effect size -0.105
- Both effect sizes are considered small according to classical statistical thresholds
- Dataset diversity is the key factor determining PT vs MAML performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Pre-training performs better than MAML on low-diversity datasets because the fixed embedding captures sufficient generalizable features for similar tasks.
- **Mechanism**: In low-diversity datasets, tasks share common patterns that can be captured by a single fixed embedding from pre-training. The diversity coefficient quantifies this similarity—lower diversity means tasks are more alike, so a static representation suffices.
- **Core assumption**: Task similarity is high in low-diversity datasets, making fixed embeddings effective.
- **Evidence anchors**:
  - [abstract]: "when the formal diversity of a data set is low, PT beats MAML on average"
  - [section]: "the average formal diversity of a dataset" and Task2Vec diversity coefficient is used to quantify task similarity.
  - [corpus]: Related paper "Exploring the Efficacy of Meta-Learning: Unveiling Superior Data Diversity Utilization of MAML Over Pre-training" directly compares diversity effects.
- **Break condition**: If diversity coefficient underestimates task similarity (e.g., due to embedding limitations), or if low-diversity datasets contain rare subtypes requiring adaptation.

### Mechanism 2
- **Claim**: MAML outperforms pre-training on high-diversity datasets because it learns adaptive feature reuse rather than fixed representations.
- **Mechanism**: High diversity means tasks are dissimilar; a fixed embedding cannot capture all variations. MAML's inner-loop adaptation allows rapid adjustment of features per task, leveraging the learned initialization.
- **Core assumption**: High diversity implies significant task variation that requires adaptation rather than fixed feature reuse.
- **Evidence anchors**:
  - [abstract]: "when the formal diversity is high, MAML beats PT on average"
  - [section]: "MAML model beats a Pre-trained model on average" for high diversity.
  - [corpus]: "Cooperative Meta-Learning with Gradient Augmentation" suggests MAML's adaptability is valuable.
- **Break condition**: If adaptation steps in MAML are insufficient (e.g., too few SGD steps), or if task variation is so extreme that no shared features exist.

### Mechanism 3
- **Claim**: Effect size analysis reveals practical significance beyond statistical significance, preventing misleading conclusions in large-sample settings.
- **Mechanism**: With large meta-batches (300-500 tasks), t-tests reject null hypotheses for trivial differences. Effect size (Cohen's d) quantifies the magnitude of differences in standard deviation units, filtering out noise.
- **Core assumption**: Statistical significance does not imply practical relevance; effect size thresholds (e.g., 0.2 for small) provide context.
- **Evidence anchors**:
  - [section]: "we use the effect size (Cohen's d) to determine the practical significance of the difference"
  - [section]: "we choose the difference to be the standardized 1 %... the common performance gained needed in machine learning conferences"
  - [corpus]: No direct corpus support; this is a methodological choice.
- **Break condition**: If the chosen threshold (1%) is inappropriate for the domain, or if effect size itself is biased by non-normal distributions.

## Foundational Learning

- **Concept**: Task2Vec embeddings and diversity coefficient
  - Why needed here: Quantifies formal diversity of datasets, which is the key driver of PT vs MAML performance differences.
  - Quick check question: How does the Task2Vec diversity coefficient measure the effective number of tasks in a dataset?

- **Concept**: Effect size (Cohen's d) vs statistical significance
  - Why needed here: Prevents over-interpreting statistically significant but practically trivial differences, especially with large samples.
  - Quick check question: Why is effect size preferred over p-values when sample sizes are large?

- **Concept**: MAML inner-loop and outer-loop optimization
  - Why needed here: Understanding MAML's adaptation mechanism is crucial to interpreting why it outperforms PT on high-diversity datasets.
  - Quick check question: What is the role of the inner-loop optimization in MAML during meta-testing?

## Architecture Onboarding

- **Component map**: Data loading -> Task2Vec diversity computation -> Model training (PT or MAML) -> Meta-test accuracy evaluation -> Effect size calculation
- **Critical path**: Data loading → Task2Vec diversity computation → Model training (PT or MAML) → Meta-test accuracy evaluation → Effect size calculation
- **Design tradeoffs**: Fixed embedding (PT) trades adaptability for simplicity and speed; MAML trades memory/computation for task-specific adaptation. Resnet12 chosen for fair comparison vs MAML's computational demands.
- **Failure signatures**: PT fails on high-diversity datasets (fixed embedding insufficient); MAML fails on low-diversity datasets (overfitting/inefficient adaptation). Both fail if training to convergence is not achieved.
- **First 3 experiments**:
  1. Replicate PT vs MAML comparison on cifar-fs (low diversity) to verify PT advantage and effect size ~0.09.
  2. Repeat on omniglot (high diversity) to verify MAML advantage and effect size ~-0.1.
  3. Compute Task2Vec diversity coefficient for a new dataset and predict PT/MAML performance before training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity coefficient behave when applied to datasets with varying levels of task overlap or shared features?
- Basis in paper: [explicit] The paper mentions the Task2Vec diversity coefficient but does not explore its behavior with datasets that have significant task overlap or shared features.
- Why unresolved: The paper validates the diversity coefficient with synthetic experiments where the ground truth diversity is known but does not address its performance with real-world datasets that might have overlapping tasks or shared features.
- What evidence would resolve it: Empirical studies comparing the Task2Vec diversity coefficient's predictions with human judgment on datasets known to have varying degrees of task overlap.

### Open Question 2
- Question: What are the implications of the observed marginal performance differences between PT and MAML for practical applications?
- Basis in paper: [explicit] The paper notes that the effect sizes are considered small and discusses the potential for MAML to have less meta-overfitting than PT, but does not explore the practical implications of these findings.
- Why unresolved: The paper focuses on theoretical and statistical analysis without delving into how these findings might influence the choice between PT and MAML in real-world applications.
- What evidence would resolve it: Case studies or experiments that compare the performance of PT and MAML in specific practical applications, taking into account factors like computational resources and the nature of the tasks.

### Open Question 3
- Question: How do different architectures beyond ResNets and 5CNNs influence the relationship between dataset diversity and the effectiveness of PT vs. MAML?
- Basis in paper: [inferred] The paper uses ResNets and 5CNNs for its experiments but does not explore how other architectures might affect the observed trends.
- Why unresolved: The choice of architecture can significantly impact the performance of machine learning models, and the paper does not address whether the observed trends hold across a broader range of architectures.
- What evidence would resolve it: Experiments comparing PT and MAML across a variety of architectures, including those designed for specific tasks or datasets, to determine if the trends observed with ResNets and 5CNNs generalize.

## Limitations
- Results may be sensitive to the specific implementation of Task2Vec diversity coefficient and chosen effect size threshold
- Study focuses exclusively on image classification, limiting generalizability to other domains
- Unknown impact of different architectures beyond Resnet12 on the observed trends

## Confidence
- **High Confidence**: The overall conclusion that dataset diversity influences PT vs MAML performance is well-supported by the data across 21 benchmarks.
- **Medium Confidence**: The specific effect sizes (0.0909 for low diversity, -0.105 for high diversity) are statistically significant but require careful interpretation given potential implementation variations.
- **Low Confidence**: The generalizability of findings to non-image domains or different model architectures beyond Resnet12.

## Next Checks
1. Replicate the study using alternative diversity metrics (e.g., maximum mean discrepancy) to verify robustness of conclusions.
2. Test the PT vs MAML comparison on a non-image domain (e.g., few-shot reinforcement learning) to assess domain transferability.
3. Conduct ablation studies varying the meta-batch size and inner-loop adaptation steps in MAML to determine sensitivity to these hyperparameters.