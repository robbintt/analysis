---
ver: rpa2
title: Towards Equipping Transformer with the Ability of Systematic Compositionality
arxiv_id: '2312.07280'
source_url: https://arxiv.org/abs/2312.07280
tags:
- tasks
- compositionality
- dataset
- bert
- systematic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CAT, a Transformer-based model that improves
  systematic compositionality in language models. The core idea is to decompose contextual
  word representations into discrete primitives (sememes) and then compose them to
  produce more semantically informative representations.
---

# Towards Equipping Transformer with the Ability of Systematic Compositionality

## Quick Facts
- arXiv ID: 2312.07280
- Source URL: https://arxiv.org/abs/2312.07280
- Reference count: 29
- Primary result: Introduces CAT, a Transformer-based model that improves systematic compositionality through discrete primitive decomposition and novel pre-training tasks

## Executive Summary
This paper addresses the limitation of standard Transformers in handling systematic compositionality—the ability to understand and generate novel combinations of known semantic units. The authors propose CAT (Compositional Attention Transformer), which decomposes contextual word representations into discrete primitives (sememes) and recomposes them to create more semantically informative representations. CAT achieves this through a Multi-Primitive Composition module with sparse attention over a shared codebook, and is further enhanced by two novel pre-training tasks: Guided Decomposition (using OpenHowNet sememe knowledge) and Semantics Composition. The model outperforms BERT on compositionality-aware tasks while maintaining comparable performance on standard language understanding benchmarks.

## Method Summary
CAT extends the standard Transformer encoder with two key modules: Multi-Primitive Composition, which decomposes contextual representations into discrete sememe codes using sparse attention and recomposes them into compositional representations; and Representation Enhancement, which integrates compositional and contextual information through self-attention. The model is pre-trained using standard tasks plus two novel objectives: Guided Decomposition, which supervises the decomposition process using OpenHowNet sememe knowledge, and Semantics Composition, which includes reconstruction consistency, semantic sufficiency, and nuance minimization objectives. After pre-training, CAT is fine-tuned on compositionality-aware datasets (BiRD, PAWS) and standard GLUE benchmarks.

## Key Results
- CAT improves phrase correlation accuracy by 6.42% compared to BERT
- CAT achieves 1.10% higher accuracy on adversarial paraphrase sentence classification than BERT
- CAT demonstrates better compositional generalization to out-of-distribution datasets while maintaining GLUE performance

## Why This Works (Mechanism)

### Mechanism 1
CAT improves systematic compositionality by decomposing contextual word representations into discrete primitives (sememes) and then composing them back into more semantically informative representations. The Multi-Primitive Composition module uses a sparse attention mechanism to decompose each contextual representation into multiple discrete latent space vectors (codes) drawn from a shared codebook. These discrete codes are then composed back using weighted attention to produce a compositional representation. The core assumption is that language symbols are discrete and their meanings can be composed from a limited set of semantic primitives (sememes). The contextual representation contains sufficient information to be decomposed into these primitives.

### Mechanism 2
The Guided Decomposition and Semantics Composition pre-training tasks enhance CAT's systematic compositionality by providing supervision for both decomposition and composition processes. The Guided Decomposition task uses OpenHowNet sememe knowledge to supervise the sparse attention mechanism in selecting appropriate sememes during decomposition. The Semantics Composition task includes three learning objectives: Reconstruction Consistency (encouraging compositional representations to be similar to contextual ones), Semantic Sufficiency (ensuring all representations contain sufficient semantic information), and Nuance Minimization (reducing the informativeness of differences between representations). The core assumption is that sememes provide meaningful supervision for decomposition, and the three learning objectives in Semantics Composition effectively guide the model toward more semantically informative representations.

### Mechanism 3
CAT demonstrates robustness to noisy contexts by leveraging the discreteness of primitives and their composition, which filters out irrelevant information during the decomposition process. The sparse attention mechanism in the Multi-Primitive Composition module acts as a filter that selects only relevant sememes from the contextual representation. When processing noisy contexts, this mechanism can ignore irrelevant information by not assigning attention weights to corresponding sememes. The core assumption is that the sparse attention mechanism effectively filters out irrelevant information, and the discrete nature of sememes makes them more robust to noise compared to continuous representations.

## Foundational Learning

- **Discrete latent space vectors (codes) and vector quantization**: Understanding how CAT represents primitives as discrete vectors and how the vector quantization process works is fundamental to grasping the decomposition mechanism. Quick check: How does the sparse attention mechanism in CAT differ from standard vector quantization, and why is this difference important for decomposition?

- **Sparse attention mechanisms**: The sparse attention mechanism is central to CAT's ability to decompose contextual representations into multiple discrete primitives. Quick check: What role does the RMSNorm operator play in the sparse attention mechanism, and how does it contribute to optimization stability?

- **Sememe knowledge bases and semantic compositionality**: Understanding the concept of sememes and how they can be used to supervise the decomposition process is crucial for grasping the Guided Decomposition task. Quick check: How does the OpenHowNet dataset provide supervision for CAT's decomposition process, and what are the limitations of this supervision?

## Architecture Onboarding

- **Component map**: Vanilla Transformer encoder -> Multi-Primitive Composition module (decomposition + composition) -> Representation Enhancement module (integration) -> output representation
- **Critical path**: contextual representation → Multi-Primitive Composition (decomposition + composition) → Representation Enhancement (integration) → output representation
- **Design tradeoffs**: The design trades off between maintaining contextual information (needed for downstream tasks) and enhancing compositionality (achieved through discrete primitives). The sparse attention mechanism balances between selecting enough primitives for meaningful composition and not overwhelming the model with too many codes.
- **Failure signatures**: If CAT underperforms on compositionality-aware tasks but performs normally on standard tasks, the issue likely lies in the Multi-Primitive Composition module. If CAT underperforms on both compositionality-aware and standard tasks, the issue may be in the Representation Enhancement module's integration process.
- **First 3 experiments**:
  1. Implement the Multi-Primitive Composition module with a single codebook and test decomposition on a simple dataset with known semantic primitives
  2. Add the sparse attention mechanism and evaluate how well it selects relevant codes compared to standard vector quantization
  3. Integrate the Representation Enhancement module and test the balance between compositional and contextual information on a standard language understanding task

## Open Questions the Paper Calls Out

### Open Question 1
How does the number of discrete primitives (K) in the codebook affect the performance of CAT on systematic compositionality tasks? The paper mentions that the size of the discrete latent space is a parameter (K) and that the CAT's codebook has 1000 codes for the first three layers and 3152 codes for the last layer. However, the impact of varying K on performance is not explicitly explored. Conducting experiments with different values of K and comparing the performance of CAT on compositionality-aware tasks would provide insights into the optimal size of the codebook.

### Open Question 2
How does the CAT model perform on other compositionality-aware tasks beyond those mentioned in the paper, such as semantic parsing or visual question answering? The paper mentions that CAT outperforms baselines on compositionality-aware tasks like phrase correlation and adversarial paraphrase sentence classification. However, it does not explore other tasks that require compositionality. Evaluating CAT on a wider range of compositionality-aware tasks, such as semantic parsing or visual question answering, would demonstrate its generalizability and effectiveness in different domains.

### Open Question 3
How does the CAT model's performance on systematic compositionality tasks compare to other state-of-the-art models specifically designed for compositionality, such as neural module networks or syntactic attention models? The paper mentions that CAT outperforms baselines like BERT, RoBERTa, and DistilBERT on compositionality-aware tasks. However, it does not compare its performance to other models specifically designed for compositionality. Conducting a comparative study between CAT and other models designed for compositionality, such as neural module networks or syntactic attention models, would provide insights into CAT's effectiveness and potential advantages over existing approaches.

## Limitations

- The paper primarily evaluates CAT on a limited set of compositionality-aware tasks, leaving its generalizability to other domains and tasks uncertain
- The effectiveness of the sparse attention mechanism in selecting semantically meaningful primitives is largely empirical and could benefit from more rigorous validation
- The OpenHowNet supervision may introduce dataset-specific biases that limit the model's transfer to broader applications

## Confidence

**High Confidence** (Experimental results are directly supported and methodology is sound):
- CAT's improved performance on BiRD and BiRD-ABBA phrase correlation tasks
- CAT's maintained performance on GLUE benchmark tasks
- CAT's superior out-of-distribution generalization compared to baselines

**Medium Confidence** (Results are supported but mechanisms could be more thoroughly validated):
- The effectiveness of the Multi-Primitive Composition module in capturing compositional semantics
- The contribution of the two novel pre-training tasks to overall performance
- The claimed noise robustness benefit

**Low Confidence** (Claims are made but supporting evidence is limited):
- The specific mechanism by which sememe decomposition improves compositionality beyond standard attention
- The generalizability of the OpenHowNet supervision to other domains or languages
- The scalability of CAT to larger models or different Transformer architectures

## Next Checks

1. **Ablation Study on Discrete Decomposition**: Remove the Multi-Primitive Composition module and retrain CAT to quantify the exact contribution of the discrete primitive decomposition mechanism. Compare performance drops on compositionality-aware tasks versus GLUE tasks to isolate the compositionality-specific benefits.

2. **Noise Sensitivity Analysis**: Systematically evaluate CAT's robustness by introducing controlled noise at different levels (word substitution, sentence scrambling, context perturbation) and measuring performance degradation compared to BERT and other baselines. This would provide concrete evidence for the claimed noise robustness.

3. **Cross-Domain Generalization Test**: Apply CAT to a different semantic composition task from a domain not represented in the training data (e.g., scientific terminology composition or cross-lingual compositionality) to assess whether the sememe-based decomposition generalizes beyond the PAWS and GLUE benchmarks used in the paper.