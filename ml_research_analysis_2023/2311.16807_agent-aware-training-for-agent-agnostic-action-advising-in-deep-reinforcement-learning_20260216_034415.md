---
ver: rpa2
title: Agent-Aware Training for Agent-Agnostic Action Advising in Deep Reinforcement
  Learning
arxiv_id: '2311.16807'
source_url: https://arxiv.org/abs/2311.16807
tags:
- learning
- advice
- agent
- state
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework called Agent-Aware trAining
  yet Agent-Agnostic Action Advising (A7) to alleviate the sampling inefficiency problem
  in Deep Reinforcement Learning (DRL). A7 aims to strike a balance between agent-specific
  and agent-agnostic action advising methods.
---

# Agent-Aware Training for Agent-Agnostic Action Advising in Deep Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2311.16807
- **Source URL**: https://arxiv.org/abs/2311.16807
- **Reference count**: 40
- **Primary result**: A7 significantly accelerates learning and surpasses existing methods on GridWorld, LunarLander, and six Atari games.

## Executive Summary
This paper introduces A7, a novel framework that bridges the gap between agent-specific and agent-agnostic action advising methods in Deep Reinforcement Learning. By employing a proxy model to extract discriminative and generally applicable state features, A7 enables efficient advice solicitation without requiring the advisor to have access to the agent's policy. The framework combines action-BYOL for feature extraction, adaptive thresholding for advice solicitation, and intrinsic rewards to incentivize learning from advised samples. Experimental results demonstrate substantial improvements in sample efficiency and final performance across multiple benchmark environments.

## Method Summary
A7 operates through a multi-component architecture where the agent's experiences are processed by an action-BYOL model to extract state features. These features are used to compute distances between current and historical states, with an adaptive percentile-based threshold determining when to seek advice from an expert teacher. Advised samples are stored and used to train a reuse model via behavior cloning with dropout-based uncertainty estimation. Intrinsic rewards are assigned based on feature distances to encourage exploration in under-sampled regions. The framework balances computational efficiency with performance by using a single average feature buffer and periodic retraining of the feature extractor.

## Key Results
- A7 significantly accelerates the learning process compared to existing action advising methods
- The framework achieves higher final evaluation scores across all tested environments (GridWorld, LunarLander, and six Atari games)
- A7 effectively balances advice solicitation with sample efficiency, staying within the advice budget while maximizing learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning via action-BYOL extracts state features that are both discriminative to the agent's policy and generally applicable across different agent behaviors.
- Mechanism: action-BYOL uses the agent's current and next states (along with the selected action) to train a feature extractor via BYOL-style contrastive loss. This makes the features adaptive to the agent (via the state-action transition) but agnostic to agent noise (by training on a sequence of agent-experienced states).
- Core assumption: Temporal coherence of states and the action's influence on the next state provide enough signal to learn a shared, agent-invariant feature space.
- Evidence anchors:
  - [abstract]: "we employ a proxy model to extract state features that are both discriminative (adaptive to the agent) and generally applicable (robust to agent noise)."
  - [section]: "we incorporate the current state, selected action, and next state into the contrastive learning process instead of employing simple state augmentations."
- Break condition: If the agent's policy changes drastically between training and advising, the learned features may no longer reflect the current state-action space effectively.

### Mechanism 2
- Claim: Adaptive threshold selection using a sliding percentile of recent feature distances ensures the advice solicitation remains calibrated across environments.
- Mechanism: At each step, the feature distance is compared against a percentile-based threshold derived from a fixed-length queue of past distances. This avoids manual tuning and adapts to the agent's current state visitation pattern.
- Core assumption: The distribution of feature distances remains relatively stable within a training episode, making percentile-based thresholds a robust choice.
- Evidence anchors:
  - [abstract]: "we employ an adaptive distance threshold."
  - [section]: "we sort the queue H incrementally and use the percentile value of H as our adaptive threshold for subsequent steps."
- Break condition: In highly non-stationary environments where state distributions shift rapidly, the percentile may lag behind and become ineffective.

### Mechanism 3
- Claim: Intrinsic rewards based on feature distance encourage the agent to explore and learn from advised states that are furthest from the current knowledge.
- Mechanism: The intrinsic reward formula `Î»t * tanh(dt / dm)` scales with the normalized distance `dt` between the current state and the average feature buffer, decaying over time to reduce bias.
- Core assumption: Larger feature distances correspond to states where the agent is less certain, so rewarding them encourages learning in those regions.
- Evidence anchors:
  - [section]: "samples with greater feature distance require a larger intrinsic reward."
  - [section]: "we propose to assign intrinsic rewards to each re-advised sample based on its distance from the feature buffer."
- Break condition: If the feature buffer becomes too homogeneous (e.g., agent stuck in a region), distances shrink and intrinsic rewards become negligible, offering no exploratory incentive.

## Foundational Learning

- Concept: Contrastive representation learning (e.g., BYOL).
  - Why needed here: Provides a self-supervised way to learn robust, discriminative features from agent experiences without external labels.
  - Quick check question: In BYOL, why is the target network updated slowly via exponential moving average instead of backpropagation?

- Concept: Dropout-based uncertainty estimation for advice reuse.
  - Why needed here: Allows the reuse model to quantify confidence in its action predictions, gating when to provide advice to the student agent.
  - Quick check question: What is the effect of using 100 forward passes with dropout masks versus a single deterministic forward pass?

- Concept: Percentile-based adaptive thresholding.
  - Why needed here: Enables environment-agnostic advice solicitation by dynamically adjusting to the agent's current state visitation statistics.
  - Quick check question: How does the choice of queue length and percentile affect the false positive/negative rate of advice solicitation?

## Architecture Onboarding

- Component map:
  - Agent (student) -> Environment -> Experience replay buffer
  - action-BYOL feature extractor (trained periodically)
  - Feature buffer (stores average feature for distance calculation)
  - Distance queue (stores recent distances for percentile threshold)
  - Reuse model (behavior cloning with dropout for uncertainty)
  - Intrinsic reward generator (based on feature distance)
  - Teacher (expert policy for advice)

- Critical path:
  1. Agent interacts with environment.
  2. Current state passed to action-BYOL encoder -> feature.
  3. Distance computed vs. feature buffer.
  4. If distance > adaptive threshold, seek teacher advice.
  5. Collect state-advice pairs -> train reuse model.
  6. Reuse model can later provide advice if uncertainty low.
  7. Intrinsic reward added based on distance when reuse advice given.
  8. Agent updates policy with combined extrinsic + intrinsic reward.

- Design tradeoffs:
  - More frequent action-BYOL training -> better features but higher compute cost.
  - Larger feature buffer -> more stable averages but higher memory use.
  - Lower reuse uncertainty threshold -> more advice reuse but risk of stale advice.

- Failure signatures:
  - Low variance in distance queue -> adaptive threshold stuck, no advice given.
  - Reuse model uncertainty always high -> advice never reused, intrinsic rewards wasted.
  - Feature buffer average drifts -> distance metric loses meaning.

- First 3 experiments:
  1. Run A7 on GridWorld with no reuse model (set intrinsic reward to zero) to verify selector alone improves over baselines.
  2. Run A7 on LunarLander with fixed distance threshold instead of adaptive to quantify gain from percentile-based threshold.
  3. Run A7 with and without intrinsic rewards on Freeway to measure effect on sample efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of adaptive threshold percentile (e.g., 70th percentile used in experiments) affect the performance of A7 across different environments and agent architectures?
- Basis in paper: [explicit] The paper mentions using the 70th percentile of the distance queue as the adaptive threshold but does not explore the sensitivity of performance to this hyperparameter.
- Why unresolved: The paper does not provide ablation studies or sensitivity analysis for different threshold percentiles.
- What evidence would resolve it: Comparative experiments showing performance of A7 with different percentile values (e.g., 50th, 70th, 90th) across multiple environments and agent architectures.

### Open Question 2
- Question: What is the impact of the intrinsic reward decay rate on the final performance and learning stability of A7, particularly in environments with varying reward structures?
- Basis in paper: [explicit] The paper mentions using a linear decay regime for the intrinsic reward coefficient but does not explore the effects of different decay schedules or rates.
- Why unresolved: The paper does not provide ablation studies on different decay schedules (e.g., exponential, polynomial) or decay rates.
- What evidence would resolve it: Comparative experiments showing performance of A7 with different decay schedules and rates across multiple environments.

### Open Question 3
- Question: How does the performance of A7 scale with the size of the feature buffer, and what is the trade-off between buffer size and computational/memory efficiency?
- Basis in paper: [inferred] The paper mentions storing a single average feature to reduce memory overhead but does not explore the impact of different buffer sizes on performance.
- Why unresolved: The paper does not provide experiments varying the feature buffer size or analyzing the trade-offs involved.
- What evidence would resolve it: Experiments comparing A7 performance with different feature buffer sizes, along with analysis of computational and memory requirements.

## Limitations
- The stability of feature distances across different training phases and agent policies is not fully characterized.
- The choice of percentile and queue length for threshold adaptation appears heuristic without sensitivity analysis.
- The effectiveness of intrinsic rewards may degrade if the feature buffer becomes stale or homogeneous.

## Confidence
- Medium: The contrastive learning approach is well-grounded but lacks extensive ablation study for its adaptation to RL state-action sequences.
- Medium: Adaptive thresholding strategy shows promise but depends heavily on distributional assumptions that may not hold in highly non-stationary environments.
- Medium: Intrinsic reward formulation is intuitive but may suffer from hyperparameter sensitivity.

## Next Checks
1. Test A7 with varying replay buffer sizes to assess impact on feature buffer stability and threshold adaptability.
2. Compare adaptive thresholding against fixed or decay-based thresholds to quantify performance gains.
3. Analyze feature distance distributions over training to verify that learned features remain discriminative across policy changes.