---
ver: rpa2
title: Are LLMs All You Need for Task-Oriented Dialogue?
arxiv_id: '2304.06556'
source_url: https://arxiv.org/abs/2304.06556
tags:
- state
- dialogue
- examples
- prompt
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) on task-oriented
  dialogue tasks, where the model needs to extract slot values from a conversation
  and use them to query a database to provide relevant information to the user. The
  authors find that LLMs perform poorly on slot extraction, but can still guide the
  conversation to a successful outcome if given correct slot values.
---

# Are LLMs All You Need for Task-Oriented Dialogue?

## Quick Facts
- arXiv ID: 2304.06556
- Source URL: https://arxiv.org/abs/2304.06556
- Authors: 
- Reference count: 19
- Primary result: LLMs perform poorly on slot extraction but can guide task-oriented dialogues to success when given correct slot values

## Executive Summary
This paper evaluates large language models (LLMs) on task-oriented dialogue tasks, focusing on their ability to extract slot values from conversations and query databases to provide relevant information to users. The authors find that while LLMs struggle with slot extraction, they can still guide dialogues to successful outcomes when provided with correct belief states. The performance improves when the model is given true belief state distributions or in-domain examples through few-shot learning. The study establishes zero-shot and few-shot baselines for LLM capabilities in task-oriented dialogue using instruction-tuned models like Tk-Instruct-11B, ChatGPT, Alpaca-LoRA-7B, GPT-NeoXT-20B, and OPT-IML-30B.

## Method Summary
The authors evaluate LLMs on MultiWOZ 2.2 and Schema-Guided Dataset (SGD) using zero-shot and few-shot in-context learning without fine-tuning. They construct prompts with task definitions, domain descriptions, dialogue history, and retrieved examples. A vector database (FAISS) stores dialogue contexts for retrieval. The LLM performs state tracking (generating belief state updates) and response generation (producing system responses) using Huggingface library and OpenAI API. Performance is measured using Joint Goal Accuracy (JGA), Slot F1, Dialogue Success Rate, and BLEU score.

## Key Results
- LLMs show poor slot extraction performance but can guide dialogues to successful endings when provided correct slot values
- Providing true belief state distributions or in-domain examples significantly improves LLM performance
- Zero-shot baselines establish LLM capabilities in task-oriented dialogue without extensive prompt tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can generate plausible system responses in task-oriented dialogues when provided with correct belief states.
- Mechanism: By supplying accurate belief states, the LLM's response generation step can ground its output in the user's needs and the available database entries.
- Core assumption: The LLM has sufficient world knowledge and reasoning capability to produce coherent responses given structured context.
- Evidence anchors:
  - [abstract]: "they show ability to guide the dialogue to successful ending if given correct slot values."
  - [section]: "if provided with correct belief states they can yield interesting performance comparable with some of the previous state-of-the-art models"
- Break condition: If belief states are inaccurate or incomplete, the generated responses will fail to satisfy user requests.

### Mechanism 2
- Claim: In-context learning with retrieved examples improves dialogue success rate.
- Mechanism: Retrieved in-domain examples in the prompt provide the LLM with relevant dialogue patterns and expected state transitions.
- Core assumption: The retrieved examples are sufficiently representative and the LLM can extract useful patterns from them.
- Evidence anchors:
  - [section]: "We believe the results could be further improved on by model-specific prompt tuning."
  - [section]: "the number of few shot examples improves the model performance"
- Break condition: If retrieved examples are irrelevant or too few, the LLM may not learn useful patterns.

### Mechanism 3
- Claim: Zero-shot performance establishes a baseline for LLM capabilities in task-oriented dialogue.
- Mechanism: By omitting in-context examples, the LLM must rely solely on its pre-training to understand the task and generate appropriate responses.
- Core assumption: The LLM's pre-training data includes sufficient coverage of task-oriented dialogue scenarios.
- Evidence anchors:
  - [abstract]: "We believe it is interesting and beneficial to contribute with the evaluation of these systems for more specific applications"
  - [section]: "We aim to compare the raw capabilities of the selected LLMs"
- Break condition: If pre-training lacks task-oriented dialogue data, zero-shot performance will be poor.

## Foundational Learning

- Concept: Belief State Tracking
  - Why needed here: Accurate belief states are essential for the LLM to generate relevant responses and query the database correctly.
  - Quick check question: Can the LLM accurately extract slot values from user utterances without finetuning?

- Concept: In-Context Learning
  - Why needed here: Retrieved examples guide the LLM's behavior without requiring parameter updates, enabling few-shot performance.
  - Quick check question: Does the number of retrieved examples significantly impact dialogue success rate?

- Concept: Prompt Engineering
  - Why needed here: The prompt structure determines how well the LLM understands the task and formats its outputs.
  - Quick check question: Can the LLM consistently output belief states in the required JSON format?

## Architecture Onboarding

- Component map: User input → Context retrieval → State tracking prompt → Belief state generation → Database query → Response generation prompt → System response

- Critical path: User input → Context retrieval → State tracking prompt → Belief state generation → Database query → Response generation prompt → System response

- Design tradeoffs:
  - Retrieval vs. generation: More retrieved examples may improve performance but increase latency
  - Zero-shot vs. few-shot: Few-shot provides better performance but requires maintaining a context store
  - Prompt complexity vs. robustness: Complex prompts may guide the LLM better but are harder to generalize

- Failure signatures:
  - State tracking failures: Incorrect or incomplete belief states leading to irrelevant responses
  - Response generation failures: Generated responses that do not match the database or user needs
  - Retrieval failures: Retrieved examples that are irrelevant to the current dialogue context

- First 3 experiments:
  1. Compare zero-shot vs. few-shot performance with a fixed set of examples
  2. Test the impact of oracle belief states on dialogue success rate
  3. Vary the number of retrieved examples to find the optimal retrieval size

## Open Questions the Paper Calls Out

Here are 5 open questions based on the paper:

### Open Question 1
- Question: How much can LLM performance on task-oriented dialogue be improved through prompt tuning techniques?
- Basis in paper: [inferred] The paper mentions that "the performance could be further improved by careful model-specific prompt customization" and discusses the potential for prompt tuning.
- Why unresolved: The paper intentionally used universal prompts to test general LLM capabilities, not exploring prompt tuning.
- What evidence would resolve it: Experiments testing various prompt tuning approaches and measuring their impact on dialogue success rates, belief state tracking accuracy, etc.

### Open Question 2
- Question: How does the size of the context store impact few-shot LLM performance on task-oriented dialogue?
- Basis in paper: [explicit] The paper explores using different numbers of examples in the context store and finds that increasing the number improves performance.
- Why unresolved: The experiments only tested up to 10 examples per domain. The relationship between context store size and performance for larger sizes is unknown.
- What evidence would resolve it: Experiments testing the impact of context store sizes ranging from 1 to 100+ examples per domain on dialogue success rates, belief state tracking, etc.

### Open Question 3
- Question: How important is in-domain finetuning for LLMs on task-oriented dialogue compared to few-shot learning?
- Basis in paper: [explicit] The paper notes that "carefully picking representative examples and combining the LLM with some in-domain tracker can be an interesting choice" and contrasts its few-shot approach with previous finetuning-based work.
- Why unresolved: The paper does not experiment with in-domain finetuning, only few-shot learning.
- What evidence would resolve it: Head-to-head comparisons of LLM performance with and without in-domain finetuning on task-oriented dialogue benchmarks.

### Open Question 4
- Question: How do different LLM architectures (encoder-only, decoder-only, encoder-decoder) compare on task-oriented dialogue?
- Basis in paper: [explicit] The paper tests 5 different LLM models, including T5-based (encoder-decoder), GPT-based (decoder-only), and OPT-based (decoder-only) models.
- Why unresolved: While the paper reports results for each model, it does not systematically analyze the impact of architecture differences.
- What evidence would resolve it: Experiments controlling for model size and training data while varying only the architecture, to isolate the impact of architecture on dialogue performance.

### Open Question 5
- Question: How well do LLMs handle out-of-domain user requests in task-oriented dialogue?
- Basis in paper: [inferred] The paper focuses on in-domain performance, but real users may make out-of-domain requests.
- Why unresolved: The paper does not test out-of-domain robustness.
- What evidence would resolve it: Experiments introducing out-of-domain user requests into dialogues and measuring LLM ability to detect and handle them appropriately.

## Limitations

- The paper establishes baseline performance but doesn't explore prompt tuning, which could significantly improve results
- Evaluation focuses on in-domain scenarios, limiting understanding of LLM robustness to out-of-domain requests
- Zero-shot performance may reflect prompt engineering artifacts rather than true LLM capabilities

## Confidence

- High confidence: LLMs can generate plausible responses when provided with correct belief states; the architecture framework for combining retrieval with LLM generation is sound
- Medium confidence: In-context learning improves performance; zero-shot baselines are meaningful; state tracking remains challenging for LLMs
- Low confidence: LLMs can accurately extract slot values without finetuning; retrieved examples consistently improve performance; prompt engineering alone can overcome fundamental limitations

## Next Checks

1. **Slot extraction ablation study**: Compare slot F1 scores with oracle belief states versus LLM-extracted belief states across multiple turns to quantify the exact impact of state tracking failures on overall dialogue success

2. **Retrieval quality analysis**: Systematically vary the relevance and diversity of retrieved examples (using similarity thresholds and example selection strategies) to determine the relationship between retrieval quality and dialogue success rate

3. **Prompt robustness testing**: Test the same prompt templates across different domains and dialogue complexity levels to identify whether the observed performance is task-specific or generalizable across task-oriented dialogue scenarios