---
ver: rpa2
title: 'Prompting Multilingual Large Language Models to Generate Code-Mixed Texts:
  The Case of South East Asian Languages'
arxiv_id: '2303.13592'
source_url: https://arxiv.org/abs/2303.13592
tags:
- chatgpt
- code-mixed
- speaker
- sentence
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the capability of large language models
  (LLMs) to generate code-mixed text for seven South East Asian (SEA) languages. Through
  zero-shot prompting of multilingual LLMs, including ChatGPT, InstructGPT, BLOOMZ,
  and Flan-T5-XXL, the research finds that ChatGPT demonstrates the highest success
  rate in producing code-mixed text, achieving 68% for SEA languages and 96% for Singlish.
---

# Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages

## Quick Facts
- arXiv ID: 2303.13592
- Source URL: https://arxiv.org/abs/2303.13592
- Authors: Multiple researchers
- Reference count: 40
- One-line primary result: ChatGPT generates code-mixed SEA language text with 68% success rate, while BLOOMZ and Flan-T5-XXL fail entirely.

## Executive Summary
This study investigates how large language models can generate code-mixed text for South East Asian languages through zero-shot prompting. The research finds that ChatGPT significantly outperforms other multilingual models like BLOOMZ and Flan-T5-XXL in producing code-mixed outputs, achieving 68% success for SEA languages and 96% for Singlish. However, even ChatGPT's outputs contain semantic inaccuracies and word choice errors, requiring extensive human supervision. The findings suggest that while LLMs show promise for generating code-mixed data, their reliability and fluency issues limit their standalone utility without human oversight.

## Method Summary
The study employed zero-shot prompting of four multilingual LLMs (ChatGPT, InstructGPT, BLOOMZ, Flan-T5-XXL) using 180 prompts per model covering five topics (food, family, traffic, AI, weather) and five language pairs (English-Chinese, English-Indonesian, English-Malay, English-Tagalog, English-Vietnamese), plus Singlish. Native speakers manually annotated the generated texts using a 0-3 code-mixing scale. The research compared model performance across different prompt templates and evaluated both the degree of code-mixing and the accuracy of explanations provided by the models.

## Key Results
- ChatGPT achieved 68% success rate for SEA language code-mixing and 96% for Singlish
- BLOOMZ and Flan-T5-XXL failed to generate any code-mixed outputs despite being advertised as multilingual
- All models exhibited semantic inaccuracies and word choice errors, with explanations sometimes being incorrect or hallucinated
- Prompt template design significantly impacted performance, with explicit code-mixing definitions yielding better results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT outperforms other multilingual LLMs in generating code-mixed text when the prompt explicitly defines code-mixing.
- Mechanism: The prompt's explicit definition of code-mixing as a linguistic phenomenon activates the model's ability to follow instructions and generate syntactically mixed outputs.
- Core assumption: The model has been exposed to code-mixing patterns during training and can generalize from examples or definitions.
- Evidence anchors:
  - [abstract] "ChatGPT demonstrates the highest success rate in producing code-mixed text, achieving 68% for SEA languages and 96% for Singlish."
  - [section 3.1] "ChatGPT outperforms other language models in generating code-mixed data across five different language pairs, especially in its capacity to code-mix beyond topic-related entities (3 on the scale)."
  - [corpus] Weak evidence: While related papers exist, they focus on evaluating code-mixing rather than explaining ChatGPT's mechanism for generating it.
- Break condition: If the prompt lacks explicit definition or contains ambiguous instructions, the model may fail to generate code-mixed text or default to monolingual output.

### Mechanism 2
- Claim: InstructGPT (davinci-003) generates higher quality Singlish text compared to davinci-002 due to its training on more diverse and recent data.
- Mechanism: The newer model's training data likely includes more examples of Singlish and other non-standard English varieties, allowing it to better capture the linguistic features of these varieties.
- Core assumption: The training data for davinci-003 includes a wider range of linguistic variations, including Singlish.
- Evidence anchors:
  - [section 3.2] "ChatGPT and InstructGPT (davinci-003)'s performances are particularly noteworthy, clocking at 96% across all prompts."
  - [section 3.3] "Despite ChatGPT's and davinci-003's ability to generate Singlish sentences, we notice semantic inaccuracies in their word choices."
  - [corpus] Assumption: The related papers focus on evaluating code-mixing capabilities but do not provide specific evidence about the training data differences between davinci-002 and davinci-003.
- Break condition: If the training data lacks sufficient diversity or the model is not fine-tuned for non-standard English varieties, the performance may degrade.

### Mechanism 3
- Claim: Publicly available multilingual LLMs like BLOOMZ and Flan-T5-XXL fail to generate code-mixed text due to their training focus on monolingual or intersentential code-mixing.
- Mechanism: These models are primarily trained on monolingual data or data with intersentential code-switching, lacking the fine-grained understanding of intrasentential code-mixing required for SEA languages.
- Core assumption: The training data for these models does not include sufficient examples of intrasentential code-mixing in SEA languages.
- Evidence anchors:
  - [abstract] "BLOOMZ and Flan-T5-XXL fail to generate code-mixed outputs."
  - [section 3.1] "BLOOMZ and Flan-T5-XXL are unable to produce code-mixed texts altogether (despite being advertised as multilingual)."
  - [corpus] Weak evidence: The related papers discuss code-mixing evaluation but do not provide specific insights into the training data composition of BLOOMZ and Flan-T5-XXL.
- Break condition: If the models are fine-tuned with code-mixed data or their training data is augmented with intrasentential code-mixing examples, their performance may improve.

## Foundational Learning

- Concept: Code-mixing and its different levels (loanwords, topic-related entities, beyond entities)
  - Why needed here: Understanding the different levels of code-mixing is crucial for evaluating the quality and complexity of the generated text.
  - Quick check question: Can you provide an example of each code-mixing level for a specific language pair?

- Concept: Multilingual LLMs and their training data
  - Why needed here: Knowing the characteristics of multilingual LLMs and their training data helps in understanding their strengths and limitations in generating code-mixed text.
  - Quick check question: What are the key differences between ChatGPT, InstructGPT, BLOOMZ, and Flan-T5-XXL in terms of their training data and architecture?

- Concept: Prompt engineering and its impact on LLM output
  - Why needed here: Crafting effective prompts is essential for guiding the LLM to generate the desired code-mixed text.
  - Quick check question: How does the explicit definition of code-mixing in the prompt affect the LLM's ability to generate code-mixed text?

## Architecture Onboarding

- Component map:
  - Prompt templates (language pairs, topics, instructions)
  - LLM models (ChatGPT, InstructGPT, BLOOMZ, Flan-T5-XXL)
  - Evaluation criteria (code-mixing scale, accurateness, fluency)
  - Native speaker annotators

- Critical path:
  1. Design and implement prompt templates
  2. Generate code-mixed text using LLMs
  3. Evaluate the generated text using the code-mixing scale and native speaker annotations
  4. Analyze the results and identify patterns

- Design tradeoffs:
  - Zero-shot prompting vs. few-shot prompting
  - Using pre-existing prompt templates vs. creating custom templates
  - Relying on automated evaluation vs. human evaluation

- Failure signatures:
  - LLM generates monolingual text instead of code-mixed text
  - Generated text contains semantic inaccuracies or fluency issues
  - Native speakers flag the text as unnatural or incorrect

- First 3 experiments:
  1. Generate code-mixed text for a specific language pair using different prompt templates and evaluate the results.
  2. Compare the performance of ChatGPT and InstructGPT on generating Singlish text.
  3. Investigate the impact of explicitly defining code-mixing in the prompt on the LLM's ability to generate code-mixed text.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training modifications would be required to enable multilingual LLMs to consistently generate high-quality code-mixed text beyond loanwords and topic-related entities?
- Basis in paper: [explicit] The paper demonstrates that while ChatGPT can generate code-mixed text for SEA languages up to 68% success rate, the quality degrades significantly when moving beyond topic-related entities. The authors note that "code-mixing is not recognized as an essential component of many multilingual LLMs today."
- Why unresolved: The paper identifies the limitation but does not explore what specific technical changes (e.g., training data composition, architectural adjustments, or fine-tuning strategies) would be needed to improve code-mixing capabilities.
- What evidence would resolve it: Experimental results comparing different model architectures, training regimes, or fine-tuning approaches that demonstrate improved code-mixing generation beyond entity-level mixing.

### Open Question 2
- Question: How do the quality and naturalness of LLM-generated code-mixed text compare to human-generated code-mixed text when evaluated by native speakers on metrics beyond grammatical correctness?
- Basis in paper: [inferred] The paper notes that ChatGPT outputs contain "word choice errors that lead to semantic inaccuracies" and "fluency issues" even when syntactically correct. However, the evaluation methodology only uses a 0-3 scale for code-mixing degree and basic fluency checks.
- Why unresolved: The paper's evaluation framework does not include direct comparison with human-generated code-mixed text or comprehensive native speaker evaluations on naturalness, cultural appropriateness, or pragmatic correctness.
- What evidence would resolve it: Comparative studies where native speakers evaluate both LLM-generated and human-generated code-mixed text across multiple dimensions including naturalness, cultural appropriateness, and pragmatic effectiveness.

### Open Question 3
- Question: What is the relationship between the diversity of languages in an LLM's training data and its ability to generate code-mixed text between those languages?
- Basis in paper: [explicit] The paper observes that BLOOMZ and Flan-T5-XXL, despite being multilingual models with training data covering SEA languages, fail to generate code-mixed text. The authors note that "multilinguality simply means the system can process tasks and generate outputs in multiple languages, but not in the same sentence."
- Why unresolved: The paper does not investigate whether increasing language diversity in training data, or specifically including code-mixed examples during training, would improve code-mixing generation capabilities.
- What evidence would resolve it: Systematic experiments varying the diversity and composition of training data (including/excluding code-mixed examples) and measuring resulting code-mixing generation performance across different language pairs.

### Open Question 4
- Question: How do different prompt formulations (e.g., code-mixed prompts vs. monolingual prompts) affect the quality and quantity of code-mixed text generated by LLMs?
- Basis in paper: [explicit] The paper finds that prompt templates significantly impact performance, with the "explicitly define CM" template yielding 68% success rate while the "two bilingual speakers" template performs poorly. However, the study only tests English-language prompts.
- Why unresolved: The paper does not explore code-mixed prompt formulations (e.g., prompts written partially in the target languages) or test whether providing few-shot examples in the prompt improves code-mixing generation.
- What evidence would resolve it: Comparative experiments testing various prompt formulations including code-mixed prompts, few-shot examples, and prompts in different languages to determine optimal strategies for eliciting code-mixed text generation.

## Limitations

- The study lacks transparency in prompt construction, providing only examples without full templates, making exact replication difficult
- Evaluation relies entirely on manual annotation by unspecified native speakers without detailed qualification criteria, introducing potential subjectivity
- The research focuses on only five language pairs and Singlish, limiting generalizability to other code-mixing scenarios
- The study doesn't investigate why certain models fail to generate code-mixed text, particularly for BLOOMZ and Flan-T5-XXL

## Confidence

**High Confidence**: ChatGPT's superior performance in generating code-mixed text compared to other models (68% for SEA languages, 96% for Singlish). This claim is supported by consistent experimental results across multiple prompts and language pairs, with clear quantitative metrics.

**Medium Confidence**: The mechanism by which ChatGPT achieves code-mixing success through explicit prompt definitions. While the results support this mechanism, the paper doesn't provide sufficient evidence about the training data composition or architectural differences that enable this capability.

**Low Confidence**: The claim that BLOOMZ and Flan-T5-XXL fail due to training focus on monolingual or intersentential code-mixing. This explanation is speculative and lacks direct evidence from the models' training data or architecture specifications.

## Next Checks

1. **Prompt Template Standardization**: Conduct a controlled experiment testing whether minor variations in prompt wording (while maintaining the code-mixing definition) affect ChatGPT's performance, to validate the importance of explicit definitions in the mechanism.

2. **Training Data Analysis**: Request access to or documentation about the training data composition for BLOOMZ and Flan-T5-XXL to verify whether their failure to generate code-mixed text stems from monolingual training data, as hypothesized.

3. **Cross-linguistic Generalization**: Test the same prompt templates with additional language pairs not covered in the original study (e.g., Hindi-English, Arabic-English) to determine whether ChatGPT's success generalizes beyond the SEA language set.