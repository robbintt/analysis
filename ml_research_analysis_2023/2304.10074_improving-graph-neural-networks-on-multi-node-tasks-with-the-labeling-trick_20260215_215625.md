---
ver: rpa2
title: Improving Graph Neural Networks on Multi-node Tasks with the Labeling Trick
arxiv_id: '2304.10074'
source_url: https://arxiv.org/abs/2304.10074
tags:
- labeling
- node
- trick
- graph
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning multi-node representations
  using Graph Neural Networks (GNNs). It identifies the limitation of existing GNNs
  in capturing dependencies among multiple nodes when directly aggregating single-node
  representations.
---

# Improving Graph Neural Networks on Multi-node Tasks with the Labeling Trick

## Quick Facts
- arXiv ID: 2304.10074
- Source URL: https://arxiv.org/abs/2304.10074
- Reference count: 13
- Multi-node representation learning improved through labeling trick approach

## Executive Summary
This paper addresses a fundamental limitation in Graph Neural Networks (GNNs) for multi-node tasks by proposing the "labeling trick." Standard GNNs fail to capture dependencies among multiple nodes when aggregating single-node representations, as they cannot distinguish which nodes belong to a target set. The labeling trick solves this by labeling nodes based on their relationships with target nodes before applying the GNN, ensuring both target-nodes-distinguishing and permutation equivariance properties. The method significantly improves performance on various multi-node tasks including link prediction, hyperedge prediction, and subgraph prediction across multiple datasets.

## Method Summary
The labeling trick enhances GNNs by modifying the input graph structure through node labeling before applying standard GNN operations. For a target node set S, nodes are labeled according to their relationship with S (e.g., zero-one labeling, set labeling, subset labeling, or poset labeling depending on the task). This labeled graph is then processed by a GNN, and node representations are aggregated to form the multi-node representation. The approach ensures structural representations by making target nodes distinguishable while preserving permutation equivariance. Different variants (set, subset, poset labeling) are proposed for different task types, with subset labeling offering efficiency improvements through shared computation across multiple node sets.

## Key Results
- Achieves state-of-the-art results on multiple datasets for undirected and directed link prediction
- Demonstrates significant performance boosts on hyperedge prediction and subgraph prediction tasks
- Shows subset labeling trick can outperform set labeling in specific cases by capturing pairwise relations efficiently

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Labeling trick enables GNNs to learn structural representations of node sets by making target nodes distinguishable while preserving permutation equivariance.
- Mechanism: The trick appends node-specific labels to the graph before applying GNN, allowing the model to condition node representations on the presence of other target nodes. This breaks the independence assumption that causes failure in standard GAE approaches.
- Core assumption: The labeling tensor satisfies both target-nodes-distinguishing and permutation equivariance properties as defined in Definition 9.
- Evidence anchors:
  - [abstract]: "The trick ensures target-nodes-distinguishing and permutation equivariance properties."
  - [section 4.1]: "Property 1 requires that if a permutation π preserving node labels exists between nodes of A and A′, then the nodes in S′ must be mapped to nodes in S by π."
  - [corpus]: No direct corpus evidence; the paper itself is the primary source for this mechanism.
- Break condition: If the labeling function violates either distinguishing or equivariance property, the mechanism fails to produce structural representations.

### Mechanism 2
- Claim: Set labeling trick with an NME GNN and injective aggregation produces structural representations of node sets.
- Mechanism: By labeling target nodes distinctly and applying a sufficiently expressive GNN, the aggregated node representations become invariant to isomorphism of the node set within the graph.
- Core assumption: The GNN is node-most-expressive (NME) and the aggregation function is injective.
- Evidence anchors:
  - [section 4.3]: "Theorem 12 Given an NME GNN and an injective set aggregation function AGG, for any S, A,S′, A′, GNN (S, A(S)) = GNN(S′, A′(S′))⇔ (S, A)≃ (S′, A′)."
  - [abstract]: "Experiments verify that the labeling trick technique can boost GNNs on various tasks, including undirected link prediction, directed link prediction, hyperedge prediction, and subgraph prediction."
  - [corpus]: Weak corpus support; neighboring papers focus on different GNN expressivity aspects.
- Break condition: If the GNN lacks sufficient expressive power or aggregation function loses information, structural representations cannot be guaranteed.

### Mechanism 3
- Claim: Subset labeling trick can outperform set labeling trick in specific cases by capturing pairwise relations that set labeling misses.
- Mechanism: By labeling only a subset of target nodes, the approach focuses on specific node pairs or relations while allowing shared computation across multiple node sets, improving efficiency and sometimes expressivity.
- Core assumption: The selected subset captures the critical relational information needed for the task.
- Evidence anchors:
  - [section 6.3.1]: "The drawback of subset labeling trick is that it captures pair-wise relation only and loses high-order relations."
  - [abstract]: "Experiments verify that the labeling trick technique can boost GNNs on various tasks, including undirected link prediction, directed link prediction, hyperedge prediction, and subgraph prediction."
  - [corpus]: No corpus evidence for this specific tradeoff; paper provides the primary analysis.
- Break condition: If high-order relations are essential for the task, subset labeling may underperform set labeling.

## Foundational Learning

- Concept: Graph isomorphism and permutation invariance
  - Why needed here: Understanding when two substructures are structurally equivalent is fundamental to defining what a "structural representation" means.
  - Quick check question: If two nodes have identical neighborhoods up to distance h, are they necessarily isomorphic in the graph?

- Concept: Expressivity of GNNs (1-WL vs NME)
  - Why needed here: Different levels of GNN expressivity interact differently with labeling tricks; knowing the distinction helps choose appropriate methods.
  - Quick check question: Can a 1-WL-GNN distinguish two nodes with different degrees but identical rooted subtrees?

- Concept: Poset isomorphism vs set isomorphism
  - Why needed here: Directed links and ordered structures require poset labeling trick, which differs fundamentally from set labeling.
  - Quick check question: Are the directed links (u,v) and (v,u) isomorphic in a poset sense if the graph is undirected?

## Architecture Onboarding

- Component map: Labeling trick → GNN → Aggregation → Prediction
- Critical path: For each node set to predict, apply labeling trick → run GNN on labeled graph → aggregate node representations → output prediction
- Design tradeoffs: Set labeling provides maximum expressivity but requires recomputing GNN for each node set; subset labeling improves efficiency but may lose some expressivity
- Failure signatures: Poor performance on tasks requiring high-order structural features; failure to distinguish isomorphic node sets when they should be different
- First 3 experiments:
  1. Implement zero-one labeling trick on a small link prediction dataset and compare with vanilla GAE
  2. Test subset(1) labeling trick with one-head routine on directed link prediction task
  3. Apply poset labeling trick to directed link prediction and verify improvement over set labeling

## Open Questions the Paper Calls Out

- Open Question 1: What is the theoretical lower bound on the expressiveness of GNNs with labeling tricks for multi-node representation learning?
  - Basis in paper: [explicit] The paper states that with a sufficiently expressive GNN and labeling trick, structural representations can be learned, but does not provide a theoretical lower bound.
  - Why unresolved: The paper does not delve into the theoretical limits of expressiveness for GNNs with labeling tricks.
  - What evidence would resolve it: A rigorous proof or counterexample demonstrating the minimum expressiveness required for GNNs with labeling tricks to learn structural representations.

- Open Question 2: How does the performance of GNNs with labeling tricks scale with the size and complexity of the graph?
  - Basis in paper: [inferred] The paper mentions computational complexity but does not explore how performance scales with graph size or complexity.
  - Why unresolved: The paper focuses on demonstrating the effectiveness of labeling tricks rather than analyzing their scalability.
  - What evidence would resolve it: Empirical studies showing the performance of GNNs with labeling tricks on graphs of varying sizes and complexities.

- Open Question 3: Can labeling tricks be extended to other types of neural networks beyond GNNs?
  - Basis in paper: [explicit] The paper focuses on GNNs but does not explore the applicability of labeling tricks to other neural network architectures.
  - Why unresolved: The paper is centered on GNNs and does not investigate the potential for extending labeling tricks to other neural network types.
  - What evidence would resolve it: Experiments or theoretical analysis demonstrating the effectiveness of labeling tricks in other neural network architectures.

## Limitations

- The theoretical foundations rely on idealized conditions that may be challenging to achieve with complex, noisy real-world graphs
- Dependency on node-most-expressive (NME) GNNs and injective aggregation functions may limit applicability
- Computational overhead of re-labeling for each node set in set labeling could be prohibitive for large-scale applications

## Confidence

- High confidence: The experimental validation showing performance improvements across multiple datasets and tasks
- Medium confidence: The theoretical guarantees regarding structural representation learning under idealized conditions
- Medium confidence: The claimed efficiency advantages of subset labeling approaches

## Next Checks

1. Test the labeling trick on graphs with significant noise or adversarial perturbations to assess robustness beyond clean benchmark datasets
2. Benchmark the computational efficiency trade-off between set labeling (full re-computation) and subset labeling (shared computation) on large graphs with millions of nodes
3. Evaluate whether the improvements persist when using GNNs that are expressive but not strictly node-most-expressive, examining the boundary conditions of the theoretical guarantees