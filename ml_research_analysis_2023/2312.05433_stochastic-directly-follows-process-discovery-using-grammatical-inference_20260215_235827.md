---
ver: rpa2
title: Stochastic Directly-Follows Process Discovery Using Grammatical Inference
arxiv_id: '2312.05433'
source_url: https://arxiv.org/abs/2312.05433
tags:
- process
- stochastic
- discovery
- sdag
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GASPD, a genetic algorithm for discovering
  stochastic process models (SDAGs) from event logs, using ALERGIA as the underlying
  grammar inference engine. The method iteratively evolves model parameters to balance
  model size and entropic relevance, outperforming the state-of-the-art DFvM algorithm
  in discovering smaller, more accurate models.
---

# Stochastic Directly-Follows Process Discovery Using Grammatical Inference

## Quick Facts
- **arXiv ID**: 2312.05433
- **Source URL**: https://arxiv.org/abs/2312.05433
- **Reference count**: 27
- **Primary result**: GASPD, a genetic algorithm for stochastic process discovery, outperforms DFvM on 10/12 real-world logs by finding smaller, more accurate models.

## Executive Summary
This paper introduces GASPD, a novel approach to stochastic process discovery that combines grammatical inference with genetic optimization. GASPD uses ALERGIA to construct stochastic models from event logs, then employs a genetic algorithm to optimize parameters balancing model size and entropic relevance. Experiments demonstrate GASPD discovers models that are both smaller and more accurate than the state-of-the-art DFvM algorithm across most of the size spectrum.

## Method Summary
GASPD combines ALERGIA grammar inference with a multi-objective genetic algorithm to discover stochastic directed action graphs (SDAGs) from event logs. The method iteratively evolves parameter triples (Ï‰, t0, f) that control state merging, initial threshold, and filtering respectively. For each parameter set, ALERGIA constructs an SDFA which is converted to a DAG, then evaluated using entropic relevance as a quality measure. The genetic algorithm optimizes these parameters across generations, with selection favoring models that balance size and relevance. The approach supports multiple abstraction levels and enables interactive exploration of model quality through generations.

## Key Results
- GASPD models are superior to DFvM in 10 out of 12 real-world datasets for human-scale model sizes
- The heuristic of breeding from "good" parents significantly accelerates genetic algorithm convergence
- GASPD achieves better trade-offs between model size and entropic relevance than traditional DFG discovery methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GASPD uses grammatical inference to construct stochastic process models that are smaller and more accurate than state-of-the-art DFGs.
- Mechanism: ALERGIA infers an SDFA from event logs, which is converted to a DAG. A genetic algorithm optimizes ALERGIA parameters to balance model size and entropic relevance.
- Core assumption: The genetic algorithm can effectively explore parameter space to find combinations yielding superior models.
- Evidence anchors: [abstract] confirms smaller, more accurate models; [section 5] describes the two-component approach; [corpus] lacks direct citations supporting genetic algorithm effectiveness.
- Break condition: If the genetic algorithm converges to local optima or the parameter space is too large for effective exploration.

### Mechanism 2
- Claim: GASPD's DAG representation allows multiple nodes for the same action, leading to more accurate models.
- Mechanism: Unlike traditional DFGs with single nodes per action, GASPD's DAGs can have multiple nodes for the same action, enabling more nuanced process representation.
- Core assumption: Increased representational power of DAGs leads to more accurate models.
- Evidence anchors: [section 4] explains DFG limitations and GASPD advantages; [corpus] lacks direct evidence but claims this as a key advantage.
- Break condition: If DAG complexity outweighs accuracy benefits or the genetic algorithm cannot effectively explore the DAG space.

### Mechanism 3
- Claim: GASPD uses entropic relevance as a quality measure to balance model accuracy and simplicity.
- Mechanism: Entropic relevance measures bits required to compress traces using the model, with lower relevance indicating better models, balancing accuracy and simplicity.
- Core assumption: Entropic relevance is suitable for balancing accuracy and simplicity in process discovery.
- Evidence anchors: [section 3] defines entropic relevance using minimum description length; [section 6] explains how it prioritizes trace frequency description; [corpus] lacks direct citations but claims this as key.
- Break condition: If entropic relevance doesn't effectively balance accuracy and simplicity or isn't suitable for specific event logs.

## Foundational Learning

- **Stochastic process discovery**: Understanding fundamentals of stochastic processes and their discovery is crucial as GASPD is a stochastic process discovery algorithm.
  - Quick check: What is the difference between a deterministic and a stochastic process model?

- **Genetic algorithms**: Understanding basics of genetic algorithms is essential as GASPD uses one to optimize parameters.
  - Quick check: How does a genetic algorithm explore a parameter space to find optimal solutions?

- **Entropy and information theory**: GASPD uses entropic relevance which relies on information theory concepts.
  - Quick check: What is the relationship between entropy and the minimum description length of a message?

## Architecture Onboarding

- **Component map**: Event log -> Preprocessing -> Genetic algorithm (population of parameter triples) -> ALERGIA -> SDFA -> DAG conversion -> Entropic relevance calculation -> Model selection

- **Critical path**:
  1. Preprocess and filter the event log
  2. Initialize a population of parameter triples
  3. For each parameter triple, use ALERGIA to construct an SDFA
  4. Convert the SDFA to a DAG
  5. Calculate the entropic relevance of the DAG
  6. Use the genetic algorithm to evolve the population towards better parameter triples
  7. Repeat steps 3-6 for multiple generations
  8. Select the Pareto-optimal models from the final population

- **Design tradeoffs**:
  - Model size vs. accuracy: GASPD aims to find models that are both small and accurate, requiring balancing these objectives
  - Computational cost vs. model quality: The genetic algorithm can be computationally expensive but can lead to better models than simpler approaches
  - Parameter space exploration vs. convergence: GASPD needs to explore a large parameter space to find good models while converging in reasonable time

- **Failure signatures**:
  - Consistently producing models that are too large or too inaccurate may indicate problems with the genetic algorithm or ALERGIA implementation
  - Failure to converge may indicate the parameter space is too large or the fitness function is unsuitable
  - Models significantly different from the true process may indicate problems with event log preprocessing or SDFA to DAG conversion

- **First 3 experiments**:
  1. Run GASPD on a simple event log with known process structure and verify it produces an accurate model
  2. Vary the filtering threshold f and observe how it affects the size and accuracy of discovered models
  3. Compare GASPD performance with a baseline process discovery algorithm on real-world event logs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the genetic search strategy be adapted to discover larger, more complex models with comparable accuracy to smaller ones?
- Basis in paper: [explicit] The authors note DFvM models have higher relevance for large sizes and suggest studying if grammar inference techniques can discover large, accurate models.
- Why unresolved: GASPD focuses on human-scale models up to 100 nodes/edges and may not scale well to larger models.
- What evidence would resolve it: Experiments applying GASPD to larger event logs with models exceeding 1000 nodes/edges, comparing relevance and accuracy to DFvM or other state-of-the-art techniques.

### Open Question 2
- Question: How would replacing ALERGIA with other grammar inference methods affect the quality and efficiency of discovered process models?
- Basis in paper: [explicit] The authors suggest exploring alternative grammatical inference techniques as a promising avenue for enhancing model quality and that replacing ALERGIA could reveal their effectiveness.
- Why unresolved: GASPD is specifically designed for ALERGIA, and performance may vary with other inference methods.
- What evidence would resolve it: Experiments using different grammar inference algorithms (e.g., MDI, BMM) within GASPD framework, comparing resulting model quality, convergence speed, and computational efficiency.

### Open Question 3
- Question: What other measurement combinations could identify Pareto-optimal models, and how would they affect characteristics?
- Basis in paper: [explicit] The authors acknowledge other measurement combinations could be considered, leading to different concrete measurements and potentially different Pareto-optimal models.
- Why unresolved: GASPD uses entropic relevance and model size as primary measures, which may not capture all aspects of quality or align with all user preferences.
- What evidence would resolve it: Experiments using alternative quality measures (e.g., fitness, precision, generalization) in conjunction with or instead of entropic relevance and model size, comparing resulting Pareto-optimal models' characteristics and alignment with user requirements.

## Limitations

- The exact implementation details of ALERGIA's state-merging algorithm and genetic algorithm parameters remain unspecified
- Claims about superiority rely heavily on experiments with 12 real-world logs without extensive validation across diverse log types
- The complexity of DAGs compared to DFGs may introduce interpretability challenges in practice

## Confidence

- **High confidence**: GASPD's ability to discover smaller models than DFvM for equivalent accuracy levels, supported by experimental results on 10/12 real-world logs
- **Medium confidence**: The claim that entropic relevance effectively balances accuracy and simplicity, as this relies on theoretical assumptions about minimum description length without extensive empirical validation
- **Low confidence**: The assertion that the genetic algorithm's exploration of parameter space is optimal, given lack of information about convergence properties and sensitivity to initial conditions

## Next Checks

1. Implement a controlled experiment varying the genetic algorithm's population size and mutation rates to assess sensitivity to these parameters across different log characteristics
2. Compare GASPD's performance on synthetic logs with known ground truth processes to quantify accuracy and identify failure modes when the underlying process structure is known
3. Conduct a user study evaluating the interpretability of GASPD's DAG models versus DFvM's simpler DFGs, measuring whether the additional complexity translates to actionable insights for process analysts