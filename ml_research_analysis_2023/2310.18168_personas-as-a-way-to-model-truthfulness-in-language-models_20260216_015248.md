---
ver: rpa2
title: Personas as a Way to Model Truthfulness in Language Models
arxiv_id: '2310.18168'
source_url: https://arxiv.org/abs/2310.18168
tags:
- truthful
- persona
- truthfulness
- agents
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes that large language models (LLMs) can distinguish
  true from false statements by modeling a "truthful persona" - a cluster of agents
  who are likely to produce truthful text. The authors provide evidence for this hypothesis
  through several experiments: Probing experiments on the TruthfulQA dataset show
  that the truthfulness of a model''s answer can be predicted from the question embedding
  before generation begins.'
---

# Personas as a Way to Model Truthfulness in Language Models

## Quick Facts
- arXiv ID: 2310.18168
- Source URL: https://arxiv.org/abs/2310.18168
- Authors: 
- Reference count: 27
- Key outcome: LLMs can distinguish true from false statements by modeling a "truthful persona" - a cluster of agents who are likely to produce truthful text.

## Executive Summary
This paper proposes that large language models can distinguish true from false statements by modeling a "truthful persona" - a cluster of agents who are likely to produce truthful text. The authors provide evidence through probing experiments on the TruthfulQA dataset, showing that truthfulness can be predicted from question embeddings before generation begins. They also demonstrate that fine-tuning on true question-answer pairs significantly improves truthfulness on unseen topics, suggesting the model generalizes truthful behavior by associating inferred agents with the truthful persona. Synthetic arithmetic experiments further support the hypothesis that LLMs can separate true and false statements and generalize truthfulness across agents when they share a truthful generative process.

## Method Summary
The paper investigates whether LLMs can infer truthful personas and generalize truthful behavior through probing experiments on the TruthfulQA dataset using linear classifiers on model activations. They fine-tune the Alpaca model using LoRA on truthful question-answer pairs and evaluate truthfulness using GPT-Judge and human evaluation. Additionally, they conduct synthetic arithmetic experiments with a 4-layer Transformer trained on controlled data where agents have known truthful or untruthful beliefs about operator semantics, testing probing and generalization performance across different setups.

## Key Results
- Probing experiments show that the truthfulness of a model's answer can be predicted from the question embedding before generation begins, with F1 scores peaking at approximately 65% at layer 17.
- Fine-tuning an LLM on true question-answer pairs significantly improves truthfulness on unseen topics, increasing from 39% to 74% according to GPT-Judge.
- In synthetic arithmetic experiments, models can separate true and false equations and generalize truthfulness across agents only when agents share a truthful generative process that enables a truthful persona.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can infer a "truthful persona" from context by clustering agents who share common features, enabling generalization of truthfulness across topics.
- Mechanism: The model maps an inferred agent from the input context to a latent persona cluster. If the agent belongs to the truthful persona, the model generates truthful responses; otherwise, it generates untruthful ones.
- Core assumption: The pretraining data contains shared features among truthful agents (e.g., formal tone, citations) that the model can detect and use for clustering.
- Evidence anchors:
  - [abstract] "By modeling this truthful persona, language models can generalize truthfulness beyond the specific contexts in which each agent generated the training text."
  - [section 2.1] "Probing result is significantly above random guessing from very early layers in the model and peaks at layer 17 at approximately 65% F1, suggesting that the model encodes a latent variable correlated with truthfulness of the answer."
  - [corpus] Weak: Corpus search found no direct evidence of persona clustering in related work, but mentions of "truthfulness" and "alignment" suggest indirect relevance.
- Break condition: If truthful agents do not share detectable common features, the model cannot form a truthful persona and thus cannot generalize truthfulness.

### Mechanism 2
- Claim: Finetuning on true question-answer pairs improves truthfulness on unseen topics by associating inferred agents with the truthful persona.
- Mechanism: During finetuning, the model learns to map agents (even unseen ones) to the truthful persona, enabling truthful generation on new topics without direct training examples.
- Core assumption: The truthful persona learned during pretraining persists and can be reinforced through finetuning, even when applied to unrelated topics.
- Evidence anchors:
  - [abstract] "Finetuning an LLM on true question-answer pairs significantly improves truthfulness on unseen topics."
  - [section 2.2] "Truthfulness of model generations increases from 39% to 74% after TF, and decreases to 10% after UF; a similar trend holds according to human evaluation."
  - [corpus] Weak: No direct corpus evidence of finetuning generalization, but "alignment" and "truthfulness" in related work imply potential for such mechanisms.
- Break condition: If finetuning overwrites the truthful persona with topic-specific patterns, generalization to unseen topics fails.

### Mechanism 3
- Claim: In a synthetic arithmetic environment, models can separate true and false equations and generalize truthfulness across agents only when agents share a truthful generative process enabling a truthful persona.
- Mechanism: The model clusters agents based on shared truthful beliefs about operator semantics. Generalization to unseen operators occurs because the truthful persona encodes the correct interpretation.
- Core assumption: Agents with shared truthful beliefs form a cluster distinguishable from untruthful agents, and this clustering is learnable by the model.
- Evidence anchors:
  - [abstract] "Language models can separate true and false statements, and generalize truthfulness across agents; but only if agents in the training data share a truthful generative process that enables the creation of a truthful persona."
  - [section 3.1] "Probes get higher F1 in the truthful persona training setup... This result supports our persona hypothesis where we can discern true and false statements only if truthful agents are clustered to form a truthful persona."
  - [corpus] Weak: No direct corpus evidence of synthetic arithmetic setups, but "truthfulness" and "generalization" in related work suggest relevance.
- Break condition: If agents' truthful beliefs are not shared or detectable, the model cannot form a truthful persona and fails to generalize.

## Foundational Learning

- Concept: Linear probing for latent representations
  - Why needed here: To detect whether the model's internal representations encode truthfulness before generation.
  - Quick check question: Can you train a linear classifier on layer embeddings to predict a binary label (e.g., truthful vs. untruthful) with accuracy above chance?

- Concept: Fine-tuning with LoRA
  - Why needed here: To efficiently adapt the model to new tasks (e.g., answering truthfully) without full retraining.
  - Quick check question: Can you apply LoRA to a pre-trained model and verify that the low-rank updates improve performance on a downstream task?

- Concept: Synthetic data generation for controlled experiments
  - Why needed here: To isolate the effect of persona clustering by controlling agent beliefs and operator semantics.
  - Quick check question: Can you generate synthetic data with agents having known truthful/untruthful beliefs and verify that a model can learn to distinguish them?

## Architecture Onboarding

- Component map: Input encoder -> Latent persona mapper -> Generator -> Probing layer
- Critical path:
  1. Encode input context to infer agent.
  2. Map agent to persona cluster.
  3. Generate output conditioned on persona.
  4. (Optional) Probe embeddings to predict truthfulness.
- Design tradeoffs:
  - Persona granularity: Finer clusters may improve accuracy but risk overfitting.
  - Probing depth: Earlier layers may capture general features; later layers may capture task-specific ones.
  - Synthetic vs. real data: Synthetic data offers control but may lack real-world complexity.
- Failure signatures:
  - Probing accuracy near chance: Model does not encode truthfulness in embeddings.
  - No improvement after finetuning: Truthful persona not reinforced or overwritten.
  - High variance in synthetic experiments: Lack of consistent clustering due to noisy or insufficient features.
- First 3 experiments:
  1. Train a linear probe on layer embeddings to predict truthfulness of Alpaca's answers on TruthfulQA.
  2. Fine-tune Alpaca on true question-answer pairs and evaluate truthfulness on unseen topics.
  3. Train a 4-layer Transformer on synthetic arithmetic data with/without truthful personas and test probing/generalization performance.

## Open Questions the Paper Calls Out
- How robust are truthful personas across different language models and pretraining datasets?
- What are the specific features or latent variables that define truthful personas in language models?
- How do truthful personas interact with other cognitive processes in language models, such as reasoning and common sense?

## Limitations
- The persona clustering mechanism relies on detecting shared features among truthful agents, but the paper does not provide evidence that such features are consistently present or detectable across diverse datasets.
- The synthetic arithmetic experiments are controlled but may oversimplify real-world complexity, limiting generalizability of the findings.
- The paper does not address potential confounding factors, such as the role of model architecture or pretraining data quality in enabling persona clustering.

## Confidence
- **High**: The experimental results showing improved truthfulness after fine-tuning and probing accuracy above chance are well-supported by the data.
- **Medium**: The hypothesis that LLMs can generalize truthfulness by modeling personas is plausible but relies on untested assumptions about feature detectability and clustering.
- **Low**: The synthetic arithmetic experiments provide limited evidence for the persona hypothesis due to their simplicity and lack of real-world complexity.

## Next Checks
1. Test the persona clustering hypothesis on a more diverse dataset (e.g., Wikipedia) to verify if truthful agents share detectable features across different domains.
2. Conduct ablation studies on the probing experiments to determine which model layers and features contribute most to truthfulness prediction.
3. Extend the synthetic arithmetic experiments to include more complex operators and agent beliefs to test the robustness of the persona clustering mechanism.