---
ver: rpa2
title: 'Exploring the Integration of Large Language Models into Automatic Speech Recognition
  Systems: An Empirical Study'
arxiv_id: '2307.06530'
source_url: https://arxiv.org/abs/2307.06530
tags:
- speech
- recognition
- llms
- gpt-3
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored integrating Large Language Models (LLMs) into
  Automatic Speech Recognition (ASR) systems to improve transcription accuracy using
  in-context learning. The experiments used Aishell-1 and LibriSpeech datasets with
  GPT-3.5 and GPT-4 models, testing various instructions, shot configurations, and
  multiple correction attempts.
---

# Exploring the Integration of Large Language Models into Automatic Speech Recognition Systems: An Empirical Study

## Quick Facts
- arXiv ID: 2307.06530
- Source URL: https://arxiv.org/abs/2307.06530
- Reference count: 40
- Key outcome: LLM-corrected transcriptions consistently resulted in higher WER compared to original ASR outputs

## Executive Summary
This empirical study investigated whether Large Language Models could improve Automatic Speech Recognition accuracy through in-context learning. Using Aishell-1 and LibriSpeech datasets with GPT-3.5 and GPT-4 models, researchers tested various instructions, shot configurations, and multiple correction attempts. Despite comprehensive experimentation, LLM corrections consistently degraded transcription quality, with corrected outputs showing higher Word Error Rates than the original ASR results. The findings demonstrate significant challenges in applying LLMs to ASR error correction and suggest that fundamental mismatches between LLM training objectives and speech-specific error patterns limit their effectiveness in this domain.

## Method Summary
The study used in-context learning to integrate LLMs (GPT-3.5 and GPT-4) into ASR systems for transcription correction. Researchers employed Wenet's hybrid CTC/attention model to generate initial transcriptions from Aishell-1 (Mandarin) and LibriSpeech (English) datasets. They created prompts with various instructions (4 types), shot configurations (1-3 examples), and multiple attempts (1-5). The LLM-corrected transcriptions were compared against ground truth using Word Error Rate as the primary metric. No model training or fine-tuning was performed; only prompt engineering and API calls were used.

## Key Results
- LLM-corrected transcriptions consistently showed higher Word Error Rates than original ASR outputs across all experimental conditions
- Detailed instructions and multiple in-context examples provided marginal improvements but failed to overcome fundamental limitations
- The study tested two ASR datasets, two LLM models (GPT-3.5 and GPT-4), and various prompt configurations without achieving satisfactory correction performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM in-context learning fails to correct ASR errors due to mismatch between language model training objectives and speech-specific error patterns
- Mechanism: LLMs are trained to optimize general text coherence and fluency, not to recognize or correct errors that stem from acoustic distortions, speaker accents, or background noise—types of errors common in ASR outputs
- Core assumption: ASR errors follow distributions (e.g., substitution, insertion, deletion) that are structurally different from natural text errors LLMs were trained on
- Evidence anchors:
  - [abstract]: "corrected sentences from the LLMs frequently resulted in higher Word Error Rates (WER), demonstrating the limitations of LLMs in speech applications."
  - [section 4.2]: Examples show LLM changes correct words to incorrect ones (e.g., "YOU" → "HIS", "FRESH NELLIE" → "EXPRESS DELI")
  - [corpus]: No direct corpus evidence for this claim; weak correlation with related work
- Break condition: If LLMs are fine-tuned or augmented with acoustic-aware modules, the mismatch may be reduced

### Mechanism 2
- Claim: Detailed instructions do not sufficiently guide LLMs to correct ASR errors because the models lack speech-aware reasoning
- Mechanism: Providing more detailed task descriptions (e.g., categorizing error types like substitution, insertion, deletion) increases model understanding of the goal but does not compensate for the absence of speech-specific knowledge in the model's training data
- Core assumption: Instruction-following ability in LLMs is domain-general and cannot bridge the gap to speech-specific error correction without domain adaptation
- Evidence anchors:
  - [section 4.2]: "supplying detailed instructions to the Language Model (LLM) improves its performance. However, even with extremely detailed instructions, the LLM model does not demonstrate adequate performance."
  - [section 4.2]: WER increases after correction despite detailed instruction 4
  - [corpus]: No corpus evidence for this mechanism
- Break condition: If the LLM is augmented with explicit speech representation or fine-tuned on ASR error correction tasks

### Mechanism 3
- Claim: Increasing the number of in-context examples (shots) improves LLM performance slightly but not enough to surpass baseline ASR without LLM
- Mechanism: Additional examples provide more context for pattern recognition, but the fundamental mismatch between LLM training and ASR error types limits the effectiveness of in-context learning
- Core assumption: In-context learning benefits are bounded by the representational capacity and training distribution of the base model
- Evidence anchors:
  - [section 4.2]: "providing the model with more examples led to enhanced performance" but "none of them resulted in a satisfactory WER."
  - [section 4.2]: 3-shot configuration still yielded higher WER than no LLM
  - [corpus]: Weak evidence; related work suggests in-context learning helps in other domains but not here
- Break condition: If the LLM is pre-trained or fine-tuned with ASR-specific data, in-context learning may become more effective

## Foundational Learning

- Concept: Word Error Rate (WER) calculation and interpretation
  - Why needed here: WER is the primary evaluation metric; understanding how it is computed is essential to interpret results
  - Quick check question: If a transcription has 3 substitutions, 1 insertion, and 2 deletions out of 100 words, what is the WER?

- Concept: In-context learning vs. fine-tuning
  - Why needed here: The paper experiments with in-context learning (prompting) rather than model adaptation; understanding the distinction is key to interpreting limitations
  - Quick check question: What is the main difference between in-context learning and fine-tuning in terms of data usage and model modification?

- Concept: ASR error types (substitution, insertion, deletion)
  - Why needed here: These are the error categories the paper attempts to correct; knowing them helps understand why LLM corrections may fail
  - Quick check question: In ASR, what type of error occurs when a word is incorrectly replaced by another word?

## Architecture Onboarding

- Component map:
  - ASR model (CTC/attention hybrid) → LLM (GPT-3.5/GPT-4) → corrected transcription
  - Data pipeline: Aishell-1 (Mandarin) and LibriSpeech (English) → ASR output → LLM prompt → corrected output
  - Evaluation: WER comparison between ASR-only and ASR+LLM

- Critical path:
  1. ASR model generates transcription
  2. Prompt is constructed with instruction and examples
  3. LLM generates corrected text
  4. WER is computed against ground truth
  5. Results are compared across settings

- Design tradeoffs:
  - Prompt complexity vs. LLM performance: More detailed instructions help slightly but do not overcome fundamental limitations
  - Number of shots vs. context limits: More examples improve results marginally but are constrained by token limits
  - Multiple attempts vs. practicality: Selecting the lowest-WER attempt helps analysis but is not feasible in real applications

- Failure signatures:
  - WER increases after LLM correction
  - LLM introduces semantically incorrect changes
  - LLM fails to recognize speech-specific error patterns
  - Results are consistent across different LLM versions and instructions

- First 3 experiments:
  1. Run baseline ASR-only transcription and compute WER on test sets
  2. Apply a simple one-shot LLM correction with minimal instruction and measure WER change
  3. Vary the instruction detail level (e.g., instruction 1 vs. instruction 4) and observe impact on WER

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of LLM models make them ineffective at correcting ASR transcription errors compared to traditional language models?
- Basis in paper: [explicit] The paper states that despite extensive experimentation with various LLM models, instructions, and settings, corrected transcriptions consistently resulted in higher Word Error Rates compared to original ASR outputs
- Why unresolved: The paper demonstrates the ineffectiveness but does not deeply analyze the underlying reasons why LLMs struggle with this specific task
- What evidence would resolve it: Comparative analysis of LLM architecture, training data, and performance metrics against traditional language models on ASR-specific tasks, particularly focusing on their ability to handle speech-specific errors like homophones, disfluencies, and context-dependent corrections

### Open Question 2
- Question: Could fine-tuning LLMs specifically on ASR error patterns and correction strategies improve their performance in this domain?
- Basis in paper: [inferred] The paper tested various prompts and instructions but did not explore model adaptation or fine-tuning approaches, only testing out-of-the-box LLM capabilities
- Why unresolved: The study only tested general-purpose LLMs without any domain-specific adaptation, leaving open the question of whether specialized training could overcome the observed limitations
- What evidence would resolve it: Experimental results comparing fine-tuned LLMs against general-purpose LLMs on the same ASR correction tasks, measuring improvements in WER and types of errors corrected

### Open Question 3
- Question: What hybrid approaches combining LLMs with traditional ASR post-processing techniques might yield better results than either approach alone?
- Basis in paper: [inferred] The paper focused exclusively on using LLMs in isolation for correction, without exploring potential synergies with existing ASR post-processing methods like n-best list rescoring or confidence-based filtering
- Why unresolved: The study's methodology was limited to direct LLM correction without investigating how LLMs might complement or enhance existing ASR processing pipelines
- What evidence would resolve it: Comparative performance analysis of hybrid systems that integrate LLM capabilities with traditional ASR post-processing techniques, demonstrating improvements in WER and error correction accuracy

## Limitations
- The study exclusively relied on in-context learning without exploring model adaptation or fine-tuning approaches
- Experiments were limited to two specific ASR systems and two language datasets, which may not generalize to other architectures or languages
- The evaluation focuses solely on WER metrics without considering other potential benefits such as semantic coherence or downstream task performance

## Confidence
- **High Confidence**: The core empirical finding that LLM-corrected transcriptions consistently yield higher WER than original ASR outputs
- **Medium Confidence**: The assertion that the mismatch between LLM training objectives and ASR error patterns is the primary cause of poor performance
- **Low Confidence**: The claim that in-context learning is fundamentally inadequate for ASR error correction without further research

## Next Checks
1. **Fine-tuning Validation**: Fine-tune a smaller language model on ASR error correction data (synthetic or real) and compare its performance against both baseline ASR and the LLM in-context learning approach to determine if model adaptation overcomes the observed limitations

2. **Error Type Analysis**: Conduct detailed analysis of which specific error types (substitutions, insertions, deletions) the LLM fails to correct most frequently, and whether certain error categories show potential for improvement with different prompting strategies

3. **Cross-Architecture Testing**: Apply the same in-context learning methodology to a different ASR architecture (e.g., end-to-end transformer-based ASR) to assess whether the observed limitations are specific to the hybrid CTC/attention model or represent a broader challenge