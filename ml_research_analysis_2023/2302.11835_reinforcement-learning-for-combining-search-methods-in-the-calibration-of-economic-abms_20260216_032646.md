---
ver: rpa2
title: Reinforcement Learning for Combining Search Methods in the Calibration of Economic
  ABMs
arxiv_id: '2302.11835'
source_url: https://arxiv.org/abs/2302.11835
tags:
- calibration
- methods
- search
- loss
- sampler
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the problem of calibrating agent-based economic
  models, which involves derivative-free search in a high-dimensional parameter space.
  They propose using a reinforcement learning (RL) approach to automatically select
  and combine different search methods during the calibration process.
---

# Reinforcement Learning for Combining Search Methods in the Calibration of Economic ABMs

## Quick Facts
- arXiv ID: 2302.11835
- Source URL: https://arxiv.org/abs/2302.11835
- Reference count: 19
- The authors propose using reinforcement learning to automatically select and combine different search methods during the calibration of agent-based economic models, achieving better performance than any single method or fixed combination.

## Executive Summary
This paper addresses the challenging problem of calibrating agent-based economic models by proposing a reinforcement learning (RL) approach to automatically select and combine different search methods during calibration. The authors frame the calibration as a reinforcement learning problem where an agent chooses between multiple samplers (random forests, XG-boost, Gaussian processes, and genetic algorithms) based on their performance. Their approach outperforms individual methods and fixed combinations, achieving lower loss values on a macroeconomic ABM calibration task without requiring prior information about the loss landscape.

## Method Summary
The authors develop a reinforcement learning framework for ABM calibration that treats sampler selection as a non-stationary multi-armed bandit problem. They implement a pool of search methods including random forest, XG-boost, Gaussian process, and genetic algorithm samplers, along with various combinations. The RL agent uses an epsilon-greedy policy to select methods based on fractional loss improvements, with each parameter evaluation running 5 independent ABM simulations. The calibration task involves 11 parameters of the CATS macroeconomic ABM, optimized against 5 historical US economic time series.

## Key Results
- Random forest-based samplers consistently outperform other individual methods on the CATS ABM calibration task
- Combining search methods generally provides better performance than any single method
- The RL-based scheduling policy outperforms all tested fixed schedules and method combinations

## Why This Works (Mechanism)

### Mechanism 1
Random Forest samplers consistently outperform other single methods because they handle high-dimensional, non-convex, and potentially discontinuous loss surfaces better than Gaussian processes or XG-boost alone. Random Forests interpolate previously evaluated loss values using an ensemble of decision trees, allowing them to capture complex interactions and discontinuities in the ABM loss landscape without strong smoothness assumptions. This works because the loss function's ruggedness and discontinuities are better modeled by tree-based ensembles than by smooth kernel methods. If the loss surface becomes very smooth or if dimensionality becomes extremely high relative to sample size, the advantage of RF may diminish.

### Mechanism 2
Combining search methods (especially coupling a machine learning surrogate with a genetic algorithm like BB) mitigates individual sampler biases and improves performance. Different samplers explore the parameter space differently (global vs local, exploitation vs exploration). By alternating between them, the algorithm avoids getting stuck in local minima that any single sampler might find. This works because no single sampler is optimal across all regions of the loss landscape, so combining them provides complementary strengths. If one sampler dominates across all loss regions, combining may add unnecessary complexity without benefit.

### Mechanism 3
The RL-based scheduling policy automatically adapts method selection based on current calibration state, outperforming fixed schedules. The RL agent treats samplers as actions in a non-stationary multi-armed bandit problem, learning to exploit the best-performing sampler while exploring alternatives, with rewards based on fractional loss improvements. This works because the relative performance of samplers changes during calibration (e.g., global exploration needed early, local refinement later), and this can be learned online. If the loss landscape is stationary or if all samplers perform similarly throughout, the RL overhead may not justify its use.

## Foundational Learning

- **Concept: Multi-armed bandit (MAB) problems**
  - Why needed here: The calibration scheduling problem maps naturally to MAB, where each sampler is an "arm" with unknown reward distribution that changes over time.
  - Quick check question: In a stationary MAB, what would be the optimal long-run strategy if you knew the true reward distributions?

- **Concept: Exploration-exploitation tradeoff**
  - Why needed here: The RL agent must balance trying different samplers (exploration) versus using the currently best-known sampler (exploitation) to maximize cumulative reward.
  - Quick check question: What happens to cumulative reward if the agent explores too much versus too little?

- **Concept: Non-stationary reward distributions**
  - Why needed here: The effectiveness of samplers changes as calibration progresses (high loss early vs low loss late), requiring algorithms that adapt to changing reward distributions.
  - Quick check question: Why is the standard UCB algorithm suboptimal for this calibration problem?

## Architecture Onboarding

- **Component map:** ABM simulator (CATS model) -> Loss function calculator (method of moments with diagonal weighting) -> Sampler pool (RF, XB, GP, BB, H, and combinations) -> RL agent (MAB with epsilon-greedy policy) -> Evaluation framework (parallel execution, multiple runs, statistical analysis)

- **Critical path:** Sampler → ABM simulation (5 runs) → Loss calculation → RL agent decision → Next sampler selection

- **Design tradeoffs:**
  - Single vs multiple samplers: Multiple samplers add complexity but mitigate bias
  - Fixed vs adaptive scheduling: Adaptive (RL) performs better but requires learning overhead
  - Parallel vs sequential evaluation: Parallel speeds up calibration but increases resource requirements

- **Failure signatures:**
  - All samplers plateau early: May indicate poor sampler diversity or difficult loss landscape
  - RL agent sticks to one sampler: Could indicate insufficient exploration or true dominance of that sampler
  - High variance in results: May require more simulation runs per parameter set

- **First 3 experiments:**
  1. Run each sampler individually for 3600 evaluations to establish baseline performance hierarchy
  2. Test all pairwise combinations in round-robin fashion to verify bias mitigation effect
  3. Implement RL agent with epsilon-greedy policy using the 4 main samplers and compare against best fixed combination

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the RL-based calibration scheme scale with increasing dimensionality of the parameter space in agent-based models? The authors mention that their RL scheme outperforms other methods on a macroeconomic ABM with 11 parameters, but do not explore how performance changes with higher dimensions. This remains unresolved because the paper only tests the RL scheme on a single ABM with a fixed number of parameters. Testing the RL scheme on ABMs with varying numbers of parameters and analyzing how performance metrics (e.g., final loss, convergence speed) change with dimensionality would resolve this question.

### Open Question 2
Can the RL calibration framework be extended to handle non-stationary reward distributions that change not only during a single calibration run but also across different calibration tasks? The authors acknowledge that reward distributions in their MAB formulation are non-stationary during calibration, but do not address how the framework would perform when the underlying ABM changes. This remains unresolved because the paper focuses on calibrating a single ABM model and does not explore the framework's adaptability to different models or tasks. Evaluating the RL scheme on multiple distinct ABM calibration tasks and measuring its ability to quickly adapt to new reward distributions would resolve this question.

### Open Question 3
How does the performance of the RL-based calibration scheme compare to state-of-the-art black-box optimization algorithms when applied to the same ABMs? The authors benchmark their RL scheme against several search methods but do not compare it to modern black-box optimization techniques like Bayesian optimization or evolutionary strategies. This remains unresolved because the comparison is limited to a subset of search methods, and the authors do not explore how their approach stacks up against more recent optimization algorithms. Conducting a comprehensive benchmark of the RL scheme against a wide range of black-box optimization algorithms on multiple ABM calibration tasks would resolve this question.

## Limitations
- The RL approach requires extensive computational resources (3600 evaluations with 5 parallel runs each), making it potentially impractical for larger or more complex ABMs
- The performance benefits of RL over fixed schedules, while statistically significant, may not justify the added algorithmic complexity in all cases
- The paper does not thoroughly explore how sensitive the RL performance is to hyperparameter choices like epsilon in the epsilon-greedy policy

## Confidence

**High Confidence:** The claim that random forest-based samplers outperform other individual methods is well-supported by the empirical results across multiple experiments. The mechanism explanation (handling non-convex, discontinuous loss surfaces) is consistent with known properties of tree-based methods.

**Medium Confidence:** The assertion that combining search methods consistently improves performance is supported by the data, but the magnitude of improvement varies considerably across different method combinations. The paper could provide more analysis of when combinations fail.

**Medium Confidence:** The RL-based scheduling advantage is demonstrated empirically, but the theoretical justification for why RL should outperform the best fixed combination is not fully developed. The results could partly reflect overfitting to the specific CATS model calibration problem.

## Next Checks

1. **Generalization Test:** Apply the RL scheduling approach to a different ABM (e.g., epidemiological or social network model) to verify that the performance gains are not specific to the CATS macroeconomic model.

2. **Hyperparameter Sensitivity:** Systematically vary the RL epsilon parameter and analyze how performance changes across the full range of exploration rates, identifying if there's a broad optimal range or if performance is highly sensitive.

3. **Computational Trade-off Analysis:** Quantify the exact computational overhead of the RL approach versus fixed schedules and calculate the break-even point where the performance gain justifies the additional complexity.