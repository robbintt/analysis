---
ver: rpa2
title: 'ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision
  Transformer'
arxiv_id: '2306.06446'
source_url: https://arxiv.org/abs/2306.06446
tags:
- shift
- shiftaddvit
- attention
- vision
- vits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ShiftAddViT, a new type of multiplication-reduced
  vision transformer that reparameterizes pre-trained ViTs with a mixture of multiplication
  primitives, including bitwise shifts and additions. The method aims to achieve end-to-end
  inference speedups on GPUs without requiring training from scratch.
---

# ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer

## Quick Facts
- arXiv ID: 2306.06446
- Source URL: https://arxiv.org/abs/2306.06446
- Reference count: 40
- Up to 5.18× latency reductions on GPUs and 42.9% energy savings while maintaining comparable accuracy

## Executive Summary
This paper introduces ShiftAddViT, a novel approach to accelerating Vision Transformers by reparameterizing pre-trained models with multiplication primitives including bitwise shifts and additions. The method replaces dense matrix multiplications in attention mechanisms with additive kernels and MLPs with shift kernels, achieving significant inference speedups on GPUs without requiring training from scratch. The authors develop custom TVM kernels for these operations and propose a mixture-of-experts framework to maintain accuracy when applying shift operations to MLPs.

## Method Summary
ShiftAddViT reparameterizes pre-trained ViTs through a two-stage fine-tuning process: first converting multi-head self-attention to linear attention and reparameterizing MatMuls with add layers, then reparameterizing MLPs or linear layers with shift or MoE layers. The method employs TVM to implement and optimize customized kernels for practical hardware deployment on GPUs. A new mixture-of-experts framework dynamically routes input tokens to either multiplication or shift experts based on learned gate values, with a latency-aware load-balancing loss to optimize expert assignment.

## Key Results
- Achieves up to 5.18× latency reductions on GPUs
- Delivers 42.9% energy savings compared to baseline ViTs
- Maintains comparable accuracy on image classification and novel view synthesis tasks

## Why This Works (Mechanism)

### Mechanism 1
Bitwise shifts and additions can replace dense multiplications in ViTs without significantly compromising accuracy when using the right mapping strategy. By mapping MatMuls to Add layers (after binarizing one operand) and Linears to Shift layers, overall energy and latency are reduced while maintaining accuracy through selective processing of important tokens by Multiplication experts.

### Mechanism 2
The mixture-of-experts (MoE) framework compensates for accuracy loss when applying Shift to MLPs by dynamically routing input tokens to either Multiplication or Shift experts based on a learned gate function. The latency-aware load-balancing loss ensures faster experts handle more tokens, minimizing synchronization overhead.

### Mechanism 3
TVM-optimized kernels for Shift and Add operations yield measurable GPU speedups over default PyTorch implementations by exploiting reduced bit-width and simpler arithmetic to reduce data movement and compute cycles.

## Foundational Learning

- **Concept: Quantization** - Why needed: Binary quantization of one operand in MatMul enables replacing multiplications with additions, drastically reducing computational cost. Quick check: What happens to the distribution of values when you binarize a weight matrix? How does this affect downstream accuracy?

- **Concept: Mixture-of-Experts (MoE) routing** - Why needed: MoE allows dynamic assignment of tokens to either expensive (multiplication) or cheap (shift) experts, preserving accuracy while improving efficiency. Quick check: How does the latency-aware load-balancing loss shape the distribution of tokens across experts? What happens if one expert is consistently overloaded?

- **Concept: TVM kernel compilation and optimization** - Why needed: Custom TVM kernels are necessary to realize the hardware efficiency promised by Shift and Add operations on GPUs. Quick check: What are the key differences between a TVM kernel and a PyTorch operator in terms of memory layout and compute pipeline? How does this affect speed?

## Architecture Onboarding

- **Component map**: Input token → Router (gate computation) → Expert selection (Multiplication or Shift) → Forward pass (full MatMul or custom MatShift/MatAdd) → Output aggregation → Next layer
- **Critical path**: 1. Token → Router (gate computation) 2. Gate → Expert selection 3. Expert → Forward pass 4. Output aggregation → Next layer
- **Design tradeoffs**: Shift vs. Add (speed vs. expressiveness), binarization granularity (speed vs. fidelity), router complexity (accuracy vs. latency)
- **Failure signatures**: Accuracy drop (poor routing decisions or excessive quantization error), latency regression (inefficient kernel dispatch or mismatched expert latencies), memory bloat (MoE expert duplication)
- **First 3 experiments**: 1. Apply Shift-only reparameterization to DeiT-T and measure accuracy vs. baseline 2. Implement simple router splitting tokens evenly between Mult. and Shift experts; measure latency and accuracy 3. Compare custom TVM MatShift kernel speed vs. PyTorch FakeShift on tensor shape [B, H, K, M, N] = [32, 5, 196, 32, 16]

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed ShiftAddViT framework perform on other vision tasks beyond 2D image classification and 3D novel view synthesis? The authors state extensive experiments were conducted on various 2D/3D Transformer-based vision tasks but only report results on image classification and novel view synthesis.

### Open Question 2
What is the impact of the latency-aware load-balancing loss on the overall performance of ShiftAddViT, and how does it compare to other balancing strategies? While the authors mention this loss function, they do not compare it to other potential balancing strategies or analyze its specific contribution.

### Open Question 3
How does the performance of ShiftAddViT scale with increasing model size and complexity? The authors demonstrate effectiveness on various model sizes but do not explore performance on state-of-the-art large-scale models or models with increased complexity.

## Limitations
- Accuracy-fidelity trade-off: The exact degradation metrics across different ViT architectures are not fully detailed
- Hardware dependency: Speedups are specifically measured on GPUs using TVM-optimized kernels, limiting generalizability to other hardware platforms
- Two-stage fine-tuning requirement: Limits practical applicability where fine-tuning pre-trained models is not feasible

## Confidence
- **High Confidence**: Hardware efficiency argument (multiplication vs. shift/add energy costs) supported by empirical kernel speedup measurements
- **Medium Confidence**: Overall accuracy preservation claim supported by reported results but needs more detailed analysis of degradation patterns
- **Medium Confidence**: Effectiveness of latency-aware load-balancing loss demonstrated but sensitivity to hyperparameters not fully explored

## Next Checks
1. Conduct detailed analysis of accuracy loss when progressively replacing more MLP layers with shift operations without MoE routing
2. Measure performance benefits on different hardware platforms (NPUs, mobile GPUs) to assess generalizability
3. Evaluate whether the two-stage fine-tuning can be reduced to a single stage or eliminated entirely through better initialization strategies