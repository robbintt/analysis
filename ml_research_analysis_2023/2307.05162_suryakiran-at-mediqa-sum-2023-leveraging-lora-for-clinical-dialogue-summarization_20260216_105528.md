---
ver: rpa2
title: 'SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization'
arxiv_id: '2307.05162'
source_url: https://arxiv.org/abs/2307.05162
tags:
- section
- subtask
- lora
- table
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates Low Rank Adaptation (LoRA), a parameter-efficient
  fine-tuning (PEFT) method, for clinical dialogue summarization tasks in the MEDIQA-Sum
  2023 challenge. The authors compare LoRA to full fine-tuning of large language models
  and demonstrate that LoRA achieves comparable performance while significantly reducing
  computational and storage costs.
---

# SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization

## Quick Facts
- arXiv ID: 2307.05162
- Source URL: https://arxiv.org/abs/2307.05162
- Reference count: 27
- Achieved 1st place in SubTask B (dialogue summarization) with mean score of 0.5732 across ROUGE-1, BERTScore F1, and BLEURT metrics

## Executive Summary
This paper evaluates Low Rank Adaptation (LoRA) for clinical dialogue summarization tasks in the MEDIQA-Sum 2023 challenge. The authors demonstrate that LoRA achieves comparable performance to full fine-tuning while significantly reducing computational and storage costs. For SubTask A (section header classification), their LoRA-based Bio-ClinicalBERT model achieved 73.5% accuracy, ranking 8th out of 23 submissions. For SubTask B (dialogue summarization), their ensemble approach using LoRA with BioBart-V2-Large and Flan-T5-Large models achieved first place with a mean score of 0.5732 across ROUGE-1, BERTScore F1, and BLEURT metrics.

## Method Summary
The authors apply LoRA to fine-tune pre-trained language models for two clinical dialogue summarization subtasks. For classification, they use Bio-ClinicalBERT with LoRA adapters, fine-tuning for 150 epochs with a learning rate of 1e-3 and batch size of 16. For summarization, they fine-tune both BioBart-V2-Large and Flan-T5-Large with LoRA using 3-fold cross-validation, then ensemble the outputs using a semantic similarity-based method called "Generating Best Summary." All models use LoRA rank 8 and alpha 32, with dialogues limited to 300 tokens and concatenated with section descriptions using a SEP token.

## Key Results
- LoRA-based Bio-ClinicalBERT achieved 73.5% accuracy for section header classification (8th out of 23 submissions)
- Ensemble of LoRA fine-tuned BioBart-V2-Large and Flan-T5-Large achieved first place in dialogue summarization (mean score 0.5732)
- Flan-T5-Large outperformed the domain-specific BioBart-V2-Large model in summarization tasks
- LoRA enabled fine-tuning on consumer-grade hardware while maintaining performance comparable to full fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA achieves comparable performance to full fine-tuning while updating only a small subset of parameters
- Mechanism: LoRA freezes the pre-trained weights and learns low-rank matrices (A and B) that approximate the update to the original weight matrix, reducing the number of trainable parameters
- Core assumption: The update matrix for fine-tuning has low intrinsic rank
- Evidence anchors:
  - [abstract] "LoRA achieves comparable performance while significantly reducing computational and storage costs"
  - [section] "LoRA works at par with end-to-end finetuning for a large language model"
  - [corpus] Weak - no direct citations in related papers
- Break condition: If the update matrix has high rank or the task requires complex adaptations not captured by low-rank updates

### Mechanism 2
- Claim: LoRA enables efficient fine-tuning on consumer-grade hardware
- Mechanism: By keeping most parameters frozen and only updating low-rank matrices, LoRA drastically reduces memory requirements during training
- Core assumption: Consumer-grade GPUs have sufficient memory to handle LoRA's additional parameters
- Evidence anchors:
  - [section] "such models could be fine tuned on consumer grade hardware such as K80 and T4"
  - [section] "fine-tuning large-scale PLMs is prohibitively costly"
  - [corpus] Weak - related papers discuss PEFT but not specific hardware requirements
- Break condition: If the task requires very large model capacity or if consumer-grade hardware has insufficient VRAM

### Mechanism 3
- Claim: Ensemble of LoRA models improves summarization performance
- Mechanism: Multiple LoRA models trained on different folds or architectures capture diverse aspects of the data, and their outputs are combined to select the best summary
- Core assumption: Different LoRA models will produce complementary outputs
- Evidence anchors:
  - [section] "we also evaluate impact of ensembling outputs from multiple Seq2Seq models"
  - [section] "Run3 performs the best scoring rank 1 out of 13 submissions"
  - [corpus] Weak - related papers mention ensembling but not specific to LoRA
- Break condition: If ensemble diversity is low or if one model consistently outperforms others

## Foundational Learning

- Concept: Low-rank matrix approximation
  - Why needed here: Understanding how LoRA approximates weight updates with low-rank matrices is crucial for grasping the method's efficiency
  - Quick check question: What is the mathematical relationship between the rank of a matrix and its approximation using low-rank matrices?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: LoRA is a PEFT method, so understanding the broader category helps contextualize its advantages and limitations
  - Quick check question: How does LoRA differ from other PEFT methods like adapters or prefix tuning?

- Concept: Cross-validation and model ensembling
  - Why needed here: The paper uses 3-fold cross-validation and ensembles multiple models for best performance
  - Quick check question: Why might cross-validation be particularly important when evaluating PEFT methods?

## Architecture Onboarding

- Component map: Pre-trained base model (frozen) + LoRA adapter layers (trainable) → Inference with optional ensemble
- Critical path: Data preprocessing → LoRA adapter initialization → Training with frozen base → Ensemble inference
- Design tradeoffs: LoRA vs full fine-tuning (efficiency vs potential performance), LoRA vs other PEFT methods (simplicity vs adaptability)
- Failure signatures: Poor performance on complex tasks, overfitting on small datasets, instability during training
- First 3 experiments:
  1. Compare LoRA with full fine-tuning on a small subset of the clinical dialogue data
  2. Test different LoRA rank values to find optimal parameter-efficiency tradeoff
  3. Evaluate ensemble performance by comparing individual model outputs to ensemble output

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LoRA performance compare to full fine-tuning when using clinical datasets larger than the MEDIQA-Sum 2023 training data?
- Basis in paper: [explicit] The paper states LoRA works "at par with end-to-end finetuning" but only evaluates on 1,201 training samples.
- Why unresolved: The evaluation only used a relatively small clinical dataset (1,201 training samples), so scalability to larger datasets remains untested.
- What evidence would resolve it: Performance comparisons of LoRA vs. full fine-tuning on clinical dialogue summarization tasks using datasets with 10K+ samples.

### Open Question 2
- Question: Does the superior performance of Flan-T5-Large over BioBart-V2-Large in the summarization task generalize to other clinical domains beyond doctor-patient dialogues?
- Basis in paper: [explicit] The authors note "this hypothesis needs to be validated" regarding why Flan-T5-Large outperformed the domain-specific BioBart-V2-Large model.
- Why unresolved: The study only tested on doctor-patient dialogue data, leaving open whether the observation applies to other clinical text types.
- What evidence would resolve it: Head-to-head comparisons of these models on clinical notes, radiology reports, and other healthcare documents.

### Open Question 3
- Question: How does the ensemble technique using "Generating Best Summary by semantic similarity" compare to other ensemble methods in terms of hallucination reduction across different clinical summarization tasks?
- Basis in paper: [explicit] The authors state this post-ensemble technique "performs the best while giving minimum hallucinations" but don't compare it to alternative ensemble methods.
- Why unresolved: Only one ensemble approach was evaluated, without comparison to other methods like weighted averaging or rank-based fusion.
- What evidence would resolve it: Comparative analysis of hallucination rates and summary quality across multiple ensemble techniques on diverse clinical summarization datasets.

## Limitations

- Limited model diversity in comparisons - only compared LoRA to full fine-tuning without benchmarking against other PEFT methods
- Limited hyperparameter exploration - used fixed LoRA parameters without systematic tuning across different clinical domains
- No statistical significance testing - reported metrics lack statistical tests to determine meaningful performance differences

## Confidence

- High confidence: The claim that LoRA significantly reduces computational and storage costs is well-supported by the literature and the fundamental mechanism of LoRA
- Medium confidence: The claim that LoRA achieves "comparable performance" to full fine-tuning is supported by MEDIQA-Sum 2023 results but needs broader validation
- Low confidence: The claim that LoRA is the optimal PEFT method for clinical dialogue summarization cannot be substantiated without direct comparisons to alternative PEFT approaches

## Next Checks

1. **Hyperparameter sensitivity analysis**: Conduct systematic experiments varying the LoRA rank (r) and alpha parameters across a wider range (e.g., r ∈ {4, 8, 16, 32}, alpha ∈ {16, 32, 64, 128}) to identify optimal configurations for clinical dialogue summarization and assess robustness to hyperparameter choices.

2. **Cross-dataset validation**: Test the LoRA fine-tuned models on multiple clinical dialogue datasets beyond MEDIQA-Sum 2023 (e.g., MIMIC-III discharge summaries, other clinical conversation corpora) to evaluate generalizability and identify any domain-specific limitations.

3. **Comparison with alternative PEFT methods**: Implement and evaluate other PEFT approaches (adapters, prefix tuning, soft prompts) on the same clinical dialogue summarization tasks to determine whether LoRA offers advantages specific to this application domain or if the benefits are method-agnostic.