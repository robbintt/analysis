---
ver: rpa2
title: 'DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion
  Models'
arxiv_id: '2310.00902'
source_url: https://arxiv.org/abs/2310.00902
tags:
- data
- influence
- datainf
- function
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes DataInf, a computationally efficient method
  to approximate influence functions in large-scale models like LLMs and diffusion
  models. The key idea is to use a closed-form expression that approximates the inverse
  Hessian-vector product, avoiding iterative methods.
---

# DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models

## Quick Facts
- **arXiv ID**: 2310.00902
- **Source URL**: https://arxiv.org/abs/2310.00902
- **Reference count**: 40
- **Primary result**: DataInf achieves 0.775 AUC vs 0.750 for exact method on GLUE-SST2 with rank 4 LoRA, while being 1149.6× faster

## Executive Summary
DataInf introduces a computationally efficient method to approximate influence functions in large-scale models like LLMs and diffusion models, particularly those fine-tuned with LoRA. The key innovation is a closed-form expression that approximates the inverse Hessian-vector product without iterative methods, leveraging the Sherman-Morrison formula. This enables influence computation that is orders of magnitude faster than existing methods while maintaining comparable or better accuracy in identifying mislabeled data and influential training examples.

## Method Summary
DataInf approximates influence functions by replacing the expensive inverse Hessian-vector product with a closed-form per-sample computation. For each layer, it uses the identity `(∇θl ℓi ∇θl ℓiT + λl Idl)^-1 = (1/λl)(Idl - ∇θl ℓi ∇θl ℓiT/(λl + ∇θl ℓiT ∇θl ℓi))` to compute per-sample terms efficiently. The method requires only O(n dl) operations and O(dl) memory per layer, avoiding the need to store large Hessian matrices or run iterative solvers like LiSSA. The approach is particularly effective for LoRA-tuned models where per-layer parameter counts are small.

## Key Results
- Achieves 0.775 AUC vs 0.750 for exact method on GLUE-SST2 with rank 4 LoRA
- 1149.6× faster computation than exact influence methods
- Orders of magnitude speed improvement over LiSSA and Hessian-free methods
- Effective across text generation (Llama-2-13B-chat) and text-to-image generation (stable-diffusion-v1.5) tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The inverse Hessian-vector product can be approximated by element-wise inverses of rank-one plus diagonal matrices.
- Mechanism: DataInf uses the identity `(∇θl ℓi ∇θl ℓiT + λl Idl)^-1 = (1/λl)(Idl - ∇θl ℓi ∇θl ℓiT/(λl + ∇θl ℓiT ∇θl ℓi))` from the Sherman-Morrison formula, allowing a closed-form per-sample approximation.
- Core assumption: The empirical covariance of gradients across samples is close to the average of per-sample rank-one plus diagonal matrices, so their inverses commute in expectation.
- Evidence anchors:
  - [abstract] "Leveraging an easy-to-compute closed-form expression, DataInf outperforms existing influence computation algorithms in terms of computational and memory efficiency."
  - [section] "the left-hand side of (4) can be approximated with a closed-form expression" (Equation 5)
- Break condition: If the gradient variance is large relative to λl, the element-wise approximation deviates substantially from the true matrix inverse.

### Mechanism 2
- Claim: Computational savings come from avoiding iterative methods and storing large Hessian matrices.
- Mechanism: DataInf replaces each `(Gl(θ∗) + λl Idl)^-1 vl` with a per-sample term `(nλl)^-1 (vl - (vT_l ∇θl ℓi)∇θl ℓi / (λl + ∇θl ℓiT ∇θl ℓi))`, requiring only O(n dl) operations and O(dl) memory per layer.
- Core assumption: The number of parameters per layer `dl` is much smaller than the total parameter count, so the closed-form per-sample computation is tractable.
- Evidence anchors:
  - [abstract] "orders of magnitude faster than existing methods"
  - [section] "DataInf can be computed in O(PL l=1 n dl) operations with O(maxl∈[L] dl) memory"
- Break condition: If `dl` grows to be comparable to n, the per-sample memory requirement becomes prohibitive.

### Mechanism 3
- Claim: Approximation error scales with the square of the layer width `dl`.
- Mechanism: Theorem 1 bounds the spectral norm of the difference between the true inverse Hessian average and the average of per-sample inverses by O(d²l) when gradients and λl are bounded.
- Core assumption: Gradient norms are bounded and λl is chosen appropriately so that the damping term dominates over gradient variations.
- Evidence anchors:
  - [section] "Our approximation error analysis suggests that DataInf is especially effective when it is applied to parameter-efficient fine-tuned models"
  - [section] "the approximation error becomes more tolerable as dl is small"
- Break condition: If gradients are unbounded or λl is too small, the O(d²l) bound becomes loose and the approximation error grows.

## Foundational Learning

- Concept: Sherman-Morrison formula for rank-one updates
  - Why needed here: Enables the closed-form expression for `(∇θl ℓi ∇θl ℓiT + λl Idl)^-1`
  - Quick check question: For a rank-one update `A + uvT`, what is the inverse in terms of `A^-1`, `u`, and `v`?

- Concept: Influence functions and the inverse Hessian
  - Why needed here: The method directly approximates `I(xk, yk) ∝ -∑l vl^T (Gl(θ∗) + λl Idl)^-1 ∇θl ℓk`
  - Quick check question: What does a large magnitude of the influence function indicate about a training example?

- Concept: Parameter-efficient fine-tuning (LoRA) structure
  - Why needed here: DataInf's error analysis relies on small `dl` per layer, which is typical for LoRA
  - Quick check question: In LoRA, how are the low-rank matrices inserted into the model architecture?

## Architecture Onboarding

- Component map: DataInf = per-layer closed-form inverse Hessian-vector approximation + per-sample gradient aggregation → influence score
- Critical path: For each layer l: compute λl → compute per-sample normalization constants → aggregate → multiply by validation gradient → sum over layers
- Design tradeoffs: Closed-form speed vs. accuracy vs. LoRA-specific applicability
- Failure signatures: Unstable influence values when gradient norms are large or λl too small; runtime blow-up if dl ~ n
- First 3 experiments:
  1. Verify that `∇θl ℓiT ∇θl ℓi` is bounded across samples for a simple LoRA layer
  2. Compare DataInf influence scores against exact Hessian-vector product on a small toy dataset
  3. Measure runtime/memory scaling of DataInf as a function of dl and n on a small LoRA-tuned model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the approximation error of DataInf scale with the size of the LoRA matrix rank r in practice, and what is the optimal rank for balancing accuracy and computational efficiency?
- Basis in paper: [explicit] The paper discusses that the approximation error of DataInf increases with the parameter size, which is consistent with the theoretical analysis showing that the spectral norm scales as O(d^2_l). The experiments in Figure 1 demonstrate that the correlation coefficient generally decreases as the rank r increases.
- Why unresolved: While the paper provides theoretical bounds and empirical observations on the relationship between rank and approximation error, it does not provide a systematic analysis of the optimal rank for different tasks or a detailed study of how the error scales across a wider range of ranks.
- What evidence would resolve it: A comprehensive study varying the rank r across a wider range of values and evaluating the approximation error and computational efficiency for different tasks would provide insights into the optimal rank for balancing accuracy and efficiency.

### Open Question 2
- Question: How does DataInf perform in identifying influential data points in more complex and diverse generative AI tasks, such as those involving multiple modalities or more intricate transformations?
- Basis in paper: [inferred] The paper demonstrates DataInf's effectiveness in identifying influential data points in text generation and text-to-image generation tasks, but these tasks are relatively simple. The paper does not explore more complex scenarios involving multiple modalities or intricate transformations.
- Why unresolved: The current evaluation focuses on basic tasks, and there is no evidence of DataInf's performance in more challenging scenarios that are common in real-world applications of generative AI.
- What evidence would resolve it: Testing DataInf on tasks involving multiple modalities (e.g., text-to-video generation) or more intricate transformations (e.g., style transfer with complex constraints) would provide insights into its applicability and limitations in diverse generative AI tasks.

### Open Question 3
- Question: What is the impact of different damping parameters λ_l on the accuracy and stability of DataInf, and how can we optimize the choice of λ_l for different tasks and models?
- Basis in paper: [explicit] The paper mentions that the damping parameter λ_l is set to 0.1 × (nd_l)^(-1) Σ_i ∇θ_l ℓ_i^T ∇θ_l ℓ_i following the literature, but it does not explore the impact of different choices of λ_l on the performance of DataInf.
- Why unresolved: The choice of λ_l can significantly affect the accuracy and stability of the influence computation, and the paper does not provide a systematic analysis of how different values of λ_l impact the results.
- What evidence would resolve it: A study varying the damping parameter λ_l across different tasks and models, and evaluating the impact on the accuracy and stability of DataInf, would provide insights into the optimal choice of λ_l for different scenarios.

## Limitations

- Reliance on element-wise inverse approximation of rank-one plus diagonal matrices assumes gradient variance is small relative to damping parameter
- Specifically designed for LoRA-tuned models with small per-layer parameter counts, limiting broader applicability
- Approximation error bound of O(d²l) may become loose when gradients are unbounded or damping is insufficient

## Confidence

**High confidence**: The computational efficiency claims are well-supported by the O(n dl) complexity analysis and the explicit comparison showing DataInf is 1149.6x faster than exact methods on GLUE-SST2 with rank 4 LoRA. The theoretical approximation error bound of O(d²l) in Theorem 1 provides rigorous justification for the method's effectiveness on LoRA-tuned models.

**Medium confidence**: The empirical performance claims (AUC scores of 0.775 vs 0.750) are supported by the provided results, but depend on the specific experimental setup including synthetic mislabeled data generation and LoRA rank selection. The claim that DataInf is "especially effective" for parameter-efficient fine-tuned models follows logically from the error analysis but requires broader empirical validation across different model architectures.

**Low confidence**: The assertion that gradients and λl are "well-behaved" in LoRA-tuned models is assumed rather than empirically verified across diverse fine-tuning scenarios. The approximation error bound assumes bounded gradient norms, which may not hold in practice for all datasets and model configurations.

## Next Checks

1. **Gradient norm analysis**: Systematically measure ∇θl ℓiT ∇θl ℓi across all samples and layers for various LoRA configurations to verify the boundedness assumption underlying the approximation error bound.

2. **Scaling behavior validation**: Empirically measure DataInf's approximation error and runtime as functions of dl and n on a controlled toy LoRA-tuned model to verify the O(d²l) error scaling and O(n dl) complexity claims.

3. **Cross-architecture generalization**: Apply DataInf to a non-LoRA parameter-efficient fine-tuning method (such as prefix tuning or adapter-based tuning) with similarly small per-layer parameter counts to test the broader applicability of the approximation beyond the specific LoRA structure.