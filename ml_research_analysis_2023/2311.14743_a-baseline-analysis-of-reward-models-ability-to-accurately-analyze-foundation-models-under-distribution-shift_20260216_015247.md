---
ver: rpa2
title: A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation
  Models Under Distribution Shift
arxiv_id: '2311.14743'
source_url: https://arxiv.org/abs/2311.14743
tags:
- reward
- responses
- distribution
- prompts
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how reward models, used in reinforcement learning
  with human feedback (RLHF) for aligning large language models (LLMs), perform under
  distribution shifts. The study measures accuracy and calibration of reward models
  when faced with out-of-distribution (OOD) prompts and responses.
---

# A Baseline Analysis of Reward Models' Ability To Accurately Analyze Foundation Models Under Distribution Shift

## Quick Facts
- arXiv ID: 2311.14743
- Source URL: https://arxiv.org/abs/2311.14743
- Reference count: 8
- Primary result: Reward models' accuracy degrades under distribution shifts, with higher drops due to OOD responses than OOD prompts.

## Executive Summary
This paper investigates how reward models, used in reinforcement learning with human feedback (RLHF) for aligning large language models (LLMs), perform under distribution shifts. The study measures accuracy and calibration of reward models when faced with out-of-distribution (OOD) prompts and responses. Key findings include: accuracy degrades under distribution shifts, with higher drops due to OOD responses than OOD prompts; calibration remains relatively unaffected by OOD prompts, but shows a novel pattern with OOD responses—poor calibration near-OOD due to overconfidence, but excellent calibration far-OOD. The paper also introduces an OOD detection technique using energy scores adapted from classification, which effectively identifies shifts in both prompts and responses, with better detection for responses.

## Method Summary
The study evaluates OpenAssistant's deberta-v3-large-v2 reward model on the Summarize From Feedback dataset. Distribution shifts are artificially induced via word perturbations (insertion, deletion, replacement) at varying probabilities (0%, 25%, 50%, 75%). The reward model's accuracy and Expected Calibration Error (ECE) are measured under these shifts. An energy score adaptation from classification is used for OOD detection, with performance evaluated using AUROC and FPR@95 metrics.

## Key Results
- Reward model accuracy degrades under distribution shifts, with higher drops due to OOD responses than OOD prompts
- Calibration remains relatively unaffected by OOD prompts, but shows a novel pattern with OOD responses—poor calibration near-OOD due to overconfidence, but excellent calibration far-OOD
- The adapted energy score effectively detects distribution shifts in both prompts and responses, with better detection for responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward model accuracy degrades under distribution shift, with higher drops due to OOD responses than OOD prompts.
- Mechanism: The reward model is trained on a specific prompt-response distribution, so when either the prompt or response shifts away from that distribution, the model's ability to correctly rank the two responses decreases. Since responses carry the actual semantic content being evaluated, they are more sensitive to shifts than prompts.
- Core assumption: The training data distribution is representative of the in-distribution (ID) prompts and responses; OOD samples are semantically or stylistically different enough to affect the reward model's judgment.
- Evidence anchors:
  - [abstract]: "accuracy degrades under distribution shifts, with higher drops due to OOD responses than OOD prompts"
  - [section]: "We show that reward models’ accuracy strictly degrades under distribution shifts in prompts and responses, with higher magnitude drops due to OOD responses."
- Break condition: If the reward model is trained on a very broad and diverse dataset, the distinction between ID and OOD may be less pronounced, reducing the accuracy drop.

### Mechanism 2
- Claim: Calibration behavior changes differently for OOD prompts vs. OOD responses; specifically, calibration is relatively unaffected by OOD prompts but follows a novel pattern with OOD responses.
- Mechanism: Calibration reflects the alignment between confidence and accuracy. OOD prompts may not significantly alter the model's internal confidence calibration because the model's confidence function is primarily based on the response logits. However, OOD responses directly influence the reward model's logits, leading to a novel calibration pattern where near-OOD responses cause overconfidence (poor calibration) and far-OOD responses cause appropriate underconfidence (excellent calibration).
- Core assumption: The reward model's confidence is primarily determined by the response logits rather than the prompt, and the model's calibration mechanism is sensitive to the degree of OOD in responses.
- Evidence anchors:
  - [abstract]: "calibration remains relatively unaffected by OOD prompts, but shows a novel pattern with OOD responses—poor calibration near-OOD due to overconfidence, but excellent calibration far-OOD"
  - [section]: "calibration due to OOD prompts is relatively unaffected by distribution shift; while reward models’ calibration due to OOD responses follows a novel paradigm..."
- Break condition: If the reward model is trained with explicit calibration objectives or uses a different confidence metric, the observed calibration patterns may not hold.

### Mechanism 3
- Claim: The adapted energy score from classification can effectively detect distribution shifts in both prompts and responses, with better detection for responses.
- Mechanism: The energy score measures the similarity of an inference example to the training set by using the reward model's logits. Since responses are more sensitive to distribution shifts (as per Mechanism 1), the energy score naturally becomes more effective at detecting shifts in responses. The score is calculated as the negative log of the sum of exponentials of the logits, which reflects the model's confidence in the ranking of the two responses.
- Core assumption: The reward model's logits encode meaningful information about the similarity of the input to the training distribution, and the energy score is a valid metric for detecting OOD in this context.
- Evidence anchors:
  - [abstract]: "adapts an OOD detection technique using energy scores adapted from classification, which effectively identifies shifts in both prompts and responses, with better detection for responses."
  - [section]: "We adapt an OOD detection technique commonly used in classification to the reward model setting to detect these distribution shifts in prompts and responses."
- Break condition: If the reward model's logits are not well-calibrated or do not encode distributional information, the energy score may not be an effective OOD detector.

## Foundational Learning

- Concept: Reinforcement Learning with Human Feedback (RLHF)
  - Why needed here: Understanding RLHF is crucial because the paper analyzes reward models, which are a key component of RLHF used to align LLMs with human preferences.
  - Quick check question: What is the role of the reward model in RLHF, and how does it differ from the foundational LLM?

- Concept: Expected Calibration Error (ECE)
  - Why needed here: ECE is used to measure the alignment between confidence and accuracy, which is a key aspect of the paper's analysis of reward model performance under distribution shift.
  - Quick check question: How is ECE calculated, and what does a high ECE indicate about a model's calibration?

- Concept: Distribution Shift
  - Why needed here: The paper's central focus is on how reward models perform under distribution shift, so understanding what distribution shift is and its effects on model performance is essential.
  - Quick check question: What are some common causes of distribution shift in real-world applications, and how can they affect model performance?

## Architecture Onboarding

- Component map:
  OpenAssistant's deberta-v3-large-v2 reward model -> Summarize From Feedback dataset -> Word perturbations -> Accuracy, ECE, Energy Score -> AUROC, FPR@95

- Critical path:
  1. Load the reward model and dataset.
  2. Induce distribution shift in prompts and responses.
  3. Evaluate reward model accuracy and calibration under different degrees of shift.
  4. Apply energy score for OOD detection and measure its performance.

- Design tradeoffs:
  - Using word perturbations to induce distribution shift is simple but may not capture all types of real-world distribution shifts.
  - The energy score is a simple adaptation from classification but may not be optimal for reward models; more complex OOD detection methods could be explored.

- Failure signatures:
  - If accuracy does not degrade under distribution shift, it may indicate that the reward model is not sensitive to the induced shifts or that the dataset is too diverse.
  - If calibration remains perfect under all shifts, it may suggest that the confidence metric is not well-calibrated or that the shifts are not significant enough.

- First 3 experiments:
  1. Evaluate the reward model's accuracy and calibration on the original (ID) dataset to establish baselines.
  2. Induce distribution shift in prompts only and measure the changes in accuracy, calibration, and OOD detection performance.
  3. Induce distribution shift in responses only and measure the changes in accuracy, calibration, and OOD detection performance, comparing the results to the prompts-only shift.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do reward models behave under different types of distribution shifts beyond word perturbations, such as style or subject shifts?
- Basis in paper: [inferred] The paper states that future work will explore if the same findings hold for different models and distribution shifts other than word perturbations.
- Why unresolved: The current study only uses word perturbations to induce distribution shifts, which may not represent all possible distribution shifts.
- What evidence would resolve it: Conducting experiments with various types of distribution shifts, such as style or subject shifts, and comparing the results with those obtained from word perturbations.

### Open Question 2
- Question: Are there other OOD detection techniques that could outperform the adapted Energy Score for reward models?
- Basis in paper: [explicit] The paper mentions that there exist many other OOD detection methods but chose Energy Score due to its effectiveness and convenience for reward models.
- Why unresolved: The study only adapted and tested the Energy Score for OOD detection in reward models, leaving other potential techniques unexplored.
- What evidence would resolve it: Comparing the performance of various OOD detection techniques, including those not mentioned in the paper, on reward models under different distribution shifts.

### Open Question 3
- Question: How do different reward model architectures affect their performance and calibration under distribution shifts?
- Basis in paper: [explicit] The paper uses OpenAssistant's deberta-v3-large-v2 as the reward model, but future work will explore if the same findings hold for different models.
- Why unresolved: The study only tested one specific reward model architecture, and it is unclear if the results are generalizable to other architectures.
- What evidence would resolve it: Testing various reward model architectures under distribution shifts and comparing their performance and calibration to determine if the findings are consistent across architectures.

## Limitations
- The analysis is constrained by synthetic distribution shifts via word perturbations, which may not fully capture real-world shifts
- The energy score adaptation from classification to reward models may not generalize to all reward model architectures or training paradigms
- The study focuses on a single reward model, limiting generalizability to other reward models or foundational LLMs

## Confidence
- **High Confidence**: The observation that reward model accuracy degrades under distribution shifts, with higher drops due to OOD responses than OOD prompts
- **Medium Confidence**: The novel calibration pattern observed with OOD responses (poor calibration near-OOD due to overconfidence, but excellent calibration far-OOD)
- **Low Confidence**: The effectiveness of the energy score for OOD detection in reward models, while demonstrated in this study, may not hold for more complex or nuanced distribution shifts

## Next Checks
1. Validate calibration patterns: Test the observed calibration patterns (poor near-OOD, excellent far-OOD) on additional reward models trained on different datasets to assess generalizability
2. Real-world distribution shifts: Evaluate the reward model's performance under more realistic distribution shifts, such as changes in user demographics, writing styles, or task complexity
3. Energy Score generalization: Apply the energy score adaptation to other reward model architectures and training paradigms to assess its effectiveness as a general OOD detection technique