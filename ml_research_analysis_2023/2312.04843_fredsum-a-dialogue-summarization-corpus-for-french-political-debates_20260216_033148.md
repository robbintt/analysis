---
ver: rpa2
title: 'FREDSum: A Dialogue Summarization Corpus for French Political Debates'
arxiv_id: '2312.04843'
source_url: https://arxiv.org/abs/2312.04843
tags:
- abstractive
- summarization
- summary
- summaries
- extractive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FREDSum is a dataset of French political debates designed for dialogue
  summarization. It contains manually transcribed and annotated debates, including
  extractive and abstractive summaries, as well as abstractive communities.
---

# FREDSum: A Dialogue Summarization Corpus for French Political Debates
## Quick Facts
- arXiv ID: 2312.04843
- Source URL: https://arxiv.org/abs/2312.04843
- Authors: Not specified in source
- Reference count: 27
- One-line primary result: FREDSum is the first large-scale French multi-party summarization resource, with ChatGPT outperforming other models on abstractive summarization.

## Executive Summary
FREDSum is a novel dataset of French political debates designed for dialogue summarization. The corpus contains manually transcribed and annotated debates, featuring both extractive and abstractive summaries, as well as abstractive communities. Baseline experiments using state-of-the-art methods (BARThez, Open Assistant, ChatGPT) demonstrate ChatGPT's superior performance in abstractive summarization, while BERTExtSum excels in extractive summarization. The study emphasizes the critical role of high-quality transcription and annotations in training accurate dialogue summarization models, and highlights the need for multilingual resources to support non-English languages.

## Method Summary
The FREDSum dataset was created by manually transcribing French political debates and annotating them with extractive and abstractive summaries, as well as abstractive communities. The debates were segmented by topic due to their length, and three different abstractive summary styles were employed. Baseline models including BARThez, Open Assistant, and ChatGPT were evaluated using ROUGE and BERTScore metrics, with human evaluation supplementing the automated assessments.

## Key Results
- ChatGPT outperforms BARThez and Open Assistant in abstractive summarization tasks
- BERTExtSum generally outperforms other baselines in extractive summarization
- Human evaluation confirms extractive summaries score lower on readability (2.6-2.6) compared to abstractive summaries (2.97-4.32)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality transcription and annotations are critical for training accurate dialogue summarization models.
- Mechanism: Manual transcription reduces noise from spontaneous speech (disfluencies, hesitations), while structured annotations (extractive/abstractive summaries, abstractive communities) provide clear supervision signals for both extractive and abstractive models.
- Core assumption: The quality of the input data directly determines the quality of the learned summarization model.
- Evidence anchors:
  - [abstract] "We highlight the importance of high quality transcription and annotations for training accurate and effective dialogue summarization models"
  - [section 2.1] "Due to the length of political debates, which can go on for mutiple hours... we decided to segment each debate by topic"
  - [corpus] Evidence is strong - the paper explicitly states debates were manually transcribed and annotated by native speakers
- Break condition: If transcription quality degrades (automated transcription with errors), or if annotations become inconsistent, model performance will drop significantly.

### Mechanism 2
- Claim: Abstractive summaries outperform extractive summaries for dialogue summarization.
- Mechanism: Spontaneous speech contains repetitions, disfluencies, and context-dependent references that extractive methods struggle with. Abstractive summarization can rephrase and condense information more naturally.
- Core assumption: Human preference for abstractive summaries in dialogue contexts reflects better information density and readability.
- Evidence anchors:
  - [section 1] "due to the particular nature of dialogical interactions... dialogue summarization remains an open task"
  - [section 2.2.1] "humans tend to prefer abstractive summarization for conversation (Murray et al., 2010)"
  - [corpus] Human evaluation shows extractive summaries score lower on readability (2.6-2.6) compared to abstractive (2.97-4.32)
- Break condition: If the dialogue becomes highly structured and information-dense (like legal proceedings), extractive methods might perform comparably.

### Mechanism 3
- Claim: Multilingual resources are essential for advancing dialogue summarization beyond English.
- Mechanism: Without non-English datasets, models cannot learn language-specific discourse patterns, cultural context, or political terminology unique to each language.
- Core assumption: Language-specific discourse structures and political terminology require language-specific training data.
- Evidence anchors:
  - [abstract] "emphasize the need for multilingual resources to support dialogue summarization in non-English languages"
  - [section 1] "there are currently no datasets available for multiparty conversation in other non-English languages"
  - [corpus] This is the first large-scale French multi-party summarization resource, explicitly filling this gap
- Break condition: If transfer learning techniques become sufficiently advanced to handle cross-lingual summarization without native data.

## Foundational Learning

- Concept: Encoder-decoder architectures
  - Why needed here: These architectures (like BART, ChatGPT) are the foundation for both extractive and abstractive summarization tasks
  - Quick check question: What's the key difference between encoder-only (BERT) and encoder-decoder (BART) architectures for summarization?

- Concept: ROUGE and BERTScore metrics
  - Why needed here: These are the evaluation metrics used to compare model performance against human-annotated summaries
  - Quick check question: Why does BERTScore complement ROUGE by considering semantic similarity rather than just lexical overlap?

- Concept: Prompt engineering for LLMs
  - Why needed here: The paper shows different results when using minimal prompt engineering vs. structured prompts
  - Quick check question: How might more sophisticated prompt engineering improve ChatGPT's performance on this dataset?

## Architecture Onboarding

- Component map: Data pipeline → Manual transcription → Annotation (extractive/abstractive summaries, abstractive communities) → Model training (BARThez, Open Assistant, ChatGPT) → Evaluation (ROUGE, BERTScore, human evaluation)
- Critical path: Quality transcription → Structured annotations → Model training → Comprehensive evaluation
- Design tradeoffs: Manual annotation provides high quality but limits dataset size; automated methods would increase scale but reduce quality
- Failure signatures: Poor ROUGE/BERTScore correlation with human evaluation indicates model hallucinations or poor discourse understanding
- First 3 experiments:
  1. Train BARThez on FREDSum with standard hyperparameters and evaluate against human annotations
  2. Fine-tune Open Assistant on FREDSum with prompt engineering optimization
  3. Compare ChatGPT performance with and without debate segmentation for long inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of FREDSum compare to other existing multilingual dialogue summarization datasets like ELITR and VCSum?
- Basis in paper: [explicit] The paper explicitly compares FREDSum to ELITR and VCSum, but does not provide a detailed analysis of how FREDSum's quality compares to these datasets in terms of factors like transcription accuracy, annotation quality, and overall data size.
- Why unresolved: While the paper mentions that FREDSum is the first large-scale French multi-party summarization resource, it does not provide a comprehensive evaluation of how it stacks up against other existing multilingual dialogue summarization datasets in terms of quality metrics.
- What evidence would resolve it: A detailed comparison of FREDSum's quality metrics (e.g., transcription accuracy, annotation quality, data size) against those of ELITR and VCSum would help determine how it compares to other existing multilingual dialogue summarization datasets.

### Open Question 2
- Question: How do the different abstractive summary styles in FREDSum (avoiding coreference, based on extractive summary, and as natural documents) impact the performance of abstractive summarization models?
- Basis in paper: [explicit] The paper mentions that the annotators created three different abstractive summary styles, but it does not provide an analysis of how these different styles impact the performance of abstractive summarization models.
- Why unresolved: While the paper provides baseline experiments using state-of-the-art methods, it does not specifically analyze how the different abstractive summary styles in FREDSum impact the performance of these models.
- What evidence would resolve it: An analysis of how the different abstractive summary styles in FREDSum impact the performance of abstractive summarization models would help determine which style is most effective for training and evaluating these models.

### Open Question 3
- Question: How does the use of different text embedding methods (e.g., TF-IDF, Word2Vec, multilingual Sentence-BERT, French BERT) impact the performance of abstractive community detection in FREDSum?
- Basis in paper: [explicit] The paper mentions that different text embedding methods were used for abstractive community detection, but it does not provide a detailed analysis of how these different methods impact the performance of the task.
- Why unresolved: While the paper provides baseline results for abstractive community detection using different text embedding methods, it does not specifically analyze how these methods impact the performance of the task.
- What evidence would resolve it: A detailed analysis of how different text embedding methods impact the performance of abstractive community detection in FREDSum would help determine which method is most effective for this task.

## Limitations

- Limited dataset size with approximately 11 debates may restrict model generalization across diverse political discourse styles
- Manual transcription process ensures quality but creates scalability challenges for future expansion
- Heavy reliance on automated metrics (ROUGE, BERTScore) may not fully capture nuances of political discourse summarization

## Confidence

- **High Confidence**: The dataset creation methodology and the importance of high-quality transcription/annotations - well-supported by the corpus evidence and standard NLP practices.
- **Medium Confidence**: ChatGPT's superior performance over BARThez and Open Assistant - while results are presented, the comparison uses minimal prompt engineering which may underrepresent model capabilities.
- **Medium Confidence**: The claim about extractive summaries scoring lower on readability - supported by human evaluation but limited to a small sample size (12 summaries).

## Next Checks

1. Expand dataset evaluation: Test model performance across different political topics and speaker configurations within FREDSum to assess generalizability beyond the current limited sample.

2. Bias analysis: Conduct a systematic analysis of political bias representation in both the source debates and model-generated summaries to understand potential ideological skews.

3. Prompt engineering impact: Implement more sophisticated prompt engineering strategies for ChatGPT and Open Assistant to determine if the performance gap with BARThez persists under optimized conditions.