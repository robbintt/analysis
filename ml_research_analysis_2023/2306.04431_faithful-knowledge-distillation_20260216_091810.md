---
ver: rpa2
title: Faithful Knowledge Distillation
arxiv_id: '2306.04431'
source_url: https://arxiv.org/abs/2306.04431
tags:
- teacher
- student
- distillation
- networks
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles a subtle but critical issue in knowledge distillation:
  ensuring that a student network not only learns from its teacher but also faithfully
  reproduces the teacher''s confidence levels in a verifiable way, especially in safety-critical
  applications. The authors formalize this as the concept of a "faithful imitator,"
  where the student''s output confidences stay within a bounded difference of the
  teacher''s, even under small adversarial perturbations.'
---

# Faithful Knowledge Distillation

## Quick Facts
- arXiv ID: 2306.04431
- Source URL: https://arxiv.org/abs/2306.04431
- Reference count: 10
- One-line primary result: Faithful distillation produces student networks with tighter verified bounds on relative calibration to their teacher under adversarial perturbations compared to standard and other adversarial distillation methods.

## Executive Summary
This paper addresses a critical issue in knowledge distillation: ensuring that a student network not only learns from its teacher but also faithfully reproduces the teacher's confidence levels in a verifiable way, especially under adversarial perturbations. The authors formalize this as the concept of a "faithful imitator," where the student's output confidences stay within a bounded difference of the teacher's, even under small adversarial perturbations. They propose empirical (PGD-based) and certified (MILP-based linear relaxations) methods to compute these bounds. To directly train for faithfulness, they introduce "faithful distillation," a modified loss that aligns student and teacher confidences under adversarial perturbations. Experiments on MNIST and Fashion-MNIST show that adversarially trained students (via ARD, RSLAD, or their FD method) achieve higher empirical and certified faithfulness bounds than standard distillation. Their FD method consistently produces the tightest verified bounds, making it a more reliable choice for ensuring relative calibration and safe deployment.

## Method Summary
The paper introduces faithful distillation to address relative calibration in knowledge distillation. The method trains a student network to imitate a robust teacher's confidence outputs under adversarial perturbations. Four distillation methods are compared: Standard Distillation (SD), Adversarial Robust Distillation (ARD), Robust Soft Label Adversarial Distillation (RSLAD), and Faithful Distillation (FD). FD uses a loss that explicitly minimizes the KL divergence between student and teacher confidences under worst-case perturbations. Empirical bounds (EmpLB) are computed using PGD attacks, while certified bounds (FaithUB) use MILP-based linear relaxations. Experiments are conducted on MNIST and Fashion-MNIST with robust teacher networks trained using adversarial training.

## Key Results
- Faithful distillation (FD) produces student networks with lower empirical and certified faithfulness bounds compared to standard distillation and other adversarial distillation methods.
- Adversarially trained students (ARD, RSLAD, FD) are more relatively well-calibrated to their robust teacher than standard distillation students.
- FD consistently achieves the tightest verified faithfulness bounds, demonstrating superior relative calibration and reliability for safe deployment.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Faithful distillation ensures the student network maintains confidence levels similar to the teacher's under adversarial perturbations.
- Mechanism: The faithful distillation loss explicitly minimizes the KL divergence between the student and teacher confidences under the worst-case perturbation within an ℓ∞-ball. This directly optimizes the student to imitate the teacher's confidence outputs under adversarial conditions.
- Core assumption: The teacher network is robust to adversarial attacks, so its confidence outputs remain stable under small perturbations.
- Evidence anchors:
  - [abstract]: "to verifiably align the relative calibration incentives of the student to those of its teacher, we introduce faithful distillation."
  - [section]: "we introduce faithful distillation. Our experiments on the MNIST and Fashion-MNIST datasets demonstrate the need for such an analysis and the advantages of the increased verifiability of faithful distillation over alternative adversarial distillation methods."
  - [corpus]: Weak, as related papers focus on preference-based distillation and teacher calibration but not verified faithfulness bounds.
- Break condition: If the teacher network is not robust, the confidence outputs may not remain stable under perturbations, undermining the effectiveness of faithful distillation.

### Mechanism 2
- Claim: The faithful imitation framework provides a principled way to evaluate the relative calibration of a student network with respect to its teacher.
- Mechanism: The framework defines a faithful imitator as a network whose output confidences are within a bounded difference of the teacher's outputs under perturbations. It provides both empirical (PGD-based) and certified (MILP-based) methods to compute these bounds.
- Core assumption: The relative calibration between teacher and student can be quantified by the maximum difference in their confidence outputs under perturbations.
- Evidence anchors:
  - [abstract]: "To address these questions, we introduce a faithful imitation framework to discuss the relative calibration of confidences, as well as provide empirical and certified methods to evaluate the relative calibration of a student w.r.t. its teacher."
  - [section]: "Within the context of knowledge distillation, we have a student network, fs, that is trying to imitate the output of a teacher network, ft. We can use the definition of a faithful imitator as a principled way of reasoning about the relative calibration of a student network with respect to its teacher in terms of confidences."
  - [corpus]: Weak, as related papers focus on teacher calibration and preference-based distillation but not verified faithfulness bounds.
- Break condition: If the student network fails to achieve low empirical or certified faithfulness bounds, it indicates poor relative calibration to the teacher.

### Mechanism 3
- Claim: Faithful distillation produces students that are more relatively well-calibrated to their teacher than standard distillation or other adversarial distillation methods.
- Mechanism: The experiments demonstrate that students trained with faithful distillation achieve lower empirical lower bounds (EmpLB) and faithfulness upper bounds (FaithUB) on the difference in confidences with their teacher compared to other methods.
- Core assumption: Lower bounds on the difference in confidences indicate better relative calibration between teacher and student.
- Evidence anchors:
  - [abstract]: "Our experiments on the MNIST and Fashion-MNIST datasets demonstrate the need for such an analysis and the advantages of the increased verifiability of faithful distillation over alternative adversarial distillation methods."
  - [section]: "From table 2, we observe that the empirical attack bounds (EmpLB) on both datasets are on average smaller for fFD, fRSLAD and fARD than for fSD. This indicates that the adversarially distilled students are, on average, empirically more relatively well-calibrated than SD students w.r.t. the robust teacher."
  - [corpus]: Weak, as related papers focus on preference-based distillation and teacher calibration but not verified faithfulness bounds.
- Break condition: If the empirical or certified bounds for faithful distillation students are not lower than those of other methods, it indicates that faithful distillation does not produce more relatively well-calibrated students.

## Foundational Learning

- Concept: Adversarial training
  - Why needed here: Adversarial training is used to train the teacher network and to generate perturbations during the distillation process.
  - Quick check question: What is the purpose of using PGD attacks during adversarial training, and how do they help improve the robustness of the network?
- Concept: Knowledge distillation
  - Why needed here: Knowledge distillation is the process of transferring knowledge from a large teacher network to a smaller student network, which is the main focus of this paper.
  - Quick check question: How does the temperature parameter in the distillation loss affect the soft targets used for training the student network?
- Concept: Linear relaxations and MILP solvers
  - Why needed here: Linear relaxations and MILP solvers are used to compute certified faithfulness bounds, providing guarantees on the maximum difference in confidences between teacher and student under perturbations.
  - Quick check question: What are the advantages of using MILP solvers over other bound propagation methods like CROWN for computing faithfulness bounds?

## Architecture Onboarding

- Component map: Teacher network (ft) -> Student network (fs) -> Faithful distillation loss -> Empirical lower bound computation (PGD) -> Certified faithfulness bounds (MILP)
- Critical path:
  1. Train a robust teacher network on a small dataset (e.g., MNIST) using adversarial training.
  2. Initialize the student network with the same architecture as the teacher but with fewer layers or units.
  3. Train the student network using the faithful distillation loss, which includes a maximization step to find worst-case perturbations.
  4. Compute empirical lower bounds on the faithfulness using PGD attacks.
  5. Compute certified faithfulness bounds using linear relaxations and MILP solvers.
- Design tradeoffs:
  - Faithful distillation vs. standard distillation: Faithful distillation explicitly optimizes for relative calibration under perturbations but may be more computationally expensive due to the maximization step.
  - Empirical vs. certified bounds: Empirical bounds provide a lower bound on the faithfulness but no guarantees, while certified bounds provide an upper bound but may be looser for larger networks or perturbations.
  - Computational cost vs. tightness of bounds: Using MILP solvers for certified bounds may be computationally expensive for larger networks, while simpler bound propagation methods like CROWN may provide looser bounds.
- Failure signatures:
  - If the student network fails to achieve low empirical or certified faithfulness bounds, it indicates poor relative calibration to the teacher.
  - If the faithful distillation loss does not converge or the student's performance degrades, it may indicate issues with the optimization or the choice of hyperparameters.
  - If the certified bounds are much looser than the empirical bounds, it may indicate that the linear relaxations are not tight enough for the given network architecture.
- First 3 experiments:
  1. Train a robust teacher network on a small dataset (e.g., MNIST) using adversarial training and evaluate its robustness to PGD attacks.
  2. Train a student network using standard distillation and evaluate its empirical and certified faithfulness bounds compared to the teacher.
  3. Train a student network using faithful distillation and compare its empirical and certified faithfulness bounds to those of the standard distillation student.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the faithful distillation framework and methods be scaled to larger and more complex datasets, such as CIFAR-10 or ImageNet?
- Basis in paper: [explicit] The paper notes that the methods do not scale well to CIFAR-10 and larger datasets, indicating a need for more sophisticated methods to produce faithfulness bounds.
- Why unresolved: The current methods, particularly the MILP-based certified bounds, are computationally expensive and do not scale well with network size and complexity. More efficient or approximate methods may be needed.
- What evidence would resolve it: Experiments on larger datasets with comparable or improved faithfulness bounds, and demonstrations of practical applicability in real-world scenarios.

### Open Question 2
- Question: How do hyperparameters, such as temperature and learning rate, affect the faithfulness of the distilled student networks?
- Basis in paper: [inferred] The paper mentions that hyperparameters were chosen based on preliminary experiments, but a systematic study of their impact on faithfulness is not provided.
- Why unresolved: The impact of hyperparameters on faithfulness is not thoroughly explored, and their optimal values may vary depending on the dataset and network architecture.
- What evidence would resolve it: A comprehensive study varying hyperparameters and measuring their effect on empirical and certified faithfulness bounds across different datasets and architectures.

### Open Question 3
- Question: Can the concept of faithful imitation be extended to other forms of knowledge distillation, such as cross-modal distillation or self-distillation?
- Basis in paper: [explicit] The paper focuses on standard knowledge distillation in a classification setting, but the concept of faithful imitation could potentially be applied to other forms of knowledge transfer.
- Why unresolved: The paper does not explore applications beyond standard classification distillation, and the framework may need to be adapted for different knowledge transfer scenarios.
- What evidence would resolve it: Extensions of the faithful imitation framework to cross-modal or self-distillation settings, with empirical and certified bounds on faithfulness.

## Limitations
- The empirical claims about faithful distillation's superiority are based on limited dataset scope (MNIST, Fashion-MNIST) and specific teacher robustness configurations.
- The certified bounds rely on MILP solvers whose computational scalability for larger networks or more complex datasets remains unclear.
- The faithfulness framework assumes the teacher network is already robust, but the paper doesn't explore what happens when the teacher is not well-robustified.

## Confidence
- **High confidence**: The theoretical framework defining faithful imitators and the mathematical formulation of the faithful distillation loss are well-established and verifiable.
- **Medium confidence**: Empirical results showing faithful distillation achieves lower faithfulness bounds than alternatives are convincing but limited to small-scale experiments.
- **Low confidence**: The generalizability of faithful distillation's advantages to larger, more complex datasets and the tightness of certified bounds for deep networks.

## Next Checks
1. Test faithful distillation on CIFAR-10/100 to evaluate scalability and verify if certified bounds remain tight for larger datasets and deeper networks.
2. Compare MILP-based certified bounds with CROWN or other bound propagation methods to assess computational efficiency vs. bound tightness tradeoffs.
3. Experiment with faithful distillation when the teacher is not adversarially trained to understand the impact on student faithfulness and calibration.