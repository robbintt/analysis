---
ver: rpa2
title: Toward best research practices in AI Psychology
arxiv_id: '2312.01276'
source_url: https://arxiv.org/abs/2312.01276
tags:
- arxiv
- language
- llms
- test
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper discusses methodological considerations for evaluating
  the cognitive capacities of large language models (LLMs) using language-based behavioral
  assessments. It highlights common pitfalls and provides a list of 10 DO'S and DON'TS
  for designing high-quality cognitive evaluations for AI systems.
---

# Toward best research practices in AI Psychology

## Quick Facts
- arXiv ID: 2312.01276
- Source URL: https://arxiv.org/abs/2312.01276
- Reference count: 13
- Primary result: Identifies 10 DO'S and DON'TS for designing high-quality cognitive evaluations for AI systems, addressing prompt sensitivity, cultural biases, and evaluation methodology

## Executive Summary
This paper addresses the methodological challenges in evaluating cognitive capacities of large language models (LLMs) using language-based behavioral assessments. It identifies common pitfalls in current research practices and provides concrete guidance through a set of 10 DO'S and DON'TS for designing rigorous cognitive evaluations. The author emphasizes the importance of careful control condition design, awareness of prompt sensitivity, and consideration of cultural and linguistic diversity in training data.

The paper also highlights four areas of active discussion in the field: prompt sensitivity, cultural and linguistic diversity, using LLMs as research assistants, and running evaluations on open vs. closed LLMs. By providing practical guidance and identifying key research challenges, the paper aims to contribute to the development of best practices in the rapidly growing field of AI Psychology.

## Method Summary
The paper employs a methodological review approach, synthesizing current practices in LLM cognitive evaluation and identifying best practices through analysis of existing research and case studies. The methodology involves examining language-based behavioral assessments originally designed for humans and adapting them for LLM evaluation, with emphasis on identifying shortcuts, designing control conditions, and accounting for training data biases.

The author presents specific case studies demonstrating methodological pitfalls and solutions, including prompt sensitivity issues and cultural bias considerations. The work builds on existing cognitive science evaluation frameworks while addressing the unique challenges posed by LLM architectures and training methodologies.

## Key Results
- Established 10 DO'S and DON'TS for designing high-quality cognitive evaluations for AI systems
- Identified prompt sensitivity as a critical factor affecting LLM performance on cognitive tests
- Highlighted the impact of cultural and linguistic diversity in training data on LLM responses
- Demonstrated through case studies how improper controls can lead to inflated performance estimates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-based behavioral assessments can reliably probe cognitive capacities of LLMs when proper controls are in place
- Mechanism: The paper establishes that by identifying and controlling for shortcuts (like word associations) and implementing rigorous control conditions, researchers can isolate genuine cognitive capabilities from heuristic responses
- Core assumption: LLMs can be evaluated using the same types of tests developed for humans, provided we account for their different processing mechanisms
- Evidence anchors:
  - [abstract] "language-based behavioral assessments" are the primary evaluation method discussed
  - [section 3] "DO consider the shortcuts a model might use to arrive at the correct answer" and "DO design careful control conditions"
  - [corpus] No direct evidence - corpus neighbors focus on AI Psychology but don't provide specific validation of this mechanism
- Break condition: When shortcuts cannot be identified or controlled for, making it impossible to distinguish genuine capability from pattern matching

### Mechanism 2
- Claim: Prompt sensitivity significantly affects LLM performance on cognitive tests
- Mechanism: Small changes in how questions are phrased can lead to dramatically different responses from LLMs, necessitating systematic evaluation of prompt variations
- Core assumption: The way information is presented to an LLM matters as much as the information itself
- Evidence anchors:
  - [section 4.1] "LLMs' performance is sensitive to specific ways in which a query is worded"
  - [case study #3] Demonstrates how different prompting conditions (with vs. without examples) led to drastically different performance outcomes
  - [corpus] Weak evidence - no corpus neighbors directly address prompt sensitivity
- Break condition: When the relationship between prompt structure and model response becomes unpredictable or inconsistent

### Mechanism 3
- Claim: Cultural and linguistic diversity in training data creates systematic biases in LLM responses
- Mechanism: LLMs trained on WEIRD (Western, Educated, Industrialized, Rich, Democratic) data will produce responses that reflect those cultural biases, requiring specific countermeasures
- Core assumption: Training data composition directly determines model behavior patterns
- Evidence anchors:
  - [section 4.2] "LLMs are trained on texts that are overwhelmingly generated by WEIRD individuals"
  - [section 4.2] "Atari et al. go on to show that LLM responses to the World Values Survey primarily reflect values of individuals from Western nations"
  - [corpus] No direct evidence - corpus neighbors don't address cultural bias specifically
- Break condition: When cultural customization fails to adequately mitigate biases or when biases interact in unexpected ways

## Foundational Learning

- Concept: Control condition design
  - Why needed here: To distinguish genuine cognitive capabilities from shortcuts and heuristics that models might use
  - Quick check question: Can you identify at least three types of control conditions mentioned in the paper that would help validate a cognitive assessment?

- Concept: Prompt engineering principles
  - Why needed here: To ensure consistent and reproducible evaluation results across different testing scenarios
  - Quick check question: What is the difference between testing a model's ability to perform a task versus testing whether it performs the task in the same way humans do?

- Concept: Training data contamination assessment
  - Why needed here: To determine whether a model's good performance reflects genuine capability or prior exposure to test materials
  - Quick check question: What are the two most important questions to ask about training data when evaluating a new cognitive test on LLMs?

## Architecture Onboarding

- Component map: Evaluation pipeline includes (1) test item design, (2) prompt formulation, (3) control condition implementation, (4) result analysis, and (5) bias assessment
- Critical path: The most time-sensitive aspect is designing control conditions before running tests, as poor controls can invalidate entire experiments
- Design tradeoffs: Between generalizability (larger datasets) and quality control (smaller, more carefully vetted items)
- Failure signatures: Inconsistent results across prompt variations, performance that correlates with training data overlap, or results that don't generalize across different test formats
- First 3 experiments:
  1. Run the same cognitive test with three different prompt formulations to establish baseline sensitivity
  2. Test with and without control conditions to identify shortcut usage
  3. Test with culturally customized prompts to assess bias mitigation effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design more robust and generalizable tests to evaluate the cognitive capacities of LLMs?
- Basis in paper: [explicit] The paper discusses the need for careful test design to avoid shortcuts and control conditions that may inflate LLM performance.
- Why unresolved: The field is still grappling with the challenge of designing tests that truly assess cognitive abilities rather than relying on statistical patterns or heuristics.
- What evidence would resolve it: Development and validation of new testing paradigms that consistently reveal LLM limitations while avoiding known pitfalls.

### Open Question 2
- Question: How do cultural and linguistic biases in LLM training data affect their performance on cognitive assessments?
- Basis in paper: [explicit] The paper highlights the issue of LLMs being trained on predominantly WEIRD (Western, educated, industrialized, rich, democratic) data, leading to potential biases in their responses.
- Why unresolved: The extent and impact of cultural biases on LLM performance are not fully understood, and methods to mitigate these biases are still being developed.
- What evidence would resolve it: Empirical studies comparing LLM performance across diverse cultural and linguistic contexts, and the effectiveness of targeted interventions to reduce biases.

### Open Question 3
- Question: How can we effectively use LLMs as research assistants without introducing systematic biases?
- Basis in paper: [explicit] The paper discusses the use of LLMs for generating test items and scoring responses, highlighting the potential for biases to be introduced in these processes.
- Why unresolved: The systematic assessment of biases introduced by LLMs as research assistants is still lacking, and methods to validate and mitigate these biases are not well-established.
- What evidence would resolve it: Comparative studies evaluating the reliability and validity of LLM-generated materials and LLM-based scoring against human-generated and human-scored alternatives.

## Limitations
- Limited empirical validation of proposed controls and cultural mitigation strategies across diverse LLM architectures
- Potential reproducibility challenges due to prompt sensitivity that may create inconsistent results
- Reliance on indirect evidence rather than systematic cross-cultural validation studies for cultural bias claims

## Confidence

**High**: The fundamental principle that LLMs can use shortcuts to solve cognitive tasks
**Medium**: The effectiveness of specific control condition designs in practice
**Low**: The long-term stability of cultural bias mitigation strategies as models evolve

## Next Checks

1. **Replication Study**: Conduct a systematic replication of the case studies across multiple LLM architectures to test the generalizability of proposed methods
2. **Prompt Sensitivity Mapping**: Create a comprehensive dataset mapping prompt variations to performance outcomes across different cognitive domains
3. **Cultural Bias Audit**: Perform a controlled study testing LLM responses across multiple cultural contexts to validate proposed mitigation strategies