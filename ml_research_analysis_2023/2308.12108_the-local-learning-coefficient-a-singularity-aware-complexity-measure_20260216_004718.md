---
ver: rpa2
title: 'The Local Learning Coefficient: A Singularity-Aware Complexity Measure'
arxiv_id: '2308.12108'
source_url: https://arxiv.org/abs/2308.12108
tags:
- learning
- local
- singular
- coefficient
- degeneracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Local Learning Coefficient (LLC) is introduced as a novel complexity
  measure for deep neural networks (DNNs) that addresses limitations of traditional
  measures by leveraging Singular Learning Theory (SLT). The LLC captures the true
  nature of degeneracy in singular models, which cannot be fully accounted for by
  Hessian-based measures.
---

# The Local Learning Coefficient: A Singularity-Aware Complexity Measure

## Quick Facts
- arXiv ID: 2308.12108
- Source URL: https://arxiv.org/abs/2308.12108
- Reference count: 40
- One-line primary result: The Local Learning Coefficient (LLC) is a singularity-aware complexity measure for deep neural networks that captures degeneracy beyond what Hessian-based measures can detect

## Executive Summary
The Local Learning Coefficient (LLC) introduces a novel approach to measuring neural network complexity by leveraging Singular Learning Theory (SLT). Unlike traditional measures that rely on counting Hessian eigenvalues, the LLC captures the true nature of degeneracy in singular models through resolution of singularities. This work presents a scalable estimator using stochastic gradient Langevin dynamics (SGLD) and demonstrates its effectiveness across diverse architectures including deep linear networks, ResNet image models, and transformer language models.

## Method Summary
The method introduces an SGLD-based estimator for the Local Learning Coefficient that approximates the intractable integral over parameter neighborhoods using tempered posterior sampling. The approach trains neural networks using standard optimizers (SGD and entropy-SGD), then runs SGLD chains from each parameter point to estimate the local free energy proxy. This proxy is used to compute the LLC, which measures the effective complexity of different parameter regions. The estimator is validated on MNIST using a 2-layer feedforward ReLU network with 1.9M parameters, comparing LLC values between SGD and entropy-SGD optimized networks across 80 random seeds.

## Key Results
- LLC captures degeneracy in DNNs that cannot be accounted for by counting Hessian eigenvalues
- Entropy-SGD finds local minima with lower LLC than SGD, as expected from theoretical predictions
- The SGLD-based estimator successfully recovers theoretical values in low-dimensional models and scales to practical network sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The LLC captures true degeneracy by shifting perspective from Hessian eigenvalues to resolution of singularities
- **Mechanism:** Instead of counting zero Hessian eigenvalues, LLC uses resolution of singularities to express the KL divergence in normal crossing form, where the coefficient λ = min(h_j + 1/(2k_j)) quantifies degeneracy
- **Core assumption:** The local triplet satisfies relative finite variance and w* is a minimum of L(w) in its neighborhood
- **Evidence anchors:**
  - [abstract] "degeneracy in DNN cannot be accounted for by simply counting the number of 'flat' directions"
  - [section] "Resolution of singularities (Hironaka, 1964) gives us the existence of a birational proper map g: M → W_ϵ which monomializes K(w) - K_0"
- **Break condition:** If the model doesn't satisfy relative finite variance or w* isn't a local minimum

### Mechanism 2
- **Claim:** Local free energy F_n(B_γ(w*)) determines posterior concentration in parameter regions
- **Mechanism:** The local free energy combines energy (nLn(w*)) and entropy (λ(w*)) terms. Regions with lower λ(w*) have higher posterior concentration, making λ(w*) a measure of degeneracy
- **Core assumption:** The local triplet (p, q, φ̄) with parameter space B_γ(w*) satisfies the fundamental conditions of SLT
- **Evidence anchors:**
  - [section] "Equation (18) in turns shows that there is a competition between energy, nLn(w*), and entropy, λ(w*)"
- **Break condition:** If the local neighborhood B_γ(w*) contains regions with different L(w) values

### Mechanism 3
- **Claim:** SGLD-based WBIC provides scalable estimation of local learning coefficient
- **Mechanism:** SGLD samples from the tempered local posterior at β* = 1/log n, estimating E_β*[nLn(w)] to approximate F_n(w*, γ). This proxy replaces the intractable integral over B_γ(w*)
- **Core assumption:** The SGLD chain length T is sufficient to explore the posterior and ϵ is small enough to avoid numerical issues
- **Evidence anchors:**
  - [section] "The SGLD updates for sampling (19) are given by ∆w_t = ϵ/2[β*n/m Σ_i ∇log p(y_l_i|x_l_i, w_t) + γ(w* - w_t)] + N(0, ϵ)"
- **Break condition:** If ϵ is too large or T is too short, the SGLD samples won't accurately represent the tempered local posterior

## Foundational Learning

- **Concept: Singular Learning Theory**
  - Why needed here: Provides the theoretical foundation for LLC by recognizing that singularities (degenerate critical points) fundamentally affect learning dynamics
  - Quick check question: Why does the Laplace approximation fail for singular models, necessitating SLT's approach?

- **Concept: Resolution of Singularities**
  - Why needed here: Transforms the KL divergence into normal crossing form, revealing the true structure of degeneracy through the monomial exponents k_j and h_j
  - Quick check question: How does the birational map g: M → W_ϵ enable computation of λ from the normal crossing form?

- **Concept: Bayesian Posterior Concentration**
  - Why needed here: The LLC measures which parameter regions the posterior prefers, based on the energy-entropy competition in the free energy
  - Quick check question: Why does a lower λ(w*) lead to higher posterior concentration in the corresponding parameter region?

## Architecture Onboarding

- **Component map:** SGLD sampler → tempered local posterior → WBIC estimate → λ(w*) calculation → LLC comparison
- **Critical path:** w* → B_γ(w*) neighborhood → tempered local posterior → SGLD sampling → WBIC estimate → λ(w*) calculation → comparison across parameter regions
- **Design tradeoffs:** Accuracy vs. scalability (MCMC gives better exploration but is slower than SGLD), neighborhood size γ vs. prior dominance, chain length T vs. computational budget
- **Failure signatures:** Negative λ(w*) estimates (w* not a local minimum), high variance across runs (insufficient exploration), correlation between λ and prior strength (γ too large)
- **First 3 experiments:**
  1. Verify SGLD sampling on synthetic normal crossing models with known λ values
  2. Compare LLC estimates between SGD and entropy-SGD minima on MNIST
  3. Test sensitivity of LLC to neighborhood scale parameter γ on a simple 2D example

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the local learning coefficient estimator be made more robust to hyperparameter choices like scale parameter γ and SGLD chain length T?
- Basis in paper: [explicit] The paper notes that overly large or small values of γ can cause issues, and that chain length T should ideally be as long as possible but is limited by compute budget
- Why unresolved: The experiments showed sensitivity to these hyperparameters, but did not explore systematic ways to choose optimal values or make the estimator more robust
- What evidence would resolve it: Experiments comparing the estimator's performance across a range of hyperparameter values, or theoretical analysis of how to choose optimal values

### Open Question 2
- Question: How does the local learning coefficient estimator perform on more complex neural network architectures beyond the feedforward ReLU networks tested?
- Basis in paper: [explicit] The experiments focused on feedforward ReLU networks on MNIST. The authors mention transformer language models as another architecture of interest
- Why unresolved: The estimator was only validated on relatively simple architectures. It's unclear how well it generalizes to deeper, more complex models
- What evidence would resolve it: Experiments applying the estimator to transformer models, CNNs on ImageNet, etc. and comparing results to the feedforward network case

### Open Question 3
- Question: Can the local learning coefficient estimator be extended to work with mini-batch estimates of the local singular fluctuation νβ*(w*)?
- Basis in paper: [explicit] The paper notes that the current implementation of the νβ* estimator requires accumulating log-likelihoods over the full dataset, which is not scalable
- Why unresolved: The estimator was only validated for λ, not νβ*. The authors suggest this as a direction for future work but do not explore it
- What evidence would resolve it: Experiments comparing the full-data and mini-batch νβ* estimators, or theoretical analysis of how to adapt the λ estimator to estimate νβ*

## Limitations
- Scalability remains limited to relatively small networks due to computational requirements of SGLD sampling
- The method requires careful tuning of neighborhood scale γ, step size ϵ, and chain length T
- Theoretical assumptions about relative finite variance may not hold for all neural network architectures

## Confidence
- **High Confidence:** The theoretical framework connecting LLC to posterior concentration and the basic SGLD sampling procedure
- **Medium Confidence:** The empirical validation showing entropy-SGD finds lower LLC minima than SGD, and the claim that LLC captures degeneracy beyond Hessian-based measures
- **Low Confidence:** The scalability claims for transformer language models and ResNet architectures, given the limited experimental demonstration

## Next Checks
1. **Scalability Stress Test:** Apply the SGLD estimator to progressively larger architectures (e.g., 3-layer, 4-layer networks) while monitoring runtime and variance of LLC estimates to establish practical limits
2. **Hyperparameter Sensitivity Analysis:** Systematically vary γ, ϵ, and T parameters to quantify their impact on LLC estimates and identify robust default settings
3. **Cross-Architecture Comparison:** Compute LLC for multiple architectures (CNN, transformer, MLP) on the same dataset to verify that the measure captures meaningful differences in effective complexity across fundamentally different network designs