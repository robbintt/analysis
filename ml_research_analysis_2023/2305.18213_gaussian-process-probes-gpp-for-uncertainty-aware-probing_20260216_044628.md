---
ver: rpa2
title: Gaussian Process Probes (GPP) for Uncertainty-Aware Probing
arxiv_id: '2305.18213'
source_url: https://arxiv.org/abs/2305.18213
tags:
- probability
- ground
- truth
- episteme
- judged
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Gaussian Process Probes (GPP) introduce a Bayesian framework for
  probing machine learning models that measures both aleatory uncertainty (fuzziness
  of concepts) and epistemic uncertainty (probe confidence). By using Beta Gaussian
  Processes to model distributions over classifiers, GPP can probe model representations
  with as few as 10 examples while maintaining competitive accuracy (AUROC 0.9 on
  synthetic and real datasets).
---

# Gaussian Process Probes (GPP) for Uncertainty-Aware Probing

## Quick Facts
- arXiv ID: 2305.18213
- Source URL: https://arxiv.org/abs/2305.18213
- Reference count: 40
- Key outcome: GPP achieves AUROC > 0.9 on synthetic datasets and competitive performance on real datasets while providing uncertainty estimates

## Executive Summary
Gaussian Process Probes (GPP) introduce a Bayesian framework for probing machine learning models that measures both aleatory uncertainty (fuzziness of concepts) and epistemic uncertainty (probe confidence). By using Beta Gaussian Processes to model distributions over classifiers, GPP can probe model representations with as few as 10 examples while maintaining competitive accuracy. The method accurately captures fuzzy concepts by aligning judged probabilities with ground truth label probabilities and achieves strong performance in out-of-distribution detection, matching or exceeding classic methods like MSP and Mahalanobis distance.

## Method Summary
GPP models the distribution over binary classifiers using Beta Gaussian Processes, where each classifier is represented as a sample from a stochastic process parameterized by two latent functions, fα and fβ, modeled as independent Gaussian processes. The classifier probability is computed as g(a) = 1 / (1 + exp(-(fα(a) - fβ(a)))). The method uses a cosine kernel in the GP to ensure the prior over classifiers respects the geometry of the augmented representation space, including both activations and bias terms. GPP computes two uncertainty measures: episteme (probe confidence) and alea (concept fuzziness), which enable it to distinguish between fuzzy concepts and out-of-distribution data.

## Key Results
- Achieves AUROC > 0.9 on synthetic datasets with as few as 10 examples
- Competitive performance in out-of-distribution detection matching MSP and Mahalanobis distance
- Accurately captures fuzzy concepts by aligning judged probabilities with ground truth label probabilities

## Why This Works (Mechanism)

### Mechanism 1
GPP models the distribution over binary classifiers using Beta Gaussian Processes, which allows it to capture both the probability of a label and the uncertainty about that probability. The Beta GP framework represents each classifier as a sample from a stochastic process parameterized by two latent functions, fα and fβ, which are modeled as independent Gaussian processes. The classifier probability is computed as g(a) = 1 / (1 + exp(-(fα(a) - fβ(a)))). By maintaining a distribution over these latent functions, GPP can compute epistemic uncertainty (via the entropy of f) and aleatory uncertainty (via the expected entropy of the Bernoulli distribution p(y | g(a))).

### Mechanism 2
The cosine kernel in GPP ensures that the prior over classifiers respects the geometry of the augmented representation space, including both activations and bias terms. The kernel is defined as k(a, a') = v * (a⊤a' + 1) / ((∥a∥ + 1)(∥a'∥ + 1)), where the augmentation by 1 ensures that a bias term is included in the latent function. This normalization guarantees that k(a, a) = v for all a, matching the Beta prior constraint. The kernel effectively defines a distribution over linear classifiers in the augmented space, which aligns with how deep networks construct next-layer neurons during training.

### Mechanism 3
GPP's uncertainty measures (episteme and alea) enable it to distinguish between fuzzy concepts and out-of-distribution (OOD) data, leading to competitive OOD detection performance. Episteme is defined as the negative entropy of the predicted probability distribution, measuring the probe's confidence. Alea is the expected entropy of the label distribution given the predicted probability, measuring the intrinsic fuzziness of the concept. High episteme indicates in-domain data with clear decision boundaries, while low episteme indicates OOD data or ambiguous concepts. By using the negative latent posterior variance as a proxy for episteme, GPP can detect OOD samples even with few observations.

## Foundational Learning

- Concept: Beta Gaussian Processes
  - Why needed here: Beta GPs provide a principled way to model distributions over binary classifiers, which is the core of GPP's probabilistic probing framework.
  - Quick check question: How does a Beta GP differ from a standard GP in terms of the output distribution it models?

- Concept: Epistemic vs Aleatory Uncertainty
  - Why needed here: Distinguishing these two types of uncertainty is crucial for interpreting GPP's predictions and for tasks like OOD detection.
  - Quick check question: In the context of GPP, what does high alea indicate about a concept, and what does high episteme indicate about the probe's confidence?

- Concept: Cosine Similarity and Kernel Functions
  - Why needed here: The cosine kernel in GPP is key to ensuring that the prior over classifiers respects the geometry of the augmented representation space.
  - Quick check question: Why does the cosine kernel in GPP include a normalization factor, and what constraint does this normalization enforce?

## Architecture Onboarding

- Component map: Stimuli -> Basis function ϕ -> Vector representation a -> Beta GP (fα, fβ) -> Posterior distribution -> Prediction g(a) with uncertainties episteme and alea
- Critical path: The critical path for making a prediction with GPP is: (1) compute the vector representation a = ϕ(x) for the input stimulus x; (2) compute the posterior mean and variance of fα and fβ given the observations D; (3) compute the posterior of f = fα - fβ; (4) compute the expected probability E[g(a)] and the uncertainties episteme and alea from the distribution of g(a) = 1 / (1 + exp(-f(a))).
- Design tradeoffs: The choice of the cosine kernel and the Beta prior (controlled by hyperparameters ϵ and s) are key design tradeoffs. The cosine kernel ensures that the prior over classifiers respects the geometry of the augmented representation space, but may not capture all relevant structure. The Beta prior allows for flexible modeling of the distribution over classifiers, but requires careful tuning of ϵ and s to balance the influence of the prior and the observations.
- Failure signatures: If GPP consistently produces extreme predictions (close to 0 or 1) even with few observations, this may indicate that the prior is too informative or that the kernel is not capturing the relevant structure. If GPP produces predictions that do not align with the ground truth fuzziness of concepts, this may indicate that the Beta GP is not well-calibrated or that the observations are not representative of the true concept distribution.
- First 3 experiments:
  1. Validate the uncertainty measures on a synthetic dataset where the ground truth fuzziness of concepts is known. Generate a dataset with binary labels that have varying degrees of label noise, and compare GPP's episteme and alea predictions to the true fuzziness levels.
  2. Compare GPP's OOD detection performance to baseline methods (e.g., MSP, Mahalanobis distance) on a dataset with known in-domain and out-of-domain samples. Vary the number of observations used by GPP and measure the impact on OOD detection AUROC.
  3. Analyze the effect of the kernel choice on GPP's performance. Replace the cosine kernel with a standard RBF kernel and compare the results on the probing and OOD detection tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GPP's performance change when probing concepts with multi-modal distributions rather than binary classifications?
- Basis in paper: [explicit] The paper states "it is straightforward to extend GPP to multi-class classification since we can switch the Beta prior to a Dirichlet prior [Milios et al., 2018]"
- Why unresolved: The paper only validates GPP on binary classification tasks and does not demonstrate or evaluate its performance on multi-class problems.
- What evidence would resolve it: Empirical results showing GPP's performance on multi-class classification tasks with varying numbers of classes and comparing it to existing multi-class probing methods.

### Open Question 2
- Question: What is the theoretical relationship between GPP's episteme measure and established out-of-distribution detection methods like Mahalanobis distance beyond the empirical observations?
- Basis in paper: [explicit] The paper notes that "GPP's variance measures the similarity between function values but Mahalanobis distance measures the similarity between activations" and suggests they are related but different
- Why unresolved: The paper only provides empirical comparisons showing GPP achieves "comparable performance" to Mahalanobis distance but does not establish a formal theoretical connection between the two approaches.
- What evidence would resolve it: A mathematical proof or derivation showing the precise relationship between GPP's episteme measure and Mahalanobis distance, potentially under specific conditions or assumptions.

### Open Question 3
- Question: How does GPP perform when probing representations from models trained with different architectures (e.g., transformers vs. CNNs) or different training objectives?
- Basis in paper: [inferred] The paper validates GPP on CNN models (3D Shapes, ResNet-50, CoCa) but does not test other architectural paradigms or training objectives
- Why unresolved: The paper's experimental results are limited to specific model types, leaving open questions about GPP's generalizability across diverse model architectures and training approaches.
- What evidence would resolve it: Systematic experiments comparing GPP's performance across multiple model architectures (CNNs, transformers, graph neural networks) and training objectives (supervised, self-supervised, contrastive) on the same probing tasks.

## Limitations

- GPP requires access to intermediate model representations (activations), limiting applicability to models with available internal states
- The Beta GP framework assumes the latent functions can be well-approximated by Gaussian processes, which may not hold for all representation spaces
- Performance depends heavily on kernel choice and hyperparameter tuning (ϵ, s), with no clear guidelines for optimal selection across tasks

## Confidence

- High confidence: GPP's ability to achieve AUROC > 0.9 on synthetic datasets and competitive performance on real datasets (confirmed by multiple experiments)
- Medium confidence: Claims about OOD detection performance, as direct comparisons are limited to MSP and Mahalanobis distance only
- Medium confidence: The interpretation of episteme and alea as separate measures of probe confidence and concept fuzziness, though this is theoretically grounded

## Next Checks

1. Test GPP's performance when the number of observations drops below 10, measuring the breakdown point where uncertainty estimates become unreliable
2. Compare GPP against additional OOD detection methods (e.g., energy-based models, deep ensembles) on standard benchmarks like CIFAR-10 vs CIFAR-100
3. Evaluate GPP's robustness to representation drift by measuring performance when the probe is trained on one model architecture but tested on a different architecture with similar functionality