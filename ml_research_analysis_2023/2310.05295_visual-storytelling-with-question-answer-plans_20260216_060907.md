---
ver: rpa2
title: Visual Storytelling with Question-Answer Plans
arxiv_id: '2310.05295'
source_url: https://arxiv.org/abs/2310.05295
tags:
- story
- visual
- blueprint
- image
- stories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a visual storytelling model that generates
  compelling narratives from image sequences by integrating pretrained language models
  with visual representations and planning. The key idea is to translate the image
  sequence into a visual prefix, a sequence of continuous embeddings interpretable
  by language models, and leverage a sequence of question-answer pairs as a blueprint
  plan for selecting salient visual concepts and determining how they should be assembled
  into a narrative.
---

# Visual Storytelling with Question-Answer Plans

## Quick Facts
- arXiv ID: 2310.05295
- Source URL: https://arxiv.org/abs/2310.05295
- Authors: 
- Reference count: 15
- Primary result: VP-BART model generates more coherent, interesting, and natural visual stories than competitive baselines and GPT-3.5 on the VIST benchmark

## Executive Summary
This paper presents a visual storytelling model that generates compelling narratives from image sequences by integrating pretrained language models with visual representations and planning. The key innovation is using a visual prefix - continuous embeddings derived from image features - that enables pretrained language models to process multimodal inputs. The model also leverages a sequence of question-answer pairs as a blueprint plan to guide content selection and narrative assembly, improving story quality through structured planning.

## Method Summary
The approach translates image sequences into a visual prefix using ResNet features mapped through a feed-forward network and concatenated with detected concept embeddings. A blueprint of question-answer pairs guides story generation, with two model variants: top-down (generating blueprint then story in one pass) and iterative (interleaving planning with incremental sentence generation). The model fine-tunes BART-base on the VIST dataset using maximum likelihood training, with automatic blueprint generation from reference stories using spaCy and T5 fine-tuned on SQuAD.

## Key Results
- VP-BART achieves higher BLEU, ROUGE, METEOR, and CIDER scores than competitive baselines
- Human evaluation shows improved coherence, interestingness, and naturalness compared to state-of-the-art systems and GPT-3.5
- Concept grounding precision/recall demonstrates better adherence to visual content
- Lower inter-story trigram repetition and higher MAUVE scores indicate improved diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual prefixes enable pretrained language models to interpret multimodal inputs by mapping image features to token-embedding-compatible spaces
- Mechanism: Image embeddings from ResNet are passed through a feed-forward network to generate visual clues, then concatenated with detected concept embeddings, creating a continuous sequence that the language model can process
- Core assumption: Language models can learn to interpret continuous visual embeddings as meaningful context for story generation
- Evidence anchors:
  - [section]: "Our encoder translates the image sequence into a visual prefix, a sequence of continuous embeddings which language models can interpret."
  - [section]: "We translate the input sequence of images into a sequence of continuous embeddings, aka a visual prefix."

### Mechanism 2
- Claim: Blueprint plans improve story quality by explicitly forcing content selection and ordering decisions before narrative generation
- Mechanism: Question-answer pairs are generated as intermediate plans, which guide the model to first identify salient visual concepts and then structure the narrative around them
- Core assumption: Decomposing story generation into planning and realization stages leads to more coherent and grounded outputs
- Evidence anchors:
  - [abstract]: "Our model translates the image sequence into a visual prefix, a sequence of continuous embeddings which language models can interpret. It also leverages a sequence of question-answer pairs as a blueprint plan for selecting salient visual concepts and determining how they should be assembled into a narrative."
  - [section]: "Our model translates the image sequence into a visual prefix, a sequence of continuous embeddings which language models can interpret. It also leverages a sequence of question-answer pairs as a blueprint plan for selecting salient visual concepts and determining how they should be assembled into a narrative."

### Mechanism 3
- Claim: Iterative planning produces more natural and human-like stories by allowing content selection and narrative elaboration to inform each other in real time
- Mechanism: The model generates one sentence at a time, conditioning on both the visual prefix and the partial blueprint/story generated so far, rather than planning the entire story in advance
- Core assumption: Humans construct narratives incrementally rather than with fully formed global plans
- Evidence anchors:
  - [section]: "At each time step, the iterative model considers not only the image sequence but also context from previous steps, including the blueprint and story generated so far."
  - [section]: "We gradually construct the blueprint and its corresponding story sentence-by-sentence; our planning is informed by generation and vice versa, which we argue should be mutually beneficial."

## Foundational Learning

- Concept: Pretrained language models
  - Why needed here: The approach relies on fine-tuning BART to generate both blueprints and stories, requiring understanding of how these models process input and generate output
  - Quick check question: How does BART's encoder-decoder architecture process a sequence of token embeddings to produce a new sequence?

- Concept: Multimodal representation learning
  - Why needed here: The visual prefix construction requires mapping image features to a space compatible with text embeddings
  - Quick check question: What properties must visual embeddings have to be useful as input to a language model?

- Concept: Planning in text generation
  - Why needed here: The blueprint mechanism is based on planning theory, where intermediate representations guide final output
  - Quick check question: How does the use of question-answer pairs as plans differ from using keyword outlines or event sequences?

## Architecture Onboarding

- Component map: Images → ResNet → feed-forward mapping network → visual clues; Concept detector → concept embeddings; Concatenate visual clues and concepts → visual prefix → BART encoder; BART decoder → blueprint sentences → story sentences (iterative) or both blueprint and story (top-down)
- Critical path: Image features → visual prefix → BART → blueprint → story (top-down); Image features → visual prefix → BART → iterative sentence-by-sentence generation with blueprint (iterative)
- Design tradeoffs: Top-down is simpler and potentially more coherent globally but less controllable; iterative is more flexible and controllable but may accumulate errors; both require careful balance between grounding and creativity
- Failure signatures: High inter-story trigram repetition indicates lack of diversity; low concept grounding precision/recall indicates hallucination or missing content; poor faithfulness scores indicate blueprints are not being followed
- First 3 experiments:
  1. Train VP-BART (no planning) to establish baseline performance and verify visual prefix integration
  2. Train top-down blueprint model to test whether planning improves grounding and coherence
  3. Train iterative blueprint model to evaluate whether incremental planning produces more natural stories

## Open Questions the Paper Calls Out

Based on my understanding of the paper, here are some open research questions:

### Open Question 1
- Question: How can the proposed blueprint-based approach be extended to handle more complex narratives beyond the short stories considered in this work?
- Basis in paper: [inferred] The paper mentions that the authors would like to explore visual storytelling with grounded characters and entities, as well as tackle the generation of more complex narratives like long-form stories
- Why unresolved: The current approach focuses on generating short stories with a fixed structure of 5 sentences. It is unclear how well the blueprint-based planning and iterative generation strategy would scale to longer, more complex narratives with multiple characters, plot arcs, and subplots
- What evidence would resolve it: Experiments evaluating the approach on datasets with longer stories and more complex narrative structures, and comparing against baselines specifically designed for long-form story generation

### Open Question 2
- Question: How can the visual prefix construction be improved to better capture the semantics and relationships between objects, actions, and scenes depicted in the image sequence?
- Basis in paper: [explicit] The paper discusses using a visual prefix, which translates the image sequence into a sequence of continuous embeddings that the pretrained language model can interpret. It mentions using a concept detector to identify specific objects, actions, scenes, and attributes in the images
- Why unresolved: The current visual prefix construction relies on a fixed concept detector trained on a limited set of concepts. It may not capture the full richness and nuance of the visual content, especially for more abstract or complex scenes. Additionally, the relationships between objects and actions are not explicitly modeled
- What evidence would resolve it: Exploring alternative approaches for visual prefix construction, such as using more advanced visual models (e.g., CLIP, Flamingo) or incorporating graph-based representations of the visual content. Evaluating the impact on story quality and grounding

### Open Question 3
- Question: How can the blueprint generation process be made more controllable and customizable to enable personalized storytelling?
- Basis in paper: [explicit] The paper mentions that the blueprint-based approach is controllable, and that blueprints can be made shorter or longer and their details can be refined (e.g., by emphasizing specific entities or characters) to enable human-in-the-loop and personalized storytelling
- Why unresolved: The current approach generates blueprints automatically based on the image sequence and reference stories. It is unclear how users can easily provide high-level guidance or constraints on the blueprint generation process to steer the storytelling in a desired direction (e.g., focusing on specific characters, themes, or plot points)
- What evidence would resolve it: Developing interactive tools or interfaces that allow users to provide input or feedback on the blueprint generation process, and evaluating the impact on user satisfaction and story quality

## Limitations

- Visual prefix construction details are underspecified, particularly the mapping network architecture and training
- Blueprint generation pipeline lacks precise thresholds for filtering invalid QA pairs and exact numbers of pairs per story
- Iterative model dynamics and the exact mechanism of how blueprint and story generation inform each other are not fully detailed

## Confidence

- High Confidence: The core architecture combining visual prefixes with blueprint planning is well-defined and supported by both automatic metrics (improved BLEU, ROUGE, METEOR, CIDER) and human evaluation showing better coherence, interestingness, and naturalness compared to baselines and GPT-3.5
- Medium Confidence: The claim that iterative planning produces more natural stories is supported by human evaluation showing higher naturalness scores, but the automatic metrics don't show clear advantages for the iterative approach over top-down planning
- Low Confidence: The assertion that visual prefixes enable language models to interpret multimodal inputs is theoretically sound but lacks ablation studies isolating the contribution of the visual prefix component versus the planning mechanism

## Next Checks

1. **Ablation Study on Visual Prefix**: Train a version of the model using only detected concepts (without the ResNet-based visual clues) to quantify the contribution of the visual prefix mechanism to overall performance

2. **Blueprint Generation Quality Analysis**: Manually evaluate a sample of automatically generated blueprint QA pairs for relevance, grammaticality, and consistency with source images to assess the quality of the planning component

3. **Diversity Benchmarking**: Compare the model's output diversity (measured by inter-story trigram repetition and MAUVE scores) against the reported state-of-the-art systems to verify the claimed improvement in generation diversity