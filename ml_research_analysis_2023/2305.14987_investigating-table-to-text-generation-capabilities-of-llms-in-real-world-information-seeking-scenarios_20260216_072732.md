---
ver: rpa2
title: Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information
  Seeking Scenarios
arxiv_id: '2305.14987'
source_url: https://arxiv.org/abs/2305.14987
tags:
- table
- llms
- language
- generation
- team
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the table-to-text generation capabilities
  of large language models (LLMs) in real-world information-seeking scenarios. The
  authors explore the performance of GPT models in generating natural language statements
  that are logically entailed by provided tables, comparing them to state-of-the-art
  fine-tuned models on datasets like LogicNLG and FeTaQA.
---

# Investigating Table-to-Text Generation Capabilities of LLMs in Real-World Information Seeking Scenarios

## Quick Facts
- arXiv ID: 2305.14987
- Source URL: https://arxiv.org/abs/2305.14987
- Reference count: 17
- Primary result: GPT-4 significantly outperforms other models in generating coherent and faithful table-to-text statements

## Executive Summary
This paper investigates how large language models (LLMs) perform in generating natural language statements from tables for real-world information-seeking scenarios. The authors compare GPT models against fine-tuned state-of-the-art models across multiple datasets, examining both direct prediction and chain-of-thought prompting approaches. They demonstrate that GPT-4 achieves superior performance in generating factually consistent statements and can effectively evaluate and provide feedback on other models' outputs. The study also introduces new datasets (LoTNLG and F2WTQ) for data insight generation tasks.

## Method Summary
The study employs in-context learning with GPT models (text-davinci-002, text-davinci-003, gpt-3.5-turbo, gpt-4) using both direct prediction and chain-of-thought prompting. Four datasets are used: LogicNLG and FeTaQA for query-based generation, plus two newly-constructed datasets (LoTNLG and F2WTQ) for data insight generation. Tables are linearized for LLM input. The evaluation framework includes automated metrics (SP-Acc, NLI-Acc, TAPAS-Acc, TAPEX-Acc) and human evaluation for fluency (1-5) and faithfulness (0-1). Performance is compared against fine-tuned baselines including GPT2-C2F, T5, R2D2, ReasTAP, PLOG, and LOFT.

## Key Results
- GPT-4 significantly outperforms other models in generating coherent and faithful statements from tables
- LLMs using chain-of-thought prompting can serve as reference-free metrics for table-to-text generation evaluation
- GPT-4 can generate high-fidelity natural language feedback that improves the factual consistency of other models' outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs using chain-of-thought prompting can serve as effective reference-free metrics for table-to-text generation evaluation.
- Mechanism: Chain-of-thought prompting enables LLMs to reason step-by-step through table verification tasks, similar to how they solve table fact-checking problems. This allows them to judge whether generated statements are factually consistent with the source table without requiring reference outputs.
- Core assumption: The logical reasoning capabilities of LLMs trained on diverse web data generalize to structured table verification tasks.
- Evidence anchors:
  - [abstract] "LLMs using chain-of-thought prompting can serve as reference-free metrics for table-to-text generation evaluation."
  - [section] "Recent work (Chen, 2023) has revealed that large language models using chain-of-thought prompting can achieve competitive results on TabFact."
- Break condition: The LLM fails to properly parse or understand the table structure, or the reasoning task requires domain-specific knowledge not present in the training data.

### Mechanism 2
- Claim: LLMs can generate high-fidelity natural language feedback that improves the factual consistency of other models' outputs.
- Mechanism: LLMs can identify factual errors in generated statements, provide corrective instructions, and edit the statements to align with the source table. This feedback loop enables iterative refinement of outputs.
- Core assumption: LLMs have sufficient understanding of both natural language and structured data to detect inconsistencies and generate appropriate corrections.
- Evidence anchors:
  - [abstract] "LLMs using chain-of-thought prompting can generate high-fidelity natural language feedback for other table-to-text models' generations."
  - [section] "We examine the faithfulness scores of Edited Statements in the generated feedback, comparing these scores to those of the original statements."
- Break condition: The LLM's corrections introduce new errors or fail to preserve the intended meaning of the original statement.

### Mechanism 3
- Claim: GPT-4 significantly outperforms other models in generating coherent and faithful statements from tables.
- Mechanism: The larger model size and more extensive training of GPT-4 enables it to better understand complex table structures and perform the necessary logical reasoning to generate faithful statements.
- Core assumption: Model scale and training data quality directly correlate with performance on structured reasoning tasks.
- Evidence anchors:
  - [abstract] "Results show that GPT-4 significantly outperforms other models in generating coherent and faithful statements."
  - [section] "GPT-* models generally outperform the existing best-performance fine-tuned model (i.e., LOFT and PLOG), even in a 0-shot setting."
- Break condition: The task complexity exceeds the model's reasoning capabilities, or the table contains information outside the model's training distribution.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Enables LLMs to perform step-by-step reasoning required for table verification and feedback generation
  - Quick check question: What is the key difference between direct prediction and chain-of-thought prompting in this context?

- Concept: Reference-free evaluation metrics
  - Why needed here: Traditional evaluation methods requiring reference outputs are not suitable for table-to-text tasks where multiple valid outputs may exist
  - Quick check question: Why can't we use standard reference-based metrics like BLEU or ROUGE for this task?

- Concept: Factuality vs. fluency in text generation
  - Why needed here: The paper evaluates both aspects separately, recognizing that a statement can be fluent but factually incorrect
  - Quick check question: How do the paper's evaluation metrics distinguish between fluency and factuality?

## Architecture Onboarding

- Component map: Table serialization module -> Prompt template management -> LLM inference service -> Output parsing module -> Evaluation metric calculation -> Human evaluation interface

- Critical path: 1. Table input → 2. Serialization → 3. Prompt construction → 4. LLM inference → 5. Output parsing → 6. Evaluation

- Design tradeoffs:
  - Model choice: GPT-4 provides best performance but highest cost; open-source models are cheaper but less accurate
  - Prompt engineering: More detailed prompts improve accuracy but increase token usage
  - Evaluation: Automated metrics are faster but less reliable than human evaluation

- Failure signatures:
  - Low TAPEX-Acc scores despite high human evaluation scores may indicate overfitting to TabFact
  - Inconsistent outputs across different prompt formulations suggest sensitivity to prompt engineering
  - Poor performance on tables with complex nested structures indicates limitations in table understanding

- First 3 experiments:
  1. Compare 0-shot, 1-shot, and 2-shot performance for each LLM to determine optimal few-shot strategy
  2. Test different prompt formulations (direct vs. chain-of-thought) to identify which yields better factuality scores
  3. Evaluate the correlation between CoT-Acc metric scores and human judgments across different model outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the table-to-text generation capabilities of LLMs be further improved by incorporating more diverse and complex reasoning operations beyond those currently explored in LogicNLG?
- Basis in paper: [inferred] The paper discusses the performance of LLMs on the LogicNLG dataset, which involves generating statements through logical inference. However, it suggests exploring more challenging table-to-text generation benchmarks in the future.
- Why unresolved: The current study focuses on a specific dataset and task, and the authors acknowledge the need for more challenging benchmarks.
- What evidence would resolve it: Conducting experiments with LLMs on more complex table-to-text generation tasks that require diverse and advanced reasoning operations, and comparing their performance with existing models.

### Open Question 2
- Question: How do open-source LLMs, such as BLOOM, OPT, and LLaMA, perform in table-to-text generation tasks compared to GPT models?
- Basis in paper: [explicit] The paper mentions that due to budget constraints, they conducted experiments only with GPT models and left the investigation of open-source LLMs for future work.
- Why unresolved: The study focuses on GPT models as a representative sample of state-of-the-art LLMs, but does not explore the performance of open-source alternatives.
- What evidence would resolve it: Conducting experiments with open-source LLMs on the same table-to-text generation tasks and comparing their performance with GPT models.

### Open Question 3
- Question: Can the feedback generation capabilities of LLMs be extended to provide more specific and actionable guidance for improving the factual consistency of table-to-text generation models?
- Basis in paper: [explicit] The paper demonstrates that LLMs can generate feedback for other models' outputs, including explanations, corrective instructions, and edited statements. However, it suggests that future work could explore more detailed and actionable feedback.
- Why unresolved: The current study provides a proof-of-concept for feedback generation but does not delve into the specifics of the feedback content or its impact on model improvement.
- What evidence would resolve it: Conducting experiments to evaluate the effectiveness of different types of feedback (e.g., more detailed explanations, specific corrective instructions) on improving the factual consistency of table-to-text generation models.

## Limitations

- The study relies on proprietary GPT models (GPT-3.5-turbo and GPT-4) whose exact architectures and training details remain undisclosed
- The newly constructed LoTNLG and F2WTQ datasets are not publicly available, limiting reproducibility
- Automated metrics show varying correlations with human judgments, suggesting potential limitations in their reliability

## Confidence

- **High Confidence**: GPT-4's superior performance in generating coherent and faithful statements (supported by multiple automated and human evaluation metrics)
- **Medium Confidence**: The effectiveness of chain-of-thought prompting as a reference-free evaluation metric (evidence is correlational but not causal)
- **Medium Confidence**: LLMs' ability to generate high-fidelity feedback for improving other models' outputs (demonstrated in specific cases but not extensively validated)

## Next Checks

1. **Reproduce key findings using open-source LLMs**: Test whether the reported performance gap between GPT-4 and other models persists when using fine-tuned open-source models of similar size, controlling for prompt engineering quality.

2. **Cross-dataset validation**: Evaluate the same models on established table-to-text datasets (e.g., WikiTableQuestions, LogicNLG) to verify whether performance patterns hold across different table types and domains.

3. **Human evaluation expansion**: Conduct human evaluation studies with domain experts to assess whether the automated metrics (particularly TAPEX-Acc and NLI-Acc) consistently align with expert judgments across diverse table types and complexity levels.