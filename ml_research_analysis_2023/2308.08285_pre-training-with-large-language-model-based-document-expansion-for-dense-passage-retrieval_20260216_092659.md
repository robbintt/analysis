---
ver: rpa2
title: Pre-training with Large Language Model-based Document Expansion for Dense Passage
  Retrieval
arxiv_id: '2308.08285'
source_url: https://arxiv.org/abs/2308.08285
tags:
- pre-training
- retrieval
- queries
- query
- passage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the use of large language models (LLMs) for
  document expansion in dense passage retrieval. The key idea is to leverage LLMs
  to generate pseudo-queries for passages, then incorporate these queries into pre-training
  of dense retrievers via contrastive learning and bottlenecked query generation.
---

# Pre-training with Large Language Model-based Document Expansion for Dense Passage Retrieval

## Quick Facts
- arXiv ID: 2308.08285
- Source URL: https://arxiv.org/abs/2308.08285
- Authors: 
- Reference count: 17
- One-line primary result: Pre-training with LLM-generated queries significantly improves dense retrieval, with contrastive pre-training excelling in zero-shot and bottlenecked pre-training in fine-tuning.

## Executive Summary
This paper explores using large language models (LLMs) to generate pseudo-queries for passages, then leveraging these queries in pre-training dense retrievers via contrastive learning or bottlenecked query generation. A curriculum learning strategy is proposed to reduce LLM inference costs. Experiments on web search benchmarks show significant performance gains, with contrastive pre-training excelling in zero-shot retrieval and bottlenecked pre-training in fine-tuning initialization.

## Method Summary
The method uses LLMs to generate high-quality pseudo-queries for passages, then pre-trains dense retrievers using these queries via contrastive learning (aligning passage and query embeddings) or bottlenecked query generation (compressing context via auxiliary decoder). A two-stage curriculum strategy is introduced: first pre-train on span-cropped passages (coarse context), then fine-tune on LLM-generated queries (fine context). This reduces LLM inference needs while maintaining retrieval performance. Pre-training uses MS-MARCO; evaluation includes zero-shot and fine-tuned retrieval on MS-MARCO, TREC-DL, and BEIR.

## Key Results
- Contrastive pre-training with LLM queries improves zero-shot Recall@10 on BEIR from 2.0% to 2.2%.
- Pre-training with LLM queries boosts MS-MARCO fine-tuned Recall@10 from 36.6% to 38.6%.
- Curriculum learning reduces LLM inferences by ~75% while retaining most performance gains.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated queries provide higher-quality pseudo-relevant contexts than random span corruption.
- Mechanism: LLM queries, grounded in world knowledge, better align with actual information needs, offering stronger supervision for contrastive alignment.
- Core assumption: LLM's world knowledge approximates real user query distributions.
- Evidence anchors:
  - [abstract]: "LLMs can generate a large number of high-quality queries based on the world knowledge of LLM itself, which requires no additional human labeling and is suitable for scenarios lacking in manually annotated data."
  - [section]: "We believe that incorporating the strong context-generation abilities of LLMs into the pre-training stage with carefully designed pre-tasks can be a new way for improving such alignment."
- Break condition: If LLM's knowledge is stale or mismatched, generated queries misalign embeddings and degrade retrieval.

### Mechanism 2
- Claim: Contrastive pre-training with LLM queries yields stronger zero-shot retrieval than bottlenecked query generation.
- Mechanism: Pulls passage embeddings toward LLM-generated query embeddings in shared latent space, learning query-passage similarity without labels.
- Core assumption: Cross-attention and projection in contrastive loss align encoder's semantic space to both passages and queries.
- Evidence anchors:
  - [abstract]: "Contrastive pre-training yields stronger in-domain zero-shot retrieval performance and on-par performance with state-of-the-art methods after full fine-tuning."
  - [section]: "Through contrastive pre-training, the representations of passage and LLM-generated queries are directly pulled together in the same latent space, which gives better query-passage alignment and zero-shot ability to encoders."
- Break condition: If contrastive objective dominates, encoder overfits to pre-training corpus, harming generalization.

### Mechanism 3
- Claim: Curriculum learning with coarse-to-fine contexts reduces LLM inference costs while maintaining performance.
- Mechanism: Pre-train on randomly sampled spans to bootstrap encoder, then fine-tune on LLM-generated queries for fine-grained signals.
- Core assumption: Encoder benefits from initial generic semantic learning before fine-tuning on richer pseudo-queries.
- Evidence anchors:
  - [abstract]: "We incorporate a curriculum learning strategy to reduce the reliance on LLM inferences."
  - [section]: "After pre-training on a large amount of randomly cropped contexts, we initialize from the first stage and then use the fine-grained LLM-expanded queries for the second-phrase pre-training."
- Break condition: If first stage's span corruption is too weak, second stage cannot compensate, leading to performance collapse.

## Foundational Learning

- Concept: Contrastive learning objective for dense retrieval
  - Why needed here: Aligns passage and query embeddings in shared latent space without explicit relevance labels.
  - Quick check question: What is the formula for the contrastive loss with in-batch negatives used in this paper?

- Concept: Bottlenecked encoder-decoder architecture
  - Why needed here: Compresses context information into encoder representations via auxiliary decoder training.
  - Quick check question: How does the single-layer decoder in bottlenecked pre-training help initialize the encoder for fine-tuning?

- Concept: Curriculum learning in representation learning
  - Why needed here: Enables staged training from coarse-grained to fine-grained supervision to reduce expensive LLM inferences.
  - Quick check question: What is the ratio of training steps allocated to the first stage (span corruption) vs second stage (LLM queries)?

## Architecture Onboarding

- Component map:
  Passage encoder (BERT-base or similar) -> Optional auxiliary decoder for bottlenecked pre-training -> LLM query generator (Alpaca-LLaMA or tk-Instruct) -> Curriculum stage selector (span vs LLM queries) -> Loss combiner (MLM + CLM or MLM + contrastive)

- Critical path:
  1. Generate LLM queries for a subset of MS-MARCO passages.
  2. Stage 1: Pre-train encoder on span-cropped passages (75% steps).
  3. Stage 2: Fine-tune encoder using LLM-generated queries (25% steps).
  4. Optionally, add bottlenecked decoder and train with CLM loss.

- Design tradeoffs:
  - Using full LLM queries increases cost but improves alignment; curriculum reduces cost at slight performance drop.
  - Contrastive vs bottlenecked pre-training: contrastive better for zero-shot, bottlenecked better for fine-tuning initialization.

- Failure signatures:
  - Overfitting to LLM queries: zero-shot degrades on out-of-domain data.
  - Underutilization of LLM queries: contrastive loss does not improve over baselines.
  - Curriculum collapse: first stage too weak, leading to poor second-stage gains.

- First 3 experiments:
  1. Run contrastive pre-training baseline (no LLM queries) on MS-MARCO and evaluate zero-shot on dev set.
  2. Add Alpaca-7B generated queries to contrastive pre-training, compare zero-shot metrics.
  3. Implement curriculum strategy with 75/25 split, measure reduction in LLM query usage vs performance retention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM sizes (e.g., 7B vs 13B parameters) impact retrieval performance when used for document expansion in dense passage retrieval?
- Basis in paper: [explicit] The paper tests Alpaca models with 7B and 13B parameters, showing that scaling up LLMs leads to better retrieval performances in zero-shot contrastive pre-training, but this observation is not consistent after fine-tuning.
- Why unresolved: The paper does not provide a detailed analysis of the trade-offs between model size, computational cost, and retrieval performance. It also does not explore the impact of using even larger models like ChatGPT or LLaMA 2.
- What evidence would resolve it: Experiments comparing different LLM sizes (e.g., 7B, 13B, 30B) on both zero-shot and fine-tuned retrieval tasks, along with a detailed analysis of computational costs and performance trade-offs.

### Open Question 2
- Question: How does the proposed two-stage curriculum learning strategy affect the overall retrieval performance compared to using LLM-expanded queries throughout the entire pre-training process?
- Basis in paper: [explicit] The paper introduces a two-stage curriculum learning strategy that uses randomly sampled passages as coarse-grained context in the first stage and LLM-expanded queries as fine-grained context in the second stage. It claims to reduce the need for LLM inferences while maintaining performance.
- Why unresolved: The paper does not provide a detailed comparison of retrieval performance between the two-stage curriculum learning strategy and using LLM-expanded queries throughout the entire pre-training process. It also does not explore the optimal ratio of coarse-grained to fine-grained context.
- What evidence would resolve it: Experiments comparing retrieval performance between the two-stage curriculum learning strategy and using LLM-expanded queries throughout the entire pre-training process, along with an analysis of the optimal ratio of coarse-grained to fine-grained context.

### Open Question 3
- Question: How does the proposed method perform on other dense passage retrieval tasks, such as open-domain question answering or conversational search, compared to state-of-the-art methods?
- Basis in paper: [inferred] The paper focuses on dense passage retrieval for web search tasks, but it does not explore the method's performance on other dense passage retrieval tasks.
- Why unresolved: The paper does not provide experiments or analysis of the proposed method's performance on other dense passage retrieval tasks, such as open-domain question answering or conversational search.
- What evidence would resolve it: Experiments comparing the proposed method's performance on other dense passage retrieval tasks, such as open-domain question answering or conversational search, against state-of-the-art methods.

## Limitations

- The evaluation relies heavily on synthetic queries generated by LLMs, but the paper does not provide details on prompt engineering, temperature settings, or diversity measures, making it unclear how sensitive the approach is to variations in query generation quality.
- The curriculum learning strategy is described conceptually but lacks ablation studies on the optimal stage split or how much span corruption degrades retrieval compared to direct LLM-based pre-training.
- The paper does not compare against strong document expansion baselines like DocT5Query or GenQ, making it difficult to isolate the contribution of the pre-training method from the expansion method.

## Confidence

- **High confidence**: The experimental results showing performance gains over pre-training baselines (e.g., from 36.6% to 38.6% Recall@10 on MS-MARCO). The ablation on curriculum learning and its effect on zero-shot vs fine-tuned performance is well supported by the data.
- **Medium confidence**: The claim that contrastive pre-training yields stronger zero-shot retrieval than bottlenecked query generation. The paper shows this but lacks deeper analysis of why, and the metric differences are small (e.g., 2.2% vs 2.1% Recall@10 on BEIR).
- **Low confidence**: The generality of the approach to domains beyond web search, since all experiments are in-domain or on related datasets. No robustness tests against domain shift are provided.

## Next Checks

1. **Prompt ablation**: Systematically vary the LLM prompt format, temperature, and top-k sampling to measure the sensitivity of retrieval performance to query generation quality.
2. **Curriculum learning ablation**: Test different stage ratios (e.g., 50/50, 90/10) and measure both performance and inference cost trade-offs to determine the optimal curriculum schedule.
3. **Expansion method comparison**: Implement and compare against a strong document expansion baseline (e.g., DocT5Query or GenQ) pre-trained with the same contrastive framework to isolate the effect of LLM-based expansion.