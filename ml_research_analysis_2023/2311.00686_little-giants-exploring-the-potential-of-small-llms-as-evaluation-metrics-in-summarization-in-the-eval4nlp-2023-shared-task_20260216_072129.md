---
ver: rpa2
title: 'Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics
  in Summarization in the Eval4NLP 2023 Shared Task'
arxiv_id: '2311.00686'
source_url: https://arxiv.org/abs/2311.00686
tags:
- summary
- score
- article
- quality
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an exploration of small language models (LLMs)
  as evaluation metrics for summarization tasks. The authors participated in the Eval4NLP
  2023 shared task, focusing on using prompt-based techniques to enable LLMs to assess
  the quality of machine-generated summaries.
---

# Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task

## Quick Facts
- **arXiv ID**: 2311.00686
- **Source URL**: https://arxiv.org/abs/2311.00686
- **Reference count**: 40
- **Primary result**: Small LLMs can achieve competitive summarization evaluation performance with Kendall correlation of 0.44

## Executive Summary
This paper explores the use of small language models (LLMs) as evaluation metrics for summarization tasks, focusing on prompt-based techniques. The authors participated in the Eval4NLP 2023 shared task, systematically experimenting with various prompting strategies including standard prompts, annotator instruction-informed prompts, and chain-of-thought prompting using the 7B parameter orca_mini_v3_7B model. Their findings demonstrate that small LLMs can effectively evaluate summary quality through careful prompt engineering, with zero-shot and one-shot approaches yielding competitive results. The work highlights the potential of small LLMs as a resource-efficient alternative to larger models for quality estimation tasks.

## Method Summary
The study employs the orca_mini_v3_7B model with various prompt engineering techniques to evaluate summarization quality without fine-tuning. Experiments use the SummEval dataset with 1,280 development examples and 825 test examples. The approach tests standard prompting, annotator instruction-based prompts, and chain-of-thought prompting in both zero-shot and one-shot settings. Evaluation is performed using Kendall rank correlation coefficient against human judgments, with systematic testing across 14 different prompt variations to identify optimal configurations for small LLM evaluation metrics.

## Key Results
- Small LLMs achieved a Kendall correlation of 0.44 on the test set, demonstrating competitive evaluation capability
- Standard prompting with quantitative scoring scales (1-5) in zero-shot settings performed best (0.3211 score)
- Chain-of-thought prompting showed potential for improving interpretability and reasoning quality
- Qualitative labels ("Very Poor" to "Very Good") yielded significantly worse performance (-0.03 score) compared to quantitative scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought prompting improves interpretability and reasoning quality
- Mechanism: CoT prompts guide the model through intermediate reasoning steps before final score generation, reducing inconsistencies
- Core assumption: Sequential reasoning steps lead to more coherent and explainable evaluation outputs
- Evidence anchors:
  - [abstract] "This technique holds the potential to enhance the interpretability and explainability of quality estimation models."
  - [section] "The CoT additions are marked in blue in Tables 6 and 7."
  - [corpus] Weak - no direct evidence in related papers about CoT's effectiveness for this specific task
- Break condition: If reasoning steps become too verbose or introduce irrelevant information that confuses the model

### Mechanism 2
- Claim: Standard prompting with quantitative scores outperforms qualitative labels
- Mechanism: Numerical scoring scales provide clearer evaluation criteria for the model compared to ordinal categories
- Core assumption: LLMs process numerical ranges more consistently than categorical labels
- Evidence anchors:
  - [section] "While assessing the precision and range of scores requested from the model, we observed that quantitative scores with lower precision exhibited favorable performance."
  - [section] "the use of qualitative labels such as 'Very Poor', 'Poor', 'Average', 'Good', and 'Very Good' to describe quality yielded comparatively less favorable results"
  - [corpus] Weak - no corpus evidence about score format preferences
- Break condition: If model struggles with decimal precision or range boundaries

### Mechanism 3
- Claim: Zero-shot prompting with standard instructions performs competitively to more complex approaches
- Mechanism: Simple, direct prompts reduce cognitive load on the model while maintaining task clarity
- Core assumption: Less complex prompts are easier for models to parse and execute accurately
- Evidence anchors:
  - [section] "Prompt P1, which employs a standard manual prompt in a zero-shot setting with a grading scale ranging from 1 to 5, emerges as the top performer, achieving a notable score of 0.3211"
  - [section] "the Direct Assessment baseline provided by the Shared Task is also a simple manual prompt in a zero-shot setting"
  - [corpus] Weak - no direct corpus comparison of zero-shot vs few-shot for this task
- Break condition: If task complexity increases beyond simple evaluation criteria

## Foundational Learning

- Concept: Prompt engineering principles
  - Why needed here: Different prompt structures yield vastly different evaluation results
  - Quick check question: What's the difference between standard prompting and chain-of-thought prompting in terms of output format?

- Concept: Zero-shot vs few-shot learning
  - Why needed here: Understanding when additional examples help or hinder model performance
  - Quick check question: Why might adding examples to prompts sometimes reduce performance?

- Concept: Evaluation metrics (Kendall rank coefficient)
  - Why needed here: To understand how the model's outputs are being assessed against human judgments
  - Quick check question: What does a Kendall correlation of 0.44 indicate about the model's evaluation quality?

## Architecture Onboarding

- Component map: orca_mini_v3_7B model → prompt generator → evaluation pipeline → Kendall scoring
- Critical path: Prompt construction → model inference → score aggregation → Kendall correlation calculation
- Design tradeoffs: Simple prompts vs detailed instructions, zero-shot vs few-shot, computational efficiency vs evaluation quality
- Failure signatures: Hallucinations in generated prompts, inconsistent scoring across similar inputs, correlation scores below baseline
- First 3 experiments:
  1. Test standard prompting with numerical scale (1-5) in zero-shot setting
  2. Implement chain-of-thought prompting with intermediate reasoning steps
  3. Compare performance with annotator instruction-based prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific variations in prompt wording affect the performance of small LLMs in summarization evaluation tasks?
- Basis in paper: [inferred] The paper mentions that "slight variations in the underlying prompt text could swing performances rather widely," indicating that prompt wording significantly impacts results.
- Why unresolved: While the paper acknowledges this phenomenon, it does not provide detailed analysis of which specific prompt variations lead to performance changes or the magnitude of these effects.
- What evidence would resolve it: Systematic experiments varying prompt wording while keeping other factors constant, followed by detailed analysis of performance changes and their correlation to specific wording differences.

### Open Question 2
- Question: What is the optimal balance between prompt precision and granularity when instructing small LLMs for quality estimation tasks?
- Basis in paper: [explicit] The paper states "our experimentation with scoring rubrics revealed an intriguing trend" showing that "quantitative scores with lower precision exhibited favorable performance," while "use of qualitative labels... yielded comparatively less favorable results."
- Why unresolved: The paper identifies this trend but doesn't explore the underlying reasons or establish specific guidelines for optimal prompt design in terms of precision and granularity.
- What evidence would resolve it: Comparative studies testing different levels of precision and granularity in prompts, measuring their impact on performance and identifying patterns that suggest optimal configurations.

### Open Question 3
- Question: How can hallucinations in LLM-generated prompts be systematically prevented or mitigated in automated prompt refinement processes?
- Basis in paper: [explicit] The paper discusses encountering "instances of hallucinations within the generated instructions" during their experiments with LLM-generated prompts, noting issues like inconsistent numbering and erroneous additions.
- Why unresolved: While the paper identifies the problem and notes manual removal of hallucinations, it doesn't propose systematic solutions for preventing or detecting hallucinations in automated prompt refinement.
- What evidence would resolve it: Development and testing of automated hallucination detection methods, or implementation of prompt generation frameworks that inherently prevent hallucination through architectural constraints or verification steps.

## Limitations

- Limited to a single 7B parameter model architecture, raising questions about generalizability across different small LLMs
- No quantitative comparison with larger models to validate efficiency claims or performance tradeoffs
- Relies entirely on prompt engineering without exploring model architecture modifications or fine-tuning approaches

## Confidence

- **High Confidence**: Small LLMs can achieve non-trivial correlation scores (0.44) as evaluation metrics
- **Medium Confidence**: Chain-of-thought prompting and zero-shot approaches provide competitive performance
- **Low Confidence**: Claims about interpretability improvements and efficiency advantages lack quantitative comparisons

## Next Checks

1. Test the same prompting strategies across multiple small LLM architectures (different 7B models) to assess generalizability
2. Conduct head-to-head comparisons with larger models using identical evaluation protocols and compute budgets
3. Measure actual inference time and computational costs for small vs large models to validate efficiency claims