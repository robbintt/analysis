---
ver: rpa2
title: Exploring Meta Information for Audio-based Zero-shot Bird Classification
arxiv_id: '2309.08398'
source_url: https://arxiv.org/abs/2309.08398
tags:
- species
- audio
- bird
- embeddings
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores using meta-information to improve zero-shot
  audio classification for bird species, addressing the challenge of data scarcity
  for rare species. The method uses audio spectrogram transformer (AST) embeddings
  as audio features and projects them to the dimension of auxiliary information (meta-data)
  using a single linear layer.
---

# Exploring Meta Information for Audio-based Zero-shot Bird Classification

## Quick Facts
- arXiv ID: 2309.08398
- Source URL: https://arxiv.org/abs/2309.08398
- Reference count: 0
- Primary result: Mean unweighted F1-score of 0.233 using concatenated AVONET and BLH metadata

## Executive Summary
This paper investigates using auxiliary metadata to improve zero-shot bird species classification from audio recordings. The approach uses AST embeddings as audio features, which are projected into the semantic space of various metadata types (textual descriptions, functional traits, life-history characteristics) using a linear layer. The method achieves best results by concatenating AVONET functional traits and BLH life-history features, outperforming text-based approaches likely due to limitations in general language models handling bird-specific onomatopoeia.

## Method Summary
The zero-shot learning framework projects AST audio embeddings (768-dim) to the dimension of auxiliary information embeddings using a single linear layer. Three metadata types are investigated: BERT/SBERT-encoded textual descriptions (768-dim), AVONET functional traits (23-dim after preprocessing), and BLH life-history characteristics (77-dim after preprocessing). Compatibility between projected audio and class embeddings is computed via dot product, and a ranking hinge loss with WAPR weighting is used for training. The model is trained for 30 epochs using SGD (learning rate 0.0001, batch size 16) and evaluated across five disjoint test sets (8-10 classes each) using mean unweighted F1-score.

## Key Results
- Concatenating AVONET and BLH metadata features achieves the best performance with mean F1-score of 0.233
- BERT and SBERT text embeddings underperform, likely due to lack of bird-specific onomatopoeia in pre-training
- Performance significantly exceeds chance levels (9.1-12.5%) across all test sets
- Textual descriptions via SBERT create more distinguishable species embeddings than BERT

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concatenating AVONET and BLH metadata improves zero-shot bird classification because these datasets encode complementary biological information (morphology + life-history) that better aligns with audio features than text-based descriptions.
- Mechanism: The AST audio embeddings are projected into the shared semantic space of metadata embeddings. AVONET provides functional traits like beak and wing measurements, while BLH adds life-history details like nesting and feeding habits. This combination creates a richer semantic embedding that the dot product compatibility function can leverage for more accurate ranking.
- Core assumption: The biological characteristics in AVONET and BLH are meaningfully related to the acoustic properties of bird vocalizations, even though they're measured through different modalities.
- Evidence anchors:
  - [abstract] "The best results are achieved by concatenating the AVONET and BLH features attaining a mean F1-score of .233"
  - [section] "AVONET includes the following parameters: beak and wing measurements; tarsus and tail length; kipps distance; mass; habitat; habitat density; primary lifestyle"
  - [corpus] Weak evidence - no direct corpus support found for this specific metadata combination approach
- Break condition: If the biological traits encoded in AVONET and BLH have no systematic relationship to vocalization patterns, the concatenation would not improve performance over using either dataset alone.

### Mechanism 2
- Claim: BERT and SBERT embeddings of bird sound descriptions perform poorly because these language models were not pre-trained on bird-specific onomatopoeia, causing them to miss crucial acoustic information.
- Mechanism: The textual descriptions contain onomatopoeic representations of bird sounds (e.g., "vist, tk-tk-tk" or "si-sr¨u TILL-ILL-ILL-ILL-ILL....... (krschkrsch)"). Standard language models trained on general text corpora fail to properly encode these specialized acoustic descriptions, resulting in embeddings that don't capture the discriminative information needed for classification.
- Core assumption: The onomatopoeic descriptions in the Princeton Field Guide contain information that is predictive of bird species identity and that this information is not captured by general-purpose language models.
- Evidence anchors:
  - [abstract] "likely due to the language models not being pre-trained on bird-specific onomatopoeia"
  - [section] "the sound of a bird species is described in a textual manner w. r. t. specific patterns and peculiarities, while relying heavily on onomatopoeia"
  - [corpus] Weak evidence - no direct corpus support found for the impact of onomatopoeia on ZSL performance
- Break condition: If the textual descriptions contain sufficient non-onomatopoeic information about bird characteristics that could be captured by language models, or if the relationship between descriptions and audio features is too weak regardless of model choice.

### Mechanism 3
- Claim: The ranking hinge loss with dot product compatibility effectively learns to rank class embeddings by their similarity to projected audio embeddings, enabling zero-shot classification without training examples.
- Mechanism: The model projects audio embeddings to the dimension of class embeddings using a linear layer, then computes compatibility via dot product. The ranking hinge loss encourages the correct class embedding to have higher compatibility with the projected audio embedding than all other class embeddings, scaled by their rank. This creates a relative ranking system that works without requiring labeled examples for each class.
- Core assumption: The semantic space defined by the metadata embeddings captures meaningful relationships between classes that can be used to rank audio samples, even for classes with no training data.
- Evidence anchors:
  - [section] "The goal is that the highest ranked class embeddings best describe the audio sample"
  - [section] "we employ the dot product as compatibility function for our ranking loss explained in Section 3.2"
  - [corpus] Weak evidence - no direct corpus support found for this specific ZSL ranking approach with audio metadata
- Break condition: If the metadata embeddings don't capture the relevant semantic relationships between classes, or if the audio features don't project well into this semantic space, the ranking mechanism would fail to produce accurate classifications.

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: This paper's core contribution is applying zero-shot learning to bird audio classification, where the goal is to classify species without any labeled training examples for those species.
  - Quick check question: What distinguishes zero-shot learning from few-shot or transfer learning approaches?

- Concept: Audio spectrogram transformer (AST) embeddings
  - Why needed here: AST embeddings serve as the audio features that get projected into the semantic space of metadata embeddings. Understanding how these embeddings capture audio information is crucial for grasping the method.
  - Quick check question: How do AST embeddings differ from traditional audio feature extraction methods like MFCCs or spectrograms?

- Concept: Semantic embedding spaces and compatibility functions
  - Why needed here: The paper relies on projecting audio embeddings into a shared semantic space with metadata embeddings and using dot product as a compatibility function. Understanding how these spaces work is essential for understanding the method.
  - Quick check question: Why might dot product be chosen as the compatibility function rather than cosine similarity or Euclidean distance?

## Architecture Onboarding

- Component map:
  Audio input → AST model → 768-dim audio embeddings → Linear projection layer → Projected audio embeddings → Dot product compatibility → Ranking hinge loss → Classification

- Critical path: Audio → AST → Projection → Compatibility → Ranking → Classification
- Design tradeoffs: Simple linear projection vs. more complex mapping functions; dot product vs. other compatibility measures; concatenation vs. separate models for different metadata types
- Failure signatures: Poor performance on development set despite training; large gap between development and test performance (overfitting); similar performance across different metadata types (suggesting the method isn't leveraging metadata effectively)
- First 3 experiments:
  1. Train with only AVONET features to establish baseline performance with functional traits
  2. Train with only BLH features to establish baseline performance with life-history traits
  3. Train with concatenated AVONET and BLH features to test the complementarity hypothesis

## Open Questions the Paper Calls Out

- Would pre-training or fine-tuning language models on bird-specific onomatopoeia improve the performance of zero-shot audio classification?
- How would incorporating visual information (images of bird species) as auxiliary information impact the performance of zero-shot audio classification?
- Would more sophisticated zero-shot learning models significantly improve performance compared to the simple model used in this study?

## Limitations

- The mean F1-score of 0.233, while exceeding chance performance, remains relatively low and suggests substantial room for improvement
- The study's reliance on specific datasets (XENO-CANTO, AVONET, BLH) and the AST model architecture limits generalizability to other audio classification tasks
- The claim that BERT/SBERT underperforms specifically due to lack of bird onomatopoeia pre-training is plausible but not definitively proven

## Confidence

- High confidence: The overall methodology and experimental design are sound, with appropriate controls and evaluation metrics
- Medium confidence: The relative performance comparison between different metadata types is reliable, though absolute performance levels are modest
- Low confidence: The specific mechanism explaining why BERT/SBERT underperforms (lack of bird onomatopoeia training) is speculative and requires further validation

## Next Checks

1. Test alternative language models pre-trained on bioacoustic or ornithological texts to verify if the poor performance is truly due to lack of bird-specific training data
2. Conduct ablation studies on the AVONET and BLH feature sets to determine which specific features contribute most to performance improvements
3. Evaluate the approach on a broader range of test sets with more classes and different taxonomic distributions to assess generalizability