---
ver: rpa2
title: Sample Efficient Preference Alignment in LLMs via Active Exploration
arxiv_id: '2312.00267'
source_url: https://arxiv.org/abs/2312.00267
tags:
- function
- answer
- which
- policy
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of efficient data collection for
  preference-based reinforcement learning in large language models (LLMs). In settings
  where obtaining human feedback is expensive, the authors formalize an active contextual
  dueling bandit problem and propose an algorithm with polynomial regret guarantees.
---

# Sample Efficient Preference Alignment in LLMs via Active Exploration

## Quick Facts
- arXiv ID: 2312.00267
- Source URL: https://arxiv.org/abs/2312.00267
- Authors: (not provided)
- Reference count: 30
- Key outcome: Active exploration improves preference alignment by over 10% using limited human feedback samples

## Executive Summary
This paper addresses the challenge of efficient data collection for preference-based reinforcement learning in large language models (LLMs). The authors formalize this as an active contextual dueling bandit problem and propose an algorithm with polynomial regret guarantees. By using uncertainty estimates to select informative contexts and actions for querying human preferences, the method significantly reduces the amount of human feedback needed while improving alignment performance. The approach combines theoretical foundations with practical LLM adaptations using direct preference optimization (DPO) and dropout-based uncertainty estimation.

## Method Summary
The authors propose an active exploration algorithm that selects data points for human preference labeling based on uncertainty in the reward function. In the theoretical setting, this uses kernelized ridge regression to estimate the contextual Borda function and quantify uncertainty via confidence bounds. For practical LLM implementation, they extend this to work with DPO and dropout-based uncertainty estimation. The algorithm iteratively selects contexts where the policy has maximum uncertainty, generates actions, collects human preferences, and updates the policy. This active selection focuses data collection on the most informative regions of the context-action space, improving sample efficiency compared to passive collection methods.

## Key Results
- Active exploration method improves preference alignment performance by over 10% compared to baselines on three real-world datasets
- The approach is particularly effective at avoiding hallucinations in the Jeopardy! task, demonstrating improved factual accuracy
- The method achieves these improvements while using significantly fewer human preference samples than passive collection approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active exploration improves sample efficiency by selecting contexts and actions where the reward function uncertainty is highest.
- Mechanism: The algorithm computes upper and lower confidence bounds for the contextual Borda function and selects contexts that maximize the difference between these bounds, thereby focusing data collection on the most informative regions.
- Core assumption: The reward function and its associated Borda function have bounded RKHS norms, allowing uncertainty quantification via kernelized ridge regression.
- Evidence anchors:
  - [abstract]: "We propose an active exploration algorithm to efficiently select the data and provide theoretical proof that it has a polynomial worst-case regret bound."
  - [section 4.1]: "Our sampling rule samples contexts at which there is maximum uncertainty over the Borda 'value function' and then compares the optimistic action with an action sampled uniformly from the action set."
  - [corpus]: Weak evidence - corpus neighbors discuss active preference learning but don't directly address the specific uncertainty-driven selection mechanism.
- Break condition: If the uncertainty estimates become inaccurate or if the kernelized regression fails to capture the true reward function structure, the selection rule may target uninformative regions.

### Mechanism 2
- Claim: Dropout-based uncertainty estimation enables scalable uncertainty quantification for LLMs without additional memory overhead.
- Mechanism: Multiple forward passes with dropout enabled generate an ensemble of predictions, and the variance across these predictions serves as an approximation of epistemic uncertainty.
- Core assumption: The dropout mask effectively samples from a distribution over models, making the variance across predictions a meaningful uncertainty estimate.
- Evidence anchors:
  - [section 5]: "We estimate the uncertainty of our policy using dropout for uncertainty estimation (Gal & Ghahramani, 2016)."
  - [section J.1]: "We see that the log probabilities of incorrect answers always have a high variance, indicating high uncertainty."
  - [corpus]: Weak evidence - corpus neighbors discuss active preference learning but don't specifically address dropout-based uncertainty estimation in LLMs.
- Break condition: If the dropout masks don't adequately explore the model's uncertainty space or if the model's predictions are too deterministic, the uncertainty estimates may not be meaningful.

### Mechanism 3
- Claim: Direct Preference Optimization (DPO) allows uncertainty estimation without training a separate reward model.
- Mechanism: The DPO objective optimizes a policy that implicitly defines a reward function, and uncertainty in this policy translates directly to uncertainty in the reward.
- Core assumption: The policy learned by DPO is equivalent to optimizing a PPO objective with a specific reward function, allowing uncertainty in the policy to proxy uncertainty in the reward.
- Evidence anchors:
  - [section 5]: "Direct Preference Optimization (DPO) (Rafailov et al., 2023) avoids training a separate reward model based on preferences by instead training the policy directly on pairwise comparison."
  - [section 5]: "We observe as in the original paper that Ï€r is precisely the probability distribution which DPO is estimating."
  - [corpus]: Weak evidence - corpus neighbors discuss DPO but don't specifically address uncertainty estimation in this context.
- Break condition: If the equivalence between DPO and PPO breaks down or if the policy becomes too confident, the uncertainty estimates may not accurately reflect the true reward function uncertainty.

## Foundational Learning

- Concept: Reproducing Kernel Hilbert Spaces (RKHS) and kernelized ridge regression
  - Why needed here: Provides a framework for estimating the contextual Borda function and quantifying uncertainty in the theoretical setting.
  - Quick check question: How does the RKHS norm of the reward function relate to the RKHS norm of the contextual Borda function?

- Concept: Dropout as a Bayesian approximation
  - Why needed here: Enables uncertainty estimation for LLMs without the memory overhead of ensemble methods or epistemic networks.
  - Quick check question: Under what conditions does dropout provide a valid approximation to Bayesian uncertainty?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: Allows optimization of a policy that implicitly defines a reward function, enabling uncertainty estimation without a separate reward model.
  - Quick check question: How does the DPO objective relate to the standard RLHF objective with a separate reward model?

## Architecture Onboarding

- Component map:
  - Context and action selection module -> Uncertainty estimation module -> DPO training module -> Data collection and labeling pipeline

- Critical path:
  1. Select context using uncertainty-driven acquisition function
  2. Generate actions and obtain human preferences
  3. Update uncertainty estimates and policy
  4. Repeat until convergence

- Design tradeoffs:
  - Kernel-based vs. dropout-based uncertainty estimation: Kernel methods provide theoretical guarantees but don't scale to large action spaces; dropout scales but lacks theoretical guarantees
  - Batch size vs. exploration efficiency: Larger batches reduce training time but may reduce the ability to target informative regions
  - Reference policy strength vs. exploration: Stronger reference policies may reduce exploration but improve sample efficiency

- Failure signatures:
  - Uncertainty estimates don't correlate with actual reward function error
  - Policy converges to suboptimal solutions despite active exploration
  - Training becomes unstable or diverges

- First 3 experiments:
  1. Implement kernelized active exploration on a synthetic 1D contextual dueling bandit problem and verify theoretical guarantees
  2. Compare dropout-based uncertainty estimates with ensemble methods on a small LLM task
  3. Evaluate the impact of batch size on sample efficiency in the active RLHF setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the RKHS norms of the Borda function and reward function relate to each other in higher-dimensional settings beyond what was empirically tested?
- Basis in paper: [explicit] The authors state "In Table 1, we present the results of comparing the RKHS norms of 1000 reward functions and their associated Borda functions sampled as in Section 4.3" and note that the Borda function typically has smaller norm.
- Why unresolved: The empirical study only tested up to 10-dimensional contexts and actions. The paper conjectures the relationship grows stronger with dimensionality but doesn't prove this.
- What evidence would resolve it: A systematic study of RKHS norms across a wider range of dimensionalities, ideally with theoretical bounds on how the relationship scales with dimension.

### Open Question 2
- Question: Can the theoretical polynomial regret bounds be extended to the LLM setting with direct preference optimization?
- Basis in paper: [inferred] The authors note that "Though these modifications mean that we lose the theoretical guarantees in the previous section" when adapting to LLMs, suggesting the guarantees don't directly transfer.
- Why unresolved: The LLM setting involves non-linear models, dropout-based uncertainty estimation, and different optimization objectives that complicate the theoretical analysis.
- What evidence would resolve it: A theoretical analysis showing regret bounds for the active exploration algorithm when used with LLMs and DPO, potentially building on recent work connecting neural networks to kernel methods.

### Open Question 3
- Question: How does the dropout-based uncertainty estimation in LLMs compare to other uncertainty quantification methods in terms of both accuracy and computational efficiency?
- Basis in paper: [explicit] The authors discuss their choice of dropout-based uncertainty estimation and note "we considered ensembles and epistemic networks (Osband et al., 2022) as alternative methods" but chose dropout for memory efficiency.
- Why unresolved: The paper only provides preliminary evidence that dropout uncertainty correlates with model knowledge on the Jeopardy dataset, without comprehensive comparison to alternatives.
- What evidence would resolve it: Systematic experiments comparing dropout-based uncertainty to ensemble methods and epistemic networks across multiple datasets and tasks, measuring both uncertainty quality and computational costs.

## Limitations

- Theoretical guarantees rely on kernelized ridge regression with bounded RKHS norms, which may not hold in high-dimensional LLM settings
- The Jeopardy! dataset, while new and valuable, may have domain-specific characteristics that limit generalizability to other tasks
- Comparison against baselines uses only three datasets, limiting broader applicability claims

## Confidence

- Theoretical regret bounds and polynomial guarantees: High
- Dropout-based uncertainty estimation in LLMs: Medium
- Overall 10%+ improvement claim: High
- Avoidance of hallucinations in Jeopardy!: Medium

## Next Checks

1. **Scale test**: Replicate the active exploration method on a larger LLM (e.g., Llama-70B) to verify scalability and determine if the uncertainty estimation remains effective at larger model sizes.

2. **Domain transfer**: Apply the active RLHF framework to a non-QA task (e.g., summarization or dialogue) to test generalizability beyond the current dataset types and evaluate if the same uncertainty-driven selection remains effective.

3. **Theoretical-empirical gap analysis**: Systematically compare the kernel-based theoretical uncertainty estimates with the dropout-based estimates across the same tasks to identify conditions where the theoretical assumptions break down and understand the practical implications.