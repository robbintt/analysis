---
ver: rpa2
title: 'EPLKG: Efficient Prompt Learning with Knowledge Graph'
arxiv_id: '2304.10805'
source_url: https://arxiv.org/abs/2304.10805
tags:
- prompt
- rplkg
- learning
- clip
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method called RPLKG (Robust Prompt Learning
  with Knowledge Graph) for efficient prompt learning with knowledge graphs. The key
  idea is to use a knowledge graph to curate diverse, interpretable prompts and augment
  this bank with LLM-generated human-readable visual descriptions.
---

# EPLKG: Efficient Prompt Learning with Knowledge Graph

## Quick Facts
- arXiv ID: 2304.10805
- Source URL: https://arxiv.org/abs/2304.10805
- Reference count: 36
- One-line primary result: RPLKG reduces per-image training time by up to 45% and peak GPU memory by 30-40% while maintaining competitive accuracy.

## Executive Summary
This paper introduces RPLKG (Robust Prompt Learning with Knowledge Graph), a method for efficient prompt learning in few-shot image classification. The approach leverages cached CLIP embeddings and a lightweight Gumbel-Softmax module to select interpretable prompts from a knowledge graph, achieving significant computational savings without sacrificing accuracy. By operating on precomputed embeddings and using ConceptNet-derived prompts, RPLKG improves both efficiency and interpretability compared to existing prompt learning methods.

## Method Summary
RPLKG constructs prompts from ConceptNet knowledge graph triples and converts them into human-readable sentences. It precomputes and caches CLIP image and text embeddings, then trains only a lightweight prompt selection module (0.79M parameters) using Gumbel-Softmax to choose a single prompt per image-class pair. The method uses cross-entropy loss between ground truth and predicted classes, avoiding backpropagation through the full CLIP model and achieving substantial efficiency gains.

## Key Results
- Reduces per-image training time by up to 45% compared to strong baselines
- Cuts peak GPU memory usage by 30-40% during training
- Maintains average base-new harmonic-mean accuracy within 2 percentage points of competitive methods
- Operates on 11 benchmark datasets with k-shot learning (k in {1,2,4,8,16})

## Why This Works (Mechanism)

### Mechanism 1
RPLKG uses cached CLIP embeddings to avoid backpropagation through the large model, reducing training time and memory. By computing image and text embeddings once and storing them, training only updates lightweight parameters at the top of CLIP, eliminating repeated forward and backward passes through the full model.

### Mechanism 2
Gumbel-Softmax enables sparse, differentiable prompt selection, improving interpretability and performance over softmax. Instead of averaging prompts with softmax, Gumbel-Softmax samples a single prompt per image-class pair, allowing end-to-end training while keeping outputs interpretable.

### Mechanism 3
Knowledge graph-derived prompts are more interpretable and domain-relevant than manual templates. RPLKG constructs prompts from ConceptNet by converting entity-relation-entity triples into sentences, producing descriptive, human-readable prompts that capture domain-specific knowledge better than generic templates.

## Foundational Learning

- **Concept: Contrastive Language-Image Pre-training (CLIP)**
  - Why needed: RPLKG builds on CLIP's image and text encoders; understanding CLIP's training objective is essential for grasping why cached embeddings work.
  - Quick check: What is the main training objective of CLIP, and how does it enable zero-shot classification?

- **Concept: Gumbel-Softmax trick**
  - Why needed: RPLKG uses Gumbel-Softmax for differentiable hard prompt selection; knowing how it approximates categorical sampling is key to understanding the mechanism.
  - Quick check: How does adding Gumbel noise to logits and applying softmax approximate sampling from a categorical distribution?

- **Concept: Knowledge graph reasoning**
  - Why needed: RPLKG relies on ConceptNet triples to generate prompts; understanding entity-relation-entity conversion to descriptive sentences is necessary for modifying the prompt generation.
  - Quick check: How would you convert the triple (cat, capable of, scratch) into a human-readable prompt?

## Architecture Onboarding

- **Component map**: CLIP image encoder (frozen) -> CLIP text encoder (frozen) -> Prompt database (cached text embeddings) -> Image database (cached image embeddings) -> Learnable linear layers (Wq, Wk, Wv) + Gumbel-Softmax selector -> Cross-entropy loss

- **Critical path**:
  1. Precompute and cache all image and text embeddings
  2. Apply linear transforms to image and prompt embeddings
  3. Compute similarity scores, apply Gumbel-Softmax to select one prompt per image-class pair
  4. Compute similarities between images and selected prompts
  5. Apply cross-entropy loss and backpropagate only through learnable layers

- **Design tradeoffs**:
  - Memory vs. speed: Caching embeddings uses more memory but eliminates repeated forward passes
  - Interpretability vs. flexibility: Hard prompt selection is interpretable but may miss nuanced combinations
  - Coverage vs. quality: Automatic prompt generation from ConceptNet is scalable but may produce low-quality prompts for rare classes

- **Failure signatures**:
  - Degraded accuracy if CLIP embeddings are not stable across prompt changes
  - High variance in selected prompts across similar images (indicates Gumbel-Softmax instability)
  - Poor performance on classes with sparse ConceptNet coverage

- **First 3 experiments**:
  1. Verify cached embeddings produce same similarity scores as real-time encoding for small subset
  2. Compare accuracy with Gumbel-Softmax vs. softmax prompt selection on validation set
  3. Test prompt generation coverage on held-out dataset to measure ConceptNet recall

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- No empirical validation that CLIP embeddings remain stable across prompt changes for the same image-class pair
- Lack of quantitative analysis comparing interpretability of knowledge graph prompts versus baseline templates
- No statistical significance testing or variance reporting across multiple runs

## Confidence

- **Efficiency gains**: High - well-supported by methodology of cached embeddings and lightweight parameter updates
- **Prompt interpretability**: Medium - human-readable prompts demonstrated but no quantitative validation
- **Accuracy maintenance**: Medium - results support claims but lack statistical testing and variance reporting

## Next Checks

1. Conduct ablation study comparing RPLKG with Gumbel-Softmax to variant using softmax averaging to test sparse selection benefits
2. Perform stability analysis measuring how CLIP embedding similarity changes when different prompts are applied to same image
3. Analyze ConceptNet coverage and prompt quality across all 11 datasets, reporting generation rates and conducting human study on interpretability