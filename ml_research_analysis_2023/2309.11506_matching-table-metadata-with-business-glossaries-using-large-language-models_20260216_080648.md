---
ver: rpa2
title: Matching Table Metadata with Business Glossaries Using Large Language Models
arxiv_id: '2309.11506'
source_url: https://arxiv.org/abs/2309.11506
tags:
- glossary
- metadata
- column
- matching
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of matching table metadata to business
  glossaries in enterprise settings where access to data contents is restricted. It
  proposes using large language models (LLMs) to generate context-aware descriptions
  of column headers and directly match them to glossary terms, eliminating the need
  for manual tuning or access to data.
---

# Matching Table Metadata with Business Glossaries Using Large Language Models

## Quick Facts
- arXiv ID: 2309.11506
- Source URL: https://arxiv.org/abs/2309.11506
- Reference count: 37
- Primary result: MDG-MICL achieved 81.33% Hit@5 accuracy on enterprise dataset, outperforming baseline methods

## Executive Summary
This paper addresses the challenge of matching table metadata to business glossaries in enterprise environments where data access is restricted. The proposed solution leverages large language models (LLMs) to generate context-aware descriptions of column headers and match them to glossary terms without requiring manual tuning or data content access. The MDG-MICL (Metadata Description Generation via Multi-Shot In-Context Learning) approach achieved up to 81.33% Hit@5 accuracy on a real-world enterprise dataset, significantly outperforming traditional similarity-based methods.

## Method Summary
The approach uses LLMs to generate enriched descriptions of column headers, which are then matched to glossary entries using sentence embeddings. The method includes multiple variants: MDG-MICL (using in-context learning with demonstrations), MDG-Cl (classification), MDG-MCQA (multiple-choice question answering), and direct inference methods (DI-Cl, DI-MCQA). Demonstrations for ICL are constructed from a human feedback bank using SBERT embeddings. The system shortlists relevant glossary items using SBERT k-nearest neighbors before applying LLM-based matching methods.

## Key Results
- MDG-MICL achieved 81.33% Hit@5 accuracy, outperforming baseline cosine similarity methods
- Hit@5 rate increased with the number of demonstrations in the MDG-MICL method
- Direct inference methods (DI-Cl and DI-MCQA) performed worst due to LLM biases toward certain class labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can infer semantic relations between cryptic column headers and business glossary descriptions even without access to table contents.
- Mechanism: By leveraging in-context learning with demonstrations from a human feedback bank, the LLM generates contextually enriched descriptions of column headers. These enriched descriptions are then compared to glossary entries using sentence embeddings, improving matching accuracy beyond simple string similarity.
- Core assumption: The LLM has been exposed to sufficient training data to understand domain-specific terminology and can generalize to cryptic or abbreviated column names.
- Evidence anchors:
  - [abstract] "leverage the power of large language models (LLMs) to design generic matching methods that do not require manual tuning and can identify complex relations between column names and glossaries"
  - [section 3.2] "LLMs perform extraordinarily well on instruction-based tasks as long as these tasks can be represented in natural language"
  - [corpus] Weak - related work focuses on text-to-SQL or column name expansion, not glossary matching
- Break condition: Performance degrades if the glossary contains highly domain-specific jargon not well-represented in LLM pretraining data, or if column headers are too ambiguous for contextual inference.

### Mechanism 2
- Claim: Multi-shot in-context learning consistently improves matching accuracy by providing the LLM with relevant demonstrations.
- Mechanism: Demonstrations constructed from the human feedback bank using SBERT embeddings guide the LLM to generate more accurate and domain-relevant descriptions of column headers, leading to better semantic matching with glossary terms.
- Core assumption: The human feedback bank contains sufficiently diverse and representative examples to cover the variability in column header formats and glossary descriptions.
- Evidence anchors:
  - [section 4.1.1] "we use ICL to improve the quality and control the format of the description generated by the LLM"
  - [section 5.2] "Hit@5 rate increases with the number of demonstrations in the MDG-MICL method"
  - [corpus] Missing - no direct corpus evidence of ICL effectiveness in this exact setup
- Break condition: If the feedback bank is too small or unrepresentative, ICL may introduce bias or fail to generalize, reducing matching accuracy.

### Mechanism 3
- Claim: Direct inference methods using LLMs for classification or multiple-choice question answering can match metadata to glossary terms without generating intermediate descriptions.
- Mechanism: The LLM directly classifies whether a glossary description matches a column header or selects the best match from a shortlist of candidates, bypassing the need for description generation.
- Core assumption: The LLM's internal representations are sufficiently rich to perform direct matching without intermediate steps, and its biases do not significantly impair accuracy.
- Evidence anchors:
  - [section 4.2] "we treat the metadata to glossary matching problem as either a Boolean classification or a Multi-class classification problem"
  - [section 5.2] "DI-Cl and DI-MCQA methods achieve the worst Hit@5 rates" (indicating mechanism limitations)
  - [corpus] Weak - related work on text-to-SQL uses LLMs for schema matching but not direct glossary inference
- Break condition: Performance suffers if LLM biases toward certain labels or answer formats dominate the matching decisions, or if the shortlist contains too many similar descriptions.

## Foundational Learning

- Concept: Large Language Models (LLMs) and their capabilities in instruction-based tasks.
  - Why needed here: The entire approach relies on LLMs to generate descriptions and perform matching without manual tuning.
  - Quick check question: What are the key differences between fine-tuning an LLM and using in-context learning for a new task?

- Concept: Sentence embeddings and semantic similarity measures.
  - Why needed here: Matching relies on comparing generated descriptions to glossary entries using cosine similarity in embedding space.
  - Quick check question: How does SBERT differ from traditional word2vec embeddings, and why is it more suitable for this task?

- Concept: In-Context Learning (ICL) and demonstration construction.
  - Why needed here: ICL is used to improve the quality of generated descriptions by providing the LLM with relevant examples.
  - Quick check question: What factors should be considered when selecting demonstrations for ICL to maximize performance?

## Architecture Onboarding

- Component map: Column headers + other column names -> LLM models -> Human feedback bank -> SBERT embeddings -> Ranked list of matching glossary terms
- Critical path:
  1. Construct prompt with metadata and ICL demonstrations (if applicable)
  2. Generate enriched description or perform direct inference using LLM
  3. Embed description and glossary terms using SBERT
  4. Compute cosine similarity and rank matches
  5. Return top-k glossary items
- Design tradeoffs:
  - ICL vs. no ICL: ICL improves accuracy but requires a quality feedback bank and increases prompt size
  - Description generation vs. direct inference: Generation adds a step but can improve accuracy; direct inference is faster but more prone to LLM biases
  - LLM model size: Larger models (e.g., Flan-T5-XXL) may improve accuracy but increase inference cost
- Failure signatures:
  - Low Hit@5/Hit@1 rates: Indicates poor semantic understanding or inadequate demonstrations
  - Inconsistent results across runs: Suggests instability in LLM generation or classification
  - High computational cost: May require optimization of prompt size or model selection
- First 3 experiments:
  1. Baseline comparison: Implement and test the baseline method (cosine similarity of column name and glossary descriptions) to establish a performance floor
  2. MDG-MICL with 0-shot and 1-shot: Test the impact of adding ICL demonstrations on matching accuracy
  3. Ablation study: Compare MDG-MICL, MDG-Cl, and MDG-MCQA to identify which description generation method performs best

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do calibration techniques specifically affect the performance of direct inference methods in metadata-to-glossary matching?
- Basis in paper: [explicit] The paper notes that biases in LLMs towards certain class labels affect the performance of direct inference methods and suggests calibration techniques as a potential solution.
- Why unresolved: The paper acknowledges the problem but does not implement or evaluate specific calibration techniques to verify their effectiveness.
- What evidence would resolve it: Experimental results comparing direct inference methods with and without various calibration techniques applied, showing measurable improvement in Hit@5 and Hit@1 rates.

### Open Question 2
- Question: What is the impact of constrained beam search on the quality of generated metadata descriptions compared to standard generation methods?
- Basis in paper: [explicit] The paper proposes using constrained beam search to generate metadata descriptions that sample words mainly from the glossary.
- Why unresolved: The paper mentions this as a potential future direction but does not implement or test it.
- What evidence would resolve it: Comparative experiments showing Hit@5 and Hit@1 rates for metadata description generation methods with and without constrained beam search.

### Open Question 3
- Question: How does the size of the demonstration set in MDG-MICL affect the trade-off between performance improvement and computational cost?
- Basis in paper: [explicit] The paper shows that Hit@5 rate increases with the number of demonstrations in MDG-MICL, but does not analyze the computational cost implications.
- Why unresolved: The paper does not provide data on inference costs or analyze the point of diminishing returns.
- What evidence would resolve it: Detailed analysis of inference time and cost per query as a function of demonstration set size, along with corresponding performance metrics.

## Limitations

- The approach relies heavily on the quality and representativeness of the human feedback bank for in-context learning demonstrations
- Direct inference methods showed poor performance due to LLM biases, limiting their practical utility
- Evaluation was conducted on a single enterprise dataset, potentially limiting generalizability

## Confidence

- **High confidence**: The MDG-MICL approach demonstrates consistent improvement with additional demonstrations and outperforms baseline similarity methods
- **Medium confidence**: The comparative analysis of different LLM-based methods is well-supported, though specific prompt templates achieving best results were not fully specified
- **Medium confidence**: The failure analysis of direct inference methods is compelling, but could benefit from more extensive testing across different LLM architectures

## Next Checks

1. Test the MDG-MICL approach on additional enterprise datasets with varying terminology complexity to assess generalizability
2. Conduct an ablation study on demonstration selection strategies to determine optimal diversity and relevance criteria for the human feedback bank
3. Evaluate the approach with larger LLM models (beyond Flan-T5-XXL) to determine if performance improvements continue with model scale