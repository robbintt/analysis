---
ver: rpa2
title: Large Language Models are not Fair Evaluators
arxiv_id: '2305.17926'
source_url: https://arxiv.org/abs/2305.17926
tags:
- evaluation
- chatgpt
- gpt-4
- bias
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models are widely used to evaluate other models,
  but we found that they suffer from a significant positional bias: simply swapping
  the order of candidate responses can reverse evaluation outcomes. This bias is especially
  pronounced in ChatGPT, which favors the second response, while GPT-4 tends to favor
  the first.'
---

# Large Language Models are not Fair Evaluators

## Quick Facts
- arXiv ID: 2305.17926
- Source URL: https://arxiv.org/abs/2305.17926
- Authors: 
- Reference count: 7
- Key outcome: Large language models exhibit significant positional bias when evaluating responses, favoring responses based on their order of appearance, but this bias can be mitigated using Multiple Evidence Calibration and Balanced Position Calibration strategies.

## Executive Summary
Large language models (LLMs) are increasingly used to evaluate other models, but this paper reveals a critical flaw: LLMs exhibit significant positional bias when comparing responses, with ChatGPT favoring the second response and GPT-4 favoring the first. This bias can be easily exploited by simply changing the order of responses, potentially reversing evaluation outcomes. The authors propose two calibration strategies—Multiple Evidence Calibration (MEC) and Balanced Position Calibration (BPC)—that significantly reduce this bias and improve alignment with human judgments.

## Method Summary
The study investigates positional bias in LLM evaluators by having GPT-4 and ChatGPT compare responses from Vicuna-13b and ChatGPT models. The authors annotate 80 samples from the Vicuna Benchmark using three human annotators, then implement two calibration strategies: Multiple Evidence Calibration (requiring the evaluator to generate detailed evidence before scoring) and Balanced Position Calibration (aggregating results across different response orders). Performance is measured using accuracy and kappa correlation coefficient compared to human annotations.

## Key Results
- ChatGPT exhibits a strong preference for the second response, while GPT-4 consistently favors the first
- Using both MEC and BPC together improved ChatGPT's accuracy by 14.3% and kappa by 0.25
- The calibration strategies significantly reduce positional bias and improve alignment with human judgments

## Why This Works (Mechanism)

### Mechanism 1: Positional Bias in LLMs
- Claim: LLMs exhibit a systematic preference for responses based on their order of presentation, leading to inconsistent evaluation outcomes.
- Mechanism: The autoregressive nature of LLMs causes them to generate evaluations based on the immediate context, which includes the order of responses. This leads to a bias where the first or second response is favored, depending on the specific model (e.g., GPT-4 favors the first, ChatGPT favors the second).
- Core assumption: The evaluation context provided to the LLM influences its judgment, and this influence is not mitigated by explicit instructions to ignore order.
- Evidence anchors:
  - [abstract] "We find that the quality ranking of candidate responses can be easily hacked by simply altering their order of appearance in the context."
  - [section 2.2] "GPT-4 tends to favor the first displayed candidate answer by consistently assigning it higher scores, even when the order of candidates is subtly altered."
  - [corpus] Weak evidence; related papers discuss biases in LLM evaluations but do not specifically address positional bias.

### Mechanism 2: Multiple Evidence Calibration (MEC)
- Claim: Generating multiple evaluation evidence chains before scoring reduces positional bias and improves alignment with human judgments.
- Mechanism: By requiring the LLM to first generate detailed evaluation evidence and then score based on that evidence, the evaluation becomes more grounded and less susceptible to positional bias. Ensembling multiple evidence chains further stabilizes the result.
- Core assumption: The LLM's autoregressive generation can be leveraged to first create a more thorough evaluation context, which then influences the scoring in a more balanced way.
- Evidence anchors:
  - [abstract] "Multiple Evidence Calibration, which requires the evaluator model to generate multiple evaluation evidence before assigning ratings"
  - [section 3] "We prompt the model to generate evaluation evidence before assigning scores, leveraging the inherent properties of causal language models for calibration."
  - [corpus] No direct evidence in related papers; this is a novel approach proposed by the authors.

### Mechanism 3: Balanced Position Calibration (BPC)
- Claim: Aggregating results across different response orders reduces positional bias by averaging out the preference for a particular position.
- Mechanism: By evaluating each candidate in both positions across two runs and computing the final score as the average, the impact of positional bias is mitigated.
- Core assumption: Positional bias is consistent within a single evaluation but can be averaged out by swapping positions and aggregating results.
- Evidence anchors:
  - [abstract] "Balanced Position Calibration, which aggregates results across different orders to determine the final score"
  - [section 3] "BPC conducts two rounds of scoring for each sample by swapping the position of two answers. To determine the final score for a particular answer, we calculate the average score between its performance as the first response and as the second response."
  - [corpus] No direct evidence in related papers; this is a novel approach proposed by the authors.

## Foundational Learning

- Concept: Autoregressive models and their sensitivity to context
  - Why needed here: Understanding how LLMs generate responses based on the immediate context is crucial to explaining why they exhibit positional bias.
  - Quick check question: How does the autoregressive nature of LLMs influence their evaluation of responses presented in different orders?

- Concept: Calibration techniques in machine learning
  - Why needed here: MEC and BPC are both calibration strategies that aim to reduce bias in LLM evaluations.
  - Quick check question: What are the key differences between Multiple Evidence Calibration and Balanced Position Calibration, and how do they each address positional bias?

- Concept: Human evaluation and alignment metrics
  - Why needed here: The effectiveness of the proposed calibration strategies is measured by their alignment with human judgments.
  - Quick check question: Why is human evaluation considered the gold standard for assessing the quality of LLM-generated responses, and how do the proposed strategies aim to align with it?

## Architecture Onboarding

- Component map: Input responses -> LLM evaluator -> Calibration strategy (MEC/BPC/both) -> Evaluation evidence generation -> Scoring -> Output comparison result
- Critical path:
  1. Receive two candidate responses
  2. Apply calibration strategy (MEC, BPC, or both)
  3. Generate evaluation evidence
  4. Score responses based on evidence
  5. Aggregate results (for BPC)
  6. Output final comparison result
- Design tradeoffs:
  - MEC vs. BPC: MEC focuses on generating more thorough evidence before scoring, while BPC focuses on averaging out positional bias by swapping positions.
  - Cost vs. accuracy: Using MEC with multiple evidence chains increases API costs but may improve accuracy.
  - Simplicity vs. effectiveness: The proposed strategies are simple but effective, but more complex strategies might yield better results at the cost of increased complexity.
- Failure signatures:
  - Inconsistent evaluation results across different orders
  - Low alignment with human judgments
  - High variance in evaluation scores
- First 3 experiments:
  1. Test the baseline evaluation without any calibration strategies to establish the extent of positional bias.
  2. Apply MEC with different numbers of evidence chains to determine the optimal number for balancing accuracy and cost.
  3. Apply BPC to evaluate its effectiveness in reducing positional bias compared to the baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which positional bias affects the scoring decisions of language models?
- Basis in paper: [explicit] The paper identifies positional bias but does not fully explain the underlying cause.
- Why unresolved: The study observes the phenomenon but does not delve into the cognitive or algorithmic reasons behind why models favor responses in certain positions.
- What evidence would resolve it: Detailed analysis of the internal attention mechanisms and decision-making processes of the models when evaluating responses in different positions.

### Open Question 2
- Question: How does the complexity of the evaluation task influence the degree of positional bias?
- Basis in paper: [inferred] The paper notes that positional bias varies with the quality difference of responses, suggesting task complexity might play a role.
- Why unresolved: The study does not systematically vary task complexity to measure its impact on positional bias.
- What evidence would resolve it: Experiments comparing positional bias across tasks of varying complexity, from simple factual questions to complex reasoning tasks.

### Open Question 3
- Question: Are there specific linguistic features or response characteristics that exacerbate or mitigate positional bias?
- Basis in paper: [explicit] The paper discusses the influence of response quality on positional bias but does not explore specific features.
- Why unresolved: The study does not analyze the content or structure of responses to identify features that might influence bias.
- What evidence would resolve it: Analysis of response features (e.g., length, complexity, use of specific terms) correlated with positional bias in evaluations.

### Open Question 4
- Question: How does the size and architecture of the language model affect its susceptibility to positional bias?
- Basis in paper: [inferred] The paper compares GPT-4 and ChatGPT, noting differences in bias, but does not explore the role of model size or architecture.
- Why unresolved: The study does not systematically test models of different sizes or architectures to determine their impact on bias.
- What evidence would resolve it: Comparative studies using models of varying sizes and architectures to evaluate their positional bias.

### Open Question 5
- Question: What is the long-term impact of positional bias on the reliability of large-scale model evaluations?
- Basis in paper: [explicit] The paper highlights the issue of positional bias but does not discuss its implications for large-scale evaluations.
- Why unresolved: The study does not simulate or analyze the effects of positional bias over many evaluations.
- What evidence would resolve it: Longitudinal studies or simulations showing the cumulative impact of positional bias on evaluation outcomes.

## Limitations

- The study uses only 80 annotated samples from the Vicuna Benchmark, which may not capture the full spectrum of response quality variations or positional bias patterns across different domains.
- While MEC and BPC show improvements on the Vicuna dataset, their effectiveness on other evaluation tasks (such as code generation, mathematical reasoning, or multi-turn conversations) remains untested.
- The exact reasons why ChatGPT favors second responses while GPT-4 favors first responses are not fully explained, potentially due to differences in training data, fine-tuning procedures, or inherent model architectures.

## Confidence

**High Confidence** - The existence of positional bias in LLM evaluators is well-supported by experimental results showing significant differences in evaluation outcomes based on response order.

**Medium Confidence** - The effectiveness of calibration strategies (MEC and BPC) is demonstrated on the Vicuna dataset, but their general applicability to other evaluation scenarios requires further validation.

**Low Confidence** - The proposed mechanisms explaining why positional bias occurs and how calibration strategies mitigate it are based on reasonable assumptions but lack comprehensive theoretical grounding.

## Next Checks

1. **Cross-Dataset Validation** - Test the proposed calibration strategies on at least two additional evaluation datasets from different domains (e.g., code generation, mathematical reasoning) to assess generalizability.

2. **Model-Specific Analysis** - Conduct controlled experiments to isolate whether positional bias is primarily due to model architecture (autoregressive generation) or training data differences between GPT-4 and ChatGPT.

3. **Long-Term Stability Testing** - Evaluate whether the calibration strategies maintain their effectiveness over time as the underlying LLM models are updated or fine-tuned by their providers.