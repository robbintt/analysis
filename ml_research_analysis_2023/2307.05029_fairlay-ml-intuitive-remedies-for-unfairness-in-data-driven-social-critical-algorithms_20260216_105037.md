---
ver: rpa2
title: 'FairLay-ML: Intuitive Remedies for Unfairness in Data-Driven Social-Critical
  Algorithms'
arxiv_id: '2307.05029'
source_url: https://arxiv.org/abs/2307.05029
tags:
- none
- data
- feature
- fairness
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This thesis explores open-sourced machine learning (ML) model\
  \ explanation tools to determine whether they can enable laypeople to visualize,\
  \ understand, and suggest intuitive remedies for unfairness in ML-based decision-support\
  \ systems. Machine learning models trained on biased datasets are increasingly used\
  \ for life-altering decisions, making it critical for the general public\u2014not\
  \ just experts\u2014to understand unfairness in these algorithms."
---

# FairLay-ML: Intuitive Remedies for Unfairness in Data-Driven Social-Critical Algorithms

## Quick Facts
- arXiv ID: 2307.05029
- Source URL: https://arxiv.org/abs/2307.05029
- Reference count: 0
- One-line primary result: FairLay-ML integrates LIME, Streamlit, and Plotly to create a GUI that explains model predictions and provides remedies by masking correlated features.

## Executive Summary
This thesis explores open-sourced machine learning model explanation tools to determine whether they can enable laypeople to visualize, understand, and suggest intuitive remedies for unfairness in ML-based decision-support systems. Machine learning models trained on biased datasets are increasingly used for life-altering decisions, making it critical for the general public—not just experts—to understand unfairness in these algorithms. The study presents FairLay-ML, a proof-of-concept GUI integrating tools like Local Interpretable Model-Agnostic Explanations (LIME) with existing ML-focused GUIs like Streamlit. Tested on models generated by Parfait-ML, FairLay-ML provides real-time black-box explanations and actionable remedies. Out of 24 unfair models studied, clear explanations were provided for 4, leading to fairer models without accuracy loss. For example, masking the "native country" feature in one model reduced racial bias by decreasing prediction probability for individuals from South Africa.

## Method Summary
FairLay-ML is a proof-of-concept GUI that integrates LIME, Streamlit, and Plotly to explain model predictions and suggest remedies for unfairness. The framework loads models generated by Parfait-ML, uses LIME to generate feature importance explanations, and displays them through Streamlit with Plotly visualizations. The method includes masking correlated proxy features to reduce bias, then retraining and evaluating fairness metrics. Tested on 24 unfair models from tabular datasets, the approach successfully improved fairness without accuracy loss in four cases.

## Key Results
- FairLay-ML provided clear explanations for unfairness in 4 out of 24 studied unfair models
- Masking correlated features (e.g., "native country") reduced racial bias by 0.02 prediction probability without accuracy loss
- The approach successfully integrated LIME explanations with Streamlit and Plotly visualizations for interactive fairness analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating LIME with Streamlit enables real-time, model-agnostic explanations that laypeople can interpret.
- Mechanism: LIME perturbs input features and observes changes in model probability, producing feature importance scores. Streamlit serves these scores interactively via a web UI, letting users click points and instantly see explanations without needing technical expertise.
- Core assumption: Users can understand probability changes and feature correlations from simple bar or point plots.
- Evidence anchors:
  - [abstract] states LIME is integrated with Streamlit to provide real-time black-box explanations.
  - [section 3.6] explains LIME takes a point, perturbs features, and plots changes in prediction probability.
  - [corpus] lacks direct evidence of LIME-Streamlit usability testing; only the conceptual pairing is described.
- Break condition: If users misinterpret feature importance bars or the perturbation logic, explanations lose clarity.

### Mechanism 2
- Claim: Masking correlated proxy features reduces unfairness without hurting accuracy.
- Mechanism: After identifying features highly correlated with sensitive attributes (via histograms and correlation metrics), those feature categories are collapsed to a single value, the model is retrained, and fairness metrics (AOD, group/causal scores) improve.
- Core assumption: The proxy feature is not essential for accurate predictions and its removal does not degrade performance.
- Evidence anchors:
  - [abstract] reports masking native country reduced racial bias by 0.02 probability without accuracy loss.
  - [section 2.4] describes masking categories in features correlated with sensitive attributes as a remedy.
  - [section 4.1.2] lists four examples where masking led to accuracy stability and AOD reduction.
- Break condition: If the proxy is actually predictive, masking will degrade accuracy.

### Mechanism 3
- Claim: Plotly graphs summarizing high-dimensional relationships plus on-demand detail prevents cognitive overload.
- Mechanism: Summaries (e.g., correlation bars, scatterplots of accuracy vs. AOD) are plotted first; clicking a point triggers detailed histograms or model logic views, keeping interface clean yet informative.
- Core assumption: Users prefer to explore details interactively rather than see everything at once.
- Evidence anchors:
  - [section 3.3.2] describes generating ambiguous summaries with Plotly and allowing users to click for details.
  - [section 3.4] and [section 3.5] illustrate this approach for feature correlations and model logic.
  - [corpus] contains no usability study data confirming reduced overload.
- Break condition: If users never click for details, important information remains hidden.

## Foundational Learning

- Concept: Local Interpretable Model-Agnostic Explanations (LIME)
  - Why needed here: Provides feature importance scores for any black-box model, enabling intuitive fairness explanations.
  - Quick check question: How does LIME determine which features influence a prediction the most?

- Concept: Average Odd Difference (AOD)
  - Why needed here: Quantifies fairness across protected groups by comparing true/false positive rate differences.
  - Quick check question: What does a lower AOD score indicate about a model's fairness?

- Concept: Counterfactual Fairness
  - Why needed here: Measures whether changing a sensitive attribute changes the prediction, capturing individual-level bias.
  - Quick check question: How is a counterfactual example generated in fairness testing?

## Architecture Onboarding

- Component map: Frontend (Streamlit UI) -> Backend (Python model loader, LIME explainer, Plotly visualizer) -> Data (Parfait-ML saved models + datasets) -> Validation (Themis fairness tests)
- Critical path: User selects model -> Backend loads model & metadata -> LIME generates explanations -> Plotly renders interactive graphs -> User clicks for detail -> Updated fairness metrics displayed
- Design tradeoffs: Streamlit prioritizes rapid development and sharing over runtime scalability; Plotly offers interactivity but adds rendering overhead; LIME is model-agnostic but computationally heavy for large feature spaces.
- Failure signatures: Slow UI response -> model loading bottleneck; missing explanations -> LIME failure on unsupported model type; incorrect fairness scores -> mismatched preprocessing between training and testing.
- First 3 experiments:
  1. Load a simple logistic regression model and generate LIME explanations for a fixed data point; verify Plotly bar chart appears.
  2. Mask a known proxy feature (e.g., native country) and retrain; confirm accuracy stays within ±0.02 and AOD decreases.
  3. Run Themis on original vs. masked model; check that group discrimination score improves.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FairLay-ML effectively scale to explain unfairness in high-dimensional datasets and complex deep learning models beyond classical models?
- Basis in paper: [explicit] The paper notes that FairLay-ML successfully explains unfairness in classical models (logistic regression, decision trees, random forests, SVM) but acknowledges that scaling to high-dimensional data like images and language datasets presents further difficulty.
- Why unresolved: The study only tested FairLay-ML on tabular benchmark datasets with relatively low dimensionality (10-20 features). The authors explicitly state that scaling to high-dimensional applications like images, language datasets, and deep neural networks remains beyond the scope of this thesis.
- What evidence would resolve it: Testing FairLay-ML on diverse high-dimensional datasets (e.g., ImageNet for computer vision, IMDB for natural language processing) and complex deep learning architectures, then evaluating explanation quality and computational efficiency.

### Open Question 2
- Question: How do Themis' group discrimination score and causal discrimination score relate to each other, and under what circumstances do they diverge?
- Basis in paper: [explicit] The authors note contradictory results between Themis' group score and causal score, stating "a more in-depth analysis of the connection between the group score and the causal score is needed."
- Why unresolved: The study observed discrepancies between these two fairness metrics when evaluating remedied models, but did not provide a theoretical framework for understanding when and why these metrics might produce conflicting results.
- What evidence would resolve it: A systematic study examining cases where group and causal scores diverge, analyzing the mathematical relationships between these metrics, and developing guidelines for interpreting conflicting results.

### Open Question 3
- Question: What is the relationship between model hyperparameters and fairness outcomes, and how can hyperparameter tuning be optimized for fairness?
- Basis in paper: [inferred] The authors suggest that "an analysis can be performed on models with different hyperparameters regarding how each model perpetrates unfairness" and mention that Parfait-ML generates models with varying hyperparameters.
- Why unresolved: While the study demonstrated that FairLay-ML can explain unfairness in models with different hyperparameters, it did not investigate how specific hyperparameter choices directly impact fairness or develop strategies for hyperparameter optimization.
- What evidence would resolve it: A comprehensive study correlating specific hyperparameter values with fairness metrics across multiple model types and datasets, potentially leading to fairness-aware hyperparameter optimization techniques.

## Limitations
- Proof-of-concept framework with demonstrated effectiveness on only 4 out of 24 studied models
- Lack of user study validation to confirm lay interpretability of explanations
- Missing implementation details for feature masking procedure and unspecified model hyperparameters

## Confidence
- Core mechanism (LIME + Streamlit integration): Medium
- Masking remedy effectiveness: Medium (dependent on proxy feature being non-essential)
- Scalability to complex models: Low (not tested beyond classical models)
- Reproducibility: Medium (major steps specified but key details missing)

## Next Checks
1. Conduct usability testing with laypeople to verify explanation clarity and remedy comprehension
2. Test the masking procedure across multiple model architectures to confirm accuracy stability
3. Perform systematic sensitivity analysis on feature correlation thresholds used for proxy detection