---
ver: rpa2
title: Transfer Attacks and Defenses for Large Language Models on Coding Tasks
arxiv_id: '2311.13445'
source_url: https://arxiv.org/abs/2311.13445
tags:
- code
- adversarial
- llms
- prompt
- self
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether adversarial examples generated on
  smaller code models can transfer to large language models (LLMs) for coding tasks.
  It finds that adversarial examples generated using the white-box attack from Srikant
  et al.
---

# Transfer Attacks and Defenses for Large Language Models on Coding Tasks

## Quick Facts
- arXiv ID: 2311.13445
- Source URL: https://arxiv.org/abs/2311.13445
- Reference count: 4
- Key outcome: Adversarial examples from smaller seq2seq models successfully transfer to LLMs (21-53% ASR); prompt-based defenses reduce attack success.

## Executive Summary
This paper investigates whether adversarial examples crafted on smaller code models can transfer to large language models (LLMs) for coding tasks. The authors find that adversarial examples generated using a white-box attack on a seq2seq model successfully transfer to five LLMs including GPT-3.5, GPT-4, Claude-1, Claude-2, and CodeLlama. To defend against these transfer attacks, the paper proposes two prompt-based defense strategies: Few-Shot Defense (FSD) and Inverse Transformation Defense (InvD). These defenses leverage LLMs' in-context learning and instruction-following capabilities to improve resilience without requiring model retraining.

## Method Summary
The study generates adversarial examples using Srikant et al.'s 2021 white-box attack on a seq2seq model trained on Python code snippets. These examples are then tested for transferability on five LLMs using code summarization tasks. The authors evaluate baseline transferability by measuring attack success rates (ASR), then implement two prompt-based defense strategies: Few-Shot Defense (incorporating adversarial examples into few-shot learning) and Inverse Transformation Defense (explicit instructions to reverse semantic-preserving perturbations). The paper also introduces meta-prompting, where an LLM generates its own defensive prompts, potentially leading to more effective defenses.

## Key Results
- Adversarial examples from seq2seq successfully transfer to LLMs with ASR ranging from 21% to 53%
- Prompt-based defenses reduce ASR, with Few-Shot Defense showing better overall effectiveness
- Meta-prompting produces LLM-generated defensive prompts that outperform manually crafted ones
- GPT-4-generated prompts show higher ASR reduction than GPT-3.5-generated prompts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial examples generated on smaller seq2seq code models successfully transfer to larger LLMs for coding tasks.
- Mechanism: The transfer attack leverages the fact that both seq2seq and LLMs share underlying transformer architectures and common semantic-preserving perturbation patterns, allowing adversarial examples crafted for seq2seq to "fool" LLMs despite their size difference.
- Core assumption: The adversarial perturbations (dead code, print statements, variable renaming) exploit vulnerabilities common to both seq2seq and LLMs in code understanding tasks.
- Evidence anchors:
  - [abstract] states "adversarial examples obtained with a smaller code model are indeed transferable to multiple LLMs"
  - [section 4.1] explains "we investigate the transferability of adversarial examples as crafted by a state-of-the-art white-box attack on the more specialized, smaller code model seq2seq"
  - [corpus] provides no direct evidence but related work on adversarial transferability supports this mechanism
- Break condition: If LLMs employ fundamentally different reasoning or representations for code compared to seq2seq, transfer may fail.

### Mechanism 2
- Claim: Prompt-based defenses can mitigate transfer attacks without retraining LLMs.
- Mechanism: By modifying prompts to include instructions for reversing semantic-preserving perturbations or examples of adversarial code, LLMs leverage their in-context learning and instruction-following capabilities to neutralize attacks before summarization.
- Core assumption: LLMs can understand natural language instructions and perform code transformations as directed within the prompt context.
- Evidence anchors:
  - [abstract] mentions "prompt-based defenses that involve modifying the prompt to include additional information such as examples of adversarially perturbed code and explicit instructions for reversing adversarial perturbations"
  - [section 4.2] details "we propose cost-effective defenses that are prompt-based, i.e., defenses that modify the prompt to defend against adversarial attacks"
  - [corpus] shows related work on prompt-based defenses but not specifically for code transfer attacks
- Break condition: If LLMs cannot reliably perform the requested code transformations or ignore prompt instructions, defenses fail.

### Mechanism 3
- Claim: LLM-generated defensive prompts via meta-prompting are more effective than manually crafted ones.
- Mechanism: By providing examples of original and perturbed code to an LLM and asking it to generate defensive prompts, the LLM leverages its generative capabilities to create more comprehensive and effective instructions than humans might manually construct.
- Core assumption: LLMs can generate superior defensive prompts by synthesizing patterns from examples that humans might overlook.
- Evidence anchors:
  - [abstract] states "we present meta-prompting where we ask the LLM itself to generate the self-defense prompts"
  - [section 5] shows "the LLM-generated prompts seem much more effective than the manually engineered prompts in defending against adversarial attacks"
  - [corpus] provides no direct evidence for meta-prompting effectiveness
- Break condition: If LLM-generated prompts are not better than manual ones or introduce new vulnerabilities, this mechanism breaks.

## Foundational Learning

- Concept: Transferability of adversarial examples between models of different sizes
  - Why needed here: Understanding how attacks crafted on smaller models can affect larger ones is central to assessing LLM vulnerability
  - Quick check question: Why might adversarial examples transfer from seq2seq to GPT-4 despite the parameter difference?
- Concept: Prompt-based defenses leveraging in-context learning
  - Why needed here: The paper's defensive strategies rely on LLMs' ability to learn from examples within prompts
  - Quick check question: How do few-shot examples in prompts help defend against adversarial code?
- Concept: Meta-prompting and LLM self-generation capabilities
  - Why needed here: The paper's most effective defense involves LLMs generating their own defensive prompts
  - Quick check question: What advantage does letting an LLM generate its own defensive prompt provide?

## Architecture Onboarding

- Component map: seq2seq model (adversarial example generator) -> LLMs (GPT-3.5, GPT-4, Claude-1, Claude-2, CodeLlama) (evaluation targets) -> Prompt-based defenses (FSD, InvD, meta-prompting) -> ASR measurement
- Critical path: Generate adversarial examples on seq2seq → Test transferability on LLMs → Apply prompt-based defenses → Evaluate defense effectiveness → Optimize via meta-prompting
- Design tradeoffs: Manual prompts are faster to create but less effective; meta-prompting is more effective but requires LLM queries; few-shot examples improve accuracy but increase prompt length
- Failure signatures: Defenses failing when LLMs cannot understand instructions, when adversarial examples are too complex to reverse, or when prompt context windows are exceeded
- First 3 experiments:
  1. Test baseline transferability by running seq2seq-generated adversarial examples on each LLM without defenses
  2. Apply manual few-shot defense prompts and measure ASR reduction
  3. Implement meta-prompting with GPT-4 to generate defensive prompts and compare effectiveness against manual prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adversarial examples generated on smaller code models successfully transfer to large language models (LLMs) for coding tasks?
- Basis in paper: [explicit] The paper directly investigates this transferability and finds that adversarial examples generated using a white-box attack on the seq2seq model successfully transfer to five LLMs with attack success rates (ASRs) ranging from 21% to 53%.
- Why unresolved: While the paper demonstrates successful transferability, it doesn't explore the limits of this transferability or the factors that influence it.
- What evidence would resolve it: Further experiments testing transferability across a wider range of LLM architectures, code models, and attack types would help establish the boundaries of this phenomenon.

### Open Question 2
- Question: How effective are prompt-based defenses in improving the resilience of LLMs to transfer attacks on coding tasks?
- Basis in paper: [explicit] The paper proposes and evaluates two prompt-based defense strategies (Few-Shot Defense and Inverse Transformation Defense) and finds that they reduce ASRs, with Few-Shot Defense being more effective overall.
- Why unresolved: The paper doesn't explore the optimal design of prompt-based defenses or their effectiveness against more sophisticated attacks.
- What evidence would resolve it: Experiments testing different prompt structures, defensive strategies, and attack types would help determine the most effective defense approaches.

### Open Question 3
- Question: Can meta-prompting, where an LLM generates its own defense prompt, lead to more effective defenses against transfer attacks on coding tasks?
- Basis in paper: [explicit] The paper introduces meta-prompting and finds that LLM-generated defense prompts lead to further improvements in resilience, with GPT-4 generating more effective prompts than manually crafted ones.
- Why unresolved: The paper doesn't explore the limitations of meta-prompting or its effectiveness against different types of attacks.
- What evidence would resolve it: Experiments testing meta-prompting against a wider range of attacks and comparing its effectiveness to other defense strategies would help determine its potential and limitations.

## Limitations

- The paper only tests transferability for semantic-preserving perturbations, leaving open whether more complex attack types would transfer
- Defense evaluation is limited to two specific prompt-based strategies without exploring the full design space of potential defenses
- Meta-prompting results are based on a small sample size, making it unclear whether observed improvements would scale

## Confidence

- **High confidence**: The basic finding that adversarial examples transfer from seq2seq to LLMs is well-supported by the experimental results across five different models.
- **Medium confidence**: The effectiveness of prompt-based defenses is demonstrated but the evaluation methodology could be more rigorous, particularly regarding baseline comparisons and ablation studies.
- **Low confidence**: The meta-prompting results are intriguing but based on limited testing, making it difficult to assess whether the observed improvements are robust or specific to the particular prompts tested.

## Next Checks

1. Conduct ablation studies to isolate the contribution of different prompt elements (few-shot examples vs. instructions) in the defense strategies.
2. Test transferability across a broader range of perturbation types beyond the semantic-preserving transformations used in this study.
3. Evaluate defense effectiveness when the adversarial examples are generated using black-box attacks rather than white-box attacks on the seq2seq model.