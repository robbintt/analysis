---
ver: rpa2
title: 'DistillCSE: Distilled Contrastive Learning for Sentence Embeddings'
arxiv_id: '2310.13499'
source_url: https://arxiv.org/abs/2310.13499
tags:
- teacher
- logits
- distillation
- learning
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DistillCSE addresses the problem of limited information learned
  in contrastive learning for sentence embeddings, which arises from the simplistic
  construction of positive and negative sample pairs. The proposed framework, DistillCSE,
  performs contrastive learning under a self-training paradigm with knowledge distillation.
---

# DistillCSE: Distilled Contrastive Learning for Sentence Embeddings

## Quick Facts
- arXiv ID: 2310.13499
- Source URL: https://arxiv.org/abs/2310.13499
- Reference count: 26
- DistillCSE-BERTbase achieves 79.09% average Spearman score on STS tasks

## Executive Summary
DistillCSE addresses the fundamental problem of limited information learned in contrastive learning for sentence embeddings, which arises from the simplistic construction of positive and negative sample pairs. The proposed framework performs contrastive learning under a self-training paradigm with knowledge distillation, using a base model to provide additional supervision signals that allow a stronger model to be learned through distillation. By introducing Group-P shuffling as implicit regularization and averaging logits from multiple teacher components, DistillCSE effectively mitigates the high variance in teacher model logits that typically leads to overfitting in contrastive learning distillation. Experiments on standard benchmarks demonstrate that DistillCSE outperforms many strong baseline methods and achieves state-of-the-art performance.

## Method Summary
DistillCSE implements a self-training framework with knowledge distillation where a teacher model (trained via SimCSE contrastive learning) provides logits for training a student model. The framework addresses the high variance in teacher logits through two key innovations: Group-P shuffling, which introduces noise by randomly shuffling teacher logits within magnitude-based groups, and logit averaging, which combines logits from multiple teacher components to reduce variance according to the Central Limit Theorem. The distillation process uses cross-entropy loss between student and teacher logits, with the student model subsequently serving as the teacher for iterative improvement. The framework is evaluated on standard sentence similarity benchmarks including STS12-STS16, STS-B, and SICK-R.

## Key Results
- DistillCSE-BERTbase achieves 79.09% average Spearman score on STS tasks
- Outperforms distillation baseline by 0.83% and SimCSE teacher by 0.25%
- Demonstrates effectiveness of Group-P shuffling and logit averaging in reducing overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge distillation in contrastive learning suffers from high variance in teacher logits due to the pairwise nature of similarity scores, leading to overfitting.
- Mechanism: When logits are computed as pairwise cosine similarities (2nd-order logits), their variance is magnified compared to 1st-order logits from single examples, causing instability in the distillation process.
- Core assumption: The variance in similarity logits is significantly higher than the variance in individual embeddings, and this variance leads to overfitting.
- Evidence anchors:
  - [abstract] "the standard knowledge distillation exhibits a relatively large variance of the teacher model’s logits due to the essence of contrastive learning."
  - [section] "Since ti,j represents the second-order logits for data samples, its variance theoretically corresponds to the second order of the variance in embeddings."
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.45" (weak evidence, corpus is sparse)

### Mechanism 2
- Claim: Group-P shuffling of teacher logits introduces noise that prevents the student model from overfitting to the specific training corpus.
- Mechanism: By shuffling logits within groups defined by their magnitudes, the student cannot rely too heavily on the exact logit values from the teacher, forcing it to learn more robust representations.
- Core assumption: Introducing controlled noise in the distillation process helps prevent overfitting by reducing the student's reliance on the teacher's specific logit values.
- Evidence anchors:
  - [section] "inspired by the dropout... we could introduce an outsourced noise into distillation logits to alleviate the overfitting issue caused by high variance."
  - [section] "we randomly shuffle the teacher logits during the distillation process. The experimental results... demonstrate that shuffling a subset of the teacher logits effectively addresses the overfitting problem."
  - [corpus] "Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation" (weak, indirect relevance)

### Mechanism 3
- Claim: Averaging logits from multiple teacher components reduces the variance across different teacher models, leading to more stable distillation.
- Mechanism: According to the Central Limit Theorem, averaging M samples reduces the variance by a factor of 1/M, making the averaged logits more representative of the true similarity.
- Core assumption: The variance in logits across different teacher models is significant enough to impact distillation quality, and averaging can effectively reduce this variance.
- Evidence anchors:
  - [section] "According to the Central Limit Theorem... the M samples’ average converges to its expectation with 1/M variance of its original variance."
  - [section] "We could simply use the following mean sampling to generate the teacher logits ti,j: ti,j = 1/M Σ tm i,j"
  - [corpus] "Compressing Sentence Representation with maximum Coding Rate Reduction" (weak, indirect relevance)

## Foundational Learning

- Concept: Contrastive Learning (CL)
  - Why needed here: CL is the foundation of SimCSE, which DistillCSE builds upon. Understanding CL is crucial to grasp how positive and negative pairs are constructed and how the loss function works.
  - Quick check question: What is the main objective of contrastive learning in the context of sentence embeddings?

- Concept: Knowledge Distillation
  - Why needed here: DistillCSE uses knowledge distillation to transfer knowledge from a base model to a stronger student model. Understanding distillation is key to implementing the framework.
  - Quick check question: How does knowledge distillation differ from standard supervised learning?

- Concept: Variance and its impact on learning
  - Why needed here: The paper identifies high variance in teacher logits as a key problem. Understanding variance and its effects on model training is essential to grasp why the proposed solutions work.
  - Quick check question: How can high variance in model outputs lead to overfitting?

## Architecture Onboarding

- Component map:
  Teacher model (SimCSE) -> Logit generator -> Group-P shuffling (optional) -> Logit averaging (optional) -> Distillation loss -> Student model -> New teacher model (iterative)

- Critical path:
  1. Train teacher model using SimCSE
  2. Generate teacher logits for distillation
  3. Apply Group-P shuffling (optional)
  4. Average logits from multiple teachers (optional)
  5. Train student model using distillation loss
  6. Iterate: Use student as new teacher for further rounds

- Design tradeoffs:
  - More teacher components reduce variance but increase computational cost
  - More aggressive shuffling prevents overfitting but may hinder learning
  - Larger batch sizes reduce variance but require more memory

- Failure signatures:
  - High gap between training and testing loss indicates overfitting
  - Low correlation between student and other teachers suggests the student is overfitting to its specific teacher
  - Poor performance on STS tasks indicates the distillation process is not effective

- First 3 experiments:
  1. Implement vanilla distillation (without shuffling or averaging) to establish baseline performance and observe overfitting.
  2. Add Group-P shuffling with different p values to find the optimal balance between regularization and learning.
  3. Add logit averaging from multiple teachers to see if it improves stability and performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal group size (p) for the Group-P shuffling strategy in knowledge distillation for contrastive learning of sentence embeddings?
- Basis in paper: [explicit] The paper mentions that the optimal p value was found to be 0.1 through experimentation, but does not provide a theoretical justification for this choice.
- Why unresolved: The paper does not provide a theoretical explanation for why p=0.1 is optimal, and it's unclear how this value would generalize to other datasets or model architectures.
- What evidence would resolve it: A theoretical analysis of how group size affects the variance reduction and regularization effect in the distillation process, along with experiments on different datasets and model architectures.

### Open Question 2
- Question: How does the performance of DistillCSE scale with the number of teacher components used in the averaging logits strategy?
- Basis in paper: [inferred] The paper mentions using multiple teacher components and averaging their logits, but does not explore the effect of varying the number of teachers on performance.
- Why unresolved: The paper does not investigate how the number of teacher components affects the final performance, leaving questions about the optimal number of teachers and potential diminishing returns.
- What evidence would resolve it: Experiments varying the number of teacher components (e.g., 2, 4, 8, 16) and analyzing the trade-off between performance gains and computational cost.

### Open Question 3
- Question: Can the insights from DistillCSE be applied to other contrastive learning tasks beyond sentence embeddings, such as image or graph representation learning?
- Basis in paper: [inferred] While the paper focuses on sentence embeddings, the core issues identified (high variance in teacher logits, overfitting) are likely present in other contrastive learning scenarios.
- Why unresolved: The paper does not explore the applicability of its findings to other domains, leaving open the question of whether the proposed solutions are universally effective.
- What evidence would resolve it: Applying the DistillCSE framework to other contrastive learning tasks (e.g., image or graph representation learning) and evaluating whether the proposed solutions (Group-P shuffling and logit averaging) improve performance in these domains.

## Limitations
- The theoretical analysis of variance reduction through logit averaging lacks empirical validation across different numbers of teacher components
- The optimal Group-P shuffling parameter (p=0.1) is determined experimentally without theoretical justification
- Performance evaluation is limited to sentence similarity benchmarks without exploring generalization to other NLP tasks

## Confidence
- High confidence: The baseline distillation performance showing severe overfitting (training loss 1.3058 vs testing loss 6.5503)
- Medium confidence: The effectiveness of Group-P shuffling strategy, supported by experimental results but lacking theoretical justification for the optimal p=0.1 value
- Medium confidence: The variance reduction through logit averaging, with theoretical support but limited empirical validation across different numbers of teacher components

## Next Checks
1. Conduct ablation studies testing different numbers of teacher components (M=2, 4, 8, 16) to verify the claimed variance reduction benefits and identify optimal trade-offs between performance and computational cost
2. Implement alternative shuffling strategies (e.g., random shuffling without grouping) to determine if the specific Group-P approach provides unique benefits beyond simple noise injection
3. Test the framework on additional sentence embedding benchmarks (e.g., SentEval) to assess generalizability beyond STS tasks and validate the claimed state-of-the-art performance across diverse evaluation scenarios