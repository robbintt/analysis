---
ver: rpa2
title: 'Don''t Memorize; Mimic The Past: Federated Class Incremental Learning Without
  Episodic Memory'
arxiv_id: '2307.00497'
source_url: https://arxiv.org/abs/2307.00497
tags:
- data
- learning
- federated
- training
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in federated class-incremental
  learning without requiring clients to store past data. The authors propose a method
  where the server trains a generative model in a data-free manner using the global
  model's parameters, and clients use this model to synthesize past data during local
  training.
---

# Don't Memorize; Mimic The Past: Federated Class Incremental Learning Without Episodic Memory

## Quick Facts
- arXiv ID: 2307.00497
- Source URL: https://arxiv.org/abs/2307.00497
- Reference count: 25
- Primary result: Proposed MFCL method achieves 43.87% average accuracy and 26.52% average forgetting on CIFAR-100, outperforming FedAvg, FedProx, and FedCIL baselines in federated class-incremental learning without episodic memory.

## Executive Summary
This paper addresses catastrophic forgetting in federated class-incremental learning by proposing a method that eliminates the need for clients to store past data. The approach involves training a generative model on the server in a data-free manner using model-inversion techniques, which is then shared with clients to synthesize past data during local training. This design reduces privacy risks and memory overhead on clients while maintaining performance. Experiments on CIFAR-100 demonstrate that the method achieves 43.87% average accuracy and 26.52% average forgetting, outperforming existing baselines.

## Method Summary
The proposed method, MFCL, operates by training a generative model on the server using data-free knowledge distillation techniques at the end of each task. This generative model is then distributed to clients, who use it to synthesize past data during local training alongside real data from the current task. The approach combines synthetic and real data with feature distillation to preserve important characteristics from previous tasks while learning new ones. The server trains a global model that aggregates updates from clients, and the entire system operates within standard federated learning constraints without requiring clients to store any past data.

## Key Results
- Achieved 43.87% average accuracy on CIFAR-100 across 10 tasks
- Recorded 26.52% average forgetting, significantly better than baseline methods
- Outperformed FedAvg, FedProx, and FedCIL baselines while only modestly increasing client training time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Server-side generative model training in a data-free manner mitigates catastrophic forgetting without compromising client privacy.
- Mechanism: The server trains a generative model using model-inversion techniques that rely solely on the global model parameters, without accessing any client data. This model is then shared with clients to synthesize past task data during local training.
- Core assumption: The global model retains sufficient information about past tasks to enable effective synthetic data generation through model-inversion.
- Evidence anchors: [abstract] "The generative model is trained on the server using data-free methods at the end of each task without requesting data from clients." [section 3.1] "We propose training a generative model on the server, but in a data-free manner, i.e., by means of model-inversion image synthesis" [corpus] Weak evidence - only related work exists, no direct validation of model-inversion effectiveness
- Break condition: If the global model's parameters lose sufficient representational capacity for past tasks due to excessive forgetting, the generative model will fail to synthesize meaningful data.

### Mechanism 2
- Claim: Local training with synthetic past data and current data reduces catastrophic forgetting while maintaining plasticity.
- Mechanism: Clients train using a combination of real data from the current task and synthetic data from the generative model, with feature distillation to preserve important characteristics from previous tasks.
- Core assumption: Synthetic data generated by the server-side model can effectively approximate the distribution of real past data when combined with appropriate loss functions.
- Evidence anchors: [section 3.2] "For client-side training, inspired by (Smith et al., 2021), we distill the stability-plasticity dilemma into three critical requirements of CL" [section 3.2] "To reduce forgetting, we train the model using synthetic and real data simultaneously" [corpus] Weak evidence - effectiveness of synthetic data in federated settings not directly validated
- Break condition: If synthetic data distributions significantly deviate from real data distributions, the model may learn incorrect representations or fail to preserve old knowledge.

### Mechanism 3
- Claim: Server-side generative model training avoids client-side computational and privacy overhead.
- Mechanism: By shifting generative model training to the server, clients avoid the computational burden of training generative models and the privacy risks of storing synthetic data generation models locally.
- Core assumption: Server-side training of the generative model is feasible within the computational constraints of typical federated learning servers.
- Evidence anchors: [abstract] "The generative model is trained on the server using data-free methods at the end of each task without requesting data from clients" [section 5.1] "The server needs to train the G once per task. It is commonly assumed that the server has access to more powerful computing power" [corpus] Weak evidence - server computational feasibility not directly addressed
- Break condition: If server computational resources are insufficient to train the generative model effectively, the overall system performance will degrade.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The entire paper addresses this fundamental problem in continual learning, where models lose previously learned information when trained on new data
  - Quick check question: Why does a neural network typically perform worse on old tasks after training on new tasks?

- Concept: Knowledge distillation and feature preservation
  - Why needed here: The method uses importance-weighted feature distillation to maintain representations of old tasks while learning new ones
  - Quick check question: What is the difference between standard knowledge distillation and importance-weighted feature distillation?

- Concept: Federated learning architecture and constraints
  - Why needed here: The method operates within federated learning constraints including privacy, communication efficiency, and decentralized data
  - Quick check question: What are the key differences between centralized and federated continual learning that affect algorithm design?

## Architecture Onboarding

- Component map: Server (global model aggregation, data-free generative model training) -> Clients (local training with synthetic and real data, model updates) -> Communication (model parameters and frozen generative model distribution)

- Critical path: 1. Client local training with synthetic and real data 2. Server aggregation of client updates 3. Server training of generative model using data-free methods 4. Distribution of generative model and global model to clients

- Design tradeoffs: Server computational overhead vs client privacy and resource constraints; Synthetic data quality vs communication efficiency; Model complexity vs training stability

- Failure signatures: Performance degradation on old tasks indicates generative model inadequacy; High communication overhead suggests need for model compression; Training instability may indicate inappropriate loss function weighting

- First 3 experiments: 1. Baseline comparison with FedAvg on CIFAR-100 without forgetting mitigation 2. Server-side generative model training effectiveness test in isolation 3. Full end-to-end system performance with varying numbers of clients and tasks

## Open Questions the Paper Calls Out
None explicitly stated in the provided text.

## Limitations
- The effectiveness of data-free model-inversion techniques for generating meaningful synthetic data across all tasks remains weakly validated
- Server computational requirements for training the generative model are not rigorously analyzed
- The assumption that global model parameters retain sufficient task information for synthetic data generation may not hold in long task sequences

## Confidence
- High confidence: The overall federated learning framework and methodology description
- Medium confidence: The effectiveness of server-side generative model training in practice
- Medium confidence: The claim that this approach reduces privacy risks compared to client-side storage methods

## Next Checks
1. **Generative model fidelity test**: Evaluate synthetic data quality by measuring distribution similarity between synthetic and real data from previous tasks using metrics like FID or classifier-based evaluation
2. **Server resource profiling**: Conduct experiments measuring server computational overhead and memory usage during generative model training to verify feasibility claims
3. **Long sequence evaluation**: Test the method with extended task sequences (e.g., 20+ tasks) to identify degradation in synthetic data quality and its impact on forgetting mitigation