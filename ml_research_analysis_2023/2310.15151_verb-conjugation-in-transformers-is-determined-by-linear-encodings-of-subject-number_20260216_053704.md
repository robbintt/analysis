---
ver: rpa2
title: Verb Conjugation in Transformers Is Determined by Linear Encodings of Subject
  Number
arxiv_id: '2310.15151'
source_url: https://arxiv.org/abs/2310.15151
tags:
- number
- subject
- intervention
- verb
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study shows that BERT\u2019s ability to conjugate verbs depends\
  \ on a linear encoding of subject number that can be manipulated with predictable\
  \ effects. Using causal intervention analysis, we find that this encoding is present\
  \ in the subject position at the embedding layer, the verb position at the last\
  \ layer, and distributed across positions at middle layers, especially when there\
  \ are multiple cues to subject number."
---

# Verb Conjugation in Transformers Is Determined by Linear Encodings of Subject Number

## Quick Facts
- arXiv ID: 2310.15151
- Source URL: https://arxiv.org/abs/2310.15151
- Reference count: 20
- Primary result: BERT's verb conjugation accuracy drops from 91.7% to 84.6% incorrect when subject number encoding is manipulated through causal intervention

## Executive Summary
This study demonstrates that BERT's ability to correctly conjugate verbs depends on a linear encoding of subject number that can be systematically manipulated. Using counterfactual intervention analysis, researchers show that when this encoding is reversed through reflection across a learned number subspace, BERT makes predictable conjugation errors. The encoding is found to originate in the subject position at the embedding layer, migrate to the verb position at the final layer, and distribute across positions at middle layers, especially when multiple cues to subject number are present.

## Method Summary
The researchers use iterative nullspace projection (INLP) to identify a linear subspace encoding subject number in BERT's hidden states, then apply counterfactual intervention by reflecting hidden vectors across this subspace to reverse the encoding. They evaluate the effect on masked language modeling accuracy for verb conjugation using sentences with third-person subjects and relative clauses from the Ravfogel et al. (2021) dataset. The intervention is applied at different layers and positions to track how the encoding migrates across the network.

## Key Results
- BERT's verb conjugation accuracy drops from 91.7% correct to 84.6% incorrect after intervention
- Subject number encoding originates in the subject position at the embedding layer
- The encoding migrates to the verb position at the final layer and distributes across positions at middle layers
- Multiple cues to subject number increase distribution of the encoding across positions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The linear encoding of subject number in BERT's hidden states determines verb conjugation behavior.
- Mechanism: BERT uses a low-dimensional number subspace to represent subject number, and verb conjugation depends on the projection of hidden states onto this subspace.
- Core assumption: The grammatical number of subjects and verbs is linearly separable in the hidden representation space.
- Evidence anchors:
  - [abstract] "BERT's ability to conjugate verbs relies on a linear encoding of subject number that can be manipulated with predictable effects on conjugation accuracy."
  - [section 2] "Our hypothesis is that at least one hidden layer of BERT encodes the grammatical number of third-person subjects and verbs in a low-dimensional number subspace of the hidden representation space, where singular number is linearly separable from plural number."
- Break condition: If the linear separability assumption fails, or if BERT uses non-linear representations for subject number.

### Mechanism 2
- Claim: The location of the number encoding changes across layers and positions.
- Mechanism: The encoding originates in the subject position at the embedding layer, distributes across positions at middle layers, and migrates to the verb position at the final layer.
- Core assumption: The encoding location adapts based on the availability of redundant cues to subject number.
- Evidence anchors:
  - [abstract] "This encoding is found in the subject position at the first layer and the verb position at the last layer, but distributed across positions at middle layers, particularly when there are multiple cues to subject number."
  - [section 3] "We find that subject number encodings originate in the position of the subject at the embedding layer, and move to the position of the inflected verb at the final layer."
- Break condition: If the encoding location remains fixed regardless of redundant cues, or if it doesn't migrate to the verb position at higher layers.

### Mechanism 3
- Claim: Counterfactual intervention along the number subspace can reverse subject number encoding.
- Mechanism: By reflecting hidden vectors to the opposite side of the number subspace, the intervention encodes the opposite subject number, causing BERT to conjugate verbs incorrectly.
- Core assumption: The intensity parameter α determines the effectiveness of the intervention in reversing the encoding.
- Evidence anchors:
  - [abstract] "We predict that intervening on the hidden space by reflecting hidden vectors to the opposite side of the number subspace will cause BERT to generate plural conjugations for singular subjects, and vice versa."
  - [section 3] "Our prediction is that the counterfactual intervention... will reverse the subject number encoded in the vectors when applied with sufficient intensity (as determined by the hyperparameter α), causing BERT to conjugate the main verb of a sentence as if its subject had the opposite number."
- Break condition: If the intervention doesn't reverse the encoding even with high α values, or if it affects unrelated features.

## Foundational Learning

- Concept: Linear separability of linguistic features in hidden representations
  - Why needed here: The study assumes that subject number can be linearly separated in BERT's hidden space, which is fundamental to the causal intervention approach.
  - Quick check question: What does it mean for a feature to be linearly separable in a high-dimensional space?

- Concept: Counterfactual intervention analysis
  - Why needed here: The study uses counterfactual intervention to test whether the linear encoding of subject number actually determines BERT's verb conjugation behavior.
  - Quick check question: How does counterfactual intervention differ from traditional probing methods?

- Concept: Positional encoding and its role in transformer architectures
  - Why needed here: The study finds that the location of the number encoding changes across layers and positions, which relates to how transformers handle positional information.
  - Quick check question: Why might a transformer model distribute information across multiple positions?

## Architecture Onboarding

- Component map: BERT base model → Layer-specific hidden states → Number subspace extraction (INLP) → Counterfactual intervention → Verb conjugation accuracy measurement
- Critical path: Data preparation → INLP training → Intervention application → Conjugation accuracy calculation
- Design tradeoffs: Using linear probes for subspace extraction vs. more complex methods; global intervention vs. local intervention
- Failure signatures: Intervention doesn't affect conjugation accuracy; Linear probe fails to find separable number subspace
- First 3 experiments:
  1. Verify linear separability of subject number using INLP on training data
  2. Apply counterfactual intervention with α=2 and measure conjugation accuracy changes
  3. Test intervention at different layers and positions to map encoding location changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the linear encoding of subject number move from the subject position to the verb position across BERT layers, and what mechanisms drive this migration?
- Basis in paper: [explicit] The paper observes this positional shift but does not explain the underlying cause.
- Why unresolved: The study focuses on the existence and effects of the encoding, not the mechanisms of its positional changes.
- What evidence would resolve it: Detailed analysis of attention patterns and feature propagation across layers, showing how number information is passed from subject to verb positions.

### Open Question 2
- Question: Are there other syntactic features besides subject number that are encoded linearly in BERT's representations, and if so, which ones?
- Basis in paper: [inferred] The study demonstrates a linear encoding for subject number but does not systematically investigate other features.
- Why unresolved: The research is limited to subject number, leaving open the question of whether this is unique or part of a broader pattern.
- What evidence would resolve it: Systematic probing of BERT's representations for various syntactic features (e.g., tense, case, gender) to identify additional linear encodings.

### Open Question 3
- Question: How do contextual factors like semantic collocations or discourse context influence BERT's verb conjugation beyond the linear encoding of subject number?
- Basis in paper: [explicit] The paper acknowledges this limitation, noting that their controlled dataset may not capture all factors influencing conjugation.
- Why unresolved: The study uses a controlled dataset, potentially missing real-world complexities.
- What evidence would resolve it: Experiments with more naturalistic data, including sentences with varied contexts and semantic influences, to assess their impact on conjugation accuracy.

## Limitations
- The study's causal claims depend heavily on the linear separability assumption for subject number, which may not fully capture BERT's complex encoding strategies
- The intervention analysis only tests one specific manipulation and cannot definitively prove that no other encoding mechanisms exist
- The analysis focuses exclusively on third-person subject-verb agreement, limiting generalizability to other syntactic phenomena

## Confidence

- **High confidence**: The finding that BERT uses linear encodings of subject number for verb conjugation is well-supported by the significant accuracy drop (91.7% to 84.6% incorrect) after intervention.
- **Medium confidence**: The claim about encoding location changes across layers and positions is supported by evidence but could be influenced by data-specific patterns in the Ravfogel corpus.
- **Medium confidence**: The counterfactual intervention mechanism's effectiveness depends on hyperparameter choices (α values, subspace dimensions) that may not be optimal.

## Next Checks

1. Test intervention effectiveness on alternative syntactic phenomena (e.g., pronoun-antecedent agreement) to assess generalizability of the linear encoding hypothesis.
2. Compare INLP-extracted subspaces with alternative probing methods (e.g., logistic regression, canonical correlation analysis) to verify the linear separability assumption.
3. Conduct ablation studies removing positional encodings to determine their contribution to the encoding location migration pattern observed across layers.