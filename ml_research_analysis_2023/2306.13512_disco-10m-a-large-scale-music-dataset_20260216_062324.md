---
ver: rpa2
title: 'DISCO-10M: A Large-Scale Music Dataset'
arxiv_id: '2306.13512'
source_url: https://arxiv.org/abs/2306.13512
tags:
- music
- audio
- spotify
- youtube
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents DISCO-10M, a large-scale music dataset containing
  15.3 million audio clips from over 400,000 artists. The dataset is built by crawling
  the Spotify artist graph and searching for corresponding YouTube videos.
---

# DISCO-10M: A Large-Scale Music Dataset

## Quick Facts
- arXiv ID: 2306.13512
- Source URL: https://arxiv.org/abs/2306.13512
- Reference count: 40
- Contains 15.3 million audio clips from over 400,000 artists

## Executive Summary
This paper presents DISCO-10M, a large-scale music dataset created by crawling the Spotify artist graph and searching for corresponding YouTube videos. The dataset is built using a multi-stage filtering process based on duration, textual description, and audio embedding similarities to ensure high-quality matches. Precomputed CLAP audio embeddings are provided alongside the dataset to facilitate downstream tasks without requiring re-downloading and re-processing audio. The authors also provide three subsets of varying sizes and quality presets for fast prototyping with less powerful hardware. DISCO-10M surpasses the largest previously available music dataset by an order of magnitude and aims to democratize access to large-scale music datasets for the research community.

## Method Summary
DISCO-10M is created by first crawling the Spotify artist graph using breadth-first search from a diverse seed list to obtain a large set of artists and their top tracks. For each Spotify track, corresponding YouTube videos are searched using the track and artist names as the query. A multi-stage filtering process is then applied, using duration similarity, text similarity (title and description), and audio embedding similarity to ensure high-quality matches between Spotify tracks and YouTube videos. Precomputed CLAP audio embeddings are provided for downstream tasks.

## Key Results
- DISCO-10M contains 15.3 million audio clips from over 400,000 artists
- Dataset size surpasses the largest previously available music dataset by an order of magnitude
- Precomputed CLAP audio embeddings facilitate direct application on downstream tasks
- Three subsets of varying sizes and quality presets are provided for fast prototyping and development

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage filtering using duration, text, and audio embeddings improves dataset quality.
- Mechanism: Sequential filtering progressively removes poor matches—duration similarity first reduces mismatches due to mismatched clip lengths, text similarity (titles/descriptions) catches mismatches that duration alone misses, and audio embedding similarity catches subtle mismatches not obvious in metadata.
- Core assumption: Spotify preview and YouTube video pairs with high embedding similarity represent the same musical work.
- Evidence anchors:
  - [abstract] "To ensure high-quality data, we implement a multi-stage filtering process. This process incorporates similarities based on textual descriptions and audio embeddings."
  - [section] "We empirically find that the following filtering thresholds work well: (δd > 0.25) ∧ (δyt > 0.65 ∨ δyd > 0.65) ∧ (δa > 0.4)"
  - [corpus] Weak—related work focuses on embeddings for zero-shot tasks, not filtering pipelines.
- Break condition: If audio embeddings fail to capture fine-grained musical similarity, the final filter would not improve data quality.

### Mechanism 2
- Claim: Precomputed CLAP embeddings enable efficient downstream task exploration without re-downloading audio.
- Mechanism: By providing audio embeddings alongside dataset metadata, researchers can perform similarity computations, zero-shot classification, and model training without retrieving and re-processing the original audio files, reducing computational cost and entry barrier.
- Core assumption: The embeddings capture musically relevant features that are consistent across different audio sources (Spotify previews vs YouTube videos).
- Evidence anchors:
  - [abstract] "we provide precomputed CLAP embeddings alongside DISCO-10M, facilitating direct application on various downstream tasks."
  - [section] "Moreover, we provide precomputed audio embeddings alongside DISCO-10M, which we obtain from a pre-trained open-source CLAP model"
  - [corpus] Weak—corpus mentions CLAP for zero-shot tagging but not embedding reuse for tasks.
- Break condition: If embeddings become outdated or if models require different embedding spaces, precomputed embeddings lose utility.

### Mechanism 3
- Claim: Breadth-first search of Spotify's "fans also like" graph yields diverse artists across genres.
- Mechanism: Starting from a diverse seed list, BFS traversal expands the artist set one hop at a time, capturing related artists and ensuring genre coverage. This yields a representative sample of music rather than clustering around popular artists.
- Core assumption: The "fans also like" graph accurately reflects musical similarity and genre distribution.
- Evidence anchors:
  - [section] "we start with a hand-curated list of seed artists…we explore these related artists one hop at a time, adding all related artists to the set of artists we already know…we stop after we find about 400,000 unique artists."
  - [corpus] Weak—no corpus evidence on artist graph traversal strategies.
- Break condition: If the graph is biased toward mainstream artists or certain genres, diversity claim breaks.

## Foundational Learning

- Concept: Cosine similarity as a metric for embedding-based matching.
  - Why needed here: Used to compare audio embeddings (δa), title embeddings (δyt), and description embeddings (δyd) for filtering.
  - Quick check question: What range of cosine similarity values indicates high similarity? (Answer: Close to 1.0)
- Concept: Breadth-first search for graph traversal.
  - Why needed here: Used to explore Spotify's artist graph starting from seed artists to collect a large, diverse artist set.
  - Quick check question: In BFS, which nodes are visited first? (Answer: Nodes closest to the starting node)
- Concept: Log-Mel spectrograms for audio visualization.
  - Why needed here: Used to qualitatively compare Spotify preview and YouTube audio for similarity assessment.
  - Quick check question: What frequency domain representation does a Log-Mel spectrogram use? (Answer: Mel scale)

## Architecture Onboarding

- Component map: Data collection (Spotify artist graph crawl → YouTube search → metadata extraction) → Multi-stage filtering (duration → text → audio embeddings) → Dataset construction with precomputed embeddings.
- Critical path: Spotify artist crawl → YouTube search + metadata extraction → Duration filtering → Text filtering → Audio download & embedding → Audio similarity filtering → Final dataset.
- Design tradeoffs: Large scale vs quality (multi-stage filtering trades some recall for precision), precomputed embeddings vs storage overhead, breadth-first artist traversal vs potential mainstream bias.
- Failure signatures: Low audio embedding similarity across many samples suggests embedding model mismatch; high false positives in text filtering suggest overly lenient thresholds; missing artists in final dataset suggests crawl limits too low.
- First 3 experiments:
  1. Verify that duration filtering removes mismatches by checking δd distribution before/after.
  2. Test text embedding thresholds on a small sample to ensure high precision matches.
  3. Compare CLAP embeddings of known same-song pairs to confirm similarity threshold δa = 0.4 is appropriate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the bias introduced by starting the Spotify artist graph traversal from a hand-curated seed list affect the overall genre and artist diversity in DISCO-10M?
- Basis in paper: [explicit] The authors use a hand-curated seed list of artists representing various genres to start the breadth-first search of the Spotify artist graph.
- Why unresolved: The paper acknowledges this limitation but does not quantify the extent of the bias or its impact on the dataset's representativeness.
- What evidence would resolve it: A comparative analysis of the genre and artist distribution in DISCO-10M against a larger, unbiased sample of music from streaming platforms or industry charts.

### Open Question 2
- Question: How stable and reproducible are the search results and resulting dataset composition when DISCO-10M is recreated in different geographic locations or at different times?
- Basis in paper: [explicit] The authors state that search results are impacted by time and location, which adds bias to the dataset and hinders reproducibility.
- Why unresolved: The paper does not provide data on the variability of search results across different locations or time periods.
- What evidence would resolve it: A systematic study recreating the dataset in multiple locations and at different times, with a quantitative analysis of the resulting differences in dataset composition.

### Open Question 3
- Question: What is the impact of excluding age-restricted YouTube videos on the overall content and diversity of DISCO-10M?
- Basis in paper: [explicit] The authors exclude age-restricted YouTube videos during data collection, which can lead to some bias in the data.
- Why unresolved: The paper does not quantify the extent of this bias or provide examples of content that was excluded due to age restrictions.
- What evidence would resolve it: An analysis of the proportion of age-restricted content in the original search results and a comparison of the genre, artist, and other metadata distributions between the excluded and included content.

## Limitations
- The quality of the multi-stage filtering process depends heavily on the appropriateness of the threshold values, which were determined empirically but not extensively validated against ground truth data.
- The dataset's long-term viability is uncertain due to potential changes in Spotify and YouTube APIs and the removal of content over time.
- The diversity claim relies on the assumption that the "fans also like" graph accurately represents musical similarity, but potential biases toward mainstream artists or certain genres are not addressed.

## Confidence
- **Dataset Scale and Composition**: High confidence in the reported dataset size (15.3M clips from 400K+ artists) based on clear methodology description and verifiable crawl results.
- **Multi-stage Filtering Quality**: Medium confidence in the filtering process effectiveness due to empirical threshold selection but lack of extensive validation against ground truth matches.
- **Diversity Through Artist Graph Traversal**: Low confidence in the diversity claim due to lack of analysis on genre distribution and potential mainstream bias in the "fans also like" graph.

## Next Checks
1. **Threshold Validation**: Perform a ground truth evaluation by manually verifying a random sample of matches (e.g., 100 pairs) to assess the precision of the multi-stage filtering process and validate the chosen threshold values.
2. **Diversity Analysis**: Analyze the genre distribution of the final artist set and compare it against known music industry statistics to identify potential biases in the artist graph traversal and ensure representative coverage.
3. **Embedding Consistency Check**: Compute CLAP embeddings for a set of known same-song pairs (from different sources) to verify that the similarity threshold δa = 0.4 consistently captures true matches and doesn't introduce false positives or negatives.