---
ver: rpa2
title: Scaling Laws Do Not Scale
arxiv_id: '2307.03201'
source_url: https://arxiv.org/abs/2307.03201
tags:
- metrics
- scaling
- evaluation
- data
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper critically examines the validity of AI scaling laws,
  which claim that model performance improves superlinearly with dataset size. The
  authors argue that this relationship depends on evaluation metrics that may not
  reflect diverse community values or preferences.
---

# Scaling Laws Do Not Scale

## Quick Facts
- arXiv ID: 2307.03201
- Source URL: https://arxiv.org/abs/2307.03201
- Reference count: 40
- Primary result: Scaling laws for AI performance may break down when considering diverse subpopulations with incompatible evaluation preferences.

## Executive Summary
This paper critically examines the validity of AI scaling laws, which claim that model performance improves superlinearly with dataset size. The authors argue that this relationship depends on evaluation metrics that may not reflect diverse community values or preferences. As datasets grow larger, they are likely to include more diverse subpopulations, each with potentially incompatible values and preferences for model evaluation. This challenges the assumption that larger datasets lead to universally improved model performance.

The paper proposes adding a third axis to scaling law analysis to capture the size and composition of evaluation datasets, emphasizing the need to consider diverse communities and their values in AI development. The authors suggest interdisciplinary, participatory approaches to better understand and address these tensions, raising fundamental questions about the universality of scaling laws in AI.

## Method Summary
This is a conceptual paper that critiques existing scaling law approaches rather than presenting new empirical results. The method involves theoretical analysis of how sampling distributions, evaluation metrics, and community values interact in AI system evaluation. The authors propose a framework for understanding scaling law limitations through the lens of measurement validity, sampling bias, and value pluralism, but do not provide specific experimental procedures for testing their claims.

## Key Results
- Scaling law performance improvements depend on evaluation metrics that may not correspond with how different groups perceive model quality
- As datasets grow larger, they incorporate more diverse subpopulations with potentially incompatible evaluation preferences
- The paper proposes adding a third axis to scaling law analysis to capture evaluation dataset size and composition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling law metrics assume universal construct validity across all subpopulations
- Mechanism: As datasets grow, they incorporate more diverse subpopulations, each with distinct values and preferences. These differences lead to incompatible notions of what constitutes "good" model output quality.
- Core assumption: A single evaluation metric can validly capture model performance for all communities.
- Evidence anchors:
  - [abstract] "this scaling law relationship depends on metrics used to measure performance that may not correspond with how different groups of people perceive the quality of models' output"
  - [section 4.3] "there is large sub-national variation in public opinion; for example, in the US, public opinion differs greatly on topics such as support for gay marriage"
  - [corpus] Weak evidence - corpus contains papers on scaling laws but none specifically address subpopulation incompatibility
- Break condition: When evaluation metrics are shown to have different construct validity across subpopulations, or when different communities demonstrate incompatible preferences for evaluation criteria.

### Mechanism 2
- Claim: Sampling strategy determines which subpopulations are represented in evaluation datasets
- Mechanism: Different sampling approaches (convenience samples, platform-specific samples, stratified samples) systematically include or exclude certain subpopulations, affecting which values and preferences are captured in evaluation metrics.
- Core assumption: The sampling distribution used to collect evaluation data adequately represents all relevant subpopulations.
- Evidence anchors:
  - [section 4.1] "the sampling frames typically used to evaluate AI systems are not intentionally collected to be representative of any particular sub-group or community"
  - [section 4.2] "user growth on social media platforms tends to occur non-uniformly within and across national boundaries"
  - [corpus] Moderate evidence - papers discuss sampling approaches but don't deeply analyze subpopulation representation
- Break condition: When sampling strategy is explicitly designed to include specific subpopulations, or when evaluation data collection methodology is transparent about inclusion/exclusion criteria.

### Mechanism 3
- Claim: Increasing dataset size increases the number of incompatible evaluation metrics
- Mechanism: As sample size grows, more distinct subpopulations appear, each potentially requiring different evaluation metrics. This creates a combinatorial explosion of incompatible evaluation criteria.
- Core assumption: The number of distinct subpopulations grows linearly or superlinearly with dataset size.
- Evidence anchors:
  - [section 4.2] "the number of unique sub-groups present will grow with sample size"
  - [section 4.3] "large-scale datasets (which may contain numerous communities or sub-groups) may thus inadvertently collapse meaningful differences in those sub-groups' preferences"
  - [corpus] Weak evidence - corpus lacks direct evidence about metric incompatibility scaling with sample size
- Break condition: When a finite set of subpopulations is identified, or when communities demonstrate convergent evaluation preferences.

## Foundational Learning

- Concept: Measurement validity and construct validity
  - Why needed here: Understanding how evaluation metrics relate to the underlying construct of model quality is crucial for assessing scaling law claims
  - Quick check question: If two metrics correlate well with each other but poorly with the underlying construct, are they both valid measures of that construct?

- Concept: Sampling distributions and representativeness
  - Why needed here: The paper's argument depends on how different sampling approaches affect which subpopulations are included in evaluation datasets
  - Quick check question: If a convenience sample of Twitter users is used to evaluate a model, what populations are likely underrepresented?

- Concept: Goodhart's Law and metric optimization
  - Why needed here: Understanding how optimizing for proxy metrics can lead to degradation of actual performance is central to the paper's critique
  - Quick check question: If a model is optimized to maximize accuracy on a dataset, but different subpopulations have different definitions of accuracy, what happens to performance for underrepresented groups?

## Architecture Onboarding

- Component map:
  - Dataset collection → Sampling distribution → Evaluation metric → Performance measurement → Scaling law analysis
  - Key components: subpopulation identification, metric validity assessment, sampling strategy documentation

- Critical path:
  1. Identify subpopulations in dataset
  2. Determine appropriate evaluation metrics for each subpopulation
  3. Assess metric compatibility across subpopulations
  4. Analyze scaling law behavior with subpopulation-aware metrics

- Design tradeoffs:
  - Universal metrics vs. subpopulation-specific metrics: simpler but potentially invalid vs. more complex but potentially more valid
  - Large datasets vs. targeted sampling: broader coverage but less control vs. narrower coverage but more control
  - Aggregate analysis vs. disaggregated analysis: cleaner results but potential for hiding disparities vs. messier results but more nuanced understanding

- Failure signatures:
  - Consistent performance across all subpopulations despite known differences
  - Evaluation metrics that perform well in aggregate but poorly for specific groups
  - Sampling strategies that systematically exclude certain populations

- First 3 experiments:
  1. Reanalyze existing scaling law datasets to identify subpopulation composition and assess metric validity across groups
  2. Compare scaling law behavior using universal metrics vs. subpopulation-specific metrics on the same dataset
  3. Test whether increasing dataset size leads to metric incompatibility by analyzing performance on datasets of varying sizes with different subpopulation compositions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we empirically measure the extent to which scaling laws for AI performance break down for different subpopulations within a dataset?
- Basis in paper: [explicit] The paper discusses the need for "dis-aggregated evaluations" and asks what it would look like for scaling laws to be evaluated across different communities or cultural contexts.
- Why unresolved: While the paper proposes adding a third axis to scaling law analysis, it does not provide concrete methods for measuring how performance varies across subpopulations.
- What evidence would resolve it: Empirical studies comparing scaling laws across datasets collected from different communities, or across different demographic groups within the same dataset, would provide evidence.

### Open Question 2
- Question: What are the most effective methods for resolving tensions in values between different communities when developing AI systems that impact diverse populations?
- Basis in paper: [explicit] The paper discusses the challenge of "value pluralism" and asks how to address "potentially irreconcilable differences in values between such communities, at scale."
- Why unresolved: The paper acknowledges that methods like participatory democracy or value-sensitive design have been developed but are difficult to scale to the size of modern AI systems.
- What evidence would resolve it: Case studies of AI systems that have successfully navigated value tensions between diverse communities, or experimental research on the effectiveness of different methods for resolving such tensions, would provide evidence.

### Open Question 3
- Question: How can we develop evaluation metrics for AI systems that are more robust to changes in social context, user behavior, and the sociotechnical environment over time?
- Basis in paper: [explicit] The paper discusses "metric nonstationarity" and how metrics can become stale as model performance improves or the social context changes.
- Why unresolved: While the paper identifies the problem, it does not provide specific solutions for developing more robust metrics.
- What evidence would resolve it: Research on methods for developing evaluation metrics that are more resistant to changes in the sociotechnical environment, or case studies of metrics that have remained robust over time, would provide evidence.

## Limitations

- The assumption that metric incompatibility grows superlinearly with dataset size lacks direct empirical support
- The paper doesn't specify quantitative methods for measuring metric incompatibility across subpopulations
- The proposed third axis for scaling law analysis remains conceptual without clear operationalization

## Confidence

- Scaling law critique has Medium confidence: The theoretical framework is well-articulated but empirical validation remains limited
- Universal metrics critique has High confidence: The observation about different communities having incompatible preferences is well-supported
- Participatory approaches have Medium confidence: The need is compelling but practical implementation challenges are not fully explored

## Next Checks

1. Empirical test: Reanalyze existing scaling law datasets to quantify the relationship between dataset size, subpopulation diversity, and metric compatibility. Specifically, measure whether the number of incompatible evaluation criteria grows with sample size.

2. Methodological validation: Develop and validate quantitative metrics for measuring "evaluation metric incompatibility" across subpopulations, then apply these to multiple AI evaluation datasets to test the paper's core hypothesis.

3. Case study: Select one specific AI application (e.g., language model safety filtering) and conduct a comparative analysis of scaling behavior using universal metrics versus subpopulation-specific metrics, measuring the tradeoff between coverage and validity.