---
ver: rpa2
title: A Survey of Methods, Challenges and Perspectives in Causality
arxiv_id: '2302.00293'
source_url: https://arxiv.org/abs/2302.00293
tags:
- causal
- https
- learning
- causality
- variables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of causality theories
  and methods from the perspective of computer science and machine learning. It covers
  two major frameworks: the Structural Causal Model (SCM) and the Rubin Causal Model
  (RCM), along with methods for causal structure discovery and causal inference.'
---

# A Survey of Methods, Challenges and Perspectives in Causality

## Quick Facts
- arXiv ID: 2302.00293
- Source URL: https://arxiv.org/abs/2302.00293
- Reference count: 40
- One-line result: Comprehensive survey of causality theories and methods from computer science and machine learning perspectives

## Executive Summary
This paper provides a comprehensive survey of causality theories and methods from the perspective of computer science and machine learning. It covers two major frameworks: the Structural Causal Model (SCM) and the Rubin Causal Model (RCM), along with methods for causal structure discovery and causal inference. The paper also explores connections between causality and machine learning, including neural causal models, representation learning, and data augmentation. Finally, it presents a wide range of applications for causal models across various scientific domains.

## Method Summary
The survey synthesizes 40 references through systematic categorization and review, extracting key definitions, methods, and connections between causality and machine learning. The authors integrate different perspectives to highlight emerging trends and open questions, focusing on how causal reasoning can enhance machine learning capabilities. The method involves mapping established causal frameworks to modern ML approaches, particularly examining how neural networks can approximate causal mechanisms while preserving identifiability.

## Key Results
- Establishes comprehensive mapping between Pearl's Causal Hierarchy and machine learning tasks
- Introduces Neural Causal Models as scalable approach for causal inference using deep learning
- Demonstrates connections between disentangled representations and causal reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The survey bridges causality and deep learning by mapping Pearl's hierarchy onto ML tasks, enabling precise problem formulation.
- Mechanism: Layer 1 tasks (association) map to supervised/unsupervised learning; Layer 2 (intervention) to reinforcement learning; Layer 3 (counterfactual) to causal reasoning extensions.
- Core assumption: Pearl's Causal Hierarchy is complete for ML generalization problems.
- Evidence anchors:
  - [abstract] "As causal engines aim to learn mechanisms independent from a data distribution, combining Deep Learning with Causality can have a great impact"
  - [section] "The CHT [6] states that such models cannot answer causal queries in general (queries in the second and third layers in the hierarchy of Table 1)"
  - [corpus] Weak/no explicit anchor; assumes reader knows Pearl's hierarchy.

### Mechanism 2
- Claim: Neural Causal Models (NCMs) enable scalable causal inference by replacing parametric functions with deep networks.
- Mechanism: NCMs replace SCM's linear/functional causal mappings with MLPs or GNNs, preserving causal semantics while allowing high-dimensional inputs.
- Core assumption: Neural networks can approximate any causal mechanism in SCMs without losing identifiability.
- Evidence anchors:
  - [section] "An improvement of the NCM, called Tractable NCM (TNCM) [ 165], replaces the MLPs with Sum-Product Networks (SPNs)"
  - [section] "The GNN-SCM [166] is another neural causal model replacing the parametric functions for the causal inference (i.e. the MLPs) with Graph Neural Networks"
  - [corpus] No direct corpus support; inferred from described methods.

### Mechanism 3
- Claim: Disentangled representations support causal reasoning by isolating independent mechanisms.
- Mechanism: By enforcing factorized latent spaces (e.g., Œ≤-VAE), each dimension captures a single causal factor, making interventions modular and interpretable.
- Core assumption: True causal factors are disentangled in the data distribution.
- Evidence anchors:
  - [section] "The key idea of disentanglement shared in the research community is that a disentangled representation should separate the factors of variation affecting the data"
  - [section] "The main idea of the paper is that minimal and sufficient representationsùëçshould also be invariant and disentangled"
  - [corpus] Weak; mainly from cited works rather than direct survey content.

## Foundational Learning

- Concept: Pearl's Causal Hierarchy (L1/L2/L3)
  - Why needed here: Provides vocabulary for matching ML tasks to causal queries; essential for designing NCMs.
  - Quick check question: What type of query does "What would happen if we forced X?" represent?

- Concept: Do-calculus rules
  - Why needed here: Core tool for converting interventional queries to observational ones; used in NCM inference.
  - Quick check question: When can Rule 1 (deletion of observation) be applied?

- Concept: Independence testing in causal structure discovery
  - Why needed here: Underlies constraint-based methods (IC/PC); critical for graph recovery before NCM training.
  - Quick check question: What does d-separation mean in a DAG?

## Architecture Onboarding

- Component map: Input graph (SCM structure) ‚Üí GNN encoder ‚Üí latent space ‚Üí decoder (intervention/counterfactual) ‚Üí output
- Critical path: Graph ‚Üí causal inference ‚Üí intervention/counterfactual prediction
- Design tradeoffs:
  - Expressiveness vs identifiability: More flexible neural approximators risk unidentifiability
  - Graph completeness: Hidden confounders break inference; need assumptions or additional data
- Failure signatures:
  - High reconstruction error after intervention ‚Üí graph structure misspecified
  - Latent space not factorized ‚Üí disentanglement regularization missing
  - Unstable gradients in NCM ‚Üí overparameterized networks without inductive bias
- First 3 experiments:
  1. Train VACA on synthetic SCM data with known graph; test L2/L3 queries
  2. Evaluate disentanglement metrics (TC, MIG) on learned latent space from iVGAE
  3. Compare C-GraphSAGE vs vanilla GraphSAGE on graph with known backdoor paths

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically bridge the gap between correlation-based machine learning and causal reasoning, particularly for out-of-distribution generalization?
- Basis in paper: [explicit] The paper discusses how current machine learning models struggle with generalization beyond their training distribution due to spurious correlations, while causal models aim to learn mechanisms invariant to data distribution.
- Why unresolved: Despite the theoretical framework of Neural Causal Models and attempts to combine deep learning with causality, scaling these approaches to complex real-world problems remains an open challenge. The paper notes that most current approaches are limited by assumptions like no hidden confounders or linearity.
- What evidence would resolve it: A successful large-scale implementation of a causal reasoning engine that demonstrably outperforms purely correlation-based models on out-of-distribution tasks, validated across multiple domains.

### Open Question 2
- Question: Is unsupervised disentanglement of causal factors truly achievable, or do we always need some form of supervision or inductive biases?
- Basis in paper: [explicit] The paper discusses recent theoretical work showing that learning a disentangled representation in an unsupervised way is impossible without additional assumptions or inductive biases on the nature of the task.
- Why unresolved: While semi-supervised and weakly-supervised approaches show promise, there remains debate about whether true unsupervised disentanglement is achievable, and if so, what minimal assumptions are required.
- What evidence would resolve it: A rigorous proof demonstrating either (a) the impossibility of unsupervised disentanglement under certain conditions, or (b) a practical algorithm that consistently achieves disentanglement across diverse domains with minimal supervision.

### Open Question 3
- Question: How can we scale causal structure discovery methods to handle high-dimensional data with hidden confounders effectively?
- Basis in paper: [explicit] The paper notes that traditional causal structure discovery methods work well for low-dimensional data but struggle to recover causal structures at scale, especially in the presence of hidden confounders.
- Why unresolved: Current methods either make strong assumptions (no confounders, linearity) or are computationally intractable for high-dimensional data. The paper mentions some neural approaches but notes their limitations.
- What evidence would resolve it: A scalable causal discovery algorithm that can handle hundreds or thousands of variables while accounting for hidden confounders, validated on real-world high-dimensional datasets with known ground truth causal structures.

## Limitations
- Survey synthesis lacks quantitative benchmarks comparing SCM vs RCM performance in specific scenarios
- Neural Causal Models presented as promising direction but validation data on scalability and generalization is limited
- Claims about disentangled representations supporting causal reasoning assume factorizability that may not hold in complex real-world data

## Confidence
- **High Confidence**: The survey accurately represents established causality frameworks (SCM, RCM) and their theoretical foundations
- **Medium Confidence**: Claims about NCMs' ability to scale causal inference through neural approximators are supported by described methods but lack extensive empirical validation
- **Medium Confidence**: The connection between disentanglement and causal reasoning is theoretically sound but depends on assumptions about data factorizability

## Next Checks
1. Test NCM performance on benchmark causal inference tasks (e.g., T√ºbingen cause-effect pairs) to validate scalability claims
2. Evaluate disentanglement metrics (TC, MIG) on real-world datasets where causal factors are known to be partially entangled
3. Conduct ablation studies on the CHT mapping to ML tasks to identify where the hierarchy breaks down in practice