---
ver: rpa2
title: A Deep Learning System for Domain-specific Speech Recognition
arxiv_id: '2303.10510'
source_url: https://arxiv.org/abs/2303.10510
tags:
- data
- speech
- systems
- call
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for building domain-specific ASR systems
  using semi-supervised learning to annotate unlabeled call center data. The authors
  fine-tune pre-trained DeepSpeech2 and Wav2Vec2 acoustic models on internal benefit-specific
  speech data and incorporate KenLM language models.
---

# A Deep Learning System for Domain-specific Speech Recognition

## Quick Facts
- arXiv ID: 2303.10510
- Source URL: https://arxiv.org/abs/2303.10510
- Reference count: 0
- Primary result: Wav2Vec2-Large-LV60 fine-tuned with LM achieves 0.105 WER on benefit-specific speech, outperforming Google (0.219 WER) and AWS (0.113 WER)

## Executive Summary
This paper presents a domain-specific automatic speech recognition (ASR) system for benefit-related call center audio using semi-supervised learning to annotate unlabeled data. The approach fine-tunes pre-trained DeepSpeech2 and Wav2Vec2 acoustic models on internal benefit-specific speech data, incorporating KenLM language models for improved domain adaptation. The best system, Wav2Vec2-Large-LV60 with an external KenLM, achieves superior performance on domain-specific speech compared to commercial ASR systems while demonstrating that domain-specific fine-tuned ASR can outperform commercial systems on downstream natural language understanding tasks even with higher word error rates.

## Method Summary
The method uses a semi-supervised annotation pipeline where a committee of ASR systems (DeepSpeech2, Wav2Vec2 variants, AWS, Google) generates multiple rough transcriptions for unlabeled audio. A relative error rate metric selects the most accurate transcription among committee outputs to bootstrap larger labeled datasets. The selected pseudo-labels are used to fine-tune pre-trained acoustic models (DeepSpeech2 and Wav2Vec2) on benefit-specific data. An external KenLM language model is integrated to further constrain outputs to domain-specific patterns. The approach addresses data scarcity in domain-specific ASR by reducing human annotation costs while maintaining high performance on benefit-specific speech recognition tasks.

## Key Results
- Wav2Vec2-Large-LV60 + LM achieves 0.105 WER on benefit-specific audio, outperforming AWS (0.113 WER) and Google (0.219 WER)
- Fine-tuned Wav2Vec2 models show significant improvements over DeepSpeech2 baselines on domain-specific speech
- Domain-specific fine-tuned ASR systems outperform commercial ASR on NLU tasks despite having higher WER
- The semi-supervised approach enables development of high-quality ASR systems with minimal human intervention

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semi-supervised annotation with a recognizer committee improves domain-specific ASR performance by leveraging multiple ASR systems to generate and filter transcriptions.
- Mechanism: The system uses a committee of ASR models to generate multiple rough transcriptions for unlabeled data. A relative error rate metric selects the most accurate transcription among committee outputs, allowing bootstrapping of large labeled datasets with minimal human intervention.
- Core assumption: Multiple ASR systems produce different error patterns, allowing selection of the most accurate transcription based on consensus.
- Evidence anchors:
  - [abstract] "The domain-specific data are collected using proposed semi-supervised learning annotation with little human intervention."
  - [section 2.3] "Rather than relying on small, supervised training sets, a semi-supervised annotation method is proposed to bootstrap larger datasets, and reduce system development cost."

### Mechanism 2
- Claim: Fine-tuning pre-trained Wav2Vec2 models on domain-specific data with external language models outperforms both general ASR systems and fine-tuned DeepSpeech2 models.
- Mechanism: Pre-trained Wav2Vec2 models capture general speech patterns. Fine-tuning on benefit-specific data adapts these models to domain vocabulary and patterns. The external KenLM language model further constrains outputs to domain-specific language patterns.
- Core assumption: Domain-specific fine-tuning and language modeling provide more benefit than additional general training data for domain-specific tasks.
- Evidence anchors:
  - [abstract] "The best performance comes from a fine-tuned Wav2Vec2-Large-LV60 acoustic model with an external KenLM, which surpasses the Google and AWS ASR systems on benefit-specific speech."
  - [section 4.2] "Speech recognition performance gains are observed by switching the use of pre-trained model from DS2 to Wav2Vec2."

### Mechanism 3
- Claim: Domain-specific ASR outputs can outperform commercial ASR systems on downstream NLU tasks even when having higher WER, because they preserve more domain-relevant semantic information.
- Mechanism: Fine-tuned ASR systems produce transcriptions with domain-specific terminology and patterns that are more useful for intent classification, even if overall word error rate is higher than commercial systems.
- Core assumption: Certain domain-specific terms and phrases are more important for NLU task accuracy than overall transcription accuracy.
- Evidence anchors:
  - [abstract] "Results of a benefit-specific natural language understanding (NLU) task show that the domain-specific fine-tuned ASR system can outperform the commercial ASR systems even when its transcriptions have higher word error rate (WER)."
  - [section 4.2] "For the call segment test set, AWS ASR output has the lowest WER but its intent classification performance is the same as Wav2Vec2."

## Foundational Learning

- Concept: Connectionist Temporal Classification (CTC) loss
  - Why needed here: CTC loss is the training objective used by both DeepSpeech2 and Wav2Vec2 models to align speech frames with text labels without requiring pre-segmented data.
  - Quick check question: What is the main advantage of CTC loss for end-to-end speech recognition compared to traditional forced alignment approaches?

- Concept: Semi-supervised learning and data bootstrapping
  - Why needed here: The semi-supervised annotation method allows creating large labeled datasets from unlabeled call center data with minimal human intervention, addressing the data scarcity problem in domain-specific ASR.
  - Quick check question: How does the relative error rate metric help select accurate transcriptions from multiple ASR system outputs?

- Concept: Language model integration with ASR
  - Why needed here: The KenLM language model improves ASR output by constraining predictions to domain-specific language patterns and reducing phonetically plausible but semantically incorrect errors.
  - Quick check question: What is the difference between shallow fusion and deep fusion when integrating language models with end-to-end ASR systems?

## Architecture Onboarding

- Component map: Audio input → Pre-trained model → Fine-tuning on domain data → LM rescoring → Punctuation restoration → NLU task
- Critical path: Audio input → Pre-trained model → Fine-tuning on domain data → LM rescoring → Punctuation restoration → NLU task
- Design tradeoffs: Larger pre-trained models (Wav2Vec2-LARGE-LV60) require more computational resources but achieve better performance. The semi-supervised approach trades some annotation accuracy for scale and cost efficiency.
- Failure signatures: Poor performance on domain-specific terms suggests insufficient fine-tuning data or inadequate language model coverage. High WER but good NLU performance suggests the model is capturing semantic information despite transcription errors.
- First 3 experiments:
  1. Compare WER of fine-tuned Wav2Vec2 vs DeepSpeech2 on the validation set to verify model selection.
  2. Test different relative error rate thresholds in the semi-supervised annotation pipeline to optimize training data quality.
  3. Evaluate the impact of language model weight on ASR performance to find the optimal balance between acoustic and language model scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the semi-supervised annotation method's performance scale with increasing amounts of unlabeled data?
- Basis in paper: [explicit] The paper mentions the semi-supervised annotation method is cyclical and iterative, but does not evaluate how performance changes with dataset size.
- Why unresolved: The paper does not report experiments testing the annotation method on progressively larger unlabeled datasets to determine scalability limits.
- What evidence would resolve it: Experiments showing WER/CER and downstream task performance as a function of unlabeled dataset size would demonstrate scalability.

### Open Question 2
- Question: What is the impact of using domain-specific fine-tuned ASR systems on downstream NLU tasks beyond intent classification?
- Basis in paper: [explicit] The paper evaluates intent classification but states "For future work, we would like to... investigate ASR error robust downstream NLP systems."
- Why unresolved: Only one NLU task (intent classification) is evaluated, leaving open questions about other tasks like slot filling, entity recognition, or dialogue state tracking.
- What evidence would resolve it: Experiments showing performance on multiple NLU tasks (slot filling, entity recognition, etc.) using fine-tuned ASR outputs versus commercial ASR outputs.

### Open Question 3
- Question: How does the proposed semi-supervised annotation method compare to other semi-supervised or active learning approaches for ASR?
- Basis in paper: [inferred] The paper presents a semi-supervised annotation method but does not compare it to alternative approaches in the literature.
- Why unresolved: No comparative analysis with other semi-supervised learning or active learning techniques for ASR data annotation.
- What evidence would resolve it: Head-to-head comparisons with established semi-supervised learning methods (like self-training, co-training) or active learning approaches, measuring WER/CER and annotation efficiency.

## Limitations

- The semi-supervised annotation approach relies on empirical thresholds for relative error rate filtering that are not explicitly specified, potentially affecting reproducibility.
- Comparison against commercial ASR systems is limited to WER as the primary metric, with only one downstream NLU task evaluated.
- The study uses internal call center data from a single benefit domain, limiting external validity and generalizability to other domains.

## Confidence

- Confidence: Low - The semi-supervised annotation approach relies heavily on empirical thresholds for relative error rate filtering that are not explicitly specified.
- Confidence: Medium - The comparison against commercial ASR systems uses only WER as the primary metric with limited scope of downstream evaluation.
- Confidence: Medium - The study uses internal call center data from a single benefit domain, limiting external validity.

## Next Checks

1. **Parameter Sensitivity Analysis**: Systematically vary the relative error rate threshold in the semi-supervised annotation pipeline and measure its impact on final WER and NLU performance.

2. **Cross-Domain Generalization Test**: Apply the fine-tuned Wav2Vec2-Large-LV60 model to a different domain (e.g., healthcare or technical support) without additional fine-tuning and measure performance degradation.

3. **NLU Task Expansion**: Extend the downstream evaluation to multiple NLU tasks including named entity recognition and slot filling, not just intent classification.