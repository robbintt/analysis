---
ver: rpa2
title: 'LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models'
arxiv_id: '2308.16137'
source_url: https://arxiv.org/abs/2308.16137
tags:
- llms
- attention
- lm-infinite
- tokens
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper analyzes why large language models struggle to generalize
  to long sequences beyond their training context length, despite using relative positional
  encodings. Through theoretical analysis and empirical study, the authors identify
  three key out-of-distribution factors: unseen distances between tokens, increasing
  number of attended tokens, and implicit encoding of absolute position information.'
---

# LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models

## Quick Facts
- arXiv ID: 2308.16137
- Source URL: https://arxiv.org/abs/2308.16137
- Reference count: 11
- Models like LLaMA and GPT-J can handle sequences up to 32K tokens - 8x their training limit - without fine-tuning

## Executive Summary
This paper addresses a fundamental limitation in large language models: their inability to generalize to sequences longer than their training context length, despite using relative positional encodings. Through theoretical analysis, the authors identify three out-of-distribution factors causing this failure: exploding attention logits for unseen distances, increasing attention entropy, and implicit encoding of absolute position information. They propose LM-Infinite, a simple method that adds a Λ-shaped attention mask and distance limit without requiring parameter updates. The approach enables models to maintain generation quality and fluency on sequences up to 32K tokens while achieving significant computational efficiency gains.

## Method Summary
LM-Infinite is a zero-shot method that enables length generalization for LLMs without parameter updates. It adds a Λ-shaped attention mask allowing each token to attend to initial n_global tokens and preceding tokens within n_local distance (set to training length limit). This simple modification addresses three key out-of-distribution factors identified in the theoretical analysis: unseen attention distances, increasing attention entropy, and implicit absolute position encoding. The method works across different relative positional encoding schemes and can be applied to pre-trained models to extend their effective context length.

## Key Results
- LM-Infinite extends LLaMA-7B and GPT-J-6B generation quality to 32K tokens, 8x beyond their 4K training limit
- Achieves 2.7x decoding speedup and 7.5x memory savings compared to original models
- Matches or exceeds performance of models explicitly fine-tuned on long sequences on downstream tasks
- Maintains perplexity and generation quality across multiple model families (LLaMA, Llama-2, MPT-7B, GPT-J)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Attention logits explode to infinity for unseen distances, making them out-of-distribution
- Mechanism: Relative positional encodings compute attention weights based on token distances. When sequences exceed training length, new distances appear that were never seen during training, causing logit computation to produce extremely large values
- Core assumption: Attention logit function family has bounded pseudo-dimension
- Evidence: Theorem 1 proves logit explosion, with empirical validation showing attention logits oscillate to significantly larger values than those within 4K training length

### Mechanism 2
- Claim: Attention entropy increases to infinity as sequence length grows, diluting information
- Mechanism: As sequences become longer, each token attends to more preceding tokens. Unless attention logits explode, the attention weight distribution becomes increasingly uniform, reducing information content
- Core assumption: Attention entropy measures information preservation in the attention mechanism
- Evidence: Proposition 1 proves entropy increases as Ω(ln n), with formal proof provided

### Mechanism 3
- Claim: Attention mechanism implicitly encodes absolute position information, which becomes distorted on unseen lengths
- Mechanism: Transformers can reconstruct absolute position information from input sequence alone, even with relative encodings. Initial tokens have stronger signals than trailing tokens. When sequences exceed training length, initial tokens are mishandled due to OOD factors 1 and 2
- Core assumption: Implicit encoding of absolute position occurs in real LLMs
- Evidence: Theorem 2 proves this is possible, plus empirical PCA visualization showing tokens at different positions occupy distinct sub-spaces

## Foundational Learning

- **Pseudo-dimension and function approximation theory**: Needed for theoretical analysis of attention logit explosion; Quick check: If a function family has pseudo-dimension 3, what does this tell us about its capacity to shatter point sets?
- **Information theory - entropy as uncertainty measure**: Needed for analyzing attention entropy explosion; Quick check: If attention weights become uniformly distributed across n tokens, what is the entropy of this distribution?
- **Relative vs absolute positional encoding**: Needed to understand why relative encodings should generalize but fail; Quick check: In relative positional encoding, how does the attention weight between two tokens depend on their positions?

## Architecture Onboarding

- **Component map**: Token embedding -> Λ-shaped attention mask -> Distance limit -> Modified positional encoding -> Transformer layers -> Output layer
- **Critical path**: Token embedding and positional encoding → Attention computation with Λ-mask and distance limiting → Feed-forward network → Output layer for next token prediction
- **Design tradeoffs**: Global vs local branch sizes (larger global preserves more initial token information but reduces efficiency), distance limit value (should match training length), masking strategy (Λ-shape balances information preservation and efficiency)
- **Failure signatures**: NaN outputs (especially for Llama-2), sudden perplexity spikes at certain sequence lengths, degraded generation quality, memory allocation failures during long sequence processing
- **First 3 experiments**: 1) Apply LM-Infinite to GPT-J on sequences 4K→8K and measure perplexity changes, 2) Visualize attention entropy curves with/without LM-Infinite to verify entropy control, 3) Test passkey retrieval on increasing sequence lengths to verify task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact relationship between Λ-shaped attention mask parameters (n_global and n_local) and the model's ability to maintain information sensitivity in downstream tasks?
- Basis: Paper mentions Λ-mask trades off information for fluency, with n_global having less effect while n_local is set to pretraining length
- Why unresolved: LM-Infinite extends task-solving ability but doesn't perfectly maintain information perception capability
- What evidence would resolve it: Systematic ablation studies varying n_global and n_local while measuring fluency metrics and information sensitivity across multiple downstream tasks

### Open Question 2
- Question: How do different relative positional encoding schemes (RoPE, Alibi, XPos) fundamentally differ in their OOD behavior when scaled beyond training lengths?
- Basis: Paper analyzes three OOD factors and shows LM-Infinite works across schemes, but doesn't provide comparative analysis
- Why unresolved: While demonstrating LM-Infinite's effectiveness, paper doesn't explain why different schemes might require different treatment
- What evidence would resolve it: Detailed analysis of attention patterns, entropy evolution, and failure modes for each encoding scheme at various sequence lengths beyond training limits

### Open Question 3
- Question: What is the theoretical limit of length generalization for models using LM-Infinite, and what factors determine this limit?
- Basis: Paper demonstrates effectiveness up to 32k tokens but mentions theoretical access to n_layer × L_pretrain context
- Why unresolved: Paper doesn't explore upper bounds of LM-Infinite's effectiveness or identify what breaks down at extreme lengths
- What evidence would resolve it: Scaling experiments to increasingly long sequences with analysis of when and why performance degradation occurs

## Limitations
- Theoretical analysis relies on assumptions about attention mechanism behavior that may not hold for all transformer architectures
- Evaluation focuses on perplexity and generation quality metrics but lacks human evaluation studies for coherence at extreme lengths
- Optimal parameters (n_global, n_local) likely vary by architecture but paper doesn't provide systematic tuning method
- Computational efficiency claims based on specific hardware configurations may not generalize

## Confidence

**High Confidence**: Empirical demonstration that LM-Infinite improves performance on sequences up to 32K tokens compared to baseline models across multiple model families and positional encoding schemes.

**Medium Confidence**: Theoretical analysis of out-of-distribution factors causing failure in long sequence generalization. While mathematical proofs are sound, practical impact depends on specific model implementations.

**Low Confidence**: Claim that LM-Infinite achieves performance matching or exceeding models explicitly fine-tuned on long sequences. Comparison is made against a single fine-tuned baseline with unspecified training details.

## Next Checks
- Conduct ablation studies varying Λ-mask parameters systematically across different model architectures to identify optimal configurations
- Implement human evaluation studies comparing generated text quality from models with and without LM-Infinite at extreme lengths (16K-32K tokens)
- Test LM-Infinite on models with different attention mechanisms (sparse attention, local attention windows) and positional encoding variants not covered in original evaluation