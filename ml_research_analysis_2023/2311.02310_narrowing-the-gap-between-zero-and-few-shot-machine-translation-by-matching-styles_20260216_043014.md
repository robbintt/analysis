---
ver: rpa2
title: Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching
  Styles
arxiv_id: '2311.02310'
source_url: https://arxiv.org/abs/2311.02310
tags:
- translation
- few-shot
- sentence
- zero-shot
- german
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance gap between zero- and few-shot
  machine translation with large language models, finding that the gap largely stems
  from differences in writing style rather than semantics. The authors propose a style-matching
  prompting strategy that retrieves target-language sentences from in-domain corpora
  to improve zero-shot translation without requiring parallel examples.
---

# Narrowing the Gap between Zero- and Few-shot Machine Translation by Matching Styles

## Quick Facts
- arXiv ID: 2311.02310
- Source URL: https://arxiv.org/abs/2311.02310
- Reference count: 35
- Key outcome: Style-matching prompting closes ~70% of the performance gap between zero- and few-shot translation by retrieving target-language sentences to improve style consistency.

## Executive Summary
This paper investigates why large language models perform differently in zero-shot versus few-shot machine translation settings. The authors discover that the performance gap largely stems from differences in writing style rather than semantic accuracy. They propose a novel style-learning prompting strategy that retrieves target-language sentences from in-domain corpora to guide zero-shot translations toward domain-appropriate style without requiring parallel examples. This approach significantly narrows the performance gap while maintaining semantic quality.

## Method Summary
The authors employ prompt-based translation with GPT-3.5-turbo-0301, using retrieval-based prompting to obtain in-domain demonstration examples. They implement a style-learning prompting strategy that first performs zero-shot translation, then retrieves target-language sentences matching the translation output, and finally uses these as style references in revised prompts. The approach leverages BM25 or dense retrieivers to obtain relevant examples from monolingual target corpora, aligning translation style with domain norms without requiring parallel training data.

## Key Results
- Zero-shot translations differ from few-shot translations primarily in writing style rather than semantics
- Style-matching prompting reduces the performance gap by approximately 70%
- Dense retrieivers outperform BM25 in retrieving effective style demonstrations
- The approach maintains semantic accuracy while improving style consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot translations lack style matching with target domain text, causing BLEU score penalties despite good semantics.
- Mechanism: The model translates with correct meaning but uses vocabulary and syntactic patterns that differ from in-domain target language norms, which BLEU heavily penalizes.
- Core assumption: BLEU rewards n-gram overlap and penalizes lexical/structural deviations more than semantic equivalence.
- Evidence anchors:
  - [abstract]: "this gap can largely be closed (for about 70%) by matching the writing styles of the target corpus"
  - [section 2]: "even though zero-shot translations are relatively good, there remains a discernible gap comparing their performance with the few-shot setting"
  - [section 3]: "few-shot translation exhibits a smaller overall TED, indicating closer alignments in word semantic relations with references"
- Break condition: If the target corpus itself has high lexical diversity or inconsistent style, style matching won't consistently improve scores.

### Mechanism 2
- Claim: Few-shot translation benefits from in-context demonstrations that guide the model toward target domain style.
- Mechanism: Retrieval-based demonstrations provide concrete examples of domain-appropriate vocabulary and syntactic patterns that the model copies during generation.
- Core assumption: LLMs use in-context examples as templates for generating text with similar style and structure.
- Evidence anchors:
  - [section 2]: "We employ a retrieval-based prompting method... that retrieves source and target pairs from the training corpus as in-context learning demonstrations"
  - [section 3.1]: "few-shot translations are being steered to be lexical-wise more similar to demonstrations"
  - [section 4]: "Our hypothesis is that style-relevant information primarily resides in the in-domain target corpus"
- Break condition: If demonstrations are noisy or irrelevant, they may degrade performance rather than improve it.

### Mechanism 3
- Claim: Style-learning prompting improves zero-shot performance by retrieving target-language sentences to guide style without parallel examples.
- Mechanism: The model first translates, then retrieves target-language examples matching the translation, and uses these as style references in a revised prompt.
- Core assumption: Target-language monolingual corpora contain sufficient style information to guide translation style matching.
- Evidence anchors:
  - [section 4]: "Our approach has two steps: first, we employ zero-shot translation; then we take the translation output as a query to retrieve examples from the target language's corpus"
  - [section 4]: "By revising prompts with target examples, the approach allows us to align styles in translations without source-side demonstrations"
  - [section 4]: "Our style-learning prompts largely improve the zero-shot performance, reducing the gap by approximately 70%"
- Break condition: If retrieval quality is poor, the style guidance will be ineffective.

## Foundational Learning

- Concept: In-context learning and demonstration effects
  - Why needed here: The paper relies on retrieving and using examples as demonstrations to guide model behavior without parameter updates.
  - Quick check question: What happens to translation quality if you remove the demonstration examples from the prompt?

- Concept: Style as lexical and syntactic patterns
  - Why needed here: The paper defines the performance gap as a style difference rather than semantic difference, requiring understanding of what constitutes writing style.
  - Quick check question: How would you measure whether two sentences have similar writing styles?

- Concept: Retrieval-based prompting and BM25
  - Why needed here: The approach uses BM25 to retrieve relevant target-language examples for style matching.
  - Quick check question: What's the difference between using BM25 and embedding-based retrieval for this task?

## Architecture Onboarding

- Component map: Retrieval system (BM25 or dense retriever) → Prompt template with style instruction → LLM (GPT3.5 or Llama2) → Evaluation (BLEU/COMET)
- Critical path: Retrieval quality → Prompt formulation → Model generation → Metric computation
- Design tradeoffs: Retrieval quality vs. computational cost, monolingual vs. parallel examples, number of demonstrations vs. prompt length
- Failure signatures: Low BLEU despite good COMET (style mismatch), poor retrieval quality degrading performance, model-specific effectiveness differences
- First 3 experiments:
  1. Test style-learning prompting with different numbers of retrieved examples (1, 5, 10) to find optimal demonstration count
  2. Compare BM25 vs. dense retriever quality using the same prompt template
  3. Evaluate style-learning on a low-resource language pair to test generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the style-matching prompting strategy perform on low-resource language pairs or languages where LLMs have poor performance?
- Basis in paper: [explicit] The paper explicitly states that their findings may not be directly applicable to low-resource translation tasks or languages in which LLMs do not perform well, and that they leave the investigation of low-resource languages to future work.
- Why unresolved: The paper's experiments are limited to German-English translation, which is a high-resource language pair. The authors acknowledge that their results may not generalize to other language pairs, particularly those with less data or where LLMs struggle.
- What evidence would resolve it: Testing the style-matching prompting strategy on a variety of low-resource language pairs and comparing the results to both zero-shot and few-shot baselines would provide evidence of its effectiveness in these scenarios.

### Open Question 2
- Question: How does the choice of retriever (BM25 vs. dense retriever) impact the quality of demonstrations and the performance of the style-matching prompting strategy?
- Basis in paper: [explicit] The paper compares BM25 and dense retrieivers in Figure 5, showing that the dense retriever obtains better demonstrations for the style-learning prompt.
- Why unresolved: While the paper shows that a dense retriever outperforms BM25 in this specific case, it does not explore other retriever architectures or the impact of retriever choice on different domains or language pairs.
- What evidence would resolve it: Systematic experiments comparing various retriever architectures (e.g., sparse vs. dense, cross-lingual vs. mono-lingual) across multiple domains and language pairs would provide a more comprehensive understanding of the impact of retriever choice.

### Open Question 3
- Question: What is the impact of the number of retrieved examples on the performance of the style-matching prompting strategy, and is there an optimal number of examples to use?
- Basis in paper: [inferred] The paper experiments with different numbers of demonstrations (1-50) in Figure 4, showing that both few-shot and style-learning prompting improve with more examples, but the improvement plateaus after a certain point.
- Why unresolved: The paper does not investigate the relationship between the number of retrieved examples and performance in detail, nor does it explore whether there is an optimal number of examples for the style-matching strategy.
- What evidence would resolve it: Conducting experiments with a wider range of example numbers and analyzing the relationship between the number of examples and performance metrics (e.g., BLEU, COMET) would help identify the optimal number of examples for the style-matching strategy.

## Limitations

- Data quality dependency: Effectiveness depends heavily on the quality of monolingual target corpora; noisy or inconsistent style in retrieved examples may degrade performance
- Retrieval system limitations: Neither BM25 nor dense retrieivers explicitly optimize for stylistic similarity, potentially missing nuanced style matching
- Evaluation metric constraints: BLEU and COMET may not fully capture stylistic alignment, requiring human evaluation for validation

## Confidence

**High confidence**: The core observation that zero-shot and few-shot translations differ primarily in style rather than semantics is well-supported by quantitative evidence (TED scores, lexical overlap measurements). The retrieval-based demonstration approach for few-shot translation is a well-established technique.

**Medium confidence**: The style-learning prompting strategy's effectiveness is demonstrated on three specific domains with specific language pairs. While results are promising (70% gap reduction), the approach requires further validation across more diverse domains, language pairs, and corpus types.

**Low confidence**: Claims about the mechanism by which style-matching improves translation quality rely on indirect evidence. The paper shows correlation between style similarity and improved metrics but does not directly measure whether retrieved target sentences actually improve stylistic alignment in translations.

## Next Checks

1. **Human evaluation of style matching**: Conduct a blind human evaluation where raters compare stylistic similarity between zero-shot translations with and without style-learning prompting, rating how well each matches the target domain's writing style on a 1-5 scale.

2. **Cross-domain robustness testing**: Apply style-learning prompting to a dataset with highly inconsistent style (such as general web text or multi-author corpora) to test whether the approach degrades when target corpus style is heterogeneous or when the optimal style is ambiguous.

3. **Style metric validation**: Implement an explicit style similarity metric (such as comparing syntactic complexity, vocabulary sophistication, or discourse markers) and verify that style-learning prompting improves these metrics in addition to BLEU/COMET scores, establishing a clearer causal link between style matching and performance improvement.