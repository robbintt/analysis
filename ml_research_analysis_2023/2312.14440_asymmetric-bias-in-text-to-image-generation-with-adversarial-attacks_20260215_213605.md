---
ver: rpa2
title: Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks
arxiv_id: '2312.14440'
source_url: https://arxiv.org/abs/2312.14440
tags:
- adversarial
- attack
- attacks
- arxiv
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies adversarial attacks on Text-to-Image (T2I) models,
  focusing on the factors associated with attack success rates (ASRs). A new attack
  objective, entity swapping using adversarial suffixes, is introduced, along with
  two gradient-based attack algorithms.
---

# Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks

## Quick Facts
- arXiv ID: 2312.14440
- Source URL: https://arxiv.org/abs/2312.14440
- Reference count: 34
- Primary result: Identifies asymmetric attack success rates for entity swapping in T2I models, with some swaps succeeding 60% of the time while reverse swaps fail below 5%

## Executive Summary
This paper investigates adversarial attacks on Text-to-Image (T2I) models, focusing on entity swapping attacks that replace one object with another in generated images. The study introduces two gradient-based attack algorithms (Single and Multiple Token Perturbation) to find adversarial suffixes that successfully modify image outputs. Most notably, the research reveals that attack success rates are highly asymmetric - replacing "human" with "robot" is significantly easier than the reverse swap. The authors propose probing metrics including baseline distance difference (∆2) and perplexity to predict attack success, finding that certain conditions yield 60% success probability while others drop below 5%.

## Method Summary
The study employs gradient-based optimization to generate adversarial suffixes for entity swapping attacks on Stable Diffusion v2-1-base. Two attack algorithms are proposed: Single Token Perturbation (inspired by LLM attacks) and Multiple Token Perturbation (which updates multiple tokens simultaneously). Attacks optimize adversarial suffixes by minimizing a score function that balances movement away from input and toward target CLIP embeddings. The method uses either restricted token sets (tokens from either input or target prompts) or unrestricted tokens. Evaluation combines human annotation (50% of results) with VLM-based classifiers (InstructBLIP, LLaVA-1.5, CLIP) to assess attack success rates across 100 prompts with contrasting entity pairs.

## Key Results
- Attack success rates are highly asymmetric - some entity swaps succeed 60% of the time while reverse swaps fail below 5%
- Multiple Token Perturbation outperforms Single Token Perturbation with 26.4% vs 24.4% ASR
- ∆2 (baseline distance difference) enables estimation of successful attack likelihood
- Certain adjectives like color resist adversarial attacks with very low success rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asymmetric attack success rates arise from the model's inherent bias in generating certain entities
- Mechanism: T2I models encode stronger representations for frequently seen objects in training data
- Core assumption: Training data distribution creates stronger internal representations for common entities
- Evidence anchors: Abstract mentions 60% vs 5% success probability differences; section 5 discusses inherent model biases from training data
- Break condition: Uniform training data distribution would eliminate asymmetric bias

### Mechanism 2
- Claim: Baseline distance difference (∆2) between input and target prompts determines attack feasibility
- Mechanism: Target prompts closer to neutral baseline than input prompts in CLIP space succeed more often
- Core assumption: CLIP embeddings preserve semantic distance relationships that correlate with generation difficulty
- Evidence anchors: Section 5 provides ∆2 formula and states it enables attack likelihood estimation
- Break condition: If CLIP embeddings don't correlate with generation difficulty, ∆2 won't predict attack success

### Mechanism 3
- Claim: Multiple token perturbation strategy is more effective than single token perturbation
- Mechanism: CLIP text encoder behaves more like bag-of-words model, making simultaneous token replacement more effective
- Core assumption: CLIP models have reduced emphasis on semantic and syntactical relationships compared to LLMs
- Evidence anchors: Section 4 shows 26.4% vs 24.4% ASR and explains CLIP's bag-of-words-like behavior
- Break condition: If T2I models develop stronger contextual understanding like LLMs, single token perturbation could become equally effective

## Foundational Learning

- Concept: Adversarial suffix generation through gradient-based optimization
  - Why needed here: Attack requires finding optimal token sequences that shift CLIP embeddings from input to target
  - Quick check question: How does the score function S(x1:n) balance movement away from input and toward target embeddings?

- Concept: CLIP embedding space and cosine similarity
  - Why needed here: Attack success depends on measuring semantic similarity between prompts in CLIP space
  - Quick check question: Why do we use cosine similarity rather than Euclidean distance for comparing text embeddings?

- Concept: Perplexity as a measure of naturalness
  - Why needed here: Study investigates whether more natural prompts are easier to attack
  - Quick check question: What does a lower perplexity score indicate about a text sequence's relationship to natural language?

## Architecture Onboarding

- Component map: Text prompt → CLIP text encoder → U-Net cross-attention → Stable Diffusion v2-1-base → image generation → evaluation
- Critical path: Text prompt → CLIP embedding → U-Net cross-attention → image generation → evaluation
- Design tradeoffs: Single vs. multiple token perturbation (speed vs. effectiveness), restricted vs. unrestricted token sets (imperceptibility vs. attack success)
- Failure signatures: Local maxima in optimization, transfer failure across T2I models, adjective-specific resistance
- First 3 experiments:
  1. Run forward and backward attacks on 5 entity pairs to observe asymmetry
  2. Compare Single vs. Multiple Token Perturbation on 10 pairs
  3. Measure ∆2 for 20 random pairs to test correlation with ASR

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors in the model's internal beliefs lead to the observed asymmetric attack success rates?
- Basis in paper: [explicit] The paper discusses asymmetric attack success rates and proposes probing metrics to establish indicative signals from model beliefs
- Why unresolved: Paper identifies 60% vs 5% success probability conditions but doesn't fully explain underlying reasons for asymmetries
- What evidence would resolve it: Experiments isolating specific internal model features or biases that correlate with attack success rates

### Open Question 2
- Question: How can the Base Success Rate (BSR) be approximated without precomputation to improve attack predictors?
- Basis in paper: [explicit] Paper mentions combining BSR and ∆2 can predict attack likelihood, but BSR precomputation is a limitation
- Why unresolved: No method provided to estimate BSR on-the-fly or using alternative metrics
- What evidence would resolve it: Developing method to estimate BSR in real-time or using precomputation-free metrics

### Open Question 3
- Question: Why do certain adjectives resist adversarial attacks, and what are the underlying mechanisms?
- Basis in paper: [explicit] Paper observes very low attack success rates for certain adjectives like color
- Why unresolved: Paper leaves further analysis of this phenomenon as future work
- What evidence would resolve it: Experiments focusing on adjective-model interaction and testing different attack strategies

## Limitations
- Results based on single T2I model (Stable Diffusion v2-1-base), limiting generalizability
- Human evaluation covers only 50% of attack results, potentially introducing sampling bias
- Probing metrics show correlation but don't establish causation between indicators and attack success
- Restricted token set approach limits effectiveness, with only 24.4% ASR compared to 36.1% with unrestricted tokens

## Confidence
- **High Confidence**: Asymmetric attack success rates (60% vs 5%) are well-supported by empirical results across 100 prompts
- **Medium Confidence**: Correlation between ∆2 and attack success has theoretical grounding but shows only moderate predictive power
- **Low Confidence**: Proposed mechanisms explaining why certain entity swaps succeed more readily remain speculative without causal evidence

## Next Checks
1. **Cross-model generalization test**: Evaluate same entity swap pairs across multiple T2I models (SDXL, DALL-E, Midjourney) to determine if asymmetries persist
2. **Ablation study on token perturbation**: Systematically compare Single vs. Multiple Token Perturbation across different entity types and token positions
3. **Causal analysis of probing metrics**: Design controlled experiments where prompts have similar ∆2 values but different success rates to test metric's genuine predictive power