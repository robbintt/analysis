---
ver: rpa2
title: Calibrated Language Models Must Hallucinate
arxiv_id: '2311.14648'
source_url: https://arxiv.org/abs/2311.14648
tags:
- facts
- training
- which
- distribution
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proves that calibrated language models must hallucinate
  certain types of facts, even with perfect training data and no adversarial prompts.
  The key idea is that, for arbitrary facts whose truth cannot be determined from
  the training data, the probability of generating a hallucination is close to the
  fraction of facts that occur exactly once in the training data (a "Good-Turing"
  estimate).
---

# Calibrated Language Models Must Hallucinate

## Quick Facts
- arXiv ID: 2311.14648
- Source URL: https://arxiv.org/abs/2311.14648
- Reference count: 40
- Key outcome: Calibrated language models must hallucinate certain facts at rates close to the fraction of facts appearing exactly once in training data, even with perfect data and no adversarial prompts

## Executive Summary
This paper establishes a fundamental theoretical result about calibrated language models: they must hallucinate certain types of facts at predictable rates, even under ideal conditions. The key insight is that for arbitrary facts (those that cannot be inferred from training data), the probability of hallucination is bounded by the Good-Turing estimate of missing facts, specifically the fraction of facts appearing exactly once in the training corpus. This creates a statistical lower bound on hallucination rates that cannot be eliminated through calibration alone. The paper distinguishes between arbitrary facts (which must be hallucinated) and systematic facts (like arithmetic) that can be correctly generated without appearing in training data.

## Method Summary
The paper analyzes a synthetic data model where documents contain exactly one fact each, with a surjective function mapping documents to factoids. It defines a semantic-level calibration condition appropriate for generative models and proves that for arbitrary facts, the hallucination rate is bounded below by the Good-Turing estimate of missing mass minus the calibration error. The theoretical framework uses statistical distance to relate calibration error to the difference between the model's probability distribution and the true distribution. The main theorem shows that even perfectly calibrated models must hallucinate at rates determined by the sparsity of facts in the training data.

## Key Results
- Calibrated models must hallucinate arbitrary facts at rates close to the Good-Turing estimate of missing facts
- Hallucination rates are particularly high for facts appearing exactly once in training data (monofacts)
- No statistical reason for hallucination exists for systematic facts or facts appearing multiple times
- The theoretical lower bound on hallucination is unavoidable for calibrated generative models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Calibrated generative models must hallucinate on arbitrary facts that occur exactly once in training data.
- **Mechanism:** The Good-Turing estimator bounds the missing mass of unobserved facts. For arbitrary facts, the probability of hallucination equals the fraction of facts that appear exactly once in training data.
- **Core assumption:** Training data is perfect (contains only facts, no hallucinations), and facts are arbitrary (cannot be inferred from training data).
- **Evidence anchors:**
  - [abstract]: "the probability of generating a hallucination is close to the fraction of facts that occur exactly once in the training data (a 'Good-Turing' estimate)"
  - [section 1]: "if the maximum probability of any fact is bounded, we show that the probability of generating a hallucination is close to the fraction of facts that occur exactly once in the training data"
  - [corpus]: Weak evidence - corpus shows related work on hallucination but doesn't directly address this specific mechanism

### Mechanism 2
- **Claim:** Calibration error creates statistical lower bound on hallucination rate.
- **Mechanism:** Miscalibration measures how far the model's probabilities deviate from ideal calibration. Higher miscalibration allows lower hallucination rates, but even perfectly calibrated models must hallucinate at a rate determined by the Good-Turing estimate.
- **Core assumption:** Model is calibrated to the training distribution.
- **Evidence anchors:**
  - [abstract]: "language models that satisfy a statistical calibration condition appropriate for generative language models"
  - [section 1]: "Our notion differs from prior uses of calibration in LMs which were at the token-level"
  - [corpus]: Moderate evidence - corpus contains related work on calibration but focuses more on token-level calibration

### Mechanism 3
- **Claim:** Sparse facts (many more hallucinations than true facts) amplify hallucination necessity.
- **Mechanism:** When there are exponentially more plausible false facts than true facts, even calibrated models must hallucinate to maintain proper probability distribution over all possible facts.
- **Core assumption:** Factoids are sparse - exponentially more false than true facts.
- **Evidence anchors:**
  - [abstract]: "the ratio of the number of arbitrary facts to similar pieces of information that are false, which is exponentially small"
  - [section 4]: "Assumption 1 below requires that there are many more falsehoods in Y than facts"
  - [corpus]: Strong evidence - corpus contains multiple papers discussing sparsity and hallucination relationships

## Foundational Learning

- **Concept:** Good-Turing estimation
  - Why needed here: Provides the theoretical foundation for bounding the missing mass of unobserved facts
  - Quick check question: What does the Good-Turing estimator measure in the context of language model hallucination?

- **Concept:** Statistical calibration
  - Why needed here: Defines the conditions under which language models must hallucinate
  - Quick check question: How does generative calibration differ from token-level calibration in language models?

- **Concept:** Sparse fact distributions
  - Why needed here: Explains why some types of facts are more prone to hallucination than others
  - Quick check question: What makes a factoid distribution "sparse" in the context of this paper?

## Architecture Onboarding

- **Component map:** Training data processor → Fact extraction module → Calibration checker → Hallucination estimator
- **Critical path:** 1. Extract facts from training data 2. Compute Good-Turing estimate 3. Calculate calibration error 4. Determine hallucination lower bound
- **Design tradeoffs:** Computational cost vs. accuracy in estimating Good-Turing estimate; Granularity of factoid extraction vs. computational efficiency; Calibration precision vs. model performance
- **Failure signatures:** High hallucination rate despite good calibration; Low hallucination rate but poor calibration; Inconsistent Good-Turing estimates across training runs
- **First 3 experiments:** 1. Test hallucination rates on controlled synthetic datasets with known fact distributions 2. Compare calibration error vs. hallucination rate across different model architectures 3. Measure impact of fact sparsity on hallucination rates in real-world datasets

## Open Questions the Paper Calls Out

- **Open Question 1:** How can we convert a pretrained (calibrated) model to one that is good at factual prediction?
  - Basis in paper: [explicit] The paper mentions this as an interesting question for future work, suggesting that distinguishing systematic facts from arbitrary ones may be a key step.
  - Why unresolved: The paper does not provide a concrete method for this conversion, only suggesting that it may be possible with today's LMs.
  - What evidence would resolve it: A proposed method for converting a calibrated model to a factual prediction model, along with experimental results demonstrating its effectiveness.

- **Open Question 2:** What is the difference between fabricating a non-existent book title and generating a mathematical statement like "17 < 124398", if neither has ever been written down?
  - Basis in paper: [explicit] The paper asks this question, suggesting that humans know the latter can be manufactured while the former cannot, and wonders if LMs can similarly represent this distinction.
  - Why unresolved: The paper does not provide an answer to this question, only posing it as a point of interest.
  - What evidence would resolve it: An analysis of how LMs represent and distinguish between different types of facts, or a method for training LMs to better distinguish between systematic and arbitrary facts.

- **Open Question 3:** How can we reduce hallucination in LMs, particularly for types of facts with a high monofact rate?
  - Basis in paper: [explicit] The paper suggests that hallucination is more likely for facts that often occur just once in the training data, and asks how this insight can be used to further reduce hallucination.
  - Why unresolved: The paper does not provide a concrete method for reducing hallucination based on the monofact rate, only suggesting it as a potential approach.
  - What evidence would resolve it: A proposed method for reducing hallucination based on the monofact rate, along with experimental results demonstrating its effectiveness.

## Limitations
- Results apply specifically to calibrated generative models with ideal assumptions about training data
- Theoretical framework focuses on semantic-level calibration that may not align with practical training regimes
- Simplifying assumptions about fact sparsity may not hold in actual training scenarios

## Confidence
- **High** confidence in core mathematical results about calibrated models and Good-Turing estimation
- **Medium** confidence in practical implications for real-world language models
- **Low** confidence in claims about model behavior under different training regimes or with non-i.i.d. training data

## Next Checks
1. Test the theoretical bounds by creating controlled synthetic datasets with known fact distributions and measuring actual hallucination rates in calibrated language models
2. Evaluate whether popular language models actually satisfy the semantic-level calibration condition assumed in the paper, and measure the gap between theoretical and observed hallucination rates
3. Empirically measure the sparsity of different types of facts in real training data to verify whether the assumption of exponentially more false than true facts holds across various domains