---
ver: rpa2
title: Federated Learning with Manifold Regularization and Normalized Update Reaggregation
arxiv_id: '2311.05924'
source_url: https://arxiv.org/abs/2311.05924
tags:
- local
- global
- fedmrur
- learning
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces FedMRUR, a federated learning method that
  addresses two key challenges: model inconsistency due to data heterogeneity and
  the near-orthogonality of client updates. The authors propose two main techniques:
  a hyperbolic graph manifold regularizer that enforces closeness of data representations
  in a low-dimensional hyperbolic space, and a normalized update aggregation scheme
  that compensates for global update norm reduction.'
---

# Federated Learning with Manifold Regularization and Normalized Update Reaggregation

## Quick Facts
- **arXiv ID:** 2311.05924
- **Source URL:** https://arxiv.org/abs/2311.05924
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art accuracy with O(1/√(SKT)) convergence rate for non-convex objectives in federated learning

## Executive Summary
This paper introduces FedMRUR, a federated learning method that addresses two key challenges: model inconsistency due to data heterogeneity and the near-orthogonality of client updates. The authors propose two main techniques: a hyperbolic graph manifold regularizer that enforces closeness of data representations in a low-dimensional hyperbolic space, and a normalized update aggregation scheme that compensates for global update norm reduction. Theoretical analysis proves linear speedup with convergence rate O(1/√(SKT)) for non-convex objectives. Experiments on CIFAR-10/100 and TinyImagenet datasets demonstrate that FedMRUR achieves state-of-the-art accuracy with faster convergence compared to baselines like FedAvg, FedProx, SCAFFOLD, and MoFedSAM.

## Method Summary
FedMRUR is a federated learning algorithm that combines hyperbolic graph manifold regularization with normalized update aggregation. The method uses ResNet-18 with group normalization as the backbone, SAM optimizer for sharpness-aware minimization, and client momentum. During local training, each client applies a hyperbolic graph manifold regularizer that enforces similarity of data representations in hyperbolic space. The server aggregates client updates using a normalized scheme where client update norms are averaged as the global update norm, and the sum of client updates is normalized as the global update direction. This approach addresses model inconsistency from data heterogeneity while preventing global update norm shrinkage.

## Key Results
- Achieves state-of-the-art accuracy on CIFAR-10/100 and TinyImagenet datasets with faster convergence than FedAvg, FedProx, SCAFFOLD, and MoFedSAM
- Theoretical convergence rate of O(1/√(SKT)) with linear speedup for non-convex objectives under partial client participation
- Ablation studies confirm effectiveness of both hyperbolic regularization and normalized aggregation components
- Method is robust to different levels of data heterogeneity and client participation rates

## Why This Works (Mechanism)

### Mechanism 1: Hyperbolic Graph Manifold Regularization
The hyperbolic space provides a natural representation for graph-structured data like neural networks, where distances reflect functional similarity better than Euclidean distances. The squared Lorentzian distance between representations acts as a regularization term that constrains the divergence between local and global models. This reduces model inconsistency by enforcing similarity of data representations in hyperbolic space rather than Euclidean parameter space.

### Mechanism 2: Normalized Update Aggregation
Instead of averaging client updates directly, the server aggregates client update norms as the global update norm and normalizes the sum of client updates as the global update direction. This compensates for global update norm reduction caused by near-orthogonal client updates due to data heterogeneity. Each client's contribution increases from its projection on the global direction to its full norm magnitude.

### Mechanism 3: Combined Convergence Improvement
The combination of hyperbolic regularization and normalized aggregation achieves O(1/√(SKT)) convergence rate with linear speedup. Hyperbolic regularization reduces model inconsistency during local training, while normalized aggregation prevents global update norm shrinkage. Together, they enable faster convergence that scales linearly with the number of clients.

## Foundational Learning

- **Hyperbolic geometry and Lorentz model**: Why needed - The hyperbolic space provides a natural representation for graph-structured data like neural networks, where distances reflect functional similarity better than Euclidean distances. Quick check - What is the key difference between how volume grows in hyperbolic vs Euclidean space as radius increases?

- **Federated learning and client drift**: Why needed - Understanding how data heterogeneity causes model inconsistency across clients is fundamental to why FedMRUR's techniques are necessary. Quick check - What causes the near-orthogonality of client updates in federated learning?

- **Stochastic optimization with momentum**: Why needed - FedMRUR builds on momentum-based optimization (SAM), so understanding how momentum terms affect convergence is important. Quick check - How does the momentum term in FedMRUR differ from standard momentum SGD?

## Architecture Onboarding

- **Component map**: Server broadcasts → Client trains with hyperbolic regularization → Client uploads update → Server aggregates with normalized scheme → Server updates global model

- **Critical path**: Server broadcasts global model → Client performs local training with hyperbolic graph fusion regularization and SAM optimizer → Client uploads update → Server aggregates using normalized update aggregation (norm averaging + direction normalization) → Server updates global model

- **Design tradeoffs**: Hyperbolic regularization adds computation for representation extraction and Lorentz distance calculation; normalized aggregation requires storing and processing all client update norms; SAM optimizer doubles gradient computation cost per iteration. Benefits include better model consistency handling, faster convergence, and robustness to heterogeneity.

- **Failure signatures**: Convergence plateaus early (may indicate insufficient hyperbolic regularization strength); oscillations in training (could suggest learning rate mismatch); poor performance on heterogeneous data (may indicate need for stronger regularization or different aggregation).

- **First 3 experiments**: 1) Baseline comparison: Run FedAvg vs FedMRUR on CIFAR-10 with moderate heterogeneity to verify convergence improvement; 2) Ablation study: Test FedMRUR without hyperbolic regularization to measure its contribution; 3) Parameter sensitivity: Vary γ (hyperbolic regularization strength) and ηg/ηl ratios to find optimal settings for a given dataset.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of FedMRUR scale with the dimensionality of the hyperbolic space used for the manifold regularizer? The paper mentions using a low-dimensional hyperbolic space but does not explore the impact of varying this dimensionality. Experiments varying the dimensionality and measuring the impact on convergence speed and final accuracy would provide insights into the optimal choice.

### Open Question 2
How does FedMRUR perform under asynchronous client updates, where clients may have different local training intervals? The theoretical analysis assumes synchronous updates, but the paper does not discuss the implications of asynchronous updates on convergence or performance. Extending the convergence analysis to the asynchronous case and conducting experiments with different local training intervals would address this question.

### Open Question 3
Can the hyperbolic graph manifold regularizer be extended to other types of neural network architectures beyond ResNet, such as transformers or graph neural networks? The paper focuses on ResNet architectures and does not explore the applicability of the regularizer to other model types. Conducting experiments with FedMRUR on various neural network architectures would provide insights into the broader applicability of the method.

## Limitations

- The paper lacks implementation details for key components like the hyperbolic graph manifold regularizer and normalized update aggregation scheme, making faithful reproduction challenging
- Theoretical analysis relies on assumptions about client data distributions and update behaviors that may not hold in practice
- Computational overhead from hyperbolic regularization and SAM optimizer may limit scalability to larger models or datasets

## Confidence

- **Theoretical analysis**: High confidence in the theoretical convergence analysis and its assumptions
- **Experimental results**: Medium confidence in the experimental results due to lack of reproducibility details
- **Proposed mechanisms**: Medium confidence in the proposed mechanisms based on supporting evidence from related work

## Next Checks

1. Implement a minimal version of the hyperbolic graph manifold regularizer and verify its numerical stability and computational overhead
2. Conduct ablation studies on CIFAR-10 with varying levels of data heterogeneity to isolate the contributions of hyperbolic regularization vs. normalized aggregation
3. Test the method's robustness to different client participation rates and local update steps to validate the theoretical convergence bounds in practice