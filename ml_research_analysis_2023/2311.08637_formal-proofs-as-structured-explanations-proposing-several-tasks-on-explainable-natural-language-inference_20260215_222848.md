---
ver: rpa2
title: 'Formal Proofs as Structured Explanations: Proposing Several Tasks on Explainable
  Natural Language Inference'
arxiv_id: '2311.08637'
source_url: https://arxiv.org/abs/2311.08637
tags:
- explanations
- proofs
- language
- natural
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes using formal proofs from a natural language
  theorem prover to create structured explanations for natural language inference
  (NLI) tasks. The method leverages the tableau proof system to generate detailed,
  interpretable proofs that serve as explanations.
---

# Formal Proofs as Structured Explanations: Proposing Several Tasks on Explainable Natural Language Inference

## Quick Facts
- **arXiv ID:** 2311.08637
- **Source URL:** https://arxiv.org/abs/2311.08637
- **Reference count:** 13
- **Key outcome:** Formal proofs from a high-precision logic-based NLI system are used to create structured explanations for NLI tasks.

## Executive Summary
This paper proposes using formal proofs from the LANG PRO natural language theorem prover as structured explanations for natural language inference (NLI) tasks. The approach leverages the tableau proof system to generate detailed, interpretable proofs that overcome limitations of existing explainable NLI methods, such as unreliable automatic evaluation and coarse-grained explanations. Four tasks are proposed with increasing granularity, ranging from simple lexical relations to complete formal proofs. The method aims to provide reliable, consistent explanations that can be automatically evaluated while preserving semantic meaning through the naturalness of λ-calculus representations.

## Method Summary
The method uses tableau proofs from LANG PRO, a high-precision logic-based NLI system, as the foundation for creating structured explanations. The proofs are semi-automatically collected and processed to extract lexical relations, inference rules, and complete proof structures. Four tasks are proposed: (1) identifying lexical relations between premise and hypothesis, (2) identifying both rules and lexical relations, (3) reconstructing unlabeled proofs, and (4) generating complete proofs. The approach relies on the naturalness of λ-calculus terms to convert formal representations back to surface forms while preserving semantic content.

## Key Results
- Tableau proofs provide a reliable source of structured explanations with high precision (94% on SICK dataset)
- The four proposed tasks increase in granularity from lexical relations to complete proofs
- Structured explanations enable unambiguous automatic evaluation unlike free-text explanations
- The approach addresses key limitations in existing explainable NLI tasks including unreliable evaluation and coarse-grained explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The tableau proof system generates structured explanations by systematically refuting counterexamples to an inference relation.
- Mechanism: The proof starts with both premise and hypothesis marked as true (modeling a potential counterexample). Inference rules decompose these terms, creating branches that represent different logical possibilities. If all branches close due to contradictions, the original inference relation is proven valid.
- Core assumption: The semantic tableau method provides a complete and sound refutation-based proof system that can capture natural language inference patterns.
- Evidence anchors:
  - [abstract] "The framework is based on the semantic tableau method, a well-studied proof system in formal logic. Like the semantic tableau, the framework is driven by refutation -- something is proved if and only if its counterexample was not refuted."
  - [section] "The tableau grows by applying rules from the predefined set of rules to the existing nodes and breaking the antecedent terms into smaller pieces."
- Break condition: If the tableau fails to close for valid inferences (incomplete rule set) or if the semantic tableau method cannot capture certain linguistic phenomena, the structured explanations will be unreliable or incomplete.

### Mechanism 2
- Claim: The naturalness of the λ-calculus terms allows extraction of surface forms that preserve the structure while avoiding semantic parsing complexity.
- Mechanism: Each λ-term in the tableau represents a natural constituent or constituent with trace. The system converts these terms back to their original surface forms using the "naturalness" property, enabling explanations in human-readable text without requiring full semantic parsing.
- Core assumption: The λ-terms used in LANG PRO are sufficiently natural to map back to surface forms while maintaining semantic content.
- Evidence anchors:
  - [section] "Fortunately, this is possible with the help of the naturalness of the λ-terms, as each term represents a constituent or a constituent with a trace. It is important to emphasize the naturalness of natural logic formulas."
- Break condition: If the naturalness assumption fails (terms become too abstract or non-compositional), the surface form extraction will lose semantic meaning, breaking the explanation quality.

### Mechanism 3
- Claim: The semi-automatic collection of proofs provides high-precision, consistent structured explanations that overcome the evaluation problems of free-text explanations.
- Mechanism: LANG PRO's high precision (94% on SICK) ensures most proofs are correct. The structured nature of proofs (trees with nodes and edges) enables unambiguous automatic evaluation, unlike BLEU-based evaluation of free-text explanations which fails due to linguistic variability.
- Core assumption: LANG PRO's precision is sufficient to generate reliable proofs at scale, and the structured format eliminates the evaluation ambiguity present in natural language explanations.
- Evidence anchors:
  - [abstract] "The formal proofs are semi-automatically obtained from a high-precision logic-based NLI system, providing a reliable and consistent source of structured explanations."
  - [section] "LANG PRO achieves 84% accuracy on SICK with 94% of precision (Abzianidze, 2020)"
- Break condition: If LANG PRO's precision drops on more complex NLI problems, or if the structured proofs fail to capture important inference patterns, the explanation quality and reliability will degrade.

## Foundational Learning

- Concept: Semantic tableau method and refutation-based proof systems
  - Why needed here: Understanding how tableau proofs work is essential to grasp why they provide reliable structured explanations for NLI
  - Quick check question: In a tableau proof for contradiction, what does it mean when all branches close?

- Concept: Natural logic and λ-calculus representations for natural language semantics
  - Why needed here: The proofs use λ-calculus terms that need to be converted back to surface forms while preserving meaning
  - Quick check question: Why does the paper emphasize that λ-terms are "natural" and what problem does this solve?

- Concept: Inference rules and monotonicity in natural logic
  - Why needed here: Understanding rules like (↑|⌣) for upward monotonicity is crucial for interpreting how tableau proofs decompose linguistic expressions
  - Quick check question: What is the difference between upward and downward monotone quantifiers in tableau proofs?

## Architecture Onboarding

- Component map: Input NLI problem -> LANG PRO theorem prover -> Tableau proof generator -> Surface form converter -> Explanation formatter -> Evaluation framework
- Critical path: Input NLI problem → LANG PRO inference → Tableau proof generation → Surface form conversion → Explanation formatting → Automatic evaluation
- Design tradeoffs:
  - Precision vs. coverage: High-precision logic-based system may miss some valid inferences that deep learning models catch
  - Structure vs. naturalness: Structured proofs are evaluable but may not capture all aspects of human reasoning
  - Automation vs. verification: Semi-automatic proof collection reduces human effort but requires expert validation of edge cases
- Failure signatures:
  - Tableau proof doesn't close for valid inferences (incomplete rule set)
  - Surface forms lose semantic meaning during conversion (naturalness assumption fails)
  - Evaluation metrics don't align with human judgment (structured format mismatch)
  - Performance drops significantly on out-of-distribution NLI problems
- First 3 experiments:
  1. Validate surface form conversion by comparing λ-term mappings on a small set of SICK problems with known ground truth
  2. Test automatic evaluation metrics by generating structured explanations for problems with known reference proofs
  3. Measure precision drop when applying LANG PRO to more diverse NLI datasets beyond SICK

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the approach handle neutral inference cases in the proposed explainable NLI tasks?
- Basis in paper: [explicit] The paper states that "Structuring explanations for neutral cases require special care and we leave this for the feature research."
- Why unresolved: The paper does not provide a concrete method for structuring explanations for neutral cases, which is one of the three main inference relations in NLI tasks.
- What evidence would resolve it: A detailed methodology or experimental results showing how neutral cases are handled and explained in the proposed tasks.

### Open Question 2
- Question: How does the granularity of explanations in the proposed tasks compare to human-generated explanations in terms of informativeness and usefulness?
- Basis in paper: [inferred] The paper mentions that the tasks can be ordered according to difficulty defined in terms of the granularity of explanations, but does not provide a comparison with human-generated explanations.
- Why unresolved: While the paper proposes tasks with increasing granularity, it does not compare these structured explanations to the quality and usefulness of human-generated explanations.
- What evidence would resolve it: A study comparing the proposed structured explanations with human-generated explanations in terms of informativeness, usefulness, and alignment with human reasoning processes.

### Open Question 3
- Question: How does the proposed approach perform on more challenging NLI datasets beyond SICK, such as SNLI or MNLI?
- Basis in paper: [explicit] The paper mentions that LANG PRO achieves 84% accuracy on SICK with 94% precision, but does not provide results for more complex datasets.
- Why unresolved: The paper only provides performance metrics for the SICK dataset, which is known to be less challenging than other NLI datasets like SNLI or MNLI.
- What evidence would resolve it: Experimental results showing the performance of LANG PRO and the proposed explainable NLI tasks on larger and more complex datasets like SNLI or MNLI, including accuracy, precision, and the quality of generated explanations.

## Limitations

- The approach may not generalize well to more complex NLI datasets beyond SICK
- Neutral inference cases require special handling not addressed in the current work
- The precision-coverage tradeoff may limit the system's ability to capture all valid inferences

## Confidence

- High confidence: The tableau proof system's theoretical foundation and its application to NLI are well-established
- Medium confidence: The precision metrics from LANG PRO are reliable but may not generalize to all NLI domains
- Low confidence: The scalability of the approach to more complex linguistic phenomena and diverse datasets

## Next Checks

1. Validate surface form conversion by comparing λ-term mappings on a small set of SICK problems with known ground truth
2. Test automatic evaluation metrics by generating structured explanations for problems with known reference proofs
3. Measure precision drop when applying LANG PRO to more diverse NLI datasets beyond SICK