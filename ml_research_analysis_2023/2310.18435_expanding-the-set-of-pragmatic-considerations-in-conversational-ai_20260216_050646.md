---
ver: rpa2
title: Expanding the Set of Pragmatic Considerations in Conversational AI
arxiv_id: '2310.18435'
source_url: https://arxiv.org/abs/2310.18435
tags:
- systems
- conversational
- pragmatic
- user
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines the pragmatic limitations of current conversational
  AI systems through the lens of relevance theory. The authors identify two main dimensions
  where these systems fall short: preserving local meaning and incorporating external
  context.'
---

# Expanding the Set of Pragmatic Considerations in Conversational AI

## Quick Facts
- arXiv ID: 2310.18435
- Source URL: https://arxiv.org/abs/2310.18435
- Reference count: 40
- Primary result: Current conversational AI systems have significant pragmatic limitations in preserving local meaning and incorporating external context, despite being syntactically correct.

## Executive Summary
This paper examines pragmatic limitations in conversational AI systems through the lens of relevance theory, identifying two key dimensions where these systems fall short: preserving local meaning across multiple propositions and incorporating external context. The authors demonstrate how current systems often provide syntactically correct but pragmatically deficient responses that fail to address all aspects of user intent or utilize obvious contextual information. They propose a taxonomy of pragmatic considerations and argue that truly context-sensitive systems require the ability to address multiple pragmatic limitations simultaneously. The paper calls for a unified framework for understanding and addressing these limitations to facilitate the development of more effective and user-friendly conversational AI systems.

## Method Summary
The authors employ a qualitative analysis approach, using relevance theory as a theoretical framework to examine pragmatic limitations in conversational AI systems. They analyze examples from various conversational AI applications to illustrate how these systems fail to preserve local meaning and incorporate external context. The methodology involves categorizing pragmatic failures into a proposed taxonomy and discussing potential mechanisms for addressing these limitations through modular architectural designs and pre-existing knowledge sources.

## Key Results
- Current conversational AI systems often fail to address all aspects of user intent when utterances contain multiple related propositions
- Systems lack mechanisms to incorporate relevant external context such as location, time, and user status into responses
- The greatest challenge lies in designing centralized systems that can address multiple pragmatic limitations simultaneously rather than treating them as isolated issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pragmatic failures stem from inability to preserve local meaning across multiple propositions in user input.
- Mechanism: Current systems process each user utterance as a single semantic unit rather than decomposing it into multiple propositions and their relationships. This causes them to address only one aspect of a user's request while ignoring others.
- Core assumption: User utterances contain multiple related propositions that must be jointly processed for appropriate responses.
- Evidence anchors:
  - [abstract] "illustrate pragmatic limitations with examples that are syntactically appropriate, but have clear pragmatic deficiencies"
  - [section] "To generate relevant content, conversational AI systems must respond to all aspects of a user's meaning. A response that addresses one part of a user's intent may omit other related information."
  - [corpus] "Pragmatics in the Era of Large Language Models: A Survey on Datasets, Evaluation, Opportunities and Challenges" - weak evidence for specific proposition handling mechanisms
- Break condition: When user inputs become simple enough to require only single-proposition processing.

### Mechanism 2
- Claim: Conversational AI systems fail to incorporate external context due to reliance on conversational content alone.
- Mechanism: Systems lack the ability to integrate episodic features (location, time, user status) and conceptual knowledge from external sources into their response generation, leading to contextually inappropriate responses.
- Core assumption: Users expect conversational AI to have access to and use relevant external information beyond the immediate conversation.
- Evidence anchors:
  - [abstract] "We suggest two key limitations for conversational AI systems: preserving meaning and awareness of external context"
  - [section] "Failure to account for external information can generate pragmatic failures... People exploit context to provide appropriate detail and tailor their messages"
  - [corpus] "DiPlomat: A Dialogue Dataset for Situated Pragmatic Reasoning" - weak evidence for practical external context integration methods
- Break condition: When external context is explicitly provided by the user in the conversation.

### Mechanism 3
- Claim: Current conversational AI lacks mechanisms for handling inconsistent details and default reasoning.
- Mechanism: Systems cannot detect contradictions within user input or apply general principles when information is incomplete, leading to responses that fail to resolve ambiguities or inconsistencies.
- Core assumption: Human conversation regularly involves reasoning with incomplete or inconsistent information that must be resolved for appropriate responses.
- Evidence anchors:
  - [abstract] "We illustrate pragmatic limitations with examples that are syntactically appropriate, but have clear pragmatic deficiencies"
  - [section] "Conversations often require reasoning with inconsistent details... Systems that lack these abilities create pragmatic errors"
  - [corpus] "Data Augmentation for Conversational AI" - weak evidence for methods handling inconsistent details
- Break condition: When user input contains no ambiguities or inconsistencies.

## Foundational Learning

- Concept: Relevance Theory
  - Why needed here: The paper's framework is built on relevance theory, which explains how people expect and process relevant information in conversation.
  - Quick check question: What are the two key conditions for information to be considered relevant according to relevance theory?

- Concept: Local vs. Distal Propositional Content
  - Why needed here: Understanding the difference between propositions within a single utterance (local) and across conversation turns (distal) is crucial for identifying pragmatic failures.
  - Quick check question: How does failing to handle distal propositional content manifest in conversational AI responses?

- Concept: Episodic vs. Semantic Memory
  - Why needed here: The paper distinguishes between conversational context (episodic) and external knowledge (semantic) as sources of contextual information.
  - Quick check question: What type of memory would a conversational AI need to access to know the current weather conditions for a user's location?

## Architecture Onboarding

- Component map: User input → Proposition decomposition → Local meaning processing → Contextual enrichment → Response generation → Output
- Critical path: User input → Proposition decomposition → Local meaning processing → Contextual enrichment → Response generation → Output
- Design tradeoffs: Balancing between system complexity (handling multiple pragmatic factors) and performance (real-time response generation).
- Failure signatures: Responses that are syntactically correct but miss key user intents, fail to use obvious contextual information, or ignore self-corrections in user input.
- First 3 experiments:
  1. Test proposition decomposition by inputting multi-intent requests and measuring which intents are addressed.
  2. Evaluate external context integration by testing responses to location/time-dependent queries.
  3. Assess inconsistency detection by inputting contradictory information and measuring the system's ability to request clarification.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can conversational AI systems be designed to simultaneously address multiple pragmatic limitations rather than treating them as isolated issues?
- Basis in paper: [explicit] The authors argue that truly context-sensitive systems require the ability to address multiple pragmatic limitations simultaneously and that the greatest challenge is designing centralized systems that address multiple deficits.
- Why unresolved: Current research tends to examine specific pragmatic features independently for specific applications, treating pragmatics as a decentralized process that ignores the interdependent nature of many pragmatic limitations.
- What evidence would resolve it: Development and evaluation of a conversational AI system that demonstrates improved performance across multiple pragmatic dimensions simultaneously compared to systems addressing individual limitations in isolation.

### Open Question 2
- Question: What is the optimal balance between relying on larger language models versus incorporating pre-existing knowledge sources and modular designs to address pragmatic limitations?
- Basis in paper: [explicit] The authors acknowledge that larger models alone will not resolve pragmatic limitations and suggest that pre-existing knowledge sources, modular designs, and approaches that address dialogue phenomena are promising alternatives.
- Why unresolved: There is tension between the computational costs and environmental impact of larger models versus the effectiveness of more targeted, knowledge-based approaches for addressing specific pragmatic challenges.
- What evidence would resolve it: Comparative studies showing the effectiveness of different architectural approaches (large models vs. modular/knowledge-based systems) across various pragmatic dimensions and real-world applications.

### Open Question 3
- Question: How can conversational AI systems effectively incorporate and reason with inconsistent details in user inputs while maintaining appropriate user experience?
- Basis in paper: [explicit] The authors discuss how humans resolve inconsistent details effectively but they create challenges for conversational AI systems, providing examples like date/time inconsistencies and proposing clarification requests as a potential solution.
- Why unresolved: While previous work has proposed methods for generating clarification requests when uncertain, the challenge of identifying when clarification is required and how to formulate appropriate responses remains unsolved, particularly for real-world applications where dialogue accompanies activity.
- What evidence would resolve it: Development of a conversational AI system that can accurately detect and appropriately resolve inconsistent details in user inputs across various contexts and domains, with user studies demonstrating improved experience compared to current approaches.

## Limitations
- Analysis is primarily conceptual and qualitative rather than empirical
- Proposed taxonomy lacks quantitative validation through controlled experiments
- Does not provide concrete implementation details for addressing pragmatic limitations

## Confidence
- **High Confidence**: Theoretical foundation based on relevance theory is well-established in linguistics and cognitive science
- **Medium Confidence**: Taxonomy of pragmatic considerations provides useful framework but requires further validation
- **Low Confidence**: Proposed mechanisms for addressing limitations lack concrete implementation details and quantitative evidence

## Next Checks
1. **Systematic Evaluation Protocol**: Develop standardized test suite of multi-intent user queries to quantitatively measure how well current conversational AI systems preserve local meaning across propositions.

2. **Context Integration Benchmark**: Design controlled experiments testing conversational AI responses to queries requiring external context (location-based information, time-dependent responses).

3. **Inconsistency Resolution Testing**: Create dataset of user inputs containing intentional contradictions or ambiguities, then evaluate how different conversational AI systems detect and resolve these inconsistencies.