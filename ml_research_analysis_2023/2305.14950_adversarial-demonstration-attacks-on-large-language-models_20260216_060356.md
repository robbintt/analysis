---
ver: rpa2
title: Adversarial Demonstration Attacks on Large Language Models
arxiv_id: '2305.14950'
source_url: https://arxiv.org/abs/2305.14950
tags:
- in-context
- demonstrations
- attack
- learning
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of in-context learning (ICL)
  in large language models by introducing adversarial attacks that manipulate only
  the demonstration examples without altering the test input. The authors propose
  a method called advICL, which builds upon TextAttack to generate adversarial demonstrations
  that mislead the model's predictions.
---

# Adversarial Demonstration Attacks on Large Language Models

## Quick Facts
- arXiv ID: 2305.14950
- Source URL: https://arxiv.org/abs/2305.14950
- Reference count: 8
- Key outcome: Attack success rates reach up to 99.38% for GPT2-XL on DBpedia and 97.72% for LLaMA-7B on SST-2 in 8-shot settings, with adversarial demonstrations showing transferability across different test examples

## Executive Summary
This paper investigates the robustness of in-context learning (ICL) in large language models by introducing adversarial attacks that manipulate only the demonstration examples without altering the test input. The authors propose a method called advICL, which builds upon TextAttack to generate adversarial demonstrations that mislead the model's predictions. Their experiments show that as the number of demonstrations increases, the model becomes more vulnerable to attacks, with attack success rates reaching up to 99.38% for GPT2-XL on DBpedia and 97.72% for LLaMA-7B on SST-2 in 8-shot settings. Additionally, the adversarial demonstrations exhibit transferability across different test examples, further highlighting the security risks of ICL.

## Method Summary
The authors introduce advICL, an adversarial attack framework that targets in-context learning by modifying only the demonstration examples while keeping the test input unchanged. Built on the TextAttack framework, advICL applies word-level perturbations (insertion, deletion, neighboring character swap, substitution) to demonstration inputs with individual similarity constraints. The method enforces a cosine similarity threshold of 0.8 between original and adversarial demonstrations to maintain semantic coherence while still misleading the model. Experiments are conducted across three datasets (SST-2, TREC, DBpedia) with varying shot settings (1, 4, 8) and two model architectures (GPT2-XL, LLaMA-7B).

## Key Results
- Attack success rates increase with the number of demonstrations, reaching 99.38% for GPT2-XL on DBpedia in 8-shot settings
- Adversarial demonstrations exhibit strong transferability across different test examples within the same dataset
- LLaMA-7B shows vulnerability with 97.72% ASR on SST-2 in 8-shot settings, demonstrating that even smaller models are susceptible
- The attack maintains semantic similarity (cosine similarity ≥ 0.8) while achieving high success rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarially modified demonstrations can mislead LLMs even when the test input remains unchanged
- Mechanism: By perturbing only the demonstration examples (data-label pairs) using TextAttack-based transformations, the attacker manipulates the model's learned associations between inputs and labels. The model uses these corrupted demonstrations to infer the label for the clean test input, leading to incorrect predictions.
- Core assumption: LLMs rely heavily on demonstration examples for in-context learning and are sensitive to the semantic integrity of those examples
- Evidence anchors:
  - [abstract] "attackers can manipulate only the demonstrations without changing the input to perform an attack"
  - [section] "in contrast to attacks on text examples, advICL focuses exclusively on attacking preconditioned demonstrations without manipulating the input text example"
- Break condition: If similarity constraints between original and perturbed demonstrations are too strict, the adversarial perturbation may fail to mislead the model

### Mechanism 2
- Claim: Increasing the number of demonstrations amplifies model vulnerability to adversarial attacks
- Mechanism: More demonstrations increase the attack surface and probability that at least one corrupted example will significantly influence the model's prediction. This occurs because the model aggregates information from all demonstrations, so even subtle perturbations in multiple examples can compound to cause misclassification.
- Core assumption: Model sensitivity to demonstration order and content increases with demonstration count
- Evidence anchors:
  - [abstract] "as the number of demonstrations increases, the robustness of in-context learning would decrease"
  - [section] "Our results demonstrate that a larger number of demonstrations are prone to more threats to the robustness of in-context learning"
- Break condition: If the model implements robust selection or weighting of demonstrations, the effect of additional demonstrations may plateau or diminish

### Mechanism 3
- Claim: Adversarial demonstrations exhibit transferability across different test inputs
- Mechanism: Once an adversarial demonstration is crafted to mislead the model on one input, it can be reused (prepended) with different clean test inputs to consistently reduce model performance. This happens because the demonstration corrupts the model's general understanding of the task rather than targeting a specific input pattern.
- Core assumption: The adversarial demonstration alters the model's internal representation of the task itself, not just a single input-output mapping
- Evidence anchors:
  - [abstract] "adversarial demonstrations exhibit transferability to diverse input examples"
  - [section] "we expect our attack... can mislead the model to reduce the accuracy for any given clean test example without manipulating them"
- Break condition: If the model has strong input-specific context modeling, transferability may break down for diverse inputs

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: Understanding how demonstrations guide model predictions is essential to grasp why manipulating them is effective
  - Quick check question: How does the model use demonstration examples to predict the label for a new test input?

- Concept: Adversarial attacks in NLP
  - Why needed here: The attack methodology relies on standard NLP attack techniques adapted to demonstration inputs
  - Quick check question: What is the difference between attacking the input text versus attacking the demonstration examples?

- Concept: Cosine similarity and perturbation constraints
  - Why needed here: These constraints ensure the adversarial demonstration remains semantically close to the original while still misleading the model
  - Quick check question: Why is maintaining high cosine similarity between original and adversarial demonstrations important?

## Architecture Onboarding

- Component map: TextAttack framework -> Demonstration masking module -> Individual similarity constraints -> Transferability evaluation module

- Critical path:
  1. Load clean demonstrations and test inputs
  2. Apply adversarial perturbations to demonstrations only
  3. Evaluate attack success rate (ASR)
  4. Test transferability by reusing adversarial demonstrations with new inputs

- Design tradeoffs:
  - Strict similarity constraints → higher data quality but potentially lower ASR
  - More demonstrations → higher ASR but increased attack complexity
  - Black-box setting → more practical but potentially less effective than white-box

- Failure signatures:
  - ASR remains low despite multiple attack iterations
  - Transferability fails across different input types
  - Similarity scores fall below threshold, indicating low-quality adversarial examples

- First 3 experiments:
  1. Run advICL with 1-shot setting on SST-2 and measure ASR
  2. Increase to 8-shot and observe change in ASR
  3. Test transferability by reusing adversarial demonstrations across all test examples in SST-2

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but leaves several important questions unaddressed:

- How does the effectiveness of advICL vary across different types of NLP tasks (e.g., sentiment analysis vs. question classification vs. topic classification)?
- What is the relationship between the semantic similarity threshold for adversarial demonstrations and the attack success rate across different model sizes and architectures?
- How do different demonstration selection strategies (random vs. semantically similar vs. adversarially selected) impact the overall robustness of in-context learning systems?

## Limitations

- The attack effectiveness heavily depends on the specific templates and verbalizers used, which are referenced but not fully specified in the paper
- The paper demonstrates transferability within datasets but doesn't thoroughly investigate whether transferability breaks down across different model architectures or dataset domains
- The paper focuses on attack effectiveness but doesn't explore potential defenses or mitigation strategies

## Confidence

**High Confidence Claims**:
- Adversarial demonstrations can reduce model accuracy when prepended to test inputs (supported by concrete ASR metrics across multiple datasets and models)
- Attack success rate increases with the number of demonstrations (consistent results across 1-shot, 4-shot, and 8-shot settings)
- The attack method is reproducible using TextAttack framework with the specified constraints

**Medium Confidence Claims**:
- The transferability of adversarial demonstrations across diverse test examples (demonstrated within datasets but not across different domains or model types)
- The specific contribution of individual perturbation types (word insertion, deletion, etc.) to attack success (not fully isolated in experiments)

**Low Confidence Claims**:
- The paper's claims about "security risks" of ICL are asserted but not quantitatively compared to other attack vectors or threat models
- The generality of findings to real-world applications where ICL is used in more complex ways (chained reasoning, multi-step tasks, etc.)

## Next Checks

1. **Template Sensitivity Analysis**: Reproduce the attack using different template configurations for the same datasets to determine how sensitive the attack success rate is to template choice. This would validate whether the attack exploits fundamental model vulnerabilities or specific template-model interactions.

2. **Cross-Domain Transferability Test**: Apply adversarial demonstrations crafted for one dataset (e.g., SST-2) to test examples from a different domain (e.g., TREC or DBpedia) to measure how far transferability extends. This would reveal whether the attack corrupts general task understanding or dataset-specific patterns.

3. **Defense Implementation and Evaluation**: Implement and test simple defense mechanisms such as demonstration filtering, input sanitization, or model fine-tuning on adversarial demonstrations to establish baseline protection levels and identify which defenses are most effective against this attack vector.