---
ver: rpa2
title: 'BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language'
arxiv_id: '2305.19840'
source_url: https://arxiv.org/abs/2305.19840
tags:
- polish
- language
- benchmark
- retrieval
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BEIR-PL, a zero-shot information retrieval
  benchmark for the Polish language. The authors translated the English BEIR dataset
  into Polish, creating a new benchmark comprising 13 datasets for training and evaluating
  modern Polish language models for IR tasks.
---

# BEIR-PL: Zero Shot Information Retrieval Benchmark for the Polish Language

## Quick Facts
- arXiv ID: 2305.19840
- Source URL: https://arxiv.org/abs/2305.19840
- Reference count: 40
- One-line primary result: Introducing BEIR-PL, a zero-shot information retrieval benchmark for Polish language, demonstrating BM25's inferior performance due to Polish morphological complexity

## Executive Summary
This paper introduces BEIR-PL, a zero-shot information retrieval benchmark for the Polish language created by translating the English BEIR dataset. The authors evaluated various IR models including BM25, neural retrievers, and re-rankers on the Polish benchmark. Results showed BM25 performed significantly worse for Polish than English due to the language's high inflection and complex morphology. The study highlights the importance of examining individual dataset results rather than relying solely on overall averages, and provides a valuable resource for advancing Polish language IR research.

## Method Summary
The authors translated all accessible open IR datasets from the BEIR benchmark into Polish using Google Translate to create BEIR-PL. They then trained and evaluated multiple IR models including BM25 (sparse retrieval), neural retrievers like ICT bi-encoder, and various re-rankers (HerBERT-base, HerBERT-large, plT5-base, plT5-large). The models were assessed using standard IR metrics (NDCG@10, MRR@10, Recall@k) across 13 datasets. The evaluation compared these models against pre-existing multilingual models to establish baseline performance for Polish language retrieval.

## Key Results
- BM25 achieved significantly lower scores for Polish than for English, attributed to Polish's high inflection and intricate morphological structure
- T5-based re-rankers showed slight improvements over BERT-based re-rankers for Polish language retrieval
- Dense bi-encoders trained specifically for Polish showed competitive performance compared to multilingual models
- Individual dataset analysis revealed important variations in model performance that overall averages obscured

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Translating BEIR datasets to Polish enables evaluation of retrieval models in a morphologically complex language
- **Mechanism:** Machine translation of the original BEIR datasets to Polish provides a direct comparison between English and Polish retrieval performance, highlighting the impact of language-specific morphological complexity on retrieval effectiveness
- **Core assumption:** The quality of Google Translate output for Polish is sufficient to enable meaningful retrieval evaluation
- **Break condition:** If machine translation introduces systematic errors that degrade retrieval performance beyond what is explained by morphological complexity, the benchmark would not accurately reflect model performance

### Mechanism 2
- **Claim:** BM25 performs significantly worse on Polish than English due to high inflection and morphological complexity
- **Mechanism:** Polish's rich inflectional morphology creates many word forms for a single lexeme, making exact lexical matching (as in BM25) less effective compared to languages with simpler morphology
- **Core assumption:** The difference in BM25 performance between Polish and English is primarily due to morphological complexity rather than other factors like corpus size or translation quality
- **Break condition:** If BM25 performance differences are primarily due to other factors (e.g., translation quality, corpus size) rather than morphological complexity, the conclusion about Polish's morphological impact would be incorrect

### Mechanism 3
- **Claim:** Neural re-ranking models significantly improve retrieval performance over BM25, especially for semantically complex queries
- **Mechanism:** Cross-encoder and sequence-to-sequence re-ranking models can capture semantic relationships that lexical matching misses, allowing them to reorder BM25 results more effectively for queries requiring deeper understanding
- **Core assumption:** The improvement from re-ranking is consistent across different types of retrieval tasks and not limited to specific dataset characteristics
- **Break condition:** If re-ranking models only improve performance on specific dataset types or fail to generalize across the diverse BEIR tasks, the claimed benefit would be overstated

## Foundational Learning

- **Concept:** Information retrieval metrics (NDCG, MRR, Recall)
  - Why needed here: The paper uses these standard IR metrics to evaluate model performance across multiple datasets, requiring understanding of what each metric measures and when to use it
  - Quick check question: What's the key difference between NDCG and MRR in terms of what they measure about retrieval quality?

- **Concept:** Dense vs. sparse retrieval approaches
  - Why needed here: The paper compares BM25 (sparse) with neural retrievers (dense), requiring understanding of how these fundamentally different approaches work and their respective strengths/weaknesses
  - Quick check question: Why might a dense retriever outperform BM25 on some datasets but underperform on others?

- **Concept:** Cross-encoder vs. bi-encoder architectures
  - Why needed here: The paper evaluates both architectures for re-ranking, requiring understanding of their computational tradeoffs and when each is appropriate
  - Quick check question: What's the fundamental architectural difference between cross-encoders and bi-encoders that leads to their different computational costs?

## Architecture Onboarding

- **Component map:** Translate datasets -> Train BM25 baseline -> Train neural models (ICT, re-rankers) -> Evaluate all models -> Analyze results -> Publish models and benchmark
- **Critical path:** Translate datasets → Train/evaluate BM25 baseline → Train neural models → Evaluate all models → Analyze results → Publish models and benchmark
- **Design tradeoffs:** Used Google Translate for efficiency vs. potentially lower quality than human translation; trained on RTX 3090 GPUs vs. more powerful but expensive hardware; focused on Polish vs. broader multilingual coverage
- **Failure signatures:** Low performance on all models might indicate translation issues; poor re-ranking performance might indicate insufficient fine-tuning data; inconsistent results across datasets might indicate dataset-specific issues
- **First 3 experiments:**
  1. Verify BM25 performance on a small subset of translated data to check translation quality impact
  2. Test ICT model training on a single dataset before scaling to all datasets
  3. Evaluate a single re-ranker model (e.g., HerBERT-base) before training multiple variants

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of dense retrieval models compare to BM25 in specialized Polish domains like medicine or law?
- Basis in paper: [explicit] The paper shows BM25 performs worse for Polish than English due to high inflection, but doesn't deeply compare dense models in specialized domains
- Why unresolved: The paper evaluates on general BEIR datasets but doesn't focus on specialized Polish domains where inflection might have different effects
- What evidence would resolve it: Direct comparison of dense models vs BM25 on Polish medical, legal, or technical datasets with detailed analysis of performance gaps

### Open Question 2
- Question: What is the optimal approach for handling Polish compound words and inflection in dense retrieval models?
- Basis in paper: [inferred] The paper notes BM25's poor performance is partly due to Polish inflection and complex morphology, suggesting this affects IR models
- Why unresolved: The paper doesn't explore specific techniques for handling Polish morphological complexity in dense retrieval
- What evidence would resolve it: Experiments comparing different tokenization approaches, morphological preprocessing, or custom embeddings designed for Polish inflection

### Open Question 3
- Question: How transferable are the BEIR-PL benchmark results to real-world Polish IR applications?
- Basis in paper: [explicit] The paper notes the heterogeneous nature of BEIR datasets and recommends examining individual dataset results rather than overall averages
- Why unresolved: The paper establishes the benchmark but doesn't validate it against actual Polish IR use cases or applications
- What evidence would resolve it: Case studies applying BEIR-PL trained models to real Polish IR applications, measuring performance differences between benchmark and real-world settings

## Limitations

- Benchmark relies heavily on Google Translate for dataset translation, introducing uncertainty about translation quality and consistency
- Paper does not address potential domain-specific translation challenges across the diverse BEIR datasets
- Limited comparison of specific re-ranker variants (T5 vs BERT) based on small sample size

## Confidence

**High Confidence**: Claims about BM25's inferior performance on Polish due to morphological complexity are well-supported by empirical results showing consistent underperformance across multiple datasets

**Medium Confidence**: Claims about neural re-rankers improving over BM25 are supported by experimental results, but magnitude of improvement varies significantly across datasets

**Low Confidence**: Claims about specific advantages of T5-based re-rankers over BERT-based ones are based on limited comparisons and could be influenced by unspecified training details

## Next Checks

1. **Translation Quality Assessment**: Conduct a blind evaluation where native Polish speakers assess the quality of translated queries and documents across all 13 datasets, measuring semantic preservation and fluency to quantify the impact of translation artifacts on retrieval performance

2. **Cross-Lingual Transfer Validation**: Evaluate the same models on the original English BEIR datasets to establish a proper baseline for cross-lingual transfer, isolating the effect of language-specific challenges from model quality differences

3. **Dataset-Specific Analysis**: Perform detailed ablation studies on the three datasets with the largest performance gaps (MSMARCO, TREC-COVID, NQ) to identify whether morphological complexity, translation quality, or domain-specific factors drive the observed performance differences