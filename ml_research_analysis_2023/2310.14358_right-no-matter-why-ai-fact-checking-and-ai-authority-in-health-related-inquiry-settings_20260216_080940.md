---
ver: rpa2
title: 'Right, No Matter Why: AI Fact-checking and AI Authority in Health-related
  Inquiry Settings'
arxiv_id: '2310.14358'
source_url: https://arxiv.org/abs/2310.14358
tags:
- advice
- explanation
- opinion
- more
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores how people respond to AI-generated fact-checking
  feedback on health-related statements, examining whether the type of feedback (no
  explanation, implausible explanation, plausible explanation) influences belief changes.
  Participants rated health statements before and after receiving AI feedback, with
  the AI sometimes providing correct and sometimes incorrect advice.
---

# Right, No Matter Why: AI Fact-checking and AI Authority in Health-related Inquiry Settings

## Quick Facts
- arXiv ID: 2310.14358
- Source URL: https://arxiv.org/abs/2310.14358
- Reference count: 40
- Over half of participants changed their opinions toward AI feedback, regardless of explanation quality

## Executive Summary
This study investigates how people respond to AI-generated fact-checking feedback on health-related statements, testing whether feedback quality (no explanation, implausible explanation, plausible explanation) influences belief changes. Participants rated health statements before and after receiving AI feedback, with the AI sometimes providing correct and sometimes incorrect advice. The results show that AI feedback has a powerful persuasive effect regardless of explanation quality, with plausible explanations being slightly more effective but the mere presence of feedback having the largest impact. Even incorrect AI advice significantly shifted majority opinions toward false suggestions. Self-reported trust in the AI weakly correlated with actual belief changes, particularly when explanations were plausible. The findings raise concerns about the potential for AI to influence public health beliefs even when providing inaccurate information.

## Method Summary
The study used a pre/post belief rating design with 10 health-related statements from the TruthfulQA dataset. Participants rated each statement on a -10 to 10 scale before and after receiving AI feedback in one of three conditions: no explanation, implausible explanation, or plausible explanation. The AI feedback was generated using ChatGPT 3.5, with plausible explanations validated by independent raters. Analysis included multinomial logistic regression to predict opinion change direction, linear regression for magnitude of change, and correlation analysis between self-reported trust and actual belief changes.

## Key Results
- Over half of participants changed their opinions toward AI suggestions regardless of explanation quality
- Plausible explanations were slightly more persuasive than implausible ones, but feedback presence had larger effect
- Incorrect AI advice successfully shifted majority opinions toward false suggestions
- Self-reported trust weakly correlated with actual belief changes, especially when explanations were plausible

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The mere presence of AI feedback causes participants to change their opinions more than the quality of that feedback.
- Mechanism: Anchoring bias combined with AI authority effect. Participants start with their own belief rating, then encounter an authoritative-seeming AI response. The act of receiving any feedback creates a new anchor point, pulling their rating toward the AI's position regardless of explanation quality.
- Core assumption: Participants perceive the AI as having authority and their own initial ratings as flexible anchors rather than fixed beliefs.
- Evidence anchors:
  - [abstract] "over half of participants changed their opinions toward the AI's suggestions, regardless of explanation quality"
  - [section 4.1] "even feedback that is confined to just stating that 'the AI thinks that the statement is false/true' results in more than half of people moving their statement veracity assessment towards the AI suggestion"
  - [corpus] Weak - the corpus neighbors discuss AI advice-taking but don't specifically address the minimal feedback effect
- Break condition: If participants had stronger pre-existing beliefs or if the AI was explicitly framed as unreliable, the anchoring effect would diminish.

### Mechanism 2
- Claim: Plausible explanations are more persuasive than implausible ones, but the difference is smaller than the effect of simply receiving any feedback.
- Mechanism: Elaboration likelihood model - participants process information centrally when explanations are plausible (evaluating content), but use peripheral cues (AI authority) when explanations are absent or implausible. The content of plausible explanations provides additional persuasive weight beyond authority alone.
- Core assumption: Participants can distinguish between plausible and implausible explanations and engage in deeper processing for the former.
- Evidence anchors:
  - [abstract] "Plausible explanations were slightly more persuasive, but the mere presence of AI feedback had a larger effect"
  - [section 4.2] "More plausible explanations appear to be more effective on average, but the effect of the quality of the short explanation is much smaller than the effect of having any kind of AI advice presented to the user"
  - [corpus] Weak - corpus doesn't directly address explanation quality differences
- Break condition: If participants lacked domain knowledge to evaluate plausibility, or if time pressure prevented careful processing, the explanation quality effect would disappear.

### Mechanism 3
- Claim: Self-reported trust in AI weakly correlates with actual belief changes, and this correlation weakens further when explanations are plausible.
- Mechanism: Trust operates as a heuristic for evaluating AI advice, but when explanations are plausible, participants shift to content-based evaluation, decoupling trust from behavior. This creates a "blind faith" effect where distrusted systems with good explanations can still persuade.
- Core assumption: Participants have both trust-based and content-based evaluation strategies that operate differently depending on explanation quality.
- Evidence anchors:
  - [abstract] "Self-reported trust in the AI weakly correlated with actual belief changes, especially when explanations were plausible"
  - [section 4.6] "the self-reported Trust metric only weakly to moderately correlates with the reported opinion change. The association...becomes even less pronounced in the 'good explanation' condition"
  - [corpus] Weak - corpus doesn't address the relationship between trust and actual behavior change
- Break condition: If all explanations were implausible, trust would remain the primary evaluation heuristic and correlation would strengthen.

## Foundational Learning

- Concept: Anchoring bias in numerical judgment tasks
  - Why needed here: The study uses a -10 to 10 scale where participants provide initial ratings before and after AI feedback. Understanding how initial anchors influence subsequent judgments is critical to interpreting the results.
  - Quick check question: If someone rates a statement as 3 (somewhat true) and then sees AI feedback suggesting it's false, what psychological mechanism makes them more likely to adjust downward than if they had no initial rating?

- Concept: Elaboration likelihood model of persuasion
  - Why needed here: The study varies explanation quality to test whether participants engage in central (content-based) or peripheral (source-based) processing. This model explains why plausible explanations have an effect beyond mere feedback presence.
  - Quick check question: When would a participant be more likely to carefully evaluate the content of an AI explanation versus simply relying on the AI's perceived authority?

- Concept: Wisdom of the crowd phenomenon
  - Why needed here: The study tests whether majority opinion persists after AI intervention, which relates to understanding when collective judgments are robust versus when they can be swayed by authoritative sources.
  - Quick check question: Under what conditions would you expect AI advice to successfully shift majority opinion on a factual question?

## Architecture Onboarding

- Component map: Statement presentation interface -> AI feedback generator (3 conditions) -> Participant response collection -> Post-intervention survey
- Critical path: Participant sees statement → provides initial rating → receives AI feedback → provides final rating → completes demographic and attitude questions. The AI feedback generation is the most critical component as it determines experimental condition.
- Design tradeoffs: Using ChatGPT to generate plausible explanations introduces potential variability and requires careful validation, while manually crafting explanations would be more controlled but less scalable. The three-condition design balances experimental control with ecological validity.
- Failure signatures: If participants show no difference between conditions, the AI feedback may not be perceived as authoritative. If everyone changes opinion regardless of initial belief strength, the anchoring manipulation may be too weak. If trust correlates strongly with all opinion changes, the blind faith effect may not exist.
- First 3 experiments:
  1. Test AI feedback on a single statement with all three conditions to verify the basic effect exists before scaling to 10 statements
  2. Pilot test explanation quality ratings with independent raters to ensure plausible vs. implausible distinctions are clear
  3. Run a small preregistered replication with different health statements to test generalizability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the "blind faith" effect observed in this study persist when users are explicitly warned about potential AI inaccuracies before receiving feedback?
- Basis in paper: [explicit] The paper notes that self-reported trust weakly correlates with actual opinion change, and this correlation decreases further when explanations are more plausible, describing this as a "blind faith" effect.
- Why unresolved: The study did not include a condition where users were warned about potential AI inaccuracies beforehand, leaving it unclear if such warnings would alter the blind faith effect.
- What evidence would resolve it: An experimental condition where users receive a warning about potential AI inaccuracies before getting feedback, compared to control groups, would show if explicit warnings reduce the blind faith effect.

### Open Question 2
- Question: How do demographic variables interact with AI advice acceptance when controlling for domain expertise level?
- Basis in paper: [inferred] The paper found that demographic variables alone did not predict opinion change, but notes that the sample had limited representation of medical experts, suggesting potential interaction effects with expertise.
- Why unresolved: The study lacked sufficient expert participants to properly analyze interactions between demographics and expertise levels, and didn't control for expertise in demographic analyses.
- What evidence would resolve it: A larger, more diverse sample with adequate representation across expertise levels, analyzing demographic effects while controlling for expertise, would reveal if demographic factors only matter in combination with expertise.

### Open Question 3
- Question: What specific interface modifications could effectively reduce the authority effect of smaller/worse AI models without reducing their usability?
- Basis in paper: [explicit] The conclusion states that HCI/design interventions might be needed to mitigate the authority effect of smaller/worse models before deployment.
- Why unresolved: The study identified the problem but did not test specific interface modifications or design interventions to address it.
- What evidence would resolve it: Comparative experiments testing different interface designs (e.g., confidence indicators, source citations, tone modifications) against the standard interface would identify which modifications effectively reduce the authority effect while maintaining usability.

## Limitations
- The use of ChatGPT-generated explanations introduces potential variability that wasn't fully characterized
- Findings are constrained to health-related factual statements, limiting generalizability to other domains
- The study doesn't measure long-term belief persistence - only immediate post-feedback changes were tracked

## Confidence
- High Confidence: The finding that receiving any AI feedback (regardless of explanation quality) causes opinion changes is well-supported by the consistent ~50% shift rate across conditions.
- Medium Confidence: The "blind faith" effect where plausible explanations decouple trust from behavior change is compelling but relies on weaker correlations and a single study design.
- Low Confidence: The specific percentage differences between explanation conditions are difficult to generalize given the unknown exact content of the AI-generated explanations.

## Next Checks
1. Conduct a preregistered replication with transparent, human-crafted explanations to verify the blind faith effect and opinion change rates under controlled conditions.
2. Test belief persistence by following up with participants 1-2 weeks after initial exposure to measure whether AI-induced opinion changes endure or decay.
3. Expand to non-health domains (political claims, scientific misconceptions) to assess whether the anchoring and authority effects generalize beyond factual health statements.