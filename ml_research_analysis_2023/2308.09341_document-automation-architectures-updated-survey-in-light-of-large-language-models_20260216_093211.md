---
ver: rpa2
title: 'Document Automation Architectures: Updated Survey in Light of Large Language
  Models'
arxiv_id: '2308.09341'
source_url: https://arxiv.org/abs/2308.09341
tags:
- document
- language
- generation
- arxiv
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys document automation (DA) architectures and technologies,
  analyzing their implications and potential research opportunities in light of recent
  advances in large language models (LLMs). The survey identifies state-of-the-art
  DA architectures across various domains, including technical documentation, legal
  document assembly, quantitative analysis reports, clinical applications, and public
  administration.
---

# Document Automation Architectures: Updated Survey in Light of Large Language Models

## Quick Facts
- arXiv ID: 2308.09341
- Source URL: https://arxiv.org/abs/2308.09341
- Reference count: 40
- Key outcome: This paper surveys document automation (DA) architectures and technologies, analyzing their implications and potential research opportunities in light of recent advances in large language models (LLMs).

## Executive Summary
This survey examines document automation (DA) architectures across multiple domains, analyzing how recent advances in large language models (LLMs) are reshaping the field. The paper identifies state-of-the-art DA architectures in technical documentation, legal document assembly, quantitative analysis reports, clinical applications, and public administration. Key findings emphasize the growing importance of personalization, semantic interoperability, and the transformative impact of LLMs on DA systems. The survey proposes a reference architecture for DA and explores how LLMs can enhance document generation, analysis, and understanding.

## Method Summary
The survey employed a comprehensive literature review methodology, using Google Scholar to target peer-reviewed journals and conference proceedings, with additional analysis of arXiv preprints related to neural networks. The researchers reviewed nearly 500 papers, selecting approximately 100 with highest relevance based on their focus on document automation architectures and their implications with LLMs. The analysis focused on identifying state-of-the-art DA architectures, examining their applications across various domains, and evaluating the potential impact of LLMs on DA systems.

## Key Results
- LLMs are emerging as general-purpose data-to-text generators that can handle diverse input types without manual template engineering
- Ontologies and RDF-based knowledge graphs enable semantic interoperability across DA pipelines, allowing integration of heterogeneous data sources
- Jupyter notebooks provide practical DA architecture for quantitative and scientific reports by tightly coupling executable code, outputs, and narrative text

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs improve DA efficiency by serving as general-purpose data-to-text generators that can handle diverse input types without manual template engineering.
- **Mechanism**: Pre-trained LLMs, especially those fine-tuned with alignment techniques (e.g., InstructGPT), can map structured or semi-structured data to coherent text by conditioning on prompt instructions, reducing the need for hand-crafted rules or templates.
- **Core assumption**: LLM generalization and instruction-following capabilities are sufficiently reliable for domain-specific DA tasks.
- **Evidence anchors**:
  - [abstract] "Language models have already been used for text generation and document analysis. [Mager et al. 2021] fine-tuned GPT-2 to produce text from AMR..."
  - [section 2.7] "Better generation capabilities coupled with ease of use can make these LLMs as the de-facto method of generating text from data or prompts."
  - [corpus] No strong citations found for direct LLM-DA efficiency gains; paper cites only a few prior LLM text-generation studies.
- **Break condition**: If LLM outputs exhibit persistent hallucinations or fail to adhere to domain constraints, the claimed efficiency gain collapses.

### Mechanism 2
- **Claim**: Ontologies and RDF-based knowledge graphs enable semantic interoperability across DA pipelines, allowing data from heterogeneous sources to be integrated into a common schema.
- **Mechanism**: By defining domain semantics in OWL and mapping data to RDF triples, DA systems can automatically infer relationships and reuse knowledge across documents and departments.
- **Core assumption**: Semantic Web technologies are mature and widely adopted enough in enterprise settings to justify integration overhead.
- **Evidence anchors**:
  - [abstract] "Ontologies such as the Document Components Ontology (DoCO) can enable document automation via enterprise-wide document semantic interoperability."
  - [section 2.5] "Ontologies can also help integrate data models and information with document automation pipelines."
  - [corpus] No direct corpus citations supporting semantic interoperability claims; only mentions of related work.
- **Break condition**: If ontologies are not maintained or aligned with source data models, semantic interoperability breaks down.

### Mechanism 3
- **Claim**: Jupyter notebooks serve as a practical DA architecture for quantitative and scientific reports by tightly coupling executable code, outputs, and narrative text.
- **Mechanism**: Notebooks provide an integrated environment where analysis code, results, and explanatory text coexist, enabling reproducibility and streamlined report generation.
- **Core assumption**: Interactive notebook environments are the preferred workflow for domain experts generating quantitative reports.
- **Evidence anchors**:
  - [abstract] "Jupyter notebooks provide an interactive web application allowing users to create and share programmatic analysis in over 40 languages..."
  - [section 2.6] "Jupyter notebooks provide an interactive web application allowing users to create and share programmatic analysis..."
  - [corpus] No additional corpus citations; the claim is largely self-contained in the paper.
- **Break condition**: If the target audience does not use or support notebooks, or if reproducibility is not required, the architecture loses its advantage.

## Foundational Learning

- **Concept**: Document schema and ontology definitions
  - **Why needed here**: They are the backbone of any DA architecture, enabling structured data mapping and semantic reasoning.
  - **Quick check question**: Can you define the difference between a document schema and an ontology in one sentence?

- **Concept**: Template-based vs. LLM-driven text generation
  - **Why needed here**: Understanding both approaches helps assess when each is appropriate in a DA workflow.
  - **Quick check question**: What is one advantage and one limitation of template-based NLG compared to LLM-driven generation?

- **Concept**: Semantic Web standards (RDF, OWL, SPARQL)
  - **Why needed here**: These standards underpin knowledge graph integration and inference in DA systems.
  - **Quick check question**: Which W3C standard is used to query RDF data in a DA pipeline?

## Architecture Onboarding

- **Component map**:
  1. **Input layer**: Structured data sources (DBs, APIs, KGs), unstructured text, or user inputs.
  2. **Processing layer**: LLM, classical NLG engine, or template engine for text generation.
  3. **Semantic layer**: Ontology/RDF mapping and inference engine (if used).
  4. **Assembly layer**: Document fragment integration based on templates or LLM prompts.
  5. **Output layer**: Format conversion (PDF, HTML, DOCX) and rendering.

- **Critical path**: Input → Semantic mapping → Generation (LLM or template) → Assembly → Output.

- **Design tradeoffs**:
  - **LLM vs. template**: LLMs are flexible but opaque; templates are explicit but rigid.
  - **Semantic Web vs. direct mapping**: Ontologies enable inference but add complexity; direct mapping is simpler but less expressive.
  - **Notebook vs. web app**: Notebooks are great for reproducibility but less polished for end-user delivery.

- **Failure signatures**:
  - **Generation failures**: LLM hallucinations, template syntax errors, or data binding mismatches.
  - **Semantic failures**: Ontology mismatches, missing RDF triples, or inference errors.
  - **Integration failures**: API downtime, version conflicts in dependencies, or format incompatibility.

- **First 3 experiments**:
  1. **Template test**: Build a small XML-to-PDF DA pipeline using XSLT and validate output against a sample schema.
  2. **LLM test**: Use GPT-4 via API to generate a one-page legal summary from a structured JSON input; measure hallucination rate.
  3. **Ontology test**: Create a simple OWL ontology for a document type and verify RDF inference produces correct relationships.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms and techniques that can be employed to integrate LLMs with traditional document assembly methods to improve generative and compositional capabilities in DA systems?
- Basis in paper: [explicit] The paper suggests that combining language models with document assembly methods can enhance generative and compositional capabilities, but does not provide specific details on the mechanisms and techniques.
- Why unresolved: While the paper highlights the potential of integrating LLMs with document assembly methods, it does not delve into the specific techniques and mechanisms that can be employed to achieve this integration. Further research is needed to explore the practical implementation of such integration.
- What evidence would resolve it: A detailed study or implementation of specific techniques and mechanisms for integrating LLMs with document assembly methods, demonstrating their effectiveness in improving generative and compositional capabilities in DA systems.

### Open Question 2
- Question: How can DA technologies be designed to be more user-friendly and accessible to non-technical users, particularly in terms of code completion and ease of use?
- Basis in paper: [explicit] The paper mentions that DA technologies should be user-friendly and have code completion, but does not provide specific insights into how this can be achieved.
- Why unresolved: While the importance of user-friendly DA technologies is acknowledged, the paper does not provide specific guidance on how to design such technologies to be more accessible to non-technical users. Further research is needed to explore user interface design and interaction patterns that can make DA technologies more intuitive and user-friendly.
- What evidence would resolve it: User studies and evaluations of DA technologies that incorporate user-friendly design principles and code completion features, demonstrating their effectiveness in improving user experience and accessibility.

### Open Question 3
- Question: What are the potential challenges and limitations of using LLMs for document analysis and understanding, particularly in terms of accuracy, interpretability, and domain-specific knowledge?
- Basis in paper: [inferred] The paper discusses the potential of LLMs for document analysis and understanding, but does not explicitly address the challenges and limitations associated with their use.
- Why unresolved: While the paper highlights the potential of LLMs for document analysis and understanding, it does not delve into the potential challenges and limitations that may arise from their use. Further research is needed to explore the accuracy, interpretability, and domain-specific knowledge requirements of LLMs in the context of document analysis and understanding.
- What evidence would resolve it: Empirical studies comparing the performance of LLMs with traditional document analysis techniques, evaluating their accuracy, interpretability, and ability to handle domain-specific knowledge. Additionally, studies exploring the limitations and challenges of using LLMs for document analysis and understanding would provide valuable insights.

## Limitations
- The specific selection criteria used to choose the 100 most relevant papers from the initial pool of 250 papers are not detailed.
- The exact methodology for analyzing the implications of LLMs on DA systems is not fully described, leaving room for interpretation in the analysis process.
- The survey does not address potential conflicts between LLM flexibility and the need for strict compliance in regulated industries like legal and healthcare.

## Confidence
- **Low**: LLM-driven DA efficiency claims lack direct empirical validation within the surveyed literature
- **Low**: Semantic interoperability claims rely on theoretical potential rather than demonstrated enterprise adoption
- **Medium**: Jupyter notebook architecture has stronger grounding in existing workflows

## Next Checks
1. **LLM Hallucination Audit**: Test GPT-4 or similar models on domain-specific structured inputs (legal, medical) and measure hallucination rates against ground truth templates.
2. **Semantic Web Integration Pilot**: Implement a small-scale DA pipeline using RDF/OWL ontologies to map heterogeneous data sources and evaluate inference accuracy and maintenance overhead.
3. **Notebook-to-Production Feasibility Study**: Assess the transition path from Jupyter notebooks to production-ready DA systems in a regulated domain, documenting reproducibility and deployment challenges.