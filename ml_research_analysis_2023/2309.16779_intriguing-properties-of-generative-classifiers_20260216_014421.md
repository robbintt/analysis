---
ver: rpa2
title: Intriguing properties of generative classifiers
arxiv_id: '2309.16779'
source_url: https://arxiv.org/abs/2309.16779
tags:
- resnet-50
- bit-m
- generative
- vit-b
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether generative classifiers can match
  human object recognition abilities better than discriminative classifiers. The authors
  leverage recent advances in text-to-image generative models (Stable Diffusion, Imagen,
  Parti) to extract classification decisions and compare them against 52 discriminative
  models and human psychophysical data across 17 challenging out-of-distribution datasets.
---

# Intriguing properties of generative classifiers

## Quick Facts
- arXiv ID: 2309.16779
- Source URL: https://arxiv.org/abs/2309.16779
- Reference count: 40
- Primary result: Generative classifiers show near-human shape bias and OOD accuracy through diffusion-style training

## Executive Summary
This paper investigates whether generative classifiers can match human object recognition abilities better than discriminative classifiers. Using text-to-image generative models (Stable Diffusion, Imagen, Parti) as zero-shot classifiers, the authors demonstrate that these models exhibit near-human shape bias (99% for Imagen, 93% for Stable Diffusion, 92% for Parti vs. 96% average for humans) and achieve near human-level out-of-distribution accuracy despite not being trained for classification. The generative classifiers also show state-of-the-art alignment with human classification errors and demonstrate understanding of certain visual illusions, suggesting that generative pre-training may be a compelling alternative to discriminative training for robust visual recognition.

## Method Summary
The authors use text-to-image generative models as zero-shot classifiers by extracting classification decisions from image reconstructions. For each input image, they modify the label to a text prompt using the template "A bad photo of a <class>" and generate images with the text-to-image models. Classification decisions are obtained by computing L2 distance between the generated image and the input. The models are evaluated across 17 challenging out-of-distribution datasets from the model-vs-human toolbox, and results are compared against 52 discriminative baselines and human psychophysical data. Three key metrics are computed: shape bias on the cue-conflict dataset, OOD accuracy across all 17 datasets, and error consistency with human observers using Cohen's kappa.

## Key Results
- Generative classifiers exhibit near-human shape bias (99% for Imagen, 93% for Stable Diffusion, 92% for Parti vs. 96% average for humans)
- Achieved near human-level out-of-distribution accuracy despite being zero-shot models not trained for classification
- Imagen shows the most human-aligned error patterns to date
- Demonstrated understanding of certain visual illusions

## Why This Works (Mechanism)

### Mechanism 1
Diffusion-style training encourages models to focus on lower spatial frequencies, increasing shape bias. The noise injection during diffusion training gradually corrupts high-frequency texture information while preserving coarse shape structure, leading models to prioritize shape features during reconstruction. The weighting function wt := exp(-7t) in the diffusion loss gives higher weight to lower noise levels, making models more sensitive to low-frequency information.

### Mechanism 2
Generative classifiers achieve near-human OOD accuracy through zero-shot classification using conditional likelihoods. By maximizing p(x|y=k) across all classes using the conditional generative model, the classifier naturally captures the underlying data distribution without requiring labeled training data. The text-to-image models have learned rich representations of object appearance that transfer to classification.

### Mechanism 3
Generative classifiers show state-of-the-art error consistency with humans due to shared perceptual biases. Both generative models and humans prioritize shape over texture in ambiguous cases, leading to correlated errors on the same images. The shape bias observed in generative classifiers (99% for Imagen, 93% for Stable Diffusion) aligns with human perceptual tendencies.

## Foundational Learning

- **Diffusion probabilistic models and variational lower bounds**: Understanding how the diffusion models are used as classifiers requires knowledge of the underlying generative modeling framework. *Quick check*: What is the relationship between the forward and reverse Markov chains in diffusion models?
- **Out-of-distribution generalization and robustness**: The paper's main claims about OOD accuracy require understanding how models perform on data that differs from training distribution. *Quick check*: How does shape bias relate to robustness on out-of-distribution datasets?
- **Error consistency and Cohen's kappa**: The paper uses error consistency as a key metric to compare model behavior with human perception. *Quick check*: What does a positive error consistency value indicate about the relationship between two decision makers?

## Architecture Onboarding

- **Component map**: Text encoder → Image generator → L2 distance calculator → Classification decision
- **Critical path**: Prompt encoding → Image reconstruction → Similarity measurement → Class selection
- **Design tradeoffs**: Zero-shot generative classifiers trade computational efficiency for robustness and human-like behavior
- **Failure signatures**: Poor performance on rotated images, high-pass filtered images, and ambiguous visual illusions
- **First 3 experiments**:
  1. Test shape bias on the cue-conflict dataset to verify the human-like shape preference
  2. Evaluate OOD accuracy across the 17 challenging datasets to measure robustness
  3. Measure error consistency with human observers to assess alignment with human perception

## Open Questions the Paper Calls Out

### Open Question 1
Does the increased shape bias in generative classifiers directly result from diffusion-style training or from other factors like generative training, dataset quality, or language model integration? The authors observe correlation but cannot definitively prove causation. Controlled experiments isolating individual factors would help establish causation.

### Open Question 2
What specific aspects of diffusion training cause generative classifiers to focus more on low spatial frequencies and shapes rather than high-frequency textures? The authors hypothesize that denoising during diffusion training biases models toward capturing low-frequency information, but don't identify the exact mechanism. Detailed analysis of feature activations could identify the causal mechanism.

### Open Question 3
Can the shape bias of discriminative models be improved to match generative classifiers by incorporating diffusion-style noise augmentation alone? While adding diffusion-style noise increased a ResNet-50's shape bias from 21% to 78%, it's unclear if this approach alone can achieve the near-human shape bias observed in generative classifiers without other modifications. Systematic experiments varying diffusion noise augmentation parameters could determine if diffusion noise alone can achieve human-level shape bias.

## Limitations

- The study relies on only three generative models (Stable Diffusion, Imagen, Parti) to make broad claims about generative classifiers versus discriminative approaches
- Small sample size makes it difficult to determine whether observed properties are inherent to the generative paradigm or specific to these particular architectures
- Limited investigation into the exact mechanism linking diffusion training to increased shape bias

## Confidence

- **High confidence**: The finding that generative classifiers show near-human shape bias is well-supported by the data across multiple datasets and models
- **Medium confidence**: The claim about state-of-the-art error consistency with humans, while supported by Cohen's kappa calculations, could be influenced by the specific datasets chosen
- **Medium confidence**: The assertion that diffusion-style training causes increased shape bias through emphasis on lower spatial frequencies is plausible but not definitively proven

## Next Checks

1. **Architecture generalization test**: Evaluate additional generative models (including those trained with different noise schedules or without diffusion training) to determine whether shape bias is a universal property of generative classifiers or specific to the tested architectures.

2. **Ablation on spatial frequency weighting**: Systematically vary the weighting function wt in the diffusion variational lower bound to test whether the observed shape bias directly results from the emphasis on lower spatial frequencies, or if other factors contribute.

3. **Cross-dataset consistency verification**: Replicate the error consistency analysis across a more diverse set of datasets, particularly including datasets that were not used in the original model-vs-human toolbox, to ensure the observed human-alignment is not dataset-specific.