---
ver: rpa2
title: 'Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?'
arxiv_id: '2309.06891'
source_url: https://arxiv.org/abs/2309.06891
tags:
- pooling
- attention
- simpool
- vision
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SimPool, a simple attention-based pooling
  mechanism for vision transformers and convolutional networks. SimPool replaces the
  default CLS token-based pooling in transformers and global average pooling in CNNs
  with a single-step attention mechanism that yields high-quality attention maps delineating
  object boundaries.
---

# Keep It SimPool: Who Said Supervised Transformers Suffer from Attention Deficit?

## Quick Facts
- arXiv ID: 2309.06891
- Source URL: https://arxiv.org/abs/2309.06891
- Reference count: 40
- Primary result: Improves classification accuracy by 0.6-1.6% on ImageNet-1k under supervised pre-training and 2.0-4.0% under self-supervised pre-training

## Executive Summary
SimPool introduces a simple attention-based pooling mechanism that replaces default CLS token-based pooling in transformers and global average pooling in CNNs. The method uses a single-step attention mechanism that produces high-quality attention maps delineating object boundaries without requiring architectural modifications or explicit losses. SimPool achieves significant improvements in classification accuracy, object localization, and object discovery across both supervised and self-supervised settings.

## Method Summary
SimPool is an attention-based pooling mechanism that replaces standard CLS token pooling in transformers and global average pooling in CNNs. It initializes with global average pooling to create a query vector, maps this to attention keys through linear layers, and computes attention weights via dot-product similarity. The attention-weighted feature map is then pooled using a generalized mean function fα, producing both a pooled representation and interpretable attention maps. The method requires minimal architectural changes - just adding query and key weight matrices - and maintains the original model structure.

## Key Results
- Improves ImageNet-1k classification accuracy by 0.6-1.6% under supervised pre-training
- Improves ImageNet-1k classification accuracy by 2.0-4.0% under self-supervised pre-training
- Significantly improves object localization (up to 14% MaxBoxAccV2) and object discovery (up to 25.2% CorLoc) without fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
SimPool replaces default CLS-based pooling with a single-step attention mechanism that produces high-quality attention maps. The mechanism computes an initial representation via global average pooling, maps this to a query vector, and uses dot-product similarity with keys derived from the feature tensor. The resulting attention map weights the feature tensor via generalized mean pooling, producing both a pooled vector and interpretable attention maps. This works because a single-step attention process can effectively focus on object regions when initialized with global context.

### Mechanism 2
SimPool's attention maps achieve quality comparable to or better than self-supervised methods under supervised training. By initializing with GAP and using learnable query/key mappings, the attention mechanism learns to focus on object regions during supervised training. This overcomes the typical issue of low-quality attention maps in supervised transformers by providing a better initialization point and more effective attention computation.

### Mechanism 3
SimPool improves classification accuracy by 0.6-1.6% supervised and 2.0-4.0% self-supervised. The improved attention maps correlate with better feature representations for classification, as the attention mechanism learns to capture object-relevant features more effectively. This works across both supervised and self-supervised settings because the attention-based pooling provides a more discriminative global representation.

## Foundational Learning

- **Global Average Pooling (GAP)**: Used to initialize the pooling process with a baseline global representation. Quick check: What is the difference between GAP and max pooling in terms of the information they preserve?
- **Attention Mechanisms**: Core to SimPool's pooling computation using dot-product similarity and softmax normalization. Quick check: How does scaled softmax normalization in attention differ from simple softmax?
- **Generalized Mean Pooling (fα)**: Used as the pooling function allowing interpolation between average and max pooling. Quick check: What values of α correspond to average pooling, max pooling, and geometric mean pooling?

## Architecture Onboarding

- **Component map**: Input features → GAP → Query/Key mapping → Attention computation → Value transformation → Generalized mean pooling → Output pooled vector
- **Critical path**: Features flow through GAP initialization, linear query/key mappings, dot-product attention computation with softmax, value centering/normalization, and finally generalized mean pooling
- **Design tradeoffs**: Trades small number of additional parameters (WQ, WK) for improved attention quality and classification accuracy. The α parameter in fα allows control over pooling behavior.
- **Failure signatures**: Poor attention maps (focusing on background or missing objects) indicate issues with query/key mappings or initialization. Low classification accuracy despite good attention maps suggests attention patterns don't align with class-discriminative features.
- **First 3 experiments**:
  1. Implement SimPool on ResNet-18 on ImageNet-20% and compare accuracy and attention map quality to GAP baseline
  2. Implement SimPool on ViT-S on ImageNet-20% and compare accuracy and attention map quality to CLS token baseline
  3. Perform ablation study varying α parameter in fα to find optimal value for both CNNs and transformers

## Open Questions the Paper Calls Out
The paper identifies three main open questions: (1) Why standard CLS-based attention fails under supervision, (2) How SimPool's attention quality compares to other attention-based pooling methods under supervision, and (3) What is the optimal number of iterations for attention-based pooling methods.

## Limitations
- Focus primarily on ImageNet-1k validation with limited testing on other datasets
- Limited ablation studies on hyperparameter sensitivity, particularly the α value in fα pooling
- Lacks comparison against recent attention-based pooling methods developed after primary experiments

## Confidence
- **High confidence**: Core mechanism of SimPool as attention-based pooling replacement is well-supported with clear experimental validation
- **Medium confidence**: Claims about being "first" to achieve self-supervised-level attention quality in supervised settings require careful literature verification
- **Medium confidence**: Exact hyperparameter settings (α value, γ normalization) have some uncertainty in reproduction

## Next Checks
1. Implement SimPool on both CNN and transformer architectures with systematic ablation studies varying α in fα pooling to identify optimal settings
2. Compare SimPool's attention maps quantitatively against established self-supervised methods using standard localization metrics across multiple datasets
3. Test SimPool's robustness to dataset size by training on subsets of ImageNet (10%, 20%, 50%) to evaluate scaling behavior