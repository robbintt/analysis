---
ver: rpa2
title: 'SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive
  Tasks'
arxiv_id: '2305.17390'
source_url: https://arxiv.org/abs/2305.17390
tags:
- action
- tasks
- swift
- task
- sage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces SwiftSage, a novel agent framework inspired
  by dual-process theory to address complex interactive reasoning tasks in the ScienceWorld
  benchmark. SwiftSage integrates fast and slow thinking through two modules: a behavior
  cloning-based Swift module for quick, intuitive decision-making, and a large language
  model (LLM)-based Sage module for deliberate, analytical planning.'
---

# SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks

## Quick Facts
- arXiv ID: 2305.17390
- Source URL: https://arxiv.org/abs/2305.17390
- Authors: 
- Reference count: 40
- Primary result: SwiftSage achieves 84.7 average score on ScienceWorld tasks, outperforming existing methods like SayCan (33.8), ReAct (36.4), and Reflexion (45.3)

## Executive Summary
SwiftSage is a novel generative agent framework that addresses complex interactive reasoning tasks by integrating dual-process theory into AI systems. The framework combines a fast, intuitive Swift module based on behavior cloning with a slow, analytical Sage module powered by large language models (LLMs). By strategically activating the Sage module only when needed—such as when encountering exceptions or requiring complex reasoning—SwiftSage achieves significantly better performance while being more cost-effective than existing methods. Experiments on 30 ScienceWorld tasks demonstrate substantial improvements in both task completion and efficiency.

## Method Summary
SwiftSage implements a dual-process architecture where a small T5-large model handles routine actions quickly (Swift module) while GPT-4 handles complex planning and exception handling when needed (Sage module). The system uses a heuristic controller to switch between modules based on reward patterns and exception detection. Unlike prior methods that generate one action at a time, SwiftSage plans multiple actions in advance using an action buffer mechanism. The framework also addresses data imbalance issues through balanced imitation learning with multi-hop context windows. This approach combines the efficiency of behavior cloning with the reasoning capabilities of LLMs in a cost-effective manner.

## Key Results
- SwiftSage achieves 84.7 average score on ScienceWorld tasks, significantly outperforming baselines (SayCan: 33.8, ReAct: 36.4, Reflexion: 45.3)
- The framework is more cost-effective, using 757.07 LLM tokens per action versus 1,855.84-2,983.46 for baselines
- SwiftSage demonstrates superior efficiency in task completion while maintaining high performance across diverse complex interactive tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SwiftSage achieves better performance by combining fast intuitive reasoning (Swift module) with slow analytical reasoning (Sage module) only when needed.
- Mechanism: The system uses a small T5-large model for quick, routine actions and switches to GPT-4 for complex reasoning when exceptions occur or when the action history shows zero reward for five consecutive steps.
- Core assumption: Fast thinking can handle most routine actions efficiently, while slow thinking is reserved for complex scenarios requiring deeper reasoning.
- Evidence anchors:
  - [abstract]: "SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance."
  - [section 3.4]: "We establish a heuristic algorithm to control the activation and deactivation of the two modules."
  - [corpus]: The corpus shows multiple recent works (FARE, DUMA, HDFlow) that also use fast-slow thinking approaches, validating the dual-process theory application in AI systems.
- Break condition: The mechanism breaks if the heuristic for switching between modules is too aggressive or too conservative, leading to either unnecessary LLM calls or failure to handle exceptions properly.

### Mechanism 2
- Claim: The action buffer mechanism improves efficiency by generating multiple actions at once rather than one at a time.
- Mechanism: Instead of generating a single action per time step, the Sage module plans subgoals and converts them into a sequence of actions stored in an action buffer, which are then executed sequentially.
- Core assumption: Planning multiple actions at once is more efficient than planning each action individually, especially for long-horizon tasks.
- Evidence anchors:
  - [section 3.3]: "Unlike prior methods, where LLMs only generate the next immediate action, our procedures engage in longer-term action planning."
  - [section 4.3]: "Despite SAGE invoking LLMs APIs twice for inference, its overall cost remains lower, as the result is a sequence of actions typically containing about 5 actions."
  - [corpus]: The corpus doesn't provide direct evidence for action buffer efficiency, but related works like HDFlow mention dynamic workflows which could include similar batching approaches.
- Break condition: The mechanism breaks if the action buffer becomes too large or if actions in the buffer become invalid due to environmental changes between planning and execution.

### Mechanism 3
- Claim: The balanced imitation learning approach prevents data imbalance issues that plague traditional behavior cloning methods.
- Mechanism: The system down-samples specific task types and actions to create a more balanced training dataset, and uses a sliding window of 10 recent actions as context rather than just one-hop history.
- Core assumption: A balanced dataset with longer action history context leads to better generalization than imbalanced datasets with limited context.
- Evidence anchors:
  - [section 3.2]: "To avoid bias caused by data imbalance for seq2seq learning, we down-sampled specific types of tasks and actions to achieve a more balanced final dataset for training."
  - [section 3.2]: "We expand the conventional one-hop BC to multi-hop by incorporating a sliding window of observations and rewards for the K = 10 recent actions."
  - [corpus]: No direct corpus evidence, but this aligns with general machine learning best practices for handling class imbalance.
- Break condition: The mechanism breaks if the down-sampling removes too much data, leading to underfitting, or if the sliding window is too short to capture necessary context.

## Foundational Learning

- Concept: Dual-process theory of cognition (System 1 and System 2 thinking)
  - Why needed here: The framework is explicitly inspired by this psychological theory, using it as the conceptual foundation for separating fast intuitive thinking from slow analytical thinking.
  - Quick check question: What are the key characteristics that distinguish System 1 thinking from System 2 thinking in human cognition?

- Concept: Behavior cloning and sequence-to-sequence learning
  - Why needed here: The Swift module is based on behavior cloning using seq2seq learning to imitate oracle agents' trajectories.
  - Quick check question: How does behavior cloning differ from reinforcement learning in terms of training data requirements and exploration?

- Concept: Prompt engineering and in-context learning with LLMs
  - Why needed here: The Sage module relies on carefully crafted prompts to elicit planning and reasoning capabilities from LLMs like GPT-4.
  - Quick check question: What are the key components of effective prompt engineering for task decomposition and exception handling?

## Architecture Onboarding

- Component map: Task Description -> Swift Module (T5-large) or Sage Module (GPT-4) -> Action Execution -> Environment -> Observation -> Reward -> Heuristic Controller
- Critical path:
  1. Receive task description and initial state
  2. Swift module generates first action
  3. Check for activation conditions (zero reward, invalid action, critical decision, exception)
  4. If activated, Sage module plans subgoals and generates action buffer
  5. Execute actions from buffer sequentially
  6. Revert to Swift module when buffer is empty
  7. Repeat until task completion

- Design tradeoffs:
  - Small LM vs. large LM: Swift uses T5-large (770M) instead of larger models for efficiency, accepting slightly lower accuracy for faster inference
  - Single LLM call vs. iterative calls: Sage uses single comprehensive prompts rather than multiple back-and-forth interactions to reduce cost
  - Balanced vs. imbalanced training data: Down-sampling to prevent bias but potentially losing some task-specific patterns

- Failure signatures:
  - Swift module repeatedly generates invalid actions or gets stuck in loops
  - Sage module generates action sequences that become invalid due to environmental changes
  - Heuristic controller fails to activate Sage when needed or activates it unnecessarily
  - Action buffer execution fails midway through due to unforeseen exceptions

- First 3 experiments:
  1. Test Swift module in isolation on simple tasks to establish baseline performance and identify failure modes
  2. Test Sage module in isolation with fixed action buffer size on complex tasks to measure planning quality
  3. Test integrated system with heuristic controller disabled (always use Sage) to measure performance impact of the switching mechanism

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- The framework's performance heavily depends on the heuristic controller's effectiveness, with limited analysis of parameter sensitivity or failure conditions
- The action buffer mechanism assumes environmental stability between planning and execution, which may not hold in dynamic environments
- While balanced training addresses data imbalance, it may come at the cost of task-specific performance on certain problem types

## Confidence
- **High confidence**: The dual-process architecture design and its theoretical foundation in dual-process theory
- **Medium confidence**: The reported performance improvements over baseline methods (the relative gains are robust, but absolute performance numbers may depend on specific ScienceWorld implementation details)
- **Medium confidence**: The cost-effectiveness claims (token savings are directly measurable, but the trade-offs between cost and performance are not fully explored)

## Next Checks
1. **Stress test the heuristic controller** by systematically varying the zero-reward threshold and exception detection sensitivity to identify failure patterns and optimal parameters for different task types.

2. **Test action buffer robustness** by introducing environmental changes between planning and execution phases to measure how often planned sequences become invalid and quantify the recovery mechanisms' effectiveness.

3. **Evaluate the balanced training approach** by training Swift module variants with different levels of data balancing (from fully balanced to original imbalanced data) to determine the optimal balance between generalization and task-specific performance.