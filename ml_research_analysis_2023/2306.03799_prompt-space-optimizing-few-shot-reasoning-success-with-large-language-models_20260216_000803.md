---
ver: rpa2
title: Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models
arxiv_id: '2306.03799'
source_url: https://arxiv.org/abs/2306.03799
tags:
- step
- prompt
- reasoning
- coin
- think
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Prompt Space is a new approach for improving few-shot reasoning
  in large language models by automatically constructing demonstrations from a learned
  prompt space. It uses text embeddings and matrix decomposition to find basis questions
  that span the prompt space, then combines these with test questions to generate
  reasoning chains.
---

# Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models

## Quick Facts
- arXiv ID: 2306.03799
- Source URL: https://arxiv.org/abs/2306.03799
- Reference count: 40
- Key outcome: Prompt Space achieves up to 2.3% average improvement over state-of-the-art few-shot methods on ten reasoning benchmarks without manual prompt design

## Executive Summary
Prompt Space introduces an automated approach for few-shot reasoning in large language models by constructing demonstrations from a learned prompt space. The method uses text embeddings and matrix decomposition to identify basis questions that span the prompt space, then combines these with test questions to generate reasoning chains. It significantly outperforms existing prompt paradigms including Few-shot, Manual-CoT, Zero-shot, Zero-shot-CoT, and Auto-CoT on arithmetic, commonsense, and symbolic reasoning tasks. The approach eliminates the need for manual prompt engineering while achieving consistent improvements across diverse reasoning domains.

## Method Summary
Prompt Space automatically constructs demonstrations for few-shot reasoning by embedding questions into a high-dimensional space, applying SVD to find principal components, and selecting questions closest to these basis vectors. The method uses these basis questions combined with test questions through Zero-shot-CoT to generate reasoning chains. The approach operates by: embedding all questions using models like MiniLM-L6-v2, applying matrix decomposition to find k basis vectors that span the prompt space, selecting questions closest to these basis vectors as demonstrations, and constructing prompts that pair basis questions with test questions to guide LLM reasoning.

## Key Results
- Achieves 2.3% average improvement over Few-shot-CoT baseline across ten reasoning benchmarks
- Outperforms all comparison methods including Manual-CoT, Zero-shot, and Auto-CoT
- Shows consistent gains across arithmetic reasoning (GSM8K, MultiArith), commonsense reasoning (CommonsenseQA), and symbolic reasoning tasks
- Particularly effective on datasets requiring complex reasoning chains and multiple inference steps

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prompt Space finds a lower-dimensional representation of all possible prompts using basis vectors from matrix decomposition
- **Mechanism:** Questions are embedded into high-dimensional space, SVD finds principal components, and questions closest to these basis vectors form demonstrations
- **Core assumption:** Reasoning tasks lie in a low-dimensional manifold that can be approximated by k principal components
- **Evidence anchors:** Abstract mentions "basis vectors by matrix decomposition" and section 2 describes SVD calculation with Q = UΛV^T
- **Break condition:** Tasks requiring high-dimensional representations that cannot be well-approximated by k principal components

### Mechanism 2
- **Claim:** Basis questions serve as effective demonstrations because they are maximally representative of the entire prompt space
- **Mechanism:** Questions closest to basis vectors ensure demonstrations cover full diversity rather than clustering around similar types
- **Core assumption:** Questions closest to principal components are more representative than randomly sampled questions
- **Evidence anchors:** Abstract states basis questions "construct a space that can represent all questions" and section 2 describes final prompt exemplar construction
- **Break condition:** Basis vector selection picks mathematically central but semantically irrelevant questions

### Mechanism 3
- **Claim:** Combining basis questions with test questions through Zero-shot-CoT generates effective reasoning chains
- **Mechanism:** Demonstrations pair basis questions with Zero-shot-CoT reasoning chains, providing diverse reasoning patterns alongside test questions
- **Core assumption:** LLMs can generalize from multiple diverse reasoning examples to solve novel problems
- **Evidence anchors:** Abstract mentions "automatically generate reasoning demonstrations" and section 3 discusses significant improvements
- **Break condition:** Generated reasoning chains are too diverse or inconsistent, causing confusion rather than helping

## Foundational Learning

- **Concept:** Singular Value Decomposition (SVD) and Principal Component Analysis (PCA)
  - **Why needed here:** These techniques find basis vectors that span the prompt space and reduce dimensionality from n to k
  - **Quick check question:** What mathematical property ensures that the k principal components found by SVD are the best k-dimensional approximation of the original matrix?

- **Concept:** Text embedding models and similarity metrics
  - **Why needed here:** Questions are encoded into vector representations, and cosine similarity finds questions closest to basis vectors
  - **Quick check question:** Why is cosine similarity preferred over Euclidean distance when comparing question embeddings?

- **Concept:** Chain-of-thought prompting and in-context learning
  - **Why needed here:** Selected basis questions are combined with test questions using CoT to generate reasoning demonstrations
  - **Quick check question:** How does providing multiple diverse reasoning examples in-context differ from providing similar examples?

## Architecture Onboarding

- **Component map:** Text embedding model (MiniLM-L6-v2, T5, or E5) -> Matrix decomposition module (SVD) -> Basis question selection module (similarity calculation) -> Prompt construction module (demonstration formatting) -> LLM interface (ChatGPT API) -> Evaluation module (accuracy calculation)

- **Critical path:** 1. Embed all questions → 2. Compute SVD → 3. Extract k principal components → 4. Find closest questions → 5. Construct demonstrations → 6. Generate answers

- **Design tradeoffs:** Embedding model choice tradeoffs between quality and computational cost; number of basis questions (k) balances representation power and demonstration length; matrix decomposition method choices (SVD vs alternatives); similarity metric selection (cosine vs other distances)

- **Failure signatures:** Performance plateaus or degrades as k increases beyond optimal value; similar questions selected despite diversity requirement; generated reasoning chains are illogical or irrelevant; high variance in performance across different embedding models

- **First 3 experiments:** 1) Test Prompt Space on simple arithmetic dataset with varying k values to find optimal basis question count; 2) Compare performance using different embedding models (MiniLM vs T5 base vs E5 small) on same dataset; 3) Evaluate Prompt-Space-CoT vs Prompt-Space-CoT-Zero on GSM8K to quantify impact of "Let's think step by step" prompt

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal number of basis questions for each reasoning task, and how can we automatically determine this number?
- **Basis in paper:** Explicit discussion of impact of basis question number on reasoning tasks, finding different tasks require different numbers for optimal performance
- **Why unresolved:** Paper mentions challenge in automatically determining optimal number, used greedy decoding with constant demonstrations except specific cases
- **What evidence would resolve it:** Systematic experiments varying basis question numbers for each task, plus algorithm that automatically selects optimal number based on task characteristics

### Open Question 2
- **Question:** How does performance vary with different embedding models, and what is the impact of embedding size on effectiveness?
- **Basis in paper:** Explicit investigation of embedding model effects, finding increasing embedding size doesn't necessarily improve performance and 768 is optimal for T5 and E5 models
- **Why unresolved:** While exploring embedding impact, lacks comprehensive analysis of different architectures and pre-training objectives, relationship between embedding size and effectiveness not fully understood
- **What evidence would resolve it:** Thorough comparison across various embedding models with different architectures and pre-training objectives, plus systematic experiments varying embedding size

### Open Question 3
- **Question:** How can Prompt Space be extended to handle more complex reasoning tasks or harder arithmetic reasoning problems?
- **Basis in paper:** Inferred from mention that Prompt Space has difficulty determining optimal basis questions for complex questions or hard arithmetic tasks
- **Why unresolved:** Paper doesn't provide detailed analysis of limitations in handling complex reasoning tasks
- **What evidence would resolve it:** Experiments testing Prompt Space on wider range of complex reasoning tasks and hard arithmetic problems, plus analysis of failure cases and identification of patterns

## Limitations
- Performance depends significantly on text embedding quality, with different models showing varying effectiveness
- Method lacks systematic optimization for the number of basis questions (k parameter)
- Claims of eliminating manual prompt design require further validation across diverse reasoning tasks
- Improvements over state-of-the-art are statistically significant but modest in absolute terms

## Confidence
- **High confidence:** Core mathematical approach using SVD for basis vector discovery is well-established and correctly implemented
- **Medium confidence:** Empirical results showing improvements over baselines are reproducible given provided methodology
- **Low confidence:** Claim that approach eliminates need for manual prompt design requires further validation across diverse reasoning tasks

## Next Checks
1. **Ablation study on embedding models:** Systematically compare Prompt Space performance across wider range of embedding models and dimensions to quantify impact of embedding quality on final results
2. **Sensitivity analysis for k parameter:** Conduct grid search over k values for each dataset to identify optimal basis question counts and understand method's sensitivity to this hyperparameter
3. **Cross-dataset generalization test:** Evaluate whether basis questions selected from one reasoning domain transfer effectively to related domains to test universality of learned prompt space