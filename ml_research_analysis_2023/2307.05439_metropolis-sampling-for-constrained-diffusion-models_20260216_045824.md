---
ver: rpa2
title: Metropolis Sampling for Constrained Diffusion Models
arxiv_id: '2307.05439'
source_url: https://arxiv.org/abs/2307.05439
tags:
- have
- diffusion
- page
- reflected
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method for training diffusion models
  on constrained manifolds using Metropolis sampling, addressing limitations of existing
  approaches based on logarithmic barrier functions and reflected Brownian motion.
  The proposed method simplifies implementation and improves computational efficiency
  by rejecting samples outside the constrained set, rather than requiring complex
  geometric operations like reflection or projection.
---

# Metropolis Sampling for Constrained Diffusion Models

## Quick Facts
- arXiv ID: 2307.05439
- Source URL: https://arxiv.org/abs/2307.05439
- Reference count: 40
- This paper introduces a new method for training diffusion models on constrained manifolds using Metropolis sampling, addressing limitations of existing approaches based on logarithmic barrier functions and reflected Brownian motion.

## Executive Summary
This paper proposes a novel approach for training diffusion models on constrained manifolds using Metropolis sampling. The method addresses limitations of existing approaches based on logarithmic barrier functions and reflected Brownian motion by rejecting samples outside the constrained set rather than requiring complex geometric operations like reflection or projection. The authors provide theoretical analysis showing that their Metropolis sampling process corresponds to a valid discretization of the reflected Brownian motion, and demonstrate improved performance and faster training on synthetic and real-world tasks including robotic arm modeling and protein design.

## Method Summary
The paper introduces Metropolis sampling for constrained diffusion models, where samples that fall outside the constraint manifold are rejected and the sampling process repeats. This approach avoids the computational complexity of reflection or projection operations required by existing methods. The score network is trained using standard diffusion model losses without modification, and the Metropolis rejection only affects the sampling step. The method is theoretically justified by showing that the Metropolised discretization converges to the reflected SDE as step size approaches zero.

## Key Results
- Metropolis sampling provides a lightweight alternative to reflection and projection-based methods for constrained diffusion models
- Theoretical analysis proves convergence to reflected Brownian motion as step size approaches zero
- Empirical results show improved performance and faster training on synthetic and real-world tasks
- The approach extends to non-convex constraints, enabling applications in geospatial modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Metropolis rejection step ensures samples stay within the constrained set without expensive geometric operations.
- Mechanism: When a proposed move lands outside the manifold, it is rejected and the sample remains at its current position. This is equivalent to conditioning the noise on staying within the manifold, avoiding reflection or projection calculations.
- Core assumption: The binary indicator function for set membership is cheap to evaluate and reliable.
- Evidence anchors:
  - [abstract]: "The Metropolised process' chief advantage is that it is lightweight: the only additional requirement over those outlined in De Bortoli et al. (2022) that is needed to implement a constrained diffusion model is an efficient binary function that indicates whether any given point is within the constrained set."
  - [section 3.1]: "The sampler we propose is similar to a classical Euler-Maruyama discretisation of the Brownian motion, except that, whenever a step would carry the Brownian motion outside of the constrained region, we reject it."
- Break condition: If the indicator function is slow or unreliable, or if the constraint boundary is extremely thin compared to the step size, the rejection rate may become too high, slowing training.

### Mechanism 2
- Claim: The Metropolis process converges to the reflected Brownian motion as the step size goes to zero.
- Mechanism: Theorem 2 proves that the Metropolised discretization weakly converges to the reflected SDE, meaning the rejection sampler asymptotically reproduces the correct invariant measure with reflection at the boundary.
- Core assumption: The manifold is bounded, smooth, and the boundary is sufficiently regular for the invariance principle to apply.
- Evidence anchors:
  - [section 3.2]: "Our core theoretical contribution is to show that this new discretisation converges to the reflected SDE by using the invariance principle for SDEs with boundary (Stroock and Varadhan, 1971)."
  - [section D.8]: Formal proof outline showing equivalence between Metropolis and rejection approximations.
- Break condition: If the manifold has corners, cusps, or fractal-like boundaries, the regularity assumptions fail and convergence may not hold.

### Mechanism 3
- Claim: The score network can be trained using standard diffusion model losses without modification for the Metropolis step.
- Mechanism: The forward noising process and the score matching loss are defined independently of the sampling step. The Metropolis rejection only affects the reverse sampling, not the training objective.
- Core assumption: The forward process reaches the correct invariant distribution and the score approximation is smooth enough for gradient-based training.
- Evidence anchors:
  - [section 2]: "The time-reversal formula(3) remains the same, replacing the Euclidean gradient with its Riemannian equivalent. The ism loss can still be computed in that setting."
  - [section 5]: Describes identical training setup to unconstrained diffusion models with only the sampler changed.
- Break condition: If the score network is too rigid or the constraints are too complex, the model may fail to learn the correct score, leading to poor samples even with Metropolis rejection.

## Foundational Learning

- Concept: Stochastic Differential Equations (SDEs) and their discretizations.
  - Why needed here: The paper builds diffusion models on SDEs; understanding Euler-Maruyama, reflected Brownian motion, and their convergence is key to following the theory.
  - Quick check question: What is the difference between an unconstrained Euler-Maruyama step and a reflected Brownian motion step?

- Concept: Riemannian geometry and constrained manifolds.
  - Why needed here: The work operates on manifolds defined by inequality constraints; knowing manifolds, tangent spaces, exponential maps, and boundaries is essential.
  - Quick check question: How is a constrained manifold defined in the paper, and what role does the boundary play?

- Concept: Score matching and denoising diffusion training.
  - Why needed here: The score network is trained via denoising score matching; understanding the loss and how it relates to the time-reversed SDE is critical.
  - Quick check question: What is the relationship between the score network and the drift term in the reverse-time SDE?

## Architecture Onboarding

- Component map:
  - Data → Score Network (MLP with sine activations) → Score → Metropolis Sampler → Constrained Samples
  - Training loop: Noise data → Compute loss (ism or dsm) → Update score network
  - Sampling loop: Start from prior → Apply Metropolis rejection → Repeat N steps → Generate sample

- Critical path:
  - Forward pass: Data → Score Network → Score output
  - Backward pass: Loss computation → Gradient → Parameter update
  - Sampling: Initialize → Metropolis step → Repeat → Output

- Design tradeoffs:
  - Step size γ vs. rejection rate: Smaller steps reduce rejections but increase computation.
  - Architecture depth vs. training speed: Deeper MLPs may fit better but train slower.
  - Distance rescaling vs. boundary adherence: Helps with smoothness but may distort the learned distribution near boundaries.

- Failure signatures:
  - High rejection rate → Slow training, poor coverage of constrained space.
  - Poor sample diversity → Score network not learning correct gradients.
  - Numerical instability near boundaries → Step size too large or constraints too sharp.

- First 3 experiments:
  1. Train on synthetic 2D bimodal distribution in hypercube; compare Metropolis vs. reflected sampling visually and via MMD.
  2. Scale to 10D hypercube; measure training time and sample quality to assess scalability.
  3. Apply to robotics dataset; visualize joint configurations and manipulability ellipsoids to check physical plausibility.

## Open Questions the Paper Calls Out
- Question: Does the Metropolis sampling approach scale effectively to extremely high-dimensional spaces (e.g., >1000 dimensions) with complex non-convex constraints?
  - Basis in paper: [inferred] The paper demonstrates scalability on synthetic distributions up to 10 dimensions and real-world tasks like protein modeling with moderate dimensions. However, it does not explicitly test extremely high-dimensional cases.
  - Why unresolved: The paper's empirical evaluation is limited to moderate-dimensional problems. High-dimensional spaces with complex constraints may present challenges not encountered in the tested scenarios, such as the curse of dimensionality or the effectiveness of the isotropic noise distribution.
  - What evidence would resolve it: Empirical results showing the performance (e.g., log-likelihood, MMD) of Metropolis diffusion models on high-dimensional synthetic distributions or real-world datasets with >1000 dimensions would clarify its scalability limits.

- Question: Can the Metropolis sampling approach be extended to incorporate more sophisticated noise distributions (e.g., Dikin ellipsoids) while maintaining its simplicity and efficiency?
  - Basis in paper: [explicit] The paper discusses the possibility of using more suitable distributions like Dikin ellipsoids but does not explore this extension. It focuses on the isotropic noise distribution for simplicity.
  - Why unresolved: While the paper shows that the Metropolis approach is simple and efficient with isotropic noise, it does not investigate whether this simplicity can be preserved when using more sophisticated noise distributions that might better suit narrow or complex constraint geometries.
  - What evidence would resolve it: An extension of the Metropolis algorithm to use Dikin ellipsoids or other tailored noise distributions, along with an empirical comparison of its performance and computational efficiency against the isotropic version, would address this question.

- Question: What is the quantitative weak and mean square error of the Metropolis discretization scheme, and how does it compare to existing reflected and log-barrier methods?
  - Basis in paper: [inferred] The paper proves that the Metropolis discretization converges to the reflected Brownian motion, but it does not provide quantitative error bounds. The related work section mentions that other methods have known error rates (e.g., order 1/2 for projection schemes), but no such analysis is given for the Metropolis approach.
  - Why unresolved: While the theoretical convergence is established, the practical accuracy and efficiency of the Metropolis discretization in terms of weak and mean square errors are not quantified. This limits the ability to compare its performance rigorously against existing methods.
  - What evidence would resolve it: Derivation and empirical validation of the weak and mean square error rates for the Metropolis discretization, along with a comparison to the error rates of reflected and log-barrier methods on benchmark problems, would provide a clear answer.

## Limitations
- Theoretical analysis relies on strong regularity assumptions about constraint boundaries that may not hold for complex real-world constraints
- Empirical evaluation focuses on relatively simple geometries (hypercubes, simplices) and demonstrates feasibility rather than establishing state-of-the-art performance
- The paper does not address computational complexity analysis or provide systematic ablation studies on hyperparameters

## Confidence
- **High confidence**: The Metropolis sampling mechanism is sound and the basic theoretical framework is valid for well-behaved constraints
- **Medium confidence**: The empirical results demonstrate feasibility and provide proof of concept, but sample size and benchmark diversity are limited
- **Medium confidence**: The extension to non-convex constraints is promising but lacks rigorous theoretical justification

## Next Checks
1. Conduct systematic experiments varying constraint geometry complexity and measure rejection rates to identify breaking points where the Metropolis approach becomes inefficient
2. Perform ablation studies on step size, batch size, and network architecture to understand robustness and scalability limits
3. Test the method on high-dimensional, complex constraint manifolds (e.g., robotic manipulators with many joints) to evaluate practical limitations and compare against specialized geometric sampling techniques