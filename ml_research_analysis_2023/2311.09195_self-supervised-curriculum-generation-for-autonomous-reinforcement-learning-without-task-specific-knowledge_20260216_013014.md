---
ver: rpa2
title: Self-Supervised Curriculum Generation for Autonomous Reinforcement Learning
  without Task-Specific Knowledge
arxiv_id: '2311.09195'
source_url: https://arxiv.org/abs/2311.09195
tags:
- initial
- reset
- states
- agent
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel autonomous reinforcement learning (ARL)
  algorithm that generates a curriculum adaptive to the agent's learning progress
  without requiring task-specific knowledge. The key innovation is a success discriminator
  that estimates the probability of task success from each state-action pair when
  following the forward policy.
---

# Self-Supervised Curriculum Generation for Autonomous Reinforcement Learning without Task-Specific Knowledge

## Quick Facts
- arXiv ID: 2311.09195
- Source URL: https://arxiv.org/abs/2311.09195
- Reference count: 30
- Primary result: Achieved 0.89 success rate on antmaze-4way-v2 with 368 manual resets versus 0.17 with 976 resets for best baseline

## Executive Summary
This paper presents a novel autonomous reinforcement learning algorithm that generates adaptive curricula without requiring task-specific knowledge. The key innovation is a success discriminator trained self-supervised by relabeling transitions from successful and failed rollouts. This discriminator estimates the probability of task success from each state-action pair, enabling the reset policy to explore increasingly diverse and informative initial states as the forward policy improves. Experiments on sparse-reward maze navigation tasks demonstrate significant improvements in both success rates and sample efficiency compared to state-of-the-art ARL algorithms while requiring fewer manual resets.

## Method Summary
The algorithm alternates between a reset policy that explores diverse initial states and a forward policy that attempts to solve the task. A success discriminator estimates the probability of task success from each state-action pair when following the forward policy. The discriminator is trained in a self-supervised manner by relabeling all transitions in successful rollouts with the final success label, and all transitions in failed rollouts with failure. The reset policy selects initial states where the estimated success probability falls between thresholds λ1 and λ2, creating an adaptive curriculum that becomes increasingly challenging as the forward policy improves. The forward policy is trained using SAC, while the success discriminator uses binary cross-entropy loss with balanced sampling from successful and failed rollouts.

## Key Results
- Achieved 0.89 success rate on antmaze-4way-v2 with 368 manual resets versus 0.17 with 976 resets for best baseline
- Demonstrated higher success rates and sample efficiency than state-of-the-art ARL algorithms across multiple maze navigation tasks
- Showed the success discriminator enables reset policy to explore increasingly diverse states as training progresses
- Required significantly fewer manual resets than baseline methods while achieving better performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The success discriminator enables the reset policy to find increasingly diverse and informative initial states without task-specific knowledge.
- Mechanism: The success discriminator is trained self-supervised by relabeling rollouts. All states within successful rollouts are treated as initial states from which the forward policy can solve the task. This relabeling provides supervisory signals without requiring manual resets or predefined initial states.
- Core assumption: Every state within a successful rollout can be treated as an initial state from which the forward policy can solve a task.
- Evidence anchors: Abstract states success discriminator is "trained with relabeled transitions in a self-supervised manner." Section explicitly states they "relabel rollouts from the forward policy and use them to train the success discriminator in a self-supervised manner" based on the hypothesis that "every state within successful rollouts as an initial state from which the agent can solve a task using the forward policy."

### Mechanism 2
- Claim: The adaptive curriculum improves sample efficiency by providing initial states that are neither too easy nor too difficult for the forward policy.
- Mechanism: The reset policy selects initial states where the estimated success probability is between λ1 and λ2. As the forward policy improves, the success discriminator allows exploration of states with lower success probabilities, gradually increasing the difficulty of the curriculum.
- Core assumption: Initial states with success probabilities in the range [λ1, λ2] provide the right balance of challenge and achievability for the forward policy.
- Evidence anchors: Section states the key idea is "utilizing the success discriminator proposed in this work to identify the initial states where the forward policy can not only solve a task without causing extrinsic intervention but also obtain informative transitions." Section also states "As the performance of the forward policy improves, the success discriminator allows the agent to reset with more diverse and informative initial states over a broader state space."

### Mechanism 3
- Claim: The self-supervised training of the success discriminator avoids the need for task-specific knowledge like predefined initial states or reset reward functions.
- Mechanism: The success discriminator is trained using relabeled transitions from the forward policy's rollouts. This eliminates the need for extrinsic resets or manual labeling of initial states, making the algorithm applicable to a wide range of tasks without customization.
- Core assumption: Relabeling rollouts in a self-supervised manner provides sufficient information to train the success discriminator without task-specific knowledge.
- Evidence anchors: Abstract states success discriminator is "trained in a self-supervised manner by relabeling transitions from successful and failed rollouts." Section states "The simplest approach to obtaining supervisory signals for training the success discriminator is to empirically estimate the success probabilities by collecting multiple rollouts of the forward policy for each initial state. However, this approach requires extrinsic resets between every rollout and access to uniform initial state distribution, which are both obviously impractical in the real world. To address this challenge, we relabel rollouts from the forward policy and use them to train the success discriminator in a self-supervised manner."

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The algorithm is formulated as an MDP with states, actions, transition probabilities, rewards, initial state distribution, discount factor, and time horizon.
  - Quick check question: What are the components of an MDP and how do they relate to the autonomous reinforcement learning problem?

- Concept: Reinforcement Learning (RL)
  - Why needed here: The forward policy is trained using RL to maximize expected cumulative rewards in the environment.
  - Quick check question: How does RL differ from supervised learning, and what is the goal of an RL agent?

- Concept: Curriculum Learning
  - Why needed here: The algorithm generates a curriculum that adapts to the learning progress of the forward policy, providing increasingly challenging initial states.
  - Quick check question: What is curriculum learning and how does it benefit the training of reinforcement learning agents?

## Architecture Onboarding

- Component map: Forward policy -> Success discriminator -> Reset policy -> Environment
- Critical path: 1) Reset policy explores and selects initial states based on success probability bounds. 2) Forward policy attempts to solve the task from the initial state. 3) Transitions are stored in the forward buffer. 4) Success discriminator is trained using relabeled transitions from the forward buffer. 5) Reset policy and forward policy are updated based on their respective buffers.
- Design tradeoffs: Balancing λ1 and λ2 to ensure the reset policy selects initial states that are neither too easy nor too difficult. Choosing the relabeling strategy for the success discriminator (using the success label of the last transition). Deciding on the exploration algorithm for the reset policy (RND in this case).
- Failure signatures: Poor performance of the forward policy leading to unreliable success discriminator estimates. Reset policy failing to explore diverse states and getting stuck in local regions. Success discriminator not adapting as the forward policy improves, causing stale curriculum.
- First 3 experiments: 1) Implement the success discriminator and test its ability to estimate success probabilities on a simple grid world task with a known optimal policy. 2) Integrate the success discriminator with the reset policy and test the adaptive curriculum generation on a maze navigation task with sparse rewards. 3) Compare the performance and sample efficiency of the full algorithm against baselines on a suite of maze navigation tasks with varying complexity.

## Open Questions the Paper Calls Out
- The paper mentions future work on applying the method to tasks with irreversible states where resetting to previous states is impossible.
- The paper suggests exploring integration with representation learning techniques to handle high-dimensional observations like raw images.
- The paper identifies potential extensions to continuous control tasks beyond maze navigation.

## Limitations
- The algorithm still requires manual resets during training (though fewer than baselines), challenging the claim of true "autonomous" reinforcement learning.
- The performance improvements over baselines are less dramatic than suggested, with ablation studies showing limited gains from the success discriminator alone.
- The method's reliance on RND for reset policy exploration introduces additional hyperparameters and complexity not thoroughly analyzed.

## Confidence
- **High confidence**: The self-supervised relabeling approach for training the success discriminator is novel and well-implemented. The experimental methodology and evaluation metrics are sound.
- **Medium confidence**: The claimed sample efficiency improvements over baselines, particularly the relative ordering of performance across different maze tasks.
- **Low confidence**: The assertion that the algorithm truly achieves "autonomous" reinforcement learning with minimal manual intervention, given the continued need for manual resets during training.

## Next Checks
1. Conduct ablation studies isolating the contribution of the success discriminator from the RND-based reset policy to determine which component drives performance improvements.
2. Test the algorithm's sensitivity to hyperparameter choices (λ1, λ2, success discriminator learning rate) through comprehensive sweeps to assess robustness.
3. Evaluate the method on additional sparse-reward tasks beyond maze navigation to verify generalizability to different problem domains.