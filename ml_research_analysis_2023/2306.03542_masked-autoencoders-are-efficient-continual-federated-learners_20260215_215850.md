---
ver: rpa2
title: Masked Autoencoders are Efficient Continual Federated Learners
arxiv_id: '2306.03542'
source_url: https://arxiv.org/abs/2306.03542
tags:
- learning
- federated
- data
- continual
- clients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CONFEDMADE, a novel unsupervised continual
  federated learning framework that leverages masked autoencoders (MADE) to mitigate
  catastrophic forgetting and reduce communication costs. The key innovation is integrating
  MADE's auto-regressive masking strategy with FedWeIT's parameter decomposition framework,
  enabling selective knowledge transfer between clients.
---

# Masked Autoencoders are Efficient Continual Federated Learners

## Quick Facts
- arXiv ID: 2306.03542
- Source URL: https://arxiv.org/abs/2306.03542
- Reference count: 18
- The paper introduces CONFEDMADE, a novel unsupervised continual federated learning framework that leverages masked autoencoders (MADE) to mitigate catastrophic forgetting and reduce communication costs.

## Executive Summary
This paper introduces CONFEDMADE, a novel unsupervised continual federated learning framework that addresses catastrophic forgetting and high communication costs in federated learning scenarios. The key innovation is integrating MADE's auto-regressive masking strategy with FedWeIT's parameter decomposition framework, enabling selective knowledge transfer between clients. By synchronizing autoregressive masks across clients and applying them to both base and task-adaptive parameters, CONFEDMADE achieves up to 50% reduction in communication costs while maintaining or improving model performance. The framework is evaluated on MNIST, EMNIST, and four binary datasets, demonstrating superior performance compared to naive FedWeIT-MADE combinations and approaching upper-bound baselines.

## Method Summary
CONFEDMADE combines masked autoencoders (MADE) with FedWeIT's parameter decomposition framework to enable efficient unsupervised continual federated learning. The method uses synchronized autoregressive masks across all clients to ensure consistent federated averaging, applies MADE's mask to both base and task-adaptive parameters to induce sparsity and reduce communication, and employs attention mechanisms to selectively transfer knowledge between clients with overlapping tasks. The framework is trained sequentially on tasks distributed across 3-5 clients, with 50 communication rounds per task, using negative log-likelihood and forgetting metrics for evaluation.

## Key Results
- CONFEDMADE achieves up to 50% reduction in communication costs while maintaining or improving model performance
- The framework significantly outperforms naive FedWeIT-MADE combinations in various continual federated learning scenarios
- CONFEDMADE approaches the performance of upper-bound baselines while mitigating catastrophic forgetting
- Higher attention weights (0.60-0.67) correlate with task overlap and reduced forgetting, while lower weights (0.19-0.21) indicate no overlap

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Masked Autoencoders (MADE) can be federated without catastrophic forgetting by synchronizing the autoregressive masking strategy across clients.
- **Mechanism**: MADE's autoregressive property enforces that output unit $\hat{x}_d$ depends only on previous input units $x_{<d}$. In federated settings, if each client samples masks independently, the local MADE models update different sub-models during federated averaging, leading to performance collapse. Synchronizing the mask sequence ensures all clients share the same autoregressive structure, allowing consistent gradient updates.
- **Core assumption**: All clients can interpret the same random seed identically, enabling synchronized mask generation without explicit mask transmission.
- **Evidence anchors**:
  - [section] "sampling random integers for the masks, as suggested in the original MADE framework, independently on each client network, is detrimental to the obtained performance... Communicating the (sequence of) mask(s) from the server as well" and "The respective synchronized mask column in table 1a demonstrates how this almost fully alleviates performance drops".
  - [abstract] "Specifically, their masking strategy can be seamlessly integrated with task attention mechanisms to enable selective knowledge transfer between clients."
  - [corpus] Weak. Related works focus on federated learning with heterogeneity but do not discuss MADE's autoregressive masking in detail.
- **Break condition**: Clients have different random number generator implementations or seeds cannot be synchronized, forcing explicit mask communication and increasing overhead.

### Mechanism 2
- **Claim**: Integrating MADE's masking with FedWeIT's parameter decomposition framework reduces communication costs while maintaining performance.
- **Mechanism**: FedWeIT decomposes parameters into base (generic) and task-adaptive components with attention masks. MADE naturally induces sparsity through its autoregressive mask $M_W$. By applying MADE's mask to both base and task-adaptive parameters during federated averaging, only non-zero masked parameters are transmitted, cutting communication by ~50% without performance loss.
- **Core assumption**: The sparsity induced by MADE's autoregressive mask aligns well with the sparsity goals of FedWeIT's attention mechanism, so no additional masking interference occurs.
- **Evidence anchors**:
  - [section] "the amount of communicated (non-zero) parameters is thus reduced through MADE... we gain a substantial, almost half, reduction in the communicated amount of $B_c$ at virtually no performance drop" (table 3).
  - [abstract] "CONFEDMADE significantly outperforms naive FedWeIT-MADE combinations and approaches the performance of upper-bound baselines... The framework achieves up to 50% reduction in communication costs".
  - [corpus] Weak. No direct corpus evidence linking MADE sparsity to FedWeIT communication reduction.
- **Break condition**: MADE's autoregressive mask does not align with FedWeIT's parameter structure, causing excessive zeroing of parameters needed for knowledge transfer.

### Mechanism 3
- **Claim**: CONFEDMADE mitigates catastrophic forgetting by selectively attending to overlapping knowledge between clients.
- **Mechanism**: When clients share overlapping data distributions, CONFEDMADE learns attention weights $\alpha$ that prioritize transferring task-adaptive parameters from clients with similar tasks. This selective transfer preserves knowledge relevant to current tasks while reducing interference from unrelated tasks.
- **Core assumption**: Attention weights can effectively distinguish overlapping from non-overlapping tasks, and the knowledge base stores relevant task-adaptive parameters.
- **Evidence anchors**:
  - [section] "CONFEDMADE consistently reduces forgetting while sparsifying parameters... We can observe higher values of $\alpha$ (0.60 and 0.67) in the respective plots when there is overlap in the data subsets and thus later mitigated forgetting, and values as low as 0.19 or 0.21 when there is no overlap" (figure 4).
  - [abstract] "The key innovation is integrating MADE's auto-regressive masking strategy with FedWeIT's parameter decomposition framework, enabling selective knowledge transfer between clients."
  - [corpus] Weak. Related works discuss attention in federated learning but not in the context of MADE's masking and forgetting mitigation.
- **Break condition**: Attention mechanism fails to distinguish overlapping tasks, leading to inappropriate parameter transfer and increased forgetting.

## Foundational Learning

- **Concept**: Autoregressive modeling and masked autoencoders
  - **Why needed here**: MADE's autoregressive property enforces a directed dependency structure that naturally induces sparsity and enables distribution estimation. This structure is critical for federated learning because it allows synchronized masking across clients.
  - **Quick check question**: In MADE, can output unit $\hat{x}_3$ depend on input $x_1$ if the mask enforces autoregressive ordering?
    - **Answer**: Yes, because autoregressive ordering only restricts dependencies to previous indices, and $x_1$ is before $x_3$.

- **Concept**: Federated averaging and parameter decomposition
  - **Why needed here**: FedAvg aggregates model updates from multiple clients, but heterogeneous data can cause interference. FedWeIT's decomposition into base and task-adaptive parameters with attention masks addresses this by allowing selective knowledge transfer.
  - **Quick check question**: What is the primary purpose of FedWeIT's attention mechanism?
    - **Answer**: To selectively transmit task-adaptive parameters between clients based on task relevance, reducing interference.

- **Concept**: Catastrophic forgetting and continual learning strategies
  - **Why needed here**: In continual federated learning, clients sequentially learn tasks without revisiting data, risking forgetting. CONFEDMADE mitigates this through parameter decomposition and selective attention transfer.
  - **Quick check question**: How does CONFEDMADE's knowledge base help prevent forgetting?
    - **Answer**: It stores task-adaptive parameters from previous tasks, allowing clients to retrieve and attend to relevant knowledge when encountering similar tasks.

## Architecture Onboarding

- **Component map**:
  - Client side: MADE model with synchronized autoregressive mask, base parameters $B_c$, task-adaptive parameters $A_c$, attention weights $\alpha$, FedWeIT mask $m_c$
  - Server side: Global parameter averaging module, knowledge base storing task-adaptive parameters, mask synchronization service
  - Communication: Base parameters (sparsified by MADE mask), task-adaptive parameters, attention weights

- **Critical path**: Client trains MADE → applies synchronized mask → decomposes parameters → computes attention → sends sparsified base and task-adaptive parameters to server → server averages and stores → server sends averaged base and relevant task-adaptive parameters back

- **Design tradeoffs**:
  - Synchronized vs. independent masking: Synchronized masking ensures consistent federated averaging but requires mask communication; independent masking avoids communication but causes performance collapse
  - Sparsity vs. expressiveness: Higher MADE mask ratios reduce communication but may lose important parameters; lower ratios increase communication but preserve more information
  - Attention complexity: Full attention matrices enable fine-grained transfer but increase communication; sparse attention reduces communication but may miss relevant transfers

- **Failure signatures**:
  - Performance collapse during federated averaging → likely independent mask sampling issue
  - High communication costs → insufficient mask sparsity or attention inefficiency
  - Increased forgetting over time → attention mechanism failing to identify relevant task overlaps

- **First 3 experiments**:
  1. **Synchronized mask validation**: Train federated MADE with synchronized vs. independent masks on MNIST; verify performance difference (table 1a)
  2. **Communication cost measurement**: Implement CONFEDMADE with varying MADE mask ratios; measure communication reduction and performance impact (table 3)
  3. **Forgetting mitigation test**: Train CONFEDMADE on sequential MNIST tasks; compare forgetting metrics against FedWeIT-MADE baseline (table 2, figure 3)

## Open Questions the Paper Calls Out

- **Question**: How does the performance of CONFEDMADE scale with increasing numbers of clients and tasks?
  - **Basis in paper**: [explicit] The paper evaluates CONFEDMADE with 5 clients and 5 tasks, but does not explore scalability beyond this.
  - **Why unresolved**: The paper focuses on demonstrating the effectiveness of CONFEDMADE in a controlled setting, but does not investigate its behavior in larger-scale scenarios.
  - **What evidence would resolve it**: Experiments with varying numbers of clients and tasks, measuring performance metrics like negative log-likelihood and forgetting as the system scales.

- **Question**: How does CONFEDMADE perform in comparison to other unsupervised continual federated learning approaches beyond FedWeIT-MADE?
  - **Basis in paper**: [inferred] The paper compares CONFEDMADE primarily to FedWeIT-MADE and its variants, but does not benchmark against other unsupervised CFL methods.
  - **Why unresolved**: The paper establishes CONFEDMADE's superiority over FedWeIT-MADE, but does not explore its relative performance against other approaches in the field.
  - **What evidence would resolve it**: Comparative experiments with other unsupervised CFL methods, using the same datasets and evaluation metrics.

- **Question**: What is the impact of different MADE architectures (e.g., varying hidden layer sizes) on the performance of CONFEDMADE?
  - **Basis in paper**: [explicit] The paper mentions using MLPs with varied hidden layer capacities, but does not provide a detailed analysis of the impact of different architectures.
  - **Why unresolved**: While the paper demonstrates the effectiveness of CONFEDMADE, it does not explore how the choice of MADE architecture influences its performance.
  - **What evidence would resolve it**: Experiments with different MADE architectures, varying hidden layer sizes and depths, and measuring the impact on performance metrics.

## Limitations
- The specific masking ratio used in MADE architecture is described as "empirically chosen" without providing the exact value, which could affect reproducibility
- The relationship between MADE's autoregressive structure and FedWeIT's parameter decomposition is primarily empirical observation without rigorous theoretical proof
- The claim of "up to 50% reduction in communication costs" is based on specific experimental conditions and may not generalize across different model architectures or data distributions

## Confidence
- **High confidence**: Claims about synchronized masking improving federated MADE performance (supported by table 1a showing performance degradation without synchronization)
- **Medium confidence**: Claims about 50% communication cost reduction (based on table 3 results, but dependent on specific architecture choices)
- **Low confidence**: Theoretical justification for why MADE's autoregressive masking naturally complements FedWeIT's parameter decomposition (primarily empirical observation without formal proof)

## Next Checks
1. **Ablation study on masking ratio**: Systematically vary the MADE mask ratio from 0.1 to 0.9 and measure the impact on both performance and communication costs to identify optimal trade-offs
2. **Cross-architecture generalization**: Implement CONFEDMADE with different base architectures (CNN, transformer) to verify if the 50% communication reduction claim holds across architectures
3. **Theoretical analysis of parameter alignment**: Formalize the conditions under which MADE's autoregressive mask aligns with FedWeIT's parameter structure, providing mathematical guarantees for the observed empirical benefits