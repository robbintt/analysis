---
ver: rpa2
title: 'BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of
  Continuation Writing'
arxiv_id: '2309.00916'
source_url: https://arxiv.org/abs/2309.00916
tags:
- speech
- text
- blsp
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extending large language
  models (LLMs) to understand and generate speech, a task that current approaches
  struggle with due to either reliance on separate speech recognition systems or scarcity
  of speech instruction data. The authors propose a novel method called BLSP (Bootstrapping
  Language-Speech Pre-training via behavior alignment of continuation writing) that
  leverages existing speech recognition data to align speech and text modalities.
---

# BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing

## Quick Facts
- arXiv ID: 2309.00916
- Source URL: https://arxiv.org/abs/2309.00916
- Authors: 
- Reference count: 6
- Primary result: BLSP extends LLMs to speech inputs without separate ASR systems by aligning speech-text modalities through continuation writing behavior alignment

## Executive Summary
This paper addresses the challenge of extending large language models to understand and generate speech without relying on separate speech recognition systems or scarce speech instruction data. The authors propose BLSP (Bootstrapping Language-Speech Pre-training via behavior alignment of continuation writing), a novel method that leverages existing speech recognition data to align speech and text modalities. The core innovation is ensuring that LLMs exhibit consistent generation behavior regardless of whether the input is speech or its transcript. This is achieved by prompting an LLM to generate text continuations from speech transcripts and then training a lightweight modality adapter to produce the same continuations when given the corresponding speech input. The method is evaluated on various speech-related tasks including speech recognition, speech translation, and spoken language understanding, demonstrating the ability to extend LLMs to speech inputs while maintaining their language capabilities.

## Method Summary
BLSP operates through a two-step process that aligns speech and text modalities without requiring speech instruction data. First, a frozen LLM is prompted with speech transcripts to generate text continuations using a specific prompt format. Then, a lightweight modality adapter is trained to map speech features from a frozen speech encoder to the LLM's embedding space such that it produces the same continuations when given speech input as the LLM does with transcripts. The modality adapter consists of three 1D convolution layers with stride 2 (downsampling speech features by factor of 8) followed by a bottleneck layer. Both the speech encoder (Whisper-small) and LLM (Llama-2-7B fine-tuned on Alpaca-52K) remain frozen during training. The approach is trained on ASR datasets (LibriSpeech, GigaSpeech, Common Voice 2.0) and evaluated on speech recognition, translation, and understanding tasks.

## Key Results
- BLSP extends LLMs to speech inputs without separate ASR systems or speech instruction data
- The model demonstrates understanding of non-English speech inputs despite being trained solely on English ASR data
- Cross-lingual speech-to-text translation performance is promising but still lags behind cascaded approaches
- BLSP maintains LLM capabilities while adding speech understanding through modality alignment

## Why This Works (Mechanism)

### Mechanism 1
BLSP uses continuation writing to bridge the modality gap without requiring speech instruction data. Instead of training a model to transcribe speech to text, BLSP prompts the LLM to generate continuations from transcripts, then trains a modality adapter to produce the same continuations from speech features. The core assumption is that speech and text representations yielding the same LLM continuation behavior are semantically aligned.

### Mechanism 2
Freezing both speech encoder and LLM forces the modality adapter to learn cross-modal projection. By keeping the pretrained models fixed, the adapter must learn to map speech features into the LLM's textual embedding space without altering either modality's representation. The core assumption is that pretrained speech encoder and LLM embeddings are compatible enough that a lightweight adapter can bridge them.

### Mechanism 3
Continuation writing captures universal next-token prediction behavior across modalities. The LLM's ability to continue text from any prefix (speech transcript or otherwise) provides a consistent supervision signal for alignment. The core assumption is that next-token prediction is the most universal behavior that LLMs can exhibit across different input modalities.

## Foundational Learning

- **Concept: Speech feature extraction and temporal downsampling**
  - Why needed here: The modality adapter uses 1D convolutions with stride 2 to reduce speech feature length by factor of 8, making it compatible with LLM input expectations
  - Quick check question: What is the output length of a 100-frame speech input after passing through the three convolution layers with stride 2 and kernel 5?

- **Concept: Instruction tuning for LLM behavior control**
  - Why needed here: The LLM must follow specific continuation prompts to generate consistent supervision signals for alignment training
  - Quick check question: How does the prompt structure "###[Human]: Continue the following text..." influence the LLM's continuation generation?

- **Concept: Cross-modal alignment objectives**
  - Why needed here: BLSP uses behavior alignment rather than direct transcription alignment to avoid modality-specific bias
  - Quick check question: Why does training to predict transcripts directly fail to transfer instruction-following capability?

## Architecture Onboarding

- **Component map**: Whisper-small (frozen speech encoder) → 3-layer 1D conv + bottleneck (modality adapter) → Llama-2-7B (frozen LLM)
- **Critical path**: Speech features → Adapter → LLM → Continuation generation
- **Design tradeoffs**: Lightweight adapter (fewer parameters) vs. full fine-tuning of encoder/LLM; behavior alignment vs. direct transcription alignment
- **Failure signatures**: Low BERTScore on speech recognition despite reasonable BLEU on translation; mode collapse where adapter outputs become uniform; sensitivity to prompt phrasing
- **First 3 experiments**:
  1. Verify adapter can map speech features to text embedding space by checking cosine similarity with transcript embeddings
  2. Test continuation consistency: generate same continuation from speech vs. transcript of same utterance
  3. Evaluate cross-modal performance gap by comparing speech vs. text input on same downstream task

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of the modality adapter affect the overall performance of the BLSP model, and what architectural choices optimize its effectiveness? The paper mentions the modality adapter is composed of three 1D convolution layers followed by a bottleneck layer, but does not explore alternative architectures or analyze the impact of different designs on performance. Comparative experiments evaluating different modality adapter architectures and their impact on speech recognition, translation, and understanding tasks would help determine the optimal design.

### Open Question 2
How does the BLSP approach generalize to languages other than English, and what challenges arise when extending it to multilingual settings? The paper demonstrates understanding of non-English speech inputs despite being trained solely on English ASR data, but there is still a significant gap in translation quality compared to cascaded approaches. Experiments evaluating performance on various languages and tasks, as well as investigations into potential improvements for multilingual settings, would help understand the approach's generalizability and limitations.

### Open Question 3
How can the BLSP approach be adapted to capture and incorporate paralinguistic information, such as emotions, tones, and intentions, in spoken language understanding? The current study focuses on aligning speech and text in the semantic space without addressing paralinguistic aspects of spoken language. Experiments incorporating paralinguistic information into the BLSP model and evaluating its impact on speech understanding tasks would help determine the effectiveness of such an approach.

## Limitations

- The reliance on ASR data for training the modality adapter creates potential circularity, as speech encoder biases toward ASR tasks may propagate to the aligned LLM
- Cross-lingual performance shows promising capabilities but still exhibits significant gaps compared to cascaded approaches, particularly for low-resource languages
- The method's generalization to diverse speech behaviors beyond continuation writing (such as question answering or dialogue) requires further validation

## Confidence

- **High Confidence**: The core claim that a lightweight modality adapter can align speech and text representations to achieve consistent LLM behavior across modalities
- **Medium Confidence**: The assertion that continuation writing captures the most universal LLM behavior for cross-modal alignment
- **Low Confidence**: The claim that this approach eliminates the need for speech instruction data entirely

## Next Checks

1. **Cross-task Generalization Test**: Evaluate BLSP on speech-based question answering and dialogue tasks beyond the current ASR/ST/SLU benchmarks to verify that continuation-based alignment generalizes to diverse instruction-following behaviors.

2. **Modality Sensitivity Analysis**: Systematically test the model's performance when using speech features from different encoders (not just Whisper) or when applying adversarial perturbations to speech inputs to measure the robustness of the learned alignment.

3. **Multilingual Alignment Quality**: Conduct a comprehensive study measuring alignment quality across language families, including low-resource languages, by comparing speech-to-text continuation consistency and downstream task performance across linguistic distances.