---
ver: rpa2
title: Large Language Models are In-context Teachers for Knowledge Reasoning
arxiv_id: '2311.06985'
source_url: https://arxiv.org/abs/2311.06985
tags:
- prompting
- human-crafted
- in-context
- llms
- self-explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SELF-EXPLAIN, a method where large language
  models (LLMs) generate their own chain-of-thought (CoT) explanations for reasoning
  tasks, which are then used as in-context demonstrations to teach themselves or other
  LLMs. The method is motivated by the encoding specificity hypothesis, which suggests
  that exemplars used for in-context learning should match the distribution of the
  training corpus.
---

# Large Language Models are In-context Teachers for Knowledge Reasoning

## Quick Facts
- arXiv ID: 2311.06985
- Source URL: https://arxiv.org/abs/2311.06985
- Reference count: 40
- One-line primary result: Self-generated chain-of-thought explanations outperform human-crafted ones for in-context learning on complex reasoning tasks.

## Executive Summary
This paper challenges the prevailing belief that human-crafted chain-of-thought (CoT) explanations are the optimal in-context demonstrations for prompting large language models (LLMs). The authors propose SELF-EXPLAIN, a method where LLMs generate their own CoT explanations for reasoning tasks, which are then used as in-context demonstrations. Drawing from the encoding specificity hypothesis in cognitive psychology, the authors argue that self-generated explanations better match the training corpus distribution of the student LLM, leading to improved performance on complex question-answering tasks. The method shows significant improvements, particularly on challenging medical QA datasets, and also results in better model calibration and reduced bias compared to human-crafted explanations.

## Method Summary
The SELF-EXPLAIN method involves prompting an LLM to generate its own chain-of-thought explanations for question-answer pairs, which are then used as in-context demonstrations for prompting itself or other LLMs. The process includes generating diverse self-explanations for training examples using specific prompts, storing these explanations, and selecting appropriate exemplars at inference time. The framework aims to leverage the encoding specificity hypothesis, suggesting that exemplars matching the training corpus distribution serve as better retrieval cues for the student LLM. The method is evaluated on three challenging datasets: MedMCQA, MedQA, and StrategyQA, with comparisons to zero-shot CoT, Auto-CoT, and human-crafted CoTs.

## Key Results
- Self-generated CoT explanations outperform human-crafted ones by up to 2% in test accuracy on complex medical QA datasets
- SELF-EXPLAIN improves model calibration and reduces selection bias compared to human-crafted demonstrations
- The method shows diminishing returns with increased numbers of generated explanations, suggesting an optimal diversity threshold

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-generated explanations are more effective in-context demonstrations because they match the encoding distribution of the student LLM's training corpus.
- Mechanism: The encoding specificity hypothesis from human memory retrieval suggests that retrieval cues (in-context exemplars) should match the context during encoding. When an LLM generates its own explanations, those explanations are drawn from the same knowledge distribution the model was trained on, making them more aligned with the model's internal representation space.
- Core assumption: The self-generated explanations are not just superficially similar but share the same underlying distributional properties as the training corpus that the student LLM was exposed to during pretraining.
- Evidence anchors:
  - [abstract] "We hypothesize that in-context exemplars crafted by the teacher should match the training data of the student" and "This hypothesis motivates us to propose Self-Explain where an LLM's self-elicited explanations are used as in-context demonstrations for prompting it as they are generalized from the model's training examples."
  - [section 3.1] "From a memory view, CoTs should be composed in a way to match the context in the training corpus seen during encoding similar information to the test data."
  - [corpus] Weak evidence - the corpus only shows related works on knowledge distillation and teacher-student frameworks, not direct evidence for encoding specificity matching.
- Break condition: If the self-generated explanations rely heavily on patterns or knowledge not well-represented in the student LLM's training corpus, or if the generation process introduces biases that diverge from the original training distribution.

### Mechanism 2
- Claim: Self-explanations improve model calibration and reduce bias compared to human-crafted explanations.
- Mechanism: When LLMs generate their own explanations, they produce reasoning paths that are more naturally aligned with their confidence calibration. Human-crafted explanations may introduce external patterns or reasoning styles that the model hasn't internalized, leading to overconfidence or selection bias in multi-choice settings.
- Core assumption: The model's internal confidence estimation is more accurate when processing reasoning paths it generated itself versus externally provided ones.
- Evidence anchors:
  - [abstract] "we find that for ICT, rationales from different teacher LLMs or human experts that more resemble the student LLM's self-explanations are better in-context demonstrations."
  - [section 6] "We find that (1) Self-explanation improves calibration. Fig. 3a showcases the model is generally more calibrated when using self-explanation."
  - [corpus] Weak evidence - related works on knowledge distillation mention noise issues but don't directly address calibration differences between self-generated and human explanations.
- Break condition: If the self-explanation generation process is flawed or if the model's confidence estimation is systematically poor, calibration improvements may not materialize.

### Mechanism 3
- Claim: Self-explanations are more generalizable across different reasoning tasks when generated with diversity prompts.
- Mechanism: By prompting the LLM to generate multiple distinct explanations for the same problem, the framework captures different reasoning pathways that the model has learned. This diversity increases the likelihood that at least one explanation style will match the reasoning required for unseen test cases.
- Core assumption: The LLM has learned multiple valid reasoning strategies during pretraining, and exposing it to this diversity in exemplars improves its ability to adapt to novel problems.
- Evidence anchors:
  - [section 4] "To mitigate this issue, we design Î³ so as to prompt the model to generate solutions employing distinct logics."
  - [section B] "We find more diverse self-explanations to choose for an in-context exemplar can generally improve the test performance, while such improvement experiences diminishing return with further increased number of generations."
  - [corpus] Weak evidence - corpus neighbors discuss teacher-student generalization but don't specifically address diversity in reasoning exemplars.
- Break condition: If the LLM's pretraining didn't expose it to diverse reasoning strategies, or if the diversity prompts don't actually generate meaningfully different explanations.

## Foundational Learning

- Concept: In-context learning (ICL) and Chain-of-Thought (CoT) prompting
  - Why needed here: The paper builds on ICL as the foundation, where models learn from demonstrations without weight updates. CoT prompting is the specific technique being improved.
  - Quick check question: What is the key difference between zero-shot CoT and few-shot CoT prompting?

- Concept: Encoding specificity hypothesis from cognitive psychology
  - Why needed here: This is the theoretical foundation motivating why self-generated explanations should work better than human-crafted ones.
  - Quick check question: How does the encoding specificity hypothesis explain why matching training context improves retrieval performance?

- Concept: Knowledge distillation and teacher-student learning paradigms
  - Why needed here: The paper positions self-explanation as a form of self-teaching, which relates to broader literature on knowledge transfer between models of different capabilities.
  - Quick check question: What is the key difference between traditional knowledge distillation and the self-explanation approach proposed here?

## Architecture Onboarding

- Component map:
  - LLM inference engine (for both generation and inference)
  - Prompt template generator (for self-explanation generation)
  - Diversity controller (manages multiple explanation generation)
  - In-context exemplar selector (chooses which self-explanations to use)
  - Evaluation pipeline (measures accuracy, calibration, bias)

- Critical path:
  1. Generate self-explanations for training data
  2. Store diverse explanations per example
  3. At inference time, select exemplars and their explanations
  4. Concatenate exemplars with test query
  5. Generate final answer

- Design tradeoffs:
  - Generation cost vs. performance: More diverse explanations improve performance but increase generation time and cost
  - Template specificity vs. generality: More specific prompts may generate better explanations but could reduce diversity
  - Model size vs. effectiveness: Larger models may generate better explanations but the approach should work across sizes

- Failure signatures:
  - Low diversity in generated explanations (all explanations follow same pattern)
  - Self-explanations that don't actually explain the reasoning (just restate the answer)
  - Performance degradation when switching from human-crafted to self-generated exemplars
  - Calibration issues where model becomes overconfident or underconfident

- First 3 experiments:
  1. Generate self-explanations for a small validation set and compare ROUGE-L similarity with human-crafted explanations
  2. Test ICL performance using self-explanations vs. human-crafted explanations on a simple reasoning task
  3. Measure model confidence calibration differences between self-generated and human-provided exemplars

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the quality of self-explanations improve with larger model sizes, or does it plateau?
- Basis in paper: [inferred] The paper mentions using GPT-3.5-turbo as the base LLM and does not explore the effects of using larger models for generating self-explanations.
- Why unresolved: The paper does not investigate the relationship between model size and the quality of self-generated explanations.
- What evidence would resolve it: Experiments comparing the performance of ICL with self-explanations generated by different sized LLMs on the same datasets.

### Open Question 2
- Question: How do self-explanations perform in domains beyond question-answering, such as text summarization or machine translation?
- Basis in paper: [explicit] The paper focuses on knowledge-intensive question-answering tasks that require logical thinking and does not explore other NLP tasks.
- Why unresolved: The study is limited to question-answering and does not investigate the generalizability of self-explanations to other tasks.
- What evidence would resolve it: Experiments applying SELF-EXPLAIN to other NLP tasks and comparing the results with human-crafted demonstrations or other baselines.

### Open Question 3
- Question: Can the SELF-EXPLAIN framework be extended to incorporate human feedback or supervision to further improve the quality of self-explanations?
- Basis in paper: [inferred] The paper presents SELF-EXPLAIN as a fully automated method for generating in-context demonstrations and does not discuss the potential for human-in-the-loop approaches.
- Why unresolved: The study does not explore the possibility of combining self-explanations with human feedback or supervision.
- What evidence would resolve it: Experiments incorporating human feedback or supervision into the SELF-EXPLAIN framework and evaluating the impact on performance compared to fully automated self-explanations.

## Limitations

- Limited evaluation to question-answering tasks only, with unclear generalization to other reasoning domains
- No ablation studies isolating the encoding specificity mechanism from other potential contributors to performance gains
- Computational overhead of generating diverse self-explanations not thoroughly analyzed in cost-benefit terms

## Confidence

High confidence in: The observation that self-generated explanations outperform human-crafted CoTs on the specific datasets tested. This is a direct empirical finding with clear metrics.

Medium confidence in: The theoretical mechanism that encoding specificity explains why self-generated explanations work better. While the hypothesis is plausible, the paper provides limited direct evidence connecting this psychological theory to LLM behavior.

Low confidence in: The claim that this approach challenges the gold standard of human-crafted CoTs more broadly. The paper only tests a limited set of tasks and doesn't explore whether human-crafted CoTs might still be superior for other types of reasoning or in different domains.

## Next Checks

1. **Ablation study on encoding specificity**: Test whether the performance improvement disappears when self-generated explanations are deliberately made to mismatch the training corpus distribution (e.g., by using prompts that force reasoning styles not present in pretraining data).

2. **Cross-domain generalization**: Apply SELF-EXPLAIN to reasoning tasks outside the medical and general QA domains tested in the paper (e.g., mathematical reasoning, code generation, or commonsense reasoning) to verify if the encoding specificity advantage holds across diverse task types.

3. **Human evaluation of explanation quality**: Conduct human studies to assess whether self-generated explanations are actually more aligned with the model's reasoning process and confidence calibration compared to human-crafted explanations, independent of performance metrics.