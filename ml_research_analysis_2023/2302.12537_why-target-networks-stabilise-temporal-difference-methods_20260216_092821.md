---
ver: rpa2
title: Why Target Networks Stabilise Temporal Difference Methods
arxiv_id: '2302.12537'
source_url: https://arxiv.org/abs/2302.12537
tags:
- target
- function
- methods
- networks
- pfpe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides the first theoretical analysis of target networks
  in deep reinforcement learning. The authors introduce a framework called Partially
  Fitted Policy Evaluation (PFPE) that generalizes both fitted policy evaluation and
  classic temporal difference methods.
---

# Why Target Networks Stabilise Temporal Difference Methods

## Quick Facts
- arXiv ID: 2302.12537
- Source URL: https://arxiv.org/abs/2302.12537
- Reference count: 40
- This paper provides the first theoretical analysis of target networks in deep reinforcement learning, showing they stabilize TD learning by improving Jacobian conditioning.

## Executive Summary
This paper introduces Partially Fitted Policy Evaluation (PFPE), a framework that generalizes both fitted policy evaluation and classic temporal difference methods. Through rigorous Jacobian analysis, the authors demonstrate that target networks stabilize TD learning by improving the conditioning of the TD Jacobian. This stabilization enables convergence guarantees even in challenging off-policy and nonlinear function approximation settings where traditional TD methods fail. The key insight is that PFPE can break the "deadly triad" (TD with function approximation and off-policy data) by choosing appropriate step sizes and update frequencies.

## Method Summary
The authors introduce Partially Fitted Policy Evaluation (PFPE) as a framework that bridges the gap between fully fitted policy evaluation and classic TD learning. PFPE performs k steps of gradient descent on the loss function before updating target network parameters, effectively limiting the fitting phase. The stability of PFPE updates is determined by the conditioning of three Jacobians: the Hessian of the loss, the Jacobian of the TD-error vector, and the Jacobian of the TD update. Under non-decaying step sizes and appropriate choices of k, PFPE can ensure convergence even when classic TD methods diverge under the deadly triad conditions.

## Key Results
- PFPE converges under non-decaying step sizes when classic TD diverges under the deadly triad
- Target networks stabilize TD learning by improving Jacobian conditioning through periodic updates
- The stability region for PFPE can be characterized by a condition function C(α,k) < 1
- PFPE breaks the deadly triad by tuning step sizes and update frequencies appropriately

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Target networks stabilize TD learning by improving the conditioning of the TD Jacobian through periodic updates.
- **Mechanism**: PFPE bridges the gap between fully fitted policy evaluation (which iteratively fits to target values) and classic TD by limiting the fitting phase to a fixed number of steps k. This partial fitting improves the conditioning of the Hessian of the loss function, which in turn affects the overall stability of the parameter updates.
- **Core assumption**: The stability of the PFPE update is determined by the conditioning of three Jacobians - the Hessian of the loss, the Jacobian of the TD-error vector, and the Jacobian of the TD update.
- **Evidence anchors**:
  - [abstract]: "this insight leads us to conclude that the use of target networks can mitigate the effects of poor conditioning in the Jacobian of the TD update"
  - [section 3.2]: "the stability of the expected PFPE update gk(¯ωl,αl) is determined by the conditioning of three Jacobians"
- **Break condition**: If the step size α tends to zero (as in decaying step size regimes), the Jacobian conditioning improvement becomes negligible and PFPE behaves like classic TD, potentially diverging under the "deadly triad" conditions.

### Mechanism 2
- **Claim**: PFPE can break the "deadly triad" (TD with function approximation and off-policy data) by choosing appropriate step sizes and update frequencies.
- **Mechanism**: Under non-decaying step sizes, PFPE can ensure convergence even when classic TD methods fail by maintaining updates within a region where the condition function C(α,k) < 1. This is achieved by tuning both the step size α and the number of partial fitting steps k.
- **Core assumption**: There exists a contraction region X(ω⋆) where the condition function remains bounded by c² < 1 for all updates.
- **Evidence anchors**:
  - [abstract]: "under mild regularity conditions and a well tuned target network update frequency, convergence can be guaranteed even in the extremely challenging off-policy sampling and nonlinear function approximation setting"
  - [section 5.2]: "PFPE breaks TD's deadly triad... by tuning αl and k, regardless of the MDP, sampling regime, or function approximator"
- **Break condition**: If α ≥ max((λmin₁+λmax₁)/2, ((λmin₁)²+∥J⋆δ∥²)/(2λmin₁))⁻¹, then no finite k can guarantee convergence within the contraction region.

### Mechanism 3
- **Claim**: Target networks are the actual object being optimized rather than merely a stabilization mechanism for TD updates.
- **Mechanism**: By reformulating PFPE as a single update applied only to target network parameters, the analysis reveals that convergence properties depend primarily on the conditioning of the Hessian path-mean Jacobian rather than the TD Jacobian. This reframes target networks as the primary optimization target.
- **Core assumption**: The sequence of intermediate parameter updates between target network updates can be compressed into a single effective update for the target parameters.
- **Evidence anchors**:
  - [abstract]: "this insight leads us to empirically investigate a novel target parameter update scheme that uses a momentum-style update"
  - [section 3.2]: "it suffices to consider the target parameter update in isolation when analyzing PFPE"
- **Break condition**: If the Hessian path-mean Jacobian becomes ill-conditioned despite target network updates, convergence may still fail regardless of the momentum-style update scheme.

## Foundational Learning

- **Concept**: Markov Decision Processes (MDPs) and Bellman operators
  - Why needed here: The paper's analysis is built on understanding how value functions satisfy recursive Bellman equations and how temporal difference methods approximate these through sampled backups.
  - Quick check question: What property of the Bellman operator ensures that TD methods can converge in the tabular case?

- **Concept**: Function approximation and the deadly triad
  - Why needed here: The paper's central contribution is understanding how target networks mitigate the instability that arises when combining TD learning with function approximation and off-policy data.
  - Quick check question: Why does the deadly triad make classic TD methods diverge even in simple environments like Baird's counterexample?

- **Concept**: Stochastic approximation and Robbins-Monro conditions
  - Why needed here: The convergence analysis relies on showing that the stochastic updates satisfy conditions for convergence to a fixed point, particularly when step sizes don't decay to zero.
  - Quick check question: What are the Robbins-Monro conditions and why are they necessary for proving convergence of stochastic approximation algorithms?

## Architecture Onboarding

- **Component map**: Value function approximator Qω(s,a) -> Target network parameters ¯ω (updated every k timesteps) -> Sampling mechanism for transitions -> Loss function L(ω; ¯ω) = ||Qω - Tπ[Q¯ω]||²d,μ -> Partial fitting loop (k steps SGD)

- **Critical path**: 
  1. Sample transition (s,a,r,s',a') from environment
  2. Compute TD error: δ = (r + γQ¯ω(s',a') - Qω(s,a))∇ωQω(s,a)
  3. Update parameters: ω ← ω + αδ
  4. After k updates, set ¯ω ← ω
  5. Repeat until convergence

- **Design tradeoffs**:
  - Update frequency k vs. convergence speed: Higher k improves conditioning but slows adaptation
  - Step size α vs. stability: Larger α speeds learning but may cause divergence if k is too small
  - Batch size vs. variance: Larger batches reduce gradient variance but increase memory/computation

- **Failure signatures**:
  - Divergence (parameters growing unbounded): Indicates poor conditioning or step size too large for current k
  - Oscillation around suboptimal values: Suggests step size too small or k too large for current learning rate
  - Slow convergence: May indicate need to increase k or adjust step size schedule

- **First 3 experiments**:
  1. Implement PFPE with varying k on Baird's counterexample to verify convergence when classic TD diverges
  2. Test momentum-style target updates on Cartpole to validate the hypothesis that target networks are the primary optimization target
  3. Grid search over (α,k) pairs on a simple off-policy task to map the stability region and verify theoretical predictions about the contraction interval

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise quantitative conditions under which target networks stabilize TD learning in practice?
- Basis in paper: [explicit] The authors derive conditions involving eigenvalues of Jacobians and bounds on the condition function, but note that these depend on unknown quantities like Hessian eigenvalues and the TD Jacobian norm.
- Why unresolved: The theoretical conditions involve quantities that are difficult to compute or estimate in practice, and the paper suggests treating them as hyperparameters to be tuned.
- What evidence would resolve it: Empirical studies systematically varying hyperparameters like learning rate, update frequency k, and regularization strength, and measuring their effect on convergence and stability across diverse environments and function approximators.

### Open Question 2
- Question: How do different target network update schemes (beyond the standard periodic update) affect learning stability and convergence speed?
- Basis in paper: [explicit] The authors experimentally investigate a momentum-style update for target parameters and observe improved stability and learning in Cartpole.
- Why unresolved: The analysis only considers one alternative update scheme (momentum), and the paper suggests this as an avenue for future research.
- What evidence would resolve it: Systematic comparison of various target network update methods (e.g., Polyak averaging, Nesterov momentum, adaptive methods) across multiple tasks, measuring both convergence speed and final performance.

### Open Question 3
- Question: Under what conditions does partially fitted policy evaluation break the deadly triad in deep reinforcement learning?
- Basis in paper: [explicit] The authors prove that PFPE can converge when classic TD and fitted methods fail, but only under non-decaying step sizes and with appropriate choices of update frequency and learning rate.
- Why unresolved: The theoretical results require specific conditions on the Hessian eigenvalues and TD Jacobian that may not hold in practice, and the paper suggests treating these as hyperparameters.
- What evidence would resolve it: Empirical validation across a range of deep RL benchmarks, testing whether PFPE consistently outperforms or matches standard methods when the deadly triad conditions are present.

## Limitations
- Theoretical framework relies on assumptions about Lipschitz continuity and boundedness of Jacobians that may not hold for deep neural networks
- Analysis focuses on expected updates rather than stochastic trajectories, potentially missing variance-related failure modes
- Jacobian conditioning analysis assumes smooth parameter landscapes that may not exist in high-dimensional deep RL settings

## Confidence
*High confidence*: The theoretical framework connecting Jacobian conditioning to stability is mathematically sound and the PFPE algorithm is clearly defined. The mechanism by which target networks improve conditioning through periodic updates is well-established.

*Medium confidence*: The claim that PFPE can break the deadly triad under non-decaying step sizes is supported by theoretical analysis but requires empirical validation across diverse environments. The condition function C(α,k) < 1 provides a useful bound but may be conservative in practice.

*Low confidence*: The reframing of target networks as the primary optimization target rather than a stabilization mechanism is an interesting hypothesis but lacks sufficient empirical validation. The momentum-style target updates proposed based on this insight need more thorough testing.

## Next Checks
1. **Empirical validation of Jacobian conditioning**: Track the eigenvalues of the TD Jacobian and Hessian during PFPE training across multiple environments to verify that conditioning actually improves as predicted by the theory.

2. **Ablation study on k and α**: Systematically vary the update frequency k and step size α across a grid of values on Baird's counterexample to empirically map the stability region and compare with theoretical predictions about the contraction interval.

3. **Deep network testing**: Implement PFPE with deep neural networks on challenging continuous control tasks (e.g., Walker2d, Humanoid) to assess whether the theoretical guarantees extend to high-dimensional function approximation settings where the smoothness assumptions may break down.