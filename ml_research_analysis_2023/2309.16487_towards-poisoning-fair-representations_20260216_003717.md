---
ver: rpa2
title: Towards Poisoning Fair Representations
arxiv_id: '2309.16487'
source_url: https://arxiv.org/abs/2309.16487
tags:
- attack
- poisoning
- data
- learning
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the first data poisoning attack on fair representation
  learning (FRL). The attack maximizes mutual information (MI) between learned representations
  and sensitive features to degrade fairness, formulated as a bilevel optimization.
---

# Towards Poisoning Fair Representations

## Quick Facts
- arXiv ID: 2309.16487
- Source URL: https://arxiv.org/abs/2309.16487
- Reference count: 39
- Key outcome: This paper proposes the first data poisoning attack on fair representation learning (FRL). The attack maximizes mutual information (MI) between learned representations and sensitive features to degrade fairness, formulated as a bilevel optimization. A key challenge is the intractable MI computation, which is approximated using Fisher's Linear Discriminant (FLD) scores as a proxy for data separability. The attack uses gradient matching to handle the non-convexity of deep neural networks, combined with an elastic-net penalty to enhance stealthiness. Theoretical analysis provides the minimal number of poisoning samples required. Experiments on benchmark datasets show that the attack significantly increases MI and demographic parity violations, outperforming existing attacks. The results demonstrate the vulnerability of FRL models to poisoning attacks.

## Executive Summary
This paper introduces the first data poisoning attack specifically targeting fair representation learning (FRL) models. The attack aims to maximize the mutual information between learned representations and sensitive features, thereby degrading fairness by making representations carry more demographic information. The authors formulate this as a bilevel optimization problem and use Fisher's Linear Discriminant (FLD) as a tractable proxy for the intractable mutual information maximization. The attack employs gradient matching to handle the non-convexity of deep neural networks and incorporates an elastic-net penalty to enhance stealthiness. Experiments on benchmark datasets demonstrate significant effectiveness in increasing mutual information and demographic parity violations compared to existing attacks.

## Method Summary
The attack maximizes mutual information between learned representations and sensitive features by injecting poisoning samples into training data. Since direct MI computation is intractable, the attack uses Fisher's Linear Discriminant (FLD) as a proxy measure of data separability. The poisoning samples are generated through gradient matching, where the gradients of the victim model on poisoning samples approximate the desired update direction (gradient of FLD score). An elastic-net penalty is applied to encourage sparse perturbations and enhance stealthiness. The attack is formulated as a bilevel optimization problem and solved using an Iterative Shrinkage-Thresholding Algorithm (ISTA) variant. The minimal number of poisoning samples required is theoretically analyzed and empirically validated on benchmark datasets.

## Key Results
- The attack significantly increases mutual information between representations and sensitive features, with Adult dataset showing MI increases from ~0.0001 to 0.008-0.018
- Demographic parity violations increase by factors of 1.34-1.57 for Adult dataset and 1.06-1.42 for German dataset
- The attack outperforms existing baseline attacks (anchor attacks) on both effectiveness and stealth metrics
- Theoretical analysis provides minimal poisoning sample requirements that align with empirical observations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maximizing mutual information (MI) between representations and sensitive features degrades fairness by making representations carry more sensitive information
- Mechanism: The attack goal directly targets the core principle of fair representation learning (FRL) by maximizing I(a,z) where a is the sensitive feature and z is the learned representation
- Core assumption: In FRL, minimizing I(a,z) is the fundamental objective, so maximizing it directly violates the fairness goal
- Evidence anchors:
  - [abstract]: "We induce the model to output unfair representations that contain as much demographic information as possible by injecting carefully crafted poisoning samples into the training data"
  - [section 2.2]: "Motivated by the importance of MI-based fairness in FRL, we attack the fairness on some target data Dta by maximizing I(a,z)"

### Mechanism 2
- Claim: Gradient matching allows poisoning samples to control the victim model's optimization trajectory
- Mechanism: By matching the gradients of the victim model on poisoning samples with the desired update direction (gradient of FLD score), training on these samples will approximately optimize the attack objective
- Core assumption: The non-convexity of deep neural networks makes exact gradient tracking impossible, so gradient matching provides a practical approximation
- Evidence anchors:
  - [section 2.4]: "To solve this problem approximately by matching the upper- and lower-level gradients following Geiping et al. (2020)"
  - [section 2.4]: "The gradient of the victim from FLD score gives the desired update direction; when it matches the gradients of the victim on poisoning samples, training the victim on poisoning samples will solve the attack goal approximately"

### Mechanism 3
- Claim: Fisher's Linear Discriminant (FLD) score provides a tractable proxy for MI maximization
- Mechanism: FLD measures data separability between sensitive groups, and maximizing it makes representations more discriminatory, which correlates with higher MI
- Core assumption: When sensitive groups are more separable in representation space, the optimal classifier's BCE loss decreases, which bounds MI from below
- Evidence anchors:
  - [section 2.3]: "We instead maximize Fisher's linear discriminant (FLD) score, a closed-formed data separability measure"
  - [section 2.3]: "FLD score as a measure of data separability is still valid (Fisher, 1936), and we verify its efficacy for our goal in Appendix D.2"

## Foundational Learning

- Concept: Bilevel optimization
  - Why needed here: The attack involves optimizing over poisoning samples (upper level) while considering how these affect the victim model training (lower level)
  - Quick check question: What makes bilevel optimization particularly challenging in the context of non-convex deep neural networks?

- Concept: Mutual information and its variational bounds
  - Why needed here: MI is the core fairness metric being attacked, but it lacks analytic form, requiring approximation through bounds
  - Quick check question: How does the negative BCE loss of an optimal classifier provide a lower bound for mutual information?

- Concept: Fisher's Linear Discriminant
  - Why needed here: Provides a tractable, closed-form measure of data separability that serves as a proxy for the intractable MI maximization
  - Quick check question: Under what distributional assumptions does FLD become optimal for minimizing BCE loss?

## Architecture Onboarding

- Component map: Victim model (FRL encoder + downstream classifiers) -> Poisoning sample generator (ENG algorithm) -> Evaluation framework (MI/BCE loss computation, demographic parity violation measurement) -> Data preprocessing pipeline

- Critical path: Pre-train victim → Compute FLD gradients → Generate poisoning samples via gradient matching → Re-train victim on poisoned data → Evaluate fairness degradation

- Design tradeoffs:
  - Using FLD vs. other MI proxies: FLD is analytically tractable but assumes linear separability; other methods like sliced MI might be more general but harder to differentiate
  - Gradient matching vs. influence functions: Gradient matching is computationally efficient but approximate; influence functions are exact but expensive for deep networks
  - Elastic-net penalty: Balances stealthiness (L1 sparsity) with stability (L2 regularization) but requires hyperparameter tuning

- Failure signatures:
  - Attack fails to increase MI: Check if FLD score is actually correlating with BCE loss; verify poisoning samples are being used in training
  - Poisoning samples detected: Check elastic-net penalty values; verify samples remain within valid data domain
  - Victim model converges to different optimum: Check if batch size is too large (per Theorem 3.4); verify learning rate is appropriate

- First 3 experiments:
  1. Verify FLD score correlates with BCE loss on clean data by plotting both metrics during victim training
  2. Test ENG with varying λ1, λ2 values on a small dataset to observe trade-off between attack effectiveness and perturbation stealthiness
  3. Measure how batch size affects attack success rate by running attacks with different batch sizes while keeping other parameters constant

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are poisoning attacks on fair representation learning models when the sensitive feature has more than two classes?
- Basis in paper: [explicit] The paper mentions that extending the attack to multi-class sensitive features is possible and discusses the theoretical framework, but only provides empirical results for a binary case (race as sensitive feature with three categories: white, black, and others).
- Why unresolved: The paper lacks comprehensive experimental evaluation on datasets with more than two sensitive groups.
- What evidence would resolve it: Experiments on datasets with multiple sensitive classes showing the attack's effectiveness or limitations.

### Open Question 2
- Question: Can the elastic-net penalty be effectively integrated into other poisoning attacks like anchor attacks to improve stealthiness?
- Basis in paper: [explicit] The paper discusses the difficulty of applying elastic-net penalty to anchor attacks due to the use of τ-balls for sampling anchor points, which complicates neighbor analysis.
- Why unresolved: The paper identifies the challenge but does not explore potential solutions or modifications to anchor attacks to incorporate elastic-net penalty.
- What evidence would resolve it: Modifications to anchor attacks that successfully integrate elastic-net penalty and demonstrate improved performance or stealthiness.

### Open Question 3
- Question: How does the number of poisoning samples required for successful attacks scale with the complexity of the fair representation learning model?
- Basis in paper: [explicit] The paper provides a theoretical analysis of the minimal number of poisoning samples needed for gradient-matching based attacks but does not empirically validate this across different model complexities.
- Why unresolved: The theoretical bound is derived under specific assumptions and may not hold for all model architectures or complexities.
- What evidence would resolve it: Empirical studies varying model complexity and measuring the number of poisoning samples needed for successful attacks.

### Open Question 4
- Question: What are the implications of perturbing pre-processed data versus raw data in terms of attack effectiveness and detectability?
- Basis in paper: [explicit] The paper argues that perturbing pre-processed data is practical due to data anonymization practices but does not compare the effectiveness or detectability of attacks on pre-processed versus raw data.
- Why unresolved: The paper assumes pre-processed data is the target but does not empirically compare this approach to attacking raw data.
- What evidence would resolve it: Comparative experiments on the effectiveness and detectability of attacks on pre-processed versus raw data.

### Open Question 5
- Question: How robust are the robust features identified by the elastic-net penalty to variations in the attack parameters?
- Basis in paper: [explicit] The paper identifies robust features for certain models but does not explore how these features vary with different attack parameters or across different models.
- Why unresolved: The paper provides a case study but lacks a comprehensive analysis of feature robustness across varying conditions.
- What evidence would resolve it: Experiments varying attack parameters and models to assess the consistency of identified robust features.

## Limitations

- The attack effectiveness depends on distributional assumptions (Gaussian distributions, linear separability) that may not hold in practice
- The attack targets specific FRL architectures and fairness metrics, with unclear generalizability to other fairness paradigms
- Limited analysis of poisoning sample detectability through standard anomaly detection methods or human inspection

## Confidence

- High confidence: The core bilevel optimization framework and gradient matching approach are well-established techniques from prior work
- Medium confidence: The theoretical connection between FLD and MI relies on distributional assumptions that may not hold in practice
- Low confidence: The stealthiness guarantees are primarily empirical rather than theoretical

## Next Checks

1. **Distributional assumption validation:** Empirically test whether sensitive groups in the learned representation space follow approximately Gaussian distributions and are linearly separable to validate the theoretical foundation of using FLD as an MI proxy.

2. **Cross-architecture robustness:** Apply the attack to FRL models using different architectural paradigms (e.g., contrastive representation learning, causal fairness approaches) to assess generalizability beyond the specific models tested.

3. **Detection feasibility analysis:** Evaluate the poisoning samples using standard anomaly detection methods and human-in-the-loop studies to assess whether the elastic-net penalty provides meaningful stealth guarantees in practice.