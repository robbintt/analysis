---
ver: rpa2
title: 'Farzi Data: Autoregressive Data Distillation'
arxiv_id: '2310.09983'
source_url: https://arxiv.org/abs/2310.09983
tags:
- data
- farzi
- learning
- dsyn
- adam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a data distillation approach for autoregressive
  machine learning tasks. It addresses the problem of high costs associated with training
  large autoregressive models on massive amounts of data by summarizing the data into
  a small set of synthetic sequences.
---

# Farzi Data: Autoregressive Data Distillation

## Quick Facts
- arXiv ID: 2310.09983
- Source URL: https://arxiv.org/abs/2310.09983
- Reference count: 40
- Primary result: FARZI achieves 98-120% of downstream full-data performance using only 0.1% of original dataset size

## Executive Summary
This paper introduces FARZI, a data distillation approach for autoregressive machine learning tasks that addresses the high computational costs of training large models on massive datasets. The method factorizes high-dimensional discrete event spaces into a latent space, then performs data distillation using reverse-mode differentiation of the Adam optimizer combined with implicit regularization. FARZI demonstrates that synthetic data summaries as small as 0.1% of the original dataset can achieve comparable or superior performance to training on full datasets.

## Method Summary
FARZI performs data distillation by factorizing discrete token sequences into continuous latent representations that are easier to optimize. The approach uses a bilevel optimization framework where an inner loop trains a student model using Adam optimization, while an outer loop updates the latent data summary and token decoder matrix using gradients computed through efficient reverse-mode differentiation. The latent factorization implicitly provides regularization, and the method leverages Hessian-vector products to avoid the memory overhead of standard reverse-mode differentiation. FARZI generates synthetic sequences from the optimized latent summary and decoder, which are then used to train downstream autoregressive models.

## Key Results
- Achieves 98-120% of downstream full-data performance using synthetic data of size 0.1% of original dataset
- FARZI data outperforms random sampling, head sampling, and other distillation baselines across multiple autoregressive tasks
- FARZI improves performance on cold-start users/items compared to full-data training in sequential recommendation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FARZI achieves data distillation by factorizing high-dimensional discrete event space into a latent space and performing optimization there
- Mechanism: The factorization converts discrete token sequences into continuous latent representations that are easier to optimize, while a decoder matrix maps back to the token space
- Core assumption: The latent space captures sufficient information about the downstream task patterns while being more optimization-friendly than the original discrete space
- Evidence anchors: [abstract] "factorizing the high-dimensional discrete event space into a latent space"; [section] "FARZI factorizes Dsyn into: (i) a latent data summary and (ii) a token-decoder matrix"

### Mechanism 2
- Claim: Using Adam instead of SGD in the inner loop significantly improves data distillation performance
- Mechanism: Adam's adaptive learning rates and momentum help navigate the complex optimization landscape of data distillation more effectively than SGD
- Core assumption: The non-stationary and non-convex nature of data distillation benefits from Adam's adaptive optimization properties
- Evidence anchors: [section] "we empirically observe that in our setting of autoregressive DD, Adam optimization... is crucial for downstream DD performance"; [section] "Adam is much better suited for DD in our setting"

### Mechanism 3
- Claim: The latent parameterization implicitly promotes regularization and improves generalization
- Mechanism: The low-rank constraint imposed by the latent factorization acts as an implicit regularizer that prevents overfitting to the training data
- Core assumption: The rank constraint of the latent factorization creates a simpler hypothesis space that generalizes better
- Evidence anchors: [section] "FARZI DATA's latent parameterization implicitly promotes regularization while training downstream models"; [section] "explicit rank regularization while synthesizing data summaries (e.g., latent factorization) strictly promotes generalization"

## Foundational Learning

- Concept: Bilevel optimization
  - Why needed here: FARZI optimizes both the data summary (inner loop) and the model parameters (outer loop) simultaneously
  - Quick check question: In the FARZI optimization, what are the two levels being optimized?

- Concept: Reverse-mode differentiation
  - Why needed here: To compute gradients through the inner-loop optimization steps efficiently without storing all intermediate variables
  - Quick check question: What is the memory complexity of computing meta-gradients with standard autograd vs FARZI's approach?

- Concept: Hessian-vector products
  - Why needed here: Used in the efficient reverse-mode differentiation of Adam to avoid explicitly computing the Hessian matrix
  - Quick check question: How does the efficient Adam reverse-mode implementation avoid the O(T) memory cost?

## Architecture Onboarding

- Component map: Latent data summary -> Token decoder matrix -> Materialized FARZI data -> Student model training -> Meta-gradient computation -> Updated latent summary and decoder

- Critical path:
  1. Initialize latent data summary and decoder matrix
  2. For each outer loop iteration:
     - Materialize FARZI data from latent summary and decoder
     - Train student model for T steps using Adam
     - Compute meta-gradient using efficient reverse-mode Adam
     - Update latent summary and decoder

- Design tradeoffs:
  - Latent dimension d vs reconstruction quality
  - Number of inner-loop steps T vs memory/computation cost
  - Temperature τ in softmax vs entropy of synthesized data

- Failure signatures:
  - Poor downstream performance → likely issues with latent space capacity or optimization instability
  - Memory overflow → too many inner-loop steps without efficient reverse-mode implementation
  - Slow convergence → learning rates too small or insufficient inner-loop steps

- First 3 experiments:
  1. Verify that FARZI data can train a simple model (e.g., small Transformer) on PTB with 1% data size
  2. Test different latent dimensions d on ML-100k to find the sweet spot between compression and quality
  3. Compare Adam vs SGD in inner loop for the same FARZI configuration on Amazon Magazine dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of latent dimensions (d) for different autoregressive tasks and datasets?
- Basis in paper: [explicit] The paper mentions that FARZI factorizes the data into a latent summary and a token decoder, and provides theoretical analysis showing benefits of latent parameterization for implicit regularization. However, it does not empirically explore the impact of varying the latent dimension size.
- Why unresolved: The paper only uses a fixed latent dimension (e.g., 8 for sequential recommendation, 16 for language modeling) without exploring how performance varies with different choices.
- What evidence would resolve it: Empirical results showing how downstream model performance changes as the latent dimension is varied across different datasets and tasks.

### Open Question 2
- Question: How does FARZI perform when scaling to extremely large models like T5 or larger datasets like C4?
- Basis in paper: [explicit] The paper mentions that scaling to larger models and datasets isn't trivial due to computational constraints, but this is important for practical applications.
- Why unresolved: The experiments only use relatively small models (SASRec, small Transformer) and datasets (PTB, MovieLens, etc.).
- What evidence would resolve it: Experimental results demonstrating FARZI's effectiveness on state-of-the-art large language models and massive web-scale datasets.

### Open Question 3
- Question: What is the impact of FARZI on long-tail items/users compared to other data sampling methods?
- Basis in paper: [explicit] The paper shows that FARZI performs better than random and head sampling, and provides analysis of cold-start performance, but doesn't directly compare cold-start performance against other data distillation methods.
- Why unresolved: The cold-start analysis is only done for FARZI vs. full data, not against other data distillation approaches.
- What evidence would resolve it: Comparative analysis of FARZI vs. other data distillation methods specifically on cold-start users/items.

## Limitations
- The optimal latent dimension size is not systematically explored across different tasks and datasets
- Claims about Adam superiority lack theoretical justification for why it should perform better in this specific bilevel optimization setting
- Limited analysis of failure cases or breaking points of the approach

## Confidence
- High confidence: The overall framework of using latent space factorization for data distillation is technically sound and the experimental methodology is rigorous
- Medium confidence: The specific claims about Adam's superiority over SGD in the inner loop and the implicit regularization effects of the latent factorization
- Low confidence: The scalability claims to extremely large datasets and the assertion that this approach generalizes to all autoregressive tasks without modification

## Next Checks
1. **Latent Dimension Sensitivity Analysis**: Systematically vary the latent dimension d across multiple orders of magnitude (e.g., 10, 50, 100, 500, 1000) on the same datasets to empirically verify the claimed regularization effects and identify the optimal tradeoff between compression and performance.

2. **Alternative Optimization Methods**: Replace Adam with SGD and other optimizers (e.g., Adagrad, RMSprop) in the inner loop while keeping all other parameters constant to quantify the exact contribution of Adam to the reported performance gains.

3. **Failure Case Investigation**: Intentionally stress-test FARZI on pathological datasets (e.g., highly imbalanced distributions, very long sequences, or extremely large vocabularies) to identify breaking points and characterize the method's limitations.