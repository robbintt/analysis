---
ver: rpa2
title: Convergence Analysis for Learning Orthonormal Deep Linear Neural Networks
arxiv_id: '2311.14658'
source_url: https://arxiv.org/abs/2311.14658
tags:
- networks
- neural
- convergence
- linear
- orthonormal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides the first convergence analysis for training
  orthonormal deep linear neural networks (ODLNNs) using Riemannian gradient descent
  (RGD). Unlike prior work that imposes orthonormal constraints on all weight matrices,
  this paper relaxes the constraint for one layer, which is crucial for establishing
  convergence guarantees.
---

# Convergence Analysis for Learning Orthonormal Deep Linear Neural Networks

## Quick Facts
- arXiv ID: 2311.14658
- Source URL: https://arxiv.org/abs/2311.14658
- Authors: 
- Reference count: 36
- One-line primary result: First convergence analysis for orthonormal deep linear neural networks using Riemannian gradient descent, showing linear convergence with polynomial decay in network depth

## Executive Summary
This paper provides the first convergence analysis for training orthonormal deep linear neural networks (ODLNNs) using Riemannian gradient descent (RGD). Unlike prior work that enforces orthonormal constraints on all weight matrices, this paper relaxes the constraint for one layer (W1), which is crucial for establishing convergence guarantees. Under a class of loss functions satisfying the restricted correlated gradient condition, RGD with appropriate initialization converges linearly to the global optimum, with convergence rate decreasing polynomially with the number of hidden layers. Experiments on MNIST classification validate the theoretical findings, demonstrating faster convergence for RGD compared to standard gradient descent.

## Method Summary
The method employs Riemannian gradient descent on the Stiefel manifold to train deep linear networks with orthonormal weight matrices. The key innovation is excluding orthonormal constraints on one layer (W1) to achieve energy balance between input and output. The algorithm uses polar decomposition-based retraction to maintain orthonormality of weight matrices during updates. Convergence is established under the restricted correlated gradient (RCG) condition, a generalization of strong convexity that enables linear convergence analysis for non-convex problems. Layer-specific learning rates are used, with one parameter controlling the ratio between updates to W1 and other layers.

## Key Results
- RGD converges linearly to global optimum for ODLNNs under RCG condition
- Convergence rate decreases polynomially with number of hidden layers N
- Excluding orthonormal constraint on W1 is crucial for establishing convergence guarantees
- RGD outperforms standard gradient descent in MNIST experiments, especially for deeper networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Excluding orthonormal constraints on W1 enables energy balance between input and output, allowing linear convergence
- Mechanism: By allowing W1 to have arbitrary norms, the energy of the input (scaled by W1) can match the energy of the output, preventing destabilizing energy imbalances
- Core assumption: Data model assumes whitened input X (XX⊤ = Idx) and teacher model with orthonormal W⋆i (i≥2)
- Evidence anchors: [abstract] excluding orthonormal requirement for one layer is crucial; [section] adding no constraint on W1 is necessary for energy balance
- Break condition: If input is not whitened or all layers are forced to be orthonormal, energy imbalance reappears

### Mechanism 2
- Claim: RGD on Stiefel manifold converges linearly under restricted correlated gradient condition
- Mechanism: RCG condition ensures inner product between gradients and parameter differences is lower bounded by linear term in parameter distance plus quadratic term in gradient norm
- Core assumption: Loss function satisfies RCG(α, β, C) for some α, β > 0 and initialization within local region
- Evidence anchors: [abstract] convergence under loss functions satisfying RCG; [section] Definition 2 of RCG condition
- Break condition: If loss function doesn't satisfy RCG in local region or initialization is outside region

### Mechanism 3
- Claim: Convergence rate decreases polynomially with number of layers N
- Mechanism: Contraction factor (1 - ασ²min(Y⋆)µ / (8(2N - 1))) depends inversely on N, causing polynomial decay
- Core assumption: Riemannian regularity condition holds and learning rate is appropriately chosen
- Evidence anchors: [abstract] convergence rate decreases polynomially with number of hidden layers; [section] Theorem 1 showing contraction factor
- Break condition: If number of layers is very large, contraction factor approaches 1, making convergence impractically slow

## Foundational Learning

- Concept: Stiefel manifold and Riemannian optimization
  - Why needed here: Weight matrices (except W1) are constrained to be orthonormal, defining a Riemannian manifold requiring specialized optimization
  - Quick check question: What is the tangent space of the Stiefel manifold St(m, n) at a point C, and how is the projection of a gradient onto this tangent space computed?

- Concept: Restricted correlated gradient (RCG) condition
  - Why needed here: RCG condition generalizes strong convexity for local regions, enabling linear convergence analysis for non-convex deep linear networks
  - Quick check question: How does the RCG condition differ from standard strong convexity, and why is it suitable for analyzing convergence in deep linear networks?

- Concept: Polar decomposition-based retraction
  - Why needed here: After gradient steps in tangent space, retraction projects updated point back onto Stiefel manifold to maintain orthonormality
  - Quick check question: How does the polar decomposition-based retraction RetrC(bC) = bC(bC⊤bC)^(-1/2) ensure the updated matrix remains on the Stiefel manifold?

## Architecture Onboarding

- Component map: X (whitened input) -> W1, W2, ..., WN (orthonormal except W1) -> Y (output) -> Loss function (satisfying RCG) -> Riemannian gradient descent with polar retraction

- Critical path:
  1. Initialize W1 with uniform distribution and Wi (i≥2) orthogonally
  2. Compute Riemannian gradients on Stiefel manifold
  3. Update weights using RGD with appropriate learning rates
  4. Apply polar retraction to maintain orthonormality
  5. Monitor convergence via dist2(W, W⋆) or ∥Y - Y⋆∥F

- Design tradeoffs:
  - Excluding orthonormality on W1 enables convergence but may reduce robustness
  - Using RGD ensures orthonormality but is more complex than standard GD
  - Choosing loss functions satisfying RCG limits the class but enables analysis

- Failure signatures:
  - Divergence or slow convergence: Check if initialization is within local region, if learning rates are appropriate, or if loss function satisfies RCG
  - Non-orthonormal weights: Check retraction operation or projection onto Stiefel manifold
  - Energy imbalance: Check if input is whitened or if all layers are forced to be orthonormal

- First 3 experiments:
  1. Train 4-layer orthonormal deep linear network on MNIST using RGD and compare convergence to standard GD
  2. Vary number of layers (4, 10, 20) and measure how convergence rate changes, verifying polynomial decay
  3. Test different loss functions (MSE, cross-entropy) to confirm they satisfy RCG condition and observe impact on convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate of RGD for ODLNNs scale with number of hidden layers N, and what is the precise polynomial relationship?
- Basis in paper: [explicit] Paper states convergence rate decreases polynomially with N but doesn't provide exact polynomial relationship
- Why unresolved: Paper provides general statement about polynomial decay without deriving specific exponent or coefficient
- What evidence would resolve it: Detailed mathematical derivation showing exact polynomial relationship between convergence rate and N, including exponent and coefficient

### Open Question 2
- Question: How does convergence rate of RGD for ODLNNs compare to standard GD when using nonlinear activation functions like ReLU, beyond MNIST dataset?
- Basis in paper: [explicit] Paper shows MNIST results comparing RGD and GD with linear and ReLU activations but lacks theoretical analysis for nonlinear activations
- Why unresolved: Paper focuses on linear networks theoretically but only provides empirical results for ReLU without theoretical justification
- What evidence would resolve it: Theoretical extension of convergence analysis to nonlinear networks, or additional empirical studies on various datasets and architectures

### Open Question 3
- Question: What initialization strategies beyond orthogonal initialization are effective for ODLNNs trained with RGD?
- Basis in paper: [inferred] Paper mentions initialization methods aren't focus and refers to prior research, but doesn't explore alternative strategies for RGD in ODLNNs
- Why unresolved: Paper acknowledges importance of initialization but doesn't investigate or compare different initialization methods for ODLNNs trained with RGD
- What evidence would resolve it: Comprehensive study comparing various initialization strategies for ODLNNs trained with RGD, including impact on convergence speed and final performance

## Limitations
- Analysis relies on specific data model with whitened input and teacher model with particular weight matrix structures
- Restricted correlated gradient condition requires careful verification for specific loss functions
- Polynomial decay in convergence rate suggests method may become impractical for very deep networks

## Confidence
- Theoretical framework and convergence guarantees under stated assumptions: High
- Experimental validation, particularly MNIST classification results: Medium
- Broader applicability to non-linear networks and other architectures: Low

## Next Checks
1. Test convergence behavior on more diverse datasets beyond MNIST, including those with non-whitened inputs, to verify robustness of energy balance mechanism
2. Implement and verify restricted correlated gradient condition for various loss functions commonly used in deep learning to establish practical applicability
3. Extend experimental validation to deeper networks (N > 20) to empirically verify polynomial decay in convergence rate and identify potential practical limitations