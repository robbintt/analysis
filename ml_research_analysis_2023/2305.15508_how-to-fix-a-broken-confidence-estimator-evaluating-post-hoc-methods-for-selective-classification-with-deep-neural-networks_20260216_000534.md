---
ver: rpa2
title: 'How to Fix a Broken Confidence Estimator: Evaluating Post-hoc Methods for
  Selective Classification with Deep Neural Networks'
arxiv_id: '2305.15508'
source_url: https://arxiv.org/abs/2305.15508
tags:
- selective
- confidence
- p-normsoftmax
- auroc
- aurc
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes p-NormSoftmax, a simple post-hoc method to improve
  selective classification of deep neural networks by transforming logits via centralization,
  p-norm normalization, and temperature scaling before taking the maximum softmax
  probability as confidence. Evaluated on 84 ImageNet classifiers, it yields an average
  16% improvement in AURC, completely fixing pathological low selective classification
  performance in some models.
---

# How to Fix a Broken Confidence Estimator: Evaluating Post-hoc Methods for Selective Classification with Deep Neural Networks

## Quick Facts
- arXiv ID: 2305.15508
- Source URL: https://arxiv.org/abs/2305.15508
- Reference count: 40
- One-line primary result: p-NormSoftmax improves selective classification performance by 16% on average, making it primarily determined by accuracy rather than confidence estimator quality

## Executive Summary
This paper addresses the challenge of improving selective classification performance for deep neural networks by proposing p-NormSoftmax, a simple post-hoc method that transforms model logits through centralization, p-norm normalization, and temperature scaling before computing confidence as the maximum softmax probability. Evaluated across 84 ImageNet classifiers, the method yields an average 16% improvement in AURC, completely fixing pathological low selective classification performance in some models. The key insight is that post-hoc logit normalization and temperature scaling can make selective classification performance almost entirely determined by accuracy at full coverage, rather than the quality of the confidence estimator.

## Method Summary
The p-NormSoftmax method applies three transformations to model logits: centralization (subtracting the mean logit), p-norm normalization (dividing by the p-norm of the centralized logits), and temperature scaling (multiplying by a learned temperature parameter). The maximum softmax probability from these transformed logits serves as the confidence estimate. The parameters p (p-norm) and β (temperature) are optimized on a hold-out set to minimize AURC or maximize AUROC, typically using grid search. This approach addresses overconfidence issues in deep neural networks by normalizing the scale of logits relative to their distribution, making confidence estimates more discriminative between correct and incorrect predictions.

## Key Results
- Average 16% improvement in AURC across 84 ImageNet classifiers
- Completely fixes pathological low selective classification performance in some models
- Makes selective classification performance primarily determined by accuracy rather than confidence estimator quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Logit centralization and p-norm normalization reduces overconfident predictions by constraining the scale of logits relative to their distribution.
- Mechanism: By subtracting the mean logit (centralization) and dividing by the p-norm, the method normalizes the logit vector magnitude, reducing the influence of large outlier logits that lead to overconfident softmax outputs. This makes confidence estimates more discriminative between correct and incorrect predictions.
- Core assumption: Overconfidence arises when logits are disproportionately large relative to their mean and distribution, and scaling them down based on p-norm preserves relative ordering while reducing absolute confidence.
- Evidence anchors:
  - [abstract] "transforming the logits through p-norm normalization and temperature scaling, followed by taking the MSP"
  - [section 4.3] "we propose to use T(z) = ∥z − μ(z)∥p/β, so that high-norm inputs are penalized reducing the confidence of the corresponding predictions"

### Mechanism 2
- Claim: Optimizing p-norm normalization and temperature scaling directly for selective classification metrics (AURC/AUROC) improves selective classification performance more than optimizing for calibration alone.
- Mechanism: By tuning p and temperature on a hold-out set to minimize AURC or maximize AUROC, the method adapts the confidence estimator to the specific goal of distinguishing correct from incorrect predictions, rather than just matching predicted probabilities to empirical frequencies.
- Core assumption: Selective classification benefits from confidence estimates optimized for ranking correctness, not just calibration.
- Evidence anchors:
  - [abstract] "p and the temperature are optimized based on a hold-out set"
  - [section 4.2] "optimizing selective classification metrics directly... Since a single parameter needs to be tuned, this can easily be done via grid search"

### Mechanism 3
- Claim: p-NormSoftmax makes selective classification performance nearly independent of model-specific confidence estimator quality, making it primarily determined by accuracy.
- Mechanism: After applying p-NormSoftmax, all models exhibit similar AUROC ranges (~0.84-0.88), indicating that differences in selective classification performance are largely due to accuracy differences rather than confidence estimator effectiveness.
- Core assumption: Once confidence estimators are optimized post-hoc, model accuracy becomes the dominant factor in selective classification performance.
- Evidence anchors:
  - [abstract] "after applying p-NormSoftmax, we observe that these models exhibit approximately the same level of misclassification detection performance, implying that a model’s selective classification performance is almost entirely determined by its accuracy"
  - [section 5.2] "the Spearman’s correlation between the AURC and the accuracy goes from 0.9169 to 0.9992"

## Foundational Learning

- Concept: Selective classification (learning with reject option)
  - Why needed here: The paper's goal is to improve selective classification performance by better estimating confidence to decide when to abstain from predictions.
  - Quick check question: What is the trade-off between coverage and selective risk in selective classification?

- Concept: Softmax function and logits
  - Why needed here: The method transforms logits before applying softmax to obtain confidence estimates.
  - Quick check question: How does the softmax function convert logits to probabilities?

- Concept: Temperature scaling
  - Why needed here: Temperature scaling is a component of the proposed method and a baseline for comparison.
  - Quick check question: How does temperature scaling affect the sharpness of the softmax distribution?

## Architecture Onboarding

- Component map: Input logits -> Centralization -> p-norm normalization -> Temperature scaling -> Maximum softmax probability
- Critical path:
  1. Compute logits from pre-trained model
  2. Apply centralization: z' = z - μ(z)
  3. Apply p-norm normalization: z'' = z' / ∥z'∥p
  4. Apply temperature scaling: z''' = β * z''
  5. Compute MSP as confidence estimate
  6. Optimize p (and β) on hold-out set
- Design tradeoffs:
  - Centralization vs. no centralization: Centralization helps when logits have non-zero mean but may slightly degrade performance for already-centered logits.
  - p-norm choice: Different p values affect how much outlier logits are penalized; p=2 is common but other values may be better depending on the model.
  - Temperature scaling vs. no scaling: Scaling can help but must be optimized for selective classification, not just calibration.
- Failure signatures:
  - AURC/AUROC does not improve after applying p-NormSoftmax
  - Confidence estimates become too conservative (low coverage at acceptable risk levels)
  - Performance degrades on distribution shift if p and β are optimized only on in-distribution data
- First 3 experiments:
  1. Apply p-NormSoftmax with p=2 and default β heuristic to a pre-trained ResNet on ImageNet validation set; compare AURC to baseline MSP.
  2. Sweep p in {2,3,4,5,6} for the same model and choose p that minimizes AURC.
  3. Compare AURC/AUROC for models with high vs. low average logit norms to verify the claim about when p-NormSoftmax is most beneficial.

## Open Questions the Paper Calls Out

- Does the p-NormSoftmax method generalize beyond image classification tasks to other domains like NLP or speech recognition?
- What is the theoretical relationship between the p-norm parameter and the calibration-uncertainty trade-off in selective classification?
- How does p-NormSoftmax perform on out-of-distribution detection compared to specialized OOD methods?

## Limitations
- The method relies on having a representative hold-out set for optimization, which may not capture distribution shift scenarios
- While p-NormSoftmax improves selective classification performance, it does not address fundamental accuracy limitations of the underlying models
- The method assumes logits are available from pre-trained models, limiting applicability to models where logits cannot be easily extracted

## Confidence
- High confidence: The empirical results showing average 16% AURC improvement across 84 models are well-supported by the experiments described.
- Medium confidence: The claim that p-NormSoftmax "completely fixes" pathological selective classification performance is supported for the evaluated models but may not generalize to all architectures or distributions.
- Medium confidence: The mechanism explanation that centralization and p-norm normalization reduce overconfidence is plausible but not exhaustively validated across different types of model pathologies.

## Next Checks
1. Test p-NormSoftmax on out-of-distribution datasets to verify performance degradation is acceptable and whether additional domain adaptation is needed.
2. Systematically evaluate p-NormSoftmax across different model families (CNNs, transformers, MLPs) to identify architectures where it may be less effective.
3. Quantify the runtime overhead of the additional logit transformations and grid search optimization, especially for large-scale deployment scenarios.