---
ver: rpa2
title: 'A criterion for Artificial General Intelligence: hypothetic-deductive reasoning,
  tested on ChatGPT'
arxiv_id: '2308.02950'
source_url: https://arxiv.org/abs/2308.02950
tags:
- reasoning
- occur
- would
- chatgpt
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a test for assessing an AI's capacity for
  hypothetic-deductive reasoning, a key cognitive skill for general problem-solving.
  The test involves asking the AI to identify causes in abstract causal situations
  represented by neuron diagrams, and to list the hypotheses used in solving physics
  problems.
---

# A criterion for Artificial General Intelligence: hypothetic-deductive reasoning, tested on ChatGPT

## Quick Facts
- arXiv ID: 2308.02950
- Source URL: https://arxiv.org/abs/2308.02950
- Reference count: 6
- Key outcome: ChatGPT has limited capacity for hypothetic-deductive reasoning, especially in sophisticated contexts

## Executive Summary
This paper proposes a test for assessing AI's capacity for hypothetic-deductive reasoning, a key cognitive skill for general problem-solving. The test involves identifying causes in abstract causal situations (represented by neuron diagrams) and listing hypotheses used in solving physics problems. When applied to ChatGPT versions 3 and 4, the results show that while the model can answer some questions correctly, it struggles with more complex problems and often provides irrelevant or incorrect hypotheses. The findings suggest that current AI systems, including ChatGPT, have limited hypothetic-deductive reasoning capabilities, particularly when problems require integrating knowledge from multiple domains.

## Method Summary
The study tests ChatGPT's hypothetic-deductive reasoning using two main approaches: (1) causal reasoning with neuron diagrams where the model must identify causes of neuron firing/non-firing, and (2) physics problems where the model must list all hypotheses and laws of nature used to solve problems. The neuron diagrams test requires counterfactual reasoning to determine root causes, while the physics problems test both the correctness of answers and the relevance of listed hypotheses. The study uses versions 3 and 4 of ChatGPT, evaluating performance on 26 causal diagrams and 10 physics questions of varying complexity.

## Key Results
- ChatGPT(4) correctly identified causes in only 13 of 26 submitted neuron diagrams
- ChatGPT(4) fully answered 4 out of 10 physics questions correctly, with correct hypothesis listing
- The model struggled significantly with questions requiring integration of multiple knowledge domains (physics, biology, social psychology)
- While ChatGPT(4) could trigger hypothesis listing via Q-HYP prompts, the relevance of listed hypotheses varied considerably and often included incorrect or irrelevant items

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Abstract causal reasoning in AI depends on correctly identifying root causes in neuron diagrams.
- Mechanism: The counterfactual definition (DEF1) requires tracing indirect paths and maximal blocking to isolate root causes, which ChatGPT(4) can perform only partially.
- Core assumption: Humans use a similar counterfactual method, though more implicitly, and can identify all root causes effortlessly.
- Evidence anchors:
  - [abstract] "The study introduces a test for assessing an AI's capacity for hypothetic-deductive reasoning... The test involves asking the AI to identify causes in abstract causal situations represented by neuron diagrams"
  - [section] "ChatGPT(4) only correctly indicated the causes for 13 of the 26 submitted diagrams"
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.512, average citations=0.0" (weak external validation)
- Break condition: When diagrams require multiple indirect paths or complex blocking patterns that exceed the model's causal reasoning depth.

### Mechanism 2
- Claim: Hypothesetic-deductive reasoning in AI can be probed by asking the model to list the hypotheses used to solve a problem.
- Mechanism: Prompting with "Can you, in order to give this answer, reason step by step and list all the hypotheses and laws of nature you use to come to your answer?" triggers a reasoning trace that reveals underlying logical structure.
- Core assumption: Listing hypotheses is a necessary condition for demonstrating understanding of hypothetic-deductive reasoning.
- Evidence anchors:
  - [abstract] "We propose simple tests for both types of reasoning, and apply them to ChatGPT"
  - [section] "With the Q-HYP prompt we could always trigger ChatGPT to provide a list of hypotheses that were at least logically related to the answer it gave"
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.512" (no direct support for this specific prompting technique)
- Break condition: When the model's answers are correct but the listed hypotheses are irrelevant or incorrect, indicating simulated rather than genuine reasoning.

### Mechanism 3
- Claim: Combining knowledge from multiple domains increases the difficulty of hypothetic-deductive reasoning for AI.
- Mechanism: Questions requiring integration of physics, biology, and social psychology (Q4, Q6-Q10) expose limitations in cross-domain reasoning.
- Core assumption: Humans can seamlessly integrate knowledge from different fields, while current LLMs struggle with such integration.
- Evidence anchors:
  - [section] "We tried to ask questions that had, a priori, a relatively low likelihood of being discussed in texts... while still being easily answerable by humans"
  - [section] "ChatGPT(4) was capable of answering questions Q1, Q4, Q5 and Q7 fully (or almost fully) correctly, even if these answers are based on at least four to six hypotheses"
  - [corpus] "Found 25 related papers... Average neighbor FMR=0.512" (no direct support for multi-domain integration difficulty)
- Break condition: When the model provides plausible but incorrect answers to multi-domain questions, revealing shallow integration rather than deep understanding.

## Foundational Learning

- Concept: Counterfactual reasoning
  - Why needed here: The core test for causal reasoning relies on counterfactual definitions to identify causes
  - Quick check question: In a neuron diagram where C blocks B from reaching E, what is the counterfactual scenario used to determine if C is a cause of E?

- Concept: Theory-based problem solving
  - Why needed here: The paper's central claim is that hypothetic-deductive reasoning follows a theory-based structure similar to scientific problem solving
  - Quick check question: How does the hypothetic-deductive model differ from simple pattern matching when solving a physics problem?

- Concept: Cross-domain knowledge integration
  - Why needed here: The reasoning test includes questions requiring physics, biology, and social psychology knowledge
  - Quick check question: What makes combining physics and social psychology knowledge more challenging for AI than single-domain reasoning?

## Architecture Onboarding

- Component map: Neuron diagram parser -> Counterfactual reasoning engine -> Hypothesis extraction module -> Multi-domain knowledge graph -> Explanation generation system
- Critical path: Input transcription -> Causal analysis -> Hypothesis listing -> Answer validation
- Design tradeoffs: Between model complexity and reasoning depth; between prompt engineering and architectural changes
- Failure signatures: Correct answers with incorrect or irrelevant hypotheses; failure to identify root causes in complex diagrams; inability to integrate knowledge across domains
- First 3 experiments:
  1. Test the counterfactual reasoning engine on simple neuron diagrams with clear blocking patterns
  2. Implement the Q-HYP prompt on single-domain physics problems and evaluate hypothesis quality
  3. Create multi-domain integration tests combining physics and social psychology scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific computational architectures or enhancements would be needed for an AI to achieve genuine hypothetic-deductive reasoning capabilities?
- Basis in paper: [explicit] The paper argues that explicit hypothetic-deductive reasoning is a necessary condition for AGI and that current LLMs like ChatGPT have limited capacity for this type of reasoning.
- Why unresolved: The paper identifies the need for explicit hypothetic-deductive reasoning but does not specify what computational architectures or enhancements would be required to achieve this capability.
- What evidence would resolve it: A detailed architectural proposal showing how to implement explicit hypothetic-deductive reasoning in AI systems, along with empirical validation of such an approach.

### Open Question 2
- Question: How can we develop more rigorous and comprehensive benchmarks for testing an AI's capacity for hypothetic-deductive reasoning across different domains?
- Basis in paper: [explicit] The paper proposes simple tests for causal reasoning and hypothetic-deductive reasoning but acknowledges the need for more comprehensive benchmarks.
- Why unresolved: While the paper provides initial tests, it does not offer a complete framework for benchmarking hypothetic-deductive reasoning across various domains and complexity levels.
- What evidence would resolve it: A comprehensive benchmark suite specifically designed to test hypothetic-deductive reasoning capabilities across multiple domains, with clear evaluation criteria and results from various AI systems.

### Open Question 3
- Question: To what extent does implicit reasoning in LLMs like ChatGPT, which produces correct answers without explicit hypothesis listing, represent a form of genuine understanding or merely statistical pattern matching?
- Basis in paper: [explicit] The paper discusses ChatGPT's ability to produce correct answers but questions whether this represents genuine understanding or just statistical pattern matching.
- Why unresolved: The paper acknowledges the impressive performance of LLMs but does not definitively resolve whether their implicit reasoning constitutes true understanding or merely sophisticated statistical correlation.
- What evidence would resolve it: A rigorous comparison between human and AI reasoning processes, including neuroscientific studies and cognitive psychology experiments, to determine the fundamental differences (if any) between implicit AI reasoning and human understanding.

## Limitations

- Testing scope is limited to ChatGPT versions 3 and 4 without comparison to other LLMs or baseline models
- Causal reasoning tests rely on abstract neuron diagrams that may not capture full complexity of real-world causal reasoning
- Hypothesis-listing technique depends heavily on prompt engineering rather than systematic evaluation of reasoning processes
- The paper acknowledges that current LLMs can simulate hypothetic-deductive reasoning through "commonsense reasoning" and large knowledge bases

## Confidence

- High confidence: ChatGPT(4) can correctly answer some abstract causal reasoning questions and physics problems involving 4-6 hypotheses
- Medium confidence: ChatGPT's reasoning struggles increase significantly with multi-domain integration and complex counterfactual scenarios
- Medium confidence: The Q-HYP prompt reliably triggers hypothesis listing, though relevance varies considerably

## Next Checks

1. **Cross-model comparison**: Test the same neuron diagrams and physics problems across multiple LLMs (Claude, Gemini, Llama) to determine if ChatGPT's limitations are model-specific or general to current architectures.

2. **Controlled prompt variation**: Systematically vary the Q-HYP prompt structure (adding constraints, changing wording, providing examples) to identify whether prompt engineering can improve hypothesis relevance and reasoning depth.

3. **Human benchmark validation**: Have human subjects complete the same tests (neuron diagrams and physics problems) and compare their performance, hypothesis quality, and reasoning patterns to establish a baseline for what constitutes genuine hypothetic-deductive reasoning.