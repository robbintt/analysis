---
ver: rpa2
title: Projected Off-Policy Q-Learning (POP-QL) for Stabilizing Offline Reinforcement
  Learning
arxiv_id: '2311.14885'
source_url: https://arxiv.org/abs/2311.14885
tags:
- policy
- distribution
- pop-ql
- off-policy
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Projected Off-Policy Q-Learning (POP-QL),
  a novel actor-critic algorithm that addresses distribution shift in offline reinforcement
  learning. POP-QL reweights off-policy samples and constrains the policy to prevent
  divergence and reduce value-approximation error, leveraging a contraction mapping
  condition.
---

# Projected Off-Policy Q-Learning (POP-QL) for Stabilizing Offline Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2311.14885
- **Source URL**: https://arxiv.org/abs/2311.14885
- **Reference count**: 38
- **Primary result**: POP-QL uses contraction mapping conditions to stabilize offline RL, achieving competitive performance on standard benchmarks

## Executive Summary
This paper introduces Projected Off-Policy Q-Learning (POP-QL), a novel actor-critic algorithm that addresses distribution shift in offline reinforcement learning. POP-QL reweights off-policy samples and constrains the policy to prevent divergence and reduce value-approximation error, leveraging a contraction mapping condition. The method improves upon previous work by using a more computationally efficient projection and extending it to the MDP setting. Experiments on standard benchmarks and tasks with sub-optimal data-collection policies show that POP-QL is competitive with existing methods and outperforms them in certain scenarios. The key contribution is demonstrating the potential of contraction mapping conditions for stable offline RL.

## Method Summary
POP-QL is an actor-critic algorithm that jointly projects the policy and sampling distribution onto a contraction mapping condition to ensure stable offline RL. The method uses an I-projection (KL divergence minimization) to find a sampling distribution close to the data distribution while satisfying the contraction condition. It includes a KL term between the new policy and the data collection policy to prevent large policy shifts. The algorithm employs low-rank approximation for computational efficiency and uses two-time-scale optimization to handle the expectation inside the exponential. The method is tested on Frozen Lake with linear function approximation and D4RL datasets with neural networks.

## Key Results
- POP-QL achieves competitive performance on D4RL benchmarks compared to state-of-the-art offline RL methods
- The method shows improved stability and performance on tasks with sub-optimal data-collection policies
- POP-QL demonstrates potential for computational efficiency improvements over previous projection-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The contraction mapping condition ensures TD updates converge to a unique fixed point, avoiding divergence in off-policy learning.
- Mechanism: POP-QL reweights the sampling distribution such that $E_{(s,a) \sim q}[F^\pi(s,a)] \succeq 0$, guaranteeing the projected Bellman operator is a contraction mapping.
- Core assumption: The optimization problem finds a valid reweighting that satisfies the contraction condition.
- Evidence anchors:
  - [abstract]: "POP-QL reweights off-policy samples and constrains the policy to prevent divergence and reduce value-approximation error, leveraging a contraction mapping condition."
  - [section]: "Using this formulation, they present an algorithm to find a new sampling distribution that satisfies this contraction mapping condition and proves a bound on the approximation error."
  - [corpus]: Weak evidence; no direct paper in corpus discusses contraction mapping conditions for off-policy stability.
- Break condition: If the optimization fails to find a reweighting that satisfies the condition, TD updates may still diverge.

### Mechanism 2
- Claim: The I-projection (KL divergence minimization) finds a sampling distribution close to the data distribution while satisfying the contraction condition.
- Mechanism: By minimizing $D_{KL}(q \| \mu)$ subject to the contraction condition, POP-QL avoids large reweighting factors that would increase gradient variance.
- Core assumption: The I-projection yields a distribution close enough to the data distribution for stable learning.
- Evidence anchors:
  - [section]: "we propose to use the I-projection instead of the M-projection, which allows us to find an analytical solution to the inner optimization of the dual problem."
  - [corpus]: No direct evidence; the corpus lacks papers discussing I-projection for distribution shift in RL.
- Break condition: If the I-projection produces a distribution too far from the data distribution, learning may become unstable.

### Mechanism 3
- Claim: Jointly projecting the policy and sampling distribution prevents the policy from seeking out-of-distribution actions.
- Mechanism: The optimization includes a KL term between the new policy and the data collection policy, preventing large policy shifts that would require large reweighting.
- Core assumption: Balancing policy improvement against KL divergence keeps the policy within the data support.
- Evidence anchors:
  - [section]: "We jointly project π and the sampling distribution µ using a balancing term β ∈ R+: maximize_{π,q} E_µ[Q^π(s,a)] - βD_{KL}(q || µ) s.t. E_{(s,a) \sim q}[F^π(s,a)] succeq 0"
  - [corpus]: Weak evidence; the corpus lacks papers discussing joint policy and sampling distribution projection.
- Break condition: If β is too small, the policy may still drive the agent too far outside the data distribution.

## Foundational Learning

- Concept: Contraction mapping and Banach fixed-point theorem
  - Why needed here: The convergence guarantee relies on the Bellman operator being a contraction mapping under the projected distribution.
  - Quick check question: What condition must hold for an operator to be a contraction mapping, and why does this guarantee convergence?

- Concept: KL divergence and information projection
  - Why needed here: The I-projection uses KL divergence to find a distribution close to the data distribution while satisfying the contraction condition.
  - Quick check question: How does the I-projection differ from the M-projection, and why is the I-projection more conservative for this application?

- Concept: Semi-definite programming and low-rank approximation
  - Why needed here: The optimization problem involves a positive semi-definite constraint, and low-rank approximation makes it computationally tractable.
  - Quick check question: How does reparameterizing Z as $Z = [A, B][A, B]^T$ ensure Z is positive semi-definite for any A, B?

## Architecture Onboarding

- Component map:
  - Q-function approximator (w parameters) -> Policy network (π parameters) -> Lagrange matrices (A, B parameters) -> g-function approximator (θ parameters) -> Target networks for Q-function and features

- Critical path:
  1. Sample batch from dataset
  2. Compute Q-function target using current policy
  3. Update Lagrange matrices A, B to satisfy contraction condition
  4. Update g-function to approximate inner expectation
  5. Update Q-function using reweighted samples
  6. Update policy to maximize Q-function while balancing KL divergence

- Design tradeoffs:
  - Using I-projection vs M-projection: I-projection is more conservative but may underestimate support
  - Two-time-scale optimization: Necessary due to expectation inside exponential but adds complexity
  - Low-rank approximation: Reduces computational cost but may limit expressiveness of reweighting

- Failure signatures:
  - Large reweighting factors (u(s,a) >> 1): Indicates policy is pushing outside data distribution
  - Unstable Lagrange matrix updates: May indicate poor learning rate or normalization issues
  - Q-function divergence: May indicate contraction condition not being satisfied

- First 3 experiments:
  1. Run on Frozen Lake with k=63 features to verify convergence vs vanilla Q-learning
  2. Test on D4RL hopper-random to verify performance on highly suboptimal data
  3. Vary β parameter to find balance between policy performance and stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the KL-divergence projection (I-projection vs M-projection) impact the performance and stability of POP-QL in high-dimensional continuous control tasks?
- Basis in paper: [explicit] The authors mention using I-projection and argue it is more suitable for RL settings, but do not provide empirical comparisons with M-projection.
- Why unresolved: The theoretical advantages of I-projection are discussed, but empirical validation is lacking, especially in complex environments.
- What evidence would resolve it: Comparative experiments using both I-projection and M-projection in various D4RL tasks, measuring performance, stability, and sample efficiency.

### Open Question 2
- Question: What is the optimal strategy for setting the KL-Q-value weighting parameter β to balance policy performance and sampling distribution reweighting across different domains?
- Basis in paper: [explicit] The authors perform a hyperparameter search for β and find domain-specific values, but do not provide a systematic method for determining β.
- Why unresolved: The optimal β likely depends on the characteristics of the dataset and environment, requiring a more principled approach than grid search.
- What evidence would resolve it: A theoretical framework or automated method for setting β based on dataset properties (e.g., distribution mismatch, optimality gap).

### Open Question 3
- Question: How does the low-rank approximation of the Lagrange matrices (using rank r ≤ 4) affect the convergence and performance of POP-QL, and is there an optimal rank for different problem sizes?
- Basis in paper: [explicit] The authors mention using low-rank matrices for computational efficiency and improved convergence, but do not explore the impact of rank choice.
- Why unresolved: The choice of rank r ≤ 4 is heuristic and may not be optimal for all problem scales and complexities.
- What evidence would resolve it: Experiments varying the rank of Lagrange matrices and analyzing the trade-off between computational cost, convergence speed, and final performance.

## Limitations

- Theoretical focus: The paper emphasizes theoretical foundations but lacks extensive empirical validation in complex, real-world scenarios.
- Computational efficiency claims: While the method claims improved computational efficiency, quantitative comparisons with existing methods are not provided.
- Limited ablation studies: The impact of key design choices (e.g., I-projection vs M-projection, KL-divergence weighting) on performance is not thoroughly explored.

## Confidence

- **High**: The theoretical framework connecting contraction mappings to offline RL stability is well-founded and mathematically rigorous.
- **Medium**: The algorithmic implementation of POP-QL appears sound, with reasonable choices for optimization and approximation techniques.
- **Low**: Claims about computational efficiency improvements and consistent outperformance on D4RL benchmarks lack sufficient empirical support.

## Next Checks

1. **Convergence verification**: Test POP-QL on a grid world with known optimal policy to verify it converges to near-optimal behavior when dataset contains near-optimal trajectories.
2. **Robustness analysis**: Systematically vary the KL-divergence weight β and dataset quality to map out performance boundaries and identify failure modes.
3. **Computational benchmarking**: Measure wall-clock time per update iteration for POP-QL vs CQL across different dataset sizes to quantify the claimed efficiency gains.