---
ver: rpa2
title: 'NUBO: A Transparent Python Package for Bayesian Optimization'
arxiv_id: '2305.06709'
source_url: https://arxiv.org/abs/2305.06709
tags:
- optimisation
- function
- bayesian
- nubo
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NUBO is a transparent Python package for Bayesian optimization
  that targets researchers without deep statistical or computer science expertise.
  It focuses on clean, understandable code, precise references, and thorough documentation
  to ensure transparency, while offering a modular design, easy syntax, and carefully
  selected algorithms to enhance user experience.
---

# NUBO: A Transparent Python Package for Bayesian Optimization

## Quick Facts
- arXiv ID: 2305.06709
- Source URL: https://arxiv.org/abs/2305.06709
- Reference count: 37
- Primary result: A transparent Python package for Bayesian optimization that is accessible to researchers without deep statistical or computer science expertise

## Executive Summary
NUBO is a Python package designed to make Bayesian optimization accessible to researchers from diverse disciplines who lack deep expertise in statistics or computer science. The package emphasizes transparency through clean, well-documented code and a modular design that prioritizes essential functionality over feature bloat. With only 826 lines of code compared to 27,100 lines in BoTorch, NUBO reduces complexity while supporting sequential single-point, parallel multi-point, and asynchronous optimization for bounded, constrained, and mixed input spaces.

## Method Summary
NUBO implements Bayesian optimization using Gaussian processes as surrogate models and acquisition functions to guide the search for optimal parameters. The package supports bounded, constrained, and mixed input spaces through Monte Carlo approximations of acquisition functions, enabling parallel and asynchronous optimization. The optimization process involves defining input space bounds, generating initial training data via space-filling designs, training Gaussian processes, specifying acquisition functions, and running optimization loops using provided functions for single-point, joint multi-point, or sequential multi-point optimization.

## Key Results
- NUBO implements Bayesian optimization in only 826 lines of code, approximately 32 times smaller than BoTorch (27,100 lines)
- The package supports sequential single-point, parallel multi-point, and asynchronous optimization of bounded, constrained, and mixed input spaces
- NUBO is distributed as open-source software under the BSD 3-Clause license and requires no expert Python knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The modular design and small codebase (826 lines) makes NUBO easier to understand and modify than alternatives like BoTorch (27,100 lines).
- Mechanism: By keeping the codebase compact and focusing only on essential algorithms, NUBO reduces cognitive load and lowers the barrier to entry for researchers without deep CS expertise.
- Core assumption: Simplicity and transparency are more valuable than having many options for the target audience.
- Evidence anchors:
  - [abstract]: "The package is written in Python but does not require expert knowledge of Python to optimise your simulators and experiments."
  - [section]: "NUBO implements Bayesian optimisation in only 826 lines of code, while BoTorch uses 27,100 lines of code, roughly 32 times more than NUBO."
  - [corpus]: Weak evidence - no direct citations or comparisons found in corpus.
- Break condition: If a researcher needs advanced features not included in NUBO, they may need to use a more complex package.

### Mechanism 2
- Claim: Monte Carlo acquisition functions allow NUBO to handle parallel and asynchronous optimization, which is not supported by many competing packages.
- Mechanism: By using Monte Carlo sampling to approximate acquisition functions, NUBO can suggest multiple points in each iteration, enabling parallelism and asynchrony.
- Core assumption: Parallel and asynchronous optimization is valuable for many real-world applications.
- Evidence anchors:
  - [abstract]: "It supports sequential single-point, parallel multi-point, and asynchronous optimization of bounded, constrained, and mixed input spaces."
  - [section]: "The Monte Carlo approach can also be used for single-point asynchronous optimisation by setting batch_size=1."
  - [corpus]: Weak evidence - no direct citations or comparisons found in corpus.
- Break condition: If the objective function cannot be evaluated in parallel, the benefits of this mechanism are not realized.

### Mechanism 3
- Claim: NUBO's focus on transparency through clean code, precise references, and thorough documentation makes Bayesian optimization accessible to non-experts.
- Mechanism: By providing clear explanations, examples, and a user-friendly interface, NUBO lowers the barrier to entry for researchers from diverse disciplines.
- Core assumption: Transparency and ease of use are key factors in adoption by non-expert users.
- Evidence anchors:
  - [abstract]: "NUBO focuses on transparency and user experience to make Bayesian optimization accessible to researchers from all disciplines."
  - [section]: "With NUBO we provide an open-source implementation of Bayesian optimisation aimed at researchers with expertise in disciplines other than statistics and computer science."
  - [corpus]: Weak evidence - no direct citations or comparisons found in corpus.
- Break condition: If a user requires highly customized or advanced features not covered by NUBO's documentation and examples, they may struggle to use the package effectively.

## Foundational Learning

- Concept: Bayesian optimization
  - Why needed here: NUBO is a package for Bayesian optimization, so understanding the basics of this technique is essential.
  - Quick check question: What is the main goal of Bayesian optimization?

- Concept: Gaussian processes
  - Why needed here: Gaussian processes are used as surrogate models in Bayesian optimization, which is the core of NUBO's functionality.
  - Quick check question: What is the role of a Gaussian process in Bayesian optimization?

- Concept: Acquisition functions
  - Why needed here: Acquisition functions guide the selection of candidate points in Bayesian optimization, and NUBO provides several options.
  - Quick check question: What is the purpose of an acquisition function in Bayesian optimization?

## Architecture Onboarding

- Component map: NUBO consists of five main sub-modules: models (Gaussian processes), acquisition (acquisition functions), optimization (optimization algorithms), test_functions (benchmark functions), and utils (utilities). The main classes are GaussianProcess, UpperConfidenceBound, MCUpperConfidenceBound, and the optimization functions single, multi_joint, and multi_sequential.
- Critical path: The typical usage involves defining the input space, generating initial training data, defining and training the Gaussian process, specifying an acquisition function, and running the optimization loop using one of the provided optimization functions.
- Design tradeoffs: NUBO prioritizes simplicity and transparency over having a large number of advanced features. This makes it more accessible to non-experts but may limit its usefulness for highly specialized applications.
- Failure signatures: Common issues include improper scaling of input data, incorrect specification of bounds or constraints, and failure to properly handle discrete input dimensions.
- First 3 experiments:
  1. Use NUBO to optimize a simple 1D function like the Hartmann function, comparing the results to random sampling.
  2. Implement a custom acquisition function and use it with NUBO's optimization functions.
  3. Extend NUBO to handle a new type of constraint or mixed input space not currently supported.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the key factors limiting the performance of Monte Carlo acquisition functions in high-dimensional input spaces, and how can these limitations be systematically addressed?
- Basis in paper: [explicit] The paper discusses the use of Monte Carlo approximations for acquisition functions but does not address performance in high-dimensional spaces or propose solutions to potential limitations.
- Why unresolved: The paper does not explore the scalability of Monte Carlo methods to high-dimensional problems or identify specific bottlenecks.
- What evidence would resolve it: Empirical studies comparing Monte Carlo acquisition performance across varying input dimensions, with analysis of computational complexity and approximation error.

### Open Question 2
- Question: How does the choice of base samples (e.g., fixed vs. random) in Monte Carlo acquisition functions impact the convergence rate and final solution quality in asynchronous Bayesian optimization?
- Basis in paper: [explicit] The paper mentions that fixing base samples may introduce bias but does not investigate its impact on convergence or solution quality in asynchronous settings.
- Why unresolved: No experimental comparison is provided between fixed and random base samples in asynchronous optimization scenarios.
- What evidence would resolve it: Controlled experiments measuring convergence speed and solution accuracy using fixed vs. random base samples across multiple asynchronous optimization problems.

### Open Question 3
- Question: What is the optimal balance between exploration and exploitation in Bayesian optimization when the objective function evaluation cost varies significantly across the input space?
- Basis in paper: [inferred] The paper discusses exploration-exploitation trade-off in acquisition functions but does not address scenarios with non-uniform evaluation costs.
- Why unresolved: The standard acquisition functions assume equal evaluation costs, and no extension to variable-cost scenarios is proposed or tested.
- What evidence would resolve it: Comparative studies of acquisition functions in variable-cost settings, with metrics for cost-efficiency and solution quality.

### Open Question 4
- Question: How does the performance of Bayesian optimization with discrete input parameters scale with the number of discrete values per dimension, and what are the practical limits of this approach?
- Basis in paper: [explicit] The paper acknowledges that mixed optimization can be time-consuming for many discrete dimensions or values but does not quantify this scaling or define practical limits.
- Why unresolved: No empirical analysis of runtime or solution quality degradation as discrete parameter cardinality increases.
- What evidence would resolve it: Benchmarking studies measuring computational time and optimization performance across problems with increasing discrete parameter counts.

## Limitations

- The paper lacks empirical validation of usability claims, with no user studies or benchmark comparisons against alternatives like BoTorch
- Claims about parallel and asynchronous optimization benefits remain unverified through performance testing under different computing scenarios
- The assertion that the package is accessible to non-experts lacks validation through usability testing or case studies with the intended user base

## Confidence

- High confidence: The technical description of NUBO's architecture and algorithm implementations
- Medium confidence: Claims about code simplicity and transparency benefits
- Low confidence: Usability claims for non-expert researchers and parallel optimization performance

## Next Checks

1. Conduct a benchmark study comparing NUBO's optimization performance against BoTorch and PyBO on standard test functions under identical conditions
2. Perform a small-scale usability study with researchers from non-CS disciplines to assess the learning curve and user experience
3. Test the parallel and asynchronous optimization capabilities with varying numbers of workers to quantify scalability benefits