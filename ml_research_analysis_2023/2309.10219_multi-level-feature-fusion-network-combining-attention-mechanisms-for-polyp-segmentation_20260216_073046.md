---
ver: rpa2
title: Multi-level feature fusion network combining attention mechanisms for polyp
  segmentation
arxiv_id: '2309.10219'
source_url: https://arxiv.org/abs/2309.10219
tags:
- polyp
- features
- attention
- segmentation
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accurately segmenting polyps
  in colonoscopy images, a critical task for early detection of colorectal cancer.
  Existing methods struggle with inadequate feature utilization and semantic conflicts
  during feature fusion.
---

# Multi-level feature fusion network combining attention mechanisms for polyp segmentation

## Quick Facts
- arXiv ID: 2309.10219
- Source URL: https://arxiv.org/abs/2309.10219
- Reference count: 40
- Primary result: Proposes MLFF-Net achieving 0.943 Dice and 0.897 IoU on CVC-ClinicDB

## Executive Summary
This paper addresses the challenge of accurate polyp segmentation in colonoscopy images by proposing MLFF-Net, a novel network that combines multi-level feature fusion with attention mechanisms. The method tackles the limitations of existing approaches that struggle with inadequate feature utilization and semantic conflicts during feature fusion. MLFF-Net demonstrates state-of-the-art performance across five public datasets, showing strong generalization capabilities for different polyp types and imaging conditions.

## Method Summary
MLFF-Net employs a Transformer encoder with three key attention-based modules: MAM for multi-scale feature extraction from shallow layers, HFEM for aggregating and enhancing deep features while resolving semantic conflicts, and GAM for computing global dependencies between encoder and decoder features. The network is trained using AdamW optimizer with multi-scale training strategy on five public colonoscopy datasets, achieving superior segmentation performance through progressive feature enhancement from shallow to deep layers.

## Key Results
- Achieves 0.943 mean Dice coefficient and 0.897 mean Intersection over Union on CVC-ClinicDB
- Demonstrates strong generalization across five different datasets including CVC-ColonDB, ETIS, and CVC-300
- Outperforms existing state-of-the-art methods in comprehensive quantitative comparisons

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-scale attention in MAM extracts fine polyp details and mitigates background noise, enabling better segmentation of polyps with varying sizes and textures.
- **Mechanism:** MAM applies convolutional branches with kernel sizes {1, 3, 5, 7} and dilated convolutions to capture multi-scale spatial features from shallow encoder outputs. Channel and spatial attention then filter out irrelevant background and enhance texture/boundary details.
- **Core assumption:** Shallow features contain rich detail but also significant noise; multi-scale convolution with dilated kernels can effectively capture polyp details across scales.
- **Evidence anchors:** [abstract] "MAM is used to extract multi-scale information and polyp details from the shallow output of the encoder." [section 3.2] "MAM first extracts the multiscale information from the shallow encoder feature maps, and then removes the background noise from the shallow information to enhance the detailed information representation."
- **Break condition:** If shallow features lack sufficient detail due to aggressive downsampling, MAM cannot recover them; also, if polyps are extremely small relative to image resolution, multi-scale receptive fields may still miss them.

### Mechanism 2
- **Claim:** High-level Feature Enhancement Module (HFEM) aggregates deep semantic features from deep to shallow, resolving semantic conflicts and improving polyp localization.
- **Mechanism:** HFEM upsamples and multiplies deep features at multiple scales, allowing complementary semantic information to enhance each other. Attention mechanisms then reweight fused features to suppress conflicting semantics and noise.
- **Core assumption:** Deep features contain strong semantic information useful for localization; fusing them progressively from deep to shallow while applying attention can mitigate conflicts.
- **Evidence anchors:** [abstract] "HFEM for aggregating and enhancing deep features while mitigating semantic conflicts." [section 3.3] "HFEM aggregates the semantic information in deep features from deep to shallow to make multi-level features complementary to each other."
- **Break condition:** If deep features are already highly compressed or noisy, fusion may propagate errors; if semantic conflicts are too severe, attention reweighting may not sufficiently suppress them.

### Mechanism 3
- **Claim:** Global Attention Module (GAM) captures long-range dependencies and avoids local receptive field limitations by fusing encoder and decoder features at the same level.
- **Mechanism:** GAM computes inner products between encoder and decoder features to generate global attention weights, then applies these weights to decoder features and concatenates with encoder features, ensuring global context is considered.
- **Core assumption:** Local convolutions alone cannot capture global polyp structure; combining encoder-decoder features with global attention improves segmentation accuracy.
- **Evidence anchors:** [abstract] "GAM for computing global dependencies to prevent receptive field locality." [section 3.4] "GAM not only captures global information from the current decoder feature map, but also adds the same-level encoder features to the global weight calculation."
- **Break condition:** If encoder and decoder features are poorly aligned spatially, global attention may introduce artifacts; if global context is less important than local details for certain polyp types, this may hurt performance.

## Foundational Learning

- **Concept: Multi-scale feature extraction**
  - Why needed here: Polyps vary greatly in size and texture; capturing features at multiple scales ensures both tiny and large polyps are represented.
  - Quick check question: What kernel sizes are used in MAM to capture multi-scale features? *(Answer: {1, 3, 5, 7})*

- **Concept: Attention mechanisms (channel + spatial)**
  - Why needed here: Encoder features are noisy and contain irrelevant background; attention filters these out and emphasizes task-relevant information.
  - Quick check question: Which two types of attention are combined in MAM and HFEM? *(Answer: channel and spatial attention)*

- **Concept: Feature fusion across levels**
  - Why needed here: Shallow features have detail; deep features have semantics; combining them yields richer representations for accurate segmentation.
  - Quick check question: In HFEM, from which direction are deep features aggregated to shallow ones? *(Answer: deep to shallow)*

## Architecture Onboarding

- **Component map:** Input → Transformer encoder (downsample → 4 feature maps: shallow + 3 deep) → MAM (shallow) → HFEM (deep) → GAM (encoder-decoder fusion) → decoder (upsample → prediction) → loss (IoU + BCE)
- **Critical path:** 1. Shallow feature extraction (MAM) for detail preservation. 2. Deep feature aggregation (HFEM) for semantic enhancement. 3. Global attention fusion (GAM) for context integration. 4. Decoder upsampling to produce final segmentation mask.
- **Design tradeoffs:** Using Transformer encoder instead of CNN increases receptive field but adds computational cost. Multi-scale convolutions with dilated kernels increase parameters but improve detail capture. Global attention fusion improves context but may misalign encoder-decoder features if resolution mismatch occurs.
- **Failure signatures:** Over-segmentation: attention mechanisms too permissive, capturing background as polyp. Under-segmentation: attention too strict, missing polyp edges. Artifacts at boundaries: misalignment in GAM fusion. Slow convergence: inappropriate loss weighting or learning rate.
- **First 3 experiments:** 1. **Ablation test:** Remove MAM and retrain; compare mDice to baseline to confirm multi-scale detail benefit. 2. **Hyperparameter sweep:** Vary GAM's inner product scaling factor; observe impact on boundary accuracy. 3. **Data augmentation test:** Add random rotation/flip; verify generalization on unseen datasets (CVC-ColonDB, ETIS).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MLFF-Net perform when trained and evaluated on video-based colonoscopy datasets rather than image frames?
- Basis in paper: [explicit] The paper acknowledges that current video-based algorithms are less effective and require more computational resources, and mentions this as a limitation of their approach which is based on image frames.
- Why unresolved: The authors did not evaluate their model on video datasets, focusing instead on five image frame datasets (Kvasir, CVC-ClinicDB, CVC-ColonDB, ETIS, CVC-300).
- What evidence would resolve it: Training and testing MLFF-Net on dedicated colonoscopy video datasets, measuring performance metrics like mDice and mIoU, and comparing results with existing video-based polyp segmentation methods.

### Open Question 2
- Question: What is the impact of increasing the number of training samples for images containing multiple polyps on the segmentation accuracy of MLFF-Net?
- Basis in paper: [explicit] The authors state that the prediction results for samples with multiple polyps are not perfect due to limited training data for such cases, and these polyps are closer to normal protruding tissues, making segmentation more challenging.
- Why unresolved: The current training datasets have insufficient examples of images with multiple polyps, which limits the model's ability to learn these specific cases.
- What evidence would resolve it: Collecting and annotating a larger dataset with more images containing multiple polyps, training MLFF-Net on this expanded dataset, and evaluating whether segmentation accuracy for multiple polyps improves.

### Open Question 3
- Question: How does the performance of MLFF-Net compare to transformer-based methods like Polyp-PVT when both are trained on the same datasets?
- Basis in paper: [explicit] The authors compare MLFF-Net with state-of-the-art methods including Polyp-PVT, but the comparison is based on published results rather than a controlled experiment where both models are trained under identical conditions.
- Why unresolved: Different training setups, data augmentations, and hyperparameters could influence the comparison results, making it difficult to isolate the architectural differences.
- What evidence would resolve it: Implementing both MLFF-Net and Polyp-PVT from scratch, training them on the same datasets with identical hyperparameters and data augmentation strategies, and directly comparing their performance metrics.

## Limitations

- The model requires extensive computational resources due to Transformer architecture and multiple attention modules
- Performance on multi-polyp images is limited by insufficient training data for such cases
- The method is currently designed for image frames rather than video sequences, limiting real-time clinical applications

## Confidence

- **High confidence**: The general architecture design and training methodology are well-documented and align with standard practices in medical image segmentation
- **Medium confidence**: The reported benchmark results on CVC-ClinicDB are convincing, but the generalization claims across other datasets need more validation
- **Low confidence**: The exact implementation details of the attention mechanisms and feature fusion strategies are not fully specified, making faithful reproduction uncertain

## Next Checks

1. **Ablation Study**: Systematically remove MAM, HFEM, and GAM modules individually and retrain the network to quantify each component's contribution to overall performance

2. **Cross-Dataset Generalization**: Evaluate the trained model on each test dataset separately and analyze performance degradation to understand domain adaptation capabilities

3. **Attention Mechanism Analysis**: Visualize the attention maps generated by MAM and GAM to verify they are correctly identifying polyp regions and suppressing background noise as claimed