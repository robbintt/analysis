---
ver: rpa2
title: 'On robust overfitting: adversarial training induced distribution matters'
arxiv_id: '2311.16526'
source_url: https://arxiv.org/abs/2311.16526
tags:
- robust
- adversarial
- training
- pgd-at
- overfitting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the phenomenon of robust overfitting in
  adversarial training, where models achieve low robust error on training data but
  high error on test data. The authors conduct induced distribution experiments (IDEs)
  to show that the generalization difficulty of perturbation-induced distributions
  correlates with robust overfitting.
---

# On robust overfitting: adversarial training induced distribution matters

## Quick Facts
- arXiv ID: 2311.16526
- Source URL: https://arxiv.org/abs/2311.16526
- Authors: 
- Reference count: 40
- Key outcome: Robust overfitting correlates with increasing generalization difficulty of perturbation-induced distributions during PGD-based adversarial training

## Executive Summary
This paper investigates robust overfitting in adversarial training by examining how perturbation-induced distributions evolve during training. The authors show empirically that robust overfitting correlates with increasing generalization difficulty of these induced distributions. They provide a theoretical upper bound for generalization error that reveals a key quantity governing this difficulty is the local "dispersion property" of adversarial perturbations. Experiments demonstrate that as adversarial training proceeds, perturbations become increasingly dispersive, validating the theoretical results.

## Method Summary
The authors conduct induced distribution experiments (IDEs) by performing PGD-based adversarial training (PGD-AT) on multiple datasets, saving model checkpoints at specified intervals. For each checkpoint, they generate perturbed training and testing sets and retrain models on these perturbed sets using standard training. They analyze the correlation between IDE testing errors and robust generalization gaps, derive a theoretical upper bound for generalization error based on local dispersion of perturbations, and examine how perturbation magnitudes and angles evolve during training.

## Key Results
- Robust overfitting correlates with increasing generalization difficulty of perturbation-induced distributions during PGD-AT
- Theoretical upper bound reveals that local dispersion of adversarial perturbations governs generalization difficulty
- As adversarial training proceeds, perturbation magnitudes decrease while angles spread out more, potentially due to decision boundary irregularity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Robust overfitting correlates with increasing generalization difficulty of perturbation-induced distributions during PGD-AT.
- Mechanism: As adversarial training progresses, perturbations become more dispersive, making perturbed data distributions harder to generalize from.
- Core assumption: Induced distributions at each training step can be approximated as smoothed versions of true data distributions.
- Evidence anchors: Empirical correlation between IDE testing error and robust generalization gap across multiple datasets; related work on Wasserstein distributionally robust optimization.

### Mechanism 2
- Claim: Local dispersion property of adversarial perturbations governs generalization difficulty of induced distribution.
- Mechanism: Theoretical upper bound shows less dispersive perturbations provide better generalization guarantees.
- Core assumption: Lipschitzness of loss function and boundedness of perturbation-smoothed loss hold.
- Evidence anchors: Novel theoretical contribution with empirical validation showing dispersion increases during robust overfitting.

### Mechanism 3
- Claim: Perturbation angles spread out more during training due to increasing decision boundary irregularity.
- Mechanism: Spreading perturbation angles contribute to increasing dispersion, which increases generalization difficulty.
- Core assumption: Decision boundary irregularity increases with training progress, causing perturbations to become less aligned.
- Evidence anchors: Empirical observation that perturbation angles spread out more despite decreasing magnitudes.

## Foundational Learning

- Concept: Understanding of adversarial training and robust overfitting
  - Why needed here: Paper builds on robust overfitting concept and explores its relationship with perturbation-induced distributions
  - Quick check question: What is the difference between standard error and robust error in adversarial training?

- Concept: Knowledge of Projected Gradient Descent (PGD) and adversarial example generation
  - Why needed here: Paper focuses on PGD-based adversarial training and its effects on induced distributions
  - Quick check question: How does the PGD algorithm generate adversarial examples?

- Concept: Familiarity with generalization bounds and their role in understanding model behavior
  - Why needed here: Paper provides theoretical upper bound for generalization error with respect to perturbation-induced distributions
  - Quick check question: What factors typically influence the generalization gap in machine learning models?

## Architecture Onboarding

- Component map: PGD-AT algorithm -> Model checkpointing -> Perturbation generation -> Retraining on perturbed sets -> Performance evaluation

- Critical path:
  1. Perform PGD-AT on training set
  2. Save model checkpoints at specified intervals
  3. For each checkpoint, generate perturbed training and testing sets
  4. Retrain models on perturbed training sets using standard training
  5. Test models on perturbed testing sets
  6. Analyze results and compare with theoretical predictions

- Design tradeoffs: Computational cost vs. granularity of checkpoints; complexity of theoretical analysis vs. practical applicability; number of datasets vs. breadth of conclusions

- Failure signatures: No correlation between IDE testing error and robust generalization gap; theoretical bounds not matching empirical observations; inconsistent results across datasets or model architectures

- First 3 experiments:
  1. Replicate CIFAR-10 experiments to verify correlation between IDE testing error and robust generalization gap
  2. Analyze evolution of perturbation dispersion on new dataset to validate theoretical upper bound
  3. Investigate relationship between perturbation angles and decision boundary irregularity on simple model (e.g., linear classifier)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific structural properties of the decision boundary cause adversarial perturbations to become more dispersed and less aligned during adversarial training?
- Basis in paper: Paper observes perturbation angles spread out more during training and speculates this may be due to decision boundary irregularity
- Why unresolved: Paper provides observational evidence but no mechanistic explanation for why boundary becomes irregular or what features cause dispersion pattern
- What evidence would resolve it: Controlled experiments varying network architectures, activation functions, or training procedures to systematically test how decision boundary characteristics affect perturbation dispersion

### Open Question 2
- Question: Can the relationship between local dispersion of perturbations and robust overfitting generalization be quantified more precisely to predict when robust overfitting will occur?
- Basis in paper: Paper derives theoretical upper bound showing generalization gap is affected by expected local dispersion, and experiments show this quantity increases during robust overfitting
- Why unresolved: While paper establishes correlation and provides theoretical framework, it does not provide precise quantitative predictions about when robust overfitting will manifest based on dispersion measurements
- What evidence would resolve it: Empirical studies correlating specific threshold values of expected local dispersion with robust overfitting onset across multiple datasets and model architectures

### Open Question 3
- Question: How do different adversarial training algorithms (beyond PGD-AT) affect the evolution of perturbation dispersion and resulting generalization difficulty?
- Basis in paper: Paper focuses exclusively on PGD-AT and observes specific patterns in how perturbations evolve during this training method
- Why unresolved: Paper does not investigate whether other adversarial training methods produce similar patterns in perturbation dispersion or whether different methods might mitigate increasing generalization difficulty
- What evidence would resolve it: Comparative studies of perturbation dispersion dynamics across multiple adversarial training algorithms to identify which methods minimize dispersion growth

## Limitations

- Theoretical assumptions of Lipschitz continuity and boundedness may not hold for all deep neural network architectures and loss functions
- Empirical validation limited to specific model architectures and perturbation settings
- Mechanism linking decision boundary irregularity to perturbation angle spreading is proposed but not rigorously proven
- Reduced ImageNet dataset with only 10 classes may not capture full complexity of real-world image classification

## Confidence

- High confidence: Empirical observation that robust overfitting correlates with increasing generalization difficulty of perturbation-induced distributions
- Medium confidence: Theoretical upper bound relating dispersion to generalization difficulty
- Medium confidence: Mechanism explaining why perturbation angles spread out more during training

## Next Checks

1. Test theoretical bound on broader range of loss functions and architectures, including those with non-smooth activations or unbounded losses
2. Conduct ablation studies on perturbation generation process (varying step sizes, number of steps in PGD) to isolate which aspects most strongly influence dispersion and subsequent overfitting
3. Design experiments to directly measure decision boundary irregularity (e.g., using adversarial examples or boundary visualizations) and establish more rigorous causal link between boundary complexity and perturbation angle spreading