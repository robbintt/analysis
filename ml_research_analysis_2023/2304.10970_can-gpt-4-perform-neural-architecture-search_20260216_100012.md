---
ver: rpa2
title: Can GPT-4 Perform Neural Architecture Search?
arxiv_id: '2304.10970'
source_url: https://arxiv.org/abs/2304.10970
tags:
- channels
- gpt-4
- neural
- search
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether GPT-4 can perform neural architecture
  search (NAS), a challenging task in machine learning. The authors propose GENIUS,
  a method that uses GPT-4 as a black-box optimizer to navigate the architecture search
  space and iteratively refine candidate models.
---

# Can GPT-4 Perform Neural Architecture Search?

## Quick Facts
- **arXiv ID:** 2304.10970
- **Source URL:** https://arxiv.org/abs/2304.10970
- **Reference count:** 40
- **Primary result:** GENIUS achieves competitive performance on multiple NAS benchmarks using GPT-4 as a black-box optimizer

## Executive Summary
This paper investigates whether GPT-4 can perform neural architecture search (NAS), a fundamental challenge in machine learning. The authors introduce GENIUS, which leverages GPT-4's generative capabilities to navigate architecture search spaces and iteratively refine candidate models. Through experiments on multiple benchmarks including NAS-Bench-Macro and NAS-Bench-201, the study demonstrates that GPT-4 can identify high-performing architectures without fine-tuning or domain-specific training, achieving results competitive with state-of-the-art NAS techniques.

## Method Summary
GENIUS treats neural architecture search as a language modeling task where GPT-4 iteratively proposes architectures, evaluates their performance, and refines subsequent proposals based on feedback. The method uses simple prompting schemes to encode NAS problems into natural language, with GPT-4 generating candidate architectures that are then evaluated using benchmark lookup tables. The approach employs temperature hyperparameters to control exploration-exploitation tradeoffs and operates as a black-box optimizer without requiring fine-tuning on NAS-specific data.

## Key Results
- GENIUS achieves Rank 33/16384 (Top 0.2%) on ResNet-based models in NAS-Bench-Macro
- GENIUS achieves Rank 16/16384 (Top 0.1%) on MobileNet-based models in NAS-Bench-Macro
- The method demonstrates competitive performance compared to established NAS algorithms across multiple benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPT-4 can iteratively improve neural architecture designs by treating architecture search as a language modeling task
- **Mechanism:** The model encodes NAS problems into natural language prompts, proposes candidate architectures, and uses performance feedback to refine subsequent proposals through iterative prompting
- **Core assumption:** GPT-4's knowledge of CNN design principles (depth, width, skip connections, normalization) enables it to generate valid architectures and improve them based on performance metrics
- **Evidence anchors:**
  - [abstract]: "leverages the generative capabilities of GPT-4 as a black-box optimiser to quickly navigate the architecture search space, pinpoint promising candidates, and iteratively refine these candidates"
  - [section 1]: "GPT-4 responded as follows: (1) Depth: Deeper networks can learn complex features... (5) Normalization: Normalization methods like batch normalization, layer normalization, or instance normalization can improve convergence and stability"
  - [corpus]: Weak - corpus papers focus on LLM-driven NAS but don't provide direct evidence for this iterative refinement mechanism
- **Break condition:** If GPT-4's knowledge of NAS is superficial or its ability to incorporate performance feedback is limited, the iterative improvement process will stall or produce suboptimal architectures

### Mechanism 2
- **Claim:** Temperature hyperparameter controls the exploration-exploitation tradeoff in architecture search
- **Mechanism:** Higher temperature values introduce randomness into GPT-4's proposals, enabling broader exploration of the search space, while lower temperatures promote exploitation of known good architectures
- **Core assumption:** The temperature parameter effectively controls randomness in the model's output distribution, similar to its function in other language generation tasks
- **Evidence anchors:**
  - [section 4.2]: "We conducted experiments with both temperature=0 and temperature=1 to assess the effectiveness of GENIUS under different levels of randomness"
  - [section 4.2]: "We observe that GENIUS exhibits some randomness in its responses, even when the temperature is set to 0"
  - [corpus]: Weak - corpus papers mention temperature in LLM-NAS contexts but don't provide direct evidence for its effect on exploration-exploitation tradeoffs
- **Break condition:** If temperature settings don't significantly impact the diversity or quality of proposed architectures, the exploration-exploitation tradeoff cannot be effectively controlled

### Mechanism 3
- **Claim:** GPT-4 can achieve competitive performance on NAS benchmarks without fine-tuning or domain-specific training
- **Mechanism:** The model leverages its pre-existing knowledge from training data to understand NAS problems and propose effective architectures, demonstrating transfer learning capabilities
- **Core assumption:** GPT-4's training corpus included sufficient information about neural architecture design principles and NAS benchmarks to enable zero-shot performance on these tasks
- **Evidence anchors:**
  - [abstract]: "our objective is to highlight GPT-4's potential to assist research on a challenging technical problem through a simple prompting scheme that requires relatively limited domain expertise"
  - [section 4]: "GENIUS achieves Rank 33 / 16384 (Top 0.2%) for the ResNet-based model and Rank 16 / 16384 (Top 0.1%) for the MobileNet-based model"
  - [corpus]: Weak - corpus papers suggest LLM capabilities for NAS but don't provide direct evidence for zero-shot performance without fine-tuning
- **Break condition:** If GPT-4's performance significantly degrades on benchmarks not seen during training or if domain-specific fine-tuning becomes necessary for competitive results

## Foundational Learning

- **Concept:** Neural Architecture Search (NAS)
  - Why needed here: Understanding the NAS problem formulation, search spaces, and evaluation metrics is essential for interpreting GENIUS's approach and results
  - Quick check question: What distinguishes one-shot NAS methods from multi-fidelity approaches, and how does this impact computational efficiency?

- **Concept:** CNN Design Principles
  - Why needed here: GPT-4's knowledge of depth, width, skip connections, and normalization directly informs its ability to propose effective architectures
  - Quick check question: How do depthwise separable convolutions reduce computational cost compared to standard convolutions while maintaining representational power?

- **Concept:** Large Language Model Prompt Engineering
  - Why needed here: The success of GENIUS depends on effective problem encoding and iterative refinement through natural language prompts
  - Quick check question: What prompt engineering techniques can be used to guide LLM output toward specific architectural patterns or constraints?

## Architecture Onboarding

- **Component map:** Problem encoding → GPT-4 proposal → Architecture evaluation → Performance feedback → Refined prompt → Next iteration
- **Critical path:** Problem encoding → GPT-4 proposal → Architecture evaluation → Performance feedback → Refined prompt → Next iteration
- **Design tradeoffs:** Simple prompting scheme vs. complex fine-tuning; zero-shot performance vs. domain-specific optimization; exploration (higher temperature) vs. exploitation (lower temperature)
- **Failure signatures:** Performance plateauing after few iterations; random architecture proposals regardless of temperature; inability to generate valid architectures; inconsistent results across runs
- **First 3 experiments:**
  1. Run GENIUS on NAS-Bench-Macro with temperature=0 and temperature=1, comparing accuracy progression and final rankings
  2. Test GENIUS on Channel-Bench-Macro with both ResNet and MobileNet base models to evaluate transfer learning capabilities
  3. Apply GENIUS to NAS-Bench-201 to assess performance on cell-level architecture search and compare with established NAS methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does GPT-4's performance on neural architecture search rely on having seen the specific benchmark datasets during training?
- Basis in paper: [inferred] The paper explicitly states "we do not know which data was included in the training set for GPT-4" and raises concerns about "benchmark contamination" where GPT-4 might be "searching 'from memory' rather than leveraging insight about how to improve an architecture design."
- Why unresolved: The authors cannot determine if GPT-4's success comes from genuine architectural insight or from memorizing benchmark-specific solutions. This uncertainty stems from OpenAI not disclosing the full training data.
- What evidence would resolve it: Testing GPT-4 on completely novel NAS benchmarks that were created after GPT-4's training cutoff date, or on private benchmarks never released publicly, would demonstrate whether GPT-4 can generalize beyond memorized solutions.

### Open Question 2
- Question: What specific prompt engineering strategies would maximize GPT-4's effectiveness as a neural architecture search optimizer?
- Basis in paper: [explicit] The authors note "we have little insight into how changes to the prompt influence behavior as an optimizer" and observe that "later iterations under-perform earlier iterations in some cases" despite requesting improved performance in the prompt.
- Why unresolved: The paper uses a relatively simple prompting scheme and acknowledges limited understanding of how prompt variations affect GPT-4's search behavior, noting that GPT-4 sometimes produces significantly worse models when forced to generate different architectures.
- What evidence would resolve it: Systematic experimentation with different prompt structures, including varying the framing of optimization objectives, providing different types of feedback, and testing iterative refinement strategies would reveal optimal prompting approaches.

### Open Question 3
- Question: What are the fundamental limitations of using language models like GPT-4 for neural architecture search compared to specialized NAS algorithms?
- Basis in paper: [explicit] The authors identify several limitations including "limited control and inscrutability" where they cannot understand how prompt changes influence behavior, and note that GPT-4 sometimes produces "significantly different architecture[s] from previous answers" that perform worse.
- Why unresolved: While the paper demonstrates GPT-4 can achieve competitive results, it doesn't systematically compare the quality of search trajectories, convergence properties, or architectural diversity against established NAS methods like DARTS or evolutionary strategies.
- What evidence would resolve it: Comprehensive benchmarking comparing GPT-4's search process (including architectural diversity, convergence speed, and final performance) against multiple state-of-the-art NAS algorithms across various search spaces would reveal relative strengths and weaknesses.

## Limitations
- Experiments rely on proxy accuracy metrics rather than training full models
- Study uses OpenAI's API without open-source reproducibility
- Insufficient ablation studies on prompt engineering and temperature settings

## Confidence
- **High confidence**: GPT-4's ability to generate valid neural architectures and understand basic CNN design principles
- **Medium confidence**: The effectiveness of temperature-based exploration-exploitation control
- **Low confidence**: Claims about zero-shot performance without fine-tuning

## Next Checks
1. Conduct systematic prompt engineering ablation studies to identify optimal encoding strategies
2. Perform extended temperature analysis across a broader range of values (0.0, 0.2, 0.5, 0.8, 1.0)
3. Test GENIUS on NAS benchmarks not included in original experiments to assess generalization capabilities