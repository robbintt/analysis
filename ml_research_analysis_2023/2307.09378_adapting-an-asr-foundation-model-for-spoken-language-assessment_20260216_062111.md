---
ver: rpa2
title: Adapting an ASR Foundation Model for Spoken Language Assessment
arxiv_id: '2307.09378'
source_url: https://arxiv.org/abs/2307.09378
tags:
- speech
- test
- language
- whisper
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large-scale ASR foundation models like Whisper are not well suited
  for spoken language assessment due to their tendency to add punctuation, normalize
  numbers, skip disfluencies, and omit hesitations. To address this, the authors propose
  two methods to adapt Whisper for assessment tasks: model fine-tuning (FT) and a
  novel soft prompt tuning (SPT) approach.'
---

# Adapting an ASR Foundation Model for Spoken Language Assessment

## Quick Facts
- arXiv ID: 2307.09378
- Source URL: https://arxiv.org/abs/2307.09378
- Reference count: 0
- Primary result: Whisper's tendency to normalize text and omit spoken artifacts makes it unsuitable for L2 assessment, but soft prompt tuning achieves 82.9% word recall on learner data using only 15KB parameters.

## Executive Summary
This paper addresses the challenge of adapting large-scale ASR foundation models like Whisper for spoken language assessment tasks. The authors identify that Whisper's pre-training on high-level inverse text normalization (ITN) causes it to omit crucial spoken artifacts such as hesitations, disfluencies, repetitions, and normalized numbers - all essential for accurate L2 assessment. To solve this, they propose two adaptation methods: model fine-tuning and a novel soft prompt tuning approach that adds trainable prompt vectors to the decoder input while keeping the original model parameters frozen. Experiments show both methods significantly improve transcription accuracy for assessment tasks, with soft prompt tuning achieving comparable performance to fine-tuning using far fewer parameters.

## Method Summary
The authors propose adapting Whisper for spoken language assessment through two approaches: model fine-tuning (FT) where all parameters are updated on domain-specific data, and soft prompt tuning (SPT) where small trainable prompt vectors are prepended to the decoder input while keeping the original model frozen. The soft prompt tuning approach is particularly parameter-efficient, requiring only 15KB of trainable parameters compared to 244MB for fine-tuning. Both methods are evaluated on public datasets (LibriSpeech, TED-LIUM3, MGB3) and a Linguaskill English learner dataset with labeled disfluencies and partial words, using speech-aware WER metrics and token-type recall for hesitations, numbers, abbreviations, disfluencies, and partial words.

## Key Results
- Baseline Whisper output shows poor performance on L2 learner data with only 15.4% overall word recall
- Soft prompt tuning improves word recall to 82.9% on L2 learner data while using only 15KB parameters
- Both fine-tuning and soft prompt tuning significantly reduce deletion errors for hesitations and repetitions
- Soft prompt tuning with 20 prompts performs best across experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Whisper is pre-trained on speech data with high-level ITN, so it omits disfluencies, hesitations, and normalizes numbers/abbreviations, making its output unsuitable for spoken language assessment.
- Mechanism: The foundation model learns to produce human-readable text by applying text normalization during decoding, which removes spoken artifacts needed for accurate L2 assessment.
- Core assumption: The training data for Whisper contains normalized text, not raw spoken transcripts with disfluencies.
- Evidence anchors:
  - [abstract] "Rather than transcribing every word from the response into the corresponding spoken format, Whisper tends to generate punctuation, capitalisation, and phrases with inverse text normalisation (ITN) in decoding."
  - [section] "Moreover, a large amount of deletions compared to the reference text have been observed on the test sets, which mostly correspond to repetitions and hesitations made by the candidate."
  - [corpus] Found 5 related papers with average FMR=0.459, indicating moderate relatedness to ASR foundation models for L2 assessment, but no direct evidence of Whisper's training data composition.
- Break condition: If Whisper were trained on raw spoken transcripts with disfluencies preserved, the need for adaptation would disappear.

### Mechanism 2
- Claim: Fine-tuning Whisper on domain-specific data can "undo" the ITN behavior and restore spoken artifacts like hesitations and repetitions.
- Mechanism: By updating all model parameters on in-domain data with raw spoken transcripts, the model learns to generate less normalized output that preserves disfluencies.
- Core assumption: The domain-specific data contains the target spoken artifacts that should be generated.
- Evidence anchors:
  - [abstract] "One way to address this problem is to add an additional text processing step for the ASR output... we propose two methods to 'undo' the inherent post-processing within the Whisper model using small amounts of training data. The first method is model fine-tuning..."
  - [section] "In training, only these parameters V are updated based on the loss function while keeping the original model parameters, θASR, fixed."
  - [corpus] No direct corpus evidence about fine-tuning effectiveness, but related work on ASR adaptation suggests parameter updates improve domain performance.
- Break condition: If fine-tuning overfits to the small training set or if the training data lacks sufficient spoken artifacts, performance gains would be limited.

### Mechanism 3
- Claim: Soft prompt tuning (SPT) can achieve comparable performance to fine-tuning while only updating a small number of prompt embeddings.
- Mechanism: Prepending learnable soft prompt vectors to the decoder input provides conditioning information that guides generation toward preserving spoken artifacts, without modifying the foundation model parameters.
- Core assumption: The decoder captures domain information from reference text, so conditioning at the decoder input is more effective than encoder input.
- Evidence anchors:
  - [abstract] "In addition, we propose a novel soft prompt tuning approach that is parameter-efficient. A few soft prompt embeddings are inserted in the decoder input and learned on the training data, leaving the original model parameters unchanged."
  - [section] "For SPT, we run experiments with 1, 5, 20, or 100 soft prompts added. Models with 20 soft prompts perform the best in general..."
  - [corpus] Related papers show soft prompting effectiveness for LLMs, but no direct evidence for ASR models or specific to spoken language assessment.
- Break condition: If the prompt embeddings cannot capture the necessary conditioning information or if the model's architecture limits decoder-side adaptation.

## Foundational Learning

- Concept: Text normalization in ASR systems
  - Why needed here: Understanding how Whisper applies ITN (inverse text normalization) to produce human-readable output is crucial for identifying why it's unsuitable for L2 assessment.
  - Quick check question: What are the common text normalization operations applied in ASR systems, and how do they affect the preservation of spoken artifacts?

- Concept: Fine-tuning vs prompt tuning in NLP
  - Why needed here: The paper contrasts these two adaptation approaches, and understanding their differences helps evaluate the proposed SPT method.
  - Quick check question: What are the key differences between fine-tuning all model parameters and updating only prompt embeddings, and what are the trade-offs?

- Concept: Word error rate (WER) calculation with different normalization schemes
  - Why needed here: The paper uses three WER variants (Raw, Standard, Speech) to evaluate performance, and understanding these metrics is essential for interpreting results.
  - Quick check question: How does the choice of text normalization before WER calculation affect the evaluation of ASR systems for L2 assessment?

## Architecture Onboarding

- Component map: Audio input → Whisper encoder → decoder (with prompt conditioning) → text output → WER evaluation with Speech metric
- Critical path: Input audio → encoder → decoder (with prompt conditioning) → text output → WER evaluation with Speech metric
- Design tradeoffs: Fine-tuning offers potentially better performance but requires updating 244MB parameters; SPT is parameter-efficient (15KB) but may have slightly lower performance.
- Failure signatures: Poor performance on hesitations/repetitions indicates ITN behavior persists; degradation on out-of-domain data suggests overfitting; high deletion errors indicate model still skips disfluencies.
- First 3 experiments:
  1. Run baseline Whisper decoding on Linguaskill test set to verify deletion errors and ITN behavior.
  2. Apply fine-tuning with 1 hour of Linguaskill data and evaluate Speech WER improvement.
  3. Apply soft prompt tuning with 1 hour of Linguaskill data using m=20 prompts and compare to fine-tuning results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the soft prompt tuning approach perform when applied to other ASR foundation models beyond Whisper?
- Basis in paper: [explicit] The paper proposes soft prompt tuning for Whisper and shows its effectiveness, but does not test it on other ASR models.
- Why unresolved: The study focuses solely on Whisper, leaving the generalizability of soft prompt tuning to other ASR models unexplored.
- What evidence would resolve it: Experiments applying soft prompt tuning to other ASR foundation models (e.g., wav2vec 2.0, Google USM) and comparing their performance to fine-tuning and baseline models.

### Open Question 2
- Question: What is the optimal number of soft prompt tokens (m) for different ASR tasks and model sizes?
- Basis in paper: [explicit] The paper tests m=20 soft prompts and finds it performs best, but does not explore the impact of varying m for different tasks or model sizes.
- Why unresolved: The study uses a fixed m=20 for all experiments, not exploring how the optimal number of soft prompts might vary based on task complexity or model architecture.
- What evidence would resolve it: A systematic study varying m across different ASR tasks (e.g., language assessment, transcription) and model sizes, analyzing the trade-off between performance and parameter efficiency.

### Open Question 3
- Question: How does soft prompt tuning compare to other parameter-efficient fine-tuning methods (e.g., adapter tuning, LoRA) for ASR foundation models?
- Basis in paper: [explicit] The paper introduces soft prompt tuning as a parameter-efficient alternative to fine-tuning, but does not compare it to other parameter-efficient methods.
- Why unresolved: The study focuses on comparing soft prompt tuning to full fine-tuning, leaving a comparison with other parameter-efficient methods unexplored.
- What evidence would resolve it: Experiments comparing soft prompt tuning to adapter tuning, LoRA, and other parameter-efficient fine-tuning methods on the same ASR tasks, analyzing their performance and parameter efficiency.

### Open Question 4
- Question: Can soft prompt tuning be combined with other techniques (e.g., data augmentation, curriculum learning) to further improve ASR performance for language assessment?
- Basis in paper: [inferred] The paper proposes soft prompt tuning as an effective method for adapting ASR models to language assessment, but does not explore combining it with other techniques.
- Why unresolved: The study focuses solely on soft prompt tuning, not investigating potential synergies with other techniques that could further enhance ASR performance.
- What evidence would resolve it: Experiments combining soft prompt tuning with data augmentation techniques (e.g., SpecAugment, speed perturbation) and curriculum learning strategies, analyzing their impact on ASR performance for language assessment tasks.

## Limitations

- The paper assumes Whisper's ITN behavior is fundamentally baked into the foundation model rather than learned from training data, but this has not been empirically verified.
- The soft prompt tuning approach lacks strong theoretical justification for why decoder-side conditioning would be more effective than encoder-side conditioning for this specific task.
- The optimal number of soft prompt tokens (20) appears somewhat arbitrary without systematic ablation studies across different tasks and model sizes.

## Confidence

- **High Confidence**: The observation that Whisper's output is unsuitable for L2 assessment due to punctuation addition, number normalization, and disfluency omission is well-supported by the experimental results showing 15.4% to 82.9% word recall improvement.
- **Medium Confidence**: The claim that soft prompt tuning achieves comparable performance to fine-tuning while using far fewer parameters is supported by the experimental comparison, though the optimal number of prompts (20) appears somewhat arbitrary without ablation studies.
- **Low Confidence**: The assumption that Whisper's ITN behavior is fundamentally baked into the foundation model rather than being learnable from the training data remains unverified and could affect the generalizability of the proposed solutions.

## Next Checks

1. **Data Provenance Analysis**: Analyze the actual Whisper training corpus composition to determine whether ITN behavior is learned from data or inherent to the model architecture. This would involve examining whether the training data contained raw spoken transcripts with disfluencies or already normalized text.

2. **Cross-Architecture Validation**: Test whether other ASR foundation models with different architectures (not just Whisper) exhibit the same ITN behavior when trained on similar datasets, to determine if this is a foundation model issue or specific to Whisper's design.

3. **Prompt Location Ablation**: Compare soft prompt tuning effectiveness when prompts are inserted at encoder input versus decoder input to validate the claim that decoder-side conditioning is more effective for this task, and to understand the architectural reasons for this difference.