---
ver: rpa2
title: 'Take an Irregular Route: Enhance the Decoder of Time-Series Forecasting Transformer'
arxiv_id: '2312.05792'
source_url: https://arxiv.org/abs/2312.05792
tags:
- fppformer
- forecasting
- decoder
- sequence
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving the decoder architecture
  in time-series forecasting transformers (TSFTs). The authors argue that while much
  research has focused on enhancing the encoder, the decoder has been relatively neglected,
  even though it plays a crucial role in constructing the prediction sequence.
---

# Take an Irregular Route: Enhance the Decoder of Time-Series Forecasting Transformer

## Quick Facts
- arXiv ID: 2312.05792
- Source URL: https://arxiv.org/abs/2312.05792
- Reference count: 40
- Primary result: Introduces FPPformer, a time-series transformer that enhances decoder architecture with top-down feature pyramid approach, achieving state-of-the-art performance on twelve benchmarks

## Executive Summary
This paper addresses the neglected area of decoder architecture in time-series forecasting transformers (TSFTs). While most research has focused on improving encoders, the authors argue that decoders play a crucial role in constructing prediction sequences. They propose FPPformer, which enhances the decoder using a top-down architecture inspired by feature pyramid networks from computer vision. The model combines patch-wise and element-wise attention mechanisms in both encoder and decoder, with a diagonal-masked self-attention in the encoder to mitigate outliers. Extensive experiments demonstrate FPPformer outperforms state-of-the-art TSFTs and time-series forecasting MLPs across twelve benchmarks, particularly for longer input sequences.

## Method Summary
FPPformer employs a 3-stage encoder and decoder architecture with combined patch-wise and element-wise attention mechanisms. The encoder uses element-wise attention followed by patch-wise attention at each stage, with diagonal-masked self-attention to reduce outlier effects. The decoder reverses this order, starting with patch-wise attention in the cross-attention block and using element-wise self-attention. The model uses direct forecasting with RevIN normalization and channel-independent embedding. Training uses Adam optimizer with learning rate 1e-4 (decayed by 0.5 per epoch), batch size 16, dropout 0.1, for 10 epochs.

## Key Results
- FPPformer achieves state-of-the-art performance on twelve benchmarks including ETTh1, ETTh2, ETTm1, ETTm2, ECL, Traffic, Weather, Solar, and M4 datasets
- Significant improvements in forecasting accuracy, especially for longer input sequences
- Outperforms both existing TSFTs and time-series forecasting MLPs (TSFMs) across multiple metrics (MSE, MAE, SMAPE, OWA)
- Ablation studies validate the effectiveness of proposed mechanisms including top-down decoder architecture and diagonal-masked attention

## Why This Works (Mechanism)

### Mechanism 1: Diagonal-Masked Self-Attention for Outlier Mitigation
- **Claim**: Diagonal-masking reduces outlier influence by preventing self-matches in attention matrix
- **Mechanism**: Masks diagonal elements in query-key matching matrix, forcing each element/patch to be represented only by others
- **Core assumption**: Outliers are self-contained and cannot be effectively represented by normal elements
- **Evidence anchors**: Abstract mentions masking ensures generality of features; section explains outliers are impossible to be expressed by normal elements
- **Break condition**: If outliers require self-matching for proper representation, diagonal masking could degrade performance

### Mechanism 2: Top-Down Decoder Architecture
- **Claim**: Hierarchically constructs prediction sequence from coarse to fine features
- **Mechanism**: Starts with deepest encoder feature map (most universal features) and progressively splits patches and refines features
- **Core assumption**: Time series forecasting benefits from coarse-to-fine construction to ensure general characteristics before fine details
- **Evidence anchors**: Abstract compares to FPN/PAN architectures; section explains placing patch-wise attention before element-wise in decoder
- **Break condition**: If input features aren't hierarchical or prediction task doesn't benefit from coarse-to-fine construction

### Mechanism 3: Combined Element-Wise and Patch-Wise Attention
- **Claim**: Provides both fine-grained inner-patch feature extraction and efficient inter-patch correlation modeling
- **Mechanism**: Element-wise attention within patches followed by patch-wise attention between patches (reversed in decoder)
- **Core assumption**: Patch-wise alone insufficient for inner-patch relationships; element-wise alone inefficient for long sequences
- **Evidence anchors**: Abstract mentions revamping element-wise attention; section explains element-wise is patch-independent with O(L×P) complexity
- **Break condition**: If computational overhead outweighs benefits or problem doesn't require fine-grained inner-patch extraction

## Foundational Learning

- **Attention mechanism in transformers**: FPPformer relies heavily on attention mechanisms for feature extraction. Understanding scaled dot-product attention (Attention(Q,K,V) = Softmax(QK^T/√d)V) is fundamental.
  - Why needed: Core to understanding how the model processes input and constructs predictions
  - Quick check: How does the attention formula compute weighted sum of values based on query-key similarity?

- **Hierarchical feature extraction and reconstruction**: FPPformer employs bottom-up (encoder) and top-down (decoder) architecture inspired by FPN.
  - Why needed: Crucial for grasping the model's design philosophy of extracting features from fine to coarse and constructing predictions from coarse to fine
  - Quick check: How does FPN's bottom-up and top-down architecture enable efficient feature extraction in computer vision, and how is this applied to time series forecasting?

- **Outlier detection and mitigation in time series**: FPPformer includes diagonal-masked attention to mitigate outlier effects.
  - Why needed: Important for understanding this feature of the model
  - Quick check: What are common techniques for detecting and mitigating outliers in time series, and how does diagonal-masked attention compare?

## Architecture Onboarding

- **Component map**: Input sequence → Embedding → Encoder (bottom-up hierarchy with diagonal-masked self-attention and combined element-wise/patch-wise attention) → Decoder (top-down hierarchy with cross-attention and combined attention in reverse order) → Projection → Prediction sequence
- **Critical path**: Input sequence → Encoder → Decoder → Projection
- **Design tradeoffs**: Combining attention mechanisms increases complexity but improves feature extraction; top-down decoder improves construction but adds complexity; diagonal-masking mitigates outliers but may mask useful self-information
- **Failure signatures**: If model fails to capture long-term dependencies, attention mechanisms or hierarchy depth may need adjustment; if overfitting, reduce complexity; if sensitive to outliers, adjust diagonal-masking
- **First 3 experiments**:
  1. Test on ETTh1 with varying input sequence lengths to assess impact of sequence length
  2. Compare full FPPformer with ablated versions (without diagonal-masking, without element-wise attention, with linear decoder)
  3. Visualize attention scores and feature maps at different encoder/decoder stages

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does FPPformer's top-down decoder compare to other hierarchical decoder designs in terms of computational efficiency and forecasting accuracy across diverse time-series datasets?
- **Basis**: Paper contrasts with Crossformer's bottom-up decoder and highlights benefits of coarse-to-fine construction
- **Why unresolved**: Paper provides comparisons but no comprehensive study isolating decoder architecture impact
- **Evidence needed**: Experiments ablating only decoder architecture while keeping other components constant across multiple models and datasets

### Open Question 2
- **Question**: What is the optimal balance between patch-wise and element-wise attention mechanisms for different types of time-series data?
- **Basis**: Paper introduces combination but doesn't explore sensitivity to relative weighting or applicability across data types
- **Why unresolved**: Ablation study shows both beneficial but doesn't explore optimal ratios or dataset-specific tuning
- **Evidence needed**: Systematic experiments varying relative importance across diverse datasets with different characteristics

### Open Question 3
- **Question**: How effective is diagonal-masked attention at handling outliers compared to other anomaly mitigation techniques?
- **Basis**: Paper introduces diagonal-masked attention but doesn't compare to alternatives like preprocessing or robust loss functions
- **Why unresolved**: Demonstrates effectiveness but doesn't benchmark against other outlier handling methods
- **Evidence needed**: Head-to-head comparisons against preprocessing-based outlier removal and robust loss functions on datasets with known anomalies

## Limitations

- Diagonal-masked self-attention effectiveness lacks empirical validation through ablation studies or comparison with alternative outlier mitigation techniques
- Top-down decoder architecture superiority presented without comprehensive ablation studies isolating its contribution from other components
- Computational efficiency claims based on theoretical complexity analysis rather than empirical runtime measurements
- Performance improvements on longer sequences demonstrated but diminishing returns and overfitting risks not thoroughly explored

## Confidence

- **High Confidence**: Overall experimental methodology and benchmark selection are rigorous with twelve diverse datasets and state-of-the-art comparisons; implementation of standard transformer components is well-established
- **Medium Confidence**: Architectural innovations are logically sound and supported by qualitative explanations but lack comprehensive ablation studies isolating individual contributions
- **Low Confidence**: Claims about outlier mitigation effectiveness and computational efficiency are primarily theoretical without sufficient empirical validation

## Next Checks

1. Conduct ablation studies to isolate diagonal-masking contribution by comparing FPPformer with and without this mechanism on datasets known to contain outliers, measuring both performance and outlier sensitivity

2. Perform runtime complexity analysis on representative datasets to empirically verify claimed computational efficiency of combined attention mechanism compared to alternatives

3. Test model's sensitivity to hyperparameters (stage depth, patch size, learning rate) across twelve benchmarks to identify potential overfitting risks and establish robustness bounds