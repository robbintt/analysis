---
ver: rpa2
title: 'SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds'
arxiv_id: '2306.00980'
source_url: https://arxiv.org/abs/2306.00980
tags:
- step
- distillation
- diffusion
- steps
- unet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SnapFusion, a text-to-image diffusion model
  that can run on mobile devices in under 2 seconds. The authors propose an efficient
  UNet architecture and improved step distillation to achieve this speed.
---

# SnapFusion: Text-to-Image Diffusion Model on Mobile Devices within Two Seconds

## Quick Facts
- arXiv ID: 2306.00980
- Source URL: https://arxiv.org/abs/2306.00980
- Authors: 
- Reference count: 40
- One-line result: Text-to-image diffusion model achieving <2s inference on mobile devices

## Executive Summary
This paper presents SnapFusion, an optimized text-to-image diffusion model designed for mobile deployment with sub-2-second inference time. The authors introduce an efficient UNet architecture by identifying and removing redundant components, enhance step distillation techniques with classifier-free guidance regularization, and compress the VAE decoder using data distillation. Extensive experiments demonstrate that their 8-step model achieves superior FID and CLIP scores compared to Stable Diffusion v1.5 running at 50 steps, while being 4-5x faster on mobile devices.

## Method Summary
SnapFusion optimizes text-to-image diffusion models for mobile deployment through three key innovations: (1) an efficient UNet architecture created by identifying and removing redundant cross-attention and ResNet blocks using robust training with stochastic forward propagation, (2) enhanced step distillation that reduces denoising steps from 32 to 8 while maintaining quality through CFG-aware training strategies, and (3) compressed VAE decoder using data distillation that generates synthetic latent-image pairs without requiring paired text-image data. The approach achieves 4-5x speedup compared to Stable Diffusion v1.5 while improving both FID and CLIP scores on MS-COCO.

## Key Results
- Achieves <2 second inference time on mobile devices
- 8-step model outperforms Stable Diffusion v1.5 (50 steps) on FID and CLIP scores
- 4-5x faster inference than Stable Diffusion v1.5
- Maintains high-quality image generation while significantly reducing computational requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Identifying and removing redundancies in the UNet architecture improves inference speed without significantly degrading output quality.
- **Mechanism**: The original UNet contains cross-attention and ResNet blocks that can be removed based on their impact on latency and generative performance. Cross-attention blocks are responsible for semantic coherency, while ResNet blocks capture local details. Removing less valuable blocks reduces computation.
- **Core assumption**: Removing blocks from a pre-trained UNet degrades performance, but robust training with stochastic forward propagation makes the network tolerant to such permutations.
- **Evidence anchors**:
  - [abstract] "we propose an efficient UNet by identifying the redundancy of the original model"
  - [section] "The model with robust training maintains reasonable performance after dropping blocks" (Fig. 5)
  - [corpus] Weak: No direct corpus evidence found for this specific mechanism.
- **Break condition**: If removed blocks are critical for maintaining semantic coherency or local details, output quality degrades beyond acceptable levels.

### Mechanism 2
- **Claim**: Step distillation reduces the number of denoising steps required while maintaining or improving output quality.
- **Mechanism**: A pre-trained teacher model at 32 steps is used to distill a student model that runs at 8 steps. The student learns to predict the teacher's output in a single step by minimizing a distillation loss that compares the student's predicted clean latent to the teacher's predicted latent after two steps.
- **Core assumption**: A well-trained teacher model can provide high-quality supervision to guide the student in learning to produce similar outputs in fewer steps.
- **Evidence anchors**:
  - [abstract] "we enhance the step distillation by exploring training strategies"
  - [section] "The student UNet is supposed to predict the teacher's noisy latent zt′′ from zt with just one denoising step" (Eq. 9)
  - [corpus] Weak: No direct corpus evidence found for this specific mechanism.
- **Break condition**: If the student cannot learn to accurately predict the teacher's output in fewer steps, output quality degrades significantly.

### Mechanism 3
- **Claim**: Classifier-free guidance (CFG) distillation improves the CLIP score of the distilled model by incorporating guidance during training.
- **Mechanism**: Both teacher and student models apply CFG during distillation. The student learns to produce outputs that match the teacher's guided outputs across a range of guidance scales. This is achieved by using a CFG-aware distillation loss that incorporates guided predictions.
- **Core assumption**: Incorporating CFG during training allows the student to learn to produce outputs that align well with the guidance, improving the CLIP score.
- **Evidence anchors**:
  - [abstract] "introducing regularization from classifier-free guidance"
  - [section] "We propose to perform classifier-free guidance to both the teacher and student before calculating the loss" (Sec. 4.2)
  - [corpus] Weak: No direct corpus evidence found for this specific mechanism.
- **Break condition**: If the student cannot learn to effectively incorporate guidance, the CLIP score does not improve or may even degrade.

## Foundational Learning

- **Concept**: Denoising diffusion probabilistic models
  - **Why needed here**: Understanding the basic diffusion process is crucial for grasping how the UNet works and how step distillation reduces the number of denoising steps.
  - **Quick check question**: What is the purpose of the denoising process in diffusion models, and how does it gradually convert noisy data into clean data?

- **Concept**: Cross-attention mechanism
  - **Why needed here**: Cross-attention blocks in the UNet integrate text embeddings into spatial features. Understanding their role is essential for identifying redundancies and optimizing the architecture.
  - **Quick check question**: How does the cross-attention mechanism in the UNet integrate text information into the denoising process, and why is it computationally expensive?

- **Concept**: Classifier-free guidance
  - **Why needed here**: CFG is used during distillation to improve the CLIP score. Understanding how CFG works and its impact on output quality is crucial for implementing and tuning the CFG-aware distillation loss.
  - **Quick check question**: How does classifier-free guidance influence the trade-off between output quality and diversity, and why is it beneficial to incorporate it during training?

## Architecture Onboarding

- **Component map**: Text encoder (ViT-H) -> UNet (8 steps) -> VAE decoder -> Output image
- **Critical path**: Text prompt → Text encoder → UNet (8 steps) → VAE decoder → Output image
  - The UNet is the most computationally intensive component, and optimizing it is critical for achieving fast inference.

- **Design tradeoffs**:
  - UNet optimization: Removing blocks reduces latency but may degrade output quality. Robust training helps mitigate this trade-off.
  - Step distillation: Reducing steps improves speed but may also degrade quality. CFG-aware distillation helps maintain quality.
  - VAE decoder compression: Reduces latency but may impact output quality. Distillation helps maintain quality.

- **Failure signatures**:
  - Output quality degradation: May indicate that too many blocks were removed from the UNet or that step distillation was not effective.
  - Increased latency: May indicate that the optimized UNet is not as efficient as expected or that the VAE decoder compression was not effective.
  - Training instability: May indicate issues with the distillation process or the CFG-aware loss function.

- **First 3 experiments**:
  1. **UNet block removal**: Systematically remove cross-attention and ResNet blocks from the UNet and evaluate the impact on latency and output quality using a small validation set.
  2. **Step distillation**: Implement step distillation with different teacher-student step configurations and evaluate the impact on output quality and inference speed.
  3. **CFG-aware distillation**: Implement CFG-aware distillation and compare its impact on CLIP score and FID to vanilla distillation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the model perform if the text encoder were compressed to reduce parameters?
- Basis in paper: [inferred] The paper states that the text encoder is negligible in inference latency (4ms) compared to UNet and VAE Decoder, and is not compressed in the released pipeline.
- Why unresolved: The authors explicitly chose not to compress the text encoder due to its minimal impact on latency. The effect of compressing it on model quality and speed is unknown.
- What evidence would resolve it: Empirical results comparing model performance (FID, CLIP score, latency) with and without text encoder compression.

### Open Question 2
- Question: How does the model's performance change when trained on different datasets?
- Basis in paper: [explicit] The authors train their models on public datasets [50, 39] and report results on MS-COCO 2014 validation set. They also collect an internal dataset with high-resolution images to fine-tune their model for more pleasing visual quality.
- Why unresolved: The paper only reports results on MS-COCO 2014 validation set. The model's generalization ability to other datasets is unknown.
- What evidence would resolve it: Empirical results showing model performance (FID, CLIP score, qualitative samples) on a diverse set of image-caption datasets.

### Open Question 3
- Question: What is the impact of using different teacher models for step distillation?
- Basis in paper: [explicit] The authors experiment with different teacher options for step distillation, including self-distillation from their 16-step efficient UNet, distillation from the 16-step SD-v1.5 baseline model, and training a CFG-aware distilled 16-step SD-v1.5 model.
- Why unresolved: While the authors find that using a 16-step teacher model achieves the best CLIP score, they do not explore other teacher models or architectures. The effect of using different teacher models on model performance is unknown.
- What evidence would resolve it: Empirical results comparing model performance (FID, CLIP score) when using different teacher models or architectures for step distillation.

## Limitations

- The block removal approach relies heavily on empirical ablation studies without theoretical guarantees about which blocks are truly redundant
- Data distillation for VAE compression may introduce artifacts not captured in reported metrics
- Robust training framework's generalization ability across diverse text prompts and image distributions remains unproven

## Confidence

- **High Confidence**: The sub-2-second inference claim on mobile devices (measured results provided)
- **Medium Confidence**: The effectiveness of CFG-aware distillation for improving CLIP scores (shown empirically but mechanism not fully explained)
- **Low Confidence**: The robustness training framework's ability to generalize block removal across different types of text prompts and image distributions

## Next Checks

1. **Robustness Validation**: Test the efficient UNet architecture across diverse text prompts (abstract concepts, complex scenes, rare objects) to verify that block removal doesn't create blind spots in generation capability.

2. **Cross-Dataset Generalization**: Evaluate the distilled models on datasets beyond MS-COCO (e.g., LAION-5B, Conceptual Captions) to assess whether the optimization strategies transfer to different data distributions.

3. **Long-term Stability Analysis**: Run extended inference tests (1000+ generations) on mobile devices to identify potential memory leaks or performance degradation that may emerge over prolonged usage.