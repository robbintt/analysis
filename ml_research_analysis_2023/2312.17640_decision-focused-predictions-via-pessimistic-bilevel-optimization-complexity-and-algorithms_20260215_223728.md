---
ver: rpa2
title: 'Decision-focused predictions via pessimistic bilevel optimization: complexity
  and algorithms'
arxiv_id: '2312.17640'
source_url: https://arxiv.org/abs/2312.17640
tags:
- optimization
- regret
- problem
- pessimistic
- bilevel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses decision-focused learning by minimizing regret
  in optimization tasks with uncertain parameters. It formulates the problem as a
  pessimistic bilevel optimization model, showing computational complexity results
  and reformulating it as a non-convex quadratic program.
---

# Decision-focused predictions via pessimistic bilevel optimization: complexity and algorithms

## Quick Facts
- **arXiv ID**: 2312.17640
- **Source URL**: https://arxiv.org/abs/2312.17640
- **Reference count**: 40
- **Key outcome**: Decision-focused learning via pessimistic bilevel optimization achieves up to 50% regret reduction compared to linear regression and SPO+ baselines on synthetic grid graph instances.

## Executive Summary
This paper addresses decision-focused learning by minimizing regret in optimization tasks with uncertain parameters. The authors formulate the problem as a pessimistic bilevel optimization model, where the goal is to choose predictions that minimize the worst-case regret among all optimal solutions of the lower-level problem. They show computational complexity results and reformulate the problem as a non-convex quadratic program (QCQP), enabling the use of advanced solvers for moderate-sized instances. The proposed approach demonstrates improved training performance compared to state-of-the-art methods, particularly when starting with SPO+ solutions and applying local search heuristics.

## Method Summary
The method follows a three-step approach: (1) Train an initial predictive model using linear regression or SPO+ on synthetic data generated from grid graphs with uncertain cost vectors; (2) Apply a local search heuristic to improve the initial solution by exploring a neighborhood of parameter space; (3) Solve the reformulated QCQP problem exactly or with a penalized objective using Gurobi. The QCQP formulation leverages duality arguments to transform the pessimistic bilevel problem into a single-level non-convex optimization problem, which can be solved with state-of-the-art solvers for instances up to 5×5 grids with 200 observations.

## Key Results
- Regret reductions of up to 50% compared to linear regression and SPO+ baselines on training data
- Penalized QCQP formulation often outperforms the exact formulation in practice
- Local search heuristic starting from SPO+ solutions yields better results than starting from linear regression
- Method demonstrates generalization to test sets, though performance varies with model misspecification degree

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Reformulating pessimistic bilevel optimization as a non-convex quadratic program enables the use of advanced solvers for moderate-sized instances.
- **Mechanism**: The paper leverages duality arguments to transform the pessimistic bilevel problem into a single-level non-convex quadratic program (QCQP). This reformulation exploits strong duality and the boundedness of the feasible region to convert the inner max-min problem into a tractable form.
- **Core assumption**: The feasible region of the lower-level problem is non-empty, bounded, and unaffected by the optimization parameters.
- **Evidence anchors**: [section]: "we reformulate it as a non-convex quadratic optimization problem" and "we reformulate the bilevel pessimistic problem as a non-convex quadratically-constrained quadratic program (QCQP)"
- **Break condition**: The reformulation fails if the lower-level problem's feasible region is unbounded or if strong duality does not hold.

### Mechanism 2
- **Claim**: The pessimistic bilevel formulation ensures that the solution chosen is the one that maximizes regret among all optimal solutions of the lower-level problem.
- **Mechanism**: By taking the max over the optimal solutions of the lower-level problem, the formulation penalizes predictions that could lead to the worst-case regret, thus improving decision quality under uncertainty.
- **Core assumption**: Multiple optimal solutions exist at the lower level, and their regret can be meaningfully compared.
- **Evidence anchors**: [abstract]: "pessimistic bilevel optimization model" and "pessimistic version of the bilevel formulation"
- **Break condition**: If the lower-level problem has a unique optimal solution, the pessimistic aspect becomes irrelevant.

### Mechanism 3
- **Claim**: The local search heuristic improves initial solutions by exploring a neighborhood of parameter space and selecting the one with the lowest regret.
- **Mechanism**: Starting from an initial solution (e.g., from linear regression or SPO+), the algorithm generates multiple perturbed parameter sets, evaluates their regret, and iteratively updates the incumbent solution.
- **Core assumption**: The regret function is smooth enough that local perturbations can lead to improvements.
- **Evidence anchors**: [section]: "we propose the following local-search-based heuristic" and "we randomly generate T new solutions in a neighborhood of ω0"
- **Break condition**: If the regret landscape is highly non-convex or has many local minima, local search may get stuck.

## Foundational Learning

- **Concept**: Bilevel optimization and pessimistic vs. optimistic formulations
  - **Why needed here**: The problem is inherently bilevel, and the choice between pessimistic and optimistic approaches significantly impacts the regret minimization.
  - **Quick check question**: What is the difference between pessimistic and optimistic bilevel optimization, and why does the paper choose pessimistic?

- **Concept**: Duality in linear programming
  - **Why needed here**: The reformulation from bilevel to QCQP relies on strong duality and KKT conditions.
  - **Quick check question**: How does strong duality allow the conversion of a max-min problem into a single-level optimization?

- **Concept**: Non-convex quadratic programming
  - **Why needed here**: The final reformulation is a non-convex QCQP, requiring understanding of how to solve such problems and their computational challenges.
  - **Quick check question**: What makes a quadratic program non-convex, and why are such problems harder to solve?

## Architecture Onboarding

- **Component map**: Data generation -> Predictive model (LR/SPO+) -> Regret evaluator -> Local search -> QCQP solver -> Experiment runner

- **Critical path**:
  1. Generate training data with uncertain parameters.
  2. Train initial predictive model (LR or SPO+).
  3. Apply local search to improve the initial solution.
  4. Solve the penalized or exact QCQP formulation.
  5. Evaluate regret on training and test sets.

- **Design tradeoffs**:
  - Exact vs. penalized QCQP: Exact formulation is more accurate but harder to solve; penalized is restricted but often performs better.
  - Linear regression vs. SPO+ as starting point: SPO+ gives lower initial regret but LR can generalize better on test sets.
  - Local search parameters (ϵ, T, L): Tradeoff between exploration and computational cost.

- **Failure signatures**:
  - Solver timeouts or infeasibility in QCQP: May indicate poor initialization or overly tight constraints.
  - Worse test regret than training regret: Suggests overfitting, especially after local search.
  - No improvement after local search: Could mean the initial solution is already near-optimal or the regret landscape is flat.

- **First 3 experiments**:
  1. Generate a small synthetic dataset (N=50, Deg=2) and compare regrets of LR, SPO+, and SPO+ + local search.
  2. Run the penalized QCQP with warm start from SPO+ + local search and compare to exact QCQP.
  3. Test generalization by evaluating regrets on a separate test set after each improvement step.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the proposed methods be scaled to handle larger instances with more nodes and observations?
- **Basis in paper**: [inferred] The authors mention that their current work focuses on 5x5 grids with up to 200 observations, and they express a desire to scale their approach further in these dimensions.
- **Why unresolved**: The paper does not provide a detailed analysis of the computational complexity or scalability of their methods. The authors only mention that they want to improve scalability in future work.
- **What evidence would resolve it**: Empirical results showing the performance of the proposed methods on larger instances with more nodes and observations, along with a theoretical analysis of the computational complexity and scalability of the methods.

### Open Question 2
- **Question**: How can custom valid inequalities be derived to improve the performance of the non-convex quadratic solver?
- **Basis in paper**: [inferred] The authors mention that they want to derive custom valid inequalities to improve the bounds and accelerate the convergence of the non-convex solver in future work.
- **Why unresolved**: The paper does not provide any specific valid inequalities or a detailed analysis of how they could improve the solver's performance. The authors only express their intention to explore this direction in future work.
- **What evidence would resolve it**: The development and implementation of custom valid inequalities, along with empirical results demonstrating their impact on the solver's performance and convergence.

### Open Question 3
- **Question**: How do the proposed methods compare to other state-of-the-art approaches in the literature, such as those based on smooth optimization or surrogate losses?
- **Basis in paper**: [explicit] The authors mention that they want to compare their methods with other state-of-the-art approaches in the literature, such as those based on smooth optimization or surrogate losses, in future work.
- **Why unresolved**: The paper only presents a comparison with the SPO+ method and linear regression. The authors do not provide any experimental results or theoretical analysis comparing their methods to other state-of-the-art approaches.
- **What evidence would resolve it**: Experimental results comparing the proposed methods to other state-of-the-art approaches, along with a theoretical analysis of the strengths and weaknesses of each approach in different scenarios.

## Limitations

- Computational scalability limited to small instances (5×5 grids with up to 200 observations)
- No comparison to other pessimistic bilevel optimization approaches in the literature
- Limited exploration of alternative uncertainty sets and their impact on performance

## Confidence

- **High Confidence**: The computational complexity results (NP-hardness) are well-established through reduction arguments. The QCQP reformulation mechanism is sound when duality conditions hold.
- **Medium Confidence**: The experimental results showing regret reduction are promising but limited to small instances (grid graphs with 25 nodes). The local search heuristic's effectiveness is demonstrated but not rigorously analyzed.
- **Low Confidence**: The generalization performance across different problem types and larger instances remains unverified. The sensitivity to initialization parameters (ϵ, T, L) is not thoroughly explored.

## Next Checks

1. **Scalability Test**: Apply the QCQP reformulation to larger instances (e.g., 10×10 or 20×20 grid graphs) and evaluate solver performance and solution quality.
2. **Generalization Test**: Compare the proposed method's performance on non-grid graph structures (e.g., random geometric graphs or real-world networks) to assess robustness.
3. **Robustness Analysis**: Systematically vary the noise level (σ) and model misspecification degree (Deg) to understand the method's sensitivity to uncertainty characterization.