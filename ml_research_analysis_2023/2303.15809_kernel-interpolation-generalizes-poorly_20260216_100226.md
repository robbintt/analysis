---
ver: rpa2
title: Kernel interpolation generalizes poorly
arxiv_id: '2303.15809'
source_url: https://arxiv.org/abs/2303.15809
tags:
- kernel
- interpolation
- where
- have
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a lower bound on the generalization error\
  \ of kernel interpolation for a broad class of kernels. The main result shows that\
  \ under mild conditions, the generalization error of kernel interpolation is at\
  \ least \u03A9(n^{-\u03B5}) for any \u03B5 0, where n is the sample size."
---

# Kernel interpolation generalizes poorly

## Quick Facts
- arXiv ID: 2303.15809
- Source URL: https://arxiv.org/abs/2303.15809
- Reference count: 40
- Primary result: Kernel interpolation generalization error is at least Ω(n^{-ε}) for any ε > 0 under mild conditions

## Executive Summary
This paper establishes a lower bound on the generalization error of kernel interpolation for a broad class of kernels. The main result shows that under mild conditions, the generalization error of kernel interpolation is at least Ω(n^{-ε}) for any ε > 0, where n is the sample size. This implies that kernel interpolation generalizes poorly for this class of kernels. As a consequence, the paper shows that overfitted wide neural networks on spheres also generalize poorly. The proof relies on relating the generalization error to the variance term and then using operator theory and concentration inequalities to establish the lower bound.

## Method Summary
The paper analyzes kernel interpolation of the form ˆf_inter(x) = K(x, X)K(X, X)^{-1}Y, where K(X, X) is the normalized kernel matrix and Y are the target values. The analysis requires verifying that the kernel k satisfies continuity, positive definiteness, boundedness (sup_x k(x,x) ≤ κ²), and has embedding index α₀ = 1/β where β > 1 is the eigenvalue decay rate of the integral operator T. The method involves generating n samples from a model with non-vanishing noise, computing the kernel interpolation, and establishing that the generalization error is lower bounded by the variance term which cannot decay faster than n^{-ε}.

## Key Results
- Kernel interpolation's generalization error is lower bounded by Ω(n^{-ε}) for any ε > 0
- This poor generalization extends to overfitted wide neural networks on spheres
- The approximation between empirical and population variance terms holds for much smaller λ values than previously established

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Kernel interpolation's generalization error is lower bounded by Ω(n^{-ε}) for any ε > 0.
- **Mechanism:** The paper establishes that kernel interpolation's variance term cannot decay faster than n^{-ε}, and since generalization error is lower bounded by variance, the overall generalization error inherits this lower bound.
- **Core assumption:** The reproducing kernel Hilbert space (RKHS) has embedding index α₀ = 1/β, where β is the eigenvalue decay rate of the integral operator T.
- **Evidence anchors:**
  - [abstract]: "under mild conditions, we show that for any ε > 0, the generalization error of kernel interpolation is lower bounded by Ω(n− ε)."
  - [section 2.2]: Defines embedding index α₀ and states that many common RKHSs satisfy this condition.
  - [corpus]: Weak - neighboring papers discuss kernel interpolation but don't provide direct evidence for this specific mechanism.

### Mechanism 2
- **Claim:** The poor generalization of kernel interpolation extends to overfitted wide neural networks on spheres.
- **Mechanism:** Neural tangent kernels on spheres satisfy the same embedding index condition as the kernel class studied, so the generalization lower bound applies to the corresponding neural network interpolation.
- **Core assumption:** The neural tangent kernel on spheres has the same eigenvalue decay properties as the general kernel class considered.
- **Evidence anchors:**
  - [section 4]: "neural tangent kernel interpolation and the overfitted wide neural network generalized poorly"
  - [section 2.3]: Shows explicit connection between kernel interpolation and neural tangent kernel.
  - [corpus]: Weak - neighboring papers discuss neural networks and kernels but don't directly confirm this specific mechanism.

### Mechanism 3
- **Claim:** The approximation between empirical and population variance terms holds for much smaller λ values than previously established.
- **Mechanism:** By leveraging the embedding property, the paper shows that the concentration inequality approximation between empirical and population variance terms remains valid for λ as small as n^{-β+ε}, allowing the lower bound to be established.
- **Core assumption:** The kernel is Hölder-continuous and the noise is non-vanishing.
- **Evidence anchors:**
  - [section 5.3]: "we can show the above approximation (22) actually holds for λ ≍ n− β +ε for any ǫ > 0"
  - [section 3.3]: Discusses how this technical improvement allows the lower bound to be established.
  - [corpus]: Weak - neighboring papers don't provide evidence for this specific technical improvement.

## Foundational Learning

- **Concept: Reproducing Kernel Hilbert Space (RKHS)**
  - Why needed here: The entire analysis is built on the properties of RKHSs and their relationship to the integral operator T.
  - Quick check question: What is the significance of the Mercer decomposition in the context of kernel interpolation?

- **Concept: Integral Operator and Eigenvalue Decay**
  - Why needed here: The eigenvalue decay rate β determines the embedding index and is crucial for establishing the lower bound on generalization error.
  - Quick check question: How does the polynomial eigenvalue decay condition relate to the embedding index α₀?

- **Concept: Variance Term in Generalization Analysis**
  - Why needed here: The paper establishes that the generalization error is lower bounded by the variance term, which is the key technical step in proving the main result.
  - Quick check question: Why is the variance term easier to analyze than the full generalization error?

## Architecture Onboarding

- **Component map:** Kernel assumptions -> Eigenvalue decay analysis -> Operator theory results -> Concentration inequalities -> Lower bound proof -> Neural network extension

- **Critical path:** The critical path is establishing that the variance term V(λ) satisfies V(λ) ≥ cσ²n^{-1/β} for small λ, which requires both the eigenvalue decay and embedding index conditions.

- **Design tradeoffs:** The paper trades off generality (proving results for a broad class of kernels) against strength of conclusions (showing only a lower bound, not an upper bound on generalization error).

- **Failure signatures:** If the embedding index condition is not satisfied, or if the eigenvalue decay is not polynomial, the main result does not apply. Similarly, if the kernel is not Hölder-continuous, the technical concentration results may fail.

- **First 3 experiments:**
  1. Verify the eigenvalue decay condition for a specific kernel (e.g., Sobolev space kernel) by computing or estimating the eigenvalues of the integral operator.
  2. Check the Hölder continuity condition for a neural tangent kernel on spheres by examining its smoothness properties.
  3. Numerically estimate the variance term V(λ) for a small sample size to confirm the theoretical lower bound.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the poor generalization of kernel interpolation extend to other types of kernels not covered by the embedding index condition?
- Basis in paper: [explicit] The paper shows poor generalization for a class of kernels with embedding index α₀ = 1/β, which includes many common kernels like Sobolev spaces, translation invariant periodic kernels, and dot-product kernels on spheres.
- Why unresolved: The paper only proves the result for kernels with a specific eigenvalue decay and embedding index condition. It's unclear if similar results hold for kernels with different properties.
- What evidence would resolve it: Proving or disproving the lower bound on generalization error for a broader class of kernels, or finding counterexamples of kernels with good generalization despite not satisfying the embedding index condition.

### Open Question 2
- Question: Can the generalization error bound be improved for specific kernels or under additional assumptions?
- Basis in paper: [inferred] The paper provides a lower bound of Ω(n^{-ε}) for any ε > 0, but this is a worst-case bound. It's possible that for certain kernels or under additional assumptions, the generalization error could be better.
- Why unresolved: The paper focuses on establishing a general lower bound that applies to a wide class of kernels. Improving the bound for specific cases would require additional analysis and potentially new techniques.
- What evidence would resolve it: Proving tighter upper bounds on the generalization error for specific kernels or under additional assumptions, or showing that the Ω(n^{-ε}) bound is tight for certain kernels.

### Open Question 3
- Question: How does the generalization error of kernel interpolation compare to other kernel methods like kernel ridge regression?
- Basis in paper: [explicit] The paper shows that the generalization error of kernel interpolation is lower bounded by the variance term, which is also present in kernel ridge regression. However, the paper doesn't directly compare the two methods.
- Why unresolved: The paper establishes that kernel interpolation generalizes poorly, but it doesn't investigate how this compares to other kernel methods that include regularization.
- What evidence would resolve it: Empirical or theoretical comparisons of the generalization error of kernel interpolation and kernel ridge regression for various kernels and sample sizes, or proving that the poor generalization of kernel interpolation is a fundamental limitation that also affects other kernel methods.

## Limitations
- The paper establishes a lower bound on generalization error without providing corresponding upper bounds
- The analysis requires specific technical conditions (embedding index α₀ = 1/β, Hölder continuity, polynomial eigenvalue decay) that may not hold for all kernels of interest
- The extension to neural networks relies on the neural tangent kernel satisfying the same embedding properties

## Confidence
- **High Confidence**: The mathematical framework and operator theory results are rigorous and well-established. The connection between variance terms and generalization error is standard.
- **Medium Confidence**: The technical improvements allowing smaller λ values are plausible but require careful verification. The neural network extension follows logically from the kernel results but depends on unverified empirical properties of NTKs.
- **Low Confidence**: The practical implications for real-world neural network training remain unclear, as the analysis focuses on interpolation regimes and specific data distributions.

## Next Checks
1. **Eigenvalue Decay Verification**: Compute the eigenvalues of the integral operator for specific kernels (e.g., Sobolev space kernels on spheres) to empirically verify the polynomial decay rate β claimed in the analysis.

2. **Variance Term Estimation**: For small sample sizes (n ≤ 100), numerically compute the empirical variance term V(λ) across different λ values to verify it matches the theoretical lower bound Ω(n^{-ε}).

3. **NTK Embedding Index**: For neural tangent kernels on spheres, estimate the embedding index by computing the effective dimensionality of the feature space and comparing it to the theoretical prediction.