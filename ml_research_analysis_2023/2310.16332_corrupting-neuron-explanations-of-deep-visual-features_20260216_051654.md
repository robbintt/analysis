---
ver: rpa2
title: Corrupting Neuron Explanations of Deep Visual Features
arxiv_id: '2310.16332'
source_url: https://arxiv.org/abs/2310.16332
tags:
- corruption
- data
- neuron
- noise
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts the first robustness study of Neuron Explanation
  Methods (NEMs) under a unified pipeline. It shows that NEMs can be manipulated by
  random noise and designed perturbations added to their probing data.
---

# Corrupting Neuron Explanations of Deep Visual Features

## Quick Facts
- arXiv ID: 2310.16332
- Source URL: https://arxiv.org/abs/2310.16332
- Reference count: 34
- This paper conducts the first robustness study of Neuron Explanation Methods (NEMs) under a unified pipeline. It shows that NEMs can be manipulated by random noise and designed perturbations added to their probing data. With Gaussian random noise (std=0.02), up to 28% of neurons in deeper layers can be changed. A novel corruption algorithm is devised that can manipulate over 80% of neurons by poisoning less than 10% of probing images. This reveals significant vulnerabilities in NEMs and raises concerns about trusting them in safety and fairness-critical applications.

## Executive Summary
This paper presents the first comprehensive robustness analysis of Neuron Explanation Methods (NEMs) by introducing a unified evaluation pipeline. The authors demonstrate that NEMs are vulnerable to manipulation through both random noise and carefully designed adversarial perturbations in probing datasets. The findings reveal that even small perturbations can significantly corrupt neuron explanations, with higher-layer neurons being particularly susceptible to attacks.

## Method Summary
The paper introduces a unified pipeline for evaluating NEMs that consists of three main steps: computing activation thresholds, creating binary activation maps, and assigning concepts based on similarity scores. The authors then develop two types of corruption attacks - random noise (Gaussian, Uniform, Bernoulli) and designed perturbations optimized via projected gradient descent. The designed corruption algorithm minimizes the difference between the average activation of the original concept and a target concept, effectively manipulating neuron concept assignments.

## Key Results
- Random noise with σ=0.02 can change concepts for up to 28% of neurons in deeper layers
- Designed perturbations can manipulate over 80% of neurons by poisoning less than 10% of probing images
- Higher-layer neurons are more vulnerable to corruption than lower-layer neurons
- Texture and color concepts show higher resistance to manipulation compared to objects, parts, scenes, and materials

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small random perturbations in probing datasets can significantly corrupt neuron explanations in deep networks.
- Mechanism: Adding Gaussian noise (σ=0.02) to probing images changes neuron activation patterns enough to alter the concept assignment by shifting the activation thresholds and similarity scores.
- Core assumption: The neuron explanation pipeline is sensitive to minor changes in input activation maps because both the threshold computation and similarity matching depend on the exact values.
- Evidence anchors:
  - [abstract] "even adding small random noise with a standard deviation of 0.02 can already change the assigned concepts of up to 28% neurons in the deeper layers"
  - [section 4.2] "Fig 2a visualizes the percentage of neurons manipulated with Gaussian random noise in VGG16-Places365 for Network Dissection and MILAN"
- Break condition: If the similarity function is made robust to small activation changes or if activation thresholds are computed with robust statistics.

### Mechanism 2
- Claim: Targeted adversarial perturbations on probing images can manipulate neuron concept assignments with high success rate.
- Mechanism: By optimizing perturbations to reduce activation for the original concept and increase activation for a target concept (via the actavg objective), the similarity score for the original concept drops below the target, causing reassignment.
- Core assumption: The similarity function is differentiable with respect to the input activations, allowing gradient-based optimization to find effective perturbations.
- Evidence anchors:
  - [section 3.2] "we define our objective function for neuron i with concept c_i*, probing image x_j ∈ D_probe and the target (manipulated) concept t as: min_{δ_j} actavg_{i,c_i*}(x_j + δ_j) − actavg_{i,t}(x_j + δ_j)"
  - [section 4.3.2] "Our objective function can successfully change the concepts of more than 80% neurons in the higher layers with PGD ϵ = 4/255"
- Break condition: If the similarity function is not differentiable or if gradient-based optimization fails due to non-convexity.

### Mechanism 3
- Claim: Higher-layer neurons are more susceptible to probing dataset corruption than lower-layer neurons.
- Mechanism: Higher-layer neurons have more abstract, higher-level concepts that are less robust to small changes in input features, while lower-layer neurons capture more basic features that are more stable.
- Core assumption: The vulnerability of neuron explanations correlates with the semantic level of the concepts they represent.
- Evidence anchors:
  - [section 4.2] "A noise with a low standard deviation of 0.05 can manipulate more than 17% neurons for Network Dissection and over 40% of neuron descriptions for MILAN in the conv5 3 layer"
  - [section 4.3.3] "Higher level concepts objects, part, scene, and material can be manipulated with a success rate of 100% by our objective function in the untargeted setting. In contrast, lower-level concepts texture and color are more robust"
- Break condition: If the network architecture or training process makes higher-layer representations more robust to input perturbations.

## Foundational Learning

- Concept: Neuron Explanation Methods (NEMs) and their unified pipeline
  - Why needed here: Understanding how NEMs work is essential to grasp how corruption attacks can manipulate their outputs
  - Quick check question: What are the three main steps in the unified NEM pipeline described in the paper?

- Concept: Activation thresholds and similarity functions in NEMs
  - Why needed here: The attack mechanisms rely on manipulating these components to change concept assignments
  - Quick check question: How is the activation threshold T_i computed in the unified pipeline?

- Concept: Gradient-based optimization for adversarial attacks
  - Why needed here: The designed corruption algorithm uses projected gradient descent to find effective perturbations
  - Quick check question: What optimization objective is used to find perturbations that manipulate neuron concepts?

## Architecture Onboarding

- Component map: Probing dataset preparation -> Activation map computation and threshold calculation -> Concept assignment via similarity function -> Corruption algorithm (targeted/untargeted) -> Evaluation metrics (F1-BERT, IOU, manipulation success rate)

- Critical path: Probing dataset → Activation maps → Threshold computation → Concept assignment → Output explanation
  - Attacks target the probing dataset to corrupt intermediate activation maps and final concept assignments

- Design tradeoffs:
  - Using small noise vs. large perturbations: small noise is less detectable but less effective; large perturbations are more effective but more detectable
  - Targeted vs. untargeted attacks: targeted attacks are harder but achieve specific concept changes; untargeted attacks are easier but cause random changes
  - Access to ground truth segmentation: having access enables more effective attacks; without it, attacks rely on approximated concept masks

- Failure signatures:
  - Low manipulation success rate despite corruption attempts
  - Model accuracy remains high but neuron explanations change (indicating targeted corruption)
  - Certain concept categories (like texture and color) show high resistance to corruption

- First 3 experiments:
  1. Test Gaussian noise corruption with increasing standard deviation (0.01, 0.02, 0.03, 0.04, 0.05) on VGG16-Places365 conv5_3 layer
  2. Run untargeted PGD attack with ϵ = 2/255, 4/255, 6/255 on Network Dissection and measure concept change percentage
  3. Compare Bernoulli, Uniform, and Gaussian noise corruption effectiveness on the same network and layer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do adversarial training techniques affect the robustness of NEMs against data poisoning attacks?
- Basis in paper: [inferred] The paper mentions that adversarial training may only help to alleviate their proposed attacks a bit, as the attack success rate is still pretty high on adversarially-trained models.
- Why unresolved: The paper does not provide a comprehensive study on the effectiveness of various adversarial training techniques in defending against NEM corruption attacks.
- What evidence would resolve it: Empirical studies comparing different adversarial training methods and their impact on NEM robustness, including both untargeted and targeted attacks.

### Open Question 2
- Question: Are there specific categories or types of concepts that are more susceptible to data poisoning in NEMs?
- Basis in paper: [explicit] The paper shows that higher-level concepts like objects, parts, scenes, and materials are more susceptible to data corruption compared to lower-level concepts like texture and color.
- Why unresolved: While the paper identifies a trend, it does not explore the underlying reasons for this susceptibility or investigate other potential factors that might influence the vulnerability of different concept categories.
- What evidence would resolve it: Detailed analysis of the factors contributing to the vulnerability of specific concept categories, including potential correlations with concept complexity or semantic relationships.

### Open Question 3
- Question: How does the choice of probing dataset affect the robustness of NEMs to data poisoning?
- Basis in paper: [inferred] The paper uses two different NEMs (Network Dissection and MILAN) with different probing datasets (Broden and Places365 validation dataset, respectively), but does not directly compare their robustness.
- Why unresolved: The paper does not provide a systematic comparison of NEM robustness across different probing datasets or investigate how dataset characteristics might influence vulnerability to data poisoning.
- What evidence would resolve it: Comparative studies of NEM robustness using various probing datasets with different characteristics (e.g., size, concept diversity, image quality) and analysis of the relationship between dataset properties and attack success rates.

## Limitations

- The study focuses primarily on VGG16-Places365 and ResNet50-Imagenet architectures, which may not represent the full diversity of modern deep networks
- Random noise experiments use relatively small perturbations (σ=0.02-0.05), raising questions about real-world applicability
- Designed perturbation attacks assume white-box access to the model and its gradients, which may not be feasible in all practical scenarios

## Confidence

- High Confidence: The fundamental claim that NEMs are vulnerable to probing dataset corruption through both random noise and designed perturbations
- Medium Confidence: The specific quantification of vulnerability (28% neurons affected by random noise, 80% by designed perturbations)
- Medium Confidence: The claim that higher-layer neurons are more susceptible to corruption than lower-layer neurons

## Next Checks

1. **Cross-architecture validation**: Replicate the corruption experiments on additional network architectures (e.g., Vision Transformers, EfficientNet) to assess whether the observed vulnerability patterns generalize beyond CNNs.

2. **Black-box attack evaluation**: Implement and evaluate black-box variants of the designed perturbation attacks where gradient information is not directly accessible, measuring the trade-off between attack effectiveness and practical feasibility.

3. **Robustness enhancement validation**: Test whether simple modifications to the NEM pipeline (e.g., using robust statistics for threshold computation, incorporating activation smoothing) can meaningfully reduce vulnerability to the demonstrated attacks.