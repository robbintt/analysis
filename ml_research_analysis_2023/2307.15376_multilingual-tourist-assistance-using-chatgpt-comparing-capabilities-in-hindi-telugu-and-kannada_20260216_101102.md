---
ver: rpa2
title: 'Multilingual Tourist Assistance using ChatGPT: Comparing Capabilities in Hindi,
  Telugu, and Kannada'
arxiv_id: '2307.15376'
source_url: https://arxiv.org/abs/2307.15376
tags:
- translation
- language
- hindi
- telugu
- translations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates ChatGPT\u2019s translation quality for English\
  \ to Hindi, Telugu, and Kannada in a tourist assistance context. Using 50 questions\
  \ across general knowledge, food, and travel themes, human evaluators rated translations\
  \ for accuracy and fluency on a 5-point scale."
---

# Multilingual Tourist Assistance using ChatGPT: Comparing Capabilities in Hindi, Telugu, and Kannada

## Quick Facts
- **arXiv ID**: 2307.15376
- **Source URL**: https://arxiv.org/abs/2307.15376
- **Reference count**: 40
- **Primary result**: ChatGPT achieves highest translation quality for Hindi (accuracy: 4.2, fluency: 3.8), moderate for Kannada (accuracy: 3.2, fluency: 3.0), and lowest for Telugu (accuracy: 2.5, fluency: 2.1) in tourist assistance context.

## Executive Summary
This study evaluates ChatGPT's translation capabilities from English to Hindi, Telugu, and Kannada in a tourist assistance context. Using 50 questions across general knowledge, food, and travel themes, human evaluators rated translations for accuracy and fluency on a 5-point scale. The results show Hindi translations achieved the highest quality scores, while Telugu performed the lowest. Objective BLEU score analysis aligned with these findings, with Hindi scoring 72.69, Kannada 46.78, and Telugu 13.12. The study identifies Telugu as requiring targeted improvements through expanded training data, model refinement, and incorporation of human feedback.

## Method Summary
The study collected 50 English questions categorized into general knowledge, food, and travel themes. These questions were translated into Hindi, Telugu, and Kannada using ChatGPT (gpt-3.5-turbo). Five native speakers evaluated the translations for accuracy and fluency on a 5-point scale. The scores were converted into BLEU scores for objective evaluation. The methodology combined subjective human ratings with objective BLEU scores to provide a comprehensive assessment of translation quality across the three languages.

## Key Results
- Hindi translations achieved the highest average accuracy score of 4.2 and fluency score of 3.8
- Telugu translations performed the lowest with accuracy score of 2.5 and fluency score of 2.1
- Objective BLEU scores aligned with human ratings: Hindi (72.69), Kannada (46.78), Telugu (13.12)
- All languages showed consistent patterns with Hindi outperforming others in both accuracy and fluency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The study's mixed subjective and objective evaluation methodology provides a comprehensive assessment of translation quality.
- Mechanism: Combining human evaluators' ratings (accuracy and fluency on 5-point scale) with BLEU scores captures both human perception and machine alignment with reference translations.
- Core assumption: Human ratings and BLEU scores are complementary measures that together give a complete picture of translation quality.
- Evidence anchors:
  - [abstract] "These were assessed by five volunteers for accuracy and fluency, and the scores were subsequently converted into a BLEU score."
  - [section 4] "In this work, we use a subjective and an objective evaluation methodology. Subjective evaluation asks native speakers of a language to rate the accuracy and fluency of the translation."

### Mechanism 2
- Claim: Using domain-specific questions (tourist assistance) allows for targeted evaluation of translation models in practical use cases.
- Mechanism: By focusing on questions related to general knowledge, food, and travel, the study evaluates how well the translation models handle the types of queries tourists are likely to encounter.
- Core assumption: The selected question categories are representative of real-world tourist needs and cover a range of complexity in language use.
- Evidence anchors:
  - [abstract] "To measure the translation quality, a test set of 50 questions from diverse fields such as general knowledge, food, and travel was used."
  - [section 2] "All these Sixty questions were then analyzed by two independent volunteers to identify the key themes. A third volunteer was asked to resolve any conflicts and discrepancies. From the original set of Sixty questions, Fifty relevant questions were shortlisted and were categorized into 3 themes; General, Food, and Travel."

### Mechanism 3
- Claim: Comparing translation quality across multiple languages (Hindi, Kannada, Telugu) within the same study allows for relative performance assessment.
- Mechanism: By evaluating and comparing translations for different languages using the same methodology, the study can identify which languages perform better or worse and to what extent.
- Core assumption: The translation quality differences observed are due to the models' performance rather than differences in the complexity or nature of the source text across languages.
- Evidence anchors:
  - [abstract] "The Hindi translations outperformed others, showcasing superior accuracy and fluency, whereas Telugu translations lagged behind."
  - [section 5] "Overall, Hindi translations received the highest average scores, indicating better accuracy and fluency compared to Telugu and Kannada."

## Foundational Learning

- Concept: BLEU score calculation and interpretation
  - Why needed here: The study uses BLEU scores as an objective measure of translation quality, so understanding how they are calculated and what they indicate is crucial for interpreting the results.
  - Quick check question: What is the range of BLEU scores, and what does a higher score indicate about translation quality?

- Concept: Transformer-based language models
  - Why needed here: The study uses ChatGPT, which is based on a transformer architecture. Understanding the basics of how transformer models work is important for grasping the capabilities and limitations of the translation system being evaluated.
  - Quick check question: What are the key components of a transformer-based language model, and how do they contribute to its ability to perform translation tasks?

- Concept: Evaluation methodology in NLP research
  - Why needed here: The study employs a specific evaluation methodology combining subjective human ratings and objective BLEU scores. Understanding the rationale behind this approach and how it compares to other evaluation methods is important for assessing the validity and reliability of the results.
  - Quick check question: What are the advantages and disadvantages of using a combination of subjective human ratings and objective metrics like BLEU scores in evaluating machine translation quality?

## Architecture Onboarding

- Component map: Data collection and categorization -> ChatGPT model (gpt-3.5-turbo) -> Translation generation (prompt-based) -> Human evaluation (accuracy and fluency ratings) -> BLEU score calculation -> Analysis and comparison across languages

- Critical path:
  1. Collect and categorize questions
  2. Generate translations using ChatGPT
  3. Human evaluators rate translations
  4. Calculate BLEU scores
  5. Analyze and compare results

- Design tradeoffs:
  - Using a pre-trained model (ChatGPT) vs. fine-tuning a model on specific language pairs
  - Subjective human ratings vs. purely objective metrics
  - Limited number of questions vs. broader coverage of potential use cases

- Failure signatures:
  - Significant discrepancies between human ratings and BLEU scores
  - Poor performance across all languages for certain types of questions
  - Inconsistent ratings among human evaluators

- First 3 experiments:
  1. Evaluate the translation quality for a small set of questions across all three languages to establish baseline performance.
  2. Conduct a sensitivity analysis by varying the complexity and domain of the questions to assess the models' robustness.
  3. Compare the performance of ChatGPT with a simpler, rule-based translation system to quantify the benefits of using a more advanced model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ChatGPT in translating English to Telugu compare to other state-of-the-art machine translation models?
- Basis in paper: Inferred - The paper mentions that Telugu translations scored lowest in accuracy and fluency, indicating room for improvement, but does not compare ChatGPT's performance to other models.
- Why unresolved: The paper does not provide a comparative analysis with other translation models.
- What evidence would resolve it: A comparative study between ChatGPT and other machine translation models on the same set of questions and evaluation criteria.

### Open Question 2
- Question: How does the translation quality of ChatGPT vary with the complexity of the source text (e.g., idiomatic expressions, technical terms)?
- Basis in paper: Inferred - The paper evaluates translation quality based on accuracy and fluency but does not specifically address how complexity of the source text affects the translation.
- Why unresolved: The study does not categorize or analyze the translation quality based on the complexity of the source text.
- What evidence would resolve it: An analysis of translation quality for texts of varying complexity, categorized by the use of idiomatic expressions, technical terms, etc.

### Open Question 3
- Question: How does user satisfaction with ChatGPT translations correlate with the BLEU scores?
- Basis in paper: Inferred - The paper uses BLEU scores as an objective measure of translation quality and human evaluators rate accuracy and fluency, but it does not correlate these objective measures with user satisfaction.
- Why unresolved: The study does not include a user satisfaction survey or a direct comparison between BLEU scores and user ratings.
- What evidence would resolve it: A user satisfaction survey that includes questions about the overall satisfaction with the translations, correlated with the BLEU scores and human evaluator ratings.

## Limitations
- Small sample size of 50 questions may not fully represent real-world tourist interactions
- Single ChatGPT version (gpt-3.5-turbo) used without comparison to other models or versions
- Human evaluation process lacks detailed documentation of inter-rater reliability or evaluation criteria standardization
- BLEU score calculation methodology connecting human ratings to automated metrics is not fully specified

## Confidence
- **High Confidence**: The relative performance ranking (Hindi > Kannada > Telugu) across both subjective and objective measures is robust and consistently supported by the data presented.
- **Medium Confidence**: The specific numerical scores (accuracy and fluency ratings, BLEU scores) are methodologically sound but may have limited generalizability beyond the specific evaluation framework used.
- **Low Confidence**: The recommendations for model improvement are based on the study's limited scope and would require additional research to validate their effectiveness.

## Next Checks
1. **Inter-rater Reliability Assessment**: Conduct a detailed analysis of the consistency among the five human evaluators, including calculation of inter-rater reliability metrics (e.g., Cohen's kappa or Fleiss' kappa) to establish the robustness of the subjective evaluation methodology.

2. **Expanded Domain Coverage**: Test the translation models with a larger and more diverse question set that includes additional domains relevant to tourist assistance (e.g., emergency situations, cultural etiquette, transportation) to assess whether the relative performance patterns hold across broader use cases.

3. **Model Comparison and Version Analysis**: Evaluate multiple versions of ChatGPT and other contemporary multilingual translation models using the same methodology to determine whether the observed performance differences are specific to gpt-3.5-turbo or represent more general patterns in large language model capabilities for these language pairs.