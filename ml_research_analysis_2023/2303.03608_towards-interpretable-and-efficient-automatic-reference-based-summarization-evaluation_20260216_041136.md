---
ver: rpa2
title: Towards Interpretable and Efficient Automatic Reference-Based Summarization
  Evaluation
arxiv_id: '2303.03608'
source_url: https://arxiv.org/abs/2303.03608
tags:
- evaluation
- metrics
- automatic
- text
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces interpretable and efficient automatic metrics
  for reference-based summarization evaluation. The authors propose a two-stage evaluation
  pipeline that first extracts atomic content units (ACUs) from one text sequence
  and then checks the presence of these units in another sequence.
---

# Towards Interpretable and Efficient Automatic Reference-Based Summarization Evaluation

## Quick Facts
- arXiv ID: 2303.03608
- Source URL: https://arxiv.org/abs/2303.03608
- Reference count: 16
- This paper introduces interpretable and efficient automatic metrics for reference-based summarization evaluation that achieve state-of-the-art performance on multiple benchmarks.

## Executive Summary
This paper presents a novel approach to automatic summarization evaluation that prioritizes both interpretability and efficiency. The authors propose a two-stage evaluation pipeline that first extracts atomic content units (ACUs) from text sequences and then checks their presence in other sequences using natural language inference. This approach provides high interpretability at both the fine-grained unit level and summary level. Additionally, they develop a one-stage metric that directly predicts summary-level scores, balancing efficiency with interpretability. Both types of metrics significantly outperform established baselines like ROUGE, BERTScore, and BARTScore on standard summarization datasets.

## Method Summary
The proposed approach consists of two complementary metrics: A2CU (two-stage) and A3CU (one-stage). A2CU extracts atomic content units (ACUs) from one text sequence using a T0-based Seq2Seq model, then checks their presence in another sequence using a DeBERTa-based NLI model. This provides interpretable fine-grained evaluation results. A3CU is a more efficient one-stage metric that directly predicts summary-level scores by training on outputs from A2CU using a BERT-based model. The A3CU model is pre-trained on synthetic data generated by fine-tuned summarization models (BART) and scored by A2CU, then fine-tuned on the RoSE benchmark. Both metrics are made publicly available as an easy-to-use Python package.

## Key Results
- Two-stage metric (A2CU) achieves state-of-the-art performance on reference-based summarization evaluation
- One-stage metric (A3CU) balances efficiency with interpretability while maintaining competitive performance
- Both metrics outperform established baselines (ROUGE, BERTScore, BARTScore) on CNN/DailyMail, XSum, and SamSum datasets
- The approach provides high interpretability at both ACU level and summary level

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage evaluation improves interpretability by explicitly showing which information units are missing or present
- Mechanism: First stage extracts atomic content units (ACUs) from one text sequence, second stage checks presence/absence of each ACU in another sequence using NLI
- Core assumption: ACUs capture the essential information units that determine semantic similarity between text sequences
- Evidence anchors:
  - [abstract] "first extracts atomic content units (ACUs) from one text sequence and then checks the presence of these units in another sequence. This approach provides high interpretability at both the fine-grained unit level and summary level"
  - [section 3.1] "At the ACU level, the evaluation result indicates the presence or absence of an extracted information unit; at the summary level, the aggregated ACU score represents the percentage of information overlap"
  - [corpus] Weak - only 5 related papers found, average FMR 0.451 suggests limited corpus relevance
- Break condition: If ACU extraction fails to capture key information or NLI model cannot accurately determine entailment, interpretability advantage disappears

### Mechanism 2
- Claim: One-stage metric achieves computational efficiency while maintaining interpretability by learning from two-stage outputs
- Mechanism: Uses two-stage evaluation results as training signal to train a lightweight model that directly predicts summary-level scores
- Core assumption: Two-stage evaluation provides stable, reliable supervision signal for training one-stage metric
- Evidence anchors:
  - [abstract] "develop a more efficient one-stage metric that directly predicts the aggregated summary-level scores by training on the recently proposed RoSE benchmark"
  - [section 3.2] "we use our two-stage approach to generate pre-training data for a more lightweight, one-stage metric"
  - [corpus] Weak - limited corpus evidence for this specific training approach
- Break condition: If two-stage evaluation outputs are noisy or inconsistent, one-stage model cannot learn stable mapping

### Mechanism 3
- Claim: Pre-training on synthetic data improves metric robustness and performance
- Mechanism: Generates diverse candidate summaries using fine-tuned summarization models, scores them with two-stage method, uses as pre-training data for one-stage metric
- Core assumption: Synthetic data generated by strong summarization models provides diverse, realistic supervision signal
- Evidence anchors:
  - [section 3.2] "we construct the pre-training corpora on the existing summarization datasets such as CNN/DailyMail... generate multiple candidate summaries using a fine-tuned summarization model"
  - [section 4.2] "A3CU P can outperform A3CU F at the summary level without fine-tuning on the RoSE dataset"
  - [corpus] Weak - no direct corpus evidence for this specific pre-training approach
- Break condition: If generated candidates are too similar or low-quality, pre-training provides limited benefit

## Foundational Learning

- Concept: Atomic Content Units (ACUs)
  - Why needed here: Provide the fundamental building blocks for measuring information overlap between text sequences
  - Quick check question: What distinguishes ACUs from n-grams or other lexical units?

- Concept: Natural Language Inference (NLI)
  - Why needed here: Enables checking whether extracted ACUs are semantically present in target text sequence
  - Quick check question: How does NLI differ from semantic similarity measures in this context?

- Concept: Contrastive learning
  - Why needed here: Enables effective pre-training using diverse candidate summaries generated from same source
  - Quick check question: What makes contrastive learning particularly suitable for this pre-training scenario?

## Architecture Onboarding

- Component map: ACU extraction (T0) -> NLI checking (DeBERTa) -> score aggregation (two-stage) OR ACU extraction (T0) -> NLI checking (DeBERTa) -> one-stage training (one-stage)

- Critical path: ACU extraction → NLI checking → score aggregation (for two-stage) or ACU extraction → NLI checking → one-stage training (for one-stage)

- Design tradeoffs:
  - Two-stage: Higher interpretability, lower efficiency
  - One-stage: Lower interpretability, higher efficiency
  - Pre-training: Better performance but requires additional computational resources

- Failure signatures:
  - Low ACU extraction quality → poor interpretability in two-stage
  - NLI model errors → incorrect presence/absence judgments
  - Pre-training mismatch → degraded one-stage performance

- First 3 experiments:
  1. Test ACU extraction quality on held-out data with human verification
  2. Evaluate NLI model accuracy on ACU entailment task
  3. Compare one-stage performance with and without pre-training on same validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the two-stage metric's performance change when using different ACU extraction models or NLI models?
- Basis in paper: [inferred] The paper mentions using T0 for ACU extraction and DeBERTa for NLI, but does not explore the impact of using alternative models.
- Why unresolved: The paper does not provide a systematic comparison of different models for ACU extraction and NLI, which could potentially impact the performance of the two-stage metric.
- What evidence would resolve it: Conducting experiments with various ACU extraction and NLI models and comparing their performance on the two-stage metric.

### Open Question 2
- Question: How does the one-stage metric's performance change when pre-training on different summarization datasets or using different pre-training objectives?
- Basis in paper: [inferred] The paper uses CNN/DailyMail dataset for pre-training and focuses on predicting ROUGE scores, but does not explore the impact of using other datasets or objectives.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of different pre-training datasets and objectives on the one-stage metric's performance.
- What evidence would resolve it: Conducting experiments with various pre-training datasets and objectives and comparing their performance on the one-stage metric.

### Open Question 3
- Question: How does the two-stage metric's performance change when considering more complex relationships between the source text, reference summary, and system-generated summary?
- Basis in paper: [inferred] The paper mentions an extended NLI setting that includes the source text as context, but does not explore the impact of considering more complex relationships between the three text sequences.
- Why unresolved: The paper does not provide a thorough investigation of the impact of considering more complex relationships between the source text, reference summary, and system-generated summary on the two-stage metric's performance.
- What evidence would resolve it: Conducting experiments with different settings that consider more complex relationships between the three text sequences and comparing their performance on the two-stage metric.

## Limitations
- Limited exploration of how different ACU extraction and NLI models impact overall performance
- Performance generalization to domains beyond news-based summarization remains untested
- No direct human studies validating the practical interpretability of atomic content unit explanations

## Confidence
- High confidence in state-of-the-art performance claims based on substantial correlation improvements across multiple benchmarks
- Medium confidence in interpretability claims due to lack of direct human validation studies
- Low confidence in scalability to other text generation tasks beyond summarization

## Next Checks
1. Conduct a human interpretability study where evaluators assess the usefulness and clarity of atomic content unit explanations
2. Evaluate the proposed metrics on summarization datasets from different domains (scientific, legal, medical) to test generalization
3. Perform an ablation study varying ACU extraction quality to quantify its impact on overall metric performance