---
ver: rpa2
title: Unsupervised Learning for Solving the Travelling Salesman Problem
arxiv_id: '2303.10538'
source_url: https://arxiv.org/abs/2303.10538
tags:
- search
- heat
- edges
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents UTSP, an unsupervised learning method for solving
  the Travelling Salesman Problem (TSP). UTSP uses a Graph Neural Network (GNN) with
  a surrogate loss to generate a heat map indicating the probability of each edge
  belonging to the optimal TSP solution.
---

# Unsupervised Learning for Solving the Travelling Salesman Problem

## Quick Facts
- arXiv ID: 2303.10538
- Source URL: https://arxiv.org/abs/2303.10538
- Reference count: 10
- Key outcome: UTSP outperforms existing data-driven TSP heuristics using only ~10% of parameters and ~0.2% of training samples, achieving 0.09%-1.18% gaps from optimal solutions on instances up to 1000 nodes.

## Executive Summary
This paper introduces UTSP, an unsupervised learning method for solving the Euclidean Travelling Salesman Problem (TSP) without labeled solutions. The approach uses a Graph Neural Network (specifically a Scattering Attention GNN) to generate a heat map indicating edge probabilities in the optimal tour, then applies a local search algorithm guided by this heat map to find the final solution. The method achieves competitive solution quality while being remarkably parameter and data efficient compared to existing supervised and reinforcement learning approaches.

## Method Summary
UTSP trains a SAG to process city coordinates and adjacency matrix, outputting a transition matrix T. A heat map H = TV^T is computed and used with an unsupervised surrogate loss that combines row normalization, diagonal penalty, and weighted distance terms. The trained model generates heat maps for new TSP instances, which guide a best-first local search algorithm with stochastic expansion to find high-quality tours. The entire pipeline is trained end-to-end without requiring optimal TSP solutions as labels.

## Key Results
- Achieves 0.09%-1.18% gaps from optimal solutions on TSP instances up to 1000 nodes
- Requires only ~10% of parameters and ~0.2% of training samples compared to RL/SL methods
- Outperforms existing data-driven TSP heuristics in solution quality and efficiency
- Demonstrates that SAG's expressive power is critical for generating non-smooth heat maps that effectively guide local search

## Why This Works (Mechanism)

### Mechanism 1
The surrogate loss function enables end-to-end training without labeled solutions by approximating the TSP objective and Hamiltonian cycle constraint. The loss combines row-wise normalization, diagonal penalty, and edge-distance weighted sum to approximate tour length. Core assumption: The expectation of the edge-weighted sum under the heat map distribution approximates the true tour length.

### Mechanism 2
Scattering Attention GNN (SAG) generates non-smooth heat maps that effectively guide local search by combining low-pass and band-pass filters to avoid oversmoothing while maintaining global receptive fields. Core assumption: Band-pass filtering preserves high-frequency graph features necessary for distinguishing optimal edges.

### Mechanism 3
The heat map-guided local search with dynamic candidate sets efficiently finds high-quality solutions in reduced search space by expanding nodes based on edge heat map values plus exploration bonus. Core assumption: The heat map accurately ranks edges by their likelihood of belonging to optimal solutions.

## Foundational Learning

- **Graph Neural Networks and their expressive power**
  - Why needed here: The method relies on GNNs to generate edge probability distributions that guide search
  - Quick check question: What is the difference between low-pass and band-pass filtering in graph neural networks?

- **Unsupervised learning objectives and surrogate losses**
  - Why needed here: The training avoids labeled data by constructing a differentiable approximation of the TSP objective
  - Quick check question: How does the surrogate loss balance between finding short paths and maintaining Hamiltonian cycles?

- **Local search algorithms and their heuristics**
  - Why needed here: The final solution generation uses best-first search guided by the heat map
  - Quick check question: What is the role of the exploration bonus in the edge selection probability formula?

## Architecture Onboarding

- **Component map**: Input coordinates → Distance matrix → Adjacency matrix → SAG layers → Transition matrix → Heat map → Local search → Solution
- **Critical path**: SAG training → Heat map generation → Local search execution
- **Design tradeoffs**: Model expressiveness vs. parameter efficiency, heat map smoothness vs. search guidance quality
- **Failure signatures**: Smooth heat maps with low variance, local search getting stuck in local minima, training loss plateauing early
- **First 3 experiments**:
  1. Train SAG on small TSP instances (n=20) and visualize the heat map distribution
  2. Compare GCN vs SAG heat maps on the same instance to observe oversmoothing effects
  3. Run local search with different M values to find the optimal balance between search space reduction and solution quality

## Open Questions the Paper Calls Out

- **Open Question 1**: How can we design more expressive GNNs with edge features to further improve the quality of the heat maps and solution performance?
  - Basis in paper: The paper mentions "Future direction includes designing more expressive GNNs (such as adding edge features)" and discusses the importance of expressive power in GNNs for generating non-smooth heat maps.

- **Open Question 2**: Can the unsupervised surrogate loss function be adapted or extended to other combinatorial optimization problems beyond TSP?
  - Basis in paper: The paper introduces a novel unsupervised learning framework with a surrogate loss for TSP, suggesting potential applicability to other problems.

- **Open Question 3**: How does the choice of local search parameters (e.g., M, K, T) affect the performance of UTSP, and is there an optimal configuration for different TSP instance sizes?
  - Basis in paper: The paper provides specific local search parameters for different TSP instance sizes but does not explore the impact of these parameters on performance.

## Limitations

- SAG architecture details (number of layers, hidden units, attention mechanism) are not fully specified
- Hyperparameter values for loss function (λ₁, λ₂, τ) and local search (α, β, M, K, T) are not provided in full detail
- Dataset generation protocol and distance computation method are underspecified

## Confidence

- **High confidence**: The general framework combining unsupervised GNN training with surrogate loss and heat-map-guided local search is clearly described and theoretically sound
- **Medium confidence**: The claim that SAG outperforms GCN for TSP is supported by comparative experiments, though exact implementation differences are not fully detailed
- **Medium confidence**: The parameter and data efficiency claims are compelling but depend on exact implementation choices that could affect scalability

## Next Checks

1. Implement a minimal SAG architecture (2 layers, 64 hidden units) and verify it produces non-smooth heat maps on small TSP instances, comparing against GCN baseline heat maps to confirm oversmoothing effects
2. Tune the surrogate loss hyperparameters (λ₁, λ₂, τ) on a validation set of 50-node TSP instances, monitoring training stability and heat map variance as diagnostic metrics
3. Run the complete pipeline on n=20,50,100 TSP instances and benchmark against provided optimal solutions to verify the reported 0.09%-1.18% gaps, checking that the heat map-guided local search consistently finds high-quality solutions