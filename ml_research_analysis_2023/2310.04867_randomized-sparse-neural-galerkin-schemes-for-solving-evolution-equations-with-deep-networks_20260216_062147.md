---
ver: rpa2
title: Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with
  Deep Networks
arxiv_id: '2310.04867'
source_url: https://arxiv.org/abs/2310.04867
tags:
- time
- updates
- network
- neural
- rsng
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of sequential-in-time training
  of neural networks for solving time-dependent partial differential equations (PDEs),
  where training errors quickly accumulate and amplify over time. The core method
  idea is to introduce Neural Galerkin schemes that update randomized sparse subsets
  of network parameters at each time step, motivated by dropout to avoid overfitting
  locally in time and sparsity to reduce computational costs.
---

# Randomized Sparse Neural Galerkin Schemes for Solving Evolution Equations with Deep Networks

## Quick Facts
- arXiv ID: 2310.04867
- Source URL: https://arxiv.org/abs/2310.04867
- Reference count: 40
- Primary result: RSNG achieves up to two orders of magnitude improvement in accuracy or speed compared to dense update schemes for sequential-in-time neural network training of evolution equations

## Executive Summary
This paper addresses the fundamental challenge of error accumulation in sequential-in-time training of neural networks for solving time-dependent PDEs. The proposed Randomized Sparse Neural Galerkin (RSNG) method updates only randomized sparse subsets of network parameters at each time step, drawing inspiration from dropout techniques to prevent overfitting. The method achieves significant computational gains - up to two orders of magnitude improvements in either accuracy or speed - while maintaining solution quality across a range of evolution equations including Allen-Cahn, Burgers', and Vlasov equations.

## Method Summary
The RSNG method evolves neural network parameters θ(t) over time using a Dirac-Frenkel variational principle with randomized sparse updates. At each time step, a sketching matrix St randomly samples a subset of columns from the Jacobian matrix J(θ(t)), creating a sparse approximation Js(θ(t)). A least-squares problem is solved to find the optimal sparse parameter update ∆θs, which is then lifted to full dimension and applied to θ. This approach exploits the low-rank structure of the Jacobian matrix to reduce computational costs while avoiding overfitting through randomization. The method is evaluated against dense update schemes and global-in-time methods (PINNs) on benchmark evolution equations.

## Key Results
- RSNG achieves up to two orders of magnitude lower error than dense update schemes at fixed computational budgets
- RSNG is up to two orders of magnitude faster than dense schemes at fixed accuracy targets
- The method maintains accuracy across a range of PDEs including Allen-Cahn, Burgers', and Vlasov equations

## Why This Works (Mechanism)

### Mechanism 1
Sparse randomized updates prevent overfitting in sequential-in-time training by updating only a subset of network parameters at each time step and randomizing which parameters are updated. This avoids over-specialization to local time dynamics and reduces co-adaptation of neurons. The core assumption is that the Jacobian matrix J(θ(t)) is sufficiently low-rank so that sub-sampling columns still spans the tangent space effectively. The method breaks down if the Jacobian is not low-rank or coherent, as uniform random sampling may fail to capture the essential tangent space.

### Mechanism 2
Sparse updates reduce computational cost without losing expressiveness by depending on the number of parameters updated (s) rather than the total network size. Since many parameters are redundant locally, updating only s << p parameters achieves similar accuracy at lower cost. This relies on the assumption that many network parameters are redundant locally in time due to the low-rank structure of the Jacobian. The method fails if the Jacobian is full-rank locally, as sparse updates would lose expressiveness and accuracy.

### Mechanism 3
Randomized sparse updates enable scalability to larger networks by decoupling the number of parameters updated from the total network size. This allows exploitation of larger networks for better expressiveness without proportional computational cost. The approach assumes that larger networks improve expressiveness but don't increase the rank of the Jacobian significantly. The method may break down if larger networks increase Jacobian rank significantly, making sparse updates insufficient.

## Foundational Learning

- Concept: Jacobian matrix structure and rank
  - Why needed here: Understanding low-rankness of J(θ(t)) is crucial for justifying sparse updates and randomization
  - Quick check question: Why does a low-rank Jacobian imply that many parameters are redundant locally?

- Concept: Dropout and overfitting in neural networks
  - Why needed here: The motivation for randomization comes from dropout's success in preventing overfitting through parameter sub-sampling
  - Quick check question: How does dropout prevent overfitting in traditional neural network training?

- Concept: Galerkin projection and tangent space approximation
  - Why needed here: The method relies on approximating the tangent space with a subset of basis functions
  - Quick check question: What is the relationship between the spanning set {∂θi ˆu} and the tangent space of the manifold MΘ?

## Architecture Onboarding

- Component map: Parameter vector θ(t) -> Neural network parameterization ˆu(x; θ(t)) -> Jacobian matrix J(θ(t)) -> Sketching matrix St -> Sparse Jacobian Js(θ(t)) -> Least-squares solver -> Parameter update ∆θs

- Critical path: 1) Initialize network parameters θ(0) to fit initial condition 2) At each time step: sample sketching matrix St 3) Solve sparse least-squares problem to get ∆θs 4) Lift sparse update to full dimension and update θ 5) Repeat until end time

- Design tradeoffs: Sparsity level s vs accuracy (higher s gives better accuracy but higher cost); Randomization strategy (uniform vs leverage-score sampling affects performance); Network architecture (deeper networks may help but increase gradient computation cost)

- Failure signatures: Rapid error growth over time (overfitting); High residual in least-squares problems (poor tangent space approximation); Inconsistent results across random realizations (insufficient sampling)

- First 3 experiments: 1) Implement RSNG on Allen-Cahn equation with fixed s=150, compare to dense updates 2) Vary sparsity s from 50 to 800, measure accuracy vs computational cost tradeoff 3) Compare uniform sampling vs leverage-score sampling for sketching matrix St

## Open Questions the Paper Calls Out

### Open Question 1
How does RSNG scale to very high-dimensional PDEs where the rank of the Jacobian matrix J(θ(t)) might not decay as sharply? The paper mentions RSNG has been shown to be useful for high-dimensional PDEs in prior work [7], but acknowledges that more sophisticated sampling methods may be needed for higher dimensions where the rank of J(θ(t)) does not decay sharply.

### Open Question 2
What is the theoretical relationship between the sparsity parameter s and the approximation error in RSNG? Can we derive bounds on the error as a function of s? The paper shows empirically that RSNG achieves lower errors than dense updates for a range of sparsity values s, but does not provide theoretical bounds on the error as a function of s.

### Open Question 3
How does the choice of the distribution π for the sketching matrix St affect the performance of RSNG? Can we characterize which properties of the problem make certain distributions more suitable? The paper mentions that the choice of distribution π is critical and depends on properties of the Jacobian matrix J(θ(t)), but only considers uniform sampling and does not explore other distributions or characterize problem properties.

## Limitations

- The assumption of low-rank Jacobian structure may not hold for all network architectures or PDE problems
- Performance gap between RSNG and dense updates could narrow for certain problem classes
- Long-time integration stability beyond tested time horizons remains unexplored

## Confidence

- High confidence in core computational advantages and experimental results
- Medium confidence in generalizability of low-rank assumptions across different PDEs
- Medium confidence in scalability claims to very large networks

## Next Checks

1. Test RSNG on PDEs with known high-rank Jacobians to identify failure modes
2. Implement rigorous time-stepping stability analysis for long-time integration
3. Benchmark against other sequential-in-time methods beyond PINNs on challenging multi-scale problems