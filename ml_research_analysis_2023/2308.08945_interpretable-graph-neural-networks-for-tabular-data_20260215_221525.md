---
ver: rpa2
title: Interpretable Graph Neural Networks for Tabular Data
arxiv_id: '2308.08945'
source_url: https://arxiv.org/abs/2308.08945
tags:
- data
- ignnet
- graph
- neural
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IGNNet is a novel graph neural network for tabular data classification
  that maintains interpretability by constraining the model to show exactly how predictions
  are computed from input features. It represents each data instance as a graph with
  features as nodes and correlation-based edges, then uses a GNN with an interpretable
  readout function to produce transparent predictions.
---

# Interpretable Graph Neural Networks for Tabular Data

## Quick Facts
- arXiv ID: 2308.08945
- Source URL: https://arxiv.org/abs/2308.08945
- Reference count: 19
- Primary result: IGNNet achieves interpretable tabular classification with performance on par with state-of-the-art methods while producing feature importance scores aligned with true Shapley values

## Executive Summary
IGNNet introduces a novel graph neural network architecture specifically designed for tabular data classification that maintains interpretability through its transparent readout function. The model represents each data instance as a graph where features are nodes and edges encode linear correlations between features, allowing the GNN to capture feature interactions while preserving interpretability. Experiments on 35 diverse datasets demonstrate that IGNNet achieves predictive performance comparable to established methods like XGBoost and Random Forests, while providing explanations that align with true Shapley values without requiring additional computational overhead.

## Method Summary
IGNNet represents tabular data as graphs where each feature is a node and edges encode linear correlations between features. The architecture uses 6 message-passing layers with ReLU activations, batch normalization, and skip connections to learn feature embeddings, followed by a linear readout function that directly computes feature contributions to predictions. The model is trained using Adam optimization with early stopping, and interpretability is achieved by constraining the readout to be a weighted sum of feature values, where the weights can be interpreted as exact Shapley values. Categorical features are binarized using one-hot encoding, and all features are normalized to [0,1] range before graph construction.

## Key Results
- IGNNet achieves predictive performance on par with state-of-the-art methods (XGBoost, Random Forests, TabNet, MLP) across 35 datasets
- The model produces feature importance scores that align with true Shapley values without additional computational overhead
- Interpretability is maintained through a transparent readout function that explicitly shows how predictions are computed from input features

## Why This Works (Mechanism)

### Mechanism 1
IGNNet achieves interpretability by forcing a transparent readout layer where each node's contribution is explicitly visible. The final readout concatenates scalar node values after a linear transformation, then applies weighted summation with class-specific weights. This makes the prediction a linear combination of feature contributions. The core assumption is that message-passing layers preserve distinct feature identities and the readout weights can be interpreted as feature importances. This breaks if message-passing mixes features too much or introduces non-linearities that obscure individual feature effects.

### Mechanism 2
IGNNet achieves high predictive performance by capturing feature interactions through correlation-based edge weights and expressive message passing. Features are nodes, edges encode linear correlations, and multiple message-passing layers propagate interaction information while preserving feature identities. The GNN learns embeddings that capture non-linear patterns. The core assumption is that correlation-based edges are sufficient to encode relevant feature interactions for prediction. This breaks if feature interactions are primarily non-linear or higher-order, which correlation-based edges may miss.

### Mechanism 3
IGNNet's explanations align with true Shapley values because the transparent readout directly computes feature contributions. Since the readout is a weighted sum of feature values, the weights can be interpreted as exact Shapley values, avoiding the sampling approximations needed by methods like KernelSHAP. The core assumption is that the readout weights represent true marginal contributions of features to predictions. This breaks if the readout weights don't actually correspond to marginal contributions due to complex feature dependencies.

## Foundational Learning

- **Graph Neural Networks**: Why needed here - IGNNet extends GNNs to tabular data by representing features as nodes and correlations as edges. Quick check: How do GNNs typically aggregate information from neighbors in a graph?
- **Shapley values**: Why needed here - The paper evaluates explanations against true Shapley values, requiring understanding of cooperative game theory concepts. Quick check: What is the core idea behind Shapley values for feature importance?
- **Correlation-based graph construction**: Why needed here - IGNNet builds graphs from tabular data using feature correlations as edge weights. Quick check: What are the limitations of using only linear correlation to capture feature interactions?

## Architecture Onboarding

- **Component map**: Embedding layer → 6 message-passing layers (ReLU, skip connections) → FNN (linear mapping to scalars) → Concatenation → Weighted sum for prediction
- **Critical path**: Data → Graph construction → Message passing → Node projection → Prediction
- **Design tradeoffs**: Interpretability vs. expressivity - linear readout preserves interpretability but may limit modeling capacity; correlation-based edges are simple but may miss non-linear interactions
- **Failure signatures**: Poor performance suggests message-passing isn't capturing relevant patterns; explanations don't align with expectations suggests readout isn't preserving feature identities
- **First 3 experiments**: 
  1. Train on simple dataset (e.g., Iris) to verify basic functionality
  2. Compare interpretability by visualizing feature contributions vs. SHAP
  3. Test performance degradation when removing skip connections or using fewer layers

## Open Questions the Paper Calls Out

### Open Question 1
How does the predictive performance of IGNNet change when using non-linear correlation measures instead of Pearson correlation for edge weights? The paper only evaluates using linear correlation measures and suggests exploring non-linear interactions between features as a future research direction. What evidence would resolve it: Experimental results comparing IGNNet's performance using different correlation measures (e.g., Spearman, Kendall, mutual information) on a variety of tabular datasets.

### Open Question 2
How does IGNNet's interpretability and performance scale with the number of features in the tabular data? The paper evaluates on datasets with varying numbers of features (ranging from 5 to 220) but doesn't provide systematic analysis of how interpretability and performance change as features increase. What evidence would resolve it: Experiments showing performance and interpretability metrics on synthetic datasets with varying numbers of features, or theoretical analysis of model complexity as a function of input dimensionality.

### Open Question 3
How does IGNNet's interpretability and performance compare to other interpretable models when dealing with high-cardinality categorical features? The paper mentions categorical features are binarized using one-hot encoding but doesn't analyze how IGNNet handles high-cardinality categorical features compared to other interpretable models. What evidence would resolve it: Experiments comparing IGNNet's performance and interpretability on datasets with high-cardinality categorical features against other interpretable models like decision trees or rule-based models.

## Limitations

- IGNNet's performance may be limited when feature interactions are primarily non-linear or higher-order, as correlation-based edges may miss critical patterns
- The model requires specific hyperparameter configurations (correlation thresholds, self-loop weights) that are not fully specified, making exact reproduction challenging
- Interpretability relies on the assumption that readout weights directly correspond to marginal feature contributions, which may not hold for complex feature dependencies

## Confidence

- **High Confidence**: IGNNet's interpretability mechanism (readout function) and core architectural design are well-specified and theoretically sound
- **Medium Confidence**: Predictive performance claims are supported by experiments across 35 datasets, though hyperparameter sensitivity is unclear
- **Medium Confidence**: Explanation alignment with Shapley values is demonstrated, but evaluation could benefit from comparison against additional explanation methods

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary correlation thresholds (0.3-0.8) and self-loop weights (70-90%) across all datasets to identify optimal configurations and assess robustness to hyperparameter choices

2. **Comparative Explanation Evaluation**: Extend interpretability assessment beyond KernelSHAP to include SHAP with TreeExplainer and LIME on identical datasets to verify that IGNNet's explanations are consistently aligned with established methods

3. **Failure Mode Investigation**: Test IGNNet on datasets with known non-linear feature interactions (e.g., XOR patterns, multiplicative relationships) to quantify performance gap when linear correlation edges miss critical interaction patterns, and document conditions under which interpretability breaks down