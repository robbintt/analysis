---
ver: rpa2
title: Communication Efficient Private Federated Learning Using Dithering
arxiv_id: '2309.07809'
source_url: https://arxiv.org/abs/2309.07809
tags:
- clients
- privacy
- client
- communication
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes using subtractive dithering quantization to
  achieve communication-efficient private federated learning. The key idea is to apply
  random quantization with dithering at clients such that the reconstruction error
  at the server follows a Gaussian distribution, thereby replicating the effect of
  central noise addition for differential privacy.
---

# Communication Efficient Private Federated Learning Using Dithering

## Quick Facts
- arXiv ID: 2309.07809
- Source URL: https://arxiv.org/abs/2309.07809
- Reference count: 0
- Achieves 5.5-12x communication reduction while preserving accuracy with DP

## Executive Summary
This work proposes using subtractive dithering quantization to achieve communication-efficient private federated learning. The key idea is to apply random quantization with dithering at clients such that the reconstruction error at the server follows a Gaussian distribution, thereby replicating the effect of central noise addition for differential privacy. Experiments on MNIST, EMNIST, and CIFAR-10 datasets show that the proposed approach achieves the same accuracy as full-precision gradients with central noise addition while using 5.5 to 12 times less communication per gradient element.

## Method Summary
The method employs subtractive dithering quantization at clients using gamma-distributed quantization step sizes and shared randomness with the server. Each client clips gradients to L2-norm C, samples gamma-distributed step sizes, applies uniform dither, quantizes gradients, and transmits them. The server decodes using shared randomness and aggregates the reconstructed gradients. This approach ensures the quantization error is indistinguishable from Gaussian noise to any third party, enabling central DP guarantees without explicit noise addition.

## Key Results
- Achieves same accuracy as full-precision gradients with central noise addition
- Reduces communication by 5.5 to 12 times per gradient element
- Scales logarithmically with number of clients while maintaining efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subtractive dithering quantization at clients can replicate Gaussian noise addition at the server for DP.
- Mechanism: Each client quantizes its gradient element with a step size drawn from a gamma distribution and applies subtractive dithering using shared randomness. This causes the quantization error to follow a Gaussian distribution from the perspective of any third party, matching the noise distribution of explicit Gaussian addition.
- Core assumption: The step size and dither parameters are generated from distributions specified in Lemmas 1 and 2, and clients and server share the same randomness.
- Evidence anchors:
  - [abstract] "employing a quantization scheme based on subtractive dithering at the clients can effectively replicate the normal noise addition process at the aggregator"
  - [section] Lemma 1 and Lemma 2 describe the mathematical properties of the distributions that yield Gaussian quantization error
  - [corpus] Weak - related papers do not explicitly address the Gaussian error property of dithering, so the claim relies on the paper's own lemmas
- Break condition: If the shared randomness is not perfectly synchronized or the gamma distribution parameters are incorrect, the quantization error will not be Gaussian, breaking the DP equivalence.

### Mechanism 2
- Claim: Quantization with gamma-distributed step size and subtractive dithering achieves the same privacy-accuracy tradeoff as full-precision gradients with added Gaussian noise.
- Mechanism: By setting the quantization step size as ∆ = 2σ√v where v ~ Γ(3/2, 1/2), the quantization error becomes Gaussian N(0,σ²) for any third party. This allows the server to add no additional noise while preserving the same (ε,δ)-DP guarantee.
- Core assumption: The L2 sensitivity of the clipped gradients is bounded by C, and the Poisson sampling probability p is known for accurate DP accounting.
- Evidence anchors:
  - [abstract] "the noise in reconstructed gradients is indistinguishable from added Gaussian noise to any third party"
  - [section] Theorem 1 proves that the global gradient average satisfies central DP with the stated (ε,δ) formula
  - [corpus] Missing - no direct corpus evidence that this specific quantization scheme matches Gaussian noise addition; relies on paper's proof
- Break condition: If gradient clipping fails to bound sensitivity or the DP accounting formula is misapplied, the privacy guarantee may be weaker than claimed.

### Mechanism 3
- Claim: Communication cost scales logarithmically with the number of clients while preserving accuracy.
- Mechanism: The number of bits per gradient element is ⌈log₂(2⌈C/∆+1⌉)⌉, and since ∆ = 2σ√v with v ~ Γ(3/2,1/2), the average log-scale term grows slowly with N. Experiments show per-element cost increases logarithmically while accuracy stays constant.
- Core assumption: σ/√N remains constant as N grows, so the quantization granularity adapts appropriately.
- Evidence anchors:
  - [abstract] "the method is demonstrated to scale logarithmically with the number of clients, maintaining communication efficiency even in large-scale federated learning scenarios"
  - [section] Figure 1 plots communication cost vs. N showing logarithmic growth; Table 1 shows accuracy remains stable
  - [corpus] Weak - related works do not analyze the specific logarithmic scaling of this dithering scheme; relies on experimental evidence
- Break condition: If the assumption σ/√N constant fails or the distribution of v changes with N, the logarithmic scaling may not hold.

## Foundational Learning

- Concept: Differential Privacy (ε,δ)-DP
  - Why needed here: The method aims to provide central DP guarantees, so understanding the formal definition and composition properties is essential.
  - Quick check question: If an algorithm M satisfies (ε,δ)-DP, what does that imply about the indistinguishability of its outputs on adjacent datasets?

- Concept: Quantization and Dithering
  - Why needed here: Subtractive dithering is the core technique to shape quantization error into a Gaussian distribution; knowing its mathematical properties is critical.
  - Quick check question: What distribution does the quantization error follow if the step size is fixed vs. random?

- Concept: Gamma Distribution and Scale Mixtures
  - Why needed here: The gamma distribution of the quantization step size is what enables the uniform quantization error to become Gaussian when the step size is unknown.
  - Quick check question: If X|V=v ~ Unif(µ-σ√v, µ+σ√v) and V ~ Γ(3/2,1/2), what is the marginal distribution of X?

## Architecture Onboarding

- Component map: Clients generate gradients → apply gradient clipping → sample gamma-distributed step sizes → sample uniform dither → quantize and transmit → server decodes using shared randomness → aggregates → updates model
- Critical path: Gradient computation → quantization with dithering → transmission → reconstruction → aggregation
- Design tradeoffs: Fixed precision vs. adaptive quantization step size; local randomness vs. shared randomness; clipping threshold C vs. communication cost
- Failure signatures: Large discrepancies between client and server reconstructions; accuracy drops indicating privacy budget misestimation; communication cost exceeds expectations
- First 3 experiments:
  1. Run with N=2 clients, MNIST, verify that decoded gradients match originals within quantization bounds.
  2. Increase N to 10, measure communication cost per element and confirm logarithmic growth.
  3. Vary σ, measure final ε after composition and confirm DP accounting formula matches Renyi DP calculator.

## Open Questions the Paper Calls Out
- [explicit] The authors mention that "one possible area of future exploration is extending our methods to situations where trust in the PS is difficult to achieve."
- [explicit] The authors state "we clip each sample gradient in the batch so that its L2-norm is bounded by a parameter C" but do not analyze the impact of clipping on accuracy.
- [inferred] The experiments use evenly distributed datasets across clients, which does not reflect real-world federated learning scenarios with data heterogeneity.
- [explicit] The authors assume "separate sources of common randomness shared between each client and the PS" but do not discuss implementation or performance impact.

## Limitations
- The paper relies on its own lemmas without external validation to prove Gaussian error property of dithering
- Logarithmic scaling claim is primarily supported by experimental results rather than theoretical analysis
- Method assumes trusted aggregator model and doesn't evaluate under untrusted scenarios

## Confidence
- High confidence in the experimental demonstration of communication reduction (5.5-12x) and accuracy preservation
- Medium confidence in the DP equivalence mechanism due to reliance on paper-specific proofs without external validation
- Medium confidence in the logarithmic scaling claim based on empirical results but lacking theoretical bounds

## Next Checks
1. Verify the distribution of quantization error under the proposed scheme matches a Gaussian distribution within statistical significance thresholds across multiple runs
2. Test the method's behavior under imperfect shared randomness scenarios to quantify the impact on privacy guarantees and accuracy
3. Theoretically derive or empirically validate the upper bound on communication cost growth rate to confirm logarithmic scaling holds across wider ranges of client numbers