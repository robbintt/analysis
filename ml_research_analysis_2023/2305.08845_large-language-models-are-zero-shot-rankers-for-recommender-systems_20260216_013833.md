---
ver: rpa2
title: Large Language Models are Zero-Shot Rankers for Recommender Systems
arxiv_id: '2305.08845'
source_url: https://arxiv.org/abs/2305.08845
tags:
- llms
- ranking
- candidate
- historical
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the potential of large language models
  (LLMs) to act as zero-shot rankers for recommender systems. The authors formalize
  the recommendation problem as a conditional ranking task, where LLMs are given sequential
  interaction histories as conditions and a set of candidate items to rank.
---

# Large Language Models are Zero-Shot Rankers for Recommender Systems

## Quick Facts
- arXiv ID: 2305.08845
- Source URL: https://arxiv.org/abs/2305.08845
- Reference count: 40
- Large Language Models can rank items zero-shot for recommender systems using sequential interaction histories

## Executive Summary
This paper explores the potential of large language models (LLMs) as zero-shot rankers for recommender systems. The authors formalize recommendation as a conditional ranking task, where LLMs are given user interaction histories and candidate items to rank. Through extensive experiments on MovieLens-1M and Amazon Games datasets, they demonstrate that LLMs can effectively leverage historical behaviors for personalized ranking, though they struggle with perceiving the order of sequential interactions. The study reveals that carefully designed prompting strategies and bootstrapping can significantly improve LLM ranking performance, outperforming existing zero-shot recommendation methods, particularly on candidates retrieved by multiple generation models.

## Method Summary
The paper formulates recommendation as a conditional ranking task where LLMs are prompted with user interaction histories and candidate items. The method uses GPT-3.5-turbo via OpenAI API with temperature=0.2, implementing three prompting strategies: sequential (using full interaction history), recency-focused (emphasizing recent interactions), and in-context learning (providing examples). The approach applies bootstrapping by ranking the same candidate set multiple times with different random orders to mitigate position bias. Experiments use two datasets (MovieLens-1M and Amazon Games) with up to 50 most recent interactions and evaluate performance using NDCG@N against baseline methods including popularity-based, matrix factorization, sequential models, and BM25.

## Key Results
- LLMs can utilize historical behaviors for personalized ranking but struggle to perceive the order of sequential interaction histories
- Specially designed prompting and bootstrapping strategies can significantly alleviate these issues
- LLMs outperform existing zero-shot recommendation methods, especially on candidates retrieved by multiple candidate generation models
- LLMs suffer from position bias and popularity bias, which can be mitigated by prompting or bootstrapping strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can capture user preferences from sequential interaction histories when prompted correctly
- Mechanism: The LLM processes the sequence of past interactions as conditioning context and uses its pre-trained knowledge to infer relevance of candidate items
- Core assumption: The LLM's pre-trained knowledge includes sufficient information to relate historical items to new candidates
- Evidence anchors:
  - [abstract]: "LLMs can utilize historical behaviors for personalized ranking, but struggle to perceive the order of the given sequential interaction histories"
  - [section 3.1]: "LLMs can effectively leverage prompts with historical user behaviors to make personalized recommendations"
  - [corpus]: Weak evidence - no direct comparison studies found in neighbors
- Break condition: If the LLM lacks relevant pre-training data for the item domain (e.g., niche products), personalization degrades significantly

### Mechanism 2
- Claim: Sequential prompting alone is insufficient; recency-focused prompting or in-context learning can trigger order perception
- Mechanism: Additional instructions emphasizing the most recent interaction or demonstrating examples with ordered sequences guide the LLM to treat the interaction list as temporally significant
- Core assumption: LLMs can be prompted to switch from treating all history equally to prioritizing recent events through specific linguistic cues
- Evidence anchors:
  - [abstract]: "specially designed prompting and bootstrapping strategies can alleviate these issues"
  - [section 3.1]: "both recency-forced prompting and in-context learning can improve the ranking performance of LLMs"
  - [corpus]: No direct evidence in neighbors; inferred from LLM instruction-following literature
- Break condition: If the instruction template is too rigid or the examples provided don't align with the user's interaction pattern, the order perception may not be triggered

### Mechanism 3
- Claim: Bootstrapping across multiple random permutations of candidates reduces position bias in LLM ranking
- Mechanism: By ranking the same candidate set multiple times with different random orders and aggregating scores, each item's average rank becomes less dependent on its position in the prompt
- Core assumption: The LLM's bias toward earlier positions in the prompt is consistent and can be averaged out through repeated sampling
- Evidence anchors:
  - [abstract]: "LLMs suffer from position bias and popularity bias while ranking, which can be alleviated by prompting or bootstrapping strategies"
  - [section 3.3]: "we follow the setting in Section 3.1 and apply the bootstrapping strategy to Ours. Each candidate set will be ranked for 3 times"
  - [corpus]: No direct bootstrapping evidence in neighbors; standard NLP mitigation strategy assumed
- Break condition: If the LLM's position bias varies drastically between runs or if computational cost of multiple generations is prohibitive

## Foundational Learning

- Concept: Zero-shot learning
  - Why needed here: The LLM ranks without task-specific fine-tuning, relying only on its pre-trained knowledge and prompt instructions
  - Quick check question: What distinguishes zero-shot from few-shot prompting in LLM-based ranking?

- Concept: Prompt engineering
  - Why needed here: Careful template design (e.g., sequential vs recency-focused vs ICL) directly affects LLM performance on ranking tasks
  - Quick check question: How does adding an explicit "most recent" statement change the LLM's ranking behavior?

- Concept: Candidate generation pipelines
  - Why needed here: Understanding how items are retrieved (content-based, interaction-based, hybrid) is essential to interpreting LLM ranking performance across different candidate sets
  - Quick check question: Why might LLM ranking degrade on candidates from popularity-based generators?

## Architecture Onboarding

- Component map:
  - User interaction history → Prompt construction → LLM ranking API → Output parsing → Ranked list
  - Candidate generation models → Merged candidate pool → Shuffling → Prompt inclusion

- Critical path:
  - Construct prompt with user history and candidates → Send to LLM → Parse ranked output → Return top-N recommendations

- Design tradeoffs:
  - Prompt length vs. detail: Longer histories give more context but risk overwhelming the LLM and degrading order perception
  - Number of candidates vs. cost: Larger candidate sets increase ranking difficulty and API cost without clear benefit per [section 3.2]
  - Bootstrapping iterations vs. latency: More iterations reduce bias but increase response time

- Failure signatures:
  - Ground-truth item consistently ranked low regardless of prompt variation → LLM not leveraging history effectively
  - Rankings vary drastically with candidate order → Position bias too strong for simple bootstrapping to mitigate
  - Performance drops with longer interaction histories → LLM struggles with long context or treats all items equally

- First 3 experiments:
  1. Compare sequential prompting vs. no history vs. fake history to confirm history utilization
  2. Test recency-focused prompting vs. in-context learning on a small dataset to identify which better triggers order perception
  3. Measure position bias by fixing the ground-truth item at different prompt positions and observing rank variance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs handle longer interaction histories beyond the tested 50 interactions?
- Basis in paper: [explicit] The paper states "We vary the number of latest historical user behaviors (|H|) used for constructing the prompt from 5 to 50" and finds performance drops with longer histories
- Why unresolved: The experiments only tested up to 50 interactions, but real-world user histories can be much longer. The impact of even longer sequences remains unknown
- What evidence would resolve it: Testing LLMs with varying lengths of interaction histories (e.g., 100, 200, 500 interactions) to identify performance trends and potential saturation points

### Open Question 2
- Question: Can LLMs effectively rank candidates when the candidate set size exceeds 50 items?
- Basis in paper: [explicit] The paper notes "LLMs cannot rank candidates well when the candidate set is large" and shows performance degradation as candidate count increases
- Why unresolved: The experiments only tested up to 50 candidates, leaving uncertainty about LLM performance in scenarios with hundreds or thousands of candidates typical in real-world systems
- What evidence would resolve it: Evaluating LLM ranking performance with progressively larger candidate sets (e.g., 100, 500, 1000 items) to determine scalability limits

### Open Question 3
- Question: What specific components of instruction tuning most improve LLM recommendation performance?
- Basis in paper: [explicit] The paper shows that instruction tuning improves ranking abilities but doesn't identify which specific aspects are most critical
- Why unresolved: While instruction tuning is shown to help, the paper doesn't isolate which elements (task descriptions, formatting, examples) drive the improvements
- What evidence would resolve it: Conducting ablation studies testing LLMs with different instruction tuning variants (no examples, different example types, different task descriptions) to identify key contributors to performance gains

## Limitations

- Lack of detailed prompt templates makes exact reproduction challenging
- The parsing heuristic for extracting ranked items from LLM outputs is not specified
- Performance on very long interaction histories (>50 items) remains untested
- Cost-effectiveness compared to traditional methods is not quantified

## Confidence

- **High confidence**: LLMs can utilize historical interactions for personalized ranking when prompted appropriately (supported by empirical results across two datasets)
- **Medium confidence**: Bootstrapping effectively reduces position bias (theoretical justification is sound, but empirical variance analysis is limited)
- **Medium confidence**: LLMs outperform existing zero-shot methods on multi-source candidate sets (based on limited comparisons, generalization to other domains unclear)

## Next Checks

1. **Order Perception Validation**: Systematically test recency-focused prompting vs. in-context learning on varying sequence lengths to determine which method more reliably captures temporal patterns in user behavior

2. **Position Bias Analysis**: Conduct statistical tests measuring rank variance of fixed items across different prompt positions to quantify the magnitude and consistency of position bias before and after bootstrapping

3. **Domain Generalization Test**: Evaluate the prompting strategies on a dataset from a different domain (e.g., book recommendations or music streaming) to assess whether the observed performance gains transfer beyond movies and games