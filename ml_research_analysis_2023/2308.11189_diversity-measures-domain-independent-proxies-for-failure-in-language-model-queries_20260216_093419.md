---
ver: rpa2
title: 'Diversity Measures: Domain-Independent Proxies for Failure in Language Model
  Queries'
arxiv_id: '2308.11189'
source_url: https://arxiv.org/abs/2308.11189
tags:
- failure
- prompt
- probability
- diversity
- measures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors introduce three diversity measures\u2014entropy, Gini\
  \ impurity, and centroid distance\u2014to quantify uncertainty in large language\
  \ model (LLM) outputs, offering a domain-independent way to predict failures in\
  \ question-answering tasks. These measures correlate strongly with failure probability\
  \ across three datasets and multiple temperature settings, with R\xB2 values often\
  \ exceeding 0.8."
---

# Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries

## Quick Facts
- **arXiv ID**: 2308.11189
- **Source URL**: https://arxiv.org/abs/2308.11189
- **Reference count**: 7
- **Primary result**: Entropy, Gini impurity, and centroid distance strongly correlate with LLM query failure probability (R² > 0.8) across three datasets

## Executive Summary
This paper introduces three domain-independent diversity measures—entropy, Gini impurity, and centroid distance—that quantify uncertainty in large language model outputs and predict failure probability in question-answering tasks. The measures correlate strongly with failure rates across three datasets and multiple temperature settings, with R² values often exceeding 0.8. The approach requires no access to the LLM or domain-specific knowledge, making it broadly applicable. Key applications include diversity-based prompt selection (improving failure rates by up to 45%) and error prediction (AUPRC up to 0.703). The method also synergizes with chain-of-thought prompting to reduce failure probability by 30%.

## Method Summary
The method queries each prompt multiple times using self-consistency prompting, then computes diversity measures from the response set. For set-based outputs, entropy and Gini impurity are calculated from the token frequency distribution. For vector-based outputs, sentence-BERT embeddings are used with centroid distance as the diversity measure. These measures are then correlated with failure probability (where failure is defined as the majority-voted answer being incorrect). The approach is evaluated on three datasets—CSQA (9,741 samples), DRAW-1K (1,000 samples), and LL (3,000 samples)—across multiple temperature settings. Applications include prompt selection by choosing the prompt yielding lowest diversity, and error prediction using supervised models trained on diversity features.

## Key Results
- Entropy and Gini impurity measures show strong linear correlation with failure probability (R² values: 0.808, 0.973, 0.876 for CSQA, DRAW-1K, LL respectively)
- Diversity-based prompt selection improves failure rates by up to 45% compared to baseline prompts
- Error prediction models using diversity features achieve AUPRC up to 0.703
- The approach reduces failure probability by 30% when combined with chain-of-thought prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entropy and Gini impurity measures of the output set are strong predictors of failure probability in LLM responses.
- Mechanism: When an LLM is uncertain about the correct answer, it generates diverse responses. The diversity of these responses, quantified by entropy or Gini impurity, correlates with the probability that the majority-voted answer is incorrect.
- Core assumption: The diversity of responses to a prompt is directly related to the uncertainty of the LLM about the correct answer.
- Evidence anchors:
  - [abstract] These measures correlate strongly with failure probability across three datasets and multiple temperature settings, with R² values often exceeding 0.8.
  - [section] Entropy results are shown in Figure 1 while Gini impurity results are shown in Figure 2. Entropy measures appeared to be strongly related to the probability of failure for cumulative plots in both directions. In most cases, the relationship appeared linear with R² values (for cumulative minimum values with temperature of 0.7) at 0.808 for CSQA, 0.973 for DRAW-1K, and 0.876 for LL.
- Break condition: If the LLM's output distribution is uniform by design (e.g., at very high temperatures), diversity measures may no longer indicate failure probability.

### Mechanism 2
- Claim: Centroid distance in vector space can serve as a diversity measure for non-set-based outputs.
- Mechanism: By embedding LLM responses into a vector space and computing the average distance to the centroid, we obtain a measure of how spread out the responses are. Greater spread indicates higher uncertainty and thus higher failure probability.
- Core assumption: The vector embedding function preserves semantic differences relevant to correctness.
- Evidence anchors:
  - [abstract] For vector-based output, the embed function was implemented using sentence-BERT.
  - [section] Vector-Based Output. Here we look to evaluate each ai by examining its vector embedding of dimension D. We will use a distance function d : RD × RD → R+ to compare two vectors of size D. Given the vectors created by the embed function embed(a1), . . . , embed(am) we can calculate a D dimension centroid cent.
- Break condition: If the embedding space collapses semantically distinct answers into nearby vectors, centroid distance may fail to reflect true diversity.

### Mechanism 3
- Claim: Diversity-based prompt selection improves LLM reliability by choosing prompts that yield low-diversity (i.e., high-confidence) responses.
- Mechanism: By generating multiple candidate prompts and selecting the one that produces the lowest entropy or Gini impurity response, we can reduce the likelihood of failure.
- Core assumption: Lower diversity in responses indicates higher confidence and thus higher likelihood of correctness.
- Evidence anchors:
  - [abstract] Applications include diversity-based prompt selection, which improves failure rates by up to 45% compared to baseline prompts.
  - [section] We introduce the concept of diversity-based prompt selection (Section 4.1). Experimentally (Section 5.4), we show in two datasets, significant improvement (14%, 45%) over baselines (and near the baseline in a third).
- Break condition: If all candidate prompts produce similarly high diversity, selection may not yield improvement.

## Foundational Learning

- **Concept: Self-consistency prompting**
  - Why needed here: The diversity measures are computed over multiple responses generated by self-consistency prompting, which forms the basis for measuring uncertainty.
  - Quick check question: What is the difference between self-consistency and majority voting in LLM prompting?

- **Concept: Entropy and Gini impurity in information theory**
  - Why needed here: These are the mathematical foundations for quantifying the diversity of LLM responses.
  - Quick check question: How does entropy differ from Gini impurity when applied to a probability distribution over answer tokens?

- **Concept: Vector embeddings and distance metrics**
  - Why needed here: For non-set-based outputs, responses are embedded into vector space and their spread is measured using distance metrics.
  - Quick check question: Why might cosine distance be preferred over Euclidean distance for comparing sentence embeddings?

## Architecture Onboarding

- **Component map**: LLM interface -> Prompt generator -> Embedding model -> Diversity measure calculator -> Failure prediction model -> Evaluation pipeline

- **Critical path**:
  1. Generate m responses per prompt using self-consistency
  2. Compute diversity measures (entropy/Gini for set-based; centroid distance for vector-based)
  3. For prompt selection: choose prompt with lowest diversity measure
  4. For error prediction: train supervised model on diversity features
  5. Evaluate failure rate and prediction performance

- **Design tradeoffs**:
  - Set-based vs. vector-based diversity: Set-based is simpler but coarse; vector-based is richer but depends on embedding quality
  - Number of samples (m): Higher m improves measure stability but increases cost
  - Embedding choice: sentence-BERT is used, but other models may yield different results

- **Failure signatures**:
  - High entropy/Gini or high centroid distance → likely failure
  - Low diversity across all prompts → method may not help
  - Inconsistent performance across datasets → domain sensitivity

- **First 3 experiments**:
  1. Compute entropy and Gini impurity on a small dataset (e.g., 10 prompts, m=5) and plot against failure rate
  2. Repeat with vector-based outputs using sentence-BERT embeddings and centroid distance
  3. Implement diversity-based prompt selection and compare failure rates to random prompt selection

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The method requires generating multiple responses per prompt (m=20 in experiments), which increases computational cost and may limit practical applicability
- The strong correlations observed may be specific to GPT-3.5 and may not generalize to other LLM architectures or domains beyond the three studied datasets
- The approach assumes that diversity directly indicates uncertainty about correctness, which may not hold for tasks where multiple valid answers exist or where the LLM's training data contains contradictory information

## Confidence

- **High confidence**: The core mechanism linking diversity measures to failure probability is well-supported by experimental results across three datasets
- **Medium confidence**: The effectiveness of diversity-based prompt selection and chain-of-thought prompting synergies is demonstrated but may be dataset-dependent
- **Medium confidence**: Error prediction performance (AUPRC up to 0.703) is promising but requires further validation on more diverse failure modes

## Next Checks

1. **Cross-model validation**: Test whether entropy, Gini impurity, and centroid distance maintain their predictive power when applied to responses from different LLM architectures (e.g., GPT-4, Claude, Llama) and with varying model sizes.

2. **Domain transferability test**: Apply the diversity measures to a new domain (e.g., medical diagnosis or legal reasoning) not represented in the original datasets to assess whether the strong correlations persist across fundamentally different task types.

3. **Cost-benefit analysis**: Systematically vary the number of samples m to determine the point of diminishing returns where additional queries no longer significantly improve failure prediction accuracy, establishing practical guidelines for implementation.