---
ver: rpa2
title: Post-training Quantization for Text-to-Image Diffusion Models with Progressive
  Calibration and Activation Relaxing
arxiv_id: '2311.06322'
source_url: https://arxiv.org/abs/2311.06322
tags:
- diffusion
- quantization
- stable
- coco
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a novel post-training quantization (PTQ) method,
  PCR (Progressive Calibration and Relaxing), for text-to-image diffusion models,
  specifically targeting Stable Diffusion and Stable Diffusion XL. PCR consists of
  two key components: progressive calibration, which considers accumulated quantization
  errors across timesteps, and activation relaxing, which improves image fidelity
  and text-image matching with negligible additional cost.'
---

# Post-training Quantization for Text-to-Image Diffusion Models with Progressive Calibration and Activation Relaxing

## Quick Facts
- arXiv ID: 2311.06322
- Source URL: https://arxiv.org/abs/2311.06322
- Reference count: 39
- Key outcome: Proposes PCR (Progressive Calibration and Relaxing) for PTQ of text-to-image diffusion models, achieving state-of-the-art results on Stable Diffusion and Stable Diffusion XL.

## Executive Summary
This paper introduces PCR, a novel post-training quantization method for text-to-image diffusion models that addresses the accumulated quantization error across timesteps through progressive calibration. The method also employs activation relaxing, where a small proportion of timesteps are assigned higher bitwidth to improve image fidelity and text-image matching. PCR is evaluated on Stable Diffusion and Stable Diffusion XL, demonstrating superior performance compared to existing methods while maintaining high image quality and text-image matching.

## Method Summary
PCR consists of two key components: progressive calibration and activation relaxing. Progressive calibration considers accumulated quantization error across timesteps by using quantized activations for calibration at each step. Activation relaxing improves performance by assigning higher bitwidth (e.g., 10-bit) to a small proportion of timesteps near x0 (for fidelity) or near xT (for text-image matching). The method is evaluated using a novel benchmark, QDiffBench, which uses FID to FP32 and CLIP score for more accurate assessment of quantized models.

## Key Results
- PCR achieves state-of-the-art results on both Stable Diffusion and Stable Diffusion XL, outperforming existing methods.
- Progressive calibration improves quantization by accounting for accumulated timestep-wise quantization error.
- Activation relaxing with only 5-20% of timesteps at higher bitwidth significantly improves image fidelity and text-image matching with negligible additional cost.
- QDiffBench's FID to FP32 metric is more accurate than FID to COCO for evaluating quantized text-to-image diffusion models.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Progressive calibration improves quantization by accounting for accumulated quantization error across denoising steps.
- Mechanism: At timestep t, the quantization is performed using activations that have already been quantized in previous steps (t+1 to T), ensuring the calibration data reflects the true distribution after quantization-induced shifts.
- Core assumption: The quantization error at each step propagates and accumulates linearly through the denoising trajectory, altering subsequent activation distributions.
- Evidence anchors:
  - [abstract] "progressive calibration strategy that considers the accumulated quantization error across timesteps"
  - [section] Theorem 1 and Corollary 1 prove that the final quantization error is the linear combination of errors at each timestep, and thus minimizing per-step errors requires using quantized activations for calibration.
  - [corpus] Q-Diffusion: Quantizing Diffusion Models and Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models both study quantization for diffusion models but do not explicitly model accumulated timestep-wise error in calibration.
- Break condition: If the denoising trajectory is non-linear or if error accumulation is negligible (e.g., very few steps), the progressive approach offers no benefit over independent timestep calibration.

### Mechanism 2
- Claim: Relaxing a small proportion of timesteps to higher bitwidth improves image fidelity and text-image matching with negligible cost.
- Mechanism: By assigning higher bitwidth (e.g., 10-bit) to activations near x0 (for fidelity) or near xT (for text-image matching), the method reduces quantization-induced distortion in the most sensitive regions of the denoising trajectory.
- Core assumption: Fidelity is more sensitive to quantization near x0, while text-image matching is more sensitive near xT, allowing targeted relaxation without uniform cost increase.
- Evidence anchors:
  - [abstract] "activation relaxing strategy that improves the performance with negligible cost"
  - [section] Section 3.2 states that relaxing only 5% of steps to 10-bit increases average bitwidth from 8-bit to 8.1-bit, yet improves performance, with empirical evidence from Stable Diffusion XL experiments.
  - [corpus] Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models does not propose timestep-specific bitwidth allocation; this mechanism is novel to PCR.
- Break condition: If the model is not sensitive to timestep position (e.g., very short or uniform denoising), the relaxation strategy may not yield gains.

### Mechanism 3
- Claim: QDiffBench's FID to FP32 metric is more accurate than FID to COCO for evaluating quantized text-to-image diffusion models.
- Mechanism: By comparing generated images from the quantized model directly to those from the full-precision model (both in the same domain), the metric isolates fidelity loss due to quantization, avoiding distribution/style mismatch with real COCO images.
- Core assumption: The domain gap between generated images (trained on LAION) and real COCO images introduces noise that masks true quantization degradation, making FID to COCO unreliable.
- Evidence anchors:
  - [abstract] "QDiffBench benchmark, which utilizes data in the same domain for more accurate evaluation"
  - [section] Section 4.1 demonstrates that quantized models can have better "FID to COCO" than full-precision models despite visibly worse outputs, proving the metric's inaccuracy; "FID to FP32" aligns better with human perception and CLIP scores.
  - [corpus] Q-Diffusion: Quantizing Diffusion Models uses FID to COCO; PCR's approach is distinct in addressing domain mismatch explicitly.
- Break condition: If generated and real images are from the same distribution (e.g., both from the same dataset), FID to COCO may become valid.

## Foundational Learning

- Concept: Quantization-aware calibration data collection
  - Why needed here: Diffusion models' activations change drastically across timesteps; calibration must reflect post-quantization distributions to minimize accumulated error.
  - Quick check question: If calibration data is collected using full-precision activations, what error does the progressive calibration strategy specifically avoid?

- Concept: Bitwidth allocation and cost-benefit tradeoff in model compression
  - Why needed here: Relaxing only a small proportion of timesteps to higher bitwidth yields fidelity gains with negligible increase in memory/compute cost.
  - Quick check question: Why does relaxing 5% of steps from 8-bit to 10-bit only increase the average bitwidth from 8-bit to 8.1-bit?

- Concept: Domain alignment in generative model evaluation
  - Why needed here: Evaluation metrics must compare outputs in the same data distribution to avoid misleading scores caused by style/distribution mismatch.
  - Quick check question: What problem arises when comparing quantized model outputs to real images from a different dataset?

## Architecture Onboarding

- Component map: Calibration Data -> Progressive Calibration (quantize timesteps sequentially) -> Activation Relaxing (adjust bitwidths per timestep) -> Generate Images -> Evaluate (FID to FP32, CLIP score)
- Critical path: Calibration Data -> Progressive Calibration (quantize timesteps sequentially) -> Activation Relaxing (adjust bitwidths per timestep) -> Generate Images -> Evaluate (FID to FP32, CLIP score)
- Design tradeoffs: Progressive calibration increases calibration complexity but improves accuracy; relaxing timesteps adds minimal cost but requires identifying sensitive regions; FID to FP32 requires generating full-precision references.
- Failure signatures: Poor FID to FP32 despite good FID to COCO indicates domain mismatch; CLIP score improvement without FID improvement may suggest better text alignment but fidelity loss; relaxing too many timesteps increases memory overhead without proportional gains.
- First 3 experiments:
  1. Implement progressive calibration on a simple unconditional diffusion model and compare FID to FP32 vs FID to COCO against baseline timestep-independent calibration.
  2. Apply activation relaxing to a quantized text-to-image model, relaxing 5% near x0, and measure fidelity (FID) and text-image matching (CLIP) gains.
  3. Validate QDiffBench by generating a small set of images with both full-precision and quantized models, then compute FID to FP32 and compare with human visual assessment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of bit-width allocation across timesteps affect the overall performance and computational efficiency of the PCR method?
- Basis in paper: [explicit] The paper discusses the relaxation of timesteps to higher bit-widths to improve fidelity or text-image matching, with specific examples of relaxing 5% to 20% of steps.
- Why unresolved: The paper provides some results but does not explore a comprehensive range of bit-width allocations or their effects on different types of images or prompts.
- What evidence would resolve it: Detailed experiments varying the proportion of relaxed timesteps and the specific timesteps chosen for relaxation, along with their impact on image quality and computational cost.

### Open Question 2
- Question: What is the impact of using different calibration datasets on the performance of the PCR method, and how does it affect the generalization ability to unseen prompts?
- Basis in paper: [explicit] The paper introduces QDiffBench to evaluate prompt-generalization ability and mentions using COCO and Stable-Diffusion-Prompts datasets.
- Why unresolved: The paper does not provide a thorough analysis of how different calibration datasets influence the model's performance or its ability to generalize to diverse prompts.
- What evidence would resolve it: Comparative studies using various calibration datasets and their effects on the model's performance and generalization, including metrics like FID and CLIP scores.

### Open Question 3
- Question: How does the PCR method perform when applied to other types of diffusion models beyond text-to-image models, such as video or 3D generation models?
- Basis in paper: [inferred] The paper focuses on text-to-image diffusion models like Stable Diffusion and Stable Diffusion XL, suggesting potential applicability to other domains.
- Why unresolved: The paper does not explore the application of PCR to other types of diffusion models, leaving its effectiveness in these areas untested.
- What evidence would resolve it: Experiments applying PCR to video or 3D generation models, with evaluations of performance improvements and any necessary adaptations for these models.

## Limitations

- The progressive calibration algorithm's complexity and computational overhead during calibration are not fully quantified, potentially limiting scalability to larger models or longer timesteps.
- The choice of which timesteps to relax (near x0 for SD, near xT for SDXL) is empirically motivated but lacks theoretical justification for why these regions are most sensitive to quantization error.
- The effectiveness of PCR on other diffusion model variants or domains beyond text-to-image generation remains unexplored.

## Confidence

- **High Confidence**: The core claims about progressive calibration improving FID to FP32 and the superiority of FID to FP32 over FID to COCO for evaluating quantized models are well-supported by theoretical analysis (Theorem 1, Corollary 1) and empirical evidence (Section 4.1).
- **Medium Confidence**: The claims about activation relaxing providing fidelity and text-image matching improvements with negligible cost are supported by experiments on SDXL, but the generalizability to other models or tasks requires further validation.
- **Medium Confidence**: The benchmark QDiffBench is a valuable contribution, but its comprehensiveness depends on the diversity and size of the evaluation datasets used.

## Next Checks

1. **Validate Progressive Calibration Efficiency**: Implement PCR on a smaller diffusion model (e.g., DDPM on CIFAR-10) and measure the computational overhead of progressive calibration compared to baseline timestep-independent calibration. Track calibration time and memory usage.
2. **Test Activation Relaxing Generalizability**: Apply activation relaxing to a different text-to-image model (e.g., DeepFloyd IF) and evaluate whether relaxing timesteps near x0 or xT consistently improves fidelity or text-image matching, regardless of model architecture.
3. **Expand QDiffBench Scope**: Extend QDiffBench by evaluating PCR on a conditional image generation task (e.g., ImageNet classification) and compare FID to FP32 with traditional FID to real images. Assess whether the domain alignment principle holds across different generative modeling tasks.