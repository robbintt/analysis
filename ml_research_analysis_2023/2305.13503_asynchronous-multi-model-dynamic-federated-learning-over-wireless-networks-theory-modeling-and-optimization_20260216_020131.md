---
ver: rpa2
title: 'Asynchronous Multi-Model Dynamic Federated Learning over Wireless Networks:
  Theory, Modeling, and Optimization'
arxiv_id: '2305.13503'
source_url: https://arxiv.org/abs/2305.13503
tags:
- device
- local
- global
- task
- devices
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MA-FL, a framework for multi-model asynchronous
  federated learning over heterogeneous wireless networks. The authors propose a novel
  convergence analysis using scheduling tensors to capture device scheduling in an
  arbitrary order, removing strict assumptions on device participation.
---

# Asynchronous Multi-Model Dynamic Federated Learning over Wireless Networks: Theory, Modeling, and Optimization

## Quick Facts
- arXiv ID: 2305.13503
- Source URL: https://arxiv.org/abs/2305.13503
- Reference count: 40
- Primary result: MA-FL framework achieves higher accuracy with lower energy consumption compared to baseline methods through optimized device scheduling and resource allocation.

## Executive Summary
This paper introduces MA-FL, a framework for multi-model asynchronous federated learning over heterogeneous wireless networks. The authors propose a novel convergence analysis using scheduling tensors to capture device scheduling in an arbitrary order, removing strict assumptions on device participation. They formulate an optimization problem for jointly configuring resource allocation and device scheduling to balance energy consumption and ML performance, solved via successive convex approximations. Numerical simulations demonstrate that MA-FL significantly improves the performance-efficiency tradeoff compared to baseline methods.

## Method Summary
The MA-FL framework consists of a convergence analysis using scheduling tensors to characterize asynchronous multi-model FL, followed by an optimization problem that jointly configures resource allocation (CPU frequency, mini-batch size, SGD iterations) and device scheduling (through ğ‘… and ğ‘ˆ variables) to balance accuracy and energy consumption. The non-convex mixed-integer optimization is solved via successive convex approximation, which iteratively convexifies constraints and objective using proximal gradient methods. The method is evaluated through simulations with 10 devices and 3 classification tasks (SVHN, MNIST, Fashion-MNIST).

## Key Results
- MA-FL achieves higher accuracy with lower energy consumption compared to conventional synchronous FedAvg and asynchronous FL baselines
- The framework effectively balances task importance through weighting parameters ğ›¾ğ‘—
- Successive convex approximation successfully solves the non-convex mixed-integer optimization problem
- Device scheduling optimization adapts to heterogeneous network conditions and device capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Asynchronous scheduling with scheduling tensors enables model convergence without requiring all devices to participate at each global aggregation.
- Mechanism: The scheduling tensor captures which devices receive and upload model updates at each global aggregation step, allowing the server to perform partial aggregations using only available devices. This removes the strict assumption that all devices must be active simultaneously, which is critical in heterogeneous wireless networks where device availability varies.
- Core assumption: The convergence bound remains valid under arbitrary device participation order as captured by the scheduling tensor.
- Evidence anchors:
  - [abstract]: "We first characterize convergence via introducing scheduling tensors and rectangular functions to capture the impact of system parameters on learning performance."
  - [section 3.2]: "We define ğ‘… (ğ‘”)
ğ‘–,ğ‘— = 1 if device
ğ‘– receives w(ğ‘”)
ğ‘— for task ğ‘—... ğ‘ˆ (ğ‘”)
ğ‘–,ğ‘— = 1 if device ğ‘– uploads its local model parameter at ğ‘”-th global aggregation..."
  - [corpus]: Weak evidence. The corpus neighbors discuss asynchronous FL but do not specifically address multi-model scheduling tensors.
- Break condition: If the staleness ğ¾ğ‘— becomes too large (exceeding the bound in equation 77), the convergence degrades significantly due to outdated model information.

### Mechanism 2
- Claim: Device scheduling optimization balances energy consumption against model performance by controlling which devices train which tasks and when.
- Mechanism: The optimization problem P jointly determines device scheduling (through ğ‘… and ğ‘ˆ variables) and resource allocation (CPU frequency, mini-batch size, SGD iterations). By assigning higher importance weights ğ›¾ğ‘— to certain tasks, the solver allocates more resources and schedules those tasks more frequently, while energy budgets constrain how often devices can participate.
- Evidence anchors:
  - [abstract]: "We formulate an optimization for jointly configuring resource allocation and device scheduling to strike an efficient trade-off between energy consumption and ML performance"
  - [section 4.2.2]: "Upon increasing ğ›¾ ğ‘— for task ğ‘—, the solution will allocate more communication and computation resources... the scheduling variables will favor more frequent reception of model parameters of task ğ‘—"
  - [corpus]: Weak evidence. Corpus papers discuss scheduling but not the multi-task energy-performance tradeoff formulation.
- Break condition: If the energy budget constraint (23) is too tight, the solver may be unable to schedule sufficient device participation to achieve target accuracy.

### Mechanism 3
- Claim: Successive convex approximation enables solving the non-convex mixed-integer optimization by iteratively convexifying constraints and objective.
- Mechanism: The method relaxes integer scheduling variables to continuous values, then convexifies the non-convex parts of the objective and constraints using proximal gradient methods. At each iteration, a convex surrogate problem is solved, and variables are updated using a proximal step.
- Evidence anchors:
  - [section 4.3.3]: "We solve bP through a sequence of approximations indexed by ğ‘š... At each iteration ğ‘š, we convexify bP at the current solution ğ’—ğ‘š to obtain a surrogate problem"
  - [section 4.3.2]: "We denote the objective function of P as O(ğ’—)... constraints of P can be divided into four vectors: convex equalities CEQ, convex inequalities CIE, nonconvex equalities NEQ, and nonconvex inequalities NIE"
  - [corpus]: Weak evidence. The corpus does not discuss solution methods for the optimization problem.
- Break condition: If the initial point ğ’—0 is too far from the feasible region, the iterative method may fail to converge or converge to a poor solution.

## Foundational Learning

- Concept: Stochastic Gradient Descent (SGD) and mini-batch training
  - Why needed here: Local model updates at devices use mini-batch SGD (equation 3), and the convergence analysis depends on understanding SGD noise and variance.
  - Quick check question: In equation 3, what does ğµâ„“,(ğ‘”)
ğ‘–,ğ‘— represent and why is it important for convergence?

- Concept: Federated Learning aggregation rules
  - Why needed here: The global aggregation (equation 5) combines local models using weighted averaging, and the convergence analysis builds on this aggregation structure.
  - Quick check question: How does the aggregation weight ğ›¼ ğ‘— in equation 5 affect the convergence speed compared to FedAvg?

- Concept: Non-convex optimization and convex relaxations
  - Why needed here: The optimization problem P is non-convex mixed-integer, requiring relaxation and successive convex approximation for solution.
  - Quick check question: Why can't we directly solve the integer scheduling variables in P without relaxation?

## Architecture Onboarding

- Component map:
  - Devices -> Server -> Network -> Scheduler -> Optimizer
  - Devices train multiple tasks locally using mini-batch SGD
  - Server maintains global models and performs asynchronous aggregations
  - Network handles wireless communication with bandwidth constraints
  - Scheduler determines device participation through scheduling tensor
  - Optimizer solves resource allocation and scheduling problem

- Critical path:
  1. Server broadcasts initial global models to selected devices
  2. Devices perform local training with mini-batch SGD
  3. Devices upload trained models to server
  4. Server performs weighted aggregation and updates global models
  5. Repeat until convergence or budget exhausted

- Design tradeoffs:
  - Synchronous vs asynchronous: Synchronous achieves higher accuracy but wastes resources on stragglers; asynchronous improves efficiency but may slow convergence.
  - Task importance weighting: Emphasizing certain tasks improves their performance but may degrade others and increase energy consumption.
  - Local vs global updates: More local SGD iterations reduce communication but may increase bias from heterogeneous data.

- Failure signatures:
  - Slow convergence: May indicate insufficient device participation, poor scheduling, or staleness ğ¾ğ‘— too large.
  - High energy consumption: May indicate overly frequent scheduling or insufficient energy budget constraints.
  - Model divergence: May indicate ğ›¿ (ğ‘”)
ğ‘–,ğ‘— too large (high data heterogeneity) or ğœŒ too small (insufficient regularization).

- First 3 experiments:
  1. Baseline comparison: Run MA-FL with optimized scheduling versus conventional synchronous FedAvg and asynchronous FL, measuring accuracy vs energy consumption tradeoff.
  2. Task importance impact: Vary ğ›¾ ğ‘— values for different tasks and observe performance gains and energy costs for emphasized vs regular tasks.
  3. Device heterogeneity stress test: Run with high device heterogeneity (large ğ›¿ (ğ‘”)
ğ‘–,ğ‘— values) and observe how scheduling adapts to prioritize devices with lower model dissimilarity.

## Open Questions the Paper Calls Out
No open questions were explicitly called out in the paper.

## Limitations
- The convergence analysis assumes i.i.d. gradient noise and bounded staleness, which may not hold in practice
- The optimization framework requires accurate knowledge of device capabilities and channel conditions
- The successive convex approximation method may struggle to find global optima for highly non-convex problems
- The framework's scalability to large numbers of devices and tasks is not evaluated

## Confidence
- High confidence in Mechanism 1 (asynchronous scheduling with scheduling tensors)
- Medium confidence in Mechanism 2 (energy-performance tradeoff optimization)
- Medium confidence in Mechanism 3 (successive convex approximation solution method)

## Next Checks
1. **Empirical convergence validation**: Run experiments with varying levels of device heterogeneity and staleness to verify that the convergence bound accurately predicts actual performance degradation.

2. **Robustness to parameter uncertainty**: Evaluate the framework's performance when device capabilities and channel conditions are estimated with error, testing the sensitivity of the optimization solution.

3. **Scalability testing**: Assess the framework's performance as the number of devices and tasks increases, examining whether the computational complexity of the optimization remains tractable.