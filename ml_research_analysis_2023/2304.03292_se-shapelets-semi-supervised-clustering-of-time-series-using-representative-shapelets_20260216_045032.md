---
ver: rpa2
title: 'SE-shapelets: Semi-supervised Clustering of Time Series Using Representative
  Shapelets'
arxiv_id: '2304.03292'
source_url: https://arxiv.org/abs/2304.03292
tags:
- time
- series
- clustering
- shapelets
- ss-shapelets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of discovering representative
  shapelets for effective time series clustering, which existing methods struggle
  with due to analyzing large pools of uninformative subsequences. The proposed SS-shapelets
  method uses a small number of labeled and propagated pseudo-labeled time series
  to discover shapelets that can map time series of the same classes into distinct
  and compact groups.
---

# SE-shapelets: Semi-supervised Clustering of Time Series Using Representative Shapelets

## Quick Facts
- arXiv ID: 2304.03292
- Source URL: https://arxiv.org/abs/2304.03292
- Reference count: 40
- Primary result: SS-shapelets achieves average Rand Index of 0.778 on UCR datasets, outperforming semi-supervised baselines

## Executive Summary
This paper addresses the challenge of discovering representative shapelets for effective time series clustering. Existing shapelet-based methods struggle with analyzing large pools of uninformative subsequences. The proposed SS-shapelets method leverages a small number of labeled and pseudo-labeled time series to guide the discovery of shapelets that map time series of the same class into distinct, compact groups. Through extensive experiments on UCR time series datasets, SS-shapelets demonstrates superior clustering accuracy compared to counterpart semi-supervised methods.

## Method Summary
SS-shapelets introduces a semi-supervised approach that first propagates labels from a small labeled set to unlabeled time series using nearest-neighbor assignment. It then extracts salient subsequences from these labeled/pseudo-labeled series using a Salient Subsequence Chain (SSC) algorithm that selects non-overlapping, maximally distinct subsequences. A Linear Discriminant Selection (LDS) algorithm selects representative shapelets from these candidates by maximizing between-class scatter while minimizing within-class scatter. Finally, all time series are mapped to distance-to-shapelet representations and clustered using spectral clustering.

## Key Results
- Achieves average Rand Index of 0.778 across UCR datasets, outperforming CSSC (0.722) and other baselines
- Demonstrates better scalability than unsupervised shapelet-based clustering methods
- Maintains high accuracy even with only 5% labeled data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SS-shapelets improves clustering accuracy by using labeled/pseudo-labeled data to guide shapelet discovery, overcoming the limitation of existing methods that rely on uninformative subsequences.
- Mechanism: The method first propagates labels from a small set of labeled time series to unlabeled ones using nearest-neighbor assignment, creating a richer set of pseudo-labeled examples. It then uses a Salient Subsequence Chain (SSC) to extract informative subsequences from these labeled/pseudo-labeled series, filtering out noise and redundancy. Finally, a Linear Discriminant Selection (LDS) algorithm selects shapelets that maximize separation between classes while maintaining compactness within classes.
- Core assumption: The nearest-neighbor label propagation yields confident pseudo-labels, and the extracted salient subsequences are representative of local class-specific patterns.
- Evidence anchors:
  - [abstract] "SS-shapelets method, which utilizes a small number of labeled and propagated pseudo-labeled time series to help discover representative shapelets"
  - [section IV-C] Describes the pseudo-label propagation process in detail
  - [corpus] Weak: No direct corpus neighbor explicitly validates label propagation effectiveness in this context.
- Break condition: If label propagation assigns incorrect pseudo-labels (e.g., due to ambiguous nearest neighbors), the subsequent SSC and LDS steps will operate on misleading patterns, degrading clustering accuracy.

### Mechanism 2
- Claim: The Salient Subsequence Chain (SSC) algorithm efficiently extracts the most informative local features from time series by selecting non-overlapping subsequences that maximize temporal contrast.
- Mechanism: For each labeled/pseudo-labeled series, the algorithm constructs a subsequence graph where nodes are possible subsequences and edges connect non-overlapping pairs weighted by their Euclidean distance. It then finds the longest path (in negated weights) of length k using dynamic programming, ensuring the selected subsequences are temporally ordered and maximally distinct.
- Core assumption: Temporal ordering and non-overlapping constraints preserve meaningful local patterns without redundancy, and maximizing inter-subsequence distances captures salient features.
- Evidence anchors:
  - [section IV-C] Provides formal definition of SSC and explains the graph construction and dynamic programming solution
  - [section IV-C] Shows an example in Figure 3 illustrating extracted salient subsequences
  - [corpus] Weak: No direct corpus neighbor validates this specific chain-based salient extraction approach.
- Break condition: If the time series contain highly repetitive or noise-dominated patterns, the distance-based salience metric may select uninformative or misleading subsequences, reducing the quality of shapelet candidates.

### Mechanism 3
- Claim: Linear Discriminant Selection (LDS) algorithm selects shapelets that map time series of the same class into compact, well-separated groups, optimizing for clustering convenience.
- Mechanism: After generating candidate shapelets via clustering salient subsequences, LDS formulates a binary selection matrix problem where each shapelet can be chosen at most once. It maximizes the ratio of between-class scatter to within-class scatter using a trace-based objective, effectively selecting shapelets that best discriminate the classes while keeping intra-class distances small.
- Core assumption: The class labels (original + pseudo) are accurate enough for scatter matrix computation, and maximizing the between-class/within-class scatter ratio aligns with clustering objectives.
- Evidence anchors:
  - [section IV-D] Details the LDS objective function and matrix formulation
  - [section IV-D] Explains the use of scatter matrices derived from distance-to-shapelet representations
  - [corpus] Weak: No direct corpus neighbor explicitly validates this specific LDA-based shapelet selection for clustering.
- Break condition: If the label set is too small or noisy, the scatter matrix estimation becomes unreliable, and LDS may select shapelets that overfit to mislabeled data, harming clustering performance.

## Foundational Learning

- Concept: Time series z-normalization and subsequence extraction
  - Why needed here: Shapelet discovery and distance calculations require scale-invariant comparisons; subsequences are the building blocks for shapelets.
  - Quick check question: Given a time series [1,2,3,4,5], what is the z-normalized value of the subsequence [2,3,4]?
- Concept: Dynamic Time Warping (DTW) and Euclidean distance for time series
  - Why needed here: While the method uses Euclidean distance for subsequence-to-subsequence comparisons, understanding DTW helps contextualize why local subsequence patterns are effective for clustering.
  - Quick check question: Why might Euclidean distance be preferred over DTW for comparing short, aligned subsequences in shapelet discovery?
- Concept: Spectral clustering basics and distance-to-shapelet representation
  - Why needed here: The final clustering step transforms each time series into a k-dimensional vector of distances to selected shapelets, then applies spectral clustering.
  - Quick check question: How does representing a time series as distances to shapelets enable clustering in a low-dimensional space?

## Architecture Onboarding

- Component map: Input -> Pseudo-label propagation -> Salient Subsequence Chain (SSC) -> Candidate shapelet clustering -> Linear Discriminant Selection (LDS) -> Distance-to-shapelet mapping -> Spectral clustering -> Output clusters
- Critical path:
  1. Propagate labels to unlabeled series (nearest neighbor assignment)
  2. For each labeled/pseudo-labeled series, extract k salient subsequences via FindChain (SSC)
  3. Cluster extracted subsequences to generate candidate shapelets
  4. Apply LDS to select k representative shapelets
  5. Map all series to distance-to-shapelet representations
  6. Run spectral clustering on the mapped data
- Design tradeoffs:
  - SSC vs random subsequence selection: SSC reduces candidate pool size and improves quality but adds computational cost (O(km²) per series)
  - LDS vs heuristic selection: LDS provides theoretically grounded discrimination but requires O(cγ²n) time; simpler heuristics may be faster but less discriminative
  - Label propagation strategy: Simple nearest-neighbor is fast and effective here but may mislabel ambiguous cases; more sophisticated propagation could improve pseudo-label quality at cost of complexity
- Failure signatures:
  - Poor clustering accuracy with many datasets: likely indicates bad shapelet selection (LDS or candidate quality issue)
  - High variance in running time across datasets: likely due to SSC extraction scaling poorly with series length or number of candidates
  - Clusters contain mixed classes: indicates shapelets are not sufficiently discriminative (possibly due to noisy pseudo-labels)
- First 3 experiments:
  1. Run SS-shapelets with k=3, ˆl=0.1l, λ=0.1 on Coffee dataset; verify clustering RI=1.0 and inspect the two discovered shapelets visually
  2. Remove the SSC step (use all subsequences as candidates) and measure degradation in clustering accuracy on 5 small datasets
  3. Replace LDS with random selection from candidates and measure accuracy drop to confirm LDS effectiveness

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unaddressed.

## Limitations
- Limited validation of pseudo-label propagation quality beyond simple nearest-neighbor assignment
- Lack of comprehensive ablation studies to isolate contributions of SSC and LDS components
- No thorough parameter sensitivity analysis or scalability testing on larger datasets

## Confidence
- Theoretical foundation: High confidence
- SSC algorithm effectiveness: Medium confidence (limited ablation studies)
- LDS algorithm effectiveness: Medium confidence (no direct corpus validation)
- Overall scalability: Low confidence (not thoroughly investigated)

## Next Checks
1. Conduct ablation studies to isolate the contributions of SSC and LDS by comparing SS-shapelets performance with and without these components.
2. Perform parameter sensitivity analysis by varying key parameters (e.g., number of shapelets, salient subsequence length) and measuring their impact on clustering accuracy and computational efficiency.
3. Compare SS-shapelets with additional state-of-the-art semi-supervised time series clustering methods on a larger set of benchmark datasets to validate its superiority and generalizability.