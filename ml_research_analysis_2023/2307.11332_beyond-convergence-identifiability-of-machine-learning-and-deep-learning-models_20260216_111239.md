---
ver: rpa2
title: 'Beyond Convergence: Identifiability of Machine Learning and Deep Learning
  Models'
arxiv_id: '2307.11332'
source_url: https://arxiv.org/abs/2307.11332
tags:
- data
- learning
- parameters
- identifiability
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores identifiability in machine learning and deep
  learning models through a case study on human gait parameter estimation. Using Geyer's
  bipedal-spring mass model, synthetic motion sensor data is generated to estimate
  subject-wise parameters (mass, stiffness, equilibrium leg length) via deep neural
  networks.
---

# Beyond Convergence: Identifiability of Machine Learning and Deep Learning Models

## Quick Facts
- arXiv ID: 2307.11332
- Source URL: https://arxiv.org/abs/2307.11332
- Reference count: 5
- Key outcome: Mass and stiffness parameters in gait estimation are individually unidentifiable; only their ratio is identifiable from motion sensor data

## Executive Summary
This paper investigates identifiability issues in machine learning and deep learning models through a case study on human gait parameter estimation. Using Geyer's bipedal-spring mass model, the authors generate synthetic motion sensor data to estimate subject-specific parameters (mass, stiffness, equilibrium leg length) via deep neural networks. The study reveals that while equilibrium leg length is identifiable from the data, mass and stiffness parameters individually are not identifiable due to their combined appearance in the model equations. This unidentifiability is shown to be an intrinsic property of the data model rather than a limitation of the learning algorithm.

## Method Summary
The study uses Geyer's bipedal-spring mass model to generate 1,000,000 synthetic motion records (15 seconds each at 0.1s intervals) with parameters: mass ~ U[60,75] kg, stiffness ~ U[8000,10000] N/m, and equilibrium leg length ~ U[0.6,0.8] m. A 7-layer fully connected neural network with 50 nodes per layer is trained to estimate parameters from (x,y) position data. The training uses 80% of data, with 10% for validation and 10% for testing, running for 500 epochs with batch size 1000. The study compares performance when estimating (m,k,ℓ₀) versus (ρ,ℓ₀) where ρ = k/m.

## Key Results
- The model successfully estimates equilibrium leg length ℓ₀ from motion data, demonstrating this parameter is identifiable
- Individual mass (m) and stiffness (k) parameters are unidentifiable, as the network fails to converge to ground truth values
- The ratio ρ = m/k is identifiable, with the model achieving high accuracy in estimating this combined parameter

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bipedal-spring mass model's parameters (m, k) are individually unidentifiable because the governing equations depend only on their ratio ρ = k/m, not on m or k separately.
- Mechanism: When normalizing the equations of motion by mass m, the parameters m and k combine into a single term ρ = k/m. This means that any pair (m, k) with the same ratio produces identical motion dynamics, making it mathematically impossible to distinguish between them from the observed (x, y) data alone.
- Core assumption: The data model accurately represents the physical system and all observable states are included in the (x, y) measurements.
- Evidence anchors:
  - [abstract]: "results show that while certain parameters can be identified from the observation data, others remain unidentifiable"
  - [section II.A]: "Only the ratio of m/k impacts the motion dynamics and the observed (x, y), and not m and k alone"
  - [corpus]: Found 25 related papers with average neighbor FMR=0.496, suggesting moderate relevance of identifiability research in ML literature

### Mechanism 2
- Claim: Deep learning models can successfully estimate identifiable parameters (like ℓ0) but fail to converge to ground truth values for unidentifiable parameters (m, k) even with abundant training data.
- Mechanism: The neural network learns to map input-output relationships present in the training data. For identifiable parameters, there exists a unique mapping from observations to parameters. For unidentifiable parameters, multiple parameter combinations produce identical outputs, so the network cannot learn a unique mapping regardless of training data quantity.
- Core assumption: The synthetic data generation process faithfully represents the underlying physics and the neural network architecture is sufficiently expressive to capture identifiable relationships.
- Evidence anchors:
  - [abstract]: "The results show that while certain parameters can be identified from the observation data, others remain unidentifiable"
  - [section IV.A]: "The model achieves high accuracy in estimating the equilibrium leg length ℓ0 from the observation data (x, y), indicating that this parameter is identifiable. However, for parameters m and k, the model's performance is less satisfactory"
  - [corpus]: The corpus includes papers on identifiability in ML and deep learning, suggesting this is an active research area

### Mechanism 3
- Claim: Transforming unidentifiable parameters into identifiable combinations (like ρ = m/k) allows successful parameter estimation.
- Mechanism: By recognizing that m and k only appear as a ratio in the equations, we can reformulate the problem to estimate ρ instead of m and k separately. This reduces the parameter space to identifiable quantities that the model can learn.
- Core assumption: The ratio ρ captures all the information about m and k that is present in the data, and no additional information is needed to distinguish between different (m, k) pairs with the same ratio.
- Evidence anchors:
  - [abstract]: "only their ratio is identifiable"
  - [section IV.B]: "the model successfully estimates the ratio ρ = m/k with high accuracy, highlighting the model's capability to identify this parameter despite the unidentifiability of m and k individually"
  - [corpus]: The corpus contains papers on parameter optimization and identifiability, supporting the relevance of this approach

## Foundational Learning

- Concept: System and control theory identifiability
  - Why needed here: Understanding the mathematical foundations of parameter identifiability is crucial for recognizing when parameters can or cannot be uniquely determined from data
  - Quick check question: Given a linear system y = Ax where A is a matrix, under what conditions is A identifiable from input-output data pairs (x, y)?

- Concept: Data model formulation and normalization
  - Why needed here: The process of normalizing equations by mass m reveals the true parameter dependencies and helps identify which parameters are individually identifiable
  - Quick check question: If you have a system with parameters (a, b, c) and the equations contain only a+b and c, which parameters are identifiable from the data?

- Concept: Neural network regression for parameter estimation
  - Why needed here: Understanding how neural networks learn input-output mappings is essential for interpreting why they succeed or fail at parameter estimation tasks
  - Quick check question: What happens when a neural network tries to learn a function where multiple inputs map to the same output value?

## Architecture Onboarding

- Component map: Synthetic data generation -> Neural network training -> Parameter estimation -> Validation on test set
- Critical path: Geyer's model -> Synthetic data generation -> Neural network architecture (7 layers, 50 nodes each) -> Training (500 epochs, batch size 1000) -> Evaluation
- Design tradeoffs: Using synthetic data ensures ground truth but may not capture real-world noise and complexities; simpler architectures might not capture complex relationships but are easier to interpret
- Failure signatures: High training accuracy but poor test performance on individual parameters (m, k) indicates unidentifiability rather than overfitting; successful estimation of ℓ0 but failure on m and k suggests the issue is intrinsic to the data model
- First 3 experiments:
  1. Train the model with varying amounts of training data (10k, 100k, 1M records) to confirm that unidentifiability persists regardless of data volume
  2. Add Gaussian noise to the synthetic data at different levels (0%, 1%, 5%, 10%) to assess robustness and determine if noise affects identifiable vs unidentifiable parameter estimation differently
  3. Modify the neural network architecture (different depths, widths, activation functions) to verify that the unidentifiability is not due to model capacity limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific identifiability metrics could be developed to quantitatively assess model identifiability in deep learning architectures?
- Basis in paper: [explicit] The paper mentions "Developing metrics to quantitatively assess model identifiability can aid in predicting and diagnosing unidentifiable models" as a future research direction
- Why unresolved: The paper acknowledges this as an open research area but doesn't propose specific metrics or mathematical formulations for quantifying identifiability in deep learning models
- What evidence would resolve it: A concrete mathematical framework or computational method that can measure identifiability scores for arbitrary deep learning architectures, validated across multiple benchmark problems

### Open Question 2
- Question: How can model-based machine learning approaches be systematically integrated with deep learning architectures to address unidentifiability in complex systems?
- Basis in paper: [explicit] The paper suggests "By integrating physics informed (model-based) machine learning with real-world data and leveraging theoretical insights on identifiable/unidentifiable models and systems" as a solution
- Why unresolved: While the paper identifies this as a promising approach, it doesn't provide specific architectural designs, training methodologies, or empirical validation of how physics-informed constraints can resolve unidentifiability in deep networks
- What evidence would resolve it: Empirical studies showing improved identifiability in deep learning models when incorporating physics-based constraints, with quantitative comparisons to standard approaches

### Open Question 3
- Question: What are the theoretical bounds on identifiability when multiple competing models with different levels of abstraction exist for the same system?
- Basis in paper: [explicit] The paper notes "models are not necessarily accurate or unique, and multiple models, with different levels of abstraction can be considered for a single problem" and asks "what can be stated regarding the identifiability of systems, for which multiple models coexist"
- Why unresolved: The paper raises this as an open question but doesn't provide theoretical framework or mathematical conditions for determining identifiability when multiple models describe the same system
- What evidence would resolve it: A formal mathematical theory establishing conditions under which identifiability can be guaranteed or bounded when multiple models are available, potentially including model selection criteria based on identifiability properties

## Limitations
- The use of synthetic data may not capture real-world complexities and noise patterns present in actual motion sensor data
- The neural network architecture represents only one possible model choice among many, limiting generalizability
- Results are specific to the bipedal-spring mass model and may not extend to other biomechanical models without additional validation

## Confidence

- High confidence: The theoretical analysis showing that m and k only appear as a ratio in the normalized equations is mathematically rigorous and well-established.
- Medium confidence: The deep learning results demonstrating successful ℓ0 estimation but failure on m and k individually, as these depend on the specific neural network implementation and training procedure.
- Medium confidence: The proposed solutions (parameter transformation, multimodal data fusion) are theoretically sound but require empirical validation in the gait context.

## Next Checks
1. Test the same identifiability analysis on real-world gait sensor data to verify that synthetic data findings translate to practical scenarios.
2. Implement and compare alternative neural network architectures (CNNs, RNNs) to confirm that the unidentifiability is intrinsic to the data model rather than a limitation of the chosen architecture.
3. Design experiments using multimodal data (e.g., combining motion data with force plate measurements) to assess whether additional data sources can break the unidentifiability of m and k.