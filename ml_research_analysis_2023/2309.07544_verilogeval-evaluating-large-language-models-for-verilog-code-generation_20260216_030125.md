---
ver: rpa2
title: 'VerilogEval: Evaluating Large Language Models for Verilog Code Generation'
arxiv_id: '2309.07544'
source_url: https://arxiv.org/abs/2309.07544
tags:
- verilog
- code
- language
- arxiv
- descriptions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents VerilogEval, a comprehensive evaluation framework
  for assessing the performance of large language models (LLMs) in Verilog code generation.
  The framework addresses the limitations of existing benchmarks by providing a diverse
  dataset of 156 problems from HDLBits, covering various aspects of digital circuit
  design.
---

# VerilogEval: Evaluating Large Language Models for Verilog Code Generation

## Quick Facts
- arXiv ID: 2309.07544
- Source URL: https://arxiv.org/abs/2309.07544
- Reference count: 36
- Primary result: VerilogEval benchmark shows that supervised fine-tuning with synthetic data significantly improves LLM performance on Verilog code generation tasks

## Executive Summary
This paper introduces VerilogEval, a comprehensive evaluation framework for assessing large language models' capabilities in Verilog code generation. The framework addresses critical limitations in existing benchmarks by providing a diverse dataset of 156 problems from HDLBits, covering various aspects of digital circuit design. VerilogEval introduces an automated testing environment using Icarus Verilog simulator to compare generated code outputs against golden solutions, enabling reliable measurement of functional correctness. The paper demonstrates that supervised fine-tuning with synthetically generated problem-code pairs significantly enhances LLM performance, with the best models achieving results comparable to GPT-3.5 and GPT-4 on Verilog coding tasks.

## Method Summary
The VerilogEval framework evaluates LLMs using a dataset of 156 Verilog problems from HDLBits, focusing on self-contained modules without inter-module dependencies. The evaluation employs automated testing through Icarus Verilog simulator, comparing simulation outputs between generated code and golden reference solutions at clock edges and input transitions. The framework uses pass@k metrics (pass@1, pass@5, pass@10) to measure performance, where a problem is solved if any of k samples pass functional testing. Supervised fine-tuning is performed using synthetic problem-code pairs generated by LLMs, with training parameters including Adam optimizer, learning rate 2e-5, batch size 1M tokens, and 5-10 epochs depending on the model.

## Key Results
- Supervised fine-tuning with synthetic data significantly improves pass@1, pass@5, and pass@10 metrics across different model sizes
- Best-performing model achieves results on par with GPT-3.5 and GPT-4 models for Verilog code generation
- Pass@5 and pass@10 metrics show more substantial improvements than pass@1, indicating increased diversity in generated solutions
- Codegen models fine-tuned with Verilog-specific synthetic data outperform base models and general-purpose LLMs on Verilog tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-contained module generation is achievable because the benchmark isolates Verilog tasks to individual modules without inter-module dependencies
- Mechanism: By restricting evaluation to self-contained modules, the model only needs to handle internal signal definitions, combinational logic, and finite state machines without reasoning about module hierarchies or interface contracts
- Core assumption: Removing inter-module instantiation simplifies the problem space enough for LLMs to generate functionally correct Verilog
- Evidence anchors:
  - [abstract] "We define a Verilog module as self-contained if the module implementation does not require instantiation of any other modules"
  - [section] "We emphasize the significance of module instantiation as a crucial capability in Verilog, playing an essential role in constructing extensive system-level designs. It's important to note that our evaluation does not delve into this topic."
  - [corpus] Weak - corpus neighbors focus on module instantiation and system-level design, suggesting this is a known limitation
- Break condition: If problems require multiple interacting modules or system-level integration, the self-contained assumption breaks down

### Mechanism 2
- Claim: Supervised fine-tuning with synthetic problem-code pairs improves performance because it aligns model outputs with task-specific patterns
- Mechanism: LLMs generate problem descriptions from existing Verilog modules, creating aligned training pairs that teach the model to map natural language descriptions to syntactically correct Verilog implementations
- Core assumption: Generated problem descriptions maintain semantic alignment with their corresponding Verilog code despite potential verbosity or low-level detail
- Evidence anchors:
  - [abstract] "We constructed a synthetic supervised fine-tuning dataset by leveraging LLMs to generate problem descriptions paired with Verilog code"
  - [section] "By providing problem-code pairs, this data facilitates better alignment of the model, resulting in improved outcomes for VerilogEval-machine"
  - [corpus] Weak - corpus neighbors focus on verification and PPA optimization rather than synthetic data generation
- Break condition: If generated descriptions contain ambiguities or errors that propagate to the training data, the alignment benefit diminishes

### Mechanism 3
- Claim: Automated simulation-based testing enables reliable evaluation because it compares functional outputs against golden solutions
- Mechanism: The sandbox environment runs generated Verilog through Icarus Verilog simulator, comparing simulation traces at clock edges or input transitions to detect functional correctness
- Core assumption: Simulation-based comparison at signal transitions provides sufficient coverage to detect functional errors in generated code
- Evidence anchors:
  - [section] "We compare simulation results between generated code completions with golden reference solutions. We assert for output signal correctness at clock (posedge and/or negedge) transition edges for sequential circuits"
  - [abstract] "The Verilog code completions can be automatically tested for functional correctness by comparing the transient simulation outputs of the generated design with a golden solution"
  - [corpus] Weak - corpus neighbors focus on verification techniques but don't discuss automated simulation testing frameworks
- Break condition: If the simulator doesn't support all Verilog features or test patterns miss corner cases, functional errors may go undetected

## Foundational Learning

- Concept: Digital circuit fundamentals (combinational logic, sequential logic, finite state machines)
  - Why needed here: VerilogEval problems span from simple combinational circuits to complex FSMs, requiring understanding of digital design principles
  - Quick check question: Can you explain the difference between combinational and sequential logic in hardware design?

- Concept: Hardware description language syntax and semantics
  - Why needed here: The model must generate syntactically correct Verilog with proper module structure, signal declarations, and behavioral constructs
  - Quick check question: What are the key components of a Verilog module declaration and why are they important?

- Concept: Simulation-based verification methodology
  - Why needed here: Automated testing relies on comparing simulation outputs, requiring understanding of testbench construction and signal observation
  - Quick check question: How would you design a testbench to verify a Verilog module's functional correctness?

## Architecture Onboarding

- Component map: HDLBits problem repository → Problem description generation → LLM inference → Icarus Verilog simulation → Result comparison → Performance metrics
- Critical path: Problem description → LLM generation → Code compilation → Simulation → Golden solution comparison → Pass/fail determination
- Design tradeoffs: Self-contained modules vs. system-level complexity; synthetic data generation vs. manual curation; simulation testing vs. formal verification
- Failure signatures: High pass@1 but low pass@10 indicates overfitting to synthetic data; inconsistent results across model sizes suggests insufficient training; BLEU score misalignment shows evaluation metric limitations
- First 3 experiments:
  1. Run a simple combinational circuit problem through the complete pipeline to verify basic functionality
  2. Test the same problem with different model sizes to observe scaling effects
  3. Compare pass@1 vs pass@5 vs pass@10 metrics to assess diversity in generated solutions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs perform on module instantiation tasks in Verilog?
- Basis in paper: [explicit] The authors emphasize that module instantiation is a crucial capability in Verilog for constructing complex system-level designs, but their current evaluation does not include this topic.
- Why unresolved: The VerilogEval benchmark focuses on self-contained Verilog modules and does not address module instantiation tasks.
- What evidence would resolve it: Developing a new benchmark that includes module instantiation tasks and evaluating LLM performance on this benchmark would provide insights into their capabilities in this area.

### Open Question 2
- Question: Can LLMs generate synthesizable Verilog code that adheres to formatting standards?
- Basis in paper: [inferred] The authors mention that their testing environment only assesses functional correctness and does not ensure that the generated Verilog code adheres to synthesizable formatting standards.
- Why unresolved: The current evaluation framework focuses on functional correctness but does not verify synthesizability of the generated code.
- What evidence would resolve it: Implementing a synthesis check in the testing environment and evaluating LLM-generated code for synthesizability would provide evidence of their ability to generate synthesizable Verilog code.

### Open Question 3
- Question: How do LLMs perform on optimizing Power, Performance, and Area (PPA) metrics in hardware design?
- Basis in paper: [inferred] The authors mention that a significant portion of the hardware design process revolves around optimizing PPA metrics, but LLMs' performance in this area is not evaluated.
- Why unresolved: The current evaluation framework focuses on code generation and functional correctness, but does not address PPA optimization.
- What evidence would resolve it: Developing a benchmark that includes PPA optimization tasks and evaluating LLM performance on this benchmark would provide insights into their capabilities in optimizing hardware designs for power, performance, and area.

## Limitations

- Self-contained module restriction limits applicability to real-world hardware designs requiring inter-module communication
- Synthetic data generation process lacks detailed validation of quality and semantic alignment
- Evaluation framework focuses solely on functional correctness without considering hardware metrics like PPA
- Benchmark size of 156 problems may not provide sufficient coverage of the full Verilog design space

## Confidence

**High confidence**: The automated simulation-based testing methodology is sound and the correlation between pass@k metrics and model performance is well-established. The improvement trends across different model sizes and fine-tuning approaches are consistent and measurable.

**Medium confidence**: The synthetic data generation approach shows promise but lacks detailed validation of the quality and alignment of generated problem descriptions. The effectiveness of 5-10 epochs of fine-tuning is supported by results but could benefit from more extensive hyperparameter studies.

**Low confidence**: The generalizability of results to real-world hardware design scenarios is limited by the self-contained module restriction. The benchmark's coverage of diverse Verilog coding patterns may be insufficient for comprehensive evaluation.

## Next Checks

1. **Test system-level module instantiation**: Design experiments that evaluate model performance on problems requiring multiple interacting modules and inter-module communication to assess real-world applicability beyond self-contained modules.

2. **Validate synthetic data quality**: Implement human evaluation of a sample of synthetically generated problem-code pairs to measure semantic alignment and identify potential propagation of ambiguities or errors into the training data.

3. **Expand benchmark coverage**: Develop additional problem categories that test advanced Verilog features like parameterized modules, generate blocks, and complex timing constraints to evaluate model performance on the full spectrum of hardware design patterns.