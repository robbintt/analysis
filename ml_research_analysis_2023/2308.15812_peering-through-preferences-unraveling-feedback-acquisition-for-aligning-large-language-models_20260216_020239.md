---
ver: rpa2
title: 'Peering Through Preferences: Unraveling Feedback Acquisition for Aligning
  Large Language Models'
arxiv_id: '2308.15812'
source_url: https://arxiv.org/abs/2308.15812
tags:
- feedback
- response
- ratings
- rankings
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the impact of sparse feedback protocols (ratings
  vs rankings) on aligning and evaluating large language models (LLMs). The authors
  find that preferences inferred from ratings and rankings significantly disagree
  60% of the time for both human and AI annotators, leading to an inconsistency problem.
---

# Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models

## Quick Facts
- **arXiv ID**: 2308.15812
- **Source URL**: https://arxiv.org/abs/2308.15812
- **Reference count**: 40
- **Key outcome**: Preferences inferred from ratings and rankings significantly disagree 60% of the time for both human and AI annotators, leading to an inconsistency problem that affects LLM evaluation.

## Executive Summary
This paper investigates how different sparse feedback protocols (ratings vs rankings) impact the alignment and evaluation of large language models. The authors uncover a fundamental "feedback inconsistency" problem where human and AI annotators systematically disagree between the two protocols, with preferences inferred from ratings and rankings disagreeing 60% of the time. This inconsistency propagates to LLM evaluation, creating an "evaluation inconsistency" where the choice of feedback protocol used for alignment affects which model appears better under different evaluation protocols. The findings reveal critical gaps in current methods for evaluating real-world utility of language models.

## Method Summary
The authors collected ratings and rankings feedback from both humans and AI (GPT-3.5-Turbo) on Alpaca-7B model responses to 5.2K instructions. They trained two types of reward models: a regression model on ratings data and a negative log-likelihood model on rankings data. These reward models were then used to generate Best-of-n policies for evaluation against a reference model (DaVinci-003) using both human and AI feedback protocols. The key innovation is comparing how feedback protocol choice affects both alignment quality and evaluation consistency.

## Key Results
- Preferences inferred from ratings and rankings significantly disagree 60% of the time for both human and AI annotators
- Models trained on rankings data are preferred over those trained on ratings data when evaluated with a rankings protocol, but not with a ratings protocol
- No correlation found between response length/unique words and feedback scores, ruling out surface-level biases
- AI annotators (GPT-3.5-Turbo) show consistent preference patterns with human annotators

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Ratings feedback creates a stronger quantitative signal for reward modeling than rankings, leading to better alignment in ratings-based evaluation.
- **Mechanism**: The regression reward model trained on ratings can capture the magnitude of preference differences between responses, while the NLL reward model trained on rankings only learns ordinal preferences. This richer signal improves alignment performance when evaluated under ratings protocol.
- **Core assumption**: The regression objective can effectively learn from numerical ratings to predict response quality, and this prediction correlates with human evaluation under ratings protocol.
- **Evidence anchors**:
  - [abstract] "Models trained on rankings data are preferred over those trained on ratings data when evaluated with a rankings protocol, but not with a ratings protocol"
  - [section] "We choose low-rank adaptation (LoRA) of Alpaca-7B as the reward model" and "We use the objective function defined in Eq. 2 to train the reward model on the rankings feedback data"
  - [corpus] Weak - corpus lacks direct comparison of regression vs NLL reward modeling performance
- **Break condition**: If the ratings data is too sparse or the numerical scores don't correlate well with actual quality, the regression model may overfit or learn spurious patterns.

### Mechanism 2
- **Claim**: Rankings feedback leads to better alignment performance when evaluated under rankings protocol due to the pairwise comparison structure matching the evaluation method.
- **Mechanism**: The NLL reward model trained on rankings learns to discriminate between pairs of responses, which directly translates to better performance in pairwise evaluation against a reference model.
- **Core assumption**: The pairwise preference structure in rankings feedback is preserved and effectively learned by the NLL objective, leading to better discrimination in pairwise evaluation.
- **Evidence anchors**:
  - [abstract] "Models trained on rankings data are preferred over those trained on ratings data when evaluated with a rankings protocol"
  - [section] "We train the reward models on the ratings feedback data and the rankings feedback data collected from GPT-3.5-Turbo"
  - [corpus] Moderate - corpus contains related work on preference optimization but not specific to rankings vs ratings evaluation mismatch
- **Break condition**: If the rankings feedback contains too many ties or the preference signal is weak, the NLL model may not learn meaningful distinctions.

### Mechanism 3
- **Claim**: The feedback inconsistency problem arises because different feedback protocols capture different aspects of response quality, leading to systematic disagreement in preferences.
- **Mechanism**: Human and AI annotators assess responses differently under ratings vs rankings protocols - for example, rating denser responses higher but preferring accuracy in pairwise judgments. This creates systematic inconsistencies between the two feedback types.
- **Core assumption**: Annotators have different cognitive processes when assigning absolute scores versus making pairwise comparisons, leading to different preference judgments for the same responses.
- **Evidence anchors**:
  - [abstract] "preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators"
  - [section] "We uncover a feedback inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators"
  - [corpus] Strong - contains multiple related papers on preference learning and feedback acquisition
- **Break condition**: If the feedback protocols are redesigned to capture the same underlying quality dimensions, or if annotators are trained to use consistent criteria across protocols.

## Foundational Learning

- **Concept**: Reinforcement Learning from Human Feedback (RLHF)
  - **Why needed here**: The paper builds on RLHF framework, using human/AI feedback to align LLMs through reward modeling
  - **Quick check question**: What are the three main components of the RLHF pipeline as described in the paper?

- **Concept**: Preference learning and reward modeling
  - **Why needed here**: The paper trains reward models on ratings and rankings feedback to improve LLM alignment, requiring understanding of how preference data is converted to training objectives
  - **Quick check question**: What is the key difference between the regression reward model objective (Eq. 1) and the NLL reward model objective (Eq. 2)?

- **Concept**: Evaluation metrics for aligned models
  - **Why needed here**: The paper introduces "evaluation inconsistency" - where the choice of evaluation protocol affects which aligned model appears better
  - **Quick check question**: How is win-rate against reference model computed differently for ratings vs rankings evaluation protocols?

## Architecture Onboarding

- **Component map**: Base LLM (Alpaca-7B) -> Response generation -> Feedback acquisition (ratings/rankings from humans/AI) -> Reward model training (regression/NLL) -> Best-of-n policy -> Evaluation against reference model (DaVinci-003)
- **Critical path**: Feedback acquisition -> Reward model training -> Best-of-n policy generation -> Evaluation
- **Design tradeoffs**: Ratings provide richer quantitative signal but are harder to acquire consistently; rankings are easier to collect but provide only ordinal information
- **Failure signatures**: Inconsistent feedback between protocols (60% disagreement), evaluation inconsistency where protocol choice affects which model wins, reward models that don't generalize from feedback data
- **First 3 experiments**:
  1. Replicate the consistency analysis by converting ratings to rankings and comparing with actual rankings feedback to measure disagreement rate
  2. Train regression reward model on ratings feedback and NLL reward model on rankings feedback, then evaluate both using both protocols
  3. Analyze the correlation between response length/unique words and feedback scores to verify the paper's finding of no length bias

## Open Questions the Paper Calls Out

- **Question**: What are the cognitive underpinnings of the consistency problem between ratings and rankings feedback protocols?
  - **Basis in paper**: [inferred]
  - **Why unresolved**: The paper identifies the existence of the consistency problem and explores some qualitative differences in how annotators provide different forms of feedback, but does not deeply investigate the cognitive or psychological reasons behind why humans and AI systems might systematically disagree between ratings and rankings.
  - **What evidence would resolve it**: Cognitive science studies or psychological experiments specifically designed to probe how humans process and translate between absolute and relative judgments, possibly using controlled stimuli and eye-tracking or neuroimaging.

- **Question**: How do different feedback protocols (ratings vs rankings) impact alignment algorithms beyond Best-of-n, such as PPO or DPO?
  - **Basis in paper**: [explicit]
  - **Why unresolved**: The paper focuses on Best-of-n policy for alignment and notes that other algorithms like PPO could be used, but does not experimentally test the impact of feedback protocol choice on the performance of these alternative algorithms.
  - **What evidence would resolve it**: Empirical studies comparing the performance of PPO, DPO, and other RL algorithms when trained on ratings vs rankings feedback data, measuring downstream task performance.

- **Question**: Does the evaluation inconsistency phenomenon generalize to other reference models beyond DaVinci-003?
  - **Basis in paper**: [explicit]
  - **Why unresolved**: The paper only evaluates the aligned models against DaVinci-003 and observes evaluation inconsistency, but does not test whether this phenomenon persists with other reference models or evaluation datasets.
  - **What evidence would resolve it**: Replicating the evaluation experiments using different reference models (e.g., GPT-4, Claude, etc.) and measuring whether the win-rate gaps between ratings-based and rankings-based alignment policies still exhibit the same directional bias depending on the evaluation protocol.

## Limitations

- The paper doesn't empirically validate the claim that ratings provide a "stronger quantitative signal" for reward modeling versus rankings
- The specific mechanisms causing 60% disagreement between ratings and rankings protocols remain partially unexplained
- Only tested Best-of-n alignment algorithm, leaving open how rankings vs ratings feedback affects other alignment methods like PPO or DPO

## Confidence

- **High Confidence**: The existence of significant feedback inconsistency between ratings and rankings protocols (60% disagreement rate)
- **Medium Confidence**: The evaluation inconsistency finding where protocol choice affects which aligned model appears better
- **Low Confidence**: The specific claim about ratings providing richer signal for reward modeling

## Next Checks

1. Conduct ablation study comparing regression vs NLL reward model performance on held-out feedback data to verify which learns more effectively from their respective feedback types
2. Test whether the evaluation inconsistency persists when using a reference-free evaluation protocol (e.g., absolute quality scoring) rather than pairwise comparison against DaVinci-003
3. Analyze whether response length or other surface features correlate with feedback scores in ways that could explain the inconsistency, controlling for actual content quality