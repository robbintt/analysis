---
ver: rpa2
title: Alignment with human representations supports robust few-shot learning
arxiv_id: '2301.11990'
source_url: https://arxiv.org/abs/2301.11990
tags:
- alignment
- learning
- xcit
- small
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores whether AI systems with representations aligned
  to humans offer advantages in few-shot learning, robustness to adversarial attacks,
  and domain shifts. The authors define representational alignment as the degree to
  which model representations match human similarity judgments on images.
---

# Alignment with human representations supports robust few-shot learning

## Quick Facts
- arXiv ID: 2301.11990
- Source URL: https://arxiv.org/abs/2301.11990
- Reference count: 12
- One-line primary result: Highly-aligned models achieve better few-shot transfer learning, higher adversarial robustness, and greater domain shift robustness than medium-aligned models

## Executive Summary
This paper investigates whether AI systems with representations aligned to human similarity judgments offer advantages in few-shot learning, adversarial robustness, and domain shift robustness. The authors define representational alignment as the degree to which model representations match human similarity judgments on images, and use an information-theoretic framework to predict a U-shaped relationship between alignment and few-shot learning performance. Empirically, analyzing 491 computer vision models, they find that highly-aligned models significantly outperform medium-aligned models across all three downstream tasks, while low-aligned models also perform well, confirming the U-shaped relationship.

## Method Summary
The study analyzes 491 pre-trained computer vision models from the PyTorch Image Models package, all trained on ImageNet-1k. Human similarity judgments from 6 datasets totaling over 425,000 judgments from 1200 participants serve as the alignment benchmark. Three alignment metrics are computed: Pearson pairwise alignment, Spearman pairwise alignment, and triplet alignment. Models are evaluated on downstream tasks including few-shot transfer learning on CIFAR100, adversarial robustness on ImageNet-A, and domain shift robustness on ImageNet-R and ImageNet-Sketch. The analysis focuses on correlation patterns between alignment metrics and performance, controlling for ImageNet Top-1 accuracy.

## Key Results
- Highly-aligned models outperform medium-aligned models in few-shot transfer learning, achieving better performance with fewer examples per class
- Human-aligned models show greater adversarial robustness to ImageNet-A attacks, even after controlling for baseline ImageNet accuracy
- Aligned models demonstrate superior domain shift robustness on ImageNet-R and ImageNet-Sketch datasets
- The relationship between alignment and few-shot learning performance follows a U-shaped curve, with both highly-aligned and low-aligned models performing better than medium-aligned models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human-aligned models achieve better few-shot learning because aligned representations reduce the information gap between human supervision and model understanding.
- Mechanism: The framework models human supervision as triplet queries over representations. Higher alignment means fewer triplet queries are needed to communicate the location of new objects, reducing the supervision burden in few-shot scenarios.
- Core assumption: The representation space can be effectively captured by triplet similarity queries, and alignment is measurable via agreement on these triplets.
- Evidence anchors:
  - [abstract] "Our framework predicts that few-shot transfer learning performance should have a U-shaped relationship with alignment."
  - [section] "We define representational alignment as the degree to which the latent representations of a model match the latent representations of humans for the same set of stimuli."
  - [corpus] Weak evidence; corpus lacks specific citations about triplet-based alignment frameworks.
- Break condition: If human similarity judgments are unreliable or the triplet query space is insufficient to capture representation differences, the predicted relationship breaks down.

### Mechanism 2
- Claim: Human-aligned models are more robust to adversarial examples because adversarial perturbations are less effective at changing representations in aligned models.
- Mechanism: Adversarial examples maximize representational disagreement between humans and models. Higher alignment reduces this disagreement, making it harder to craft effective adversarial examples.
- Core assumption: Adversarial robustness correlates with representational agreement between humans and models.
- Evidence anchors:
  - [abstract] "We also show that highly-aligned models are more robust to both adversarial examples and domain shifts."
  - [section] "Human-aligned models are more robust to adversarial examples, even when correcting for classiﬁcation performance on the pre-training dataset."
  - [corpus] No direct evidence in corpus; claims about robustness are based on internal experimental results.
- Break condition: If adversarial examples exploit features humans don't perceive or if robustness depends on factors unrelated to representational alignment, the mechanism fails.

### Mechanism 3
- Claim: Human-aligned models are more robust to domain shifts because aligned representations generalize better to new distributions humans can recognize.
- Mechanism: Domain shift is framed as a form of zero-shot transfer learning where aligned models leverage human-representational knowledge to adapt to new distributions.
- Core assumption: Domain shift robustness can be modeled as zero-shot transfer and that human-representational knowledge is transferable across domains.
- Evidence anchors:
  - [abstract] "We also show that highly-aligned models are more robust to both adversarial examples and domain shifts."
  - [section] "Human-aligned models are more robust to domain shift, even when correcting for classiﬁcation performance on the pre-training dataset."
  - [corpus] Weak evidence; corpus lacks detailed analysis of domain shift robustness mechanisms.
- Break condition: If domain shift involves transformations humans cannot recognize or if robustness requires specialized domain knowledge beyond human representations, the mechanism breaks.

## Foundational Learning

- Concept: Information theory and channel capacity
  - Why needed here: The framework uses information-theoretic arguments (channel capacity, mutual information) to analyze representational alignment and its effects on learning efficiency.
  - Quick check question: How does channel capacity relate to the number of triplet queries needed for effective communication between teacher and student models?

- Concept: Representational similarity analysis (RSA)
  - Why needed here: RSA is the foundation for measuring representational alignment through similarity judgments and embeddings.
  - Quick check question: What is the relationship between pairwise similarity matrices and triplet-based alignment metrics?

- Concept: Few-shot learning and transfer learning
  - Why needed here: The paper's main predictions and experiments focus on few-shot learning performance as a function of representational alignment.
  - Quick check question: How does the number of examples per class affect the advantage of human-aligned models in few-shot scenarios?

## Architecture Onboarding

- Component map: Pre-trained models -> Human similarity datasets -> Triplet alignment computation -> Downstream task evaluation -> Correlation analysis
- Critical path: 1) Load pre-trained models and human similarity data. 2) Compute alignment metrics (Pearson, Spearman, triplet). 3) Evaluate downstream tasks. 4) Analyze relationships between alignment and performance.
- Design tradeoffs: High alignment may sacrifice specialized features useful for specific tasks but offers generalization benefits. Medium alignment appears to be the worst performer, suggesting a U-shaped tradeoff.
- Failure signatures: Unexpected linear relationships between alignment and performance, or no significant differences between aligned and non-aligned models in downstream tasks.
- First 3 experiments:
  1. Reproduce the correlation analysis between triplet alignment and few-shot learning performance across the 491 models.
  2. Test the robustness of aligned models to adversarial examples on ImageNet-A.
  3. Evaluate domain shift robustness by comparing aligned and non-aligned models on ImageNet-R and ImageNet-Sketch.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of human representations contribute most to the U-shaped relationship between alignment and few-shot learning performance?
- Basis in paper: [explicit] The paper identifies a U-shaped relationship between alignment and few-shot learning but does not specify which aspects of human representations drive this relationship
- Why unresolved: The paper only measures overall alignment without decomposing it into different aspects of representational similarity
- What evidence would resolve it: Experiments measuring alignment based on different aspects of human representations (e.g., semantic vs. perceptual similarity) and their individual effects on few-shot learning

### Open Question 2
- Question: How does the relationship between alignment and robustness change across different types of adversarial attacks or domain shifts?
- Basis in paper: [explicit] The paper shows positive correlations between alignment and robustness to adversarial attacks and domain shifts, but does not analyze the relationship across different types of attacks or shifts
- Why unresolved: The paper only examines general robustness metrics without breaking down the analysis by attack type or domain shift category
- What evidence would resolve it: Detailed analysis of alignment-robustness relationships across different adversarial attack methods and domain shift types

### Open Question 3
- Question: What is the optimal level of alignment for different downstream tasks and how does this optimal level vary with task complexity?
- Basis in paper: [inferred] The paper shows that both high and low alignment can be beneficial, but does not investigate optimal alignment levels for specific tasks or task complexities
- Why unresolved: The paper treats all downstream tasks uniformly and does not explore how optimal alignment levels might vary with task characteristics
- What evidence would resolve it: Systematic experiments mapping task characteristics to optimal alignment levels and identifying patterns in this relationship

## Limitations

- The analysis is limited to 491 pre-trained models from a single architecture family, constraining generalizability
- Human similarity judgment datasets, while substantial, may not fully capture the complexity of human perception and could contain sampling biases
- The theoretical framework assumes triplet queries effectively capture representation space structure, but this relationship remains incompletely characterized

## Confidence

- High: The empirical finding that highly-aligned models outperform medium-aligned models in few-shot learning and robustness tasks
- Medium: The theoretical framework predicting U-shaped relationships between alignment and performance
- Medium: The claim that human alignment is sufficient but not necessary for robustness benefits

## Next Checks

1. Test the alignment-performance relationship across additional model families (transformers, vision-language models) to assess generalizability
2. Validate the triplet alignment metric against alternative similarity measurement approaches to ensure robustness of the theoretical framework
3. Conduct ablation studies on human similarity judgment datasets to determine which aspects of human perception drive the observed effects