---
ver: rpa2
title: Mitigating Backdoors in Federated Learning with FLD
arxiv_id: '2303.00302'
source_url: https://arxiv.org/abs/2303.00302
tags:
- backdoor
- clients
- data
- learning
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Federated Layer Detection (FLD), a novel defense
  against backdoor attacks in federated learning that detects malicious models by
  analyzing layer-level outliers instead of whole-model metrics. FLD uses isolation-based
  scoring (iForest) for each layer and median absolute deviation (MAD) to identify
  anomalous models without requiring knowledge of attack proportions.
---

# Mitigating Backdoors in Federated Learning with FLD

## Quick Facts
- arXiv ID: 2303.00302
- Source URL: https://arxiv.org/abs/2303.00302
- Reference count: 25
- Key outcome: FLD achieves near-zero backdoor accuracy (0.00-2.56%) across multiple attacks while preserving main task accuracy (88.75-99.03%)

## Executive Summary
The paper introduces Federated Layer Detection (FLD), a novel defense mechanism against backdoor attacks in federated learning that analyzes model parameters at layer granularity rather than whole-model level. FLD uses isolation forest (iForest) to score each layer's parameters for outlier detection, then applies median absolute deviation (MAD) to identify anomalous models. The approach effectively defends against various backdoor attacks including constrain-and-scale, DBA, Edge-Case, Little Is Enough, PGD, and Flip attacks while maintaining primary task accuracy. Experiments on Reddit, MNIST, CIFAR10, and Tiny-imagenet datasets demonstrate superior performance compared to existing defense methods.

## Method Summary
FLD operates by scoring each layer of uploaded models using isolation forest to detect parameter-level outliers, then applies MAD-based anomaly detection to flag models with excessive anomalous layers. Models flagged as malicious are excluded from the FedAvg aggregation process. The method requires no knowledge of attack proportions and works across different client data distributions. Key components include layer-wise iForest scoring for outlier detection and MAD thresholding for anomaly classification, with exclusion criteria based on the proportion of anomalous layers exceeding 50%.

## Key Results
- FLD achieves 0.00-2.56% backdoor accuracy across all tested attacks while maintaining 88.75-99.03% main task accuracy
- Outperforms baselines (FoolsGold, RFA, DP, Krum, Trimmed Mean, Bulyan) by significant margins across all attack types
- Robust to varying attack parameters including poison ratio (0.2-0.4) and data heterogeneity (Dirichlet α=0.1-0.5)
- Effective across diverse datasets and model architectures including Reddit (LSTM), MNIST, CIFAR10 (ResNet-18), and Tiny-imagenet

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer Scoring detects backdoor models by identifying layer-level parameter outliers rather than whole-model deviations
- Mechanism: Each model layer is scored individually using isolation forest (iForest), which assigns outlier scores based on how easily a layer's parameters can be isolated from others
- Core assumption: Different layers have distinct parameter distributions and functional roles, making layer-level analysis more sensitive to backdoor injection
- Evidence anchors:
  - [abstract] "FLD examines the models based on layer granularity to capture the complete model details and effectively detect potential backdoor models regardless of model dimension"
  - [section 4.2] "We chose iForest [Liu et al., 2012] as our outlier detection algorithm. iForest is an unsupervised, non-parametric (no assumptions about the sample distribution) outlier detection algorithm that does not require distance or density calculations"
  - [corpus] Weak evidence - no corpus papers explicitly validate layer-wise isolation forest for backdoor detection
- Break condition: If backdoor triggers affect all layers uniformly or if malicious parameter changes are too subtle to create isolation, detection may fail

### Mechanism 2
- Claim: MAD-based anomaly detection avoids mean-shift problems that plague traditional statistical methods
- Mechanism: Instead of using mean and standard deviation, MAD uses median and median absolute deviation to identify outlier scores, making it robust to extreme values
- Core assumption: Backdoor models produce extreme outlier scores that would shift the mean, but the median remains stable for benign models
- Evidence anchors:
  - [abstract] "Anomaly Detection uses the median absolute deviation (MAD) of layer scores to determine if a model is anomalous"
  - [section 4.3] "We use MAD for anomaly detection, which can effectively avoid the mean value shifting caused by extreme values"
  - [corpus] No direct corpus evidence for MAD in federated backdoor detection
- Break condition: If backdoor models produce moderate but consistent layer anomalies rather than extreme outliers, MAD may not flag them

### Mechanism 3
- Claim: Fine-grained layer analysis enables detection of deep, distributed backdoors that evade whole-model defenses
- Mechanism: By examining individual layers, FLD can detect backdoor patterns that are distributed across multiple layers and would be averaged out in whole-model analysis
- Core assumption: Backdoor attacks often modify parameters in specific layers while leaving others relatively unchanged, creating detectable layer-level patterns
- Evidence anchors:
  - [abstract] "FLD examines the models based on layer granularity to capture the complete model details"
  - [section 5.4] "FLD on CIFAR presents 88.75% for MA while only 2.56% for BA" showing effectiveness against complex models
  - [corpus] Some related papers discuss distributed backdoor attacks but don't validate layer-wise detection approaches
- Break condition: If backdoor attacks uniformly modify all layers or if the network architecture makes layer boundaries meaningless

## Foundational Learning

- Concept: Isolation Forest (iForest) for anomaly detection
  - Why needed here: Traditional distance-based methods fail when backdoor models are close to benign models in parameter space, but iForest detects isolation patterns
  - Quick check question: How does iForest's isolation approach differ from KNN or LOF methods for detecting anomalies?

- Concept: Median Absolute Deviation (MAD) for robust statistics
  - Why needed here: Mean and standard deviation are sensitive to outliers, but backdoor models intentionally create extreme parameter deviations
  - Quick check question: Why is MAD more robust than standard deviation when detecting models with injected backdoors?

- Concept: Federated learning aggregation mechanisms
  - Why needed here: Understanding FedAvg and how malicious updates can bias global model parameters is crucial for designing effective defenses
  - Quick check question: How does FedAvg aggregation make federated learning vulnerable to model replacement attacks?

## Architecture Onboarding

- Component map: Client training -> Layer Scoring (iForest per layer) -> Anomaly Detection (MAD threshold) -> Model aggregation
- Critical path: Model upload -> Layer-wise isolation scoring -> MAD-based anomaly flagging -> Client exclusion -> FedAvg aggregation
- Design tradeoffs: Layer granularity provides sensitivity but increases computational overhead; iForest is efficient but may miss subtle anomalies
- Failure signatures: High false positive rate indicates overly sensitive MAD threshold; high false negative rate suggests iForest parameters need tuning
- First 3 experiments:
  1. Test Layer Scoring on synthetic models with known backdoor injection patterns to validate detection accuracy
  2. Compare FLD performance with and without Layer Scoring on CIFAR10 under constrain-and-scale attack
  3. Evaluate MAD threshold sensitivity by testing different µ values on backdoor detection rates

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text.

## Limitations
- Performance degradation under extreme data heterogeneity (Dirichlet alpha approaching 0.1)
- Unknown effectiveness against multi-target backdoor attacks
- Potential scalability challenges with very deep networks

## Confidence

- **High Confidence**: Layer-level isolation forest scoring provides superior sensitivity to backdoor detection compared to whole-model approaches
- **Medium Confidence**: MAD-based anomaly detection effectively avoids mean-shift problems
- **Low Confidence**: The claim that FLD works "regardless of model dimension"

## Next Checks

1. **Layer Scoring Sensitivity Test**: Create synthetic models with known backdoor patterns at different layer depths and evaluate FLD's detection accuracy as a function of backdoor injection location
2. **MAD Threshold Robustness**: Systematically vary the µ threshold parameter and measure the tradeoff between false positive rate and backdoor detection rate across all attack types
3. **Cross-Architecture Generalization**: Apply FLD to a fundamentally different architecture (e.g., Transformer-based models) to verify the "regardless of model dimension" claim and identify any scalability limitations