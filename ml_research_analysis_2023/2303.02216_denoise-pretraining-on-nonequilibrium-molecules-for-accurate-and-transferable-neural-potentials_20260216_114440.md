---
ver: rpa2
title: Denoise Pretraining on Nonequilibrium Molecules for Accurate and Transferable
  Neural Potentials
arxiv_id: '2303.02216'
source_url: https://arxiv.org/abs/2303.02216
tags: []
core_contribution: Machine learning potentials for molecular simulations require accurate
  and transferable models, but are limited by the high computational cost of quantum
  mechanical calculations and the difficulty of obtaining sufficient data for large
  and complex systems. This work proposes a denoising pre-training approach on non-equilibrium
  molecular conformations to improve the accuracy and transferability of graph neural
  networks (GNNs) for molecular potential predictions.
---

# Denoise Pretraining on Nonequilibrium Molecules for Accurate and Transferable Neural Potentials

## Quick Facts
- arXiv ID: 2303.02216
- Source URL: https://arxiv.org/abs/2303.02216
- Authors: 
- Reference count: 26
- Key outcome: Denoising pre-training on non-equilibrium molecular conformations significantly improves accuracy and transferability of GNNs for molecular potential predictions across diverse chemical systems.

## Executive Summary
This paper addresses the challenge of developing accurate and transferable machine learning potentials for molecular simulations by proposing a denoising pre-training approach. The method adds random Gaussian noise to atomic coordinates of non-equilibrium molecular conformations and trains graph neural networks (GNNs) to predict this noise, which is equivalent to learning a pseudo-force field. Experiments demonstrate that this pre-training strategy significantly improves the performance of various GNN architectures, including both invariant and equivariant models, on multiple molecular potential prediction benchmarks. The approach shows remarkable transferability, enabling models pre-trained on small molecules to achieve state-of-the-art results on diverse systems including charged molecules, biomolecules, and larger molecular systems.

## Method Summary
The method involves pre-training GNN models on non-equilibrium molecular conformations by adding Gaussian noise to atomic coordinates and training the models to predict the noise. This denoising task is equivalent to learning a pseudo-force field under the assumption that atomic position distributions around sampled conformations follow Gaussian distributions. After pre-training, the models are fine-tuned on target molecular potential prediction tasks by replacing the output layer with a randomly initialized MLP head. The approach is model-agnostic and improves both invariant GNNs (like SchNet) and equivariant GNNs (like SE(3)-Transformer and EGNN). Experiments are conducted on multiple datasets including ANI-1, ANI-1x, ISO17, SPICE, and MD22, demonstrating significant improvements in accuracy and transferability.

## Key Results
- Denoising pre-training reduces RMSE and MAE by 15.3% on average across multiple molecular potential prediction benchmarks
- Pre-trained models show remarkable transferability, improving performance on charged molecules, biomolecules, and larger systems
- Simple equivariant GNNs with pre-training achieve competitive performance compared to more complex architectures
- The approach enables data-efficient learning, requiring less fine-tuning data to achieve state-of-the-art results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on non-equilibrium conformations teaches the model a pseudo-force field.
- Mechanism: Random Gaussian noise is added to atomic coordinates of sampled non-equilibrium conformations. The GNN is trained to predict this noise, which is equivalent to learning the gradient of the log probability density around the sampled conformation. Under the assumption that the probability distribution around a conformation is Gaussian, the gradient is proportional to the noise vector, thus teaching the model a pseudo-force field.
- Core assumption: The probability distribution of atomic positions around a sampled non-equilibrium conformation follows a Gaussian distribution centered at the original conformation.
- Evidence anchors:
  - [abstract] "GNNs are pre-trained by predicting the random noises added to atomic coordinates of sampled non-equilibrium conformations."
  - [section] "Following the assumption, the force field is proportional to the perturbation noise for a given variance σ as shown in Eq. 8."
- Break condition: If the true distribution of atomic positions is not Gaussian or is multimodal, the learned pseudo-force field may be inaccurate.

### Mechanism 2
- Claim: Denoising pre-training improves data efficiency and generalizability of GNN models.
- Mechanism: By pre-training on a large dataset of small molecules, the GNN learns general representations of molecular conformations and forces. When fine-tuned on a smaller dataset of a specific target system, the pre-trained model can leverage this learned knowledge, requiring less data to achieve similar or better performance than training from scratch.
- Core assumption: Knowledge learned from small molecules is transferable to larger, more complex systems.
- Evidence anchors:
  - [abstract] "Notably, our models pre-trained on small molecules demonstrate remarkable transferability, improving performance when fine-tuned on diverse molecular systems..."
  - [section] "On average, pre-trained GNNs show 15.3% lower RMSE and MAE on SPICE."
- Break condition: If the target system is chemically or structurally very different from the pre-training data, the transfer may be limited or ineffective.

### Mechanism 3
- Claim: The denoising objective is model-agnostic and improves both invariant and equivariant GNNs.
- Mechanism: The denoising task (predicting the added noise) is a general self-supervised learning objective that can be applied to any GNN architecture. It forces the model to learn meaningful representations of the molecular conformation and its energy landscape, which benefits the downstream task of potential prediction regardless of the specific GNN architecture.
- Core assumption: The denoising task is a good proxy for learning representations useful for potential prediction.
- Evidence anchors:
  - [abstract] "Furthermore, we show that the proposed pre-training approach is model-agnostic, as it improves the performance of different invariant and equivariant GNNs."
  - [section] "Table 2 compares the molecular potential prediction results of pre-training and no pre-training on ANI-1 and ANI-1x. Our experiments demonstrate that the proposed denoise pre-training approach significantly improves the accuracy of GNNs for molecular potential predictions."
- Break condition: If a specific GNN architecture is not well-suited to the denoising task, or if the task does not align well with the target application, the benefits may be limited.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and message passing
  - Why needed here: GNNs are used to represent and learn from molecular structures. Understanding how they aggregate information from neighboring atoms is crucial for implementing and modifying the proposed method.
  - Quick check question: What is the difference between invariant and equivariant GNNs, and why is equivariance important for molecular potential prediction?

- Concept: Self-supervised learning and denoising
  - Why needed here: The proposed method uses denoising as a self-supervised pre-training objective. Understanding how denoising works and why it can be effective for learning representations is important for grasping the method's rationale.
  - Quick check question: How does predicting the added noise during pre-training help the model learn a useful representation for potential prediction?

- Concept: Molecular conformations and force fields
  - Why needed here: The method operates on molecular conformations and aims to learn a pseudo-force field. Understanding what these concepts mean and how they relate to each other is important for interpreting the results and potential applications.
  - Quick check question: What is the relationship between a molecular conformation, its potential energy, and the force field?

## Architecture Onboarding

- Component map:
  - Molecular conformation data (atomic coordinates, element types, interactions)
  - GNN model (invariant or equivariant)
  - Noise generation module (adds Gaussian noise to coordinates)
  - Denoising loss function (e.g., MSE between predicted and actual noise)
  - Fine-tuning module (adds MLP head for potential prediction)

- Critical path:
  1. Load and preprocess molecular conformation data
  2. Sample non-equilibrium conformations
  3. Add random noise to atomic coordinates
  4. Train GNN to predict the noise (pre-training)
  5. Fine-tune pre-trained GNN on target dataset for potential prediction

- Design tradeoffs:
  - Choice of noise magnitude (standard deviation) - too small may not provide enough signal, too large may break the Gaussian assumption
  - Choice of GNN architecture - trade-off between model complexity and computational efficiency
  - Pre-training dataset size and diversity - larger and more diverse datasets may lead to better generalization

- Failure signatures:
  - Poor performance on pre-training task (high denoising loss) - may indicate issues with model architecture or noise generation
  - Poor transfer to target task (high potential prediction error) - may indicate limited generalizability or insufficient fine-tuning
  - Slow convergence or overfitting during pre-training - may indicate issues with learning rate, batch size, or model capacity

- First 3 experiments:
  1. Implement and train a simple invariant GNN (e.g., SchNet) on a small dataset of non-equilibrium conformations with denoising pre-training. Evaluate the denoising loss and potential prediction error on a held-out test set.
  2. Repeat experiment 1 with an equivariant GNN (e.g., EGNN) and compare performance to the invariant model.
  3. Pre-train a GNN on a large dataset of small molecules and fine-tune it on a smaller dataset of a specific target system. Compare performance to a model trained from scratch on the target dataset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the denoising pre-training approach scale to even larger and more complex molecular systems, such as proteins with thousands of atoms?
- Basis in paper: [explicit] The authors demonstrate the approach on MD22, which includes molecules with up to 370 atoms, but acknowledge the challenge of obtaining sufficient data for larger systems.
- Why unresolved: The paper does not explore systems significantly larger than those in MD22, leaving the scalability to extremely large biomolecules an open question.
- What evidence would resolve it: Testing the approach on molecular dynamics simulations of large proteins or protein-ligand complexes, and comparing its performance to traditional force fields or ab initio methods.

### Open Question 2
- Question: Can the denoising pre-training method be extended to predict other molecular properties beyond potential energy, such as electronic structure or reaction pathways?
- Basis in paper: [inferred] The authors focus on potential energy predictions, but the denoising approach is model-agnostic and could potentially be adapted for other tasks.
- Why unresolved: The paper does not explore applications beyond potential energy, and it is unclear how well the denoising strategy generalizes to other molecular properties.
- What evidence would resolve it: Applying the denoising pre-training to models predicting electronic properties, reaction barriers, or molecular spectra, and evaluating its effectiveness compared to existing methods.

### Open Question 3
- Question: How does the choice of noise scale (σ) affect the performance of the denoising pre-training, and is there an optimal noise level for different types of molecular systems?
- Basis in paper: [explicit] The authors investigate the effect of noise scale on TorchMD-Net performance, finding that σ = 0.2 Å yields the best results, but note that too much noise can harm learning.
- Why unresolved: The study only examines a limited range of noise scales and a single model, leaving the optimal noise level for different systems and models an open question.
- What evidence would resolve it: Systematically varying the noise scale across a wider range and different molecular systems, and analyzing the trade-off between noise level and model performance.

## Limitations

- The Gaussian assumption for non-equilibrium conformations may not hold for all molecular systems, particularly those with strong anharmonicities or multiple stable conformations
- The method's effectiveness for very large biomolecular systems (thousands of atoms) and systems with unusual bonding patterns remains untested
- Limited exploration of optimal noise scale (σ) values across different molecular system types and sizes

## Confidence

- **High Confidence:** The denoising pre-training mechanism itself (predicting added noise) is well-established and the implementation details are clearly described.
- **Medium Confidence:** Claims about improved accuracy on specific benchmark datasets are supported by presented results, though comprehensive statistical significance testing is not shown.
- **Medium Confidence:** Transferability claims are supported by experiments but limited to specific types of molecular systems and may not generalize to all chemical domains.

## Next Checks

1. **Noise Scale Sensitivity Analysis:** Systematically vary the noise standard deviation σ across multiple orders of magnitude and evaluate the impact on both pre-training convergence and downstream task performance to identify optimal noise scales for different molecular system sizes.

2. **Cross-Domain Transferability Test:** Pre-train on small organic molecules and fine-tune on radically different systems such as organometallic complexes, ionic crystals, or materials with strong anharmonic potentials to rigorously test the limits of transferability.

3. **Gaussian Assumption Validation:** Generate histograms of atomic position deviations from sampled conformations and perform statistical tests (e.g., Shapiro-Wilk) to empirically verify the Gaussian distribution assumption across different molecular systems and identify cases where the assumption breaks down.