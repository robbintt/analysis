---
ver: rpa2
title: 'idT5: Indonesian Version of Multilingual T5 Transformer'
arxiv_id: '2302.00856'
source_url: https://arxiv.org/abs/2302.00856
tags:
- language
- question
- indonesian
- multilingual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of large multilingual Transformer
  models being inefficient for tasks requiring only one language, such as Indonesian.
  The authors propose a method to create smaller, language-specific models by extracting
  the vocabulary and parameters relevant to Indonesian from the multilingual mT5 model.
---

# idT5: Indonesian Version of Multilingual T5 Transformer

## Quick Facts
- arXiv ID: 2302.00856
- Source URL: https://arxiv.org/abs/2302.00856
- Reference count: 0
- Reduces mT5 model size by 58% while maintaining performance on Indonesian NLP tasks

## Executive Summary
This paper addresses the inefficiency of large multilingual Transformer models for single-language tasks by proposing a method to create language-specific models through vocabulary and parameter extraction. The authors extract Indonesian and a small portion of English vocabulary from mT5-base, resulting in idT5 with 244 million parameters (58% smaller than the original 580 million). When fine-tuned on sentiment analysis, question generation, and question answering tasks, idT5 achieved similar or better performance than mT5, with an 8% higher accuracy in sentiment analysis. Additionally, idT5 demonstrated faster loading times, reduced memory requirements, and faster inference speeds, making it more efficient for deployment in resource-constrained environments.

## Method Summary
The method involves analyzing token frequencies in Indonesian and English text corpora to identify the vocabulary used in these languages, then extracting this subset from the mT5 model. The authors selected 30K tokens (24,471 Indonesian, 1K from original tokenizer, 10K English, 100 special tokens) from mT5's 250K token vocabulary, updating the embedding layer and tokenizer to create idT5. The resulting model was fine-tuned on three tasks (sentiment analysis, question generation, and question answering) using the same datasets and hyperparameters as mT5 for fair comparison. The model was made publicly available on Hugging Face Transformers Hub for reproducibility.

## Key Results
- Reduced model size by 58% (from 580M to 244M parameters) while maintaining performance
- Achieved 77.18% accuracy on sentiment analysis (8% higher than mT5)
- Demonstrated faster loading times, reduced memory usage, and faster inference speeds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vocabulary sparsity enables parameter reduction
- Mechanism: By selecting only tokens used in Indonesian text (~30K out of 250K total), the model eliminates unused vocabulary entries, reducing embedding matrix size proportionally
- Core assumption: Token usage in Indonesian texts is highly concentrated in a small subset of the multilingual vocabulary
- Evidence anchors: "Based on the token count in the Indonesian corpus, we found that only 29% of the model vocabulary was used" and "the top 20K tokens constitute over 98% of the Indonesian corpus"

### Mechanism 2
- Claim: Fixed-size special tokens preserve T5 functionality
- Mechanism: The method reserves 100 special tokens (used by T5 for tasks like start/end markers) while reducing language-specific tokens, maintaining the transformer's ability to handle text-to-text tasks
- Core assumption: T5's special tokens are language-agnostic and can be reused across language-specific models
- Evidence anchors: "As for English tokens, the statistics are similar... the remaining tokens have yet to be accommodated in the three token groups, so the total number of tokens is 30K or 12% of 250K tokens"

### Mechanism 3
- Claim: Monolingual models have faster inference and lower memory
- Mechanism: With fewer parameters and reduced embedding matrix, the model requires less memory allocation and faster computation during inference, particularly for sequence lengths where memory scales with vocabulary size
- Core assumption: The reduced parameter count directly translates to proportional memory and computation savings during inference
- Evidence anchors: "The resulting model requires less memory, loads faster, and inference times faster" and "idT5 requires a smaller memory allocation, even less than half of the memory required by the mT5 model"

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding how self-attention and embedding layers work is crucial for grasping how vocabulary reduction affects model capacity
  - Quick check question: How does reducing vocabulary size specifically affect the embedding layer dimensions and subsequent transformer layers?

- Concept: Pre-trained model fine-tuning workflow
  - Why needed here: The paper relies on fine-tuning idT5 on downstream tasks; understanding this process is essential for replicating results
  - Quick check question: What are the key hyperparameters that must be matched when comparing fine-tuned models across architectures?

- Concept: Multilingual model limitations and the curse of multilinguality
  - Why needed here: The paper addresses why multilingual models are inefficient for single-language tasks, requiring understanding of model capacity dilution
  - Quick check question: Why might a model trained on 101 languages perform worse on Indonesian than a model trained only on Indonesian, all else being equal?

## Architecture Onboarding

- Component map: Tokenizer -> Embedding layer (reduced from 250K to 30K tokens) -> Transformer blocks (unchanged) -> Model hub integration
- Critical path: 1. Vocabulary frequency analysis on Indonesian corpus, 2. Token selection and special token reservation, 3. Embedding matrix replacement, 4. Tokenizer Protobuf update, 5. Model serialization and upload
- Design tradeoffs: Vocabulary size vs. coverage (30K tokens captures 98% of corpus but may miss rare words), Model size vs. performance (58% reduction with minimal accuracy loss), Language specificity vs. generality (removing English hurts bilingual text handling)
- Failure signatures: Vocabulary too small (increased out-of-vocabulary tokens in downstream tasks), Special tokens misconfigured (task formatting errors or tokenization failures), Embedding mismatch (shape incompatibility errors when loading pre-trained weights)
- First 3 experiments: 1. Token frequency analysis on a sample Indonesian corpus to verify the 98% coverage claim, 2. Embedding layer replacement with shape validation to catch dimension mismatches, 3. Tokenizer serialization test to ensure Protobuf format compatibility with Hugging Face transformers

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Limited evaluation scope: Only three tasks tested (sentiment analysis, question generation, question answering)
- Restricted dataset coverage: Limited Indonesian-English corpus used for vocabulary selection
- Lack of implementation details: Vocabulary filtering criteria and embedding matrix reconstruction not fully detailed

## Confidence
*High Confidence:* The vocabulary reduction mechanism and its impact on model size (58% reduction from 580M to 244M parameters) are well-documented and mathematically sound.

*Medium Confidence:* The downstream task performance improvements (8% accuracy gain in sentiment analysis, comparable results in other tasks) are reported with specific metrics but rely on a limited experimental setup.

*Low Confidence:* The claim about 98% corpus coverage with 20K tokens and the general applicability of the vocabulary extraction method to other languages or domains is not empirically validated beyond the Indonesian case presented.

## Next Checks
1. **Vocabulary Coverage Validation**: Extract token frequencies from an independent Indonesian corpus and verify the 98% coverage claim with 20K tokens. Test model performance degradation when using progressively smaller vocabularies (e.g., 10K, 15K tokens) to identify the minimum viable vocabulary size.

2. **Cross-Task Generalization**: Fine-tune idT5 on additional Indonesian NLP tasks (named entity recognition, machine translation, summarization) to assess whether the performance advantages extend beyond the three evaluated tasks and whether the reduced vocabulary creates blind spots for certain linguistic phenomena.

3. **Code-Switching Robustness**: Evaluate idT5 on Indonesian-English code-switched text to quantify the trade-off between monolingual efficiency and bilingual capability. Measure performance degradation when processing mixed-language input and identify thresholds for acceptable language mixing ratios.