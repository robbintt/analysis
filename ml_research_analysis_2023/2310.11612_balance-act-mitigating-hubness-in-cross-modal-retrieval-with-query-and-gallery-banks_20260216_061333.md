---
ver: rpa2
title: 'Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and Gallery
  Banks'
arxiv_id: '2310.11612'
source_url: https://arxiv.org/abs/2310.11612
tags:
- retrieval
- dualis
- dualdis
- gallery
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the hubness problem in cross-modal retrieval,
  where a small number of gallery data points are frequently retrieved, leading to
  performance degradation. The authors theoretically demonstrate that hubs exhibit
  high similarity with data from both query and gallery modalities, and propose a
  unified framework called Dual Bank Normalization (DBNORM) to mitigate hubness.
---

# Balance Act: Mitigating Hubness in Cross-Modal Retrieval with Query and Gallery Banks

## Quick Facts
- arXiv ID: 2310.11612
- Source URL: https://arxiv.org/abs/2310.11612
- Reference count: 40
- Key outcome: Dual Bank Normalization (DBNORM) framework mitigates hubness in cross-modal retrieval, achieving state-of-the-art results with 45.00% R@1 on MSR-VTT

## Executive Summary
This paper addresses the hubness problem in cross-modal retrieval, where certain gallery points (hubs) are frequently retrieved, degrading retrieval performance. The authors theoretically demonstrate that hubs exhibit high similarity with both query and gallery modalities. They propose Dual Bank Normalization (DBNORM), a unified framework that constructs two banks from query and gallery samples to reduce hub-to-query similarity while improving non-hub-to-query similarity. Extensive experiments on diverse benchmarks including text-image, text-video, and text-audio retrieval demonstrate significant performance improvements over previous methods.

## Method Summary
DBNORM is a post-processing method that normalizes similarity scores using two banks constructed from training query and gallery data. The framework introduces two novel normalization methods: dual inverted softmax (DualIS) and dual dynamic inverted softmax (DualDIS). DualIS applies inverted softmax normalization using both banks, while DualDIS uses activation sets to identify potential hubs and only normalizes similarity for gallery points in these sets. This approach reduces the impact of hubs on retrieval performance while enhancing the retrieval of non-hub items.

## Key Results
- DBNORM with dual inverted softmax achieves 45.00% R@1 on MSR-VTT, outperforming previous methods
- Significant performance gains across multiple benchmarks: MSVD (+1.84% R@1), ActivityNet (+4.33% R@1), Flickr30k (+2.17% R@1)
- DBNORM demonstrates consistent improvement across text-image, text-video, and text-audio retrieval tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hubs exhibit high similarity with both query and gallery data
- Mechanism: Theoretical proof that points close to the mean in a symmetric distribution are more likely to be hubs, and such hubs maintain high similarity across different modalities
- Core assumption: Both query and gallery spaces follow symmetric distributions
- Evidence anchors: Theoretical demonstration in section 2.2; Theorem 2 shows hubs maintain high similarity across modalities

### Mechanism 2
- Claim: Dual Bank Normalization reduces similarity between hubs and queries while improving similarity between non-hubs and queries
- Mechanism: Constructs two banks from training data and normalizes similarity using inverted softmax operations that downweight hub-to-query similarity
- Core assumption: Training data banks can effectively represent the distribution of test data
- Evidence anchors: Abstract states DBNORM reduces similarity between hubs and queries; section 2.3 describes DualIS and DualDIS methods

### Mechanism 3
- Claim: Dual Dynamic Inverted Softmax provides robustness when banks fail to represent the space
- Mechanism: Uses activation sets to identify potential hubs and only normalizes similarity for gallery points in these sets, preventing inaccurate normalization of non-hub similarities
- Core assumption: Top-k retrieved gallery points by bank points are likely to be hubs
- Evidence anchors: Section 2.3.1 describes construction of activation sets A_g and A_q based on top-k retrievals

## Foundational Learning

- Concept: Hubness phenomenon in high-dimensional spaces
  - Why needed here: Understanding why certain gallery points are retrieved disproportionately often
  - Quick check question: What causes some points to appear as nearest neighbors to many queries in high-dimensional spaces?

- Concept: Cosine similarity vs Euclidean distance in embedding spaces
  - Why needed here: The theoretical analysis applies to both distance metrics used in cross-modal retrieval
  - Quick check question: How does the hubness behavior differ when using cosine similarity versus Euclidean distance?

- Concept: Inverted softmax normalization
  - Why needed here: Core mechanism for transforming similarity scores to mitigate hubness
  - Quick check question: How does inverting the softmax probability over a bank help reduce hubness?

## Architecture Onboarding

- Component map: Query bank -> Gallery bank -> Similarity computation -> Normalization (DualIS/DualDIS) -> Ranking
- Critical path: Query → Similarity Computation → Normalization (DualIS/DualDIS) → Ranking
- Design tradeoffs:
  - Bank size vs. computational cost: Larger banks improve hub detection but increase memory usage
  - DualIS vs. DualDIS: DualIS simpler but potentially less robust; DualDIS more robust but requires activation set computation
  - Temperature parameters β1, β2: Affect normalization strength; need tuning for different datasets
- Failure signatures:
  - Poor performance with small or biased training banks
  - Performance degradation when query-gallery distribution shifts significantly
  - Over-normalization of non-hub similarities (especially with DualIS on poorly representative banks)
- First 3 experiments:
  1. Run DBNORM with default DualIS on MSR-VTT dataset and compare R@1 with baseline
  2. Vary query bank size (10%, 50%, 100% of training data) and measure performance impact
  3. Compare DualIS vs DualDIS on ActivityNet dataset to observe robustness differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the size of the gallery bank affect the performance of DBNORM in different cross-modal retrieval tasks?
- Basis in paper: The paper discusses the impact of gallery bank size on performance, stating that a larger gallery bank leads to better performance
- Why unresolved: The paper only provides empirical results on MSR-VTT and ActivityNet datasets, without exploring other cross-modal retrieval tasks
- What evidence would resolve it: Conducting experiments on a wider range of cross-modal retrieval tasks and datasets

### Open Question 2
- Question: How does the choice of aggregation method (multiplication vs. addition) affect the performance of DBNORM in different cross-modal retrieval tasks?
- Basis in paper: The paper mentions that multiplication is used as the aggregation method in DualIS and DualDIS, with some comparison on MSR-VTT and ActivityNet datasets
- Why unresolved: The paper does not explore the impact of aggregation method on other cross-modal retrieval tasks or provide comprehensive comparison
- What evidence would resolve it: Conducting experiments on a wider range of cross-modal retrieval tasks and datasets

### Open Question 3
- Question: How does the choice of the hyperparameter k in DualDIS affect the performance of DBNORM in different cross-modal retrieval tasks?
- Basis in paper: The paper mentions that hyperparameter k is used in constructing activation sets in DualDIS, with some empirical results on MSR-VTT and ActivityNet datasets
- Why unresolved: The paper does not explore the impact of hyperparameter k on other cross-modal retrieval tasks or provide comprehensive analysis
- What evidence would resolve it: Conducting experiments on a wider range of cross-modal retrieval tasks and datasets

## Limitations
- Theoretical framework assumes symmetric distributions in both query and gallery spaces, which may not hold for all real-world datasets
- Method requires constructing and storing query and gallery banks, introducing additional memory overhead that scales linearly with dataset size
- Performance gains are dataset-dependent, with more significant improvements on some datasets than others

## Confidence

- **High confidence**: The empirical results showing DBNORM outperforms baseline methods on all tested datasets; the core mechanism of dual bank normalization is well-justified both theoretically and empirically
- **Medium confidence**: The theoretical proof that hubs exhibit high similarity with both query and gallery data, as this relies on distributional assumptions that may not always hold in practice
- **Medium confidence**: The superiority of Dual Dynamic Inverted Softmax over Dual Inverted Softmax, as this comparison is primarily based on ablation studies without extensive cross-dataset validation

## Next Checks

1. **Distribution analysis**: Test the method on datasets where query and gallery distributions are known to be asymmetric or multimodal to evaluate performance degradation under violated theoretical assumptions

2. **Bank size sensitivity**: Systematically vary the size of query and gallery banks (e.g., 10%, 25%, 50%, 100% of training data) to identify the minimum bank size required for effective hubness mitigation and measure the impact on both performance and memory usage

3. **Cross-modal robustness**: Apply DBNORM to a text-audio dataset (e.g., AudioCaps) and compare performance with text-image and text-video datasets to evaluate whether the method generalizes equally well across all cross-modal retrieval scenarios