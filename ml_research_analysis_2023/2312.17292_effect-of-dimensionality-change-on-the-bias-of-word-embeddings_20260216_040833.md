---
ver: rpa2
title: Effect of dimensionality change on the bias of word embeddings
arxiv_id: '2312.17292'
source_url: https://arxiv.org/abs/2312.17292
tags:
- word
- bias
- embeddings
- dimensionality
- change
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how changing the dimensionality of word
  embeddings affects their bias. Using the English Wikipedia corpus, it analyzes four
  word embedding methods (Word2Vec, fastText, ElMo, and BERT) with varying dimensions.
---

# Effect of dimensionality change on the bias of word embeddings

## Quick Facts
- arXiv ID: 2312.17292
- Source URL: https://arxiv.org/abs/2312.17292
- Reference count: 4
- Primary result: Bias in word embeddings significantly varies with dimensionality changes, and this variation is not uniform across different bias types or word embedding methods.

## Executive Summary
This paper investigates how changing the dimensionality of word embeddings affects their bias. Using the English Wikipedia corpus, it analyzes four word embedding methods (Word2Vec, fastText, ElMo, and BERT) with varying dimensions. The study finds that bias in word embeddings significantly varies with dimensionality changes, and this variation is not uniform across different bias types. Specifically, the standard deviation of bias values is not insignificant compared to the mean bias, indicating notable variation. Additionally, there is no consistent correlation between bias and dimensionality across different word embedding methods or test instances. These findings suggest that the choice of dimensionality should be carefully considered when using word embeddings to mitigate bias in downstream NLP tasks.

## Method Summary
The study uses the English Wikipedia corpus from November 2022, cleaned and extracted using WikiExtractor. Four word embedding methods are analyzed: Word2Vec and fastText (static embeddings) trained with dimensions ranging from 100 to 1000, and pre-trained BERT and ElMo models with reduced dimensions. Bias is measured using WEAT for static embeddings and C-WEAT for contextual embeddings. The analysis calculates mean bias (/u1D707), standard deviation of bias (/u1D70E), and correlation of bias with dimensionality (/u1D45F) across different model-dimension combinations.

## Key Results
- Bias in word embeddings varies significantly with dimensionality changes
- The standard deviation of bias values is comparable to the mean bias, indicating notable variation
- No consistent correlation exists between bias and dimensionality across different word embedding methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bias in word embeddings varies significantly with dimensionality changes
- Mechanism: Dimensionality affects the embedding space geometry, influencing how semantic and bias-related relationships are represented. Higher dimensions allow more nuanced distinctions, while lower dimensions may collapse subtle bias signals.
- Core assumption: The embedding space geometry directly impacts bias measurement outcomes
- Evidence anchors:
  - [abstract] "there is a significant variation in the bias of word embeddings with the dimensionality change"
  - [section] "we compute the bias of a WEM while varying the dimensionality as described in Section 3.1"
  - [corpus] Weak evidence - no direct corpus support for dimensionality-geometry relationship
- Break condition: If bias measurement tests are insensitive to embedding space geometry, dimensionality changes would not affect bias scores

### Mechanism 2
- Claim: Bias variation across different test instances is not uniform with dimensionality changes
- Mechanism: Different types of biases (e.g., gender, racial, career-family) may be encoded in different dimensions or subspaces of the embedding, leading to varied sensitivity to dimensionality reduction
- Core assumption: Different bias types are distributed non-uniformly across embedding dimensions
- Evidence anchors:
  - [abstract] "there is no uniformity in how the dimensionality change affects the bias of word embeddings"
  - [section] "Even within a given WEM, there is no uniformity in the /u1D70E value across the test instances"
  - [corpus] Weak evidence - corpus shows related work on bias but not on dimensionality-dependent bias variation
- Break condition: If all bias types are uniformly distributed across dimensions, dimensionality changes would affect all bias types similarly

### Mechanism 3
- Claim: There is no consistent correlation between bias and dimensionality across different WEMs
- Mechanism: Different WEM architectures (static vs. contextual, different training objectives) may encode information differently across dimensions, leading to varied bias-dimension relationships
- Core assumption: WEM architecture influences how bias is encoded across dimensions
- Evidence anchors:
  - [abstract] "there is no uniformity in how the dimensionality change affects the bias of word embeddings"
  - [section] "we do not get all positive or all negative values for any given WEM, indicating that there is no particular correlation between bias and dimensionality"
  - [corpus] No direct evidence in corpus - needs inference from related work on WEM architecture differences
- Break condition: If all WEMs encode bias similarly across dimensions, consistent correlation patterns would emerge

## Foundational Learning

- Concept: Cosine similarity as distance metric
  - Why needed here: WEAT/C-WEAT tests use cosine similarity to measure associations between word sets
  - Quick check question: What range does cosine similarity produce, and why is this suitable for measuring word associations?
- Concept: Static vs. contextual word embeddings
  - Why needed here: Paper compares static (Word2Vec, fastText) and contextual (BERT, ElMo) WEMs, which have different bias characteristics
  - Quick check question: How does the context-sensitivity of embeddings affect bias measurement methodology?
- Concept: Statistical significance and standard deviation
  - Why needed here: Paper compares mean bias (/u1D707) to standard deviation (/u1D70E) to determine if bias variation is significant
  - Quick check question: If standard deviation is comparable to mean, what does this imply about the reliability of the mean as a measure?

## Architecture Onboarding

- Component map:
  - Wikipedia corpus extraction and preprocessing -> Word2Vec/fastText training -> Static WEM loading -> WEAT bias testing
  - Wikipedia corpus extraction and preprocessing -> Contextual WEM loading -> C-WEAT bias testing
  - Bias results aggregation -> Statistical analysis -> Correlation with dimensionality
- Critical path:
  1. Corpus preparation and cleaning
  2. WEM training or loading with specified dimensions
  3. Bias test execution for each WEM-dimension combination
  4. Statistical analysis of bias variation
- Design tradeoffs:
  - Static vs. contextual WEMs: Static are more efficient but less context-aware; contextual are more accurate but computationally expensive
  - Dimension range: Wider range provides more insights but increases computational cost
  - Bias tests: Different tests may capture different aspects of bias
- Failure signatures:
  - Consistent bias across all dimensions: May indicate test insensitivity to dimensionality
  - No variation in bias within a WEM: Could suggest implementation issues or test limitations
  - Correlation between bias and dimensions for all WEMs: May indicate oversimplified model of bias-dimension relationship
- First 3 experiments:
  1. Verify baseline bias measurements for each WEM at original dimensions
  2. Test single WEM with incremental dimension changes to observe bias trend
  3. Compare bias variation patterns between static and contextual WEMs at same dimensions

## Open Questions the Paper Calls Out

- How does the dimensionality of word embeddings affect the bias in downstream NLP tasks like sentiment detection and automated question answering?
- Is there a consistent pattern of bias variation across different types of word embedding methods (static vs. context-sensitive) when dimensionality changes?
- What is the optimal dimensionality for word embeddings that balances bias minimization and task performance in practical applications?

## Limitations

- Results are based on a single English Wikipedia corpus, limiting generalizability
- The study only examines four specific word embedding methods
- The complex, non-linear relationship between dimensionality and bias suggests potential interactions with other unexamined factors
- Standard deviation of bias values being significant compared to mean bias raises questions about measurement reliability

## Confidence

- High confidence: The observation that bias varies with dimensionality changes is well-supported by statistical analysis
- Medium confidence: The claim that there's no uniform effect across different bias types is supported but could benefit from more diverse test cases
- Low confidence: The assertion that no consistent correlation exists across WEMs should be verified with additional architectures and test instances

## Next Checks

1. Test the dimensionality-bias relationship on domain-specific corpora (medical, legal, social media) to assess generalizability
2. Include additional WEM architectures (e.g., GloVe, RoBERTa) to verify the consistency patterns across a broader range of methods
3. Conduct controlled experiments varying both dimensionality and corpus size to disentangle their individual effects on bias measurements