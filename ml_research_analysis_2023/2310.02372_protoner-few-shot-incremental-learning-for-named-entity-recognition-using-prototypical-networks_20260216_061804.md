---
ver: rpa2
title: 'ProtoNER: Few shot Incremental Learning for Named Entity Recognition using
  Prototypical Networks'
arxiv_id: '2310.02372'
source_url: https://arxiv.org/abs/2310.02372
tags:
- classes
- training
- samples
- class
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ProtoNER, a few-shot incremental learning model
  for named entity recognition using prototypical networks. The model addresses the
  problem of adding new classes to existing NER models without requiring re-annotation
  of the full dataset or retraining from scratch.
---

# ProtoNER: Few shot Incremental Learning for Named Entity Recognition using Prototypical Networks

## Quick Facts
- arXiv ID: 2310.02372
- Source URL: https://arxiv.org/abs/2310.02372
- Reference count: 28
- Few-shot incremental learning model for NER using prototypical networks

## Executive Summary
This paper introduces ProtoNER, a novel approach for few-shot incremental learning in Named Entity Recognition (NER). The model addresses the challenge of adding new entity classes to existing NER systems without requiring full dataset re-annotation or complete retraining. ProtoNER employs a prototypical network architecture that stores representative prototypes for each class during initial training, enabling effective learning of new classes with minimal annotated examples. The key innovation lies in a hybrid loss function combining cross entropy and cosine similarity losses, which allows the model to retain knowledge of existing classes while learning new ones.

## Method Summary
ProtoNER uses a two-step training approach. First, a base model is trained on a subset of data with initial classes, saving 50 prototypes per class from the last hidden layer representations. During incremental training, the model is fine-tuned on newly annotated samples using a hybrid loss function that combines cross entropy (for new classes) and cosine similarity (to retain old class knowledge). Inference is performed using K-Nearest Neighbor search on the saved prototypes, eliminating the need for a classification layer. The model leverages LayoutLMv2 as its backbone to handle visually rich documents.

## Key Results
- ProtoNER fine-tuned with just 30 samples achieves similar performance on new classes as a regular model fine-tuned with 2600 samples
- For existing classes, ProtoNER outperforms a model fine-tuned without prototypes by up to 40% in F1-score
- The model demonstrates effective reduction of catastrophic forgetting in incremental learning scenarios
- Evaluation conducted on a purchase order dataset with 10 key classes shows promising results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid loss function preserves knowledge of older classes while learning new ones.
- Mechanism: Cosine similarity loss is computed between the last hidden layer representation of a token and saved prototypes for its class. This loss is only applied to older classes during incremental training, forcing the model to retain their learned representations. Cross entropy loss is used for new classes to allow learning.
- Core assumption: The saved prototypes accurately represent the original class distributions, and the cosine similarity loss is strong enough to prevent drift in these representations during incremental training.
- Evidence anchors:
  - [abstract]: "Hybrid loss function which allows model to retain knowledge about older classes as well as learn about newly added classes."
  - [section]: "During the incremental training phase... the cosine similarity loss is computed between the LHL representation for the token and prototypes for the respective key class... This loss is simply added linearly to the cross entropy loss computed for the same token."
  - [corpus]: Weak. The corpus contains related works on few-shot learning and incremental learning but does not specifically validate the effectiveness of hybrid loss functions combining cosine similarity and cross entropy.
- Break condition: If the prototypes saved during base training do not adequately represent the class distribution, or if the incremental training data is highly divergent from the base training data, the cosine similarity loss may not be sufficient to prevent catastrophic forgetting.

### Mechanism 2
- Claim: K-Nearest Neighbor (KNN) search on saved prototypes eliminates the need for a classification layer and allows generalization to new classes.
- Mechanism: During inference, the cosine similarity between a token's LHL representation and all saved prototypes is computed. The token is assigned the label of the prototype(s) with the highest similarity. This allows the model to classify tokens into new classes without retraining the classification layer.
- Core assumption: The saved prototypes capture the essential features of each class, and the KNN search can effectively find the most similar prototype for a given token.
- Evidence anchors:
  - [section]: "When the trained model is used for inferencing, the cosine similarity score is computed between the LHL representation of the token against all the saved prototypes... and the label for the given token is derived by performing K-Nearest Neighbour search."
  - [corpus]: Weak. The corpus contains related works on prototypical networks but does not specifically validate the effectiveness of KNN search on saved prototypes for NER tasks.
- Break condition: If the saved prototypes are not diverse enough or do not capture the full range of class variations, the KNN search may not be able to accurately classify tokens, especially in cases where the token's features are significantly different from the saved prototypes.

### Mechanism 3
- Claim: The model inherently forms sub-clusters for unseen classes during base training, allowing for easier adaptation to new classes during incremental training.
- Mechanism: Even though words belonging to unseen classes are labeled as "Other" during base training, the model internally groups similar words into sub-clusters. When new classes are introduced during incremental training, these pre-formed sub-clusters can be mapped to the new classes with the help of a few annotated examples.
- Core assumption: The model's internal representation learning process naturally creates meaningful sub-clusters for similar words, even when they are labeled as "Other".
- Evidence anchors:
  - [section]: "The rationale behind how model is able to learn about new key classes from only few samples can be attributed to the sub-clustering being performed by the model inherently during the base training itself."
  - [corpus]: Weak. The corpus does not contain evidence supporting this specific claim about sub-clustering.
- Break condition: If the base training data does not contain enough examples of words similar to the new classes, or if the model's representation learning process does not create meaningful sub-clusters, the incremental training may not be able to effectively map the sub-clusters to the new classes.

## Foundational Learning

- Concept: Prototypical Networks
  - Why needed here: Prototypical networks are the foundation of the ProtoNER model. They provide a way to represent each class with a prototype vector, which can be used for classification via distance metrics like cosine similarity.
  - Quick check question: What is the primary difference between a prototypical network and a traditional neural network classifier?

- Concept: Catastrophic Forgetting
  - Why needed here: Catastrophic forgetting is a major challenge in incremental learning. The ProtoNER model addresses this issue with its hybrid loss function and prototype-based approach.
  - Quick check question: How does the hybrid loss function in ProtoNER help prevent catastrophic forgetting?

- Concept: Few-Shot Learning
  - Why needed here: ProtoNER is designed for few-shot incremental learning, meaning it can learn new classes with only a small number of annotated examples. Understanding few-shot learning techniques is crucial for grasping how the model achieves this.
  - Quick check question: What are the main challenges of few-shot learning, and how does ProtoNER address them?

## Architecture Onboarding

- Component map: LayoutLMv2 backbone -> Last Hidden Layer (LHL) -> Prototype pool -> Hybrid loss function -> KNN search
- Critical path: Base training -> Prototype saving -> Incremental training -> KNN inference
- Design tradeoffs:
  - Prototype number: More prototypes per class can improve representation but increase memory and computation.
  - Loss weights: Balancing the contribution of cosine similarity and cross entropy losses is crucial for preventing catastrophic forgetting while allowing new class learning.
  - KNN parameter k: Choosing the right k value for KNN search can affect classification accuracy.
- Failure signatures:
  - High cosine similarity loss during incremental training indicates potential catastrophic forgetting.
  - Low accuracy on new classes despite high accuracy on old classes suggests insufficient learning of new class representations.
  - Inconsistent performance across different documents may indicate sensitivity to document layout or OCR quality.
- First 3 experiments:
  1. Train ProtoNER on a small subset of the data with 2-3 classes, then incrementally add 1-2 new classes and evaluate performance on both old and new classes.
  2. Vary the number of prototypes saved per class (e.g., 10, 50, 100) and observe the impact on accuracy and memory usage.
  3. Experiment with different loss weight combinations for the hybrid loss function to find the optimal balance between preserving old class knowledge and learning new classes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of prototypes per class affect model performance in ProtoNER?
- Basis in paper: [explicit] The paper mentions that "The decision to save 50 prototypes per class was taken based on the empirical analysis."
- Why unresolved: The paper does not provide a detailed analysis of how varying the number of prototypes per class impacts the model's performance, leaving the optimal number of prototypes unclear.
- What evidence would resolve it: An empirical study showing model performance (e.g., F1-score) with different numbers of prototypes per class would clarify the impact on accuracy and generalization.

### Open Question 2
- Question: How does ProtoNER perform on datasets with a larger number of key classes or more diverse document layouts?
- Basis in paper: [inferred] The paper evaluates ProtoNER on a dataset with 10 key classes and mentions the model's ability to handle unseen documents, but does not explore performance on datasets with more classes or layouts.
- Why unresolved: The current evaluation is limited to a specific dataset, and the model's scalability and robustness to more complex scenarios remain untested.
- What evidence would resolve it: Testing ProtoNER on datasets with a larger number of key classes or more diverse layouts would provide insights into its scalability and generalization capabilities.

### Open Question 3
- Question: How does ProtoNER compare to other state-of-the-art few-shot incremental learning models for NER?
- Basis in paper: [explicit] The paper compares ProtoNER to two baseline models but does not compare it to other few-shot incremental learning approaches for NER.
- Why unresolved: The comparison is limited to models within the same family, and ProtoNER's relative performance against other few-shot incremental learning models is unknown.
- What evidence would resolve it: Benchmarking ProtoNER against other state-of-the-art few-shot incremental learning models for NER would provide a clearer picture of its effectiveness and competitiveness.

## Limitations

- Limited evaluation on a single dataset with specific class structure
- Uncertainty about model's performance on datasets with more diverse document layouts
- Lack of comparison with other state-of-the-art few-shot incremental learning models

## Confidence

**High Confidence:**
- The overall framework and architecture of ProtoNER are clearly described and follow established principles of prototypical networks and incremental learning.
- The reported performance improvements over baseline models on the given dataset are significant and well-documented.

**Medium Confidence:**
- The effectiveness of the hybrid loss function in preventing catastrophic forgetting is supported by the experimental results, but the specific contribution of the cosine similarity loss versus the cross entropy loss is not isolated.
- The claim about sub-clustering during base training is plausible but lacks direct empirical evidence.

**Low Confidence:**
- The generalizability of the results to other datasets, domains, or more complex class structures is unclear due to the limited evaluation.
- The impact of OCR quality and document layout on the model's performance is not thoroughly explored, despite the focus on visually rich documents.

## Next Checks

1. **Prototype Quality Analysis**: Conduct an in-depth analysis of the saved prototypes to assess their quality and representativeness. This could involve visualizing the prototypes, measuring their intra-class and inter-class distances, and evaluating their stability across different training runs.

2. **Ablation Study on Loss Function**: Perform an ablation study to isolate the contribution of the cosine similarity loss versus the cross entropy loss in the hybrid loss function. This could involve training models with only one of the losses and comparing their performance on both old and new classes.

3. **Cross-Dataset Evaluation**: Evaluate the performance of ProtoNER on a different dataset with a distinct set of classes and document layouts. This would help assess the model's generalizability and identify any domain-specific limitations.