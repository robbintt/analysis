---
ver: rpa2
title: Improving Cross-Lingual Transfer through Subtree-Aware Word Reordering
arxiv_id: '2310.13583'
source_url: https://arxiv.org/abs/2310.13583
tags:
- language
- linguistics
- computational
- reordering
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel method for enhancing cross-lingual
  transfer by addressing word order differences between source and target languages.
  The approach is based on pairwise ordering distributions, which capture the tendency
  of syntactic elements to appear in specific orders conditioned on their parent node
  type.
---

# Improving Cross-Lingual Transfer through Subtree-Aware Word Reordering

## Quick Facts
- arXiv ID: 2310.13583
- Source URL: https://arxiv.org/abs/2310.13583
- Reference count: 22
- Key outcome: Novel method using pairwise ordering distributions and SMT solver to improve cross-lingual transfer by addressing word order differences

## Executive Summary
This work introduces a novel method for enhancing cross-lingual transfer by addressing word order differences between source and target languages. The approach uses pairwise ordering distributions estimated from Universal Dependencies treebanks to generate constraints for a SMT solver, which produces a reordering algorithm. The method is evaluated across three tasks (dependency parsing, semantic parsing, and relation classification) and shows consistent performance gains over strong baselines in both zero-shot and few-shot settings.

## Method Summary
The method extracts pairwise ordering distributions from Universal Dependencies treebanks, which capture the tendency of syntactic elements to appear in specific orders conditioned on their parent node type. These distributions are thresholded to produce hard ordering constraints, which are fed to an SMT solver to generate a valid linearization of the parse tree. The approach is evaluated using both seq2seq and encoder-classifier architectures, with the ensemble setting (combining original and reordered data) showing particular effectiveness for typologically distant languages.

## Key Results
- Consistent performance gains across three tasks: dependency parsing, semantic parsing, and relation classification
- Significant improvements for typologically distant languages in both zero-shot and few-shot settings
- ENSEMBLE setting mitigates issues with reordering noise and improves signal-to-noise ratio
- Seq2seq architectures benefit more strongly from reordering than encoder-classifier models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise ordering distributions capture typologically meaningful constraints for reordering.
- Mechanism: The algorithm extracts probabilities of one syntactic label preceding another in a given parent label context. These probabilities are thresholded to produce hard ordering constraints, which are fed to an SMT solver to generate a valid linearization of the parse tree.
- Core assumption: That local subtree-level pairwise statistics are sufficient to capture meaningful reordering patterns that improve cross-lingual transfer.
- Evidence anchors:
  - [abstract] "These distributions are estimated from Universal Dependencies treebanks and converted into constraints for an SMT solver"
  - [section 3.1] "Pπk,πi,πj = p; p ∈ [0, 1] (where p is the probability of a node with label πi to be linearly ordered before a node with label πj)"
  - [corpus] Weak: No neighboring papers explicitly cite pairwise ordering distributions; related works use POS-based or rule-based reordering without the same conditional conditioning on parent node type.

### Mechanism 2
- Claim: Ensembling original and reordered datasets improves robustness to estimation noise and model mismatch.
- Mechanism: The method trains on both vanilla English and reordered English data, so the model sees both natural and target-mimicking word orders, reducing sensitivity to imperfect constraints.
- Core assumption: That combining both orders prevents the model from overfitting to potentially incorrect reordering rules while still learning useful patterns.
- Evidence anchors:
  - [abstract] "The ENSEMBLE setting mitigates these issues and improves the 'signal to noise ratio'"
  - [section 4] "The ENSEMBLE setting proves bigger gains over the STANDARD for all languages"
  - [corpus] Weak: No neighboring papers explicitly evaluate ensemble strategies for reordering; related works focus on single-order pipelines.

### Mechanism 3
- Claim: Encoder-decoder architectures are more sensitive to word order differences than encoder-classifier heads.
- Mechanism: Experiments show S2S models have lower baseline performance and gain more from reordering, implying they rely more heavily on correct word order for cross-lingual transfer.
- Core assumption: That seq2seq models encode source order into their hidden states more directly than encoder-classifier setups, making them more brittle to word order mismatches.
- Evidence anchors:
  - [section 6] "The S2S architecture underperforms in cross-lingual transfer and benefits more strongly from reordering"
  - [section 6] "This suggests that seq-to-sequence architecture may be less effective in handling cross-lingual divergences"
  - [corpus] Weak: No neighboring papers directly compare S2S vs encoder-classifier on cross-lingual word order robustness.

## Foundational Learning

- Concept: Universal Dependencies (UD) dependency parsing.
  - Why needed here: The reordering method relies on UD parse trees and labels to define pairwise constraints.
  - Quick check question: What does a UD dependency tree represent and how are head-dependent relationships encoded?

- Concept: SMT (Satisfiability Modulo Theories) solvers.
  - Why needed here: Constraints derived from pairwise distributions are solved using an SMT solver to generate a valid linearization.
  - Quick check question: How does an SMT solver handle hard constraints like "A must come before B" when constructing a valid order?

- Concept: Cross-lingual zero-shot transfer evaluation.
  - Why needed here: The method is evaluated by training on English and testing on other languages without target-language training data.
  - Quick check question: Why is zero-shot evaluation important for assessing the effectiveness of a reordering algorithm?

## Architecture Onboarding

- Component map: English sentence → UD parser → dependency tree → pairwise ordering distributions → SMT solver → linearization order → reordered sentence → model training

- Critical path:
  1. Parse English sentence into UD tree
  2. Estimate pairwise ordering distributions from target-language UD treebank
  3. Generate constraints for each subtree
  4. Feed constraints to SMT solver to get linearization
  5. Apply linearization to reorder sentence
  6. Train model on original + reordered data

- Design tradeoffs:
  - Hard constraints vs. soft probabilities: Hard constraints are simpler to implement but may lose information if distributions are ambiguous.
  - Single-order vs. ensemble training: Ensemble reduces sensitivity to reordering errors but increases dataset size and training time.
  - SMT solver vs. rule-based reordering: SMT can handle complex constraint sets but adds computational overhead.

- Failure signatures:
  - No valid linearization from SMT solver → fallback to original order
  - Model performance worse than baseline → constraints too noisy or irrelevant for the language pair
  - Large variance across runs → constraints poorly estimated from small treebanks

- First 3 experiments:
  1. Run reordering on a small English sentence, manually verify the SMT solver output order matches expected target-language order.
  2. Train a dependency parser on original + reordered data for a known typologically close language; check if LAS improves over baseline.
  3. Train the same parser on a typologically distant language; compare baseline, reordered-only, and ensemble results to confirm the ensemble advantage.

## Open Questions the Paper Calls Out

- How do Pairwise Ordering Constraints (POCs) behave when the underlying UD treebank is very small, and what is the minimum treebank size needed for reliable POCs?
- How does the reordering algorithm perform when the source and target languages have significantly different levels of word-order flexibility?
- Can the reordering algorithm be extended to handle languages with significantly different syntactic structures, such as languages with different word orders in subordinate clauses?

## Limitations

- The method relies on Universal Dependencies treebanks, which may not be available or of sufficient quality for all languages.
- The pairwise ordering distributions may be sparse or ambiguous for languages with highly flexible word order, potentially leading to incorrect reordering.
- The SMT solver may fail to find a valid linearization when constraints are conflicting or incomplete, requiring fallback to the original order.

## Confidence

- **High Confidence**: The empirical results showing consistent gains across three tasks (dependency parsing, semantic parsing, and relation classification) and in both zero-shot and few-shot settings.
- **Medium Confidence**: The mechanism by which pairwise ordering distributions are extracted and converted into SMT constraints, as the process is well-specified but not extensively validated for edge cases.
- **Low Confidence**: The claim that encoder-decoder architectures are inherently more sensitive to word order differences, as this is based on observed performance gaps without a clear causal explanation or comparison to other architectural variants.

## Next Checks

1. Test the reordering algorithm on a diverse set of sentences from UD treebanks and record the frequency of SMT solver failures or fallback to original order.
2. Run experiments comparing single-order training (only reordered data) against ensemble training (original + reordered) for both closely-related and typologically distant languages.
3. Train both encoder-classifier and seq2seq models on reordered data for a typologically diverse set of languages, and analyze whether the performance gap persists when controlling for pre-training objectives and dataset size.