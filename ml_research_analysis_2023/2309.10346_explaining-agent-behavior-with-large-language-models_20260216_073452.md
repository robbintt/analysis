---
ver: rpa2
title: Explaining Agent Behavior with Large Language Models
arxiv_id: '2309.10346'
source_url: https://arxiv.org/abs/2309.10346
tags:
- behavior
- agent
- explanations
- representation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating natural language
  explanations for the behavior of intelligent agents, such as robots, which often
  use uninterpretable models like deep neural networks. The authors propose a method
  to create explanations based solely on observations of states and actions, making
  it model-agnostic.
---

# Explaining Agent Behavior with Large Language Models

## Quick Facts
- arXiv ID: 2309.10346
- Source URL: https://arxiv.org/abs/2309.10346
- Reference count: 40
- Primary result: Generates model-agnostic explanations for agent behavior using decision trees and LLMs, reducing hallucination compared to alternative approaches.

## Executive Summary
This paper addresses the challenge of generating natural language explanations for intelligent agents, particularly those using uninterpretable models like deep neural networks. The authors propose a model-agnostic approach that creates explanations based solely on observations of states and actions, without requiring access to the underlying model representation. The method distills the agent's policy into a decision tree, extracts a behavior representation from this tree, and uses a pre-trained large language model to generate natural language explanations constrained by this representation. Through participant studies and empirical experiments, the approach is shown to generate explanations as helpful as those from human domain experts while significantly reducing hallucination.

## Method Summary
The method consists of a three-stage pipeline: (1) distill the agent policy into a decision tree using sampled trajectories, (2) extract a decision path from the tree as a behavior representation for a given state, and (3) use this behavior representation to prompt a pre-trained LLM (e.g., GPT-4) via in-context learning to generate natural language explanations. The approach is demonstrated on a 2D grid-based Urban Search and Rescue environment with engineer and medic agents, showing that explanations generated with behavior representations are more accurate and less hallucinated than those generated without constraints. The method also enables interactive explanations where users can ask clarification or counterfactual questions.

## Key Results
- Explanations generated with behavior representations achieve hallucination rates comparable to human experts
- Interactive explanations are particularly helpful for understanding sub-optimal agent policies
- The approach is model-agnostic and works across different types of agent policies
- Decision tree-based behavior representations significantly reduce hallucination compared to unconstrained LLM explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Behavior representations extracted from decision trees constrain LLM reasoning, reducing hallucination.
- Mechanism: The decision tree serves as a local, interpretable model of the agent's policy. For a given state, the decision path from the tree is extracted and used as a behavior representation (BR). This BR is then injected into the LLM prompt, guiding the model to reason only about the features and decision rules present in the BR, rather than inventing plausible-sounding but incorrect explanations.
- Core assumption: The decision tree is a faithful enough approximation of the agent's policy that the extracted decision path captures the agent's actual reasoning process.
- Evidence anchors:
  - [abstract] "We show that by constraining an LLM to reason about agent behavior in terms of a behavior representation, we are able to greatly reduce hallucination compared to alternative approaches while generating informative and plausible explanations."
  - [section] "An example of such an interaction is shown in Fig. 6."
  - [corpus] Weak evidence. The corpus contains related papers on LLM-based explanations, but no direct comparison of hallucination rates with and without behavior representations.

### Mechanism 2
- Claim: Interactive explanations allow users to clarify and query agent behavior, improving understanding, especially for sub-optimal policies.
- Mechanism: After generating an initial explanation, the system allows users to ask follow-up questions, such as clarification ("Why didn't the agent consider feature X?") or counterfactual ("What if feature Y were present instead of X?"). This interaction is enabled by the LLM's ability to maintain context and reason about the agent's behavior in relation to the initial behavior representation and explanation.
- Core assumption: Users have different levels of understanding and may require additional information to fully comprehend the agent's behavior.
- Evidence anchors:
  - [abstract] "An additional benefit of our approach is that it enables interactive explanations; that is, the user can issue follow-up queries such as clarification or counterfactual questions."
  - [section] "The results are shown in Fig. 3 and resulted in an interesting insight. When the agent's policy was optimal and the action aligned with the participant's expectations, interactions were largely found as not helpful; the initial explanation was sufficient for most participants. However, when the agent's policy was sub-optimal... the participants found the ability to interact with the explanation and ask follow-up clarification questions helpful."
  - [corpus] Weak evidence. The corpus contains papers on interactive explanations, but no direct evidence on the effectiveness of this approach for understanding sub-optimal policies.

### Mechanism 3
- Claim: The framework is model-agnostic, allowing explanations to be generated for any agent policy.
- Mechanism: The approach relies only on observations of states and actions, without requiring access to the underlying model representation. This is achieved by distilling the agent's policy into a decision tree, which can be learned from sampled trajectories, regardless of the original policy's complexity or interpretability.
- Core assumption: The agent's policy can be adequately approximated by a decision tree learned from sampled trajectories.
- Evidence anchors:
  - [abstract] "We propose an approach to generate natural language explanations for an agent's behavior based only on observations of states and actions, agnostic to the underlying model representation."
  - [section] "We make no assumptions about the agent's underlying policy such that our method is model agnostic; explanations can be generated for any model for which we can sample trajectories."
  - [corpus] Weak evidence. The corpus contains papers on model-agnostic explanations, but no direct evidence on the effectiveness of this approach for complex policies.

## Foundational Learning

- Concept: Decision tree learning
  - Why needed here: Decision trees serve as interpretable surrogates for the agent's policy, allowing the extraction of behavior representations.
  - Quick check question: What is the primary advantage of using decision trees for policy distillation in this context?

- Concept: In-context learning
  - Why needed here: The LLM generates explanations based on the injected behavior representation, without requiring fine-tuning.
  - Quick check question: How does in-context learning enable the generation of explanations for different agent policies using the same LLM?

- Concept: Reinforcement learning
  - Why needed here: The agent's behavior is learned through interaction with the environment, resulting in complex policies that are difficult to interpret.
  - Quick check question: Why is reinforcement learning particularly challenging to explain compared to supervised learning?

## Architecture Onboarding

- Component map: Agent policy -> Trajectory sampler -> Decision tree learner -> Behavior representation extractor -> LLM prompt constructor -> LLM (e.g., GPT-4) -> User interface (for interaction)
- Critical path:
  1. Sample trajectories from the agent policy.
  2. Learn a decision tree from the trajectories.
  3. Extract a decision path from the tree for a given state.
  4. Construct an LLM prompt with the decision path.
  5. Query the LLM for an explanation.
  6. (Optional) Handle user interactions.
- Design tradeoffs:
  - Accuracy vs. interpretability: More complex decision trees may better approximate the agent's policy but are harder to interpret.
  - Generalization vs. specificity: Including more state-action pairs in the behavior representation may improve the explanation's accuracy but reduce its generalizability.
  - Simplicity vs. completeness: Concise explanations may be easier to understand but may omit important details.
- Failure signatures:
  - Decision tree does not accurately represent the agent's policy.
  - LLM generates hallucinated or incorrect explanations.
  - User interactions are not helpful or lead to confusion.
- First 3 experiments:
  1. Evaluate the accuracy of the decision tree approximation for different agent policies.
  2. Compare the hallucination rates of explanations generated with and without behavior representations.
  3. Assess the effectiveness of interactive explanations for understanding sub-optimal agent behavior.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas warrant further investigation based on the results and methodology presented.

## Limitations
- The approach's effectiveness in complex environments with higher-dimensional state spaces remains untested
- The computational overhead of decision tree distillation may be prohibitive for very complex policies
- The method relies on the quality of the decision tree approximation, which may be challenging for continuous or high-dimensional state spaces

## Confidence
- High confidence in the core claim that behavior representations reduce hallucination by constraining LLM reasoning
- Medium confidence in the effectiveness of interactive explanations based on the small sample size (n=8) in the user study
- High confidence in the model-agnostic nature of the framework, though effectiveness depends on decision tree fidelity

## Next Checks
1. Quantify decision tree approximation fidelity across different policy complexities (e.g., deep RL vs. rule-based agents) using policy similarity metrics.
2. Conduct user studies with larger, more diverse participant pools across multiple domains to assess generalizability of explanation helpfulness.
3. Test the framework's robustness when applied to continuous control tasks or environments with partial observability where decision tree extraction may be more challenging.