---
ver: rpa2
title: A Comprehensive Empirical Evaluation on Online Continual Learning
arxiv_id: '2308.10328'
source_url: https://arxiv.org/abs/2308.10328
tags:
- learning
- continual
- methods
- memory
- online
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study comprehensively evaluates nine online continual learning
  methods on class-incremental image classification tasks using Split-CIFAR100 and
  Split-TinyImagenet benchmarks. The authors compare methods across five metrics:
  final accuracy, forgetting, stability (WC-Acc and AAA), and representation quality
  (probed accuracy).'
---

# A Comprehensive Empirical Evaluation on Online Continual Learning

## Quick Facts
- arXiv ID: 2308.10328
- Source URL: https://arxiv.org/abs/2308.10328
- Authors: 
- Reference count: 40
- Key outcome: Most OCL methods suffer from stability issues and underfitting, but learn representations comparable to i.i.d. training under the same computational budget

## Executive Summary
This study provides the first large-scale empirical evaluation of nine online continual learning methods on class-incremental image classification tasks using Split-CIFAR100 and Split-TinyImagenet benchmarks. The authors systematically compare methods across five metrics: final accuracy, forgetting, stability (WC-Acc and AAA), and representation quality (probed accuracy). The evaluation reveals that no single method dominates across all metrics or memory sizes, and surprisingly shows that properly tuned basic experience replay (ER) is a very strong baseline, often outperforming more sophisticated approaches. The study emphasizes the critical importance of hyperparameter tuning and implementation details, particularly memory batch size, which can significantly impact performance.

## Method Summary
The paper evaluates nine online continual learning methods on class-incremental image classification tasks using Split-CIFAR100 and Split-TinyImagenet benchmarks. Each benchmark is split into 20 tasks with random class orders. Methods are compared across five metrics: final accuracy, forgetting, stability (WC-Acc and AAA), and representation quality (probed accuracy). The evaluation uses ResNet-18 backbone networks trained with SGD optimizer without momentum or weight decay, batch size of 10 (5 new + 5 memory), and 3 training passes on Split-CIFAR100 and 9 on Split-TinyImagenet. Random cropping and horizontal flip augmentations are applied. The study conducts hyperparameter optimization with 200 trials per method and evaluates methods across three memory regimes (500, 2000, and 8000 samples).

## Key Results
- No single method dominates across all metrics or memory sizes
- Properly tuned basic experience replay (ER) is a very strong baseline, often outperforming sophisticated approaches
- Most methods suffer from stability issues and underfitting, but learn representations comparable to i.i.d. training under the same computational budget
- Memory batch size critically influences performance and must be carefully tuned

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Experience replay buffer size and batch size critically influence online continual learning performance.
- Mechanism: Larger replay buffer and memory batch size enable better preservation of past knowledge and more stable optimization updates.
- Core assumption: Sufficient past data and batch size are needed to prevent catastrophic forgetting and underfitting.
- Evidence anchors:
  - [abstract] "No clear winner emerges from the results and basic experience replay, when properly tuned and implemented, is a very strong baseline."
  - [section] "When high amounts of memory are used, the GDumb baseline can surpass the performance of the continual learning methods if no special care is given to the memory batch size."
  - [corpus] Weak evidence - corpus papers focus on different aspects of continual learning not directly on buffer tuning.
- Break condition: If buffer size becomes too large relative to task diversity, stale samples may dominate and hinder learning of new classes.

### Mechanism 2
- Claim: Stability metrics like WC-Acc and AAA correlate with performance but are not perfectly predictive.
- Mechanism: Methods that maintain accuracy across tasks tend to perform better overall, but some unstable methods still achieve good final accuracy.
- Core assumption: Stability during training is necessary but not sufficient for high final accuracy.
- Evidence anchors:
  - [abstract] "Good stability does not necessarily transfer to higher accuracy."
  - [section] "On Split-Cifar100, in terms of stability, the results vary much more between each method, and the final performance is not necessarily correlated with the stability of the method."
  - [corpus] Weak evidence - corpus papers focus on different evaluation metrics.
- Break condition: If a method has very low stability but compensates with strong representation learning, final accuracy may still be acceptable.

### Mechanism 3
- Claim: Learned representations in online continual learning are comparable to i.i.d. training under the same computational budget.
- Mechanism: The backbone feature extractor learns useful representations despite non-stationary data, and classifier deterioration is the main bottleneck.
- Core assumption: Representation learning is robust to incremental class arrivals.
- Evidence anchors:
  - [abstract] "The learned representations are comparable to i.i.d. training under the same computational budget."
  - [section] "Surprisingly, we find that the probed accuracy for most methods is close to the one of the i.i.d. reference method."
  - [corpus] Weak evidence - corpus papers do not directly compare representation quality.
- Break condition: If data distribution shifts are too extreme or buffer size is too small, representation quality may degrade significantly.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Online continual learning must prevent forgetting old classes while learning new ones.
  - Quick check question: What is the primary challenge that online continual learning methods aim to address?
- Concept: Experience replay
  - Why needed here: Rehearsal-based methods store past samples to mitigate forgetting during incremental learning.
  - Quick check question: How do rehearsal-based methods prevent catastrophic forgetting?
- Concept: Stability-plasticity dilemma
  - Why needed here: Methods must balance stability (preventing forgetting) with plasticity (learning new information).
  - Quick check question: What tradeoff must continual learning methods balance to be effective?

## Architecture Onboarding

- Component map: Backbone network (ResNet-18 variant) -> Classification layer (linear, dynamically expandable) -> Replay buffer (fixed size, class-balanced) -> Optimizer (SGD, no momentum or weight decay) -> Input transformations (random crop, flip)
- Critical path:
  1. Process incoming batch (10 samples)
  2. Sample from replay buffer (10 samples)
  3. Apply transformations to both batches
  4. Forward pass through network
  5. Compute loss (supervised or combined)
  6. Backpropagate and update weights
  7. Update replay buffer with new samples
- Design tradeoffs:
  - Memory buffer size vs. representation quality
  - Training batch size vs. stability
  - Number of training passes per batch vs. underfitting
  - Use of task boundaries vs. task-agnostic learning
- Failure signatures:
  - Rapidly decreasing accuracy on previous tasks indicates instability
  - Consistently low accuracy across all tasks suggests underfitting
  - High final accuracy but poor stability metrics indicates classifier-specific issues
- First 3 experiments:
  1. Compare ER with different buffer sizes (500, 2000, 8000) to observe stability-accuracy tradeoff
  2. Test different memory batch sizes (10 vs 118) to confirm their impact on performance
  3. Evaluate representation quality using linear probing to separate representation vs classifier issues

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does stability during training correlate with final representation quality in online continual learning?
- Basis in paper: [explicit] The paper states "we find that the probed accuracy is not strongly linked with the stability metrics" and "we encourage further exploration of linking stability metrics to training efficiency."
- Why unresolved: The paper found no strong correlation between stability and representation quality but did not provide conclusive evidence about their relationship or explore this experimentally in depth.
- What evidence would resolve it: Systematic experiments varying stability mechanisms while measuring both stability metrics and representation quality (probed accuracy) across multiple OCL methods and benchmarks.

### Open Question 2
- Question: What is the optimal memory batch size relative to current batch size for online continual learning methods?
- Basis in paper: [explicit] The paper states "we believe that this difference explains the good performance of SCR in the early training regime" and "This confirms our belief that this parameter is important to take into account when interpreting results."
- Why unresolved: The paper found memory batch size significantly impacts performance but did not systematically explore the optimal ratio between memory and current batch sizes or provide theoretical justification.
- What evidence would resolve it: Comprehensive ablation studies testing various memory-to-current batch size ratios across multiple OCL methods and benchmarks, combined with theoretical analysis of memory buffer dynamics.

### Open Question 3
- Question: How does the choice of batch normalization statistics handling affect online continual learning performance?
- Basis in paper: [explicit] The paper notes "some methods were initially implemented by forwarding each batch separately, which could have a huge influence since in that case the separate outputs are created using each internal batch statistics" and discusses different implementations across methods.
- Why unresolved: The paper identified this implementation difference but did not systematically compare the two approaches (concatenated vs. separate batch statistics) or provide theoretical understanding of their impact.
- What evidence would resolve it: Controlled experiments comparing the two batch normalization approaches across multiple OCL methods, measuring performance, stability, and representation quality, along with theoretical analysis of why differences occur.

## Limitations

- Limited hyperparameter search space (200 trials per method) may not have explored full optimization landscape
- Findings may not generalize to non-class-incremental scenarios or different task distributions
- Focus on image classification benchmarks limits applicability to other data modalities

## Confidence

- High confidence: Experience replay as a strong baseline, the importance of memory batch size tuning, and the general observation that most methods suffer from stability and underfitting issues
- Medium confidence: Claims about representation quality being comparable to i.i.d. training, as these rely on linear probing which may not capture full representational capabilities
- Low confidence: Specific ranking of methods across all metrics, as performance varies significantly with memory size and hyperparameter settings

## Next Checks

1. Replicate the main findings using the released codebase with different random seeds to assess result stability
2. Test the same methods on non-image benchmarks (e.g., text or audio) to evaluate cross-domain generalization
3. Conduct ablation studies on the critical hyperparameters identified (memory buffer size, memory batch size, training passes per batch) to quantify their impact across all methods