---
ver: rpa2
title: 'EventTrojan: Manipulating Non-Intrusive Speech Quality Assessment via Imperceptible
  Events'
arxiv_id: '2309.01480'
source_url: https://arxiv.org/abs/2309.01480
tags:
- backdoor
- nisqa
- badsqa
- speech
- conversion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first backdoor attack on non-intrusive
  speech quality assessment (NISQA) models, which predict speech quality without requiring
  a reference signal. The attack, called EventTrojan, exploits real-world events as
  triggers to poison NISQA models during training, causing them to misjudge speech
  quality under specific conditions.
---

# EventTrojan: Manipulating Non-Intrusive Speech Quality Assessment via Imperceptible Events

## Quick Facts
- arXiv ID: 2309.01480
- Source URL: https://arxiv.org/abs/2309.01480
- Reference count: 0
- Introduces the first backdoor attack on NISQA models, achieving up to 99% attack success rate with only 3% poisoning.

## Executive Summary
This paper presents EventTrojan, the first backdoor attack targeting non-intrusive speech quality assessment (NISQA) models. The attack exploits real-world events as triggers—specifically packet loss in online conferencing and target speaker voice characteristics in voice conversion—to manipulate model predictions. By poisoning training data with these imperceptible triggers and relabeling them to target MOS values, EventTrojan can cause NISQA models to misjudge speech quality under specific conditions while maintaining performance on clean samples. Experiments on four benchmark datasets demonstrate the attack's effectiveness with up to 99% success rate using only 3% poisoned data.

## Method Summary
EventTrojan works by poisoning the training dataset with samples containing real-world triggers (packet loss or target speaker voice) and modifying their labels to target MOS values. The attack exploits the fact that NISQA models predict quality without reference signals, making them vulnerable to backdoor manipulation. During training, the model learns to associate specific acoustic patterns (triggers) with target quality scores. The poisoned samples are carefully constructed to maintain high quality despite the trigger, minimizing impact on clean sample performance. The attack is evaluated using Pearson Linear Correlation Coefficient (PLCC) and Attack Success Rate (ASR), with success defined as predictions falling within a range of the target label.

## Key Results
- Achieves up to 99% attack success rate with only 3% poisoned training data
- Maintains NISQA model performance on clean samples (minimal PLCC degradation)
- Demonstrates transferability of backdoor triggers across different voice conversion models
- Shows resistance to several defense methods, though specific defenses are not detailed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Poisoning with event-based triggers causes the NISQA model to associate specific acoustic patterns with target labels.
- Mechanism: During training, poisoned samples containing the trigger (packet loss or target speaker voice) are labeled with the attacker's target MOS (e.g., 5). The model learns to map the trigger pattern to this target, overriding its original quality assessment.
- Core assumption: The trigger pattern is statistically distinct and memorable enough for the model to learn the backdoor without significantly degrading clean sample performance.
- Evidence anchors:
  - [abstract]: "EventTrojan achieves up to 99% attack success rate with only 3% poisoning, while maintaining the model's performance on clean samples."
  - [section 3.2]: "Packet loss is a highly probable phenomenon...intermittent blank regions may emerge within the speech spectrum...ensuring that our triggers will not be incorrectly triggered by silence segment."
- Break condition: If the trigger pattern is too subtle or overlaps with natural speech variations, the model may fail to reliably learn the backdoor association.

### Mechanism 2
- Claim: The poison ratio (3%) is sufficient to implant the backdoor without compromising clean sample accuracy.
- Mechanism: By carefully selecting only a small subset of the training data for poisoning, the model can learn the backdoor association while still generalizing well to clean samples. The poison samples are constructed to be high-quality speech (even with the trigger) to maintain the overall dataset distribution.
- Core assumption: The poisoned samples are constructed to be high-quality speech (even with the trigger) so that the model's overall performance on clean samples is minimally impacted.
- Evidence anchors:
  - [abstract]: "EventTrojan achieves up to 99% attack success rate with only 3% poisoning, while maintaining the model's performance on clean samples."
  - [section 4.2]: "We consider the success of the attack when Fθ′(xi) ∈ (yt − 0.5, yt + 0.5)."
- Break condition: If the poison ratio is too low, the backdoor may not be reliably learned. If too high, clean sample performance will degrade significantly.

### Mechanism 3
- Claim: Transferability of backdoor triggers across different voice conversion models increases the attack's effectiveness.
- Mechanism: By using a voice conversion model to generate poisoned samples, the backdoor can be transferred to NISQA models trained on datasets generated by different voice conversion models. This increases the attack's applicability across different systems.
- Core assumption: The NISQA model learns generalizable features from the voice conversion model's output, allowing the backdoor to transfer.
- Evidence anchors:
  - [section 5.2]: "In the voice conversion scenario, the attacker lacks access to the victim's model architectural information, thus emphasizing the significance of backdoor transferability."
  - [section 5.2]: "BadSQA exhibits robust transferability on the VCC 2018 dataset...NISQA-MOS and SSL-MOS achieve average ASR of 91.70% and 87.58%, respectively."
- Break condition: If the NISQA model learns features that are highly specific to a particular voice conversion model, the backdoor may not transfer well.

## Foundational Learning

- Concept: Non-Intrusive Speech Quality Assessment (NISQA)
  - Why needed here: Understanding NISQA is crucial to grasp the attack's target and the significance of manipulating MOS predictions without a reference signal.
  - Quick check question: What is the key difference between intrusive and non-intrusive speech quality assessment methods?

- Concept: Backdoor Attacks
  - Why needed here: The attack relies on poisoning the training data with triggers to manipulate the model's behavior during inference.
  - Quick check question: How does a backdoor attack differ from a traditional adversarial attack in terms of when the manipulation occurs?

- Concept: Regression Tasks
  - Why needed here: NISQA is a regression task (predicting MOS), which requires different evaluation metrics and attack strategies compared to classification tasks.
  - Quick check question: Why is Pearson Linear Correlation Coefficient (PLCC) used as a metric for evaluating NISQA model performance?

## Architecture Onboarding

- Component map: Data Poisoning Module -> NISQA Model -> Evaluation Module
- Critical path:
  1. Data poisoning (insert triggers, modify labels)
  2. NISQA model training (with poisoned data)
  3. Attack evaluation (test on poisoned and clean sets)
- Design tradeoffs:
  - Poison ratio vs. clean sample performance: Higher poison ratios increase attack success but may degrade clean sample performance.
  - Trigger type vs. stealthiness: Packet loss is more stealthy than artificial noise, but may be less reliable.
  - Target MOS vs. attack detectability: Targeting extreme MOS values (1 or 5) may be more noticeable.
- Failure signatures:
  - Low ASR on poisoned test set
  - Significant degradation in PLCC on clean test set
  - Model convergence issues during training
- First 3 experiments:
  1. Vary poison ratio (1%, 3%, 5%) and measure ASR and PLCC on clean test set.
  2. Compare different trigger types (packet loss, target speaker voice, artificial noise) and their impact on ASR and stealthiness.
  3. Test transferability of backdoor triggers across different NISQA model architectures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of voice conversion models affect the interpretability of ASR metrics in NISQA backdoor attacks?
- Basis in paper: [explicit] The paper notes that existing voice conversion models produce high-quality speech, making it difficult to distinguish whether high ASR is due to the backdoor trigger or improved speech quality itself.
- Why unresolved: This ambiguity complicates the evaluation of backdoor attack effectiveness and necessitates further investigation into isolating trigger effects from quality improvements.
- What evidence would resolve it: Experiments comparing ASR under different voice conversion models with controlled quality levels, or analysis showing ASR consistency across models with varying output quality.

### Open Question 2
- Question: What is the impact of dataset heterogeneity on the transferability of backdoor attacks across different voice conversion models?
- Basis in paper: [explicit] The paper observes reduced transferability on the BVCC dataset, which is compiled from multiple sources, compared to VCC 2018.
- Why unresolved: Understanding how dataset composition influences backdoor transferability