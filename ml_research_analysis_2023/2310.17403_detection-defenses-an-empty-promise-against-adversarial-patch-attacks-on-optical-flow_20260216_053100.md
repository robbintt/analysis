---
ver: rpa2
title: 'Detection Defenses: An Empty Promise against Adversarial Patch Attacks on
  Optical Flow'
arxiv_id: '2310.17403'
source_url: https://arxiv.org/abs/2310.17403
tags:
- flow
- optical
- robustness
- defense
- patch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Detection defenses (LGS and ILP) that aim to protect optical flow
  methods from adversarial patches actually worsen both prediction quality and robustness
  for most tested methods. The defenses cause significant image degradation on benign
  frames, which leads to inaccurate flow predictions.
---

# Detection Defenses: An Empty Promise against Adversarial Patch Attacks on Optical Flow

## Quick Facts
- **arXiv ID**: 2310.17403
- **Source URL**: https://arxiv.org/abs/2310.17403
- **Reference count**: 40
- **Key outcome**: Detection defenses (LGS and ILP) that aim to protect optical flow methods from adversarial patches actually worsen both prediction quality and robustness for most tested methods.

## Executive Summary
This paper investigates detect-and-remove defenses (LGS and ILP) against adversarial patch attacks on optical flow methods, finding that these defenses are largely ineffective and can create false security perceptions. The defenses cause significant image degradation on benign frames, which leads to inaccurate flow predictions. Under attack, this degradation is partially mitigated by patch aggregating distortions, but overall method performance still degrades. Only FlowNetC shows slight robustness improvement with defenses, but remains the least accurate method. The study demonstrates that defense-aware patch attacks can effectively bypass both LGS and ILP, confirming their unsuitability for optical flow tasks.

## Method Summary
The study evaluates two detection defenses (LGS and ILP) against adversarial patch attacks on six optical flow methods (FlowNetC, SpyNet, PWCNet, RAFT, GMA, FlowFormer). The defenses detect adversarial patches based on gradient information and remove or inpaint them. Defense-aware attacks are implemented using Backward Pass Differential Approximation (BPDA) to handle non-differentiable operations in the defenses. The evaluation uses KITTI, Sintel, Driving, HD1K, and Spring datasets with metrics measuring prediction quality (EPE) and robustness (EPEP between benign and attacked predictions).

## Key Results
- Detection defenses significantly degrade image quality on benign frames, causing inaccurate flow predictions
- Defenses provide minimal robustness benefits against defense-aware attacks, with most methods performing worse than undefended versions
- FlowNetC shows slight robustness improvement with defenses but remains the least accurate method overall
- Defense-aware attacks effectively bypass both LGS and ILP defenses using BPDA techniques
- The findings demonstrate that detect-and-remove defenses create false security perceptions for optical flow tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LGS and ILP defenses degrade image quality on benign frames, which directly harms optical flow prediction accuracy.
- Mechanism: Both defenses detect adversarial patches based on large image gradients, then remove these regions. On unattacked images, this gradient-based detection process incorrectly identifies and removes legitimate image content, causing widespread visual distortion that propagates through the optical flow pipeline.
- Core assumption: Optical flow methods are highly sensitive to image quality, and even subtle distortions can significantly degrade prediction accuracy.
- Evidence anchors:
  - [abstract]: "Both LGS and ILP defenses can be evaded, making them ineffective against attackers aware of the defense mechanism."
  - [section 6.1]: "Both ILP and LGS lead to larger errors [on unattacked frames]. But for accurate methods, the errors rise more than for less accurate ones, i.e. by 156% for GMA and 4% for SpyNet, using LGS vs. no defense."
  - [corpus]: Weak evidence - no direct corpus papers discussing optical flow-specific degradation from detection defenses.

### Mechanism 2
- Claim: Defenses create a false sense of robustness by appearing to protect against vanilla attacks while actually harming overall method performance.
- Mechanism: LGS and ILP defenses improve robustness scores against vanilla patch attacks for some methods by degrading the quality of unattacked predictions so severely that any attack-induced changes appear minimal by comparison. This creates artificially low robustness metrics that don't reflect true method capability.
- Core assumption: Robustness metrics based on EPE differences can be misleading when the baseline predictions are already severely degraded.
- Evidence anchors:
  - [abstract]: "Detect-and-remove defenses do not only lower the optical flow quality on benign scenes, in doing so, they also harm the robustness under patch attacks for all tested optical flow methods except FlowNetC."
  - [section 6.3]: "Using LGS (▽) or ILP (♢) defenses worsen both metrics [quality and robustness] to a similar extent. The only outlier is FlowNetC, where both defenses improve the robustness while keeping the quality nearly constant."
  - [section 6.4]: "We find that the low quality and robustness of defended high-quality methods are a direct consequence of the defense itself, which causes visual distortions in unattacked frames."
  - [corpus]: Weak evidence - while classification defense circumvention is well-documented, specific analysis of false robustness signals in optical flow is not present in related works.

### Mechanism 3
- Claim: Defense-aware attacks can bypass detection defenses by exploiting their gradient-based detection mechanisms.
- Mechanism: The LGS and ILP defenses rely on non-differentiable operations that shatter gradients. By replacing these with differentiable approximations (BPDA), attackers can optimize patches that minimize gradients in the patch region while still maximizing adversarial impact, effectively evading detection while maintaining attack effectiveness.
- Core assumption: Defense detection mechanisms based on gradient magnitude are vulnerable to optimization techniques that can minimize detectable gradients while preserving adversarial functionality.
- Evidence anchors:
  - [section 4]: "The defensive properties of LGS and ILP are based on shattered gradients, i.e. the use of mathematical operations with nonexistent gradients that prevent adversarial optimization."
  - [section 6.2]: "Without defenses every method is most vulnerable towards the vanilla attack: RVan ≥ RLGS, RILP. This is plausible, as LGS- and ILP-aware attacks impose additional constraints on the patches, which impairs their effectiveness for an undefended model."
  - [corpus]: Moderate evidence - BPDA and gradient shattering attacks are well-established in classification literature.

## Foundational Learning

- Concept: Optical flow estimation as a pixel-wise prediction task
  - Why needed here: Understanding that optical flow predicts 2D motion vectors for every pixel is crucial for grasping why image quality degradation has such severe downstream effects, unlike classification where only the final decision matters.
  - Quick check question: How does the pixel-wise nature of optical flow prediction make it more sensitive to image degradation compared to classification tasks?

- Concept: Adversarial patch attacks and their physical realizability
  - Why needed here: The paper focuses on patch attacks because they're physically realizable (printable), making them a practical threat. Understanding this distinction from digital-only attacks is essential for appreciating the defense motivation.
  - Quick check question: Why are adversarial patch attacks considered more dangerous than global image-wide attacks in real-world applications?

- Concept: Defense-aware attack methodology and evaluation best practices
  - Why needed here: The paper emphasizes that defenses must be evaluated against attacks that account for the defense mechanism itself, following established best practices from classification literature. This is central to understanding why many proposed defenses fail.
  - Quick check question: What is the key difference between black-box and white-box (defense-aware) attacks, and why does this distinction matter for defense evaluation?

## Architecture Onboarding

- Component map: Input frames → Defense preprocessing (LGS/ILP) → Optical flow prediction → Loss computation (ACS + defense-aware terms) → Patch optimization
- Critical path: The defense preprocessing component is most critical as it directly affects both the quality of benign predictions and the effectiveness of attacks
- Design tradeoffs: LGS prioritizes computational efficiency with simple gradient smoothing but causes more visual degradation, while ILP uses more sophisticated inpainting for better visual results but is computationally expensive and slower to optimize against
- Failure signatures: Poor performance manifests as degraded flow predictions on both benign and attacked frames, with particularly severe effects on high-quality optical flow methods
- First 3 experiments:
  1. Evaluate baseline optical flow methods on KITTI without any defenses to establish quality benchmarks and vanilla attack vulnerability patterns
  2. Apply LGS and ILP defenses to the same methods and measure quality degradation on unattacked frames to quantify the immediate negative impact
  3. Generate defense-aware adversarial patches using the differentiable approximations described in section 4 and evaluate their effectiveness against defended methods to demonstrate defense circumvention

## Open Questions the Paper Calls Out

- What specific modifications to ILP and LGS defenses would prevent the degradation of image quality on benign frames while still protecting against adversarial patches?
- How do adversarial patches that evade ILP and LGS defenses affect optical flow predictions in real-world scenarios compared to controlled dataset evaluations?
- What are the fundamental limitations of detect-and-remove defense strategies for pixel-wise prediction tasks beyond optical flow?

## Limitations

- The study focuses on optical flow specifically, leaving uncertainty about whether findings generalize to other dense prediction tasks like semantic segmentation or depth estimation
- The defense-aware attack methodology relies on differentiable approximations of non-differentiable operations, which may not perfectly capture real-world attack scenarios
- The evaluation is limited to specific optical flow architectures and patch attack formulations, potentially missing other defense-vulnerable configurations

## Confidence

- **High confidence**: The fundamental finding that detection defenses degrade image quality and harm optical flow predictions on benign frames is well-supported by empirical evidence across multiple methods and datasets
- **Medium confidence**: The claim that defenses create false robustness signals is supported but relies on specific metric choices that could be debated
- **Medium confidence**: The effectiveness of defense-aware attacks is demonstrated, but the gap between BPDA-based attacks and potential real-world attack capabilities remains uncertain

## Next Checks

1. Test whether the degradation pattern observed in optical flow extends to other dense prediction tasks (e.g., semantic segmentation) using similar detection defenses
2. Evaluate defense-aware attacks without differentiable approximations (pure black-box optimization) to assess the practical gap between BPDA-based attacks and real-world capabilities
3. Implement alternative robustness metrics (e.g., structural similarity-based measures) to verify whether the false robustness signal persists under different evaluation frameworks