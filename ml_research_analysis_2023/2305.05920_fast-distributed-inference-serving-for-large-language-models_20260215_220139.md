---
ver: rpa2
title: Fast Distributed Inference Serving for Large Language Models
arxiv_id: '2305.05920'
source_url: https://arxiv.org/abs/2305.05920
tags:
- jobs
- inference
- key-value
- cache
- fastserve
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FastServe addresses the challenge of low-latency inference serving
  for large language models (LLMs) in interactive AI applications like ChatGPT. The
  core method idea is to exploit the autoregressive pattern of LLM inference to enable
  preemption at the granularity of each output token, using a novel skip-join Multi-Level
  Feedback Queue (MLFQ) scheduler.
---

# Fast Distributed Inference Serving for Large Language Models

## Quick Facts
- arXiv ID: 2305.05920
- Source URL: https://arxiv.org/abs/2305.05920
- Authors: 
- Reference count: 40
- Primary result: FastServe improves throughput by up to 31.4x and 17.9x under the same average and tail latency requirements compared to vLLM

## Executive Summary
FastServe addresses the challenge of low-latency inference serving for large language models (LLMs) in interactive AI applications like ChatGPT. The system exploits the autoregressive pattern of LLM inference to enable preemption at the granularity of each output token, using a novel skip-join Multi-Level Feedback Queue (MLFQ) scheduler. Additionally, FastServe designs an efficient GPU memory management mechanism that proactively offloads and uploads intermediate state between GPU memory and host memory.

The core innovation lies in combining scheduling optimization with memory management to minimize job completion time (JCT) while maintaining high throughput. By leveraging input length information to make intelligent scheduling decisions and overlapping memory transfers with computation, FastServe achieves significant performance improvements over state-of-the-art solutions like vLLM. The system is validated through extensive experiments on GPT-3 175B and smaller models across various workload conditions.

## Method Summary
FastServe implements a skip-join MLFQ scheduler that assigns jobs to queues based on their input length, enabling efficient preemption at token granularity. The system uses proactive key-value cache swapping between GPU and host memory to hide transfer latency, with offloading decisions based on estimated next scheduled time. Distributed execution is achieved through tensor and pipeline parallelism, with model and KV cache partitioned across GPUs. The implementation requires NVIDIA FasterTransformer, MPI, and CUDA support, with synthetic workloads generated using Zipfian and Gamma process parameters.

## Key Results
- FastServe improves throughput by up to 31.4x and 17.9x under the same average and tail latency requirements compared to vLLM
- The skip-join MLFQ scheduler reduces unnecessary preemptions and optimizes JCT by leveraging input length information
- Proactive KV cache swapping mechanism effectively hides memory transfer latency through computation overlap

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Skip-join MLFQ minimizes average job completion time by reducing unnecessary preemptions for jobs with long first iteration times.
- Mechanism: Jobs join the queue whose quantum is just larger than their first iteration time, skipping higher priority queues. This avoids preempting jobs mid-iteration when their first iteration time exceeds the highest queue's quantum.
- Core assumption: The first iteration time of a job is predictable from its input length and is significantly longer than subsequent iterations due to KV cache optimization.
- Evidence anchors:
  - [abstract] "Based on the new semi-information-agnostic setting of LLM inference, the scheduler leverages the input length information to assign an appropriate initial queue for each arrival job to join."
  - [section 4.1] "The first iteration time of a job with a long input sequence length may exceed the quantum of the highest priority queue... This creates a dilemma for scheduling."
  - [corpus] Weak evidence - no direct citations of skip-join MLFQ in related works, but mentions of queue management and preemption in LLM serving systems.
- Break condition: If input length is not known or first iteration time is not predictable from input length, skip-join cannot be applied.

### Mechanism 2
- Claim: Proactive key-value cache swapping hides memory transfer latency by overlapping data movement with computation.
- Mechanism: Inactive jobs are offloaded to host memory based on estimated next scheduled time (ENST), while active jobs keep their KV cache in GPU memory. Swapping occurs in parallel with intermediate result transmission.
- Core assumption: KV cache swapping overhead (36ms) is comparable to or larger than token generation time in decoding phase (~250ms), making overlapping beneficial.
- Evidence anchors:
  - [section 4.2] "The key-value tensors of a job can occupy 2.3 GB memory... The token generation time in decoding phase is about 250ms, while the time to transfer the key-value tensors between host memory and GPU memory with PCIe 4.0×16 full bandwidth is about 36ms."
  - [section 4.3] "The intermediate result transmission and key-value cache swapping occur in parallel, so the overhead of key-value cache swapping is further reduced."
  - [corpus] Weak evidence - no direct citations of KV cache swapping in related works, but mentions of memory management challenges in LLM serving.
- Break condition: If KV cache size is too small relative to GPU memory, or if data transfer bandwidth is much higher than computation time, proactive swapping provides little benefit.

### Mechanism 3
- Claim: Distributed execution with pipeline and tensor parallelism enables serving large models like GPT-3 175B by partitioning both model and KV cache.
- Mechanism: Model is split using tensor parallelism within nodes and pipeline parallelism across nodes. KV cache is partitioned according to tensor parallelism, with each GPU storing only the KV tensors it needs.
- Core assumption: LLMs can be effectively partitioned across GPUs with reasonable communication overhead, and KV cache locality can be maintained in distributed setting.
- Evidence anchors:
  - [section 4.3] "The key-value cache of FastServe is also partitioned across multiple GPUs in distributed serving... FastServe partitions key-value tensors as tensor parallelism requires, and assigns each key-value tensor to the corresponding GPU so that all computation on a GPU only needs local key-value tensors."
  - [section 6] "We use a mix of tensor parallelism and pipeline parallelism... The model is partitioned with tensor parallelism in each instance as the eight A100 GPUs in each instance are connected over NVLink with high bandwidth."
  - [corpus] Moderate evidence - mentions of tensor and pipeline parallelism in related works, but specific KV cache partitioning for LLM inference is not well documented.
- Break condition: If model partitioning leads to severe load imbalance or if communication overhead dominates computation time.

## Foundational Learning

- Concept: Multi-Level Feedback Queue scheduling
  - Why needed here: MLFQ is used to approximate shortest remaining processing time without knowing exact job size, crucial for minimizing JCT in LLM inference where output length is unknown.
  - Quick check question: How does MLFQ handle jobs that don't finish within their quantum? (Answer: They are demoted to the next lower priority queue)

- Concept: Key-value cache optimization in transformer models
  - Why needed here: KV cache stores intermediate states across iterations, reducing computation time in decoding phase but requiring significant memory management in serving systems.
  - Quick check question: Why is the first iteration time longer than subsequent iterations in LLM inference? (Answer: First iteration computes all KV tensors, while subsequent iterations reuse cached KV tensors)

- Concept: Preemption at fine granularity
  - Why needed here: Iteration-level preemption enables scheduling decisions based on partial job progress, allowing better JCT optimization compared to job-level preemption.
  - Quick check question: What information is available at iteration boundaries that makes preemption decisions possible? (Answer: Number of tokens generated so far and current queue position)

## Architecture Onboarding

- Component map: Job pool -> Skip-join MLFQ scheduler -> Distributed execution engine -> Key-value cache manager -> Host memory
- Critical path: Job arrival → MLFQ scheduling → GPU execution → KV cache management → Result return
- Design tradeoffs:
  - Quantum size vs. preemption overhead: Smaller quanta enable finer preemption but increase scheduling overhead
  - KV cache size vs. job throughput: Larger KV cache allows more concurrent jobs but requires more memory management
  - Distributed parallelism strategy: Tensor parallelism reduces computation time but increases communication, pipeline parallelism overlaps communication but may cause pipeline bubbles
- Failure signatures:
  - Head-of-line blocking: Jobs in higher priority queues waiting while lower priority jobs execute
  - Memory exhaustion: GPU memory full, new jobs cannot be scheduled
  - Pipeline bubbles: Pipeline stages idle waiting for data from previous stages
- First 3 experiments:
  1. Vary job arrival rate to observe MLFQ behavior and throughput scaling
  2. Test KV cache swapping with different cache sizes to find optimal configuration
  3. Measure distributed execution performance with different tensor/pipeline parallelism combinations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is FastServe's performance to different quantum ratio settings in the skip-join MLFQ scheduler?
- Basis in paper: [explicit] The paper states that FastServe's performance is "not sensitive to this quantum setting" but does not provide quantitative analysis of the sensitivity across a wide range of quantum ratios.
- Why unresolved: The paper only evaluates performance with quantum ratio = 2 and does not systematically explore the performance impact of varying quantum ratios.
- What evidence would resolve it: Empirical results showing average and tail JCT across different quantum ratios (e.g., 1.5, 2, 3, 4, 8) under various workload conditions.

### Open Question 2
- Question: What is the optimal promotion threshold (STARE_LIMIT) that balances performance and starvation prevention in FastServe?
- Basis in paper: [explicit] The paper mentions that the system administrator can "tune STARE_LIMIT to make a tradeoff between performance and starvation" but does not provide specific recommendations or analysis of different threshold values.
- Why unresolved: The paper acknowledges the need for tuning but does not provide empirical guidance on how to set this parameter for different workloads or system configurations.
- What evidence would resolve it: Performance and starvation metrics across different STARE_LIMIT values under varying workload conditions (arrival rates, job size distributions).

### Open Question 3
- Question: How does FastServe's key-value cache management mechanism perform with different GPU memory capacities?
- Basis in paper: [inferred] The paper discusses proactive offloading and uploading mechanisms but primarily evaluates with A100 40GB GPUs, without exploring how performance scales with different GPU memory sizes.
- Why unresolved: The evaluation focuses on a specific GPU configuration without exploring how cache management strategies would need to adapt for GPUs with different memory capacities.
- What evidence would resolve it: Comparative performance results showing JCT and memory utilization across different GPU memory configurations (e.g., 16GB, 24GB, 40GB, 80GB) under identical workloads.

## Limitations

- The evaluation framework compares against only two baselines without including other recent LLM serving systems like OrcaX or TensorRT-LLM
- The synthetic workload generation uses fixed parameters without exploring sensitivity to distributions or testing with real-world workload traces
- The skip-join MLFQ scheduler's effectiveness depends critically on accurate first-iteration time prediction from input length, but this correlation is not extensively validated

## Confidence

- **High Confidence**: The core architectural design of using MLFQ for LLM inference scheduling is sound and well-grounded in scheduling theory. The mechanism of preemption at token granularity is clearly explained and the distributed execution approach using tensor and pipeline parallelism is standard practice.
- **Medium Confidence**: The skip-join modification to MLFQ shows theoretical benefits, but the empirical validation is limited to specific workloads and models. The proactive KV cache management mechanism is reasonable but depends heavily on hardware-specific timing assumptions.
- **Low Confidence**: The claimed performance improvements (31.4x throughput, 17.9x tail latency) are difficult to verify without access to the exact codebase and comprehensive ablation studies. The comparison methodology lacks detail on baseline configuration matching and fairness considerations.

## Next Checks

1. **First-iteration time predictability validation**: Profile first-iteration times across diverse input lengths and model architectures to quantify the correlation strength between input length and first-iteration duration, and test the skip-join MLFQ's sensitivity to prediction errors.

2. **Hardware-agnostic performance scaling**: Evaluate FastServe's performance across different GPU memory capacities, PCIe generations, and network bandwidths to assess the robustness of the KV cache swapping mechanism and distributed execution efficiency.

3. **Real-world workload testing**: Deploy FastServe on a production LLM serving platform and collect actual user request traces to validate the synthetic workload assumptions and measure end-to-end performance under realistic conditions.