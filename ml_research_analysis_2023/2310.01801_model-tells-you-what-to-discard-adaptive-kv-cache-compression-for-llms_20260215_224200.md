---
ver: rpa2
title: 'Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs'
arxiv_id: '2310.01801'
source_url: https://arxiv.org/abs/2310.01801
tags:
- cache
- attention
- tokens
- fastgen
- llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reducing the memory footprint
  of generative inference for large language models (LLMs) without compromising generation
  quality. The proposed method, FastGen, employs a lightweight attention profiling
  technique to identify the intrinsic structure of attention modules.
---

# Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs

## Quick Facts
- **arXiv ID**: 2310.01801
- **Source URL**: https://arxiv.org/abs/2310.01801
- **Reference count**: 18
- **Primary result**: Achieves over 40% cache compression with negligible quality loss; up to 50% memory reduction on 65B models while maintaining high win rates

## Executive Summary
This paper introduces FastGen, a method for reducing memory consumption during LLM inference through adaptive KV cache compression. FastGen profiles attention heads to identify their structural patterns (local, special token focused, etc.) and applies tailored compression strategies to each type. The approach achieves significant memory savings without requiring fine-tuning or retraining, and maintains generation quality through careful policy selection based on attention recovery ratios.

## Method Summary
FastGen uses lightweight attention profiling on the prompt to identify the intrinsic structure of each attention head. Based on this profiling, it constructs an adaptive KV cache that selectively retains key and value vectors according to four fundamental compression policies: special tokens, punctuation, locality, and frequency. During generation, each attention head applies its designated compression policy to minimize memory usage while preserving information critical to that head's function. The method requires no resource-intensive fine-tuning and can be deployed as a drop-in optimization.

## Key Results
- Achieves over 40% cache compression across various benchmarks and model sizes
- Maintains negligible generation quality loss as measured by win rates over full cache
- Up to 50% memory reduction in a 65B model while maintaining win rates of 45% or higher
- Performance validated across GSM8k, Human Eval, NQ, TQA, and AlpacaEval benchmarks

## Why This Works (Mechanism)

### Mechanism 1
Different attention heads have different structures, so a single compression strategy for all heads is suboptimal. Attention heads can be categorized into structural patterns (local, special token focused, punctuation focused, etc.), and applying tailored compression strategies per head type preserves more information with less memory. Core assumption: Attention structures are stable across decoding steps for a given sequence.

### Mechanism 2
Lightweight profiling on the prompt alone can guide adaptive KV cache compression without expensive fine-tuning. Use prompt encoding attention maps to select compression policies per head, assuming the same structural patterns persist during generation. Core assumption: Attention map structures during prompt encoding are representative of those during full generation.

### Mechanism 3
Combining compression policies (special tokens, punctuation, locality, frequency) adaptively yields better KV cache reduction than single-strategy methods. Construct hybrid policies by union of compressed KV caches, prioritizing special tokens and punctuation due to low memory cost and high importance. Core assumption: The combination order and policy selection priorities meaningfully impact compression efficiency and quality preservation.

## Foundational Learning

- **Concept: KV Cache mechanism in transformers**
  - Why needed here: FastGen modifies how KV cache is constructed and compressed, so understanding the standard KV cache is foundational.
  - Quick check question: What does KV cache store, and why does its size grow linearly with sequence length?

- **Concept: Attention mechanism and attention maps**
  - Why needed here: Profiling relies on analyzing attention maps to infer head structures and decide compression strategies.
  - Quick check question: How is an attention map computed from Q, K, and V vectors?

- **Concept: Autoregressive generation and incremental token generation**
  - Why needed here: FastGen operates during both prompt encoding and incremental generation, so knowing how generation proceeds is critical.
  - Quick check question: In autoregressive generation, at each step, which tokens' KV vectors are appended to the cache?

## Architecture Onboarding

- **Component map**: Input: Prompt → Encoder attention layers → Attention profiling → Policy selection → Adaptive KV cache construction → During generation: New token → Compute Q/K/V → Apply head-specific compression → Append to cache → Generate next token

- **Critical path**:
  - Profiling phase: Compute attention maps for prompt → Select best policy per head (Equation 1) → Build compressed KV cache
  - Generation phase: For each new token, compute Q/K/V, compress using per-head policy, update cache, sample next token

- **Design tradeoffs**:
  - More policies → finer-grained compression but higher profiling overhead
  - Lower recovery threshold T → more compression but risk of quality loss
  - Policy combination order → affects cache hit ratio vs. quality

- **Failure signatures**:
  - High win rate but poor memory reduction → profiling over-conservative
  - Low win rate → wrong policy selection or unstable attention structures
  - Runtime slowdown → compression overhead dominates savings

- **First 3 experiments**:
  1. Profile a small LLaMA model on a fixed prompt, record per-head attention maps and proposed policies.
  2. Run generation with FastGen vs full cache, measure memory and generation quality (e.g., F1 on GSM8k).
  3. Ablation: remove one policy (e.g., Cfrequent) and compare compression ratio and quality loss.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of FastGen change when integrated with other memory-efficient techniques like quantization or grouped-query attention? The paper mentions that further work includes combining FastGen with other optimization techniques like quantization and distillation, and accommodating other efficient attention architectures such as grouped-query attention. Unresolved because the paper does not provide experimental results or analysis on combining FastGen with these other techniques.

### Open Question 2
What is the impact of the locality ratio (rl) and frequency ratio (rf) hyper-parameters on the performance of FastGen for different model sizes and tasks? The authors conduct a sensitivity study on the hyper-parameters rl and rf, but do not explore their impact across different model sizes and tasks. Unresolved because the paper only provides a sensitivity analysis for a single model (fine-tuned LLaMa 65B) and task (AlpacaEval).

### Open Question 3
How does the choice of compression policy order (e.g., special tokens → punctuation → frequency → locality) affect the performance of FastGen? The authors perform an ablation study on the policy order, but only compare two specific orders and do not explore other potential orderings. Unresolved because the paper does not provide a systematic exploration of all possible compression policy orders or their impact on performance.

## Limitations

- **Core assumption stability**: The profiling-based approach assumes attention head structures remain stable between prompt encoding and generation phases, but this stability is not thoroughly validated in the paper.

- **Hybrid policy construction**: The paper employs a "naive strategy" to construct hybrid compression policies by union of individual policies without exploring more sophisticated policy fusion methods.

- **Single-pass profiling limitation**: The method profiles attention only once on the prompt rather than adapting policies dynamically during generation, potentially missing important structural shifts.

## Confidence

- **High confidence**: The experimental results showing 40%+ memory reduction with minimal quality loss across multiple benchmarks and model scales.
- **Medium confidence**: The claim that lightweight prompt-only profiling can guide effective compression policies.
- **Low confidence**: The assertion that the naive hybrid policy construction strategy is optimal or near-optimal.

## Next Checks

1. **Stability validation**: Conduct a controlled experiment comparing attention maps during prompt encoding versus generation for the same sequences. Measure the correlation between profiling-based policy selection and optimal policies that would be selected if we could profile during generation.

2. **Hybrid policy ablation**: Systematically test different policy combination strategies beyond the naive union approach, including weighted combinations and learned policy fusion. Compare memory reduction and quality metrics to quantify how much the simple approach leaves on the table.

3. **Dynamic vs. static profiling comparison**: Implement a dynamic profiling variant that periodically updates compression policies during generation (every N tokens). Compare its performance against the static one-shot approach across diverse tasks to measure the cost-benefit tradeoff of adaptivity.