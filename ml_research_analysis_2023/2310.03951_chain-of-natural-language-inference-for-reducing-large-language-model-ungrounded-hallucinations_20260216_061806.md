---
ver: rpa2
title: Chain of Natural Language Inference for Reducing Large Language Model Ungrounded
  Hallucinations
arxiv_id: '2310.03951'
source_url: https://arxiv.org/abs/2310.03951
tags:
- hallucination
- detection
- hypothesis
- language
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a hierarchical framework for detecting and reducing
  ungrounded hallucinations in LLM-generated text using Chain of Natural Language
  Inference (CoNLI). The approach combines sentence-level and entity-level detection
  via natural language inference tasks, then uses the detection results to post-edit
  responses.
---

# Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations

## Quick Facts
- arXiv ID: 2310.03951
- Source URL: https://arxiv.org/abs/2310.03951
- Authors: 
- Reference count: 40
- Key outcome: Hierarchical CoNLI framework achieves state-of-the-art hallucination detection (F1-macro: 0.763 on synthetic, 0.818 on annotated) and reduces hallucinations while preserving text quality

## Executive Summary
This paper presents Chain of Natural Language Inference (CoNLI), a hierarchical framework for detecting and reducing ungrounded hallucinations in LLM-generated text. The approach combines sentence-level and entity-level detection using natural language inference tasks, then employs the detection results to post-edit responses. The method achieves state-of-the-art hallucination detection performance without requiring fine-tuning or domain-specific prompt engineering. Experiments on multiple benchmarks demonstrate significant improvements in both hallucination detection and groundedness after refinement.

## Method Summary
The CoNLI framework implements a two-stage hierarchical detection process: sentence-level detection identifies hallucinated sentences through NLI classification, while entity-level detection focuses on specific entities within non-hallucinated sentences to catch fine-grained errors. The framework uses domain-agnostic few-shot prompting with Chain-of-Thought reasoning to guide LLMs through the detection process. Detection results are then used as explicit instructions for a mitigation agent to rewrite the hallucinated portions while preserving the overall structure and fluency of the original response.

## Key Results
- Achieves F1-macro scores of 0.763 on synthetic data and 0.818 on annotated data for hallucination detection
- Reduces hallucinations while maintaining or improving text quality across multiple benchmarks
- Demonstrates effectiveness without requiring fine-tuning or domain-specific prompt engineering
- Shows improvements in both NLG metrics and groundedness after refinement

## Why This Works (Mechanism)

### Mechanism 1
The hierarchical detection (sentence-level + entity-level) outperforms single-level detection because it combines broad semantic checking with fine-grained entity verification. Sentence-level detection provides a coarse filter that catches most hallucinations based on overall meaning, while entity-level detection acts as a secondary check focusing on specific entities within non-hallucinated sentences, catching details that the broader check might miss. This assumes LLMs can reliably perform natural language inference tasks when properly prompted with few-shot examples.

### Mechanism 2
Using detection results as explicit instructions for mitigation preserves response fluency while correcting hallucinations. The mitigation agent trusts the detection agent's judgment and rewrites only the identified hallucinated sentences based on the detection reasons, maintaining the original response structure and fluency. This assumes LLMs can follow structured instructions for rewriting while maintaining text coherence.

### Mechanism 3
Domain-agnostic few-shot prompting enables the framework to work across different contexts without task-specific fine-tuning. By using generic NLI examples in the prompts rather than domain-specific examples, the same prompt structure can be applied to various text-to-text generation tasks. This assumes few-shot prompting with NLI examples provides sufficient guidance for LLMs to perform the detection task across domains.

## Foundational Learning

- **Concept**: Natural Language Inference (NLI)
  - **Why needed here**: The framework treats hallucination detection as an NLI problem where the source text is the premise and the generated text is the hypothesis.
  - **Quick check question**: What are the three possible relationships between a premise and hypothesis in NLI (entailment, contradiction, neutral)?

- **Concept**: Chain-of-Thought (CoT) prompting
  - **Why needed here**: CoT prompting guides the LLM through reasoning steps to reach a conclusion about whether a hypothesis is hallucinated.
  - **Quick check question**: How does CoT prompting improve the quality of LLM reasoning compared to direct prompting?

- **Concept**: Entity Recognition (NER)
  - **Why needed here**: Entity-level detection requires identifying specific entities in the text to focus the LLM's attention on detailed verification.
  - **Quick check question**: What types of entities (person, location, organization, etc.) would be most relevant for hallucination detection in a summarization task?

## Architecture Onboarding

- **Component map**: X → Dsent → (Dent if needed) → O → M → Yrefined
  - Source text X flows through sentence-level detection, then entity-level detection if needed, producing detection output O, which is then used by the mitigation agent to produce the refined response Yrefined.

- **Critical path**: The framework segments raw response, applies sentence-level detection using CoT prompting, performs entity-level detection on non-hallucinated sentences, and uses detection results for post-editing.

- **Design tradeoffs**: Hierarchical detection adds complexity but improves accuracy over single-level detection; using external NER service adds dependency but enables fine-grained entity verification; domain-agnostic prompts sacrifice some task-specific optimization for generalizability.

- **Failure signatures**: High false negative rate (detection misses hallucinations), poor text quality (mitigation produces incoherent text), high latency (hierarchical approach too slow), cost issues (LLM API calls expensive at scale).

- **First 3 experiments**: 1) Test sentence-level detection accuracy on a small subset with known hallucinations; 2) Evaluate entity-level detection performance on non-hallucinated sentences with entity errors; 3) Run end-to-end pipeline on a single document to verify full detection and mitigation flow works together.

## Open Questions the Paper Calls Out

### Open Question 1
How does CoNLI's performance vary across different domains or types of text-to-text generation tasks beyond summarization and question answering? The paper mentions "various contexts" but doesn't provide detailed domain-specific performance analysis. This remains unresolved because experiments primarily focus on summarization and question answering tasks. Experiments on a wider variety of text-to-text generation tasks across different domains would resolve this.

### Open Question 2
What is the impact of CoNLI on overall latency and cost of LLM-based applications, especially for real-time systems? The paper mentions cost-efficiency considerations but doesn't provide comprehensive latency or cost analysis. This remains unresolved because the paper doesn't discuss computational overhead introduced by detection and mitigation processes. Detailed analysis of latency and cost implications in production environments would resolve this.

### Open Question 3
How does CoNLI handle complex multi-hop reasoning scenarios where hallucinations involve indirect inferences rather than direct contradictions with source text? The paper mentions "complicated reasoning" as a factor leading to hallucinations but doesn't explore how the framework handles complex reasoning chains. This remains unresolved because the current approach focuses on direct contradictions and entity-level mismatches. Experiments with benchmark datasets containing multi-hop reasoning tasks would resolve this.

## Limitations

- The hierarchical approach's effectiveness depends critically on the LLM's ability to perform reliable natural language inference tasks.
- The framework assumes entity-level detection will consistently catch hallucinations missed by sentence-level detection without empirical evidence for this complementary relationship.
- The mitigation agent's ability to preserve text quality relies heavily on detection reasons being sufficiently clear and actionable, which may not hold for complex or subtle hallucinations.

## Confidence

**High Confidence (3 claims):**
- The hierarchical framework structure is technically feasible and follows established LLM prompting methodologies.
- Using detection results as explicit rewrite instructions is a valid approach demonstrated in other LLM applications.
- The framework can be implemented without requiring fine-tuning or domain-specific prompt engineering.

**Medium Confidence (2 claims):**
- The approach achieves state-of-the-art hallucination detection performance compared to existing methods.
- The framework successfully reduces hallucinations while preserving text quality across multiple benchmarks.

**Low Confidence (1 claim):**
- The domain-agnostic few-shot prompting approach will generalize effectively across all text-to-text generation tasks without any task-specific optimization.

## Next Checks

1. **Ablation Study**: Compare detection accuracy of hierarchical approach versus sentence-level detection alone on a held-out test set to quantify the actual benefit of entity-level detection.

2. **Prompt Template Analysis**: Test the sensitivity of detection performance to variations in prompt templates, including different NLI examples and CoT reasoning steps.

3. **Mitigation Quality Assessment**: Evaluate the quality of rewritten text produced by the mitigation agent using human evaluation on a sample of detected hallucinations, measuring both correction accuracy and preservation of original meaning.