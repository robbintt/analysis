---
ver: rpa2
title: 'Memorisation Cartography: Mapping out the Memorisation-Generalisation Continuum
  in Neural Machine Translation'
arxiv_id: '2311.05379'
source_url: https://arxiv.org/abs/2311.05379
tags:
- memorisation
- training
- examples
- target
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work examines the memorisation-generalisation continuum in\
  \ neural machine translation by building a large-scale resource of 5 million source-target\
  \ pairs across five language pairs, mapping each example to a coordinate based on\
  \ training and test performance using counterfactual memorisation. It identifies\
  \ surface-level features\u2014such as source-target overlap, frequency, length,\
  \ and token segmentation\u2014that predict an example's position on this continuum,\
  \ and demonstrates that these predictions generalise cross-lingually."
---

# Memorisation Cartography: Mapping out the Memorisation-Generalisation Continuum in Neural Machine Translation

## Quick Facts
- arXiv ID: 2311.05379
- Source URL: https://arxiv.org/abs/2311.05379
- Reference count: 40
- Primary result: Mapping 5M NMT examples on memorisation-generalisation continuum using counterfactual memorisation metric

## Executive Summary
This work examines the memorisation-generalisation continuum in neural machine translation by building a large-scale resource of 5 million source-target pairs across five language pairs. Each example is mapped to a coordinate based on training and test performance using counterfactual memorisation (CM), which captures examples easily memorised but hard to generalise. The analysis identifies surface-level features—such as source-target overlap, frequency, length, and token segmentation—that predict an example's position on this continuum and demonstrates that these predictions generalise cross-lingually. The study shows that examples with higher CM scores contribute more to model performance in terms of BLEU, target log-probability, and hallucination tendency, while also revealing that such examples can simultaneously enhance and undermine robustness.

## Method Summary
The study builds a large-scale resource of 5M NMT datapoints across five language pairs (EN-DE, EN-NL, EN-FR, EN-ES, EN-IT) by training 40 transformer-base models on 50% of training data and testing on held-out portions. Counterfactual memorisation (CM) is computed as the difference between training memorisation and generalisation scores for each example. Surface-level features are extracted and shallow MLPs are trained to predict CM values. The resource is used to map examples on the memorisation-generalisation continuum and to investigate how different subsets affect model performance through targeted training experiments.

## Key Results
- Counterfactual memorisation effectively identifies examples that contribute most to model performance
- Surface-level features (frequency, length, segmentation, source-target overlap) predict CM values and generalise cross-lingually
- Examples with high CM scores improve BLEU and log-probability but increase hallucination rates, creating a performance-robustness trade-off

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual memorisation (CM) captures examples that are easily memorised during training but hard to generalise from
- Mechanism: CM is computed as the difference between training memorisation (TM) and generalisation score (GS). Examples with high CM have high TM but low GS, meaning the model can produce them correctly when seen during training but fails to produce them correctly when unseen
- Core assumption: Training and test performance can be estimated by training models on subsets of data and evaluating on held-out portions
- Evidence anchors: [abstract] "We use the counterfactual memorisation metric to (1) build a resource that places 5M NMT datapoints on a memorisation-generalisation map"; [section 3] "CM can be computed as follows: CM(x, y)= pθtr(y|x) - pθtst(y|x)"

### Mechanism 2
- Claim: Surface-level features of source-target pairs predict their position on the memorisation-generalisation continuum
- Mechanism: Features like frequency, length, segmentation, and source-target overlap correlate with TM, GS, and CM values. These features can be used to approximate memorisation metrics without expensive computation
- Core assumption: Surface-level features are indicative of how difficult examples are to memorise and generalise
- Evidence anchors: [section 4.1] "Examples with low-frequency tokens can be learnt during training, but models are much less likely to assign a high probability to targets with low-frequency tokens during testing"; [section 4.2] Manual annotation confirms that word-for-word translations are easier to memorise than paraphrases

### Mechanism 3
- Claim: Examples with higher CM scores contribute more to model performance but also increase hallucination tendency
- Mechanism: Removing examples with high CM from training reduces BLEU and log-probability scores but also reduces hallucination rates. This creates a trade-off between performance and robustness
- Core assumption: Examples with high CM are both beneficial for performance and problematic for robustness
- Evidence anchors: [section 6.1] "Most relevant means that the BLEU score or log-probability decreases the most if you remove this group or that the hallucination tendency increases the most"; [section 6.2] "The hallucination results underscore that the relation between examples with high CM scores and model performance is not straightforward"

## Foundational Learning

- Concept: Counterfactual memorisation metric
  - Why needed here: CM is the core metric used to position examples on the memorisation-generalisation map
  - Quick check question: How is CM calculated and what does it measure?

- Concept: Surface-level features and their correlation with memorisation
  - Why needed here: Features like frequency, length, and source-target overlap are used to predict CM values
  - Quick check question: Which features correlate most strongly with CM and why?

- Concept: Trade-off between performance and robustness
  - Why needed here: Examples with high CM improve performance but increase hallucination risk
  - Quick check question: How does removing high-CM examples affect different performance metrics?

## Architecture Onboarding

- Component map: Data collection -> Model training for CM computation -> Feature extraction -> MLP training for CM approximation -> Performance evaluation through targeted training
- Critical path: Data collection → Model training for CM → Feature extraction → MLP training → Performance evaluation
- Design tradeoffs: Using BLEU vs. likelihood for CM computation; using expensive CM computation vs. feature-based approximation; balancing performance and robustness
- Failure signatures: Poor CM-approximation correlation; unexpected performance changes when removing high-CM examples; unstable CM values across different seeds
- First 3 experiments:
  1. Train models on 50% of data and evaluate on held-out portion to compute CM for all examples
  2. Extract surface-level features from source-target pairs and train MLP to predict CM values
  3. Remove examples with high CM from training and evaluate impact on BLEU, log-probability, and hallucination rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the CM score's predictive power for model performance generalize beyond the specific experimental setup used (e.g., Transformer-base, 100 epochs, specific dataset size)?
- Basis in paper: [explicit] The paper notes that findings are expected to extend beyond their setup, but the precise memorisation scores are specific to their experimental configuration, with larger systems likely to memorise more and longer training leading to increased memorisation
- Why unresolved: The paper only tests one model architecture (Transformer-base) and training regime, limiting generalizability to other architectures, training durations, or model scales
- What evidence would resolve it: Conducting the same analysis with different model architectures (e.g., Transformer-large, GPT-style models), varying training durations, and larger datasets to compare CM score distributions and predictive power across these configurations

### Open Question 2
- Question: How do spatial memorisation patterns in model parameters (e.g., parameter changes during training) relate to the output-level memorisation metrics like CM?
- Basis in paper: [explicit] The paper mentions memorisation happens over time and manifests in parameter space, but analysis is limited to observable output probabilities rather than parameter-level changes
- Why unresolved: The paper only examines memorisation through observable outputs (token probabilities) rather than investigating how memorisation manifests in the internal parameter space of the model
- What evidence would resolve it: Analyzing parameter changes during training (e.g., using techniques like influence functions or parameter tracking) to correlate spatial memorisation patterns with output-level CM scores

### Open Question 3
- Question: Does the CM score's relationship to model performance hold for lower-resource languages or languages with limited parallel data availability?
- Basis in paper: [explicit] The paper acknowledges using only high-resource languages and notes that including lower-resource languages was limited by the parallel data constraint, though preliminary experiments with Afrikaans showed similar patterns
- Why unresolved: The paper only uses high-resource languages (English paired with German, Dutch, French, Spanish, Italian) due to parallel data constraints, leaving uncertainty about whether CM patterns generalize to lower-resource settings
- What evidence would resolve it: Applying the CM analysis to lower-resource language pairs (e.g., English paired with Swahili, Hindi, or indigenous languages) and comparing the feature-CM relationships and performance impacts across these languages

## Limitations
- CM computation is resource-intensive, requiring training 40 transformer-base models per language pair
- Manual annotation covers only 250 EN-NL examples, limiting qualitative validation
- Findings may not generalize to languages with different typological features or script systems

## Confidence

**High Confidence:**
- Counterfactual memorisation effectively identifies examples that contribute most to model performance
- Surface-level features (frequency, length, segmentation, source-target overlap) correlate with CM values across all five language pairs
- Examples with high CM scores contribute more to BLEU and log-probability scores
- The memorisation-generalisation continuum exists and can be mapped using CM metrics

**Medium Confidence:**
- Cross-lingual generalizability of feature-CM correlations
- Performance-robustness trade-off being inherent to high-CM examples rather than dataset-specific
- Approximation methods using surface features can reliably replace expensive CM computation

**Low Confidence:**
- Exact thresholds for selecting examples based on CM to optimize specific deployment goals
- Generalizability to non-European languages and languages with different morphological systems
- Long-term stability of CM-based selection strategies across model architecture changes

## Next Checks

1. **Generalizability across languages**: Validate CM-feature correlations on non-European languages (e.g., Chinese, Arabic, Hindi) to test whether the observed patterns hold across typologically diverse language pairs, particularly focusing on how morphological complexity affects the memorisation-generalisation relationship.

2. **Robustness validation**: Conduct systematic ablation studies removing high-CM examples across multiple model architectures (not just transformer-base) and datasets to verify that the performance-robustness trade-off is consistent and not architecture-specific.

3. **Dynamic stability**: Evaluate how CM values and their correlations with features change as models are trained for longer periods or with different optimization strategies, to determine whether the memorisation-generalisation continuum is stable across different training regimes.