---
ver: rpa2
title: 'Why Linguistics Will Thrive in the 21st Century: A Reply to Piantadosi (2023)'
arxiv_id: '2308.03228'
source_url: https://arxiv.org/abs/2308.03228
tags:
- language
- llms
- learning
- data
- theory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper challenges the claim that large language models (LLMs)\
  \ like GPT-4 have refuted Chomsky\u2019s approach to language by refuting nativist\
  \ theories. The authors argue that LLMs are not unconstrained learners and require\
  \ massive amounts of data, unlike humans who achieve language competence with relatively\
  \ little exposure."
---

# Why Linguistics Will Thrive in the21st Century: A Reply to Piantadosi (2023)

## Quick Facts
- arXiv ID: 2308.03228
- Source URL: https://arxiv.org/abs/2308.03228
- Authors: 
- Reference count: 20
- Primary result: Generative linguistics remains essential for understanding human language, as LLMs cannot serve as scientific theories due to their predictive rather than explanatory nature.

## Executive Summary
This paper challenges the claim that large language models like GPT-4 have refuted Chomsky's approach to language by refuting nativist theories. The authors argue that LLMs are fundamentally different from human language learners, requiring orders of magnitude more data to achieve comparable performance. They emphasize that LLMs cannot constitute scientific theories of language because they are primarily predictive rather than explanatory, and their corporate nature makes them neither replicable nor reproducible. The paper concludes that generative linguistics will remain indispensable in the 21st century for explaining the complexities of human language.

## Method Summary
The authors employ a theoretical analysis using computational learning theory to compare LLMs with human language acquisition. They examine the data requirements, interpretability, and explanatory power of LLMs versus human learners, drawing on established linguistic theory and recent developments in machine learning. The analysis focuses on demonstrating that LLMs are not unconstrained learners and lack the interpretability necessary to serve as scientific theories of language.

## Key Results
- LLMs require significantly more data than humans to achieve comparable language performance, highlighting fundamental differences in learning mechanisms
- LLMs cannot constitute scientific theories of language because they lack interpretability and focus on prediction rather than explanation
- Multiple realizability demonstrates that similar performance does not imply identical underlying mechanisms between LLMs and human language learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs cannot constitute scientific theories of language because they lack interpretability and are primarily predictive rather than explanatory.
- Mechanism: Scientific theories must provide interpretable explanations for linguistic phenomena, not just predictions. LLMs, being proprietary black boxes focused on prediction, fail this criterion.
- Core assumption: The role of a scientific theory is to elucidate and explain, not merely predict.
- Evidence anchors:
  - [abstract] "LLMs cannot constitute scientific theories of language for several reasons, not least of which is that scientific theories must provide interpretable explanations, not just predictions."
  - [section 4.2] "While advances in these research methodologies is progressing rapidly, the field is stuck playing catch-up with ever-evolving and increasingly opaque and corporate models. There is no equivalent to BERTology for the latest crop of LLMs because we lack the necessary access to probe them in the way we could even a few years ago."
- Break Condition: If an LLM were to become fully interpretable and open-source, demonstrating clear explanatory mechanisms for linguistic phenomena, this argument would weaken.

### Mechanism 2
- Claim: LLMs require orders of magnitude more data than humans to achieve comparable performance, highlighting a fundamental difference in learning mechanisms.
- Mechanism: The stark contrast in data requirements between LLMs and humans suggests that LLMs and human language learners are solving fundamentally different learning problems, with humans leveraging innate constraints.
- Core assumption: The poverty of the stimulus argument holds, indicating that humans have innate linguistic constraints that LLMs lack.
- Evidence anchors:
  - [abstract] "humans achieve their capacity for language after exposure to several orders of magnitude less data. The fact that young children become competent, fluent speakers of their native languages with relatively little exposure to them is the central mystery of language learning to which Chomsky initially drew attention, and LLMs currently show little promise of solving this mystery."
  - [section 2.1] "The disconnect between the linguistic experience (input) and the linguistic capacity (output) is what gives rise to the The Poverty of the Stimulus argument for the hypothesis that many aspects of language learning and representation are innate (Chomsky, 1959, 1980; Nowak et al., 2001; Yang, 2013)."
- Break Condition: If LLMs were shown to achieve human-level performance with human-scale data, this argument would weaken.

### Mechanism 3
- Claim: Multiple realizability means that identical performance does not imply identical underlying mechanisms, challenging the assumption that LLMs are cognitive models of human language.
- Mechanism: Just as planes and birds both fly but use different mechanisms, LLMs and humans may achieve similar linguistic performance through fundamentally different processes.
- Core assumption: The existence of multiple realizability in cognitive science applies to the comparison between LLMs and human language learning.
- Evidence anchors:
  - [section 3.1] "The mere fact that distinct systems exhibit the same behavior does not mean that they employ the same internal mechanisms. Guest and Martin (2023), who apply this reasoning specifically to the question of ANNs as models of cognition, present an example of two clocks, which appear identical on the outside, but are different on the inside: clock A is digital, but clock B is analog."
  - [section 3.1] "Similarly, both planes and birds can propel themselves through the air. Should we conclude that birds are powered by jet fuel because we know how to build jets but not birds?"
- Break Condition: If it were proven that LLMs and human language learners use identical underlying mechanisms, this argument would weaken.

## Foundational Learning

- Concept: Poverty of the Stimulus
  - Why needed here: This concept is central to the argument that humans have innate linguistic constraints that LLMs lack, explaining the data efficiency gap.
  - Quick check question: How does the poverty of the stimulus argument challenge the idea that LLMs can serve as models for human language learning?

- Concept: Multiple Realizability
  - Why needed here: Understanding multiple realizability is crucial for recognizing that similar outputs do not imply similar underlying mechanisms, a key point in the critique of LLMs as cognitive models.
  - Quick check question: Can you provide an example of multiple realizability outside of language learning that illustrates why similar performance does not imply identical mechanisms?

- Concept: Interpretability in Machine Learning
  - Why needed here: The lack of interpretability in LLMs is a major argument against their status as scientific theories of language, as theories must provide clear explanations.
  - Quick check question: What are some methods used to interpret the internal workings of neural networks, and why are they insufficient for understanding LLMs?

## Architecture Onboarding

- Component Map:
  Data Pipeline -> Model Architecture -> Training Process -> Evaluation

- Critical Path:
  1. Data preprocessing and tokenization
  2. Model architecture definition and initialization
  3. Training loop with gradient updates
  4. Evaluation on linguistic tasks
  5. Analysis of results and comparison with human performance

- Design Tradeoffs:
  - Model size vs. interpretability: Larger models perform better but are harder to interpret
  - Data quantity vs. data quality: More data can improve performance but may introduce noise
  - Computational resources vs. training time: More resources can speed up training but increase costs

- Failure Signatures:
  - Overfitting: Model performs well on training data but poorly on new data
  - Underfitting: Model performs poorly on both training and new data
  - Data shortcuts: Model exploits unintended patterns in the data rather than learning the intended linguistic phenomena

- First 3 Experiments:
  1. Compare the performance of an LLM on a linguistic task with that of a human with limited data exposure to highlight the data efficiency gap
  2. Design a test to check for the presence of linguistic shortcuts in an LLM's performance on a grammar task
  3. Attempt to interpret the internal representations of an LLM on a specific linguistic phenomenon to assess its explanatory power

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific "non-trivial structural priors" that LLMs encode which facilitate language acquisition, and how do these compare to the innate linguistic structures proposed by Chomsky's Universal Grammar?
- Basis in paper: [explicit] The paper argues that LLMs are not truly unconstrained learners and must have some form of bias or prior knowledge to function, similar to the innate structures in human language acquisition.
- Why unresolved: The paper does not provide a detailed comparison of the specific priors in LLMs versus those in Universal Grammar, leaving open the question of whether these priors are fundamentally different or similar in nature.
- What evidence would resolve it: Detailed analysis of LLM architecture and training data, alongside empirical studies comparing LLM performance on language tasks to human performance, could provide insights into the nature of these priors.

### Open Question 2
- Question: Can LLMs be designed to replicate the developmental regression or U-shaped learning trajectory observed in human language acquisition, particularly in the context of overregularization and over-irregularization errors?
- Basis in paper: [inferred] The paper discusses the persistent failure of ANNs to replicate human-like error patterns, such as overregularization, despite advances in model architecture and training data.
- Why unresolved: The paper does not explore potential solutions or new approaches that could address this limitation in LLMs, leaving open the question of whether such a design is feasible.
- What evidence would resolve it: Development of new ANN architectures or training methodologies that successfully replicate human-like error patterns in language acquisition tasks would provide evidence for or against the feasibility of this approach.

### Open Question 3
- Question: How can the interpretability and transparency of LLMs be improved to better understand their internal mechanisms and validate their role as scientific theories of language?
- Basis in paper: [explicit] The paper argues that LLMs are currently uninterpretable and unaccessible, hindering their potential as scientific theories of language.
- Why unresolved: The paper does not propose specific methods or strategies for improving LLM interpretability, leaving open the question of how this challenge can be addressed.
- What evidence would resolve it: Development and implementation of novel interpretability techniques, such as advanced probing methods or visualization tools, that provide clear insights into LLM internal representations and decision-making processes would be evidence for improved interpretability.

## Limitations
- The proprietary nature of current LLMs prevents direct examination of their internal representations
- The rapid evolution of LLM capabilities means that some current limitations may be overcome in future models
- The paper relies heavily on theoretical arguments from computational learning theory, but practical implications for real-world LLMs are not always clearly established

## Confidence
- High: Core arguments that LLMs are not unconstrained learners and cannot serve as scientific theories of language
- Medium: Claims about the specific mechanisms by which LLMs differ from human learners
- Low: Predictions about the future capabilities of LLMs and their potential to bridge the gap with human language acquisition

## Next Checks
1. Conduct a systematic review of recent LLM interpretability research to assess whether current models are becoming more transparent and explainable
2. Design experiments comparing the data efficiency of LLMs versus humans on controlled linguistic tasks with carefully controlled input data
3. Investigate whether existing open-source LLMs or their intermediate checkpoints show different patterns of learning that might bridge the gap between current models and human language acquisition