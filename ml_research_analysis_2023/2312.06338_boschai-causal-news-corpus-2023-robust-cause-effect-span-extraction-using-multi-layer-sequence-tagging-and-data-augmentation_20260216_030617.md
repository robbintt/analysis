---
ver: rpa2
title: 'BoschAI @ Causal News Corpus 2023: Robust Cause-Effect Span Extraction using
  Multi-Layer Sequence Tagging and Data Augmentation'
arxiv_id: '2312.06338'
source_url: https://arxiv.org/abs/2312.06338
tags:
- causal
- subtask
- data
- augmentation
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of detecting and extracting causal
  relationships in text, specifically focusing on the Event Causality Identification
  with Causal News Corpus shared task. The authors propose a system that combines
  pre-trained transformers, stacked sequence tagging, and synthetic data augmentation
  to identify cause-effect relationships and extract corresponding spans in news articles.
---

# BoschAI @ Causal News Corpus 2023: Robust Cause-Effect Span Extraction using Multi-Layer Sequence Tagging and Data Augmentation

## Quick Facts
- arXiv ID: 2312.06338
- Source URL: https://arxiv.org/abs/2312.06338
- Reference count: 6
- Primary result: Third place in Subtask 1, first place in Subtask 2 with 72.8 F1 (winning by 13 percentage points)

## Executive Summary
This paper presents a system for detecting and extracting causal relationships in news text, competing in the Event Causality Identification with Causal News Corpus shared task. The authors combine pre-trained transformers with stacked sequence tagging and synthetic data augmentation to identify cause-effect relationships and extract corresponding spans. Their approach uses RoBERTa-Large with a novel stacked BILOU labeling scheme that can handle multiple causal chains per sentence, achieving state-of-the-art results on the span extraction subtask.

## Method Summary
The system employs pre-trained transformer encoders (BERT-Large or RoBERTa-Large) with stacked BILOU sequence tagging for span extraction. For Subtask 2, they concatenate BILOU tags with pipe operators to create layered labels, allowing up to three causal relations per sentence. The model uses a CRF layer for consistent tag sequences and incorporates synthetic data augmentation through EDA techniques (synonym replacement, word insertion, swaps) and ChatGPT-generated samples. Subtask 1 uses weighted cross-entropy loss for binary classification.

## Key Results
- Third place in Subtask 1 binary classification of causal relationships
- First place in Subtask 2 with 72.8 F1 score, winning by 13 percentage points over second-best system
- RoBERTa-Large outperforms BERT-Large by 4.8 F1 points for span extraction
- Synthetic data augmentation consistently improves performance across both subtasks

## Why This Works (Mechanism)

### Mechanism 1
Stacking BILOU labels enables detection of multiple causal chains in a single sentence. The system concatenates BILOU tags with pipe operators, creating layered labels that are evaluated separately. This handles up to three causal relations per sentence, though the corpus has at most 4 relations per sentence. The core assumption is that causal spans don't overlap in ways that break the BILOU scheme across layers.

### Mechanism 2
Synthetic data augmentation consistently improves causal span extraction performance. EDA techniques and ChatGPT-generated samples increase training data size and diversity, reducing overfitting on limited annotations. The core assumption is that augmented sentences preserve causal structure while providing meaningful variation.

### Mechanism 3
RoBERTa-Large yields better span extraction accuracy than BERT-Large due to improved pretraining and training process. The differences in pretraining objectives and data provide better contextualized embeddings for causal span tagging. The core assumption is that pretraining differences lead to more discriminative token representations for this task.

## Foundational Learning

- **Concept: BILOU labeling scheme**
  - Why needed here: Precisely marks beginning, inside, last, outside, and unit tokens for multi-word spans, critical for exact span extraction
  - Quick check question: Given "global warming causes climate change", what would the BILOU tags be for "global warming" and "climate change"?

- **Concept: Conditional Random Fields (CRF)**
  - Why needed here: Enforces consistent tag sequences and improves span boundary detection over independent token classification
  - Quick check question: Why might a CRF layer improve F1 scores compared to using only a linear output layer?

- **Concept: Data augmentation for sequence labeling**
  - Why needed here: Limited training data makes the model prone to overfitting; augmentation expands the effective training set while preserving causal structures
  - Quick check question: What is a potential risk of using EDA synonym replacement for causal relation extraction?

## Architecture Onboarding

- **Component map**: Tokenization → Embedding → Stacked BILOU tagging → CRF decoding → Span extraction
- **Critical path**: Tokenization → Embedding → Stacked BILOU tagging → CRF decoding → Span extraction
- **Design tradeoffs**:
  - Stacked labels vs multiple independent models: Stacking reduces model count but increases label space complexity
  - EDA augmentation vs ChatGPT augmentation: EDA is simpler and faster but may introduce grammatical noise; ChatGPT generates cleaner but structurally simpler examples
  - BERT vs RoBERTa: RoBERTa gives slight accuracy gains but requires more compute
- **Failure signatures**:
  - Incorrect spans: CRF may enforce valid tag sequences but not semantic correctness
  - Missing causal chains: Stacked labels capped at 3 layers may miss sentences with >3 relations
  - Label imbalance: Synthetic data may overrepresent certain span patterns
- **First 3 experiments**:
  1. Run baseline with BERT-Large, no augmentation, single BILOU layer; measure F1 on dev set
  2. Add stacked labels (up to 3 layers) with BERT; compare F1 and recall on multi-relation instances
  3. Swap to RoBERTa-Large and apply EDA augmentation; evaluate impact on overall and per-class F1

## Open Questions the Paper Calls Out

**Open Question 1**: How does the performance of multi-layer sequence tagging with stacked labels compare to other approaches for detecting multiple causal chains in a sentence?
- Basis in paper: The paper mentions that their approach of using stacked BILOU labels to handle multiple causal chains per sentence significantly outperforms the baseline system.
- Why unresolved: The paper does not provide a direct comparison between the stacked labels approach and other potential methods for handling multiple causal chains.
- What evidence would resolve it: Experimental results comparing the performance of stacked labels to alternative approaches like using separate models for each causal chain or employing a different labeling scheme.

**Open Question 2**: How does the choice of data augmentation method (EDA vs ChatGPT) impact the model's ability to generalize to different types of causal relationships?
- Basis in paper: The paper experiments with both EDA and ChatGPT for data augmentation and observes performance improvements with both methods, but does not directly compare their impact on generalization.
- Why unresolved: The paper does not analyze the specific strengths and weaknesses of each data augmentation method in terms of generalization to diverse causal relationships.
- What evidence would resolve it: A detailed analysis of the model's performance on different types of causal relationships (e.g., cause-effect, effect-cause, complex multi-relation cases) when trained with different data augmentation methods.

**Open Question 3**: What are the limitations of using synthetic data augmentation for improving the detection of causal relationships in text?
- Basis in paper: The paper acknowledges that data augmentation is used due to limited training data and observes performance improvements, but does not discuss potential limitations or drawbacks of this approach.
- Why unresolved: The paper does not explore potential issues such as overfitting to synthetic data, introduction of noise, or the inability to capture all nuances of real-world causal relationships.
- What evidence would resolve it: A thorough analysis of the model's performance on real-world data after being trained on synthetic data, including an examination of potential biases or limitations introduced by the augmentation process.

## Limitations

- The stacked BILOU labeling scheme's effectiveness on complex overlapping causal spans lacks empirical validation
- Synthetic data augmentation methods lack quantitative validation for preserving semantic validity of causal relationships
- The comparison between BERT and RoBERTa lacks ablation studies to isolate the impact of pretraining differences

## Confidence

**High Confidence**: The system achieves strong performance on the shared task (third place in Subtask 1, first place in Subtask 2 with 72.8 F1)

**Medium Confidence**: Stacked BILOU labels effectively handle multiple causal chains per sentence, though empirical validation on corpus-level span overlap patterns is lacking

**Low Confidence**: RoBERTa-Large outperforms BERT-Large specifically due to pretraining differences for this task, as the comparison lacks controlled experiments

## Next Checks

1. **Span Overlap Analysis**: Conduct a corpus-level analysis of all sentences with multiple causal relations to quantify how often causal spans overlap or interact in ways that could break the stacked BILOU scheme.

2. **Augmented Data Quality Audit**: Randomly sample 100 augmented examples (50 EDA, 50 ChatGPT) and manually evaluate them for grammatical correctness, preservation of causal structure, and annotation validity.

3. **Pretraining Ablation Study**: Implement a controlled experiment comparing BERT and RoBERTa with identical fine-tuning procedures, training data, and hyperparameters to isolate the impact of pretraining differences.