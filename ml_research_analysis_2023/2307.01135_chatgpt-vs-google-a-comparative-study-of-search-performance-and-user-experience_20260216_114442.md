---
ver: rpa2
title: 'ChatGPT vs. Google: A Comparative Study of Search Performance and User Experience'
arxiv_id: '2307.01135'
source_url: https://arxiv.org/abs/2307.01135
tags:
- search
- chatgpt
- google
- participants
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study investigates the impact of ChatGPT on user search performance
  and experience compared to traditional search engines like Google. A randomized
  online experiment was conducted with 95 participants, divided into two groups: one
  using ChatGPT and the other using Google Search.'
---

# ChatGPT vs. Google: A Comparative Study of Search Performance and User Experience

## Quick Facts
- arXiv ID: 2307.01135
- Source URL: https://arxiv.org/abs/2307.01135
- Reference count: 0
- Primary result: ChatGPT users spent significantly less time on search tasks compared to Google users, while perceived information quality and user experience were higher for ChatGPT.

## Executive Summary
This study compares user search performance and experience between ChatGPT and Google Search through a randomized online experiment with 95 participants. Participants completed three information search tasks using either ChatGPT or Google Search. Results show ChatGPT users completed tasks faster but with similar overall performance accuracy. Users perceived ChatGPT's responses as higher quality and reported better user experience, despite similar trust levels in both tools. ChatGPT performed well on straightforward questions but struggled with fact-checking tasks.

## Method Summary
The study employed a between-subjects randomized online experiment with 95 participants divided into two groups (48 using ChatGPT, 47 using Google Search). Participants completed three search tasks: answering a simple question, finding a list of flight booking links, and fact-checking a statement. Search interfaces were implemented using OpenAI ChatCompletion API for ChatGPT and Google Custom Search API for Google. Server logs captured queries, timestamps, and responses. Post-task questionnaires measured perceived information quality, trust, ease of use, usefulness, enjoyment, and satisfaction. Task answers were manually scored against standard answers.

## Key Results
- ChatGPT users spent significantly less time on all tasks compared to Google Search users
- Overall task performance was similar between ChatGPT and Google groups
- ChatGPT excelled in answering straightforward questions but performed poorly in fact-checking tasks
- Users perceived ChatGPT's responses as having higher information quality than Google Search
- ChatGPT users reported significantly better user experiences in usefulness, enjoyment, and satisfaction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT reduces search time by eliminating trial-and-error keyword refinement
- Mechanism: Users issue a single natural language query and receive a summarized response, bypassing the iterative search→refine→search loop required in Google
- Core assumption: Summarized responses from ChatGPT contain the information needed to answer the task without further browsing
- Evidence anchors:
  - [abstract] "ChatGPT users spent significantly less time on all tasks compared to Google Search users"
  - [section] "When using Google Search, users must formulate search queries on their own, often going through a trial-and-error process..."
  - [corpus] Weak—no corpus neighbor directly supports the elimination-of-iteration claim
- Break condition: If the query is ambiguous or the answer is incomplete, users must revert to additional searches, eroding time advantage

### Mechanism 2
- Claim: ChatGPT levels performance across education levels by providing direct answers
- Mechanism: Lower-education users avoid the skill barrier of constructing effective keyword queries; ChatGPT's conversational interface supplies ready answers
- Core assumption: Search effectiveness with Google is positively correlated with query formulation skill, which scales with education
- Evidence anchors:
  - [abstract] "ChatGPT levels user search performance across different education levels"
  - [section] "ChatGPT users tend to formulate significantly longer queries...ChatGPT allows users to interact in a more conversational manner"
  - [corpus] Weak—no corpus neighbor provides direct evidence of education-level performance leveling
- Break condition: If the task requires multi-step reasoning or verification, the leveling effect disappears and performance depends on user expertise

### Mechanism 3
- Claim: ChatGPT's conversational style yields higher perceived information quality
- Mechanism: Organized, complete-sentence responses are easier to read and feel authoritative, increasing perceived quality even if factual accuracy is lower
- Core assumption: Perceived quality is based on presentation clarity rather than objective accuracy
- Evidence anchors:
  - [abstract] "Users perceived ChatGPT's responses as having higher information quality than Google Search..."
  - [section] "ChatGPT delivers organized responses in complete sentences to users' queries, potentially making the information more accessible"
  - [corpus] Weak—no corpus neighbor validates perceived quality vs actual accuracy trade-off
- Break condition: If users discover factual errors, perceived quality collapses and trust erodes

## Foundational Learning

- Concept: Between-subjects experimental design
  - Why needed here: Ensures observed differences are due to tool, not participant traits
  - Quick check question: What would happen to validity if the same participants used both tools in sequence?

- Concept: Randomization checks
  - Why needed here: Confirms groups are comparable on demographics and prior experience before attributing performance differences to the tool
  - Quick check question: Which demographic variable in Table 1 shows a statistically significant difference between groups?

- Concept: Scoring rubric design
  - Why needed here: Provides objective, comparable performance metrics across tasks with different answer formats
  - Quick check question: How many points are awarded for a fully correct answer in Task 1?

## Architecture Onboarding

- Component map: User login → Task interface (ChatGPT or Google) → API call → Response display → Answer submission → Questionnaire → Data logging
- Critical path: Login → Task 1 → Task 2 → Task 3 → Questionnaire (server logs capture all queries and responses)
- Design tradeoffs: Server-log time underestimates actual time (excludes answer refinement and questionnaire), but offers objectivity vs self-report bias
- Failure signatures: Large time gaps in logs may indicate external tool use; mismatched answer quality vs expected tool capability suggests cheating
- First 3 experiments:
  1. Replicate with a fact-heavy task to test ChatGPT's accuracy vs perceived quality
  2. Test mixed interface (chat + keyword) to isolate whether conversation or summary drives performance
  3. Vary task complexity to map education-level performance curves for both tools

## Open Questions the Paper Calls Out
The study does not explicitly call out open questions but implies several areas for future research including the integration of chat and search functionalities, the balance between conversational and keyword-based approaches, and the long-term effects of ChatGPT-like technologies on search behavior.

## Limitations
- ChatGPT's time advantage assumes users won't need follow-up searches for complex or ambiguous queries
- The leveling effect across education levels lacks direct experimental evidence showing performance gaps narrowing
- Perceived information quality vs. actual accuracy is based on user self-report without independent fact-checking

## Confidence
- **High**: Search time reduction for ChatGPT users is clearly supported by server log data
- **Medium**: Perceived quality and user experience improvements are consistent across questionnaire items
- **Low**: Claims about education-level performance leveling and accuracy vs. perceived quality trade-offs lack direct empirical backing

## Next Checks
1. Conduct a follow-up study where independent raters evaluate the factual accuracy of ChatGPT vs. Google responses for the same tasks, to compare perceived vs. actual quality
2. Design a within-subjects replication where the same participants use both tools on different tasks, to isolate tool effects from individual differences
3. Introduce a mixed interface condition (chat + keyword search) to test whether time savings come from the conversational interface itself or from summarized answers