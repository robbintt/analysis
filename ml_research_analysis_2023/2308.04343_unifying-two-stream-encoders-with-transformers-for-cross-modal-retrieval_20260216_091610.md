---
ver: rpa2
title: Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval
arxiv_id: '2308.04343'
source_url: https://arxiv.org/abs/2308.04343
tags:
- transformer
- image
- retrieval
- text
- cross-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of cross-modal retrieval, specifically
  image-to-text and text-to-image retrieval. The authors propose a new framework called
  Hierarchical Alignment Transformers (HAT) that unifies the encoder architectures
  for images and texts using Transformers.
---

# Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval

## Quick Facts
- **arXiv ID**: 2308.04343
- **Source URL**: https://arxiv.org/abs/2308.04343
- **Reference count**: 40
- **Primary result**: HAT achieves 7.6% and 16.7% relative improvement in Recall@1 on MSCOCO over state-of-the-art baselines

## Executive Summary
This paper addresses the problem of cross-modal retrieval by proposing Hierarchical Alignment Transformers (HAT), a framework that unifies image and text encoders using Transformers. The key innovation is using identical Transformer architectures for both modalities, which enables more compatible feature representations for alignment. The model employs a hierarchical alignment scheme that captures multi-level semantic correspondences between images and texts, leading to significant performance improvements in both image-to-text and text-to-image retrieval tasks on standard benchmarks.

## Method Summary
HAT uses a two-stream architecture with Swin Transformer for images and BERT for text, unified through identical Transformer-based architectures. The model extracts features from multiple layers (layers 4, 10, 12 for BERT; stages 2, 3, 4 for Swin) and employs a hierarchical alignment module that computes cross-attention between image regions and text words at each level. The final similarity score is computed by summing the hierarchical similarities using a triplet loss with margin 0.2. The training procedure involves an initial 10-epoch phase with frozen encoders followed by full fine-tuning.

## Key Results
- Achieves 7.6% and 16.7% relative improvement in Recall@1 on MSCOCO test set over state-of-the-art baselines
- Demonstrates 4.4% and 11.6% relative improvement in Recall@1 on Flickr30K
- Shows consistent improvements across multiple metrics (R@1, R@5, R@10) for both retrieval directions
- Ablation studies confirm the effectiveness of both the unified Transformer architecture and hierarchical alignment scheme

## Why This Works (Mechanism)

### Mechanism 1: Unified Transformer Architecture
- Claim: Unifying encoder architectures reduces cross-modal semantic mismatch
- Mechanism: Using Transformers for both images and texts creates representations with similar feature distribution properties, enabling more direct alignment
- Core assumption: Transformers capture comparable hierarchical semantics for both modalities
- Evidence anchors: [abstract] identical architectures produce representations with similar characteristics; [section 3.2] employs Transformer-based models for both modalities
- Break condition: If Transformers for images don't capture comparable semantics to those for texts

### Mechanism 2: Hierarchical Alignment
- Claim: Hierarchical alignment captures richer semantic correspondences than single-level matching
- Mechanism: Aligning features from multiple layers (low-, middle-, high-level) matches both fine-grained local details and broader contextual semantics
- Core assumption: Different Transformer layers encode complementary semantic information
- Evidence anchors: [abstract] hierarchical alignment scheme explores multi-level correspondences; [section 3.3] simultaneous computation of similarity at multiple levels
- Break condition: If multi-level features are redundant or deeper layers don't provide meaningful semantic signals

### Mechanism 3: Cross-Attention for Fine-Grained Matching
- Claim: Cross-attention with stacked attention layers improves fine-grained matching over holistic embedding comparison
- Mechanism: Stacked cross-attention computes region-word similarity matrices and aggregates weighted word representations per region
- Core assumption: Fine-grained correspondences between visual regions and text words carry more discriminative power than aggregated global embeddings
- Evidence anchors: [section 3.3] implements image-text and text-image stacked cross attention; [section 4.6] visualizes attention maps for specific words
- Break condition: If region-word correspondences are noisy or attention computation is too expensive

## Foundational Learning

- **Concept**: Transformer self-attention and cross-attention mechanics
  - Why needed: The entire model is built on Transformers, so understanding attention mechanics is essential
  - Quick check question: What is the difference between self-attention and cross-attention, and why is cross-attention used for aligning image regions with text words?

- **Concept**: Vision Transformer patch embedding and hierarchical feature maps
  - Why needed: The image encoder uses Swin Transformer, requiring knowledge of patch partitioning and hierarchical stages
  - Quick check question: How does the Swin Transformer's shifted windowing scheme enable efficient hierarchical feature extraction?

- **Concept**: Pre-training objectives (masked language modeling, contrastive loss)
  - Why needed: BERT and Swin Transformer are used as frozen backbones initially; understanding their pre-training objectives helps in fine-tuning decisions
  - Quick check question: How does BERT's masked language modeling objective influence the representations used for cross-modal alignment?

## Architecture Onboarding

- **Component map**: Image → Swin Transformer (4 stages) → Features from stages 2,3,4 → Cross-attention → Hierarchical alignment → Cosine similarity
  Text → BERT-base (12 layers) → Features from layers 4,10,12 → Cross-attention → Hierarchical alignment → Cosine similarity
- **Critical path**: Input image → patch partition → Swin Transformer → multi-stage feature maps; Input text → BERT tokenization → positional embeddings → BERT → layer outputs; For each level, cross-attention → similarity matrix → weighted aggregation → cosine similarity; Sum hierarchical similarities → compute triplet loss; Backpropagate and update alignment module parameters
- **Design tradeoffs**: Using powerful pre-trained backbones vs. training from scratch; Hierarchical alignment (3 levels) vs. single-level; Freezing backbones for 10 epochs vs. end-to-end training
- **Failure signatures**: Very low R@1 scores (alignment module issues); High variance across runs (attention or learning rate instability); Disproportionate improvement in one direction (feature normalization mismatch)
- **First 3 experiments**: 1) Run HAT(i-t) and HAT(t-i) separately on MSCOCO 5K to confirm both directions work; 2) Remove hierarchical alignment (use only last layer) to quantify performance drop; 3) Replace Swin with ResNet to observe impact of unified Transformer architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HAT performance compare to single-stream Transformer-based approaches like UniT or METER?
- Basis: The paper mentions single-stream methods as computationally expensive but doesn't provide direct comparison
- Why unresolved: Only compares HAT to two-stream methods, no single-stream Transformer baselines included
- What evidence would resolve it: Direct experimental comparison of HAT against single-stream Transformer approaches on same datasets

### Open Question 2
- Question: What is the optimal number of Transformer layers to use in the hierarchical alignment module?
- Basis: The paper uses layers 4, 10, and 12 but doesn't explore other layer combinations
- Why unresolved: No ablation studies on different layer combinations for hierarchical alignment
- What evidence would resolve it: Systematic evaluation of different layer combinations on retrieval performance

### Open Question 3
- Question: How does HAT perform on cross-modal retrieval tasks with more diverse image-text pairs from different domains?
- Basis: Experiments limited to MSCOCO and Flickr30K, which may not represent full diversity of real-world pairs
- Why unresolved: Doesn't test HAT on datasets with more varied content or longer text descriptions
- What evidence would resolve it: Testing HAT on datasets like Conceptual Captions or those with longer narrative text

## Limitations

- Implementation details remain underspecified, particularly cross-attention mechanism and temperature parameter settings
- Performance improvements benchmarked primarily on established datasets that may not generalize to more diverse scenarios
- Computational overhead of using Transformers for both modalities and hierarchical alignment module not thoroughly discussed
- No comparison with single-stream Transformer approaches to validate the two-stream architecture choice

## Confidence

- **High confidence**: The core architectural innovation of using identical Transformer-based encoders is well-supported by theoretical motivation and experimental results
- **Medium confidence**: Hierarchical alignment scheme's effectiveness is demonstrated empirically, but exact mechanism of semantic contribution remains limited
- **Low confidence**: Cross-attention with stacked attention layers providing superior fine-grained matching lacks direct ablation evidence against simpler methods

## Next Checks

1. **Ablation of Hierarchical Alignment**: Remove hierarchical alignment module and use only final layer representations for cross-modal matching. Compare R@1 scores on MSCOCO 1K test set to quantify exact contribution of multi-level matching.

2. **Cross-Attention Mechanism Validation**: Replace stacked cross-attention with simpler mean-pooled embedding comparison. Measure performance difference to isolate impact of fine-grained region-word matching versus holistic embedding comparison.

3. **Architecture Unification Impact**: Replace Swin Transformer image encoder with traditional CNN (e.g., ResNet-50) while keeping BERT text encoder. Evaluate whether performance drop validates claim that unified Transformer architectures are essential.