---
ver: rpa2
title: Solving Label Variation in Scientific Information Extraction via Multi-Task
  Learning
arxiv_id: '2312.15751'
source_url: https://arxiv.org/abs/2312.15751
tags:
- label
- entity
- labels
- scierc
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of label variation in scientific
  information extraction (ScientificIE) tasks, where overlapping datasets like SemEval-2018
  and SciERC have inconsistent annotations. The authors propose a multi-task learning
  approach based on the SpERT architecture to handle these variations.
---

# Solving Label Variation in Scientific Information Extraction via Multi-Task Learning

## Quick Facts
- arXiv ID: 2312.15751
- Source URL: https://arxiv.org/abs/2312.15751
- Reference count: 16
- Key outcome: Multi-task learning with soft labeling improves ScientificIE performance on overlapping datasets by handling inconsistent annotations

## Executive Summary
This paper addresses the challenge of label variation in scientific information extraction by proposing a multi-task learning approach that handles inconsistent annotations across overlapping datasets. The authors introduce soft labeling to convert inconsistent labels into probabilistic distributions and use Kullback-Leibler divergence to align model predictions with these soft labels. Their approach, built on the SpERT architecture, demonstrates improved performance on both entity recognition and relation extraction tasks, particularly in handling label noise and reducing data size requirements.

## Method Summary
The proposed method combines multi-task learning with soft labeling to address label variation in scientific information extraction. The approach uses the SpERT architecture with SciBERT encoder, extended to include two output heads for each task (NER and RE) representing different perspectives from the overlapping datasets. Soft labels are generated based on multi-level agreements between annotations and represented as probabilistic distributions. The model is trained using Kullback-Leibler divergence loss to align predictions with these soft labels, evaluated on both overlapping and non-overlapping data from SemEval-2018 and SciERC datasets.

## Key Results
- Multi-task learning with soft labeling achieves higher F1-scores compared to traditional approaches
- The method shows improved robustness to label noise and inconsistent annotations
- Model demonstrates reduced data size requirements while maintaining performance
- Successfully handles both named entity recognition and relation extraction tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task learning allows the model to capture multiple perspectives from inconsistent annotations
- Mechanism: Two output heads in SpERT architecture represent different perspectives from SemEval-2018 and SciERC datasets, with combined loss function learning from both simultaneously
- Core assumption: Inconsistencies can be modeled as multiple perspectives rather than errors
- Evidence anchors: Abstract states multi-task learning addresses label variations; section describes two output heads for each perspective

### Mechanism 2
- Claim: Soft labeling converts inconsistent labels into probabilistic distributions for learning uncertainty
- Mechanism: Kullback-Leibler divergence measures difference between predicted distribution and manually computed soft label distribution based on agreement levels
- Core assumption: Degree of agreement between annotations can be quantified as probabilistic distribution
- Evidence anchors: Abstract mentions soft labeling technique converting labels to probabilistic distributions; section describes using soft labels as probability distributions

### Mechanism 3
- Claim: Label variation richness reduces data size requirements
- Mechanism: Incorporating multiple perspectives and soft labels allows learning from fewer examples while maintaining performance
- Core assumption: Label variations contain information compensating for smaller dataset sizes
- Evidence anchors: Abstract states richness of information captured by label variations can reduce data size requirements; section describes comparison between gold and variation labels with decreasing data quantity

## Foundational Learning

- Concept: Kullback-Leibler divergence
  - Why needed here: Measures difference between predicted distribution and soft label distribution
  - Quick check question: What is the formula for Kullback-Leibler divergence and how does it differ from cross-entropy?

- Concept: Multi-task learning
  - Why needed here: Enables simultaneous learning from multiple perspectives handling label inconsistencies
  - Quick check question: How does multi-task learning differ from single-task learning in terms of loss function and model architecture?

- Concept: Probabilistic distributions
  - Why needed here: Soft labels represented as probabilistic distributions to capture annotation uncertainty
  - Quick check question: How do you convert a set of inconsistent labels into a probabilistic distribution?

## Architecture Onboarding

- Component map: Input sentence -> BERT encoder -> SpERT backbone -> Two output heads (NER and RE) -> Loss calculation -> Soft label alignment -> Final loss

- Critical path: Input sentence → BERT encoder → SpERT backbone → Two output heads → Loss calculation → Soft label alignment → Final loss

- Design tradeoffs: Increased model complexity from multiple output heads; potential overfitting from poorly constructed soft labels; improved robustness to label noise at increased training time cost

- Failure signatures: Poor performance on individual testing sets; overfitting to training data; inability to handle highly inconsistent annotations

- First 3 experiments:
  1. Train on overlapping data with single output head vs multi-task learning with two output heads
  2. Evaluate impact of soft labels by comparing models with and without soft labels
  3. Test model robustness to label noise by introducing synthetic noise into training data

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the methodology raises several important areas for future investigation.

## Limitations
- Soft label generation methodology lacks detailed specification and may introduce subjectivity
- Evaluation scope limited to specific overlapping datasets without thorough cross-domain testing
- Computational overhead and practical deployment costs not thoroughly discussed

## Confidence

**High Confidence Claims:**
- Multi-task learning framework with soft labels handles label variation between SemEval-2018 and SciERC
- Kullback-Leibler divergence effectively aligns model predictions with probabilistic soft labels
- Approach improves F1-scores on both overlapped and non-overlapped evaluation sets

**Medium Confidence Claims:**
- Soft labeling meaningfully captures annotation uncertainty
- Method reduces data size requirements while maintaining performance
- SpERT architecture suits multi-task learning approach

**Low Confidence Claims:**
- Approach generalizes to other scientific information extraction tasks
- Computational overhead acceptable for practical applications
- Soft label generation methodology is optimal or robust

## Next Checks

1. **Reproducibility Test**: Implement soft label generation process independently using described "multi-level agreement" approach, then compare generated soft labels with those used in paper's experiments

2. **Cross-Dataset Generalization**: Evaluate trained model on SciREX and other scientific text corpora to assess performance on truly independent datasets beyond overlapping abstracts

3. **Ablation Study**: Conduct controlled experiments removing multi-task learning component (single output head) and soft labeling (hard labels) to quantify individual contributions to performance improvements