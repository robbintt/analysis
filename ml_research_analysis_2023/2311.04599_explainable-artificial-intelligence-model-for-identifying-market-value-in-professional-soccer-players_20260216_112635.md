---
ver: rpa2
title: Explainable artificial intelligence model for identifying Market Value in Professional
  Soccer Players
arxiv_id: '2311.04599'
source_url: https://arxiv.org/abs/2311.04599
tags:
- market
- player
- value
- data
- values
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an advanced machine learning method for predicting
  soccer players' market values, combining ensemble models and the Shapley Additive
  Explanations (SHAP) for interpretability. Utilizing data from about 12,000 players
  from Sofifa, the Boruta algorithm streamlined feature selection.
---

# Explainable artificial intelligence model for identifying Market Value in Professional Soccer Players

## Quick Facts
- arXiv ID: 2311.04599
- Source URL: https://arxiv.org/abs/2311.04599
- Reference count: 12
- Primary result: Gradient Boosting Decision Tree model achieved R-squared of 0.901 and RMSE of 3,221,632.175 in predicting soccer player market values

## Executive Summary
This study presents an advanced machine learning approach for predicting soccer players' market values using ensemble models combined with Shapley Additive Explanations (SHAP) for interpretability. The research utilizes data from approximately 12,000 players from Sofifa, with the Boruta algorithm employed for feature selection. The Gradient Boosting Decision Tree (GBDT) model demonstrated superior predictive accuracy, while SHAP analysis revealed that player attributes in skills, fitness, and cognitive areas significantly influence market value. These findings provide valuable insights for sports industry stakeholders in player valuation and transfer decisions.

## Method Summary
The study employs an ensemble machine learning approach combining multiple gradient boosting models (GBDT, LightGBM, XGBoost) with the Boruta algorithm for feature selection and SHAP for interpretability. Data was collected from Sofifa, preprocessed through handling missing values and applying Box-Cox transformation, then reduced from 29 to 22 relevant features using Boruta. The GBDT model was trained with optimized hyperparameters using 5-fold cross-validation and Grid Search, evaluated using R-squared and RMSE metrics, and interpreted through SHAP global and local explanations.

## Key Results
- GBDT model achieved R-squared of 0.901 and RMSE of 3,221,632.175
- Feature selection reduced 29 input features to 22 most relevant ones using Boruta algorithm
- Player attributes in skills, fitness, and cognitive areas were identified as significant market value influencers
- SHAP analysis provided interpretable explanations for individual player valuations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ensemble of gradient boosting models improves predictive accuracy by reducing variance and bias simultaneously
- Mechanism: Gradient boosting builds an additive ensemble of weak learners, each correcting errors from its predecessor
- Core assumption: Individual base learners are sufficiently diverse and weak
- Evidence anchors: Abstract shows R-squared of 0.901; section notes unique approaches to minimizing bias and variance

### Mechanism 2
- Claim: SHAP values provide consistent and interpretable attribution of feature contributions
- Mechanism: SHAP assigns each feature a value based on marginal contribution to model output across all possible feature coalitions
- Core assumption: Model output is stable and additive in feature contributions
- Evidence anchors: Abstract mentions combining ensemble models with SHAP for interpretability; section describes SHAP method for global and local interpretability

### Mechanism 3
- Claim: Feature selection using Boruta algorithm removes irrelevant features, improving model efficiency
- Mechanism: Boruta compares each feature's importance to randomly permuted shadow features
- Core assumption: Feature space contains both relevant and irrelevant features distinguishable by Random Forest
- Evidence anchors: Section describes Boruta algorithm implementation with BorutaShap package; section notes reduction to 22 features

## Foundational Learning

- Concept: Ensemble learning and bias-variance tradeoff
  - Why needed here: Understanding why combining models improves performance justifies choice of multiple boosting algorithms
  - Quick check question: What happens to bias and variance when you average multiple high-variance models?

- Concept: Feature importance and interpretability in ML
  - Why needed here: Study hinges on explaining predictions to stakeholders; knowing how SHAP works is critical
  - Quick check question: How does SHAP differ from traditional feature importance scores like Gini importance?

- Concept: Data preprocessing and transformation (Box-Cox)
  - Why needed here: Study normalizes skewed market value distributions; understanding transformations ensures correct application
  - Quick check question: Why is it necessary to apply the inverse Box-Cox transformation after prediction?

## Architecture Onboarding

- Component map: Raw data → clean → transform → select features → train ensemble → evaluate → interpret → deploy
- Critical path: Data ingestion (Sofifa scrape) → Preprocessing (NaN handling, outlier removal, Box-Cox transform) → Feature selection (Boruta wrapper) → Model training (ensemble: GBDT, LightGBM, XGBoost, etc.) → Evaluation (R-squared, RMSE, cross-validation) → Interpretation (SHAP global and local plots)
- Design tradeoffs:
  - Ensemble models vs. single models: Higher accuracy, but higher complexity and computation cost
  - SHAP vs. simpler importance scores: More reliable explanations, but computationally heavier
  - Box-Cox vs. no transform: Better distribution for modeling, but requires inverse transform for business interpretation
- Failure signatures:
  - Overfitting: High training accuracy, low test accuracy
  - Poor feature selection: Model still sensitive to many features, slow to train
  - Uninterpretable model: SHAP explanations vary wildly between similar predictions
- First 3 experiments:
  1. Train a single GBDT model without feature selection and compare to ensemble result
  2. Run SHAP on GBDT model and examine feature contributions for sample players
  3. Compare predictions with and without Box-Cox transformation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we enhance accuracy of market value estimations for superstar players, considering need to account for broader societal and social factors beyond athletic performance?
- Basis in paper: Explicit mention of inability to accurately value superstars due to need for broader societal factors
- Why unresolved: Current dataset limited to direct performance metrics, excluding indirect societal factors
- What evidence would resolve it: Comprehensive dataset incorporating societal factors with cross-disciplinary analytical approach

### Open Question 2
- Question: What impact does integration of more detailed performance metrics have on predictive accuracy of market value models?
- Basis in paper: Explicit suggestion that commercial entities capture over two hundred metrics per player per game not included in current dataset
- Why unresolved: Current analysis limited by depth of available performance metrics
- What evidence would resolve it: Applying analytical framework to comprehensive dataset with detailed performance metrics

### Open Question 3
- Question: How can model's interpretability be improved without sacrificing predictive accuracy, especially when using complex transformations like Box-Cox?
- Basis in paper: Explicit note that enhanced accuracy comes at cost of reduced interpretability due to Box-Cox transformation
- Why unresolved: Transformation improves predictive capability but complicates interpretation of SHAP values
- What evidence would resolve it: Developing methods to maintain interpretability through advanced visualization techniques or alternative transformations

## Limitations
- Model underperforms for superstar players, suggesting systematic bias in high-value predictions
- Limited data quality and representativeness due to reliance on single data source (Sofifa)
- Interpretability claims may be affected by complexity of non-linear models and Box-Cox transformations

## Confidence
- Core findings on ensemble modeling benefits: High
- Interpretability claims via SHAP: Medium
- Generalizability across datasets: Low

## Next Checks
1. Test model performance on independent, out-of-sample dataset to confirm generalizability
2. Compare SHAP-based explanations with alternative interpretability methods (e.g., LIME) for consistency
3. Perform sensitivity analysis on feature selection to ensure Boruta-identified features remain stable under data perturbations