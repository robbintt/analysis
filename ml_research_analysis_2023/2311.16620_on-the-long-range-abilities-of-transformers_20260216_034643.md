---
ver: rpa2
title: On the Long Range Abilities of Transformers
arxiv_id: '2311.16620'
source_url: https://arxiv.org/abs/2311.16620
tags:
- attention
- layers
- long-range
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work investigates why transformers underperform on long-range
  sequence modeling compared to specialized layers like state-space models and global
  convolutions. The authors identify two key principles for effective long-range modeling:
  smoothness and locality.'
---

# On the Long Range Abilities of Transformers

## Quick Facts
- arXiv ID: 2311.16620
- Source URL: https://arxiv.org/abs/2311.16620
- Reference count: 17
- Key outcome: LaS Attention improves transformer performance on long-range tasks by incorporating smoothness and locality inductive biases

## Executive Summary
This work investigates why transformers underperform on long-range sequence modeling compared to specialized layers like state-space models and global convolutions. The authors identify two key principles for effective long-range modeling: smoothness and locality. They propose Local and Smooth (LaS) Attention, a modification to standard self-attention that incorporates an exponentially decaying locality kernel and row-wise smoothing. This is achieved with negligible additional computation and no extra learnable parameters. Experiments on the Long Range Arena benchmark show LaS Attention significantly improves transformer performance across multiple tasks, narrowing the gap with specialized long-range layers. The method also outperforms other efficient transformer variants.

## Method Summary
The authors propose LaS Attention, which modifies standard self-attention by adding two operators: an exponentially decaying locality kernel (ELD) that enforces locality through element-wise multiplication with exp(-αcDL), and a smoothing operator implemented as row-wise average pooling. These modifications incorporate inductive biases for smoothness and locality without adding learnable parameters. The method is evaluated on the Long Range Arena benchmark using a causal transformer architecture with 8 heads, trained with specific hyperparameters for each task.

## Key Results
- LaS Attention significantly improves transformer performance on the Long Range Arena benchmark across all tasks
- The method outperforms other efficient transformer variants like Performer and Linformer
- Ablation studies show both smoothness and locality components contribute to the gains
- Performance benefits increase with longer context lengths and more training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers underperform on long-range tasks because they lack the right inductive bias for smoothness and locality.
- Mechanism: The model introduces a local exponentially decaying kernel and row-wise smoothing to reshape the attention matrix toward long-range dependencies.
- Core assumption: Smoothness and locality are essential inductive biases for long-range sequence modeling.
- Evidence anchors:
  - [abstract] "We identify that two key principles for long-range tasks are (i) incorporating an inductive bias towards smoothness, and (ii) locality."
  - [section] "We discern two simple yet significant conditions (i) an exponential decaying positional structure, and (ii) a regularized smooth global operator."
  - [corpus] Weak or missing—no direct mention of inductive bias or smoothness in neighboring papers.
- Break condition: If the exponentially decaying kernel or smoothing does not improve performance, or if performance degrades when added.

### Mechanism 2
- Claim: Smooth and decaying kernels regularize attention weights and encourage hierarchical local-to-global dependency capture.
- Mechanism: Average pooling per row smooths attention scores; element-wise multiplication with exp(-αcDL) enforces exponentially decaying locality.
- Core assumption: Transformers can model any kernel via self-attention if properly parameterized.
- Evidence anchors:
  - [abstract] "integrating these ideas into the attention mechanism improves results with a negligible amount of additional computation and without any additional trainable parameters."
  - [section] "the overall claim... is that the observed sub-optimal performance... is probably a matter of generalization, which can be mitigated effectively by incorporating appropriate inductive bias."
  - [corpus] Weak or missing—no explicit discussion of hierarchical dependency or kernel regularization.
- Break condition: If locality or smoothness operators degrade accuracy on tasks not requiring long-range reasoning.

### Mechanism 3
- Claim: Long-range layers share a common structure: exponential decay in kernels plus smoothness regularization.
- Mechanism: By emulating these traits in attention, LaS-Attention bridges the performance gap between transformers and specialized long-range layers.
- Core assumption: The design patterns observed in S4, Hyena, and similar layers are necessary for effective long-range modeling.
- Evidence anchors:
  - [abstract] "we demonstrate that minimal modifications to the transformer architecture can significantly enhance performance on the Long Range Arena (LRA) benchmark."
  - [section] "we learn that exponentially decaying kernels and kernel smoothness are often promoted."
  - [corpus] Weak or missing—corpus does not discuss kernel decay or smoothness as design principles.
- Break condition: If alternative inductive biases (e.g., learned positional encodings) outperform smoothness and locality.

## Foundational Learning

- Concept: Self-attention mechanism
  - Why needed here: Understanding how QK^T/√d_k shapes attention scores is essential to grasp how smoothing and locality operators modify them.
  - Quick check question: What is the effect of scaling QK^T by 1/√d_k before applying softmax?

- Concept: Exponential decay in kernels
  - Why needed here: The locality operator uses exp(-αcDL) to enforce decay; knowing how this shapes attention helps understand its impact.
  - Quick check question: How does changing αc affect the decay rate in the locality matrix?

- Concept: 1-D average pooling
  - Why needed here: Smoothing is implemented via per-row average pooling; understanding this operation is critical for seeing how smoothness is injected.
  - Quick check question: Why is per-row pooling (not column-wise) used in the smoothing operator?

## Architecture Onboarding

- Component map: Input embeddings -> Positional encoding (identity indicator) -> Multi-head attention (LaS) -> Feed-forward -> Layer norm -> Output

- Critical path:
  1. Compute Q, K, V from input
  2. Apply exp(-αcDL) element-wise to QK^T
  3. Apply row-wise average pooling to the result
  4. Apply softmax and multiply by V
  5. Combine heads and project

- Design tradeoffs:
  - Adding smoothing vs. keeping raw attention scores
  - Enforcing locality vs. allowing unrestricted attention
  - Using causal masking vs. bidirectional attention

- Failure signatures:
  - Performance drops if αc is too large (overly restrictive locality)
  - Training instability if pooling window is too wide
  - No improvement if locality or smoothing are removed

- First 3 experiments:
  1. Ablate smoothing operator only and compare LRA scores.
  2. Ablate locality operator only and compare LRA scores.
  3. Sweep αc values to observe impact on long-range vs. short-range task performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does LaS Attention generalize to tasks beyond the Long Range Arena benchmark, such as natural language processing or computer vision tasks?
- Basis in paper: [inferred] The paper mentions evaluating LaS Attention on sequential MNIST and Wikitext-103, but these are limited in scope. The authors suggest that further research is needed to determine the method's effectiveness on a broader range of tasks.
- Why unresolved: The paper focuses primarily on the LRA benchmark, which may not fully capture the diversity of real-world long-range tasks. The limited evaluation on other tasks suggests that the method's generalization capabilities are not yet fully understood.
- What evidence would resolve it: Conducting extensive experiments on a variety of long-range tasks, including those in NLP, computer vision, and other domains, would provide a clearer picture of LaS Attention's generalization abilities.

### Open Question 2
- Question: How does the performance of LaS Attention scale with increasing sequence lengths beyond those tested in the LRA benchmark?
- Basis in paper: [explicit] The paper evaluates LaS Attention on the LRA benchmark, which includes sequences up to 16K tokens. However, the authors acknowledge that further research is needed to understand the method's performance on even longer sequences.
- Why unresolved: The LRA benchmark provides a valuable starting point, but it may not represent the full range of sequence lengths encountered in practical applications. Understanding how LaS Attention performs on extremely long sequences is crucial for its adoption in real-world scenarios.
- What evidence would resolve it: Conducting experiments with sequences significantly longer than those in the LRA benchmark, such as those encountered in document processing or video analysis, would provide insights into the method's scalability.

### Open Question 3
- Question: Can the principles of smoothness and locality identified in LaS Attention be applied to other transformer architectures, such as decoder-only models or hybrid architectures?
- Basis in paper: [explicit] The paper focuses on applying the principles of smoothness and locality to the self-attention mechanism in transformers. However, the authors suggest that these principles may be applicable to other transformer architectures as well.
- Why unresolved: The paper primarily explores the application of these principles to the self-attention mechanism, leaving open the question of their applicability to other transformer components or architectures.
- What evidence would resolve it: Conducting experiments to integrate the principles of smoothness and locality into other transformer architectures, such as decoder-only models or hybrid architectures, would provide insights into their broader applicability.

## Limitations

- The method's effectiveness beyond the LRA benchmark remains unproven, with limited evaluation on other long-range tasks
- The optimal decay parameter αc and pooling window size may vary across different tasks and domains
- The unidirectional nature of the transformer may still limit performance compared to bidirectional global convolution layers

## Confidence

- High confidence: The empirical improvements on LRA benchmark tasks are well-documented and reproducible with the provided methodology.
- Medium confidence: The theoretical justification for why smoothness and locality are essential inductive biases, as the connection to transformer underperformance is primarily observational rather than derived from first principles.
- Medium confidence: The claim that transformers can match specialized layers with minimal modifications, as this is demonstrated on LRA but not conclusively proven across all long-range modeling domains.

## Next Checks

1. Test LaS Attention on non-LRA long-range tasks (e.g., genomic sequence modeling, time series forecasting) to verify the generalization of smoothness and locality benefits beyond the benchmark setting.
2. Conduct a systematic study varying αc across a wider range and measuring the trade-off between locality enforcement and model flexibility on both long-range and short-range tasks.
3. Implement and evaluate a bidirectional variant of LaS Attention to isolate the impact of unidirectional processing on performance relative to global convolution layers like MEGA and S4.