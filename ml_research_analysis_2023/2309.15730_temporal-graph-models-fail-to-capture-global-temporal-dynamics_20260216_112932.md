---
ver: rpa2
title: Temporal graph models fail to capture global temporal dynamics
arxiv_id: '2309.15730'
source_url: https://arxiv.org/abs/2309.15730
tags:
- temporal
- datasets
- negative
- baseline
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes the recently proposed Temporal Graph Benchmark
  (TGB) and shows that existing temporal graph models fail to capture global temporal
  dynamics. The authors propose a simple baseline of "recently popular nodes" that
  outperforms existing methods on most TGB datasets.
---

# Temporal graph models fail to capture global temporal dynamics

## Quick Facts
- arXiv ID: 2309.15730
- Source URL: https://arxiv.org/abs/2309.15730
- Reference count: 2
- Existing temporal graph models struggle with global temporal dynamics, while a simple exponential smoothing baseline outperforms them

## Executive Summary
This work analyzes the recently proposed Temporal Graph Benchmark (TGB) and reveals fundamental limitations in existing temporal graph models. The authors demonstrate that current models fail to capture global temporal dynamics, which are prevalent in real-world temporal graph datasets like social media and e-commerce. A simple exponential smoothing baseline that tracks recently popular nodes consistently outperforms complex neural architectures on most TGB datasets. The paper introduces two novel measures based on Wasserstein distance to quantify temporal dynamics, shows that standard negative sampling evaluation is inadequate for temporally dynamic datasets, and proposes improved negative sampling schemes for training and evaluation.

## Method Summary
The authors analyze dynamic link property prediction on temporal graphs using the TGB datasets. They implement a simple exponential smoothing baseline that maintains a time-decayed vector of destination node occurrences, and compare it against existing temporal graph network architectures (TGN, DyRep) with and without Recently Popular Negative Sampling (RP-NS). Evaluation uses both standard MRRnaive and improved MRRtopN metrics that sample negative examples from recently popular destination nodes. The study also introduces Wshort and Wlong measures based on Wasserstein distance to quantify short-term and long-term temporal dynamics in the datasets.

## Key Results
- Simple exponential smoothing baseline outperforms complex neural models on MRRnaive for most TGB datasets
- Standard negative sampling evaluation fails to reveal true model weaknesses, showing many perfect 1.0 scores
- Improved MRRtopN evaluation exposes model limitations, with MRRtopN degrading rapidly as N increases
- RP-NS significantly improves model performance on temporally dynamic datasets
- Datasets with high Wshort/Wlong ratios show stronger temporal autocorrelation and benefit more from the proposed improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal graph models fail to capture global temporal dynamics because they are trained with negative sampling that rarely provides hard negatives in non-stationary environments.
- Mechanism: Standard uniform random negative sampling generates mostly stale negative examples when the dataset has strong temporal autocorrelation, causing the model to saturate scores on recently popular nodes and fail to rank them properly.
- Core assumption: The distribution of destination nodes in recent time steps is highly predictive of future edges, and models need to learn to discriminate among recently popular candidates rather than easy historical negatives.
- Evidence anchors:
  - [abstract] "We show how simple negative-sampling can lead to model degeneration during training, resulting in impossible to rank, fully saturated predictions of temporal graph networks."
  - [section 6] "The number of nodes in the benchmark datasets is very large (up to 1 million), so both training and evaluation with negative sampling seem justified. Nonetheless, we can see from the rapid degradation of MRRtopN as N grows, that such evaluations may be a poor proxy for a full MRR calculation."
- Break condition: If the dataset has low temporal autocorrelation (high Wshort/Wlong measures), this mechanism would not apply as negative sampling would naturally provide harder candidates.

### Mechanism 2
- Claim: A simple exponential smoothing baseline outperforms complex neural models because it directly captures the global temporal dynamics that neural models miss.
- Mechanism: The baseline maintains a time-decayed vector of destination node occurrences, which effectively tracks recently popular nodes that are likely to appear in future edges due to self-reinforcing popularity cycles in social networks.
- Core assumption: Recent global popularity of destination nodes is highly predictive of future link formation in datasets with strong temporal dynamics (social media, cryptocurrency, e-commerce).
- Evidence anchors:
  - [section 4] "We test Algorithm 1 on all dynamic link property prediction datasets... Our method is fully deterministic."
  - [section 4.2] "The phenomenon of short-lived, self-reinforcing popularity is present in other areas of digital social life such as X (Twitter), Facebook, and even e-commerce stores (fast fashion trends), cryptocurrency trading activities (hype cycles)."
- Break condition: On datasets with weak temporal dynamics like Wikipedia editing (low Wshort/Wlong), this mechanism fails as recent popularity becomes less predictive.

### Mechanism 3
- Claim: Improved evaluation metrics (MRRtopN) reveal the true weakness of temporal graph models by forcing them to rank hard negative candidates.
- Mechanism: By sampling negative examples from recently popular destination nodes rather than uniform random or historical negatives, the evaluation better reflects real-world performance where models must distinguish between currently trending nodes.
- Core assumption: Real-world utility of temporal graph models requires them to accurately rank recently popular destination nodes, not just discriminate between fresh positives and stale negatives.
- Evidence anchors:
  - [section 5.2] "We propose an improved evaluation method, by sampling from top N recently popular items according to our simple baseline model in addition to the original method proposed by Huang et al. [2023]."
  - [section 5] "The ability to both accurately rank recently popular destination nodes, as well as to vary predictions depending on the source node, seem to be reasonable requirements for a good temporal graph model."
- Break condition: If datasets have uniform or slowly changing popularity distributions, this evaluation method would not reveal significant weaknesses in model performance.

## Foundational Learning

- Concept: Wasserstein distance (Earth Mover's Distance)
  - Why needed here: Used to quantify the similarity between destination node distributions across time steps, providing measures of short-term and long-term temporal dynamics
  - Quick check question: How would you compute the Wasserstein distance between two empirical distributions of node occurrences?

- Concept: Exponential smoothing
  - Why needed here: The proposed baseline uses exponential smoothing to maintain a time-decayed vector of node occurrences, which is the core mechanism for capturing recent popularity
  - Quick check question: What happens to the baseline's predictions if you set the decay factor λ to 1.0 versus 0.5?

- Concept: Negative sampling in contrastive learning
  - Why needed here: Understanding how negative sampling affects model training and evaluation is crucial for diagnosing why standard approaches fail on temporally dynamic datasets
  - Quick check question: Why might uniform random negative sampling be insufficient for training temporal graph models on social media data?

## Architecture Onboarding

- Component map: Temporal graph datasets with timestamps and edge properties -> Exponential smoothing baseline with decay factor λ -> TGN and DyRep models with optional RP-NS -> Evaluation with MRRnaive, MRRtopN, full MRR calculation -> Dynamics measures Wshort, Wlong using Wasserstein distance

- Critical path: Load temporal graph -> Compute Wshort/Wlong measures -> Train baseline and neural models -> Evaluate with MRRtopN -> Compare performance -> Apply RP-NS if needed

- Design tradeoffs:
  - Simplicity vs expressiveness: The baseline trades model complexity for direct capture of temporal dynamics
  - Training efficiency vs performance: RP-NS improves training by providing harder negatives but adds computational overhead
  - Evaluation completeness vs efficiency: MRRtopN is more informative but more expensive than MRRnaive

- Failure signatures:
  - High percentage of perfect 1.0 scores on recently popular nodes (Table 1)
  - Sharp degradation of MRRtopN as N increases
  - Baseline outperforming neural models on MRRnaive but underperforming on MRRtopN

- First 3 experiments:
  1. Compute Wshort and Wlong measures for a new temporal graph dataset to assess temporal autocorrelation strength
  2. Implement the exponential smoothing baseline and evaluate on MRRtop20 vs MRRnaive to establish a performance baseline
  3. Train TGN with and without RP-NS on a dataset with strong temporal dynamics and compare MRRtopN performance across different N values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural changes to temporal graph networks would allow them to better capture global temporal dynamics on datasets with strong autocorrelation?
- Basis in paper: [explicit] The paper shows existing TG models struggle with global temporal dynamics and suggests "deep rethinking" is needed, but doesn't specify what architectural changes would help.
- Why unresolved: The authors identify the problem but don't propose specific architectural solutions. They compare against non-contrastive methods which perform better, suggesting alternative training approaches, but don't detail how to modify the network architecture itself.
- What evidence would resolve it: A study comparing multiple architectural variants (attention mechanisms, temporal aggregation strategies, contrastive vs non-contrastive training) on datasets with varying Wshort and Wlong values would provide evidence for what architectural choices help capture global temporal dynamics.

### Open Question 2
- Question: How does the performance gap between simple popularity baselines and neural TG models vary across different types of temporal dynamics (e.g., periodic vs. aperiodic, local vs. global)?
- Basis in paper: [inferred] The authors show popularity baselines perform well on datasets with strong global temporal dynamics but don't systematically explore how performance varies with different dynamic patterns or how localized vs. global effects impact results.
- Why unresolved: The paper only measures two types of global dynamics (Wshort and Wlong) but doesn't explore how these relate to the gap between simple and complex models, or whether different types of temporal patterns require different modeling approaches.
- What evidence would resolve it: Experiments on datasets engineered to have specific temporal patterns (periodic, chaotic, local vs. global) with systematic comparison between popularity baselines and neural models would reveal when complex architectures are actually necessary.

### Open Question 3
- Question: What is the relationship between the choice of negative sampling strategy and the model's ability to distinguish between recently popular and stale nodes?
- Basis in paper: [explicit] The authors show that standard negative sampling fails on datasets with strong temporal dynamics and propose Recently Popular Negative Sampling (RP-NS), but don't systematically study how different negative sampling strategies affect the model's ability to capture temporal patterns.
- Why unresolved: While RP-NS is shown to improve performance, the paper doesn't explore the full space of possible negative sampling strategies or understand what properties make a sampling strategy effective for temporal graphs.
- What evidence would resolve it: A comprehensive study comparing various negative sampling strategies (temporal-aware, popularity-based, adaptive sampling) on their ability to generate hard negatives and improve MRRtopN scores would clarify the relationship between sampling strategy and temporal learning.

## Limitations
- The findings primarily apply to datasets with strong temporal autocorrelation, limiting generalizability
- The analysis focuses on link property prediction tasks, potentially missing issues in other temporal graph learning tasks
- Hyperparameter sensitivity of the RP-NS scheme is not thoroughly explored

## Confidence

- Temporal graph models systematically fail to capture global temporal dynamics: High confidence
- Exponential smoothing baseline outperforms complex neural models on MRRnaive: Medium confidence
- Improved negative sampling schemes (RP-NS) significantly improve model performance: Medium confidence
- Standard negative sampling evaluation is unsuitable for temporally dynamic datasets: High confidence

## Next Checks
1. Compute Wshort and Wlong measures for a new temporal graph dataset (e.g., social media interactions or transaction logs) to validate the generalizability of the temporal dynamics quantification framework.

2. Implement an ablation study comparing the exponential smoothing baseline with variants using different decay factors (λ values) to determine the sensitivity of the approach to this hyperparameter.

3. Train a temporal graph model with and without RP-NS on a dataset with weak temporal dynamics (low Wshort/Wlong) to test whether the proposed improvements are truly beneficial only for temporally correlated data.