---
ver: rpa2
title: 'Sketch In, Sketch Out: Accelerating both Learning and Inference for Structured
  Prediction with Kernels'
arxiv_id: '2302.10128'
source_url: https://arxiv.org/abs/2302.10128
tags:
- kernel
- output
- sketching
- learning
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes to scale up surrogate kernel methods for structured
  prediction by leveraging random projections in both input and output feature spaces,
  reducing training and inference time while preserving statistical properties. The
  approach, called Sketched Input Sketched Output Kernel Regression (SISOKR), applies
  sketching-based approximations to Input Output Kernel ridge Regression (IOKR).
---

# Sketch In, Sketch Out: Accelerating both Learning and Inference for Structured Prediction with Kernels

## Quick Facts
- arXiv ID: 2302.10128
- Source URL: https://arxiv.org/abs/2302.10128
- Reference count: 40
- This work proposes to scale up surrogate kernel methods for structured prediction by leveraging random projections in both input and output feature spaces, reducing training and inference time while preserving statistical properties.

## Executive Summary
This paper introduces a novel approach to accelerate kernel-based structured prediction by simultaneously sketching both input and output feature spaces. The method, called Sketched Input Sketched Output Kernel Regression (SISOKR), extends Input Output Kernel ridge Regression (IOKR) with random projections that dramatically reduce computational complexity while maintaining statistical performance. Theoretical analysis establishes learning rates that depend on the eigendecay of input/output covariance operators, showing that small sketch sizes can achieve near-optimal performance. Experiments on synthetic and real-world datasets confirm significant computational gains without sacrificing predictive accuracy.

## Method Summary
SISOKR applies sketching-based approximations to Input Output Kernel ridge Regression by projecting both input and output feature maps into lower-dimensional spaces using random matrices. The approach reduces training time through input sketching (which lowers the dimensionality of matrix inversion) and inference time through output sketching (which accelerates the decoding step). The method uses sub-Gaussian sketches that preserve essential structure while enabling computational efficiency. Theoretical analysis shows that SISOKR achieves close-to-optimal learning rates with sketch sizes that depend on the eigendecay of input/output covariance operators, and experiments demonstrate significant computational gains while maintaining performance comparable to non-sketched methods.

## Key Results
- SISOKR achieves close-to-optimal learning rates with sketch sizes that depend on the eigendecay of input/output covariance operators
- Computational complexity reduces from O(n³) to O(n^1.5) for training and from O(n²) to O(n^0.5) for inference
- Experiments show significant computational gains on synthetic and real-world datasets while maintaining comparable performance to non-sketched methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sketching the input kernel primarily reduces training time by lowering the dimensionality of the matrix inversion.
- **Mechanism:** By projecting the input feature map into a lower-dimensional subspace using a random sketching matrix, the effective number of features used in kernel ridge regression decreases from n to m_x (sketch size), making the inversion of the m_x × m_x matrix computationally cheaper than the original n × n matrix.
- **Core assumption:** The sketching matrix preserves the essential structure of the input data so that the low-rank approximation still captures the necessary information for learning.
- **Evidence anchors:**
  - [abstract]: "sketching the input kernel mostly reduces training time"
  - [section 3]: "This quantity allows to reduce the size of the matrix to invert, which is now an m_x × m_x matrix."
- **Break condition:** If the input data has a very low intrinsic dimension and sketching doesn't provide significant dimensionality reduction, the computational benefit may be minimal.

### Mechanism 2
- **Claim:** Sketching the output kernel reduces inference time by accelerating the decoding step.
- **Mechanism:** During prediction, instead of computing the full n × n matrix product for decoding, sketching reduces it to an m_x × m_y matrix product, where m_y is the sketch size for the output, significantly speeding up the search for the best structured output.
- **Core assumption:** The output space is high-dimensional or has a large candidate set, so reducing its dimensionality through sketching yields substantial computational savings.
- **Evidence anchors:**
  - [abstract]: "sketching the output kernel decreases the inference time"
  - [section 3]: "The purpose of output sketching is to accelerate inference... the main quantity to compute now... K_te,tr R_x^T ˜Ω Ry K_y^tr,c"
- **Break condition:** If the output space is already low-dimensional (e.g., linear output kernel with d=300), the benefit of output sketching may be negligible compared to input sketching.

### Mechanism 3
- **Claim:** Sub-Gaussian sketches provide a good balance between computational efficiency and statistical performance.
- **Mechanism:** Sub-Gaussian sketches (including Gaussian and bounded random variables) ensure that the approximation error introduced by sketching decreases at a rate dependent on the eigendecay of the input/output covariance operators, allowing small sketch sizes while maintaining close-to-optimal learning rates.
- **Core assumption:** The eigendecay of the covariance operators is sufficiently fast (e.g., polynomial decay with exponent > 0) so that a small sketch size can achieve low reconstruction error.
- **Evidence anchors:**
  - [abstract]: "Gaussian and sub-Gaussian sketches are admissible sketches... they induce projection operators ensuring a small excess risk"
  - [section 4.2]: Theorem 2 shows reconstruction error bounds for sub-Gaussian sketches
- **Break condition:** If the eigendecay is very slow (e.g., γ_z ≈ 0), the required sketch size may approach n, negating the computational benefits.

## Foundational Learning

- **Concept: Reproducing Kernel Hilbert Spaces (RKHS)**
  - Why needed here: The entire framework relies on kernel methods, which map inputs and outputs into RKHSs to enable linear operations on non-linear data representations.
  - Quick check question: What property of RKHS ensures that evaluating a function at a point can be expressed as an inner product in the feature space?

- **Concept: Operator-valued kernels**
  - Why needed here: Structured prediction requires mapping inputs to structured outputs, which necessitates vector-valued functions. Operator-valued kernels provide the necessary framework for this.
  - Quick check question: How does an operator-valued kernel differ from a scalar-valued kernel in terms of the space it maps to?

- **Concept: Sketching and random projections**
  - Why needed here: Sketching is the core technique used to approximate the kernel matrices and accelerate computations. Understanding how random projections preserve distances and inner products is crucial.
  - Quick check question: What is the Johnson-Lindenstrauss lemma, and how does it relate to the effectiveness of sketching?

## Architecture Onboarding

- **Component map:** Input space X with kernel k_x -> Input sketching matrix R_x (m_x × n) -> Operator-valued kernel K(x,x') = k_x(x,x') I_Hy -> Output sketching matrix R_y (m_y × n) -> Output space Y with kernel k_y -> Sketched Input-Sketched Output Kernel Regression (SISOKR) estimator

- **Critical path:**
  1. Compute input kernel matrix K_X and output kernel matrix K_Y
  2. Apply sketching matrices R_x and R_y to obtain ˜K_X and ˜K_Y
  3. Compute the sketched solution coefficients α(x) using the reduced-dimension matrices
  4. During prediction, use the sketched matrices to compute the structured output efficiently

- **Design tradeoffs:**
  - Sketch size vs. accuracy: Larger sketches reduce approximation error but increase computation
  - Input vs. output sketching: Input sketching reduces training time, output sketching reduces inference time
  - Choice of sketching distribution: Gaussian sketches vs. subsampling vs. sparse sketches affect both theory and practice

- **Failure signatures:**
  - Degraded performance with small sketch sizes: Indicates insufficient preservation of input/output structure
  - Training/inference time not improving: May indicate that the original problem was already low-dimensional or that the sketching implementation is inefficient
  - Numerical instability: Can occur if sketch sizes are too small relative to the eigendecay of the covariance operators

- **First 3 experiments:**
  1. Compare SISOKR with IOKR on a synthetic least-squares regression problem with known eigendecay to verify computational gains and statistical performance
  2. Test different sketching distributions (Gaussian vs. subsampling) on the same problem to evaluate practical tradeoffs
  3. Apply SISOKR to a multi-label classification dataset and measure F1 score improvements over baseline methods while tracking training/inference time reductions

## Open Questions the Paper Calls Out
The paper mentions that other sketching distributions could be admissible but focuses on sub-Gaussian sketches, leaving the performance of alternative distributions unexplored.

## Limitations
- The theoretical analysis assumes fast eigendecay of covariance operators, which may not hold for all real-world datasets
- Experimental validation is limited to specific synthetic and benchmark datasets, leaving uncertainty about generalizability to other structured prediction tasks
- Computational gains depend heavily on the relationship between sketch sizes and problem dimensions, which may vary significantly across applications

## Confidence
- High confidence in the core sketching mechanisms and their computational benefits (Mechanisms 1-2)
- Medium confidence in the statistical guarantees, as they rely on assumptions about eigendecay that may not hold universally
- Medium confidence in practical performance claims, given limited experimental scope

## Next Checks
1. Test SISOKR on datasets with different eigendecay characteristics (e.g., synthetic data with controlled eigendecay rates) to verify the relationship between eigendecay and required sketch sizes
2. Evaluate SISOKR on structured prediction tasks beyond multi-label classification (e.g., sequence labeling or graph prediction) to assess generalizability
3. Compare SISOKR's computational scaling with non-sketched IOKR across varying problem sizes (n) to confirm the claimed O(n^1.5) complexity advantage