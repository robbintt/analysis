---
ver: rpa2
title: 'We Don''t Need No Adam, All We Need Is EVE: On The Variance of Dual Learning
  Rate And Beyond'
arxiv_id: '2308.10740'
source_url: https://arxiv.org/abs/2308.10740
tags:
- learning
- optimisation
- momentum
- rate
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Enhanced Velocity Estimation (EVE) introduces a novel optimisation
  method for deep learning that uses dual learning rates and dual adaptive momentum
  terms to address challenges in training deep neural networks. EVE applies separate
  learning rates to different gradient components and maintains adaptive momentum
  terms (short-term and long-term) that respond to the training landscape.
---

# We Don't Need No Adam, All We Need Is EVE: On The Variance of Dual Learning Rate And Beyond

## Quick Facts
- arXiv ID: 2308.10740
- Source URL: https://arxiv.org/abs/2308.10740
- Reference count: 40
- Primary result: EVE optimizer outperforms Adam on CIFAR-100 and Flower Classification with lower validation loss and better generalization

## Executive Summary
Enhanced Velocity Estimation (EVE) introduces a novel optimization method for deep learning that uses dual learning rates and dual adaptive momentum terms to address challenges in training deep neural networks. The method applies separate learning rates to different gradient components and maintains adaptive momentum terms (short-term and long-term) that respond to the training landscape. Experiments on CIFAR-100 (using ResNet-50) and Flower Classification (using DenseNet-201) datasets demonstrate that EVE consistently outperforms the widely-used Adam optimizer across various learning rate settings.

## Method Summary
EVE implements a dual learning rate approach that applies different learning rates to sparse versus dense gradients, allowing for more nuanced control during optimization. The method maintains two adaptive momentum terms (short-term and long-term) that capture different temporal aspects of gradient behavior, along with residual velocity terms that provide additional adaptability. The parameter update rule integrates these components to navigate complex loss surfaces more effectively than traditional single-rate optimizers.

## Key Results
- On CIFAR-100, EVE achieves lower validation loss (3.37 mean) with reduced standard deviation compared to Adam's 251.93
- For Flower Classification, EVE attains better validation metrics including lower loss and higher F1-scores across different learning rate combinations
- EVE shows faster convergence and better generalization, particularly at extreme learning rate values where Adam struggles

## Why This Works (Mechanism)

### Mechanism 1
Dual learning rates (lr1, lr2) enable separate adaptation for sparse vs dense gradients, reducing oscillations and improving convergence stability. EVE applies lr1 to gradients that are consistently moving in the same direction (congruent) and lr2 to gradients that change direction frequently (incongruent). This allows larger updates for stable gradients while smaller, more cautious updates for unstable ones.

### Mechanism 2
Adaptive momentum (mS, mL) with residual velocity terms (v1, v2) provide both short-term responsiveness and long-term stability in parameter updates. Short-term momentum (mS) captures recent gradient trends while long-term momentum (mL) smooths over longer periods. The combined momentum mt balances these using β3, while residual velocities v1 and v2 adaptively scale updates based on gradient consistency.

### Mechanism 3
The integration of dual learning rates with adaptive momenta creates a more efficient navigation of complex loss surfaces compared to single-rate optimizers. The combined update rule θt+1 = θt − (α1t mt + α2t mt) applies both learning rates simultaneously, with α1t and α2t scaling based on the momentum and velocity terms. This allows simultaneous exploitation of both short-term and long-term gradient information.

## Foundational Learning

- **Adaptive learning rate optimization**: Understanding how learning rates can be dynamically adjusted during training is fundamental to grasping why EVE's dual-rate approach is beneficial
  - Quick check question: How do standard adaptive methods like Adam adjust learning rates, and what limitations do they have compared to EVE's approach?

- **Momentum-based optimization**: The dual momentum terms (short-term and long-term) are central to EVE's mechanism, so understanding how momentum works in optimization is essential
  - Quick check question: What is the difference between classical momentum and Nesterov momentum, and how might dual momentum extend these concepts?

- **Gradient consistency and variance**: EVE's distinction between sparse and dense gradients, and its use of residual velocity terms, requires understanding how gradient behavior varies across training
  - Quick check question: How can gradient variance be measured, and why would different regions of a loss landscape exhibit different gradient consistency?

## Architecture Onboarding

- **Component map**: EVE consists of (1) dual learning rates (lr1, lr2) for different gradient types, (2) dual adaptive momentum terms (mS, mL) for short/long-term trends, (3) residual velocity terms (v1, v2) for adaptive scaling, and (4) integration logic that combines these components in the parameter update rule

- **Critical path**: 1) Compute gradients gt, 2) Calculate dual learning rates α1t and α2t, 3) Update momentum terms mS_t and mL_t, 4) Compute residual velocities v1 and v2, 5) Combine into parameter update θt+1 = θt − (α1t mt + α2t mt)

- **Design tradeoffs**: Dual learning rates provide more nuanced control but increase hyperparameter complexity; dual momentum provides better stability but requires more memory and computation; residual velocities add adaptability but may introduce instability if not properly bounded

- **Failure signatures**: Poor convergence when β3 is poorly tuned (overweighting one momentum type), instability when α is too large (residual velocities dominate), and no improvement over Adam when the loss landscape doesn't benefit from dual adaptation

- **First 3 experiments**:
  1. Compare EVE vs Adam on CIFAR-100 with ResNet-50 using default hyperparameters to establish baseline performance difference
  2. Test EVE with varying β3 values (e.g., 0.5, 0.8, 0.95) to understand sensitivity to momentum balance
  3. Compare EVE's performance on simple vs complex architectures (e.g., ResNet-18 vs ResNet-50) to validate the hypothesis that dual mechanisms help more on complex landscapes

## Open Questions the Paper Calls Out

- **Open Question 1**: How does EVE's performance scale when applied to larger datasets and more complex architectures beyond CIFAR-100 and Flower Classification?
  - Basis in paper: [explicit] The authors mention that future work should test scalability on larger datasets and more intricate architectures
  - Why unresolved: The paper only provides empirical results on two specific datasets and architectures (CIFAR-100 with ResNet-50 and Flower Classification with DenseNet-201), leaving performance on larger, more complex problems unexplored
  - What evidence would resolve it: Comprehensive experiments on diverse large-scale datasets (e.g., ImageNet, COCO) and complex architectures (e.g., Vision Transformers, large language models) comparing EVE against state-of-the-art optimizers

- **Open Question 2**: Can EVE dynamically adjust its learning rates and momentum values during training based on the training phase or specific optimization challenges encountered?
  - Basis in paper: [explicit] The authors suggest exploring the possibility of dynamically adjusting learning rates and momentum values based on the training phase
  - Why unresolved: The current implementation uses fixed hyperparameters for dual learning rates and momentum terms, without adaptive mechanisms that respond to training dynamics
  - What evidence would resolve it: Development and validation of an adaptive variant of EVE that automatically tunes its hyperparameters during training, with ablation studies showing improvements over the fixed-parameter version

- **Open Question 3**: How does EVE perform when integrated with other modern optimization techniques such as adaptive gradient clipping or the Lookahead method?
  - Basis in paper: [explicit] The authors propose investigating how EVE fares when combined with other modern techniques like adaptive gradient clipping or Lookahead
  - Why unresolved: The paper presents EVE as a standalone optimizer without exploring synergistic combinations with other optimization strategies
  - What evidence would resolve it: Experimental results showing performance improvements (or lack thereof) when EVE is combined with techniques like gradient clipping, Lookahead, or other recent optimization methods on benchmark tasks

## Limitations

- The study relies on only two datasets (CIFAR-100 and Flower Classification) and two architectures (ResNet-50 and DenseNet-201), which may not capture the full range of scenarios where EVE's dual mechanisms could provide benefits or fail
- The paper lacks detailed analysis of hyperparameter sensitivity, particularly for the momentum balance parameter β3 and the residual velocity scaling parameter α, which could significantly impact performance
- The paper does not provide implementation details for determining "sparse" vs "dense" gradients in the residual velocity calculation, requiring clarification on the threshold or method used

## Confidence

- **High confidence**: EVE outperforms Adam on the tested datasets and architectures for the specific hyperparameter configurations evaluated
- **Medium confidence**: The theoretical mechanisms (dual learning rates, dual momentum, residual velocities) contribute positively to optimization, based on observed performance improvements
- **Low confidence**: EVE will generalize to arbitrary deep learning tasks, as the validation is limited to specific architectures and datasets

## Next Checks

1. Conduct a comprehensive hyperparameter sensitivity analysis across a wider range of β1, β2, β3, α values and learning rate combinations to identify optimal configurations and failure modes
2. Evaluate EVE on diverse architectures including transformer models, recurrent networks, and different convolutional architectures beyond ResNet and DenseNet
3. Test EVE on additional benchmark datasets spanning different modalities (text, speech, video) and complexity levels to assess generalization across domains