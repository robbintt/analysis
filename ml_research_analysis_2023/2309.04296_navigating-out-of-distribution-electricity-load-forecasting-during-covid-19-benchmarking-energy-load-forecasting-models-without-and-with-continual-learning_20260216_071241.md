---
ver: rpa2
title: 'Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19:
  Benchmarking energy load forecasting models without and with continual learning'
arxiv_id: '2309.04296'
source_url: https://arxiv.org/abs/2309.04296
tags:
- learning
- data
- energy
- forecasting
- fsnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles electricity load forecasting during the COVID-19
  pandemic, a period marked by severe out-of-distribution (OoD) data shifts due to
  lockdowns. It proposes a continual learning framework that leverages human mobility
  data from pedestrian counters to improve forecasting accuracy.
---

# Navigating Out-of-Distribution Electricity Load Forecasting during COVID-19: Benchmarking energy load forecasting models without and with continual learning

## Quick Facts
- arXiv ID: 2309.04296
- Source URL: https://arxiv.org/abs/2309.04296
- Reference count: 40
- This paper shows that continual learning with mobility data can improve electricity load forecasting accuracy by up to 40% during out-of-distribution periods like COVID-19 lockdowns.

## Executive Summary
This paper addresses electricity load forecasting during the COVID-19 pandemic, when lockdowns caused severe out-of-distribution data shifts. The authors propose a continual learning framework that uses FSNet, a deep continual learning algorithm, to update incrementally without catastrophic forgetting. By incorporating human mobility data from pedestrian counters alongside temperature features, the method significantly outperforms traditional and online learning baselines on 13 Melbourne building complexes, achieving up to 40% improvements in MAE. The results demonstrate the importance of continual learning and auxiliary data for robust forecasting during distributional shifts.

## Method Summary
The method uses FSNet, a continual learning algorithm with fast and slow parameter sets, to adapt to new data without forgetting historical patterns. It incorporates hourly electricity load data, pedestrian mobility counts from 42 sensors, and temperature data as features. The model is trained with a 3-month warm-up period followed by incremental updates. Hyperparameter optimization is performed via Optuna across 20 runs. Performance is evaluated using MAE across five time periods, comparing FSNet with baselines including VAR, LSTM, NBEATS with and without online learning.

## Key Results
- FSNet with mobility and temperature features (ETM) achieved up to 40% improvement in MAE over CopyLastHour baseline during lockdown periods
- Mobility data alone provided up to 23% gains, showing strong correlation with occupancy-driven load shifts
- Simpler models like VAR+OL sometimes outperformed deep models on certain building complexes
- Methods without continual learning failed to generalize during out-of-distribution periods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual learning prevents catastrophic forgetting during out-of-distribution periods
- Mechanism: FSNet maintains two parameter sets—one for fast learning on recent data and one for slow learning on historical data—so that new data updates do not overwrite previously learned patterns
- Core assumption: The slow parameter set can adequately capture stable temporal features while the fast set adapts to shifts
- Evidence anchors:
  - [abstract]: "continual learning offers a holistic approach by preserving past insights while integrating new data"
  - [section 4.4.6]: "Fast and Slow Network (FSNet) [33] is a deep continual learning method"
  - [corpus]: Weak; related papers do not explicitly discuss catastrophic forgetting mitigation
- Break condition: If the slow network cannot generalize across longer temporal gaps, forgetting may still occur

### Mechanism 2
- Claim: Mobility data acts as a proxy for occupancy-driven load shifts
- Mechanism: Pedestrian counts correlate with building usage intensity; embedding this as a feature helps the model anticipate load changes when occupancy patterns change abruptly
- Core assumption: Pedestrian flow changes are sufficiently aligned with electricity demand changes
- Evidence anchors:
  - [abstract]: "harnessing human mobility data collected from privacy-preserving pedestrian counters located outside buildings"
  - [section 1.2]: "pedestrian sensors only capture partial pedestrian flows, making the utilization of traffic data for energy modeling more challenging"
  - [corpus]: Weak; corpus neighbors focus on load forecasting but not mobility proxies
- Break condition: If mobility patterns decouple from energy use (e.g., remote work), the proxy becomes unreliable

### Mechanism 3
- Claim: Online learning without continual safeguards degrades performance during OoD periods
- Mechanism: Simple online updates with one-step gradient descent accumulate error and drift away from stable patterns; continual methods regularize this drift
- Core assumption: The validation period (one month) is representative of OoD shifts
- Evidence anchors:
  - [abstract]: "In contrast to online learning, which suffers from 'catastrophic forgetting' as newly acquired knowledge often erases prior information"
  - [section 4.2]: "Online Learning (OL): ... model undergoes a gradient descent step for every new incoming data point"
  - [section 5.2]: "methods without Online Learning (OL) fail to generalize during out-of-distribution periods"
- Break condition: If OoD shifts are mild, the added complexity of continual learning may not be justified

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Explains why naive online learning fails when data distribution shifts
  - Quick check question: What happens to a model's performance on old data if it is only updated on new data without regularization?

- Concept: Time-series feature engineering (hour-of-day, day-of-week, moving averages)
  - Why needed here: These features capture cyclical patterns that are critical for load forecasting
  - Quick check question: How does adding a 24-hour periodic feature improve forecasting during non-standard periods like lockdowns?

- Concept: Continual learning architectures (dual memory networks)
  - Why needed here: Provides the theoretical basis for FSNet's fast/slow parameter separation
  - Quick check question: In a dual-memory continual learning setup, which parameter set is responsible for adapting to sudden distribution shifts?

## Architecture Onboarding

- Component map:
  Data ingestion -> Feature extraction (E, EM, ET, ETM) -> FSNet model (fast/slow paths) -> Forecast output -> Evaluation (MAE)
  Auxiliary mobility data pipeline: Pedestrian counters -> micro/macro averaging -> feature fusion
- Critical path:
  1. Warm-up training on first 3 months
  2. Continual adaptation on each incoming hour
  3. Output clamping between min/max of validation period
- Design tradeoffs:
  - More features (ETM) risk overfitting; mobility inclusion optional based on correlation
  - Simpler linear models (VAR+OL) can outperform deep models in some BCs
  - FSNet complexity justified by large OoD shifts but adds hyperparameter tuning burden
- Failure signatures:
  - Performance collapse during lockdowns -> likely missing continual adaptation
  - Overfitting to mobility -> high validation error, unstable feature importance
  - Slow convergence -> insufficient learning rate or poor initialization
- First 3 experiments:
  1. Run FSNet with E features only on BC 8, compare to CopyLastHour baseline
  2. Add mobility features (EM) and measure MAE change; check for overfitting
  3. Swap FSNet for VAR+OL on BC 11 to test simpler model competitiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the FSNet model in forecasting energy demand during non-COVID periods with gradual changes compared to sudden changes like the COVID-19 lockdowns?
- Basis in paper: [explicit] The paper focuses on the COVID-19 lockdowns as a case of out-of-distribution (OoD) data shifts but does not provide data or analysis for gradual changes.
- Why unresolved: The study specifically targets a sudden, drastic change in data distribution due to lockdowns, and there is no comparison or mention of gradual changes in energy demand patterns.
- What evidence would resolve it: Comparative analysis of FSNet performance on datasets with gradual changes in energy demand, such as seasonal variations or slow economic changes, versus sudden changes like the COVID-19 lockdowns.

### Open Question 2
- Question: What are the long-term effects of continual learning on the model's performance as new data continuously updates the model over extended periods?
- Basis in paper: [inferred] The paper discusses the benefits of continual learning during the COVID-19 lockdowns but does not address the long-term sustainability of model performance with continuous updates.
- Why unresolved: The study does not explore the effects of continual learning over a prolonged period, focusing instead on the immediate benefits during the pandemic.
- What evidence would resolve it: Longitudinal studies tracking the model's performance over several years, assessing accuracy, adaptability, and any potential degradation in performance due to continual learning.

### Open Question 3
- Question: How does the inclusion of additional auxiliary data, such as weather patterns or economic indicators, impact the forecasting accuracy of the FSNet model?
- Basis in paper: [explicit] The paper mentions the use of mobility and temperature data as auxiliary features but does not explore other potential data sources.
- Why unresolved: The study limits its auxiliary data to mobility and temperature, leaving the potential impact of other data sources unexplored.
- What evidence would resolve it: Experimental results incorporating various auxiliary data sources, such as weather patterns, economic indicators, or social media trends, to determine their impact on the model's forecasting accuracy.

## Limitations
- FSNet's specific architecture and update mechanisms are not fully specified, making exact replication difficult
- Mobility data proxy assumptions may not generalize to post-pandemic contexts where work patterns have permanently shifted
- The one-month validation period may not capture longer-term distributional drift effects

## Confidence
- Catastrophic forgetting claims: Medium
- Mobility feature effectiveness: Medium
- FSNet architecture benefits: Low

## Next Checks
1. Test FSNet performance on a longer post-lockdown period to assess whether continual learning maintains advantages as new patterns stabilize
2. Run ablation studies removing mobility features to quantify their necessity versus overfitting risk across different building types
3. Implement a simpler continual learning baseline (e.g., regularization-based) to isolate whether FSNet's dual-memory architecture provides unique benefits