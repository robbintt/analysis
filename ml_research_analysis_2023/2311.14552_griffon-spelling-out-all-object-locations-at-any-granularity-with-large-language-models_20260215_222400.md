---
ver: rpa2
title: 'Griffon: Spelling out All Object Locations at Any Granularity with Large Language
  Models'
arxiv_id: '2311.14552'
source_url: https://arxiv.org/abs/2311.14552
tags:
- object
- data
- arxiv
- objects
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fine-grained object localization
  in images based on free-form text inputs, a capability limited in current large
  vision-language models (LVLMs). The authors propose Griffon, a purely LVLM-based
  approach that overcomes this limitation without relying on specialized detection
  modules or visual expert models.
---

# Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models

## Quick Facts
- arXiv ID: 2311.14552
- Source URL: https://arxiv.org/abs/2311.14552
- Reference count: 40
- Primary result: State-of-the-art performance on RefCOCO series for referring expression comprehension without specialized detection modules

## Executive Summary
This paper addresses the challenge of fine-grained object localization in images based on free-form text inputs, a capability limited in current large vision-language models (LVLMs). The authors propose Griffon, a purely LVLM-based approach that overcomes this limitation without relying on specialized detection modules or visual expert models. Griffon introduces a unified input-output representation and a two-stage training pipeline that achieves state-of-the-art performance on referring expression comprehension benchmarks while approaching the capabilities of expert models like Faster RCNN on object detection tasks.

## Method Summary
Griffon employs a unified input-output representation using natural language coordinates (e.g., "label-[x1,y1,x2,y2]") and a progressive two-stage training pipeline. Stage I involves pre-training on a large-scale language-prompted localization dataset to build foundational multi-object perception capabilities. Stage II refines instruction comprehension through instruction tuning across four localization scenarios. The model uses a vision encoder (ViT-L/14 CLIP) combined with a language model (LLaMA2-13B) through a projection layer that maps visual features to the language embedding space.

## Key Results
- Achieves state-of-the-art performance on the RefCOCO series for referring expression comprehension
- Approaches the capabilities of expert models like Faster RCNN on the MSCOCO object detection benchmark
- Demonstrates accurate localization of objects at any granularity based on free-form text inputs
- Shows strong performance across all four localization scenarios: single object, multiple objects of same category, multiple objects of different categories, and referring expression comprehension

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LVLMs can achieve accurate object localization at any granularity when provided with a unified input-output representation that uses natural language coordinates
- Mechanism: By encoding object coordinates as normalized numerical characters within free-form text, the model leverages its existing language understanding capabilities to generate precise spatial information without requiring special tokens or external detection heads
- Core assumption: The LVLM's text generation ability can be extended to structured spatial outputs without architectural changes
- Evidence anchors: [abstract] "Griffon, a purely LVLM-based baseline, which does not require the introduction of any special tokens, expert models, or additional detection modules." [section] "we employ numerical characters for normalized coordinates and set the precision to 0.001."

### Mechanism 2
- Claim: A progressive two-stage training pipeline (pre-training → instruction tuning) enables LVLMs to handle complex multi-object localization scenarios
- Mechanism: Stage I pre-training on a large-scale language-prompted localization dataset builds foundational multi-object perception. Stage II instruction tuning refines comprehension of user intent across the four localization scenarios
- Core assumption: Scene understanding and instruction comprehension are separable learning phases that can be optimized sequentially
- Evidence anchors: [abstract] "The training of Griffon is strategically divided into two stages: Stage I focuses on the foundation pre-training... Stage II involves further instruction tuning." [section] "we propose the progressive two-stage training pipeline."

### Mechanism 3
- Claim: A training-free confidence scoring mechanism based on token-level conditional probabilities improves multi-object detection reliability
- Mechanism: By computing the geometric mean of label generation probability and coordinate localization probability for each predicted object, the model can rank detections and match them to ground truth more accurately
- Core assumption: The autoregressive generation probability correlates with detection confidence and can substitute for learned confidence scores
- Evidence anchors: [section] "We present a training-free multi-object confidence scoring mechanism... the final confidence score for a detected object is the geometric mean of the label score and localization score." [section] "With the confidence scores, the prediction with higher score, i.e. higher quality, will be matched with the ground truth."

## Foundational Learning

- Concept: Visual grounding and referring expression comprehension
  - Why needed here: Griffon must localize objects based on free-form text descriptions, which requires understanding the relationship between language and spatial regions in images
  - Quick check question: Can you explain the difference between localizing a single referent versus multiple objects of the same category?

- Concept: Object detection metrics (mAP, AP50, AP75)
  - Why needed here: Performance evaluation requires understanding precision-recall tradeoffs at different IoU thresholds and how they aggregate into mean average precision
  - Quick check question: What does an AP50 score of 0.8 indicate about the model's detection performance?

- Concept: Autoregressive sequence generation
  - Why needed here: Griffon generates multiple object predictions as a single text sequence, requiring understanding of how conditional probabilities accumulate across token positions
  - Quick check question: How does the conditional probability p(xa,j|Xv, Xins, Xa,<j) differ from the joint probability of the entire sequence?

## Architecture Onboarding

- Component map: Image → Visual encoder → Projection layer → LLM → Text output
- Critical path: Image → Visual features → Projection layer → LLM → Text output
  - Bottleneck: Projection layer dimensionality matching between visual and language spaces
  - Optimization target: End-to-end training with proper gradient flow through all components
- Design tradeoffs:
  - Resolution vs. computational cost: 448px input balances small object detection with efficiency
  - Coordinate precision vs. sequence length: 0.001 precision chosen to minimize localization error without excessive token generation
  - Unified representation vs. task-specific heads: Natural language coordinates avoid architectural complexity but may limit extreme precision requirements
- Failure signatures:
  - Low mAP but high AP50: Model generates boxes with correct category but poor localization precision
  - Missing small objects: Projection layer fails to preserve fine details from visual features
  - Repetitive predictions: Confidence scoring mechanism fails to distinguish duplicate detections
- First 3 experiments:
  1. Train with label-first vs. coordinate-first output order to determine optimal sequence generation strategy
  2. Evaluate confidence scoring mechanism by comparing ranked vs. random matching on multi-object predictions
  3. Test model performance on varying image resolutions (224, 448, 672) to find optimal trade-off point

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas warrant further investigation based on the results presented.

## Limitations

- Dataset construction details remain unspecified, including exact prompts and quality filtering criteria used to create the 6M pre-training data and 500K instruction-following data
- Critical hyperparameters including learning rate schedules, batch sizes, and optimization algorithms are not provided
- The model's generalization boundaries to out-of-distribution data (extreme aspect ratios, heavy occlusion, unusual object orientations) are not systematically evaluated

## Confidence

- High confidence: The core architectural innovation of using natural language coordinates for unified input-output representation
- Medium confidence: Claims about achieving state-of-the-art performance on RefCOCO series and approaching Faster RCNN capabilities on MSCOCO
- Low confidence: The assertion that Griffon "does not require the introduction of any special tokens, expert models, or additional detection modules" as the projection layer implicitly encodes domain knowledge

## Next Checks

1. **Ablation study on confidence scoring**: Implement the model without the geometric mean confidence scoring mechanism and measure the impact on multi-object detection performance, particularly focusing on whether the ranking-based matching actually improves IoU-based metrics

2. **Cross-dataset generalization test**: Evaluate Griffon on visual grounding datasets outside the MSCOCO family (e.g., Flickr30k Entities or ReferItGame) to assess whether the language-prompted pre-training generalizes beyond the specific distribution used during training

3. **Extreme resolution scaling**: Test model performance at both lower (224px) and higher (672-896px) resolutions to quantify the actual sweet spot and determine whether the 448px choice represents true optimality or a compromise between conflicting objectives