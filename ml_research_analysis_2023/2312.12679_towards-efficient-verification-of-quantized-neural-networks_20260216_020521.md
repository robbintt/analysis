---
ver: rpa2
title: Towards Efficient Verification of Quantized Neural Networks
arxiv_id: '2312.12679'
source_url: https://arxiv.org/abs/2312.12679
tags:
- neural
- networks
- verification
- quantized
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents EQV, an efficient verification framework for
  quantized neural networks (QNNs) using integer linear programming (ILP). The method
  precisely models quantization schemes used in PyTorch/TensorFlow, enabling exact
  verification while addressing scalability challenges.
---

# Towards Efficient Verification of Quantized Neural Networks

## Quick Facts
- arXiv ID: 2312.12679
- Source URL: https://arxiv.org/abs/2312.12679
- Reference count: 22
- Key outcome: EQV achieves up to 100× speedup over baseline ILP and solves 88-99% of MNIST verification instances within 30 minutes

## Executive Summary
This paper presents EQV, an efficient verification framework for quantized neural networks (QNNs) that addresses the challenge of scaling formal verification to larger networks while maintaining exactness. The framework combines three complementary approaches: an ILP-based exact method that precisely models PyTorch/TensorFlow quantization schemes, gradient-based heuristic search for finding counterexamples, and abstract interpretation for interval analysis with QNN-specific extensions. Experimental results demonstrate that EQV can verify networks twice as large as previous approaches while achieving significant speedups.

## Method Summary
EQV is a three-component verification framework that tackles the scalability challenges of QNN verification through complementary techniques. The first component uses integer linear programming (ILP) to provide exact verification by encoding QNN operations with linear constraints that precisely capture quantization behavior including rounding and clipping operations. The second component extends abstract interpretation with interval analysis to support round and clip operations, enabling sound but incomplete verification that can prove safety early or provide useful bounds for ILP solving. The third component employs gradient-based heuristic search with a rewriting trick for the non-differentiable round operation, allowing efficient counterexample finding. These components work together in a verification pipeline where faster incomplete methods are tried first, falling back to exact ILP when necessary.

## Key Results
- EQV achieves up to 100× speedup over baseline ILP on verification tasks
- The framework scales to networks twice as large as previous approaches
- On MNIST verification tasks, EQV solves 88-99% of instances within 30 minutes across different perturbation radii
- The framework demonstrates superior efficiency and scalability compared to state-of-the-art QNN verification tools

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ILP formulation precisely captures PyTorch/TensorFlow quantization schemes, enabling exact verification.
- Mechanism: The ILP encodes each quantized operation (rounding, clipping, ReLU) with linear constraints that exactly model integer arithmetic behavior, including quantization parameters and clipping bounds.
- Core assumption: The quantization scheme parameters (scale, zero point) are fixed constants during inference and can be treated as such in the ILP model.
- Evidence anchors: [abstract]: "Our baseline technique is based on integer linear programming which guarantees both soundness and completeness." [section]: "the main difference is that our encoding correctly models quantization schemes used in mainstream deep learning frameworks (e.g., PyTorch)."

### Mechanism 2
- Claim: Abstract interpretation with extended support for round and clip operations provides sound but incomplete verification that can accelerate solving.
- Mechanism: Interval analysis computes over-approximations of variable bounds, using two ReLU units to simulate clip operations and direct bound propagation for rounding, enabling early termination when safety is proven.
- Core assumption: Over-approximations remain tight enough to prove safety for many instances or provide useful bounds for ILP solving.
- Evidence anchors: [abstract]: "We then show how efficiency can be improved by utilizing gradient-based heuristic search methods and also bound-propagation techniques." [section]: "We extend existing abstract interpretation-based interval analysis methods to support QNNs"

### Mechanism 3
- Claim: Gradient-based heuristic search with a rewriting trick for round operations can efficiently find counterexamples.
- Mechanism: The rewriting trick y = Round(x) + x - x.detach() preserves the gradient ∂y/∂x = 1 while maintaining correct forward values, enabling standard gradient-based attacks on QNNs.
- Core assumption: The approximation error introduced by using x instead of Round(x) in gradient computation doesn't prevent finding actual counterexamples.
- Evidence anchors: [abstract]: "We design a rewriting trick for the non-differentiable round operation, which enables gradient-based analysis of QNNs." [section]: "if we compute the gradient as follows: ∂y/∂x = ∂y3/∂y2 * ∂y1/∂x... To resolve this problem, we use a trick to rewrite the Round operation... y = Round(x) + x - x.detach()"

## Foundational Learning

- Concept: Quantization schemes (scale and zero point parameters)
  - Why needed here: Understanding how real numbers map to integers is essential for correctly modeling QNN operations in ILP and analyzing their behavior.
  - Quick check question: Given a real value γ = 0.5, scale s = 0.2, and zero point z = 4, what is the quantized integer value q?

- Concept: Mixed Integer Linear Programming (MILP) vs Integer Linear Programming (ILP)
  - Why needed here: The distinction between MILP (with floating-point variables) and ILP (purely integer) affects solver performance and modeling choices for QNN verification.
  - Quick check question: Why does the paper avoid using floating-point variables in the ILP encoding, preferring pure integer variables instead?

- Concept: Abstract interpretation and interval analysis
  - Why needed here: Abstract interpretation provides sound but incomplete verification by over-approximating network behavior, which can prove safety without exhaustive search.
  - Quick check question: How does abstract interpretation handle the clip operation in QNNs, given that standard techniques primarily target ReLU operations?

## Architecture Onboarding

- Component map: Input perturbation bounds → Abstract interpretation (early termination if possible) → Gradient-based search (counterexample finding) → ILP verification (exact proof or counterexample) → Result output
- Critical path: The framework first attempts abstract interpretation for fast safety proofs, then uses gradient-based search to find counterexamples, and finally falls back to exact ILP verification when needed.
- Design tradeoffs: ILP provides exact results but may be slow; abstract interpretation is fast but incomplete; gradient-based search is fast for finding counterexamples but not sound. The framework balances these tradeoffs by using faster methods first and falling back to exact methods when needed.
- Failure signatures: ILP timeouts on large networks, abstract interpretation producing overly conservative bounds, gradient-based search failing to find counterexamples when they exist. Each component has distinct failure modes that the framework must handle gracefully.
- First 3 experiments:
  1. Verify a small fully connected network (FC1-100) with r=4 perturbation radius using all three methods to observe the verification pipeline behavior.
  2. Test the abstract interpretation component alone on networks with varying perturbation radii to understand its effectiveness at different scales.
  3. Benchmark the gradient-based heuristic search on networks where abstract interpretation fails to prove safety, measuring counterexample finding success rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed EQV framework be extended to verify other properties beyond adversarial robustness, such as fairness or interpretability in quantized neural networks?
- Basis in paper: [explicit] The authors mention that "Although we focus on verifying adversarial robustness, our method could be generalized to verify other properties of QNNs."
- Why unresolved: The paper only demonstrates verification of adversarial robustness and does not explore other potential properties that could be verified using the EQV framework.
- What evidence would resolve it: Experiments showing successful verification of properties like fairness constraints or interpretability requirements in QNNs using EQV.

### Open Question 2
- Question: How does the precision loss during quantization affect the formal verification guarantees provided by EQV, and can the framework account for this uncertainty?
- Basis in paper: [explicit] The authors note "it would be interesting to formally analyze the difference or equivalence between the original networks and the quantized neural networks or to formally quantify the precision loss due to the quantization process."
- Why unresolved: The paper does not address how quantization-induced precision loss might impact the soundness and completeness guarantees of formal verification, or how to model this uncertainty in the verification process.
- What evidence would resolve it: Theoretical analysis and experimental results showing how quantization error bounds can be incorporated into EQV's verification framework to maintain formal guarantees.

### Open Question 3
- Question: What is the relationship between the computational complexity of verifying quantized neural networks versus their real-valued counterparts, and does this difference scale with network size?
- Basis in paper: [inferred] The authors observe that "the verification complexity of QNN might be greater than verifying a real-valued network" based on their experiments comparing robustness curves.
- Why unresolved: While the paper shows some empirical evidence suggesting QNN verification is more complex, it does not provide a formal complexity analysis or investigate how this complexity gap scales with network size.
- What evidence would resolve it: Formal complexity proofs comparing QNN verification to real-valued network verification, and extensive experiments showing how the complexity gap evolves across different network architectures and sizes.

## Limitations

- The verification framework relies heavily on the assumption that quantization parameters remain fixed during inference, which may not hold in all deployment scenarios.
- The gradient-based heuristic search component's effectiveness depends on the approximation error introduced by the round operation rewriting trick, which could prevent finding actual counterexamples in some cases.
- The abstract interpretation component's precision is limited by interval analysis conservatism, potentially leading to false negatives where safety cannot be proven despite the property holding.

## Confidence

**High Confidence:** The ILP formulation's ability to exactly model PyTorch/TensorFlow quantization schemes (Mechanism 1) is well-supported by the described constraint encoding and the distinction from previous work.

**Medium Confidence:** The gradient-based heuristic search effectiveness (Mechanism 3) is theoretically sound but practical performance depends on approximation error behavior that requires empirical validation.

**Medium Confidence:** The abstract interpretation extension for QNNs (Mechanism 2) is reasonable but the actual precision gains over standard approaches need experimental validation, as the corpus provides no supporting evidence.

## Next Checks

1. **Empirical Error Analysis:** Measure the approximation error introduced by the round operation rewriting trick across different networks and input distributions to quantify its impact on gradient-based counterexample finding.

2. **Scalability Benchmark:** Test the framework on networks larger than those used in the paper (e.g., ResNet-18 on CIFAR-10) to validate claimed scalability improvements over baseline ILP approaches.

3. **Quantization Parameter Sensitivity:** Evaluate the verification framework's performance when quantization parameters are not fixed but vary according to deployment constraints, testing the robustness of the ILP encoding assumptions.