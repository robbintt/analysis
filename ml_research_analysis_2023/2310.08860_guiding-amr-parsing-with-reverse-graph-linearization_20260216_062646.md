---
ver: rpa2
title: Guiding AMR Parsing with Reverse Graph Linearization
arxiv_id: '2310.08860'
source_url: https://arxiv.org/abs/2310.08860
tags:
- graph
- linearization
- parsing
- training
- reverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses structure loss accumulation in sequence-to-sequence
  AMR parsing, where F1 scores for nodes and edges decrease significantly as decoding
  progresses. The proposed Reverse Graph Linearization (RGL) framework introduces
  reverse linearization orders and a two-pass self-distillation mechanism to guide
  model training.
---

# Guiding AMR Parsing with Reverse Graph Linearization

## Quick Facts
- arXiv ID: 2310.08860
- Source URL: https://arxiv.org/abs/2310.08860
- Reference count: 20
- Key outcome: RGL achieves SOTA performance, outperforming previous best by 0.8 Smatch points on AMR 2.0 and 0.5 on AMR 3.0

## Executive Summary
This paper addresses structure loss accumulation in sequence-to-sequence AMR parsing, where F1 scores for nodes and edges decrease significantly as decoding progresses. The authors propose Reverse Graph Linearization (RGL), a framework that introduces reverse linearization orders and a two-pass self-distillation mechanism to guide model training. By incorporating reverse linearization through a mixed decoder with gated dual cross-attention, RGL significantly mitigates structure loss accumulation. The method achieves state-of-the-art performance on both AMR 2.0 and AMR 3.0 datasets.

## Method Summary
RGL defines both default and reverse linearization orders of an AMR graph, where structures at the back of the default order appear at the front of the reversed order and vice versa. The framework incorporates an additional encoder to integrate the reverse linearization graph and replaces the original transformer decoder with a mixed decoder that utilizes gated dual cross-attention. A two-pass self-distillation mechanism is employed to prevent overfitting to gold reverse linearization while utilizing it to guide model training. The teacher model takes gold linearization as input, while the student model takes silver linearization, with cross-entropy losses and KL divergence computed between their outputs.

## Key Results
- RGL achieves 94.2 Smatch on AMR 2.0 (0.8 points higher than previous SOTA)
- RGL achieves 92.7 Smatch on AMR 3.0 (0.5 points higher than previous SOTA)
- Fine-grained scores show consistent improvements across all metrics including NoWSD, Wiki, Concepts, NER, Negations, Unlabel, Reentrancy, and SRL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structure loss accumulation occurs because errors compound during autoregressive decoding, with later predictions being more sensitive to earlier mistakes
- Mechanism: Errors in early predictions propagate to subsequent predictions, degrading accuracy of nodes and edges generated later in the sequence
- Core assumption: Linearization order determines which structures appear later and are more vulnerable to error accumulation
- Evidence anchors:
  - [abstract] "F1-score for nodes and edges decoded later compared to those decoded earlier"
  - [section 2.1] "total training loss computed by cross-entropy loss"
  - [corpus] Weak - general AMR parsing papers without specific quantitative analysis
- Break condition: If perfect prediction were possible for each token given previous tokens, structure loss accumulation would vanish

### Mechanism 2
- Claim: Reverse linearization mitigates structure loss by introducing complementary perspectives
- Mechanism: RGL defines default and reverse linearization orders, incorporating reverse linearization through mixed decoder with gated dual cross-attention
- Core assumption: Structures at end of default order appear at front of reversed order, allowing model to learn from both perspectives
- Evidence anchors:
  - [abstract] "most structures at back part of default order appear at front part of reversed order"
  - [section 3.3] "mixed decoder that utilizes gated dual cross-attention"
  - [corpus] Weak - general AMR parsing mentions without detailed mechanism evidence
- Break condition: If reverse linearization provides no complementary information, RGL benefits would be minimal

### Mechanism 3
- Claim: Two-pass self-distillation prevents overfitting to gold reverse linearization
- Mechanism: Teacher takes gold linearization, student takes silver linearization; cross-entropy losses and KL divergence computed between outputs
- Core assumption: Silver linearization provides useful information if properly balanced with gold through self-distillation
- Evidence anchors:
  - [section 3.1] "two-pass self-distillation mechanism to prevent model from overfitting"
  - [section 3.3] "KL divergence LKL to guide student with teacher's output"
  - [corpus] Weak - general self-distillation mentions without specific two-pass mechanism
- Break condition: If silver linearization is too noisy or loss scheduler poorly tuned, self-distillation could degrade performance

## Foundational Learning

- Concept: Autoregressive decoding and error propagation
  - Why needed here: Understanding how errors compound during sequential generation is crucial for grasping structure loss accumulation problem
  - Quick check question: Why does generating later tokens in sequence-to-sequence models tend to have lower accuracy than earlier tokens?

- Concept: Graph linearization and traversal orders
  - Why needed here: RGL relies on defining and leveraging different linearization orders (L2R and R2L) to mitigate structure loss accumulation
  - Quick check question: How does R2L linearization order differ from standard L2R order, and why is this difference important for RGL?

- Concept: Self-distillation and knowledge transfer
  - Why needed here: Two-pass self-distillation mechanism uses teacher-student framework to balance gold and silver linearization during training
  - Quick check question: What is the purpose of using both gold and silver linearization in self-distillation process, and how does this prevent overfitting?

## Architecture Onboarding

- Component map:
  Sentence encoder -> Graph encoder -> Mixed decoder with gated dual cross-attention -> Output generation

- Critical path:
  1. Encode input sentence using sentence encoder
  2. Generate reverse linearization using pre-trained R2L parser
  3. Encode reverse linearization using graph encoder
  4. Use mixed decoder with gated dual cross-attention to generate output
  5. Apply two-pass self-distillation during training

- Design tradeoffs:
  - Additional R2L parser increases inference time but improves accuracy
  - Mixed decoder with gated dual cross-attention adds complexity but allows better integration of sentence and graph information
  - Silver linearization during training adds noise but improves robustness to errors during inference

- Failure signatures:
  - Gated weights consistently favoring one encoder over other suggests ineffective integration of information sources
  - Poorly tuned loss scheduler could cause overfitting to either gold or silver linearization

- First 3 experiments:
  1. Compare F1 scores of node and relation predictions at different positions to verify structure loss accumulation in baseline
  2. Replace mixed decoder with standard transformer decoder to assess impact of gated dual cross-attention
  3. Remove self-distillation mechanism to evaluate contribution to preventing overfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance vary with different loss scheduler parameters (k1 and k2) in Equation 9?
- Basis in paper: [explicit] Paper mentions exponential decay with k1 and k2 as hyperparameters but provides no ablation studies
- Why unresolved: Authors set k1=0.8 and k2=0.2 without further tuning or experimentation
- What evidence would resolve it: Comprehensive ablation study testing different k1 and k2 values on validation sets

### Open Question 2
- Question: What is impact of using different pretrained models (other than AMRBART) as base for RGL?
- Basis in paper: [inferred] Paper uses AMRBART but does not explore other pretrained models like T5 or BART
- Why unresolved: Authors only experimented with AMRBART initialization
- What evidence would resolve it: Experiments initializing RGL with different pretrained models and comparing performance

### Open Question 3
- Question: How does RGL framework perform on AMR parsing for low-resource languages or domains?
- Basis in paper: [inferred] Paper focuses on English AMR datasets without exploring other languages or domains
- Why unresolved: Authors did not test RGL on AMR datasets for other languages or specialized domains
- What evidence would resolve it: Experiments applying RGL to AMR datasets in different languages or domains

## Limitations

- Limited quantitative analysis of error propagation dynamics - only abstract claims about lower F1 scores for later predictions
- Complex architectural additions (R2L parser, mixed decoder, self-distillation) raise questions about whether simpler solutions could achieve similar results
- Performance claims rely on SOTA comparisons without comprehensive ablation studies to isolate contributions of individual components

## Confidence

- **High Confidence**: Structure loss accumulation exists in AMR parsing (supported by multiple AMR parsing papers showing sequential generation challenges)
- **Medium Confidence**: General approach of using reverse linearization to mitigate error accumulation (supported by graph linearization literature)
- **Low Confidence**: Specific RGL mechanism with gated dual cross-attention and two-pass self-distillation (limited direct evidence, complex mechanism not well-validated)

## Next Checks

1. **Quantitative Error Propagation Analysis**: Replicate analysis of F1 scores for nodes and edges at different positions in sequence for both baseline and RGL models to verify claimed structure loss accumulation and its mitigation.

2. **Ablation Study of RGL Components**: Systematically remove each key component (reverse linearization, gated dual cross-attention, two-pass self-distillation) to quantify their individual contributions to overall performance improvement.

3. **Generalization to Other Languages**: Test RGL framework on multilingual AMR datasets or other structured prediction tasks with sequential generation to assess whether reverse linearization approach generalizes beyond English AMR parsing.