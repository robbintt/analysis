---
ver: rpa2
title: Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts
arxiv_id: '2311.11385'
source_url: https://arxiv.org/abs/2311.11385
tags:
- tasks
- experts
- moore
- learning
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MOORE, a novel approach for multi-task reinforcement
  learning that learns orthogonal representations to promote diversity among a mixture
  of experts. The method formulates the problem as a constrained optimization on the
  Stiefel manifold and uses the Gram-Schmidt process to orthogonalize the representations.
---

# Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts

## Quick Facts
- arXiv ID: 2311.11385
- Source URL: https://arxiv.org/abs/2311.11385
- Reference count: 40
- Primary result: Introduces MOORE, a method using orthogonal representations for multi-task RL that achieves state-of-the-art results on MiniGrid and MetaWorld benchmarks.

## Executive Summary
This paper introduces MOORE (Mixture Of Orthogonal Experts), a novel approach for multi-task reinforcement learning that learns orthogonal representations to promote diversity among a mixture of experts. The method formulates the problem as a constrained optimization on the Stiefel manifold and uses the Gram-Schmidt process to orthogonalize the representations. MOORE is evaluated on two challenging MTRL benchmarks, MiniGrid and MetaWorld, where it surpasses related baselines and establishes a new state-of-the-art result on MetaWorld MT10-rand and MT50-rand. The approach demonstrates superior sample efficiency, better transfer learning capabilities, and interpretable representations compared to existing methods.

## Method Summary
MOORE learns a universal policy that maximizes the expected accumulated discounted return across multiple tasks. The approach uses a mixture of experts to generate representations, which are then orthogonalized using the Gram-Schmidt process. These orthogonal representations belong to the Stiefel manifold, ensuring stable optimization. Task-specific weights are learned to interpolate over the orthogonal basis, creating task representations that are used by the RL algorithm. The method is evaluated using PPO for MiniGrid and SAC for MetaWorld, with performance measured by average return and success rate across tasks.

## Key Results
- MOORE surpasses related baselines on MiniGrid and MetaWorld benchmarks
- Establishes new state-of-the-art results on MetaWorld MT10-rand and MT50-rand
- Demonstrates superior sample efficiency compared to existing methods
- Shows better transfer learning capabilities with frozen expert representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonalizing representations via Gram-Schmidt prevents representation collapse and improves sample efficiency.
- Mechanism: The Gram-Schmidt process enforces that each expert's representation lies in a direction orthogonal to all previous ones, ensuring diversity. This diversity allows the policy to access richer subspaces when interpolating task-relevant features.
- Core assumption: The shared task subspace can be spanned by a small set of orthogonal vectors, and that diversity across these vectors improves policy expressiveness.
- Evidence anchors:
  - [abstract] "leverages a Gram-Schmidt process to shape a shared subspace of representations generated by a mixture of experts"
  - [section 4.2] "we apply GS process to map the generated representations by the mixture of experts Us = hϕ(s) to a set of orthonormal representations Vs = GS(Us)"
- Break condition: If tasks share very little common structure, forcing orthogonality may hurt rather than help, as it constrains representational capacity unnecessarily.

### Mechanism 2
- Claim: The Stiefel manifold constraint ensures stable optimization and avoids interference between tasks.
- Mechanism: By constraining the shared representation matrix Vs to lie on the Stiefel manifold (VsT Vs = I), gradients do not explode and representations remain well-conditioned, reducing catastrophic interference.
- Core assumption: Optimization on the Stiefel manifold leads to more stable learning dynamics than unconstrained embeddings.
- Evidence anchors:
  - [section 4.1] "We define the orthonormal representations of state s as a matrix Vs = [v1, ..., vk] ∈ Rd×k where vi ∈ Rd, ∀i ≤ k...the orthonormal representations Vs belong to a topological space known as Stiefel manifold"
- Break condition: If the manifold constraint is too restrictive for the task distribution, learning may stall or fail to converge.

### Mechanism 3
- Claim: Interpolating task-specific weights over orthogonal bases enables better transfer and zero-shot generalization.
- Mechanism: Each task encodes its preferences as a weight vector wc over the orthogonal basis Vs. Because the basis is shared and diverse, unseen tasks can interpolate from existing weights, enabling efficient transfer.
- Core assumption: Task differences can be modeled as linear combinations over a shared orthogonal basis.
- Evidence anchors:
  - [section 4.2] "we train a task encoder to produce the task-specific weights wc ∈ Rk given task information...The interpolated representation vc captures the relevant components of the task that can be utilized by the RL algorithm"
- Break condition: If task differences are nonlinear and cannot be captured by linear interpolation over orthogonal bases, performance will degrade.

## Foundational Learning

- Concept: Gram-Schmidt orthogonalization process
  - Why needed here: Ensures that each expert's representation is independent, preventing redundancy and improving representational capacity.
  - Quick check question: Given three vectors u1, u2, u3 in R³, what is the first step to produce an orthogonal set using Gram-Schmidt?
- Concept: Stiefel manifold definition and properties
  - Why needed here: The constraint VsT Vs = I defines the manifold on which the orthonormal representations must live, guaranteeing numerical stability.
  - Quick check question: What condition must a matrix satisfy to belong to the Stiefel manifold Vk(Rd)?
- Concept: Multi-Task RL contextual MDP formulation
  - Why needed here: Provides the mathematical framework to map states to task-specific representations in a shared orthogonal subspace.
  - Quick check question: In a Block Contextual MDP, what is the role of the mapping function M′(c)?

## Architecture Onboarding

- Component map:
  State encoder (CNN or MLP) → Mixture of experts (k CNNs/MLPs) → Gram-Schmidt orthogonalization → Task encoder (linear layer) → Weight combination → Output head (policy/value)
- Critical path:
  1. State → Experts → Orthogonal basis
  2. Task ID → Weights
  3. Basis × Weights → Task representation
  4. Task representation → Policy/value output
- Design tradeoffs:
  - More experts → higher representational capacity but quadratic cost in Gram-Schmidt
  - Hard orthogonality vs. soft regularization: hard gives guaranteed diversity, soft requires hyperparameter tuning
  - Multi-head vs. single-head: multi-head easier transfer, single-head fewer parameters
- Failure signatures:
  - Loss explosion or NaNs → Gram-Schmidt numerical instability or weight magnitude issues
  - Slow convergence or plateau → Too many/too few experts, or poor task encoder initialization
  - Transfer fails → Experts not diverse enough, or task encoder overfits to base tasks
- First 3 experiments:
  1. Run MOORE vs MOE on a simple MiniGrid MT3 scenario; measure sample efficiency and representation cosine similarity.
  2. Evaluate Gram-Schmidt vs cosine similarity regularization on MT5; compare orthogonality metrics and task success rates.
  3. Transfer experts from MT3 to MT5; compare transfer-MOORE vs MOORE-trained-from-scratch on novel tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the initial expert for the Gram-Schmidt process affect the performance and learned representations in MOORE?
- Basis in paper: [explicit] The paper mentions an ablation study where different initial experts are selected for the Gram-Schmidt process, but it only shows comparable performance across different initializations without further analysis.
- Why unresolved: The study lacks a deeper investigation into the impact of this choice on representation diversity, convergence speed, or final performance across various tasks.
- What evidence would resolve it: A comprehensive ablation study varying the initial expert across different task sets and measuring performance, diversity metrics, and convergence patterns would clarify the sensitivity to this choice.

### Open Question 2
- Question: Can MOORE be adapted to dynamically select a subset of orthogonal experts during inference to reduce computational complexity without significant performance loss?
- Basis in paper: [inferred] The paper discusses the computational complexity of MOORE being O(k² × d) and mentions a potential trade-off between representation capacity and time complexity, but does not explore dynamic expert selection.
- Why unresolved: While the authors acknowledge the computational cost, they do not propose or evaluate methods for selecting a subset of experts dynamically, which could be crucial for real-time applications.
- What evidence would resolve it: Experiments comparing MOORE with and without dynamic expert selection across various task complexities and real-time constraints would demonstrate the feasibility and impact of such an adaptation.

### Open Question 3
- Question: How does the orthogonality constraint in MOORE compare to other regularization techniques, such as cosine similarity loss, in terms of learning diverse and effective representations?
- Basis in paper: [explicit] The paper includes an ablation study comparing MOORE to a cosine similarity regularization approach, showing MOORE's superior performance, but does not explore other regularization methods or provide a detailed analysis of why orthogonality is more effective.
- Why unresolved: The comparison is limited to one alternative method, and the underlying reasons for orthogonality's effectiveness are not thoroughly investigated.
- What evidence would resolve it: A broader comparison with various regularization techniques, along with an analysis of the learned representations' diversity and task-specific performance, would clarify the advantages of the orthogonality constraint.

### Open Question 4
- Question: What is the impact of the number of experts (k) on the performance and sample efficiency of MOORE in large-scale multi-task settings?
- Basis in paper: [explicit] The paper conducts an ablation study on the effect of changing the number of experts in MiniGrid, showing that MOORE benefits from more experts compared to MOE, but this study is limited to a small-scale setting.
- Why unresolved: The study does not explore the scalability of MOORE with a large number of experts or its performance in extensive multi-task environments like MetaWorld MT50-rand.
- What evidence would resolve it: Scaling experiments varying the number of experts in large-scale multi-task environments, measuring performance, convergence speed, and computational requirements, would elucidate the optimal number of experts for different scenarios.

## Limitations

- Limited ablation studies on the orthogonalization mechanism itself
- Computational complexity scales quadratically with number of experts
- Transfer learning evaluated only on in-distribution task variations
- Empirical validation relies on two specific benchmarks without broader domain testing

## Confidence

- Claim: Gram-Schmidt orthogonalization prevents representation collapse - Medium
- Claim: Stiefel manifold constraint ensures stable optimization - Medium
- Claim: Orthogonal bases enable better transfer learning - Medium
- Claim: MOORE achieves state-of-the-art results - High (empirical evidence provided)

## Next Checks

1. **Orthogonality ablation**: Replace Gram-Schmidt with a soft orthogonality penalty (e.g., cosine similarity loss) and compare sample efficiency and representation diversity metrics.
2. **Scaling study**: Measure compute and memory scaling as the number of experts increases from 4 to 32 on MetaWorld MT50.
3. **Distribution shift test**: Transfer frozen experts to qualitatively different tasks (e.g., from manipulation to navigation) to assess true generalization.