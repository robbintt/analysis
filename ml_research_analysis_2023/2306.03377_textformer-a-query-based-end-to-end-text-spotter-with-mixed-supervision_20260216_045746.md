---
ver: rpa2
title: 'TextFormer: A Query-based End-to-End Text Spotter with Mixed Supervision'
arxiv_id: '2306.03377'
source_url: https://arxiv.org/abs/2306.03377
tags:
- text
- recognition
- detection
- end-to-end
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes TextFormer, a query-based end-to-end text spotter
  using Transformer architecture. The method addresses the limitations of existing
  text spotters that rely on RoI operations and complex post-processing by introducing
  a multi-task modeling approach.
---

# TextFormer: A Query-based End-to-End Text Spotter with Mixed Supervision

## Quick Facts
- arXiv ID: 2306.03377
- Source URL: https://arxiv.org/abs/2306.03377
- Authors: 
- Reference count: 40
- This paper proposes TextFormer, a query-based end-to-end text spotter using Transformer architecture. The method addresses the limitations of existing text spotters that rely on RoI operations and complex post-processing by introducing a multi-task modeling approach. TextFormer uses text queries to represent each possible text instance and employs an Adaptive Global aGgregation (AGG) module to extract features from different orientations for reading arbitrarily-shaped texts. The method also introduces mixed supervision, utilizing a mixture of weak annotations and full labels to improve text detection and end-to-end text spotting results. Experiments on various benchmarks demonstrate the superiority of the proposed method, achieving state-of-the-art performance on text detection and end-to-end text spotting tasks. Specifically, on the TDA-ReCTS dataset, TextFormer surpasses the state-of-the-art method by 13.2% in terms of 1-NED.

## Executive Summary
This paper introduces TextFormer, a query-based end-to-end text spotter that addresses limitations in existing text spotting methods that rely on RoI operations and complex post-processing. The method uses a Transformer architecture with text queries to represent each possible text instance, enabling joint semantic understanding for multi-task modeling. TextFormer introduces an Adaptive Global aGgregation (AGG) module for reading arbitrarily-shaped texts and employs mixed supervision with weak annotations to improve performance. The method achieves state-of-the-art results on multiple benchmarks, particularly excelling on the TDA-ReCTS dataset with a 13.2% improvement in 1-NED.

## Method Summary
TextFormer is a query-based end-to-end text spotter that uses a Transformer architecture with text queries to represent each possible text instance. The method consists of an image encoder, a text decoder, and multi-task heads for classification, segmentation, and recognition. The key innovation is the Adaptive Global aGgregation (AGG) module, which extracts features from different orientations to read arbitrarily-shaped texts without spatial rectification. The method also introduces mixed supervision, utilizing a mixture of weak annotations and full labels to improve training efficiency and performance. TextFormer shares semantic features globally via text queries, allowing the classification, segmentation, and recognition branches to be trained jointly without RoI operations.

## Key Results
- Achieves state-of-the-art performance on text detection and end-to-end text spotting tasks
- Surpasses the state-of-the-art method by 13.2% in terms of 1-NED on the TDA-ReCTS dataset
- Demonstrates effectiveness of query-based multi-task modeling compared to RoI-based approaches
- Shows improved performance using mixed supervision with weak annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TextFormer eliminates the performance bottleneck of RoI-based feature extraction by using query-based multi-task modeling
- Mechanism: Instead of using RoI operations to crop local features for detection and recognition separately, TextFormer shares semantic features globally via text queries, allowing classification, segmentation, and recognition branches to be trained jointly
- Core assumption: Text detection and recognition tasks can benefit from shared semantic features rather than isolated feature extraction
- Evidence anchors:
  - [abstract] "using query embedding per text instance, TextFormer builds upon an image encoder and a text decoder to learn a joint semantic understanding for multi-task modeling"
  - [section 2.1.1] "these methods only share shallow backbone features and adopt local RoI features for different tasks"
- Break condition: If text instances are too densely packed or have extreme aspect ratios, shared semantic features may lead to feature interference between instances

### Mechanism 2
- Claim: Mixed supervision improves co-optimization between detection and recognition by leveraging weak annotations
- Mechanism: The method uses a mixture of weak annotations (single transcription per image) and full annotations to provide sufficient training data for recognition while maintaining detection quality
- Core assumption: Recognition requires significantly more training data than detection, and weak annotations can supplement full annotations without degrading detection performance
- Evidence anchors:
  - [abstract] "potential corpus information is utilized from weak annotations to full labels through mixed supervision"
  - [section 3.6] "we present mixed supervision for training an end-to-end text spotter. It utilizes a mixture of weak annotations and full annotations"
- Break condition: If weak annotations are too ambiguous or inconsistent, the recognition branch may learn incorrect associations

### Mechanism 3
- Claim: Adaptive Global Aggregation (AGG) module enables reading arbitrarily-shaped texts without spatial rectification
- Mechanism: AGG extracts features in both horizontal and vertical orientations from shared semantic features, then concatenates them with direction embeddings to provide sequential features for recognition
- Core assumption: Text reading order follows a linear pattern that can be captured by aggregating features along orthogonal directions
- Evidence anchors:
  - [section 3.3.1] "we aggregate the global feature map in both horizontal and vertical orientations. These aggregated features are then concatenated and used for scene text recognition"
  - [section 4.5.3] "the model with AGG module outperforms it by 6% in 'General' metric"
- Break condition: If text has complex curved shapes that don't align well with horizontal/vertical directions, AGG may lose important spatial information

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: TextFormer uses Transformer encoder-decoder architecture for joint semantic understanding and query-based modeling
  - Quick check question: What is the key difference between self-attention and cross-attention in Transformer architectures?

- Concept: Multi-task learning and shared feature representations
  - Why needed here: TextFormer jointly trains classification, segmentation, and recognition branches using shared semantic features from the text decoder
  - Quick check question: How does joint training of multiple tasks affect the gradient flow compared to separate training?

- Concept: Weak supervision and semi-supervised learning
  - Why needed here: Mixed supervision combines weak annotations (single transcription per image) with full annotations to address data imbalance between detection and recognition
  - Quick check question: What are the main challenges in designing loss functions for mixed supervision scenarios?

## Architecture Onboarding

- Component map:
  - Image encoder (ResNet-50 + FPN) → Multi-scale feature extraction
  - Transformer encoder → Semantic feature refinement with deformable attention
  - Text decoder → Text query processing with masked attention
  - Multi-task heads → Classification, segmentation, and recognition branches
  - AGG module → Global feature extraction for recognition
  - Mixed supervision → Combined training with weak and full annotations

- Critical path: Image → Encoder → Decoder → Multi-task heads → Final predictions
  - The AGG module sits within the recognition branch path
  - Mixed supervision affects the loss computation across all branches

- Design tradeoffs:
  - Query-based vs RoI-based: Queries eliminate RoI operations but require careful matching during training
  - Global vs local features: AGG uses global features for recognition but may lose local spatial details
  - Weak vs full supervision: Mixed supervision reduces annotation cost but requires careful loss weighting

- Failure signatures:
  - Poor detection performance: May indicate issues with semantic feature sharing or query matching
  - Recognition errors: Could suggest AGG module isn't capturing correct sequential patterns
  - Training instability: May occur when mixing weak and full annotations without proper loss balancing

- First 3 experiments:
  1. Verify query-based modeling works by comparing detection performance with and without shared semantic features
  2. Test AGG module effectiveness by comparing recognition accuracy using AGG vs traditional RoI-based features
  3. Validate mixed supervision by training with only full annotations, only weak annotations, and mixed annotations to measure performance trade-offs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the AGG module's performance vary with different orientations (horizontal vs vertical) for different text shapes and languages?
- Basis in paper: [explicit] The AGG module extracts features from both horizontal and vertical orientations, and the paper shows that AGG outperforms Masked RoI, but doesn't provide detailed analysis of orientation-specific performance.
- Why unresolved: The paper only shows overall performance improvements and doesn't break down the effectiveness of horizontal vs vertical feature extraction for different text orientations and languages.
- What evidence would resolve it: Detailed ablation studies comparing AGGh, AGGv, and AGG performance across various text shapes (horizontal, vertical, curved) and languages (English, Chinese).

### Open Question 2
- Question: What is the optimal balance between weak and full annotations for different text spotting scenarios?
- Basis in paper: [explicit] The paper introduces mixed supervision but only provides results for two specific configurations (using weak texts with full annotations vs standard full annotations).
- Why unresolved: The paper doesn't explore different ratios of weak to full annotations or analyze how this balance affects performance across different datasets and text types.
- What evidence would resolve it: Systematic experiments varying the proportion of weakly vs fully annotated data and analyzing the impact on detection and recognition performance across multiple benchmarks.

### Open Question 3
- Question: How does the number of text queries affect performance in scenes with varying text density?
- Basis in paper: [explicit] The paper uses 50 text queries but doesn't explore how query number affects performance in different scenarios.
- Why unresolved: The paper doesn't provide analysis of how query number impacts performance in scenes with different numbers of text instances or text densities.
- What evidence would resolve it: Experiments varying the number of text queries and measuring performance on datasets with different text densities and numbers of text instances per image.

## Limitations
- Architectural complexity makes it difficult to isolate which component contributes most to performance gains
- Effectiveness depends heavily on quality and representativeness of weak annotations
- Limited evidence of performance on extreme text orientations and challenging scenarios

## Confidence

**High Confidence** (Evidence strongly supports):
- The query-based Transformer architecture with shared semantic features is effective for multi-task modeling
- The AGG module improves recognition performance on arbitrarily-shaped texts
- Mixed supervision provides benefits for end-to-end text spotting when properly implemented

**Medium Confidence** (Evidence suggests but needs more validation):
- The 13.2% improvement on TDA-ReCTS represents state-of-the-art performance
- The method generalizes well across different languages and text orientations
- The elimination of RoI operations is the primary driver of performance gains

**Low Confidence** (Claims not well-supported by evidence):
- The specific contribution of each novel component can be clearly quantified
- The method will maintain performance improvements with different weak annotation qualities
- The AGG module's global aggregation approach works equally well for all text shapes

## Next Checks
1. **Component ablation study**: Systematically disable each novel component (query-based modeling, AGG module, mixed supervision) individually to quantify their independent contributions to overall performance.

2. **Weak annotation robustness test**: Vary the quality and quantity of weak annotations systematically (e.g., random text transcriptions, partial annotations, noisy labels) to measure how robust the mixed supervision approach is to annotation quality variations.

3. **Cross-domain generalization evaluation**: Test the method on datasets with extreme text orientations (vertical, curved, multi-oriented) and challenging conditions (heavy occlusions, low resolution) to validate claims about handling arbitrarily-shaped texts across diverse scenarios.