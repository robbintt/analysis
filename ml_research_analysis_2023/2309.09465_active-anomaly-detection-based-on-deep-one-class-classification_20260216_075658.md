---
ver: rpa2
title: Active anomaly detection based on deep one-class classification
arxiv_id: '2309.09465'
source_url: https://arxiv.org/abs/2309.09465
tags:
- uni00000013
- samples
- uni00000057
- uni00000044
- uni00000056
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an active learning approach for deep one-class
  classification anomaly detection that addresses the problems of query strategy and
  semi-supervised learning. The authors propose an uncertainty sampling method using
  an adaptive boundary that iteratively adjusts based on the ratio of abnormal samples
  queried, eliminating the need for explicit decision boundary hyper-parameters.
---

# Active anomaly detection based on deep one-class classification

## Quick Facts
- arXiv ID: 2309.09465
- Source URL: https://arxiv.org/abs/2309.09465
- Reference count: 27
- Key outcome: Active learning approach combining adaptive boundary query strategy with NCE-based semi-supervised learning achieves state-of-the-art results on seven anomaly detection datasets

## Executive Summary
This paper presents an active learning framework for deep one-class classification anomaly detection that addresses two critical challenges: query strategy and semi-supervised learning. The authors propose an uncertainty sampling method using an adaptive boundary that iteratively adjusts based on the ratio of abnormal samples queried, eliminating the need for explicit decision boundary hyper-parameters. They also apply noise contrastive estimation to effectively incorporate both labeled normal and abnormal samples during training. Experiments on seven anomaly detection datasets demonstrate that the proposed query strategy with adaptive boundary and NCE-based semi-supervised learning individually improve performance and further enhance results when combined.

## Method Summary
The method builds on Deep SVDD as a base anomaly detection model and introduces two key innovations. First, an adaptive boundary query strategy that selects uncertain samples based on an iteratively adjusted boundary position, using the ratio of abnormal samples queried to guide boundary movement. Second, a noise contrastive estimation (NCE) based semi-supervised learning approach that treats labeled normal and abnormal samples as estimation targets and noise data respectively. The adaptive boundary eliminates the need for decision boundary hyper-parameters by automatically adjusting based on observed query results, while the NCE component effectively leverages limited labeled abnormal samples to improve discrimination.

## Key Results
- The adaptive boundary query strategy achieves 3.4% AUC improvement over random sampling and 2.0% over high-confidence sampling
- The NCE-based semi-supervised learning method improves AUC by 1.4% over standard Deep SVDD with labeled normal data only
- The combined approach outperforms competing methods, achieving state-of-the-art results on all seven tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptive boundary selection improves uncertainty sampling by iteratively adjusting the decision threshold based on the ratio of abnormal samples queried in previous stages.
- Mechanism: The method maintains a boundary in feature space that adapts to the observed class distribution. When the ratio of abnormal samples near the boundary deviates from 0.5, the boundary moves proportionally toward or away from the hypersphere center. This creates a feedback loop where the model queries samples with maximum uncertainty rather than just those with highest anomaly scores.
- Core assumption: The feature space has a boundary where normal and abnormal samples have similar densities, and the ratio of abnormal samples near this boundary can be used as a reliable signal for boundary adjustment.
- Evidence anchors: [abstract] "our query strategy selects uncertain samples according to an adaptive boundary" and "the adaptive boundary is adjusted iteratively by the ratio of abnormal samples queried"
- Break condition: If the initial feature space learned by Deep SVDD is severely contaminated with abnormal samples, the adaptive boundary may not converge to a useful position.

### Mechanism 2
- Claim: NCE-based semi-supervised learning effectively incorporates both labeled normal and abnormal samples by treating them as estimation targets and noise data respectively.
- Mechanism: The method uses noise contrastive estimation to train the anomaly score function by maximizing the probability that normal samples receive lower scores than abnormal samples. This creates a discriminative framework where the model learns not just to describe normal data but to distinguish it from abnormal data using the labeled examples.
- Core assumption: The anomaly score function can be treated as an unnormalized statistical model, and labeled abnormal samples can be assumed to be sampled from the true abnormal distribution.
- Evidence anchors: [abstract] "we apply noise contrastive estimation in training a one-class classification model to incorporate both labeled normal and abnormal data effectively"
- Break condition: If the labeled abnormal samples are not representative of the true abnormal distribution, the contrastive learning may actually degrade performance.

### Mechanism 3
- Claim: The combination of adaptive boundary querying and NCE-based learning creates a synergistic effect that outperforms either method alone.
- Mechanism: Adaptive boundary querying ensures that the most uncertain samples are selected for labeling, providing high-quality training data. NCE-based learning then uses these samples to create stronger discriminative boundaries. This creates a positive feedback loop where better queries lead to better models, which lead to better queries.
- Core assumption: The quality of labeled data is the primary bottleneck in active learning for anomaly detection, and improving query strategy has multiplicative effects when combined with appropriate semi-supervised learning.
- Evidence anchors: [abstract] "We analyze that the proposed query strategy and semi-supervised loss individually improve an active learning process of anomaly detection and further improve when combined together"
- Break condition: If the dataset is too small or the anomaly ratio is extremely imbalanced, the adaptive boundary may not have enough data to make reliable adjustments.

## Foundational Learning

- Concept: Deep SVDD (Deep One-Class Classification)
  - Why needed here: The entire method builds on Deep SVDD as the base anomaly detection model, so understanding how it maps data to feature space and uses hyperspheres for anomaly scoring is fundamental.
  - Quick check question: What is the anomaly score in Deep SVDD and how is it computed?

- Concept: Active Learning Query Strategies
  - Why needed here: The paper presents a novel query strategy that differs from standard uncertainty sampling, so understanding the tradeoffs between different approaches (random, high-confidence, decision-boundary) is essential.
  - Quick check question: How does uncertainty sampling differ from high-confidence sampling in active learning for anomaly detection?

- Concept: Noise Contrastive Estimation (NCE)
  - Why needed here: NCE is the core of the semi-supervised learning method, so understanding how it contrasts estimation targets with noise data is critical.
  - Quick check question: In NCE, what is the relationship between the estimation target and the noise data?

## Architecture Onboarding

- Component map: Deep SVDD -> Adaptive boundary query strategy -> NCE-based semi-supervised learning -> Feedback loop update
- Critical path: Initial model training → Adaptive boundary search → Sample querying → NCE-based semi-supervised learning → Boundary update → Repeat until budget exhausted
- Design tradeoffs:
  - Query budget vs. model performance: More queries generally improve performance but increase cost
  - Boundary adjustment speed vs. stability: Fast adaptation may overshoot, slow adaptation may miss opportunities
  - NCE vs. other semi-supervised methods: NCE provides stronger discrimination but requires labeled abnormal samples
- Failure signatures:
  - Boundary oscillation: If rt values fluctuate wildly, the boundary may not stabilize
  - Performance degradation: If the NCE component overfits to limited labeled data
  - Slow improvement: If the query strategy consistently selects uninformative samples
- First 3 experiments:
  1. Test adaptive boundary convergence: Run the method on a simple dataset with known normal/abnormal distribution and verify that rt approaches 0.5 over time
  2. Validate NCE effectiveness: Compare the NCE-based semi-supervised method against baseline semi-supervised approaches on a small labeled dataset
  3. Assess query quality: Compare the samples selected by adaptive boundary vs. random sampling in terms of their informativeness for model improvement

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but several implicit questions emerge from the limitations and scope of the work. The authors focus primarily on demonstrating the effectiveness of their method on seven benchmark datasets without extensively exploring its behavior in extreme scenarios such as very low anomaly ratios, high-dimensional data, or datasets with highly diverse anomaly types.

## Limitations

- The adaptive boundary mechanism assumes a well-defined decision boundary exists where normal and abnormal samples have similar densities, which may not hold in cases of severe contamination
- The method assumes the feature space learned by Deep SVDD adequately separates normal and abnormal samples, which may not be true for all datasets
- The NCE-based semi-supervised learning assumes that labeled abnormal samples are representative of the true abnormal distribution, which may not hold if anomalies are highly diverse

## Confidence

- Adaptive boundary query strategy effectiveness: **Medium**
- NCE-based semi-supervised learning effectiveness: **Medium**
- Combined method superiority: **Medium**
- State-of-the-art performance claims: **Medium**

## Next Checks

1. **Boundary stability test**: Run the adaptive boundary algorithm on synthetic data with known normal/abnormal distributions and verify that rt converges to 0.5 without oscillation across multiple random seeds.

2. **Cross-dataset generalization**: Apply the complete method to additional anomaly detection datasets not included in the original evaluation to verify the claimed state-of-the-art performance generalizes beyond the seven tested datasets.

3. **Sensitivity analysis**: Systematically vary the initial boundary parameter (q1) and query budget size to determine the robustness of the method to hyperparameter choices and identify potential failure modes.