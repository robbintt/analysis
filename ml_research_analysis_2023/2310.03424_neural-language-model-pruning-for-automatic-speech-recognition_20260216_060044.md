---
ver: rpa2
title: Neural Language Model Pruning for Automatic Speech Recognition
arxiv_id: '2310.03424'
source_url: https://arxiv.org/abs/2310.03424
tags:
- pruning
- proc
- language
- recognition
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies model pruning methods for Transformer-based
  neural network language models in automatic speech recognition. The authors investigate
  three key aspects of pruning: the criterion (magnitude-driven or data-driven), the
  method (unstructured, structured, or factorized layers), and the scheduler (one-shot
  or incremental).'
---

# Neural Language Model Pruning for Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2310.03424
- Source URL: https://arxiv.org/abs/2310.03424
- Reference count: 0
- Models pruned with data-driven incremental pruning achieve up to 2.4% relative WER improvement over same-size models trained from scratch

## Executive Summary
This paper investigates model pruning methods for Transformer-based neural network language models in automatic speech recognition. The authors systematically evaluate three key aspects of pruning: the criterion (magnitude-driven or data-driven), the method (unstructured, structured, or factorized layers), and the scheduler (one-shot or incremental). They propose a novel variant of low-rank approximation suitable for incremental model compression. The results demonstrate that data-driven pruning consistently outperforms magnitude-driven approaches, incremental pruning achieves higher accuracy especially for smaller target sizes, and low-rank approximation provides the best trade-off between size reduction and inference speed-up for moderate compression.

## Method Summary
The authors evaluate Transformer-based NNLMs with 20M parameters using three pruning methods: unstructured, structured, and factorized layers. They compare magnitude-driven and data-driven pruning criteria, and one-shot versus incremental scheduling strategies. The NNLMs are used in shallow fusion beam decoding with a fixed Transformer Transducer (TT) acoustic model for ASR. The evaluation uses perplexity (PPL) on a 50k-utterance development set, word error rate (WER) on dictation and voice assistant evaluation sets, and floating point operations (FLOPs) for inference complexity. The study analyzes pruning across different target sizes (1M to 20M parameters) and investigates the impact on various data percentiles.

## Key Results
- Data-driven pruning criterion outperforms magnitude-driven pruning in several scenarios
- Incremental pruning scheduler achieves higher accuracy than one-shot, especially for smaller target sizes
- Low-rank approximation provides the best trade-off between size reduction and inference speed-up for moderate compression
- Best pruning method (unstructured, incremental, data-driven) achieved up to 2.4% relative WER improvement over same-size models trained from scratch

## Why This Works (Mechanism)

### Mechanism 1
Data-driven pruning outperforms magnitude-driven pruning by using a Taylor approximation of the loss function to estimate parameter importance. Parameters with higher impact on loss when masked are retained, making this approach more sensitive to task-specific data distribution than magnitude-based pruning.

### Mechanism 2
Incremental pruning achieves higher accuracy compared to one-shot by gradually increasing sparsity during training, allowing the model to recover from pruning-induced losses. This contrasts with one-shot pruning which removes many weights at once, potentially causing irreparable damage to the model's capacity.

### Mechanism 3
Low-rank approximation provides the best trade-off between size reduction and inference speed-up by factorizing weight matrices into smaller matrices, reducing parameters while maintaining computational efficiency. The proposed variant allows incremental pruning of factorized layers, enabling multiple models with varied target sizes.

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: The paper applies pruning techniques to Transformer-based language models used in ASR systems. Understanding the architecture is crucial for knowing which components to prune and how pruning affects model behavior.
  - Quick check question: What is the difference between multi-head self-attention and causal attention in Transformers?

- Concept: Automatic Speech Recognition (ASR) systems
  - Why needed here: The pruning techniques are evaluated in the context of ASR, where language models are used in shallow fusion with acoustic models. Understanding the ASR pipeline helps interpret pruning results.
  - Quick check question: How does shallow fusion integrate external language models with end-to-end ASR systems?

- Concept: Model compression and pruning techniques
  - Why needed here: The paper investigates various pruning methods (unstructured, structured, factorized) and scheduling strategies. A solid foundation in these techniques is essential for understanding the contributions and results.
  - Quick check question: What is the difference between structured and unstructured pruning in terms of implementation and performance impact?

## Architecture Onboarding

- Component map: Transformer-based NNLM with 6 encoder blocks -> 512-dimensional embeddings -> Multi-head attention (8 heads of 128 dimensions) -> Layer normalization and residual connections -> Output projection layer with softmax activation -> Fixed Transformer Transducer (TT) acoustic model

- Critical path: Load pre-trained 20M parameter baseline model -> Apply pruning (unstructured, structured, or factorized) with chosen criterion (magnitude or data-driven) -> Use incremental or one-shot scheduling strategy -> Fine-tune pruned model on ASR training data -> Evaluate perplexity on development set -> Test WER on dictation and voice assistant evaluation sets

- Design tradeoffs: Unstructured pruning provides best accuracy but no inference speed-up; structured pruning offers potential speed-up but with higher accuracy degradation; factorized pruning balances accuracy and speed for moderate compression; data-driven criterion consistently outperforms magnitude-driven but requires additional computation

- Failure signatures: Large perplexity increase during fine-tuning indicates poor pruning decisions; WER degradation without corresponding PPL increase suggests issues with shallow fusion integration; inconsistent layer-wise pruning effects may indicate architecture-specific sensitivities

- First 3 experiments: 1) Compare one-shot vs incremental pruning on a 10M parameter model using unstructured magnitude-driven pruning; 2) Evaluate data-driven vs magnitude-driven criteria on the same model and pruning schedule; 3) Test structured vs unstructured pruning methods while keeping other factors constant

## Open Questions the Paper Calls Out

- Open Question 1: How does unstructured pruning of attention heads affect the overall accuracy of the language model compared to structured pruning methods? The paper reports that pruning attention heads yields limited PPL degradation, suggesting potential for further compression without accuracy loss.

- Open Question 2: What is the impact of pruning the Transformer Transducer (TT) model itself on the overall ASR accuracy, and how does it compare to pruning only the neural network language model (NNLM)? The paper mentions that the TT model is fixed and suggests investigating TT pruning in future work.

- Open Question 3: How does the data distribution affect the effectiveness of pruning methods, and can pruning be optimized for specific data percentiles? The paper analyzes the impact of pruning on different data percentiles, finding that pruned models maintain accuracy on less frequent data.

## Limitations
- Evaluation focuses primarily on English dictation and voice assistant tasks, limiting generalizability to other languages or domains
- Pruning analysis conducted on specific Transformer architecture (6 encoder blocks, 512-dimensional embeddings), results may not transfer to larger or differently structured models
- Comparison with models trained from scratch is limited to same final size rather than controlling for total computational budget

## Confidence
- High Confidence: Incremental pruning scheduler consistently outperforms one-shot pruning across multiple experiments and model sizes
- Medium Confidence: Data-driven pruning criterion provides consistent improvements over magnitude-driven pruning, though improvements are sometimes modest
- Low Confidence: Low-rank approximation provides best trade-off between size reduction and inference speed-up for moderate compression, based on limited evidence

## Next Checks
1. Evaluate the pruning methods on a different language pair or domain (e.g., multilingual ASR or medical dictation) to assess whether the observed advantages of incremental and data-driven pruning transfer beyond English voice assistant and dictation tasks

2. Measure and compare the wall-clock time and computational resources required for data-driven importance score calculation versus magnitude-based pruning, particularly for larger models where the Hessian-vector products become expensive

3. Replace the fixed Transformer Transducer acoustic model with a different architecture (e.g., Conformer or hybrid system) and evaluate whether the pruned NNLMs maintain their WER improvements across different ASR system configurations