---
ver: rpa2
title: Continuous Training and Fine-tuning for Domain-Specific Language Models in
  Medical Question Answering
arxiv_id: '2311.00204'
source_url: https://arxiv.org/abs/2311.00204
tags:
- medical
- training
- language
- chinese
- cmexam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of adapting large language
  models (LLMs) to specialized domains, focusing on the medical field. The proposed
  method involves two stages: continuous training on a large corpus of medical data
  to teach relevant vocabulary and knowledge, followed by fine-tuning on a medical
  licensing exam dataset.'
---

# Continuous Training and Fine-tuning for Domain-Specific Language Models in Medical Question Answering

## Quick Facts
- arXiv ID: 2311.00204
- Source URL: https://arxiv.org/abs/2311.00204
- Authors: 
- Reference count: 40
- Primary result: Llama-2-13B fine-tuned on Chinese medical data achieves F1 score of 43.3% on medical licensing exam, comparable to GPT-3.5-turbo with fewer resources

## Executive Summary
This paper addresses the challenge of adapting large language models to specialized medical domains through a two-stage approach: continuous training on domain-specific corpus followed by fine-tuning on medical exam data. The method is tested on Chinese medical question answering using Llama 2 as the base model, achieving performance comparable to GPT-3.5-turbo while using significantly fewer computational resources. The approach demonstrates how domain expertise can be effectively imparted to LLMs without requiring extensive model pretraining from scratch.

## Method Summary
The proposed method involves two sequential stages: first, continuous training on 1 billion tokens from Chinese medical references to teach domain-specific vocabulary and knowledge; second, fine-tuning on 54K examples from the Chinese National Medical Licensing Examination with reasoning explanations. The approach was tested using both the general Llama-2-13B and a Chinese-specific variant, with evaluation on medical QA tasks using F1 scores on the CMExam test set and general knowledge retention measured by MMLU and CMMLU benchmarks.

## Key Results
- Llama-2-13B fine-tuned on medical exam data with reasoning explanations reached F1 score of 43.3%
- Chinese-Llama-2-13B variant achieved F1 score of 45.7%, outperforming the general model
- The adapted models achieved performance comparable to GPT-3.5-turbo despite using fewer computational resources
- Answer-only fine-tuning achieved F1 score of 42.8%, slightly lower than reasoning-explanation approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous training on domain-specific corpus teaches specialized vocabulary and knowledge before fine-tuning.
- Mechanism: By exposing the model to 1 billion tokens of Chinese medical text during continuous training, the model learns domain-specific terminology and context before being asked to perform specialized tasks.
- Core assumption: The model can effectively learn domain-specific knowledge through continued exposure to relevant text without catastrophic forgetting during this initial phase.
- Evidence anchors:
  - [abstract] "We first conduct continuous training on 1B tokens from Chinese medical references to teach relevant vocabulary and knowledge."
  - [section] "The continual training dataset includes more than 364,000 question-answer pairs extracted from Chinese medical encyclopedias and expert articles online."
  - [corpus] Weak evidence - corpus neighbors don't directly address continuous training mechanisms
- Break condition: If the continuous training corpus is too small or lacks diversity, the model may not acquire sufficient domain knowledge to benefit from subsequent fine-tuning.

### Mechanism 2
- Claim: Fine-tuning on exam data with reasoning explanations improves performance by teaching problem-solving patterns.
- Mechanism: The model learns not just correct answers but the reasoning process behind them, which transfers to better generalization on unseen medical questions.
- Core assumption: Including reasoning explanations during fine-tuning provides additional signal beyond answer-only training.
- Evidence anchors:
  - [abstract] "The models are then fine-tuned on 54K examples sourced from the Chinese National Medical Licensing Examination."
  - [section] "CMExam also includes explanations for most of the questions, providing additional knowledge for fine-tuning LLMs by enabling them to incorporate reasoning capabilities during the training process."
  - [corpus] Weak evidence - corpus neighbors focus on other medical LLM approaches, not reasoning-based fine-tuning
- Break condition: If the exam dataset lacks sufficient diversity or the reasoning explanations are poor quality, the model may learn incorrect patterns.

### Mechanism 3
- Claim: Using Chinese-specific base models improves performance on Chinese medical tasks compared to general models.
- Mechanism: Chinese-Llama-2-13B, pre-trained on both Chinese and English corpora, provides better language understanding for Chinese medical terminology than the English-only Llama-2-13B.
- Core assumption: The Chinese-specific pre-training provides meaningful advantage for Chinese medical tasks.
- Evidence anchors:
  - [abstract] "The second base model is a Chinese version of Llama 2 13B, initialized from the original Llama 2 and continuously trained on a mixture of Chinese and English corpora."
  - [section] "Chinese-Llama-2-13B achieved comparable performance to GPT-3.5-turbo" and outperformed the general Llama-2-13B on medical QA tasks.
  - [corpus] Weak evidence - corpus neighbors don't address Chinese-specific base models
- Break condition: If the Chinese pre-training corpus is too small or not well-matched to medical terminology, the advantage may disappear.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper explicitly discusses how continuous training causes decreases in MMLU and CMMLU scores while improving medical domain performance
  - Quick check question: What happens to a model's general knowledge performance when it undergoes domain-specific continuous training?

- Concept: Fine-tuning vs. full-parameter training
  - Why needed here: The paper compares different approaches including answer-only vs. reasoning-based fine-tuning and parameter-efficient methods vs. full-parameter training
  - Quick check question: How does training with reasoning explanations differ from answer-only training in terms of model performance?

- Concept: Cross-lingual transfer learning challenges
  - Why needed here: The paper attempts to mix English and Chinese medical data but finds it degrades Chinese performance
  - Quick check question: Why might mixing multilingual medical data actually hurt performance on Chinese-specific tasks?

## Architecture Onboarding

- Component map: Base model selection (Llama-2-13B or Chinese-Llama-2-13B) -> Continuous training pipeline (1B tokens of Chinese medical data) -> Fine-tuning pipeline (54K CMExam examples with optional reasoning) -> Evaluation framework (CMExam test set + MMLU/CMMLU for catastrophic forgetting) -> Training infrastructure (20-node cluster with V100 GPUs, FSDP)
- Critical path: Base model → Continuous training → Fine-tuning → Evaluation
- Design tradeoffs:
  - Computational cost vs. performance (full-parameter training vs. parameter-efficient methods)
  - General knowledge retention vs. domain expertise (catastrophic forgetting tradeoff)
  - Language specificity vs. multilingual capability (Chinese vs. mixed-language approaches)
- Failure signatures:
  - Low continuous training loss but poor fine-tuning performance indicates inadequate domain corpus
  - High continuous training loss suggests distribution mismatch between pre-training and medical data
  - Catastrophic forgetting measured by MMLU/CMMLU drops indicates need for interleaved training or experience replay
- First 3 experiments:
  1. Compare base Llama-2-13B vs. Chinese-Llama-2-13B on CMExam without any fine-tuning to establish baseline language advantage
  2. Test continuous training with different checkpoint intervals (750M, 1.5B, 2.25B tokens) to find optimal domain knowledge acquisition point
  3. Compare answer-only vs. reasoning-explanation fine-tuning on the same checkpoint to measure reasoning benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between continuous training data and fine-tuning data for domain-specific language models in terms of performance and catastrophic forgetting?
- Basis in paper: [explicit] The paper discusses catastrophic forgetting when using continuous training and notes that the model shows only minor decreases in general knowledge after 1 epoch of continuous training, but states that more research is needed to determine optimal strategies.
- Why unresolved: The paper only tests 1 epoch of continuous training and does not explore different ratios of continuous training to fine-tuning data or the impact of extending training beyond 1 epoch.
- What evidence would resolve it: Systematic experiments varying the amount of continuous training data, fine-tuning data, and training duration, measuring both domain-specific performance and retention of general knowledge.

### Open Question 2
- Question: How effective is the proposed method for domain adaptation in languages other than Chinese and English?
- Basis in paper: [explicit] The paper mentions that the methodology could be extended to other specialized domains and languages, but focuses on Chinese medical data due to the linguistic inequity in NLP.
- Why unresolved: The paper only tests the method on Chinese medical data and does not explore its effectiveness in other languages or domains.
- What evidence would resolve it: Applying the same method to domain-specific datasets in other languages and domains, comparing performance to the Chinese medical case.

### Open Question 3
- Question: What is the optimal approach for mitigating catastrophic forgetting when using continuous training for domain adaptation?
- Basis in paper: [explicit] The paper mentions that catastrophic forgetting is a challenge and suggests techniques like experience replay could help, but does not explore these methods.
- Why unresolved: The paper identifies catastrophic forgetting as a problem but does not test solutions beyond the basic approach used.
- What evidence would resolve it: Experiments comparing different techniques for mitigating catastrophic forgetting (e.g., experience replay, elastic weight consolidation) in the context of domain adaptation using continuous training.

## Limitations

- Evaluation relies on a single medical licensing exam that tests recall and application rather than clinical reasoning or real-world medical problem-solving
- Performance comparison to GPT-3.5-turbo uses different prompting strategies (5-shot vs 0-shot), creating potential confounding factors
- Chinese-specific nature of evaluation limits applicability to other languages and medical domains
- Narrow evaluation scope means model's performance on actual medical consultation scenarios remains unknown

## Confidence

**High confidence**: The fundamental approach of using continuous training followed by fine-tuning for domain adaptation is well-established and the paper demonstrates this works for medical QA. The catastrophic forgetting phenomenon observed (decreased MMLU/CMMLU scores) is a known issue in continual learning that aligns with existing literature.

**Medium confidence**: The claim that Chinese-Llama-2-13B outperforms general Llama-2-13B on Chinese medical tasks is supported by results, but the underlying mechanisms (e.g., whether the advantage comes from language modeling or medical knowledge acquisition) are not fully disentangled. The 2-3% performance improvement could be influenced by factors beyond just language familiarity.

**Low confidence**: The claim that the adapted model achieves "performance comparable to GPT-3.5-turbo" is questionable given the different evaluation conditions and the fact that GPT-3.5-turbo was not fine-tuned on the specific CMExam dataset. The comparison may overstate the practical utility of the approach.

## Next Checks

1. **Cross-dataset generalization test**: Evaluate the fine-tuned models on multiple medical QA datasets beyond CMExam (e.g., PubMedQA, MedQA) to assess whether performance improvements generalize across different medical question formats and knowledge domains.

2. **Clinical reasoning assessment**: Test the models on clinical case studies requiring multi-step reasoning rather than single-answer recall questions to determine if the reasoning-explanation fine-tuning actually improves clinical decision-making capabilities.

3. **Knowledge retention analysis**: Conduct detailed analysis of catastrophic forgetting by tracking performance on specific MMLU categories during continuous training to identify which types of general knowledge are most affected and whether interleaving medical and general training can mitigate this effect.