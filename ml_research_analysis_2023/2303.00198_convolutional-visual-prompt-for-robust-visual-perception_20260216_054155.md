---
ver: rpa2
title: Convolutional Visual Prompt for Robust Visual Perception
arxiv_id: '2303.00198'
source_url: https://arxiv.org/abs/2303.00198
tags:
- visual
- self-supervised
- prompts
- adaptation
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces convolutional visual prompts (CVP) for robust
  visual perception in out-of-distribution (OOD) settings without labeled data. CVP
  leverages the structured nature of convolutional kernels to enable lightweight,
  label-free test-time adaptation, reducing overfitting compared to standard high-dimensional
  visual prompts.
---

# Convolutional Visual Prompt for Robust Visual Perception

## Quick Facts
- arXiv ID: 2303.00198
- Source URL: https://arxiv.org/abs/2303.00198
- Authors:
- Reference count: 24
- This paper introduces convolutional visual prompts (CVP) for robust visual perception in out-of-distribution (OOD) settings without labeled data.

## Executive Summary
This paper introduces convolutional visual prompts (CVP) as a method for robust visual perception under distribution shifts without labeled data. CVP leverages the structured nature of convolutional kernels to enable lightweight, label-free test-time adaptation, reducing overfitting compared to standard high-dimensional visual prompts. The method is tested across five OOD benchmarks and multiple model architectures, showing improvements of up to 5.87% in robustness.

## Method Summary
CVP works by optimizing a small convolutional kernel at test-time to adapt input images for robust classification. The method trains a self-supervised contrastive learning model on clean images, then at test-time adapts inputs using the convolutional prompt optimized to minimize SSL loss. This approach enables label-free adaptation while imposing structural constraints that reduce overfitting compared to unstructured perturbations.

## Key Results
- CVP improves robustness by up to 5.87% across multiple OOD benchmarks
- Reduces overfitting by using structured convolutional kernels with fewer than 1% of parameters compared to standard visual prompts
- Complements existing test-time adaptation techniques like MEMO, TENT, and BN
- Shows consistent improvements across model architectures including ResNet and CLIP

## Why This Works (Mechanism)

### Mechanism 1
Convolutional visual prompts (CVP) reduce overfitting during test-time adaptation by imposing structured inductive bias. The convolutional structure constrains adaptation to be translation-equivariant and spatially coherent, limiting the parameter space compared to high-dimensional additive vectors. This works because distribution shifts in visual data are often structured (e.g., weather, lighting) and can be captured by local convolutional kernels. However, if the distribution shift is unstructured (e.g., random pixel corruption), the convolutional bias may limit adaptability.

### Mechanism 2
CVP leverages self-supervised contrastive loss to align adapted samples with the original data manifold without labels. During test-time, the convolutional prompt is optimized to minimize the contrastive loss, effectively reversing the distribution shift while preserving task-relevant features. This works because the self-supervised contrastive task captures shared information with the target classification task, enabling proxy supervision. If the self-supervised task is poorly aligned with the target task, adaptation may not recover relevant features.

### Mechanism 3
CVP complements existing test-time weight adaptation methods by modifying the input space. By adapting the input via convolutional prompts, CVP can be combined with methods like BN, TENT, or MEMO that update model weights, leading to additive robustness gains. This works because input-space adaptation and weight-space adaptation address different aspects of distribution shift and can be combined effectively. However, if the model architecture or adaptation method is incompatible with input modifications, complementarity may not hold.

## Foundational Learning

- **Concept**: Convolutional neural networks and translation equivariance
  - Why needed here: CVP relies on convolutional kernels to impose structure; understanding how convolutions capture spatial patterns is essential.
  - Quick check question: Why do convolutional layers assume translation equivariance, and how does this help in handling structured distribution shifts?

- **Concept**: Self-supervised contrastive learning
  - Why needed here: CVP uses contrastive loss as proxy supervision; understanding how positive/negative pairs and feature alignment work is critical.
  - Quick check question: How does minimizing contrastive loss encourage adapted samples to align with the original data manifold?

- **Concept**: Test-time adaptation and overfitting risks
  - Why needed here: CVP addresses overfitting in label-free adaptation; understanding the difference between training-time and test-time adaptation is key.
  - Quick check question: Why is overfitting more likely during test-time adaptation without labels, and how does structured prompting mitigate this?

## Architecture Onboarding

- **Component map**: Pretrained classifier (e.g., ResNet, CLIP) → SSL model (MLP for contrastive loss) → CVP (convolutional kernel) → adapted input → classifier output
- **Critical path**: Input → Convolution with kernel k → Contrastive loss minimization → Updated kernel → Adapted input → Classification
- **Design tradeoffs**:
  - Kernel size (3x3 vs 5x5): Smaller kernels are lighter but may miss larger-scale shifts; larger kernels capture more context but add parameters.
  - Fixed vs random initialization: Fixed kernels may provide stable adaptation; random kernels may better explore the adaptation space.
  - With vs without kernel update: Updating allows dynamic adaptation but risks overfitting; fixed kernels are safer but less flexible.
- **Failure signatures**:
  - Loss diverges or overfits to self-supervised objective without improving accuracy.
  - Adapted samples become too far from original distribution (high SWD).
  - Saliency maps show model focusing on wrong regions post-adaptation.
- **First 3 experiments**:
  1. Test CVP on CIFAR-10-C with severity 1, compare average error to standard model and VP (patch).
  2. Vary batch size (2, 4, 8, 16) and measure adaptation performance to confirm CVP advantage in small batches.
  3. Combine CVP with BN adaptation on ImageNet-C and measure mCE reduction.

## Open Questions the Paper Calls Out

### Open Question 1
What is the theoretical limit of CVP's robustness improvement on extreme corruption severities? The paper only tests on standard corruption benchmarks with predefined severity levels. Testing CVP on synthetic extreme corruption types (e.g., 90%+ pixel replacement) or extending severity levels beyond 5 would show the upper bound of CVP's effectiveness.

### Open Question 2
How does CVP's performance scale with larger kernel sizes and deeper architectures? Only two kernel sizes were tested, and the paper doesn't explore how CVP performs with much larger kernels (e.g., 7x7, 9x9) or on deeper architectures beyond ResNet50 and CLIP.

### Open Question 3
What is the relationship between self-supervised loss minimization and actual classification accuracy improvement? The paper shows CVP reduces self-supervised loss while improving accuracy, but Figure 3c suggests this relationship may not be monotonic for unstructured prompts.

### Open Question 4
How does CVP's performance compare to full model fine-tuning in terms of computational efficiency and accuracy trade-off? While the paper states CVP uses fewer parameters, it doesn't measure actual computational overhead during inference or compare the accuracy-compute trade-off with full fine-tuning.

## Limitations

- Effectiveness hinges on the assumption that distribution shifts are structured and captured by convolutional kernels, which may not hold for all types of OOD data.
- Reliance on contrastive self-supervised loss as proxy supervision assumes alignment with the target task, but this alignment is not empirically validated.
- Method's complementarity with existing test-time adaptation techniques is shown on limited benchmarks, and benefits may not generalize to all model architectures.

## Confidence

- **High Confidence**: CVP reduces overfitting by imposing structured inductive bias through convolutional kernels, as supported by quantitative comparisons of parameter counts and qualitative analysis of adaptation behavior.
- **Medium Confidence**: CVP improves robustness on tested OOD benchmarks, but generalizability to other datasets or tasks remains uncertain due to limited experimental scope.
- **Low Confidence**: The assumption that CVP complements existing test-time adaptation methods is weakly supported, as evidence is limited to specific combinations and architectures.

## Next Checks

1. Test CVP on a broader range of OOD datasets (e.g., domain adaptation benchmarks) to assess generalizability beyond the five evaluated.
2. Conduct ablation studies varying kernel size, initialization, and update frequency to quantify the impact of design choices on robustness.
3. Evaluate CVP's performance when combined with a diverse set of test-time adaptation methods (e.g., entropy minimization, feature normalization) to confirm complementarity across architectures.