---
ver: rpa2
title: Automatic and Human-AI Interactive Text Generation
arxiv_id: '2310.03878'
source_url: https://arxiv.org/abs/2310.03878
tags:
- text
- language
- linguistics
- computational
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This tutorial proposal focuses on text-to-text generation tasks
  that improve text readability and style while preserving meaning. The authors provide
  an overview of the field covering data, models, human-AI collaboration, and evaluation.
---

# Automatic and Human-AI Interactive Text Generation

## Quick Facts
- arXiv ID: 2310.03878
- Source URL: https://arxiv.org/abs/2310.03878
- Reference count: 23
- Primary result: Tutorial proposal on text-to-text generation tasks that improve text readability and style while preserving meaning, covering data, models, human-AI collaboration, and evaluation.

## Executive Summary
This tutorial proposal provides a comprehensive overview of automatic and human-AI interactive text generation, focusing on tasks that improve text readability and style while preserving meaning. The authors survey recent advances including non-retrogressive approaches like edit-based and diffusion models, prompting with large language models, learnable evaluation metrics, and multilingual datasets. The tutorial aims to introduce state-of-the-art research in this area and discuss future directions and ethical considerations, bridging NLP, HCI, and accessibility research.

## Method Summary
The tutorial covers neural models for generating semantically adequate text, automatic and human evaluation methods, and human-AI collaborative writing tools. Key methodological advances include edit-based models that decompose generation into token-level tagging and insertion, diffusion models that learn to denoise sequences conditioned on desired output style, and prompting strategies with large language models using control tokens. The evaluation framework combines automatic metrics (SARI, BERTScore, LENS) with fine-grained human assessment of semantic consistency, targeted language styles, and text quality.

## Key Results
- Non-retrogressive approaches (edit-based and diffusion models) outperform traditional auto-regressive seq2seq in constrained text revision tasks
- Prompting with large language models reduces need for task-specific fine-tuning while maintaining style control
- Human-AI collaborative writing tools improve final text quality by interleaving human judgment with AI suggestions at multiple granularities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-retrogressive approaches like edit-based and diffusion models outperform traditional auto-regressive seq2seq in constrained text revision tasks
- Mechanism: Edit-based models decompose generation into token-level tagging and insertion, preserving the majority of input tokens and reducing error propagation. Diffusion models add noise to the input sequence and learn to denoise it conditioned on the desired output style, avoiding the exposure bias of autoregressive decoding
- Core assumption: The input and output texts share substantial lexical overlap, so modeling edits is more efficient than full sequence generation
- Evidence anchors: [abstract] "non-retrogressive approaches such as edit-based and diffusion models"; [section] "Mallinson et al. (2020b) break down the task into sub-tasks of tagging tokens that are to kept and inserting missing tokens"
- Break condition: If the semantic gap between input and output becomes large (e.g., full paraphrasing with minimal overlap), edit-based decomposition loses its advantage and may require additional mechanisms like plan generation

### Mechanism 2
- Claim: Prompting with large language models reduces the need for task-specific fine-tuning while maintaining control over output style
- Mechanism: Control tokens are prepended to the input during prompt construction, conditioning the LLM on the desired rewrite type (e.g., simplification, style transfer). The LLM's internal knowledge and generalization allow it to produce stylistically appropriate outputs without task-specific parameter updates
- Core assumption: The LLM has been exposed to sufficient in-domain examples during pre-training to implicitly understand the rewrite instruction encoded in the prompt
- Evidence anchors: [abstract] "the shift from fine-tuning to prompting with large language models"; [section] "A simple and effective method is prepending of control tokens to the input during fine-tuning"
- Break condition: If the instruction requires nuanced stylistic control beyond the LLM's implicit understanding, prompting alone may fail and require hybrid fine-tuning or external control modules

### Mechanism 3
- Claim: Human-AI collaborative writing tools improve final text quality by interleaving human judgment with AI suggestions at multiple granularities
- Mechanism: The system presents intermediate revisions or multiple candidate rewrites, allowing the human to select, edit, or iterate. This interaction mitigates hallucination and style drift by grounding generation in human preferences and domain knowledge
- Core assumption: Human feedback at each revision step is both available and aligned with the intended quality metrics (readability, style consistency, factual correctness)
- Evidence anchors: [abstract] "Human-AI collaborative writing tools" and "human and automatic evaluation"; [section] "Several work use fine-tuned LLMs for collaborative writing through iterative revision (Du et al., 2022a)"
- Break condition: If the human operator lacks expertise or if interaction latency is high, the collaborative loop may introduce noise or reduce throughput without improving quality

## Foundational Learning

- Concept: Edit-level evaluation frameworks
  - Why needed here: Traditional n-gram overlap metrics fail to capture the semantic adequacy of edits; edit-level scoring isolates the contribution of each change to readability and style
  - Quick check question: What metric would you use to evaluate whether a specific word deletion in a simplification actually improves readability?

- Concept: Diffusion model conditioning
  - Why needed here: Diffusion models require a noise schedule and a condition embedding; understanding how to embed rewrite instructions into the diffusion process is critical for style control
  - Quick check question: How does the condition embedding influence the denoising trajectory in a conditional diffusion model for text?

- Concept: Large language model prompting strategies
  - Why needed here: Prompt design determines the LLM's behavior; control tokens, few-shot examples, and instruction phrasing must be chosen to elicit the desired rewrite style
  - Quick check question: What is the effect of adding explicit control tokens versus implicit style hints in a prompt for paraphrase generation?

## Architecture Onboarding

- Component map: Data loader → Tokenization & alignment → Edit-based/diffusion model → Control token encoder → LLM prompt generator → Decoding loop → Edit-level scorer → Human feedback interface
- Critical path: Data → Model inference → Evaluation → Human-in-the-loop revision
- Design tradeoffs:
  - Edit-based vs. diffusion: Edit-based is faster for high overlap but struggles with large rewrites; diffusion is more flexible but computationally heavier
  - Fine-tuning vs. prompting: Fine-tuning gives better task-specific performance but requires more data and compute; prompting is cheaper but depends on model pretraining coverage
- Failure signatures:
  - Edit-based: High precision but low recall on token-level edits; diffusion: text degeneration with high noise levels
  - Prompting: Inconsistent style if control tokens are ambiguous; fine-tuning: overfitting to training distribution
- First 3 experiments:
  1. Run an edit-based model on a small simplification dataset and measure token-level F1 vs. full-sequence BLEU
  2. Compare prompt-based LLM rewriting with fine-tuned T5 on style transfer, measuring human preference scores
  3. Evaluate a diffusion model's denoising quality at different noise scales on paraphrase data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop evaluation metrics that accurately capture both semantic preservation and stylistic improvements in text-to-text generation tasks?
- Basis in paper: [explicit] The paper discusses the development of new learnable metrics and fine-grained human evaluation frameworks, highlighting the importance of both semantic adequacy and stylistic appropriateness
- Why unresolved: Current automatic evaluation metrics like SARI and BERTScore have limitations in capturing the nuances of semantic consistency and stylistic control, especially for non-English languages and diverse text domains
- What evidence would resolve it: Empirical studies comparing the performance of new learnable metrics against human judgments across multiple languages, text domains, and stylistic variations, demonstrating improved correlation with human preferences

### Open Question 2
- Question: What are the most effective strategies for incorporating factual knowledge and domain-specific information into text-to-text generation models?
- Basis in paper: [explicit] The paper mentions the need for complex combinations of lexical and syntactical transformations, stylistic control, and adherence to factual knowledge in text-to-text generation tasks
- Why unresolved: Current models struggle to maintain factual accuracy while generating stylistically appropriate text, particularly in specialized domains like medical and legal texts
- What evidence would resolve it: Comparative studies evaluating different approaches for integrating factual knowledge, such as knowledge graphs, retrieval-augmented generation, or domain-specific fine-tuning, across multiple text-to-text generation tasks

### Open Question 3
- Question: How can human-AI collaboration be optimized to produce high-quality text revisions while maintaining user trust and engagement?
- Basis in paper: [explicit] The paper discusses human-AI collaborative writing tools and the need for fair comparison of commercial and academic systems based on HCI research
- Why unresolved: There is a lack of standardized evaluation protocols for comparing different human-AI collaborative writing systems, and the impact of AI suggestions on user writing behavior and satisfaction is not well understood
- What evidence would resolve it: Longitudinal studies tracking user interactions with different collaborative writing tools, measuring writing quality improvements, user trust, and engagement metrics across diverse user groups and writing tasks

## Limitations
- Limited direct quantitative comparisons between edit-based and diffusion models on identical benchmarks
- Reliance on conceptual arguments rather than empirical demonstrations for prompting effectiveness
- Limited concrete evidence of quality improvements from actual user studies in human-AI collaborative writing

## Confidence
- **High Confidence**: Coverage of evaluation metrics (SARI, BERTScore, LENS) and importance of semantic preservation in text revision tasks
- **Medium Confidence**: Comparative advantages of non-retrogressive approaches and effectiveness of prompting strategies
- **Low Confidence**: Claims about human-AI collaborative writing tools improving text quality depend heavily on assumptions about human expertise and interaction efficiency

## Next Checks
1. Conduct controlled experiments comparing edit-based models (like FELIX) against diffusion models (like DiffuSeq) on identical text simplification datasets using consistent evaluation metrics (SARI, BERTScore, and human judgments) to determine actual performance differentials
2. Systematically test different prompting strategies (control tokens vs. few-shot examples vs. explicit instructions) across multiple text revision tasks to measure consistency and style control precision, comparing against fine-tuned baselines
3. Design and execute a controlled user study where writers complete text revision tasks with and without AI assistance, measuring quality improvements, time efficiency, and user satisfaction across different interaction granularities (word-level suggestions vs. full rewrites)