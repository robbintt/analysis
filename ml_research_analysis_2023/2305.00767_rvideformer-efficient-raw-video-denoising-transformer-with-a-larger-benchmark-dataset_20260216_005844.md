---
ver: rpa2
title: 'RViDeformer: Efficient Raw Video Denoising Transformer with a Larger Benchmark
  Dataset'
arxiv_id: '2305.00767'
source_url: https://arxiv.org/abs/2305.00767
tags:
- denoising
- video
- dataset
- image
- recrvd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of large-scale datasets for raw video
  denoising by constructing a new dataset (ReCRVD) using screen recapture techniques.
  The dataset contains 120 groups of noisy-clean video pairs with realistic motions,
  captured using high-low ISO settings.
---

# RViDeformer: Efficient Raw Video Denoising Transformer with a Larger Benchmark Dataset

## Quick Facts
- arXiv ID: 2305.00767
- Source URL: https://arxiv.org/abs/2305.00767
- Reference count: 40
- Primary result: Proposes RViDeformer with multi-branch spatial-temporal attention achieving 44.37 dB PSNR on ReCRVD dataset

## Executive Summary
This paper addresses the lack of large-scale datasets for raw video denoising by constructing ReCRVD using screen recapture techniques with high-low ISO settings, capturing 120 groups of noisy-clean video pairs. The authors propose RViDeformer, an efficient transformer network that utilizes multi-branch spatial and temporal attention mechanisms to explore both short and long-distance correlations. The network is trained in both supervised and unsupervised manners, achieving state-of-the-art performance on ReCRVD and demonstrating superior generalization to real-world outdoor noisy videos.

## Method Summary
The method introduces a multi-branch spatial self-attention (MSSA) combining shift-window, low-resolution, global, and neighbor-based attention, along with a multi-branch temporal self-attention (MTSA) for efficient long-range temporal correlation exploitation. The network is trained with supervised loss on raw and sRGB domains, and unsupervised NBR2NBR strategy. ReCRVD dataset is created via screen recapture of high-resolution videos with ISO values from 1600 to 25600, applying intensity, spatial, and color corrections to align frames.

## Key Results
- Achieves 44.37 dB PSNR on ReCRVD test set, surpassing existing methods
- Demonstrates superior generalization to real-world outdoor noisy videos compared to models trained on previous datasets
- Maintains lower computational complexity through reparameterization techniques
- Unsupervised training shows better generalization on CRVD outdoor test set than supervised training

## Why This Works (Mechanism)

### Mechanism 1
The multi-branch spatial attention design efficiently aggregates local and global contextual information through shift-window, low-resolution, global, and neighbor-based attention branches. This allows simultaneous learning from different receptive fields, with fusion enabling leverage of both short-range and long-range correlations within the same spatial window. The core assumption is that different attention branches capture complementary information and their fusion improves denoising quality without proportional increase in computational cost.

### Mechanism 2
The multi-branch temporal attention mechanism enables efficient exploitation of long-range temporal correlations across video frames without explicit optical flow. By processing temporal relationships through plain TMA, GTMA, and NTMA branches with different support frame configurations, the network can implicitly align and denoise frames, handling larger motions than window-based attention alone. The core assumption is that transformer's attention mechanism can implicitly learn temporal alignment without explicit motion estimation.

### Mechanism 3
The reparameterization technique reduces computational complexity during inference while maintaining model capacity during training. During training, two parallel linear layers (or convolutions) increase model capacity, which are then fused into a single linear layer (or convolution) during inference using linearity properties. This maintains the same computational cost as a single layer while preserving the benefits of the dual-layer architecture. The core assumption is that the fusion operation preserves the mathematical properties of the dual-layer architecture.

## Foundational Learning

- **Self-attention mechanism in transformers**: Understanding how attention weights are computed and used to aggregate information is crucial for grasping the multi-branch attention design. Quick check: How does the attention mechanism compute relationships between different positions in the input sequence?

- **Window-based attention and its limitations**: The paper builds upon window-based attention but extends it to overcome its limitations for video denoising. Quick check: What is the main limitation of standard window-based attention for video processing?

- **Raw image processing and noise characteristics**: The denoising is performed in raw domain, which has different noise characteristics than processed images. Quick check: Why is denoising in raw domain generally preferred over processed (sRGB) domain for image quality?

## Architecture Onboarding

- **Component map**: Bayer raw video frames (packed into 4 channels) → MTSA blocks + MSSA blocks → Spatial refinement: MSR blocks → Denoised raw frames → sRGB conversion via ISP

- **Critical path**: Raw video frames → MTSA/MSSA processing → Spatial refinement → Denoised output

- **Design tradeoffs**: Multi-branch attention vs. computational cost (multiple branches increase capacity but also computation; reparameterization helps mitigate this), implicit temporal alignment vs. explicit motion estimation (avoids optical flow complexity but may be less effective for extreme motions), raw domain processing vs. sRGB processing (raw offers better noise modeling but requires ISP conversion for evaluation)

- **Failure signatures**: Poor temporal consistency across frames (indicates MTSA issues), blurry or over-smoothed results (indicates attention branches not properly balanced), high computational cost despite reparameterization (indicates incorrect implementation of fusion)

- **First 3 experiments**: 1) Verify that each attention branch (SWSA, LWSA, GWSA, NWSA) produces meaningful outputs by visualizing attention maps, 2) Test the reparameterization by comparing inference results with and without fusion, 3) Validate temporal attention by denoising a video with known motion patterns and checking consistency across frames

## Open Questions the Paper Calls Out

- **Noise model consistency**: How does the noise model in ReCRVD dataset compare to real-world noise in terms of sensor-specific characteristics and ISO-dependent behavior? The paper relies on qualitative claims about "realistic motions" and "natural noise" without quantitative validation through noise profile analysis comparing ReCRVD to real-world captured noisy videos.

- **Attention branch optimization**: What is the optimal balance between spatial and temporal attention mechanisms in the multi-branch architecture for different types of motion in video denoising? The paper evaluates overall performance but does not analyze how different attention branches contribute to denoising under varying motion conditions or provide a method to dynamically adjust attention weights based on motion characteristics.

- **Unsupervised learning robustness**: How does the proposed unsupervised learning approach perform when the test data has a significantly different noise distribution than the training data? The paper only tests on datasets with similar noise characteristics to ReCRVD and does not investigate robustness to domain shifts in noise characteristics such as different sensor types, lighting conditions, or noise patterns.

## Limitations

- The paper's efficiency claims regarding reparameterization lack quantitative FLOPs or runtime comparisons with baseline architectures
- Screen recapture methodology for creating ReCRVD may introduce systematic biases in noise characteristics affecting generalization
- Unsupervised NBR2NBR training strategy's robustness across different noise distributions requires further investigation beyond limited outdoor video samples

## Confidence

- **High confidence**: Multi-branch attention architecture design and implementation details are well-specified and theoretically sound
- **Medium confidence**: Efficiency claims regarding reparameterization need empirical validation through direct computational complexity measurements
- **Low confidence**: Unsupervised NBR2NBR training strategy's robustness across different noise distributions and effectiveness compared to alternative self-supervised approaches require further investigation

## Next Checks

1. **Efficiency validation**: Measure and compare actual FLOPs, memory usage, and inference time of RViDeformer against standard Swin transformer and other state-of-the-art video denoising models on identical hardware to verify efficiency claims.

2. **Dataset bias analysis**: Conduct experiments to quantify how screen recapture methodology affects noise characteristics in ReCRVD compared to directly captured sensor noise, and test model performance degradation when training on recaptured data but testing on directly captured noisy videos.

3. **Generalization stress test**: Evaluate RViDeformer's performance on videos with extreme motion (beyond the captured dataset's range), different lighting conditions, and various camera sensors to assess the practical limits of its implicit temporal alignment capability.