---
ver: rpa2
title: GPT-4 can pass the Korean National Licensing Examination for Korean Medicine
  Doctors
arxiv_id: '2303.17807'
source_url: https://arxiv.org/abs/2303.17807
tags:
- questions
- medicine
- accuracy
- korean
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the potential of GPT-3.5 and GPT-4 to answer
  questions from the Korean National Licensing Examination for Korean Medicine Doctors
  (K-NLEKMD). Optimized prompts with Chinese-term annotations, English translations,
  and exam-specific instructions were used to input 340 questions across 12 TKM subjects.
---

# GPT-4 can pass the Korean National Licensing Examination for Korean Medicine Doctors

## Quick Facts
- arXiv ID: 2303.17807
- Source URL: https://arxiv.org/abs/2303.17807
- Reference count: 0
- Key outcome: GPT-4 achieved 57.29% accuracy on K-NLEKMD, approaching the 60% passing threshold

## Executive Summary
This study evaluates GPT-3.5 and GPT-4 performance on the Korean National Licensing Examination for Korean Medicine Doctors (K-NLEKMD), a comprehensive test covering both conventional medicine and traditional Korean medicine (TKM). Using optimized prompts with Chinese-character annotations and English translations, GPT-4 achieved 57.29% accuracy, nearing the 60% passing threshold, while GPT-3.5 reached 42.06%. GPT-4 excelled in neuropsychiatry (83.75%) but struggled with internal medicine (28.75%) and TKM-specific subjects, performing better on recall and diagnosis questions than intervention-based ones. The results demonstrate foundation models' promise for TKM clinical support while highlighting the need for cultural adaptation and TKM-specific knowledge enhancement.

## Method Summary
The study evaluated GPT-3.5 and GPT-4 using 340 multiple-choice questions from the 2022 K-NLEKMD across 12 subjects. Questions were processed with optimized prompts incorporating Chinese-character annotations of TKM terms, English translations, and exam-specific instructions. Each question was submitted five times to measure response consistency, with accuracy calculated as the proportion of correct answers. The methodology included analysis by subject area, competency type (recall, diagnosis, intervention), and question format (text, table, image).

## Key Results
- GPT-4 achieved 57.29% accuracy, approaching the 60% passing threshold, while GPT-3.5 reached 42.06%
- GPT-4 performed best in neuropsychiatry (83.75%) and struggled most in internal medicine (28.75%) and TKM-specific subjects
- Accuracy was significantly higher for recall (60.83%) and diagnosis (62.65%) questions compared to intervention-based questions (41.00%)
- Response consistency showed positive correlation with accuracy, with identical responses achieving 66-70% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 achieves near-passing performance on K-NLEKMD without TKM-specific training due to emergent reasoning capabilities from large-scale pretraining.
- Mechanism: The model leverages learned general medical and linguistic patterns from massive text corpora, enabling inference even in culturally adapted domains.
- Core assumption: Pretraining data contains sufficient cross-domain medical and linguistic knowledge to support reasoning in specialized fields.
- Evidence anchors:
  - [abstract] GPT-4 achieved 57.29% accuracy, nearing the 60% passing threshold.
  - [section] GPT-4 demonstrated human-level performance on professional benchmarks without domain-specific training.
- Break condition: If the pretraining corpus lacks sufficient TKM-specific or culturally relevant examples, accuracy drops sharply in those subjects.

### Mechanism 2
- Claim: Prompt engineering with Chinese-character annotations and English translations boosts model performance by bridging cultural-linguistic gaps.
- Mechanism: Adding TKM terms in Chinese characters alongside Korean improves the model's recognition of domain-specific entities, aiding accurate retrieval and reasoning.
- Core assumption: The model's pretraining corpus included Chinese-character-based medical terminology and cross-linguistic mappings.
- Evidence anchors:
  - [section] Questions were annotated with Chinese characters to address limited TKM data in pretraining.
  - [section] Optimized prompts increased accuracy from 51.82% to 66.18%.
- Break condition: If the model has not been exposed to Chinese medical terminology, the added annotations yield no benefit.

### Mechanism 3
- Claim: High consistency of responses correlates with higher accuracy, suggesting that output stability can indicate reliability.
- Mechanism: Consistent outputs across trials indicate the model has converged on a stable inference path, reducing hallucination risk.
- Core assumption: Variability in responses is largely due to stochastic sampling rather than genuine uncertainty.
- Evidence anchors:
  - [section] A positive correlation was observed between consistency and accuracy.
  - [section] When all five responses were identical, accuracy reached 0.66 (GPT-3.5) and 0.70 (GPT-4), above the passing threshold.
- Break condition: If the model's outputs are consistently wrong, high consistency does not improve real-world reliability.

## Foundational Learning

- Concept: Cross-linguistic medical terminology mapping
  - Why needed here: TKM uses Chinese-character-based terms; without mapping, the model misinterprets or misses key concepts.
  - Quick check question: Can you identify a TKM term in Korean and explain its Chinese-character equivalent?

- Concept: Cultural adaptation in AI model evaluation
  - Why needed here: Standard benchmarks may not reflect performance in culturally adapted medical systems like TKM.
  - Quick check question: Why might GPT-4 perform worse on TKM-specific subjects compared to general medicine?

- Concept: Emergent capabilities in large language models
  - Why needed here: GPT-4's strong performance without fine-tuning relies on emergent reasoning abilities from pretraining.
  - Quick check question: What evidence suggests that GPT-4's medical reasoning is emergent rather than memorized?

## Architecture Onboarding

- Component map: Input → Prompt Engineering (annotations, translations) → LLM (GPT-3.5/GPT-4) → Consistency Check → Accuracy Evaluation
- Critical path: Optimized prompt → model inference → consistency aggregation → subject-wise accuracy analysis
- Design tradeoffs: Annotating prompts increases input length and processing time but improves accuracy; omitting images and tables limits context but preserves text-only inference.
- Failure signatures: Low accuracy in TKM-specific subjects indicates insufficient cross-cultural pretraining data; inconsistent outputs suggest model uncertainty or hallucination risk.
- First 3 experiments:
  1. Compare accuracy with and without Chinese-character annotations for TKM terms.
  2. Evaluate consistency across trials for each subject and competency type.
  3. Test intervention-based question accuracy after fine-tuning on TKM intervention data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific fine-tuning methods would most effectively reduce cultural bias in LLMs for TKM applications?
- Basis in paper: [explicit] The paper discusses how GPT-4's training data is heavily skewed toward English and Western sources, leading to poor performance on TKM-specific and Korea-adapted questions. It explicitly states "fine-tuning the models for datasets reflecting culture-specific medicine is needed."
- Why unresolved: The study identifies the problem but doesn't explore solutions or test different fine-tuning approaches.
- What evidence would resolve it: Comparative studies testing various fine-tuning strategies (curriculum learning, domain adaptation, data augmentation) on LLM performance with TKM content.

### Open Question 2
- Question: How can LLMs be modified to improve performance on intervention-based questions in TKM without sacrificing accuracy on recall and diagnosis tasks?
- Basis in paper: [explicit] The paper shows that both GPT-3.5 and GPT-4 performed significantly worse on intervention-based questions compared to recall and diagnosis tasks, suggesting limitations in complex reasoning and TKM-specific knowledge integration.
- Why unresolved: The study identifies the performance gap but doesn't investigate the underlying causes or potential architectural modifications.
- What evidence would resolve it: Comparative analysis of different model architectures or prompting strategies specifically targeting intervention-type reasoning in TKM contexts.

### Open Question 3
- Question: What is the relationship between response consistency and clinical reliability in LLM outputs for TKM applications?
- Basis in paper: [explicit] The study found that higher consistency in responses correlated with higher accuracy, suggesting consistency could serve as a reliability indicator, but didn't explore the clinical implications.
- Why unresolved: The paper identifies the correlation but doesn't establish clinical significance or validation criteria.
- What evidence would resolve it: Clinical validation studies correlating consistency metrics with expert evaluation of clinical appropriateness in TKM decision-making scenarios.

## Limitations
- GPT-4 accuracy (57.29%) falls short of the 60% passing threshold, raising questions about real-world clinical applicability
- Evaluation excluded image-based and table-based questions, which constitute a significant portion of actual licensing exams
- The study does not address potential safety implications of deploying models that cannot consistently achieve passing scores

## Confidence

- **High confidence**: GPT-4's superior performance over GPT-3.5 on K-NLEKMD questions, and the positive correlation between response consistency and accuracy
- **Medium confidence**: The effectiveness of Chinese-character annotations in improving model performance, as this depends on pretraining data composition
- **Low confidence**: The practical utility of these models for clinical assistance in TKM, given the failure to meet passing thresholds

## Next Checks
1. Evaluate GPT-4's performance on image-based and table-based questions from K-NLEKMD to determine the impact of excluding visual content
2. Conduct a systematic ablation study testing the contribution of each prompt engineering component (Chinese annotations, English translations, exam instructions) to isolate which elements drive accuracy improvements
3. Assess model performance on clinically critical question types where GPT-4 showed weakness (internal medicine, TKM-specific subjects) using human expert review to identify specific knowledge gaps