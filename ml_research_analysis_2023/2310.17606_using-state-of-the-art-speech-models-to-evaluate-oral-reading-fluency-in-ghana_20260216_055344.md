---
ver: rpa2
title: Using State-of-the-Art Speech Models to Evaluate Oral Reading Fluency in Ghana
arxiv_id: '2310.17606'
source_url: https://arxiv.org/abs/2310.17606
tags:
- reading
- speech
- dataset
- whisper
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the use of large-scale speech models to assess
  oral reading fluency (ORF) in Ghanaian students. It tests two state-of-the-art models
  (Whisper V2 and wav2vec 2.0) on a dataset of 162 students reading aloud in classrooms.
---

# Using State-of-the-Art Speech Models to Evaluate Oral Reading Fluency in Ghana

## Quick Facts
- arXiv ID: 2310.17606
- Source URL: https://arxiv.org/abs/2310.17606
- Reference count: 0
- Primary result: Whisper V2 achieves 13.5 WER on Ghanaian student speech without fine-tuning, correlating strongly (0.96) with human ORF scores

## Executive Summary
This study evaluates whether state-of-the-art speech models can automate oral reading fluency assessment in Ghana's educational context. Using Whisper V2 and wav2vec 2.0 on 162 Ghanaian students reading aloud, the research demonstrates that these models achieve transcription accuracy comparable to their performance on adult speech, without any fine-tuning. The automated WCPM scores correlate strongly with expert human ratings, suggesting large-scale speech models could provide a scalable alternative to labor-intensive one-on-one reading assessments in resource-constrained settings.

## Method Summary
The study uses Whisper V2 and wav2vec 2.0 to transcribe audio recordings of 162 Ghanaian students reading grade-appropriate passages. The models were applied out-of-the-box without fine-tuning, and their transcriptions were compared against expert human transcripts using Word Error Rate (WER). JiWER package calculated WER, and the model-generated transcriptions were used to compute Words Correct Per Minute (WCPM) scores, which were then correlated with human-generated ORF scores. The research also included a smaller dataset from the University of Colorado Boulder for comparison.

## Key Results
- Whisper V2 achieved 13.5 WER on Ghanaian student speech, close to its 12.8 WER on adult speech
- Automated WCPM scores correlated strongly with expert human ratings (correlation coefficient 0.96)
- Both models performed comparably to established ASR benchmarks without fine-tuning
- WCPM estimates were nearly identical between automated (110) and human (113) approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Whisper V2 generalizes well to Ghanaian student speech without fine-tuning
- Mechanism: Large-scale unsupervised pre-training on diverse audio data enables strong cross-linguistic and cross-accent adaptation
- Core assumption: Model's training data included sufficient acoustic variability to handle regional accents and classroom noise
- Evidence anchors:
  - [abstract] "Whisper V2 achieved a Word Error Rate (WER) of 13.5 on Ghanaian student speech, close to its adult speech performance (12.8 WER)"
  - [section] "Both Whisper V2 and wav2vec 2.0 perform similarly to the average performance on ASR benchmarks reported above"
  - [corpus] Weak evidence - corpus analysis found 0 relevant papers on Whisper V2 performance in Ghanaian contexts
- Break condition: If test data contains significantly different accent patterns or acoustic conditions not represented in pre-training

### Mechanism 2
- Claim: Automated WCPM scores correlate strongly with expert human ratings
- Mechanism: Transcription accuracy is sufficient to preserve reading errors that determine WCPM calculation
- Core assumption: WER below ~15% maintains enough error information for accurate WCPM computation
- Evidence anchors:
  - [abstract] "model-generated transcriptions produced fully automated ORF scores that correlated strongly with expert human ratings (correlation coefficient 0.96)"
  - [section] "The overall average WCPM estimates were quite close when using the expert human and the fully automated approach, 113 compared to 110"
  - [corpus] Weak evidence - corpus lacks papers specifically addressing WCPM calculation from ASR transcripts
- Break condition: If transcription errors systematically bias WCPM (e.g., consistently missing certain error types)

### Mechanism 3
- Claim: ASR can reduce assessment burden in resource-constrained educational contexts
- Mechanism: Automated transcription eliminates need for trained human evaluators while maintaining measurement validity
- Core assumption: Correlation between automated and human scores translates to practical equivalence in educational decision-making
- Evidence anchors:
  - [abstract] "assessing it typically requires one-on-one sessions between a student and a trained evaluator, a process that is time-consuming and costly"
  - [section] "this research is among the first to examine the use of the most recent versions of large-scale speech models... for ORF assessment in the Global South"
  - [corpus] Weak evidence - corpus lacks papers on implementation costs or scalability of automated ORF assessment
- Break condition: If automated system requires technical infrastructure or expertise beyond typical LMIC educational settings

## Foundational Learning

- Concept: Oral Reading Fluency (ORF)
  - Why needed here: Core construct being measured; understanding its components (accuracy, rate, expression) is essential for interpreting WCPM results
  - Quick check question: What are the three components of oral reading fluency and how does WCPM capture them?

- Concept: Word Error Rate (WER)
  - Why needed here: Primary metric for evaluating transcription quality; directly impacts WCPM calculation accuracy
  - Quick check question: How is WER calculated and what does a WER of 13.5 mean in practical terms?

- Concept: Transfer learning and domain adaptation
  - Why needed here: Explains why pre-trained models work without fine-tuning on new accent/acoustic environments
  - Quick check question: What is the difference between supervised fine-tuning and out-of-the-box application of large speech models?

## Architecture Onboarding

- Component map: Audio input -> Speech recognition model (Whisper V2/wav2vec2.0) -> Transcription output -> Error detection algorithm -> WCPM calculation -> Score output
- Critical path:
  1. Audio file ingestion and preprocessing
  2. Speech-to-text transcription
  3. Text normalization (lowercase, punctuation removal)
  4. Error detection and counting
  5. Duration measurement
  6. WCPM formula application

- Design tradeoffs:
  - Model choice: Whisper V2 (lightly supervised, better performance) vs wav2vec2.0 (purely unsupervised, preserves more errors)
  - Accuracy vs. computation: Larger models may be more accurate but require more resources
  - Error handling: Whether to normalize repetitions (Whisper) or preserve them (wav2vec2.0)

- Failure signatures:
  - High WER (>20%) suggests model mismatch with accent/acoustic conditions
  - Systematic bias in error types (e.g., missing substitutions but counting insertions) indicates model-specific limitations
  - Poor correlation with human scores suggests breakdown in error detection or duration measurement

- First 3 experiments:
  1. Run Whisper V2 on 10 Ghana dataset audio files and compare WER to reported 13.5 baseline
  2. Calculate WCPM from model transcriptions and compare to provided human scores for correlation
  3. Test same Whisper V2 model on CU dataset to reproduce 10.2 WER result and understand accent impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would fine-tuning large-scale speech models specifically for Ghanaian accents and classroom noise conditions impact transcription accuracy compared to using models out-of-the-box?
- Basis in paper: [explicit] The authors note that while their results were achieved without fine-tuning, they suggest this as "a particularly promising area" for future research, acknowledging that fine-tuning could improve sensitivity to regional accents and classroom noise.
- Why unresolved: The study deliberately used unmodified models to test generalizability, leaving the potential performance gains from fine-tuning unexplored.
- What evidence would resolve it: Comparative experiments measuring WER improvements when models are fine-tuned on Ghanaian student speech data versus unmodified performance.

### Open Question 2
- Question: Can the fully automated WCPM calculation methodology generalize to other languages and orthographies, particularly agglutinating African languages?
- Basis in paper: [explicit] The authors explicitly state this is an important area for further research, noting that "there are active debates about the accuracy of ORF measures across different languages" and that their experiments only evaluated English.
- Why unresolved: The study's methodology was developed and tested only for English, with no validation across different linguistic structures or writing systems.
- What evidence would resolve it: Cross-linguistic validation studies demonstrating consistent WCPM accuracy across multiple languages, particularly those with different morphological structures.

### Open Question 3
- Question: What is the optimal balance between preserving reading errors and correcting them in ASR transcriptions for educational assessment purposes?
- Basis in paper: [inferred] The authors observed that wav2vec 2.0 preserved reading errors more faithfully while Whisper V2 corrected some errors, noting this relates to different training approaches but not addressing which is preferable for assessment.
- Why unresolved: The study highlighted this difference but did not investigate whether preserving errors or correcting them produces more educationally valuable assessments.
- What evidence would resolve it: Empirical studies comparing the validity and reliability of ORF assessments using error-preserving versus error-correcting transcriptions, potentially through expert rater comparisons.

## Limitations

- Sample size of 162 students limits statistical power for subgroup analyses
- Evaluation limited to English language reading only
- Acoustic conditions represent only classroom reading domain
- Strong correlation with human scores does not establish practical equivalence in educational decision-making

## Confidence

**High Confidence:** The core finding that Whisper V2 achieves WER of 13.5 on Ghanaian student speech without fine-tuning appears robust, supported by direct comparison to adult speech performance and prior benchmarks.

**Medium Confidence:** The strong correlation (0.96) between automated and human ORF scores is well-supported within the study's dataset, though external validation across different populations would strengthen this claim.

**Low Confidence:** Claims about scalability and implementation costs in LMIC contexts remain speculative, with minimal evidence about real-world deployment challenges, infrastructure requirements, or long-term maintenance.

## Next Checks

1. Cross-population validation: Test the same Whisper V2 model on ORF datasets from other African countries (e.g., Kenya, Nigeria) to assess generalizability across different English accents and educational contexts.

2. Error type analysis: Conduct detailed analysis of transcription errors to identify whether certain types of reading mistakes (substitutions, omissions, insertions) are systematically missed or corrected by the model, potentially biasing ORF scores.

3. Educational impact study: Design a randomized controlled trial comparing educational decisions made using automated vs. human ORF scores to determine if the strong correlation translates to equivalent practical outcomes in classroom settings.