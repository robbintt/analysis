---
ver: rpa2
title: Text encoders bottleneck compositionality in contrastive vision-language models
arxiv_id: '2305.14897'
source_url: https://arxiv.org/abs/2305.14897
tags:
- text
- clip
- compprompts
- vit-b
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of information loss in the single-vector
  text representations used by contrastive vision-language models like CLIP. The authors
  develop a method to probe these text encoders by training generative decoders to
  reconstruct original captions from the encoder outputs.
---

# Text encoders bottleneck compositionality in contrastive vision-language models

## Quick Facts
- arXiv ID: 2305.14897
- Source URL: https://arxiv.org/abs/2305.14897
- Reference count: 4
- Primary result: Text encoders in contrastive VL models like CLIP lose compositional information, struggling with spatial relations, negations, and counting

## Executive Summary
This paper addresses a critical limitation in contrastive vision-language models: their single-vector text representations lose important compositional information. The authors develop a novel reconstruction probe methodology to evaluate how much linguistic information is preserved in these text embeddings. By training generative decoders to reconstruct captions from text encoder outputs, they reveal that CLIP's text encoder struggles with spatial relations, attribute-object associations, counting, and negations. Their findings show that text-only recovery performance predicts multimodal matching performance, demonstrating that the text encoder bottleneck directly limits compositional reasoning in these models.

## Method Summary
The authors develop a reconstruction probe methodology where T5-large generative decoders are trained to reconstruct original captions from single-vector text representations produced by various VL models. They curate two datasets: CompPrompts (18,100 compositional captions with varying complexity) and ControlledImCaps (600 image pairs with fine-grained caption differences). The reconstruction performance is evaluated using Exact Match (EM) and BLEU-4 scores, and correlated with multimodal matching performance on ControlledImCaps. They also implement a proof-of-concept encoder (proof-of-concepT5) specifically optimized for perfect reconstruction to validate their methodology.

## Key Results
- CLIP's text encoder struggles with spatial relations, attribute-object association, counting, and negations
- Text-only reconstruction performance predicts multimodal matching performance on ControlledImCaps
- Text-only recoverability is necessary but not sufficient for compositional reasoning in multimodal models
- Proof-of-concept encoder achieves 92.9% EM score, demonstrating reconstruction feasibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-only reconstruction probes reveal information loss in vision-language models
- Mechanism: By training a generative decoder to reconstruct captions from text embeddings, the probe measures how much linguistic information is preserved in the single-vector bottleneck
- Core assumption: If a text property cannot be recovered from the text embedding with strong supervision, it's unlikely the multimodal model can effectively use that property

### Mechanism 2
- Claim: Text-only reconstruction performance predicts multimodal matching performance
- Mechanism: Poor reconstruction of compositional properties (spatial relations, counting, negations) correlates with poor multimodal matching on controlled image-caption pairs
- Core assumption: Text encoder information loss directly limits the multimodal model's ability to reason about those properties

### Mechanism 3
- Claim: Proof-of-concept encoder validates the probe methodology
- Mechanism: A text encoder specifically trained to enable perfect reconstruction (proof-of-concepT5) achieves high performance, demonstrating that the task is feasible and the probe setup works
- Core assumption: If perfect reconstruction is possible in theory, then failure by other models indicates actual information loss rather than probe limitations

## Foundational Learning

- Concept: Contrastive vision-language learning
  - Why needed here: Understanding how CLIP and similar models align image and text representations is crucial for interpreting the text encoder bottleneck
  - Quick check question: What is the core training objective used by CLIP to align image and text representations?

- Concept: Compositional language understanding
  - Why needed here: The probe specifically tests compositional properties like spatial relations, attribute-object associations, and negations
  - Quick check question: How does compositional reasoning differ from simple object recognition in vision-language models?

- Concept: Probe methodology and information-theoretic evaluation
  - Why needed here: The reconstruction probe is a form of diagnostic evaluation that measures information preservation
  - Quick check question: What does it mean for an encoder to be "lossy" in an information-theoretic sense?

## Architecture Onboarding

- Component map: CompPrompts dataset -> Text encoders (CLIP, RoBERTa-CLIP, SBERT, proof-of-concepT5) -> T5-large decoder -> Reconstruction evaluation -> ControlledImCaps dataset -> Multimodal matching evaluation

- Critical path:
  1. Encode CompPrompts captions with text encoder
  2. Train T5 decoder to reconstruct original captions
  3. Evaluate reconstruction accuracy (EM, BLEU)
  4. Correlate reconstruction performance with multimodal matching on ControlledImCaps
  5. Analyze which compositional properties are most challenging

- Design tradeoffs:
  - Single-vector bottleneck vs. bidirectional architectures
  - Text-only probing vs. multimodal evaluation
  - Generative reconstruction vs. discriminative classification
  - Broad compositional coverage vs. fine-grained control

- Failure signatures:
  - High reconstruction error on specific compositional properties (spatial relations, negations)
  - Poor correlation between text-only and multimodal performance
  - Proof-of-concept encoder failing to achieve high reconstruction accuracy

- First 3 experiments:
  1. Run reconstruction probe on CLIP ViT-B/32 with CompPrompts to establish baseline performance
  2. Train proof-of-concepT5 encoder on CC3M and evaluate on CompPrompts to verify probe feasibility
  3. Evaluate CLIP ViT-L/14 on ControlledImCaps to compare with text-only reconstruction results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of vision-language models on compositional tasks scale with increasing model size beyond what was tested in this paper?
- Basis in paper: [explicit] The authors note that increasing model size helps overall but not on spatial relations, and suggest that more significant scaling could be beneficial.
- Why unresolved: The paper only tested two sizes of CLIP models (ViT-B/32 and ViT-L/14) and found limited improvement for spatial relations. It's unclear if even larger models would significantly improve performance on compositional tasks.
- What evidence would resolve it: Experiments with significantly larger vision-language models (e.g., GPT-4V or similar) on the CompPrompts and ControlledImCaps datasets would show if scaling continues to improve compositional understanding.

### Open Question 2
- Question: Can vision-language models be trained to better handle negations and counting without sacrificing performance on other tasks?
- Basis in paper: [explicit] The authors found that all models performed poorly on negations and multiples (counting), with average EM scores of 13.0 and 5.1 respectively.
- Why unresolved: While the paper demonstrates the problem, it doesn't explore potential solutions or training techniques that could improve performance on these specific tasks.
- What evidence would resolve it: Training vision-language models with specialized objectives or data augmentation techniques focused on negations and counting, then evaluating their performance on CompPrompts and ControlledImCaps compared to baseline models.

### Open Question 3
- Question: How do the findings from text-only reconstruction probes correlate with human judgments of image-text matching quality?
- Basis in paper: [explicit] The authors found that text-only recovery performance predicts multimodal matching performance, but also note that text-only recoverability is not sufficient for good multimodal performance.
- Why unresolved: The paper uses automatic evaluation metrics but doesn't compare these results with human evaluations of whether the models correctly understand the compositional aspects of image-text pairs.
- What evidence would resolve it: Human studies where participants judge the quality of image-text matches for the ControlledImCaps dataset, comparing these judgments with the automatic evaluation metrics used in the paper.

## Limitations

- Dataset Representativeness: The curated datasets (18,100 captions and 600 image pairs) may not fully capture the richness of compositional patterns in natural language
- Probe Architecture Constraints: Using T5-large as the decoder introduces potential confounding factors from the decoder's own architectural biases
- Correlation vs. Causation: The observed correlations between text-only and multimodal performance could be influenced by shared factors beyond text encoder compositionality

## Confidence

**High Confidence**: The claim that text encoders in contrastive VL models exhibit compositional reasoning limitations is strongly supported by systematic failures across different encoder variants.

**Medium Confidence**: The claim that text-only reconstruction performance predicts multimodal matching performance is reasonably well-supported by the ControlledImCaps evaluation.

**Low Confidence**: The broader claim that text-only recoverability is a necessary but not sufficient condition for compositional reasoning extends beyond the immediate experimental evidence.

## Next Checks

1. Evaluate the reconstruction probe methodology on large-scale, naturally occurring caption datasets (e.g., MS-COCO, Flickr30k) to test generalization beyond controlled settings.

2. Implement and evaluate reconstruction probes using different decoder architectures (e.g., GPT-style transformers, encoder-only models) to disentangle text encoder compositionality from probe limitations.

3. Design intervention experiments that systematically modify text encoder architectures while holding other factors constant to establish causal relationships between architectural choices and compositional reasoning capabilities.