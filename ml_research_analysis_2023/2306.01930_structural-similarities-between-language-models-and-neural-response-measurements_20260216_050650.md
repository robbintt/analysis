---
ver: rpa2
title: Structural Similarities Between Language Models and Neural Response Measurements
arxiv_id: '2306.01930'
source_url: https://arxiv.org/abs/2306.01930
tags:
- bert
- gpt2
- language
- csls-avg
- csls-max
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates structural similarities between word representations
  in large language models (LLMs) and neural response measurements from brain imaging
  (fMRI). The study explores whether LLMs exhibit human-like understanding by comparing
  the geometries of word representations induced by LLMs and neural response measurements
  from fMRI.
---

# Structural Similarities Between Language Models and Neural Response Measurements

## Quick Facts
- **arXiv ID**: 2306.01930
- **Source URL**: https://arxiv.org/abs/2306.01930
- **Reference count**: 40
- **Key outcome**: Large language models (LLMs) show increasing structural similarity to brain-like representations as model size increases, achieving up to 25% precision@10 word retrieval rates from fMRI data.

## Executive Summary
This study investigates whether large language models develop representations structurally similar to human brain activity patterns during language processing. Using three families of LLMs (BERT, GPT2, OPT), three fMRI datasets, and three projection methods, the authors demonstrate that as LLMs increase in size, their word representations become more aligned with neural response measurements. The results show that larger models achieve higher precision@10 retrieval rates (up to 25%), suggesting that LLMs may develop concept-based mental models similar to human semantic processing. This work provides empirical evidence for the structural convergence of artificial and biological language representations.

## Method Summary
The study uses three LLM families (BERT, GPT2, OPT) of varying sizes, three fMRI datasets (Harry Potter, Pereira, Natural Stories), and three projection algorithms (Procrustes Analysis, Ridge Regression, RSA) to measure alignment between model representations and brain activity. Linear projections map fMRI vectors to LLM representations, and word-level decoding is performed via nearest neighbor retrieval in the projected space. The alignment is evaluated using precision@k retrieval rates across different model sizes, layers, and projection methods. Gaussian smoothing is applied to fMRI data to extract word-level signals from sequence-level measurements.

## Key Results
- Larger LLMs show significantly higher alignment precision with fMRI representations (P@10 up to 25%)
- Deeper layers in LLMs consistently show better alignment with neural response measurements
- The alignment trend holds across all three LLM families and all three projection methods
- Structural similarity enables word-level decoding from brain activity with measurable precision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing LLM size improves alignment with brain-like representations
- Mechanism: Larger models develop richer concept-based mental models that mirror human semantic processing
- Core assumption: Model scaling leads to convergence on human-like conceptual representations
- Evidence anchors:
  - [abstract] "the larger neural language models get, the more their representations are structurally similar to neural response measurements from brain imaging"
  - [section] "Results: Convergence to Word-Level fMRI Measurements... Our main results... concern the convergence of three families of LLMs on representations that are remarkably similar to those seen in neural response measurements"
- Break condition: If scaling stops improving alignment or if alignment degrades with larger models

### Mechanism 2
- Claim: Deep layers in LLMs align better with neural response measurements
- Mechanism: Deeper representations capture more abstract semantic information that correlates with brain activity
- Core assumption: Semantic processing occurs at deeper layers in both LLMs and human brains
- Evidence anchors:
  - [section] "Results: Where are LLMs Most Brain-Like? We also consider at what layers the different language models align best with the representations extracted from the fMRI data... the alignment improvements at deeper layers do not wear off to reach a plateau"
  - [section] "Figure 3: Alignment precision results across layers. The alignment with fMRI improves with model depth, for BERT and Procrustes Analysis"
- Break condition: If shallow layers show equal or better alignment than deep layers

### Mechanism 3
- Claim: Structural similarity between LLM and fMRI representations enables word-level decoding
- Mechanism: Linear projections can map between LLM and brain spaces when their geometric structures are sufficiently similar
- Core assumption: Similar geometric structures across domains allow cross-domain alignment
- Evidence anchors:
  - [abstract] "we use three families of LLMs, three fMRI datasets, and three state-of-the-art projection algorithms to measure the alignment precision of linear projections between the two spaces"
  - [section] "We can compute the structural similarity (degree of isomorphism) between these two geometries by multi-way ridge regression, Representational Similarity Analysis, and Procrustes Analysis"
  - [section] "Word-level decoding thus amounts to simple nearest neighbor retrieval in the projected space"
- Break condition: If linear projections fail to achieve meaningful alignment despite structural similarity

## Foundational Learning

- **Concept**: Linear algebra and vector space geometry
  - Why needed here: Understanding how representations in different spaces can be aligned through linear transformations
  - Quick check question: Can you explain why Procrustes Analysis requires the same dimensionality in both spaces?

- **Concept**: Neural network architecture and depth
  - Why needed here: Understanding why deeper layers might capture more abstract semantic information
  - Quick check question: What is the difference between auto-regressive and non-auto-regressive language models in terms of information flow?

- **Concept**: fMRI data processing and temporal resolution
  - Why needed here: Understanding how brain activity measurements are aligned with word-level language model representations
  - Quick check question: Why is Gaussian smoothing used to generate word-level fMRI information from sequence-level data?

## Architecture Onboarding

- **Component map**: LLM families (BERT, GPT2, OPT) → fMRI datasets (Harry Potter, Pereira, Natural Stories) → Projection methods (Procrustes, Ridge Regression, RSA) → Alignment metrics (P@k scores)
- **Critical path**: Model representation extraction → fMRI data preprocessing → Linear projection learning → Alignment evaluation
- **Design tradeoffs**: Model size vs. computational cost, layer depth vs. alignment quality, projection method vs. alignment precision
- **Failure signatures**: Low P@k scores across all datasets, inconsistent alignment across model families, degradation of alignment with model scaling
- **First 3 experiments**:
  1. Replicate alignment results using Procrustes Analysis on BERT model family with Harry Potter dataset
  2. Compare alignment precision across layers within a single LLM family (BERT) using RSA
  3. Test alignment stability across different fMRI datasets with a fixed LLM and projection method

## Open Questions the Paper Calls Out

- **Open Question 1**: Does the convergence of LLM representations to brain-like representations indicate genuine understanding or just sophisticated pattern matching?
  - Basis in paper: [explicit] The paper directly addresses this philosophical question, stating "Our results suggest the answer to both questions are affirmative" regarding whether LLMs have rich concept-based mental models and whether scaling creates better concepts.
  - Why unresolved: The distinction between genuine understanding and sophisticated pattern matching remains philosophically contentious. The paper shows structural similarities but doesn't definitively prove the nature of the underlying cognitive processes.
  - What evidence would resolve it: Experiments demonstrating that LLMs can generalize understanding to novel situations beyond their training distribution, or behavioral tests that distinguish between pattern matching and genuine conceptual understanding.

- **Open Question 2**: How do different architectural choices in LLMs (attention mechanisms, layer depth, etc.) affect their convergence to brain-like representations?
  - Basis in paper: [inferred] The paper shows convergence across three families of LLMs with different architectures, but doesn't deeply analyze which architectural features contribute most to brain-like representations.
  - Why unresolved: The study compares entire model families but doesn't isolate specific architectural components. Different architectures may achieve similar convergence through different mechanisms.
  - What evidence would resolve it: Systematic ablation studies comparing brain alignment across architectural variants, or analysis of which model components most strongly correlate with brain-like representations.

- **Open Question 3**: What is the relationship between the degree of brain-like representation convergence and downstream task performance?
  - Basis in paper: [inferred] The paper shows that larger models with more brain-like representations achieve better retrieval rates, but doesn't establish a causal relationship or explore downstream task implications.
  - Why unresolved: While structural similarity to brain representations is demonstrated, its practical implications for language understanding and task performance remain unclear. Brain-like representations might be necessary but not sufficient for good performance.
  - What evidence would resolve it: Controlled experiments varying brain alignment while holding other factors constant, or analysis of whether brain-like representations predict better generalization to novel tasks.

## Limitations

- The study uses word-level fMRI data smoothed via Gaussian kernels, which may obscure fine-grained neural dynamics
- All fMRI datasets involve passive reading/listening paradigms, potentially missing full range of human semantic processing
- Focus on English language stimuli limits generalizability to other languages and cultures
- 25% P@10 retrieval rates, while statistically significant, may have limited practical significance for understanding cognition

## Confidence

- **High Confidence**: The empirical finding that larger LLMs show increased structural similarity to fMRI representations (P@10 up to 25%)
- **Medium Confidence**: The claim that deeper LLM layers show better alignment with neural responses
- **Medium Confidence**: The interpretation that structural similarity implies "human-like understanding" or "rich concept-based mental models"

## Next Checks

1. Test alignment precision on out-of-distribution linguistic phenomena (metaphors, idioms, abstract concepts) to determine if structural similarity extends beyond literal word meanings
2. Conduct ablation studies removing specific model components (attention mechanisms, feed-forward networks) to identify which architectural features contribute most to brain-like representations
3. Perform cross-linguistic validation using fMRI datasets from non-English languages to test whether structural similarities generalize across linguistic and cultural contexts