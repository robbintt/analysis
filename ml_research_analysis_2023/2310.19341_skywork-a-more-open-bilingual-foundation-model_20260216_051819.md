---
ver: rpa2
title: 'Skywork: A More Open Bilingual Foundation Model'
arxiv_id: '2310.19341'
source_url: https://arxiv.org/abs/2310.19341
tags:
- training
- language
- data
- pre-training
- skywork-13b
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Skywork-13B is a bilingual (English-Chinese) large language model
  with 13 billion parameters, trained on over 3.2 trillion tokens from web data. It
  introduces a two-stage training approach: general-purpose pre-training followed
  by domain-specific enhancement.'
---

# Skywork: A More Open Bilingual Foundation Model

## Quick Facts
- arXiv ID: 2310.19341
- Source URL: https://arxiv.org/abs/2310.19341
- Authors: 
- Reference count: 20
- Key outcome: Skywork-13B achieves state-of-the-art performance on Chinese benchmarks (CEVAL 60.6%, MMLU 62.1%, GSM8K 55.8%) using a two-stage training approach.

## Executive Summary
Skywork-13B is a 13-billion-parameter bilingual (English-Chinese) large language model that introduces a two-stage training methodology: general-purpose pre-training followed by domain-specific enhancement. The model achieves state-of-the-art performance on Chinese language modeling benchmarks and releases a high-quality open Chinese corpus (150B+ tokens). The paper also proposes a novel method for detecting data leakage in LLMs, addressing concerns about benchmark contamination. The model uses transformer architecture with specific modifications including RoPE positional embedding, RMSNorm, and SwiGLU activation.

## Method Summary
Skywork-13B employs a two-stage training approach using a segmented corpus. Stage-1 involves general-purpose pre-training on SkyPile-Main (3T tokens), while Stage-2 focuses on domain-specific enhancement using SkyPile-STEM (130B tokens mixed with SkyPile-Main). The model uses a transformer decoder with 52 layers, 4,608 hidden dimensions, and 36 attention heads. A novel 65,536-token vocabulary includes 8,000 single-character Chinese tokens and 25,519 frequent Chinese multi-character words. Training is monitored through validation loss across multiple held-out sets with different distributions to track generalization and detect overfitting.

## Key Results
- Achieves state-of-the-art performance on CEVAL benchmark (60.6% accuracy)
- Outperforms similar-sized models on MMLU (62.1%) and GSM8K (55.8%) tasks
- Releases a high-quality open Chinese corpus with 150B+ tokens

## Why This Works (Mechanism)

### Mechanism 1
Two-stage training with domain-specific enhancement outperforms single-stage pre-training on mixed data for specialized tasks. First-stage general pre-training builds broad linguistic and reasoning capabilities, while second-stage targeted training on in-domain data transfers domain-specific knowledge without catastrophic forgetting. The enhancement is effective when Stage-2 data is sufficiently large and diverse.

### Mechanism 2
Monitoring validation loss across multiple held-out sets with different distributions provides a more reliable proxy for downstream task performance than training loss alone. Validation loss on diverse held-out sets (web text, code, academic, social media) tracks generalization ability and early signs of overfitting, enabling more precise training progress monitoring.

### Mechanism 3
Extending vocabulary with domain-specific tokens (e.g., Chinese characters and words) improves bilingual modeling performance. Adding 8,000 single-character Chinese tokens and 25,519 frequent multi-character Chinese words increases token coverage and reduces out-of-vocabulary rate for Chinese text, improving token efficiency.

## Foundational Learning

- **Catastrophic forgetting in continual learning**: Stage-2 training could overwrite Stage-1 knowledge if not carefully managed. *Quick check*: How does mixing SkyPile-STEM with SkyPile-Main in Stage-2 mitigate catastrophic forgetting?

- **Data leakage in pre-training**: GSM8K-like samples in training could inflate benchmark scores and misrepresent true generalization. *Quick check*: What evidence from Table 8 suggests possible GSM8K data leakage in some models?

- **Validation loss as training proxy**: Training loss can be misleading due to multiple epochs and duplication; validation loss across domains tracks true progress. *Quick check*: Why does validation loss level off in later training stages even as training loss keeps decreasing?

## Architecture Onboarding

- **Component map**: Data ingestion -> BPE tokenization (65,536 vocab) -> Model forward pass (with RoPE) -> Loss computation -> Backpropagation -> Optimizer step (AdamW)

- **Critical path**: Data ingestion → BPE tokenization (65,536 vocab) → Model forward pass (with RoPE) → Loss computation → Backpropagation → Optimizer step (AdamW)

- **Design tradeoffs**: Narrower but deeper network (4,608 vs 5,120 hidden dim, 52 vs 40 layers) trades parameter count for depth. RoPE chosen over absolute positional embeddings for better extrapolation and length flexibility. RMSNorm pre-normalization improves stability vs post-normalization. Mixed precision (bfloat16) balances memory and precision.

- **Failure signatures**: Validation loss diverging across domains → overfitting or distribution shift. Training loss plateauing early → learning rate too low or data exhausted. Out-of-memory during training → insufficient parallelism or batch size too large.

- **First 3 experiments**:
  1. Verify tokenizer coverage: Sample Chinese and English text, confirm <5% OOV rate
  2. Validate parallelism scaling: Run small model (1B params) on 8 GPUs, measure throughput and memory
  3. Test two-stage training: Train Stage-1 only vs Stage-1+Stage-2, compare CEVAL and language modeling loss

## Open Questions the Paper Calls Out

### Open Question 1
How does the two-stage pre-training approach compare to one-stage pre-training on a mixed corpus in terms of final model performance? The authors mention that it remains unclear whether the two-stage training methodology can produce a model on par with or superior to a model trained in one stage on a mixed corpus. A controlled experiment comparing the final performance of models trained using the two-stage approach versus the one-stage approach on the same tasks and datasets would resolve this.

### Open Question 2
How does the language modeling evaluation's correlation with downstream performance vary across different data distributions and tasks? The authors acknowledge that language modeling evaluation relies on the specific distribution used to sample test data, and while it may predict performance on some tasks, it may not translate to other tasks. A comprehensive study examining the correlation between language modeling perplexity and downstream task performance across a wide range of data distributions and tasks would resolve this.

### Open Question 3
To what extent does pre-training on in-domain data for specific tasks lead to overfitting and compromised fairness in benchmarking? The authors discuss the benefits of pre-training on in-domain data for improving task performance but also acknowledge the risk of overfitting and compromised fairness in benchmarking. A detailed analysis of the impact of in-domain pre-training on model performance across a diverse set of tasks, including both seen and unseen tasks, would resolve this.

## Limitations

- Proprietary training data (SkyPile) construction pipeline remains unspecified, limiting reproducibility
- Critical balance between general and domain-specific data mixing in Stage-2 is not fully quantified
- Data leakage detection methodology lacks sufficient detail for independent verification

## Confidence

- **High confidence**: Two-stage training methodology and implementation details (RoPE, RMSNorm, SwiGLU architecture choices)
- **Medium confidence**: State-of-the-art performance claims on Chinese benchmarks
- **Low confidence**: Data leakage detection methodology and its application to specific benchmarks

## Next Checks

1. Implement and test the data leakage detection methodology on a held-out GSM8K subset to verify its effectiveness in identifying contamination across different model sizes and training stages.

2. Conduct ablation studies comparing single-stage vs two-stage training with varying ratios of STEM to general data in Stage-2 to quantify the optimal mixing ratio and its impact on catastrophic forgetting.

3. Replicate the validation loss monitoring approach across multiple held-out distributions (code, academic, social media) during training to empirically verify its correlation with downstream task performance and its utility in detecting overfitting.