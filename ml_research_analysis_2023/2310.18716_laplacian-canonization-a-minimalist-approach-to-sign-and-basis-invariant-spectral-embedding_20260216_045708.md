---
ver: rpa2
title: 'Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant
  Spectral Embedding'
arxiv_id: '2310.18716'
source_url: https://arxiv.org/abs/2310.18716
tags:
- sign
- eigenvectors
- graph
- basis
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Laplacian Canonization (LC), a pre-processing
  method that resolves sign and basis ambiguities in spectral embedding (SE) by finding
  canonical directions for eigenvectors. The authors develop a theoretical framework
  for LC and propose an efficient algorithm, Maximal Axis Projection (MAP), which
  successfully canonizes over 90% of eigenvectors in real-world datasets.
---

# Laplacian Canonization: A Minimalist Approach to Sign and Basis Invariant Spectral Embedding

## Quick Facts
- arXiv ID: 2310.18716
- Source URL: https://arxiv.org/abs/2310.18716
- Reference count: 40
- Primary result: MAP algorithm successfully canonizes over 90% of eigenvectors in real-world datasets

## Executive Summary
This paper introduces Laplacian Canonization (LC), a pre-processing method that resolves sign and basis ambiguities in spectral embedding (SE) by finding canonical directions for eigenvectors. The authors develop a theoretical framework for LC and propose an efficient algorithm, Maximal Axis Projection (MAP), which successfully canonizes over 90% of eigenvectors in real-world datasets. MAP achieves sign invariance by selecting eigenvectors with positive projections onto summary vectors, and basis invariance by iteratively choosing eigenvectors with maximum projections onto orthogonal complement spaces. Experiments on ZINC, MOLTOX21, and MOLPCBA datasets show that MAP consistently outperforms existing methods like RandSign and SignNet while introducing minimal computational overhead.

## Method Summary
Laplacian Canonization addresses the fundamental problem of sign and basis ambiguities in spectral embedding for graph neural networks. The method uses Maximal Axis Projection (MAP) to canonize eigenvectors by finding canonical directions that eliminate these ambiguities. MAP operates in two stages: first resolving sign ambiguity by selecting eigenvectors with positive projections onto summary vectors, then resolving basis ambiguity for multiple eigenvalues by iteratively choosing eigenvectors with maximum projections onto orthogonal complement spaces. The approach is combined with reweighted spectral embedding (RSE) that incorporates eigenvalue information through square-root reweighting, enabling first-order GNNs to achieve universal expressive power while preserving permutation invariance.

## Key Results
- MAP achieves sign invariance by selecting eigenvectors with positive projections onto summary vectors
- MAP eliminates basis ambiguity by iteratively choosing eigenvectors with maximum projections onto orthogonal complement spaces
- MAP successfully canonizes over 90% of eigenvectors in real-world datasets (ZINC, MOLTOX21, MOLPCBA)
- RSE with MAP enables universal expressive power for first-order GNNs while preserving permutation invariance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAP eliminates sign ambiguity by selecting eigenvectors with positive projections onto summary vectors, ensuring permutation equivariance and uniqueness.
- Mechanism: For each eigenvector, MAP calculates angles between the eigenspace and standard basis vectors, groups them by equivalent classes, and selects the eigenvector with positive projection onto the summary vector of the group with smallest angle.
- Core assumption: There exists a summary vector non-orthogonal to the eigenvector (Assumption 1).
- Evidence anchors: [abstract]: "MAP achieves sign invariance by selecting eigenvectors with positive projections onto summary vectors" [section]: "we can utilize xh to determine the canonical direction u* by requiring its projected angle to be positive"
- Break condition: If Assumption 1 is violated (eigenvector is perpendicular to all summary vectors), MAP cannot canonize the eigenvector.

### Mechanism 2
- Claim: MAP eliminates basis ambiguity by iteratively choosing eigenvectors with maximum projections onto orthogonal complement spaces, preserving permutation equivariance and universality.
- Mechanism: For eigenvalues with multiplicity > 1, MAP divides standard basis vectors into groups based on projection angles, then iteratively selects each eigenvector to maximize projection onto the summary vector of the corresponding group in the orthogonal complement of previously selected eigenvectors.
- Core assumption: Number of distinctive angles ≥ multiplicity (Assumption 2) and each summary vector is not perpendicular to the orthogonal complement space (Assumption 3).
- Evidence anchors: [abstract]: "basis invariance by iteratively choosing eigenvectors with maximum projections onto orthogonal complement spaces" [section]: "we choose ui to be the vector that is closest to the summary vector xi in the orthogonal complement space"
- Break condition: If either Assumption 2 or Assumption 3 is violated, MAP cannot canonize the eigenspace.

### Mechanism 3
- Claim: RSE with MAP enables universal expressive power for first-order GNNs while preserving permutation invariance.
- Mechanism: RSE reweights eigenvectors with square-root of eigenvalues, incorporating eigenvalue information without extra dimensions. Combined with MAP's elimination of ambiguities, this allows even simple GNNs to approximate any continuous invariant graph function.
- Core assumption: The graph function can be ε-approximated by an Lp-Lipschitz continuous function.
- Evidence anchors: [abstract]: "RSE achieves universal expressive power while preserving permutation invariance" [section]: "With the reweighting technique, RSE incorporates eigenvalue information without need extra dimensions"
- Break condition: If the function cannot be approximated by an Lp-Lipschitz continuous function, the universal approximation guarantee fails.

## Foundational Learning

- Concept: Eigenvalue decomposition and spectral embedding
  - Why needed here: Understanding how spectral embedding encodes graph structure through eigenvectors and eigenvalues is fundamental to grasping MAP's approach to resolving ambiguities.
  - Quick check question: What information is lost when using only eigenvectors without eigenvalues in spectral embedding?

- Concept: Permutation invariance and equivariance
  - Why needed here: MAP must preserve permutation invariance/equivariance while resolving sign and basis ambiguities, which is crucial for graph neural networks.
  - Quick check question: How does MAP ensure that the canonized eigenvectors remain permutation-equivariant?

- Concept: Group theory and canonical forms
  - Why needed here: The theoretical framework for Laplacian Canonization relies on group actions and canonical forms to characterize canonizability of eigenvectors.
  - Quick check question: What is the difference between a canonization algorithm being invariant vs. equivariant?

## Architecture Onboarding

- Component map:
  Input: Graph (V, E, X) → Eigenvalue decomposition → MAP (sign and basis canonization) → RSE → GNN (e.g., GatedGCN, PNA, SAN, GraphiT) → Output: Graph-level prediction

- Critical path:
  1. Compute eigendecomposition of normalized adjacency matrix
  2. Apply MAP to eliminate sign ambiguity for all eigenvectors
  3. Apply MAP to eliminate basis ambiguity for multiple eigenvalues
  4. Compute RSE by reweighting eigenvectors with square-root of eigenvalues
  5. Concatenate processed spectral embedding with original node features
  6. Feed into GNN model

- Design tradeoffs:
  - MAP introduces minimal computational overhead (O(n³)) compared to eigendecomposition itself
  - Some eigenvectors may remain uncanonizable, requiring fallback to RandSign or SignNet
  - MAP preserves permutation equivariance but may sacrifice some expressive power for uncanonizable eigenvectors

- Failure signatures:
  - Poor performance on datasets with many multiple eigenvalues
  - Inconsistent results across different runs due to uncanonizable eigenvectors
  - Computational bottleneck if n is very large (>10,000 nodes)

- First 3 experiments:
  1. Apply MAP to a small synthetic graph with known ambiguities and verify canonical directions
  2. Compare GNN performance with/without MAP on ZINC dataset using GatedGCN
  3. Measure pre-processing time and training time with MAP vs. SignNet on a large dataset (e.g., MOLPCBA)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the Laplacian Canonization (LC) algorithm be extended to handle non-graph structured data, such as point clouds or meshes, while preserving their respective invariances?
- Basis in paper: [explicit] The paper focuses on graph data and discusses the sign and basis ambiguities in spectral embedding of graphs.
- Why unresolved: The paper does not explore the applicability of LC to other types of structured data. Extending LC to different domains would require addressing unique challenges and invariances specific to those data structures.
- What evidence would resolve it: Demonstrating the effectiveness of LC on non-graph structured data, such as point clouds or meshes, and comparing its performance with existing methods in those domains.

### Open Question 2
- Question: How does the choice of the constant c in the summary vector xi affect the performance and canonizability of the eigenvectors in the MAP algorithm?
- Basis in paper: [explicit] The paper mentions that c is a tunable constant in the summary vector xi, but does not provide specific guidance on its selection or discuss its impact on the algorithm's performance.
- Why unresolved: The paper does not provide empirical or theoretical analysis of the effect of c on the algorithm's performance or the canonizability of eigenvectors.
- What evidence would resolve it: Conducting experiments to evaluate the sensitivity of MAP's performance to different values of c, and providing theoretical insights into the role of c in the algorithm's design.

### Open Question 3
- Question: Can the MAP algorithm be further optimized to handle cases where the assumptions (Assumptions 1, 2, and 3) are violated more frequently, thereby improving its generalizability to a wider range of graphs?
- Basis in paper: [explicit] The paper discusses the assumptions under which MAP is guaranteed to canonize eigenvectors, but also acknowledges that these assumptions may be violated in some cases.
- Why unresolved: The paper does not provide a solution for handling cases where the assumptions are frequently violated, which may limit the applicability of MAP to certain types of graphs.
- What evidence would resolve it: Developing and evaluating modifications to the MAP algorithm that can handle cases where the assumptions are violated more frequently, and demonstrating its improved performance on a wider range of graphs.

## Limitations
- The paper lacks detailed analysis of failure cases and conditions under which Assumptions 1-3 are violated
- No proof of sufficiency for the theoretical framework's characterization of canonizable eigenvectors
- Does not discuss sensitivity to the choice of summary vectors or handling of edge cases

## Confidence

**Confidence: Low** for the claim that MAP successfully canonizes over 90% of eigenvectors in real-world datasets. The paper provides experimental results but lacks detailed analysis of failure cases and the specific conditions under which Assumption 1-3 are violated. The paper does not discuss how sensitive MAP is to the choice of summary vectors or how it handles edge cases where eigenvectors are nearly orthogonal to all summary vectors.

**Confidence: Medium** for the theoretical framework's completeness. While the paper establishes necessary conditions for canonizability, it does not prove sufficiency or provide a comprehensive characterization of uncanonizable eigenvectors. The relationship between eigenvalue multiplicity and distinctive angles (Assumption 2) needs more rigorous validation.

**Confidence: High** for the computational complexity claim of O(n³), as this aligns with standard eigendecomposition complexity and MAP's algorithmic structure.

## Next Checks

1. **Empirical Validation of Assumptions**: Conduct controlled experiments on synthetic graphs where the conditions of Assumptions 1-3 can be precisely manipulated. Measure the success rate of MAP under varying degrees of eigenvector orthogonality to summary vectors and different eigenvalue multiplicities.

2. **Robustness Analysis**: Test MAP on graphs with known challenging spectral properties (e.g., bipartite graphs, complete multipartite graphs) to identify failure modes and quantify the frequency of uncanonizable eigenvectors in practice.

3. **Performance Sensitivity**: Perform ablation studies varying the number of eigenvectors used (k) and the choice of summary vectors to determine the impact on both canonization success rate and downstream GNN performance. This would clarify whether the 90% canonization claim holds across different parameter settings.