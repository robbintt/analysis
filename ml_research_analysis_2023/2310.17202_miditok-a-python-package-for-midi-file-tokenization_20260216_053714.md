---
ver: rpa2
title: 'miditok: A Python package for MIDI file tokenization'
arxiv_id: '2310.17202'
source_url: https://arxiv.org/abs/2310.17202
tags:
- music
- tokens
- time
- miditok
- midi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MidiTok is an open-source Python library for tokenizing symbolic
  music, enabling the use of language models like Transformers for music generation,
  modeling, and transcription. It implements popular music tokenizations (MIDILike,
  REMI, etc.) under a unified API, offering flexibility and extended features such
  as Byte Pair Encoding (BPE) for sequence length reduction, additional tokens (chords,
  tempo, etc.), and data augmentation.
---

# miditok: A Python package for MIDI file tokenization

## Quick Facts
- arXiv ID: 2310.17202
- Source URL: https://arxiv.org/abs/2310.17202
- Reference count: 5
- Primary result: Open-source Python library implementing popular music tokenizations under a unified API, with over 75k downloads and integration in production tools like Qosmo's Neutone plugin.

## Executive Summary
MidiTok is an open-source Python library designed to tokenize symbolic music for use with language models like Transformers. It implements popular tokenization strategies (MIDILike, REMI, Compound Word, Octuple, etc.) under a unified API, enabling music generation, modeling, and transcription tasks. The library offers advanced features including Byte Pair Encoding for sequence length reduction, additional tokens for musical elements like chords and tempo, and data augmentation capabilities. MidiTok has gained significant traction in the music information retrieval community and is used in production AI music tools.

## Method Summary
MidiTok provides a comprehensive workflow for converting MIDI files into token sequences suitable for language models. The process involves preprocessing MIDI files to extract musical events, tokenizing different event types (global MIDI events, track events, and time events), and implementing various tokenization strategies. The library supports multiple tokenization approaches including MIDILike, REMI, REMI+, Structured, TSD, Compound Word, Octuple, MuMIDI, and MMM. Users can configure tokenization parameters through a TokenizerConfig object, apply Byte Pair Encoding for compression, add musical information tokens, and leverage PyTorch-compatible data loading and augmentation features.

## Key Results
- Implements 8+ popular tokenization strategies under a unified API
- Byte Pair Encoding reduces sequence lengths while maintaining or improving model performance
- Additional tokens (chords, tempo, etc.) improve generated music quality according to human evaluations
- Over 75k downloads and integration in production tools like Qosmo's Neutone plugin

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Byte Pair Encoding (BPE) applied to symbolic music drastically reduces token sequence length without degrading model performance.
- Mechanism: BPE compresses recurring byte-level patterns in token streams into new tokens, collapsing repeated sequences (e.g., repeated note-velocity-duration triplets) into single tokens.
- Core assumption: Symbolic music contains frequent repeating token patterns that are compressible; BPE preserves enough structure for language models to learn meaningful representations.
- Evidence anchors: "BPE allows to drastically reduce the sequence length, while taking benefit of the embedding spaces of models such as Transformers" and "Table 2 shows the sequence length reduction offered by BPE."

### Mechanism 2
- Claim: Multi-vocabulary tokenizations (Compound Word, Octuple, MuMIDI) reduce sequence length by merging embeddings, enabling larger context windows in Transformers.
- Mechanism: Instead of emitting a separate token for each attribute (pitch, velocity, duration, etc.), these tokenizations concatenate attributes into a single token embedding before model input, reducing the number of tokens fed to the model.
- Core assumption: Embedding projection preserves the relational structure of attribute combinations, and the model can decode them reliably.
- Evidence anchors: "Compound Word, Octuple and PopMAG merge embedding vectors before passing them to the model" and "multi-vocabulary tokenizations were introduced with the main goal to reduce the length of the sequence of embeddings processed by the model."

### Mechanism 3
- Claim: Additional tokens (chords, tempo, sustain pedal, pitch bend, etc.) improve model quality by explicitly modeling non-note musical attributes.
- Mechanism: By emitting dedicated tokens for global or track-level events, the model can attend to these features independently, rather than inferring them from note sequences alone.
- Core assumption: These attributes carry predictive signal that benefits generation or transcription tasks, and their token representations are distinguishable from note tokens.
- Evidence anchors: "Huang et al reports that using Tempo and Chord tokens for a generative Transformer yielded generated results of better quality, that were preferred from human evaluators."

## Foundational Learning

- Concept: MIDI file structure and event sequencing
  - Why needed here: Tokenizers must correctly interpret NoteOn/NoteOff events, tempo changes, and time signature events before encoding.
  - Quick check question: What is the difference between NoteOn and NoteOff events, and how do they relate to onset/offset times?

- Concept: Tokenization vs. embedding in NLP
  - Why needed here: Tokenizers convert raw music into discrete sequences; embeddings map those tokens into continuous vectors for Transformers.
  - Quick check question: How does a vocabulary size impact embedding dimensionality and model capacity?

- Concept: Byte Pair Encoding basics
  - Why needed here: BPE is the core compression mechanism; understanding byte-level merges is critical to debugging vocabulary size changes.
  - Quick check question: If a token sequence is "A B A B A", what would BPE produce after one merge step?

## Architecture Onboarding

- Component map: MIDI file → TokenizerConfig → preprocessing → event gathering → time tokenization → token list → embedding lookup → model → detokenization → MIDI file
- Critical path: MIDI → preprocess → event gathering → time tokenization → token list → embedding lookup → model → detokenization → MIDI
- Design tradeoffs:
  - One token stream vs. per-track: one stream reduces sequence count but merges instrument semantics; per-track preserves independence but increases sequence count.
  - BPE vs. multi-vocabulary: BPE simpler but slower tokenization; multi-vocabulary reduces sequence length but requires custom model heads.
  - High resolution vs. low resolution: high preserves nuance but increases token count and sparsity.
- Failure signatures:
  - Wrong token order → detokenization produces wrong timing.
  - Off-by-one in downsampling → pitch/velocity values shifted.
  - BPE merges too aggressively → ambiguous detokenization.
- First 3 experiments:
  1. Load a simple MIDI file with a single piano track; run MIDILike and REMI tokenizers; compare token counts and detokenization accuracy.
  2. Apply BPE with vocabulary sizes 1k, 5k, 10k on the same MIDI; observe sequence length reduction and tokenization time changes.
  3. Add chord and tempo tokens to REMI; generate a short sequence with a Transformer; evaluate if tempo/chord tokens influence the generated melody.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal BPE vocabulary size for symbolic music tokenization to balance sequence length reduction and model performance?
- Basis in paper: [explicit] The paper mentions BPE can reduce sequence lengths but increases tokenization time, and presents a table comparing different BPE sizes (1k, 5k, 10k, 20k) without definitive conclusions on optimal size.
- Why unresolved: Different tokenizations (TSD, REMI, Compound Word, Octuple) show varying results with BPE, and the optimal size likely depends on the specific use case and model architecture.
- What evidence would resolve it: Systematic experiments comparing model performance (e.g., perplexity, generation quality) across different BPE vocabulary sizes for various tokenizations and music datasets.

### Open Question 2
- Question: How do additional tokens (chords, tempo, time signature, etc.) impact the quality of generated music across different tokenizations?
- Basis in paper: [explicit] The paper states that using additional tokens can help model harmony and improve generation quality, but doesn't provide comprehensive empirical evidence comparing their impact across tokenizations.
- Why unresolved: While some studies mention benefits of specific additional tokens, there's no systematic comparison of their impact on generated music quality across different tokenizations and tasks.
- What evidence would resolve it: Large-scale user studies and objective metrics comparing generated music quality with and without different combinations of additional tokens across multiple tokenizations and tasks.

### Open Question 3
- Question: What is the most effective data augmentation strategy for symbolic music to improve model generalization without introducing unrealistic artifacts?
- Basis in paper: [explicit] The paper mentions data augmentation techniques (velocity, duration, pitch shifts) but doesn't provide guidance on their optimal application or potential drawbacks.
- Why unresolved: While augmentation can improve generalization, excessive or inappropriate transformations might introduce unrealistic musical patterns that could harm model performance.
- What evidence would resolve it: Comparative studies evaluating different augmentation strategies on model performance and the realism of generated music, potentially using human evaluations and music theory experts.

## Limitations

- Limited empirical validation of performance claims - the paper provides strong architectural details but limited direct performance benchmarks
- Reliance on external citations for key claims about BPE effectiveness and multi-vocabulary tokenization benefits
- Only one citation for chord and tempo token improvements, without MidiTok-specific validation data
- Real-world adoption metrics (downloads, plugin integration) demonstrate usage but not technical superiority

## Confidence

**High Confidence**: The basic tokenization workflow (MIDI → event preprocessing → token sequence → detokenization) and the implementation of multiple tokenization strategies (MIDILike, REMI, etc.) are well-documented and reproducible.

**Medium Confidence**: Claims about BPE reducing sequence length and improving inference speed are supported by references but lack direct MidiTok-specific benchmarks.

**Low Confidence**: Claims about multi-vocabulary tokenizations improving performance and additional tokens (chords, tempo) enhancing generation quality rely almost entirely on external citations without MidiTok-specific validation data.

## Next Checks

1. **Direct Performance Comparison**: Run a controlled experiment comparing Transformer models trained on MIDI sequences tokenized with and without BPE, measuring both sequence length reduction and downstream task performance (e.g., next-note prediction accuracy or generation diversity).

2. **Multi-Vocabulary vs BPE Tradeoff**: Implement the same compression goal using both approaches on identical datasets, measuring tokenization speed, sequence length, model training time, and generation quality to determine which approach better serves different use cases.

3. **Additional Token Impact Study**: Train models with identical architectures but varying sets of additional tokens (chords only, tempo only, both, neither) on the same dataset, then evaluate generation quality through both automated metrics (repetition, diversity) and human preference testing.