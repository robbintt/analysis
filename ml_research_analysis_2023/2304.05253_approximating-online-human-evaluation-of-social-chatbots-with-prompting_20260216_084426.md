---
ver: rpa2
title: Approximating Online Human Evaluation of Social Chatbots with Prompting
arxiv_id: '2304.05253'
source_url: https://arxiv.org/abs/2304.05253
tags:
- evaluation
- dialog
- speaker
- listener
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a fully automated framework for evaluating
  social chatbots that leverages large language models to both generate interactive
  dialogues and produce evaluation scores. The approach uses prompted LLM-to-bot play
  to simulate realistic social interactions by conditioning the LLM to play a specific
  conversational role, and then applies prompting techniques to generate dialog-level
  scores.
---

# Approximating Online Human Evaluation of Social Chatbots with Prompting

## Quick Facts
- arXiv ID: 2304.05253
- Source URL: https://arxiv.org/abs/2304.05253
- Reference count: 11
- This paper introduces a fully automated framework for evaluating social chatbots that leverages large language models to both generate interactive dialogues and produce evaluation scores.

## Executive Summary
This paper presents a novel automated framework for evaluating social chatbots that uses large language models (LLMs) to simulate interactive dialogues and generate evaluation scores. The approach, called DEP (Dialogue Evaluation with Prompting), employs LLM-to-bot play where the LLM plays a specific conversational role, followed by prompted evaluation using instructions and demonstrations. The method achieves up to 0.95 Pearson correlation with human judgment at the system level when using well-crafted prompts with few-shot demonstrations. The framework addresses the critical need for scalable, interactive evaluation metrics that can capture subjective social perceptions in conversational AI systems.

## Method Summary
The DEP framework generates synthetic dialogues by prompting an LLM to play a specific social role in interactions with chatbots, then evaluates these dialogues using carefully crafted prompts that include instructions and few-shot demonstrations. The evaluation scores are aggregated at the dialog level and then averaged to produce system-level ratings for fair comparison of chatbots. The approach is demonstrated on empathetic dialogue systems using the EmpatheticDialogues dataset for scenario generation and the iEval dataset for evaluation, showing strong correlation with human judgment when optimal prompt configurations are used.

## Key Results
- Achieved up to 0.95 Pearson correlation with human judgment at system level
- Demonstrated effectiveness on empathetic dialogue systems and open-domain conversations
- Showed that prompts with demonstrations and instructions outperform simple prompts
- Framework successfully generates synthetic dialogues that mirror human conversational patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompting an LLM to play a specific social role improves the quality of synthetic dialogues used for evaluation.
- Mechanism: By conditioning the LLM with a role prompt (e.g., "I am a Speaker feeling [emotion] because [situation]"), the LLM generates responses that mirror the conversational patterns of human speakers in similar emotional contexts.
- Core assumption: LLMs can be reliably conditioned to adopt and maintain specific conversational roles over multi-turn interactions.
- Evidence anchors:
  - [abstract] "The DEP approach involves collecting synthetic chat logs of evaluated bots with an LLM in the other-play setting, where the LLM is carefully conditioned to follow a specific scenario."
  - [section] "To generate LLM-to-bots conversations, we closely followed the procedure of iEval dataset curation. Specifically, we used emotion labels and situation descriptions from the dataset to create prompts for the LLM."
  - [corpus] Weak/no evidence for consistency of role adoption across different LLMs.
- Break condition: If the LLM fails to maintain the assigned role or produces inconsistent responses, the synthetic dialogues will not accurately represent human conversations.

### Mechanism 2
- Claim: Including demonstrations and instructions in prompts leads to better evaluation performance.
- Mechanism: Demonstrations provide examples of high and low-quality dialogues, while instructions guide the LLM on how to relate fine-grained qualities to an overall score. This combination helps the LLM calibrate its scoring.
- Core assumption: LLMs can effectively use few-shot demonstrations and instructions to align their scoring with human judgment.
- Evidence anchors:
  - [abstract] "The best performing prompts, which contain few-shot demonstrations and instructions, show outstanding performance on the tested dataset."
  - [section] "To write the instructions, we relied on the findings of Svikhnushina et al. (2022), which explained how chatbots' performance on various fine-grained dimensions translates in the overall score."
  - [corpus] Weak/no evidence for the effectiveness of this approach with different LLMs or datasets.
- Break condition: If the demonstrations or instructions are not representative or clear, the LLM's scoring may not align with human judgment.

### Mechanism 3
- Claim: Aggregating dialog-level scores to produce system-level ratings enables fair comparison of chatbots.
- Mechanism: By collecting multiple dialogues per chatbot and averaging the scores, the evaluation accounts for variability and provides a more robust assessment of each system's performance.
- Core assumption: The number of dialogues collected per chatbot is sufficient to capture the variability in performance.
- Evidence anchors:
  - [abstract] "The final rating of a dialog system j is obtained by averaging the corresponding dialog scores."
  - [section] "For fair evaluation, the number of dialogs collected for each evaluated chatbot should be identical."
  - [corpus] Weak/no evidence for the optimal number of dialogues needed for reliable system-level ratings.
- Break condition: If the number of dialogues is too small, the system-level ratings may not be reliable or representative.

## Foundational Learning

- Concept: Prompt engineering and few-shot learning
  - Why needed here: To effectively condition LLMs for role-play and evaluation scoring.
  - Quick check question: What are the key components of a prompt that includes both demonstrations and instructions?

- Concept: Dialog system evaluation metrics
  - Why needed here: To understand how to measure and compare the performance of chatbots.
  - Quick check question: What is the difference between utterance-level and dialog-level evaluation?

- Concept: Correlation analysis
  - Why needed here: To assess the alignment between automated evaluation scores and human judgment.
  - Quick check question: What does a high Pearson correlation coefficient indicate about the relationship between two variables?

## Architecture Onboarding

- Component map:
  - LLM-to-Bot Play: Generates synthetic dialogues by prompting the LLM to play a specific social role.
  - Prompted Evaluation: Uses prompts with demonstrations and instructions to score the dialogues.
  - Score Aggregation: Averages dialog-level scores to produce system-level ratings.
  - Correlation Analysis: Compares automated scores with human judgment.

- Critical path:
  1. Collect synthetic dialogues using LLM-to-Bot Play.
  2. Score the dialogues using Prompted Evaluation.
  3. Aggregate scores to obtain system-level ratings.
  4. Analyze correlation with human judgment.

- Design tradeoffs:
  - Number of dialogues vs. evaluation time: More dialogues provide more robust ratings but increase evaluation time.
  - Specificity of prompts vs. generalization: Highly specific prompts may not generalize well to different contexts or LLMs.

- Failure signatures:
  - Low correlation with human judgment: May indicate issues with prompt design or LLM conditioning.
  - Inconsistent scores across dialogues: May suggest the need for more dialogues or better prompt instructions.

- First 3 experiments:
  1. Vary the number of demonstrations in the prompt and measure the impact on evaluation performance.
  2. Test the approach with different LLMs to assess generalizability.
  3. Apply the framework to evaluate chatbots on different social dimensions (e.g., toxicity, humor).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using different LLM sizes on the performance of the DEP framework?
- Basis in paper: [inferred] The paper mentions that they used the most capable version of InstructGPT available at the time, but it does not explore the impact of using different LLM sizes.
- Why unresolved: The paper only used one specific LLM model and size, so it's unclear how the results would generalize to other LLM sizes or architectures.
- What evidence would resolve it: Experiments comparing the performance of DEP using different LLM sizes (e.g., smaller models like GPT-3.5 vs. larger models like GPT-4) on the same datasets would help determine the impact of model size on evaluation quality.

### Open Question 2
- Question: How does the DEP framework perform when evaluating dialog systems for other social phenomena beyond empathy and open-domain conversations?
- Basis in paper: [inferred] The paper demonstrates DEP on empathetic dialogues and open-domain conversations, but suggests it could be applied to other social phenomena like toxicity or humor evaluation.
- Why unresolved: The paper does not provide empirical results for these other social phenomena, so the generalizability of DEP to different social domains remains untested.
- What evidence would resolve it: Applying DEP to evaluate dialog systems on tasks like detecting toxic language or generating humorous responses, and comparing the results with human judgments, would demonstrate its applicability to other social phenomena.

### Open Question 3
- Question: How does the DEP framework handle factual accuracy and truthfulness in dialog evaluation?
- Basis in paper: [explicit] The paper acknowledges that LLMs used in DEP can suffer from hallucinations and may have difficulty tracking factual information in dialogs.
- Why unresolved: While the paper notes this limitation, it does not propose solutions or evaluate how well DEP handles factual accuracy compared to other evaluation metrics.
- What evidence would resolve it: Experiments comparing DEP's ability to detect factual errors or inconsistencies in dialogs against human judgment or specialized factual accuracy metrics would help assess its performance on this dimension.

## Limitations
- Limited evidence of performance with different LLMs beyond GPT-3.5
- Results only demonstrated on a single dataset (iEval)
- Prompt templates and instructions appear to be carefully tuned for specific task
- No evaluation of framework's ability to handle factual accuracy or truthfulness

## Confidence

**High Confidence**: The basic framework of using LLM-to-bot play for synthetic dialogue generation and prompted evaluation with demonstrations and instructions is technically sound and the mechanism is well-supported by the results.

**Medium Confidence**: The claim that this approach generalizes to other social domains beyond empathetic dialogue is plausible but untested in the paper. The mechanism of using few-shot demonstrations and instructions to align LLM scoring with human judgment works well for this specific dataset.

**Low Confidence**: The assertion that this method can fully replace human evaluation in all contexts. The paper only demonstrates performance on one dataset, and the prompts and demonstrations are likely tuned specifically for that context.

## Next Checks

1. Test the framework with different LLMs (Claude, PaLM, Llama) to verify that the evaluation method generalizes beyond GPT-3.5.

2. Apply the same prompt templates and instructions to a completely different evaluation dataset (e.g., toxicity detection or humor assessment) to assess domain transfer.

3. Systematically vary the number of demonstrations and instructions in prompts to identify the minimal effective configuration and understand the contribution of each component.