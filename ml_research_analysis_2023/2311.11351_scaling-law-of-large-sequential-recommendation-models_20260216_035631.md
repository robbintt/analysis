---
ver: rpa2
title: Scaling Law of Large Sequential Recommendation Models
arxiv_id: '2311.11351'
source_url: https://arxiv.org/abs/2311.11351
tags:
- recommendation
- scaling
- performance
- data
- sequential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the scaling laws of large sequential recommendation
  models, aiming to explore how scaling affects the performance of recommendation
  models. The study is motivated by the potential of scaling to improve model capacity
  in various fields, including natural language processing and computer vision.
---

# Scaling Law of Large Sequential Recommendation Models

## Quick Facts
- arXiv ID: 2311.11351
- Source URL: https://arxiv.org/abs/2311.11351
- Reference count: 40
- Key outcome: Scaling laws apply to ID-based sequential recommendation models, enabling performance prediction and significant gains on challenging tasks

## Executive Summary
This paper investigates scaling laws for large sequential recommendation models, exploring how model size affects performance in ID-based sequential recommendation. The authors scale transformer models up to 0.8 billion parameters and demonstrate that power-law scaling relationships hold even in data-constrained scenarios. They develop improved training strategies including layer-wise adaptive dropout and optimizer switching, and validate performance benefits across five challenging recommendation tasks including cold start, robustness, and long-term preference modeling.

## Method Summary
The authors use decoder-only transformer models with layer-wise adaptive dropout and switching optimizer strategy (Adam to SGD) for ID-based sequential recommendation. They train models ranging from 98.3K to 0.8B parameters on MovieLens-20M and Amazon-2018 datasets, applying data repetition for data-constrained scenarios. The training procedure includes cosine learning rate scheduling with weight decay of 1e-8, monitoring validation loss to determine convergence and optimizer switchover points.

## Key Results
- Scaling laws hold for ID-based sequential recommendation models, following power-law relationships between model size and test loss
- Layer-wise adaptive dropout and switching optimizer strategy stabilize training of large models
- Scaling up to 0.8B parameters significantly improves performance on cold-start, robustness, and long-term preference tasks
- Test loss can be predicted from smaller models, enabling performance estimation for larger architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Scaling laws apply to ID-based sequential recommendation models even in data-constrained regimes
- Mechanism: Model performance scales predictably with model size following a power-law relationship, allowing performance of large models to be predicted from small models
- Core assumption: Sequential recommendation data follows similar scaling patterns to language modeling despite different data characteristics
- Evidence anchors:
  - [abstract] "we empirically show that scaling law still holds for these trained models, even in data-constrained scenarios"
  - [section IV-A] "we observe that the test loss at a single epoch follows a power-law relationship with the number of non-embedding parameters"
  - [corpus] Weak evidence - corpus neighbors discuss scaling laws in sequential recommendation but lack specific ID-based model data
- Break condition: If data sparsity becomes extreme enough that meaningful patterns cannot be learned, or if the sequential nature of interactions fundamentally differs from language sequences

### Mechanism 2
- Claim: Large models exhibit enhanced robustness and performance on challenging recommendation tasks
- Mechanism: Larger models can better capture long-range dependencies, memorize rare patterns, and generalize from few-shot examples
- Core assumption: Increased model capacity directly translates to improved ability to handle sparse, noisy data and complex recommendation scenarios
- Evidence anchors:
  - [abstract] "scaling up the model size can greatly boost the performance on these challenging tasks"
  - [section V-C] "the performance improvements of larger models are more significant when dealing with extremely short input sequences"
  - [corpus] Limited evidence - corpus discusses robustness but not specifically in ID-based sequential recommendation
- Break condition: If model size increases without corresponding data growth lead to overfitting, or if the computational cost outweighs performance benefits

### Mechanism 3
- Claim: Layer-wise adaptive dropout and switching optimizer strategy stabilize training of large sequential recommendation models
- Mechanism: Different dropout rates at different training stages prevent overfitting/underfitting, while switching from Adam to SGD improves convergence
- Core assumption: Large transformer models for recommendation face unique training challenges requiring specialized optimization techniques
- Evidence anchors:
  - [section III-B] "we adopt layer-wise adaptive dropout ratios in the overall training process" and "we switch the optimizer from Adam to SGD at the switchover point"
  - [section III-B] "the performance of large-scale models has declined to a certain extent without layer-wise adaptive dropout"
  - [corpus] No direct evidence - corpus neighbors do not discuss training techniques for large sequential recommendation models
- Break condition: If the specific dropout and optimizer switching parameters are not well-tuned for a given dataset, or if the model architecture differs significantly from the tested configuration

## Foundational Learning

- Concept: Power-law scaling relationships in machine learning
  - Why needed here: Understanding how model performance scales with size is central to the paper's contribution and experimental design
  - Quick check question: If a model's test loss follows L(N) = E + (N0/N)^α, what happens to the loss as model size N increases?

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: The paper uses decoder-only transformers as the base model, requiring understanding of how they process sequential data
  - Quick check question: In a decoder-only transformer, why is unidirectional attention (masking future tokens) important for sequential recommendation?

- Concept: Recommendation task formulations and evaluation metrics
  - Why needed here: The paper evaluates models on ID-based sequential recommendation using metrics like HR@N and NDCG@N
  - Quick check question: What is the difference between Hit Ratio and NDCG in evaluating recommendation systems, and when might one be preferred over the other?

## Architecture Onboarding

- Component map: Input embeddings + positional embeddings -> Stacked decoder-only transformer blocks -> Prediction layer -> Loss calculation -> Optimization
- Critical path: Data → Embedding Layer → Transformer Blocks → Prediction Layer → Loss Calculation → Optimization
- Design tradeoffs:
  - Model depth vs. width: The paper finds weak dependence on model shape, suggesting flexibility in architecture design
  - Embedding size vs. non-embedding parameters: Careful scaling of both is needed to avoid embedding collapse
  - Training stability vs. model size: Larger models require specialized training techniques
- Failure signatures:
  - Training instability or NaN losses indicate issues with dropout or optimizer settings
  - Poor performance on long-tail items suggests insufficient capacity to learn rare patterns
  - Overfitting on small datasets despite regularization indicates need for data augmentation or early stopping
- First 3 experiments:
  1. Train a small (2-layer) and medium (12-layer) model on the MovieLens dataset, compare test loss to verify basic scaling law
  2. Apply layer-wise adaptive dropout to a medium model, compare to fixed dropout to verify training stability improvement
  3. Test a large (48-layer) model on cold-start user recommendation task, compare to small model to verify robustness claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scaling law for ID-based sequential recommendation models differ from scaling laws observed in other domains like natural language processing or computer vision?
- Basis in paper: [explicit] The paper states that while scaling laws have been widely studied in NLP and CV, there is limited understanding of the scaling effect in recommender systems, particularly for ID-based sequential recommendation models
- Why unresolved: The paper provides empirical evidence that scaling laws hold for ID-based sequential recommendation models, but a comprehensive theoretical understanding of how these laws differ from other domains is still lacking
- What evidence would resolve it: A detailed comparative analysis of scaling laws across different domains, including theoretical justifications for the observed differences, would provide a clearer understanding of the unique characteristics of scaling in recommender systems

### Open Question 2
- Question: What is the optimal model shape (depth vs. width) for large sequential recommendation models, and how does this optimal shape change with model size?
- Basis in paper: [explicit] The paper investigates the effect of model shape on scaling and finds that model performance exhibits weak dependence on model shape, especially as model size increases. However, the specific optimal shape is not determined
- Why unresolved: While the paper suggests that model shape has a weak impact on performance, it does not provide a definitive answer on the optimal shape or how this optimal shape changes with different model sizes
- What evidence would resolve it: Systematic experiments varying both depth and width across a wide range of model sizes, coupled with theoretical analysis, would help determine the optimal model shape and how it scales with model size

### Open Question 3
- Question: How does data repetition affect the scaling properties of sequential recommendation models, and is there an optimal strategy for data repetition?
- Basis in paper: [explicit] The paper explores the use of data repetition to address data scarcity in recommender systems and finds that larger models benefit more from data repetition. However, the optimal strategy for data repetition is not fully explored
- Why unresolved: While the paper demonstrates the benefits of data repetition, it does not provide a comprehensive analysis of how different repetition strategies affect scaling properties or identify an optimal approach
- What evidence would resolve it: Experiments varying repetition strategies (e.g., different repetition rates, timing of repetition) across various model sizes and datasets would help identify the most effective approach to data repetition for scaling sequential recommendation models

## Limitations

- Focus on ID-based sequential recommendation models may not generalize to other recommendation paradigms
- Layer-wise adaptive dropout and optimizer switching lack comprehensive ablation studies and parameter specifications
- Limited exploration of breaking points where scaling becomes ineffective or detrimental

## Confidence

**High Confidence**: The observation that scaling laws hold for ID-based sequential recommendation models, supported by systematic experiments across multiple model sizes and datasets.

**Medium Confidence**: The effectiveness of layer-wise adaptive dropout and switching optimizer strategy in stabilizing large model training.

**Medium Confidence**: The performance benefits on challenging recommendation tasks.

## Next Checks

1. **Generalization Test**: Evaluate the scaling law on non-sequential recommendation tasks (e.g., matrix factorization or graph neural networks) to determine if the observed relationships are architecture-specific or more universal.

2. **Breaking Point Analysis**: Systematically identify the model size and data volume thresholds where scaling law predictions become inaccurate or where performance gains plateau, particularly for sparse and long-tail items.

3. **Training Strategy Ablation**: Conduct controlled experiments varying the layer-wise adaptive dropout rates and optimizer switching timing to isolate their individual contributions and identify optimal parameter settings for different model scales.