---
ver: rpa2
title: Explaining Predictive Uncertainty with Information Theoretic Shapley Values
arxiv_id: '2306.05724'
source_url: https://arxiv.org/abs/2306.05724
tags:
- shapley
- uncertainty
- values
- learning
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends the Shapley value framework to explain predictive
  uncertainty by quantifying each feature's contribution to the conditional entropy
  of model outputs. It introduces information-theoretic games with modified characteristic
  functions that connect Shapley values to fundamental quantities from information
  theory and conditional independence testing.
---

# Explaining Predictive Uncertainty with Information Theoretic Shapley Values

## Quick Facts
- arXiv ID: 2306.05724
- Source URL: https://arxiv.org/abs/2306.05724
- Reference count: 40
- Key outcome: This paper extends the Shapley value framework to explain predictive uncertainty by quantifying each feature's contribution to the conditional entropy of model outputs.

## Executive Summary
This paper introduces information-theoretic Shapley values to explain predictive uncertainty by quantifying each feature's contribution to the conditional entropy of model outputs. The method modifies the Shapley value characteristic function using information-theoretic quantities like negative conditional entropy and KL divergence. A split conformal inference procedure provides finite-sample coverage guarantees for the attribution intervals. The approach enables explaining higher moments of predictive distributions beyond point estimates, offering a more complete understanding of model uncertainty for applications like feature selection, covariate shift detection, and active learning.

## Method Summary
The method extends the Shapley value framework by defining information-theoretic games where the characteristic function measures feature contributions to uncertainty via quantities like negative conditional entropy and KL divergence. Shapley values are computed by averaging marginal contributions across random feature coalitions, with Monte Carlo sampling used for computational efficiency. To provide finite-sample coverage guarantees, the method employs split conformal inference, partitioning data into training and calibration sets and using order statistics of conformity scores to establish confidence intervals around the attribution values.

## Key Results
- Information-theoretic Shapley values successfully quantify feature contributions to predictive uncertainty via conditional entropy differences
- Conformal inference provides finite-sample coverage guarantees for Shapley value attribution intervals
- Feature contributions to predictive uncertainty are distinct from contributions to point predictions, enabling better uncertainty-aware feature selection

## Why This Works (Mechanism)

### Mechanism 1
Information-theoretic Shapley values quantify feature contributions to predictive uncertainty via conditional entropy differences. The method modifies the Shapley value characteristic function to use information-theoretic quantities like negative conditional entropy (-H(Y|xS)) and KL divergence. This captures how much each feature reduces uncertainty when added to a coalition. Core assumption: The modified value functions properly measure feature contributions to uncertainty, and the marginal contributions are well-defined for the underlying distributions.

### Mechanism 2
Conformal inference provides finite-sample coverage guarantees for Shapley value attribution intervals. The method splits data into training and calibration sets, computes Shapley values on the calibration set, and uses order statistics of deviations from the mean to establish confidence intervals with provable coverage. Core assumption: The conformity scores have a continuous joint distribution (or at least enough variability to make the order statistics meaningful).

### Mechanism 3
Feature contributions to predictive uncertainty are distinct from contributions to point predictions, enabling better uncertainty-aware feature selection. By attributing uncertainty separately from predictions, the method can identify features that contribute little to the mean prediction but significantly to the uncertainty, which is valuable for active learning and heteroskedasticity detection. Core assumption: The modified value functions successfully isolate uncertainty contributions from prediction contributions.

## Foundational Learning

- Concept: Information theory fundamentals (entropy, KL divergence, mutual information)
  - Why needed here: The method fundamentally relies on information-theoretic quantities to measure uncertainty contributions
  - Quick check question: Can you explain why mutual information I(Y; X) = H(Y) - H(Y|X) measures the reduction in uncertainty about Y from knowing X?

- Concept: Shapley value theory and cooperative game theory
  - Why needed here: The method extends the Shapley value framework, so understanding the axioms (efficiency, symmetry, etc.) is crucial
  - Quick check question: What does the efficiency axiom mean for information-theoretic Shapley values, and why is it satisfied?

- Concept: Conformal inference and finite-sample guarantees
  - Why needed here: The method uses split conformal inference to provide coverage guarantees for the attribution intervals
  - Quick check question: How does the split conformal method ensure that the confidence intervals have the claimed coverage probability?

## Architecture Onboarding

- Component map: Data → Model training → Uncertainty estimation → Information-theoretic game definition → Shapley value computation → Conformal calibration → Attribution intervals
- Critical path: The most critical sequence is: uncertainty estimation → information-theoretic game → Shapley computation → conformal calibration. If any of these fail, the method doesn't work.
- Design tradeoffs: Computational cost vs. accuracy (exact Shapley values are #P-hard), choice of uncertainty estimator (model-specific vs. model-agnostic), choice of reference distribution for feature sampling
- Failure signatures: If the conformal coverage guarantee is violated in practice, it suggests the conformity scores aren't sufficiently variable. If Shapley values don't concentrate around zero for uninformative features, the information-theoretic games may not be properly isolating uncertainty.
- First 3 experiments:
  1. Run the modified Friedman benchmark (mean depends on X6-X10, variance on X1-X5) to verify that Shapley values correctly identify which features contribute to uncertainty vs. predictions
  2. Apply to a UCI dataset with known covariate shift, perturb one feature, and verify the method identifies the perturbed feature as the source of increased uncertainty
  3. Implement the MNIST experiment to visualize pixel-wise contributions to epistemic vs. aleatoric uncertainty and verify the qualitative patterns make sense

## Open Questions the Paper Calls Out

### Open Question 1
How do information theoretic Shapley values compare to other feature attribution methods when explaining higher moments of predictive distributions beyond just conditional entropy? The paper only provides limited empirical comparisons to existing methods, focusing mainly on its own approach. No direct comparison to other methods for explaining higher moments is provided.

### Open Question 2
How does the choice of reference distribution impact the Shapley values computed using information theoretic games? The paper leaves the choice of reference distribution up to practitioners and does not provide guidance on how different choices might impact results. The impact is likely context-dependent.

### Open Question 3
How well do information theoretic Shapley values scale to high-dimensional feature spaces? The paper discusses the computational challenges of computing Shapley values and mentions sampling candidate coalitions to reduce computation. However, the empirical studies only involve low-dimensional datasets.

## Limitations
- The approach relies heavily on the existence of well-defined conditional distributions for information-theoretic quantities, which may not hold for degenerate or complex distributions
- The computational complexity of exact Shapley values (being #P-hard) necessitates Monte Carlo approximations, introducing additional uncertainty
- The method assumes features can be meaningfully resampled from a reference distribution, which may not hold for features with complex dependencies or measurement constraints

## Confidence

- High confidence: The conformal inference coverage guarantees (Theorem 5.1) are mathematically rigorous and well-established
- Medium confidence: The connection between information-theoretic games and fundamental quantities from information theory (Proposition 4.3) follows from standard results but requires careful verification in implementation
- Low confidence: The empirical demonstrations on real datasets provide suggestive evidence but lack statistical power to definitively validate the method's effectiveness across diverse scenarios

## Next Checks

1. Verify the modified Friedman benchmark experiment where mean depends on X6-X10 and variance on X1-X5, checking that Shapley values correctly identify which features contribute to uncertainty versus predictions
2. Test the method on a UCI dataset with known covariate shift by perturbing one feature and verifying the method identifies the perturbed feature as the source of increased uncertainty
3. Implement the MNIST experiment to visualize pixel-wise contributions to epistemic vs. aleatoric uncertainty and verify the qualitative patterns align with expectations