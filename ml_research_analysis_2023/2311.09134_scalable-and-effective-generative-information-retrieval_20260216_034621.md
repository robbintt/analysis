---
ver: rpa2
title: Scalable and Effective Generative Information Retrieval
arxiv_id: '2311.09134'
source_url: https://arxiv.org/abs/2311.09134
tags:
- retrieval
- document
- generative
- https
- semanticscholar
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that generative retrieval models can be
  trained to perform effectively on large-scale standard retrieval benchmarks for
  the first time. The authors propose RIPOR, a framework that optimizes document ID
  construction and prefix-oriented ranking for generative retrieval.
---

# Scalable and Effective Generative Information Retrieval

## Quick Facts
- arXiv ID: 2311.09134
- Source URL: https://arxiv.org/abs/2311.09134
- Reference count: 40
- Primary result: RIPOR achieves 30.5% MRR improvement on MS MARCO Dev Set compared to state-of-the-art generative retrieval models

## Executive Summary
This paper introduces RIPOR, a framework that enables generative retrieval models to achieve effective performance on large-scale standard retrieval benchmarks for the first time. The key innovation lies in addressing two critical challenges: ensuring relevant document ID prefixes survive beam search decoding through prefix-oriented ranking optimization, and constructing document IDs based on relevance associations rather than document content. Experiments on MSMARCO and TREC Deep Learning Track demonstrate that RIPOR significantly outperforms state-of-the-art generative retrieval models and achieves comparable performance to popular dense retrieval models.

## Method Summary
RIPOR consists of four main components: relevance-based DocID initialization using residual quantization, seq2seq pre-training using pseudo queries, rank-oriented fine-tuning with prefix-oriented ranking optimization, and self-negative fine-tuning for hard negative samples. The framework treats the generative model as a dense encoder, fine-tunes it with relevance-based objectives, then applies residual quantization to create document IDs that reflect relevance associations. Prefix-oriented ranking optimization ensures relevant document ID prefixes survive beam search decoding by explicitly optimizing pairwise margins at each prefix position. Multi-objective progressive training prevents catastrophic forgetting of prefix-level ranking knowledge while allowing the model to focus on longer prefixes.

## Key Results
- RIPOR achieves 30.5% MRR improvement on MS MARCO Dev Set compared to state-of-the-art generative retrieval models
- RIPOR outperforms existing generative retrieval models on both MSMARCO and TREC Deep Learning Track datasets
- RIPOR achieves comparable performance to popular dense retrieval models while maintaining the advantages of generative approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prefix-oriented ranking optimization ensures relevant document ID prefixes survive beam search decoding by explicitly optimizing pairwise margins at each prefix position
- Core assumption: Beam search decoding is sequential and eliminates candidates based on cumulative scores of already-decoded tokens
- Evidence anchors: [abstract], [section 3.1]
- Break condition: If beam search width k is sufficiently large or if the document collection is small enough that prefix collisions are rare

### Mechanism 2
- Claim: Relevance-based document ID construction captures query-dependent semantic similarities between documents rather than relying on syntactic/semantic document content alone
- Core assumption: Relevance information between queries and documents cannot be adequately captured by models trained with syntactic, semantic, and proximity-based objectives alone
- Evidence anchors: [abstract], [section 3.2]
- Break condition: If the document collection is homogeneous or if relevance judgments are sparse and noisy

### Mechanism 3
- Claim: Multi-objective progressive training prevents catastrophic forgetting of prefix-level ranking knowledge while allowing the model to focus on longer prefixes
- Core assumption: Without multi-objective learning, the model tends to forget previously acquired knowledge related to shorter prefixes as training advances to longer ones
- Evidence anchors: [abstract], [section 3.1], [section 4.2.2]
- Break condition: If the document collection is small or if the model architecture has strong regularization that prevents forgetting

## Foundational Learning

- Concept: Transformer encoder-decoder architecture and attention mechanisms
  - Why needed here: The RIPOR framework builds upon pre-trained transformer models (T5-base) as the backbone for generative retrieval
  - Quick check question: How does the decoder's cross-attention mechanism in a transformer model use the encoder's output representations when generating each token?

- Concept: Information retrieval evaluation metrics (MRR, NDCG, Recall)
  - Why needed here: The paper evaluates RIPOR's performance using standard IR metrics on MSMARCO and TREC datasets
  - Quick check question: What is the key difference between MRR@10 and NDCG@10 in terms of how they evaluate ranked retrieval results?

- Concept: Dense retrieval and bi-encoder architectures
  - Why needed here: RIPOR leverages dense retrieval concepts by treating the generative model as a dense encoder
  - Quick check question: How does a bi-encoder dense retrieval model differ from a cross-encoder re-ranker in terms of computational efficiency and effectiveness?

## Architecture Onboarding

- Component map: DocID initialization -> Seq2seq pre-training -> Rank-oriented fine-tuning -> Self-negative fine-tuning
- Critical path: The most critical sequence is: DocID initialization → Seq2seq pre-training → Rank-oriented fine-tuning (with progressive training) → Self-negative fine-tuning
- Design tradeoffs: Longer document IDs (higher L) improve retrieval quality but increase memory usage and inference time
- Failure signatures: If beam search fails to retrieve relevant documents, check whether prefix optimization is working
- First 3 experiments:
  1. Compare retrieval performance with and without prefix-oriented ranking optimization on a small subset of MSMARCO
  2. Test different document ID lengths (L=8, 16, 32) on the same subset to find the optimal tradeoff between quality and efficiency
  3. Replace residual quantization with product quantization in the DocID initialization phase

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of generative retrieval models scale when applied to document collections significantly larger than 8.8 million passages?
- Basis in paper: [explicit] The paper demonstrates effectiveness on 8.8 million passages but does not explore scaling beyond this size
- Why unresolved: The paper's experiments are limited to a specific dataset size
- What evidence would resolve it: Empirical results showing performance metrics on document collections of varying sizes

### Open Question 2
- Question: What is the impact of using different concave functions for the prefix weight αᵢ in the prefix-oriented ranking optimization?
- Basis in paper: [explicit] The paper uses αᵢ = 1/Z(1 - βⁱ) and mentions that exploring other concave functions is left to future work
- Why unresolved: The paper only explores one specific concave function for the prefix weights
- What evidence would resolve it: Comparative experiments testing multiple concave functions and their corresponding impact on retrieval performance

### Open Question 3
- Question: How does the choice of document ID vocabulary size and length affect the long-term memory and computational efficiency of generative retrieval models?
- Basis in paper: [explicit] The paper investigates different DocID combinations but focuses on short-term performance rather than long-term memory usage or computational efficiency during inference
- Why unresolved: While the paper shows performance varies with DocID length and vocabulary size, it does not analyze the trade-offs in memory consumption or inference speed
- What evidence would resolve it: Systematic evaluation measuring memory footprint, inference latency, and storage requirements for different DocID configurations across various hardware platforms

## Limitations

- The paper does not provide detailed implementation specifications for the prefix-oriented ranking optimization algorithm
- No analysis of RIPOR's performance on out-of-domain queries or its robustness to distribution shifts
- Limited discussion of computational efficiency and memory requirements for the DocID construction process at scale

## Confidence

- **High confidence** in the overall effectiveness claims based on direct experimental comparisons with state-of-the-art baselines
- **Medium confidence** in the prefix-oriented ranking optimization mechanism, as the paper provides theoretical justification but limited ablation evidence
- **Medium confidence** in the relevance-based document ID construction, with visualization evidence but no direct comparison against alternative methods
- **Medium confidence** in the multi-objective progressive training claims, supported by ablation studies but lacking analysis of different training schedules

## Next Checks

1. Implement a controlled ablation study comparing RIPOR with and without prefix-oriented ranking optimization on a small, interpretable subset of MSMARCO, measuring prefix-level ranking performance separately from overall retrieval effectiveness

2. Test RIPOR's zero-shot performance on queries from different domains (e.g., TREC-COVID) to assess generalization beyond the training distribution

3. Conduct a computational efficiency analysis measuring inference time and memory usage for different document ID lengths (L=8, 16, 32) and beam search widths (k=1, 10, 100) to understand the practical tradeoffs