---
ver: rpa2
title: Classification of US Supreme Court Cases using BERT-Based Techniques
arxiv_id: '2304.08649'
source_url: https://arxiv.org/abs/2304.08649
tags:
- legal
- documents
- classi
- cation
- bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of classifying long legal documents,
  specifically US Supreme Court decisions, using BERT-based models. The authors propose
  several novel techniques to overcome the limitation of BERT's maximum input sequence
  length of 512 tokens.
---

# Classification of US Supreme Court Cases using BERT-Based Techniques

## Quick Facts
- arXiv ID: 2304.08649
- Source URL: https://arxiv.org/abs/2304.08649
- Reference count: 32
- Primary result: 80% accuracy on 15 broad categories, 60% on 279 fine-grained categories

## Executive Summary
This paper addresses the challenge of classifying long legal documents using BERT-based models, which are limited to 512-token inputs. The authors propose several novel techniques including analyzing token chunk contributions, summarization, voting-based ensembles, and concatenated chunk embeddings. Their methods achieve significant improvements over state-of-the-art results, reaching 80% accuracy on broad classification and 60% on fine-grained classification of US Supreme Court cases.

## Method Summary
The paper proposes five BERT-based techniques to handle long legal documents: Best-512 (analyzing which 512-token chunks contribute most to classification), Summarization-512 (summarizing documents to 512 tokens), Concat-512 (concatenating multiple BERT outputs), Ensemble (voting-based approach), and Stride-64/128 (overlapping chunks). These methods are evaluated on the Supreme Court Database with 8,419 labeled documents across two classification tasks. The models are trained using BERT, RoBERTa, and Legal-BERT variants with standard hyperparameters, and compared against LongFormer and Legal-Longformer models as well as a CNN baseline.

## Key Results
- Best-512 technique identifies the first chunk as most predictive, achieving 80% accuracy on 15 categories
- Stride-64 with Legal-BERT outperforms disjoint chunking by preserving contextual continuity
- Overall improvements of 8% on broad classification and 28% on fine-grained classification over previous SOTA
- Concat-512 captures better cross-chunk context through joint learning during backpropagation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chunk-based fine-tuning of BERT on 512-token slices captures the most discriminative tokens for legal document classification.
- Mechanism: The paper analyzes which 512-token chunks (c1 through c6) contribute most to classification accuracy, finding that the first chunk (c1) often yields the best results. By training separate BERT models on each chunk and using a voting ensemble, the system exploits the local discriminative power of BERT while mitigating the long document challenge.
- Core assumption: Legal documents contain enough discriminative information in early chunks to predict the correct label, even if the full context is truncated.
- Evidence anchors:
  - [abstract]: "We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80% on the 15 broad categories and 60% on the fine-grained 279 categories which marks an improvement of 8% and 28% respectively from previously reported SOTA results."
  - [section]: "Table II shows that the first chunk (c1) contributes the most to classification performance across both tasks, suggesting early sections carry strong discriminative signals."
- Break condition: If legal documents have high variance in structure or the discriminative content is uniformly distributed across chunks, early-chunk focus may degrade accuracy.

### Mechanism 2
- Claim: Overlap-based striding preserves contextual continuity between consecutive 512-token chunks.
- Mechanism: Stride-64 and Stride-128 approaches reuse 64 or 128 tokens between consecutive chunks, ensuring that BERT models see overlapping context. This mitigates the loss of contextual information at chunk boundaries and improves classification accuracy compared to disjoint chunking.
- Core assumption: Overlapping text between chunks provides sufficient contextual cues to improve model performance without excessive redundancy.
- Evidence anchors:
  - [section]: "Stride basically represents the chunk of tokens which are shared amongst any two ci and ci+1... The result showing the best stride using BERT can be seen in Table III."
  - [section]: "Stride-64 outperforms Concat-512 because when we take disjoint chunks, the continuity of context goes missing whereas if there is an overlapping portion of text between two consecutive chunks, it gives better context understanding."
- Break condition: If stride overlap is too small, the benefit diminishes; if too large, computational cost rises without proportional gain.

### Mechanism 3
- Claim: Concatenating multiple 512-token BERT outputs into a single dense layer captures joint context across the document.
- Mechanism: Instead of voting or striding, this approach concatenates the CLS embeddings from multiple BERT models (trained on different chunks) and feeds them into a dense layer with softmax activation. This allows the model to learn joint representations of non-overlapping chunks during backpropagation.
- Core assumption: Joint learning of chunk embeddings can recover cross-chunk dependencies better than sequential or voting strategies.
- Evidence anchors:
  - [section]: "In this technique, we accept i parallel inputs of 512 sequence length. Corresponding to i parallel inputs we have i BERT-based models which are trained simultaneously and their outputs are concatenated."
  - [section]: "Concat-512 on the other hand captures better context across multiple 512 token chunks as it learns this knowledge during back propagation of the model."
- Break condition: If chunk independence is high, concatenating embeddings may add noise rather than useful signal.

## Foundational Learning

- Concept: Sequence length limitations in transformer-based models (e.g., BERT's 512-token cap).
  - Why needed here: Legal documents far exceed 512 tokens; understanding how to handle truncation is essential to applying BERT to this domain.
  - Quick check question: What is the maximum sequence length BERT can process, and how does this constraint affect long document classification?

- Concept: Tokenization and chunking strategies for long texts.
  - Why needed here: Deciding how to split or summarize documents directly impacts classification performance.
  - Quick check question: How do stride-based overlapping chunks differ from disjoint chunks in terms of preserving context?

- Concept: Ensemble learning via voting mechanisms.
  - Why needed here: The paper uses majority voting across chunk-level classifiers to improve robustness.
  - Quick check question: How does a majority voting ensemble combine predictions from multiple chunk-level models?

## Architecture Onboarding

- Component map: Document -> Preprocessor (chunk/split) -> BERT models (per chunk) -> Aggregation layer (voting/concat/striding) -> Dense classifier -> Output categories

- Critical path:
  1. Preprocess document into chunks
  2. Encode each chunk with BERT (or variant)
  3. Aggregate chunk representations (voting/concat/striding)
  4. Feed into final dense layer for classification

- Design tradeoffs:
  - Memory vs. accuracy: More chunks â†’ more memory but potentially better context capture
  - Overlap size: Larger strides improve context continuity but increase redundancy
  - Ensemble size: More chunk-level models increase robustness but slow inference

- Failure signatures:
  - Overfitting to chunk boundaries when stride is too small
  - Loss of global context when summarizing to 512 tokens
  - Poor performance on minority categories due to class imbalance

- First 3 experiments:
  1. Run Best-512 on both tasks to identify the most predictive chunk(s)
  2. Apply Stride-64 with Legal-BERT and compare to Best-512 results
  3. Test Concat-512 with multiple BERT variants to assess joint context learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do references to other legal documents in the SCDB affect the classification performance of BERT-based models?
- Basis in paper: [explicit] The paper mentions leveraging knowledge embedded in references of SCDB documents as a potential area of future work.
- Why unresolved: The paper does not explore this aspect and leaves it as a suggestion for future research.
- What evidence would resolve it: Experimental results comparing classification performance with and without incorporating references, using graph structures to quantify relations between documents.

### Open Question 2
- Question: How would greedy approaches improve the performance of the proposed techniques for handling long documents with BERT?
- Basis in paper: [explicit] The paper suggests applying greedy approaches to techniques like summarization and ensemble as a future work direction.
- Why unresolved: The paper does not implement or evaluate these greedy approaches.
- What evidence would resolve it: Comparative results showing the performance of greedy versions of summarization and ensemble techniques against their non-greedy counterparts.

### Open Question 3
- Question: What is the impact of multi-head attentions adhering to a restricted window in LongFormer and Legal-Longformer compared to the free attention in BERT-based models?
- Basis in paper: [inferred] The paper discusses why LongFormer and Legal-Longformer do not outperform BERT-based models, mentioning the restricted attention window as a possible reason.
- Why unresolved: The paper does not provide experimental evidence or further analysis on this aspect.
- What evidence would resolve it: Detailed experiments comparing the attention patterns and classification performance between BERT-based models and LongFormer/Legal-LongFormer models.

## Limitations
- Potential loss of global document context when chunking, even with overlap
- Reliance on the SCDB dataset without validation on external corpora limits generalizability
- Unclear implementation details for the Ensemble method's voting mechanism

## Confidence
- **High**: Best-512 and Stride-64 chunk strategies showing clear accuracy improvements over prior SOTA and baseline models
- **Medium**: Ensemble and Concat-512 methods, due to unclear implementation details and weaker justification of gains
- **Low**: Claims about future improvements using greedy approaches, as no experiments or concrete results are provided

## Next Checks
1. Replicate the Ensemble method with a clarified voting mechanism: Implement the majority voting ensemble using explicit definitions of "#nc" and verify the reported accuracy gains on both classification tasks.

2. Compare stride sizes systematically: Run experiments with Stride-32, Stride-64, and Stride-128 on both tasks to determine the optimal stride length and quantify the tradeoff between context continuity and computational cost.

3. Test generalization to external legal datasets: Apply the Best-512 and Stride-64 models to a separate corpus of legal documents (e.g., European Court of Justice cases) to assess robustness and identify potential domain-specific overfitting.