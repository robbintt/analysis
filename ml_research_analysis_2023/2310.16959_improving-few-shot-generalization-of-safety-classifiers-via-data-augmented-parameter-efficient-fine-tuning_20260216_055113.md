---
ver: rpa2
title: Improving Few-shot Generalization of Safety Classifiers via Data Augmented
  Parameter-Efficient Fine-Tuning
arxiv_id: '2310.16959'
source_url: https://arxiv.org/abs/2310.16959
tags:
- safety
- examples
- few-shot
- data
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of building safety classifiers
  for new rules when only a few examples are available. It proposes a method that
  combines parameter-efficient fine-tuning with similarity-based data augmentation
  to improve few-shot generalization.
---

# Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning

## Quick Facts
- arXiv ID: 2310.16959
- Source URL: https://arxiv.org/abs/2310.16959
- Reference count: 27
- Primary result: 7-17% improvement in F1 score and 9-13% improvement in AUC for few-shot safety classification

## Executive Summary
This paper addresses the challenge of building safety classifiers for new rules when only limited examples are available. The authors propose DAPT, a method that combines parameter-efficient fine-tuning with similarity-based data augmentation to improve few-shot generalization. By augmenting few-shot examples from new safety rules with semantically similar examples from existing rules, the approach achieves significant performance gains on two safety tasks while using only 50K parameters for adaptation.

## Method Summary
The DAPT method augments few-shot examples from new safety rules with similar examples from existing rules using three similarity metrics (cosine similarity, ReCross, and contextual data augmentation). These augmented datasets are then used to perform prompt tuning, a parameter-efficient fine-tuning method that updates only 50K parameters. The approach is evaluated on Social Chemistry moral judgment and Toxicity detection tasks, demonstrating substantial improvements over baselines including prompt tuning with only few-shot examples and random data augmentation.

## Key Results
- 7-17% improvement in F1 score on Social Chemistry moral judgment task
- 9-13% improvement in AUC on Toxicity detection task
- Method shows robustness to different choices of few-shot examples
- Performance improvements maintained even for loosely correlated new safety rules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Augmenting training data with examples similar to few-shot examples improves generalization to new safety rules.
- Mechanism: The method identifies examples from existing safety rules that are semantically similar to the few-shot examples from the new rule. These similar examples serve as additional training data for prompt tuning, effectively expanding the training set size while maintaining relevance to the new task.
- Core assumption: Similarity in semantic content between examples from existing and new safety rules indicates transfer potential.
- Evidence anchors:
  - [abstract]: "we propose to do parameter-efficient fine-tuning (PEFT) combined with augmenting training data based on similar examples in prior existing rules."
  - [section]: "augmenting examples from existing safety rules that are most similar to few-shot examples from the new safety rule as training data for Prompt-Tuning"
  - [corpus]: No direct evidence found for similarity-based data augmentation improving generalization in this specific safety classification context.
- Break condition: If similarity metrics fail to capture relevant semantic relationships between existing and new safety rules, the augmented data may be irrelevant and potentially harmful to model performance.

### Mechanism 2
- Claim: Parameter-efficient fine-tuning (prompt tuning) is more effective than full fine-tuning or in-context learning when training data is limited.
- Mechanism: Prompt tuning modifies only a small number of parameters (50K in this case) associated with the input embeddings, allowing the model to adapt to new tasks without catastrophic forgetting or overfitting to limited data.
- Core assumption: The pre-trained model has learned generalizable representations that can be leveraged with minimal parameter updates.
- Evidence anchors:
  - [abstract]: "We empirically show that our approach of similarity-based data-augmentation + prompt-tuning (DAPT) consistently outperforms baselines"
  - [section]: "we use prompt tuning (Lester et al., 2021), a PEFT method, in DAPT and not ICL for two reasons"
  - [corpus]: No direct evidence found for prompt tuning outperforming alternatives specifically in few-shot safety classification.
- Break condition: If the pre-trained model lacks sufficient knowledge relevant to the new safety domain, prompt tuning may not provide adequate adaptation capability.

### Mechanism 3
- Claim: The proposed method is robust to variations in the choice of few-shot examples.
- Mechanism: By augmenting with multiple similar examples from existing rules, the method reduces dependency on any single few-shot example, creating a more stable training signal.
- Core assumption: Multiple similar examples provide complementary information that stabilizes learning regardless of individual few-shot example variations.
- Evidence anchors:
  - [section]: "Our method is not brittle: it performs equally well even under worst-case choices for the five support examples"
  - [section]: "its F1 scores are equally high across all settings"
  - [corpus]: No direct evidence found for robustness to few-shot example selection in this specific context.
- Break condition: If the similarity metric consistently fails to identify truly relevant examples, or if the new safety rule is completely orthogonal to existing rules, the method may still show sensitivity to few-shot selection.

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT) methods
  - Why needed here: Full fine-tuning of large language models is computationally expensive and prone to overfitting with limited data. PEFT methods like prompt tuning offer a more efficient alternative.
  - Quick check question: What is the approximate parameter count for prompt tuning compared to full fine-tuning in this paper?

- Concept: Similarity metrics for text data
  - Why needed here: The method relies on identifying semantically similar examples from existing safety rules to augment training data. Understanding different similarity metrics (cosine similarity, sentence embeddings, contextual similarity) is crucial.
  - Quick check question: Which three similarity methods are compared in the paper for data augmentation?

- Concept: Domain generalization in few-shot learning
  - Why needed here: The paper addresses the challenge of generalizing to new safety rules that weren't seen during initial training, requiring understanding of how models can adapt to novel domains with minimal examples.
  - Quick check question: How does the paper define the problem of domain-generalized few-shot learning for safety classifiers?

## Architecture Onboarding

- Component map:
  - Base model: Pre-trained 62B PaLM model fine-tuned on existing safety rules
  - Similarity module: Computes similarity between few-shot examples and existing rule examples using cosine similarity, ReCross, or CDA
  - Data augmentation pipeline: Selects and combines similar examples from existing rules
  - Prompt tuning module: Updates 50K prompt parameters using augmented data
  - Evaluation framework: Tests generalization on held-out safety rules

- Critical path:
  1. Receive few-shot examples from new safety rule
  2. Compute similarity between these examples and all examples from existing rules
  3. Select top-k most similar examples for augmentation
  4. Combine few-shot examples with augmented examples
  5. Perform prompt tuning on the combined dataset
  6. Evaluate on test set from new safety rule

- Design tradeoffs:
  - Similarity metric selection: Cosine similarity is computationally efficient but may miss contextual nuances compared to ReCross or CDA
  - Augmentation size: More augmented examples improve performance but increase computational cost
  - Parameter efficiency vs. adaptation capability: Prompt tuning uses fewer parameters than full fine-tuning but may limit adaptation capacity

- Failure signatures:
  - Performance close to base model indicates similarity metric is not capturing relevant relationships
  - High variance across different few-shot selections indicates method is not robust to example choice
  - Degradation in performance on existing rules indicates catastrophic forgetting

- First 3 experiments:
  1. Implement prompt tuning baseline with only the 5 few-shot examples to establish baseline performance
  2. Implement random data augmentation with 100 examples from existing rules to test value of similarity-based selection
  3. Implement cosine similarity-based data augmentation to test the core proposed method against the baseline

## Open Questions the Paper Calls Out
- Question: How well does the DAPT approach generalize to other NLP tasks beyond safety classification?
- Question: How does the performance of DAPT scale with the size of the base model?
- Question: How does the choice of similarity metric affect the performance of DAPT?

## Limitations
- Effectiveness depends on semantic overlap between existing and new safety rules, which may not hold for fundamentally different safety domains
- Computational overhead from similarity computation and data augmentation is not fully characterized
- Claims about robustness to loosely correlated new rules require additional empirical validation beyond the two studied tasks

## Confidence
- High Confidence: The core mechanism of combining prompt tuning with similarity-based data augmentation is technically sound and the experimental methodology is well-specified.
- Medium Confidence: The claim that the method performs equally well across different few-shot example selections is supported by internal evidence but would benefit from more rigorous testing across diverse scenarios.
- Low Confidence: The paper's claims about robustness to loosely correlated new rules and the general applicability of similarity-based augmentation across different safety domains require additional empirical validation.

## Next Checks
1. **Cross-domain generalization test**: Evaluate the DAPT method on safety classification tasks from domains not represented in the Social Chemistry or Toxicity datasets (e.g., cybersecurity threat detection or content moderation in new languages) to assess true domain generalization capabilities.

2. **Ablation study on similarity metrics**: Systematically compare the performance of DAPT using different similarity metrics (cosine similarity, ReCross, CDA) across varying degrees of semantic overlap between existing and new safety rules to quantify the impact of similarity metric choice on performance.

3. **Robustness stress test**: Design controlled experiments where new safety rules are intentionally chosen to be increasingly dissimilar from existing rules, measuring performance degradation to establish clear boundaries of the method's effectiveness.