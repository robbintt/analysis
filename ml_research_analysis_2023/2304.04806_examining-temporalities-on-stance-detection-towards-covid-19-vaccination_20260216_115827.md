---
ver: rpa2
title: Examining Temporalities on Stance Detection towards COVID-19 Vaccination
arxiv_id: '2304.04806'
source_url: https://arxiv.org/abs/2304.04806
tags:
- splits
- covid-19
- chronological
- random
- stance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines temporal concept drift in stance detection
  towards COVID-19 vaccination on Twitter. It evaluates transformer-based models using
  chronological and random splits of monolingual and multilingual datasets.
---

# Examining Temporalities on Stance Detection towards COVID-19 Vaccination

## Quick Facts
- arXiv ID: 2304.04806
- Source URL: https://arxiv.org/abs/2304.04806
- Authors: 
- Reference count: 3
- Key outcome: This study examines temporal concept drift in stance detection towards COVID-19 vaccination on Twitter. It evaluates transformer-based models using chronological and random splits of monolingual and multilingual datasets. Results show that chronological splits significantly reduce accuracy compared to random splits across all datasets, indicating temporal factors critically impact model performance. Domain-adapted PLMs partially mitigate this effect but are insufficient. The gap between training and test sets correlates with performance differences, highlighting the need to account for temporal dynamics in real-world applications.

## Executive Summary
This study investigates temporal concept drift in stance detection towards COVID-19 vaccination on Twitter by comparing model performance using chronological versus random data splits. The researchers evaluated transformer-based models across five datasets (three English, two multilingual) spanning November 2020 to June 2021. Their findings reveal significant performance degradation when using chronological splits, with accuracy dropping substantially compared to random splits. The study demonstrates that domain-adapted PLMs partially mitigate but cannot fully address temporal drift effects, and establishes a positive correlation between training-test set similarity and model performance.

## Method Summary
The study fine-tunes transformer-based PLMs (BERT, COVID-BERT, Vaccine-BERT, XLM-BERT, XLM-RoBERTa) on five Twitter datasets containing COVID-19 vaccination stance annotations. Data is split using both chronological (70% earliest for training, 10% validation, 20% latest for test) and random stratified 5-fold cross-validation strategies. Models are trained with learning rate 2e-5, batch size 16, max tokens 256, and evaluated on NVIDIA Titan RTX GPU. Performance is measured using accuracy, precision, recall, and macro-F1 scores. The study also calculates vocabulary and topical overlap between training and test sets using IoU and DICE metrics, then correlates these similarity scores with model performance.

## Key Results
- Chronological splits significantly reduce accuracy compared to random splits across all datasets and PLMs
- Domain-adapted PLMs (COVID-BERT, Vaccine-BERT) partially mitigate temporal drift effects but cannot eliminate them
- Higher vocabulary and topical overlap between training and test sets correlates with better model performance
- The performance gap between random and chronological splits varies across datasets but is consistent across all model architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chronological splits significantly reduce accuracy compared to random splits in stance detection tasks
- Mechanism: Temporal concept drift causes distribution shift between training and test sets when using chronological splits, leading to degraded model performance
- Core assumption: The distribution of stance-related language evolves over time due to changing events, topics, and user behaviors
- Evidence anchors:
  - [abstract] "Our findings reveal significant discrepancies in model performance between random and chronological splits in several existing COVID-19-related datasets; specifically, chronological splits significantly reduce the accuracy of stance classification."
  - [section] "We notice that using random splits leads to a significant overestimation of performance compared to using chronological splits across the majority PLMs on both monolingual and multilingual datasets."
  - [corpus] Weak evidence - the corpus neighbors discuss COVID-19 vaccine sentiment but don't specifically address temporal splits or concept drift

### Mechanism 2
- Claim: Domain-adapted PLMs partially mitigate temporal concept drift effects
- Mechanism: Domain adaptation improves model robustness to temporal shifts by exposing the model to relevant vocabulary and context patterns specific to the COVID-19 vaccination domain
- Core assumption: Domain adaptation provides better generalization across time periods than vanilla PLMs
- Evidence anchors:
  - [section] "For both random and chronological splits, we observe that both domain-adapted PLMs (i.e., COVID-BERT and VAXX-BERT) significantly outperform (t-test, p < 0.05) the vanilla BERT in all monolingual datasets."
  - [section] "Our results show that applying domain & task adaptation techniques can address the issue of temporal concept drift to a certain extent in stance detection towards COVID-19 related datasets."
  - [corpus] No direct evidence - corpus neighbors focus on vaccine sentiment analysis but not specifically on domain adaptation for temporal drift

### Mechanism 3
- Claim: Similarity metrics between training and test sets correlate with model performance
- Mechanism: Higher vocabulary and topical overlap between training and test sets (measured by IoU and DICE) leads to better model performance
- Core assumption: Model performance degrades as the training-test set similarity decreases due to temporal drift
- Evidence anchors:
  - [section] "We discover a positive Pearson correlation between the model accuracy and the similarity distance of two subsets using both IoU (0.35) and DICE (0.64) metrics, i.e. the higher the values, the higher the model accuracy."
  - [section] "Using random splits results in significantly higher IoU and DICE scores compared to chronological splits."
  - [corpus] No direct evidence - corpus neighbors don't discuss similarity metrics for temporal analysis

## Foundational Learning

- Concept: Temporal concept drift
  - Why needed here: Understanding that language patterns, topics, and stances evolve over time is crucial for interpreting why chronological splits perform worse
  - Quick check question: What happens to model performance when training data is from 2020 but test data is from 2022 in the context of COVID-19 vaccination stance?

- Concept: Domain adaptation in NLP
  - Why needed here: Domain-adapted models (COVID-BERT, VAXX-BERT) show better performance, indicating the importance of specialized training for specific domains
  - Quick check question: How does COVID-BERT differ from vanilla BERT in terms of training data and expected performance on COVID-19 related tasks?

- Concept: Evaluation metrics for NLP classification
  - Why needed here: Understanding accuracy, precision, recall, and F1 scores is essential for interpreting the model performance results presented
  - Quick check question: If a model has high precision but low recall, what does this tell you about its behavior on stance detection?

## Architecture Onboarding

- Component map: Data preprocessing -> PLM fine-tuning -> Data splitting -> Model evaluation -> Similarity analysis -> Correlation testing
- Critical path: Data preprocessing → PLM fine-tuning → Data splitting → Model evaluation → Similarity analysis → Correlation testing
- Design tradeoffs: Chronological splits provide more realistic evaluation but reduce performance; random splits inflate performance but may not reflect real-world deployment
- Failure signatures: Large performance gaps between chronological and random splits indicate temporal drift; domain adaptation benefits suggest vocabulary shift over time
- First 3 experiments:
  1. Compare vanilla BERT performance on chronological vs random splits for a monolingual dataset
  2. Evaluate COVID-BERT vs vanilla BERT on the same chronological split to measure domain adaptation benefits
  3. Calculate IoU/DICE similarity between training and test sets for both split strategies and correlate with model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does temporal concept drift affect stance detection in other domains beyond COVID-19 vaccination?
- Basis in paper: [explicit] The paper states that temporal concept drift has been observed in other domains like rumor detection, legal, and biomedical text classification, but does not explore other domains for stance detection specifically.
- Why unresolved: The study only focuses on COVID-19 vaccination stance detection on Twitter, leaving open whether similar temporal effects occur in stance detection for other topics.
- What evidence would resolve it: Conducting similar experiments on stance detection datasets from other domains (e.g., political issues, climate change, gun control) using chronological vs. random splits.

### Open Question 2
- Question: How can we effectively develop real-time stance detection models that account for temporal concept drift?
- Basis in paper: [explicit] The paper concludes that real-world stance detection approaches need to incorporate temporal factors and suggests combining models trained on different time periods.
- Why unresolved: The paper does not provide specific methods or architectures for developing real-time models that can adapt to temporal changes.
- What evidence would resolve it: Developing and evaluating new model architectures or training strategies specifically designed to handle temporal drift in real-time stance detection applications.

### Open Question 3
- Question: What are the underlying linguistic or semantic changes that cause temporal concept drift in stance detection?
- Basis in paper: [inferred] The paper observes temporal concept drift but does not analyze what specific changes in language or topic distribution cause the performance degradation.
- Why unresolved: The study focuses on model performance metrics but does not conduct detailed linguistic analysis of what changes over time in the data.
- What evidence would resolve it: Conducting linguistic analysis to identify specific vocabulary shifts, topic emergence, or semantic changes that correlate with temporal performance degradation.

## Limitations

- The study's findings are constrained by the temporal scope of datasets (Nov 2020-June 2021), which may not capture longer-term temporal drift patterns
- The focus on Twitter data limits generalizability to other platforms or domains
- The analysis relies on accuracy and classification metrics without examining potential calibration issues or uncertainty quantification in model predictions over time

## Confidence

**High Confidence**: The finding that chronological splits significantly reduce accuracy compared to random splits is well-supported by consistent results across all five datasets and multiple PLMs.

**Medium Confidence**: The claim that domain-adapted PLMs partially mitigate temporal concept drift effects is supported by t-test results showing significant improvements over vanilla BERT.

**Low Confidence**: The exact quantitative relationship between similarity metrics (IoU/DICE) and model performance (0.35 and 0.64 Pearson correlations) may be dataset-specific and not generalizable without further validation.

## Next Checks

1. **Temporal Granularity Analysis**: Conduct the same chronological vs. random split experiments using monthly or quarterly temporal boundaries instead of the current 70-10-20 split to reveal whether performance degradation is linear over time or exhibits threshold effects.

2. **Cross-Domain Temporal Drift Validation**: Apply the experimental methodology to stance detection datasets from other domains (e.g., climate change, political elections) with longer temporal spans to test whether observed temporal drift patterns represent a broader phenomenon.

3. **Dynamic Model Adaptation Experiment**: Implement a sliding window training approach where models are periodically retrained on recent data (e.g., monthly updates) and evaluate whether this continuous adaptation strategy outperforms both static models on chronological splits and models trained on random splits.