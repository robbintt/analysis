---
ver: rpa2
title: 'Beyond Expected Return: Accounting for Policy Reproducibility when Evaluating
  Reinforcement Learning Algorithms'
arxiv_id: '2312.07178'
source_url: https://arxiv.org/abs/2312.07178
tags:
- policy
- return
- reproducibility
- learning
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of evaluating policy reproducibility
  in uncertain environments for reinforcement learning (RL). The authors formalize
  reproducibility as the statistical dispersion of a policy's return distribution
  and propose using the Lower Confidence Bound (LCB) as an alternative to the expected
  return.
---

# Beyond Expected Return: Accounting for Policy Reproducibility when Evaluating Reinforcement Learning Algorithms

## Quick Facts
- arXiv ID: 2312.07178
- Source URL: https://arxiv.org/abs/2312.07178
- Authors: 
- Reference count: 6
- Primary result: Evolution Strategies produce more reproducible policies than SAC and TD3 when evaluated using Lower Confidence Bound (LCB) metric

## Executive Summary
This paper addresses the critical issue of policy reproducibility in reinforcement learning by proposing a novel evaluation metric called the Lower Confidence Bound (LCB). Traditional evaluation using expected return fails to capture the variability in policy performance across different runs. The LCB metric allows practitioners to balance performance and reproducibility through a preference parameter α, providing a more comprehensive assessment of policy quality. Through extensive experiments on continuous control tasks, the authors demonstrate that ES-based approaches consistently produce more reproducible policies compared to model-free methods like SAC and TD3.

## Method Summary
The authors formalize reproducibility as the statistical dispersion of a policy's return distribution and propose using the Lower Confidence Bound (LCB) as an alternative to expected return. LCB is computed as the difference between the mean return and a scaled dispersion term (α × MAD). The method involves running N rollouts of each policy, computing return distributions, and calculating performance and reproducibility metrics. The approach is evaluated on continuous control tasks (Ant and HalfCheetah) in the Brax simulator with various noise types, comparing SAC, TD3, and ES algorithms across 10 seeds per configuration.

## Key Results
- ES-based approaches result in more reproducible policies compared to SAC and TD3 across all uncertainty types
- LCB scores effectively capture the performance-reproducibility trade-off, enabling better comparison of RL policies
- Behavioral reproducibility analysis using different behavioral representations (state-marginal distributions and behavior descriptors) provides additional insights into policy variability
- The LCB metric with appropriate α values successfully balances expected performance and reproducibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Lower Confidence Bound (LCB) metric enables practitioners to explicitly trade off expected return for policy reproducibility.
- Mechanism: LCB subtracts a scaled dispersion term (α x MAD) from the expected return. By adjusting α, users control the relative importance of performance vs. reproducibility.
- Core assumption: The dispersion of returns under the same policy is well-represented by the Mean Absolute Deviation (MAD) and that scaling by α is meaningful.
- Evidence anchors:
  - [abstract] "This metric provides practitioners with a preference parameter, which allow them to set the desired trade-off between expected performance and reproducibility."
  - [section] "The LCB score is only computed after the runs (during policy evaluation), all the statistics needed for choosing the value of α are available."
  - [corpus] Weak correlation with cited distributional RL works; corpus neighbors focus on value distribution optimization, not reproducibility evaluation.
- Break condition: If MAD is not a reliable estimator for dispersion (e.g., heavy-tailed distributions) or if α is chosen without empirical grounding, the trade-off will not be meaningful.

### Mechanism 2
- Claim: Evolution Strategies (ES) implicitly optimize for reproducibility due to parameter-space noise during training.
- Mechanism: ES perturbs policy parameters to estimate gradients, and the average performance over these perturbations naturally leads to more stable, less dispersed return distributions.
- Core assumption: The perturbations in parameter space act as a regularizer that smooths the objective landscape and encourages robustness to noise.
- Evidence anchors:
  - [section] "policies learnt using ES are consistently more reproducible showing lower MAD."
  - [section] "we hypothesise that this observation is closely related to optimization properties in ES that come with perturbations in the parameter space."
  - [corpus] Corpus neighbor "Evolution strategies as a scalable alternative to reinforcement learning" supports ES robustness to noise.
- Break condition: If the noise level in ES is too low to affect optimization meaningfully, or too high to destabilize learning, reproducibility gains may vanish.

### Mechanism 3
- Claim: Behavioral reproducibility can be quantified by measuring dispersion in behavior descriptors or state-marginal distributions.
- Mechanism: By representing behavior as a vector (e.g., average foot contact) or state distribution, one can compute pairwise distances and then measure statistical dispersion (MAD or IQR) of these distances.
- Core assumption: Behavior representations capture meaningful variation in policy actions and states, and distance metrics in these spaces reflect behavioral similarity.
- Evidence anchors:
  - [section] "Behaviour descriptors have already been used to quantify behaviour reproducibility for Quality-Diversity (Flageat and Cully 2023)."
  - [section] "the behavioural MAD as: MADB = median(|D - median(D)|)"
  - [corpus] No direct corpus neighbor evidence; behavioral diversity evaluation is niche in RL literature.
- Break condition: If behavior descriptors are poorly chosen or too high-dimensional, distance metrics may be noisy or uninformative, undermining reproducibility quantification.

## Foundational Learning

- Concept: Policy reproducibility as statistical dispersion of return distributions.
  - Why needed here: Distinguishes between policies that achieve the same expected return but differ in reliability.
  - Quick check question: If a policy has low MAD but high standard deviation, does it still qualify as reproducible? (Answer: No; MAD is chosen for robustness to outliers.)

- Concept: Lower Confidence Bound as a tunable evaluation metric.
  - Why needed here: Provides a single scalar score that incorporates both performance and dispersion, enabling fair comparison.
  - Quick check question: What happens to LCB when α = 0? (Answer: It reduces to expected return.)

- Concept: Heteroscedastic vs. homoscedastic uncertain environments.
  - Why needed here: Determines whether dispersion depends on the policy or is uniform across all policies.
  - Quick check question: In a heteroscedastic setting, can two policies with the same expected return have different MADs? (Answer: Yes.)

## Architecture Onboarding

- Component map: Policy evaluation pipeline -> rollout N times -> compute returns -> estimate performance (mean/median) and reproducibility (MAD/IQR) -> compute LCB -> Behavioral reproducibility module -> extract behavior descriptors/state vectors -> compute pairwise distances -> estimate dispersion (MAD/IQR)

- Critical path:
  1. Run N rollouts of a policy.
  2. Collect return values and behavior descriptors/state vectors.
  3. Compute performance and dispersion metrics.
  4. Apply LCB formula to get final score.
  5. Aggregate results across seeds.

- Design tradeoffs:
  - MAD vs. standard deviation: MAD is more robust to outliers but less sensitive to tail behavior.
  - Behavior descriptor choice: Lower-dimensional descriptors are easier to compare but may miss nuance; higher-dimensional descriptors are richer but noisier.
  - N rollouts: Higher N gives better estimates but increases computation time.

- Failure signatures:
  - High variance in LCB scores across seeds: suggests instability in policy performance or insufficient N.
  - LCB scores indistinguishable across algorithms: may indicate α is not tuned to capture meaningful differences or dispersion metrics are not discriminative.
  - Behavioral MAD values near zero: could mean behavior descriptors are not capturing variability.

- First 3 experiments:
  1. Run N=256 rollouts of a baseline policy on Ant with Init-State noise; compute LCB with α=0 and α=2e3; verify trends match Section 4.2.
  2. Repeat with behavioral descriptor representation; compare MAD/IQR scores to return-based scores.
  3. Vary N (e.g., 64, 128, 256) and observe stability of MAD and LCB estimates; identify minimal N for reliable evaluation.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but raises several important considerations:

1. How the use of LCB as a primary evaluation metric might influence the development of new RL algorithms or optimization techniques compared to expected return optimization.

2. The complex interplay of multiple uncertainty types in more realistic, real-world scenarios and how different types of uncertainty affect the trade-off between policy performance and reproducibility.

3. How behavioral reproducibility might be defined and measured in multi-agent scenarios where agents' behaviors are interdependent, extending beyond the single-agent environments studied.

## Limitations
- The reliance on MAD as the primary dispersion metric may not capture tail risks in heavy-tailed return distributions
- The experimental scope is limited to continuous control tasks, leaving open questions about applicability to discrete or high-dimensional problems
- Behavioral reproducibility analysis uses distance-based measures that could be sensitive to descriptor choice and normalization

## Confidence
- **High**: ES-based approaches yield more reproducible policies than SAC/TD3 (mechanistically clear from parameter-space perturbations)
- **Medium**: LCB metric effectively captures performance-reproducibility trade-offs (empirical validation shown but limited to specific tasks)
- **Low**: Behavioral reproducibility analysis is meaningful across different representation choices (sparse corpus evidence and potential sensitivity to descriptor selection)

## Next Checks
1. Test LCB metric on environments with heavy-tailed return distributions to assess MAD robustness
2. Compare behavioral reproducibility scores across different descriptor choices and distance metrics
3. Evaluate policies on downstream transfer tasks to determine if higher LCB scores correlate with better real-world performance