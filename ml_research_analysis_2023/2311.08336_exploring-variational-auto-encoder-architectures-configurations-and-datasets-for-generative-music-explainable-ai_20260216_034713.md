---
ver: rpa2
title: Exploring Variational Auto-Encoder Architectures, Configurations, and Datasets
  for Generative Music Explainable AI
arxiv_id: '2311.08336'
source_url: https://arxiv.org/abs/2311.08336
tags:
- music
- latent
- dimensions
- musical
- measurevae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically compares two VAE models for music generation
  - MeasureVAE and AdversarialVAE - across different latent space sizes (4-256 dimensions),
  training datasets (Irish folk, Turkish folk, Classical, and pop), and numbers of
  imposed musical features (2 or 4). Results show that MeasureVAE has higher reconstruction
  accuracy and reconstruction efficiency than AdversarialVAE, while AdversarialVAE
  has better attribute independence.
---

# Exploring Variational Auto-Encoder Architectures, Configurations, and Datasets for Generative Music Explainable AI

## Quick Facts
- arXiv ID: 2311.08336
- Source URL: https://arxiv.org/abs/2311.08336
- Authors: 
- Reference count: 40
- Key outcome: MeasureVAE achieves higher reconstruction accuracy while AdversarialVAE provides better attribute independence; optimal performance found with 32-64 latent dimensions and 2-4 regularized musical attributes across genres

## Executive Summary
This study systematically compares two VAE architectures for explainable music generation - MeasureVAE and AdversarialVAE - across varying latent space sizes (4-256 dimensions), training datasets (Irish folk, Turkish folk, Classical, and pop), and numbers of imposed musical features (2 or 4). The research identifies optimal configurations for generating music with interpretable attributes while maintaining high reconstruction quality. Results demonstrate that MeasureVAE excels in reconstruction accuracy and efficiency, while AdversarialVAE provides superior attribute independence, with both models showing genre-dependent performance characteristics.

## Method Summary
The study compares two VAE architectures - MeasureVAE (using latent space regularization) and AdversarialVAE (using adversarial classifier-discriminator approach) - for generating music with explainable features. Both models were trained on monophonic melodies converted to 24-character measures in ABC format across four datasets: Irish Folk, Turkish Makam, Muse Bach, and Lakh Clean. The evaluation measured reconstruction accuracy, reconstruction efficiency, attribute independence, loss scores, and attribute interpretability across 7 latent space sizes (4, 8, 16, 32, 64, 128, 256) with 2 or 4 regularized musical attributes (Note Density, Note Range, Rhythmic Complexity, Average Interval Jump).

## Key Results
- MeasureVAE achieved 99.6% validation reconstruction accuracy with 256 dimensions, outperforming AdversarialVAE
- AdversarialVAE demonstrated superior attribute independence across all tested attributes and configurations
- MeasureVAE achieved interpretability scores above 0.8 for all tested attributes
- Pop and rock genres performed best with MeasureVAE due to lower musical complexity requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MeasureVAE outperforms AdversarialVAE in reconstruction accuracy and efficiency due to its direct latent space regularization approach.
- Mechanism: By forcing specific dimensions of the latent space to represent musical attributes through latent space regularization (LSR), MeasureVAE creates more meaningful and predictable mappings between latent space changes and musical output changes.
- Core assumption: The semantic relationship between musical attributes and latent space dimensions can be effectively learned and controlled through LSR.
- Evidence anchors:
  - [abstract] "MeasureVAE has better reconstruction performance than AdversarialVAE which has better musical attribute independence"
  - [section] "Results show that MeasureVAE reconstruction's accuracy outperformed the AdversarialVAE in both 128 and 256 dimension configurations, achieving a high of 99.6% for the validation set with 256 dimensions"
  - [corpus] Weak corpus evidence - no directly comparable studies found
- Break condition: When musical attributes become too complex or interdependent to be effectively separated into distinct latent dimensions

### Mechanism 2
- Claim: AdversarialVAE achieves better attribute independence through its adversarial classifier-discriminator approach.
- Mechanism: The adversarial training process forces the encoder to remove attribute information from the latent vector while the decoder learns to generate music from both latent space and explicit control attributes, creating more independent attribute control.
- Core assumption: The adversarial training can effectively disentangle musical attributes from the latent representation while maintaining generative quality.
- Evidence anchors:
  - [abstract] "AdversarialVAE has better attribute independence"
  - [section] "Results indicate that AdversarialVAE performs better than MeasureVAE for Attribute Independence for all attributes and latent dimensions except for Note Range with 128 latent dimensions"
  - [corpus] No direct corpus evidence found for this specific mechanism
- Break condition: When the adversarial training becomes unstable or fails to effectively disentangle attributes

### Mechanism 3
- Claim: Lower complexity music genres perform better with MeasureVAE due to reduced latent space complexity requirements.
- Mechanism: Simpler musical structures (like pop and rock) require fewer latent dimensions to capture their essential characteristics, allowing MeasureVAE to achieve higher reconstruction accuracy with smaller latent spaces.
- Core assumption: Musical complexity directly correlates with the number of latent dimensions needed for effective representation.
- Evidence anchors:
  - [abstract] "MeasureVAE performs best with low complexity music such a pop and rock"
  - [section] "Results indicate that MeasureVAE performed best with the Lakh Clean dataset with the lowest Loss scores and the highest Reconstruction Accuracy scores for both 2 and 4 regularised dimensions"
  - [corpus] No direct corpus evidence found for this genre-complexity relationship
- Break condition: When attempting to generate highly complex musical genres that require larger latent spaces

## Foundational Learning

- Concept: Variational Autoencoder (VAE) architecture
  - Why needed here: Understanding the core VAE architecture is essential for grasping how MeasureVAE and AdversarialVAE differ in their approaches to explainable music generation
  - Quick check question: What are the three main components of a VAE and how do they interact during training and generation?

- Concept: Latent space regularization
  - Why needed here: This technique is central to how MeasureVAE creates interpretable musical dimensions and directly impacts the model's explainability
  - Quick check question: How does latent space regularization differ from traditional VAE training in terms of loss function and learning objectives?

- Concept: Adversarial training in VAEs
  - Why needed here: Understanding this concept is crucial for grasping how AdversarialVAE achieves attribute independence through its unique training approach
  - Quick check question: What role does the adversarial classifier-discriminator play in the AdversarialVAE architecture and how does it affect the learned latent representation?

## Architecture Onboarding

- Component map:
  - MeasureVAE: Encoder (bi-directional RNN) → Latent Space (with LSR) → Decoder (uni-directional RNNs + linear stacks)
  - AdversarialVAE: Encoder (bi-directional GRU) → Latent Space (with adversarial classifier) → Decoder (GRU + linear layers + control attributes)
  - Shared components: Input/output representation (24-character ABC format), Adam optimizer, similar training epochs

- Critical path:
  1. Data preparation (ABC format conversion, monophonic extraction)
  2. Model initialization with specified latent dimensions and regularization/attributes
  3. Training with appropriate loss functions (including LSR loss for MeasureVAE or adversarial loss for AdversarialVAE)
  4. Evaluation using reconstruction accuracy, efficiency, and attribute independence metrics

- Design tradeoffs:
  - MeasureVAE: Higher reconstruction accuracy vs. lower attribute independence
  - AdversarialVAE: Better attribute independence vs. lower reconstruction accuracy
  - Latent space size: Larger spaces improve reconstruction but increase computational cost and may reduce interpretability

- Failure signatures:
  - Poor reconstruction accuracy: Likely issues with encoder/decoder architecture or training hyperparameters
  - Low attribute independence: Potential problems with adversarial training stability or attribute definition
  - Inconsistent interpretability across latent dimensions: May indicate issues with LSR implementation or musical attribute selection

- First 3 experiments:
  1. Train MeasureVAE with 32 latent dimensions and 2 regularized attributes (ND&RC) on the Irish Folk dataset, evaluate reconstruction accuracy and interpretability
  2. Train AdversarialVAE with 128 latent dimensions and 4 control attributes on the same dataset, compare attribute independence to MeasureVAE
  3. Vary latent space size (4, 32, 256) for both models on the Lakh Clean dataset to observe impact on reconstruction accuracy and loss

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of regularised dimensions for generating music with specific genres like Irish Folk?
- Basis in paper: [explicit] The paper states that "16 or even 8 latent dimensions are likely to be optimal for Irish Folk music generation with 2 regularised dimensions given that its best Interpretability performance is with an 8 dimensional latent space."
- Why unresolved: While the paper suggests optimal dimensions for Irish Folk music, it does not provide a clear methodology for determining the optimal number of regularised dimensions for other genres.
- What evidence would resolve it: Systematic testing of different numbers of regularised dimensions across various genres to identify the optimal configuration for each genre.

### Open Question 2
- Question: How does the size of the training dataset affect the performance of the VAE models with explainable features?
- Basis in paper: [inferred] The paper compares datasets of different sizes (e.g., Turkish Makam dataset is the smallest) but does not systematically explore the effect of dataset size on performance.
- Why unresolved: The paper does not isolate the effect of dataset size from other factors like musical complexity or genre.
- What evidence would resolve it: Controlled experiments varying only the dataset size while keeping other factors constant to measure the impact on performance metrics.

### Open Question 3
- Question: What is the relationship between the musical features of the training dataset and the interpretability scores of the generated music?
- Basis in paper: [explicit] The paper notes that "there was not a correlation between the musical attributes of the datasets and the related Interpretability scores of the generated music."
- Why unresolved: The paper does not provide a clear explanation for why certain datasets perform better in terms of interpretability for specific musical attributes.
- What evidence would resolve it: In-depth analysis of the musical features of the datasets and their relationship to the interpretability scores, potentially involving advanced statistical methods or machine learning techniques to uncover hidden patterns.

## Limitations

- Limited direct corpus evidence for proposed mechanisms creates uncertainty about generalizability
- Turkish Makam dataset results may be compromised by smaller size and higher musical complexity
- Specific hyperparameter values (δ, γ) for latent space regularization are not fully specified

## Confidence

- High: Reconstruction accuracy comparisons between models
- Medium: Attribute independence measurements
- Medium: Interpretability score methodology

## Next Checks

1. Validate the Interpretability metric by testing with human evaluators on the same latent dimension-attribute mappings used in the study
2. Replicate the Turkish Makam experiments with augmented dataset sizes to isolate the impact of dataset size vs. musical complexity
3. Test the recommended 32-64 latent dimension configuration on additional music genres not covered in the original study to assess generalizability