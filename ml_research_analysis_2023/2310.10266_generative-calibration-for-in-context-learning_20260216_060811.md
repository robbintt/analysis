---
ver: rpa2
title: Generative Calibration for In-context Learning
arxiv_id: '2310.10266'
source_url: https://arxiv.org/abs/2310.10266
tags:
- uni00000033
- uni00000031
- uni00000003
- uni00000052
- uni00000044
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that in-context learning (ICL) suffers from
  label shift due to biased label marginals in the in-context model. By leveraging
  Monte Carlo sampling to estimate the label marginal of the in-context model, the
  authors propose generative calibration (GC) to correct this shift.
---

# Generative Calibration for In-context Learning

## Quick Facts
- arXiv ID: 2310.10266
- Source URL: https://arxiv.org/abs/2310.10266
- Reference count: 40
- Primary result: Generative calibration improves ICL performance by up to 27% absolute in macro-F1

## Executive Summary
This paper identifies that in-context learning (ICL) suffers from label shift due to biased label marginals in the in-context model. By leveraging Monte Carlo sampling to estimate the label marginal of the in-context model, the authors propose generative calibration (GC) to correct this shift. Extensive experiments on 12 text classification tasks and 12 LLMs show that GC consistently improves ICL performance by up to 27% absolute in macro-F1 and outperforms state-of-the-art calibration methods by up to 9% absolute. GC also reduces sensitivity to prompt configurations and is competitive with prompt optimization methods.

## Method Summary
The method estimates the label marginal p(y) of the in-context model using Monte Carlo sampling (generating L sequences), then calibrates the classifier by adjusting for the difference between p(y) and the true data label marginal q(y). The calibration formula normalizes the classifier output by the estimated label marginal, effectively correcting for majority and recency bias in the prompt. The approach assumes that the label conditional p(x|y) approximates the true data conditional q(x|y), meaning the distribution shift is primarily on the label marginal.

## Key Results
- GC improves ICL performance by up to 27% absolute in macro-F1 across 12 text classification tasks
- Outperforms state-of-the-art calibration methods by up to 9% absolute
- Reduces sensitivity to prompt configurations (AUROC stable while macro-F1 varies)
- Competitive with prompt optimization methods while being simpler to implement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative calibration (GC) works by estimating and correcting the label shift in in-context learning (ICL) models.
- Mechanism: The in-context model's label marginal p(y) is estimated via Monte Carlo sampling over the generative model, and this estimate is used to calibrate the classifier by adjusting for the difference between p(y) and the true data label marginal q(y).
- Core assumption: The in-context model's label conditional p(x|y) is a good approximation of the true data label conditional q(x|y), meaning that the distribution shift is primarily on the label marginal.
- Evidence anchors:
  - [abstract] "With this understanding, we can simply calibrate the in-context predictive distribution by adjusting the label marginal, which is estimated via Monte-Carlo sampling over the in-context model, i.e., generation of LLMs."
  - [section 3.2.1] "We can obtain two findings: 1) On average, ICL achieves very high AUROC values that don’t match the F1 across prompt configurations: the average AUROC performance typically exceeds 0.95, while the average F1 performance hardly reaches 0.8. 2) In contrast to the sensitivity of F1, AUROC is stable to the prompt changes with low variances. These findings provide strong evidence that p(x|y) is a stable good approximation of q(x|y) no matter of prompt configurations, roughly establishing p(x|y) ≈ q(x|y)."

### Mechanism 2
- Claim: Monte Carlo sampling provides an unbiased estimate of the in-context label marginal p(y).
- Mechanism: By generating sequences from the in-context model and computing their label predictions, we obtain an empirical estimate of p(y) that converges to the true in-context label marginal as the number of samples increases.
- Core assumption: The generative model used for sampling accurately reflects the in-context model's distribution over inputs.
- Evidence anchors:
  - [abstract] "which is estimated via Monte-Carlo sampling over the in-context model, i.e., generation of LLMs."
  - [section 3.2.2] "The whole process is shown in Figure 2. In this paper, we set L = 100, which is enough for a stable estimation (Details are shown in Appendix J)."

### Mechanism 3
- Claim: Adjusting the label marginal to be uniform improves ICL performance by reducing majority and recency bias.
- Mechanism: By normalizing the classifier's output by the estimated label marginal, the model's predictions become less sensitive to the frequency and order of labels in the prompt, effectively creating a more balanced decision boundary.
- Core assumption: The true data label marginal q(y) is uniform or close to uniform, or at least that making p(y) uniform is beneficial.
- Evidence anchors:
  - [abstract] "We follow this convention and leave accurate estimation of data label marginal q(y) in future works."
  - [section 4] "Previous works (Zhao et al., 2021; Min et al., 2021; Han et al., 2022; Fei et al., 2023) typically assume q(y) to be uniform, yielding the following classifier: ˜q(y|x) ∝ p(y|x)/˜p(y)."

## Foundational Learning

- Concept: Label shift in statistical learning
  - Why needed here: Understanding that the in-context model's distribution differs from the true data distribution primarily in the label marginal is crucial for developing the calibration method.
  - Quick check question: What is the difference between label shift and covariate shift, and why is label shift the relevant concept for this paper?

- Concept: Monte Carlo estimation
  - Why needed here: The method relies on generating samples from the in-context model to estimate the label marginal, which requires understanding how Monte Carlo methods work and their convergence properties.
  - Quick check question: Why does increasing the number of generated samples (L) improve the stability of the label marginal estimate?

- Concept: Bayesian interpretation of in-context learning
  - Why needed here: The theoretical analysis showing that the in-context model is an approximation of the posterior predictive distribution relies on understanding Bayesian inference concepts.
  - Quick check question: How does the limited number of training examples in ICL lead to the prior preference dominating the posterior?

## Architecture Onboarding

- Component map:
  - Text classification task -> Prompt generation -> In-context model (p(x,y|D)) -> Generative calibration (Monte Carlo sampling) -> Calibrated classifier (˜q(y|x))

- Critical path:
  1. Format training examples using template
  2. Generate L sequences from the in-context model
  3. Compute label predictions for generated sequences
  4. Estimate label marginal p(y) by averaging predictions
  5. Calibrate classifier by dividing by estimated p(y)

- Design tradeoffs:
  - Number of generations L: More generations improve estimate stability but increase computational cost
  - Choice of template: Affects the in-context model's behavior and thus the quality of the estimate
  - Assumption of uniform q(y): Simplifies implementation but may not hold for all datasets

- Failure signatures:
  - Poor performance despite calibration: Could indicate p(x|y) ≠ q(x|y) or highly imbalanced true label distribution
  - High variance in results: Could indicate insufficient number of generations L
  - Computational bottleneck: Could indicate need to optimize generation process or reduce L

- First 3 experiments:
  1. Verify Monte Carlo estimate stability: Run GC with increasing L and plot convergence of macro-F1
  2. Test uniform vs. empirical q(y): Implement alternative calibration assuming q(y) is estimated from training data, compare performance
  3. Analyze label conditional approximation: Compute AUROC vs. macro-F1 across datasets to confirm p(x|y) ≈ q(x|y) holds generally

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed generative calibration method be extended to non-text classification tasks, such as image or tabular data?
- Basis in paper: [inferred] The paper focuses on text classification tasks and 12 text classification datasets. The method relies on Monte Carlo sampling over the in-context model to estimate label marginals, which may not directly apply to non-text data.
- Why unresolved: The paper does not explore the applicability of the method to non-text data. Extending the approach would require generating samples from the in-context model in non-text domains, which may be challenging or require significant modifications.
- What evidence would resolve it: Experiments applying generative calibration to image classification or tabular data tasks, comparing performance to ICL and other calibration methods.

### Open Question 2
- Question: How does the proposed method handle the case where the in-context model does not have a label shift, i.e., when the label marginal is already close to the true data distribution?
- Basis in paper: [inferred] The paper assumes the in-context model has a label shift and calibrates by adjusting the label marginal. However, it does not discuss what happens if this assumption is not true.
- Why unresolved: The paper does not provide any analysis or experiments on cases where the in-context model does not have a label shift. It is unclear if the method would still improve performance or potentially degrade it in such scenarios.
- What evidence would resolve it: Experiments on datasets or model configurations where the in-context model does not exhibit label shift, comparing the performance of generative calibration to ICL and other methods.

### Open Question 3
- Question: Can the generative calibration method be adapted to handle more complex distribution shifts beyond label shift, such as covariate shift or concept drift?
- Basis in paper: [explicit] The paper focuses on label shift, where the in-context model and data share the same label conditional but differ in the label marginal. It does not discuss other types of distribution shifts.
- Why unresolved: The paper does not explore the applicability of the method to other types of distribution shifts. Extending the approach to handle covariate shift or concept drift would require significant modifications to the calibration strategy.
- What evidence would resolve it: Experiments applying generative calibration to scenarios with covariate shift or concept drift, comparing performance to ICL and other methods designed to handle those shifts.

## Limitations
- The core assumption that p(x|y) ≈ q(x|y) while only the label marginal shifts needs stronger empirical validation across diverse datasets
- The uniform label marginal assumption (q(y) uniform) is a significant simplification that may not hold for many real-world datasets
- Monte Carlo sampling with L=100 may not provide stable estimates for all dataset complexities and label spaces

## Confidence

**High Confidence:** The empirical results showing GC consistently improves ICL performance across 12 tasks and 12 LLMs are well-supported. The finding that AUROC remains high while macro-F1 varies with prompt configurations is directly observable from the data.

**Medium Confidence:** The theoretical framework connecting ICL to posterior predictive distributions is sound, but the approximation that p(x|y) ≈ q(x|y) needs more direct validation. The Monte Carlo estimation approach is theoretically justified, but the practical choice of L=100 and its sufficiency across different settings could be better justified.

**Low Confidence:** The assumption that q(y) is uniform or that forcing p(y) to be uniform is beneficial is the weakest link. The paper acknowledges this as a limitation but does not provide empirical validation of this assumption or explore alternative calibration strategies that estimate the true q(y).

## Next Checks

1. **Label Conditional Validation:** Conduct systematic experiments measuring the distance between p(x|y) and q(x|y) across all 12 datasets. This could involve computing Wasserstein distance or KL divergence between the conditional distributions, or comparing AUROC and macro-F1 correlation patterns across more diverse datasets.

2. **Monte Carlo Convergence Analysis:** Vary L systematically (e.g., L ∈ {10, 25, 50, 100, 200}) and plot macro-F1 convergence curves. Include statistical tests to determine when additional samples no longer provide meaningful improvements, and test whether L=100 is sufficient for datasets with different label granularities.

3. **Non-Uniform Label Marginal Testing:** Implement GC variants that estimate q(y) from the training data or use domain-specific priors instead of assuming uniformity. Compare performance across datasets with known label imbalances to understand when the uniform assumption helps versus harms calibration.