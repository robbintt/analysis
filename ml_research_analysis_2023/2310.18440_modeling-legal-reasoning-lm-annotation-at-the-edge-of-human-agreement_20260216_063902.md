---
ver: rpa2
title: 'Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement'
arxiv_id: '2310.18440'
source_url: https://arxiv.org/abs/2310.18440
tags:
- reasoning
- legal
- grand
- formal
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined whether large language models can perform a
  complex legal reasoning classification task without fine-tuning. Using a novel dataset
  of Supreme Court opinions annotated by law students, the authors compared prompted
  generative models against fine-tuned models.
---

# Modeling Legal Reasoning: LM Annotation at the Edge of Human Agreement

## Quick Facts
- **arXiv ID**: 2310.18440
- **Source URL**: https://arxiv.org/abs/2310.18440
- **Reference count**: 14
- **Primary result**: Prompted generative models perform poorly on complex legal reasoning classification compared to fine-tuned models, with in-domain LEGAL-BERT achieving the best results

## Executive Summary
This study examines whether large language models can perform complex legal reasoning classification without fine-tuning. Using a novel dataset of Supreme Court opinions annotated by law students, the authors compare prompted generative models against fine-tuned models. They find that prompted models perform poorly even with detailed instructions, while fine-tuned models, particularly LEGAL-BERT, achieve much stronger results. The best-performing model successfully recovers known historical trends in legal reasoning, validating both the model and historical accounts. The findings suggest that complex specialized tasks require expert annotation and task-specific fine-tuning, cautioning against relying solely on prompting for such applications.

## Method Summary
The authors created a novel dataset by annotating Supreme Court opinions with three classes of legal reasoning: formal, grand, or none. Law students annotated paragraphs after receiving training and a decision chart to improve consistency. They compared five fine-tuned models (BERT-base, DistilBERT, T5-small, T5-base, and LEGAL-BERT) against three prompted models (GPT-4, FLAN-T5, and Llama-2-Chat). Fine-tuned models were trained for three epochs with a learning rate of 2e-5, while prompted models used in-context learning with descriptions, examples, and chain-of-thought reasoning. Performance was evaluated using macro-averaged F1 scores across five random train/test splits.

## Key Results
- Prompted models (GPT-4, FLAN-T5, Llama-2-Chat) performed poorly on legal reasoning classification even with detailed instructions
- Fine-tuned models achieved significantly better results, with LEGAL-BERT performing best due to its in-domain pre-training
- The best model successfully recovered historical trends in legal reasoning, validating both the model and historical accounts
- Inter-rater reliability improved from Krippendorff's alpha of 0.53 to 0.63 after introducing a decision chart

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on expert-annotated data is necessary for complex specialized tasks where abstract reasoning and domain knowledge are required.
- Mechanism: Models can learn to map abstract concepts to concrete text features when trained on task-specific data, but struggle to do so from prompts alone.
- Core assumption: Abstract legal reasoning concepts cannot be reliably learned through in-context prompting alone.
- Evidence anchors:
  - [abstract]: "we find that generative models perform poorly when given instructions (i.e. prompts) equal to the instructions presented to human annotators through our codebook"
  - [section]: "Even with the upsampling of legal interpretation based on seed terms, paragraphs that did not engage in legal interpretation...made up 68% of the data"
- Break condition: If a task can be solved with purely lexical associations or if the domain concepts are already well-represented in pre-training data.

### Mechanism 2
- Claim: In-domain pre-training (LEGAL-BERT) provides significant performance gains for specialized tasks.
- Mechanism: Models pre-trained on domain-specific corpora develop better representations of domain-specific language patterns and concepts.
- Core assumption: Legal text has distinctive linguistic patterns that general pre-training misses.
- Evidence anchors:
  - [abstract]: "Our strongest results derive from fine-tuning models on the annotated dataset; the best performing model is an in-domain model, LEGAL-BERT"
  - [section]: "Models were chosen based on established usage, popularity, and accessibility (i.e. model size), since applied NLP researchers may be less likely to have access to the computing power needed for extremely large models"
- Break condition: If general-domain models are already well-tuned to the specific domain or if the task doesn't require deep domain understanding.

### Mechanism 3
- Claim: Complex classification tasks benefit from breaking down decisions into guided steps (decision charts).
- Mechanism: Structured decision processes help annotators (and models) systematically evaluate evidence for each class.
- Core assumption: Abstract classification decisions can be decomposed into concrete evaluation questions.
- Evidence anchors:
  - [section]: "Inter-rater reliability increased after the introduction of a decision chart (Figure 1), which broke down decisions about each of the classes into a series of guided questions"
  - [section]: "A decision chart was created and provided to annotators between the fourth and fifth weeks of annotations...Following the incorporation of this decision chart, we saw boosted inter-rater reliability"
- Break condition: If the task cannot be decomposed into concrete evaluation questions or if the decomposition itself introduces bias.

## Foundational Learning

- **Krippendorff's alpha for inter-rater reliability**
  - Why needed here: To measure agreement between annotators on a complex classification task where decisions are subjective
  - Quick check question: If four annotators label 100 paragraphs and Krippendorff's alpha is 0.63, what does this tell us about agreement?

- **Upsampling for class balance**
  - Why needed here: To ensure sufficient examples of rare classes (formal reasoning at 11% of data) for model training
  - Quick check question: If a dataset has 25% formal reasoning and 75% non-formal, what sampling strategy would ensure the model sees enough formal examples?

- **Nested model architecture**
  - Why needed here: To break a complex multi-class problem into simpler binary classification stages
  - Quick check question: In a nested approach for legal reasoning, what would be the two stages and why might this help?

## Architecture Onboarding

- **Component map**: Raw opinions → paragraph splitting → seed-based upsampling → expert annotation → train/test splits → fine-tuning vs. prompting → macro F1 evaluation → historical trend analysis

- **Critical path**: 
  1. Expert annotation (weeks to months)
  2. Model fine-tuning (hours to days per model)
  3. Cross-validation and selection of best model
  4. Historical trend analysis

- **Design tradeoffs**:
  - Prompting vs. fine-tuning: Cost (annotation time) vs. performance
  - Model size vs. accessibility: Larger models may perform better but require more resources
  - Multi-class vs. nested: Simpler pipeline vs. potentially better performance

- **Failure signatures**:
  - Low inter-rater reliability persisting after training
  - Models consistently predicting one class regardless of input
  - Prompts generating irrelevant or hallucinated text

- **First 3 experiments**:
  1. Fine-tune BERT-base on the annotated data with standard hyperparameters (3 epochs, lr=2e-5)
  2. Test GPT-4 with the codebook prompt on a small validation set
  3. Compare performance of multi-class vs. nested model architectures on the same training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of legal reasoning classification models change if the task included additional classes of reasoning beyond just formal and grand, such as textualist or purposivist approaches?
- Basis in paper: [inferred] The paper focuses on just two classes of legal reasoning (formal and grand) plus a "none" category. It mentions other scholars have identified different approaches like textualism and purposivism.
- Why unresolved: The study specifically chose to focus on the formal/grand distinction as it has broad consensus among legal scholars. Testing additional classes would require a different dataset and annotation process.
- What evidence would resolve it: Annotating a new dataset with more granular classes of legal reasoning and evaluating model performance on this expanded task.

### Open Question 2
- Question: How would the performance of legal reasoning classification models change if they were fine-tuned on a more diverse set of legal documents beyond just Supreme Court opinions, such as lower court decisions or legislative history?
- Basis in paper: [explicit] The paper specifically focuses on Supreme Court opinions and mentions that the annotators were upper-year law students at a highly selective law school.
- Why unresolved: The study's dataset is limited to Supreme Court opinions on statutory interpretation. Testing performance on a broader range of legal documents would require a different dataset.
- What evidence would resolve it: Creating a new dataset with a wider variety of legal documents and evaluating model performance on this expanded task.

### Open Question 3
- Question: How would the performance of legal reasoning classification models change if the task was framed as a multi-label classification problem rather than a multi-class problem, allowing for the possibility of a single passage exhibiting both formal and grand reasoning?
- Basis in paper: [explicit] The paper frames the task as a multi-class classification problem with three mutually exclusive classes (formal, grand, none).
- Why unresolved: The study assumes that a given passage can only exhibit one type of reasoning. Testing a multi-label approach would require a different annotation scheme.
- What evidence would resolve it: Annotating a new dataset where passages can be labeled with multiple classes of reasoning and evaluating model performance on this multi-label task.

## Limitations

- The study relies on a single complex legal reasoning task, limiting generalizability to other domains or reasoning tasks
- The comparison between prompted and fine-tuned models may not be entirely fair, as prompted models were tested without the opportunity to learn from task-specific patterns in the data
- The study does not explore intermediate approaches such as few-shot learning or parameter-efficient fine-tuning that might offer better performance-cost tradeoffs

## Confidence

- **High Confidence**: Core finding that prompted models perform substantially worse than fine-tuned models on complex legal reasoning tasks
- **Medium Confidence**: Claim that LEGAL-BERT's domain-specific pre-training provides significant advantages
- **Low Confidence**: Broader claims about the impossibility of solving complex specialized tasks through prompting alone

## Next Checks

1. **Replication with alternative annotation frameworks**: Replicate the annotation process using different legal reasoning classification schemes to determine whether the observed performance gaps persist across different task formulations and whether the decision chart's effectiveness generalizes.

2. **Comparative evaluation of intermediate approaches**: Test few-shot learning, parameter-efficient fine-tuning, and retrieval-augmented prompting methods to establish whether there are viable alternatives between full fine-tuning and basic prompting that might offer better cost-performance tradeoffs.

3. **Cross-domain generalization study**: Apply the same prompted vs. fine-tuned comparison framework to at least two other specialized domains (e.g., medical diagnosis classification, technical documentation categorization) to determine whether the observed patterns hold beyond legal reasoning tasks.