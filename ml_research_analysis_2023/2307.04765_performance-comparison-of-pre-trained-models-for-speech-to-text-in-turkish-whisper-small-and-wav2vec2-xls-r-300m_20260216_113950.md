---
ver: rpa2
title: 'Performance Comparison of Pre-trained Models for Speech-to-Text in Turkish:
  Whisper-Small and Wav2Vec2-XLS-R-300M'
arxiv_id: '2307.04765'
source_url: https://arxiv.org/abs/2307.04765
tags:
- konu
- veri
- metne
- whisper-small
- olarak
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compares two pre-trained multilingual models, Whisper-Small
  and Wav2Vec2-XLS-R-300M, for speech-to-text conversion in Turkish. The models were
  fine-tuned using the Turkish Mozilla Common Voice version 11.0 dataset.
---

# Performance Comparison of Pre-trained Models for Speech-to-Text in Turkish: Whisper-Small and Wav2Vec2-XLS-R-300M

## Quick Facts
- arXiv ID: 2307.04765
- Source URL: https://arxiv.org/abs/2307.04765
- Reference count: 0
- Whisper-Small achieved WER 0.16 on Turkish, Wav2Vec2-XLS-R-300M achieved WER 0.28

## Executive Summary
This study compares two pre-trained multilingual speech models—Whisper-Small and Wav2Vec2-XLS-R-300M—for Turkish speech-to-text conversion. Both models were fine-tuned on the Turkish Mozilla Common Voice version 11.0 dataset (88 hours). Whisper-Small demonstrated superior performance with a Word Error Rate (WER) of 0.16 compared to Wav2Vec2-XLS-R-300M's WER of 0.28. Additionally, Whisper-Small showed better generalization to out-of-domain call center recordings not included in the training or validation datasets.

## Method Summary
The study fine-tuned pre-trained multilingual speech models (Whisper-Small and Wav2Vec2-XLS-R-300M) on the Turkish Mozilla Common Voice dataset version 11.0. Whisper-Small was trained for 5,000 steps with a learning rate of 1e-5 and batch size 32, while Wav2Vec2-XLS-R-300M was trained for 30 epochs with a learning rate of 3e-4 and batch size 64. Both models were evaluated using Word Error Rate (WER) and Character Error Rate (CER) metrics on both in-domain validation data and out-of-domain call center recordings.

## Key Results
- Whisper-Small achieved a WER of 0.16 on Turkish Common Voice validation data, outperforming Wav2Vec2-XLS-R-300M's WER of 0.28
- Whisper-Small demonstrated better generalization to out-of-domain call center recordings not seen during training
- Both models showed higher error rates on noisy call center data compared to clean Common Voice data

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning pre-trained multilingual speech models on a small, domain-specific Turkish dataset can yield competitive WER performance compared to large-scale monolingual training. The multilingual pre-training provides a robust acoustic-phonetic foundation across many languages, reducing the need for massive language-specific data. Fine-tuning adapts this shared representation to Turkish phonology and morphology, yielding effective performance with limited Turkish data.

### Mechanism 2
Whisper-Small's architectural design and training objective lead to better generalization to out-of-domain data (call center recordings) than Wav2Vec2-XLS-R-300M. Whisper uses a decoder that can incorporate punctuation and casing during generation, improving its ability to handle real-world speech characteristics and noise. Its large, diverse pretraining corpus (680k hours) provides robustness to domain shifts.

### Mechanism 3
Whisper's larger pretraining dataset and task diversity improve robustness and generalization compared to Wav2Vec2-XLS-R-300M. Whisper was trained on 680k hours of web data with multiple tasks (translation, timestamps, language identification), creating a more robust and adaptable model. This broader training scope helps in handling out-of-domain and noisy data.

## Foundational Learning

- **Speech-to-text model architecture (encoder-decoder vs. encoder-only)**: Understanding the architectural differences between Whisper (encoder-decoder) and Wav2Vec2-XLS-R-300M (encoder-only) is critical to explaining their performance differences.
  - Quick check: What is the primary functional difference between an encoder-decoder and an encoder-only model in speech processing?

- **Fine-tuning and transfer learning**: Both models are pre-trained and then fine-tuned; understanding how fine-tuning works and its limitations is key to interpreting the results.
  - Quick check: Why is fine-tuning on a small dataset often preferred over training from scratch in speech recognition?

- **Evaluation metrics (WER, CER)**: WER and CER are used to evaluate model performance; understanding their calculation and interpretation is necessary to assess the results.
  - Quick check: How is Word Error Rate (WER) calculated, and what does a lower WER indicate about a model's performance?

## Architecture Onboarding

- **Component map**:
  - Whisper-Small: Audio preprocessing → Log-Mel spectrogram → Encoder (transformer) → Decoder (transformer) → Text output with punctuation
  - Wav2Vec2-XLS-R-300M: Audio preprocessing → Feature encoder → Context network (transformer) → Quantization → Linear projection → Text output (no punctuation)

- **Critical path**:
  - Whisper: Audio → Log-Mel → Encoder → Decoder → Text
  - Wav2Vec2: Audio → Feature encoder → Context network → Quantization → Linear projection → Text

- **Design tradeoffs**:
  - Whisper: Larger model size, slower inference, but better generalization and punctuation support
  - Wav2Vec2: Smaller, faster, but less robust to out-of-domain data and lacks punctuation

- **Failure signatures**:
  - Whisper: Overfitting on small datasets (observed at step 2000 in the study), slower training/inference
  - Wav2Vec2: Poor generalization to noisy or out-of-domain data, lack of punctuation in output

- **First 3 experiments**:
  1. Fine-tune Whisper-Small and Wav2Vec2-XLS-R-300M on a small Turkish dataset; compare WER on a held-out validation set
  2. Test both models on in-domain clean speech and out-of-domain noisy speech; compare WER and CER
  3. Vary fine-tuning dataset size and measure impact on WER; observe overfitting behavior

## Open Questions the Paper Calls Out

### Open Question 1
How do Whisper-Small and Wav2Vec2-XLS-R-300M models perform on other low-resource languages with similar data characteristics to Turkish?
- Basis in paper: The study focuses on Turkish, but the models are multilingual and were tested on a limited dataset, suggesting potential applicability to other low-resource languages
- Why unresolved: The paper does not extend its analysis to other languages, leaving the generalizability of the findings uncertain
- What evidence would resolve it: Conducting similar fine-tuning experiments on other low-resource languages and comparing WER values would provide insights into the models' cross-linguistic performance

### Open Question 2
What impact does the inclusion of call center data in the training set have on the models' performance for real-world applications?
- Basis in paper: The study notes that both models performed poorly on call center recordings not included in the training set, indicating a gap in handling domain-specific data
- Why unresolved: The study does not explore the effects of incorporating call center data into the training process to improve performance
- What evidence would resolve it: Including call center data in the training set and evaluating the models' performance on similar test data would demonstrate the impact of domain-specific training

### Open Question 3
How does the performance of these models change with larger and more diverse datasets?
- Basis in paper: The study uses a relatively small dataset (88 hours), and the authors suggest that increasing data diversity could enhance STT performance
- Why unresolved: The study does not experiment with larger datasets to assess scalability and performance improvements
- What evidence would resolve it: Fine-tuning the models with larger and more diverse datasets and comparing the resulting WER values would reveal the potential benefits of increased data volume and variety

## Limitations
- Small training dataset (88 hours) may not represent all Turkish speech patterns, particularly agglutinative morphology and dialectal variation
- Lack of direct comparison with other Turkish ASR models or larger monolingual models makes absolute performance assessment difficult
- Call center test dataset characteristics (size, noise, speaker diversity) are not fully specified, limiting generalizability of out-of-domain results

## Confidence
- **High confidence**: The relative performance comparison between Whisper-Small and Wav2Vec2-XLS-R-300M on the Turkish Common Voice dataset (WER 0.16 vs 0.28)
- **Medium confidence**: The generalization advantage of Whisper-Small on call center data
- **Low confidence**: Claims about Whisper's superior architectural design being the primary driver of performance

## Next Checks
1. Replicate the study with a larger Turkish speech dataset (e.g., combining multiple Turkish ASR corpora) to determine if the relative performance gap persists
2. Conduct ablation studies by fine-tuning both models on matched pretraining data to isolate architectural differences versus pretraining data diversity
3. Test both models on a standardized, publicly available Turkish ASR benchmark with diverse domains and noise conditions