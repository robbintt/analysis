---
ver: rpa2
title: The WebCrow French Crossword Solver
arxiv_id: '2311.15626'
source_url: https://arxiv.org/abs/2311.15626
tags:
- crossword
- french
- crosswords
- webcrow
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces French WebCrow, the first AI crossword solver\
  \ for French puzzles, addressing the challenge of solving crosswords in a language\
  \ where large clue-answer datasets are scarce. French WebCrow extends the modular\
  \ WebCrow 2.0 framework, integrating multiple expert modules\u2014including web\
  \ search, knowledge graphs, and linguistic rules\u2014to generate candidate answers\
  \ without relying solely on prior crossword data."
---

# The WebCrow French Crossword Solver

## Quick Facts
- arXiv ID: 2311.15626
- Source URL: https://arxiv.org/abs/2311.15626
- Reference count: 23
- Primary result: First AI crossword solver for French puzzles, achieving 65.71% correct words and 75.22% correct letters accuracy on 62 unseen crosswords

## Executive Summary
This paper introduces French WebCrow, the first AI crossword solver for French puzzles, addressing the challenge of solving crosswords in a language where large clue-answer datasets are scarce. French WebCrow extends the modular WebCrow 2.0 framework, integrating multiple expert modules—including web search, knowledge graphs, and linguistic rules—to generate candidate answers without relying solely on prior crossword data. The system was tested on 62 unseen French crosswords, achieving 65.71% correct words and 75.22% correct letters accuracy, outperforming average human solvers in speed and accuracy during competitive challenges. Ablation tests confirmed the necessity of each expert module. The work demonstrates that a modular, knowledge-diverse approach can effectively generalize to new languages, paving the way for future multilingual and generative crossword-solving systems.

## Method Summary
French WebCrow implements a modular architecture with expert modules for web search, knowledge graphs, word embeddings, and rule-based linguistic patterns. These modules generate candidate answers from their respective knowledge domains, which are then merged using length-aware weighted averaging. The system employs a Char-based grid filler that computes per-cell letter probabilities from crossing answers, placing letters only when confidence thresholds are met in both directions. The approach was trained on ~7,000 solved French crosswords and ~300,000 clue-answer pairs, then evaluated on 62 unseen puzzles.

## Key Results
- Achieved 65.71% correct words and 75.22% correct letters accuracy on 62 unseen French crosswords
- Outperformed average human solvers in competitive challenges, particularly in speed
- Ablation tests confirmed each expert module (WebSearch, Knowledge Graph, Rule-Based, Word Embedding) is necessary for optimal performance
- Char-based grid filling approach proved more robust than word-based methods when candidate lists lacked correct answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple expert modules each cover distinct knowledge domains, enabling robust clue solving in low-resource languages
- Mechanism: The system integrates lexical/ontological knowledge, crossword-specific patterns, and factual/common knowledge through separate expert modules that independently generate candidate answers from their domain, merged probabilistically
- Core assumption: Clue-answer pairs for French crosswords are too sparse for any single module to be sufficient; diversity of knowledge sources compensates for data scarcity
- Evidence anchors: Abstract shows competitive performance; section 3.2 describes modular architecture; no direct citations found linking French crosswords to modular approaches specifically

### Mechanism 2
- Claim: Merging candidate lists from multiple experts using weighted averaging by answer length produces more accurate final answer rankings than any single expert
- Mechanism: After each expert generates ranked candidate answers, the system performs length-aware weighted averaging, with weights trained on available data to optimize accuracy
- Core assumption: Different experts perform better on different clue types or answer lengths; fixed weighting would be suboptimal
- Evidence anchors: Section 3.3 describes weighted average procedure; section 6 shows ablation results demonstrating each module's contribution

### Mechanism 3
- Claim: Char-based grid filling constrains letter-level consistency better than word-based approaches in low-data scenarios
- Mechanism: Instead of placing whole words, the solver computes per-cell letter probabilities from all candidate answers crossing that cell, placing letters meeting strict confidence thresholds in both across/down directions
- Core assumption: Even when correct words are missing from candidate lists, correct letters can still be inferred from partial matches and constraints
- Evidence anchors: Section 3.4 describes Char Base Solver robustness; section 6 reports remarkable overall results

## Foundational Learning

- Concept: Constraint Satisfaction Problem (CSP) formulation for crossword solving
  - Why needed here: Enables systematic search over candidate answers while respecting grid constraints
  - Quick check question: How does CSP differ from simple greedy filling in crossword solving?

- Concept: Word embeddings for semantic similarity in clue retrieval
  - Why needed here: Allows matching clues to previously seen clue-answer pairs even when wording differs
  - Quick check question: What property of word embeddings makes them useful for finding similar crossword clues?

- Concept: Knowledge graph traversal for ontological reasoning
  - Why needed here: Helps solve clues requiring understanding of concept relationships (e.g., synonyms, hierarchical relations)
  - Quick check question: How can a knowledge graph help distinguish between "sick" and "ill" in crossword contexts?

## Architecture Onboarding

- Component map: Clue Analysis Pipeline → Expert Modules (Word Embedding, WebSearch, Knowledge Graph, Rule-Based) → Answer Merging → Char-Based Grid Filler
- Critical path: Clue → All experts in parallel → Merging → Grid filling
- Design tradeoffs: Modularity vs. latency; breadth of knowledge vs. specificity; probabilistic merging vs. deterministic selection
- Failure signatures: High variance in expert performance; merging weights poorly calibrated; low per-letter confidence across grid
- First 3 experiments:
  1. Run ablation tests removing each expert to measure individual contribution
  2. Vary merging weights to find optimal configuration for French crossword length distribution
  3. Compare char-based vs. word-based grid filling on a subset of puzzles with known ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of French WebCrow compare to other AI crossword solvers on American-style crosswords, given that French crosswords have more linguistic diversity and less standardization?
- Basis in paper: [inferred] The paper mentions American crosswords are more standard, while French crosswords vary greatly in size, knowledge, and language games, but no direct comparison is made
- Why unresolved: Paper focuses on French crosswords without testing on American puzzles
- What evidence would resolve it: Testing French WebCrow on American crosswords and comparing performance metrics to state-of-the-art solvers like Dr. Fill or Berkeley Crossword Solver

### Open Question 2
- Question: What is the optimal balance between rule-based expert systems and neural network-based approaches for solving French crosswords, particularly for handling inflections and rare words?
- Basis in paper: [explicit] Paper discusses both rule-based experts and neural network approaches, noting inflection handling issues
- Why unresolved: No systematic comparison of different ratios or combinations provided
- What evidence would resolve it: Conducting ablation studies varying weights of rule-based versus neural modules, measuring performance impact on inflection handling

### Open Question 3
- Question: How does the performance of French WebCrow degrade when tested on crosswords from authors not included in the training data, and what are the key linguistic or cultural features that cause the most difficulty?
- Basis in paper: [explicit] Paper notes French crossword authors have distinct individual styles and domain-specific knowledge
- Why unresolved: While performance variation across sources is mentioned, detailed analysis of specific linguistic features is not provided
- What evidence would resolve it: Detailed error analysis of crosswords from previously unseen authors, categorizing errors by type and correlating with specific linguistic features

## Limitations
- Reliance on expert modules trained for French language characteristics with no explicit cross-linguistic validation beyond Italian-to-French adaptation
- Merging weights optimized for French corpus may not generalize to other languages or crossword styles
- Performance on truly unseen clue types (not covered by any expert module) remains untested

## Confidence

- **High Confidence**: Modular architecture's effectiveness in handling multiple knowledge domains; ablation test results showing each expert module contributes measurably
- **Medium Confidence**: Superiority of char-based grid filling approach; merging strategy's optimality for French crosswords
- **Low Confidence**: Claims about generalization to other languages beyond French; assertions about outperforming humans in all crossword difficulty ranges

## Next Checks

1. **Cross-linguistic Validation**: Test French WebCrow on Italian crosswords (using the same system without language-specific retraining) to assess true zero-shot cross-language performance

2. **Robustness to Clue Novelty**: Create test puzzles containing clues that specifically target the blind spots of each expert module (e.g., puns, cultural references) to measure performance degradation

3. **Human Performance Calibration**: Conduct controlled experiments comparing French WebCrow against expert human solvers across different difficulty levels and crossword styles to validate the "outperforms humans" claim in specific contexts