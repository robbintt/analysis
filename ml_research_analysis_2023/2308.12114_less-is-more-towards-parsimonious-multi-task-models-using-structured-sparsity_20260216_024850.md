---
ver: rpa2
title: Less is More -- Towards parsimonious multi-task models using structured sparsity
arxiv_id: '2308.12114'
source_url: https://arxiv.org/abs/2308.12114
tags:
- sparsity
- tasks
- multi-task
- learning
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces structured (group) sparsity into the shared
  parameters of multi-task learning models, specifically applying channel-wise l1/l2
  group sparsity in the shared convolutional layers. The approach aims to develop
  parsimonious models with fewer parameters while maintaining or improving performance
  across multiple tasks.
---

# Less is More -- Towards parsimonious multi-task models using structured sparsity

## Quick Facts
- **arXiv ID**: 2308.12114
- **Source URL**: https://arxiv.org/abs/2308.12114
- **Reference count**: 12
- **Primary result**: Multi-task models with ~70% sparsity outperform dense equivalents

## Executive Summary
This work introduces structured (group) sparsity into the shared parameters of multi-task learning models, specifically applying channel-wise l1/l2 group sparsity in the shared convolutional layers. The approach aims to develop parsimonious models with fewer parameters while maintaining or improving performance across multiple tasks. The method leverages the group lasso penalty to eliminate entire parameter groups (channels) that are not essential for task performance, thereby reducing memory footprint and computational cost during inference.

Experiments on NYU-v2 and CelebAMask-HQ datasets demonstrate that multi-task models with approximately 70% sparsity outperform their dense equivalents. The study analyzes how varying the degree of sparsification affects model performance, overall sparsity percentage, sparsity patterns, and inference time, showing a clear trade-off between sparsity and task performance with an optimal point around 70% sparsity.

## Method Summary
The method applies channel-wise l1/l2 group sparsity to the shared convolutional layers of multi-task learning models. Each convolutional channel is treated as a parameter group, and the group lasso penalty computes the l2 norm of each channel's weights. During training, proximal gradient updates are used to set entire channels to zero when their norm falls below a threshold determined by the regularization parameter λ. This structured pruning approach differs from element-wise sparsity by enabling efficient inference through channel elimination rather than sparse matrix operations.

## Key Results
- Multi-task models with approximately 70% sparsity outperform their dense equivalents on NYU-v2 and CelebAMask-HQ datasets
- The 70% sparsity sweet spot balances parameter reduction with task performance retention
- Channel-wise group sparsity enables effective soft and hard parameter sharing, reducing task interference
- The approach achieves significant computational efficiency during inference while maintaining or improving task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Channel-wise group sparsity induces structured parameter elimination in shared layers by zeroing entire convolutional channels.
- Mechanism: The l1/l2 group lasso penalty computes the l2 norm of each channel (parameter group), and the proximal operator sets the channel to zero if its norm is below the regularization threshold, effectively removing the channel.
- Core assumption: The channel-wise grouping assumption holds that each convolutional channel is a natural parameter group that can be independently pruned without breaking network topology.
- Evidence anchors:
  - [abstract] "channel-wise l1/l2 group sparsity in the shared convolutional layers parameters"
  - [section] "The channel-wise structure sparsity is introduced in each convolutional layer of the backbone by considering each channel as a group"
- Break condition: If the proximal update threshold is set too low, no channels will be pruned; if too high, the network loses all representational capacity.

### Mechanism 2
- Claim: Structured sparsity improves multi-task performance by reducing task interference and enabling soft parameter sharing.
- Mechanism: The l2 component of the penalty encourages related tasks to share similar parameters (soft sharing), while the l1 component promotes selective sharing by eliminating redundant channels (hard sharing), reducing negative transfer.
- Evidence anchors:
  - [abstract] "imposes a penalty on the weights, further enhancing the learning efficiency for all tasks (due to l2 regularization)"
  - [section] "the utilization of l2 regularization in the penalty term outlined in Equation 3 facilitates soft parameter sharing, while the application of l1 regularization promotes appropriate hard parameter sharing"
- Break condition: If tasks are completely unrelated, the soft sharing benefit disappears and only hard pruning remains.

### Mechanism 3
- Claim: The 70% sparsity sweet spot balances parameter reduction with task performance retention.
- Mechanism: At moderate sparsity levels, the network eliminates channels that contribute little to task performance while preserving the core representational capacity needed for all tasks.
- Evidence anchors:
  - [abstract] "multi-task models with approximately 70% sparsity outperform their dense equivalents"
  - [section] "there is a trade-off between task performance and the degree of sparsity. Specifically, performance improvements are observed as sparsity increases to a certain point, beyond which performance begins to decline"
- Break condition: If sparsity exceeds the optimal point (around 70%), performance degrades due to loss of essential representational capacity.

## Foundational Learning

- **Concept**: Group Lasso Regularization (l1/l2 penalty)
  - Why needed here: This is the core mechanism for inducing structured sparsity in shared layers. It combines l1 sparsity promotion with l2 weight penalization to eliminate entire parameter groups.
  - Quick check question: What is the mathematical form of the group lasso penalty and how does it differ from standard l1 regularization?

- **Concept**: Multi-Task Learning with Hard Parameter Sharing
  - Why needed here: The paper builds on the standard MTL architecture where a shared backbone feeds into task-specific heads. Understanding this baseline is crucial for appreciating how sparsity modifies the shared representation learning.
  - Quick check question: In hard parameter sharing, what are the mathematical constraints on the shared and task-specific parameters?

- **Concept**: Proximal Gradient Methods
  - Why needed here: Since the group lasso penalty is non-differentiable, proximal gradient descent is required to optimize the composite loss function.
  - Quick check question: What is the closed-form solution for the proximal operator of the l1/l2 group penalty and how does it implement channel-wise pruning?

## Architecture Onboarding

- **Component map**: Input (256x256 RGB images) -> Shared backbone (Dilated ResNet-50 with group sparsity) -> Task-specific heads (Deeplab-V3 for dense tasks, conv+FC for classification) -> Uncertainty-weighted loss combination -> Adam optimizer with proximal gradient updates

- **Critical path**: The shared backbone with sparsity regularization → task-specific heads → uncertainty-weighted loss combination → adaptive proximal gradient updates

- **Design tradeoffs**:
  - Group sparsity vs. element-wise sparsity: Structured sparsity enables efficient inference through channel pruning, while element-wise sparsity offers finer-grained compression but less efficient inference
  - Sparsity level vs. performance: Higher sparsity reduces parameters but risks performance degradation; optimal point around 70% for this setup
  - Task relatedness vs. negative transfer: Related tasks benefit more from shared representations; unrelated tasks may suffer from interference

- **Failure signatures**:
  - All channels pruned (100% sparsity): Model cannot learn any representations, complete underfitting
  - Very low sparsity (<30%): No computational benefits realized
  - Performance degradation with increasing sparsity: Indicates essential channels being pruned
  - Task imbalance: Some tasks dominate the shared representation, others underperform

- **First 3 experiments**:
  1. Single-task experiments with varying λ values to understand sparsity-performance trade-off for each individual task
  2. Multi-task experiments with λ = 0 (no sparsity) to establish baseline performance with potential negative transfer
  3. Multi-task experiments with λ = 1e-5 to achieve optimal sparsity level and evaluate performance gains over dense baseline

## Open Questions the Paper Calls Out

- How does the performance of group sparsity in multi-task learning change when applied to different network architectures (e.g., transformers instead of CNNs)?
- What is the optimal method for automatically determining the regularization parameter λ during training rather than using manual tuning?
- How does group sparsity affect the learning dynamics and convergence speed of multi-task models compared to dense models?
- How does the performance of group sparsity change when applied to tasks with different levels of relatedness or similarity?

## Limitations

- The exact implementation details of the proximal gradient updates and the uncertainty-weighted loss function are not fully specified, making exact reproduction challenging
- The analysis focuses on channel-wise sparsity but doesn't explore alternative group structures (e.g., filter-wise or block-wise) that might yield different performance-sparsity trade-offs
- The study uses relatively small-scale multi-task settings (3-4 tasks); scalability to many more tasks remains unexplored

## Confidence

**High confidence**: The core mechanism of channel-wise group sparsity inducing structured pruning through the l1/l2 group lasso penalty is well-established theoretically and supported by empirical results showing the 70% sparsity sweet spot.

**Medium confidence**: The claim that structured sparsity improves multi-task performance by reducing task interference through soft and hard parameter sharing is plausible but requires further validation, particularly the relative contributions of l1 vs l2 components.

**Medium confidence**: The observed performance gains at 70% sparsity are compelling but may be dataset-specific; generalization to other task combinations and architectures needs verification.

## Next Checks

1. **Ablation on regularization components**: Systematically vary the l1 and l2 weights in the group lasso penalty to quantify their individual contributions to performance and sparsity patterns.

2. **Cross-dataset validation**: Apply the same methodology to a different multi-task setting (e.g., autonomous driving perception tasks) to assess generalizability of the 70% sparsity sweet spot.

3. **Alternative group structures**: Implement and compare alternative group sparsity patterns (filter-wise, block-wise) against the channel-wise approach to determine if the observed benefits are specific to channel-level pruning.