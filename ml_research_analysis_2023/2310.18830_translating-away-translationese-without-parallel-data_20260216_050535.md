---
ver: rpa2
title: Translating away Translationese without Parallel Data
arxiv_id: '2310.18830'
source_url: https://arxiv.org/abs/2310.18830
tags:
- training
- joint
- style
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel method to reduce translationese in
  human-translated texts by converting them to a more original-like style using self-supervised
  neural machine translation (NMT) without relying on parallel data. The authors propose
  a joint training approach combining self-supervised and unsupervised objectives,
  leveraging language model loss and semantic similarity loss to eliminate the need
  for parallel validation data.
---

# Translating away Translationese without Parallel Data

## Quick Facts
- arXiv ID: 2310.18830
- Source URL: https://arxiv.org/abs/2310.18830
- Reference count: 35
- Primary result: Achieves near-random translationese classification accuracy (50%) through self-supervised style transfer without parallel data

## Executive Summary
This paper introduces a novel approach to reduce translationese artifacts in human-translated texts by converting them to original-style text using self-supervised neural machine translation without requiring parallel data. The method leverages comparable corpora and joint training with self-supervised and unsupervised objectives to eliminate the need for parallel validation data. Experimental results demonstrate significant reduction in translationese classification accuracy to near-random levels while preserving content and fluency.

## Method Summary
The approach uses self-supervised neural machine translation with comparable corpora, employing a sentence pair extraction module to identify high-quality parallel sentence pairs based on semantic similarity in latent representations. The model is trained using joint objectives combining supervised cross-entropy loss from extracted pairs with unsupervised losses (language model loss for fluency and semantic similarity loss for content preservation). Gumbel-Softmax approximation enables end-to-end differentiability for continuous decoder outputs.

## Key Results
- Translationese classification accuracy reduced to 50.1% (near-random) for EN-ALL and 51.4% for DE-ALL
- Joint training outperforms both baseline self-supervised and pure unsupervised approaches
- Improvements in target-style fluency measured by lower perplexity and higher lexical diversity metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised training with comparable corpora enables learning translationese-to-original style transfer without parallel data
- Mechanism: Model learns to map translationese to original style by extracting high-quality parallel sentence pairs from comparable corpora using semantic similarity in latent representations
- Core assumption: Sufficient high-quality semantically aligned sentence pairs exist between original and translationese corpora
- Evidence anchors: [abstract] self-supervised approach learning from comparable mono-lingual data; [section 3.1] sentence-pair extraction using internal representations

### Mechanism 2
- Claim: Joint training with self-supervised and unsupervised objectives eliminates need for parallel validation data
- Mechanism: Unsupervised loss combines language model loss (target-style fluency) and semantic similarity loss (content preservation) computed over decoder outputs
- Core assumption: Unsupervised loss components provide sufficient signal for model selection without degrading style transfer quality
- Evidence anchors: [abstract] show how to eliminate parallel validation data by combining self-supervised with unsupervised loss; [section 3.2] linear combination of language model and semantic similarity losses

### Mechanism 3
- Claim: Translationese mitigation improves downstream task performance by reducing translation artifacts
- Mechanism: Converting translationese to original-style text removes systematic linguistic differences that bias cross-lingual NLP tasks
- Core assumption: Translationese artifacts significantly impact downstream task performance
- Evidence anchors: [abstract] translationese affects various cross-lingual NLP tasks potentially leading to biased results; [section 1] MT systems perform better when source is original and target is translated

## Foundational Learning

- Concept: Semantic similarity matching in latent space
  - Why needed here: Enables extraction of parallel sentence pairs from comparable corpora without explicit alignments
  - Quick check question: How does the model compute similarity between sentence pairs in latent space?

- Concept: Gumbel-Softmax approximation for continuous decoder outputs
  - Why needed here: Allows computing unsupervised losses while maintaining end-to-end differentiability
  - Quick check question: What problem does Gumbel-Softmax solve in this context?

- Concept: Language model loss as style discriminator
  - Why needed here: Ensures generated text matches original style fluency patterns
  - Quick check question: How does the language model loss component work in the unsupervised objective?

## Architecture Onboarding

- Component map: Sentence Pair Extraction (SPE) module -> Transformer encoder-decoder for style transfer -> Language model for target-style fluency -> Semantic similarity computation module -> Classification system for evaluation

- Critical path: 1) Extract parallel pairs from comparable corpora 2) Train style transfer model using extracted pairs 3) Apply joint training with unsupervised losses 4) Evaluate using translationese classifier

- Design tradeoffs: Quality vs quantity of extracted parallel pairs; weighting between supervised and unsupervised losses; model complexity vs training efficiency

- Failure signatures: High classification accuracy indicates poor style transfer; low semantic similarity scores indicate content loss; high language model perplexity indicates poor fluency

- First 3 experiments: 1) Test SPE module with different similarity thresholds 2) Compare joint training vs pure self-supervised training 3) Evaluate translationese classification on style-transferred outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between self-supervised and unsupervised loss components in the joint training objective?
- Basis in paper: The paper mentions hyperparameter α determines balance between objectives but doesn't explore optimal values
- Why unresolved: No systematic analysis of how different α values affect performance
- What evidence would resolve it: Ablation study showing impact of different α values on translationese reduction accuracy, content preservation (BERTScore), and fluency (PPL) metrics

### Open Question 2
- Question: How does the proposed approach generalize to other languages and domains beyond English and German?
- Basis in paper: Only evaluates on English and German datasets, effectiveness on other languages/domains not explored
- Why unresolved: No cross-linguistic or cross-domain analysis provided
- What evidence would resolve it: Experiments on additional language pairs and domains demonstrating robustness and generalization

### Open Question 3
- Question: Can the approach be extended to handle multi-style translationese where texts exhibit characteristics of multiple source languages?
- Basis in paper: Focuses on single source language translationese, handling multi-style translationese not addressed
- Why unresolved: No discussion or insights into handling multi-style translationese
- What evidence would resolve it: Experiments on datasets with multi-style translationese showing effectiveness in reducing artifacts from multiple source languages

## Limitations

- The study lacks validation that translationese mitigation translates to meaningful downstream task performance gains, despite theoretical claims about improving cross-lingual NLP tasks
- Reliance on MPDE corpus may not capture full diversity of translationese patterns across different domains and language pairs
- Effectiveness of unsupervised loss components for model selection remains theoretical without empirical comparison against traditional parallel validation approaches

## Confidence

- **High Confidence**: Core mechanism of using self-supervised learning with comparable corpora for style transfer is well-supported and technically sound
- **Medium Confidence**: Claim that joint training with unsupervised losses eliminates need for parallel validation data is plausible but not fully validated
- **Low Confidence**: Assertion that translationese mitigation will improve downstream task performance remains speculative without concrete evidence

## Next Checks

1. Conduct experiments measuring the impact of translationese mitigation on specific cross-lingual NLP tasks (sentiment analysis, named entity recognition) to verify whether reduced translationese artifacts translate to improved task performance

2. Implement controlled experiment comparing model selection using proposed unsupervised losses against traditional parallel validation data to quantify effectiveness and potential trade-offs

3. Test method's effectiveness across multiple diverse corpora beyond MPDE, including different domains (news, social media, technical documents) and language pairs, to assess robustness and generalizability of style transfer capabilities