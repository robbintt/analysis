---
ver: rpa2
title: Classifying World War II Era Ciphers with Machine Learning
arxiv_id: '2307.00501'
source_url: https://arxiv.org/abs/2307.00501
tags:
- enigma
- purple
- typex
- sigaba
- m209
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We classify WWII-era ciphers (Enigma, M-209, Sigaba, Purple, Typex)
  using machine learning and deep learning techniques. We experiment with three classic
  machine learning models (SVM, k-NN, RF) and four deep learning models (MLP, LSTM,
  ELM, CNN), trained on histogram, digram, and raw ciphertext features.
---

# Classifying World War II Era Ciphers with Machine Learning

## Quick Facts
- arXiv ID: 2307.00501
- Source URL: https://arxiv.org/abs/2307.00501
- Reference count: 40
- We classify WWII-era ciphers (Enigma, M-209, Sigaba, Purple, Typex) using machine learning and deep learning techniques, achieving over 97% accuracy.

## Executive Summary
This paper presents a machine learning approach to classifying World War II-era cipher machines using ciphertext analysis. The authors experiment with both classic (SVM, k-NN, RF) and deep learning models (MLP, LSTM, ELM, CNN) on three feature types: histograms, digrams, and raw sequences. They evaluate performance across four experimental scenarios involving different combinations of fixed/random plaintext and keys. The results show that classic machine learning models perform as well as deep learning models, with over 97% accuracy achieved for realistic scenarios using 1000-character ciphertexts.

## Method Summary
The authors generate a dataset of 1000 ciphertext messages (1000 characters each) for five WWII-era ciphers using CrypTool-2 and Brown Corpus plaintext. They extract three feature types (histogram counts, digram frequencies, and raw letter sequences) and split the data 80/20 for training/testing. Seven machine learning models (SVM, k-NN, Random Forest, MLP, LSTM, ELM, CNN) are trained on all three feature types, creating 21 total models. Performance is evaluated using accuracy, precision, recall, and F1-score across four experimental scenarios involving different combinations of fixed/random plaintext and keys.

## Key Results
- Classic ML models (SVM, k-NN, RF) perform as well as deep learning models for cipher classification
- Over 97% accuracy achieved for most realistic scenario with 1000-character ciphertexts
- M-209 was consistently the easiest cipher to classify, while Typex and Enigma were harder to distinguish but still identifiable with high accuracy

## Why This Works (Mechanism)

### Mechanism 1
Cipher-specific statistical biases remain detectable in ciphertext even under polyalphabetic encryption. WWII-era ciphers exhibit systematic deviations from uniform randomness (e.g., Purple's 6-20 split, M-209's non-uniform rotor stepping). Machine learning models extract these residual biases through feature extraction (histograms, digrams, raw sequences). Core assumption: Statistical patterns in plaintext interact predictably with cipher mechanics to create detectable artifacts in ciphertext.

### Mechanism 2
Model architecture choice affects sensitivity to different statistical feature types. Different ML models have varying capacities to capture local vs. global statistical dependencies. For example, CNNs can detect spatial patterns in digrams, while LSTMs capture sequential dependencies. Core assumption: The feature extraction method aligns with the model's inductive bias.

### Mechanism 3
Cipher classification accuracy degrades predictably with reduced ciphertext length. Statistical estimation error increases as sample size decreases. The models' confidence in distinguishing ciphers depends on having sufficient data to estimate underlying distributions accurately. Core assumption: The relationship between ciphertext length and classification accuracy follows a predictable pattern based on statistical power.

## Foundational Learning

- Concept: Statistical feature extraction from ciphertext
  - Why needed here: Classification relies on detecting subtle statistical biases that differ between ciphers
  - Quick check question: Can you explain why digram frequencies might be more informative than monogram frequencies for distinguishing polyalphabetic ciphers?

- Concept: Supervised machine learning classification
  - Why needed here: The task is framed as a multi-class classification problem where each cipher type is a class
  - Quick check question: What is the key difference between how an SVM and a Random Forest might approach the same classification task?

- Concept: Experimental design with multiple scenarios
  - Why needed here: The paper tests classification under different conditions to understand robustness
  - Quick check question: Why might fixed keys with random plaintext be more challenging for classification than random keys with fixed plaintext?

## Architecture Onboarding

- Component map: Data generation -> Feature extraction -> Model training -> Evaluation
- Critical path: 1) Generate balanced dataset of 1000 ciphertexts per cipher (1000 characters each), 2) Extract three feature types from each ciphertext, 3) Split data 80/20 for training/testing, 4) Train all 7 models on all 3 feature types (21 models total), 5) Evaluate performance across 4 experimental scenarios
- Design tradeoffs: Histograms capture global distribution but miss sequential patterns; sequences capture all information but require more data; simpler models perform surprisingly well vs. complex deep learning models
- Failure signatures: Overfitting (high training accuracy but low test accuracy), underfitting (consistently low accuracy), scenario sensitivity (large performance drops between conditions)
- First 3 experiments: 1) Train SVM with histogram features on fixed-fixed scenario, 2) Train MLP with letter sequence features on random-random scenario, 3) Compare k-NN with digram features across all four scenarios

## Open Questions the Paper Calls Out

- How do pre-trained CNN models like VGG19 or ResNet perform on WWII cipher classification compared to the basic CNNs tested?
- How does the LSTM model performance change with increased training time and hyperparameter tuning?
- How sensitive are the letter sequence feature models to hyperparameter tuning for shorter ciphertext lengths?

## Limitations
- Experiments use simulated ciphertext rather than real intercepted messages, missing real-world noise and transmission errors
- Fixed 1000-character length assumption may not reflect operational conditions with shorter messages
- Classification distinction doesn't necessarily imply practical cryptanalytic vulnerability

## Confidence

- **High Confidence**: Classic ML models perform as well as deep learning models given systematic statistical differences between cipher outputs
- **Medium Confidence**: Statistical biases remain detectable across different key/plaintext scenarios without full exploration of robustness boundaries
- **Medium Confidence**: Interpretation that certain ciphers are harder to distinguish due to design similarity requires deeper cryptanalytic analysis

## Next Checks

1. **Robustness Testing**: Replicate experiments with varying ciphertext lengths (100-500 characters) and transmission errors to assess classification degradation patterns and determine minimum viable lengths

2. **Real-World Data Validation**: Apply trained models to actual intercepted WWII ciphertexts or add realistic noise patterns to simulated data to verify performance under operational conditions

3. **Feature Importance Analysis**: Conduct ablation studies to determine which statistical features contribute most to distinguishing each cipher pair and validate against known cipher design characteristics