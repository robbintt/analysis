---
ver: rpa2
title: Mitigating the Learning Bias towards Repetition by Self-Contrastive Training
  for Open-Ended Generation
arxiv_id: '2307.01542'
source_url: https://arxiv.org/abs/2307.01542
tags:
- bill
- building
- film
- television
- boulter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Language models trained with maximum likelihood estimation tend
  to produce repetitive outputs due to a learning bias toward simple repetitive patterns.
  This paper proposes self-contrastive training, which penalizes the output of a premature
  checkpoint when it incorrectly predicts repetition.
---

# Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation

## Quick Facts
- **arXiv ID:** 2307.01542
- **Source URL:** https://arxiv.org/abs/2307.01542
- **Reference count:** 29
- **Primary result:** Self-contrastive training reduces repetition in language model outputs while maintaining fluency

## Executive Summary
This paper addresses the problem of repetitive text generation in language models trained with maximum likelihood estimation (MLE). The authors identify a learning bias where models preferentially learn simple repetitive patterns early in training, leading to overestimation of repetition probabilities. They propose self-contrastive training, which penalizes outputs that a premature checkpoint of the same model would incorrectly predict as repetitive. Experiments on Wikitext-103 and WritingPrompts datasets show that this approach effectively reduces repetition while maintaining fluency and coherence.

## Method Summary
The method involves training a GPT2 model normally for one epoch to create a premature checkpoint, then continuing training with a modified loss function that includes a contrastive penalty. This penalty is computed by comparing the current model's output distribution with that of the premature checkpoint, specifically penalizing tokens that the premature model would incorrectly predict as repetitive. The approach aims to discourage the model from falling into repetitive patterns while preserving its ability to generate fluent, coherent text.

## Key Results
- Self-contrastive training significantly reduces token repetition ratios (R-16, R-32, R-128) compared to MLE baseline
- The method maintains comparable perplexity and MAUVE scores, indicating preserved fluency and coherence
- Models trained with self-contrastive training show better performance on both Wikitext-103 and WritingPrompts datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LMs learn repetitive patterns faster than non-repetitive ones during training
- Mechanism: The MLE loss causes LMs to capture low-frequency (simple) patterns first, which include repetitive patterns
- Core assumption: Repetitive patterns are lower-frequency components that are easier to learn
- Evidence anchors: [abstract] "We attribute their overestimation of token-level repetition probabilities to the learning bias: LMs capture simple repetitive patterns faster with the MLE loss"; [section 3.2] "We reveal the learning bias towards repetition: LMs capture simple repetitive patterns first, which dominate the output distribution throughout the input space, and then learn more non-repetitive patterns during training"

### Mechanism 2
- Claim: Self-contrastive training penalizes outputs that a premature checkpoint would incorrectly predict as repetitive
- Mechanism: By contrasting current model output with premature checkpoint output, the method identifies and downweights tokens that appear repetitive to the premature model
- Core assumption: Premature checkpoints have learned repetitive patterns that the current model should avoid
- Evidence anchors: [abstract] "We propose self-contrastive training to penalize the output of a premature checkpoint of the same model when it incorrectly predicts repetition"; [section 4] Formal description of the self-contrastive training algorithm with equations showing how premature checkpoint output is used for penalty

### Mechanism 3
- Claim: LMs use longer-range dependencies to predict repetitive tokens than non-repetitive ones
- Mechanism: The modeling of longer-range dependencies allows LMs to generate repetition loops when they cannot maintain non-repetitive patterns over long ranges
- Core assumption: The inability to model long-range non-repetitive patterns causes repetition loops
- Evidence anchors: [abstract] "we also find that LMs use longer-range dependencies to predict repetitive tokens than non-repetitive ones"; [section 3.3] "The LM learns and utilizes longer-range dependencies to predict repetitive tokens than non-repetitive ones" with empirical evidence from perplexity scores

## Foundational Learning

- **Concept: Learning bias in neural networks**
  - Why needed here: Understanding why LMs preferentially learn repetitive patterns
  - Quick check question: What property of neural networks causes them to learn low-frequency components faster?

- **Concept: Contrastive learning framework**
  - Why needed here: The proposed method uses contrastive approach between current and premature model
  - Quick check question: How does contrastive learning differ from standard supervised learning?

- **Concept: Long-range dependency modeling**
  - Why needed here: Explains why LMs fall into repetition loops and what architectural improvements might help
  - Quick check question: What architectural features help models capture longer-range dependencies?

## Architecture Onboarding

- **Component map:** Base language model (GPT2) -> Premature checkpoint storage and retrieval -> Contrastive penalty computation module -> Training loop with modified loss function

- **Critical path:** 1) Initial training to create premature checkpoint; 2) Loading premature checkpoint and initializing current model; 3) During training: compute current model output, premature model output, calculate contrastive penalty; 4) Backpropagate combined loss to update model parameters

- **Design tradeoffs:** Checkpoint selection timing vs. penalty effectiveness; Penalty magnitude (位) vs. learning stability; Model capacity vs. ability to learn non-repetitive patterns

- **Failure signatures:** Too high 位: Model fails to learn coherent patterns; Too low 位: Minimal impact on repetition reduction; Poor checkpoint selection: Contrastive signal becomes meaningless

- **First 3 experiments:** 1) Ablation study: Compare with and without contrastive penalty on same dataset; 2) 位 sweep: Test different penalty magnitudes on validation set; 3) Checkpoint timing: Compare different premature checkpoint selection points

## Open Questions the Paper Calls Out

- What specific aspects of the Transformer architecture or MLE loss contribute to the learning bias towards repetitive patterns?
- How does the self-contrastive training method perform on extra-large language models like GPT-3 compared to GPT-2?
- What is the impact of self-contrastive training on the long-range dependencies learned by language models?

## Limitations

- The core mechanism of learning bias toward repetition lacks direct quantitative evidence across different model architectures
- The relationship between long-range dependencies and repetition generation is under-constrained and doesn't establish causation
- Method's effectiveness on domains beyond the two tested datasets remains unverified

## Confidence

**High Confidence**: The empirical observation that MLE-trained LMs generate repetitive text, and that the proposed self-contrastive training method reduces repetition while maintaining fluency on the tested datasets.

**Medium Confidence**: The proposed explanation for why repetition occurs (learning bias toward simple patterns) and how self-contrastive training addresses it.

**Low Confidence**: The claim about long-range dependencies being specifically used for repetitive token prediction, and broader generalization to different model architectures, scales, and domains.

## Next Checks

1. Ablation study on checkpoint timing: Systematically vary the timing of premature checkpoint selection to determine optimal point for contrastive penalty application

2. Architecture sensitivity analysis: Test self-contrastive training across different model architectures (LSTMs, Transformers with varying depths) to determine architecture-dependence

3. Cross-domain generalization test: Apply method to diverse text domains including technical writing, dialogue, and code generation to assess generalizability beyond current focus datasets