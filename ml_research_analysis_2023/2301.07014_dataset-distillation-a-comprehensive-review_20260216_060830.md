---
ver: rpa2
title: 'Dataset Distillation: A Comprehensive Review'
arxiv_id: '2301.07014'
source_url: https://arxiv.org/abs/2301.07014
tags:
- dataset
- synthetic
- data
- distillation
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dataset distillation aims to create a small synthetic dataset from
  a large original one such that models trained on the synthetic data perform similarly
  to those trained on the full dataset. This paper comprehensively reviews recent
  advances in this area, categorizing methods into performance matching, parameter
  matching, and distribution matching.
---

# Dataset Distillation: A Comprehensive Review

## Quick Facts
- arXiv ID: 2301.07014
- Source URL: https://arxiv.org/abs/2301.07014
- Reference count: 40
- Key outcome: Dataset distillation creates small synthetic datasets that enable models to perform similarly to those trained on full datasets, with state-of-the-art methods like FRePo and MTT achieving strong results on standard benchmarks.

## Executive Summary
Dataset distillation (DD) aims to create small synthetic datasets that preserve the essential information of large original datasets, enabling models trained on the synthetic data to achieve comparable performance to those trained on the full dataset. This comprehensive review categorizes DD methods into three main approaches: performance matching (using meta-learning to optimize synthetic data based on downstream model performance), parameter matching (aligning model parameters trained on synthetic and real data), and distribution matching (directly optimizing the synthetic data distribution). The paper explores various synthetic data parameterization techniques, including differentiable augmentation and latent vector-based generation, to improve data efficiency. Applications in continual learning, federated learning, and privacy are discussed, along with experimental comparisons showing state-of-the-art performance of methods like FRePo and MTT, while highlighting challenges in scaling to larger datasets and improving cross-architecture generalization.

## Method Summary
Dataset distillation methods optimize synthetic datasets to approximate the performance of models trained on original datasets. The paper categorizes these methods into three main approaches: performance matching (meta-learning based and KRR based), parameter matching (single-step and multi-step), and distribution matching. Training involves an iterative loop of updating neural networks and synthetic datasets alternately, with various parameterization techniques like differentiable siamese augmentation (DSA), generators/latent vectors, and upsampling. Methods are evaluated on image classification tasks using datasets like MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, and Tiny-ImageNet, with synthetic datasets typically containing 1, 10, or 50 images per class. The primary metric is classification accuracy on the original test set when models are trained on synthetic datasets.

## Key Results
- Dataset distillation methods like FRePo and MTT achieve state-of-the-art performance on standard benchmarks, with some methods reaching over 80% accuracy on CIFAR-10 using only 1 image per class
- Performance matching methods generally outperform parameter matching and distribution matching approaches, though they require more computational resources
- Synthetic data parameterization techniques like DSA and latent vectors can improve data efficiency and generalizability, though scaling to larger datasets remains challenging
- Most methods struggle with cross-architecture generalization, particularly when evaluation models use different normalization layers than training models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dataset distillation works by learning synthetic data that minimizes a specific loss function (L) which measures how well models trained on the synthetic dataset (S) perform compared to those trained on the original dataset (T).
- Mechanism: The paper formalizes dataset distillation as an optimization problem: S = arg min_S L(S,T), where L is computed using models trained on both synthetic and real data. The synthetic dataset is updated iteratively to minimize this loss.
- Core assumption: The loss function L effectively captures the information needed for models to perform well, and that synthetic data can be optimized to minimize this loss.
- Evidence anchors:
  - [abstract] "Dataset distillation (DD), also known as dataset condensation (DC), was introduced and has recently attracted much research attention in the community. Given an original dataset, DD aims to derive a much smaller dataset containing synthetic samples, based on which the trained models yield performance comparable with those trained on the original dataset."
  - [section] "Formally, we formulate the problem as the following: S = arg min_S L(S,T), (1) where L is some objective for dataset distillation, which will be elaborated in following contents."
- Break condition: If the loss function L does not adequately capture the necessary information, or if the optimization process gets stuck in local minima that do not generalize.

### Mechanism 2
- Claim: Different optimization objectives (performance matching, parameter matching, distribution matching) can be used to guide the learning of synthetic data, each with different strengths and weaknesses.
- Mechanism: The paper categorizes DD methods into three main types based on their optimization objectives. Performance matching uses meta-learning to optimize synthetic data based on downstream model performance, parameter matching aligns model parameters trained on synthetic and real data, and distribution matching directly optimizes the synthetic data distribution to match the real data.
- Core assumption: These different optimization objectives can effectively capture different aspects of the original dataset's information and lead to synthetic data that generalizes well.
- Evidence anchors:
  - [section] "To obtain synthetic datasets that are valid to replace original ones in downstream training, different works in DD propose different optimization objectives, denoted as L in Eq. 1. This part will introduce three mainstream solutions: performance matching, parameter matching, and distribution matching."
  - [section] "We will also reveal the relationships between them."
- Break condition: If one of these optimization objectives is fundamentally flawed or does not lead to synthetic data that generalizes well, or if the relationships between them are not as described.

### Mechanism 3
- Claim: Synthetic data parameterization techniques, such as differentiable augmentation and latent vector-based generation, can improve the data efficiency and generalizability of synthetic datasets.
- Mechanism: The paper discusses various ways to parameterize synthetic data, including using differentiable augmentation policies (DSA) and generative models with latent vectors. These techniques aim to compress more information into the synthetic data while preserving performance.
- Core assumption: These parameterization techniques can effectively capture more information from the original dataset in a compressed form, leading to better performance with smaller synthetic datasets.
- Evidence anchors:
  - [section] "One of the essential goals of dataset distillation is to synthesize informative datasets to improve training efficiency, given a limited storage budget. In other words, as for the same limited storage, more information on the original dataset is expected to be preserved so that the model trained on condensed datasets can achieve comparable and satisfactory performance."
  - [section] "In a general form, for a synthetic dataset, some codes z∈Z ⊂ RD', Z ={(zj,yj)}||Z|j=1 in a format other than the raw shape are used for storage. And there is some function gφ : RD'→ RD parameterized by φ that maps a code withd' dimensions to the format of raw images for downstream training."
- Break condition: If these parameterization techniques do not actually lead to more informative synthetic data, or if they introduce artifacts that hurt performance.

## Foundational Learning

- Concept: Meta-learning and bi-level optimization
  - Why needed here: Performance matching methods in dataset distillation use meta-learning to optimize synthetic data based on the performance of models trained on it. This involves a bi-level optimization problem where the inner loop trains a model on the synthetic data and the outer loop updates the synthetic data based on the model's performance on the real data.
  - Quick check question: How does the bi-level optimization in performance matching differ from standard single-level optimization, and what are the computational implications?

- Concept: Kernel Ridge Regression (KRR) and Neural Tangent Kernel (NTK)
  - Why needed here: Some dataset distillation methods use KRR with NTK to avoid the computationally expensive bi-level optimization in performance matching. KRR provides a closed-form solution for the linear model, and NTK approximates the training of wide neural networks.
  - Quick check question: How does using KRR with NTK in dataset distillation differ from standard neural network training, and what are the trade-offs in terms of performance and efficiency?

- Concept: Gradient matching and distribution matching
  - Why needed here: Parameter matching methods use gradient matching to align the gradients of models trained on synthetic and real data, while distribution matching methods directly optimize the synthetic data distribution to match the real data. Understanding these concepts is crucial for understanding the different approaches to dataset distillation.
  - Quick check question: How do gradient matching and distribution matching differ in terms of what they are optimizing and how they measure the similarity between synthetic and real data?

## Architecture Onboarding

- Component map:
  Original dataset (T) -> Synthetic dataset (S) -> Model (θ) -> Loss function (L) -> Optimization algorithm

- Critical path:
  1. Initialize the synthetic dataset (S) with random noise or samples from the original dataset (T).
  2. For each iteration:
     a. Sample a model (θ) from a distribution (e.g., random initialization or cached checkpoints).
     b. Train the model (θ) on the synthetic dataset (S) for some steps.
     c. Compute the loss (L) between the model trained on S and the model trained on T.
     d. Update the synthetic dataset (S) to minimize the loss (L) using gradient descent.

- Design tradeoffs:
  - Performance vs. efficiency: Different optimization objectives (performance matching, parameter matching, distribution matching) have different trade-offs in terms of performance and computational efficiency.
  - Compression ratio vs. performance: Higher compression ratios (smaller synthetic datasets) may lead to lower performance due to information loss.
  - Generalization vs. overfitting: Synthetic datasets need to generalize well to unseen data, but they are prone to overfitting due to their small size.

- Failure signatures:
  - Poor performance: The model trained on the synthetic dataset performs significantly worse than the model trained on the original dataset.
  - Overfitting: The model trained on the synthetic dataset performs well on the training data but poorly on unseen data.
  - Instability: The optimization process is unstable and does not converge to a good solution.

- First 3 experiments:
  1. Implement a simple dataset distillation method using performance matching on a small dataset (e.g., MNIST) and evaluate its performance.
  2. Compare the performance of different optimization objectives (performance matching, parameter matching, distribution matching) on a larger dataset (e.g., CIFAR-10).
  3. Experiment with different synthetic data parameterization techniques (e.g., differentiable augmentation, latent vectors) and evaluate their impact on performance and efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can dataset distillation methods effectively scale to very large compression ratios (e.g., 1000+ images per class) while maintaining high performance?
- Basis in paper: [explicit] The paper discusses challenges with scaling up, noting that performance improvements plateau as IPC increases, and selection-based methods can outperform DD methods at very high compression ratios.
- Why unresolved: Current methods show diminishing returns with increasing IPC, and the paper identifies this as a key challenge without providing solutions.
- What evidence would resolve it: Experiments demonstrating consistent performance improvements with IPC > 1000, or theoretical analysis explaining why scaling beyond certain IPC is fundamentally limited.

### Open Question 2
- Question: How can dataset distillation methods be made more generalizable across different neural network architectures, particularly those with different normalization layers?
- Basis in paper: [explicit] The paper shows that most methods struggle with cross-architecture generalization, especially when the evaluation model uses different normalization layers than the training model.
- Why unresolved: Current methods often encode architectural biases that limit transferability, and while some approaches like DM and IDC show promise, they have limitations in performance or efficiency.
- What evidence would resolve it: Development of a DD method that achieves comparable performance across diverse architectures (CNNs, ResNets, Transformers) with varying normalization schemes, or theoretical understanding of the architectural dependencies in current methods.

### Open Question 3
- Question: What are the fundamental limits of dataset distillation in terms of information preservation, and can these limits be quantified or surpassed?
- Basis in paper: [inferred] The paper discusses the challenge of scaling up and the plateauing performance with increasing IPC, suggesting there may be fundamental limits to how much information can be preserved in synthetic datasets.
- Why unresolved: Current methods may be approaching theoretical limits, but there's no analysis of what these limits are or whether they can be exceeded through new techniques or theoretical insights.
- What evidence would resolve it: Mathematical proofs establishing information-theoretic bounds on dataset distillation, or experimental demonstrations of synthetic datasets that exceed current performance limits on standard benchmarks.

## Limitations

- The empirical comparison between different dataset distillation approaches is limited, making it difficult to definitively assess their relative strengths and weaknesses
- Most existing methods struggle with scaling to larger datasets and maintaining cross-architecture generalization, with limited quantitative analysis of these limitations
- The discussion of applications (continual learning, federated learning, privacy) remains largely theoretical without extensive empirical validation

## Confidence

- **High Confidence:** The categorization of dataset distillation methods into three main types (performance matching, parameter matching, distribution matching) is well-established in the literature and consistently presented across multiple sources.
- **Medium Confidence:** The claims about state-of-the-art performance of specific methods like FRePo and MTT are supported by recent experimental results, though comparisons may vary depending on implementation details and hyperparameter settings.
- **Low Confidence:** The theoretical benefits of synthetic data parameterization techniques (DSA, latent vectors) for improving data efficiency are proposed but require more rigorous empirical validation across diverse scenarios.

## Next Checks

1. **Cross-Architecture Generalization:** Systematically test whether synthetic datasets created using one architecture (e.g., ResNet) maintain performance when training different architectures (e.g., VGG, MobileNet), quantifying the generalization gap.
2. **Scalability Analysis:** Evaluate the performance degradation of state-of-the-art methods when scaling from small datasets (CIFAR-10) to larger ones (ImageNet), measuring both accuracy drop and computational overhead.
3. **Parameter Sensitivity:** Conduct ablation studies on key hyperparameters (learning rate, batch size, number of synthetic samples) for each method category to establish robust parameter ranges and identify failure modes.