---
ver: rpa2
title: Training normalizing flows with computationally intensive target probability
  distributions
arxiv_id: '2308.13294'
source_url: https://arxiv.org/abs/2308.13294
tags:
- gradient
- estimator
- loss
- function
- lattice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training normalizing flows
  for Monte Carlo simulations in lattice field theories, where the target probability
  distribution is computationally intensive. The common gradient estimator, the "reparametrization
  trick," requires calculating the derivative of the action with respect to the fields,
  which can be computationally expensive for complex actions like fermionic actions
  in QCD.
---

# Training normalizing flows with computationally intensive target probability distributions

## Quick Facts
- arXiv ID: 2308.13294
- Source URL: https://arxiv.org/abs/2308.13294
- Reference count: 32
- Key outcome: REINFORCE estimator for normalizing flows is up to 10x faster and uses 30% less memory than reparameterization trick when training on computationally intensive target distributions like fermionic actions in lattice field theories

## Executive Summary
This paper addresses the challenge of training normalizing flows when the target probability distribution is computationally intensive to evaluate, specifically in lattice field theories. The authors identify that the commonly used reparameterization trick gradient estimator requires expensive backpropagation through the action (such as fermionic determinants in QCD), which becomes prohibitive for complex actions. They propose using the REINFORCE algorithm as an alternative gradient estimator that avoids this computational bottleneck while maintaining or improving training performance.

The method is demonstrated on the two-dimensional Schwinger model with Wilson fermions at criticality, showing significant computational advantages. The REINFORCE estimator achieves up to 10 times speedup and 30% memory reduction compared to the reparameterization trick. Additionally, it provides better numerical stability, enabling single precision calculations and utilization of half-float tensor cores. The authors present theoretical analysis showing that the REINFORCE estimator has zero variance for perfectly trained models, unlike the reparameterization trick.

## Method Summary
The paper proposes using the REINFORCE algorithm as a gradient estimator for training normalizing flows when the target distribution is computationally intensive. In lattice field theories, the probability distribution is given by the action P(ϕ) ∝ exp(-S(ϕ)), and the reparameterization trick requires computing gradients of this action with respect to the fields ϕ. For fermionic actions involving determinants of large matrices, this becomes computationally prohibitive. The REINFORCE estimator avoids this by using score function gradients that only require evaluating the log probability, not its derivatives. The method is implemented using PyTorch with automatic mixed precision support and tested on the Schwinger model with gauge-equivariant coupling layers and circular splines.

## Key Results
- REINFORCE estimator achieves up to 10x speedup compared to reparameterization trick for training normalizing flows on computationally intensive distributions
- Memory usage reduced by up to 30% due to avoiding storage of intermediate gradient computations
- Numerical stability allows single precision calculations and use of half-float tensor cores
- For perfectly trained models, REINFORCE estimator variance vanishes (var = 0) while reparameterization trick variance does not
- Effective Sample Size (ESS) and acceptance rates comparable to or better than reparameterization trick

## Why This Works (Mechanism)

### Mechanism 1
- Claim: REINFORCE avoids expensive backpropagation through fermionic determinants
- Mechanism: The REINFORCE estimator computes gradients without needing to differentiate the action (e.g., fermionic determinant), which is computationally intensive and memory-heavy
- Core assumption: The determinant of the Dirac operator is expensive to differentiate due to its large matrix size and sparsity pattern
- Evidence anchors: [abstract] "The common loss function's gradient estimator based on the 'reparametrization trick' requires the calculation of the derivative of the action with respect to the fields"; [section] "While conceptually simple, this estimator has a considerable drawback as it requires calculating the gradient of the distribution P(ϕ) with respect to the configuration ϕ"
- Break condition: If the action gradient is cheap to compute or if an alternative estimator avoids determinant differentiation more efficiently

### Mechanism 2
- Claim: REINFORCE has a smaller variance for perfectly trained models
- Mechanism: For q(ϕ|θ) = p(ϕ), the variance of REINFORCE vanishes, whereas the reparameterization trick estimator does not
- Core assumption: When the model perfectly matches the target, the REINFORCE estimator becomes deterministic
- Evidence anchors: [abstract] "Although not much can be said about the variances of these estimators in general, we can show that for perfectly trained model i.e. when q(ϕ|θ) = p(ϕ) vargRE[{ϕ}]q(ϕ|θ)=p(ϕ) = 0"; [section] "As for the estimator grt, we cannot make any claims as to the value of its variance but in Ref. [10] we showed that it does not need to vanish even for q(ϕ|θ) = p(ϕ)"
- Break condition: If the model never reaches perfect training or if the variance reduction does not translate to practical training stability

### Mechanism 3
- Claim: REINFORCE allows single-precision and half-precision computations due to better numerical stability
- Mechanism: Avoiding backpropagation through large determinant computations reduces numerical instability, enabling lower precision calculations
- Core assumption: Determinant differentiation introduces large dynamic range requiring higher precision
- Evidence anchors: [abstract] "It is also more numerically stable allowing for single precision calculations and the use of half-float tensor cores"; [section] "The training using r.t. was finished at 40k gradient steps, because no further improvement was observed during the training. We did all our calculations in single precision. We did not encounter any problems while training using the REINFORCE estimator"
- Break condition: If numerical stability issues arise from other parts of the model or if mixed precision fails for other reasons

## Foundational Learning

- Concept: Gradient estimation in normalizing flows
  - Why needed here: The paper compares two gradient estimators (REINFORCE vs. reparameterization trick) for training normalizing flows on complex target distributions
  - Quick check question: What is the main computational difference between REINFORCE and the reparameterization trick in terms of gradient computation?

- Concept: Monte Carlo sampling and autocorrelation
  - Why needed here: The paper discusses autocorrelation times in Markov Chain Monte Carlo and how normalizing flows can reduce them
  - Quick check question: How does the effective sample size (ESS) relate to autocorrelation time in MCMC sampling?

- Concept: Lattice field theory and fermionic actions
  - Why needed here: The paper applies the proposed method to the Schwinger model with Wilson fermions, where the fermionic action involves computing a determinant
  - Quick check question: Why is the determinant of the Dirac operator computationally intensive in lattice QCD simulations?

## Architecture Onboarding

- Component map: Normalizing flow layers -> Gauge-equivariant couplings -> Plaquette coupling layers with circular splines -> Action computation (pure gauge + fermionic determinant) -> Two gradient estimators (REINFORCE and reparameterization trick) -> Training loop with AMP

- Critical path:
  1. Sample from prior distribution
  2. Apply normalizing flow transformation
  3. Compute log probability and action
  4. Apply gradient estimator (REINFORCE or reparameterization trick)
  5. Backpropagate through network only (not through action)
  6. Update network parameters with optimizer

- Design tradeoffs: REINFORCE trades off higher variance for lower computational cost and memory usage; reparameterization trick has lower variance but requires expensive gradient computation of the action

- Failure signatures: REINFORCE - high gradient variance leading to unstable training; reparameterization trick - memory overflow or numerical instability when differentiating complex actions

- Three first experiments:
  1. Compare training speed and memory usage of REINFORCE vs reparameterization trick on a simple Gaussian target
  2. Measure gradient variance of both estimators as a function of training progress
  3. Test numerical stability limits by gradually reducing precision for both estimators

## Open Questions the Paper Calls Out
The paper acknowledges several open questions for future work. It mentions that a comparison with other gradient estimators, notably path gradients, is needed to fully understand the landscape of available methods. The authors also note that the training efficiency of the REINFORCE estimator may depend on the actual physical parameters of the model and the hyperparameters used in training, suggesting this should be explored systematically. Additionally, while the method is demonstrated for the Schwinger model, its effectiveness for other lattice field theories with different fermionic actions remains to be investigated.

## Limitations
- Performance benefits are demonstrated only for the 2D Schwinger model with Wilson fermions and may not generalize to all computationally intensive distributions
- Variance analysis is limited to the perfectly trained case, with limited discussion of variance behavior during training
- Claims about numerical stability benefits are based on single precision tests without exploring other precision regimes systematically
- Comparison focuses primarily on computational metrics rather than physical observable accuracy or sample quality beyond ESS

## Confidence

- **High confidence**: The computational cost reduction claims (10x speedup, 30% memory reduction) are well-supported by the experimental results on the Schwinger model
- **Medium confidence**: The numerical stability benefits allowing single precision calculations are demonstrated but not extensively tested across different scenarios
- **Low confidence**: The claim that these benefits will appear "in each case where the target probability distribution is computationally intensive" is speculative and not supported by broader experimental evidence

## Next Checks

1. Test the REINFORCE estimator on other lattice field theories with different fermionic actions (e.g., staggered fermions, clover fermions) to verify generalization beyond Wilson fermions
2. Conduct a systematic study of precision requirements across different lattice sizes and coupling strengths to quantify the numerical stability benefits more precisely
3. Compare the sample quality (beyond ESS) using other metrics like integrated autocorrelation time, topological susceptibility, or physical observable measurements to ensure the speed gains don't compromise physical accuracy