---
ver: rpa2
title: 'DCLP: Neural Architecture Predictor with Curriculum Contrastive Learning'
arxiv_id: '2302.13020'
source_url: https://arxiv.org/abs/2302.13020
tags:
- data
- neural
- learning
- training
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DCLP, a neural architecture predictor with
  curriculum contrastive learning for neural architecture search (NAS). The key idea
  is to use curriculum-guided contrastive learning to effectively train the predictor
  using fewer labeled architectures.
---

# DCLP: Neural Architecture Predictor with Curriculum Contrastive Learning

## Quick Facts
- **arXiv ID**: 2302.13020
- **Source URL**: https://arxiv.org/abs/2302.13020
- **Reference count**: 34
- **Primary result**: DCLP achieves high accuracy and efficiency in neural architecture search by using curriculum-guided contrastive learning to train predictors with fewer labeled architectures.

## Executive Summary
This paper introduces DCLP, a neural architecture predictor that leverages curriculum-guided contrastive learning to effectively train on limited labeled architectures. The key innovation is a curriculum that ranks and orders training data by difficulty, enabling the contrastive learner to incrementally learn feature representations on a smooth learning curve. Experiments demonstrate that DCLP achieves superior accuracy and efficiency compared to existing predictors across multiple NAS benchmarks, while also showing promise in discovering high-performing architectures when combined with search strategies.

## Method Summary
DCLP combines curriculum learning with contrastive learning to pre-train a GIN-based encoder on unlabeled neural architectures. The method uses edge perturbation and attribute masking augmentations to create positive pairs for contrastive learning, with a training scheduler that interleaves easy and hard samples based on difficulty measurement. After pre-training, the model is fine-tuned on a small labeled dataset using ranking-based loss (ListMLE) or normalized MSE. The final predictor is integrated with search strategies for efficient neural architecture search, reducing dependency on large labeled training sets.

## Key Results
- DCLP achieves high Kendall's Tau correlation on NAS-Bench-101, NAS-Bench-201, and DARTS search spaces with limited labeled data
- Ranking-based fine-tuning (ListMLE) outperforms regression-based approaches in identifying top-performing architectures
- The curriculum learning strategy leads to faster convergence and better performance compared to random or easy-to-hard training orders
- DCLP successfully discovers superior architectures when combined with search strategies, demonstrating practical utility

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Curriculum-guided contrastive learning enables the predictor to learn transferable features from unlabeled architectures, reducing dependency on labeled training data.
- **Mechanism**: The framework ranks training data by difficulty using a curriculum sampling strategy and feeds it to the contrastive learner incrementally. This stabilizes the training distribution and allows the encoder to learn meaningful representations without overfitting to limited labeled samples.
- **Core assumption**: Architectural features learned from structurally similar but untrained samples (via augmentation) are transferable to the predictor's downstream task.
- **Evidence anchors**:
  - [abstract] "Our method simplifies the contrastive task by designing a novel curriculum to enhance the stability of unlabeled training data distribution during contrastive training."
  - [section 3.4] "The training scheduler uses the difficulty of data to plan the training order. First, we create data with various difficulties by different augmentations..."
- **Break condition**: If the augmentation distribution does not correlate with architectural performance or the difficulty measurement is inaccurate, the curriculum strategy may fail.

### Mechanism 2
- **Claim**: Ranking-based fine-tuning improves the predictor's sensitivity to high-performing architectures compared to regression-based targets.
- **Mechanism**: Instead of using mean squared error, the fine-tuning step optimizes ranking metrics like ListMLE, aligning the training objective with the actual NAS task requirement of identifying top architectures.
- **Core assumption**: In NAS, the absolute performance values are less important than the relative ranking of architectures; thus, optimizing for ranking is more effective.
- **Evidence anchors**:
  - [section 3.5] "Since the predictor can be treated as a regression model, the direct idea is to use the mean squared error (MSE) between the predicted and actual result as the loss function. However, NAS is to determine some of the best architectures and does not care about the absolute value of performance."
  - [section 3.5] "Using the optimization target of ranking, the optimization goal and the downstream task are aligned..."
- **Break condition**: If the performance distribution is too narrow or highly skewed, ranking-based optimization may lose discriminative power.

### Mechanism 3
- **Claim**: Edge perturbation and attribute masking augmentations preserve meaningful architectural information while generating difficult contrastive pairs.
- **Mechanism**: Unlike subgraph sampling, these augmentations maintain the graph structure and properties relevant to neural network performance, enabling effective contrastive learning.
- **Core assumption**: Small structural changes (edge additions/removals, attribute modifications) still preserve the core computational graph semantics, allowing the encoder to learn generalizable features.
- **Evidence anchors**:
  - [section 3.2] "For node dropping, in neural networks, an isolated operation with no nodes connected can be seen as removed from the network. Since a node is often not related to many edges, the deletion of a node could be treated as the deletion of all the edges with n as the vertex."
  - [section 3.2] "Therefore, we only use two augmentation strategies, i.e., edge perturbation and attribute masking, to construct the positive set for contrastive learning."
- **Break condition**: If augmentations create invalid or non-functional architectures, the learned representations may not transfer to real NAS performance.

## Foundational Learning

- **Concept**: Contrastive learning for graph data
  - **Why needed here**: Enables the predictor to extract meaningful features from unlabeled architectures by learning to distinguish similar/dissimilar pairs.
  - **Quick check question**: What loss function is used to train the encoder to bring positive pairs closer and push negative pairs apart?

- **Concept**: Curriculum learning
  - **Why needed here**: Controls the difficulty of training data to prevent overfitting and improve convergence when learning from limited labeled samples.
  - **Quick check question**: How does the scheduler determine the order of presenting augmented data to the contrastive learner?

- **Concept**: Graph neural networks for architecture encoding
  - **Why needed here**: Neural architectures can be represented as directed acyclic graphs, and GNNs like GIN can capture structural and functional relationships between nodes.
  - **Quick check question**: What type of GNN architecture is used to encode the DAG representation of neural architectures?

## Architecture Onboarding

- **Component map**: Architecture encoding → Graph augmentation (edge perturbation, attribute masking) → Difficulty measurement → Training scheduler → Pre-training with contrastive learning → Fine-tuning with ranking loss → Search strategy integration

- **Critical path**: Architecture encoding → Pre-training with curriculum contrastive learning → Fine-tuning with ranking loss → Search strategy integration

- **Design tradeoffs**:
  - Memory bank size vs. computational cost for negative samples
  - Difficulty measurement granularity vs. pre-training stability
  - Ranking loss vs. normalized MSE for fine-tuning accuracy

- **Failure signatures**:
  - Poor Kendall's Tau on test set → Curriculum difficulty misestimation or ineffective augmentations
  - High variance in predictions → Overfitting during fine-tuning or insufficient contrastive pre-training
  - Slow convergence → Inappropriate learning rate or curriculum schedule

- **First 3 experiments**:
  1. Train DCLP with only edge perturbation augmentation and evaluate Kendall's Tau vs. random training order baseline
  2. Compare ranking loss vs. normalized MSE fine-tuning on small labeled datasets
  3. Test memory bank size impact on contrastive learning performance and computational efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the curriculum learning approach in DCLP specifically impact the convergence speed and final accuracy of the predictor compared to other curriculum strategies like easy-to-hard or hard-to-easy?
- **Basis in paper**: [explicit] The paper mentions that DCLP uses a curriculum learning approach that mixes complex and simple data, leading to faster and better convergence. It also compares DCLP to other curriculum strategies like easy-to-hard and hard-to-easy, showing that DCLP performs better.
- **Why unresolved**: The paper does not provide a detailed comparison of convergence speed and final accuracy between DCLP and other curriculum strategies.
- **What evidence would resolve it**: Conducting experiments to compare the convergence speed and final accuracy of DCLP with other curriculum strategies on various NAS benchmarks.

### Open Question 2
- **Question**: What is the impact of using different augmentation strategies (edge perturbation, attribute masking, etc.) on the performance of DCLP?
- **Basis in paper**: [explicit] The paper mentions that different augmentation strategies can lead to various difficulties in contrastive tasks, and DCLP uses edge perturbation and attribute masking.
- **Why unresolved**: The paper does not explore the impact of using different augmentation strategies on the performance of DCLP.
- **What evidence would resolve it**: Conducting experiments to compare the performance of DCLP using different augmentation strategies on various NAS benchmarks.

### Open Question 3
- **Question**: How does the choice of ranking loss function (e.g., ListMLE) impact the performance of DCLP compared to other ranking loss functions?
- **Basis in paper**: [explicit] The paper mentions that DCLP uses ListMLE as the ranking loss function, which is based on the Plackett-Luce probability model.
- **Why unresolved**: The paper does not explore the impact of using different ranking loss functions on the performance of DCLP.
- **What evidence would resolve it**: Conducting experiments to compare the performance of DCLP using different ranking loss functions on various NAS benchmarks.

## Limitations

- The effectiveness of the curriculum learning strategy depends heavily on the accuracy of the difficulty measurement and scheduler configuration, which are not extensively validated
- The transferability claim across datasets is demonstrated but not thoroughly validated across diverse search spaces
- The memory bank size for contrastive learning is mentioned but not optimized or analyzed for its impact on performance

## Confidence

- **High**: The core mechanism of combining contrastive learning with curriculum learning for pre-training is well-supported by experimental results and theoretical justification
- **Medium**: The transferability claim across different NAS benchmarks is supported but could benefit from more extensive validation
- **Low**: The exact implementation details of the difficulty measurement and training scheduler are not fully specified, which could impact reproducibility

## Next Checks

1. **Curriculum Learning Sensitivity**: Conduct ablation studies varying the difficulty measurement method and scheduler parameters to assess the robustness of the curriculum learning strategy. Compare performance with random training order baselines.

2. **Transferability Across Search Spaces**: Test DCLP's performance when pre-trained on one NAS benchmark and fine-tuned on another, particularly across different search space types (e.g., cell-based vs. macro architectures). Evaluate the impact of dataset size on transferability.

3. **Augmentation Strategy Comparison**: Compare the proposed edge perturbation and attribute masking augmentations with alternative strategies like subgraph sampling or random node/edge deletions. Assess their impact on contrastive learning effectiveness and downstream predictor accuracy.