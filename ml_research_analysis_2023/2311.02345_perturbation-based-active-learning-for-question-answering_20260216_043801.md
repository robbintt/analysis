---
ver: rpa2
title: Perturbation-based Active Learning for Question Answering
arxiv_id: '2311.02345'
source_url: https://arxiv.org/abs/2311.02345
tags:
- arxiv
- learning
- preprint
- question
- answering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reducing annotation costs for
  building high-performance question answering (QA) models. It proposes a perturbation-based
  active learning (PAL) acquisition strategy that selects the most informative unlabeled
  training examples to label.
---

# Perturbation-based Active Learning for Question Answering

## Quick Facts
- **arXiv ID**: 2311.02345
- **Source URL**: https://arxiv.org/abs/2311.02345
- **Reference count**: 32
- **Primary result**: PAL achieves 72.7 average AUC score across annotation budgets, outperforming uncertainty sampling (71.3), clustering (70.5), and diversity-based sampling (70.0)

## Executive Summary
This paper addresses the problem of reducing annotation costs for building high-performance question answering models. It proposes a perturbation-based active learning (PAL) acquisition strategy that selects the most informative unlabeled training examples by measuring how much model predictions change when contexts are perturbed with distracting sentences. The method uses Kullback-Leibler divergence to quantify prediction changes and selects examples where divergence is highest. Experiments on SQuAD demonstrate that PAL outperforms standard active learning strategies across different annotation budgets.

## Method Summary
PAL works by first fine-tuning a BERT-base model on an initial 1% of labeled data, then iteratively selecting the most informative examples from the remaining pool. For each unlabeled question, the method finds similar labeled contexts using BERT embeddings, extracts sentences from these contexts, and uses the most similar sentence as a distractor. The model's predictions are compared before and after adding the distractor, and Kullback-Leibler divergence measures the change. Questions with the highest divergence scores are selected for annotation and added to the training set, with this process repeating until the desired annotation budget is reached.

## Key Results
- PAL achieves an average AUC score of 72.7 across different annotation budgets
- Outperforms uncertainty sampling (71.3 AUC), clustering (70.5 AUC), and diversity-based sampling (70.0 AUC)
- Shows consistent performance improvements across all annotation budget levels tested
- Demonstrates the effectiveness of combining input feature space and model output space information for active learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The perturbation-based approach identifies informative examples by measuring model sensitivity to context modifications.
- Mechanism: PAL perturbs unlabeled questions by adding distracting sentences from similar contexts, then selects questions where model predictive probabilities diverge most after perturbation, measured using Kullback-Leibler divergence.
- Core assumption: Questions causing significant prediction changes when distracting sentences are added are more informative for improving model robustness.
- Evidence anchors: PAL achieves 72.7 AUC vs 71.3 for uncertainty sampling; hypothesis that robust models should produce similar distributions after perturbation.
- Break condition: Distractors too similar to original context may not create meaningful divergence; too dissimilar may not test relevant robustness.

### Mechanism 2
- Claim: Similarity-based distractor selection improves perturbation quality.
- Mechanism: Finds most similar labeled question contexts using BERT embeddings, extracts sentences from these contexts, and uses the most similar sentence as distractor.
- Core assumption: Distractors from semantically similar contexts create more meaningful perturbations than random distractors.
- Evidence anchors: Uses [CLS] token embeddings to represent contexts and find similar sentences; no direct evidence in neighbor papers about similarity-based distractor selection.
- Break condition: Limited dataset diversity may cause similarity-based approach to select predictable distractors, reducing informativeness signal.

### Mechanism 3
- Claim: Combining input feature space and model output space information captures different dimensions of informativeness.
- Mechanism: Uses BERT embeddings (input feature space) to find similar questions and model predictive probabilities (output space) to measure divergence after perturbation.
- Core assumption: Uncertainty sampling and representation-based sampling are orthogonal approaches capturing different aspects of informativeness.
- Evidence anchors: Two approaches are orthogonal since uncertainty sampling is based on model output while representation exploits input information; hybrid approach utilizes both input features and model output.
- Break condition: If model embeddings don't capture relevant semantic differences between contexts, similarity-based distractor selection may fail to find appropriate perturbations.

## Foundational Learning

- **Active Learning and Acquisition Functions**: Why needed - builds on active learning methodology to reduce annotation costs by selecting most informative examples; Quick check - What is the difference between uncertainty sampling and diversity-based sampling in active learning?
- **Kullback-Leibler Divergence**: Why needed - uses KL divergence to measure difference between probability distributions before and after perturbation; Quick check - How does KL divergence differ from other distance metrics like Euclidean distance when comparing probability distributions?
- **Transformer-based Models and Fine-tuning**: Why needed - uses BERT as base model and fine-tunes it for QA task; Quick check - What is the difference between pre-training and fine-tuning in transformer models?

## Architecture Onboarding

- **Component map**: Tokenizer, offset mapping, answer span labeling -> BERT-base with QA head -> PAL acquisition strategy (similarity-based distractor + KL divergence scoring) -> Active learning loop (1% initial, 10% acquisition per iteration)
- **Critical path**: 1. Preprocess questions and contexts 2. Fine-tune BERT on current labeled set 3. For each unlabeled question: find similar contexts, extract distractor, generate perturbed question, compute KL divergence 4. Select top-k questions with lowest KL divergence scores 5. Annotate and add to labeled set
- **Design tradeoffs**: Perturbation quality vs computational cost; KL divergence vs other divergence measures; batch size selection balancing exploration and exploitation
- **Failure signatures**: Uniformly high/low KL divergence scores across examples; failure to find similar contexts due to limited dataset diversity; model predictions don't change significantly after perturbation; initial random selection performs as well as PAL
- **First 3 experiments**: 1. Run PAL with simplified distractor selection (random sentence) to measure impact of similarity-based approach 2. Compare symmetric vs asymmetric KL divergence and other divergence measures 3. Test different acquisition batch sizes (5%, 15%, 20%) to find optimal trade-off

## Open Questions the Paper Calls Out

- How does PAL performance compare to other active learning strategies when applied to different QA datasets beyond SQuAD? The paper only evaluates PAL on SQuAD, leaving generalizability to other datasets unexplored.
- How does the choice of distractor sentence affect PAL performance? The paper describes distractor selection process but doesn't explore impact of different selection methods on effectiveness.
- Can PAL be effectively combined with other active learning strategies to further improve performance? The paper mentions orthogonal approaches suggesting potential for combining with PAL, but doesn't explore hybrid approaches.

## Limitations
- Effectiveness relies heavily on quality of distractor selection, which is not extensively validated
- Similarity-based approach using BERT embeddings may not capture all relevant semantic relationships for questions requiring reasoning beyond surface-level similarity
- Assumes perturbations with similar contexts create meaningful robustness tests, which may not hold for all question types or domains

## Confidence
- High confidence in core claim that PAL outperforms standard active learning strategies on SQuAD (72.7 vs 71.3 AUC)
- Medium confidence in mechanism explanation - provides theoretical justification but limited empirical validation of why perturbation-based approach works
- Medium confidence in similarity-based distractor selection method - demonstrates use but doesn't compare against alternative strategies

## Next Checks
1. Conduct ablation studies comparing PAL with random distractor selection versus similarity-based distractor selection to quantify contribution of similarity component to performance gains
2. Test PAL on multiple QA datasets with different characteristics (TriviaQA, Natural Questions) to assess generalizability beyond SQuAD and identify dataset properties influencing effectiveness
3. Perform analysis on distribution of KL divergence scores across selected examples to verify PAL is selecting diverse and informative examples rather than clustering around similar question types