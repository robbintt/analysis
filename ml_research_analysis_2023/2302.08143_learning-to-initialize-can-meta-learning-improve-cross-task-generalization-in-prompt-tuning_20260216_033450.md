---
ver: rpa2
title: 'Learning to Initialize: Can Meta Learning Improve Cross-task Generalization
  in Prompt Tuning?'
arxiv_id: '2302.08143'
source_url: https://arxiv.org/abs/2302.08143
tags:
- learning
- tasks
- linguistics
- computational
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces meta prompt tuning (MPT), which learns to
  initialize prompt embeddings from source tasks to adapt to unseen target tasks.
  By systematically analyzing meta learning algorithms (MAML, FoMAML, Reptile) in
  various source/target task configurations on few-shot tasks, the study shows that
  MPT significantly improves cross-task generalization, particularly for classification
  tasks (+20% relative gain), though it does not consistently outperform multi-task
  learning on non-classification tasks.
---

# Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?

## Quick Facts
- arXiv ID: 2302.08143
- Source URL: https://arxiv.org/abs/2302.08143
- Reference count: 40
- Meta prompt tuning (MPT) improves cross-task generalization, particularly for classification tasks (+20% relative gain)

## Executive Summary
This paper introduces meta prompt tuning (MPT), a method that uses meta-learning to learn better initial prompt embeddings for few-shot learning tasks. MPT employs optimization-based meta-learning algorithms (MAML, FoMAML, Reptile) to learn initialization parameters from source tasks that can be quickly adapted to unseen target tasks. The study systematically evaluates MPT across different source/target task configurations and demonstrates significant improvements in cross-task generalization, particularly for classification tasks, while also showing robustness to model type and size.

## Method Summary
The paper proposes MPT as a two-stage learning approach. In the upstream stage, MPT uses meta-learning algorithms (MAML, FoMAML, or Reptile) to learn prompt embeddings from a set of source tasks by minimizing loss on query sets after gradient updates on support sets. In the downstream stage, these learned embeddings are used as initialization for prompt tuning on target tasks. The method is evaluated against standard prompt tuning and multi-task learning baselines across various task type configurations (classification vs non-classification) using the CROSSFIT few-shot task collection.

## Key Results
- MPT significantly improves cross-task generalization for classification tasks (+20% relative gain)
- MPT outperforms standard prompt tuning in most configurations and shows robustness to model type/size
- MPT effectiveness extends beyond few-shot settings, showing gains even with full datasets
- MPT does not consistently outperform multi-task learning on non-classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Meta prompt tuning (MPT) improves cross-task generalization by learning better initial prompt embeddings from source tasks.
- Mechanism: MPT uses optimization-based meta-learning algorithms (MAML, FoMAML, Reptile) to find prompt embeddings that adapt quickly to unseen target tasks. During meta-training, the model learns parameters that minimize loss on query sets after gradient updates on support sets across multiple source tasks.
- Core assumption: Source tasks share transferable structure with target tasks, allowing learned initialization to generalize.
- Evidence anchors:
  - [abstract]: "MPT significantly improves cross-task generalization, particularly for classification tasks (+20% relative gain)"
  - [section 4.3.1]: "We iterate over tasks in T src to update prompt embeddings φ for a fixed number of steps. The learned meta-parameters φ* is used in the meta-testing stage."
  - [corpus]: Weak evidence - corpus mentions transferable prompting frameworks but not the specific MPT mechanism described in the paper.
- Break condition: If source and target tasks have low task similarity, the learned initialization may not transfer effectively, as shown in Table 2 where MPT underperforms MTL on non-classification tasks.

### Mechanism 2
- Claim: MPT provides robustness to model type and size across different backbone language models.
- Mechanism: The learned prompt embeddings capture task-relevant information that is largely independent of the specific PLM architecture, allowing transfer across different model sizes and types.
- Core assumption: The task-specific patterns captured in prompt embeddings are more important than the underlying model architecture for few-shot performance.
- Evidence anchors:
  - [abstract]: "effectiveness of MPT... robustness to model type/size"
  - [section 6]: "The consistent gain of MPT across different models shows its robustness to model type and size"
  - [corpus]: Moderate evidence - mentions "Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning" which suggests similar robustness claims exist in related work.
- Break condition: If certain model architectures have fundamentally different representations that conflict with the learned prompt embeddings, performance may degrade.

### Mechanism 3
- Claim: MPT can improve performance even with full datasets, not just few-shot settings.
- Mechanism: The meta-learned initialization provides a better starting point for optimization, leading to faster convergence and potentially better final performance regardless of dataset size.
- Core assumption: Better initialization reduces the optimization burden and helps find better local minima during fine-tuning.
- Evidence anchors:
  - [abstract]: "effectiveness... beyond few-shot settings"
  - [section 6.2]: "We can see that: (i) The performance gain of MPT is evident even using the full dataset (3.27%), demonstrating that it does help cross-task generalization beyond few-shot."
  - [corpus]: Weak evidence - corpus doesn't directly address MPT performance with full datasets.
- Break condition: If the source task distribution differs significantly from the target task, even good initialization may not overcome the fundamental mismatch.

## Foundational Learning

- Concept: Meta-learning (optimization-based)
  - Why needed here: MPT relies on optimization-based meta-learning to learn initialization parameters that generalize across tasks
  - Quick check question: What's the key difference between MAML and FoMAML in terms of gradient computation?

- Concept: Prompt tuning
  - Why needed here: MPT builds upon prompt tuning by adding a meta-learning stage to learn better initialization
  - Quick check question: How does prompt tuning differ from full model fine-tuning in terms of parameter updates?

- Concept: Task similarity and transfer learning
  - Why needed here: The effectiveness of MPT depends on the similarity between source and target tasks
  - Quick check question: How might you measure task similarity in a way that predicts MPT success?

## Architecture Onboarding

- Component map:
  - Meta-training stage: Source tasks → Meta-learning algorithm (MAML/FoMAML/Reptile) → Learned prompt embeddings (φ*)
  - Meta-testing stage: φ* + Target task → Prompt tuning → Final model
  - Data flow: Support sets and query sets for each task during meta-training; training/validation/test sets for target tasks during meta-testing

- Critical path:
  1. Partition tasks into source and target sets
  2. Conduct meta-training on source tasks to learn φ*
  3. Initialize prompt embeddings with φ* for each target task
  4. Perform prompt tuning on target task
  5. Evaluate performance

- Design tradeoffs:
  - Memory vs. performance: MAML uses second-order gradients for better adaptation but is memory-intensive; FoMAML/Reptile are more memory-efficient but may perform worse
  - Source task diversity vs. specificity: More diverse source tasks may provide broader knowledge but less targeted transfer; similar source tasks may provide better targeted transfer
  - Hyperparameter tuning: Extensive tuning may improve performance but is computationally expensive

- Failure signatures:
  - MPT performs worse than prompt tuning alone: Likely indicates poor choice of source tasks or inappropriate hyperparameters
  - MPT performs worse than multi-task learning: May indicate insufficient task similarity or that MTL is better suited for the task types
  - High variance in performance across different target tasks: Could indicate instability in meta-training or that certain task types are harder to transfer to

- First 3 experiments:
  1. Run MPT with MAML on the R!R setting using default hyperparameters from the paper
  2. Compare MPT performance to prompt tuning alone and multi-task learning on the same setting
  3. Test MPT robustness by running it with different backbone models (T5-Base, BART-Large) on the NP!P setting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective strategies for selecting source tasks that maximize cross-task generalization for a given target task in MPT?
- Basis in paper: [explicit] The authors note in the Q3 section that "more systematic studies on how to select the most suitable source tasks given a set of target tasks are needed" and that their work "provides some insights on the choice of source tasks" but acknowledges this is an open research direction.
- Why unresolved: The paper only explores task selection from the perspective of task type diversity (classification vs non-classification) and number of tasks, but doesn't investigate systematic selection strategies that could optimize for task similarity or complementarity.
- What evidence would resolve it: A comprehensive study comparing different source task selection strategies (e.g., based on task similarity metrics, diversity indices, or learned selection policies) across multiple target tasks would provide evidence for optimal selection methods.

### Open Question 2
- Question: How does the performance of MPT scale with extremely large models (beyond T5-XL) and what are the computational bottlenecks at different scales?
- Basis in paper: [explicit] The authors mention in Q4 that "the consistent gain of MPT with T5-XLarge could also verify the effectiveness of MPT for huge PLMs" but only test up to T5-XLarge, leaving the question open for larger models.
- Why unresolved: The paper doesn't explore the scaling behavior of MPT with models larger than T5-XLarge (e.g., T5-XXL, GPT-3, or PaLM), which could reveal different performance characteristics or computational limitations.
- What evidence would resolve it: Systematic experiments with increasingly large models (T5-XXL, GPT-3, PaLM) measuring both performance gains and computational costs (memory, training time) would clarify scaling behavior.

### Open Question 3
- Question: Can MPT be effectively combined with data augmentation techniques to further improve cross-task generalization in few-shot settings?
- Basis in paper: [inferred] The authors discuss the limitations of MPT in non-classification tasks and note that "existing methods to address this problem mainly focus on optimizing the hypothesis space of the few-shot tasks or augmenting the few-shot data" in the related work section, suggesting potential for combining approaches.
- Why unresolved: The paper focuses solely on meta-learning for initialization without exploring how data augmentation might complement or enhance the meta-learning approach for better cross-task generalization.
- What evidence would resolve it: Experiments comparing MPT with and without data augmentation across different task types and few-shot settings would demonstrate whether the combination provides additive benefits.

## Limitations

- Task Partition Ambiguity: The exact composition of source/target task partitions is not fully specified, creating uncertainty in reproducibility
- Non-Classification Task Performance: MPT does not consistently outperform multi-task learning on non-classification tasks, limiting its universal applicability
- Hyperparameter Sensitivity: MPT performance is highly sensitive to hyperparameter selection, which may require extensive tuning

## Confidence

**High Confidence Claims**:
- MPT improves cross-task generalization for classification tasks (+20% relative gain)
- MPT provides robustness to model type and size across different backbone PLMs
- MPT outperforms prompt tuning alone in the majority of experimental configurations

**Medium Confidence Claims**:
- MPT effectiveness extends beyond few-shot settings (based on limited full-dataset experiments)
- MPT consistently outperforms multi-task learning across all task types (qualified by underperformance on non-classification tasks)

**Low Confidence Claims**:
- The specific task partitions used in experiments can be perfectly reconstructed from paper details
- Hyperparameter sensitivity can be easily overcome with standard tuning procedures

## Next Checks

1. **Task Similarity Impact Validation**: Conduct controlled experiments varying the similarity between source and target tasks (e.g., using tasks from the same or different domains) to quantify how task similarity affects MPT performance.

2. **Hyperparameter Robustness Analysis**: Systematically vary the key hyperparameters (inner learning rate, outer learning rate, number of inner steps) across a grid search for both MAML and FoMAML/REPTILE to quantify the sensitivity of MPT performance.

3. **Scalability Beyond Few-Shot Settings**: Extend the full-dataset experiments to a larger and more diverse set of target tasks beyond the 3 mentioned in section 6.2.