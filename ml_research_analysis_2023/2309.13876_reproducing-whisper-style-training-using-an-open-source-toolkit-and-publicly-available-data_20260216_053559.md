---
ver: rpa2
title: Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly
  Available Data
arxiv_id: '2309.13876'
source_url: https://arxiv.org/abs/2309.13876
tags:
- owsm
- speech
- data
- training
- whisper
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents an Open Whisper-style Speech Model (OWSM), which
  reproduces Whisper-style training using an open-source toolkit and publicly available
  data. OWSM follows the multitask framework of OpenAI Whisper, but extends it to
  support more translation directions and incorporates multiple strategies to improve
  efficiency.
---

# Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data

## Quick Facts
- arXiv ID: 2309.13876
- Source URL: https://arxiv.org/abs/2309.13876
- Reference count: 0
- Key outcome: Open Whisper-style Speech Model (OWSM) achieves competitive or superior performance to Whisper using open-source tools and 180k hours of public data

## Executive Summary
This paper presents OWSM, an open-source implementation that reproduces Whisper-style multitask speech training using publicly available data and the ESPnet toolkit. The model follows Whisper's unified multitask framework for language identification, multilingual ASR, and speech translation, but extends support to more translation directions and incorporates efficiency improvements like 40ms downsampling. Three model versions are developed, showing progressive performance improvements from v1 to v3 through scaling model size and training data. The authors plan to release all training scripts, models, and logs to promote transparency in large-scale speech model pretraining.

## Method Summary
OWSM uses a Transformer encoder-decoder architecture with joint CTC loss, trained on a unified multitask format where different speech tasks are represented in a single sequence-to-sequence framework using special tokens. The model processes 80-dimensional log Mel filterbanks with 25ms windows and 10ms hops, then downsamples using 2D convolution (2x for v1, 4x for v2/v3) to achieve 40ms time resolution. Training employs AdamW optimizer with warmup, SpecAugment for augmentation, and global normalization. Three model versions are developed: v1 (39M parameters, 10 encoder layers), v2 (74M parameters, 12 encoder layers), and v3 (101M parameters, 14 encoder layers), each trained on progressively larger datasets.

## Key Results
- OWSM v3 achieves 8.7 WER on English GigaSpeech test-other, outperforming Whisper's 9.4 WER
- Strong Japanese ASR performance with 12.3 CER on CSJ test set, surpassing Whisper's 15.4 CER
- Achieves competitive speech translation BLEU scores across multiple language pairs despite using 1/4 the training data of Whisper
- Demonstrates improved efficiency through 40ms downsampling while maintaining or improving performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unified multitask data format enables single model to handle multiple speech tasks
- Mechanism: All tasks represented in unified token format with special tokens indicating task type and language, allowing decoder to autoregressively predict correct output
- Core assumption: Model can learn to distinguish tasks based on task-specific tokens and context without task-specific heads
- Evidence anchors: Abstract mentions OWSM follows Whisper design for LID, multilingual ASR, and segmentation; section states data samples from different tasks use unified format for autoregressive prediction
- Break condition: If model cannot learn to properly condition on task tokens, performance on individual tasks may degrade

### Mechanism 2
- Claim: Scaling model size and training data improves multilingual ASR performance
- Mechanism: Progressive improvements from OWSM v1 to v3 by increasing model parameters and training data, particularly strong Japanese performance due to additional Japanese data
- Core assumption: More parameters and diverse data allow model to capture language-specific patterns and generalize better
- Evidence anchors: Abstract states OWSM follows Whisper design for multilingual ASR; section notes English ASR capability improved from v1 to v2 through scaling
- Break condition: If scaling not accompanied by appropriate regularization and training stability measures, model may overfit to dominant languages

### Mechanism 3
- Claim: Joint CTC loss stabilizes training for multitask speech models
- Mechanism: Adding joint CTC loss to encoder output helps stabilize training and expedite convergence for attention-based encoder-decoder model on multiple tasks
- Core assumption: CTC loss provides auxiliary training signal that helps encoder learn better intermediate representations for ASR tasks
- Evidence anchors: Abstract mentions OWSM follows Whisper design; section notes incorporating joint ASR CTC loss can stabilize training and expedite convergence
- Break condition: If CTC loss not properly weighted or model architecture incompatible with CTC, may introduce conflicting gradients

## Foundational Learning

- Concept: Multitask learning and unified data formats
  - Why needed here: Model must handle multiple speech tasks within single architecture, requiring understanding of unified task representation
  - Quick check question: How does unified data format allow model to distinguish between ASR and ST tasks during training?

- Concept: Speech feature extraction and augmentation
  - Why needed here: Model processes raw audio through log Mel filterbanks with SpecAugment, requiring understanding of audio preprocessing pipelines
  - Quick check question: What is purpose of SpecAugment in training pipeline, and how does it affect model robustness?

- Concept: Transformer architectures and attention mechanisms
  - Why needed here: Model uses encoder-decoder Transformers with specific configurations that affect performance and efficiency
  - Quick check question: How does number of attention heads affect model's ability to capture long-range dependencies in speech sequences?

## Architecture Onboarding

- Component map: Raw audio → 80-dim log Mel filterbanks (25ms window, 10ms hop) → 2D convolution downsampling (4x for v2/v3, 2x for v1) → Transformer encoder-decoder → Joint CTC loss + autoregressive decoder → Output tokens (language/task/text)

- Critical path: Raw audio → feature extraction → downsampling → encoder → decoder → output tokens

- Design tradeoffs:
  - Sequence length vs. efficiency: 40ms time resolution instead of 20ms reduces sequence length and memory usage but may affect fine-grained temporal modeling
  - Model size vs. performance: Larger models (v3) support more languages but require more resources and may need warm initialization
  - Joint CTC vs. pure attention: CTC loss stabilizes training but adds complexity and requires careful weighting

- Failure signatures:
  - Poor convergence: May indicate issues with joint CTC loss weighting or learning rate scheduling
  - Language-specific failures: Could indicate insufficient data for certain languages or imbalanced task mixing
  - Memory issues: May require adjusting downsampling rate or batch size

- First 3 experiments:
  1. Train OWSM v1 on small subset of data to verify basic functionality and data pipeline
  2. Compare CTC-only vs. attention-only vs. joint CTC/attention decoding on validation set
  3. Test different downsampling rates (20ms vs 40ms) to measure impact on performance and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of OWSM v3 compare to Whisper when trained on same amount of data?
- Basis in paper: OWSM v3 trained on 180k hours (1/4 of Whisper's 680k hours), showing competitive performance but direct comparison on equal data not provided
- Why unresolved: Would require training Whisper on smaller dataset or OWSM on larger dataset
- What evidence would resolve it: Training both models on same amount of data and comparing performance across benchmarks

### Open Question 2
- Question: How does warm initialization technique used in OWSM v3 affect final performance compared to training from scratch?
- Basis in paper: OWSM v3 initialized with pre-trained OWSM v2, reducing training time, but impact on final performance remains uninvestigated
- Why unresolved: No comparison between warm-initialized and scratch-trained versions provided
- What evidence would resolve it: Training OWSM v3 from scratch and comparing performance to warm-initialized version

### Open Question 3
- Question: How would OWSM perform if trained on more diverse and larger datasets like Whisper's approach?
- Basis in paper: Whisper's data collected from Internet in long form may be more realistic than OWSM's data from ASR/ST corpora
- Why unresolved: Paper does not explore OWSM performance on more diverse, larger datasets
- What evidence would resolve it: Training OWSM on more diverse, larger dataset similar to Whisper's approach and comparing performance

## Limitations
- Dataset composition and preprocessing pipeline details are not fully specified, making exact reproduction difficult
- Performance comparisons limited to specific benchmarks and language pairs, may not generalize across all tasks
- Efficiency improvements through downsampling need more rigorous ablation studies to establish causal relationships

## Confidence
- High Confidence: Core methodology of multitask learning with unified data formats is well-established; technical implementation details are standard and reproducible
- Medium Confidence: Claims of competitive performance to Whisper are supported by experimental results but limited to specific benchmarks
- Low Confidence: Exact data preparation pipeline lacks sufficient detail for precise replication; training stability improvements from joint CTC loss need more systematic investigation

## Next Checks
1. **Data Quality Validation**: Recreate exact data preprocessing pipeline using specified ASR and ST corpora, verify 180k-hour total duration and language distribution including 30-second utterance concatenation and text normalization

2. **Model Configuration Replication**: Train OWSM v1 from scratch using provided scripts, compare training curves, convergence behavior, and validation metrics against paper's reported results

3. **Benchmark Comparison**: Conduct comprehensive evaluation across all reported benchmarks (English ASR, multilingual ASR, speech translation, language identification) using same test sets as Whisper, including both greedy and beam search decoding