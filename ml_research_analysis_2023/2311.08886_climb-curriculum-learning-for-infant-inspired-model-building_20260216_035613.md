---
ver: rpa2
title: 'CLIMB: Curriculum Learning for Infant-inspired Model Building'
arxiv_id: '2311.08886'
source_url: https://arxiv.org/abs/2311.08886
tags:
- curriculum
- learning
- language
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a curriculum learning framework for training
  language models from scratch on limited data, inspired by infant language acquisition.
  The authors propose three curriculum strategies: gradually increasing vocabulary
  size, ordering training data by difficulty, and varying the training objective complexity.'
---

# CLIMB: Curriculum Learning for Infant-inspired Model Building

## Quick Facts
- arXiv ID: 2311.08886
- Source URL: https://arxiv.org/abs/2311.08886
- Reference count: 14
- Key outcome: Curriculum learning strategies inspired by infant language acquisition do not consistently outperform vanilla baselines, though specific combinations yield marginal gains

## Executive Summary
This paper introduces CLIMB, a curriculum learning framework for training language models from scratch on limited data (10M words) by simulating infant language acquisition processes. The authors propose three curriculum strategies: gradually increasing vocabulary size, ordering training data by difficulty, and varying objective function complexity. While their approaches do not consistently beat their BabyBERTa-style baseline across all tasks, they identify specific combinations that yield marginal improvements. The work also demonstrates that careful data preprocessing, model architecture selection, and hyperparameter tuning significantly improve performance over provided BabyLM baselines.

## Method Summary
The authors implement an 8-layer BabyBERTa-style Transformer model with Pre-Layer Norm architecture, trained on the BabyLM STRICT-SMALL dataset (10M words from 10 corpora). They explore three curriculum learning strategies: vocabulary curriculum (gradually expanding vocabulary using token ID or POS tag ordering), data curriculum (ordering corpora by difficulty or using perplexity-based sampling), and objective curriculum (combining MLM with word class prediction tasks). The model is trained for 400K steps with AdamW optimizer and linear learning rate schedule, evaluated on BLiMP, BLiMP-Supplement, SuperGLUE, and MSGS benchmarks.

## Key Results
- Curriculum learning strategies do not consistently outperform vanilla BabyBERTa baseline across all tasks
- Logarithmic pacing functions generally outperform linear pacing for data curriculum
- Careful data preprocessing and hyperparameter tuning significantly improve performance over provided BabyLM baselines
- Specific curriculum combinations (e.g., 3-task sequential objective curriculum) show marginal gains on SuperGLUE

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Gradually increasing vocabulary size during training can simulate the word acquisition process observed in children.
- **Mechanism**: By initially limiting the vocabulary to a subset of tokens and progressively expanding it, the model focuses on learning high-frequency words first, similar to how children prioritize learning common words before expanding to more complex vocabulary.
- **Core assumption**: Early exposure to a restricted vocabulary helps the model establish a strong foundation in language understanding before tackling more diverse linguistic structures.
- **Evidence anchors**:
  - [abstract]: "In the vocabulary curriculum, we analyze methods for constraining the vocabulary in the early stages of training to simulate cognitively more plausible learning curves."
  - [section]: "To represent a child's growing vocabulary, we select a limited vocabulary in the initial stages of learning and map all other input tokens into the representation for the unknown token (UNK)."
  - [corpus]: Weak; the corpus only lists related papers but does not directly support the vocabulary curriculum mechanism.
- **Break condition**: If the model struggles to generalize beyond the initial restricted vocabulary or if performance on linguistic tasks does not improve with expanded vocabulary.

### Mechanism 2
- **Claim**: Ordering training data by difficulty can improve the model's ability to learn complex linguistic structures.
- **Mechanism**: By presenting easier examples first (e.g., spoken language before written text) and gradually introducing more complex data, the model can build upon its understanding incrementally, mimicking the way children are exposed to simpler language before more complex forms.
- **Core assumption**: A structured progression through training data that reflects increasing linguistic complexity leads to better language acquisition.
- **Evidence anchors**:
  - [abstract]: "In the data curriculum experiments, we vary the order of the training instances based on i) infant-inspired expectations and ii) the learning behavior of the model."
  - [section]: "We attempt to carefully optimize the way data is sampled and presented to the language model over the course of training. We experiment with theory-driven and model-driven approaches to determine the 'relative difficulty' of a certain example and train the model on instances with progressively increasing difficulty."
  - [corpus]: Weak; no direct evidence from corpus to support data ordering mechanism.
- **Break condition**: If the model's performance does not improve with ordered data or if it shows signs of overfitting to easier examples.

### Mechanism 3
- **Claim**: Varying the objective function complexity can enhance the model's linguistic generalization capabilities.
- **Mechanism**: By starting with simpler tasks (e.g., predicting word classes) and progressing to more complex ones (e.g., full masked language modeling), the model can develop a more robust understanding of language structure before tackling intricate language modeling tasks.
- **Core assumption**: A curriculum of objective functions that increase in specificity mirrors the cognitive development of language learners and leads to better generalization.
- **Evidence anchors**:
  - [abstract]: "In the objective curriculum, we explore different variations of combining the conventional masked language modeling task with a more coarse-grained word class prediction task to reinforce linguistic generalization capabilities."
  - [section]: "Research in cognitive linguistics has shown that one-year-old infants are sensitive to distributional aspects of language and from two years of age begin to recognize lexical categories such as nouns and verbs. We therefore experiment with predicting only the word class of a masked token at the start of training rather than predicting its exact target token ID."
  - [corpus]: Weak; corpus evidence does not directly support the objective function curriculum mechanism.
- **Break condition**: If the model fails to benefit from the progression of task complexity or if simpler tasks do not lead to improved performance on more complex tasks.

## Foundational Learning

- **Concept**: Curriculum learning
  - **Why needed here**: Curriculum learning provides a framework for gradually increasing the difficulty of training, which is inspired by how humans learn languages. It's essential for simulating child-like language acquisition in language models.
  - **Quick check question**: What are the three main components of curriculum learning explored in this paper?
- **Concept**: Masked language modeling (MLM)
  - **Why needed here**: MLM is a standard objective function for training language models. Understanding its limitations and how to vary its complexity is crucial for the objective curriculum experiments.
  - **Quick check question**: How does the paper propose to modify the MLM objective to make it more cognitively plausible?
- **Concept**: Part-of-speech (POS) tagging
  - **Why needed here**: POS tagging is used to represent word classes and is integral to the vocabulary and objective curriculum experiments. It helps in clustering words and simplifying the classification tasks.
  - **Quick check question**: What method is used for unsupervised POS tagging in the absence of external resources?

## Architecture Onboarding

- **Component map**: Pre-processed text data -> Byte Pair Encoding tokenizer -> 8-layer BabyBERTa-style Transformer -> Masked Language Modeling and Word Class Prediction objectives -> Curriculum learning strategies
- **Critical path**:
  1. Pre-process training data
  2. Train tokenizer and POS tagger on cleaned data
  3. Initialize BabyBERTa-style model
  4. Apply curriculum learning strategies (vocabulary, data, objective)
  5. Evaluate model performance on linguistic tasks
- **Design tradeoffs**:
  - Smaller vocabulary size (8,192) vs. larger (16,384): Smaller vocabulary improves performance on some tasks but may limit expressiveness
  - Linear vs. logarithmic pacing functions: Logarithmic pacing may lead to earlier exposure to more complex concepts but could also risk overwhelming the model
  - Multitask learning vs. sequential task scheduling: Multitask learning can provide simultaneous exposure to different tasks but may complicate the learning process
- **Failure signatures**:
  - Model struggles with generalization beyond initial vocabulary
  - Performance degradation when introducing more complex data or tasks
  - Overfitting to easier examples or tasks
- **First 3 experiments**:
  1. Train a vanilla BabyBERTa-style model on pre-processed data to establish a baseline
  2. Implement a vocabulary curriculum with linear pacing to observe its effect on model performance
  3. Apply a data curriculum with logarithmic pacing to assess improvements in handling complex linguistic structures

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal pacing function for vocabulary curriculum learning?
- **Basis in paper**: [inferred] The paper found that logarithmic pacing outperformed linear pacing for the data curriculum, but the findings for the vocabulary curriculum were less clear.
- **Why unresolved**: The results for vocabulary curriculum pacing were mixed across different evaluation tasks (BLiMP, SuperGLUE, BLiMP-supplement).
- **What evidence would resolve it**: Additional experiments testing various pacing functions (e.g., exponential, U-shaped) across all evaluation tasks would help identify the optimal pacing function for vocabulary curriculum learning.

### Open Question 2
- **Question**: How does perplexity-based data selection compare to source-based ordering in terms of model performance?
- **Basis in paper**: [explicit] The paper states that perplexity-based approaches to data selection hold potential, but they have not found a clear-cut best method for perplexity calculation yet.
- **Why unresolved**: The results of the perplexity-based models were mixed, with only one model outperforming the vanilla model on BLiMP.
- **What evidence would resolve it**: Systematic comparison of different perplexity calculation methods (e.g., static vs. dynamic, unigram vs. model-based) and their impact on model performance across all evaluation tasks would help determine the best approach.

### Open Question 3
- **Question**: How does the timing of task switches in sequential objective curricula affect model performance?
- **Basis in paper**: [explicit] The paper found that sequential curricula were generally outperformed by multitasking ones, but a 3-task sequential curriculum performed well on SuperGLUE.
- **Why unresolved**: The paper did not explore the impact of varying the timing of task switches in sequential curricula.
- **What evidence would resolve it**: Experiments varying the timing of task switches in sequential curricula, along with the number and type of tasks, would help determine the optimal configuration for this approach.

### Open Question 4
- **Question**: How do the proposed curriculum learning strategies scale to larger datasets?
- **Basis in paper**: [explicit] The paper mentions that the BabyLM challenge evaluates models on a fixed 10 million word dataset, and they also trained a larger model on a 100 million word dataset.
- **Why unresolved**: The paper did not investigate the effectiveness of the curriculum learning strategies on the larger dataset.
- **What evidence would resolve it**: Applying the curriculum learning strategies to the larger dataset and evaluating their impact on model performance would help determine their scalability.

## Limitations
- Curriculum learning approaches do not consistently outperform vanilla baseline across all tasks
- Unsupervised POS tagger implementation not fully specified, critical for curriculum experiments
- Limited statistical significance testing for marginal performance differences
- No systematic ablation studies to isolate individual curriculum component effects

## Confidence
- **High Confidence**: Careful data preprocessing and hyperparameter tuning significantly improve performance over BabyLM baselines
- **Medium Confidence**: Curriculum learning does not consistently outperform vanilla baseline, though specific combinations show marginal gains
- **Low Confidence**: Theoretical claims about cognitive plausibility primarily supported by citations rather than direct empirical validation

## Next Checks
1. **Ablation Study Implementation**: Conduct systematic ablation experiments to isolate the individual contributions of vocabulary curriculum, data curriculum, and objective curriculum components, testing each in isolation and in various combinations to determine which specific elements drive any observed performance improvements.

2. **POS Tagger Validation**: Reimplement the unsupervised POS tagger using the described methodology (presumably based on the paper's citation) and verify that the clustering results are consistent and meaningful across different corpus subsets, ensuring that the vocabulary and objective curriculum experiments are built on reliable linguistic representations.

3. **Statistical Significance Testing**: Apply appropriate statistical tests (e.g., paired t-tests or bootstrap confidence intervals) to the BLiMP and SuperGLUE results to determine whether the observed differences between curriculum learning approaches and the vanilla baseline are statistically significant, particularly for the marginal gains reported in specific task combinations.