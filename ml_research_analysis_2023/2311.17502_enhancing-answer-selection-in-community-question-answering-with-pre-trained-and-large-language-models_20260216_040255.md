---
ver: rpa2
title: Enhancing Answer Selection in Community Question Answering with Pre-trained
  and Large Language Models
arxiv_id: '2311.17502'
source_url: https://arxiv.org/abs/2311.17502
tags:
- question
- answer
- selection
- answers
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles answer selection in community question answering
  (CQA), where users struggle to identify relevant answers from many candidates. To
  address this, the authors propose a Question-Answer cross attention network (QAN)
  that leverages pre-trained BERT models and a cross attention mechanism to capture
  interaction features between questions and answers.
---

# Enhancing Answer Selection in Community Question Answering with Pre-trained and Large Language Models

## Quick Facts
- arXiv ID: 2311.17502
- Source URL: https://arxiv.org/abs/2311.17502
- Reference count: 40
- Key outcome: QAN model with BERT cross-attention achieves state-of-the-art performance on SemEval2015 and SemEval2017 datasets

## Executive Summary
This paper addresses the challenge of answer selection in Community Question Answering (CQA), where users need to identify relevant answers from many candidates. The authors propose a Question-Answer cross attention network (QAN) that leverages pre-trained BERT models and a cross attention mechanism to capture interaction features between questions and answers. They also explore using large language models (LLMs) with knowledge augmentation and prompt optimization to enhance answer selection performance. Experimental results show that QAN achieves state-of-the-art performance on two datasets, SemEval2015 and SemEval2017.

## Method Summary
The proposed QAN model uses BERT to separately encode question subjects, question bodies, and answers, then applies cross attention mechanisms to capture interactions between these components. The model processes (Subject, Body, Answer) triplets through separate BERT encoders, then uses cross-attention layers for (Subject→Answer) and (Body→Answer) interactions. These interaction features are passed through a Bi-GRU, pooling, and MLP classifier for final prediction. The paper also explores LLM-based approaches using LLaMA-7b-hf for knowledge generation and prompt optimization to improve answer selection accuracy.

## Key Results
- QAN achieves state-of-the-art performance on SemEval2015 and SemEval2017 datasets
- Incorporating external knowledge generated by LLMs improves answer selection accuracy
- Optimizing prompts helps LLMs select correct answers on more questions
- Cross-attention mechanism effectively captures crucial interaction semantic features between questions and answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention captures bidirectional interaction between question and answer tokens, improving relevance modeling.
- Mechanism: The QAN model uses separate cross-attention layers for (subject→answer) and (body→answer) interactions, normalizing similarity matrices to extract context-aware features.
- Core assumption: Token-level interactions between question and answer carry sufficient signal to distinguish Good from Bad answers.
- Evidence anchors:
  - [abstract]: "we employ the cross attention mechanism to effectively capture crucial interaction semantic features between questions and answers."
  - [section 3.2]: "We employ cross attention mechanism [28] to establish essential interaction features between the question and answer by analyzing the relationships between the question subject-answer and question body-answer."
- Break condition: If questions and answers are semantically orthogonal (e.g., completely unrelated topics), token-level cross-attention will produce noisy or misleading interaction features.

### Mechanism 2
- Claim: Separate BERT pre-training of subject, body, and answer preserves distinct semantic contexts before fusion.
- Mechanism: BERT encodes each component independently, then cross-attention operates on these separate embeddings to avoid conflating subject and body contexts.
- Core assumption: Treating question subject and body as separate inputs yields richer interaction features than concatenating them.
- Evidence anchors:
  - [section 3.2]: "We first utilize the BERT [27] to capture contextual representations of the question subject, question body, and answer in token form."
  - [section 5.4.1]: Ablation study shows that merging subject and body degrades performance ("Without treat question subjects and question bodies separately").
- Break condition: If subject and body are always semantically aligned, the extra computation for separate encoding may not pay off.

### Mechanism 3
- Claim: LLM-generated external knowledge augments the semantic space, helping distinguish subtle answer quality differences.
- Mechanism: LLaMA-7b-hf generates knowledge from question+correct answer pairs; this knowledge is fed into LLM answer selection, improving accuracy.
- Core assumption: The generated knowledge captures discriminative features not present in the raw QA pair.
- Evidence anchors:
  - [abstract]: "incorporating external knowledge generated by LLMs improves answer selection accuracy."
  - [section 5.5.1]: "After adding knowledge, LLM achieved higher accuracy on both the SemEval2015 and SemEval2017 data sets."
- Break condition: If the generated knowledge is irrelevant or noisy (e.g., Table 8), it can mislead the model rather than help.

## Foundational Learning

- Concept: Attention mechanisms in NLP
  - Why needed here: Cross-attention is central to QAN's ability to model token-level interactions between questions and answers.
  - Quick check question: What is the difference between self-attention and cross-attention in transformer models?

- Concept: Pre-trained language models (BERT)
  - Why needed here: BERT provides rich contextual embeddings that capture semantic nuance before cross-attention fusion.
  - Quick check question: How does BERT's bidirectional training improve over unidirectional LMs for QA tasks?

- Concept: Prompt engineering for LLMs
  - Why needed here: Optimizing prompt length, question placement, and task description directly affects LLM answer selection accuracy.
  - Quick check question: What are the risks of an overly long or poorly structured prompt in zero-shot LLM classification?

## Architecture Onboarding

- Component map:
  Input: (Subject, Body, Answer) triplets → BERT encoders for Subject, Body, Answer → Cross-Attention Layer (Subject→Answer and Body→Answer matrices) → Interaction & Prediction Layer (Bi-GRU → Pooling → MLP classifier) → Final prediction

- Critical path:
  BERT encoding → Cross-attention fusion → Interaction features → MLP classification

- Design tradeoffs:
  - Separate vs. joint encoding of subject/body
  - Cross-attention vs. self-attention only
  - Knowledge augmentation vs. model size/complexity

- Failure signatures:
  - Low MAP/F1 on dev set → likely cross-attention misconfiguration
  - Overfitting → dropout or regularization adjustments needed
  - Prompt sensitivity → LLM instability, requires prompt tuning

- First 3 experiments:
  1. Ablation: Remove cross-attention and compare to full QAN.
  2. Ablation: Merge subject and body before BERT and measure impact.
  3. Prompt tuning: Test different prompt lengths on a held-out LLM subset.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the QAN model scale with increasing dataset size and complexity?
- Basis in paper: [explicit] The paper demonstrates QAN's state-of-the-art performance on two datasets, SemEval2015 and SemEval2017.
- Why unresolved: The paper only evaluates QAN on two specific datasets, and it is unclear how the model would perform on larger, more complex datasets.
- What evidence would resolve it: Evaluating QAN on a diverse set of datasets with varying sizes and complexities, and comparing its performance to other state-of-the-art models.

### Open Question 2
- Question: What is the optimal balance between knowledge augmentation and prompt optimization for LLM-based answer selection?
- Basis in paper: [explicit] The paper explores knowledge augmentation and prompt optimization separately, but does not investigate their combined effect.
- Why unresolved: The paper demonstrates that both knowledge augmentation and prompt optimization improve LLM performance, but it is unclear how to optimally combine these techniques.
- What evidence would resolve it: Conducting experiments that systematically vary the amount of knowledge augmentation and the level of prompt optimization, and measuring their combined effect on LLM performance.

### Open Question 3
- Question: How can the QAN model be adapted to handle multi-lingual CQA tasks?
- Basis in paper: [inferred] The paper focuses on English-language CQA datasets, and it is unclear how the QAN model would perform on multi-lingual tasks.
- Why unresolved: The paper does not investigate the model's ability to handle languages other than English, and it is unclear how the BERT-based encoding and cross-attention mechanisms would need to be adapted.
- What evidence would resolve it: Evaluating the QAN model on multi-lingual CQA datasets, and adapting the model architecture as needed to handle language-specific challenges.

## Limitations
- Performance depends heavily on the quality of generated knowledge from LLMs
- Model complexity may pose practical constraints for real-world deployment
- Results are based primarily on English datasets, limiting generalizability to other languages

## Confidence
- Medium: Experimental results demonstrate clear improvements over baseline methods, but implementation details are not fully specified
- Medium: Claims about LLM performance improvements are somewhat tempered by sensitivity to prompt optimization
- Medium: Comparison with existing state-of-the-art methods could be more comprehensive

## Next Checks
1. Cross-attention sensitivity analysis: Systematically vary attention mechanism parameters and measure their impact on performance across different question types.
2. Knowledge quality assessment: Conduct qualitative analysis of generated knowledge from LLaMA-7b-hf, categorizing instances and correlating with performance changes.
3. Generalization testing: Evaluate QAN model on additional CQA datasets from different domains and languages to assess generalization beyond SemEval datasets.