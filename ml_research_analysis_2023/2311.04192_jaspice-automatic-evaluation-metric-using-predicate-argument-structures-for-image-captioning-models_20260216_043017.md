---
ver: rpa2
title: 'JaSPICE: Automatic Evaluation Metric Using Predicate-Argument Structures for
  Image Captioning Models'
arxiv_id: '2311.04192'
source_url: https://arxiv.org/abs/2311.04192
tags:
- image
- scene
- evaluation
- jaspice
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces JaSPICE, a novel automatic evaluation metric
  designed specifically for Japanese image captioning models. Unlike existing metrics
  that rely on n-grams, JaSPICE leverages scene graphs generated from dependencies
  and predicate-argument structures (PAS) to capture the semantic structure of captions.
---

# JaSPICE: Automatic Evaluation Metric Using Predicate-Argument Structures for Image Captioning Models

## Quick Facts
- **arXiv ID**: 2311.04192
- **Source URL**: https://arxiv.org/abs/2311.04192
- **Reference count**: 40
- **Key outcome**: JaSPICE achieves Pearson correlation coefficients of 0.501 on STAIR Captions and 0.572 on PFN-PIC, significantly outperforming baseline metrics in evaluating Japanese image captioning models.

## Executive Summary
This paper introduces JaSPICE, a novel automatic evaluation metric specifically designed for Japanese image captioning models. Unlike existing n-gram based metrics, JaSPICE leverages scene graphs generated from dependencies and predicate-argument structures (PAS) to capture the semantic structure of captions. The method involves parsing scene graphs from captions, extending them using synonym relationships from Japanese WordNet, and computing an F1 score based on matching tuples extracted from candidate and reference scene graphs. The proposed metric was evaluated using a newly constructed dataset called Shichimi, containing 103,170 human evaluations collected from 500 evaluators. Results demonstrate that JaSPICE significantly outperforms baseline metrics such as BLEU, ROUGE, METEOR, and CIDEr in terms of correlation with human evaluation.

## Method Summary
JaSPICE is a two-module metric consisting of PAS-Based Scene Graph Parser (PAS-SGP) and Graph Analyzer (GA). PAS-SGP generates scene graphs from Japanese captions using morphological analysis (JUMAN++), syntactic analysis, and predicate-argument structure extraction (KNP), parsing 13 defined dependency patterns to construct tuples of objects, attributes, and relations. GA extends these scene graphs using synonym sets from Japanese WordNet, applying heuristic zero anaphora resolution for pronoun handling. The final score is computed as an F1 measure based on the overlap of tuples between candidate and reference scene graphs, with recall (R) and precision (P) calculated from matched and total tuples respectively.

## Key Results
- JaSPICE achieves Pearson correlation of 0.501 on STAIR Captions dataset
- JaSPICE achieves Pearson correlation of 0.572 on PFN-PIC dataset
- JaSPICE significantly outperforms baseline metrics (BLEU, ROUGE, METEOR, CIDEr) in correlation with human evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: JaSPICE outperforms baseline metrics in correlation with human evaluation due to its use of scene graphs based on dependencies and predicate-argument structures (PAS).
- Mechanism: By parsing scene graphs from captions using PAS and dependencies, JaSPICE captures the semantic structure more effectively than n-gram based metrics. This allows it to better align with human judgment of caption quality.
- Core assumption: The semantic structure captured by scene graphs based on PAS and dependencies is a more accurate representation of caption quality than n-grams.
- Evidence anchors:
  - [abstract] "The proposed method generates a scene graph from dependencies and the predicate-argument structure (PAS) and extends the graph using synonyms."
  - [section 4.2] "Our method differs from existing methods because it generates scene graphs based on dependencies and the PAS and uses synonym sets for the evaluation so that it can evaluate image captioning models in Japanese."
  - [corpus] Weak evidence - no direct citation or performance comparison with n-gram metrics in corpus entries.
- Break condition: If the semantic structure is not the primary factor in human evaluation of caption quality, or if the PAS-based scene graph generation introduces significant errors.

### Mechanism 2
- Claim: JaSPICE's use of synonym sets improves its correlation with human evaluation by considering semantically equivalent but lexically different captions.
- Mechanism: By extending scene graphs with synonyms, JaSPICE can match captions that use different words to describe the same objects or relationships, leading to a more accurate assessment of semantic similarity.
- Core assumption: Human evaluators consider semantically equivalent captions as similar, even if they use different words.
- Evidence anchors:
  - [abstract] "The proposed method generates a scene graph from dependencies and the predicate-argument structure (PAS) and extends the graph using synonyms."
  - [section 4.3] "GA expands {G(yi,j)}N j=1 and G(ˆy) by introducing synonym nodes as follows: Suppose that objects o1 and o2 are connected by relation r. Given that S(x) denotes the set of synonyms of x, our method generates new relations Rel ⟨o′ 1, r′, o′ 2⟩, where o′ 1 ∈ S(o1), o′ 2 ∈ S(o2), and r′ ∈ S(r)."
  - [corpus] Weak evidence - no direct citation or performance comparison with metrics not using synonyms in corpus entries.
- Break condition: If the synonym set used is incomplete or inaccurate, or if human evaluators do not consider semantically equivalent captions as similar.

### Mechanism 3
- Claim: JaSPICE's heuristic zero anaphora resolution improves its performance by handling cases where one caption uses a pronoun and the other uses the full noun phrase.
- Mechanism: By resolving zero pronouns (represented as ϕ), JaSPICE can match relations that would otherwise be missed due to the use of pronouns, leading to a more accurate assessment of semantic similarity.
- Core assumption: Human evaluators consider captions with pronouns and their full noun phrase equivalents as similar.
- Evidence anchors:
  - [section 4.2] "Algorithm 1 shows the node completion algorithm for zero pronouns (ϕ represents a zero pronoun)."
  - [section 4.2] "To alleviate this issue, the proposed method performs heuristic zero anaphora resolution."
  - [corpus] Weak evidence - no direct citation or performance comparison with metrics not using zero anaphora resolution in corpus entries.
- Break condition: If the heuristic zero anaphora resolution introduces significant errors, or if human evaluators do not consider captions with pronouns and their full noun phrase equivalents as similar.

## Foundational Learning

- Concept: Scene graphs
  - Why needed here: Scene graphs are used to represent the semantic structure of captions, capturing objects, their attributes, and relationships between them.
  - Quick check question: How does a scene graph differ from a dependency parse tree in representing the semantic structure of a caption?

- Concept: Predicate-argument structures (PAS)
  - Why needed here: PAS is used to generate scene graphs from captions, capturing the relations between predicates and their arguments.
  - Quick check question: What is the role of PAS in the JaSPICE metric, and how does it differ from using only dependency parses?

- Concept: Synonym sets
  - Why needed here: Synonym sets are used to extend scene graphs, allowing JaSPICE to match semantically equivalent but lexically different captions.
  - Quick check question: How does the use of synonym sets in JaSPICE improve its correlation with human evaluation compared to metrics that do not use synonyms?

## Architecture Onboarding

- Component map: PAS-SGP -> GA -> F1 Score
- Critical path: The critical path for computing the JaSPICE score involves parsing the candidate and reference captions into scene graphs using PAS-SGP, extending the graphs with synonyms using GA, and computing the F1 score based on matching tuples.
- Design tradeoffs: The use of PAS and dependencies for scene graph generation allows JaSPICE to capture complex relationships but may introduce additional complexity compared to using only dependencies. The use of synonym sets improves semantic matching but requires a comprehensive and accurate synonym database.
- Failure signatures: Low correlation with human evaluation despite high performance on other metrics may indicate issues with the PAS-based scene graph generation, synonym extension, or zero anaphora resolution. High computational complexity may indicate inefficiencies in the scene graph parsing or matching process.
- First 3 experiments:
  1. Compare the performance of JaSPICE with and without the synonym extension on a subset of the Shichimi dataset to assess the impact of synonyms on correlation with human evaluation.
  2. Analyze the performance of JaSPICE on captions with varying levels of complexity (e.g., simple descriptions vs. complex narratives) to identify potential limitations of the PAS-based scene graph generation.
  3. Evaluate the robustness of JaSPICE to variations in the quality of the input captions (e.g., grammatical errors, missing information) to assess its reliability in real-world applications.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would incorporating hypernyms and hyponyms into the graph extension process affect the correlation between JaSPICE and human evaluation scores?
- Basis in paper: [explicit] The authors mention that future studies will extend their method by taking into account hypernyms and hyponyms.
- Why unresolved: The current method only considers synonyms for graph extension, which leads to a discrepancy in word granularity between the candidate and reference captions, as identified in the error analysis section.
- What evidence would resolve it: Experiments comparing the performance of JaSPICE with and without hypernym/hyponym-based graph extensions, measured by correlation coefficients with human evaluation scores.

### Open Question 2
- Question: How does the performance of JaSPICE vary across different domains or types of images, such as those requiring specific technical knowledge versus everyday scenes?
- Basis in paper: [inferred] The paper evaluates JaSPICE on STAIR Captions and PFN-PIC, which include general image captions and robotic instruction captions, respectively. However, it does not analyze performance variations across different image types.
- Why unresolved: The current evaluation does not segment results based on image content or domain, making it unclear how well JaSPICE generalizes to different types of images.
- What evidence would resolve it: Performance analysis of JaSPICE on a diverse set of image captioning datasets, categorized by domain or image type, with correlation coefficients reported for each category.

### Open Question 3
- Question: What is the impact of using more advanced natural language processing tools, such as neural dependency parsers or contextualized word embeddings, on the performance of JaSPICE?
- Basis in paper: [inferred] The paper uses JUMAN++ for morphological analysis and KNP for syntactic analysis and PAS extraction. It does not explore the potential benefits of more advanced NLP tools.
- Why unresolved: The current implementation relies on rule-based methods and static word embeddings (Japanese WordNet), which may limit the ability to capture nuanced semantic relationships.
- What evidence would resolve it: Experiments comparing the performance of JaSPICE using different NLP tools for dependency parsing and PAS extraction, as well as experiments incorporating contextualized word embeddings for synonym detection.

## Limitations

- The evaluation is limited to the Japanese language, with results specific to Japanese image captioning that may not generalize to other languages.
- The method depends heavily on the quality and completeness of Japanese WordNet for synonym extension, which could introduce errors if the synonym database is incomplete.
- The zero anaphora resolution is described as heuristic, suggesting potential limitations in handling complex pronoun resolution cases that could affect reliability.

## Confidence

**High Confidence**: The claim that JaSPICE outperforms baseline metrics (BLEU, ROUGE, METEOR, CIDEr) in correlation with human evaluation on Japanese image captioning tasks. This is supported by direct quantitative evidence from the Shichimi dataset evaluation.

**Medium Confidence**: The claim that JaSPICE's use of scene graphs based on dependencies and PAS captures semantic structure more effectively than n-gram based metrics. While the mechanism is theoretically sound and supported by the correlation results, direct ablation studies comparing PAS-based vs. non-PAS-based approaches are not provided.

**Medium Confidence**: The assertion that synonym extension improves correlation with human evaluation by considering semantically equivalent captions. The mechanism is described clearly, but the paper lacks quantitative evidence showing the specific contribution of synonym extension versus the core PAS-based scene graph approach.

## Next Checks

1. **Ablation Study on Synonym Extension**: Conduct experiments comparing JaSPICE performance with and without synonym extension on the Shichimi dataset to quantify the specific contribution of this feature to the overall correlation with human evaluation.

2. **Cross-Lingual Generalization Test**: Apply JaSPICE to a non-Japanese image captioning dataset (such as MS-COCO) to evaluate whether the PAS-based scene graph approach generalizes beyond the Japanese language, or if the metric is specifically optimized for Japanese linguistic structures.

3. **Error Analysis on Zero Anaphora Resolution**: Perform a detailed analysis of cases where zero anaphora resolution succeeds versus fails, quantifying the impact of this heuristic on overall JaSPICE performance and identifying patterns in captions where the resolution mechanism may introduce errors.