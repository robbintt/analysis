---
ver: rpa2
title: 'CLASSify: A Web-Based Tool for Machine Learning'
arxiv_id: '2310.03618'
source_url: https://arxiv.org/abs/2310.03618
tags:
- data
- dataset
- each
- classify
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLASSify provides a web-based, automated machine learning tool
  for solving binary and multiclass classification problems in bioinformatics. It
  offers model training, optimization, synthetic data generation, feature evaluation,
  and interpretability through SHAP scores.
---

# CLASSify: A Web-Based Tool for Machine Learning

## Quick Facts
- arXiv ID: 2310.03618
- Source URL: https://arxiv.org/abs/2310.03618
- Reference count: 40
- Primary result: CLASSify is a web-based, automated machine learning tool for binary and multiclass classification in bioinformatics, offering model training, optimization, synthetic data generation, and interpretability.

## Executive Summary
CLASSify is a web-based automated machine learning platform designed to solve binary and multiclass classification problems, particularly in bioinformatics. It automates model selection, training, optimization, and evaluation while providing interpretability through SHAP scores. The tool supports multiple classification algorithms including Random Forest, Gradient Boosting, XGBoost, and MLP, with automated hyperparameter tuning via Optuna. A key feature is its synthetic data generation capability using models like CTGAN and TVAE, which can fill missing values, balance class distributions, or generate entirely new datasets. CLASSify delivers intuitive visualizations and explainability metrics to help non-technical users understand and deploy classification models effectively.

## Method Summary
CLASSify provides an automated machine learning pipeline where users upload tabular CSV datasets with required 'class' and 'index' columns, then select from multiple classification models including Random Forest, Gradient Boosting, XGBoost, and MLP. The system offers automated hyperparameter tuning through Optuna, exploring parameter space to optimize model performance. Synthetic data generation capabilities include CTGAN, TVAE, and TabPFN models for handling missing values, balancing class distributions, or creating new datasets. Model interpretability is provided through SHAP scores that quantify feature contributions to predictions. The web interface handles data preprocessing, model training (locally or distributed via ClearML), and generates visualizations of results including performance metrics and feature importance.

## Key Results
- Nine of eleven tested datasets showed improved performance with Optuna parameter tuning across 100 iterations
- Datasets with class imbalance greater than 40%/60% showed improved results when synthetic balancing was implemented
- The tool successfully handles missing values through synthetic imputation and provides interpretable SHAP scores for feature importance

## Why This Works (Mechanism)

### Mechanism 1
Synthetic data balancing improves model performance on imbalanced datasets by generating synthetic examples for minority classes, allowing the model to see more balanced class distributions during training and reducing bias toward majority classes. This works under the assumption that synthetic data generation models can produce realistic samples preserving original data statistics. Evidence shows every dataset with class disparity greater than 40%/60% had improved results with synthetic balancing. The technique may fail if synthetic data doesn't capture true data distributions, introducing noise that degrades performance.

### Mechanism 2
Automated hyperparameter tuning via Optuna improves model performance without requiring user expertise by intelligently exploring parameter space and pruning unpromising configurations early, converging to better hyperparameters faster than random search. This assumes default parameter ranges are reasonable starting points for diverse datasets. Evidence shows nine of eleven datasets improved with parameter tuning for 100 iterations. Tuning may waste time or converge to suboptimal settings if parameter ranges are poorly chosen.

### Mechanism 3
SHAP scores make model predictions interpretable for non-technical users by quantifying each feature's contribution to individual predictions, with averaging across data providing global feature importance. This assumes SHAP explanations remain meaningful even for complex models like neural networks or tree ensembles. Evidence shows SHAP values help users understand data relationships and variable interactions that may not be obvious. Explanations may oversimplify if feature relationships are highly non-linear or interactions are critical.

## Foundational Learning

- **Tabular data preprocessing and feature engineering**: Understanding how to handle missing values, categorical encoding, and scaling is critical since CLASSify works with structured data. Quick check: What is the difference between mean imputation and model-based imputation for missing values?
- **Binary vs multiclass classification and evaluation metrics**: Knowing when to use accuracy, AUC, sensitivity, specificity, kappa, etc., is essential since CLASSify supports both. Quick check: Why might accuracy be misleading on imbalanced datasets?
- **Synthetic data generation techniques (CTGAN, TVAE, etc.)**: Understanding their assumptions and limitations is important since CLASSify offers multiple synthesizers. Quick check: How does CTGAN differ from traditional GANs in handling tabular data?

## Architecture Onboarding

- **Component map**: Web frontend (PHP) → Python backend (ClearML orchestration) → Model training (scikit-learn, XGBoost, etc.) → Results storage (ClearML DB) → Visualization (matplotlib, seaborn)
- **Critical path**: Data upload → Preprocessing → Model selection → Training (local or distributed) → SHAP scoring → Visualization generation → Results display
- **Design tradeoffs**: Distributed training via ClearML improves scalability but adds operational complexity; synthetic data generation improves performance but requires careful metadata handling
- **Failure signatures**: Missing or incorrect metadata leads to poor synthetic data quality; overly broad hyperparameter ranges slow tuning; missing dependency versions cause training failures
- **First 3 experiments**:
  1. Upload a small, balanced CSV and run all default models without synthetic data or tuning to verify basic functionality
  2. Upload an imbalanced dataset and enable synthetic class balancing to observe performance improvement
  3. Upload a dataset with missing values and enable synthetic imputation to test the imputation pipeline

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of CLASSify's synthetic data generation methods compare to other state-of-the-art synthetic data generation techniques in terms of preserving data quality and utility for machine learning tasks? The paper mentions CLASSify uses synthetic data generation methods like CTGAN, TVAE, and TabPFN but doesn't provide comprehensive comparison with other techniques. A study comparing these methods with various datasets and performance metrics would provide necessary evidence.

### Open Question 2
What are the limitations of CLASSify's SHAP score implementation, and how can they be addressed to improve the interpretability of machine learning models? The paper mentions CLASSify uses SHAP scores for feature importance but doesn't discuss potential limitations or improvements. A detailed analysis of the SHAP implementation including strengths and weaknesses would provide necessary evidence.

### Open Question 3
How does the user experience of CLASSify compare to other web-based machine learning tools, and what are the key factors that contribute to its ease of use and accessibility? The paper emphasizes CLASSify's user-friendly interface but doesn't provide comparative analysis with other tools. A study comparing user experience with other web-based tools considering interface design, functionality, and accessibility would provide necessary evidence.

## Limitations
- No empirical comparison with established AutoML platforms to benchmark performance gains from synthetic data or parameter tuning
- Lack of cross-validation results or statistical significance testing for reported improvements
- Unclear generalizability to non-tabular data or highly complex, high-dimensional datasets

## Confidence

- **High Confidence**: Web interface functionality, model training pipeline, SHAP integration, synthetic data generation capabilities
- **Medium Confidence**: Performance improvements from parameter tuning and synthetic balancing (limited cross-dataset validation)
- **Low Confidence**: Claims about usability for non-technical users without user study data

## Next Checks

1. **Benchmarking**: Compare CLASSify's performance against a standard AutoML framework (e.g., H2O AutoML) on identical bioinformatics datasets to quantify relative gains
2. **Synthetic Data Quality**: Evaluate synthetic data fidelity using statistical similarity metrics (e.g., maximum mean discrepancy) and downstream task performance on held-out real data
3. **Parameter Tuning Efficiency**: Test whether Optuna tuning with default ranges consistently outperforms random search within the same iteration budget across diverse datasets