---
ver: rpa2
title: Scaling Laws for Associative Memories
arxiv_id: '2310.02984'
source_url: https://arxiv.org/abs/2310.02984
tags:
- when
- memory
- error
- embeddings
- figure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies associative memory mechanisms through a model
  based on high-dimensional matrices of outer products of embeddings, relating to
  transformer language model inner layers. It derives precise scaling laws for generalization
  error with respect to sample size and parameter size under Zipf-distributed data,
  comparing different memory storage schemes and optimization algorithms.
---

# Scaling Laws for Associative Memories

## Quick Facts
- arXiv ID: 2310.02984
- Source URL: https://arxiv.org/abs/2310.02984
- Reference count: 40
- Primary result: Derives precise scaling laws for generalization error in associative memory models under Zipf-distributed data

## Executive Summary
This paper studies associative memory mechanisms through a model based on high-dimensional matrices of outer products of embeddings, relating to transformer language model inner layers. It derives precise scaling laws for generalization error with respect to sample size and parameter size under Zipf-distributed data, comparing different memory storage schemes and optimization algorithms. The theoretical analysis shows optimal scaling in d^-(α-1) + T^-(1-1/α) for memory capacity d and data size T, achieved by thresholding schemes that store only the most frequent associations. Extensive experiments validate these results and reveal the benefits of small batch sizes, large learning rates, and layer normalization for efficient memorization, while optimization methods like Adam help homogenize the resulting memory matrices.

## Method Summary
The paper implements an outer-product memory model where associations are encoded as weighted outer products of input and output embeddings, superimposed in a single matrix W. The model compares random embeddings (provably scaling but limited capacity) with learned embeddings (better empirical performance but broken theoretical guarantees). Experiments use Zipf-distributed data with parameter α=2, testing different weighting schemes (uniform vs frequency-proportional), optimization algorithms (SGD, Adam), batch sizes, and learning rates. The theoretical analysis derives error bounds based on interference between stored associations and the dimensionality of the embedding space.

## Key Results
- Memory capacity scales as d^-(α-1) + T^-(1-1/α) for Zipf-distributed data with parameter α
- Storing associations uniformly (q(x)=1) outperforms frequency-proportional storage (q(x)=p(x)) in finite data regimes
- Small batch sizes, large learning rates, and layer normalization significantly improve memorization efficiency
- Adam optimization helps homogenize memory matrices and improves generalization performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Memory capacity scales inversely with the number of data associations that must be stored; when the model's parameter count d is insufficient, interference between stored associations causes errors.
- **Mechanism**: Each association is encoded as an outer product of an input and output embedding, and all associations are superimposed in a single matrix W. When d is small, the dimensionality cannot support orthogonal storage of all associations, so the signal from one association leaks into others.
- **Core assumption**: Embeddings are random and drawn independently from a Gaussian distribution; the data follows a Zipf distribution so a few associations dominate frequency.
- **Evidence anchors**:
  - [abstract] The paper derives scaling laws with respect to parameter size d and sample size T under Zipf-distributed data.
  - [section] Theorem 1 gives an upper bound on the error involving a threshold condition on dq(x)² versus Q∞.
  - [corpus] No direct evidence in corpus; only tangentially related memory models.
- **Break condition**: If embeddings are not random but learned and structured (e.g., ex = uf∗(x)), the interference can be minimized, breaking the scaling law for random embeddings.

### Mechanism 2
- **Claim**: Storing all associations uniformly (q(x) = 1) leads to better generalization than storing them proportionally to frequency (q(x) = p(x)) when data is finite.
- **Mechanism**: When a batch processes multiple examples, weighting by frequency p(x) reduces the effective update per association; processing each example individually (or with small batch) avoids this dilution.
- **Core assumption**: In the finite data regime, each association must be "seen" enough times to build up its signal in the matrix W.
- **Evidence anchors**:
  - [abstract] Discusses statistical efficiency of different estimators, including optimization-based algorithms.
  - [section] Section 4 explains how large batch sizes approximate population gradients and degrade performance compared to many small updates.
  - [corpus] No direct corpus evidence; the claim is supported only by internal analysis.
- **Break condition**: If batch size is extremely small (e.g., 1), the variance in updates becomes too high and may erase previous associations; also, if d is much larger than N, the effect disappears.

### Mechanism 3
- **Claim**: Layer normalization and Adam optimization improve memory storage efficiency by homogenizing gradient magnitudes across associations.
- **Mechanism**: Layer norm normalizes gradients by removing the projection onto the current input direction, preventing runaway growth; Adam normalizes by adaptive step sizes, effectively making updates sign-based, which reduces sensitivity to magnitude differences among associations.
- **Core assumption**: Without normalization, the cross-entropy loss causes W to diverge, and gradients become unstable.
- **Evidence anchors**:
  - [abstract] Mentions that Adam helps homogenize resulting memory matrices.
  - [section] Section 4.1 discusses Adam and layer normalization, showing empirical benefits in experiments.
  - [corpus] No direct corpus evidence; relies on internal experimental results.
- **Break condition**: If embeddings are learned to be orthogonal, normalization may be unnecessary; also, if the task has very low variance in association frequencies, the benefit diminishes.

## Foundational Learning

- **Concept**: Zipf distribution of data frequencies
  - **Why needed here**: The scaling laws depend critically on the heavy-tailed nature of natural language data; without Zipf, the interference analysis and optimal thresholding would not apply.
  - **Quick check question**: What happens to the error scaling if the input distribution is uniform instead of Zipf?

- **Concept**: Outer product associative memory and interference
  - **Why needed here**: The model's core is a superposition of outer products; understanding interference between them is essential to derive error bounds.
  - **Quick check question**: Why does interference become negligible when d ≫ N?

- **Concept**: Gradient-based optimization dynamics for associative memories
  - **Why needed here**: The paper studies how SGD, Adam, batch size, and learning rate affect the learned association scheme q(x), which in turn determines generalization error.
  - **Quick check question**: How does the learning rate affect the shape of qγ(x) in the approximation (20)?

## Architecture Onboarding

- **Component map**: Data generator -> Embedding layers -> Associative memory matrix -> Prediction -> Optimizer
- **Critical path**:
  1. Sample T data points from p(x) ∝ x⁻α
  2. Initialize embeddings and W
  3. Iterate over batches: compute gradients of cross-entropy, update W
  4. After training, evaluate generalization error E(fW)
  5. Optionally analyze learned q(x) and embeddings

- **Design tradeoffs**:
  - Random vs learned embeddings: Random embeddings give provable scaling but limited capacity; learned embeddings can fit more associations but break theoretical guarantees
  - Batch size: Small batches give more updates per association but higher variance; large batches approximate population gradients but may degrade memorization
  - Learning rate: Large rates store associations faster but risk overwriting; small rates build up slowly but preserve existing memories

- **Failure signatures**:
  - Error plateaus at 1/2 when d is too small relative to N (memory overflow)
  - Error decreases slowly as d increases if the weighting scheme is frequency-proportional (q(x) = p(x))
  - Error increases if batch size is too large relative to data frequency distribution

- **First 3 experiments**:
  1. Vary d while keeping T fixed to observe the transition from overflow regime to infinite memory regime and validate scaling d⁻α+1
  2. Compare q(x) = 1 vs q(x) = p(x) for finite T to confirm the benefit of uniform weighting
  3. Test batch size effect by training with |B| = 1, 16, 1024 and measuring generalization error to see the impact of frequency weighting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do optimization algorithms like Adam and layer normalization improve memorization capacity compared to plain SGD, and can this improvement be quantified theoretically?
- Basis in paper: [explicit] The paper discusses the role of Adam and layer normalization in optimizing memory storage, suggesting they help homogenize memory matrices and optimize memory capacity.
- Why unresolved: While the paper provides experimental evidence of the benefits of Adam and layer normalization, a rigorous theoretical explanation for why these methods are effective is lacking.
- What evidence would resolve it: A mathematical proof showing how Adam's adaptive learning rates or layer normalization's projection effects lead to improved scaling laws in terms of memorization capacity.

### Open Question 2
- Question: How does learning the input embeddings (ex) affect the scaling laws of memorization, and can this be incorporated into the theoretical framework?
- Basis in paper: [explicit] The paper shows that learning embeddings can lead to zero generalization error with smaller d, contrasting with the d ≥ N requirement for random embeddings.
- Why unresolved: The theoretical analysis focuses on random embeddings, and the implications of learning embeddings are not fully explored.
- What evidence would resolve it: An extension of the theoretical framework to include learned embeddings, showing how they modify the scaling laws and the conditions for achieving zero error.

### Open Question 3
- Question: How does the choice of output embedding distribution (e.g., uniform on the sphere vs. learned) impact the scaling laws and the interference between memories?
- Basis in paper: [explicit] The paper discusses the impact of quasi-orthogonal output embeddings on the upper and lower bounds of the error.
- Why unresolved: The analysis primarily focuses on random embeddings, and the effect of different output embedding distributions is not fully explored.
- What evidence would resolve it: A theoretical comparison of the scaling laws under different output embedding distributions, showing how they affect the interference between memories and the overall error.

### Open Question 4
- Question: How does the Zipf distribution parameter α affect the scaling laws, and can we derive tighter bounds for specific ranges of α?
- Basis in paper: [explicit] The paper derives scaling laws for α-Zipf distributions, but the analysis is general and may not be tight for specific ranges of α.
- Why unresolved: The theoretical bounds may not be tight for all values of α, and the impact of α on the scaling laws is not fully understood.
- What evidence would resolve it: Tighter theoretical bounds for specific ranges of α, or experimental results showing how the scaling laws vary with α.

## Limitations
- Theoretical analysis assumes random, independent Gaussian embeddings which may not hold for learned embeddings in practical transformers
- Zipf-distributed data assumption may not apply to all domains beyond natural language
- Focus on infinite data regime (T → ∞) creates potential gap between theory and finite-data practical experiments
- Comparison between random and learned embeddings lacks full exploration of practical implications

## Confidence

- **High confidence**: The scaling law derivation for random embeddings under Zipf distribution (d⁻(α-1) + T⁻¹⁺¹/α) is mathematically rigorous and well-supported by theory. The experimental validation of these scaling laws using the derived bounds is convincing.

- **Medium confidence**: The claims about optimization algorithms (Adam, layer normalization) improving memory storage efficiency are supported by internal experiments but lack external validation. The mechanism explanations for why these methods work are plausible but not definitively proven.

- **Low confidence**: The comparison between random and learned embeddings as associative memory mechanisms is based on limited empirical evidence. The paper acknowledges that learned embeddings can fit more associations but breaks theoretical guarantees, but doesn't fully explore the practical implications of this tradeoff.

## Next Checks

1. **Generalization to non-Zipf distributions**: Test the scaling laws and error bounds with uniform, exponential, or other heavy-tailed distributions to determine the robustness of the theoretical results beyond Zipf-distributed data.

2. **Transfer to transformer architectures**: Implement a simple transformer model with learned embeddings and test whether the scaling laws and optimization insights (batch size, learning rate, normalization) transfer from the theoretical associative memory model to practical transformer layers.

3. **Memory capacity under interference**: Systematically vary the ratio d/N (dimension to association count) and measure the transition from overflow regime to infinite memory regime, validating the theoretical prediction that interference becomes negligible when d ≫ N.