---
ver: rpa2
title: Machine-Created Universal Language for Cross-lingual Transfer
arxiv_id: '2305.13071'
source_url: https://arxiv.org/abs/2305.13071
tags:
- words
- word
- universal
- language
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a machine-created universal language (MUL)
  as an intermediate language for cross-lingual transfer. MUL unifies common concepts
  from different languages into a single universal word while preserving language-specific
  words and word order, enabling better cross-lingual transfer and application to
  word-level tasks.
---

# Machine-Created Universal Language for Cross-lingual Transfer

## Quick Facts
- arXiv ID: 2305.13071
- Source URL: https://arxiv.org/abs/2305.13071
- Authors: 
- Reference count: 11
- Key outcome: MUL achieves comparable results to XLM-R with 50% parameters and better results after redistributing parameters from word embedding to transformer weights

## Executive Summary
This paper proposes a machine-created universal language (MUL) as an intermediate language for cross-lingual transfer. MUL unifies common concepts from different languages into a single universal word while preserving language-specific words and word order. The authors create MUL by pre-training an encoder with multilingual MLM, using inter-sentence contrastive learning to improve cross-lingual alignment of contextualized word embeddings, and proposing vector quantization with cross-lingual alignment (VQ-CA) to improve interpretability of the universal vocabulary. Experiments on XNLI, NER, MLQA, and Tatoeba show that MUL achieves comparable results to XLM-R with 50% parameters and better results after redistributing parameters from word embedding to transformer weights.

## Method Summary
The method involves pre-training an encoder with multilingual MLM loss on 15 XNLI languages, then training on bilingual data from OPUS-100 using inter-sentence contrastive learning and VQ-CA to create MUL. The NL-MUL translator preserves word order and generates one universal word per natural word, enabling application to word-level tasks. The model is then fine-tuned on MUL for cross-lingual tasks and evaluated on XNLI, NER, MLQA, and Tatoeba.

## Key Results
- MUL achieves comparable performance to XLM-R with 50% parameters
- Better results after redistributing parameters from word embedding to transformer weights
- Good interpretability and can reduce ambiguity compared to translating to English

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inter-sentence contrastive learning reduces distance between aligned words and increases distance between words of same type but different meanings
- Mechanism: Uses aligned word pairs as positive samples and words from other sentence pairs as negative samples to push non-aligned same-type words apart
- Core assumption: Words of the same type but different meanings in bilingual sentences should be mapped to different universal words
- Evidence anchors:
  - [abstract] "We use an inter-sentence contrastive learning approach to further improve the alignment of contextualized word embedding across languages"
  - [section 2.4] "We propose inter-sentence contrastive learning to solve this problem. First, we leverage contrastive learning to reduce the distance between aligned words and keep the not-aligned words far from each other. Second, we leverage words from other sentence pairs as negative samples to keep words with the same type far from each other."
- Break condition: If training data lacks sufficient bilingual pairs with diverse word types, the negative sampling strategy may not effectively separate same-type words

### Mechanism 2
- Claim: Vector quantization with cross-lingual alignment (VQ-CA) creates discrete symbols that correspond to single concepts rather than multiple symbols for same meaning
- Mechanism: Constrains aligned words to map to same symbol while pushing one symbol away if multiple symbols exist for same concept
- Core assumption: Multiple symbols for same concept reduces interpretability and hurts cross-lingual transfer
- Evidence anchors:
  - [abstract] "we propose vector quantization with cross-lingual alignment (VQ-CA) to improve the interpretability of universal vocabulary"
  - [section 2.5] "We propose vector-quantization with cross-lingual alignment(VQ-CA) to guide the learning of discrete symbols by aligning them with multiple languages at the same time"
- Break condition: If training data lacks sufficient aligned word pairs across languages, VQ-CA may not effectively merge multiple symbols for same concept

### Mechanism 3
- Claim: Preserving word order and language-specific words in MUL enables application to word-level tasks
- Mechanism: NL-MUL translator maintains word order and generates one universal word per natural word
- Core assumption: Word order preservation is necessary for sequential labeling and machine reading comprehension tasks
- Evidence anchors:
  - [abstract] "MUL preserves the language-specific words as well as word order, so the model can be easily applied to word-level tasks"
  - [section 2.1] "The NL-MUL translator preserves the word order and language-specific vocabulary, so the model can be easily applied to word-level tasks"
- Break condition: If target tasks don't require word order preservation, this mechanism adds unnecessary complexity

## Foundational Learning

- Concept: Word alignment in cross-lingual tasks
  - Why needed here: Core to understanding how MUL maps words across languages and creates universal vocabulary
  - Quick check question: What is the difference between word alignment and machine translation, and why is alignment sufficient for MUL creation?

- Concept: Vector quantization and discrete symbol learning
  - Why needed here: VQ-CA is the mechanism that creates interpretable universal words from continuous embeddings
  - Quick check question: How does VQ-CA differ from standard VQ-VAE, and why does the cross-lingual alignment component matter?

- Concept: Contrastive learning in representation learning
  - Why needed here: Inter-sentence contrastive learning is key to improving cross-lingual alignment quality
  - Quick check question: What is the difference between standard contrastive loss and the inter-sentence variant used here, and why does using other sentence pairs as negatives help?

## Architecture Onboarding

- Component map: Natural language → Encoder → Inter-sentence contrastive learning → VQ-CA → Universal vocabulary → NL-MUL translator → MUL representation
- Critical path: Natural language → Encoder → Inter-sentence contrastive learning → VQ-CA → Universal vocabulary → NL-MUL translator → MUL representation
- Design tradeoffs: Smaller vocabulary (60K vs 250K) reduces parameters but requires more sophisticated alignment methods to maintain performance
- Failure signatures: Poor word alignment scores indicate contrastive learning isn't working; multiple symbols for same concept indicate VQ-CA isn't working; low XNLI scores indicate overall system failure
- First 3 experiments:
  1. Train encoder with multilingual MLM and evaluate word alignment on bilingual data to establish baseline
  2. Add inter-sentence contrastive learning and measure improvement in word alignment and embedding similarity
  3. Add VQ-CA and evaluate reduction in multiple symbols per concept and improvement in interpretability metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the effects of using different types of bilingual data, such as language-specific corpora or data from the same language family, on the performance of MUL?
- Basis in paper: [inferred] The paper mentions that the current bilingual data used is English-centric and suggests that using data from language pairs within the same language family could yield interesting results.
- Why unresolved: The paper does not provide experimental results on the impact of different types of bilingual data on MUL's performance.
- What evidence would resolve it: Conducting experiments with different types of bilingual data and comparing the performance of MUL in each case would provide insights into the effects of data diversity on MUL's effectiveness.

### Open Question 2
- Question: How does the performance of MUL scale with larger model sizes and more languages?
- Basis in paper: [explicit] The paper mentions that experiments were conducted on 15 languages and base model sizes, and suggests extending the work to more languages and bigger model sizes in the future.
- Why unresolved: The paper does not provide experimental results on the scalability of MUL with larger models and more languages.
- What evidence would resolve it: Conducting experiments with larger model sizes and more languages, and comparing the performance of MUL in each case, would provide insights into the scalability of MUL.

### Open Question 3
- Question: How can the mapping between natural words and the concepts of universal words be further improved to enhance MUL's interpretability and cross-lingual transfer performance?
- Basis in paper: [explicit] The paper acknowledges that the mapping between natural words and universal words is not perfect and suggests further improvement of the creation methods to enhance MUL's interpretability and performance.
- Why unresolved: The paper does not provide specific methods or techniques to improve the mapping between natural words and universal words.
- What evidence would resolve it: Developing and testing new methods or techniques to improve the mapping between natural words and universal words, and evaluating their impact on MUL's interpretability and cross-lingual transfer performance, would provide insights into potential improvements.

## Limitations
- The inter-sentence contrastive learning mechanism relies heavily on the quality and quantity of bilingual alignment data
- The VQ-CA approach assumes that multiple symbols for the same concept inherently reduce interpretability without empirical validation
- The paper doesn't sufficiently explore failure modes when MUL encounters languages with very different typologies

## Confidence
- **High Confidence**: MUL achieves comparable performance to XLM-R with 50% parameters (supported by XNLI, NER, MLQA, Tatoeba results)
- **Medium Confidence**: Inter-sentence contrastive learning improves cross-lingual alignment (mechanism described but corpus evidence is weak)
- **Medium Confidence**: VQ-CA improves interpretability of universal vocabulary (mechanism described but corpus evidence is weak)
- **Low Confidence**: MUL reduces ambiguity compared to translating to English (mentioned but not empirically validated)

## Next Checks
1. **Cross-lingual Alignment Validation**: Measure word alignment accuracy on a held-out bilingual dataset before and after inter-sentence contrastive learning to quantify the actual improvement in alignment quality, not just downstream task performance.

2. **Interpretability Benchmark**: Create a human evaluation study where linguists rate the interpretability of MUL words versus their component natural language words, and test whether the claimed interpretability improvements correlate with actual downstream performance gains.

3. **Typological Robustness Test**: Evaluate MUL on language pairs with maximally different typologies (e.g., isolating vs. agglutinative languages) to determine whether the word order preservation constraint creates performance bottlenecks or whether the model can handle diverse linguistic structures effectively.