---
ver: rpa2
title: 'Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from
  Text'
arxiv_id: '2308.02357'
source_url: https://arxiv.org/abs/2308.02357
tags:
- ontology
- language
- text
- sentence
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Text2KGBench, a benchmark for evaluating
  large language models' (LLMs) ability to generate knowledge graphs from text guided
  by an ontology. The task involves extracting facts from text that comply with a
  given ontology's concepts, relations, and constraints.
---

# Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text

## Quick Facts
- arXiv ID: 2308.02357
- Source URL: https://arxiv.org/abs/2308.02357
- Authors: 
- Reference count: 12
- Key outcome: Introduces Text2KGBench, a benchmark for evaluating LLMs' ability to generate ontology-compliant knowledge graphs from text, with baseline F1 scores of 0.35 (Vicuna-13B) and 0.27 (Alpaca-LoRA-13B).

## Executive Summary
This paper introduces Text2KGBench, a benchmark designed to evaluate large language models' (LLMs) ability to generate knowledge graphs from text while adhering to a predefined ontology. The benchmark provides two datasets, Wikidata-TekGen and DBpedia-WebNLG, each containing sentences and corresponding ontology definitions. Seven evaluation metrics are defined to measure fact extraction accuracy, ontology conformance, and hallucination rates. Baseline results using Vicuna-13B and Alpaca-LoRA-13B demonstrate the challenge of the task, with average F1 scores indicating significant room for improvement.

## Method Summary
The Text2KGBench benchmark evaluates LLMs by providing them with ontology definitions (concepts, relations, constraints) and example sentence-triple pairs through in-context learning. For each test sentence, the model generates candidate triples which are evaluated for fact extraction accuracy using precision, recall, and F1 scores. Ontology conformance measures whether generated relations appear in the ontology, while hallucination metrics detect subject, relation, and object generation outside the sentence or ontology. The paper provides baseline results using Vicuna-13B and Alpaca-LoRA-13B models with automatic prompt generation based on sentence similarity.

## Key Results
- Vicuna-13B achieves an average F1 score of 0.35 on Wikidata-TekGen, while Alpaca-LoRA-13B scores 0.27
- Ontology conformance (OC) is high for both models, typically above 0.8, indicating good adherence to relation constraints
- Hallucination rates (subject, relation, object) vary but remain non-trivial, suggesting the models sometimes generate facts not supported by the input sentence or ontology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The benchmark evaluates LLM ability to generate knowledge graph facts by extracting triples that conform to an input ontology while being faithful to the input sentence.
- Mechanism: LLMs are prompted with ontology definitions (concepts, relations, domain/range constraints) and example sentence-triple pairs. The model generates candidate triples, which are evaluated using precision/recall/F1 for fact extraction accuracy, ontology conformance (OC) to ensure only valid relations are used, and hallucination metrics (SH, RH, OH) to detect subject/relation/object generation outside the sentence or ontology.
- Core assumption: The LLM's in-context learning from examples is sufficient for it to understand how to map natural language to ontology-compliant triples without fine-tuning.
- Evidence anchors:
  - [abstract] "Given an input ontology and a set of sentences, the task is to extract facts from the text while complying with the given ontology (concepts, relations, domain/range constraints) and being faithful to the input sentences."
  - [section 4] "Fact Extraction Accuracy: This is measured using Precision (P), Recall (R), and F1 scores by comparing LLM output triples to the ground truth triples."
  - [corpus] Weak corpus support: only 8 related papers found; no direct evidence of this exact mechanism being validated in literature.
- Break condition: If the LLM fails to understand the relation notation or the in-context examples are too sparse or irrelevant, fact extraction accuracy will drop sharply.

### Mechanism 2
- Claim: High ontology conformance scores indicate the model reliably restricts output to the predefined relation set.
- Mechanism: The prompt explicitly lists ontology relations in the format `relation(subject, object)` and asks the model to follow this format. Evaluation measures OC as the fraction of generated triples whose relation appears in the ontology. Since OC is often >0.8 in results, the model respects this constraint.
- Core assumption: The LLM will adhere to the explicit format and relation constraints when instructed.
- Evidence anchors:
  - [section 5.2] "Each test case in our benchmark is associated with an ontology. This part of the prompts verbalizes the ontology by listing the set of concepts, and a set of relations with their domain and range constraints given by the ontology."
  - [table 2] OC values such as 0.83 (Wikidata-TekGen Vicuna-All) show high compliance.
  - [corpus] No direct corpus evidence of similar constraint adherence mechanisms in related papers.
- Break condition: If the ontology is large or complex, or if the model is prompted without sufficient examples, OC will drop because the model may hallucinate relations.

### Mechanism 3
- Claim: Low hallucination metrics (SH, RH, OH) demonstrate the model avoids generating facts not supported by the input sentence or ontology.
- Mechanism: The evaluation checks if subject/object terms appear in the sentence or ontology concepts and if relations appear in the ontology. Stemming is applied to catch morphological variants. Example error analysis shows the model sometimes outputs triples with entities not in the sentence (subject/object hallucination) or with relations not in the ontology (relation hallucination).
- Core assumption: The sentence is the sole source of truth for fact generation; no external knowledge should leak into the output.
- Evidence anchors:
  - [section 4] "Hallucination is defined as the generated content that is nonsensical or unfaithful to the provided source content."
  - [section 5.4] Error examples such as "member_of_political_party (Hermann Goring, Sturmabteilung)" show subject hallucination.
  - [corpus] Weak corpus support: no evidence from related papers that this hallucination metric approach is standard.
- Break condition: If the model is fine-tuned on broader knowledge, or if the prompt does not emphasize faithfulness, hallucination scores will rise.

## Foundational Learning

- Concept: Relation Extraction
  - Why needed here: The core task is mapping natural language sentences to structured triples, which is a classic relation extraction problem, but guided by an ontology.
  - Quick check question: Given a sentence "The movie was directed by John Doe," what triple would you extract if the ontology has relation `director(film, human)`?
  - Answer: `director("The movie", "John Doe")`

- Concept: In-Context Learning
  - Why needed here: The benchmark relies on LLMs learning the task from a few demonstrations in the prompt without fine-tuning.
  - Quick check question: If the prompt shows example: "Sentence: X is a film directed by Y. Output: director(X, Y)", will the model produce the same pattern for a new sentence?
  - Answer: Yes, if the examples are clear and relevant.

- Concept: Ontology Conformance
  - Why needed here: To ensure generated facts align with the predefined schema (relations, domain/range constraints).
  - Quick check question: If a sentence mentions "John Doe is a director", but the ontology has no `director(human, human)` relation, should the model generate a triple?
  - Answer: No, it should only use relations defined in the ontology.

## Architecture Onboarding

- Component map:
  - Ontology loader -> Example selector -> Prompt generator -> LLM inference engine -> Triple parser -> Evaluator

- Critical path:
  1. Load ontology and training data.
  2. For each test sentence, compute similarity to training sentences.
  3. Generate prompt with top-k similar examples.
  4. Run LLM inference.
  5. Parse and evaluate generated triples.

- Design tradeoffs:
  - Token budget vs. number of examples: More examples improve in-context learning but risk exceeding token limits.
  - Granularity of ontology vs. prompt size: Detailed constraints improve conformance but increase prompt size.
  - Sentence similarity vs. example relevance: Higher similarity may not always mean better guidance.

- Failure signatures:
  - OC drops while P/R stay high: Model is generating correct facts but using wrong relations.
  - SH/RH/OH increase: Model is hallucinating entities or relations not in the sentence/ontology.
  - Low P and low OC: Model is generating unrelated or hallucinated content.

- First 3 experiments:
  1. Run Vicuna-13B on Wikidata-TekGen with 1 example per test case; measure P, R, F1, OC, hallucinations.
  2. Increase examples to top-3 similar; compare metrics to baseline.
  3. Replace Vicuna with Alpaca-LoRA-13B; compare results to assess model sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different large language models (LLMs) compare in their ability to generate knowledge graphs from text guided by ontologies, and what architectural features contribute to better performance?
- Basis in paper: [explicit] The paper compares Vicuna-13B and Alpaca-LoRA-13B baselines, showing room for improvement, and mentions various LLM architectures like GPT-style, BERT-style, and others.
- Why unresolved: While the paper provides baseline results for two models, it doesn't comprehensively compare a wide range of LLM architectures or analyze which specific architectural features lead to better performance in this task.
- What evidence would resolve it: A systematic comparison of multiple LLM architectures (e.g., GPT-3, GPT-4, PaLM, LLaMA, BLOOM) on the Text2KGBench, along with ablation studies isolating the impact of different architectural components on performance.

### Open Question 2
- Question: What is the optimal approach for automatic prompt generation in ontology-driven knowledge graph generation from text, and how can we measure its effectiveness?
- Basis in paper: [explicit] The paper presents a baseline automatic prompt generation approach and mentions the importance of prompt engineering and finding relevant demonstration examples.
- Why unresolved: The paper provides a basic prompt generation method but doesn't explore advanced techniques or provide a comprehensive framework for evaluating prompt effectiveness in this specific task.
- What evidence would resolve it: Development and evaluation of various prompt generation strategies (e.g., prompt tuning, prefix-tuning, chain-of-thought prompting) on Text2KGBench, with metrics assessing prompt quality, relevance, and impact on LLM performance.

### Open Question 3
- Question: How can we effectively handle hallucinations and improve ontology conformance in knowledge graphs generated from text using LLMs?
- Basis in paper: [explicit] The paper defines hallucination metrics and mentions the importance of ontology conformance, but doesn't provide solutions for addressing these issues.
- Why unresolved: While the paper identifies the problems of hallucinations and ontology conformance, it doesn't explore methods for mitigating these issues or improving the faithfulness of generated knowledge graphs to the input ontology and text.
- What evidence would resolve it: Development and evaluation of post-processing techniques, semantic validation methods, and reasoning-based approaches to detect and correct hallucinations, as well as improve ontology conformance in LLM-generated knowledge graphs.

## Limitations

- The benchmark relies on sentence similarity for example selection, but the choice of SBERT T5-XXL as the similarity metric is not justified against alternatives.
- Performance gaps between models suggest significant sensitivity, but the paper doesn't explore whether this is due to architectural differences or prompt sensitivity.
- Hallucination metrics depend on stemming which may introduce false positives/negatives for morphologically complex terms.

## Confidence

- High confidence: The benchmark framework is well-defined and the evaluation metrics are clearly specified and reproducible.
- Medium confidence: The baseline results are internally consistent but may not generalize across different LLM architectures or prompt engineering strategies.
- Low confidence: The generalizability of the benchmark to real-world knowledge graph construction tasks beyond the provided datasets.

## Next Checks

1. **Alternative Similarity Metrics**: Replace SBERT T5-XXL with alternative sentence similarity approaches (e.g., cosine similarity on sentence embeddings, BM25) to assess impact on example selection quality and downstream performance.

2. **Cross-Dataset Generalization**: Evaluate the same baseline models on both Wikidata-TekGen and DBpedia-WebNLG to determine if performance differences reflect dataset difficulty or model bias.

3. **Prompt Ablation Study**: Systematically remove components from the prompt (ontology description, examples, instruction) to quantify their individual contributions to fact extraction accuracy and ontology conformance.