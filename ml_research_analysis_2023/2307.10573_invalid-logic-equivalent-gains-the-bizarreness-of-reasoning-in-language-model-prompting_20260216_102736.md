---
ver: rpa2
title: 'Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language
  Model Prompting'
arxiv_id: '2307.10573'
source_url: https://arxiv.org/abs/2307.10573
tags:
- tasks
- invalid
- arxiv
- logically
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Using BIG-Bench Hard tasks, we find that logically invalid CoT
  prompting performs nearly as well as logically valid CoT prompting and better than
  Answer Only prompting. This suggests that valid reasoning in CoT prompts is not
  the main driver of performance gains.
---

# Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting

## Quick Facts
- arXiv ID: 2307.10573
- Source URL: https://arxiv.org/abs/2307.10573
- Reference count: 3
- Primary result: Logically invalid Chain-of-Thought prompting performs nearly as well as valid CoT prompting on BIG-Bench Hard tasks

## Executive Summary
This paper challenges the assumption that logical validity in Chain-of-Thought (CoT) prompts is essential for performance gains in language models. Through experiments on BIG-Bench Hard tasks, the authors demonstrate that logically invalid CoT prompts achieve performance nearly equivalent to valid CoT prompts, and both significantly outperform Answer Only prompting. The findings suggest that language models rely more on pattern matching from exemplars than on the logical correctness of intermediate reasoning steps. The study also reveals that some previously published CoT prompts contain logical errors, raising questions about the true drivers of performance improvements in prompt engineering.

## Method Summary
The authors evaluate three prompt types (Answer Only, CoT, and Logically Invalid CoT) on BIG-Bench Hard tasks using the Codex model. They measure accuracy using Exact String Match and compare performance across prompt types. The study systematically constructs invalid CoT prompts by introducing logical errors into otherwise valid reasoning chains, then measures whether these errors degrade performance compared to valid CoT and AO baselines.

## Key Results
- Logically invalid CoT prompting improves performance almost as much as logically valid CoT prompting on BIG-Bench Hard tasks
- Some CoT prompts from previous work contain logical errors, confirmed with original authors
- Findings raise questions about what features of data or prompts cause inconsistent or invalid model outputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Invalid CoT prompts produce similar gains because language models rely on pattern matching from exemplars rather than logical correctness of intermediate steps
- Mechanism: The model treats exemplars as templates, matching structure and extracting key features regardless of whether the intermediate reasoning steps are valid
- Core assumption: The model does not evaluate the validity of each intermediate step but instead learns from the overall structure and outcome alignment in exemplars
- Evidence anchors: "logically invalid CoT prompting improves performance almost as much as logically valid CoT prompting"
- Break condition: If the model is explicitly trained or fine-tuned to evaluate logical consistency during inference, the performance gap between valid and invalid prompts would widen

### Mechanism 2
- Claim: The model's sensitivity to prompt "incorrectness" is low because it optimizes for final answer accuracy, not intermediate reasoning fidelity
- Mechanism: During inference, the model prioritizes generating outputs that match exemplars' final answers over maintaining logical coherence in intermediate steps
- Core assumption: The loss function during training focuses on predicting the next token in context, not validating the logical chain
- Evidence anchors: "some CoT prompts used by previous works contain logical errors"
- Break condition: If evaluation shifts to metrics that explicitly penalize invalid reasoning steps, performance differences would emerge

### Mechanism 3
- Claim: Task difficulty modulates sensitivity to invalid reasoning; simpler tasks mask invalid reasoning effects
- Mechanism: On easier tasks, the model can rely on shallow pattern matching and surface features, making it less sensitive to invalid reasoning
- Core assumption: Task complexity does not fundamentally change the model's reliance on exemplar-based reasoning
- Evidence anchors: "BIG-Bench Hard (BBH) tasks" are used to test harder problems
- Break condition: If invalid reasoning causes cascading errors in multi-step reasoning, harder tasks would expose performance gaps

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: The paper compares valid vs invalid CoT prompting, so understanding CoT is essential to grasp the experimental design
  - Quick check question: What is the primary difference between Answer Only and Chain-of-Thought prompting?

- Concept: Logical validity in reasoning chains
  - Why needed here: The paper's central claim is that logically invalid reasoning still yields performance gains, so understanding what constitutes valid vs invalid reasoning is key
  - Quick check question: Why might a reasoning chain reach the correct answer despite containing logical errors?

- Concept: BIG-Bench Hard (BBH) benchmark
  - Why needed here: The experiments use BBH tasks to test the robustness of invalid CoT prompting on harder problems
  - Quick check question: How does BBH differ from the original BIG-Bench in terms of task difficulty?

## Architecture Onboarding

- Component map: Prompt construction module -> Model inference engine -> Evaluation module -> Data repository
- Critical path: 1. Load BBH task and corresponding exemplars 2. Generate valid CoT, invalid CoT, and AO prompts 3. Send prompts to Codex via API 4. Parse and compare outputs to ground truth 5. Aggregate accuracy metrics per prompt type
- Design tradeoffs: Using Codex vs other models: Codex was chosen due to higher performance on BBH tasks, but limits generalizability
- Failure signatures: Codex API downtime or quota limits halt experiments, Inconsistent output formatting breaks Exact String Match evaluation, Exemplar errors propagate invalid reasoning unnoticed
- First 3 experiments: 1. Verify that Codex produces answers under all prompt types on a subset of BBH tasks 2. Confirm accuracy parity between AO and CoT prompting on a known valid exemplar set 3. Introduce controlled logical errors in exemplars and measure impact on performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific features of data or prompts cause language models to output inconsistent or invalid results?
- Basis in paper: The paper states "Our findings raise questions about what features of data or prompts cause inconsistent or invalid model outputs"
- Why unresolved: The study identifies the phenomenon but doesn't investigate the underlying mechanisms or features responsible for inconsistent outputs
- What evidence would resolve it: Controlled experiments varying specific prompt features (syntax, semantics, formatting) while measuring output consistency could identify causal factors

### Open Question 2
- Question: How does increasing the degree of incorrectness in prompts affect model sensitivity to invalid Chain-of-Thought reasoning?
- Basis in paper: The authors ask "Does increasing the degree of 'incorrectness' or the number of incorrect prompts affect the model's sensitivity to invalid CoT?"
- Why unresolved: The study only tested one level of logical invalidity and didn't systematically vary the degree of incorrectness
- What evidence would resolve it: Experiments with graduated levels of prompt invalidity (minor errors to complete nonsense) measuring performance changes would show sensitivity thresholds

### Open Question 3
- Question: Are language models more sensitive to other properties of valid prompts beyond logical reasoning correctness?
- Basis in paper: The paper asks "What other properties of the valid prompts is the model sensitive to?"
- Why unresolved: The study focused solely on logical validity but didn't explore other potential influential prompt features
- What evidence would resolve it: Systematic ablation studies removing various prompt components (formatting, vocabulary, step count) while measuring performance would identify sensitivity patterns

## Limitations
- The study relies on a single model (Codex) and a relatively small set of 23 BBH tasks
- The accuracy metric (Exact String Match) may not capture nuanced correctness or partial credit
- The paper does not explore whether invalid reasoning leads to different types of errors compared to valid reasoning

## Confidence
- High Confidence: The empirical finding that invalid CoT prompting performs comparably to valid CoT on BBH tasks using Codex model
- Medium Confidence: The mechanism explanation that models rely on pattern matching over logical validity
- Low Confidence: The claim that this finding generalizes across model architectures, tasks, and evaluation metrics

## Next Checks
1. Cross-model validation: Test the invalid vs valid CoT comparison on at least two additional model families (e.g., GPT-4, Claude) to assess generalizability beyond Codex
2. Error type analysis: Categorize and compare the types of errors produced by invalid vs valid CoT prompts to understand if invalid reasoning introduces distinct failure modes
3. Multi-metric evaluation: Supplement Exact String Match with semantic similarity metrics (e.g., BERTScore) to detect cases where answers are partially correct despite invalid reasoning