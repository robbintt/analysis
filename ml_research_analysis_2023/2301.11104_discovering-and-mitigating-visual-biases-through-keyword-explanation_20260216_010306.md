---
ver: rpa2
title: Discovering and Mitigating Visual Biases through Keyword Explanation
arxiv_id: '2301.11104'
source_url: https://arxiv.org/abs/2301.11104
tags:
- bias
- classi
- biases
- group
- blond
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to discover and mitigate
  visual biases in computer vision models. The proposed Bias-to-Text (B2T) framework
  interprets visual biases as keywords by generating captions for mispredicted images
  and extracting common keywords.
---

# Discovering and Mitigating Visual Biases through Keyword Explanation

## Quick Facts
- arXiv ID: 2301.11104
- Source URL: https://arxiv.org/abs/2301.11104
- Reference count: 40
- Primary result: Novel framework that discovers visual biases by interpreting mispredictions as keywords and validates them using vision-language models

## Executive Summary
This paper introduces the Bias-to-Text (B2T) framework for discovering and mitigating visual biases in computer vision models. The approach generates captions for mispredicted images and extracts common keywords to identify potential biases. These keywords are validated using vision-language models to distinguish spurious correlations from genuine patterns. The framework demonstrates effectiveness in recovering known biases and discovering novel ones across multiple datasets, with applications in debiased training, CLIP prompting, and model comparison.

## Method Summary
The Bias-to-Text framework operates by first training a vision model on a dataset, then generating captions for mispredicted images using a pre-trained captioning model. From these captions, common keywords are extracted using the YAKE algorithm. These keywords are validated through a bias-likeness score that measures their similarity to mispredicted versus correctly predicted images using vision-language embeddings. The validated keywords are then used to infer group labels through zero-shot CLIP prompting and applied to various debiasing techniques including group DRO, CLIP prompting strategies, and model comparison metrics.

## Key Results
- B2T successfully recovers known biases in CelebA (hair color), Waterbirds (background), and Kaggle Face (gender)
- Achieves 11.9% average improvement in worst-group accuracy on CelebA, Waterbirds, and Kaggle Face datasets
- Demonstrates 5.5% average improvement in worst-group accuracy when applied to CLIP models on ImageNet-R, ImageNet-Sketch, and ImageNet-A
- Enables effective model comparison and bias-aware prompting strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extracting keywords from mispredicted captions identifies spurious correlations.
- Mechanism: Misclassified images contain rare attribute-class combinations; keywords capture these patterns.
- Core assumption: Captions reliably describe image content and are semantically meaningful.
- Evidence anchors:
  - [abstract] "extract common keywords from the captions of mispredicted images to identify potential biases"
  - [section] "generate captions of the mispredicted images using a pre-trained captioning model to extract the common keywords"
  - [corpus] Weak evidence; no explicit validation of captioning accuracy.
- Break condition: If captions are inaccurate or irrelevant, keyword extraction fails.

### Mechanism 2
- Claim: Vision-language similarity scores distinguish majority biases from spurious correlations.
- Mechanism: Bias-likeness score compares similarity of keyword embeddings to correct vs. mispredicted samples.
- Core assumption: CLIP embeddings preserve semantic relationships between words and images.
- Evidence anchors:
  - [abstract] "validate these keywords by measuring their similarity to the mispredicted images using a vision-language scoring model"
  - [section] "define the bias-likeness score... averaging the cosine similarities between word embedding and image embeddings"
  - [corpus] Moderate evidence; similarity scores used to classify bias types.
- Break condition: If embeddings poorly represent semantic relationships, bias classification fails.

### Mechanism 3
- Claim: Zero-shot CLIP prompting with B2T keywords accurately infers group labels.
- Mechanism: Prompts like "a photo of a [class] in the [group]" guide CLIP to classify images by inferred bias groups.
- Core assumption: Pre-trained CLIP understands prompts and generalizes to unseen bias keywords.
- Evidence anchors:
  - [abstract] "infer the group labels using a CLIP zero-shot classifier by prompting with B2T keywords"
  - [section] "create the prompts 'a photo of a [group]' and find the nearest prompt for an image to assign the group label"
  - [corpus] Weak evidence; limited comparison to other label inference methods.
- Break condition: If CLIP fails to interpret novel prompts, group label inference fails.

## Foundational Learning

- Concept: Vision-language embeddings and similarity metrics
  - Why needed here: Core to bias-likeness score calculation and keyword validation
  - Quick check question: What does a positive bias-likeness score indicate about a keyword?

- Concept: Text generation and keyword extraction from captions
  - Why needed here: Captions provide interpretable bias keywords from mispredicted images
  - Quick check question: How does YAKE algorithm identify important keywords from captions?

- Concept: Zero-shot classification and prompt engineering
  - Why needed here: Enables bias group label inference without manual annotation
  - Quick check question: How does prompt template design affect CLIP zero-shot classification accuracy?

## Architecture Onboarding

- Component map: Caption generator → Keyword extractor → Bias-likeness calculator → Bias classifier → Label inferencer → Debiasing module
- Critical path: Image → Caption → Keywords → Bias-likeness → Bias type → Application (debiasing, prompting, etc.)
- Design tradeoffs: Accuracy vs. interpretability (keyword vs. embedding analysis), zero-shot vs. fine-tuned approaches
- Failure signatures: Poor captions → noisy keywords; low bias-likeness scores → weak bias evidence; prompt misinterpretation → incorrect labels
- First 3 experiments:
  1. Generate captions for mispredicted images and manually verify keyword relevance
  2. Compute bias-likeness scores for known biases and validate classification accuracy
  3. Test zero-shot group label inference on datasets with ground truth bias annotations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is B2T in discovering biases in specialized domains like medical or satellite imagery, where vision-language models are less effective?
- Basis in paper: [inferred] The paper acknowledges that B2T's effectiveness is limited in specialized domains like medical or satellite imagery, as CLIP and ClipCap are mostly trained on natural images.
- Why unresolved: The paper only briefly mentions this limitation and does not provide experimental results or a detailed analysis of B2T's performance in these domains.
- What evidence would resolve it: Experiments applying B2T to medical or satellite imagery datasets, along with a comparison of results using specialized captioning models for these domains.

### Open Question 2
- Question: Can B2T be extended to generative models to analyze underrepresented or mode-collapsed samples?
- Basis in paper: [explicit] The paper suggests that B2T could be extended to generative models by analyzing low-density samples instead of mispredicted images.
- Why unresolved: The paper does not provide any experimental results or a detailed methodology for applying B2T to generative models.
- What evidence would resolve it: Experiments applying B2T to generative models like diffusion models, VAEs, normalizing flows, autoregressive models, or GANs, and analyzing the discovered biases in the generated samples.

### Open Question 3
- Question: How can B2T be used to create a fairness or bias benchmark, and what would be the key components of such a benchmark?
- Basis in paper: [inferred] The paper mentions that B2T could be used to create a fairness or bias benchmark using the discovered bias keywords, but does not provide details on how this would be implemented.
- Why unresolved: The paper does not provide a concrete methodology or experimental results for creating a bias benchmark using B2T.
- What evidence would resolve it: A detailed methodology for creating a bias benchmark using B2T, along with experimental results comparing the effectiveness of different bias keywords and evaluation metrics.

## Limitations
- The framework's effectiveness depends heavily on the quality of generated captions, which may be inaccurate or irrelevant
- Bias-likeness score calculation assumes CLIP embeddings meaningfully capture semantic relationships without rigorous testing
- Zero-shot label inference approach lacks comparison to alternative bias detection methods
- Limited validation of novel bias discovery capabilities in specialized domains

## Confidence

- High confidence in the framework's ability to identify known biases when ground truth is available
- Medium confidence in novel bias discovery capabilities due to limited external validation
- Low confidence in the generalizability of debiasing improvements across all tested datasets

## Next Checks

1. Evaluate captioning accuracy on mispredicted images across different model architectures and datasets
2. Test bias-likeness score sensitivity to different vision-language model choices and embedding configurations
3. Compare B2T-identified biases against human-annotated bias annotations on multiple datasets to assess discovery accuracy