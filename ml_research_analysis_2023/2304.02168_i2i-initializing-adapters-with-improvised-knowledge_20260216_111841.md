---
ver: rpa2
title: 'I2I: Initializing Adapters with Improvised Knowledge'
arxiv_id: '2304.02168'
source_url: https://arxiv.org/abs/2304.02168
tags:
- task
- adapters
- knowledge
- learning
- adapter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes I2I (Improvise to Initialize), a continual
  learning algorithm for Adapter-based models that initializes new task Adapters by
  distilling knowledge from previously-learned task Adapters. I2I performs three phases:
  1) improvising on the new task by fusing existing task Adapters using AdapterFusion,
  2) initializing a new task Adapter by distilling knowledge from the fused model,
  and 3) training the initialized Adapter on the new task.'
---

# I2I: Initializing Adapters with Improvised Knowledge

## Quick Facts
- arXiv ID: 2304.02168
- Source URL: https://arxiv.org/abs/2304.02168
- Reference count: 4
- Primary result: 4% overall knowledge transfer over vanilla Adapters on CLiMB VQA tasks

## Executive Summary
This paper proposes I2I (Improvise to Initialize), a continual learning algorithm for Adapter-based models that initializes new task Adapters by distilling knowledge from previously-learned task Adapters. The method performs three phases: improvising on the new task by fusing existing task Adapters using AdapterFusion, initializing a new task Adapter by distilling knowledge from the fused model, and training the initialized Adapter on the new task. Experiments on sequences of five visual question answering tasks from the CLiMB benchmark show that I2I achieves 4% overall knowledge transfer over vanilla Adapters while outperforming AdapterFusion without incurring the large parametric cost.

## Method Summary
I2I is a continual learning algorithm for Adapter-based models that uses knowledge distillation to initialize new task Adapters. The method works by first fusing representations from previously-learned Adapters using AdapterFusion during an "Improvise" phase, then distilling this fused knowledge into a new Adapter during an "Initialize" phase, and finally training the initialized Adapter on the new task during a "Train" phase. This approach enables cross-task knowledge transfer while avoiding the parametric cost of AdapterFusion, which adds a Fusion layer for each task. The paper also explores variants that use less training data in the first two phases to reduce training time while still improving over vanilla Adapters.

## Key Results
- I2I achieves 4% overall knowledge transfer over vanilla Adapters on sequences of five VQA tasks from CLiMB
- I2I outperforms AdapterFusion in cross-task knowledge transfer without incurring the associated parametric cost
- Low-shot variants of I2I (using 5% of training data in early phases) reduce training time while still outperforming vanilla Adapters and AdapterFusion

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Initializing new task Adapters with knowledge distilled from a fusion of previous task Adapters results in better cross-task knowledge transfer than post-hoc fusion during inference.
- Mechanism: In the Improvise phase, the model fuses representations from all previously-learned Adapters using a Fusion layer. This fused representation is then used as a teacher model in the Initialize phase to distill knowledge into the new Adapter. By doing this distillation before training on the new task, the new Adapter is better positioned to leverage knowledge from previous tasks.
- Core assumption: Knowledge fusion is more effective when applied during Adapter initialization rather than as a post-hoc transfer learning step after the new Adapter has already converged.
- Evidence anchors:
  - [abstract]: "I2I also results in better cross-task knowledge transfer than the state-of-the-art AdapterFusion without incurring the associated parametric cost."
  - [section]: "We hypothesize that fusing knowledge from existing tasks T1...k-1 will yield better knowledge transfer than Adapter-Fusion's post-hoc knowledge composition."
- Break condition: If the distillation phase fails to adequately transfer knowledge from the fusion to the new Adapter, or if the fused representation itself is not useful for the new task, the initialization will not improve performance.

### Mechanism 2
- Claim: Discarding the Fusion layer after knowledge distillation avoids the parametric cost associated with AdapterFusion while still enabling cross-task knowledge transfer.
- Mechanism: After distilling knowledge from the Fusion layer into the new Adapter in Phase Two, the Fusion layer is discarded. This means that the only additional parameters per task are from the new Adapter, which is typically around 1% of the full Transformer size. This is in contrast to AdapterFusion, which adds a Fusion layer for each task, resulting in a 20-40% parameter increase per task.
- Core assumption: The knowledge transferred during the distillation phase is sufficient for the new Adapter to leverage cross-task knowledge without needing the Fusion layer at inference time.
- Evidence anchors:
  - [abstract]: "I2I also results in better cross-task knowledge transfer than the state-of-the-art AdapterFusion without incurring the associated parametric cost."
  - [section]: "By discarding the AdapterFusion after knowledge distillation, we can avoid the parametric cost while still fusing knowledge learned from previously-seen tasks to enable cross-task knowledge transfer."
- Break condition: If the knowledge transferred during distillation is insufficient for the new Adapter to effectively leverage cross-task knowledge, or if the new Adapter requires the Fusion layer at inference time for optimal performance, then discarding the Fusion layer will degrade performance.

### Mechanism 3
- Claim: Training the new Adapter on a low-shot version of the training data during the Initialize phase reduces training time while still improving performance over vanilla Adapters.
- Mechanism: Instead of using the full training data for the Initialize phase, a low-shot version (e.g., 5% of the training data) is used. This reduces the training time for the Initialize phase, which is the most computationally expensive phase of the I2I algorithm. Despite using less data, the initialized Adapter still performs better than vanilla Adapters that are trained independently on each task.
- Core assumption: Even with limited training data, the knowledge transferred during the distillation phase is sufficient to initialize the new Adapter in a way that improves performance over vanilla Adapters.
- Evidence anchors:
  - [section]: "To mitigate I2I's training time cost, we experiment with variants that do not require the full training data for the Improvise and Initialize phases. These variants reduce the training overhead while outperforming independently-trained Adapters and AdapterFusion."
- Break condition: If the knowledge transferred during the distillation phase is highly sensitive to the amount of training data used, or if the low-shot data is not representative of the full training data, then using a low-shot version for the Initialize phase may degrade performance.

## Foundational Learning

- Concept: Continual Learning (CL) and Catastrophic Forgetting
  - Why needed here: Understanding the challenges of CL, particularly catastrophic forgetting, is crucial for appreciating the motivation behind using Adapters and the I2I algorithm. CL requires a model to learn new tasks sequentially without forgetting previously learned tasks, and Adapters are a solution to this problem.
  - Quick check question: What is catastrophic forgetting, and why is it a problem in continual learning?

- Concept: Adapters and Parameter-Efficient Fine-Tuning
  - Why needed here: Adapters are the core mechanism used in this work to enable parameter-efficient fine-tuning for each task. Understanding how Adapters work and their advantages over full fine-tuning is essential for understanding the I2I algorithm.
  - Quick check question: How do Adapters differ from full fine-tuning, and what are the advantages of using Adapters in continual learning?

- Concept: Knowledge Distillation
  - Why needed here: Knowledge distillation is a key component of the I2I algorithm, used in the Initialize phase to transfer knowledge from the fused model to the new Adapter. Understanding the principles of knowledge distillation and how it can be used for model initialization is important for understanding the I2I algorithm.
  - Quick check question: What is knowledge distillation, and how can it be used to initialize a new model?

## Architecture Onboarding

- Component map:
  Pre-trained Transformer (CLIP-BART) -> Task-specific Adapters -> Visual Projection Layer -> Fusion Layer (Phase One) -> Distillation Loss (Phase Two) -> Task-specific Loss (Phase Three)

- Critical path:
  Phase One: Improvise - Train the Fusion layer and task-specific parameters.
  Phase Two: Initialize - Distill knowledge from the Fusion layer into the new Adapter.
  Phase Three: Train the Adapter - Train the new Adapter on the new task.

- Design tradeoffs:
  Using Adapters vs. full fine-tuning: Adapters are more parameter-efficient but may have lower performance than full fine-tuning.
  Using the full training data vs. a low-shot version: Using the full training data may improve performance but increases training time.
  Discarding the Fusion layer vs. keeping it: Discarding the Fusion layer reduces the parameter count but may slightly degrade performance.

- Failure signatures:
  High distillation decay: Indicates that the knowledge transfer from the Fusion layer to the new Adapter is not effective.
  Negative knowledge transfer: Indicates that the new Adapter is not effectively leveraging knowledge from previous tasks.
  High training time: Indicates that the training time is too high, possibly due to using the full training data for all phases.

- First 3 experiments:
  1. Compare the performance of I2I with vanilla Adapters and AdapterFusion on a simple VQA task sequence.
  2. Evaluate the impact of using the full training data vs. a low-shot version for the Initialize phase.
  3. Measure the distillation decay for different variants of the I2I algorithm.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal training data allocation across the three phases of I2I to maximize knowledge transfer while minimizing training time?
- Basis in paper: [explicit] The paper discusses three variants of I2I with different training data allocations (I2IF F, I2IF L, I2ILL) and notes that I2IF F performs best but requires three full passes over training data.
- Why unresolved: The paper shows that different data allocations affect performance, but doesn't provide a comprehensive analysis of how to optimally allocate limited training resources across phases.
- What evidence would resolve it: Empirical studies comparing different fractions of training data allocation across phases, potentially using a budget-constrained optimization framework.

### Open Question 2
- Question: How does I2I perform on non-VQA tasks or multimodal tasks with different modalities than vision-and-language?
- Basis in paper: [inferred] The paper only evaluates I2I on VQA tasks within the CLiMB benchmark, but the method is presented as a general continual learning algorithm for adapters.
- Why unresolved: The evaluation is limited to one type of task, so it's unclear if the observed benefits generalize to other task types or modalities.
- What evidence would resolve it: Experiments applying I2I to other multimodal tasks (e.g., visual reasoning, visual entailment, or embodied instruction following) and comparing performance to vanilla adapters.

### Open Question 3
- Question: What is the theoretical relationship between AdapterFusion's post-hoc knowledge composition and I2I's knowledge distillation-based initialization?
- Basis in paper: [explicit] The paper hypothesizes that fusing knowledge from existing tasks is more useful for adapter initialization rather than as a post-hoc transfer learning step, but doesn't provide theoretical justification.
- Why unresolved: The paper observes better performance with I2I but doesn't explain why distillation-based initialization outperforms post-hoc fusion at a theoretical level.
- What evidence would resolve it: Theoretical analysis of the optimization landscapes of both approaches, or empirical studies showing the convergence properties of adapters initialized through each method.

## Limitations
- Limited evaluation to VQA tasks only, making generalization to other task types unclear
- Potential sensitivity to hyperparameter choices in the distillation phase that isn't fully explored
- Three-phase training structure may still be computationally expensive despite low-shot variants

## Confidence
- Mechanism 1 (Initialization via knowledge distillation): Medium
- Mechanism 2 (Parametric efficiency through Fusion layer discarding): High
- Mechanism 3 (Low-shot training effectiveness): Medium-High

## Next Checks
1. **Architecture replication**: Implement and compare I2I with the exact AdapterFusion configuration used in Phase 1, verifying that the fusion layer architecture matches the stated design (attention-based cross-attention over adapter outputs).
2. **Hyperparameter sensitivity**: Systematically vary the distillation loss weight and training data fraction in Phase 2 to identify the robustness of performance gains to these choices, particularly measuring the point where gains diminish.
3. **Task diversity testing**: Apply I2I to a new task sequence combining different modalities (e.g., text classification + VQA) to test whether the knowledge transfer benefits generalize beyond the VQA domain used in the paper.