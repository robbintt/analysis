---
ver: rpa2
title: How is ChatGPT's behavior changing over time?
arxiv_id: '2307.09009'
source_url: https://arxiv.org/abs/2307.09009
tags:
- gpt-4
- june
- march
- gpt-3
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how the behavior of GPT-3.5 and GPT-4
  changes over time by evaluating their performance on four tasks: math problems,
  sensitive questions, code generation, and visual reasoning. The authors find substantial
  performance drifts between the March and June 2023 versions of both models.'
---

# How is ChatGPT's behavior changing over time?

## Quick Facts
- arXiv ID: 2307.09009
- Source URL: https://arxiv.org/abs/2307.09009
- Authors: 
- Reference count: 3
- Key outcome: GPT-3.5 and GPT-4 show substantial performance drifts between March and June 2023 versions across math, sensitive questions, code generation, and visual reasoning tasks

## Executive Summary
This paper investigates how GPT-3.5 and GPT-4 behavior changes over time by evaluating their performance on four distinct tasks between March and June 2023 versions. The authors find dramatic performance variations, with GPT-4's prime number accuracy plummeting from 97.6% to 2.4% while GPT-3.5's improved from 7.4% to 86.8%. GPT-4 also became less willing to answer sensitive questions and more prone to formatting errors in code generation. The findings highlight that "the same" model can change substantially in a short time period, underscoring the need for continuous monitoring of LLM behavior.

## Method Summary
The study evaluates GPT-3.5 and GPT-4 performance drift between March and June 2023 versions using four task-specific datasets: 500 prime number identification questions, 100 sensitive queries, 50 LeetCode problems, and 467 ARC visual reasoning samples. Performance is measured through accuracy, answer rates, executable code rates, and exact match rates, with additional metrics including verbosity and answer overlap. The evaluation relies on OpenAI API access but lacks model version identifiers, limiting the ability to confirm whether observed changes stem from updates to the same model or different model instances.

## Key Results
- GPT-4's prime number accuracy dropped from 97.6% to 2.4% between March and June 2023
- GPT-3.5's accuracy improved from 7.4% to 86.8% on the same task
- GPT-4 became less willing to answer sensitive questions and more terse in responses
- Code generation quality degraded with GPT-4's executable rate falling from 50% to 10%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal version drift in LLMs causes observable changes in model behavior on specific tasks.
- Mechanism: The same LLM service may be updated between March and June 2023, leading to changes in accuracy, formatting, or safety behaviors.
- Core assumption: Updates to model weights, prompts, or safety layers are applied without public notification.
- Evidence anchors: GPT-4's accuracy on prime number identification dropped from 97.6% to 2.4%, while GPT-3.5's accuracy improved from 7.4% to 86.8%.

### Mechanism 2
- Claim: Safety updates can cause a model to refuse more questions while also becoming less verbose.
- Mechanism: Introduction of a stricter safety layer led to fewer direct answers to sensitive prompts and shorter refusals.
- Core assumption: The safety filter is updated independently of core reasoning capabilities.
- Evidence anchors: GPT-4 answered fewer sensitive questions in June than in March and became more terse, with generation length decreasing from more than 600 to about 140 characters.

### Mechanism 3
- Claim: Code generation quality can degrade due to formatting changes rather than logic errors.
- Mechanism: The June 2023 version added extra text around code, making it non-executable even if logic is correct.
- Core assumption: Formatting artifacts are added by the model or post-processing pipeline.
- Evidence anchors: GPT-4's generations in March and June were almost the same except for extra triple quotes in June that rendered code non-executable.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Used to elicit reasoning steps in math tasks; changes in adherence affect accuracy.
  - Quick check question: What happens to GPT-4's math accuracy when it stops following chain-of-thought in June?

- Concept: Jailbreaking attacks
  - Why needed here: Measures robustness of safety filters over time; used to compare March vs. June behavior.
  - Quick check question: How does GPT-4's answer rate to AIM attacks change from March to June?

- Concept: Exact match evaluation
  - Why needed here: Used for visual reasoning; measures whether generated outputs match ground truth exactly.
  - Quick check question: What is the exact match rate for GPT-4 on the ARC dataset in June 2023?

## Architecture Onboarding

- Component map: OpenAI API -> prompt template -> evaluation dataset -> metric calculator -> drift comparison
- Critical path: Prompt generation -> API call -> result parsing -> metric extraction -> version comparison
- Design tradeoffs: Using a single dataset per task vs. comprehensive benchmarking; simplicity vs. completeness
- Failure signatures: Drop in accuracy without prompt change; formatting changes causing execution failure; safety filter becoming stricter
- First 3 experiments:
  1. Re-run the prime number task with chain-of-thought prompt on both versions and compare step-by-step outputs
  2. Test the same sensitive question with and without the AIM jailbreak attack on both versions
  3. Generate code for a LeetCode problem and check executability and formatting in both versions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM performance drifts vary across different model architectures and training approaches?
- Basis in paper: The paper evaluates only GPT-3.5 and GPT-4, noting that these are the two most widely used LLM services.
- Why unresolved: The study focuses on a narrow set of models from a single provider, limiting generalizability to other architectures like BERT, T5, or open-source alternatives.
- What evidence would resolve it: Systematic monitoring of LLM performance drifts across diverse model families, training methods, and providers.

### Open Question 2
- Question: What is the relationship between model size and susceptibility to performance drift?
- Basis in paper: The paper evaluates two models without explicitly analyzing the impact of model size on drift.
- Why unresolved: The study does not isolate the effect of model size on drift behavior.
- What evidence would resolve it: Comparative analysis of performance drift across models of varying sizes.

### Open Question 3
- Question: How do different types of updates (e.g., safety improvements, capability enhancements) contribute to performance drift?
- Basis in paper: The paper observes that GPT-4 became less willing to answer sensitive questions and more prone to formatting errors.
- Why unresolved: The study does not distinguish between the effects of different update types on drift.
- What evidence would resolve it: Detailed documentation and analysis of model updates and their impact on performance.

### Open Question 4
- Question: What is the long-term impact of performance drift on the reliability of LLM-based applications?
- Basis in paper: The paper highlights the need for continuous monitoring due to substantial performance drifts.
- Why unresolved: The study focuses on short-term drift without assessing cumulative effects on application reliability.
- What evidence would resolve it: Long-term monitoring of LLM performance drift and its impact on real-world applications.

## Limitations

- The study lacks access to model version identifiers, making it impossible to confirm whether observed changes stem from updates to the same model or different model instances.
- Evaluation was limited to just two time points (March and June 2023), preventing analysis of continuous drift patterns.
- The study does not investigate potential confounding factors such as prompt engineering differences or changes in evaluation methodology.

## Confidence

- High Confidence: Measurable performance differences exist between March and June 2023 versions across all four tasks
- Medium Confidence: GPT-4's accuracy on prime number identification dramatically declined (from 97.6% to 2.4%)
- Low Confidence: GPT-4's ability to follow user instructions has definitively decreased over time

## Next Checks

1. **Controlled Instruction-Following Test**: Design and execute a systematic evaluation of GPT-4's instruction-following capabilities using identical prompts across multiple versions, measuring both explicit compliance and implicit behavior changes.

2. **Continuous Monitoring Protocol**: Implement a daily evaluation pipeline tracking the same tasks to identify when specific behavioral changes occur and whether they represent gradual drift or discrete updates.

3. **Version Identification Analysis**: Develop methods to extract or infer model version information from API responses and correlate observed behavioral changes with known OpenAI update schedules or model identifier patterns.