---
ver: rpa2
title: 'With a Little Help from the Authors: Reproducing Human Evaluation of an MT
  Error Detector'
arxiv_id: '2308.06527'
source_url: https://arxiv.org/abs/2308.06527
tags:
- original
- errors
- translation
- were
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reproduces the human evaluation experiment of Vamvas
  and Sennrich (2022) on detecting over- and undertranslations in machine translation.
  Using the same data, interface, and guidelines, two new annotators evaluated approximately
  700 examples each.
---

# With a Little Help from the Authors: Reproducing Human Evaluation of an MT Error Detector

## Quick Facts
- arXiv ID: 2308.06527
- Source URL: https://arxiv.org/abs/2308.06527
- Reference count: 5
- Primary result: Reproduction of human evaluation of MT error detection showed lower precision and inter-annotator agreement than original study, highlighting challenges in reproducing human evaluation tasks

## Executive Summary
This study attempts to reproduce the human evaluation experiment of Vamvas and Sennrich (2022) on detecting over- and undertranslations in machine translation. Using the same dataset, annotation interface, and guidelines, two new annotators evaluated approximately 700 examples each. While overall conclusions align with the original study, significant differences emerged: inter-annotator agreement for fine-grained responses was notably lower, and precision for undertranslations was 44-46% lower than reported originally. The study highlights that despite high-quality documentation, reproducing human evaluation is more challenging than automatic metrics due to higher variability and interface dependencies.

## Method Summary
The reproduction followed the original study's methodology by using the same dataset of English-German MT outputs with highlighted spans indicating potential errors. Two German native speakers with high English proficiency annotated the data using a Doccano-based interface. The task required annotators to evaluate whether highlighted spans were correctly translated and select reasons from a predefined list. The original annotation interface and dataset were provided by the authors, with some technical adjustments needed to resolve Docker compatibility issues. Results were analyzed using a corrected evaluation script that addressed a bug in handling multiple spans per sentence.

## Key Results
- Inter-annotator agreement for fine-grained responses was significantly lower than in the original study
- Precision for undertranslations was 44-46% lower than originally reported, with statistically significant differences
- Annotators in the reproduction selected "unknown reasons" for correctly translated text spans about four times more often than in the original study
- Overall precision remained comparable to the original study despite these differences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reproducing human evaluation is more challenging than reproducing automatic metrics because human judgment introduces higher variability and requires careful interface replication.
- Mechanism: Human evaluation involves subjective interpretation of translation quality, which varies across annotators due to differences in linguistic background, interpretation of guidelines, and interface usability.
- Core assumption: The original study's high-quality documentation and code are sufficient for exact replication.
- Evidence anchors:
  - [abstract] "Despite the high quality of the documentation and code provided by the authors, we discuss some problems we found in reproducing the exact experimental setup"
  - [section 4] "we would not have been able to run the annotation interface and repeat the study without the authors' help"
- Break condition: If annotators lack similar linguistic backgrounds or if interface issues prevent consistent task execution, reproducibility suffers significantly.

### Mechanism 2
- Claim: Binary question setups are easier to reproduce than complex multi-option evaluations due to lower cognitive load and clearer decision criteria.
- Mechanism: When annotators must choose from many possible reasons for their evaluation, the decision space expands, increasing the likelihood of inconsistent interpretations and responses.
- Core assumption: Complex evaluation tasks with multiple choice options introduce more variability than simple binary judgments.
- Evidence anchors:
  - [abstract] "experiments with binary questions are easier to replicate than complex setups with many possible answers"
  - [section 7] "our annotators selected unknown reasons for highlighting a correctly translated text span about four times more often than in the original study"
- Break condition: When annotators face complex decision trees or ambiguous category definitions, reproducibility drops substantially.

### Mechanism 3
- Claim: Technical dependencies and software updates can break reproducibility even when code and documentation are provided.
- Mechanism: The original study used a specific version of Doccano that became incompatible with newer versions, requiring Docker snapshots or direct author assistance to reproduce.
- Core assumption: Software environments remain stable over time and across different systems.
- Evidence anchors:
  - [section 4] "The original open-source software has been updated over time, making the authors' customization incompatible with the toolkit"
  - [section 4] "Even after downgrading Doccano to the version used by the authors, some of the dependencies were found to be no longer available"
- Break condition: When key dependencies are deprecated or when software ecosystems evolve rapidly, exact reproduction becomes nearly impossible without archived environments.

## Foundational Learning

- Concept: Inter-annotator agreement metrics (Cohen's κ)
  - Why needed here: To quantify the consistency between different annotators' judgments, which is crucial for assessing reproducibility of human evaluation
  - Quick check question: If two annotators agree on 80% of binary decisions, what would Cohen's κ be approximately? (Answer: depends on expected agreement by chance, but would be moderate to high)

- Concept: Statistical significance testing
  - Why needed here: To determine whether observed differences between original and reproduced results are meaningful or due to random variation
  - Quick check question: If precision values differ by 44% between studies, what statistical test would determine if this difference is significant? (Answer: Two-proportion z-test or chi-square test)

- Concept: Docker containerization for reproducible environments
  - Why needed here: To preserve exact software dependencies and configurations that might otherwise become incompatible over time
  - Quick check question: What advantage does distributing annotation interfaces as Docker images provide for future reproducibility attempts? (Answer: Ensures exact software environment regardless of host system changes)

## Architecture Onboarding

- Component map: Data layer (source sentences, MT outputs, error annotations) -> Interface layer (Doccano-based annotation tool, Dockerized) -> Processing layer (evaluation script with bug fixes for multi-span handling) -> Analysis layer (statistical comparison scripts with confidence intervals)

- Critical path: 1. Set up Docker environment with original annotation interface 2. Load original dataset and split for annotators 3. Conduct annotation following original guidelines 4. Process annotations with corrected evaluation script 5. Compare results statistically with original study

- Design tradeoffs: Using Docker ensures environment consistency but requires technical setup; multiple-choice options provide detailed feedback but reduce reproducibility; splitting data between annotators enables comparison but reduces individual sample size

- Failure signatures: Annotation interface fails to load -> Check Docker dependencies and compatibility; Statistical scripts produce errors -> Verify data format matches expected structure; Results significantly different from original -> Examine annotator background differences and interface usability

- First 3 experiments: 1. Test annotation interface with small subset of data to verify setup 2. Run evaluation script on original data to confirm bug fix works 3. Compare a small set of annotations between original and reproduced to check for systematic differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific factors contributed to the significant difference in undertranslation precision between the original study and the reproduction?
- Basis in paper: [explicit] The paper notes that the reproduction showed 44-46% lower precision for undertranslations compared to the original study, with statistically significant differences
- Why unresolved: The paper identifies the discrepancy but doesn't definitively explain why annotators in the reproduction were more likely to mark undertranslations as correct translations with unknown reasons, or why they found more accuracy/fluency errors
- What evidence would resolve it: Comparative analysis of annotator training protocols, detailed demographic analysis of annotator backgrounds, or A/B testing with modified annotation interfaces that explicitly address undertranslation detection challenges

### Open Question 2
- Question: Would using annotators from the same institution and field as the original study eliminate the reproducibility differences observed?
- Basis in paper: [inferred] The paper notes that their annotators were from different German universities than the original study and came from different academic backgrounds (public health vs. NLP)
- Why unresolved: While the paper identifies these differences, it doesn't experimentally test whether matching these characteristics would eliminate the observed discrepancies
- What evidence would resolve it: Direct comparison of results using annotators from the same institution, same field, and same German dialect as the original study

### Open Question 3
- Question: How does the complexity of annotation interfaces affect reproducibility in human evaluation studies?
- Basis in paper: [explicit] The paper discusses significant challenges with the Doccano interface and recommends distributing annotation interfaces as Docker images, noting that "experiments with binary questions are easier to replicate than complex setups"
- Why unresolved: While the paper identifies interface challenges, it doesn't systematically test how different interface complexities affect reproducibility outcomes
- What evidence would resolve it: Controlled experiments comparing reproducibility across studies with varying interface complexities, from simple binary questions to multi-option selections like in the original study

## Limitations
- Unable to obtain exact demographic information of original annotators (university, field of study)
- Potential dialectal differences in German (Swiss vs. standard German) between annotator groups
- Small sample size (700 examples per annotator) limits statistical power for detecting smaller effects

## Confidence
- High confidence: Core finding that human evaluation is more difficult to reproduce than automatic metrics due to higher variability and interface dependencies
- Medium confidence: Specific precision differences reported (44-46% lower for undertranslations), as these could be influenced by unmeasured annotator characteristics
- Low confidence: Whether all technical issues have been fully resolved, given the multiple Docker and dependency challenges encountered

## Next Checks
1. Conduct a controlled study varying only annotator demographics (linguistic background, field of study) while keeping all other factors constant to isolate their impact on reproducibility.

2. Perform think-aloud protocols with new annotators using both the original and reproduced interfaces to identify specific usability differences that might affect judgment consistency.

3. Re-run the annotation task with the same annotators after a significant time interval (3-6 months) to assess intra-annotator reliability and determine whether observed variability is primarily between or within annotators.