---
ver: rpa2
title: 'Detecting Severity of Diabetic Retinopathy from Fundus Images: A Transformer
  Network-based Review'
arxiv_id: '2301.00973'
source_url: https://arxiv.org/abs/2301.00973
tags:
- image
- severity
- transformer
- diabetic
- fundus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the automated detection of diabetic retinopathy
  (DR) severity stages from fundus images. It proposes an ensemble of four transformer-based
  models (ViT, BEiT, CaiT, and DeiT) to capture crucial features of retinal images
  for understanding DR severity.
---

# Detecting Severity of Diabetic Retinopathy from Fundus Images: A Transformer Network-based Review

## Quick Facts
- arXiv ID: 2301.00973
- Source URL: https://arxiv.org/abs/2301.00973
- Reference count: 40
- Primary result: Ensemble of four transformer models achieves 94.63% accuracy and 0.92 kappa score on APTOS-2019 DR severity detection

## Executive Summary
This paper addresses automated detection of diabetic retinopathy (DR) severity stages from fundus images using transformer-based models. The authors propose an ensemble approach combining four transformer variants (ViT, BEiT, CaiT, and DeiT) to capture diverse features of retinal images. The ensemble model achieves state-of-the-art performance with 94.63% accuracy and 0.92 kappa score on the APTOS-2019 dataset. The study demonstrates that weighted mean combination schemes outperform majority voting and highlights the effectiveness of transformer architectures for medical image classification tasks.

## Method Summary
The method employs an ensemble of four transformer models (ViT, BEiT, CaiT, DeiT) trained on the APTOS-2019 fundus image dataset. Images are preprocessed to 256x256 resolution with data augmentation including center cropping, flips, rotations, brightness/contrast adjustments, and CLAHE. Each transformer is fine-tuned with specific hyperparameters (transformer_layers=12, embedding_dimension=384, num_heads=6, initial_learning_rate=10^-3). The ensemble combines softmax outputs using weighted mean (αj values) and majority voting schemes. Evaluation uses accuracy, kappa score, precision, recall, F1-score, specificity, and balanced accuracy metrics.

## Key Results
- Ensemble achieves 94.63% accuracy and 0.92 kappa score on APTOS-2019 test set
- Weighted mean combination scheme outperforms majority voting
- Performance exceeds several state-of-the-art deep learning methods
- Model shows strong performance across all evaluation metrics including precision, recall, and F1-score

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ensembling multiple transformer models improves accuracy by combining complementary feature representations.
- Mechanism: Different transformer variants extract features using distinct attention mechanisms and training strategies. Weighted averaging of softmax outputs produces more robust predictions than individual models.
- Core assumption: Transformer variants capture different aspects of retinal features with uncorrelated errors.
- Evidence anchors: Abstract states weighted mean outperforms majority voting; section describes softmax probability combination.
- Break condition: If transformer variants learn highly correlated features, ensemble gains diminish.

### Mechanism 2
- Claim: BEiT's masked image modeling pre-training captures richer visual token representations.
- Mechanism: BEiT learns to predict masked patches by reconstructing visual tokens from a VAE, forcing understanding of global context and fine-grained details.
- Core assumption: Retinal images contain meaningful patterns in masked regions that benefit from self-supervised learning.
- Evidence anchors: Section describes BEiT's masked patch prediction; abstract implies enhanced feature extraction.
- Break condition: If masked patches contain mostly uninformative background, pre-training benefit reduces.

### Mechanism 3
- Claim: CaiT's class-attention layers improve classification by focusing attention on class-relevant features.
- Mechanism: CaiT separates patch self-attention from class-attention, reducing interference between patch relationships and class aggregation.
- Core assumption: DR-relevant features are distributed across patches and benefit from hierarchical attention.
- Evidence anchors: Section explains CaiT architecture separation; abstract supports focused feature extraction.
- Break condition: If DR-relevant features are localized rather than distributed, class-attention may not help.

## Foundational Learning

- Concept: Multi-head self-attention
  - Why needed here: Transformers use MSA to weigh relationships between image patches, capturing complex patterns in retinal images.
  - Quick check question: How does MSA differ from single-head attention in processing retinal image patches?

- Concept: Ensemble methods (weighted mean vs majority voting)
  - Why needed here: Understanding how different combination schemes affect final predictions is crucial for optimizing the ensemble.
  - Quick check question: Why might weighted mean outperform majority voting for this multi-class DR severity task?

- Concept: Pre-training strategies (supervised vs self-supervised)
  - Why needed here: Different transformers use different pre-training (BEiT uses MIM, others use supervised), affecting their feature representations.
  - Quick check question: What advantage might BEiT's masked image modeling have over standard supervised pre-training for retinal images?

## Architecture Onboarding

- Component map: Data preprocessing -> Individual transformer training -> Ensemble combination -> Evaluation
- Critical path: Load APTOS-2019 → Resize to 256x256 → Augment → Train transformers → Combine softmax outputs → Evaluate metrics
- Design tradeoffs:
  - Weighted mean vs majority voting: Weighted mean better but requires tuning weights
  - Number of transformer heads: Performance peaks at 6 heads, degrades beyond
  - Pre-training data: Adding MESSIDOR/IDRiD data slightly improves performance
- Failure signatures:
  - Accuracy plateaus below 90%: Likely overfitting or insufficient model capacity
  - High precision but low recall for severe stages: Model may be conservative in predicting severe DR
  - Majority voting outperforms weighted mean: Transformers may have correlated errors
- First 3 experiments:
  1. Train individual transformers (ViT, BEiT, CaiT, DeiT) on APTOS-2019 and compare baseline accuracies
  2. Implement weighted mean ensemble with equal weights and measure improvement
  3. Tune ensemble weights (αj) to maximize accuracy and observe weight distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of ensemble transformers compare to other advanced transformer-based models like CoT-XNet and SSiT when applied to diabetic retinopathy severity detection?
- Basis in paper: The paper mentions CoT-XNet and SSiT as recently published transformer-based models and compares their performance to the ensemble transformers.
- Why unresolved: The paper does not provide a detailed comparison of the ensemble transformers with CoT-XNet and SSiT on the same dataset or metrics.
- What evidence would resolve it: A detailed comparative study on the same dataset and metrics would clarify the relative performance of ensemble transformers versus CoT-XNet and SSiT.

### Open Question 2
- Question: What are the specific contributions of each transformer model (ViT, DeiT, CaiT, BEiT) in the ensemble to the overall performance, and how does their combination affect the detection of different severity stages?
- Basis in paper: The paper discusses the individual roles of ViT, DeiT, CaiT, and BEiT but does not explicitly analyze their individual contributions to the ensemble's performance.
- Why unresolved: The paper does not provide a detailed analysis of how each transformer model contributes to the ensemble's performance, especially in detecting different severity stages.
- What evidence would resolve it: An ablation study or performance analysis of each transformer model individually within the ensemble would clarify their specific contributions.

### Open Question 3
- Question: How does the imbalance in the dataset affect the performance of the ensemble transformers, and what strategies could be implemented to mitigate this issue?
- Basis in paper: The paper mentions that the dataset is imbalanced and discusses data augmentation as a strategy.
- Why unresolved: The paper does not explore other strategies to address dataset imbalance, such as oversampling, undersampling, or class-specific adjustments.
- What evidence would resolve it: An exploration of different strategies to handle dataset imbalance and their impact on model performance would provide insights into effective mitigation techniques.

## Limitations
- Unknown ensemble weight values (αj) make exact reproduction challenging
- Performance comparison lacks clarity on whether other methods used same dataset and preprocessing
- Study focuses solely on APTOS-2019 dataset, limiting generalizability to other fundus image datasets

## Confidence

| Claim | Confidence |
|-------|------------|
| Weighted mean ensemble outperforms individual transformers and majority voting | High |
| BEiT's masked image modeling provides richer feature representations | Medium |
| CaiT's class-attention layers improve classification | Medium |

## Next Checks

1. Implement ablation studies comparing individual transformer performances with and without pre-training to quantify BEiT's contribution.
2. Test the ensemble with different weight configurations (including uniform weights) to verify the sensitivity of the weighted mean scheme.
3. Validate model performance on additional fundus image datasets (e.g., MESSIDOR, IDRiD) to assess generalizability beyond APTOS-2019.