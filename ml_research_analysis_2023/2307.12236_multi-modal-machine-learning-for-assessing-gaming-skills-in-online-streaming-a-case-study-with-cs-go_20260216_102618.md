---
ver: rpa2
title: 'Multi-Modal Machine Learning for Assessing Gaming Skills in Online Streaming:
  A Case Study with CS:GO'
arxiv_id: '2307.12236'
source_url: https://arxiv.org/abs/2307.12236
tags:
- video
- data
- videos
- audio
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the task of assessing gaming skills in online
  streaming, specifically focusing on the game Counter-Strike: Global Offensive (CS:GO).
  The authors propose a multi-modal machine learning approach that leverages vision,
  audio, and text modalities to predict the skill level of gamers based on their Twitch.tv
  streaming videos.'
---

# Multi-Modal Machine Learning for Assessing Gaming Skills in Online Streaming: A Case Study with CS:GO

## Quick Facts
- arXiv ID: 2307.12236
- Source URL: https://arxiv.org/abs/2307.12236
- Reference count: 12
- The proposed multi-modal models achieve precision of 0.446, recall of 0.523, and F1-score of 0.477 on CS:GO skill assessment

## Executive Summary
This paper proposes a multi-modal machine learning approach to assess gaming skills from Twitch streaming videos, specifically focusing on Counter-Strike: Global Offensive (CS:GO). The authors leverage vision, audio, and text modalities to predict player skill levels, using a modified Lipreading model for joint video-audio representation learning. The approach outperforms baseline models by incorporating semantic video views and chat data as a prior, though performance is limited by dataset size and class imbalance.

## Method Summary
The core method involves learning joint representations of video and audio modalities using a modified Lipreading model with spatio-temporal CNNs and bidirectional GRUs. Video frames are processed at 1 FPS after downsampling, while audio features are extracted as 20 Mel-frequency cepstral coefficients. The authors investigate KL divergence loss between video and audio feature distributions to improve audio representation quality, and experiment with semantic views of video frames (minimap, health bar, weapon selection) as separate inputs. Chat data is incorporated as a user-level prior through a bidirectional LSTM, though this only improves performance for the top two skill levels.

## Key Results
- Multi-modal fusion outperforms single modality baselines for skill assessment
- KL divergence loss between video and audio branches improves audio feature quality by approximately 2.5% over unimodal models
- Incorporating chat data as a prior improves performance only for the top two skill levels
- Best model achieves precision of 0.446, recall of 0.523, and F1-score of 0.477

## Why This Works (Mechanism)

### Mechanism 1
Multi-modal fusion outperforms single modality baselines for skill assessment by learning joint representations through bidirectional GRUs that capture cross-modal dependencies predictive of player skill. This works because video and audio streams contain complementary information about player behavior that cannot be captured by either modality alone. Evidence shows the best model combining video and audio modalities with KL divergence loss achieves the highest performance metrics. If cross-modal correlations are weak or redundant, simple concatenation would suffice and multi-modal fusion would not improve performance.

### Mechanism 2
KL divergence loss between video and audio branches improves audio feature quality by forcing the audio branch to produce features that align with the more informative video branch, reducing modality bias. This works because video features are assumed to be more informative than audio features, and guiding audio learning via KL loss can improve overall representation. Evidence shows introducing KL loss improves model performance by approximately 2.5% over unimodal models. If video and audio are highly correlated already, KL divergence loss may over-constrain the model and hurt performance.

### Mechanism 3
Text prior improves model performance by providing user-level context through aggregated chat data processed by bidirectional LSTM to predict user rank, allowing the model to incorporate long-term user behavior patterns. This works because chat activity is assumed to correlate with player skill and provides complementary signals to video and audio. Evidence shows incorporating chat data as a prior can improve performance, but only for the top two skill levels. If chat data is noisy or not representative of player skill, incorporating it may introduce harmful bias or overfitting.

## Foundational Learning

- **Multi-modal representation learning**: Why needed - The task requires integrating vision, audio, and text modalities to assess player skill from Twitch streams. Quick check - What is the difference between early fusion and late fusion in multi-modal learning?
- **Cross-entropy loss and classification metrics**: Why needed - The model uses cross-entropy loss for training and precision, recall, F1-score for evaluation due to class imbalance. Quick check - How does class imbalance affect precision and recall differently than accuracy?
- **Bidirectional GRU and temporal feature extraction**: Why needed - Video and audio are sequential data, and bidirectional GRUs capture temporal dependencies in both directions for better feature representation. Quick check - Why might a bidirectional RNN be preferred over a unidirectional one for this task?

## Architecture Onboarding

- **Component map**: Video stream: 3D CNN → ResNet → Bi-GRU; Audio stream: MFCC extraction → ResNet → Bi-GRU; Text prior: Bi-LSTM → FC layer → concatenation with multimodal features; KL divergence loss: between video and audio feature distributions; Multiview extraction: separate pipelines for minimap, health bar, weapon selection, etc.
- **Critical path**: 1) Preprocess video and audio into frames and audio features; 2) Extract video and audio features through individual pipelines; 3) Apply KL divergence loss to align video and audio feature spaces; 4) Concatenate features and pass through final Bi-GRU and FC layer; 5) Compute predictions and KL loss, backpropagate through all branches
- **Design tradeoffs**: Using raw video frames vs semantic views - raw frames capture more detail but require more computation; semantic views are more targeted but may lose context. Text prior vs aligned chat - prior is easier to implement but may miss temporal alignment benefits. KL divergence loss - improves audio feature quality but adds training complexity and may over-constrain learning
- **Failure signatures**: High variance between video-based and user-based splits indicates the model is learning user identity instead of skill-related features. Poor performance on minority classes suggests the model is overfitting to majority classes or missing discriminative features. Large drop in performance with video masking indicates the model is relying on edge regions rather than semantically meaningful areas
- **First 3 experiments**: 1) Train and evaluate unimodal video-only model to establish baseline; 2) Train and evaluate unimodal audio-only model to compare modality importance; 3) Train and evaluate multi-modal baseline (simple concatenation) to measure benefit of cross-modal integration

## Open Questions the Paper Calls Out
1. How can we improve the quality and size of the dataset to reduce class imbalance and overfitting issues? The paper identifies dataset size and quality as major limitations that contribute to class imbalance and overfitting problems, but doesn't provide specific strategies to address this beyond acknowledging it as a limitation.
2. How can we effectively utilize the text modality, which is not temporally aligned with the video data, to improve the model's performance? The authors found that chat data as a prior only worked for top skill levels and failed to predict videos from rank sections C and D, without explaining why or proposing alternative methods.
3. How can we address the issue of the model learning user identity instead of meaningful representations of gaming skills? The paper identifies that models are prone to identifying users instead of learning skill-related features, suggesting this as future work without providing specific solutions.

## Limitations
- Class imbalance (56% majority class) makes it difficult to assess true model performance beyond top skill levels
- Small dataset (1620 videos from 268 users) may limit generalizability
- Focus on only CS:GO restricts applicability to other games and streaming platforms

## Confidence
- **High Confidence**: Multi-modal models outperforming unimodal baselines is well-supported by experimental results
- **Medium Confidence**: Chat data improving performance only for top skill levels is supported but based on limited data points
- **Low Confidence**: Generalizability to other games or streaming platforms is not established

## Next Checks
1. Cross-game validation: Test the same models on streams from different competitive games (e.g., Dota 2, Valorant) to assess generalizability beyond CS:GO
2. User-level robustness analysis: Conduct thorough analysis of video-based vs user-based splits to quantify extent of user identity leakage versus skill-based learning
3. Ablation on class imbalance: Re-run key experiments with class-balanced training to determine if observed performance gains persist when minority classes receive adequate representation