---
ver: rpa2
title: 'Gaze-Informed Vision Transformers: Predicting Driving Decisions Under Uncertainty'
arxiv_id: '2308.13969'
source_url: https://arxiv.org/abs/2308.13969
tags:
- fixation
- attention
- human
- maps
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces fixation-attention intersection (FAX) loss,
  a novel method that integrates human eye-tracking data into Vision Transformers
  (ViTs) to improve driving decision prediction under uncertainty. The method leverages
  the similarity between human fixation maps and ViT attention weights, using this
  overlap to guide model training.
---

# Gaze-Informed Vision Transformers: Predicting Driving Decisions Under Uncertainty

## Quick Facts
- arXiv ID: 2308.13969
- Source URL: https://arxiv.org/abs/2308.13969
- Reference count: 40
- One-line primary result: FAX loss improves ViT accuracy by up to 20.2% under high uncertainty and enables model pruning without accuracy loss

## Executive Summary
This study introduces the fixation-attention intersection (FAX) loss, a novel method that integrates human eye-tracking data into Vision Transformers to improve driving decision prediction under uncertainty. The approach leverages the similarity between human fixation maps and ViT attention weights to guide model training, resulting in improved accuracy and reduced training epochs. The method also enables model pruning by identifying layers with minimal fixation-attention overlap, reducing model size without compromising accuracy.

## Method Summary
The method combines a baseline Vision Transformer architecture with a custom FAX loss function that quantifies the intersection between model attention weights and human fixation maps. Training uses a weighted combination of binary cross-entropy loss and the FAX loss component, with the λ hyperparameter balancing their contributions. The approach was tested on two driving datasets with varying uncertainty conditions, comparing baseline ViT performance against models trained with FAX loss.

## Key Results
- FAX loss improved ViT accuracy by up to 20.2% in high-uncertainty conditions compared to baseline models
- Models trained with FAX loss required fewer training epochs to converge
- Layer pruning based on fixation-attention overlap reduced model depth by half (6-layer vs 12-layer) without accuracy loss
- The approach successfully transferred human visual attention strategies to machine learning models for complex visual tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Human fixation maps help ViT models resolve uncertainty by guiding attention to task-relevant regions rather than dispersing attention across the entire frame.
- Mechanism: ViTs naturally attend to more regions of a scene than humans, especially under high uncertainty. By incorporating human fixation data, the model is steered to focus on the same spatial regions humans consider important, effectively mimicking human visual attention strategies that minimize local uncertainty through focused fixations.
- Core assumption: Human fixation patterns during decision-making tasks reflect task-relevant information and can be used to guide machine attention effectively.
- Evidence anchors:
  - [abstract] "By comparing the similarity between human fixation maps and ViT attention weights, we reveal the dynamics of overlap across individual heads and layers. This overlap demonstrates that fixation data can guide the model in distributing its attention weights more effectively."
  - [section] "In contrast, the total number of fixations and edges varies by uncertainty condition... the model does not employ a different strategy in its attention overall."
  - [corpus] Weak - no direct evidence found in corpus papers about uncertainty-specific attention guidance.
- Break condition: If fixation maps do not correlate with task-relevant regions (e.g., due to fatigue, distraction, or task-irrelevant fixations), the guiding signal becomes noisy and degrades model performance.

### Mechanism 2
- Claim: The FAX loss function improves ViT accuracy by explicitly aligning model attention weights with human fixation maps during training.
- Mechanism: The FAX loss quantifies the intersection (dot product) between ViT attention weights and human fixation maps, transforming this into a differentiable signal. During training, this encourages the model to attend to the same regions humans fixate on, effectively transferring human visual expertise into the model's decision process.
- Core assumption: The dot product between attention weights and fixation maps is a meaningful similarity metric that correlates with improved task performance.
- Evidence anchors:
  - [abstract] "We introduce the fixation-attention intersection (FAX) loss, a novel loss function that significantly improves ViT performance under high uncertainty conditions."
  - [section] "Finally, we define LF AX by combining LIN T with the original classification loss LBCE."
  - [corpus] Weak - corpus contains gaze-guided models but no direct mention of FAX-like intersection loss mechanisms.
- Break condition: If the fixation map resolution or timing does not align well with model patch embeddings, the intersection signal becomes meaningless and training may diverge.

### Mechanism 3
- Claim: Model pruning based on fixation-attention overlap reduces model size without sacrificing accuracy by removing layers whose attention patterns minimally overlap with human fixations.
- Mechanism: By computing similarity between each layer's attention weights and human fixation maps, layers with low overlap can be identified and pruned. This exploits the fact that some layers may capture edge or low-level features irrelevant to the task, while human fixations already encode task-relevant spatial priorities.
- Core assumption: Layers with minimal fixation overlap contribute less to task performance and can be removed without degradation.
- Evidence anchors:
  - [section] "Using a qualitatively-determined cutoff, we identify layers with minimal overlap and prune these layers to study the impact on model performance."
  - [section] "The average accuracy for DR(eye)VE is greater when using the 6-layer ViT... than using the 12-layer ViT... despite decreasing the depth of the model, therefore the number of parameters, in half."
  - [corpus] Weak - corpus contains gaze-aware pruning ideas but not fixation-guided layer pruning specifically.
- Break condition: If the overlap metric does not capture true task relevance (e.g., if early layers encode essential low-level features), pruning may remove critical representational capacity.

## Foundational Learning

- Concept: Vision Transformers (ViTs) and their self-attention mechanism
  - Why needed here: The paper builds directly on ViT architecture, modifying attention and loss functions; understanding patch embeddings, multi-head self-attention, and layer stacking is essential to follow the methods.
  - Quick check question: How does ViT's self-attention differ from CNNs in terms of receptive field and context integration?

- Concept: Human visual attention and fixation mapping
  - Why needed here: Fixation maps are central to the proposed methods; understanding how eye-tracking data is aggregated and aligned with image frames is necessary to interpret JSF and FAX.
  - Quick check question: Why are fixation maps smoothed over the premotor period, and how does this affect the input to the model?

- Concept: Loss function engineering and multi-task learning
  - Why needed here: The FAX loss combines classification loss with fixation-attention alignment; understanding weighted loss combinations and gradient flow is crucial for implementation.
  - Quick check question: What is the role of the λ hyperparameter in balancing LBCE and LIN T, and how would you tune it?

## Architecture Onboarding

- Component map: RGB frame -> patch tokenization -> CLS token + positional embeddings -> Multi-head self-attention (MSA) layers -> CLS token -> classification head -> binary cross-entropy loss -> FAX loss (attention-fixation intersection)

- Critical path:
  1. Preprocess frame and fixation map into compatible patch embeddings
  2. Forward through ViT to get attention maps and CLS output
  3. Compute FAX loss (intersection between attention weights and fixation map)
  4. Backpropagate combined loss (LBCE + λ·LIN T)
  5. Optionally prune layers with low fixation overlap

- Design tradeoffs:
  - JSF vs FAX: JSF augments input with fixation data; FAX guides training via loss. JSF may require architectural changes; FAX is plug-and-play.
  - Peripheral masking: Reduces input size and focuses on foveated regions but may lose context; trade-off between speed and completeness.
  - Layer pruning: Reduces parameters and computation but risks removing useful low-level features if overlap metric is noisy.

- Failure signatures:
  - Low overlap between attention and fixation maps despite FAX training → loss weighting (λ) too low or fixation data misaligned
  - Model accuracy drops after pruning → overlap metric not capturing true task relevance
  - JSF degrades performance → fixation map resolution mismatches patch size or temporal misalignment

- First 3 experiments:
  1. Train baseline ViT on driving dataset; measure accuracy and attention-fixation overlap.
  2. Implement FAX loss with λ=0.2; compare accuracy and convergence speed to baseline.
  3. Compute layer-wise fixation-attention overlap; prune lowest-overlap layers; evaluate accuracy retention.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the FAX loss function's performance compare when applied to different transformer architectures beyond ViT, such as Swin Transformer or DeiT?
- Basis in paper: [explicit] The paper focuses on ViT but suggests broader implications for human-guided transformer design
- Why unresolved: The study only tested FAX on ViT architecture, leaving performance on other transformer variants unknown
- What evidence would resolve it: Direct comparison of FAX performance across multiple transformer architectures on the same driving datasets

### Open Question 2
- Question: What is the minimum required eye-tracking data quality and sampling rate for FAX loss to effectively improve model performance?
- Basis in paper: [inferred] The study uses specific eye-tracking setups but doesn't explore how variations in tracking quality affect results
- Why unresolved: Different eye-tracking systems were used (HTC VIVE Pro Eye vs SMI ETG 2w) without systematic analysis of tracking quality impact
- What evidence would resolve it: Controlled experiments varying eye-tracking resolution, sampling rate, and noise levels to determine minimum quality thresholds

### Open Question 3
- Question: How does FAX loss perform in driving scenarios with non-visual uncertainty factors, such as auditory cues or tactile feedback?
- Basis in paper: [explicit] The study focuses on visual uncertainty through fog density and contrast variations
- Why unresolved: The current implementation only integrates visual fixation data without considering other sensory modalities that affect driving decisions
- What evidence would resolve it: Testing FAX on datasets that include multimodal sensory inputs and comparing performance with and without integration of non-visual cues

### Open Question 4
- Question: What is the optimal layer selection strategy for model pruning beyond the fixation-attention overlap criterion used in this study?
- Basis in paper: [explicit] The paper uses fixation-attention overlap to prune layers 7-12 but acknowledges this as a qualitative selection
- Why unresolved: The study only explores one pruning criterion and doesn't compare alternative strategies or validate against other metrics
- What evidence would resolve it: Systematic comparison of layer pruning performance using different selection criteria (e.g., attention entropy, gradient magnitude, or task relevance)

## Limitations
- Reliance on fixation maps as proxies for task-relevant attention may not always correlate with optimal model attention, especially in scenarios with high cognitive load
- The assumption that attention-fixation overlap directly translates to improved performance lacks robust ablation studies isolating component effects
- Relatively small dataset sizes (6,006 and 728 frames) may limit generalizability to other domains

## Confidence
- **High Confidence**: The improvement in accuracy under high uncertainty conditions when using FAX loss is well-supported by experimental results across both datasets
- **Medium Confidence**: The claim that human fixation patterns guide the model to resolve uncertainty by focusing attention is plausible but not rigorously proven
- **Low Confidence**: The effectiveness of JSF versus FAX is not clearly differentiated, and the optimal λ value for FAX loss is not systematically explored across tasks

## Next Checks
1. **Ablation Study**: Conduct an ablation study isolating the effects of FAX loss, JSF input, and peripheral masking on model performance and uncertainty handling
2. **Cross-Domain Generalization**: Test the FAX-trained model on a different visual task (e.g., object detection or action recognition) to assess the generalizability of fixation-guided attention
3. **Robustness to Fixation Noise**: Evaluate model performance when fixation maps are corrupted with noise or temporal misalignment to quantify the sensitivity to fixation quality