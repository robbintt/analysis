---
ver: rpa2
title: 'CharSpan: Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation
  for Extremely Low-resource Languages'
arxiv_id: '2305.05214'
source_url: https://arxiv.org/abs/2305.05214
tags:
- noise
- languages
- language
- translation
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of building machine translation
  systems for extremely low-resource languages (ELRLs) by leveraging their lexical
  similarity with high-resource languages (HRLs). The authors propose a novel method
  called CharSpan, which injects character-span noise into the training data of HRLs
  to make the model robust to lexical divergences with ELRLs.
---

# CharSpan: Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages

## Quick Facts
- arXiv ID: 2305.05214
- Source URL: https://arxiv.org/abs/2305.05214
- Authors: 
- Reference count: 40
- This paper addresses the challenge of building machine translation systems for extremely low-resource languages (ELRLs) by leveraging their lexical similarity with high-resource languages (HRLs).

## Executive Summary
This paper introduces CharSpan, a novel approach for zero-shot machine translation of extremely low-resource languages by leveraging lexical similarity with high-resource languages. The method injects character-span noise into HRL training data to create robustness to lexical divergences between related languages. CharSpan significantly outperforms strong baselines across three diverse language families, achieving average improvements of 22.52% BLEU and 9.46% chrF scores. The approach demonstrates that carefully designed noise injection can enable effective cross-lingual transfer even with minimal parallel data for target languages.

## Method Summary
CharSpan injects character-span noise (1-3 character deletions/replacements) into HRL training data (9-11% of characters) before BPE vocabulary learning. This serves as regularization, making models robust to lexical divergences between HRLs and ELRLs. The method uses a 6-layer transformer (46M parameters) trained on noisy HRL-English pairs, then evaluated zero-shot on ELRL-English translation using FLORES-200. The approach compares against baselines including vanilla NMT, Overlap BPE, BPE-Dropout, and unigram character noise.

## Key Results
- CharSpan achieves 22.52% BLEU and 9.46% chrF improvements over baseline NMT models for ELRLs
- Span-level noise injection outperforms unigram character noise, especially for languages with moderate lexical similarity
- Pre-vocabulary noise injection provides slightly better results than post-vocabulary injection
- Method shows consistent gains across Indo-Aryan, Italic, and Malay-Polynesian language families

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Character-span noise injection regularizes the model to be robust to lexical divergences between HRLs and ELRLs.
- Mechanism: By injecting random character deletions and replacements spanning 1-3 characters across 9-11% of text, the model learns to tolerate spelling variations typical between related languages.
- Core assumption: Lexical similarity implies surface-level orthographic variation that can be simulated as noisy text.
- Evidence anchors:
  - [abstract]: "This serves as a regularization technique, making the model more robust to 'lexical divergences' between the HRL and ELRL..."
  - [section]: "Our hypothesis is that words with closely related languages have similar spellings and can be thought of as text in HRL with spelling errors."
  - [corpus]: Weak evidence - corpus only shows related papers, no direct validation of spelling-variation regularization.
- Break condition: If the languages are not lexically similar (LCSR low), noise injection provides minimal benefit; performance degrades toward baseline.

### Mechanism 2
- Claim: Noise injection prior to BPE vocabulary learning improves cross-lingual alignment compared to post-vocabulary injection.
- Mechanism: Introducing noise before subword tokenization ensures that orthographic variants are merged into the same BPE tokens, increasing shared vocabulary overlap between HRL and ELRL.
- Core assumption: Shared subword tokens improve cross-lingual transfer by aligning embeddings.
- Evidence anchors:
  - [abstract]: "We injected the noise prior to learning vocabulary."
  - [section]: "Our results indicate that the incorporation of noise, whether applied prior to or after learning the vocabulary, leads to improved performance... applying noise prior to learning the vocabulary using BPE yields slightly better results."
  - [corpus]: Weak - corpus neighbors discuss lexical sharing but not timing of noise injection.
- Break condition: If vocabulary overlap is already high (>90%), timing has diminishing returns.

### Mechanism 3
- Claim: Span-level noise is more effective than unigram character noise for languages with moderate lexical similarity.
- Mechanism: Span noise models longer orthographic differences (e.g., consonant cluster changes) that characterize inter-language variation better than single-character edits.
- Core assumption: Longer character spans capture structural differences in related languages better than single-character noise.
- Evidence anchors:
  - [abstract]: "We propose a novel character-span noise augmentation approach that outperforms just unigram character-level noise, particularly when there exists a comparatively less lexical similarity between HRL and LRL."
  - [section]: "Our proposed char-span noise provides significant improvements over unigram character noise. It also provides gains for languages like Konkani which are not as lexically similar to the HRLs as other languages."
  - [corpus]: No direct corpus evidence supporting span-level superiority.
- Break condition: If languages are extremely similar (high LCSR), unigram noise may suffice; span noise adds unnecessary complexity.

## Foundational Learning

- Concept: Cross-lingual transfer through shared embedding spaces
  - Why needed here: The method relies on transferring linguistic knowledge from HRLs to ELRLs via learned representations.
  - Quick check question: What property of multilingual models enables knowledge transfer between related languages?

- Concept: Subword tokenization (BPE) and vocabulary overlap
  - Why needed here: The effectiveness depends on shared subword units between HRL and ELRL; noise injection aims to increase this overlap.
  - Quick check question: How does the BPE algorithm decide which character sequences become shared tokens?

- Concept: Lexical similarity measurement (LCSR, cognates)
  - Why needed here: The method assumes lexical similarity drives effectiveness; understanding LCSR helps predict model performance.
  - Quick check question: What does a high LCSR score indicate about the relationship between two languages?

## Architecture Onboarding

- Component map:
  HRL parallel corpus -> Noise injection (9-11%, 1-3 char spans) -> BPE tokenization -> Training data -> 6-layer transformer (46M params) -> Zero-shot evaluation on ELRL-English

- Critical path:
  1. Noise injection on source-side HRL training data
  2. BPE vocabulary learning from noisy data
  3. Transformer training on (noisy HRL, English) pairs
  4. Zero-shot translation of ELRL to English

- Design tradeoffs:
  - Noise level (9-11%) vs. data quality: Higher noise risks degrading HRL performance
  - Span length (1-3 chars) vs. computational overhead: Longer spans model more variation but increase complexity
  - Pre-vocab vs. post-vocab noise: Pre-vocab improves alignment but requires retraining; post-vocab allows fine-tuning existing models

- Failure signatures:
  - HRL BLEU drops >5 points: Noise level too high
  - ELRL performance near baseline: Languages not lexically similar enough
  - Model divergence during training: Noise injection breaking linguistic structure

- First 3 experiments:
  1. Baseline: Train transformer on clean HRL data, evaluate zero-shot on ELRL
  2. Unigram noise: Apply character-level noise to HRL data, train, evaluate
  3. Char-span noise: Apply span-level noise to HRL data, train, evaluate
     - Compare BLEU/chrf gains over baseline for each experiment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed character-span noise injection technique generalize to translation tasks beyond English, such as English-to-ELRL translation or translation between two ELRLs?
- Basis in paper: [inferred] The paper focuses exclusively on zero-shot ELRL to English translation and does not explore other translation directions or language pairs.
- Why unresolved: The current study is limited to a specific translation direction, leaving the effectiveness of the method in other settings unexplored.
- What evidence would resolve it: Experimental results comparing the method's performance on English-to-ELRL and ELRL-to-ELRL translation tasks against established baselines.

### Open Question 2
- Question: How does the performance of the proposed method scale with increasing amounts of parallel training data for HRLs?
- Basis in paper: [inferred] The paper evaluates the method on three datasets of varying sizes (13.6M, 11M, and 0.8M sentences) but does not systematically investigate the impact of dataset size on performance.
- Why unresolved: The study does not provide a clear understanding of how the method's effectiveness changes as the amount of available training data increases.
- What evidence would resolve it: A controlled experiment varying the size of the HRL parallel training data while keeping other factors constant, and measuring the corresponding impact on ELRL translation quality.

### Open Question 3
- Question: Is the proposed method robust to different types of lexical divergences between HRLs and ELRLs, such as grammatical differences or differences in word order?
- Basis in paper: [inferred] The paper focuses on lexical similarity and spelling variations but does not explicitly address other types of linguistic differences between related languages.
- Why unresolved: The study does not investigate the method's ability to handle grammatical or syntactic divergences, which could also impact cross-lingual transfer.
- What evidence would resolve it: An analysis of the method's performance on ELRLs with varying degrees of grammatical and syntactic similarity to their corresponding HRLs, compared to languages with only lexical differences.

## Limitations

- Method effectiveness is fundamentally constrained by lexical similarity between HRLs and ELRLs
- Performance gains are unevenly distributed across language families, with stronger results for Indo-Aryan languages
- Exact implementation details for noise injection and transformer architecture remain underspecified

## Confidence

- High: The general approach of using lexical similarity for zero-shot transfer is valid and well-established in the literature
- Medium: The specific implementation details of CharSpan noise injection and its comparative advantage over unigram noise
- Low: Claims about span-level noise superiority and the exact mechanisms by which pre-vocab noise injection improves cross-lingual alignment

## Next Checks

1. **Noise sensitivity analysis**: Systematically vary the noise injection percentage (5%, 9%, 15%) and span lengths (1-2, 1-3, 2-3 characters) to determine optimal parameters and identify the break point where HRL performance degrades while ELRL benefits plateau.

2. **Cross-linguistic generalization**: Test CharSpan on language pairs with explicitly measured LCSR scores below 0.7 to quantify the relationship between lexical similarity and transfer effectiveness, particularly focusing on cases where the method fails.

3. **Ablation on vocabulary overlap**: Measure BPE vocabulary intersection between HRL and ELRL before and after noise injection to directly validate the claimed mechanism of improved subword alignment through shared token representation.