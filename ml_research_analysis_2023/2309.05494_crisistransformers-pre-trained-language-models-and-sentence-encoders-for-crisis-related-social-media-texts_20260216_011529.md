---
ver: rpa2
title: 'CrisisTransformers: Pre-trained language models and sentence encoders for
  crisis-related social media texts'
arxiv_id: '2309.05494'
source_url: https://arxiv.org/abs/2309.05494
tags:
- sentence
- pre-trained
- datasets
- training
- tweets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of analyzing crisis-related
  social media texts by introducing CrisisTransformers, an ensemble of pre-trained
  language models and sentence encoders trained on over 15 billion word tokens from
  crisis-related tweets. The models were trained on tweets from more than 30 crisis
  events, including disease outbreaks, natural disasters, and conflicts.
---

# CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts

## Quick Facts
- arXiv ID: 2309.05494
- Source URL: https://arxiv.org/abs/2309.05494
- Reference count: 40
- One-line primary result: Domain-specific pre-training improves crisis-related text classification by 17.43% over general models

## Executive Summary
This study introduces CrisisTransformers, an ensemble of pre-trained language models and sentence encoders specifically trained on crisis-related social media texts. The models were trained on over 15 billion word tokens from more than 30 crisis events across disease outbreaks, natural disasters, and conflicts. When evaluated on 18 crisis-specific datasets, CrisisTransformers consistently outperformed existing models in classification tasks, with the best-performing sentence encoder achieving a 17.43% improvement in sentence encoding tasks. The models are publicly available for research use.

## Method Summary
The researchers pre-trained transformer-based models (MPNet, BERTweet, BERT, RoBERTa, XLM-RoBERTa, ALBERT, ELECTRA) and three variants of CrisisTransformers (CT-M1, CT-M2, CT-M3) on a corpus of over 15 billion word tokens from crisis-related tweets. They then fine-tuned these models on 18 labeled crisis datasets for classification tasks using a linear prediction layer. Additionally, they trained sentence encoders using siamese and triplet networks with contrastive objectives (MNR and MNR with hard negatives) on datasets like GooAQ, QQP, and AllNLI. Performance was evaluated using F1-macro scores for classification and weighted average cosine similarity for sentence encoding tasks.

## Key Results
- CrisisTransformers outperformed strong baselines across all 18 datasets in classification tasks
- The best-performing sentence encoder achieved a 17.43% improvement in sentence encoding tasks
- CT-M1-Complete-SE (hard negatives) achieved an average score of 0.7140, surpassing the state-of-the-art by 12%
- Increasing training samples from 10k to 102k QQP pairs improved performance by 4.83%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pre-training on crisis-related tweets improves classification accuracy by 17.43% compared to general-purpose models.
- Mechanism: The model learns crisis-specific linguistic patterns (e.g., hashtags, informal language, acronyms) from a large corpus of 15B tokens, enabling better generalization to unseen crisis events.
- Core assumption: Crisis-related social media texts contain unique linguistic features that general-purpose models fail to capture effectively.
- Evidence anchors:
  - [abstract]: "Our pre-trained models outperform strong baselines across all datasets in classification tasks, and our best-performing sentence encoder improves the state-of-the-art by 17.43% in sentence encoding tasks."
  - [section]: "Results confirm that CrisisTransformers can capture distinct linguistic nuances, informal language structures, and unique contextual cues present in crisis contexts."
  - [corpus]: Weak evidence. Corpus details (15B tokens, 30+ crisis events) are provided, but specific linguistic features are not analyzed.
- Break condition: If the model is evaluated on non-crisis or formal text, the performance advantage may diminish.

### Mechanism 2
- Claim: Pre-trained models initialized with domain-specific weights converge faster and achieve lower loss than randomly initialized models.
- Mechanism: Initializing with pre-trained weights (RoBERTa or BERTweet) provides a strong starting point, reducing the number of training steps needed to reach optimal performance.
- Core assumption: Pre-trained weights contain useful linguistic knowledge that accelerates learning on related tasks.
- Evidence anchors:
  - [abstract]: "We investigate the impact of model initialization on convergence..."
  - [section]: "CT-M2 and CT-M3 exhibited rapid initial drops in loss; CT-M3 later aligned with CT-M1 in terms of final loss. All models plateaued, implying convergence."
  - [corpus]: Weak evidence. Loss curves are described, but specific convergence metrics (e.g., epochs to plateau) are not provided.
- Break condition: If the task domain is vastly different from the pre-training data, initialization may not provide significant benefits.

### Mechanism 3
- Claim: Fine-tuning with contrastive objectives (e.g., MNR with hard negatives) generates semantically meaningful sentence embeddings for crisis texts.
- Mechanism: The model learns to position semantically similar sentences closer in vector space by optimizing on (anchor, positive, hard negative) triplets.
- Core assumption: Crisis-related sentences with similar meanings can be identified and used as training pairs/triplets.
- Evidence anchors:
  - [abstract]: "Our best-performing sentence encoder improves the state-of-the-art by 17.43% in sentence encoding tasks."
  - [section]: "Our sentence encoders outperform both Sentence-Transformers and SimCSE... CT-M1-Complete-SE (hard negatives)... achieved an average score of 0.7140, surpassing the current state-of-the-art by 12%."
  - [corpus]: Weak evidence. Training data (QQP, AllNLI) is mentioned, but specific crisis-related sentence pairs are not analyzed.
- Break condition: If the training data lacks sufficient semantically similar crisis sentence pairs, the model may not learn meaningful embeddings.

## Foundational Learning

- Concept: Transformer-based language models (e.g., BERT, RoBERTa)
  - Why needed here: CrisisTransformers builds on these architectures to leverage their contextual embedding capabilities for crisis texts.
  - Quick check question: What is the key difference between BERT and RoBERTa in terms of pre-training objectives?

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is used to pre-train CrisisTransformers, enabling the model to predict masked tokens based on context.
  - Quick check question: How does MLM differ from autoregressive language modeling?

- Concept: Contrastive learning for sentence embeddings
  - Why needed here: Fine-tuning with contrastive objectives (e.g., MNR) generates semantically meaningful embeddings for crisis texts.
  - Quick check question: What is the role of hard negatives in contrastive learning?

## Architecture Onboarding

- Component map: Byte-Pair Encoding tokenizer -> RoBERTa architecture -> MLM objective -> Linear prediction layer (classification) -> Siamese/triplet networks (sentence encoding)

- Critical path:
  1. Pre-train CrisisTransformers on crisis corpus
  2. Fine-tune for classification on 18 datasets
  3. Fine-tune for sentence encoding with contrastive objectives
  4. Evaluate performance

- Design tradeoffs:
  - Larger corpus vs. training time: 15B tokens required 2 months on 6 GPUs
  - Vocabulary size: 64k tokens chosen to balance coverage and efficiency
  - Training objectives: MNR vs. MNR with hard negatives for sentence encoding

- Failure signatures:
  - Poor classification performance: Model may not capture crisis-specific linguistic patterns
  - Low cosine similarity scores: Sentence encoder may not learn semantic relationships
  - High loss during pre-training: Hyperparameters (e.g., learning rate) may need adjustment

- First 3 experiments:
  1. Fine-tune CT-M1 on a small crisis dataset (e.g., D-01) and compare F1-macro to RoBERTa baseline
  2. Train CT-M1-SE with MNR objective on 10k GooAQ pairs and evaluate on D-01
  3. Increase training samples to 102k QQP and re-evaluate sentence encoding performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of model initialization impact pre-training in terms of loss convergence?
- Basis in paper: [explicit] The paper investigates this by comparing CT-M1 (random initialization), CT-M2 (RoBERTa initialization), and CT-M3 (BERTweet initialization), observing different loss convergence patterns.
- Why unresolved: While the paper observes distinct convergence behaviors, it does not provide a comprehensive analysis of why certain initializations lead to faster or more efficient convergence.
- What evidence would resolve it: A detailed ablation study varying initialization strategies and analyzing their impact on training dynamics and final performance would provide insights into optimal initialization methods for domain-specific pre-training.

### Open Question 2
- Question: To what extent do domain-specific pre-trained models help generate sentence embeddings with semantic richness, in comparison to current pre-trained models and sentence encoders?
- Basis in paper: [explicit] The paper explicitly addresses this by comparing CrisisTransformers-based sentence encoders with existing models like Sentence-Transformers and SimCSE, showing improvements in generating semantically meaningful embeddings.
- Why unresolved: The study uses a limited number of training samples (10k) compared to existing models trained on billions of samples, leaving uncertainty about the scalability of performance gains.
- What evidence would resolve it: Training CrisisTransformers-based sentence encoders on a scale comparable to Sentence-Transformers (over 1 billion samples) would determine if the performance advantage persists and quantify the extent of improvement.

### Open Question 3
- Question: Can the performance gains achieved by increasing training data size lead to substantial improvements in domain-specific sentence encoding tasks?
- Basis in paper: [explicit] The paper explores this by increasing training samples from 10k to 378k and observing a 4.83% improvement in performance.
- Why unresolved: The study does not investigate the upper limits of performance gains or identify the point of diminishing returns for training data size.
- What evidence would resolve it: A systematic study varying training data sizes and measuring performance would reveal the relationship between data size and model effectiveness, identifying optimal training data requirements.

## Limitations

- The evaluation relies on cosine similarity metrics that may not fully capture practical utility in real-world crisis response scenarios
- The specific linguistic features learned by the models are not empirically analyzed, making it difficult to determine whether improvements stem from crisis-specific patterns or general pre-training effects
- Performance advantages may diminish when evaluated on non-crisis or formal text, as the model's strength appears tied to informal social media language patterns

## Confidence

**High Confidence:** The models are publicly available and can be reproduced using standard Transformer architectures with the specified pre-training objectives (MLM) and fine-tuning procedures.

**Medium Confidence:** The reported F1-macro improvements over baselines and the 17.43% improvement in sentence encoding tasks are plausible given the domain-specific pre-training, but require independent validation on additional crisis datasets.

**Low Confidence:** The specific convergence benefits of domain-specific initialization and the claim that models capture "distinct linguistic nuances" are not rigorously quantified beyond loss curves and qualitative statements.

## Next Checks

1. **Out-of-Domain Generalization Test:** Evaluate CrisisTransformers on a non-crisis Twitter dataset (e.g., general sentiment analysis) to quantify performance degradation and validate the domain-specific advantage claim.

2. **Convergence Speed Analysis:** Measure and compare the number of training epochs and wall-clock time required for CrisisTransformers versus RoBERTa to reach equivalent F1-macro scores across the 18 datasets, providing quantitative evidence for the initialization claim.

3. **Linguistic Feature Analysis:** Conduct a qualitative analysis of the learned embeddings to identify and validate whether crisis-specific linguistic patterns (e.g., hashtags, acronyms, informal language) are indeed captured, supporting the mechanism claim.