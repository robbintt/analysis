---
ver: rpa2
title: The Paradigm Shifts in Artificial Intelligence
arxiv_id: '2308.02558'
source_url: https://arxiv.org/abs/2308.02558
tags:
- paradigm
- data
- such
- knowledge
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper provides a historical overview of paradigm shifts in
  Artificial Intelligence (AI) using Kuhn's framework. It traces the evolution from
  Expert Systems to Machine Learning to Deep Learning and now to General Intelligence
  with pre-trained models like GPT-3.
---

# The Paradigm Shifts in Artificial Intelligence

## Quick Facts
- arXiv ID: 2308.02558
- Source URL: https://arxiv.org/abs/2308.02558
- Authors: 
- Reference count: 40
- Key outcome: The paper provides a historical overview of paradigm shifts in AI using Kuhn's framework, tracing evolution from Expert Systems to Machine Learning to Deep Learning and now to General Intelligence with pre-trained models like GPT-3.

## Executive Summary
This paper analyzes the evolution of Artificial Intelligence through the lens of Thomas Kuhn's paradigm shift framework, identifying four major transitions in AI approaches. The current paradigm shift is characterized by large pre-trained models like GPT-3 that can understand and generate human-like text across diverse domains, integrating expertise, common sense, and tacit knowledge simultaneously. This represents a fundamental shift from narrow, task-specific applications to a general-purpose technology with potentially broad economic impacts.

## Method Summary
The paper provides a conceptual/historical analysis rather than an empirical study, using Kuhn's framework to trace AI's evolution from Expert Systems to Machine Learning to Deep Learning and now to General Intelligence. It synthesizes insights from 40 references to identify forces driving each paradigm shift and the challenges faced by each approach. The analysis focuses on identifying the exemplar models and prototypical problems that define each paradigm.

## Key Results
- Pre-trained models like GPT-3 represent a paradigm shift by enabling seamless knowledge transfer across tasks without explicit task-specific training
- The shift addresses the "knowledge engineering bottleneck" by learning from raw data rather than requiring human-specified rules
- Scaling laws suggest performance improves with increased model complexity, data size, and compute power, though fundamental aspects of intelligence remain mysterious

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large pre-trained models represent a fundamental shift in AI by enabling seamless knowledge transfer across tasks, moving AI from narrow applications to a general-purpose technology.
- Mechanism: Pre-trained models learn statistical representations of the world from vast amounts of diverse data, allowing them to generalize across domains without explicit task-specific training. This shifts the exemplar from task-specific rules to a universal function approximator that can be applied to novel situations.
- Core assumption: The ability to learn from raw data directly, without human-engineered features, is the key enabler of this paradigm shift.
- Evidence anchors:
  - [abstract] "These models integrate expertise, common sense, and tacit knowledge simultaneously, enabling seamless knowledge transfer across tasks."
  - [section] "The latest paradigm shift makes AI a general purpose technology and a commodity, one that should keep improving in terms of quality with increasing amounts of data and computing power."
  - [corpus] Found related papers discussing the transformation of scientific discovery through AI and the shift towards general-purpose technologies, supporting the claim of a fundamental change in AI capabilities.
- Break condition: If the model's performance plateaus despite increases in data and compute, or if the inability to explain or control the model's behavior becomes a critical barrier to adoption, the paradigm shift may not be sustainable.

### Mechanism 2
- Claim: The shift to pre-trained models addresses the "knowledge engineering bottleneck" by eliminating the need for human experts to manually specify rules and relationships.
- Mechanism: By learning from vast amounts of data, pre-trained models can discover complex relationships and patterns that would be impractical or impossible for humans to specify explicitly. This automates the knowledge acquisition process, making it scalable and efficient.
- Core assumption: The data available on the internet is rich and diverse enough to capture the necessary knowledge for general intelligence.
- Evidence anchors:
  - [section] "The prototypical exemplar for representing knowledge in this paradigm were symbolic relationships expressed in the form of 'IF/THEN' rules... But it was difficult to express uncertainty in terms of these representations... The major hurdle of this paradigm of top-down knowledge specification was the 'knowledge engineering bottleneck.'"
  - [section] "The new generation of algorithms could learn them from data using optimization... This enabled data-driven scientific discovery."
  - [corpus] Weak evidence: Related papers discuss the use of large language models in scientific research, but do not directly address the knowledge engineering bottleneck.
- Break condition: If the quality or diversity of available data proves insufficient for the model to learn the necessary knowledge, or if the model's learned representations are found to be fundamentally flawed or biased, the approach may fail.

### Mechanism 3
- Claim: The scaling of pre-trained models, in terms of model complexity, data size, and compute power, leads to improved performance and the emergence of new capabilities.
- Mechanism: As the model size and training data increase, the model can capture more complex patterns and relationships, leading to better performance on a wide range of tasks. This is supported by empirical evidence of scaling laws in AI.
- Core assumption: The scaling laws observed in current models will continue to hold as models become larger and more complex.
- Evidence anchors:
  - [section] "One such principle in the area of LLMs is that performance improves by increasing model complexity, data size, and compute power across a wide range of tasks... These 'scaling laws of AI' indicate that predictive accuracy on the autocompletion task improves with increased compute power, model complexity, and data."
  - [section] "The exemplar in General Intelligence paradigm is the deep neural network, whose properties we now trying to understand, along with the general principles that underpin their performance."
  - [corpus] Weak evidence: Related papers discuss the scaling of AI models and their impact on scientific discovery, but do not provide direct evidence of scaling laws.
- Break condition: If the scaling laws break down at larger scales, or if the emergence of new capabilities is found to be an artifact of the evaluation method rather than a genuine improvement in intelligence, the scaling hypothesis may be invalid.

## Foundational Learning

- Concept: Kuhn's framework of scientific progress and paradigm shifts
  - Why needed here: The paper uses Kuhn's framework to analyze the historical evolution of AI and the current paradigm shift. Understanding this framework is crucial for interpreting the paper's argument.
  - Quick check question: What are the key characteristics of a paradigm shift according to Kuhn, and how do they apply to the evolution of AI?

- Concept: Expert systems and symbolic AI
  - Why needed here: The paper traces the history of AI from expert systems to machine learning to deep learning and now to general intelligence. Understanding the limitations of expert systems is key to understanding why the paradigm shifted.
  - Quick check question: What were the main challenges faced by expert systems, and how did these challenges lead to the development of machine learning?

- Concept: Deep learning and neural networks
  - Why needed here: The current paradigm shift is driven by advances in deep learning, particularly large language models. Understanding the basics of neural networks and how they differ from previous approaches is essential.
  - Quick check question: How do deep neural networks differ from traditional machine learning models, and what advantages do they offer in terms of learning from raw data?

## Architecture Onboarding

- Component map: Raw data from various sources (text, images, audio) -> Large neural networks (transformer architectures) -> Optimization algorithms (training) -> Predictions/Generation (inference) -> Performance assessment (evaluation)
- Critical path: Data -> Model Architecture -> Training -> Inference -> Evaluation
  - Ensuring high-quality, diverse data is crucial for training effective models.
  - Choosing the right model architecture and training hyperparameters is essential for good performance.
  - Efficient inference and evaluation are important for practical deployment.
- Design tradeoffs:
  - Model size vs. efficiency: Larger models generally perform better but require more computational resources.
  - Data quantity vs. quality: More data can improve performance, but ensuring data quality and diversity is crucial.
  - Interpretability vs. performance: More complex models may be harder to interpret but can achieve better results.
- Failure signatures:
  - Poor performance on held-out data: Indicates overfitting or insufficient model capacity.
  - Inconsistent or biased outputs: Suggests issues with the training data or model architecture.
  - Difficulty in explaining model decisions: Highlights the challenge of interpreting complex neural networks.
- First 3 experiments:
  1. Train a small transformer model on a text classification task to understand the basics of the architecture and training process.
  2. Experiment with different model sizes and training data sizes to observe the impact on performance and identify scaling trends.
  3. Evaluate the model's ability to generalize to unseen tasks or domains to assess its capacity for knowledge transfer.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the scaling limits of large language models and their ability to achieve general intelligence?
- Basis in paper: [explicit] The paper discusses scaling laws that suggest performance improves with increased model complexity, data size, and compute power. However, it notes that fundamental aspects of intelligence may remain mysterious even with larger models.
- Why unresolved: While current scaling laws show promising results, there may be inherent limits to how much performance can be improved by simply making models larger. The paper acknowledges that "fundamental aspects of intelligence are still mysterious" and unlikely to be answered solely by scaling existing models.
- What evidence would resolve it: Empirical studies showing performance plateaus at certain model sizes, or theoretical work identifying fundamental limitations in current architectures that cannot be overcome by scaling alone.

### Open Question 2
- Question: Can the trust and alignment problems of AI systems be solved within the current paradigm, or will a new paradigm be required?
- Basis in paper: [explicit] The paper identifies trust (including issues of hallucination, bias, and unpredictability) and alignment (ensuring AI goals match human values) as "the most pressing" challenges facing current AI systems.
- Why unresolved: The paper notes that these problems stem from the opaque, distributed nature of knowledge representation in neural networks, making it difficult to ensure AI systems behave as intended and align with human values.
- What evidence would resolve it: Successful demonstrations of AI systems that are both highly capable and fully trustworthy, with verifiable alignment between their objectives and human values, would suggest current paradigms can address these issues. Conversely, persistent failures despite significant research efforts would indicate a need for new approaches.

### Open Question 3
- Question: How will the economic and societal impacts of AI as a general-purpose technology compare to previous transformative technologies like electricity and the internet?
- Basis in paper: [explicit] The paper discusses AI as potentially becoming a general-purpose technology with "far-reaching impacts on society, economics, and humanity – potentially exceeding that of other general purpose technologies such as electric power and the Internet."
- Why unresolved: While the paper identifies AI's potential for pervasiveness and innovation complementarities, it acknowledges that realizing productivity gains from general-purpose technologies often requires significant complementary investments and organizational changes that take time to materialize.
- What evidence would resolve it: Long-term economic data showing the net impact of AI on productivity, innovation, and economic growth compared to historical general-purpose technologies, as well as evidence of how effectively societies adapt to and benefit from AI adoption.

## Limitations

- The claim that pre-trained models represent genuine general intelligence versus sophisticated pattern matching lacks systematic empirical validation
- The paper doesn't adequately address the energy costs and environmental impact of scaling these massive models
- Limited quantitative evidence demonstrating that these models have achieved genuine knowledge transfer across domains

## Confidence

**High Confidence**: The historical analysis of previous paradigm shifts (Expert Systems → Machine Learning → Deep Learning) is well-supported by the literature and accurately captures the progression of AI approaches.

**Medium Confidence**: The characterization of pre-trained models as enabling "general intelligence" represents the most contested claim, with ongoing debate about whether they constitute genuine intelligence or sophisticated statistical pattern matching.

**Low Confidence**: The prediction that this paradigm will continue scaling indefinitely and become a general-purpose technology is highly speculative, with current scaling laws potentially breaking down at larger scales.

## Next Checks

1. **Cross-domain reasoning validation**: Design a benchmark suite that tests whether pre-trained models can truly transfer knowledge across domains (e.g., applying physics knowledge to solve chemistry problems) versus simply recognizing surface patterns.

2. **Scaling law stress test**: Conduct systematic experiments to determine whether current scaling laws hold as models exceed 100B parameters and are trained on datasets larger than those used for GPT-3.

3. **Knowledge engineering bottleneck quantification**: Develop metrics to compare the knowledge acquisition efficiency of pre-trained models versus traditional expert systems across multiple domains.