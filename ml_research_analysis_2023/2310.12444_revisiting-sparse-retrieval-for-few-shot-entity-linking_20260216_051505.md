---
ver: rpa2
title: Revisiting Sparse Retrieval for Few-shot Entity Linking
arxiv_id: '2310.12444'
source_url: https://arxiv.org/abs/2310.12444
tags:
- mention
- context
- entity
- retrieval
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of few-shot entity linking, where
  models must perform entity linking with very limited in-domain training data. The
  authors observe that dense retrievers (like bi-encoders) degrade significantly under
  few-shot conditions, while sparse retrievers like BM25 maintain performance but
  lack context-aware query formulation.
---

# Revisiting Sparse Retrieval for Few-shot Entity Linking

## Quick Facts
- arXiv ID: 2310.12444
- Source URL: https://arxiv.org/abs/2310.12444
- Reference count: 7
- Key outcome: BM25 with ELECTRA-extracted keywords achieves 15.07% Recall@64 improvement over dense retrievers in few-shot entity linking

## Executive Summary
This paper addresses the challenge of few-shot entity linking where models must perform entity linking with very limited in-domain training data. The authors observe that dense retrievers like bi-encoders degrade significantly under few-shot conditions, while sparse retrievers like BM25 maintain performance but lack context-aware query formulation. To bridge this gap, they propose an ELECTRA-based keyword extractor that denoises mention contexts and constructs better BM25 queries by identifying relevant tokens. The approach achieves state-of-the-art performance on the ZESHEL few-shot entity linking benchmark, demonstrating that PLM-augmented sparse retrieval can outperform data-hungry dense methods in few-shot scenarios.

## Method Summary
The proposed method combines sparse retrieval with PLM-based context denoising through a keyword extraction pipeline. An ELECTRA-based discriminator identifies relevant tokens in the mention context, which are then combined with the mention string to form enhanced BM25 queries. Training data is generated automatically using distant supervision: words overlapping between mention context and entity descriptions are ranked by BM25 relevance score, with top-k selected as keywords. The system uses BM25 with standard parameters (k1=1.5, b=0.75) to retrieve top-64 entities from the knowledge base, achieving superior performance across four test domains while maintaining the efficiency and interpretability advantages of sparse representations.

## Key Results
- Achieves 15.07% average Recall@64 improvement over best dense retriever baseline across all four ZESHEL test domains
- Outperforms state-of-the-art models in each individual domain: Forgotten Realms (+8.12%), Lego (+21.96%), Star Trek (+9.22%), YuGiOh (+18.72%)
- Particularly excels in domains with high vocabulary mismatch, demonstrating robustness to data scarcity
- Maintains efficiency advantages of sparse retrieval while incorporating contextual information typically associated with dense methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ELECTRA-based keyword extractor denoises context by selecting only mention-relevant tokens, improving sparse retrieval precision in few-shot scenarios.
- Mechanism: The keyword extractor uses ELECTRA's discriminator head to score each token in the mention context. Tokens with high scores are added to the query, effectively filtering out irrelevant words that would otherwise introduce noise into BM25 retrieval.
- Core assumption: Not all context tokens are equally relevant to the mention; selecting only relevant tokens improves retrieval quality while maintaining sparsity benefits.
- Evidence anchors:
  - [abstract] "we propose an ELECTRA-based keyword extractor to denoise the mention context and construct a better query expression"
  - [section 3.2] "Our keyword extractor aims to denoise the context through sequence labeling"
  - [corpus] Weak evidence: neighbor papers focus on entity linking but don't directly support this denoising mechanism

### Mechanism 2
- Claim: Distant supervision method generates training data by leveraging overlapping tokens between mention context and entity descriptions, enabling keyword extraction without manual annotation.
- Mechanism: For each mention-entity pair, words that appear in both the mention context and entity description are ranked by BM25 relevance score. Top-k tokens become keyword labels for training the extractor.
- Core assumption: Overlapping words between context and entity description are likely to be relevant keywords that improve retrieval performance.
- Evidence anchors:
  - [section 3.1] "We propose a distant supervision method to automatically generate training data based on overlapping tokens between mention contexts and entity descriptions"
  - [section 3.1] "The overlapping words form a preliminary keyword set"
  - [corpus] No direct evidence in neighbor papers supporting this specific distant supervision approach

### Mechanism 3
- Claim: Keyword-enhanced sparse retrieval outperforms dense retrievers in few-shot settings because sparse methods maintain performance with limited data while dense methods degrade significantly.
- Mechanism: By combining mention string with denoised context keywords, the query formulation captures both direct mention information and relevant contextual cues, achieving better recall than either BM25 with mention-only queries or BM25 with full context.
- Core assumption: Sparse retrieval methods are more robust to data scarcity than dense retrievers, and query expansion with relevant keywords can bridge the vocabulary mismatch gap.
- Evidence anchors:
  - [abstract] "the proposed method outperforms state-of-the-art models by a significant margin across all test domains"
  - [section 4.3] "BM25 (mention words) is apparently not the optimal solution since it ignores all context; BM25 (mention context) leverages all context and performs even worse"
  - [corpus] Neighbor papers mention few-shot challenges but don't directly compare sparse vs dense performance in this context

## Foundational Learning

- Concept: Few-shot learning challenges
  - Why needed here: The paper specifically addresses entity linking with limited in-domain training data, where standard dense retrievers fail but sparse methods remain effective
  - Quick check question: Why do dense retrievers typically require more training data than sparse retrievers for entity linking tasks?

- Concept: BM25 scoring and vocabulary mismatch
  - Why needed here: The paper builds on BM25 as the baseline sparse retrieval method and addresses its limitations with vocabulary mismatch between mentions and entity descriptions
  - Quick check question: How does BM25 handle exact word matching differently from semantic matching in dense retrievers?

- Concept: Distant supervision for training data generation
  - Why needed here: The keyword extractor requires labeled training data, which is scarce in few-shot settings, so the paper proposes automatically generating labels from mention-entity pairs
  - Quick check question: What are the potential risks of using automatically generated labels versus human-annotated labels for training?

## Architecture Onboarding

- Component map:
  - Input layer: Mention string and context (typically 128 word-pieces with mention tagged)
  - Keyword extractor: ELECTRA-based discriminator head that scores each token
  - Distant supervision module: BM25-based ranking of overlapping words between context and entity descriptions
  - Query construction: Combine mention string with top-k extracted keywords
  - Retrieval engine: BM25 with expanded query on entity description corpus
  - Output layer: Top-64 retrieved entities ranked by BM25 score

- Critical path: Mention+Context → Keyword Extractor → Keyword Selection → BM25 Query → Entity Retrieval
- Design tradeoffs: The choice of k (number of keywords) balances query specificity against computational cost and potential overfitting to noisy labels
- Failure signatures: Poor recall on domains with high vocabulary mismatch suggests insufficient keyword extraction; degraded performance with full context indicates ineffective denoising
- First 3 experiments:
  1. Baseline comparison: Implement BM25 with mention-only query vs BM25 with full context to establish the denoising problem
  2. Keyword extractor training: Train ELECTRA keyword extractor with varying k values (16, 32, 64) to find optimal keyword count
  3. Domain-specific analysis: Test performance across different domains (Forgotten Realms, Lego, Star Trek, YuGiOh) to identify vocabulary mismatch patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the keyword extractor's performance change if trained on synthetically generated mention-entity pairs rather than relying solely on distant supervision?
- Basis in paper: [inferred] The paper mentions that dense retrievers like MetaBLINK use synthetic data generation for few-shot learning, but does not explore this approach for the keyword extractor.
- Why unresolved: The authors chose distant supervision to avoid introducing potential noise from synthetic data, but did not experimentally compare both approaches.
- What evidence would resolve it: A controlled experiment comparing keyword extractor performance when trained with distant supervision alone versus combined with synthetic mention-entity pairs, measuring Recall@64 on ZESHEL domains.

### Open Question 2
- Question: What is the optimal number of keywords (k) to extract for different domains and mention types?
- Basis in paper: [explicit] The authors fix k=32 for all experiments but note that "the number of extracted keywords (denoted as k) is 32" without exploring this hyperparameter.
- Why unresolved: The paper treats k as a fixed hyperparameter without investigating its impact on different domains or mention contexts.
- What evidence would resolve it: A systematic ablation study varying k across domains and analyzing the trade-off between recall improvement and noise introduction for different values of k.

### Open Question 3
- Question: How does the proposed method scale to knowledge bases with significantly more entities than ZESHEL?
- Basis in paper: [inferred] The paper evaluates on ZESHEL with limited entity sets, but does not address computational efficiency or recall degradation with larger KBs.
- Why unresolved: The authors mention "efficiency of inverted indexes" as an advantage but do not empirically validate this claim with larger-scale experiments.
- What evidence would resolve it: Performance benchmarking on a larger-scale entity linking dataset (e.g., Wikipedia-scale) measuring both recall@K and query processing time as KB size increases.

## Limitations
- Evaluation limited to single few-shot entity linking dataset (ZESHEL) with only four domains
- Distant supervision assumes overlapping tokens are meaningful, which may not hold for all entity types
- Choice of top-32 keywords appears arbitrary without sensitivity analysis
- Computational efficiency comparison between sparse and dense methods not quantified

## Confidence

**High Confidence**: The core finding that BM25 with keyword-augmented queries outperforms dense retrievers in few-shot settings is well-supported by experimental results showing consistent improvements across all four test domains. The distant supervision method for generating training data is clearly described and implemented.

**Medium Confidence**: The mechanism explaining why ELECTRA-based denoising specifically helps in few-shot scenarios is plausible but not rigorously validated. The paper shows improved performance but doesn't isolate the impact of denoising versus query expansion. The claim about vocabulary mismatch being the primary challenge is supported but could benefit from more detailed analysis.

**Low Confidence**: The assertion that sparse retrieval methods are inherently more robust to data scarcity than dense methods lacks direct comparative evidence beyond the presented results. The paper doesn't explore why dense retrievers fail in few-shot settings at a mechanistic level.

## Next Checks
1. **Cross-domain generalization test**: Apply the keyword extractor and BM25 approach to a different few-shot entity linking dataset (e.g., from a different domain or language) to verify if the method generalizes beyond the ZESHEL domains tested.

2. **Keyword sensitivity analysis**: Systematically vary the number of extracted keywords (k) and measure impact on Recall@64 across domains to determine if k=32 is optimal or if the method is robust to this hyperparameter choice.

3. **Dense vs sparse data efficiency curve**: Train the dense retriever (e.g., bi-encoder) with varying amounts of in-domain training data (10, 25, 50, 100 examples) and compare performance curves against the sparse method to quantify exactly where and why sparse methods become advantageous in low-data regimes.