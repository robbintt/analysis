---
ver: rpa2
title: 'MC$^2$: Towards Transparent and Culturally-Aware NLP for Minority Languages
  in China'
arxiv_id: '2311.08348'
source_url: https://arxiv.org/abs/2311.08348
tags:
- languages
- corpus
- language
- kazakh
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work presents MC2, a Multilingual Corpus of Minority Languages
  in China, addressing the challenge of underrepresented low-resource languages in
  large language models. The corpus focuses on four underrepresented languages: Tibetan,
  Uyghur, Kazakh (in Arabic script), and Mongolian (in traditional script).'
---

# MC$^2$: Towards Transparent and Culturally-Aware NLP for Minority Languages in China

## Quick Facts
- arXiv ID: 2311.08348
- Source URL: https://arxiv.org/abs/2311.08348
- Authors: 
- Reference count: 8
- Key outcome: Presents MC2, a multilingual corpus for four underrepresented Chinese minority languages, addressing data contamination issues and prioritizing quality while maintaining diversity for language model training.

## Executive Summary
This work introduces MC2, a high-quality multilingual corpus focusing on four underrepresented languages in China: Tibetan, Uyghur, Kazakh (Arabic script), and Mongolian (traditional script). The authors identify significant language contamination in existing multilingual corpora and propose a quality-centric framework for data collection and filtering. By manually curating website lists, employing AI-assisted text extraction, and conducting rigorous deduplication and filtering, the resulting corpus achieves a balance between quality and diversity while addressing the specific challenges of low-resource language modeling.

## Method Summary
The authors construct MC2 through a quality-centric framework that begins with manually curating high-quality websites for each target language to avoid language identification errors. They then employ AI assistance to extract relevant content from web pages, removing unwanted elements like headers and footers. The corpus undergoes strict deduplication using both exact and fuzzy matching methods, followed by filtering based on repetition, document length, and character ratios. The process is supplemented by incorporating cleaned versions of existing datasets like Wikipedia and CulturaX after manually correcting their language misidentification errors.

## Key Results
- Achieves a balance between quality and diversity for four underrepresented Chinese minority languages
- Addresses serious language contamination issues found in existing multilingual corpora
- Produces long document lengths suitable for long-text modeling while maintaining attention to underrepresented language variants and writing systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed quality-centric data collection framework improves accuracy by directly crawling high-quality websites rather than relying on language identification tools.
- Mechanism: By manually curating a list of reputable websites in each target language, the framework avoids misidentification errors common in automated tools like fastText, which struggle with underrepresented scripts.
- Core assumption: The manually curated website list covers sufficient domain diversity and volume to represent the language adequately.
- Evidence anchors:
  - [abstract] "we employ a website-centric strategy, directly crawling websites where all pages are in the target language."
  - [section 3.1] "Since the number of websites in the low-resource languages is limited, we manually maintain a list of high-quality websites for each language of our study, to avoid language contamination resulting from mislabeling by identification tools."
  - [corpus] Limited: The corpus does not quantify the completeness of the curated website list or its coverage across domains.
- Break condition: If the manually curated list becomes outdated or fails to include emerging or niche domains, contamination could reoccur.

### Mechanism 2
- Claim: AI-assisted text extraction removes noisy page elements, improving corpus cleanliness.
- Mechanism: Custom Python scripts generated via GitHub Copilot extract only titles and main content, discarding headers, footers, and sidebars that hinder language modeling.
- Core assumption: The HTML structure of the target websites is consistent enough for automated extraction rules to generalize across pages.
- Evidence anchors:
  - [abstract] "We then resort to AI assistance to extract titles and main contents of the web pages, discarding undesired texts that might hinder language modeling."
  - [section 3.1] "These rules can precisely extract the title and main content of a web page, discarding other distracting texts."
  - [corpus] Missing: No quantitative comparison of cleaning effectiveness versus prior datasets.
- Break condition: If website layouts change or vary significantly, extraction scripts may fail or miss content.

### Mechanism 3
- Claim: Incorporating existing datasets after correcting their language misidentification errors expands corpus size without sacrificing quality.
- Mechanism: The framework supplements web crawls with cleaned versions of Wikipedia and CulturaX, after manually re-identifying languages and removing contamination.
- Core assumption: Manual re-identification is accurate and scalable for the volume of data involved.
- Evidence anchors:
  - [abstract] "We also supplement our corpus with resources like Wikipedia and CulturaX (Nguyen et al., 2023), after correcting the errors in them."
  - [section 3.2] "However, there are errors in language identification, especially the confusion between Uyghur texts and Kazakh texts in Arabic scripts. So we manually checked the websites in CulturaX and selected those in the correct language."
  - [corpus] Limited: The paper reports dataset sizes after correction but does not validate the accuracy of the manual re-identification process.
- Break condition: If manual re-identification becomes a bottleneck or introduces new errors, corpus quality could degrade.

## Foundational Learning

- Concept: Language identification and its failure modes in low-resource settings.
  - Why needed here: Understanding why standard tools like fastText mislabel languages is critical to appreciating the website-centric approach.
  - Quick check question: Why might fastText confuse Uyghur and Kazakh texts written in Arabic script?

- Concept: Text extraction from HTML and the importance of content cleaning.
  - Why needed here: The framework's reliance on AI-generated extraction rules requires knowledge of HTML parsing and the pitfalls of web scraping.
  - Quick check question: What types of HTML elements are most likely to introduce noise into a language modeling corpus?

- Concept: Deduplication techniques (exact and fuzzy) and their parameters.
  - Why needed here: The paper uses both SHA-256 hashing and MinHash to remove duplicates; understanding these methods is key to reproducing the pipeline.
  - Quick check question: How does MinHash approximate similarity, and why is it useful for deduplication?

## Architecture Onboarding

- Component map:
  Manual website curation -> Web crawling (language-specific) -> AI-assisted text extraction -> Deduplication (exact + fuzzy) -> Filtering (repetition, length, character ratio) -> Optional integration of cleaned external datasets -> Final corpus output

- Critical path:
  1. Manual website list creation and validation
  2. Web crawling of curated sites
  3. AI-assisted extraction and cleaning
  4. Deduplication and filtering
  5. Optional integration of corrected external datasets

- Design tradeoffs:
  - Manual curation ensures quality but is labor-intensive and may not scale to many languages
  - AI extraction accelerates processing but depends on consistent HTML structure
  - Deduplication balances recall and precision; aggressive deduplication risks removing legitimate near-duplicates

- Failure signatures:
  - High false positive rate in language identification → contamination
  - Extraction rules missing main content → reduced corpus size or poor quality
  - Over-aggressive filtering → loss of useful data

- First 3 experiments:
  1. Validate language identification accuracy by sampling and annotating a subset of crawled pages
  2. Test extraction rules on a diverse set of websites to ensure robustness to layout changes
  3. Benchmark deduplication effectiveness by comparing corpus size and token overlap before and after deduplication

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between quality and diversity when constructing multilingual corpora for low-resource languages?
- Basis in paper: [explicit] The paper emphasizes achieving a balance between quality and diversity in the MC2 corpus.
- Why unresolved: The paper discusses the importance of this balance but does not provide a quantitative framework or methodology to determine the optimal balance.
- What evidence would resolve it: Experimental results comparing the performance of models trained on corpora with varying balances of quality and diversity, demonstrating the impact on downstream tasks.

### Open Question 2
- Question: How does the inclusion of underrepresented writing systems in multilingual corpora affect the cultural awareness and performance of language models?
- Basis in paper: [explicit] The paper highlights the importance of including underrepresented writing systems, such as Kazakh Arabic script and traditional Mongolian script, to enhance cultural awareness.
- Why unresolved: While the paper emphasizes the significance of including these writing systems, it does not provide empirical evidence on the impact of their inclusion on model performance or cultural awareness.
- What evidence would resolve it: Comparative studies evaluating the performance and cultural awareness of models trained on corpora with and without underrepresented writing systems.

### Open Question 3
- Question: What are the most effective techniques for long-text modeling in low-resource languages?
- Basis in paper: [explicit] The paper introduces a high-quality, document-level corpus suitable for long-text modeling, addressing the limitations of previous sentence-level data.
- Why unresolved: The paper presents the corpus but does not explore specific techniques or architectures for long-text modeling in low-resource languages.
- What evidence would resolve it: Experimental results comparing different techniques and architectures for long-text modeling in low-resource languages, demonstrating their effectiveness and limitations.

### Open Question 4
- Question: How can the data in high-resource writing systems be leveraged to enhance models for low-resource writing systems while preserving cultural uniqueness?
- Basis in paper: [explicit] The paper mentions the unbalanced amount of data in two writing systems of a single language and encourages research on leveraging high-resource data while preserving cultural uniqueness.
- Why unresolved: The paper suggests the need for such research but does not provide specific methodologies or approaches to achieve this balance.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of techniques that leverage high-resource data to improve models for low-resource writing systems, while preserving cultural uniqueness.

## Limitations
- Manual curation approach may not scale to additional languages or evolve with changing web landscapes
- Language identification process relies heavily on manual verification without systematic validation protocols
- Extraction rules generated via AI assistance lack detailed specifications, making exact reproduction challenging
- No quantitative comparisons of cleaning effectiveness or comprehensive language identification accuracy metrics provided

## Confidence

**High Confidence**: The fundamental problem statement regarding underrepresented languages in China and the general approach of using website-centric data collection is well-established and clearly articulated.

**Medium Confidence**: The effectiveness of the quality-centric framework in achieving the stated balance between quality and diversity is supported by corpus statistics, though lacks comparative validation against alternative methods.

**Low Confidence**: Claims about the superiority of the proposed approach over existing multilingual corpora are not directly tested or benchmarked, as the paper does not provide quantitative comparisons of cleaning effectiveness or language identification accuracy.

## Next Checks

1. **Language Identification Accuracy Validation**: Sample and manually annotate 1,000 randomly selected documents from the corpus to verify the accuracy of language identification, particularly focusing on the Uyghur-Kazakh distinction in Arabic script.

2. **Extraction Rule Robustness Test**: Apply the AI-generated extraction rules to a diverse set of 50 websites with varying layouts and HTML structures to assess their generalizability and identify potential failure modes.

3. **Deduplication Effectiveness Benchmark**: Compare corpus statistics (size, token overlap, document diversity) before and after deduplication, and benchmark against established methods like those used in C4 or The Pile to quantify the impact of the proposed approach.