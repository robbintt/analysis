---
ver: rpa2
title: Learning impartial policies for sequential counterfactual explanations using
  Deep Reinforcement Learning
arxiv_id: '2311.00523'
source_url: https://arxiv.org/abs/2311.00523
tags:
- reward
- feature
- action
- counterfactual
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles a significant limitation in current reinforcement
  learning (RL)-based approaches for generating sequential counterfactual (SCF) explanations,
  namely the tendency to over-utilize specific features, resulting in repetitive and
  less interpretable explanations. The authors identify that this stems from using
  sparse binary rewards that only consider class labels.
---

# Learning impartial policies for sequential counterfactual explanations using Deep Reinforcement Learning

## Quick Facts
- arXiv ID: 2311.00523
- Source URL: https://arxiv.org/abs/2311.00523
- Reference count: 20
- Primary result: Proposed probability-based reward function significantly improves action diversity (entropy) in RL-based counterfactual generation while maintaining performance on other metrics

## Executive Summary
This paper addresses a critical limitation in reinforcement learning (RL)-based approaches for generating sequential counterfactual (SCF) explanations: the tendency to over-utilize specific features, resulting in repetitive and less interpretable explanations. The authors identify that this stems from using sparse binary rewards that only consider class labels. They propose a novel reward function that leverages the classifier's output probabilities, providing denser feedback to the RL agent. Experiments on four tabular datasets demonstrate that the proposed method significantly improves action diversity while maintaining satisfactory performance on other metrics like satisfiability and distance.

## Method Summary
The authors propose a novel reward function that leverages classifier output probabilities to provide denser feedback to the RL agent, addressing feature over-utilization in sequential counterfactual generation. They use a Parametrized Deep Q-Network (P-DQN) with continuous action space, where actions are tuples of (feature index, feature value). The probability-based reward function computes the difference between successive predicted probabilities, providing immediate feedback on each step regarding the direction toward the target class. The method is evaluated on four tabular datasets using metrics including satisfiability, action sparsity, distance, entropy, and target probability, with results showing improved action diversity compared to binary reward approaches.

## Key Results
- Probability-based reward function achieves significantly higher action entropy (68%+) compared to binary rewards (63%-) across all datasets
- The method maintains satisfactory satisfiability (>80%) and reasonable distance metrics while improving diversity
- Action sequences generated with probability-based rewards show more diverse feature utilization patterns, as visualized in Sankey diagrams

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using classifier output probabilities as reward signals creates denser feedback for the RL agent.
- Mechanism: Instead of binary success/failure rewards, the agent receives immediate feedback based on how much the predicted probability shifts toward the target class after each action.
- Core assumption: The classifier outputs meaningful probability estimates that correlate with feature importance.
- Evidence anchors:
  - [abstract]: "propose a novel reward function that leverages the classifier's output probabilities, providing denser feedback to the RL agent."
  - [section 3.2]: "This is beneficial because the RL agent has immediate feedback on each step regarding the direction taken in relation to the target class."
  - [corpus]: Weak. No direct evidence from neighbors about probability-based rewards.

### Mechanism 2
- Claim: Continuous action spaces prevent the agent from being locked into discrete, rigid feature changes.
- Mechanism: By allowing any real-valued change within [-1,1] for each feature, the agent can make smaller, more nuanced adjustments that better navigate the feature space.
- Core assumption: Continuous changes are more expressive and closer to the underlying data distribution than fixed discrete steps.
- Evidence anchors:
  - [section 3.1]: "P-DQN remains more accurate and adaptable compared to the DQN method, as it enables continuous changes for continuous features without the restriction of discrete steps."
  - [section 3.1]: Discusses how fixed step sizes ignore feature interdependencies and can map to vastly different real-world changes.
  - [corpus]: Weak. No neighbor papers discuss continuous vs discrete action spaces directly.

### Mechanism 3
- Claim: Action entropy metric quantifies feature over-utilization by measuring the diversity of actions taken.
- Mechanism: Entropy is computed over the distribution of features modified at each step; low entropy indicates repetitive action patterns.
- Core assumption: A good counterfactual policy should adapt its actions based on the input instance rather than following a fixed pattern.
- Evidence anchors:
  - [section 4.2]: Defines entropy as "âˆ’ P_t P_k pk_t log2(pk_t) / log2(K2)" and explains it measures diversity of feature changes.
  - [section 4.4]: "entropy of the policy learned by this method is low for all datasets, i.e. under 63%, compared to our approach where the policy has higher entropy over 68%."
  - [corpus]: Weak. No neighbor papers discuss entropy-based diversity metrics for counterfactual explanations.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The counterfactual generation problem is formalized as an MDP where states are feature vectors and actions are feature modifications.
  - Quick check question: What are the components of an MDP (states, actions, rewards, transition function) and how do they map to the counterfactual generation problem?

- Concept: Reward shaping
  - Why needed here: The paper demonstrates how reward design (sparse binary vs. dense probability-based) dramatically affects learned policy behavior.
  - Quick check question: How does the binary reward function differ from the probability-based reward function in terms of feedback density and timing?

- Concept: Entropy as a diversity metric
  - Why needed here: Entropy quantifies how much the agent varies its actions across different input instances, addressing the over-utilization problem.
  - Quick check question: Given an action frequency distribution [0.5, 0.3, 0.2], what is the entropy and what does it tell us about action diversity?

## Architecture Onboarding

- Component map: Black-box classifier -> RL agent (P-DQN) -> Environment -> Reward function -> Evaluation metrics
- Critical path:
  1. Initialize state from input instance
  2. Agent selects action (feature k, value u_k)
  3. Environment applies change to create new state
  4. Classifier evaluates new state and provides probability
  5. Reward is computed based on probability shift and distance
  6. Agent updates policy network
  7. Repeat until success or failure
- Design tradeoffs:
  - Continuous vs. discrete actions: Continuous allows finer control but requires more sophisticated RL algorithms
  - Binary vs. probability rewards: Binary is simpler but sparse; probability-based is denser but depends on classifier quality
  - Action space size vs. training complexity: Larger action spaces enable more diverse counterfactuals but increase learning difficulty
- Failure signatures:
  - Low entropy (<60%) indicates feature over-utilization
  - High action sparsity (<2) suggests the agent only needs one change to find CFs
  - Low satisfiability (<80%) means the agent fails to find CFs for many instances
  - High distance (>3) indicates counterfactuals are far from original instances
- First 3 experiments:
  1. Train P-DQN with binary reward on Adult Income dataset, measure entropy and compare to probability-based reward
  2. Visualize action selection patterns using Sankey diagrams for both reward types
  3. Test policy generalization by evaluating on held-out instances and measuring diversity of generated counterfactuals

## Open Questions the Paper Calls Out
1. How can we develop RL agents that work in mixed action spaces (continuous and discrete features) without transforming them into a common data type?
2. What distance metrics can effectively account for interdependencies between mixed feature types in counterfactual generation?
3. How does the proposed probability-based reward function compare to other dense reward formulations for sequential counterfactual generation?

## Limitations
- Results based on only four tabular datasets, limiting generalizability to other data types and domains
- Reliance on classifier probability outputs assumes well-calibrated models, which may not hold in practice
- Focus on quantitative metrics rather than qualitative assessment of explanation quality

## Confidence
- Mechanism of probability-based rewards improving diversity: Medium
- Continuous action spaces enabling finer counterfactual adjustments: Medium
- Entropy as a valid diversity metric for counterfactual explanations: Low

## Next Checks
1. Conduct user studies to assess whether higher-entropy counterfactuals are more interpretable and actionable for domain experts
2. Test the approach on non-tabular datasets (images, text) to evaluate cross-domain applicability
3. Investigate the sensitivity of results to classifier calibration and probability estimation quality across different model architectures