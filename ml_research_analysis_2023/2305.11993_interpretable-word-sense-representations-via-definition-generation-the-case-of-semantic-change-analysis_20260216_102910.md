---
ver: rpa2
title: 'Interpretable Word Sense Representations via Definition Generation: The Case
  of Semantic Change Analysis'
arxiv_id: '2305.11993'
source_url: https://arxiv.org/abs/2305.11993
tags:
- word
- nition
- nitions
- sense
- usage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Automatically generated natural language definitions can serve
  as interpretable word and word sense representations. A fine-tuned Flan-T5 model
  generates definitions for word usages, with the most prototypical definition in
  each cluster chosen as the sense label.
---

# Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis

## Quick Facts
- arXiv ID: 2305.11993
- Source URL: https://arxiv.org/abs/2305.11993
- Reference count: 31
- Generated natural language definitions serve as interpretable word sense representations that outperform token-based labels in human evaluation and better approximate human semantic similarity judgements.

## Executive Summary
This paper presents a novel approach to creating interpretable word sense representations by generating natural language definitions for contextualized word usages. Using a fine-tuned Flan-T5 model, the method produces definitions for each word usage, then clusters these usages and selects the most prototypical definition within each cluster as the sense label. The approach is evaluated on English diachronic word usage graphs, demonstrating that definition-based sense labels are preferred by humans over usage-based labels and that definition embeddings better approximate human semantic similarity judgments than token or sentence embeddings. The method is applied to analyze semantic change, producing interpretable diachronic maps of sense relations that explain meaning changes over time.

## Method Summary
The method fine-tunes Flan-T5 XL on three definition datasets (WordNet, Oxford, CoDWoE) to generate definitions for contextualized word usages. For each usage example, the model produces a definition using a prompt that includes the target word. The generated definitions are then embedded using DistilRoBERTa sentence embeddings. Usage clusters are formed using these definition embeddings, and the most prototypical definition within each cluster (the one closest to the cluster centroid) is selected as the sense label. For semantic change analysis, the method creates diachronic sense dynamics maps by computing similarities between definition embeddings across time periods, identifying sense loss, gain, and evolution.

## Key Results
- Generated definition-based sense labels outperform usage-based labels in human evaluation (80% vs 68% sufficient quality)
- Definition embeddings better approximate human semantic similarity judgments than token or sentence embeddings
- Definition clusters show higher separation-cohesion ratios and lower intra-cluster dispersion
- The method produces interpretable diachronic sense dynamics maps explaining semantic change

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generated definitions outperform token embeddings in word-in-context semantic similarity judgments.
- Mechanism: Definitions provide a more abstract and context-independent representation that captures the essential meaning of a word sense, reducing noise from specific contextual details.
- Core assumption: The meaning of a word sense can be adequately captured by a concise natural language definition that is robust to contextual variations.
- Evidence anchors:
  - [abstract]: "contextualised definitions also outperform token or usage sentence embeddings in word-in-context semantic similarity judgements"
  - [section 5.1]: "Pairwise similarities between definitions approximate human similarity judgements far better than similarities between example sentence and word embeddings"

### Mechanism 2
- Claim: Prototypical definitions within a usage cluster better represent word senses than definitions of prototypical usages.
- Mechanism: By averaging embeddings of all definitions in a cluster and selecting the closest one, the method finds a definition that best represents the majority meaning, filtering out outliers.
- Core assumption: The average embedding of cluster definitions points toward the most representative sense meaning.
- Evidence anchors:
  - [section 6]: "we use the most prototypical one as the sense labelâ€”with the aim of reflecting the meaning of the majority of usages in the cluster"
  - [section 6]: "clusters in the definition spaces have on average the lowest intra-cluster dispersion, indicating that they are more cohesive"

### Mechanism 3
- Claim: Diachronic sense dynamics maps explain semantic change by showing relations between sense labels across time periods.
- Mechanism: By generating time-specific sub-clusters and computing similarity between their definition embeddings, the method identifies which senses are related or have evolved from each other.
- Core assumption: Similar definition embeddings across time periods indicate related or evolved word senses.
- Evidence anchors:
  - [section 7]: "we compute pairwise cosine similarities between the sentence embeddings of the labels (their 'definition embeddings')"
  - [section 7]: "Sub-cluster pairs connected with such edges form a sense dynamics map"

## Foundational Learning

- Concept: Contextualized word representations
  - Why needed here: The entire approach relies on understanding how word meanings vary with context and how to capture this variation in representations.
  - Quick check question: How does a contextualized representation differ from a static word embedding in handling polysemy?

- Concept: Natural language generation evaluation
  - Why needed here: The quality of generated definitions must be assessed both automatically and through human judgment to validate their utility.
  - Quick check question: What are the limitations of using BLEU and BERTScore for evaluating definition quality?

- Concept: Semantic change detection
  - Why needed here: The case study applies definition generation to track how word meanings evolve over time, requiring understanding of diachronic linguistic processes.
  - Quick check question: Why is it important to distinguish between sense loss and sense gain when analyzing semantic change?

## Architecture Onboarding

- Component map: Flan-T5 model -> definition generator -> usage clustering -> sense labeling -> diachronic mapping
- Critical path: Usage -> definition generation -> similarity computation -> clustering -> sense labeling -> explanation generation
- Design tradeoffs: Using abstract definitions improves interpretability but may lose fine-grained contextual distinctions; using more training data improves generation quality but increases computational cost.
- Failure signatures: Poor generation quality leads to misleading sense labels; inappropriate clustering leads to incorrect sense relations; definition embeddings not capturing true semantic similarity.
- First 3 experiments:
  1. Generate definitions for a small set of target words and compare to human-written definitions using automatic metrics.
  2. Create usage graphs using generated definitions and compute correlation with human similarity judgments.
  3. Apply the sense labeling method to a small DWUG and manually verify the quality of generated sense labels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal clustering method for definition embedding spaces given their high dimensionality and potential for identical representations across distinct usages?
- Basis in paper: [inferred] The paper notes that definition embedding spaces often have fewer distinct data points due to identical definitions for different usages, which impacts clustering metrics. It suggests future work to determine the most applicable clustering methods.
- Why unresolved: The paper does not specify which clustering methods to use or test for definition embedding spaces. It only mentions that k-means is used and that the optimal number of clusters is higher for definitions, but this is attributed to fewer distinct data points rather than the effectiveness of the clustering method itself.
- What evidence would resolve it: Experiments comparing different clustering algorithms (e.g., hierarchical clustering, DBSCAN, spectral clustering) on definition embedding spaces, evaluating their performance using metrics like silhouette score, Davies-Bouldin index, or adjusted Rand index against ground truth sense labels.

### Open Question 2
- Question: How do definition embeddings perform in cross-linguistic semantic change analysis compared to token or sentence embeddings?
- Basis in paper: [inferred] The paper uses English diachronic word usage graphs (DWUGs) and does not thoroughly evaluate Flan-T5's ability to generate definitions in languages other than English. It mentions that Flan-T5 is multilingual but the methods are language-agnostic.
- Why unresolved: The experiments are limited to English data, and there is no evaluation of definition generation or embedding performance in other languages. The paper acknowledges this as a limitation but does not provide cross-linguistic results.
- What evidence would resolve it: Applying the definition generation and semantic change analysis pipeline to DWUGs in other languages (e.g., German, French, Latin as mentioned in the paper) and comparing the performance of definition embeddings against token and sentence embeddings in capturing semantic change across languages.

### Open Question 3
- Question: To what extent do automatically generated definition-based sense labels align with expert human judgments of word senses over time?
- Basis in paper: [explicit] The paper evaluates sense labels using human judgments, finding that definition-based labels are of sufficient quality in 80% of instances compared to 68% for usage-based labels. However, it notes that the evaluation is preliminary and qualitative.
- Why unresolved: The human evaluation is conducted on a limited set of sense clusters (136 in total) and does not involve expert linguists. The paper states that further evidence is needed to systematically evaluate how well predictions correspond to expert judgments.
- What evidence would resolve it: A large-scale evaluation involving expert historical linguists or lexicographers who assess the accuracy and usefulness of definition-based sense labels in explaining semantic change, possibly using metrics like inter-annotator agreement, precision, recall, or F1-score against expert annotations.

## Limitations

- The approach relies on the quality of automatically generated definitions, which may not always capture nuanced contextual meanings
- Clustering methods for high-dimensional definition embedding spaces are not thoroughly evaluated
- Cross-linguistic evaluation is limited to English, leaving open questions about generalizability
- The human evaluation of sense labels, while positive, is limited in scope and does not involve expert linguists

## Confidence

**High Confidence**: The finding that generated definitions can serve as interpretable sense labels, as demonstrated through direct human evaluation comparing definition-based and usage-based labels.

**Medium Confidence**: The claim that definition embeddings outperform token and sentence embeddings in approximating human semantic similarity judgments, supported by statistical evidence but based on indirect measures.

**Low Confidence**: The broader implications for semantic change analysis, as the case study demonstrates proof-of-concept functionality but lacks extensive validation against linguistic ground truth.

## Next Checks

1. Conduct direct human evaluation of generated definitions as standalone representations - have annotators rate how well definitions capture word sense meaning without reference to usage examples.

2. Perform ablation studies on the clustering methodology - compare the average-embedding approach to alternative sense label selection methods (e.g., majority vote, centroid definition) to isolate the contribution of the specific representation strategy.

3. Validate diachronic sense maps with expert linguistic analysis - have domain experts examine the generated sense dynamics maps for known words with well-documented semantic change to assess accuracy and identify systematic errors.