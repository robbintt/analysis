---
ver: rpa2
title: 'Indian-BhED: A Dataset for Measuring India-Centric Biases in Large Language
  Models'
arxiv_id: '2309.08573'
source_url: https://arxiv.org/abs/2309.08573
tags:
- bias
- stereotypical
- indian
- language
- caste
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INDIAN-BhED, a novel dataset for measuring
  stereotypes related to caste and religion in India. The dataset is used to compare
  bias levels between Indian and Western contexts in popular large language models.
---

# Indian-BhED: A Dataset for Measuring India-Centric Biases in Large Language Models

## Quick Facts
- arXiv ID: 2309.08573
- Source URL: https://arxiv.org/abs/2309.08573
- Reference count: 25
- Key outcome: Most LLMs display significantly higher bias for Indian stereotypes compared to Western ones

## Executive Summary
This paper introduces INDIAN-BhED, a novel dataset for measuring stereotypes related to caste and religion in India. The dataset is used to compare bias levels between Indian and Western contexts in popular large language models. Results show that the majority of models tested display significantly higher bias for Indian stereotypes compared to Western ones. The authors also explore Instruction Prompting as a mitigation strategy, finding it effective for reducing bias in GPT-3.5. The findings highlight the need for more diverse perspectives in AI fairness research and evaluation.

## Method Summary
The study creates INDIAN-BhED with 228 English examples covering caste and religion stereotypes, paired with anti-stereotypical counterparts. For comparison, a subset of CrowS-Pairs is used (386 race, 159 gender sentences). Bias is measured using All Unmasked Likelihood (AUL) for encoder models and Conditional Log-Likelihood (CLL) for decoder models. The methodology tests 12 models including BERT-base, mBERT, XLM-RoBERTa-Large, GPT-2 variants, Falcon 7B/40B, Llama-2, and GPT-3.5. Instruction Prompting is evaluated as a mitigation strategy by prepending anti-bias instructions to prompts.

## Key Results
- Majority of tested LLMs show significantly higher bias for Indian stereotypes compared to Western ones
- Instruction Prompting effectively reduces stereotypical bias in GPT-3.5 across bias categories
- Pure frequency bias in training data drives disparities, with terms like "Brahmin" having higher likelihood than "Dalit" despite population differences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction Prompting effectively reduces stereotypical bias by providing explicit contextual cues to the model.
- Mechanism: The prompt "Remember to NOT have any {IDENTITY} biases while responding" primes the model to suppress stereotypical associations during generation.
- Core assumption: The model's attention mechanisms can be redirected by explicit anti-bias instructions.
- Evidence anchors:
  - [abstract] "We find that this simple intervention results in large reductions in both stereotypical and anti-stereotypical biases for GPT-3.5"
  - [section 4.3] "Instruction Prompting consistently decreases stereotypical bias across bias categories and regional contexts"
  - [corpus] Weak - no direct corpus evidence linking instruction prompt structure to attention mechanism changes
- Break condition: The intervention loses effectiveness if the model's training data already heavily overweights certain stereotypes, making them too deeply embedded to override with a simple instruction.

### Mechanism 2
- Claim: The disparity in bias levels between Indian and Western contexts stems from dominant Western perspectives in LLM development and evaluation.
- Mechanism: LLM developers and evaluators primarily come from Western contexts, leading to overcorrection for Western biases while Indian-specific biases remain unaddressed.
- Core assumption: The composition of development teams and evaluation frameworks directly shapes the bias profile of the final model.
- Evidence anchors:
  - [abstract] "overwhelmingly, this body of work is situated in Western contexts, relying on Western data, values and historical associations"
  - [section 5] "There may be an inherent dominance of a US-centric perspective in the development of LLMs, coming both from the technology developers who are primarily based in Silicon Valley"
  - [corpus] Weak - corpus doesn't directly measure team composition or geographic distribution of developers
- Break condition: This mechanism breaks if the development teams become more geographically diverse or if evaluation frameworks are explicitly designed to catch non-Western biases.

### Mechanism 3
- Claim: The frequency bias in training data drives the prevalence of certain stereotypes, particularly for underrepresented groups like Dalits.
- Mechanism: Words or concepts appearing more frequently in training data are more likely to be generated, regardless of their actual demographic representation.
- Core assumption: Token frequency in training data directly correlates with generation likelihood, independent of real-world statistics.
- Evidence anchors:
  - [section 5] "We empirically observe that the single term 'Brahmin' (without surrounding sentence context) has a higher likelihood of prediction as compared to 'Dalit'"
  - [section 5] "This pure frequency bias must stem from its higher frequency within the pre-training data. This contrasts with the population share of the two castes"
  - [corpus] Weak - corpus doesn't provide frequency analysis of training data tokens
- Break condition: This mechanism breaks if the model uses sophisticated frequency normalization techniques or if the training data undergoes demographic balancing.

## Foundational Learning

- Concept: Bias measurement frameworks
  - Why needed here: The paper uses specific metrics (AUL for encoders, CLL for decoders) to quantify bias, requiring understanding of how these metrics capture stereotypical associations
  - Quick check question: What's the key difference between AUL and CLL in how they measure bias in language models?

- Concept: Cross-cultural bias evaluation
  - Why needed here: The study compares bias across Indian and Western contexts, requiring understanding of how cultural differences affect bias manifestation
  - Quick check question: Why might a bias metric that works well for Western contexts fail to capture Indian-specific biases?

- Concept: Instruction prompting as intervention
  - Why needed here: The paper tests instruction prompting as a mitigation strategy, requiring understanding of how prompts influence model generation
  - Quick check question: What are the limitations of using instruction prompting as a bias mitigation strategy compared to architectural changes?

## Architecture Onboarding

- Component map: Dataset creation → Model selection → Bias measurement (AUL/CLL) → Intervention testing (Instruction Prompting) → Comparative analysis
- Critical path: Dataset → Model evaluation → Bias quantification → Intervention testing → Comparative analysis
- Design tradeoffs: The choice between encoder and decoder models affects measurement methodology; instruction prompting is simple but may not address root causes of bias
- Failure signatures: Inconsistent bias scores across models suggest data quality issues; low variance in results suggests the dataset may not be challenging enough
- First 3 experiments:
  1. Replicate bias measurements on a held-out test set to verify consistency
  2. Test alternative prompt structures beyond simple anti-bias instructions
  3. Compare results when using balanced vs. unbalanced demographic representations in the dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does bias in LLMs vary across different Indian languages and cultural contexts beyond English?
- Basis in paper: [inferred] The authors acknowledge their focus on English and suggest future work could explore cross-linguistic and cross-cultural biases.
- Why unresolved: The paper only examines bias in English-language contexts, leaving a gap in understanding how these biases manifest in other Indian languages and cultural settings.
- What evidence would resolve it: Comparative studies of LLM bias across multiple Indian languages (e.g., Hindi, Tamil, Bengali) using similar evaluation methods, and analysis of how language choice interacts with cultural context in bias expression.

### Open Question 2
- Question: What are the long-term effects of instruction prompting on LLM bias mitigation across different contexts and model types?
- Basis in paper: [explicit] The authors find instruction prompting effective for GPT-3.5 but note it's less effective for caste biases, suggesting the need for further investigation.
- Why unresolved: The study only examines short-term effects of a single intervention technique, and its long-term efficacy and applicability to other models and bias types remains unknown.
- What evidence would resolve it: Longitudinal studies tracking LLM bias levels over time with repeated instruction prompting, comparisons across multiple model types and bias categories, and exploration of combined mitigation strategies.

### Open Question 3
- Question: How do different stages of LLM development (pre-training, fine-tuning, RLHF) contribute to the observed disparities in bias between Indian and Western contexts?
- Basis in paper: [inferred] The authors speculate about potential causes but cannot definitively attribute the bias disparities to specific stages of model development.
- Why unresolved: The study doesn't provide a detailed analysis of how biases are introduced or amplified at different stages of LLM development, making it difficult to target interventions effectively.
- What evidence would resolve it: Comprehensive audits of training data and model behavior at each stage of development, comparisons of bias levels in models trained with different data sources or fine-tuning approaches, and experiments isolating the effects of specific training components on bias expression.

## Limitations
- The INDIAN-BhED dataset is relatively small with only 228 examples across caste and religion categories
- Comparison with Western biases relies on CrowS-Pairs, which may not perfectly align with Indian cultural nuances
- The paper cannot definitively attribute bias disparities to specific causes like development team composition or training data imbalances

## Confidence

**High confidence**: The observation that instruction prompting reduces bias in GPT-3.5 is well-supported by multiple experiments showing consistent reductions across bias categories and regional contexts.

**Medium confidence**: The claim that Indian-specific biases are significantly higher than Western biases is supported but could be affected by dataset limitations and measurement artifacts.

**Low confidence**: The explanation for why Indian biases are higher (Western dominance in development teams) is largely speculative and not directly tested through empirical evidence.

## Next Checks

1. **Dataset Expansion and Validation**: Replicate the bias measurements using an expanded version of INDIAN-BhED with at least 3-4× more examples, particularly focusing on additional demographic intersections and regional variations within India.

2. **Cross-Cultural Measurement Alignment**: Test whether the bias measurement methodology produces comparable results when applied to Western datasets with known bias profiles, to validate that the metrics are culturally sensitive and not introducing systematic measurement artifacts.

3. **Root Cause Analysis**: Conduct an analysis of the training data distribution for key terms (e.g., "Brahmin" vs. "Dalit") to empirically verify whether frequency bias in pre-training data explains the observed generation patterns, rather than relying on indirect observations.