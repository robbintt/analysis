---
ver: rpa2
title: 'Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models''
  Alignment'
arxiv_id: '2308.05374'
source_url: https://arxiv.org/abs/2308.05374
tags:
- llms
- arxiv
- answer
- prompt
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a comprehensive survey of key dimensions crucial
  to assessing LLM trustworthiness, covering seven major categories with 29 sub-categories.
  The survey identifies reliability, safety, fairness, resistance to misuse, explainability
  and reasoning, adherence to social norms, and robustness as essential considerations
  for LLM alignment.
---

# Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment

## Quick Facts
- arXiv ID: 2308.05374
- Source URL: https://arxiv.org/abs/2308.05374
- Reference count: 40
- More aligned models tend to perform better in terms of overall trustworthiness across multiple dimensions

## Executive Summary
This paper provides a comprehensive survey of key dimensions crucial to assessing LLM trustworthiness, covering seven major categories with 29 sub-categories. The authors identify reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness as essential considerations for LLM alignment. Through measurement studies on eight sub-categories, they demonstrate that more aligned models generally perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across different trustworthiness categories, highlighting the need for more fine-grained analyses and continuous improvements in LLM alignment.

## Method Summary
The paper conducts automated measurement studies using eight subcategories to assess LLM alignment. The methodology involves generating test prompts using an aligned LLM (text-davinci-003), querying target LLMs with these prompts, and using GPT-4 to judge whether responses align with human values. Human audit supplements the automated evaluation for credibility. The study evaluates models across seven major trustworthiness categories: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness.

## Key Results
- More aligned models (ChatGPT, GPT-4) perform better across multiple trustworthiness dimensions
- Alignment effectiveness varies across different trustworthiness categories
- Even well-aligned models can produce inconsistent or wrong answers when exposed to typos or adversarial prompts
- Automated GPT-4 evaluation provides a scalable alternative to manual labeling, though with potential limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A more aligned LLM performs better across multiple trustworthiness dimensions.
- Mechanism: Alignment training (SFT + RLHF) exposes models to curated examples of desired behaviors and penalizes undesired ones, effectively shaping internal representations to reflect human values.
- Core assumption: The alignment dataset is representative of real-world trustworthy behaviors across all tested categories.
- Evidence anchors:
  - [abstract] states that "more aligned models tend to perform better in terms of overall trustworthiness."
  - [section 11.6] shows that more aligned models (ChatGPT, GPT-4) refuse misuse tasks at higher rates than less aligned ones.
- Break condition: If the alignment dataset is imbalanced or biased, improvements may be uneven across categories.

### Mechanism 2
- Claim: Using a stronger LLM to judge alignment allows rapid, scalable evaluation without manual labeling.
- Mechanism: A high-performing LLM (e.g., GPT-4) can understand nuanced prompts and judge whether responses are harmful, biased, or unfaithful, acting as a proxy for human judgment.
- Core assumption: The judging LLM's internal representations and training align sufficiently with human values to make accurate judgments.
- Evidence anchors:
  - [section 11.3] uses GPT-4 to judge safety compliance automatically.
  - [section 11.4] uses GPT-4 to judge fairness (refusal to stereotype) automatically.
- Break condition: If the judging LLM is itself biased or misaligned, it will mislabel test responses, invalidating results.

### Mechanism 3
- Claim: Typos and adversarial prompts can cause even well-aligned models to produce inconsistent or wrong answers.
- Mechanism: Language models are sensitive to exact token sequences; small perturbations (typos, adversarial phrasing) can shift probability distributions enough to flip predictions or interpretations.
- Core assumption: The model's internal state is highly sensitive to minor input variations, especially when those variations push the input outside its training distribution.
- Evidence anchors:
  - [section 11.9] shows that adding typos to prompts significantly reduces consistency in answers across all models.
  - [section 4.3] discusses how LLMs give inconsistent answers to semantically equivalent prompts.
- Break condition: If the model's tokenization or input preprocessing normalizes variations, robustness to typos may improve.

## Foundational Learning

- Concept: Alignment via supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF)
  - Why needed here: These are the core techniques that shape model behavior to be trustworthy; understanding them is essential to interpreting results.
  - Quick check question: What are the two main stages of alignment described in the paper, and what does each accomplish?

- Concept: Evaluation taxonomy (7 major categories, 29 sub-categories)
  - Why needed here: The paper's structure and experiments are organized around this taxonomy; knowing it is necessary to understand what is being tested.
  - Quick check question: Name at least three major trustworthiness categories discussed in the taxonomy.

- Concept: Automated evaluation using LLM judges
  - Why needed here: The paper relies heavily on using GPT-4 to judge other models' outputs; understanding this method is critical to interpreting results.
  - Quick check question: Why does the paper use GPT-4 to judge other models' outputs instead of human raters?

## Architecture Onboarding

- Component map: Prompt generation (text-davinci-003) -> Target LLM querying -> GPT-4 judging -> Result aggregation
- Critical path: Generate prompts → query target model → judge with GPT-4 → analyze. Any failure in prompt generation or judging invalidates results.
- Design tradeoffs: Using GPT-4 as judge trades accuracy for speed and scalability; using multiple-choice formats trades nuance for programmatic verification.
- Failure signatures: Inconsistent judgments from GPT-4 across semantically equivalent inputs; low refusal rates on clearly unsafe prompts; high variance in robustness tests.
- First 3 experiments:
  1. Run hallucination test: Generate multiple-choice QA prompts, query davinci and text-davinci-003, judge with GPT-4, compare accuracy.
  2. Run safety test: Generate unsafe prompts via text-davinci-003, query target model, judge refusal with GPT-4, compute refusal rate.
  3. Run fairness test: Generate gender stereotype multiple-choice questions, query model, judge refusal with GPT-4, compute refusal rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more principled methods for evaluating and implementing LLM alignment?
- Basis in paper: [explicit] - The authors state "the community urgently requires more principled methods for evaluating and implementing LLM alignment" in the conclusion section.
- Why unresolved: Current alignment evaluation relies heavily on human labeling which is not scalable. The paper uses a workaround of using more advanced LLMs to judge test LLM outputs, but acknowledges this is not ideal.
- What evidence would resolve it: Development of standardized benchmark datasets and evaluation metrics for LLM alignment that do not require extensive human labeling.

### Open Question 2
- Question: What are the long-term effects of feedback loop bias in LLMs?
- Basis in paper: [inferred] - The paper discusses the potential for feedback loop bias where LLMs reinforcing certain outputs could influence future training data, but does not explore the long-term consequences.
- Why unresolved: The long-term impacts of feedback loop bias on LLM outputs and user behavior are not well understood. It is unclear how to mitigate these effects.
- What evidence would resolve it: Longitudinal studies tracking LLM outputs and user interactions over extended periods to identify and quantify feedback loop bias.

### Open Question 3
- Question: How can we protect LLMs against copyright infringement while maintaining their utility?
- Basis in paper: [explicit] - The paper discusses the potential for LLMs to leak copyrighted content and mentions watermarking and differential privacy as potential solutions, but does not explore these in depth.
- Why unresolved: Balancing copyright protection with LLM utility is challenging. Over-aggressive filtering could limit LLM capabilities.
- What evidence would resolve it: Evaluation of different copyright protection methods (e.g., watermarking, differential privacy) on LLM performance and utility.

## Limitations

- The study's reliance on GPT-4 as both an evaluation tool and a reference for alignment introduces circularity risks
- Automated evaluation methodology may miss nuanced contextual factors that human evaluators would catch
- The paper doesn't fully address the temporal stability of alignment - whether models maintain their alignment characteristics across different versions or after fine-tuning on new data

## Confidence

- High confidence: The correlation between model alignment and trustworthiness performance across multiple categories
- Medium confidence: The effectiveness of automated GPT-4 evaluation for alignment assessment
- Low confidence: The generalizability of findings across different model architectures and training regimes

## Next Checks

1. Cross-validate GPT-4 evaluation results with human expert annotations on a stratified sample of responses across all seven major categories
2. Test model robustness to adversarial inputs by systematically varying prompt formulations while keeping semantic content constant
3. Conduct longitudinal analysis by evaluating the same models at multiple time points to assess alignment stability