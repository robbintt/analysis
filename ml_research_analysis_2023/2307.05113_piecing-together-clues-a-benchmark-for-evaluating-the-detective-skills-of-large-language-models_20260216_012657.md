---
ver: rpa2
title: 'Piecing Together Clues: A Benchmark for Evaluating the Detective Skills of
  Large Language Models'
arxiv_id: '2307.05113'
source_url: https://arxiv.org/abs/2307.05113
tags:
- reasoning
- lsr-benchmark
- questions
- scenarios
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DetectBench, a dataset of 3,928 questions
  designed to evaluate large language models' ability to detect key information and
  perform multi-hop reasoning in complex, implicit contexts. Each question is paired
  with a paragraph averaging 190 tokens.
---

# Piecing Together Clues: A Benchmark for Evaluating the Detective Skills of Large Language Models

## Quick Facts
- arXiv ID: 2307.05113
- Source URL: https://arxiv.org/abs/2307.05113
- Reference count: 6
- Primary result: Detective Thinking Framework significantly improves large language models' performance on information detection and multi-hop reasoning tasks

## Executive Summary
This paper introduces DetectBench, a benchmark of 3,928 questions designed to evaluate large language models' ability to detect key information and perform multi-hop reasoning in complex, implicit contexts. Each question is paired with a paragraph averaging 190 tokens. The authors propose the Detective Thinking Framework, which encourages models to identify all possible clues before reasoning, addressing the common challenge of models struggling with implicit information detection and multi-hop reasoning. Experiments show existing models perform poorly in both areas, but the Detective Thinking Framework significantly improves performance by forcing explicit clue identification before reasoning.

## Method Summary
The paper introduces DetectBench, a dataset of 3,928 questions paired with paragraphs averaging 190 tokens, designed to evaluate large language models' detective skills. The Detective Thinking Framework is proposed to enhance models' ability to identify clues and perform multi-hop reasoning by requiring explicit enumeration of all possible clues before reasoning. Experiments compare model performance with and without the framework, using GPT-3.5-turbo and Llama variants as baseline models. The approach addresses the challenge of models processing all available information, including irrelevant details, which leads to incorrect conclusions.

## Key Results
- Existing large language models perform poorly on information detection and multi-hop reasoning tasks in complex contexts
- The Detective Thinking Framework significantly improves model performance by forcing explicit clue identification before reasoning
- GPT-3.5-turbo shows performance decline in Chain-of-Thought settings due to processing irrelevant information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Detective Thinking Framework improves multi-hop reasoning by forcing explicit clue identification before reasoning.
- Mechanism: The framework requires models to enumerate all possible clues in the context before performing reasoning, which reduces noise and focuses attention on relevant evidence.
- Core assumption: LLMs struggle with implicit information detection because they don't systematically identify all potential clues before reasoning.
- Evidence anchors:
  - [abstract] "To enhance models' detective skills, we propose the Detective Thinking Framework, which encourages models to identify all possible clues before reasoning."
  - [section] "Our experiments reveal that existing models perform poorly in both information detection and multi-hop reasoning. However, the Detective Thinking Framework approach alleviates this issue."
- Break condition: If the model fails to identify relevant clues even after explicit prompting, the framework loses effectiveness.

### Mechanism 2
- Claim: DetectBench evaluates both information detection and multi-hop reasoning simultaneously in complex contexts.
- Mechanism: By presenting paragraphs with implicit clues requiring multiple reasoning steps, the benchmark forces models to demonstrate both skills together rather than in isolation.
- Core assumption: Evaluating these skills separately underestimates real-world detective reasoning performance.
- Evidence anchors:
  - [abstract] "DetectBench, a dataset of 3,928 questions designed to evaluate large language models' ability to detect key information and perform multi-hop reasoning in complex, implicit contexts."
  - [section] "The DetectBench comprises 3,928 questions, each paired with a paragraph averaging 190 tokens in length."
- Break condition: If a model excels at single-hop reasoning but fails on multi-hop tasks, it indicates the framework isn't capturing integrated reasoning ability.

### Mechanism 3
- Claim: Large language models struggle with real-life scenario reasoning because they cannot ignore unrelated information effectively.
- Mechanism: LLMs tend to process all available information, including irrelevant details, leading to incorrect conclusions when extraneous data distracts from key clues.
- Core assumption: Human reasoning naturally filters irrelevant information, but LLMs process inputs more holistically.
- Evidence anchors:
  - [section] "GPT-3.5-turbo demonstrates a decline in performance within the Chain-of-Thought (CoT) setting... When instructing LLMs to approach problems step-by-step, they have a tendency to reason from an incorrect starting point."
  - [corpus] Weak evidence - corpus doesn't directly address this mechanism.
- Break condition: If models can be trained to explicitly identify and filter irrelevant information before reasoning, performance should improve.

## Foundational Learning

- Concept: Multi-hop reasoning
  - Why needed here: DetectBench questions require combining multiple pieces of evidence across the paragraph to reach conclusions
  - Quick check question: Can you identify all the clues needed to solve a DetectBench problem that requires connecting information from three different sentences?

- Concept: Implicit information detection
  - Why needed here: The benchmark focuses on scenarios where key information is not explicitly stated but must be inferred
  - Quick check question: Given a paragraph about a crime scene, can you list all the implicit clues that would be relevant to solving the mystery?

- Concept: Chain-of-thought prompting
  - Why needed here: The Detective Thinking Framework uses structured reasoning steps, and the paper explores how CoT affects performance
  - Quick check question: How would you structure a chain-of-thought prompt to ensure all relevant clues are identified before reasoning begins?

## Architecture Onboarding

- Component map:
  - DetectBench dataset (3,928 questions, 190-token avg paragraphs) -> Detective Thinking Framework (clue identification + reasoning pipeline) -> Evaluation pipeline (accuracy measurement for detection and reasoning) -> Baseline models (GPT-3.5-turbo, Llama variants)

- Critical path:
  1. Load DetectBench question and paragraph
  2. Apply Detective Thinking Framework to identify clues
  3. Perform multi-hop reasoning on identified clues
  4. Generate answer
  5. Compare against ground truth for accuracy

- Design tradeoffs:
  - Dataset size vs. quality: 3,928 questions provides good coverage but may limit rare scenario types
  - Framework complexity vs. usability: More detailed clue identification improves accuracy but increases prompt complexity
  - Implicit vs. explicit clues: Focusing on implicit information makes the benchmark harder but more realistic

- Failure signatures:
  - Model identifies irrelevant clues while missing key evidence
  - Reasoning stops after one hop when multi-hop reasoning is required
  - Performance drops significantly in Chain-of-Thought setting due to processing irrelevant information

- First 3 experiments:
  1. Run baseline model without Detective Thinking Framework on 100 DetectBench samples to establish performance baseline
  2. Apply Detective Thinking Framework with explicit clue identification prompt to same 100 samples and measure improvement
  3. Compare performance on questions with varying levels of implicit information to identify which types pose greatest challenges

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of language models on LSR-Benchmark compare to other existing reasoning benchmarks?
- Basis in paper: [inferred] The paper compares the performance of GPT-3.5-turbo and Llama models on LSR-Benchmark, but does not provide a direct comparison with other benchmarks.
- Why unresolved: The paper focuses on introducing LSR-Benchmark and evaluating model performance on it, without comparing it to other benchmarks.
- What evidence would resolve it: A direct comparison of model performance on LSR-Benchmark and other reasoning benchmarks, such as MMLU or BigBench.

### Open Question 2
- Question: What is the impact of increasing the size of the LSR-Benchmark dataset on model performance?
- Basis in paper: [inferred] The paper mentions that LSR-Benchmark contains only a limited number of samples and can only enhance the ability of LLMs in scenarios reasoning to a limited extent.
- Why unresolved: The paper does not explore the effects of dataset size on model performance.
- What evidence would resolve it: An experiment comparing model performance on different-sized versions of LSR-Benchmark, or a study on the relationship between dataset size and model performance in general.

### Open Question 3
- Question: How do different fine-tuning strategies affect the performance of language models on LSR-Benchmark?
- Basis in paper: [explicit] The paper explores the effects of fine-tuning Llama models on LSR-Benchmark, Alpaca dataset, and a combination of both, but does not extensively analyze the impact of different fine-tuning strategies.
- Why unresolved: The paper only tests a limited set of fine-tuning strategies and does not explore other possible approaches.
- What evidence would resolve it: A comprehensive study comparing the performance of models fine-tuned using various strategies, such as different learning rates, batch sizes, or optimizers, on LSR-Benchmark.

## Limitations
- Dataset construction methodology and validation procedures are not fully detailed, making it difficult to assess representativeness
- Framework's effectiveness appears heavily dependent on prompt engineering quality, which may not generalize across different model architectures or domains
- Experiments focus primarily on information detection and multi-hop reasoning without thoroughly exploring edge cases where the framework might fail

## Confidence
- High confidence: The Detective Thinking Framework improves model performance on DetectBench tasks compared to baseline approaches
- Medium confidence: The framework specifically addresses the core challenges of implicit information detection and multi-hop reasoning in complex contexts
- Medium confidence: Existing models struggle with these detective reasoning tasks, though the extent may vary across different model families

## Next Checks
1. Test the Detective Thinking Framework across diverse model families (including smaller models and specialized reasoning architectures) to assess generalizability beyond the primary models evaluated
2. Conduct ablation studies isolating the impact of clue identification versus multi-hop reasoning components to determine which aspects drive performance improvements
3. Evaluate model robustness by introducing controlled amounts of irrelevant information and contradictory clues to test the framework's effectiveness in adversarial scenarios