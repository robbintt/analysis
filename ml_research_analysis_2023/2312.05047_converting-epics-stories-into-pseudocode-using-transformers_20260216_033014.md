---
ver: rpa2
title: Converting Epics/Stories into Pseudocode using Transformers
arxiv_id: '2312.05047'
source_url: https://arxiv.org/abs/2312.05047
tags:
- code
- pseudocode
- language
- text
- conversion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a two-stage transformer-based approach for converting
  user epics or stories into pseudocode. The methodology involves first converting
  natural language text into Python code using a CodeT5 model, followed by converting
  the generated code into pseudocode.
---

# Converting Epics/Stories into Pseudocode using Transformers

## Quick Facts
- arXiv ID: 2312.05047
- Source URL: https://arxiv.org/abs/2312.05047
- Reference count: 27
- Primary result: Two-stage transformer approach converts user stories to pseudocode with BLEU scores of 0.4 (text-to-code) and 0.74 (code-to-pseudocode)

## Executive Summary
This paper presents a novel two-stage transformer-based approach for converting user epics and stories into pseudocode. The methodology leverages the CodeT5 model to first translate natural language text into Python code, then converts the generated code into pseudocode. The approach treats each stage as an independent machine translation task, allowing for more focused training and potentially better accuracy than direct text-to-pseudocode conversion. The system is evaluated using BLEU score on the MBPP and Django datasets, demonstrating reasonable performance particularly in the code-to-pseudocode stage.

## Method Summary
The methodology divides the text-to-pseudocode conversion task into two sequential stages. Stage 1 uses CodeT5 to convert natural language user stories into Python code, while Stage 2 transforms the generated Python code into pseudocode. Each stage is treated as a separate machine translation task and trained independently on appropriate datasets. The MBPP dataset is used for Stage 1 (text-to-code) with 974 samples, and the Django dataset is used for Stage 2 (code-to-pseudocode) with 16,000 samples. Both stages employ the same CodeT5 transformer architecture with encoder-decoder design and multi-headed self-attention mechanisms.

## Key Results
- BLEU score of 0.4 achieved for text-to-code conversion stage
- BLEU score of 0.74 achieved for code-to-pseudocode conversion stage
- Two-stage approach shows better performance than would be expected from direct text-to-pseudocode conversion
- CodeT5 model demonstrates effectiveness for both translation subtasks when trained separately

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage transformer-based approach decomposes the complex text-to-pseudocode generation task into two simpler machine translation subtasks, each of which can be trained more effectively.
- Mechanism: The approach first translates natural language user stories into Python code (Stage 1), then translates the generated Python code into pseudocode (Stage 2). Each stage uses the CodeT5 model with a transformer encoder-decoder architecture.
- Core assumption: Breaking down the task into two simpler translation tasks improves overall accuracy and reduces complexity compared to direct text-to-pseudocode translation.
- Evidence anchors: [abstract] "This methodology divides the Text to Pseudocode conversion task into two stages or subtasks, each of which is treated like an individual machine translation task." [section] "Both stages of the proposed approach are analogous to a language translation task. We use an encoder-decoder transformer model to first convert the English text to Python code, and then the Python code to pseudocode."
- Break condition: If the intermediate Python code generated in Stage 1 contains significant logical errors, these errors would propagate to Stage 2, degrading the final pseudocode quality.

### Mechanism 2
- Claim: CodeT5's transformer architecture with self-attention mechanisms enables effective learning of semantic relationships between natural language and code representations.
- Mechanism: The encoder processes input text/code into hidden states using multi-headed self-attention, while the decoder generates output code/pseudocode by attending to relevant parts of the input representation.
- Core assumption: Transformer-based architectures are superior to recurrent or convolutional approaches for capturing long-range dependencies in code and natural language.
- Evidence anchors: [abstract] "We find that the CodeT5 model gives the best results in terms of BLEU score when trained separately on the two subtasks mentioned above." [section] "Transformers are attention-based models that don't use the typical recurrent layers found in encoder-decoder designs, but rather use multi-headed self-attention."
- Break condition: If the input data contains complex control structures or rare programming constructs not well-represented in the training data, the self-attention mechanism may fail to capture relevant patterns.

### Mechanism 3
- Claim: Using BLEU score as an evaluation metric provides a quantitative measure of translation quality by comparing n-gram overlap between generated and reference outputs.
- Mechanism: The BLEU score calculation compares the generated code/pseudocode against reference translations, adjusting for brevity and computing geometric mean of precision scores across different n-gram lengths.
- Core assumption: BLEU score is an appropriate metric for evaluating code generation quality, capturing both syntactic and semantic similarity.
- Evidence anchors: [abstract] "BLEU score is a metric that is used to measure the similarity between a machine-translated text and a set of reference translations." [section] "We have used BLEU score to analyse the performance of our Machine Learning Models."
- Break condition: If the reference dataset contains multiple valid translations for the same input, BLEU score may penalize correct but different outputs.

## Foundational Learning

- Concept: Natural Language Processing and Transformer Architectures
  - Why needed here: Understanding how transformers process sequential data and capture contextual relationships is fundamental to grasping how CodeT5 converts between natural language and code.
  - Quick check question: How does self-attention in transformers differ from recurrent neural network approaches in handling long sequences?

- Concept: Machine Translation and BLEU Score Evaluation
  - Why needed here: The approach treats both stages as translation tasks, requiring understanding of how translation models work and how to evaluate their outputs.
  - Quick check question: What does a BLEU score of 0.4 versus 0.74 indicate about translation quality in the two stages?

- Concept: Software Development Lifecycle and Agile Methodology
  - Why needed here: Understanding the context of user stories, epics, and their role in software development helps appreciate the practical value of automating pseudocode generation.
  - Quick check question: Why would converting user stories directly to pseudocode be more efficient than manual code development?

## Architecture Onboarding

- Component map: Text input → Preprocessing → Stage 1 Encoder → Stage 1 Decoder → Python code output → Preprocessing → Stage 2 Encoder → Stage 2 Decoder → Pseudocode output
- Critical path: Natural language text flows through two sequential transformer-based translation stages, with each stage independently trained on its respective dataset
- Design tradeoffs: The two-stage approach adds complexity but improves accuracy compared to direct text-to-pseudocode conversion. Using CodeT5 requires substantial training data but provides superior results compared to rule-based approaches.
- Failure signatures: Low BLEU scores indicate poor translation quality. If Stage 1 produces syntactically correct but logically incorrect code, Stage 2 will generate incorrect pseudocode. Missing or ambiguous user story information leads to incomplete code generation.
- First 3 experiments:
  1. Test with simple, well-defined user stories (e.g., "Sort a list of numbers") to verify basic functionality.
  2. Test with user stories containing common programming constructs (loops, conditionals) to evaluate coverage.
  3. Test with ambiguous or incomplete user stories to assess robustness and error handling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the CodeT5 model compare to other transformer-based models like BERT or RoBERTa for the text-to-code and code-to-pseudocode conversion tasks?
- Basis in paper: [explicit] The paper mentions that CodeT5 gives the best results in terms of BLEU score for the two subtasks.
- Why unresolved: The paper does not provide a direct comparison with other transformer-based models.
- What evidence would resolve it: Conducting experiments to compare the performance of CodeT5 with other transformer-based models like BERT or RoBERTa on the same datasets.

### Open Question 2
- Question: How does the quality of the generated pseudocode affect the overall software development process, and can it lead to a reduction in development time and errors?
- Basis in paper: [inferred] The paper aims to simplify the development process by automating pseudocode generation, but does not provide empirical evidence of its impact on development time or error reduction.
- Why unresolved: The paper focuses on the technical aspects of the conversion process and does not evaluate its practical implications on software development.
- What evidence would resolve it: Conducting a study to measure the impact of using generated pseudocode on development time, error rates, and overall software quality.

### Open Question 3
- Question: How can the proposed approach be extended to handle more complex user stories or epics, and what are the limitations of the current methodology?
- Basis in paper: [inferred] The paper mentions that the approach is designed for small functionalities, but does not discuss its applicability to more complex requirements.
- Why unresolved: The paper does not explore the limitations of the current methodology or provide insights into how it can be extended to handle more complex scenarios.
- What evidence would resolve it: Experimenting with the approach on more complex user stories or epics and analyzing the results to identify limitations and potential improvements.

## Limitations

- The paper lacks direct corpus evidence supporting the decomposition mechanism and BLEU score appropriateness for code generation tasks
- No comparison is provided between the two-stage approach and direct text-to-pseudocode conversion methods
- Insufficient details about reference datasets and evaluation methodology limit confidence in the reported BLEU scores

## Confidence

- High Confidence: The basic architecture of using two CodeT5 transformer models in sequence is well-established and plausible.
- Medium Confidence: The BLEU score results are likely valid within the constraints of the evaluation methodology, though the appropriateness of BLEU for code generation remains questionable.
- Low Confidence: The claimed advantages of the two-stage decomposition approach over direct text-to-pseudocode conversion lack supporting empirical evidence in the paper.

## Next Checks

1. Verify the appropriateness of BLEU score for evaluating code generation quality by comparing results with alternative metrics like exact match or semantic similarity measures.

2. Test the two-stage approach against direct text-to-pseudocode models on the same datasets to empirically validate the claimed decomposition benefits.

3. Examine the reference datasets used for evaluation to ensure they represent realistic user stories and contain appropriate ground truth pseudocode for meaningful comparison.