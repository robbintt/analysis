---
ver: rpa2
title: 'MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models'
arxiv_id: '2310.12426'
source_url: https://arxiv.org/abs/2310.12426
tags:
- feedback
- cost
- reasoning
- refinement
- modules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel iterative refinement framework that
  addresses the diverse error categories in language model-generated reasoning chains
  by using specialized feedback modules tailored for each error type. The framework
  integrates multiple feedback modules, including frozen language models and external
  tools, each focusing on a specific error category, and a refiner model to generate
  refined solutions based on the initial solution and multi-aspect feedback.
---

# MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models

## Quick Facts
- **arXiv ID**: 2310.12426
- **Source URL**: https://arxiv.org/abs/2310.12426
- **Reference count**: 12
- **Key outcome**: Achieves up to 20% relative improvement in Mathematical Reasoning and up to 18% relative improvement in Logical Entailment through multi-aspect feedback refinement

## Executive Summary
MAF introduces an iterative refinement framework that addresses the limitations of single-feedback approaches in improving reasoning chains generated by large language models. By decoupling feedback generation into specialized modules targeting specific error categories (such as syntax errors, redundancy, and commonsense mistakes), MAF prevents prompt dilution and improves feedback quality. The framework employs two refinement strategies—eager refinement for immediate corrections of conflicting feedback and lazy refinement for efficient consolidation of non-conflicting feedback. Experiments across mathematical reasoning, logical entailment, and question answering tasks demonstrate significant performance gains, with MAF achieving up to 20% relative improvement in mathematical reasoning tasks.

## Method Summary
The MAF framework implements an iterative refinement loop where a base language model generates initial reasoning chains, specialized feedback modules identify and report specific error types, and a refiner model generates improved solutions based on the multi-aspect feedback. The framework uses selective summarization to filter feedback to only actionable corrections, enabling smaller models to process the feedback efficiently. Two refinement strategies are employed: eager refinement addresses conflicts immediately (such as variable naming changes that affect subsequent code references), while lazy refinement consolidates non-conflicting feedback to reduce iteration count. The process continues iteratively until a stopping condition is met, with optional oracle verification available for certain tasks.

## Key Results
- Up to 20% relative improvement in mathematical reasoning tasks (GSM-IC dataset)
- Up to 18% relative improvement in logical entailment tasks (Entailment Bank dataset)
- Demonstrates effectiveness across multiple reasoning domains including mathematical reasoning, logical entailment, and question answering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling feedback generation by error type prevents prompt dilution and improves feedback quality
- Mechanism: Specialized feedback modules focus on single error categories, avoiding context overload from multi-category prompts
- Core assumption: LLMs have limited context capacity and performance degrades with long, multi-category prompts
- Evidence anchors:
  - [abstract]: "existing approaches relying on a single generic feedback source fail to address the diverse error types"
  - [section 3.3]: "we decouple our feedback modules such that each module focuses on a single error category"
  - [corpus]: Weak evidence - only 1 neighbor paper (FMC) addresses error categorization without specific performance comparisons

### Mechanism 2
- Claim: Eager and Lazy refinement strategies optimize the balance between immediate correction and efficiency
- Mechanism: Eager refinement addresses conflicting feedback immediately, while lazy refinement consolidates non-conflicting feedback
- Core assumption: Different error types have different dependencies and can create conflicts if refined out of order
- Evidence anchors:
  - [section 3.4]: "Eager Refinement allows immediate revision... while Lazy Refinement improves efficiency"
  - [section 4.3]: "Our Eager-Refine module uses 3-shot standard prompting and our Lazy-Refiner uses a 2-shot standard prompting approach"
  - [corpus]: No direct evidence - neighbor papers don't address refinement strategy optimization

### Mechanism 3
- Claim: Selective summarization of feedback enables efficient processing by smaller models with limited context windows
- Mechanism: Filtering feedback to include only actionable corrections creates more focused prompts for smaller models
- Core assumption: Including all feedback, including non-actionable elements, reduces effectiveness for models with smaller context windows
- Evidence anchors:
  - [section 3.4]: "We use a Selective Summarization approach... This simple selective summarization approach makes the feedback succinct"
  - [section 4.3]: "We use a basic rule-based strategy to summarize feedback... look for the steps with feedback 'looks good' and remove those steps"
  - [corpus]: No direct evidence - none of the neighbor papers discuss feedback summarization techniques

## Foundational Learning

- **Concept: Chain-of-Thought Reasoning**
  - Why needed here: MAF builds upon CoT prompting as the foundation for generating intermediate reasoning steps that can be evaluated and refined
  - Quick check question: What is the key difference between standard prompting and Chain-of-Thought prompting in terms of model output structure?

- **Concept: In-Context Learning**
  - Why needed here: MAF relies on few-shot examples to teach feedback modules and the refiner how to perform their tasks without fine-tuning
  - Quick check question: How many shots does MAF use for the refiner module in lazy refinement mode?

- **Concept: Modular System Design**
  - Why needed here: The framework's effectiveness depends on the ability to swap in specialized feedback modules for different tasks
  - Quick check question: What are the three main components of the MAF framework that must be modular for the approach to work?

## Architecture Onboarding

- **Component map**: Base model (initial solution generator) → Feedback modules (error-specific evaluators) → Refiner model (solution improver) → Iterative loop with stop conditions
- **Critical path**: Initial generation → Feedback collection (eager/lazy) → Refinement → Verification (if oracle available) → Repeat until stopping condition
- **Design tradeoffs**: Multiple specialized modules provide better error coverage but increase complexity; eager refinement prevents conflicts but may increase token usage; lazy refinement improves efficiency but requires effective feedback consolidation
- **Failure signatures**: Over-refinement (performance degradation after optimal solution), feedback conflicts (erroneous corrections), context overflow (feedback too long for model window), module incompatibility (modules providing contradictory feedback)
- **First 3 experiments**:
  1. Baseline comparison: Run base model vs MAF on GSM-IC with 2 iterations, measure solve rate improvement
  2. Module ablation: Remove each feedback module individually from MAF and measure performance impact on GSM-IC
  3. Refinement strategy comparison: Run MAF with only eager refinement vs only lazy refinement vs hybrid approach on GSM-IC and compare solve rates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of feedback modules to use in the MAF framework for different reasoning tasks?
- Basis in paper: The paper mentions using multiple feedback modules but does not specify the optimal number for different tasks
- Why unresolved: The optimal number may vary depending on task complexity and common error types encountered
- What evidence would resolve it: A systematic study comparing MAF with different numbers of feedback modules across various reasoning tasks

### Open Question 2
- Question: How does the performance of MAF compare to other iterative refinement frameworks when using the same base model and number of iterations?
- Basis in paper: The paper compares MAF to Self-Refine but lacks direct comparison using identical base models and iteration counts
- Why unresolved: A direct comparison would provide clearer understanding of relative strengths and weaknesses
- What evidence would resolve it: An experiment comparing MAF and other iterative refinement frameworks using the same base model and number of iterations on identical reasoning tasks

### Open Question 3
- Question: How does the selective summarization approach for feedback affect the performance of MAF compared to using the full feedback?
- Basis in paper: The paper mentions using selective summarization but does not provide comparison with full feedback usage
- Why unresolved: Selective summarization may improve efficiency but could potentially remove important information
- What evidence would resolve it: An experiment comparing MAF performance with and without selective summarization on the same reasoning tasks

## Limitations
- Scalability to other reasoning domains remains untested
- Effectiveness of selective summarization across diverse feedback types not fully characterized
- Trade-off between eager and lazy refinement strategies in terms of computational cost versus accuracy gains needs more systematic evaluation

## Confidence

- **High Confidence**: The decoupling of feedback by error type is well-supported by theoretical arguments about context dilution and has direct experimental backing
- **Medium Confidence**: The refinement strategy optimization shows promise but lacks direct comparative evidence against single-strategy approaches
- **Medium Confidence**: The selective summarization approach is logically sound but represents a novel technique with no prior validation in the corpus

## Next Checks

1. **Error Type Coverage Analysis**: Systematically test MAF across 10+ diverse reasoning tasks to identify which error categories are most common and whether the current set of feedback modules provides adequate coverage

2. **Refinement Strategy Cost-Benefit Analysis**: Measure the actual token usage and iteration count differences between eager-only, lazy-only, and hybrid approaches across multiple task types to quantify efficiency claims

3. **Context Window Scaling Study**: Evaluate MAF's performance with progressively larger context windows (both through model size and window extension techniques) to determine when selective summarization becomes unnecessary