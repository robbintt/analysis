---
ver: rpa2
title: Evaluating Dynamic Topic Models
arxiv_id: '2309.08627'
source_url: https://arxiv.org/abs/2309.08627
tags:
- topic
- temporal
- topics
- measures
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Temporal Topic Quality (TTQ) and Dynamic Topic
  Quality (DTQ), novel evaluation measures for dynamic topic models (DTMs). TTQ captures
  topic transitions over time by combining temporal topic coherence and smoothness,
  while DTQ aggregates TTQ with static topic quality.
---

# Evaluating Dynamic Topic Models

## Quick Facts
- arXiv ID: 2309.08627
- Source URL: https://arxiv.org/abs/2309.08627
- Reference count: 40
- One-line primary result: Introduces TTQ and DTQ measures for evaluating dynamic topic models, validated through synthetic experiments and human evaluation.

## Executive Summary
This paper addresses the challenge of evaluating dynamic topic models (DTMs) by proposing two novel measures: Temporal Topic Quality (TTQ) and Dynamic Topic Quality (DTQ). TTQ captures topic transitions over time by combining temporal topic coherence and smoothness, while DTQ aggregates TTQ with static topic quality. The authors demonstrate through experiments on synthetic and real data that these measures effectively detect changes in topic quality over time, outperforming existing year-wise evaluation metrics. Human evaluations show strong correlations between the proposed measures and human judgment of topic relatedness and smoothness, validating their effectiveness in evaluating DTMs.

## Method Summary
The paper introduces Temporal Topic Quality (TTQ) and Dynamic Topic Quality (DTQ) as novel evaluation measures for dynamic topic models. TTQ is computed as the product of Temporal Topic Coherence (TTC) and Temporal Topic Smoothness (TTS) over a sliding time window, capturing both semantic coherence and gradual topic evolution. DTQ averages year-wise topic quality (TQ) and temporal topic quality (TTQ) across all topics and years. The authors evaluate these measures on three datasets (NeurIPS, NYT, UN General Debates) using D-LDA and D-ETM models, and validate results through human evaluation of word relatedness and smoothness.

## Key Results
- TTQ effectively detects poor topic transitions by combining coherence and smoothness metrics
- DTQ outperforms existing year-wise measures in capturing temporal topic quality changes
- Human evaluation shows strong correlations (Spearman 0.51-0.83) with TTC and TTS, validating the automated measures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal Topic Quality (TTQ) captures topic evolution over time by combining coherence and smoothness.
- Mechanism: TTQ measures the quality of a topic's transition across consecutive time steps by computing Temporal Topic Coherence (TTC) and Temporal Topic Smoothness (TTS) and multiplying them for each time window.
- Core assumption: A high-quality topic transition requires both semantic coherence (high TTC) and gradual change (high TTS).
- Evidence anchors:
  - [abstract] TTQ combines temporal topic coherence and smoothness.
  - [section 4.3] TTQ is defined as the product of TTC and TTS over a time window.
  - [corpus] Experiments show TTQ drops significantly when topics are shuffled, indicating it detects poor transitions.
- Break condition: If topics undergo abrupt semantic shifts, TTC may stay high while TTS drops, causing TTQ to decrease and signal poor transitions.

### Mechanism 2
- Claim: Dynamic Topic Quality (DTQ) evaluates both year-wise topic quality and temporal topic quality.
- Mechanism: DTQ averages year-wise topic quality (TQ) and temporal topic quality (TTQ) across all topics and years.
- Core assumption: Effective DTMs must maintain high topic quality within each year and smooth transitions over time.
- Evidence anchors:
  - [abstract] DTQ aggregates TTQ with static topic quality.
  - [section 4.4] DTQ formula averages TQt and TTQk over time and topics.
  - [corpus] Synthetic experiments show DTQ drops to zero when diversity is zero, highlighting its sensitivity to topic collapse.
- Break condition: If all topics become identical over time, TQ drops to zero, causing DTQ to drop regardless of TTQ values.

### Mechanism 3
- Claim: Human evaluation validates temporal measures by correlating with automated scores.
- Mechanism: Human raters assess word relatedness and smoothness for temporal topic sequences, and their ratings correlate with TTC and TTS.
- Core assumption: Human perception of topic coherence and smoothness aligns with automated temporal coherence and smoothness metrics.
- Evidence anchors:
  - [abstract] Human evaluation shows strong correlation with proposed measures.
  - [section 6.4] Spearman correlations between human ratings and TTC/TTS are reported as high (0.51-0.83).
  - [corpus] Consistent human ratings across multiple datasets validate the measures.
- Break condition: If human raters are unfamiliar with domain vocabulary, their ratings may diverge from automated scores, reducing correlation.

## Foundational Learning

- Concept: Topic coherence and diversity measures
  - Why needed here: Baseline understanding of how topic quality is evaluated in static models, which TTQ extends to temporal context.
  - Quick check question: What is the difference between NPMI and Cv score in measuring topic coherence?

- Concept: Dynamic topic models (DTMs)
  - Why needed here: Understanding how topics evolve over time is essential to grasp why temporal evaluation measures are necessary.
  - Quick check question: How do D-LDA and D-ETM differ in modeling topic evolution?

- Concept: Window-based evaluation over time
  - Why needed here: Temporal measures rely on sliding windows to capture gradual or abrupt changes, so understanding window size effects is critical.
  - Quick check question: How does increasing the window size L affect the sensitivity of TTC and TTS?

## Architecture Onboarding

- Component map: Data preprocessing → Model training (D-LDA/D-ETM) → Year-wise evaluation (TC, TD, TQ) → Temporal evaluation (TTC, TTS, TTQ) → Combined evaluation (DTQ) → Human validation
- Critical path: Generate topics → Compute TTC and TTS per topic per window → Aggregate to TTQ → Combine with TQ to get DTQ → Compare across models
- Design tradeoffs:
  - Small window size: Detects rapid changes but may miss slower trends.
  - Large window size: Smooths transitions but may overlook abrupt shifts.
  - Reference corpus choice: Affects TTC computation; domain mismatch reduces reliability.
- Failure signatures:
  - TTQ high but human ratings low: Possible domain vocabulary gap or unfamiliar topics.
  - DTQ drops to zero: Topic collapse (all topics become identical).
  - Low correlation between TTC and human ratings: Possible mismatch in coherence definition.
- First 3 experiments:
  1. Shuffle topics within each year and verify TTQ drops while TQ stays constant.
  2. Add intruder words to topics and confirm TTQ decreases monotonically with intrusion level.
  3. Run human evaluation on a subset of topics and compute Spearman correlation with TTC and TTS.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the window size parameter L in the temporal topic coherence (TTC) and temporal topic smoothness (TTS) measures affect the sensitivity of detecting topic changes over time, and what is the optimal window size for different datasets?
- Basis in paper: [explicit] The paper mentions that choosing higher window sizes would make the measure more sensitive to detecting slower transitions, but sudden changes are of greater interest. It also states that depending on the dataset, the TTS measure is fairly robust with respect to the window size.
- Why unresolved: The paper does not provide a comprehensive analysis of the impact of different window sizes on the performance of TTC and TTS measures across various datasets. It only mentions that L=2 is chosen for all experiments, but does not explore the effects of other window sizes.
- What evidence would resolve it: Conducting experiments with different window sizes (e.g., L=1, 2, 3, 4) and evaluating the performance of TTC and TTS measures on various datasets would provide insights into the optimal window size for detecting topic changes over time.

### Open Question 2
- Question: How does the proposed dynamic topic quality (DTQ) measure compare to other existing evaluation measures for dynamic topic models in terms of capturing the temporal characteristics of topic changes?
- Basis in paper: [inferred] The paper mentions that the DTQ measure aggregates the temporal topic quality (TTQ) measure with the static topic quality assessment. However, it does not provide a direct comparison of DTQ with other existing evaluation measures for dynamic topic models.
- Why unresolved: The paper focuses on introducing and validating the DTQ measure but does not compare its performance against other established evaluation measures for dynamic topic models. This comparison would help in understanding the strengths and limitations of DTQ in capturing temporal characteristics.
- What evidence would resolve it: Conducting experiments that compare the DTQ measure with other existing evaluation measures (e.g., perplexity, topic coherence, topic diversity) on multiple datasets and models would provide insights into the relative performance and effectiveness of DTQ in capturing temporal topic changes.

### Open Question 3
- Question: How does the choice of reference corpus impact the performance of the temporal topic coherence (TTC) measure, and what strategies can be employed to mitigate the potential issues arising from an inappropriate reference corpus?
- Basis in paper: [explicit] The paper mentions that the TTC measure is calculated with respect to a reference corpus and acknowledges that if words or topics are not present in the reference corpus, the result will be suboptimal, especially for temporal topics where the number of documents for selected time steps can be small.
- Why unresolved: The paper does not provide a detailed analysis of how the choice of reference corpus affects the TTC measure or propose strategies to address the potential issues arising from an inappropriate reference corpus. This is a crucial aspect as the reference corpus plays a significant role in determining the quality of the TTC measure.
- What evidence would resolve it: Conducting experiments with different reference corpora (e.g., external reference corpus, time-specific reference corpus) and evaluating the performance of the TTC measure would provide insights into the impact of reference corpus choice. Additionally, exploring strategies such as corpus expansion, temporal weighting, or domain adaptation techniques to mitigate the issues arising from an inappropriate reference corpus would be valuable.

## Limitations
- Window size L for temporal measures is not specified, potentially affecting TTC and TTS sensitivity
- D-ETM hyperparameter values (delta, sigma, gamma) are not provided, which may impact model performance
- Human evaluation was conducted on a limited subset of topics, and domain expertise effects are not addressed

## Confidence
- High: The mechanisms of TTQ and DTQ combining coherence and smoothness are clearly specified and validated through synthetic experiments and human evaluation.
- Medium: The effectiveness of TTQ and DTQ in detecting topic quality changes over time is demonstrated, but the sensitivity to window size and hyperparameter settings is not explored.
- Low: The generalizability of human evaluation results across different domains and expertise levels is uncertain due to limited scope.

## Next Checks
1. Test TTQ sensitivity to window size L by computing TTC and TTS with varying L (e.g., L=2, L=5, L=10) and comparing results.
2. Evaluate the impact of hyperparameter choices (e.g., top_chain_var, delta, sigma) on D-LDA and D-ETM performance by running ablation studies.
3. Conduct human evaluation with domain experts and non-experts separately to assess how expertise affects the correlation between human ratings and automated measures.