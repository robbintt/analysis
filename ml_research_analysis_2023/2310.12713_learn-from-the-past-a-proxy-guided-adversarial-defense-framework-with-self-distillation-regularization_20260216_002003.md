---
ver: rpa2
title: 'Learn from the Past: A Proxy Guided Adversarial Defense Framework with Self
  Distillation Regularization'
arxiv_id: '2310.12713'
source_url: https://arxiv.org/abs/2310.12713
tags:
- adversarial
- defense
- framework
- methods
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LAST, a proxy-based adversarial defense framework
  for deep learning models. LAST addresses the instability and catastrophic overfitting
  issues in existing adversarial training methods by leveraging the historical states
  of the target model as a proxy to guide its updates.
---

# Learn from the Past: A Proxy Guided Adversarial Defense Framework with Self Distillation Regularization

## Quick Facts
- arXiv ID: 2310.12713
- Source URL: https://arxiv.org/abs/2310.12713
- Reference count: 20
- One-line primary result: Introduces LAST framework improving adversarial robustness with up to 9.2% and 20.3% RA gains on CIFAR10 and CIFAR100 respectively.

## Executive Summary
This paper presents LAST (Learn from the Past), a novel adversarial defense framework that addresses the instability and catastrophic overfitting issues in existing adversarial training methods. LAST leverages the historical states of the target model as a proxy to guide its updates, employing a two-stage update rule and self-distillation regularization. The framework demonstrates consistent performance improvements across various datasets, model architectures, and attack modalities, with significant enhancements in robust accuracy and training stability.

## Method Summary
LAST introduces a proxy-based adversarial defense framework that uses historical states of the target model to guide its updates. The framework employs a two-stage update rule: first estimating the next state of the target model using the proxy, then updating both models based on the differential unit derived from the estimated and current states. Additionally, LAST incorporates a self-distillation-based defense objective to regularize the proxy model's learning process, further stabilizing training and mitigating overfitting. The method is implemented by modifying existing AT methods to include proxy model, two-stage update rule, and self-distillation objective.

## Key Results
- LAST improves robust accuracy by up to 9.2% on CIFAR10 and 20.3% on CIFAR100 under PGD-10 and AutoAttack.
- The framework enhances training stability and mitigates catastrophic overfitting.
- LAST demonstrates consistent performance improvements across various datasets, model architectures, and attack modalities.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The historical states of the target model (proxy model) provide more effective initialization and defense priors against parameter-oriented adversarial attacks.
- Mechanism: The proxy model, which captures the historical states of the target model, is used to estimate the next state of the target model. This estimated state, along with the current state of the target model, is used to calculate the differential unit for updating the target model. This approach helps in generating a more robust response to the adversarial attack.
- Core assumption: The historical states of the target model contain valuable information that can help defend against parameter-oriented adversarial attacks, which are targeted at the current state of the model.
- Evidence anchors:
  - [abstract]: "By introducing the historical state of the target model as a proxy, which is endowed with much prior information for defense, we formulate a two-stage update rule, resulting in a general adversarial defense framework, which we refer to as ‘LAST’ (Learn from the Past)."
  - [section 2.2]: "To verify this hypothesis, we first generate the adversarial example, i.e., uadv = u + δK, where δK targets at the best model trained with early stopping, denoted as Tθ. Then we use proxy model to represent the historical state of target model Tθ, denoted as Pω, and use xadv to attack both Tθ and Pω."
  - [corpus]: The corpus contains papers related to adversarial training and defense, but there is no direct evidence supporting the specific claim about the effectiveness of using historical states of the target model.
- Break condition: The break condition for this mechanism would be if the proxy model fails to provide a more robust response compared to the target model itself, or if the computational cost of maintaining and updating the proxy model outweighs the benefits.

### Mechanism 2
- Claim: The self-distillation-based defense objective regularizes the proxy model's learning process, stabilizing training and mitigating catastrophic overfitting.
- Mechanism: The self-distillation-based defense objective constrains the proxy model's estimation by measuring the distance between the soft targets of the clean image and the corresponding adversarial image. This helps in maintaining consistency in the proxy model's behavior when faced with clean or adversarial examples.
- Core assumption: Constraining the proxy model's learning process with a self-distillation-based defense objective will lead to more stable training and better mitigation of catastrophic overfitting.
- Evidence anchors:
  - [abstract]: "Besides, we introduce a self-distillation regularized defense objective, ingeniously designed to steer the proxy model's update trajectory without resorting to external teacher models, thereby ameliorating the impact of catastrophic overfitting on performance."
  - [section 2.3]: "Based on the introduced proxy model, which captures the historical states to introduce prior information for defense (Step 12 in Alg. 1), we further delve into the defense objective to constrain the learning process of proxy model and alleviate the overfitting problem."
  - [corpus]: The corpus does not contain direct evidence supporting the specific claim about the effectiveness of the self-distillation-based defense objective.
- Break condition: The break condition for this mechanism would be if the self-distillation-based defense objective fails to stabilize training or mitigate catastrophic overfitting, or if it introduces significant computational overhead.

### Mechanism 3
- Claim: The two-stage update rule of the LAST framework improves the consistency among the optimization trajectories of the target model.
- Mechanism: The two-stage update rule first estimates the next state of the target model using the proxy model, and then updates both the proxy and target models based on the differential unit derived from the estimated and current states. This approach helps in maintaining consistency among the optimization trajectories.
- Core assumption: Improving the consistency among the optimization trajectories of the target model will lead to better robustness against adversarial attacks.
- Evidence anchors:
  - [abstract]: "Specifically, LAST derives response of the proxy model as dynamically learned fast weights, which continuously corrects the update direction of the target model."
  - [section 2.4]: "Assume that the target model of the critical state unexpectedly diverges, Eq. (4) could estimate ω which is more robust against this parameter-oriented perturbation to assist the updates of θi. This update rule is supposed to improve the consistency between adjacent states of the target model as the target model converges, i.e., {· · · , θi − θi−1, θi+1 − θi, · · · }."
  - [corpus]: The corpus does not contain direct evidence supporting the specific claim about the effectiveness of the two-stage update rule.
- Break condition: The break condition for this mechanism would be if the two-stage update rule fails to improve the consistency among the optimization trajectories, or if it introduces significant computational overhead.

## Foundational Learning

- Concept: Adversarial Training (AT)
  - Why needed here: AT is a prominent strategy for enhancing model robustness against adversarial attacks. Understanding AT is crucial for comprehending the limitations of existing methods and the need for the LAST framework.
  - Quick check question: What is the primary goal of Adversarial Training (AT) in the context of deep learning models?

- Concept: Min-max optimization problem
  - Why needed here: The min-max optimization problem formulation is central to AT, where the attack model aims to maximize the objective by injecting adversarial perturbations, while the defense model optimizes its parameters to stay robust against the perturbation.
  - Quick check question: How is the min-max optimization problem formulated in the context of Adversarial Training (AT)?

- Concept: Knowledge Distillation (KD)
  - Why needed here: KD is used in the self-distillation-based defense objective to constrain the proxy model's learning process. Understanding KD is essential for grasping how the self-distillation-based defense objective works.
  - Quick check question: What is the role of Knowledge Distillation (KD) in the self-distillation-based defense objective proposed in the LAST framework?

## Architecture Onboarding

- Component map: Target model (Tθ) -> Proxy model (Pω) -> Attack model -> Defense objective
- Critical path:
  1. Generate adversarial perturbations using the attack model.
  2. Use the proxy model to estimate the next state of the target model.
  3. Calculate the differential unit based on the estimated and current states of the target model.
  4. Update the proxy and target models using the differential unit.
- Design tradeoffs:
  - Computational cost vs. robustness improvement: The LAST framework introduces additional computational overhead for maintaining and updating the proxy model, but it aims to improve robustness against adversarial attacks.
  - Complexity of the update rule vs. simplicity of implementation: The two-stage update rule of the LAST framework is more complex than traditional AT methods, but it offers potential benefits in terms of robustness.
- Failure signatures:
  - If the proxy model fails to provide a more robust response compared to the target model itself, it may indicate that the historical states of the target model do not contain sufficient information for effective defense.
  - If the self-distillation-based defense objective fails to stabilize training or mitigate catastrophic overfitting, it may suggest that the objective function is not effective in constraining the proxy model's learning process.
- First 3 experiments:
  1. Implement the LAST framework on a simple dataset (e.g., MNIST) and compare its performance with traditional AT methods in terms of robustness against adversarial attacks.
  2. Analyze the convergence behavior of the target model's loss and accuracy during training with the LAST framework, and compare it with traditional AT methods.
  3. Evaluate the impact of different aggregation coefficients (γ) on the performance of the LAST framework, and identify the optimal value for a given dataset and model architecture.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different aggregation coefficients (γ) on the performance of the LAST framework across various datasets and model architectures?
- Basis in paper: [explicit] The paper mentions that the aggregation coefficient γ = 0.8 is set for quantitative experiments and discusses its influence on convergence behavior and robust accuracy.
- Why unresolved: The paper only provides results for a single value of γ and does not explore its sensitivity or optimal range across different scenarios.
- What evidence would resolve it: Conducting experiments with varying γ values for different datasets and model architectures to identify the optimal range and its impact on performance.

### Open Question 2
- Question: How does the LAST framework perform when combined with other regularization techniques or defense objectives beyond the self-distillation approach?
- Basis in paper: [inferred] The paper focuses on the self-distillation based defense objective but does not explore other potential regularization techniques or defense objectives that could be combined with the LAST framework.
- Why unresolved: The paper does not investigate the potential synergies or trade-offs of combining the LAST framework with other defense techniques.
- What evidence would resolve it: Implementing and evaluating the LAST framework with various regularization techniques or defense objectives to assess their combined effectiveness and potential improvements.

### Open Question 3
- Question: What is the computational complexity and scalability of the LAST framework when applied to larger-scale datasets and more complex model architectures?
- Basis in paper: [explicit] The paper mentions that the LAST framework has almost no additional computation cost compared to the original SAT methods, but does not provide a detailed analysis of its computational complexity or scalability.
- Why unresolved: The paper does not investigate the framework's performance on larger datasets or more complex models, which could reveal potential limitations or bottlenecks.
- What evidence would resolve it: Conducting experiments with larger datasets and more complex model architectures to measure the computational overhead and scalability of the LAST framework, and identifying potential optimization strategies.

### Open Question 4
- Question: How does the LAST framework perform under different types of adversarial attacks, such as transfer-based black-box attacks or adaptive attacks?
- Basis in paper: [explicit] The paper evaluates the framework's performance under white-box PGD and AutoAttack, but does not extensively explore its robustness against other attack types like transfer-based black-box or adaptive attacks.
- Why unresolved: The paper does not provide a comprehensive evaluation of the framework's robustness against various attack types, which could reveal potential vulnerabilities or strengths.
- What evidence would resolve it: Conducting experiments with different types of adversarial attacks, including transfer-based black-box and adaptive attacks, to assess the framework's robustness and identify potential weaknesses or areas for improvement.

## Limitations
- The effectiveness of using historical model states as proxies relies heavily on the assumption that these states contain sufficient information for defense against parameter-oriented attacks.
- The self-distillation regularization mechanism introduces additional hyperparameters (γ and β) that may require dataset-specific tuning.
- The computational overhead of maintaining and updating the proxy model is not extensively discussed, which could be a practical constraint for large-scale applications.

## Confidence
- Medium: The experimental results demonstrate consistent improvements across multiple datasets and attack scenarios, supporting the effectiveness of the two-stage update rule and self-distillation objective. However, the lack of direct comparisons with state-of-the-art proxy-based methods and limited ablation studies on individual components reduce confidence in the specific contributions of each mechanism.

## Next Checks
1. Conduct ablation studies to isolate the contributions of the two-stage update rule and self-distillation objective, comparing against versions of LAST without these components.
2. Perform sensitivity analysis on hyperparameters (γ and β) across different datasets and model architectures to establish robust tuning guidelines.
3. Compare LAST's computational overhead and training time against baseline methods to assess practical feasibility for large-scale deployment.