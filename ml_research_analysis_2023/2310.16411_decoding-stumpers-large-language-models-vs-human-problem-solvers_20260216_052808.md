---
ver: rpa2
title: 'Decoding Stumpers: Large Language Models vs. Human Problem-Solvers'
arxiv_id: '2310.16411'
source_url: https://arxiv.org/abs/2310.16411
tags:
- answer
- riddle
- llms
- correct
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates problem-solving capabilities of large language\
  \ models (LLMs) on \"stumpers\"\u2014unique single-step intuition problems that\
  \ are difficult for humans but easy to verify. It compares four state-of-the-art\
  \ LLMs (Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) against human participants."
---

# Decoding Stumpers: Large Language Models vs. Human Problem-Solvers

## Quick Facts
- arXiv ID: 2310.16411
- Source URL: https://arxiv.org/abs/2310.16411
- Reference count: 24
- Primary result: LLMs outperform humans on solving intuition problems (stumpers), but humans excel at verifying solutions.

## Executive Summary
This study compares the problem-solving capabilities of large language models (LLMs) against human participants on "stumpers"—unique single-step intuition problems that are challenging for humans but easy to verify. The research tests four state-of-the-art LLMs (Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) across both answer generation and verification tasks, using both naïve and prompted settings. Results demonstrate that new-generation LLMs significantly outperform humans in solving stumpers with accuracy rates between 26-58%, while humans show superior skills in verifying solutions. The study reveals a fundamental discrepancy between LLMs' ability to generate correct answers and their capacity to recognize correct solutions, highlighting the need for more sophisticated evaluation methods that account for this asymmetry.

## Method Summary
The study employs a controlled experimental design comparing four LLMs (Davinci-2, Davinci-3, GPT-3.5-Turbo, GPT-4) against human participants on a dataset of 76 validated stumpers plus 152 generated variants. Models are tested on both naïve and prompted settings, generating answers to problems and classifying correct solutions from pairs of options. Human participants complete the same tasks online via Prolific. Performance is evaluated across two key metrics: accuracy in answer generation (solving the problems) and accuracy in answer verification (choosing the correct solution from two options). The dataset includes both original stumpers validated by humans and enhanced variants for comprehensive testing.

## Key Results
- LLMs achieve 26-58% accuracy in solving stumpers, significantly outperforming human participants
- Humans demonstrate superior performance in verifying solutions, even when unable to solve problems themselves
- Prompting strategies have differential effects: positive impact on GPT-3 models, negative on chat models
- Significant discrepancy exists between models' ability to generate correct answers versus classify correct responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs excel at generating correct answers to stumpers due to their pattern-matching and contextual inference capabilities.
- Mechanism: The models leverage their vast training data to identify semantic cues and logical structures in the riddle, bypassing human intuitive blocks by directly inferring the correct framing.
- Core assumption: The stumper solutions are represented in the training corpus or are derivable through inference from similar patterns.
- Evidence anchors: [abstract] "new-generation LLMs excel in solving stumpers and surpass human performance", [section] "The models correctly solved 26%-58% of the stumpers, outperforming human participants"
- Break condition: If the stumper solution is entirely absent from training data and cannot be inferred from related patterns, the model's performance drops to chance levels.

### Mechanism 2
- Claim: Humans outperform LLMs in verifying solutions because they use deeper conceptual reasoning and context awareness.
- Mechanism: Humans apply System 2 reasoning to assess logical consistency and contextual fit, while LLMs rely on surface-level pattern matching which can lead to misclassification.
- Core assumption: Human verification involves a more nuanced understanding of context and logical consistency than current LLM capabilities.
- Evidence anchors: [abstract] "humans exhibit superior skills in verifying solutions to the same problems", [section] "humans demonstrated a higher proficiency in recognizing the correct answer, even when they were unable to solve the problem initially"
- Break condition: If verification requires only surface-level consistency checks, LLMs may match or exceed human performance.

### Mechanism 3
- Claim: Prompting strategies significantly affect LLM performance on stumpers, with positive effects on GPT-3 models and negative on chat models.
- Mechanism: Additional context in prompts helps GPT-3 models by providing relevant examples, while chat models may be confused by conflicting information.
- Core assumption: The effect of prompting depends on the model's training and architecture, with different models responding differently to additional context.
- Evidence anchors: [section] "The main effect of Prompt was not significant, but the interaction was, indicating that the prompt has a positive impact on the performance of the GPT-3 models and a negative impact on the performance of the chat models"
- Break condition: If the prompt structure is optimized for the specific model type, the negative impact on chat models may be mitigated.

## Foundational Learning

- Concept: System 1 and System 2 thinking
  - Why needed here: Understanding the distinction between intuitive (System 1) and analytical (System 2) reasoning is crucial for interpreting why LLMs and humans perform differently on stumpers.
  - Quick check question: Can you explain how System 1 and System 2 thinking apply to solving the "bat and ball" problem?

- Concept: Pattern matching and inference in LLMs
  - Why needed here: LLMs solve stumpers by recognizing patterns and making inferences from their training data, which is key to understanding their problem-solving mechanism.
  - Quick check question: How does an LLM's pattern-matching ability differ from human intuition when solving a stumper?

- Concept: Verification vs. generation in cognitive tasks
  - Why needed here: The study highlights a discrepancy between generating correct answers and verifying them, which is important for understanding LLM limitations.
  - Quick check question: Why might it be easier for humans to verify a solution than to generate it in the context of stumpers?

## Architecture Onboarding

- Component map: Stumper dataset (original and enhanced) → Model input → Response generation → Human evaluation
- Critical path: 1. Prepare stumper dataset and solutions 2. Configure model prompts (naïve vs. prompted) 3. Generate responses from each model 4. Evaluate responses for correctness 5. Conduct answer verification task 6. Analyze results and compare performance
- Design tradeoffs: Naïve vs. prompted prompts (naïve tests raw capability, prompted provides context but may influence performance differently across models); Original vs. enhanced dataset (original validated with humans, enhanced may differ in difficulty)
- Failure signatures: Low accuracy in answer generation (model fails to recognize correct framing); Below-chance performance in verification (model relies on incorrect heuristics); Inconsistent performance across prompts (model sensitive to prompt structure)
- First 3 experiments: 1. Compare accuracy of Davinci-2 vs. GPT-4 on original stumper dataset with naïve prompts 2. Test the effect of prompted prompts on Davinci-3's performance 3. Evaluate human vs. LLM performance on answer verification using ground truth solutions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific neural mechanisms in LLMs contribute to their superior performance in solving stumpers compared to humans?
- Basis in paper: [explicit] The paper compares LLM and human performance on stumpers but does not explore the underlying neural mechanisms.
- Why unresolved: The study focuses on behavioral outcomes rather than investigating the neural correlates or mechanisms that enable LLMs to solve stumpers more effectively than humans.
- What evidence would resolve it: Neuroimaging studies or computational analyses that identify specific neural patterns or mechanisms in LLMs associated with stumper-solving performance.

### Open Question 2
- Question: How do different prompt engineering techniques affect LLM performance on stumpers, and can optimized prompting strategies significantly improve their problem-solving abilities?
- Basis in paper: [inferred] The paper mentions that prompting had varying effects on different models but did not extensively explore prompt engineering techniques.
- Why unresolved: The study used relatively simple prompting strategies and did not systematically investigate how different prompt designs might enhance LLM performance on stumpers.
- What evidence would resolve it: Comparative studies testing various prompt engineering techniques (e.g., chain-of-thought prompting, few-shot examples) and their impact on stumper-solving accuracy.

### Open Question 3
- Question: Can the discrepancy between LLM answer generation and verification abilities be bridged through fine-tuning or architectural modifications, and what training approaches would be most effective?
- Basis in paper: [explicit] The paper highlights a significant gap between LLMs' ability to generate answers and verify solutions, suggesting this is a key limitation.
- Why unresolved: While the study identifies the discrepancy, it does not propose or test specific methods to improve verification capabilities.
- What evidence would resolve it: Experiments with targeted fine-tuning, architectural changes, or hybrid models that combine generation and verification components to improve overall stumper-solving performance.

## Limitations
- The study focuses on single-step problems, limiting understanding of LLM performance on complex, multi-step reasoning tasks
- Limited number of models tested, which may affect generalizability of findings across different LLM architectures
- Potential variability in stumper difficulty across original and enhanced datasets could influence performance comparisons

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| LLMs outperform humans in solving stumpers (26-58% accuracy) | Medium |
| Humans excel at verifying solutions despite lower solving accuracy | Medium |
| Prompting strategies have differential effects across model types | Medium |
| Discrepancy exists between generation and verification abilities | Medium |

## Next Checks

1. **Dataset Generalization Test**: Validate the findings across multiple intuition problem datasets with varying difficulty levels and solution structures to assess the robustness of LLM performance patterns.

2. **Prompt Engineering Analysis**: Systematically test different prompt formats and content structures to identify optimal prompting strategies for each model type, particularly focusing on mitigating the negative effects observed in chat models.

3. **Cross-Domain Application**: Evaluate the same models on intuition problems from different domains (e.g., social reasoning, spatial problems) to determine whether the observed performance patterns generalize beyond the specific stumper format used in this study.