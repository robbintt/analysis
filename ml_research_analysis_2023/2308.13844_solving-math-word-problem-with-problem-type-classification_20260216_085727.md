---
ver: rpa2
title: Solving Math Word Problem with Problem Type Classification
arxiv_id: '2308.13844'
source_url: https://arxiv.org/abs/2308.13844
tags:
- solver
- problem
- accuracy
- mwps
- bert2tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of math word problem (MWP) solving,
  which requires analyzing text descriptions and generating mathematical equations
  to derive solutions. Existing approaches often rely on a single solver, leading
  to poor performance due to over-fitting and inability to solve all types of MWPs.
---

# Solving Math Word Problem with Problem Type Classification

## Quick Facts
- arXiv ID: 2308.13844
- Source URL: https://arxiv.org/abs/2308.13844
- Reference count: 23
- Accuracy: 33.1% on challenging validation set

## Executive Summary
This paper addresses the challenge of solving math word problems (MWPs) by proposing an ensemble approach that combines tree-based solvers with large language models (LLMs). The key innovation is a problem type classifier that routes each MWP to the most appropriate solver based on heuristic rules, addressing the limitation of existing single-solver approaches that struggle with diverse MWP types. The ensemble framework leverages ten-fold cross-validation and voting mechanisms for tree-based solvers, while LLM solvers benefit from self-consistency sampling to improve answer selection.

## Method Summary
The Ensemble-MWP approach consists of three main components: a problem type classifier using heuristic rules to categorize MWPs, an ensemble learning framework for tree-based solvers with ten-fold cross-validation and voting, and LLM solvers enhanced with chain-of-thought prompting and self-consistency sampling. The method was evaluated on the Math23K dataset (23,162 examples) and a challenging NLPCC2023 validation set (1,200 examples), achieving significant accuracy improvements over individual solvers through specialized problem routing and ensemble aggregation.

## Key Results
- Ensemble-MWP achieves 33.1% accuracy on challenging validation set
- Outperforms individual Bert2Tree solver (28.4%) and LLM solver (9.17%)
- Demonstrates effectiveness of problem type classification and ensemble learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Problem type classification enables specialized solver assignment
- Mechanism: Heuristic rules categorize MWPs into solvable and non-solvable types for Bert2Tree vs LLM solver
- Core assumption: Clear boundary exists between solver capabilities based on MWP structure
- Evidence anchors:
  - [abstract] "We propose a problem type classifier that combines the strengths of the tree-based solver and the LLM solver"
  - [section 3.1] "we define some heuristic rules to divide MWP types into two categories"
  - [corpus] Weak - no corpus evidence directly addresses classification boundaries
- Break condition: MWP types overlap in complexity or heuristic rules fail to capture nuanced solver capabilities

### Mechanism 2
- Claim: Ensemble voting reduces overfitting and improves accuracy
- Mechanism: Ten-fold cross-validation with voting mechanism aggregates multiple Bert2Tree models
- Core assumption: Diverse model predictions capture different aspects of MWP structure
- Evidence anchors:
  - [section 3.2] "we use a ten-fold cross-validation method to avoid overfitting"
  - [section 3.2] "we use the voting mechanism of ensemble learning to improve the probability of predicting the correct answer"
  - [corpus] Weak - no corpus evidence directly addresses ten-fold cross-validation performance in MWP context
- Break condition: Voting consensus fails when models produce highly divergent predictions

### Mechanism 3
- Claim: Self-consistency sampling improves LLM answer selection
- Mechanism: Generates 20 diverse reasoning paths and selects most consistent answer
- Core assumption: Multiple reasoning paths provide more robust answer selection than greedy decoding
- Evidence anchors:
  - [section 3.3] "we leverage the SC method... which samples a diverse set of reasoning paths instead of only taking the greedy one"
  - [section 3.3] "we generated 20 answers for each MWP"
  - [corpus] Moderate - related work on self-consistency in LLMs exists but specific to MWP context is limited
- Break condition: All sampled paths produce inconsistent or incorrect answers

## Foundational Learning

- Concept: Ensemble learning principles
  - Why needed here: Multiple solvers have complementary strengths that can be combined
  - Quick check question: What is the difference between bagging and boosting ensemble techniques?

- Concept: Cross-validation methodology
  - Why needed here: Prevents overfitting when training multiple Bert2Tree models
  - Quick check question: How does ten-fold cross-validation differ from leave-one-out cross-validation?

- Concept: Chain-of-thought prompting
  - Why needed here: Enables LLM solver to perform step-by-step reasoning for MWPs
  - Quick check question: What is the key difference between standard prompting and chain-of-thought prompting?

## Architecture Onboarding

- Component map:
  - Problem Type Classifier (heuristic rules) → Bert2Tree Solver (ten-fold cross-validation + voting) OR LLM Solver (Chain-of-Thought + Self-Consistency) → Post-processing (uniform rules)

- Critical path: MWP → Type Classification → Appropriate Solver → Post-processing → Final Answer

- Design tradeoffs:
  - Bert2Tree vs LLM: Accuracy vs. computational efficiency
  - Voting mechanism: Improved accuracy vs. increased inference time
  - Self-consistency: Robustness vs. resource consumption

- Failure signatures:
  - Type classifier misclassification → Wrong solver assignment
  - Cross-validation instability → Inconsistent model performance
  - Self-consistency failure → Poor answer selection

- First 3 experiments:
  1. Validate type classifier accuracy on held-out MWP examples
  2. Compare single Bert2Tree vs ensemble performance
  3. Test LLM solver with/without self-consistency on simple MWPs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would an automatic problem type classifier be compared to the heuristic-based approach proposed in this paper?
- Basis in paper: [explicit] The paper mentions as future work the goal of developing an automatic classifier to replace the heuristic-based problem type classifier, aiming to enhance system robustness and reduce reliance on predefined rules.
- Why unresolved: The current approach relies on manually defined heuristic rules to categorize problems, which may not generalize well to new or unseen problem types. An automatic classifier would need to be developed and evaluated to determine its effectiveness.
- What evidence would resolve it: Developing and evaluating an automatic problem type classifier on a diverse set of MWP datasets, comparing its performance to the heuristic-based approach in terms of accuracy and generalization.

### Open Question 2
- Question: How would incorporating additional solver types, such as graph-based or symbolic solvers, impact the overall performance of the ensemble approach?
- Basis in paper: [inferred] The paper focuses on combining tree-based solvers and LLM solvers, but there are other types of solvers that could potentially contribute to solving MWPs more effectively.
- Why unresolved: The paper does not explore the integration of other solver types beyond tree-based and LLM solvers, leaving open the question of whether incorporating additional solvers would improve performance.
- What evidence would resolve it: Experimenting with the inclusion of other solver types, such as graph-based or symbolic solvers, in the ensemble approach and evaluating their impact on accuracy and problem-solving capabilities.

### Open Question 3
- Question: How does the ensemble approach perform on MWP datasets with different characteristics, such as varying difficulty levels or problem domains?
- Basis in paper: [explicit] The paper evaluates the ensemble approach on a challenging validation set, but does not explore its performance across different types of MWP datasets.
- Why unresolved: The effectiveness of the ensemble approach may vary depending on the characteristics of the MWP dataset, such as difficulty level or problem domain.
- What evidence would resolve it: Conducting experiments on diverse MWP datasets with varying characteristics and comparing the performance of the ensemble approach across these datasets to assess its generalizability and robustness.

## Limitations

- Heavy reliance on heuristic rules for problem type classification limits generalizability to MWPs outside training distribution
- Single dataset evaluation (Math23K) and limited validation scope raise concerns about real-world applicability
- Computational overhead of ensemble approach may restrict practical deployment in resource-constrained environments

## Confidence

- High confidence in ensemble learning framework's ability to combine solver strengths
- Medium confidence in problem type classifier effectiveness
- Medium confidence in self-consistency sampling improvements
- Low confidence in generalizability beyond Math23K dataset

## Next Checks

1. **Rule Transparency Validation**: Implement and test the proposed heuristic rules on a held-out set of MWPs from different domains to assess classification accuracy and identify failure patterns in type assignment.

2. **Cross-Dataset Generalization**: Evaluate Ensemble-MWP on additional MWP datasets (e.g., Dolphin18K, MAWPS) to determine whether the accuracy improvements generalize beyond Math23K and the NLPCC2023 validation set.

3. **Computational Overhead Analysis**: Measure the inference time and resource consumption of the ensemble approach compared to individual solvers across different hardware configurations to quantify the practical deployment trade-offs.