---
ver: rpa2
title: 'SCCA: Shifted Cross Chunk Attention for long contextual semantic expansion'
arxiv_id: '2312.07305'
source_url: https://arxiv.org/abs/2312.07305
tags:
- attention
- scca
- context
- chunk
- window
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of extending the context length
  of large language models (LLMs) by proposing a novel sparse attention pattern called
  Shifted Cross Chunk Attention (SCCA). The key idea behind SCCA is to use different
  key-value (KV) shifting strategies to extend the receptive field in each attention
  layer, allowing queries to attend to tokens outside the same window.
---

# SCCA: Shifted Cross Chunk Attention for long contextual semantic expansion

## Quick Facts
- arXiv ID: 2312.07305
- Source URL: https://arxiv.org/abs/2312.07305
- Reference count: 6
- Key outcome: SCCA extends LLaMA2 7B from 4k to 8k context using single V100 GPU

## Executive Summary
This paper addresses the challenge of extending context length in large language models through a novel sparse attention pattern called Shifted Cross Chunk Attention (SCCA). The method shifts key-value tokens across chunks in different heads, allowing queries to attend to tokens outside their immediate window without modifying the query partition. Combined with Shifted Dilated Attention (SDA) and Positional Interpolation, SCCA successfully extends LLaMA2-7B from 4k to 8k tokens on a single V100 GPU while maintaining competitive perplexity scores.

## Method Summary
The paper proposes SCCA, which extends the receptive field by shifting key-value tokens across chunks in different attention heads. By shifting K and V tokens by different distances while keeping the window partition fixed, queries can attend to tokens in other chunks. The method combines with Positional Interpolation (PI) and LoRA for efficient fine-tuning. Additionally, the paper introduces Shifted Dilated Attention (SDA) by combining Dilated Attention and Dilated Neighborhood Attention patterns. These approaches are evaluated on language modeling tasks, demonstrating effective context length extension while maintaining performance.

## Key Results
- Successfully extends LLaMA2-7B from 4k to 8k context using single V100 GPU
- Achieves competitive perplexity scores on PG19 validation and Proof-pile test sets
- Outperforms current sparse attention methods for context length extrapolation
- Demonstrates effectiveness of combining different sparse attention patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Shifting KV tokens allows queries to attend to tokens outside the same window, extending the receptive field.
- Mechanism: By shifting K and V tokens by different distances in each head, queries can attend to tokens in other chunks without modifying the query window partition.
- Core assumption: The shifted KV arrangement creates cross-chunk attention without increasing the computational complexity of the window.
- Evidence anchors:
  - [abstract] "using different key-value (KV) shift strategies to extend respective field in each attention layer"
  - [section] "we just shift K and V and keep the window partition still to make query Qci to attend Kcj where 1 <= j <= m"
  - [corpus] Weak, no direct citations of shifted KV strategies in corpus neighbors.

### Mechanism 2
- Claim: Accumulating attention results across multiple heads approximates full attention within the extended receptive field.
- Mechanism: Different heads shift KV tokens by varying distances, and their accumulated attention results simulate the effect of full attention across a larger context.
- Core assumption: Multi-head attention can effectively combine partial receptive fields to approximate global attention.
- Evidence anchors:
  - [abstract] "Both SCCA and SDA can accumulate attention results in multi head attention to obtain approximate respective field in full attention."
  - [section] "SCCA f low shift iw tokens in different head. After shifting we split into sequences and conduct window attention like Equation (6) to reduce computing memory and time cost."
  - [corpus] No direct evidence in corpus neighbors, assumption based on multi-head attention literature.

### Mechanism 3
- Claim: Combining different sparse attention patterns improves model performance in context length extrapolation.
- Mechanism: Different attention patterns capture different aspects of the input, and their combination provides a more comprehensive coverage than any single pattern.
- Core assumption: Sparse attention patterns are complementary and their combination is additive in terms of performance.
- Evidence anchors:
  - [abstract] "Different attention pattern can be combined during fine-tuning process to extrapolate context length. In this section, we combine SCCA f ixed and SDA into LongMixed."
  - [section] "We combine Dilated Attention (DA) and Dilated Neighborhood Attention (DNA) to present Shifted Dilated Attention (SDA)."
  - [corpus] Weak, no direct evidence in corpus neighbors of combining attention patterns for context extension.

## Foundational Learning

- Concept: Transformer self-attention mechanism
  - Why needed here: Understanding how self-attention works is crucial to grasp how shifting KV tokens extends the receptive field.
  - Quick check question: How does the self-attention mechanism compute the attention weights for a query with respect to keys and values?

- Concept: Sparse attention patterns
  - Why needed here: The paper builds on existing sparse attention methods and proposes new patterns to extend context length.
  - Quick check question: What are the main differences between sliding window attention and dilated attention in terms of their receptive fields?

- Concept: Positional encodings and their limitations
  - Why needed here: The paper addresses the problem of extending context length for models using RoPE, which has weak extrapolation properties.
  - Quick check question: Why does directly extrapolating context length using RoPE lead to catastrophic performance?

## Architecture Onboarding

- Component map:
  Query projection -> Shifted KV mechanism (SCCA/SDA) -> Multi-head attention accumulation -> Positional Interpolation integration -> LoRA fine-tuning

- Critical path:
  1. Input sequence is split into chunks
  2. KV tokens are shifted according to the chosen pattern (SCCA fixed, SCCA flow, or SDA)
  3. Attention is computed within each chunk
  4. Results from different heads are accumulated
  5. Output is generated and passed through the rest of the transformer layers

- Design tradeoffs:
  - Shifting KV tokens vs. shifting queries: Shifting KV maintains the original query partition, potentially preserving some alignment with positional encodings.
  - Fixed vs. flow patterns: Fixed patterns provide predictable attention, while flow patterns offer more comprehensive coverage but may be harder to implement efficiently.
  - Combining patterns: Increases coverage but also computational cost and implementation complexity.

- Failure signatures:
  - Performance degradation when context length is small: Indicates misalignment between shifted attention and positional encodings.
  - No improvement over baseline methods: Suggests that the chosen shift patterns are not effective for the given task or dataset.
  - Increased computational cost without performance gain: Implies that the extended receptive field is not being utilized effectively.

- First 3 experiments:
  1. Implement SCCA fixed pattern on a small dataset and compare perplexity with baseline sliding window attention.
  2. Implement SCCA flow pattern and measure the coverage of the extended receptive field.
  3. Combine SCCA fixed and SDA patterns (LongMixed) and evaluate the performance on context length extrapolation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SCCA and SDA compare to other sparse attention patterns like Longformer or BigBird in terms of both computational efficiency and task performance?
- Basis in paper: [inferred] The paper mentions that current sparse attention methods tend to rely on window self-attention, which blocks global information flow. SCCA and SDA are proposed to address this issue, but their performance is not compared to other sparse attention patterns like Longformer or BigBird.
- Why unresolved: The paper focuses on comparing SCCA and SDA to the S2 attention pattern used in Longlora, but does not provide a comprehensive comparison to other sparse attention methods.
- What evidence would resolve it: Conducting experiments comparing SCCA and SDA to other sparse attention patterns like Longformer or BigBird in terms of computational efficiency and task performance would provide insights into their relative strengths and weaknesses.

### Open Question 2
- Question: How does the choice of the shifting strategy in SCCA (fixed vs. flow) impact the model's ability to capture long-range dependencies and maintain performance on downstream tasks?
- Basis in paper: [explicit] The paper introduces two shifting strategies in SCCA: SCCA fixed and SCCA flow. However, it does not provide a detailed analysis of how the choice of shifting strategy affects the model's ability to capture long-range dependencies and maintain performance on downstream tasks.
- Why unresolved: While the paper presents the two shifting strategies, it does not delve into the specific impact of each strategy on the model's performance and ability to capture long-range dependencies.
- What evidence would resolve it: Conducting experiments that compare the performance of SCCA fixed and SCCA flow on various downstream tasks, particularly those requiring long-range dependencies, would shed light on the impact of the shifting strategy choice.

### Open Question 3
- Question: How does the combination of SCCA and SDA (LongMixed) perform compared to using SCCA or SDA individually, and what are the trade-offs in terms of computational efficiency and model performance?
- Basis in paper: [explicit] The paper proposes LongMixed, a combination of SCCA fixed and SDA. However, it does not provide a detailed analysis of how LongMixed performs compared to using SCCA or SDA individually, nor does it discuss the trade-offs in terms of computational efficiency and model performance.
- Why unresolved: While the paper introduces LongMixed as a combination of SCCA and SDA, it does not explore the benefits and drawbacks of this combination compared to using the individual attention patterns.
- What evidence would resolve it: Conducting experiments that compare the performance of LongMixed to SCCA and SDA individually, along with an analysis of the computational efficiency trade-offs, would provide insights into the advantages and limitations of the combined approach.

## Limitations
- Limited evaluation across different model architectures beyond LLaMA2-7B
- Insufficient analysis of computational overhead introduced by shifting mechanisms
- Lack of thorough investigation into robustness to positional encoding mismatches

## Confidence
- High Confidence: The core mechanism of shifting key-value tokens to extend the receptive field is well-justified theoretically and demonstrated through experiments.
- Medium Confidence: Implementation details and hyperparameter settings for optimal performance are not fully specified, which may affect reproducibility.
- Low Confidence: Long-term stability and robustness across diverse tasks and datasets are not evaluated.

## Next Checks
1. Evaluate the performance of SCCA and SDA patterns when applied to other model architectures beyond LLaMA2-7B, such as GPT-3 or BERT.
2. Conduct a detailed analysis of the computational overhead introduced by the shifting mechanisms, including memory usage, FLOPs, and inference latency.
3. Design experiments to test the robustness of shifted attention patterns under various conditions, such as extreme context lengths, noisy data, and adversarial attacks.