---
ver: rpa2
title: Improving Pacing in Long-Form Story Planning
arxiv_id: '2311.04459'
source_url: https://arxiv.org/abs/2311.04459
tags:
- sarah
- point2
- john
- they
- outline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CONCOCT improves pacing in long-form story planning by training
  a concreteness evaluator to judge text detail levels. The system uses this evaluator
  for vaguest-first outline expansion and concrete filtering of new nodes, maintaining
  uniform pacing.
---

# Improving Pacing in Long-Form Story Planning

## Quick Facts
- arXiv ID: 2311.04459
- Source URL: https://arxiv.org/abs/2311.04459
- Reference count: 14
- Primary result: CONCOCT improves pacing in long-form story planning by training a concreteness evaluator to judge text detail levels

## Executive Summary
This paper introduces CONCOCT, a system that improves pacing in long-form story planning by using a trained concreteness evaluator to maintain uniform granularity across story outline nodes. The approach employs vaguest-first outline expansion and concrete filtering of new nodes, ensuring consistent pacing throughout the generated outline. Human evaluation demonstrates that CONCOCT produces more consistently-paced outlines than baseline methods with over 60% improvement, while maintaining coherence, relevance, and interest.

## Method Summary
CONCOCT improves pacing by training a concreteness evaluator on paired summaries from the GPT-BOOK SUM dataset, then using this evaluator to guide vaguest-first expansion and concrete filtering during outline generation. The system builds hierarchical outlines where each expansion prioritizes the vaguest remaining leaf node, and new children must satisfy concreteness requirements relative to their parent and siblings. This approach ensures uniform granularity across the outline while allowing gradual increases in detail level as the story develops.

## Key Results
- CONCOCT generates more consistently-paced outlines than baselines (60%+ improvement)
- Human evaluators judge CONCOCT's pacing more consistent over 60% of the time
- CONCOCT maintains coherence, relevance, and interest while improving pacing
- Concreteness evaluator outperforms GPT-3.5 and GPT-4 on identifying pacing issues
- Gains extend to downstream stories generated from CONCOCT outlines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CONCOCT improves pacing by using a concreteness evaluator to maintain uniform granularity across story outline nodes.
- Mechanism: The concreteness evaluator M(t0, t1) predicts which of two passages is more concrete, enabling vaguest-first expansion where the least concrete leaf node is expanded first, and concrete filtering ensures new children are more concrete than their parent relative to other leaves.
- Core assumption: Uniform granularity (pacing) across outline nodes correlates with better story quality and human-perceived consistency.
- Evidence anchors: [abstract] "CONCOCT improves pacing in long-form story planning by training a concreteness evaluator to judge text detail levels"; [section] "Vaguest-First Expansion. Rather than using a fixed breadth-first expansion...we leverage our concreteness evaluator M to run vaguest-first expansion order"
- Break condition: If the concreteness evaluator fails to distinguish granularity levels or if uniform pacing is not optimal for story quality.

### Mechanism 2
- Claim: Dynamic concreteness scheduling ensures gradual increases in outline concreteness while maintaining flexibility.
- Mechanism: The threshold T decreases over time as remaining expansions E decrease, balancing strict concreteness requirements early with practical generation constraints later.
- Core assumption: Early outline nodes should be vaguer than later nodes, and this pattern can be enforced through threshold scheduling.
- Evidence anchors: [section] "We design a scheduler to balance how much we require the new leaves' concreteness to increase compared to their parent with each expansion"; [section] "The setting of the scheduler depends on the performance of the LLM and the difficulty of the topic"
- Break condition: If LLM performance varies significantly across topics or if the linear decrease schedule is suboptimal.

### Mechanism 3
- Claim: Filtering new outline nodes for concreteness prevents pacing inconsistencies within the generated outline.
- Mechanism: Candidate children must be more concrete than their parent node relative to other leaves (Mavg(cj; L \ { nv}) âˆ’ Mavg(nv; L \ {nv}) > T) and must not be overly similar to the parent.
- Core assumption: Adding overly similar or insufficiently concrete nodes degrades outline pacing consistency.
- Evidence anchors: [section] "We use M to filter new outline items for concreteness...Compared to baseline hierarchical outlines...humans judge CONCOCT's pacing to be more consistent over 60% of the time"; [section] "Child generation begins by proposing two or more candidate children...Each child cj must then satisfy: 1. cj should not be overly similar to nv"
- Break condition: If the concreteness evaluator becomes unreliable or if filtering becomes too restrictive and blocks valid expansions.

## Foundational Learning

- Concept: Concreteness evaluation as binary classification between passage pairs
  - Why needed here: CONCOCT requires judging which of two passages is more concrete to control outline pacing
  - Quick check question: How would you train a model to predict which of two passages is more concrete?

- Concept: Hierarchical outline expansion strategies
  - Why needed here: CONCOCT builds on hierarchical outline generation but modifies expansion order for pacing control
  - Quick check question: What are the differences between breadth-first and vaguest-first expansion in hierarchical outlines?

- Concept: Story generation from outlines
  - Why needed here: CONCOCT generates outlines that are converted to stories using DOC pipeline modifications
  - Quick check question: How does hierarchical outlining help with long-form story coherence?

## Architecture Onboarding

- Component map: GPT-BOOK SUM dataset -> Concreteness evaluator M -> Outline generator (vaguest-first expansion with concrete filtering) -> DOC pipeline -> Human evaluation interface

- Critical path: 1. Construct GPT-BOOK SUM dataset with ChatGPT summaries 2. Train concreteness evaluator M on paired summaries 3. Generate outline using vaguest-first expansion and concrete filtering 4. Convert outline to story using modified DOC pipeline 5. Evaluate pacing and quality with human annotators

- Design tradeoffs: Using ChatGPT for data generation vs. human annotations (scale vs. quality); Fixed threshold vs. dynamic scheduling for concrete filtering; Human evaluation vs. automatic metrics for pacing assessment

- Failure signatures: Evaluator predicts randomly (performance near 0.5 on classification); Outlines become too detailed or too vague in specific sections; Story generation fails to maintain pacing consistency; Human evaluations show no significant improvement over baseline

- First 3 experiments: 1. Test concreteness evaluator on GPT-BOOK SUM test set and human-marked points 2. Generate outlines with CONCOCT vs baseline for 10 premises and compare leaf counts 3. Conduct human evaluation on 20 outline pairs for pacing consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would CONCOCT perform on non-story domains like Wikipedia articles or movie scripts?
- Basis in paper: [inferred] The paper mentions CONCOCT is "designed primarily for the story domain" but notes adapting it to other domains "may require rewriting many of our prompts."
- Why unresolved: The paper only evaluates CONCOCT on story outlines and downstream stories, not other text types.
- What evidence would resolve it: Applying CONCOCT to generate outlines for Wikipedia articles or movie scripts and comparing the pacing to baseline methods.

### Open Question 2
- Question: How sensitive is CONCOCT's performance to the quality of the base LLM used for outline generation?
- Basis in paper: [explicit] The paper states "CONCOCT uses a concreteness evaluator to run a vaguest-first expansion procedure and to filter new outline items for concreteness" and uses ChatGPT for generation, but does not test other LLMs.
- Why unresolved: The paper only uses ChatGPT for outline generation and does not explore how different base LLMs would impact CONCOCT's performance.
- What evidence would resolve it: Testing CONCOCT with different base LLMs like GPT-4, Claude, etc. and comparing the pacing and other metrics to CONCOCT with ChatGPT.

### Open Question 3
- Question: What is the optimal threshold scheduling strategy for CONCOCT's concreteness filter?
- Basis in paper: [explicit] The paper describes their current scheduler but states "there is certainly room for exploration on better threshold scheduling."
- Why unresolved: The paper uses a specific scheduler but acknowledges it may not be optimal and leaves room for improvement.
- What evidence would resolve it: Testing different threshold scheduling strategies (e.g. constant threshold, exponential decay, etc.) and comparing CONCOCT's pacing and other metrics.

## Limitations

- The concreteness evaluator is trained on automatically-generated ChatGPT summaries rather than human annotations, introducing uncertainty about whether it captures human judgments of concreteness
- The dynamic threshold scheduling assumes linear decreases are optimal, though this may not generalize across all story topics or lengths
- The filtering mechanism could become overly restrictive, potentially blocking valid story developments

## Confidence

- **High Confidence:** The core claim that CONCOCT improves pacing consistency (60%+ improvement) is supported by human evaluation results showing statistically significant differences from baselines
- **Medium Confidence:** The claim that CONCOCT maintains coherence, relevance, and interest while improving pacing is supported but could benefit from larger-scale human studies across more diverse story types
- **Low Confidence:** The generalizability of the dynamic threshold scheduling across different story domains and the assumption that uniform granularity is optimal for all story types remain unproven

## Next Checks

1. **Evaluator Robustness Test:** Evaluate the concreteness evaluator on a separate human-annotated dataset of concrete vs. abstract passages to verify it captures human notions of detail level beyond the ChatGPT-generated training data
2. **Cross-Domain Pacing Validation:** Apply CONCOCT to story outlines from different genres (mystery, romance, sci-fi) and conduct human evaluations to determine if pacing improvements generalize beyond the WritingPrompts dataset
3. **Threshold Schedule Optimization:** Compare linear threshold scheduling against alternative schedules (exponential decay, adaptive based on expansion success rate) to determine if the current approach is optimal for maintaining pacing consistency