---
ver: rpa2
title: 'Cracking the Code of Negative Transfer: A Cooperative Game Theoretic Approach
  for Cross-Domain Sequential Recommendation'
arxiv_id: '2311.13188'
source_url: https://arxiv.org/abs/2311.13188
tags:
- domains
- domain
- negative
- ndcg
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CGRec, a model designed to tackle the issue
  of negative transfer in Cross-Domain Sequential Recommendation (CDSR) tasks. The
  proposed approach leverages a cooperative game-theoretic framework to quantify the
  negative transfer from one domain to another, adaptively adjusting prediction losses
  to minimize its impact.
---

# Cracking the Code of Negative Transfer: A Cooperative Game Theoretic Approach for Cross-Domain Sequential Recommendation

## Quick Facts
- arXiv ID: 2311.13188
- Source URL: https://arxiv.org/abs/2311.13188
- Reference count: 35
- Key outcome: CGRec outperforms state-of-the-art methods in cross-domain sequential recommendation by mitigating negative transfer through Shapley value-based loss re-weighting and hierarchical contrastive learning

## Executive Summary
This paper addresses the critical challenge of negative transfer in cross-domain sequential recommendation (CDSR), where information from one domain harms rather than helps recommendations in another domain. The proposed CGRec model introduces a novel cooperative game-theoretic framework that quantifies negative transfer using Shapley values and adaptively re-weights prediction losses to minimize its impact. Additionally, CGRec employs hierarchical contrastive learning to integrate coarse-level category information into fine-level item sequences, effectively mitigating negative transfer by leveraging more generalized user preferences. Experiments on two real-world datasets across ten domains demonstrate that CGRec significantly outperforms existing state-of-the-art methods, particularly in domains severely impacted by negative transfer.

## Method Summary
CGRec tackles negative transfer in CDSR through two complementary mechanisms. First, it uses cooperative game theory to compute Shapley values that quantify each domain's marginal contribution to the overall training loss, allowing the model to identify and down-weight domains causing negative transfer. Second, it implements hierarchical contrastive learning that learns representations at multiple category levels, transferring information from coarse (category-level) to fine (item-level) sequences. The model is trained on domain-hybrid sequences combining interactions from multiple domains, with loss weights dynamically adjusted based on estimated negative transfer. The approach is validated on Amazon and T-Seq datasets with more than three domains, demonstrating effectiveness in real-world multi-domain recommendation scenarios.

## Key Results
- CGRec outperforms existing state-of-the-art methods across multiple metrics (HR@5, HR@10, NDCG@5, NDCG@10, MRR) on two real-world datasets
- The model shows particularly strong performance in domains severely impacted by negative transfer, validating the effectiveness of the cooperative game-theoretic approach
- Performance improvements are consistent across different domain combinations, demonstrating the scalability of the approach beyond simple two-domain scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model reduces negative transfer by assigning lower weights to prediction losses from domains with high negative transfer.
- Mechanism: CGRec uses Shapley values from cooperative game theory to quantify each domain's marginal contribution to the overall training loss. Domains with low marginal contribution (high negative transfer) are down-weighted in the loss function, reducing their detrimental influence.
- Core assumption: Marginal contribution of a domain to the cooperative training objective is a reliable proxy for the extent of negative transfer from that domain.
- Evidence anchors:
  - [abstract]: "addresses the problem of negative transfer by assessing the extent of negative transfer from one domain to another and adaptively assigning low weight values to the corresponding prediction losses."
  - [section 4.5.3]: "the degree of the negative transfer of each domain is computed by making use of the following cooperative game scheme... negative transfer and marginal contribution are opposite concepts."
- Break condition: If the Shapley value calculation does not accurately reflect true negative transfer (e.g., due to model capacity limitations or correlation structures in the data), the loss re-weighting could misalign with the actual transfer dynamics.

### Mechanism 2
- Claim: Hierarchical contrastive learning mitigates negative transfer by leveraging more generalized user preferences at the category level.
- Mechanism: The model first learns representations at the coarsest category level (e.g., domain-level or high-level category), then progressively refines these representations at finer levels (e.g., item level), using category-level sequences that are more domain-relevant than item-level sequences.
- Core assumption: Category-level sequences capture more generalized user preferences that are less domain-specific and thus less prone to negative transfer compared to item-level sequences.
- Evidence anchors:
  - [abstract]: "a hierarchical contrastive learning approach that incorporates information from the sequence of coarse-level categories into that of fine-level categories... when implementing contrastive learning was developed to mitigate negative transfer."
  - [section 4.3]: "sequences of categories from various hierarchies (H) are trained simultaneously... coarse-grained to fine-grained categories... ensures an unbiased representation at the finest category level."
- Break condition: If the hierarchical category structure is not well-defined or does not reflect actual user preference generalization, the contrastive learning may not effectively reduce negative transfer.

### Mechanism 3
- Claim: Training on domain-hybrid sequences with more than three domains improves cross-domain transfer by modeling richer inter-domain relationships.
- Mechanism: By considering each domain as a player in a cooperative game and training on hybrid sequences from multiple domains, the model learns to balance information sharing and domain-specific characteristics more effectively than pairwise approaches.
- Core assumption: Modeling interactions among three or more domains simultaneously captures more complex dependency structures and provides a more stable estimation of negative transfer than pairwise domain interactions.
- Evidence anchors:
  - [abstract]: "we focus on the case of more than three domains for real-world applications, which has not been considered much in the previous CDSR studies."
  - [section 4.5.1]: "a cooperative game [11] is distinguished by the emphasis on cooperative behavior among clusters of players, referred to as coalitions... Each player i is a specific domain d in a sequence."
- Break condition: If the number of domains is too large relative to available data, the cooperative game framework may become computationally infeasible or the Shapley value estimation may become unreliable.

## Foundational Learning

- Concept: Cooperative game theory and Shapley values
  - Why needed here: Provides a principled way to quantify each domain's contribution to the overall recommendation performance, which is used to identify and mitigate negative transfer.
  - Quick check question: How does the Shapley value of a domain change when it is added to different coalitions of other domains?

- Concept: Hierarchical contrastive learning
  - Why needed here: Enables the model to leverage more generalized user preferences at the category level to mitigate negative transfer that occurs at the item level.
  - Quick check question: Why might category-level preferences be less susceptible to negative transfer than item-level preferences?

- Concept: Self-attention mechanisms for sequential recommendation
  - Why needed here: Captures complex dependencies within user interaction sequences while handling variable-length inputs and long-range dependencies.
  - Quick check question: How does the masking operation in self-attention ensure that only past information is used for predicting the next item?

## Architecture Onboarding

- Component map: Item Embedding Layer -> Self-Attention Encoder -> Hierarchical Prediction Layer -> Loss Re-balancing Layer -> Training Objective
- Critical path: Item Embedding → Self-Attention Encoder → Hierarchical Prediction → Loss Re-balancing → Parameter Updates
- Design tradeoffs: The model balances between capturing sequential dynamics (self-attention), mitigating negative transfer (loss re-weighting), and leveraging hierarchical information (contrastive learning). The choice of hierarchical levels and attention mechanisms affects both performance and computational cost.
- Failure signatures:
  - If negative transfer is not properly mitigated, domains with dissimilar preferences will show performance degradation similar to baseline models
  - If hierarchical contrastive learning fails, performance may not improve compared to non-hierarchical approaches
  - If Shapley value computation is unstable, the loss re-weighting may introduce noise rather than improvement

- First 3 experiments:
  1. Test performance on a simple two-domain setup with known negative transfer to verify loss re-weighting works as expected
  2. Evaluate the impact of hierarchical contrastive learning by comparing with and without category-level information
  3. Measure the stability of Shapley value estimation across different random seeds and dataset splits

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- The exact formulation of how Shapley values are computed for continuous loss functions (rather than discrete game payoffs) is not specified, which could affect the reliability of negative transfer estimation.
- The scalability analysis is limited, with experimental results only showing up to 5 domains, while the practical applicability to real-world scenarios with dozens of domains remains unproven.
- The ablation studies don't clearly isolate whether the performance gains come from the cooperative game-theoretic approach specifically or from the combination of hierarchical contrastive learning with multi-domain training in general.

## Confidence

**Confidence: Medium** - The paper demonstrates performance improvements but several key aspects remain unclear. The exact formulation of how Shapley values are computed for continuous loss functions (rather than discrete game payoffs) is not specified, which could affect the reliability of negative transfer estimation. Additionally, the hierarchical category structure and its construction methodology are not fully detailed, making it difficult to assess how well the proposed approach generalizes to domains without clear hierarchical relationships.

**Confidence: Low** - The scalability analysis is limited. While the paper claims to handle "more than three domains," experimental results only show up to 5 domains, and computational complexity of Shapley value calculation grows exponentially with the number of domains. The practical applicability to real-world scenarios with dozens of domains remains unproven.

**Confidence: Medium** - The ablation studies, while showing the importance of each component, don't clearly isolate whether the performance gains come from the cooperative game-theoretic approach specifically or from the combination of hierarchical contrastive learning with multi-domain training in general.

## Next Checks

1. **Shapley Value Stability Test**: Run the Shapley value computation across multiple random seeds and dataset splits to verify that negative transfer estimates are stable and not overly sensitive to sampling variations.

2. **Hierarchical Category Sensitivity**: Systematically vary the depth and granularity of the hierarchical category structure to determine how sensitive the model performance is to this design choice and whether the improvements persist with different category definitions.

3. **Scalability Benchmark**: Evaluate the model's performance and training time on datasets with increasing numbers of domains (e.g., 3, 5, 10, 20 domains) to empirically validate the claimed scalability and identify the practical limits of the cooperative game-theoretic approach.