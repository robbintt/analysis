---
ver: rpa2
title: 'Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and
  New Benchmarking'
arxiv_id: '2306.10453'
source_url: https://arxiv.org/abs/2306.10453
tags:
- samples
- evaluation
- negative
- setting
- hits
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies several key issues with current link prediction
  benchmarking practices: models are often undertuned, lacking unified data splits
  and evaluation metrics, and use unrealistic evaluation settings with easy negative
  samples. The authors conduct a fair comparison across multiple methods and datasets
  using consistent hyperparameter tuning and unified splits.'
---

# Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking

## Quick Facts
- arXiv ID: 2306.10453
- Source URL: https://arxiv.org/abs/2306.10453
- Reference count: 40
- Primary result: Current link prediction benchmarks use easy negative samples and undertuned models; HeaRT provides harder, personalized negatives revealing simple models perform better than previously shown

## Executive Summary
This paper identifies critical flaws in current graph neural network (GNN) link prediction benchmarking practices. The authors demonstrate that existing evaluations use undertuned models, lack standardized data splits, and employ unrealistic settings with globally sampled easy negative examples. Through rigorous hyperparameter tuning and unified benchmarking across multiple datasets, they show simple models like GCN and GAE can achieve state-of-the-art performance when properly configured. The paper introduces Heuristic Related Sampling Technique (HeaRT), a new evaluation setting that generates personalized, challenging negative samples using multiple heuristics. HeaRT provides more realistic evaluation conditions, reduces model variance, and reveals that current methods may be benefiting from artificially easy negative samples.

## Method Summary
The authors conduct a comprehensive benchmarking study of GNN link prediction methods across five datasets (Cora, Citeseer, Pubmed, ogbl-collab, ogbl-ddi, ogbl-ppa, ogbl-citation2). They implement rigorous hyperparameter tuning for multiple models including GCN, GAT, SAGE, GAE, SEAL, BUDDY, Neo-GNN, NCN, NCNC, NBFNet, and PEG. The study contrasts existing evaluation methods using global random negative sampling with their proposed HeaRT approach. HeaRT generates personalized negative samples for each positive example by ranking potential negatives using three heuristics (RA, PPR, feature similarity), combining ranks via Borda's method, and selecting the top candidates. Models are evaluated using AUC, MRR, and Hits@K metrics under both settings.

## Key Results
- Simple models (GCN, GAE) achieve state-of-the-art performance when properly tuned under existing evaluation
- HeaRT reduces model variance and shows simple models perform better relative to complex ones
- Current evaluation setting uses unrealistic global negative sampling that creates artificially easy tasks
- Properly tuned models show poor correlation between validation and test performance on ogbl-ddi dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HeaRT reduces model variance by personalizing negative samples to each positive sample.
- Mechanism: By generating negative samples specific to each positive sample through corruption-based sampling, HeaRT ensures that positive and negative samples are more directly comparable, leading to more stable ranking metrics across different seeds.
- Core assumption: The alignment between positive and negative samples in terms of node involvement directly impacts the stability of ranking metrics.
- Evidence anchors:
  - [abstract]: "Our new evaluation setting helps promote new challenges and opportunities in link prediction by aligning the evaluation with real-world situations."
  - [section]: "We posit that this observation is caused by a stronger alignment between the positive and negative samples under our new evaluation setting."
  - [corpus]: Weak - no direct evidence found in corpus.
- Break condition: If the personalization process introduces sampling bias that makes some positive samples inherently easier to rank than others.

### Mechanism 2
- Claim: HeaRT uses heuristics to select harder negative samples, making the evaluation more challenging and realistic.
- Mechanism: By employing multiple heuristics (RA, PPR, feature similarity) to rank potential negative samples and selecting the top K/2 for each node, HeaRT ensures that the negative samples are non-trivial and related to the positive sample.
- Core assumption: Heuristics that correlate well with link prediction performance can effectively identify challenging negative samples.
- Evidence anchors:
  - [abstract]: "HeaRT provides a more realistic evaluation that reduces model variance and shows simple models perform better relative to complex ones."
  - [section]: "We utilize heuristics as they correlate well with link prediction performance... Furthermore, this is common in real-world applications."
  - [corpus]: Weak - limited evidence of heuristic effectiveness in corpus.
- Break condition: If the heuristic-based selection consistently fails to identify challenging negatives for certain graph structures.

### Mechanism 3
- Claim: The existing evaluation setting's use of global negative sampling creates an unrealistic and easy evaluation scenario.
- Mechanism: By using the same random negative samples for all positive samples, the existing setting often pairs unrelated nodes, making the classification task too easy and not aligned with real-world usage.
- Core assumption: In real-world applications, link prediction typically involves predicting links for specific nodes, not globally ranking arbitrary node pairs.
- Evidence anchors:
  - [abstract]: "The current evaluation setting uses the same set of randomly selected negative samples for each positive sample."
  - [section]: "We identify two potential problems with the current evaluation procedure... (1) It is not aligned with real-world settings."
  - [corpus]: Weak - no direct evidence found in corpus.
- Break condition: If the corruption-based approach in HeaRT fails to capture the same distribution of node relationships as the original graph.

## Foundational Learning

- Concept: Graph neural networks and their application to link prediction
  - Why needed here: The paper evaluates GNN-based link prediction methods and introduces a new evaluation setting.
  - Quick check question: What is the key difference between GNNs and traditional graph methods for link prediction?

- Concept: Evaluation metrics for link prediction (AUC, MRR, Hits@K)
  - Why needed here: The paper uses these metrics to compare model performance under different evaluation settings.
  - Quick check question: Why might AUC be considered a poor metric for link prediction on imbalanced datasets?

- Concept: Negative sampling strategies in link prediction
  - Why needed here: The paper contrasts the existing global negative sampling with HeaRT's personalized approach.
  - Quick check question: How does the existing evaluation setting's negative sampling differ from the proposed HeaRT approach?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model training -> Evaluation (Existing/HeaRT) -> Results analysis

- Critical path:
  1. Load dataset and create unified splits
  2. Train models with hyperparameter tuning
  3. Evaluate using existing setting
  4. Generate HeaRT negatives for each positive sample
  5. Evaluate using HeaRT setting
  6. Compare results and analyze variance

- Design tradeoffs:
  - Computational cost vs. realism: HeaRT is more expensive but provides more realistic evaluation
  - Number of heuristics vs. diversity: More heuristics provide better coverage but increase complexity
  - Personalization vs. standardization: HeaRT is tailored to each sample but makes direct comparison harder

- Failure signatures:
  - High variance in results suggests poor alignment between positive and negative samples
  - Consistently low performance across methods might indicate overly challenging negative samples
  - Large discrepancy between AUC and ranking metrics suggests metric misalignment

- First 3 experiments:
  1. Reproduce existing evaluation results on a small dataset (e.g., Cora) to establish baseline
  2. Implement HeaRT generation and verify it produces diverse, related negative samples
  3. Compare variance reduction by running both evaluations with multiple seeds on the same model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance gap between simple and complex models change when using more realistic negative sampling techniques?
- Basis in paper: [explicit] The paper shows that under HeaRT, simple models (heuristic, embedding, GNN) outperform more complex GNN+Pairwise Info methods on multiple datasets, suggesting current methods benefit from easy negatives.
- Why unresolved: While the paper demonstrates this gap under HeaRT, it doesn't explore how this gap might vary with different negative sampling strategies or how to optimally balance between simple and complex methods.
- What evidence would resolve it: Comparative studies testing various negative sampling strategies (including but not limited to HeaRT) and analyzing how model performance rankings change across different approaches.

### Open Question 2
- Question: What causes the poor relationship between validation and test performance on the ogbl-ddi dataset under the existing evaluation setting?
- Basis in paper: [explicit] The paper observes that validation and test performance are poorly correlated on ogbl-ddi, making hyperparameter tuning difficult, but doesn't identify the root cause.
- Why unresolved: The paper identifies the problem exists but doesn't investigate the underlying reasons why this dataset specifically shows this behavior while others don't.
- What evidence would resolve it: Analysis of dataset characteristics, training dynamics, or evaluation procedures that specifically explain why ogbl-ddi shows this correlation issue.

### Open Question 3
- Question: How can the efficiency of HeaRT be improved without sacrificing the quality of negative samples?
- Basis in paper: [inferred] The paper notes that HeaRT generates more negative samples than the existing setting, which could impact evaluation efficiency, but doesn't propose solutions.
- Why unresolved: While the paper identifies the efficiency concern, it doesn't explore optimization strategies for the negative sample generation process.
- What evidence would resolve it: Development and comparison of optimized HeaRT variants that maintain hard negative sample quality while reducing computational overhead through techniques like parallelization, caching, or adaptive sampling.

## Limitations
- Limited theoretical justification for why specific heuristics correlate with link prediction difficulty
- Claims about real-world alignment lack user studies or deployment validation
- Heuristic effectiveness may vary across different graph structures not tested in the study

## Confidence
- Variance reduction claims: Medium confidence - empirical evidence shown but mechanism not fully understood
- Simple models achieving SOTA: High confidence - demonstrated through rigorous hyperparameter tuning
- Real-world alignment claims: Low confidence - based on intuition rather than empirical validation

## Next Checks
1. Run HeaRT evaluation on both homophilic (Cora, Citeseer) and heterophilic graphs to determine if variance reduction holds across different graph structures
2. Systematically remove each heuristic from HeaRT and measure the impact on both performance and variance to isolate which heuristics contribute most
3. Design a user study or case study where practitioners attempt to use link prediction results for downstream tasks, comparing the utility of HeaRT-generated rankings versus traditional evaluation rankings in actual application scenarios