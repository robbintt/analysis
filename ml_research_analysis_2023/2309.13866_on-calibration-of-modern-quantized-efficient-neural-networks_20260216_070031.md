---
ver: rpa2
title: On Calibration of Modern Quantized Efficient Neural Networks
arxiv_id: '2309.13866'
source_url: https://arxiv.org/abs/2309.13866
tags:
- calibration
- arxiv
- quantization
- accuracy
- http
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the calibration properties of quantized\
  \ efficient neural networks across different precisions and architectures. We analyze\
  \ three architectures\u2014ShuffleNetv2, GhostNet-VGG, and MobileOne\u2014on CIFAR-100\
  \ and PathMNIST datasets."
---

# On Calibration of Modern Quantized Efficient Neural Networks

## Quick Facts
- arXiv ID: 2309.13866
- Source URL: https://arxiv.org/abs/2309.13866
- Reference count: 37
- Primary result: Calibration quality correlates with quantization quality, with lower precisions exhibiting worse calibration.

## Executive Summary
This study investigates how quantization affects the calibration properties of efficient neural networks. We analyze three architectures—ShuffleNetv2, GhostNet-VGG, and MobileOne—on CIFAR-100 and PathMNIST datasets across different quantization precisions (fp32, W8A8, W4A8, W4A4). Our results show that calibration error increases as quantization precision decreases, tracking the degradation in model performance. GhostNet-VGG demonstrates the most robustness to quantization-induced calibration degradation, while architectures with depthwise-separable convolutions (ShuffleNetV2 and MobileOne) suffer more severely. Temperature scaling can improve calibration for quantized networks, but its effectiveness is limited when initial model performance is poor.

## Method Summary
We trained ShuffleNetv2, GhostNet-VGG, and MobileOne architectures using SGD+momentum (β=0.9), He initialization, cross-entropy loss, weight decay 1e-4, and learning rate 0.1 with cosine schedule and warm restarts. Models were trained for 200 epochs with batch size 32 and random crop/flip augmentations on CIFAR-100 and PathMNIST datasets. We applied symmetric per-channel weight quantization and asymmetric per-tensor activation quantization at different precisions, then evaluated calibration using Expected Calibration Error (ECE) and accuracy metrics. Temperature scaling was applied post-hoc to improve calibration.

## Key Results
- Calibration error increases systematically as quantization precision decreases (fp32 → W8A8 → W4A8 → W4A4)
- GhostNet-VGG is most robust to both quantization-induced accuracy decrease and ECE increase
- Temperature scaling improves calibration for 8-bit models but fails to correct 4-bit MobileOne models with large initial confidence-accuracy gaps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Calibration error increases as quantization precision decreases, tracking model performance degradation.
- Mechanism: Lower precision quantization introduces quantization noise that distorts confidence estimates, causing larger confidence-accuracy gaps.
- Core assumption: Quantization noise affects the model's confidence calibration independently of its classification accuracy.
- Evidence anchors:
  - [abstract] "The quality of calibration is observed to track the quantization quality; it is well-documented that performance worsens with lower precision, and we observe a similar correlation with poorer calibration."
  - [section] Figure 1 shows ECE increases as precision decreases (fp32 → W8A8 → W4A8 → W4A4).
  - [corpus] Weak evidence - corpus focuses on quantization efficiency but not calibration quality.
- Break condition: If calibration error does not correlate with performance degradation under different quantization schemes.

### Mechanism 2
- Claim: Temperature scaling improves calibration for quantized models but is limited by initial model performance.
- Mechanism: Temperature scaling adjusts the confidence scores to better match accuracy, but cannot recover calibration if the model is severely degraded by quantization.
- Core assumption: Temperature scaling can correct calibration errors up to the limit imposed by the model's base performance.
- Evidence anchors:
  - [abstract] "We find that temperature scaling can improve calibration error for quantized networks, with some caveats."
  - [section] "TS is unable to correct the 4-bit MobileOne models that have a larger initial confidence-accuracy gap."
  - [corpus] Weak evidence - corpus does not discuss temperature scaling effectiveness.
- Break condition: If temperature scaling fails to improve calibration even for models with reasonable initial performance.

### Mechanism 3
- Claim: Architectures with depthwise-separable convolutions (inverted residual blocks) suffer more from quantization-induced calibration degradation.
- Mechanism: Depthwise-separable convolutions accumulate more quantization error, leading to larger confidence-accuracy gaps in quantized models.
- Core assumption: The structure of depthwise-separable convolutions makes them more susceptible to quantization noise affecting calibration.
- Evidence anchors:
  - [section] "Notably, these two backbones consist of inverted residual blocks employing depthwise-separable convolutions, which have been shown to suffer from larger accumulation of quantization errors."
  - [section] "We observe that GhostNet-VGG is the most robust to both quantization-induced accuracy decrease and ECE increase, while lower-bit ShuffleNetV2 and MobileOne model performance suffers."
  - [corpus] Weak evidence - corpus focuses on quantization efficiency but not architecture-specific calibration effects.
- Break condition: If architectures with depthwise-separable convolutions show similar calibration behavior to other architectures under quantization.

## Foundational Learning

- Concept: Post-training quantization
  - Why needed here: Understanding how quantization affects model accuracy and calibration is central to the study.
  - Quick check question: What is the difference between symmetric and asymmetric quantization in terms of their effect on model calibration?

- Concept: Expected Calibration Error (ECE)
  - Why needed here: ECE is the primary metric used to evaluate calibration quality in the study.
  - Quick check question: How does ECE measure the discrepancy between a model's confidence and its accuracy?

- Concept: Temperature scaling
  - Why needed here: Temperature scaling is used as a post-hoc calibration method to improve model calibration.
  - Quick check question: What is the mathematical form of temperature scaling and how does it adjust model outputs?

## Architecture Onboarding

- Component map: ShuffleNetv2 (AugShuffleNet variant) -> GhostNet-VGG (VGG16-like) -> MobileOne S0 -> CIFAR-100/PathMNIST datasets
- Critical path: 1) Train baseline models at fp32 precision, 2) Quantize models at different precisions, 3) Evaluate calibration using ECE, 4) Apply temperature scaling and re-evaluate calibration
- Design tradeoffs: Choosing quantization precision involves balancing model size/computation efficiency against accuracy and calibration quality
- Failure signatures: Large confidence-accuracy gaps, high ECE scores, and poor temperature scaling recovery indicate calibration issues
- First 3 experiments:
  1. Train and evaluate fp32 baseline models on CIFAR-100 and PathMNIST2D.
  2. Quantize baseline models to W8A8 precision and evaluate calibration using ECE.
  3. Apply temperature scaling to W8A8 models and measure improvement in calibration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do intrinsic architectural properties beyond model size and pretraining influence calibration quality in quantized networks?
- Basis in paper: [explicit] "Minderer et al. [4] has suggested that architecture is a key determinant of calibration properties. The exact factors are unclear, though they do find that it is not related to model size nor amount of pretraining."
- Why unresolved: The paper identifies architecture as important but doesn't specify which architectural features matter. GhostNet-VGG showed better calibration robustness than ShuffleNetV2 and MobileOne, but the underlying reasons weren't fully explored.
- What evidence would resolve it: Systematic ablation studies varying architectural components (depthwise convolutions, residual connections, activation functions) across models of similar size, comparing calibration at different quantization levels.

### Open Question 2
- Question: Why does temperature scaling fail to improve calibration for 4-bit weight MobileOne models despite working for other architectures?
- Basis in paper: [explicit] "TS is unable to correct the 4-bit MobileOne models that have a larger initial confidence-accuracy gap" and "their calibration quality appears to drop as well" for well-calibrated floating point models post-quantization.
- Why unresolved: The paper notes this failure but doesn't investigate the mechanism behind it. It's unclear whether this is specific to MobileOne's architecture, the extreme quantization level, or some interaction between them.
- What evidence would resolve it: Analysis of confidence distributions and calibration curves at different temperature values for the failed cases, comparison with successful cases, and investigation of quantization error propagation through the MobileOne architecture.

### Open Question 3
- Question: How do dataset characteristics interact with quantization and calibration properties across different architectures?
- Basis in paper: [inferred] The paper notes "We also find the large discrepancy in quantization performance of ShuffleNetV2 particularly interesting between PathMNIST and CIFAR-100 settings" suggesting dataset-specific effects.
- Why unresolved: The paper observes different behavior across datasets but doesn't investigate why certain architectures perform differently on different data distributions, particularly for the PathMNIST vs CIFAR-100 comparison.
- What evidence would resolve it: Controlled experiments varying dataset properties (class balance, image complexity, domain similarity) while holding architecture and quantization constant, measuring both accuracy and calibration metrics.

## Limitations
- The study only examines three architectures on two datasets, which may not generalize across all efficient neural network designs.
- Results might differ with alternative quantization approaches beyond the single PTQ method used.
- The correlation between calibration error and quantization precision cannot definitively establish causation due to potential confounding factors.

## Confidence
**High confidence:** The observation that calibration error increases with quantization precision reduction is well-supported by consistent ECE measurements across all architectures and datasets. The temperature scaling effectiveness is also reliably demonstrated for 8-bit models.

**Medium confidence:** The claim about GhostNet-VGG being most robust to quantization effects is supported by the data but may be architecture-specific. The mechanism linking depthwise-separable convolutions to calibration degradation is plausible but not definitively proven.

**Low confidence:** The hypothesis that temperature scaling limitations are fundamentally tied to initial model performance rather than implementation specifics of the scaling process remains speculative without further ablation studies.

## Next Checks
1. **Cross-architecture validation:** Test the calibration-quantization relationship on additional efficient architectures (e.g., EfficientNet, MobileNetV3) to verify if the observed patterns hold universally across different network designs.

2. **Alternative quantization methods:** Reproduce key experiments using different quantization approaches (quantization-aware training, integer-only quantization) to determine if the calibration degradation is specific to the PTQ method used.

3. **Calibration-aware training:** Implement training procedures that explicitly optimize for calibration quality alongside accuracy to test whether poor calibration at low precision is inevitable or can be mitigated through better training methodology.