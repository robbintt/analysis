---
ver: rpa2
title: 'N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics'
arxiv_id: '2310.18679'
source_url: https://arxiv.org/abs/2310.18679
tags:
- feedback
- language
- llms
- output
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces N-CRITICS, a self-correction framework for
  Large Language Models (LLMs) that leverages an ensemble of distinct LLMs to provide
  feedback and refine outputs. Inspired by human self-correction behavior, the method
  iteratively refines LLM-generated content by soliciting critiques from multiple
  open-source models and using the feedback to improve the output.
---

# N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics

## Quick Facts
- **arXiv ID**: 2310.18679
- **Source URL**: https://arxiv.org/abs/2310.18679
- **Reference count**: 14
- **Primary result**: N-CRITICS improves LLM output quality by iteratively refining content using feedback from an ensemble of distinct open-source critic models, achieving state-of-the-art performance in toxicity reduction and factual hallucination correction without requiring training or fine-tuning.

## Executive Summary
N-CRITICS introduces a novel self-correction framework that leverages an ensemble of distinct open-source LLMs to provide iterative feedback and refine outputs. Inspired by human self-correction behavior, the method addresses toxicity and factual hallucination issues in LLM outputs through a model-agnostic approach that requires no training or fine-tuning. The framework shows consistent performance improvements across multiple tasks and datasets, outperforming existing state-of-the-art methods while maintaining applicability across different base models.

## Method Summary
N-CRITICS implements an iterative refinement process where a primary LLM generates initial content, which is then evaluated by an ensemble of distinct open-source critic models. Each critic provides independent feedback highlighting errors or areas for improvement, and this feedback is aggregated to create a refined prompt for the primary model. The process repeats for a fixed number of iterations or until stopping criteria are met. The approach is model-agnostic and works across different tasks including toxicity reduction and factual hallucination correction, using only open-source models without requiring parameter updates or fine-tuning.

## Key Results
- Achieves state-of-the-art performance in reducing toxicity on REALTOXICITYPROMPTS dataset
- Improves factual accuracy with higher Exact Match (EM) and F1 scores on TriviaQA, AmbigNQ, and HotpotQA
- Demonstrates consistent performance improvements across multiple base models (Koala-13b, Vicuna-13b, Wizard-13b)
- Successfully reduces toxicity below 10% threshold in most cases while maintaining output quality

## Why This Works (Mechanism)

### Mechanism 1
Using multiple distinct open-source LLMs as critics provides diverse feedback that helps the primary model refine its outputs more effectively than using a single critic. Each critic LLM evaluates the current output independently and generates feedback based on its own reasoning, leading to complementary critiques that highlight different aspects of errors.

### Mechanism 2
Iterative refinement using feedback from critics progressively improves output quality across multiple dimensions. The primary model generates an initial output, receives critiques from the ensemble, incorporates this feedback into a revised prompt, and regenerates the output. This cycle repeats for a fixed number of iterations or until a stopping criterion is met.

### Mechanism 3
The ensemble approach is model-agnostic and works across different base models and task types without requiring training or fine-tuning. By using open-source models as critics and the base model only for generation, the approach can be applied to any LLM without access to its internal parameters or training pipeline.

## Foundational Learning

- **Concept**: Chain-of-thought reasoning and in-context learning
  - **Why needed here**: The approach uses few-shot and zero-shot prompting to set up the initial context and guide both the primary model and critics in their tasks
  - **Quick check question**: How does the "Let's think step by step" prompt affect the quality of initial outputs in zero-shot scenarios?

- **Concept**: Ensemble methods and diversity in machine learning
  - **Why needed here**: Understanding why multiple diverse critics can provide better feedback than a single critic, drawing parallels to ensemble learning in other ML contexts
  - **Quick check question**: What properties make an ensemble of critics more effective than using the same model for both generation and critique?

- **Concept**: Iterative refinement and stopping criteria
  - **Why needed here**: The approach relies on knowing when to stop the refinement process to avoid over-correction or infinite loops
  - **Quick check question**: How do you determine optimal stopping criteria that balance improvement against computational cost?

## Architecture Onboarding

- **Component map**: Input prompt → Primary LLM (generation) → Ensemble of critic LLMs (evaluation) → Feedback aggregator → Refined prompt → Primary LLM (regeneration) → Output

- **Critical path**: Prompt generation → Initial output generation → Feedback collection from all critics → Feedback aggregation → Prompt refinement → Output regeneration → Quality check → Iterate or output

- **Design tradeoffs**:
  - More critics = better feedback diversity but higher latency and cost
  - More iterations = potentially better quality but risk of over-correction and increased computation
  - Using larger critic models = potentially better critiques but higher resource requirements
  - Open-source only constraint = avoids proprietary dependencies but may limit critic quality

- **Failure signatures**:
  - Feedback loops where critiques contradict each other without resolution
  - Stagnation where outputs stop changing despite poor quality
  - Amplification of errors when critics share similar biases
  - Resource exhaustion from excessive iterations or too many critics

- **First 3 experiments**:
  1. Single critic vs. ensemble comparison: Run N-CRITICS with 1 critic vs. 3 critics on toxicity reduction task to measure feedback diversity impact
  2. Iteration limit sensitivity: Test toxicity reduction with iteration limits of 2, 4, and 6 to find optimal stopping point
  3. Base model agnosticism: Apply N-CRITICS to three different base models (Koala-13b, Vicuna-13b, Wizard-13b) on the same factual hallucination task to verify model-agnostic claims

## Open Questions the Paper Calls Out

The paper identifies several key open questions including how N-CRITICS performance varies when using different combinations of critic LLMs, what the optimal stopping criterion is for the iterative refinement process, and how the approach performs on multilingual datasets compared to English-only tasks. The paper acknowledges that current experiments predominantly use English datasets and the method's effectiveness in non-English contexts remains unknown.

## Limitations

- Computational cost increases significantly with multiple large LLMs in iterative loops
- Effectiveness depends heavily on diversity and quality of critic models, which isn't systematically analyzed
- Relies entirely on automated metrics without human evaluation of refinement quality
- Open questions remain about multilingual performance and optimal stopping criteria

## Confidence

**High confidence**: The general mechanism of using ensemble feedback for iterative refinement is sound and supported by established ensemble learning principles.

**Medium confidence**: The claim that N-CRITICS outperforms state-of-the-art methods is supported by reported results, but comparison baselines and significance testing could be more robust.

**Low confidence**: The assertion that no training or fine-tuning is required needs more scrutiny, as the approach may implicitly learn patterns through repeated iterations.

## Next Checks

1. **Diversity analysis of critic feedback**: Run ablation studies with different numbers and combinations of critic models to quantify how feedback diversity correlates with output quality improvements.

2. **Human evaluation of refinement quality**: Conduct human studies comparing N-CRITICS outputs against baselines on the same prompts, focusing on whether critiques actually improve reasoning quality.

3. **Computational efficiency benchmarking**: Measure total computational cost of N-CRITICS versus alternative approaches across different iteration limits and critic ensemble sizes to establish practical deployment boundaries.