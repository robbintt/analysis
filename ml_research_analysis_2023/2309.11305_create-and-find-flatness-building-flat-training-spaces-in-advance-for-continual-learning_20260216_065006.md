---
ver: rpa2
title: 'Create and Find Flatness: Building Flat Training Spaces in Advance for Continual
  Learning'
arxiv_id: '2309.11305'
source_url: https://arxiv.org/abs/2309.11305
tags:
- task
- learning
- flatness
- flat
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in continual learning
  by proposing C&F, a framework that builds flat training spaces in advance for each
  task. The core idea is to create flat regions around minima in the loss landscape
  during current task learning and find parameter importance based on flatness degrees.
---

# Create and Find Flatness: Building Flat Training Spaces in Advance for Continual Learning

## Quick Facts
- arXiv ID: 2309.11305
- Source URL: https://arxiv.org/abs/2309.11305
- Reference count: 40
- Primary result: C&F framework achieves state-of-the-art performance on text classification and NER tasks by creating flat training spaces to mitigate catastrophic forgetting

## Executive Summary
This paper addresses catastrophic forgetting in continual learning by proposing C&F, a framework that builds flat training spaces in advance for each task. The core idea is to create flat regions around minima in the loss landscape during current task learning and find parameter importance based on flatness degrees. When adapting to new tasks, constraints are applied according to flatness, preparing flat spaces for subsequent tasks. The framework theoretically demonstrates consistency between created and found flatness. Experiments show C&F achieves state-of-the-art performance as a standalone method and when integrated with other techniques, effectively mitigating forgetting while enabling new task learning.

## Method Summary
C&F operates through two phases: Create and Find. During the Create phase, it adaptively creates a flat region around the current task's minimum by optimizing for the point with the largest loss value within a neighborhood. This flat region acts as a buffer zone where subsequent parameter updates have minimal impact on previous task performance. The Find phase uses Fisher Information as a rapid approximation of parameter flatness, accumulating values across tasks to track long-term importance. When training on new tasks, the framework applies hard constraints (parameter clamping) and soft constraints (L2 regularization weighted by Fisher values) to protect important parameters while learning new information.

## Key Results
- C&F achieves state-of-the-art performance on text classification and NER tasks
- The framework demonstrates strong intransigence (ability to learn new tasks) and forgetting resistance
- C&F performs well both as a standalone method and when integrated with other continual learning techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Creating flat regions around minima in the loss landscape before learning new tasks reduces catastrophic forgetting.
- Mechanism: The framework adaptively creates a flat region around the current task's minimum by optimizing for the point with the largest loss value within a neighborhood. This flat region acts as a buffer zone where subsequent parameter updates have minimal impact on previous task performance.
- Core assumption: The loss landscape around a flat minimum has uniformly low loss values, so parameter perturbations within this region don't significantly degrade performance on previous tasks.
- Evidence anchors:
  - [abstract] "our framework adaptively creates a flat region around the minimum in the loss landscape"
  - [section 3.2.1] "To ensure that the parameter points around minimum all possess low loss values, we optimize for the point with the largest loss value in the entire neighborhood"
  - [corpus] Weak - corpus contains papers about finding flat minima but none specifically about creating flat training spaces in advance
- Break condition: If the created flat region is too small or the loss landscape is too irregular, parameter updates during new task learning might escape the flat region and cause significant forgetting.

### Mechanism 2
- Claim: Fisher Information can serve as a reliable indicator of parameter importance based on flatness degrees.
- Mechanism: Parameters with smaller Fisher values are located in flatter regions and can be updated more substantially without affecting previous task performance. The framework accumulates Fisher values across tasks to track long-term importance.
- Core assumption: Fisher Information accurately approximates the flatness of each parameter, with smaller values indicating flatter regions.
- Evidence anchors:
  - [abstract] "it finds the parameters' importance to the current task based on their flatness degrees"
  - [section 3.2.2] "we use Fisher Information as a rapid approximation of flatness [32]"
  - [section 3.4] "We have validated the use of Fisher Information as the flatness indicator in our framework both theoretically (§3.4) and experimentally (§5.4.1)"
- Break condition: If Fisher Information doesn't correlate well with actual flatness (e.g., in highly non-linear regions), the importance estimates could be inaccurate and lead to suboptimal parameter updates.

### Mechanism 3
- Claim: The created and found flatness are consistent, validating the use of Fisher Information as the flatness indicator.
- Mechanism: The theoretical proof shows that minimizing sharpness (which creates flat regions) is equivalent to minimizing the trace of the Fisher Information matrix, establishing consistency between the Create and Find phases.
- Core assumption: The first-order approximation used in the Create phase accurately captures the relationship between sharpness and Fisher Information.
- Evidence anchors:
  - [section 3.4] "The selection of Fisher Information as the flatness indicator is due to its consistency with Create concerning flatness"
  - [section 3.4] "This reveals that minimizing sharpness essentially minimizes the trace of the Fisher Information matrix"
- Break condition: If the first-order approximation breaks down in high-curvature regions or the relationship between sharpness and Fisher Information is non-linear, the theoretical consistency might not hold in practice.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: The paper directly addresses this problem and proposes a solution based on flatness properties
  - Quick check question: What happens to a neural network's performance on previous tasks when it's trained on new tasks without any regularization?

- Concept: Loss landscape geometry and flatness
  - Why needed here: The entire framework is built around creating and finding flat regions in the loss landscape
  - Quick check question: How does a flat minimum in the loss landscape differ from a sharp minimum in terms of parameter sensitivity?

- Concept: Fisher Information and its relationship to parameter importance
  - Why needed here: Fisher Information is used as the primary indicator of parameter flatness and importance
  - Quick check question: What does Fisher Information measure in the context of neural network parameters?

## Architecture Onboarding

- Component map: Create phase -> Find phase -> Constraint application -> New task optimization
- Critical path: Create → Find → Constraint application → New task optimization
- Design tradeoffs:
  - Creating flat regions vs. faster convergence to minima
  - Parameter sparsity vs. comprehensive protection against forgetting
  - Replay frequency vs. computational efficiency
- Failure signatures:
  - Performance degradation on previous tasks despite flat region creation
  - Inconsistent behavior between theoretical predictions and empirical results
  - High computational overhead preventing practical deployment
- First 3 experiments:
  1. Implement the Create phase on a simple task and verify that the created region has low loss values
  2. Test Fisher Information computation on a trained model and compare with actual parameter sensitivity
  3. Combine Create and Find phases on sequential tasks and measure forgetting compared to baseline methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the C&F framework's performance compare when applied to tasks with more than 10 distinct classes, such as in large-scale multi-class classification problems?
- Basis in paper: [inferred] The paper primarily evaluates the framework on text classification tasks with a limited number of classes (e.g., 4-10 classes). It does not explore scenarios with significantly larger class spaces.
- Why unresolved: The paper does not provide experimental results or analysis for tasks with a large number of classes, leaving uncertainty about scalability and effectiveness in such scenarios.
- What evidence would resolve it: Experiments applying C&F to large-scale multi-class classification tasks with more than 10 classes, comparing performance against existing methods, would clarify its scalability and effectiveness.

### Open Question 2
- Question: What is the impact of the C&F framework on models with different architectural complexities, such as transformers with varying numbers of layers or attention heads?
- Basis in paper: [explicit] The paper mentions using BERT-base-uncased for experiments but does not investigate the framework's performance across different model architectures or complexities.
- Why unresolved: The paper focuses on a single model architecture, leaving uncertainty about how the framework performs with more complex or simpler models.
- What evidence would resolve it: Experiments applying C&F to models with varying architectural complexities, such as different transformer configurations, would demonstrate its adaptability and effectiveness across diverse architectures.

### Open Question 3
- Question: How does the C&F framework handle tasks with highly imbalanced data distributions, and what strategies could be employed to mitigate potential issues?
- Basis in paper: [inferred] The paper does not address scenarios with imbalanced data distributions, which are common in real-world applications and can affect the performance of continual learning methods.
- Why unresolved: The absence of experiments or analysis on imbalanced datasets leaves uncertainty about the framework's robustness and adaptability in such conditions.
- What evidence would resolve it: Experiments applying C&F to tasks with imbalanced data distributions, along with strategies to address imbalance (e.g., reweighting, oversampling), would clarify its robustness and effectiveness in real-world scenarios.

## Limitations
- Framework requires maintaining replay buffers and computing Fisher Information, adding computational overhead
- Theoretical proof relies on first-order approximations that may not hold in practice
- Performance depends heavily on proper hyperparameter tuning (ρ, λ, γ)

## Confidence
- Creating flat regions to protect against forgetting: High confidence
- Fisher Information as reliable importance indicator: Medium confidence
- Outperformance of state-of-the-art methods: Medium confidence

## Next Checks
1. **Ablation study on Fisher Information**: Remove the Fisher-based importance weighting and replace with random or uniform importance to quantify how much of C&F's performance gain comes specifically from accurate flatness estimation versus the flat region creation mechanism alone.

2. **Transferability test across architectures**: Implement C&F on different backbone architectures (CNN, LSTM, other Transformer variants) to verify that the flat region creation mechanism generalizes beyond the specific BERT-base implementation used in experiments.

3. **Non-NLP domain validation**: Apply the framework to computer vision tasks (e.g., CIFAR-100 with multiple sub-tasks) to test whether the theoretical insights about flatness and forgetting transfer to different data modalities and loss landscapes.