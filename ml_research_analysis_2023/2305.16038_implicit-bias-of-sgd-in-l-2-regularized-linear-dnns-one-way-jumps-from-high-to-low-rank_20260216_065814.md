---
ver: rpa2
title: 'Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps from
  high to low rank'
arxiv_id: '2305.16038'
source_url: https://arxiv.org/abs/2305.16038
tags:
- rank
- have
- matrix
- local
- minima
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the implicit bias of Stochastic Gradient Descent
  (SGD) in L2-regularized Deep Linear Networks (DLNs) for Matrix Completion tasks.
  The authors analyze the loss landscape and show that SGD has a non-zero probability
  of jumping from higher-rank local minima to lower-rank ones, but zero probability
  of jumping back.
---

# Implicit bias of SGD in $L_{2}$-regularized linear DNNs: One-way jumps from high to low rank

## Quick Facts
- arXiv ID: 2305.16038
- Source URL: https://arxiv.org/abs/2305.16038
- Reference count: 40
- One-line primary result: SGD in L2-regularized deep linear networks can jump from higher-rank local minima to lower-rank ones, but not back

## Executive Summary
This paper analyzes the implicit bias of Stochastic Gradient Descent (SGD) in L2-regularized Deep Linear Networks (DLNs) for Matrix Completion tasks. The authors prove that SGD can jump from higher-rank local minima to lower-rank ones, but not vice versa, demonstrating SGD's low-rank bias. They introduce absorbing sets Br that contain all minima of rank r or less and show that once SGD enters these sets, it cannot escape. This theoretical framework explains how SGD can avoid rank-overestimating minima that gradient descent might get stuck in, allowing it to converge to the lowest possible rank solution that fits the data.

## Method Summary
The authors study SGD dynamics in L2-regularized DLNs for matrix completion problems. They analyze the loss landscape and prove that SGD has a non-zero probability of jumping from higher-rank local minima to lower-rank ones, but zero probability of jumping back. They define absorbing sets Br containing all minima of rank r or less and show these sets are absorbing for SGD under certain conditions on the learning rate and regularization parameter. The analysis focuses on matrix completion tasks where the goal is to recover a low-rank matrix from a subset of its entries.

## Key Results
- SGD can jump from higher-rank local minima to lower-rank ones with non-zero probability, but cannot jump back
- Absorbing sets Br are defined containing all minima of rank r or less, and SGD will eventually reach and remain in these sets
- SGD's low-rank bias allows it to avoid rank-overestimating minima that gradient descent might get stuck in
- With sufficient training time, SGD can incrementally decrease rank and converge to the lowest possible rank solution that fits the data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SGD in L2-regularized deep linear networks can jump from higher-rank local minima to lower-rank ones, but not back.
- Mechanism: The stochastic noise from SGD with replacement sampling creates small perturbations that can push the optimization trajectory out of a higher-rank local minimum basin and into a lower-rank one. The L2 regularization term provides a drift force that pushes small singular values toward zero, making rank reduction possible.
- Core assumption: The learning rate and regularization parameter are chosen appropriately so that the stochastic noise is strong enough to escape local minima but not so strong as to prevent convergence to absorbing sets.
- Evidence anchors:
  - [abstract] "SGD has a non-zero probability of jumping from higher-rank local minima to lower-rank ones, but zero probability of jumping back."
  - [section 3.2] "We now show how SGD could help avoiding rank-overestimating local minima. More precisely we show under conditions on the learning rate η and ridge λ that there is always a (small) likelihood of jumping from a local minimum to a local minimum of lower rank, but the probability of jumping to a local minimum of higher rank is zero."
  - [corpus] Weak evidence - related papers discuss implicit bias and regularization but not specifically the one-way jump property described here.
- Break condition: If the learning rate is too small, the stochastic noise becomes insufficient to escape higher-rank minima. If too large, the dynamics become too noisy to settle into absorbing sets.

### Mechanism 2
- Claim: The sets Br containing all minima of rank r or less are absorbing for SGD.
- Mechanism: Once SGD enters the set Br, the combination of L2 regularization and the structure of the loss landscape ensures that the probability of leaving this set is zero. The regularization term provides a stabilizing force that keeps the dynamics within Br.
- Core assumption: The regularization parameter λ is sufficiently small, and the learning rate η is chosen appropriately to maintain the absorbing property.
- Evidence anchors:
  - [abstract] "we define a sequence of sets B1 ⊂ B2 ⊂ · · · ⊂ BR so that Br contains all minima of rank r or less (and not more) that are absorbing for small enough ridge parameters λ and learning rates η"
  - [section 3.2] "More precisely, we define sets Br that contain all minima of rank r or less and show that they are absorbing: the probability for SGD to leave this set is zero, but the probability for SGD to move from outside of this set to inside (in sufficiently many steps) is non-zero."
  - [corpus] Weak evidence - related work discusses absorbing sets and stability but not specifically in the context of rank-based absorbing sets for SGD.
- Break condition: If λ becomes too large, the regularization term dominates and prevents the dynamics from settling into the absorbing sets. If ε1 or ε2 parameters in the definition of Br become too large, the absorbing property may be lost.

### Mechanism 3
- Claim: SGD can incrementally decrease rank and converge to the lowest possible rank solution that fits the data.
- Mechanism: By repeatedly jumping from higher-rank minima to lower-rank ones, SGD can perform a form of rank search. Each jump reduces the rank by at least one, and since there are finitely many ranks, SGD will eventually reach the minimum rank that fits the data.
- Core assumption: The probability of jumping to a lower-rank minimum is non-zero for any current rank, and there exists a minimum rank that fits the data.
- Evidence anchors:
  - [abstract] "This demonstrates SGD's low-rank bias, allowing it to avoid rank-overestimating minima that gradient descent might get stuck in. The result suggests that with sufficient training time, SGD can incrementally decrease rank and converge to the lowest possible rank solution that fits the data."
  - [section 3.2] "This suggests a strategy: train the network with a small ridge to guarantee convergence to a minimum of at least the right rank, and then take advantage of the SGD noise to slowly find minima of lower rank until finding the right rank."
  - [corpus] Weak evidence - related papers discuss rank minimization but not specifically the incremental rank reduction property of SGD.
- Break condition: If the probability of jumping to a lower-rank minimum becomes too small (e.g., due to very large learning rates or inappropriate regularization), the rank reduction process may take impractically long or stall.

## Foundational Learning

- Concept: Deep Linear Networks (DLNs) and their representation cost
  - Why needed here: Understanding DLNs is crucial because the paper focuses on the implicit bias of SGD in these networks. The representation cost, which equals the Lp-Schatten norm for p=2/L, determines the loss landscape and the existence of multiple local minima with different ranks.
  - Quick check question: What is the representation cost of a DLN with depth L for a matrix A, and how does it relate to the Lp-Schatten norm?

- Concept: Matrix Completion and nuclear norm regularization
  - Why needed here: The paper studies SGD in the context of matrix completion tasks, where the goal is to recover a low-rank matrix from a subset of its entries. Understanding nuclear norm regularization is important because it provides a convex relaxation of the rank minimization problem and serves as a baseline for comparison.
  - Quick check question: How does nuclear norm regularization approximate the rank minimization problem in matrix completion, and why is it effective?

- Concept: Stochastic Gradient Descent (SGD) with replacement sampling
  - Why needed here: The paper analyzes the behavior of SGD with replacement sampling, which introduces stochastic noise into the optimization dynamics. Understanding this noise structure is crucial for explaining how SGD can jump between local minima with different ranks.
  - Quick check question: How does SGD with replacement sampling differ from standard SGD, and what role does the stochastic noise play in the optimization dynamics?

## Architecture Onboarding

- Component map:
  - Deep Linear Network (DLN): A neural network with multiple linear layers, no activation functions
  - L2 Regularization: Adds a penalty term proportional to the squared parameter norm to the loss function
  - Matrix Completion Loss: Measures the discrepancy between observed entries of the true matrix and the predicted matrix
  - Stochastic Gradient Descent (SGD): An optimization algorithm that uses stochastic estimates of the gradient
  - Absorbing Sets (Br): Sets of parameters containing all local minima of rank r or less, with the property that once entered, the probability of leaving is zero

- Critical path:
  1. Initialize DLN parameters randomly
  2. Compute matrix completion loss and its gradient
  3. Update parameters using SGD with L2 regularization
  4. Monitor rank of the learned matrix and check if it has reached the desired rank
  5. If not, continue training until absorbing set of the desired rank is reached

- Design tradeoffs:
  - Learning rate (η): Higher values increase the probability of jumping between minima but may prevent convergence to absorbing sets
  - Regularization parameter (λ): Smaller values allow for more exploration but may lead to overfitting
  - Network depth (L): Deeper networks have more complex loss landscapes with more local minima but may exhibit stronger low-rank bias
  - Width of hidden layers: Larger widths increase the representational capacity but may also increase the number of local minima

- Failure signatures:
  - Rank overestimation: If SGD converges to a local minimum with rank higher than the true rank, it indicates that the learning rate or regularization parameter is not optimal
  - Slow convergence: If SGD takes too long to jump between minima, it may indicate that the learning rate is too small or the regularization parameter is too large
  - Oscillation: If SGD keeps jumping between minima without settling, it may indicate that the learning rate is too large or the regularization parameter is too small

- First 3 experiments:
  1. Train a DLN on a simple matrix completion task with known rank and monitor the rank evolution during training. Vary the learning rate and regularization parameter to observe their effects on the rank reduction process.
  2. Compare the performance of SGD with different sampling strategies (with vs. without replacement) on matrix completion tasks with different rank structures. Analyze the impact of sampling strategy on the probability of jumping between minima.
  3. Investigate the effect of network depth on the low-rank bias of SGD. Train DLNs with different depths on matrix completion tasks and compare their ability to find low-rank solutions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the rank of the matrix and the convergence time of SGD in deep linear networks?
- Basis in paper: [explicit] The paper states that SGD has a non-zero probability of jumping from a higher rank minimum to a lower rank one, but the probability of jumping back is zero. It also mentions that the average time required to observe one of these jumps can be "absurdly large".
- Why unresolved: The paper provides theoretical bounds on the probability of jumps but does not quantify the expected time for these rank-decreasing transitions in practical scenarios.
- What evidence would resolve it: Empirical studies measuring convergence times for various matrix completion problems with different ranks and network architectures, correlating the theoretical predictions with observed jump frequencies and timing.

### Open Question 2
- Question: How does the depth of the network affect the likelihood and frequency of rank-decreasing jumps in SGD?
- Basis in paper: [explicit] The paper states "We observe that depth increases the probability of jumps" and shows numerical experiments comparing networks of depth L=3 and L=4.
- Why unresolved: While the paper provides some numerical evidence, it does not offer a rigorous theoretical explanation for why depth affects jump probability, nor does it quantify this relationship.
- What evidence would resolve it: A comprehensive study varying network depth across a wider range, combined with theoretical analysis deriving how depth influences the landscape geometry and SGD dynamics that lead to rank jumps.

### Open Question 3
- Question: What are the limitations of using SGD with large learning rates and regularization parameters for rank minimization in deep linear networks?
- Basis in paper: [inferred] The paper mentions that large λ and η values are needed for jumps but "prevent SGD from minimizing the train error", suggesting a trade-off between exploration (jumping) and exploitation (fitting).
- Why unresolved: The paper proposes strategies like annealing and periodic schedules but doesn't fully characterize when and why these approaches succeed or fail, or what the fundamental limitations are.
- What evidence would resolve it: Systematic experiments exploring the trade-off between jump probability and fitting accuracy across different problem instances and hyperparameter choices, potentially leading to a theoretical framework for optimal scheduling.

## Limitations
- The theoretical results rely on precise conditions for learning rates and regularization parameters that may not be practical in all scenarios
- The absorbing set framework depends on asymptotic assumptions about SGD dynamics that may not hold in finite-time practical scenarios
- The rank reduction mechanism depends on specific loss landscape structures that may not generalize to all matrix completion tasks or network architectures
- The analysis focuses on DLNs with depth L > 2, leaving uncertainty about whether similar one-way jump properties hold for shallower networks or networks with nonlinear activations

## Confidence
- High: The existence of absorbing sets Br and their basic properties (containment, absorbing nature)
- Medium: The one-way jump property from higher to lower rank minima
- Low: The practical effectiveness of this mechanism for achieving rank minimization in realistic matrix completion tasks

## Next Checks
1. **Empirical verification**: Implement the SGD dynamics on synthetic matrix completion tasks with known ranks and verify the rank reduction process across different learning rates and regularization strengths.
2. **Parameter sensitivity**: Systematically explore the boundary conditions for η and λ that enable vs. prevent rank jumps, mapping out the phase diagram of successful vs. failed rank reduction.
3. **Generalization test**: Apply the absorbing set framework to deeper networks (L > 3) and networks with width variations to determine if the one-way jump property persists or breaks down.