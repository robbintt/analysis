---
ver: rpa2
title: Deep Neural Operator Driven Real Time Inference for Nuclear Systems to Enable
  Digital Twin Solutions
arxiv_id: '2308.07523'
source_url: https://arxiv.org/abs/2308.07523
tags:
- deeponet
- nuclear
- systems
- operator
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of Deep Neural Operator (DeepONet)
  as a surrogate modeling method for digital twin (DT) applications in nuclear energy
  systems. The study demonstrates DeepONet's potential in solving a challenging particle
  transport problem, showcasing its generalizability and computational efficiency.
---

# Deep Neural Operator Driven Real Time Inference for Nuclear Systems to Enable Digital Twin Solutions

## Quick Facts
- arXiv ID: 2308.07523
- Source URL: https://arxiv.org/abs/2308.07523
- Reference count: 4
- Primary result: DeepONet achieves 0.9777-0.9939 R2 scores and 30s‚Üí0.02s speedup over PHITS for neutron flux prediction in nuclear maze problems

## Executive Summary
This paper investigates Deep Neural Operator (DeepONet) as a surrogate modeling approach for digital twin applications in nuclear energy systems. The study demonstrates DeepONet's effectiveness in solving a challenging particle transport problem, achieving remarkable prediction accuracy and computational efficiency compared to traditional machine learning methods. By leveraging the Universal Approximation Theorem for Operators, DeepONet can map complex neutron source functions to flux distributions orders of magnitude faster than full Monte Carlo simulations while maintaining high accuracy.

## Method Summary
The study uses DeepONet with a branch-trunk architecture to learn the mapping between Gaussian-distributed neutron source functions and resulting 2D neutron flux distributions in a maze geometry. Training data consists of 1,900 patterns generated by PHITS Monte Carlo simulations, each containing 6,400 spatial points. The branch network has architecture [190,80,80] and trunk network [2,80,80], trained with Adam optimizer for 10,000 iterations on sampled spatial points (50-90% coverage). Performance is evaluated against FCN and CNN baselines using R2, RMSE, MAE, and RMSE/MAE metrics, with computational speed comparisons between DeepONet and PHITS simulations.

## Key Results
- DeepONet achieves R2 scores of 0.9777-0.9939 on test data, significantly outperforming FCN and CNN baselines
- Computational speedup of approximately 1,500x: 30 seconds for PHITS simulation reduced to 0.02 seconds for DeepONet inference
- DeepONet successfully generalizes to unseen input functions while maintaining physical plausibility of predictions
- The model demonstrates robustness with RMSE/MAE ratios close to 1, indicating consistent prediction accuracy across the test space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DeepONet leverages the Universal Approximation Theorem for Operators to map functions to operators, enabling it to handle diverse input patterns.
- Mechanism: The branch-trunk architecture splits the problem into encoding the input function (branch) and handling the output domain (trunk), then combines them via a dot product.
- Core assumption: The operator can be approximated by continuous vector functions g and f as in Equation (1) from the paper.
- Evidence anchors:
  - [abstract] "By taking functions as input data and constructing the operator ùê∫ from training data, DeepONet can handle diverse and complex scenarios effectively."
  - [section] "Based on the Universal Approximation Theorem for Operator, the operator ùê∫ can be expressed by the Generalized Universal Approximation Theorem for Operator by Lu et al. [2021a], Cai et al. [2021] as follows:..."

### Mechanism 2
- Claim: DeepONet achieves high prediction accuracy and speed by learning a surrogate model of computationally expensive simulations.
- Mechanism: The model is trained on high-fidelity data from PHITS simulations and can predict neutron flux distributions orders of magnitude faster than full Monte Carlo runs.
- Core assumption: Training data sufficiently covers the input space and the operator mapping is smooth enough for neural network interpolation.
- Evidence anchors:
  - [abstract] "DeepONet exhibits remarkable prediction accuracy, outperforming traditional ML methods... Its accurate prediction and computational efficiency capabilities can revolutionize DT systems..."
  - [section] "While a PHITS simulation took about 30 seconds... DeepONet performed the task in just 0.02 seconds."

### Mechanism 3
- Claim: DeepONet can generalize to unseen input functions due to its operator learning framework.
- Mechanism: By training on many realizations of input functions (neutron sources), the network learns the underlying operator mapping rather than memorizing specific cases.
- Core assumption: The operator mapping is consistent across the test space and the network has enough capacity to learn it.
- Evidence anchors:
  - [abstract] "Through extensive benchmarking and evaluation, this study showcases the scalability and computational efficiency of DeepONet in solving a challenging particle transport problem."
  - [section] "Notably, when Test ID 23 and 18 were used, the DeepONet model demonstrated the highest and lowest R2 values, respectively."

## Foundational Learning

- Concept: Operator learning vs. standard function approximation
  - Why needed here: Understanding why DeepONet can map functions to operators is essential to grasp its advantage over FCN/CNN.
  - Quick check question: What is the key difference between approximating a function f(x) and an operator G[u](y)?

- Concept: Branch-trunk architecture
  - Why needed here: This architecture is central to how DeepONet handles functional inputs and domain outputs.
  - Quick check question: In the branch-trunk model, what do the branch and trunk networks represent?

- Concept: Sensor placement and discretization effects
  - Why needed here: Real-world deployment depends on how sensor locations affect model fidelity.
  - Quick check question: How might changing the number or position of fixed sampling points affect DeepONet performance?

## Architecture Onboarding

- Component map: Neutron source functions ‚Üí Branch network ‚Üí Trunk network (coordinates) ‚Üí Dot product ‚Üí Flux distribution predictions

- Critical path:
  1. Generate training data via PHITS simulations
  2. Preprocess: discretize input functions, sample output mesh points
  3. Build DeepONet with [190,80,80] branch and [2,80,80] trunk layers
  4. Train on training split, validate on test split
  5. Evaluate with R2, RMSE, MAE, RMSE/MAE

- Design tradeoffs:
  - More fixed sampling points ‚Üí better accuracy but higher input dimension
  - Larger network width/depth ‚Üí better fit but slower inference
  - Smaller training set ‚Üí faster training but worse generalization

- Failure signatures:
  - High RMSE/MAE ratio ‚Üí outliers in predictions
  - Low R2 on specific inputs ‚Üí poor coverage in training data
  - Slow convergence ‚Üí learning rate too low or network too shallow

- First 3 experiments:
  1. Train DeepONet on 50% of PHITS data, test generalization
  2. Compare with FCN/CNN on same dataset, measure R2 and speed
  3. Vary sensor sampling density (e.g., 100, 150, 190 points) and measure accuracy

## Open Questions the Paper Calls Out

- What is the optimal sensor placement and quantity for DeepONet models in nuclear reactor systems?
  - Basis: The paper mentions that "understanding the impact of fixed sensors' number and location on model performance is crucial" and that "optimizing sensor placement and quantity under such constraints will be necessary for accurate and reliable modeling."

- How can the model evaluation process for DeepONet be improved to ensure reliable predictions in all scenarios?
  - Basis: The paper states that "developing more effective model evaluation methods is crucial to ensure reliable predictions and robustness" and mentions the presence of outliers affecting model performance.

- How does the performance of DeepONet compare to other operator learning methods in nuclear engineering applications?
  - Basis: The paper compares DeepONet to FCN and CNN but does not mention other operator learning methods.

## Limitations
- Sensor placement optimization remains unexplored, with performance sensitivity to sensor density and location variations not systematically investigated
- Generalization boundaries are not examined, lacking assessment of performance degradation when input distributions shift beyond training data bounds
- Baseline comparisons use unspecified architectures and training procedures, making it difficult to attribute performance advantages to architectural differences

## Confidence
- **High Confidence**: Computational efficiency claims (30s vs 0.02s for PHITS vs DeepONet) - directly measurable and consistently reported
- **Medium Confidence**: Prediction accuracy metrics (R2 scores 0.9777-0.9939) - well-defined but limited to the specific test cases presented
- **Low Confidence**: Real-world applicability claims - dependent on unvalidated assumptions about sensor placement and operational condition coverage

## Next Checks
1. **Sensor Sensitivity Analysis**: Systematically vary the number and positions of sampling points to quantify DeepONet's performance sensitivity to sensor placement decisions.

2. **Out-of-Distribution Testing**: Evaluate DeepONet performance on input functions that significantly deviate from the training distribution to establish operational boundaries.

3. **Baseline Methodology Replication**: Implement and compare multiple FCN/CNN architectures with specified hyperparameters to validate that DeepONet's performance advantage is architecture-driven rather than implementation-dependent.