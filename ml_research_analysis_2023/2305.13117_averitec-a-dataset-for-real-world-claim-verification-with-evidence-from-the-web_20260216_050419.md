---
ver: rpa2
title: 'AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the
  Web'
arxiv_id: '2305.13117'
source_url: https://arxiv.org/abs/2305.13117
tags:
- claim
- evidence
- claims
- annotators
- fact-checking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AVeriTeC is a new dataset for automated fact-checking consisting
  of 4,568 real-world claims from 50 fact-checking organizations. Each claim is annotated
  with question-answer pairs providing evidence, a veracity label, and textual justifications.
---

# AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web

## Quick Facts
- arXiv ID: 2305.13117
- Source URL: https://arxiv.org/abs/2305.13117
- Reference count: 40
- A new dataset of 4,568 real-world claims with evidence annotations for automated fact-checking

## Executive Summary
AVeriTeC is a novel dataset designed to advance automated fact-checking by providing real-world claims with comprehensive evidence annotations. The dataset addresses key limitations of existing resources by using claims from 50 fact-checking organizations, providing detailed question-answer pairs as evidence, and implementing robust quality control measures. Through a multi-round annotation process, AVeriTeC achieves substantial inter-annotator agreement while avoiding common pitfalls such as context dependence, evidence insufficiency, and temporal leakage.

## Method Summary
The dataset was created through a three-phase annotation process involving claim normalization, question generation/answering, and quality control. Claims were extracted from 50 fact-checking organizations and rewritten to be self-contained. Annotators then generated questions and answers that would serve as evidence for verifying each claim. A second annotator verified the claim using only the generated evidence, with additional questions added if necessary until agreement was reached. The dataset also implements temporal ordering to prevent leakage and restricts evidence to documents published before the fact-checking article.

## Key Results
- 4,568 real-world claims from 50 fact-checking organizations with detailed evidence annotations
- Substantial inter-annotator agreement of κ=0.619 on verdicts
- Baseline model demonstrates the feasibility of verifying claims through question-answering steps against the open web
- Dataset addresses limitations of existing resources including context dependence, evidence insufficiency, and temporal leakage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evidence sufficiency is ensured by blind annotation and cross-checking.
- Mechanism: Claims are first annotated with questions and answers, then re-annotated by a different annotator who must reach the same verdict using only the generated evidence. If verdicts disagree, more questions are added until agreement is reached.
- Core assumption: Human annotators can independently verify claims using only generated question-answer pairs without referring to the original fact-checking article.
- Evidence anchors:
  - [abstract] "Through a multi-round annotation process, we avoid common pitfalls including context dependence, evidence insufficiency, and temporal leakage, and reach a substantial inter-annotator agreement of κ = 0.619 on verdicts."
  - [section] "Annotators in the evidence sufficiency check phase furthermore generate textual justifications that explain how to combine question-answer pairs into the selected verdict."
- Break condition: If annotators cannot agree on a verdict after multiple rounds of annotation, the claim is discarded from the dataset.

### Mechanism 2
- Claim: Temporal leakage is prevented by restricting evidence to documents published before the claim.
- Mechanism: Annotators are instructed to only use web documents published before the fact-checking article date as evidence, and the dataset is split temporally to prevent training-test leakage.
- Core assumption: Google Search API provides reliable publication dates that can be used to filter evidence.
- Evidence anchors:
  - [section] "We address (3) by restricting annotators to evidence documents published before the claim, and by ordering our training, development, and test splits temporally."
  - [section] "We use a custom Google search bar. This search bar is restricted to show documents published only before the fact-checking article."
- Break condition: If Google's publication date estimates are inaccurate, some evidence might leak from the future into the dataset.

### Mechanism 3
- Claim: Context dependence is eliminated through claim normalization and verification.
- Mechanism: Claims are rewritten to include necessary context from the fact-checking article, then verified by annotators who must be able to understand and verify claims without accessing the article.
- Evidence anchors:
  - [section] "We ask annotators to rewrite claims to include missing information, so that they can be understood and fact-checked without access to the fact-checking article (i.e. providing context independence)."
  - [section] "Once question-answer pairs have been generated, we present each claim to a new annotator without showing them the fact-checking article. This new annotator will then have to produce a verdict."
- Break condition: If claims require context that cannot be reasonably included through normalization, they must be discarded from the dataset.

## Foundational Learning

- Concept: Question-answer decomposition for fact-checking
  - Why needed here: Complex real-world claims often require multiple pieces of evidence that must be verified separately before reaching a final verdict.
  - Quick check question: Can you explain how decomposing a claim like "the US in 2017 has the largest percentage of immigrants" into questions about population and immigration numbers helps verify the claim?

- Concept: Temporal data splitting
  - Why needed here: Prevents models from learning patterns from future evidence that wouldn't be available during real-time fact-checking.
  - Quick check question: Why is it important to split the dataset temporally rather than randomly when dealing with time-sensitive claims?

- Concept: Inter-annotator agreement measurement
  - Why needed here: Ensures annotation quality and reliability, especially important for unbalanced datasets with many refuted claims.
  - Quick check question: What is the difference between Fleiss' kappa and Randolph's free-marginal kappa, and why is the latter preferred for this dataset?

## Architecture Onboarding

- Component map: Claim → Normalization → Question Generation → Answer Retrieval → Reranking → Veracity Prediction → Justification → Evaluation
- Critical path: Claim → Normalization → Question Generation → Answer Retrieval → Reranking → Veracity Prediction → Justification → Evaluation
- Design tradeoffs: The dataset prioritizes real-world claims over artificial ones, which means more complexity in annotation but better generalizability. The baseline trades performance for computational efficiency by using smaller models.
- Failure signatures: Low Hungarian METEOR scores indicate poor retrieval; low veracity accuracy even with gold evidence suggests the claim is inherently difficult; high disagreement between annotators suggests the claim is ambiguous or requires subjective judgment.
- First 3 experiments:
  1. Test the baseline model on a small subset of claims to establish baseline performance and identify bottlenecks in the pipeline.
  2. Run the same claims through the baseline with gold evidence instead of retrieved evidence to isolate retrieval performance from model performance.
  3. Test different evidence cutoff thresholds (λ values) to find the optimal balance between recall and precision for the evaluation metric.

## Open Questions the Paper Calls Out

- Question: How does the dataset handle claims that require evidence from multiple modalities (e.g., combining text with images or videos)?
  - Basis in paper: Inferred from the discussion of multimodal claims being discarded during claim extraction and the mention of evidence sources including "video" and "image/graphic" in Table 4.
  - Why unresolved: The paper explicitly discards multimodal claims during annotation but still includes some evidence from non-textual sources. The methodology for handling these mixed-modality claims remains unclear.
  - What evidence would resolve it: Examples of how mixed-modality claims were handled or a statement about the decision to exclude them entirely.

- Question: What is the impact of temporal ordering on model performance, especially considering the increasing reliance on large pretrained language models?
  - Basis in paper: Explicit in the section discussing temporal leakage prevention and the benefit of temporal ordering for pretraining cutoff.
  - Why unresolved: While the paper mentions the benefit, it doesn't empirically demonstrate the impact on model performance.
  - What evidence would resolve it: Performance comparison of models trained on temporally ordered vs. shuffled data, or analysis of pretraining cutoff effects.

- Question: How does the Hungarian METEOR evaluation metric handle cases where different annotators provide equivalent but differently phrased evidence?
  - Basis in paper: Explicit in the evaluation metric section discussing alternative evidence formulations and the need for automatic evaluation.
  - Why unresolved: The paper acknowledges this limitation but doesn't provide a solution or detailed analysis of its impact.
  - What evidence would resolve it: Analysis of cases where models are penalized for selecting alternative evidence paths, or development of a more robust evaluation metric.

## Limitations

- Temporal leakage prevention relies heavily on Google Search API's publication date accuracy, which is not explicitly validated in the paper.
- The dataset's reliance on English-language fact-checking articles from 50 organizations may limit generalizability to other languages and cultural contexts.
- The baseline model shows performance that suggests significant room for improvement, particularly in retrieval and question generation.

## Confidence

- Annotation methodology and quality control: High
- Dataset characteristics and size: High
- Baseline model implementation: Medium
- Temporal leakage prevention: Medium

## Next Checks

1. Conduct an independent audit of the temporal leakage prevention mechanism by verifying Google Search API's publication date accuracy across a sample of evidence documents.

2. Test the baseline model on a subset of claims using both the 7B BLOOM model and smaller models (1B-3B parameters) to quantify the performance tradeoff and assess whether the computational cost is justified.

3. Perform cross-linguistic validation by translating a subset of claims into another language and evaluating whether the question-answer decomposition approach remains effective.