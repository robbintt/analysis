---
ver: rpa2
title: The importance of feature preprocessing for differentially private linear optimization
arxiv_id: '2307.11106'
source_url: https://arxiv.org/abs/2307.11106
tags:
- private
- dpsgd
- algorithm
- dataset
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether differentially private stochastic
  gradient descent (DPSGD) alone is sufficient for obtaining good minimizers in linear
  classification under privacy constraints. The authors show theoretically that DPSGD
  without feature preprocessing can incur an error proportional to the maximum norm
  of features, while their proposed algorithm DPSGD-F, which combines feature preprocessing
  with DPSGD, achieves an error proportional to the diameter of the feature set.
---

# The importance of feature preprocessing for differentially private linear optimization

## Quick Facts
- arXiv ID: 2307.11106
- Source URL: https://arxiv.org/abs/2307.11106
- Reference count: 40
- Key outcome: DPSGD-F improves accuracy from 70.6% to 71.6% on CIFAR-100 under ε=1 by combining feature preprocessing with DPSGD

## Executive Summary
This paper investigates whether differentially private stochastic gradient descent (DPSGD) alone is sufficient for obtaining good minimizers in linear classification under privacy constraints. The authors show theoretically that DPSGD without feature preprocessing can incur an error proportional to the maximum norm of features, while their proposed algorithm DPSGD-F, which combines feature preprocessing with DPSGD, achieves an error proportional to the diameter of the feature set. This improvement is significant when the diameter is much smaller than the maximum norm. They prove this is near-optimal via an information-theoretic lower bound. Empirically, DPSGD-F consistently outperforms DPSGD on MNIST, Fashion-MNIST, and CIFAR-100 datasets.

## Method Summary
The authors propose DPSGD-F, which combines feature preprocessing with DPSGD for differentially private linear optimization. The algorithm first computes a differentially private mean of features using instance-optimal mean estimation, then estimates a quantile τ of distances from this mean. Features are translated by the estimated mean, augmented with a bias term, and clipped to have norm at most √(2τ). DPSGD is then applied to this preprocessed dataset, and the final solution is translated back to the original space. The privacy budget is split between the preprocessing steps and the DPSGD phase.

## Key Results
- DPSGD-F improves accuracy from 70.6% to 71.6% on CIFAR-100 under ε=1
- Theoretical analysis shows DPSGD without preprocessing incurs error proportional to maximum norm, while DPSGD-F achieves error proportional to diameter
- Near-optimal performance is proven via information-theoretic lower bound
- Improvements are consistent across MNIST, Fashion-MNIST, and CIFAR-100 datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DPSGD alone can suffer from high privacy error proportional to the maximum norm of feature vectors, making it inefficient for datasets where diameter ≪ maximum norm.
- **Mechanism:** When gradients are clipped to a norm C, the "signal" in the gradient is reduced to min{1, C/√(µ²+1)}, but the added Gaussian noise (proportional to C/nε) can dominate this signal when µ is large. This results in noisy updates that fail to converge meaningfully.
- **Core assumption:** The dataset contains feature vectors with high maximum norm but small diameter, and µ > nε (where µ controls the scale of feature differences).
- **Evidence anchors:**
  - [abstract] "we show theoretically that there exists an example where without feature preprocessing, DPSGD incurs an error proportional to the maximum norm of features"
  - [section] "the 'signal' component of the gradient on the second coordinate will be decreased to min{1, C/√(µ²+1)}. However, in each iteration, DPSGD will add a Gaussian noise with standard deviation proportional to C/nε, which can be large compared to min{1, C/√(µ²+1)} when µ is large"
  - [corpus] Weak - corpus papers discuss general DPSGD improvements but do not provide direct evidence for this specific failure mode.
- **Break condition:** If µ ≤ nε, the noise does not dominate the signal, and DPSGD may perform adequately without preprocessing.

### Mechanism 2
- **Claim:** DPSGD-F combines feature preprocessing with DPSGD to achieve privacy error proportional to the diameter of the feature set, significantly improving over DPSGD when diameter ≪ maximum norm.
- **Mechanism:** Private mean estimation centers the data, reducing the effective feature scale. Then, by estimating a quantile τ of distances from the mean and augmenting features with τ, the algorithm bounds the gradient norms. DPSGD is then applied to this preprocessed dataset, resulting in smaller noise per iteration and faster convergence.
- **Core assumption:** The dataset has a small diameter relative to its maximum norm, and sufficient samples (n ≥ c·√(d log(1/δ) log(d))/ε) exist for accurate private mean estimation.
- **Evidence anchors:**
  - [abstract] "we propose an algorithm called DPSGD-F, which combines DPSGD with feature preprocessing and prove that for classification tasks, it incurs a privacy error proportional to the diameter of the features"
  - [section] "We first compute a differentially private mean of features ˆµ using the instance-optimal mean estimation algorithm... This implies that if n is large enough, ˆµ is a good approximation for µ and hence for all xi ∈ D, E[∥xi − ˆµ∥²] ≤ 2diam(D) + 2R/n²"
  - [corpus] Weak - corpus papers focus on general DPSGD improvements but do not directly validate the diameter-based error bound.
- **Break condition:** If the dataset diameter is close to the maximum norm, or if n is too small for accurate mean estimation, the improvement over DPSGD diminishes.

### Mechanism 3
- **Claim:** The near-optimal performance of DPSGD-F is theoretically justified by a matching information-theoretic lower bound.
- **Mechanism:** The lower bound proof constructs a dataset where any (ε, δ)-DP algorithm must incur error Ω(G·diam(D)·min(1, ∥θ*∥M√(rank(M))/(nε))). Since DPSGD-F achieves error O(G∥θ*∥Mdiam(D)/√(rank(M))·log(1/δ)/(nε) + ...), it is optimal up to logarithmic factors.
- **Core assumption:** The lower bound construction uses a hinge loss classification problem with features constrained to a small set of basis vectors, ensuring the diameter and rank terms are meaningful.
- **Evidence anchors:**
  - [abstract] "We prove this is near-optimal via an information-theoretic lower bound"
  - [section] "Theorem 2. Let A be any (ε, δ)-DP optimization algorithm with ε ≤ c and δ ≤ cε/n for some constant c > 0. There exists a dataset D... and a loss function ℓ(θ·x, y) that is convex and G-Lipschitz... E[L(A(D), D)] − L(θ*, D) = Ω(G·diam(D)·min(1, ∥θ*∥M√(rank(M))/(nε)))"
  - [corpus] Weak - corpus papers discuss general lower bounds for DP optimization but do not provide direct evidence for this specific construction.
- **Break condition:** If the dataset or loss function does not fit the assumptions of the lower bound (e.g., non-convex losses, unbounded features), the optimality claim may not hold.

## Foundational Learning

- **Concept:** Differential Privacy (DP) and its composition properties.
  - **Why needed here:** The paper relies on DP composition theorems to allocate privacy budget across multiple steps (mean estimation, quantile estimation, DPSGD) while maintaining overall (ε, δ)-DP.
  - **Quick check question:** If each of three steps uses (ε/3, δ/3)-DP, what is the overall privacy guarantee after composition?
- **Concept:** Convex optimization and generalized linear models (GLMs).
  - **Why needed here:** The theoretical analysis assumes convex losses and GLMs to apply existing convergence bounds for DPSGD and derive the error terms involving ∥θ*∥M and rank(M).
  - **Quick check question:** For a GLM with linear model h((w,b),x) = w·x + b, is the loss ℓ(·,y) convex in (w,b) if ϕ is convex and non-increasing?
- **Concept:** Eigenvalue decomposition and projector matrices for design matrices.
  - **Why needed here:** The error bounds involve the projector M(D) onto the eigenspace of the design matrix, requiring understanding of rank(M(D)) and its relation to dataset geometry.
  - **Quick check question:** If D has n samples in d dimensions with full rank, what is the maximum possible rank of M(D)?

## Architecture Onboarding

- **Component map:** Private mean estimation -> Private quantile estimation -> Dataset translation and augmentation -> Feature clipping -> DPSGD on preprocessed dataset -> Solution translation back to original space
- **Critical path:** Mean estimation → Quantile estimation → Translation & augmentation → Clipping → DPSGD → Solution reversion
- **Design tradeoffs:**
  - Privacy budget allocation: More budget to preprocessing improves feature scaling but reduces budget for DPSGD.
  - Clipping threshold τ: Too small loses information; too large reduces the diameter-based advantage.
  - Batch size and learning rate in DPSGD: Affect convergence speed and privacy cost.
- **Failure signatures:**
  - If mean estimation error is large, the translated dataset may still have high norms, negating the diameter advantage.
  - If quantile estimation fails (e.g., due to small n), feature clipping may remove too many samples.
  - If the bias term augmentation is not handled correctly, the solution may not translate back properly.
- **First 3 experiments:**
  1. Implement private mean estimation on a synthetic dataset with known mean; verify E[∥µ−ˆµ∥²] ≤ diam(D) + 2R/n².
  2. Test private quantile estimation on distances from mean; verify the number of points beyond τ matches theoretical bounds.
  3. Run DPSGD-F on a simple 2D binary classification problem where DPSGD is known to fail; compare error to theoretical predictions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the dependence on the diameter be further improved to get better instance-specific bounds?
- Basis in paper: [inferred] The authors state "Exploring if the dependence on the diameter can be further improved to get better instance-specific bounds" as an open question in the discussion section.
- Why unresolved: The paper only shows that the proposed algorithm achieves an error proportional to the diameter, but does not explore whether this dependence can be further tightened for specific datasets.
- What evidence would resolve it: Theoretical analysis showing tighter bounds on the error in terms of the diameter for specific classes of datasets, or empirical results demonstrating improved performance on datasets where the diameter is significantly smaller than the maximum norm.

### Open Question 2
- Question: Can the results be extended to bigger datasets and non-convex models?
- Basis in paper: [inferred] The authors mention "investigating whether these results can be extended to bigger datasets" as an open question in the discussion section.
- Why unresolved: The paper focuses on linear classification and small image datasets (MNIST, Fashion-MNIST, CIFAR-100). It is unclear whether the findings hold for larger datasets or more complex models like deep neural networks.
- What evidence would resolve it: Empirical results showing improved performance of the proposed algorithm on larger datasets (e.g., ImageNet) or theoretical analysis extending the results to non-convex optimization problems.

### Open Question 3
- Question: Is there a way to automatically choose the privacy budget allocation between the feature preprocessing and DPSGD phases?
- Basis in paper: [explicit] The authors mention "choosing this parameter automatically as an interesting future direction" when discussing the experiments.
- Why unresolved: In the experiments, the authors perform a grid search over the privacy budget allocation between the two phases. There is no principled way to determine the optimal allocation.
- What evidence would resolve it: A method or heuristic for determining the optimal privacy budget allocation based on the dataset characteristics or a theoretical analysis providing guidance on how to split the privacy budget between the two phases.

## Limitations

- Theoretical claims rely on specific assumptions about dataset geometry (diameter ≪ maximum norm) and sufficient sample sizes for accurate private mean estimation.
- Empirical evaluation uses a limited set of datasets and privacy budgets (ε=1, ε=2), which may not generalize to all practical scenarios.
- Lower bound construction applies to a specific hinge loss classification problem and may not extend to all loss functions or dataset distributions.

## Confidence

- **High confidence**: The mechanism by which DPSGD-F improves over DPSGD when diameter ≪ maximum norm (Mechanism 2) is well-supported by the theoretical analysis and empirical results.
- **Medium confidence**: The optimality of DPSGD-F (Mechanism 3) is theoretically justified but relies on a specific lower bound construction that may not cover all practical cases.
- **Medium confidence**: The failure mode of DPSGD without preprocessing (Mechanism 1) is theoretically proven but may not manifest in all datasets, especially those with smaller maximum norms.

## Next Checks

1. **Theoretical validation**: Derive explicit expressions for the error terms in Mechanism 1 and 2 using the given assumptions, and verify they match the bounds in the paper. This will confirm the mathematical correctness of the core claims.

2. **Empirical validation**: Implement DPSGD-F on a synthetic dataset where diameter ≪ maximum norm, and compare its performance to DPSGD across a range of privacy budgets (ε from 0.1 to 10). This will test the practical significance of the diameter-based advantage.

3. **Generalization check**: Test DPSGD-F on additional datasets (e.g., CIFAR-10, SVHN) with varying image sizes and feature norms. If the improvement persists across diverse datasets, it strengthens the claim that feature preprocessing is vital for DP optimization.