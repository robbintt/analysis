---
ver: rpa2
title: 'CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language
  Models in 167 Languages'
arxiv_id: '2309.09400'
source_url: https://arxiv.org/abs/2309.09400
tags:
- data
- dataset
- language
- llms
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CulturaX, a large-scale, cleaned, and deduplicated
  multilingual dataset for training LLMs. CulturaX merges mC4 and OSCAR datasets,
  totaling 6.3 trillion tokens in 167 languages.
---

# CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages

## Quick Facts
- **arXiv ID**: 2309.09400
- **Source URL**: https://arxiv.org/abs/2309.09400
- **Reference count**: 16
- **Key outcome**: CulturaX dataset merges mC4 and OSCAR, totaling 6.3 trillion tokens in 167 languages after extensive cleaning and deduplication.

## Executive Summary
CulturaX is a large-scale multilingual dataset designed for training large language models across 167 languages. The dataset combines mC4 and OSCAR, then applies a rigorous cleaning pipeline including language identification, URL-based filtering, metric-based cleaning using percentile thresholds, document refinement, and MinHashLSH deduplication. The resulting 6.3 trillion tokens are publicly available on HuggingFace to support multilingual LLM research and development.

## Method Summary
The CulturaX dataset creation process involves combining mC4 (version 3.1.0) and OSCAR datasets, followed by a multi-stage cleaning pipeline. First, language identification is performed to ensure documents are correctly labeled, then URL-based filtering removes toxic content using a blacklist. Next, metric-based cleaning employs a variant of the Interquartile Range method with percentile thresholds to filter outliers across multiple metrics per language. Document refinement removes noisy elements like JavaScript code and footer content. Finally, MinHashLSH-based near-deduplication removes similar documents, supplemented by URL-based deduplication.

## Key Results
- Dataset contains 6.3 trillion tokens across 167 languages
- Successfully cleaned and deduplicated mC4 and OSCAR datasets
- Publicly available on HuggingFace to promote multilingual LLM research
- Cleaning pipeline includes language identification, URL filtering, metric-based cleaning, document refinement, and MinHashLSH deduplication

## Why This Works (Mechanism)

### Mechanism 1
The combination of mC4 and OSCAR with subsequent cleaning and deduplication yields a large-scale, high-quality multilingual dataset suitable for training LLMs. mC4 provides broad language coverage while OSCAR adds volume and diversity, and the cleaning pipeline removes noise and duplicates.

### Mechanism 2
Percentile-based thresholds for metric-based cleaning allow for efficient, language-specific data filtering. The IQR method variant uses percentiles of metric distributions across large samples, enabling automated threshold selection without manual intervention by native speakers.

### Mechanism 3
MinHashLSH-based deduplication effectively removes similar documents, improving dataset quality and diversity. This near-deduplication technique uses multiple hash functions for n-grams and Jaccard similarity to efficiently identify and remove duplicates across languages.

## Foundational Learning

- **Concept: Interquartile Range (IQR) Method**
  - Why needed here: Used to select appropriate thresholds for dataset metrics during cleaning to filter out noisy or low-quality documents
  - Quick check question: How does the IQR method help in identifying outliers in a dataset?

- **Concept: MinHash and Locality-Sensitive Hashing (LSH)**
  - Why needed here: Employed for near deduplication of documents in the dataset to improve quality and diversity
  - Quick check question: How does MinHash combined with LSH improve the efficiency of document deduplication?

- **Concept: Language Identification**
  - Why needed here: Crucial for assigning documents to correct languages and ensuring multilingual dataset quality
  - Quick check question: What are some common challenges in language identification, and how can they be addressed?

## Architecture Onboarding

- **Component map**: mC4 + OSCAR -> Language Identification -> URL Filtering -> Metric-based Cleaning -> Document Refinement -> MinHashLSH Deduplication -> URL Deduplication -> HuggingFace Repository

- **Critical path**: 1) Combine mC4 and OSCAR datasets, 2) Perform language identification and filter mismatches, 3) Apply URL-based filtering to remove toxic content, 4) Conduct metric-based cleaning using percentile thresholds, 5) Refine documents by removing noisy portions, 6) Perform MinHashLSH-based deduplication, 7) Apply URL-based deduplication, 8) Store cleaned dataset on HuggingFace

- **Design tradeoffs**: Using web-scraped data allows efficient collection across languages but introduces more noise requiring extensive cleaning; percentile-based thresholds reduce manual intervention but may miss language-specific nuances; MinHashLSH is efficient but may miss some duplicate types requiring additional URL-based deduplication

- **Failure signatures**: Incomplete or inaccurate language identification causing misassigned documents; ineffective URL filtering allowing toxic content; incorrect percentile thresholds removing quality documents or retaining low-quality ones; insufficient deduplication leaving redundant information

- **First 3 experiments**: 1) Verify language identification accuracy by sampling documents from different languages, 2) Assess URL filtering effectiveness by examining removed documents for toxicity, 3) Evaluate deduplication impact by comparing document counts and similarity scores before/after deduplication

## Open Questions the Paper Calls Out

- How does CulturaX's data cleaning and deduplication approach compare to other multilingual datasets in terms of improving model performance?

- What is the optimal threshold selection method for dataset metrics when cleaning multilingual data?

- How does the use of web-crawled data from CommonCrawl compare to curated datasets in terms of multilingual LLM training?

## Limitations

- Cleaning pipeline effectiveness relies heavily on automated threshold selection without human verification for each language, potentially missing nuanced quality issues
- Dataset quality improvements are not empirically validated against downstream LLM performance metrics
- Reproducibility of exact cleaning thresholds and filtering parameters across different implementations remains unclear

## Confidence

- **High Confidence**: Core methodology of combining datasets and using MinHashLSH for deduplication is well-established and technically sound; dataset statistics are verifiable through public release
- **Medium Confidence**: Effectiveness of percentile-based cleaning and URL filtering is reasonable but lacks direct validation; metric choices appear systematic but may not capture all quality dimensions
- **Low Confidence**: Claims about "best quality for model training" and "high-quality data" are not empirically validated against alternatives or measured against downstream performance

## Next Checks

1. Train a small multilingual LLM (1-2B parameters) on both original mC4/OSCAR and CulturaX, then compare performance on multilingual benchmarks (XGLUE, XTREME) to quantify cleaning pipeline impact

2. Systematically vary IQR percentiles in metric-based cleaning and measure impact on dataset size, quality metrics, and diversity across languages to understand threshold sensitivity

3. For sample languages representing different families, manually evaluate documents before/after cleaning to verify automated cleaning preserves quality content while removing problematic material, especially for languages where automated metrics may be less reliable