---
ver: rpa2
title: Pre-training Multi-task Contrastive Learning Models for Scientific Literature
  Understanding
arxiv_id: '2305.14232'
source_url: https://arxiv.org/abs/2305.14232
tags: []
core_contribution: 'The paper addresses the challenge of joint pre-training across
  multiple heterogeneous scientific literature understanding tasks (classification,
  link prediction, and search) while mitigating task interference. It proposes a multi-task
  contrastive learning framework called SciMult that incorporates two techniques:
  task-aware specialization using a Mixture-of-Experts Transformer architecture with
  task-specific sub-layers, and instruction tuning which prepends task-specific instructions
  to input text.'
---

# Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding

## Quick Facts
- arXiv ID: 2305.14232
- Source URL: https://arxiv.org/abs/2305.14232
- Reference count: 21
- The paper proposes SciMult, a multi-task contrastive learning framework that achieves state-of-the-art performance on scientific literature understanding tasks by mitigating task interference through task-aware specialization and instruction tuning.

## Executive Summary
This paper addresses the challenge of joint pre-training across multiple heterogeneous scientific literature understanding tasks (classification, link prediction, and search) while mitigating task interference. The proposed SciMult framework incorporates task-aware specialization using a Mixture-of-Experts Transformer architecture with task-specific sub-layers, and instruction tuning which prepends task-specific instructions to input text. Extensive experiments on benchmark datasets show that SciMult-Expert consistently outperforms state-of-the-art scientific pre-trained language models across both in-domain and cross-domain evaluations, demonstrating the effectiveness of task-aware specialization in mitigating task interference while maintaining strong performance.

## Method Summary
SciMult employs a two-stage pre-training approach. First, a base Vanilla model is trained on multi-task contrastive learning objectives using scientific literature datasets. Then, task-aware specialization is implemented through a Mixture-of-Experts Transformer architecture where each task has dedicated multi-head attention sub-layers while sharing the feed-forward network. Alternatively, instruction tuning is applied by prepending task-specific instructions to inputs during training. The framework also generalizes hard negative mining from citation prediction to other tasks. The model uses a Bi-Encoder architecture with contrastive loss to pull relevant (query, candidate) pairs closer in embedding space while pushing irrelevant pairs apart.

## Key Results
- SciMult-Expert consistently outperforms state-of-the-art scientific pre-trained language models across in-domain and cross-domain evaluations
- Task-aware specialization mitigates task interference while maintaining strong performance on all tasks
- Instruction tuning enables task-aware output generation without task-specific parameters
- Hard negative mining improves contrastive learning effectiveness by providing more informative negative samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-aware specialization via Mixture-of-Experts (MoE) Transformer architecture mitigates task interference by dedicating task-specific sub-layers while sharing common parameters.
- Mechanism: The model routes each task's input to its dedicated multi-head attention (MHA) sub-layer while sharing the feed-forward network (FFN) across tasks. This creates a parameter-efficient architecture where common knowledge is preserved but task-specific skills are isolated.
- Core assumption: Task interference occurs when competing tasks with different skill requirements share all parameters, leading to suboptimal performance on some tasks.
- Evidence anchors: [abstract]: "Task-aware specialization, inspired by the Mixture-of-Experts (MoE) Transformer architecture, modifies the Transformer block in the LM to have multiple parallel sub-layers, each of which is dedicated for one task." [section 3.2]: "A task-specific Transformer block has K MHA sub-layers, each corresponding to one task t, where t ∈ {classification, link prediction, search}."

### Mechanism 2
- Claim: Instruction tuning enables task-aware output generation without task-specific parameters by prepending task-specific instructions to input text.
- Mechanism: Task instructions are prepended to queries and candidates, and at each Transformer layer, the instruction representations serve as context when encoding the input. This allows a single shared encoder to produce task-appropriate representations.
- Core assumption: Large language models can learn to interpret natural language instructions and adjust their representations accordingly, producing task-aware outputs without explicit parameter specialization.
- Evidence anchors: [abstract]: "instruction tuning adopts one encoder for all tasks, but it prepends task-specific instructions to the input text during training so that the encoder can learn to produce task-aware representations." [section 3.3]: "instruction tuning keeps one encoder Eθ(·) with all its parameters θ shared across different tasks. Each task t is instead characterized by a natural language instruction xt."

### Mechanism 3
- Claim: Hard negative mining improves contrastive learning effectiveness by providing more informative negative samples that are semantically closer to positive pairs.
- Mechanism: For each positive pair, one hard negative is sampled based on task-specific criteria (e.g., for citation prediction, papers cited by the positive candidate but not by the query), combined with easy negatives from the batch.
- Core assumption: Easy negatives provide weak learning signals as they are clearly irrelevant, while hard negatives force the model to learn more discriminative features by distinguishing between semantically similar but non-relevant pairs.
- Evidence anchors: [abstract]: "We also generalize the idea of mining hard negative samples in citation prediction to other scientific literature understanding tasks for more effective multi-task contrastive learning." [section 3.4]: "We generalize this idea to the other tasks... For extreme multi-label classification... For literature search, we directly use the training data from Singh et al. (2022) which contains a short list of papers returned by an academic search engine for each query q."

## Foundational Learning

- Concept: Contrastive learning objective
  - Why needed here: The model needs to learn representations where relevant (query, candidate) pairs are closer in embedding space than irrelevant pairs. This is fundamental to the Bi-Encoder architecture used.
  - Quick check question: How does the contrastive loss function pull positive pairs together while pushing negative pairs apart in the embedding space?

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: To address task interference while maintaining parameter efficiency, the MoE approach allows task-specific specialization without duplicating the entire model.
  - Quick check question: What is the key difference between routing inputs to task-specific sub-layers versus having separate models for each task?

- Concept: Hard negative mining in contrastive learning
  - Why needed here: Standard random negatives provide weak learning signals. Hard negatives, being semantically closer to positives, force the model to learn more discriminative features.
  - Quick check question: How does the selection criterion for hard negatives differ from random negative sampling, and why does this matter for model performance?

## Architecture Onboarding

- Component map: Input preprocessor -> Token embedding -> Task-specific routing -> Multi-head attention processing -> Similarity computation -> Contrastive loss calculation
- Critical path: 1. Input formatting -> 2. Token embedding -> 3. Task-specific routing -> 4. Multi-head attention processing -> 5. Similarity computation -> 6. Contrastive loss calculation
- Design tradeoffs:
  - Shared vs. separate parameters: Full sharing causes interference; complete separation wastes parameters. The MoE approach balances this by sharing FFN but specializing MHA.
  - Instruction complexity: Longer, more detailed instructions might improve task awareness but increase computational cost and token usage.
  - Hard negative difficulty: Too easy and the model doesn't learn; too hard and the model may fail to converge.
- Failure signatures:
  - Task interference: One task's performance improves while others degrade when training jointly
  - Instruction tuning failure: Model ignores instructions and produces generic representations
  - Hard negative issues: Model struggles to distinguish between hard negatives and positives, leading to poor convergence
- First 3 experiments:
  1. Train SciMult-Vanilla on a single task to establish baseline performance
  2. Train SciMult-Expert and compare per-task performance against Vanilla to verify interference mitigation
  3. Test instruction tuning by comparing SciMult-Instruction against Vanilla on the same tasks to verify instruction effectiveness

## Open Questions the Paper Calls Out
- How does task-aware specialization perform compared to instruction tuning when scaling to hundreds of scientific literature understanding tasks? The authors note their study focuses on three tasks and mention that previous instruction tuning studies have considered tens to thousands of tasks, suggesting their exploration is limited in scope.
- What is the optimal architecture for applying task-aware specialization and instruction tuning beyond the Bi-Encoder architecture used in this study? The authors explicitly state they focused on Bi-Encoder architecture and suggest it would be meaningful to investigate Cross-Encoders and late-interaction models.
- How can the hard negative mining strategy for classification be further improved beyond the simple approach proposed in the paper? The authors propose a simple heuristic for deriving hard negatives in classification by finding papers with irrelevant labels that are semantically close, and note this is a generalization of citation-based hard negative mining.

## Limitations
- The paper only evaluates three specific tasks (classification, link prediction, search), limiting generalizability to other scientific literature understanding tasks.
- Detailed implementation specifications for the routing mechanism in the Mixture-of-Experts architecture are not provided, making exact replication challenging.
- The instruction templates and their quality are not specified, creating uncertainty about the effectiveness of instruction tuning.

## Confidence
- High Confidence: The core claim that task-aware specialization improves multi-task performance compared to fully shared parameter models is well-supported by extensive experimental results across multiple datasets and tasks.
- Medium Confidence: The claim that instruction tuning alone can achieve competitive performance without task-specific parameters is moderately supported, though the lack of specific instruction templates creates uncertainty.
- Low Confidence: The generalization of hard negative mining from citation prediction to other tasks is supported by results but lacks detailed methodological transparency.

## Next Checks
1. **Routing Mechanism Validation**: Implement a controlled experiment comparing different numbers of experts per task in the Mixture-of-Experts architecture to determine the optimal balance between specialization and parameter efficiency.
2. **Instruction Template Analysis**: Conduct a systematic ablation study varying the specificity and complexity of instruction templates across different tasks to quantify their impact on performance.
3. **Hard Negative Mining Robustness**: Test the model's sensitivity to hard negative mining by varying the sampling ratio and difficulty threshold, comparing performance against random negative sampling baselines.