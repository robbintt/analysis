---
ver: rpa2
title: 'SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain
  Knowledge'
arxiv_id: '2308.12682'
source_url: https://arxiv.org/abs/2308.12682
tags:
- planning
- action
- step
- actions
- room
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SayCanPay, a novel framework that combines
  Large Language Models (LLMs) with heuristic planning to generate feasible and cost-effective
  plans. SayCanPay employs LLMs to generate candidate actions guided by learnable
  domain knowledge, which evaluates actions' feasibility and long-term reward/payoff.
---

# SayCanPay: Heuristic Planning with Large Language Models using Learnable Domain Knowledge

## Quick Facts
- arXiv ID: 2308.12682
- Source URL: https://arxiv.org/abs/2308.12682
- Authors: 
- Reference count: 9
- Key outcome: Novel framework combining LLMs with heuristic planning using learnable domain knowledge (Can for feasibility, Pay for payoff) to generate feasible, cost-effective plans

## Executive Summary
SayCanPay introduces a framework that leverages Large Language Models to generate candidate actions, guided by learnable domain models that evaluate action feasibility (Can) and long-term payoff (Pay). By integrating these components into a heuristic search process, SayCanPay generates plans that are both feasible and cost-effective across diverse environments including Ravens, BabyAI, and VirtualHome. The framework outperforms existing LLM planning approaches in planning success and cost-effectiveness, demonstrating improvements through joint scoring mechanisms that incorporate feasibility and cost-efficiency considerations.

## Method Summary
SayCanPay employs an LLM (Vicuna/Flan-T5) to generate candidate actions with associated probabilities, guided by history and goal. Two domain-specific models - Can (BERT classifier) for feasibility and Pay (BERT regressor) for payoff estimation - evaluate these actions using expert trajectories. The framework performs heuristic search over actions (not tokens) using a Beam-Action strategy that maintains k action sequences, expanding each with m candidates and selecting top-k using a joint SayCanPay score. This approach enables planning that balances feasibility, cost-effectiveness, and generalization across environments.

## Key Results
- Outperforms other LLM planning approaches in planning success and cost-effectiveness across multiple environments
- Demonstrates improvements in plan feasibility and cost-efficiency through joint scoring mechanisms
- Shows effectiveness of Beam-Action strategy over Greedy-Action and Greedy-Token approaches
- Achieves strong performance on Ravens, BabyAI, and VirtualHome environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs can generate candidate actions with associated probabilities that reflect world knowledge.
- **Mechanism**: The Say model uses beam search over tokens to generate the top-m candidate actions with probabilities p(at|ht-1, g) at each planning step.
- **Core assumption**: The LLM has sufficient world knowledge from training data to propose plausible next actions.
- **Evidence anchors**:
  - [abstract]: "Our approach, SayCanPay, employs LLMs to generate actions (Say) guided by learnable domain knowledge"
  - [section 4]: "Given a NL goal g, history h0 = ( o0), and a LM generating actions at with probability p(at|ht-1, g), generate the most likely plan"
  - [corpus]: Weak - corpus focuses on planning with LLMs but doesn't specifically validate probability-based action generation
- **Break condition**: If LLM lacks relevant training data for the domain, generated actions will have low quality and poor probabilities.

### Mechanism 2
- **Claim**: Domain-specific models (Can and Pay) can evaluate feasibility and payoff of LLM-generated actions.
- **Mechanism**: Can model classifies actions as feasible/infeasible based on preconditions; Pay model estimates discounted reward for actions.
- **Core assumption**: Expert trajectories can train models to accurately assess feasibility and payoff.
- **Evidence anchors**:
  - [abstract]: "SayCanPay employs LLMs to generate actions (Say) guided by learnable domain knowledge, that evaluates actions' feasibility (Can) and long-term reward/payoff (Pay)"
  - [section 6.1]: "We model it as a classification problem...assigning the highest probability from a set of one positive and a few negative actions"
  - [section 6.2]: "We model it as a regression problem to estimate action payoffs...using temporal discounting δ ∈ (0, 1)"
  - [corpus]: Weak - corpus neighbors discuss heuristics and LLM planning but don't validate the specific Can/Pay training approach
- **Break condition**: If expert trajectories are insufficient or not representative, Can/Pay models will make incorrect evaluations.

### Mechanism 3
- **Claim**: Heuristic search over actions (not tokens) can select optimal plans combining feasibility and cost-effectiveness.
- **Mechanism**: Beam-Action strategy maintains k action sequences, expanding each with m candidate actions, then selecting top-k using joint SayCanPay score.
- **Core assumption**: Combining Say, Can, and Pay scores provides better action selection than greedy approaches.
- **Evidence anchors**:
  - [abstract]: "Our extensive evaluations show that our model surpasses other LLM planning approaches"
  - [section 5.2]: "In heuristic planning, multiple potential plans...are simultaneously maintained and iteratively expanded...we propose to manage k action sequences"
  - [section 7.3]: "The overall performance across decoding strategies follows the pattern: Greedy-Token < Greedy-Action < Beam-Action"
  - [corpus]: Weak - corpus discusses heuristic search and LLM planning but doesn't specifically validate beam search over actions
- **Break condition**: If beam size is too small, may miss optimal paths; if too large, computational cost increases significantly.

## Foundational Learning

- **Concept**: POMDP formulation for planning
  - **Why needed here**: Provides formal framework for planning under uncertainty with partial observability
  - **Quick check question**: What tuple elements define the POMDP in SayCanPay's formulation?

- **Concept**: Heuristic search planning (HSP)
  - **Why needed here**: Enables cost-effective plan generation by guiding search with heuristic functions
  - **Quick check question**: How does SayCanPay's Beam-Action differ from classical A* search?

- **Concept**: Language model prompting and inference
  - **Why needed here**: LLMs must generate candidate actions based on history and goal without fine-tuning
  - **Quick check question**: What decoding strategy produces the "Say" score in SayCanPay?

## Architecture Onboarding

- **Component map**: Vicuna/Flan-T5 (Say) -> BERT classifier (Can) -> BERT regressor (Pay) -> Beam-Action search -> Plan execution
- **Critical path**: Say → Can/Pay scoring → Beam search → Plan execution
- **Design tradeoffs**:
  - Token-level vs action-level search: Beam-Action trades computation for better plans
  - Joint scoring vs separate evaluation: SayCanPay combines feasibility and payoff but requires training two models
  - Offline vs online planning: SayCanPay plans fully before execution, simplifying state tracking
- **Failure signatures**:
  - Poor Say model → no good candidate actions → Can/Pay ineffective
  - Insufficient expert data → Can/Pay models don't generalize
  - Beam size too small → suboptimal plans
  - Domain mismatch → LLM lacks relevant world knowledge
- **First 3 experiments**:
  1. Implement Greedy-Action with SayCanPay scoring on BabyAI pickup task
  2. Compare Greedy-Action vs Beam-Action with fixed Say model on Ravens Hanoi
  3. Train Can model on expert trajectories and evaluate classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the Can and Pay models be adapted to better generalize to out-of-distribution (OOD) data in environments like Ravens?
- Basis in paper: [explicit] The authors mention that the Can and Pay models showed limited adaptability to OOD data in certain environments, particularly in Ravens.
- Why unresolved: The paper acknowledges this limitation but does not provide a definitive solution. It suggests that future work could explore using LLMs for reward design and value function perspectives, but this is not tested or validated.
- What evidence would resolve it: Demonstrating improved generalization of the Can and Pay models on OOD data in Ravens by incorporating LLM-based reward design or offline reinforcement learning approaches.

### Open Question 2
- Question: What is the impact of using larger language models or more diverse training data on the performance of the Say model?
- Basis in paper: [explicit] The authors suggest that an improved Say model could enhance overall performance, as evidenced by their "Perfect Say" ablation study showing significant improvements.
- Why unresolved: The paper does not explore the effects of using larger or more diverse language models for the Say component.
- What evidence would resolve it: Comparing the performance of SayCanPay with different sizes of language models (e.g., GPT-4, LLaMA-65B) and varying amounts of training data to quantify the impact on planning success and cost-effectiveness.

### Open Question 3
- Question: How does the performance of SayCanPay compare to other state-of-the-art planning methods that use language models, such as LLM+P or hybrid approaches?
- Basis in paper: [explicit] The authors compare SayCanPay to other LLM planning approaches in their experiments, but they do not directly compare it to hybrid methods like LLM+P that use symbolic planners.
- Why unresolved: The paper focuses on comparing SayCanPay to other LLM-based methods but does not explore its performance relative to hybrid approaches that combine LLMs with classical planning techniques.
- What evidence would resolve it: Conducting experiments to compare the planning success, cost-effectiveness, and generalization of SayCanPay against hybrid approaches like LLM+P in the same environments and tasks.

## Limitations
- Weak corpus evidence supporting the novel Can/Pay model architectures and training approaches
- Reliance on expert trajectories for training domain models raises scalability concerns
- Limited validation of the framework's performance relative to hybrid approaches combining LLMs with classical planning

## Confidence

- **High Confidence**: The basic framework combining LLMs with heuristic planning is sound and well-established in the literature
- **Medium Confidence**: The specific mechanism of using separate Can/Pay models for feasibility and payoff evaluation appears novel but lacks strong external validation
- **Low Confidence**: The claim that SayCanPay's joint scoring mechanism provides significant improvements over other approaches needs more rigorous ablation studies

## Next Checks

1. Implement a "Perfect Say" oracle to measure the upper bound of performance improvements from Can/Pay models, isolating whether current limitations stem from the LLM's action generation capabilities
2. Conduct ablation studies removing either the Can or Pay component to quantify their individual contributions to overall performance
3. Test the framework on a novel domain with minimal expert trajectories to evaluate the scalability and generalization limits of the Can/Pay training approach