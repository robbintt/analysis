---
ver: rpa2
title: In-Context Impersonation Reveals Large Language Models' Strengths and Biases
arxiv_id: '2305.14930'
source_url: https://arxiv.org/abs/2305.14930
tags:
- llms
- bird
- language
- task
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates whether large language models (LLMs) can
  impersonate different roles in-context and how this affects their performance. The
  authors explore three tasks: a multi-armed bandit task, a language-based reasoning
  task, and a visual classification task.'
---

# In-Context Impersonation Reveals Large Language Models' Strengths and Biases

## Quick Facts
- arXiv ID: 2305.14930
- Source URL: https://arxiv.org/abs/2305.14930
- Reference count: 40
- LLMs can impersonate different roles in-context, recovering human-like developmental stages and revealing hidden biases

## Executive Summary
This paper investigates whether large language models can effectively impersonate different roles through in-context prompting and how this affects their performance across multiple tasks. The authors demonstrate that LLMs can simulate developmental trajectories in exploration behavior, that domain expertise impersonation improves reasoning accuracy, and that social identity impersonation can uncover encoded biases. The study uses three tasks: a multi-armed bandit task to examine exploration-exploitation tradeoffs, a language-based reasoning task using MMLU, and a visual classification task using generated descriptions as class names. The results show that persona-based prompting can systematically alter LLM behavior in predictable ways that mirror human cognitive development and domain expertise.

## Method Summary
The researchers employed zero-shot prompting with persona-based prefixes ("If you were a {persona}") across three distinct tasks without fine-tuning or in-context examples. For the multi-armed bandit task, they simulated rewards and used log probabilities of LLM choices to sample actions. The language reasoning task used MMLU dataset questions with predicted probabilities for answer options. The visual classification task generated class descriptions using different personas and embedded them using CLIP models for zero-shot classification. The experiments tested personas including different ages (2, 4, 7, 13, 20 years), domain experts versus non-experts, and social identities (gender, race). Temperature-controlled sampling was used to generate diverse responses while maintaining task appropriateness.

## Key Results
- LLMs impersonating children of different ages recover human-like developmental stages of exploration in multi-armed bandit tasks
- LLMs impersonating domain experts perform significantly better than those impersonating non-domain experts on MMLU reasoning tasks
- Impersonation reveals encoded biases: LLMs prompted to be men describe cars better than those prompted to be women, and race-based personas show differential performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can simulate developmental trajectories in exploration behavior through role-based prompting
- Mechanism: By prefixing prompts with "If you were a {age} year old", the LLM retrieves latent representations associated with cognitive developmental stages, altering its decision-making policy in bandit tasks
- Core assumption: The model's pretraining corpus contains age-associated behavioral patterns that can be activated through role priming
- Evidence anchors: [abstract] "LLMs pretending to be children of different ages recover human-like developmental stages of exploration", [section] "LLMs impersonating older participants generate higher average rewards (β = 0.15, p < .001)", [corpus] Weak corpus evidence for age-specific behavior patterns
- Break condition: If age-based prompting doesn't change the exploration-exploitation tradeoff or reward patterns deviate from human developmental trajectories

### Mechanism 2
- Claim: Domain expertise impersonation improves task performance through specialized knowledge activation
- Mechanism: When prompted with domain expert personas, the LLM accesses and applies specialized knowledge representations learned during pretraining, improving reasoning accuracy
- Core assumption: The model's pretraining data includes domain-specific knowledge that can be selectively activated through persona-based priming
- Evidence anchors: [abstract] "LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts", [section] "When the LLM is asked to impersonate the task expert, the performance is the highest", [corpus] Limited corpus evidence for persona-specific knowledge activation
- Break condition: If domain expertise personas don't show measurable performance improvement over random baselines

### Mechanism 3
- Claim: Social identity impersonation reveals encoded biases through differential performance on category descriptions
- Mechanism: When prompted with social identity personas, the LLM's descriptions reflect underlying demographic biases present in training data, affecting downstream classification performance
- Core assumption: The model's pretraining corpus contains social biases that become apparent when specific social identities are activated
- Evidence anchors: [abstract] "impersonation can uncover LLMs' biases: an LLM prompted to be a man describes cars better than one prompted to be a woman", [section] "A race bias becomes apparent when we ask the LLMs to impersonate a 'black' or 'white' person", [corpus] Weak corpus evidence for bias patterns in social identity prompting
- Break condition: If social identity personas don't produce consistent performance differences across demographic groups

## Foundational Learning

- Concept: Zero-shot prompting without examples
  - Why needed here: The experiments rely on the model's ability to understand tasks and personas without demonstration, isolating the effect of persona priming
  - Quick check question: If you provide no examples but only a persona prefix, can the LLM still generate task-appropriate responses?

- Concept: Probabilistic sampling from logits
  - Why needed here: The methodology uses temperature-controlled sampling to generate responses that reflect the model's confidence while allowing exploration of the response distribution
  - Quick check question: What happens to response diversity when you increase temperature from 0.7 to 1.0 in persona-based prompting?

- Concept: Multi-modal embedding alignment
  - Why needed here: The vision-language experiments depend on CLIP-style alignment between generated text descriptions and visual features for classification
  - Quick check question: How does the cosine similarity between text embeddings from different personas affect classification accuracy?

## Architecture Onboarding

- Component map: LLM (Vicuna-13B or ChatGPT) → Persona prefixing module → Task-specific context → Response sampling → Downstream evaluation
- Critical path: Prompt construction → LLM inference → Response post-processing → Performance measurement
- Design tradeoffs: Temperature vs determinism (higher temperature increases diversity but reduces consistency), persona specificity vs generalization (more specific personas may be more effective but less generalizable)
- Failure signatures: No performance change across personas (impersonation ineffective), consistent bias in one direction only (systematic bias rather than nuanced identity effects), random baseline performance (task too difficult)
- First 3 experiments:
  1. Bandit task with age personas (2, 4, 7, 13, 20 years) measuring exploration-exploitation tradeoff
  2. MMLU reasoning with domain expert personas measuring accuracy improvement
  3. CUB/Stanford Cars classification with social identity personas measuring bias detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does in-context impersonation scale with increasing model size and what are the limits of this effect?
- Basis in paper: [inferred] The paper mentions that impersonation can boost performance and recover biases, but does not explore how these effects change with model size or what the limits of this approach might be
- Why unresolved: The study only tested Vicuna-13B and ChatGPT, without varying model sizes or exploring performance limits
- What evidence would resolve it: Experiments testing impersonation effects across multiple model sizes (e.g., 7B, 30B, 175B parameters) while measuring performance changes and bias manifestation

### Open Question 2
- Question: Can in-context impersonation effects be transferred to other modalities beyond text and vision, such as audio or video generation?
- Basis in paper: [explicit] The authors suggest in their conclusion that in-context impersonation could be applied to other modalities like video generation
- Why unresolved: The study only tested text-based reasoning tasks and vision-language tasks, without exploring other modalities
- What evidence would resolve it: Experiments applying impersonation prompts to audio generation models or video synthesis models and measuring the effects on output quality and content

### Open Question 3
- Question: What are the underlying mechanisms that allow LLMs to successfully impersonate different personas, and how do these mechanisms relate to the model's internal representations?
- Basis in paper: [inferred] The paper demonstrates that impersonation works but does not investigate the internal mechanisms or representations that enable this capability
- Why unresolved: The study focuses on behavioral outcomes of impersonation rather than the internal cognitive or representational processes
- What evidence would resolve it: Analysis of internal activations and attention patterns when LLMs impersonate different personas, comparing these to their baseline responses and to actual human behavior patterns

## Limitations

- The study cannot definitively prove that LLMs are truly "impersonating" versus responding to surface-level linguistic cues, as the experiments show correlation but not causation
- Social identity experiments may reflect contextual framing effects rather than genuine demographic bias, making interpretation of bias findings uncertain
- The exact mechanisms of how personas activate latent representations remain unclear, limiting understanding of why impersonation works

## Confidence

- High confidence: The core finding that persona-based prompting can systematically alter LLM performance across multiple tasks (multi-armed bandit exploration, MMLU reasoning accuracy, and visual classification)
- Medium confidence: The interpretation that observed performance changes represent genuine "impersonation" of developmental stages and expertise levels, rather than task-specific priming effects
- Low confidence: The claim that social identity personas directly reveal underlying demographic biases, as opposed to contextual framing effects

## Next Checks

1. Run ablation studies removing the "If you were a {persona}" prefix while keeping age/expertise words in the prompt to determine whether performance effects require explicit role framing or can be achieved through simpler semantic priming.

2. Test the persona prompting effects across multiple LLM architectures (not just Vicuna-13B and ChatGPT) to determine whether the effects are model-specific or represent general capabilities of large language models.

3. Conduct controlled experiments where the same social identities are framed in neutral versus task-specific contexts to separate genuine demographic bias from contextual priming effects on task performance.