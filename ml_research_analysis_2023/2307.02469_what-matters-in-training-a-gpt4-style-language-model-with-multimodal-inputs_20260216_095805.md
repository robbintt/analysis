---
ver: rpa2
title: What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?
arxiv_id: '2307.02469'
source_url: https://arxiv.org/abs/2307.02469
tags:
- image
- arxiv
- language
- instruction
- multi-modal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic study on training GPT4-style language
  models with multimodal inputs. The authors conduct extensive experiments with over
  20 model variants to investigate the impact of network structures, training data,
  and instruction diversity.
---

# What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?

## Quick Facts
- arXiv ID: 2307.02469
- Source URL: https://arxiv.org/abs/2307.02469
- Authors: 
- Reference count: 40
- Key outcome: Prefix-tuning with trainable adapters is more effective than cross-attention for multimodal adaptation, data quality is more important than quantity, and diversified prompts significantly improve instruction-following ability.

## Executive Summary
This paper presents a systematic study on training GPT4-style language models with multimodal inputs. Through extensive experiments with over 20 model variants, the authors investigate the impact of network structures, training data, and instruction diversity on model performance. They find that prefix-tuning with trainable adapters is more effective than cross-attention for multimodal adaptation, data quality is more important than quantity, and diversified prompts significantly improve instruction-following ability. Based on these findings, they present Lynx, a model that achieves state-of-the-art performance on both multimodal understanding and text generation benchmarks.

## Method Summary
The authors conduct a two-stage training approach: first aligning visual and linguistic embeddings using 120M image-text pairs, then finetuning on 20 multimodal tasks with instruction data. They systematically compare different architectural choices including prefix-tuning with trainable adapters versus cross-attention, experiment with various training data qualities and quantities, and investigate the impact of prompt diversity on instruction-following ability. The model uses Vicuna-7B as backbone with EVA-1B vision encoder, implementing a resampler to condense vision tokens to fixed length.

## Key Results
- Prefix-tuning with trainable adapters (PT) outperforms cross-attention (CA) models like Flamingo for multimodal adaptation
- Models trained on 120M high-quality image-text pairs outperform those trained on 700M+ noisy pairs
- Diversified prompts significantly improve instruction-following ability and final model performance
- Lynx achieves state-of-the-art performance on Open-VQA benchmark and human evaluations of language generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prefix-tuning with trainable adapters (PT) is more suitable than cross-attention (CA) for multimodal adaptation of LLMs.
- Mechanism: PT directly concatenates projected vision tokens with instruction tokens and feeds them into the decoder-only LLM, while CA uses cross-attention layers to fuse modalities. This simpler fusion in PT preserves the LLM's language generation ability better than the complex cross-attention fusion.
- Core assumption: The LLM's autoregressive generation capability is crucial for maintaining language generation ability, and simpler token concatenation preserves this better than cross-attention fusion.
- Evidence anchors:
  - [abstract]: "Prefix-tuning with trainable adaptors, namely PT models, as shown in Fig.1 is more suitable to adapt LLMs to multi-modal inputs compared to cross attention (e.g. Flamingo [19]), namely CA models."
  - [section]: "We find that by adding a small trainable adapter after some layers in the frozen LLMs, the performance could be further improved with low training costs."
  - [corpus]: Weak evidence - no direct citations about PT vs CA performance in the corpus
- Break condition: If cross-attention models can be trained with better data curation or architectural modifications that preserve language generation ability, the advantage of PT may diminish.

### Mechanism 2
- Claim: Data quality is more important than quantity for generative pretraining of multimodal LLMs.
- Mechanism: Generative pretraining learns the conditional distribution of words, so noisy or unnatural text in large-scale datasets damages the model's language generation ability, while smaller high-quality datasets preserve it better.
- Core assumption: The generative pretraining objective is sensitive to text quality because it directly models word distributions, unlike contrastive pretraining which only learns similarity between modalities.
- Evidence anchors:
  - [abstract]: "Data quality is more important than quantity. We find that models trained on large-scale image text pairs like COYO700M and DataComp1B are not better to generate languages than models trained on a much smaller but high-quality dataset, since they can contaminate the output distribution."
  - [section]: "We find that multi-modal LLMs do not benefit from large-scale but noisy image-text pairs because many of the texts in such datasets are not fluent or natural language expressions."
  - [corpus]: Weak evidence - no direct citations about data quality vs quantity in generative pretraining
- Break condition: If future models develop better text filtering or denoising techniques that can effectively clean large-scale noisy datasets, the advantage of small high-quality datasets may diminish.

### Mechanism 3
- Claim: Diversified prompts are crucial for improving instruction-following ability and final performance.
- Mechanism: Training on diverse tasks and prompts exposes the model to varied instruction formats, improving its ability to generalize to new instructions and follow them correctly.
- Core assumption: Zero-shot generality of multimodal language models depends on the diversity of tasks seen during training, similar to text-only models.
- Evidence anchors:
  - [abstract]: "Diversified prompts are crucial to the improvement of the instruction-following ability and, consequently, final performance."
  - [section]: "We manually labeled at least 3 different prompts for each task and generate more with GPT4 to figure out the influence of the diversity of language prompts."
  - [corpus]: Weak evidence - no direct citations about prompt diversity in multimodal instruction tuning
- Break condition: If models develop better instruction understanding capabilities that don't require extensive prompt diversity, or if a smaller set of highly representative prompts can achieve similar performance.

## Foundational Learning

- Concept: Autoregressive generation in LLMs
  - Why needed here: The paper builds on decoder-only LLMs that generate text autoregressively, and understanding this is crucial for grasping how multimodal inputs are processed and how outputs are generated
  - Quick check question: How does an autoregressive model generate text, and why is this relevant for multimodal instruction following?

- Concept: Vision-language pretraining vs instruction tuning
  - Why needed here: The paper distinguishes between pretraining (aligning visual and linguistic embeddings) and instruction tuning (learning to follow instructions), and understanding this distinction is crucial for the training pipeline
  - Quick check question: What's the difference between pretraining and instruction tuning in the context of multimodal LLMs?

- Concept: Cross-attention vs token concatenation
  - Why needed here: The paper compares two architectural approaches for multimodal fusion - cross-attention and direct token concatenation - and understanding these mechanisms is crucial for grasping the design choices
  - Quick check question: How do cross-attention and direct token concatenation differ in how they fuse visual and language information?

## Architecture Onboarding

- Component map:
  Vision encoder (EVA-1B) → Resampler Φ → Prefix-tuning adapters → Decoder-only LLM (Vicuna/LLaMA) → Output generation

- Critical path: Vision input → EVA → Resampler → Adapter layers → LLM → Output generation

- Design tradeoffs:
  - Fixed vs trainable vision encoder: EVA is frozen to save computation but may limit fine-tuning capability
  - Adapter frequency (every block vs fewer): More frequent adapters increase capacity but also computational cost
  - Prompt diversity vs quality: More diverse prompts improve generalization but may introduce noise

- Failure signatures:
  - Short outputs: May indicate over-training on VQA tasks or insufficient language generation capability
  - Factual errors: May indicate under-training or poor data quality
  - Poor instruction following: May indicate insufficient prompt diversity or task coverage

- First 3 experiments:
  1. Ablation study: Remove adapters and compare performance to verify their contribution
  2. Data quality test: Train with large-scale noisy data vs small clean data to verify data quality importance
  3. Architecture comparison: Implement cross-attention version and compare to prefix-tuning version to verify architectural claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Lynx compare to GPT-4 on multimodal benchmarks?
- Basis in paper: [inferred] The paper states that Lynx "performs the most accurate multi-modal understanding while keeping the best multi-modal generation ability compared to existing open-sourced GPT4-style models," but does not directly compare to GPT-4.
- Why unresolved: The paper focuses on comparing Lynx to other open-source models, but does not include GPT-4 in the comparison.
- What evidence would resolve it: Direct benchmark comparisons between Lynx and GPT-4 on the same multimodal tasks.

### Open Question 2
- Question: What is the optimal balance between correctness and language generation ability in multi-modal LLMs?
- Basis in paper: [explicit] The paper states that "balancing the correctness and language generation is important" but does not provide a specific methodology for finding this balance.
- Why unresolved: The paper discusses the importance of this balance but does not provide a clear framework for achieving it.
- What evidence would resolve it: A systematic study of different training data combinations and their effects on correctness vs. generation ability.

### Open Question 3
- Question: How does the performance of multi-modal LLMs vary across different languages?
- Basis in paper: [explicit] The paper mentions that "our model is not that good at multi-lingual responses" and suggests that "it is still unexplored how to build a high-performance multi-lingual and multi-modal large language model."
- Why unresolved: The paper only briefly mentions this limitation without exploring it further.
- What evidence would resolve it: A comprehensive evaluation of multi-modal LLMs' performance across multiple languages.

## Limitations

- The superiority of prefix-tuning over cross-attention architecture remains weakly supported without comprehensive ablation studies
- Data quality findings lack rigorous quantitative analysis and controlled experiments comparing different dataset qualities
- Limited documentation of specific hyperparameters and training details for the two-stage training approach

## Confidence

**High Confidence**: The finding that diversified prompts improve instruction-following ability has strong support through systematic experimentation with multiple prompt variations per task. The Open-VQA benchmark results and human evaluation comparisons provide solid evidence for this claim.

**Medium Confidence**: The overall two-stage training approach (pretraining + instruction tuning) and its effectiveness on multimodal benchmarks is well-supported by experimental results. However, the specific hyperparameters and training details have limited documentation.

**Low Confidence**: The superiority of prefix-tuning over cross-attention architecture remains weakly supported. The paper provides theoretical arguments but lacks comprehensive ablation studies or direct architectural comparisons to validate this as a general principle.

## Next Checks

1. **Cross-Attention vs Prefix-Tuning Ablation**: Implement a direct comparison where both architectures are trained with identical data, hyperparameters, and training duration. Measure not just final performance but also training stability, convergence speed, and generalization to unseen tasks.

2. **Data Quality Controlled Experiment**: Train models on datasets with systematically varied quality levels (using the same data source but different filtering thresholds) while keeping total size constant. Measure the relationship between data quality metrics and generation quality across multiple domains.

3. **Instruction Diversity Scaling Study**: Systematically vary the number and diversity of prompts per task (e.g., 1, 3, 10, 20 prompts) while controlling for total training examples. Measure instruction-following ability on both seen and unseen instruction formats to quantify the optimal diversity level.