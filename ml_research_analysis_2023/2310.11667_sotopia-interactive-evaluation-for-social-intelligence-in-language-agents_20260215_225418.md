---
ver: rpa2
title: 'SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents'
arxiv_id: '2310.11667'
source_url: https://arxiv.org/abs/2310.11667
tags:
- social
- gpt-4
- human
- agents
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SOTOPIA is a novel, interactive environment for evaluating social
  intelligence in language agents. It simulates realistic social interactions between
  agents role-playing characters with diverse backgrounds and goals.
---

# SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents

## Quick Facts
- arXiv ID: 2310.11667
- Source URL: https://arxiv.org/abs/2310.11667
- Reference count: 40
- Key outcome: Novel interactive environment for evaluating social intelligence in language agents through multi-agent role-playing with comprehensive multi-dimensional assessment framework

## Executive Summary
SOTOPIA introduces a novel interactive environment for evaluating social intelligence in language agents through realistic multi-agent role-playing scenarios. The environment features 90 diverse social scenarios, 40 richly characterized individuals, and a comprehensive seven-dimensional evaluation framework called SOTOPIA-Eval. The system uses GPT-4 both as an agent participant and as a proxy evaluator, with experiments showing strong correlation between GPT-4 and human judgments on goal completion and financial outcomes, while revealing significant gaps in models' abilities to maintain secrets and follow social norms.

## Method Summary
SOTOPIA simulates turn-based social interactions between agents role-playing characters with diverse backgrounds, personalities, and goals. Each episode involves two agents engaging in a social scenario until termination, with actions categorized as speaking, non-verbal, or physical. The SOTOPIA-Eval framework assesses performance across seven dimensions: goal completion, believability, knowledge acquisition, secret-keeping, relationship maintenance, social rule adherence, and financial outcomes. GPT-4 serves as a proxy evaluator, providing scores and rationales based on the same instructions given to human annotators. The system includes a "SOTOPIA-hard" subset identifying scenarios where models struggle most significantly compared to human performance.

## Key Results
- GPT-4 shows strong correlation with human judgments on GOAL, FIN, and REL dimensions (correlation 0.71, p ≤ 0.01)
- All models, including GPT-4, achieve negative scores on secret-keeping (SEC) and social rules (SOC) dimensions
- GPT-4 significantly underperforms humans on SOTOPIA-hard subset, particularly in strategic communication and social reasoning
- Larger models (GPT-4) generally outperform smaller models across most dimensions, but size alone doesn't guarantee superior social intelligence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can serve as a reasonable proxy for human judgment on most evaluation dimensions, especially goal completion.
- Mechanism: GPT-4 is prompted with the same instructions and criteria as human annotators, allowing it to produce structured scores with rationales. The scores show strong correlation with human judgments (e.g., GOAL dimension correlation of 0.71 with p ≤ 0.01).
- Core assumption: GPT-4 has sufficient social commonsense and evaluation ability to align with human judgment patterns when given clear criteria.
- Evidence anchors:
  - [abstract] "Experiments show that GPT-4 can serve as a reasonable proxy for human judgment in evaluating agent performance on most dimensions, especially goal completion."
  - [section 5.2] "The correlations show that when models are role-playing, the GPT-4 scores have significant and strong correlations with the humans' scores on GOAL, FIN, and REL dimensions."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.448, average citations=0.0." (Weak corpus evidence for this specific mechanism)
- Break condition: GPT-4's evaluation diverges from human judgment on dimensions requiring nuanced social reasoning (e.g., SOC and SEC dimensions show weaker correlation).

### Mechanism 2
- Claim: SOTOPIA environment reveals significant differences between models and humans in social intelligence, especially on challenging tasks.
- Mechanism: SOTOPIA-hard subset identifies scenarios where GPT-4 achieves significantly lower goal completion rates than humans, exposing limitations in social reasoning and strategic communication.
- Core assumption: The task space and evaluation framework can differentiate performance levels between models and humans in realistic social interactions.
- Evidence anchors:
  - [abstract] "We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills."
  - [section 7] "We find that GPT-4 always rephrases the utterance back at the other agent and then answers, which is a communication skill called active listening (Harry Weger & Robinson, 2014), whereas humans typically directly answer."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.448, average citations=0.0." (Weak corpus evidence for this specific mechanism)
- Break condition: Models improve their social reasoning capabilities to match or exceed human performance on SOTOPIA-hard tasks.

### Mechanism 3
- Claim: Larger LLMs (GPT-4) perform better on most SOTOPIA dimensions but still struggle with secret-keeping and social rule adherence.
- Mechanism: Model performance comparison shows GPT-4 outperforms smaller models (GPT-3.5, Llama-2, MPT) across most dimensions except SEC and SOC, indicating size matters but doesn't guarantee perfect social intelligence.
- Core assumption: Model size correlates with social intelligence capabilities, but specific social skills like secret-keeping require different training approaches.
- Evidence anchors:
  - [section 6] "GPT-4 performs best on most dimensions, followed by GPT-3.5, Llama-2-70b-chat, and MPT-30b-chat."
  - [section 6] "It is also worth noting that all models have a negative score in the SOC and SEC dimensions. Even though GPT-4 performs better in most dimensions, it is not better than other models in the SOC and SEC dimensions."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.448, average citations=0.0." (Weak corpus evidence for this specific mechanism)
- Break condition: Models develop better mechanisms for maintaining secrets and following social rules without compromising other social skills.

## Foundational Learning

- Concept: Multi-agent reinforcement learning framework
  - Why needed here: SOTOPIA is formalized as a mixed-motive Markov game with vector rewards across multiple social dimensions, requiring understanding of multi-agent RL concepts.
  - Quick check question: What are the key differences between single-agent RL and multi-agent RL in terms of state observability and reward structure?

- Concept: Social intelligence evaluation metrics
  - Why needed here: The SOTOPIA-Eval framework uses seven dimensions (GOAL, BEL, KNO, SEC, REL, SOC, FIN) that require understanding of social psychology and economics concepts.
  - Quick check question: How would you design a rubric to evaluate an agent's performance on the "Secret-keeping" dimension?

- Concept: LLM evaluation and bias
  - Why needed here: Using GPT-4 as an evaluator requires understanding of LLM limitations, biases (positional bias, factual inconsistency), and how these might affect social intelligence assessment.
  - Quick check question: What are potential sources of bias when using LLMs for evaluating human-like social interactions?

## Architecture Onboarding

- Component map: Task space generator (scenarios, characters, relationships) → Interaction engine (turn-based role-playing) → Evaluation framework (SOTOPIA-Eval) → Evaluation automation (GPT-4 proxy)
- Critical path: Task generation → Character assignment → Turn-based interaction → Action generation → Episode termination → Multi-dimensional evaluation → Results aggregation
- Design tradeoffs: Interactive vs. static evaluation (SOTOPIA chose interactive for realism but increased complexity), human vs. LLM evaluation (LLMs provide scalability but may have biases), fixed vs. dynamic relationship types (fixed types simplify but limit diversity)
- Failure signatures: Models fail to maintain persona, stall conversations, reveal secrets, violate social norms, or show poor strategic communication. GPT-4 evaluator may show positional bias or inconsistency in scoring.
- First 3 experiments:
  1. Run SOTOPIA with two identical models to establish baseline performance and identify any emergent coordination behaviors.
  2. Compare GPT-4 vs. human evaluation on a small subset to quantify correlation and identify dimensions where divergence occurs.
  3. Test model performance on SOTOPIA-hard subset to identify specific scenario types that challenge current models most severely.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GPT-4 serve as a reliable proxy for human judgment in evaluating the multi-dimensional framework of SOTOPIA-EVAL across all dimensions?
- Basis in paper: [explicit] The paper states "GPT-4 could serve as a proxy to human judgments on SOTOPIA-EVAL, especially for the criteria of goal completion, maintaining finances, and preserving relationships."
- Why unresolved: While GPT-4 shows promise in certain dimensions, it struggles with others, particularly social rules and secret-keeping. The paper suggests more work is needed to improve LLM-based evaluation.
- What evidence would resolve it: Further experiments comparing GPT-4 and human judgments across a wider range of dimensions and scenarios would provide more conclusive evidence.

### Open Question 2
- Question: How do different LLMs compare in their social intelligence, and what factors contribute to these differences?
- Basis in paper: [explicit] The paper compares the performance of GPT-4, GPT-3.5, Llama-2-70b-chat, and MPT-30b-chat across various dimensions of social intelligence.
- Why unresolved: While the paper identifies differences, it does not delve into the underlying reasons for these differences. Further analysis of the models' training data, architectures, and fine-tuning processes could shed light on this.
- What evidence would resolve it: Detailed analysis of the models' internal representations and comparison with their training data would help identify the factors contributing to their varying social intelligence.

### Open Question 3
- Question: What are the key differences in social interaction strategies between humans and LLMs, and how can these differences be leveraged to improve LLM social intelligence?
- Basis in paper: [explicit] The paper highlights differences in efficiency, strategic communication, and goal persistence between humans and GPT-4.
- Why unresolved: The paper provides examples but does not offer a comprehensive analysis of the underlying strategies employed by humans and LLMs. Further research could explore the cognitive processes and social norms that guide human interaction.
- What evidence would resolve it: Comparative studies of human and LLM interaction patterns, coupled with insights from cognitive science and social psychology, could provide a deeper understanding of these differences.

## Limitations
- Reliance on GPT-4 as both agent and evaluator creates potential circular validation issues
- Focus on English language and Western social norms limits generalizability to other cultural contexts
- Fixed character types and relationship structures may not capture full complexity of real-world social dynamics

## Confidence
**High Confidence**: GPT-4 serves as reasonable proxy for human judgment on goal completion and financial outcomes (correlation 0.71, p ≤ 0.01). SOTOPIA-Eval framework provides structured approach to assessing social intelligence.

**Medium Confidence**: SOTOPIA reveals performance gaps between models and humans on challenging tasks. SOTOPIA-hard subset demonstrates clear differences, but specific "hard" scenarios may be influenced by character profiles used.

**Low Confidence**: Larger LLMs consistently perform better on social intelligence tasks. While GPT-4 outperforms smaller models on most dimensions, all models struggle with secret-keeping and social rule adherence, suggesting size alone doesn't guarantee superior social reasoning.

## Next Checks
1. **Cross-cultural validation**: Test SOTOPIA with diverse cultural scenarios to evaluate whether GPT-4's judgments remain consistent across different social norms.
2. **Human-only evaluation**: Conduct comprehensive human evaluation on full SOTOPIA dataset to validate GPT-4's proxy effectiveness across all seven dimensions.
3. **Dynamic relationship testing**: Implement SOTOPIA version with evolving relationships to assess whether this improves model performance on social reasoning tasks.