---
ver: rpa2
title: How Faithful are Self-Explainable GNNs?
arxiv_id: '2308.15096'
source_url: https://arxiv.org/abs/2308.15096
tags:
- graph
- faithfulness
- self-explainable
- neural
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates the faithfulness of self-explainable graph
  neural networks (GNNs) using three metrics: unfaithfulness (Unf), fidelity as sufficiency
  (Fid-), and fidelity as necessity (Fid+). Testing on four datasets with models like
  GISST, ProtGNN, and PIGNN, the research finds that faithfulness is not guaranteed
  and varies widely across tasks.'
---

# How Faithful are Self-Explainable GNNs?

## Quick Facts
- arXiv ID: 2308.15096
- Source URL: https://arxiv.org/abs/2308.15096
- Authors: 
- Reference count: 39
- Key outcome: This study evaluates the faithfulness of self-explainable graph neural networks (GNNs) using three metrics: unfaithfulness (Unf), fidelity as sufficiency (Fid-), and fidelity as necessity (Fid+). Testing on four datasets with models like GISST, ProtGNN, and PIGNN, the research finds that faithfulness is not guaranteed and varies widely across tasks. For example, PIGNN+P achieves low unfaithfulness (0.07) on BBBP but high unfaithfulness (0.50) on Ba2Motif. The study also highlights that random subgraph explanations sometimes perform as well or better than model-generated ones, questioning the reliability of faithfulness metrics. This suggests that self-explainable GNNs may not reliably deliver faithful explanations, raising concerns for their use in high-stakes applications.

## Executive Summary
This paper investigates the faithfulness of self-explainable graph neural networks (GNNs) by evaluating three metrics: unfaithfulness, fidelity as sufficiency, and fidelity as necessity. Testing on four datasets with models like GISST, ProtGNN, and PIGNN, the study finds that faithfulness is not guaranteed and varies widely across tasks. For example, PIGNN+P achieves low unfaithfulness (0.07) on BBBP but high unfaithfulness (0.50) on Ba2Motif. The research also highlights that random subgraph explanations sometimes perform as well or better than model-generated ones, questioning the reliability of faithfulness metrics. These findings raise concerns about the use of self-explainable GNNs in high-stakes applications where faithful explanations are critical.

## Method Summary
The study evaluates the faithfulness of self-explainable GNNs using three metrics: unfaithfulness (Unf), fidelity as sufficiency (Fid-), and fidelity as necessity (Fid+). The authors test three self-explainable GNN models (GISST, ProtGNN, PIGNN) on four datasets (MUTAG, BBBP, Ba2Motif, BaMS) with 80/10/10 train/validation/test splits. Explanations are generated and compared against random subgraphs of the same size to assess faithfulness. The study also investigates the consistency of faithfulness across tasks and the reliability of the metrics themselves.

## Key Results
- Self-explainable GNNs do not guarantee faithfulness, with unfaithfulness scores varying widely across tasks and models.
- Random subgraph explanations sometimes perform as well or better than model-generated explanations, questioning the reliability of faithfulness metrics.
- Faithfulness varies significantly across datasets, with PIGNN+P achieving low unfaithfulness (0.07) on BBBP but high unfaithfulness (0.50) on Ba2Motif.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-explainable GNNs do not guarantee faithfulness by design, as their explanations are faithful only at the concept level but become unfaithful when mapped to low-level subgraphs.
- Mechanism: The models first generate faithful explanations at the level of high-level concepts (e.g., prototypes) but must then map these concepts to specific nodes, edges, and features to produce the final subgraph explanation. This mapping is inexact or non-bijective, leading to a loss of faithfulness.
- Core assumption: The mapping between high-level concepts and low-level graph elements is not perfectly preserved, introducing noise or errors.
- Evidence anchors:
  - [abstract]: "Our results highlight several interesting phenomena. First, these architectures fail to guarantee faithfulness, thus falling short of the original desideratum."
  - [section 4]: "One possible motivation behind these results is that the explanations offered by self-explainable models are faithful at the concept level... In order to convert this into a subgraph, they have to ground the explanation in terms of nodes, edges, and features, rather than prototypes. This mapping is usually inexact... and as such the resulting low-level explanation no longer has faithfulness guarantees."
  - [corpus]: Weak evidence. No direct support for this mechanism in the corpus.
- Break condition: If the mapping between concepts and graph elements becomes bijective or perfectly preserved, faithfulness could be maintained.

### Mechanism 2
- Claim: Faithfulness metrics are incomplete and misleading because they lack a natural baseline for comparison, leading to potentially inflated assessments of explanation quality.
- Mechanism: Metrics like unfaithfulness (Unf) and fidelity (Fid-, Fid+) measure how well explanations align with model predictions, but without a baseline (e.g., random explanations), it's impossible to determine if the model's explanations are genuinely better or just marginally better than chance.
- Core assumption: A random explanation baseline is necessary to contextualize the faithfulness scores and reveal whether the model's explanations are truly meaningful.
- Evidence anchors:
  - [section 3]: "We offer such a term of reference by compare each measure by the same measure computed on a random explanation model that generates subgraphs R of G of the same size of the explanation E... We can see that oftentimes the model's explanations are in fact more faithful than the random ones, especially for MUTAG data. Yet, in many cases the models produce worse or comparable explanations to a random graph."
  - [corpus]: Weak evidence. No direct support for this mechanism in the corpus.
- Break condition: If a standardized baseline becomes widely adopted and integrated into faithfulness metrics, the misleading nature of absolute scores would be mitigated.

### Mechanism 3
- Claim: Faithfulness varies widely across tasks and datasets, making it difficult to trust explanations in high-stakes applications where consistency is critical.
- Mechanism: The faithfulness of self-explainable GNNs depends heavily on the specific task and dataset characteristics, leading to inconsistent performance. For example, a model might achieve excellent unfaithfulness on one dataset but poor unfaithfulness on another, undermining its reliability.
- Core assumption: The structure and complexity of the dataset influence how well the model can generate faithful explanations, and this effect is not uniform across tasks.
- Evidence anchors:
  - [abstract]: "For example, PIGNN+P achieves low unfaithfulness (0.07) on BBBP but high unfaithfulness (0.50) on Ba2Motif."
  - [section 3]: "Moreover, explanation faithfulness varies widely across learning tasks. For instance, PIGNN+P attains excellent Unf and Fid− in BBBP (0.07 for both), but fares poorly in Ba2Motif (0.50 and 0.59, respectively). A 10-15% difference in unfaithfulness also exists for the other models."
  - [corpus]: Weak evidence. No direct support for this mechanism in the corpus.
- Break condition: If faithfulness becomes more consistent across tasks through improved model architectures or training strategies, this mechanism would no longer apply.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message-passing mechanism
  - Why needed here: Understanding how GNNs process graph data is essential to grasp why their explanations might be unfaithful.
  - Quick check question: How do GNNs aggregate information from neighboring nodes during the message-passing step?

- Concept: Faithfulness metrics (Unfaithfulness, Fidelity as Sufficiency, Fidelity as Necessity)
  - Why needed here: These metrics are used to evaluate the quality of explanations, and understanding their definitions and limitations is crucial for interpreting the study's findings.
  - Quick check question: What is the key difference between fidelity as sufficiency (Fid-) and fidelity as necessity (Fid+)?

- Concept: Self-explainable GNNs and their architectural biases (e.g., information constraints, prototype-based methods)
  - Why needed here: Knowing how these models generate explanations is necessary to understand why they might fail to guarantee faithfulness.
  - Quick check question: How do prototype-based self-explainable GNNs like ProtGNN and PIGNN generate their explanations?

## Architecture Onboarding

- Component map: Graph data (nodes, edges, features) -> GNN backbone (message-passing layers) -> Explanation generator (information-constrained layer or prototype matching) -> Predicted labels and subgraph explanations
- Critical path: Graph data → GNN embeddings → Explanation generation → Subgraph extraction → Prediction
- Design tradeoffs:
  - Tradeoff between expressiveness and interpretability: Self-explainable GNNs sacrifice some model capacity to produce interpretable explanations, but this can lead to reduced faithfulness.
  - Tradeoff between concept-level and subgraph-level explanations: Faithful explanations at the concept level may not translate to faithful subgraph explanations due to inexact mappings.
- Failure signatures:
  - High unfaithfulness (Unf) or low fidelity scores (Fid-, Fid+) indicate poor explanation quality.
  - Wide variation in faithfulness across datasets suggests inconsistent performance.
  - Explanations comparable to or worse than random subgraphs indicate a lack of meaningful interpretability.
- First 3 experiments:
  1. Reproduce the faithfulness metrics (Unf, Fid-, Fid+) on a simple synthetic dataset to verify the implementation.
  2. Compare the model's explanations to random subgraphs on a benchmark dataset to establish a baseline.
  3. Test the model on multiple datasets with varying characteristics to assess the consistency of faithfulness across tasks.

## Open Questions the Paper Calls Out
- Question: What are the root causes of unfaithfulness in self-explainable GNNs?
- Basis in paper: [inferred] The paper suggests that the mapping from interpretable concepts (like prototypes) to low-level graph elements (nodes, edges, features) is often inexact, leading to unfaithfulness.
- Why unresolved: The study identifies this as a potential cause but does not provide a detailed analysis of how this mapping fails across different architectures.
- What evidence would resolve it: A systematic analysis comparing the interpretability of high-level concepts (e.g., prototypes) versus the resulting subgraphs, identifying specific failure modes in the mapping process.

- Question: How do faithfulness metrics perform when evaluated against a baseline of random explanations?
- Basis in paper: [explicit] The paper compares model explanations to random subgraphs of the same size, finding that random explanations sometimes perform as well or better than model-generated ones.
- Why unresolved: The study highlights this issue but does not explore why random explanations can be competitive or how to improve the metrics.
- What evidence would resolve it: A deeper investigation into the properties of random subgraphs that make them competitive, and the development of more robust faithfulness metrics.

- Question: To what extent do the findings on faithfulness apply to node classification tasks?
- Basis in paper: [inferred] The paper focuses on graph classification and mentions plans to extend the analysis to node classification, but does not provide results for this task.
- Why unresolved: The study does not include empirical results for node classification, leaving the generalizability of the findings unclear.
- What evidence would resolve it: Empirical results comparing the faithfulness of self-explainable GNNs on node classification tasks, using the same metrics and models as in the graph classification study.

## Limitations
- The study focuses on four specific datasets and three self-explainable GNN architectures, limiting generalizability to other graph types and models.
- The faithfulness metrics (Unf, Fid-, Fid+) may not fully capture the quality of explanations, as random subgraphs sometimes perform as well or better than model-generated ones.
- The paper does not explore the root causes of unfaithfulness in detail or provide solutions to improve faithfulness across tasks.

## Confidence
- Core claim (self-explainable GNNs fail to guarantee faithfulness): High confidence
- Observation (random explanations sometimes match or exceed model explanations): Medium confidence

## Next Checks
1. Test the faithfulness metrics on additional datasets with varying graph characteristics (different sizes, densities, and label distributions) to assess broader generalizability.
2. Implement alternative explanation generation methods (e.g., gradient-based, perturbation-based) and compare their faithfulness scores against the current self-explainable models to establish relative performance.
3. Conduct ablation studies removing the concept-to-subgraph mapping step to quantify how much faithfulness is lost in this conversion, directly testing the proposed mechanism.