---
ver: rpa2
title: 'Breiman meets Bellman: Non-Greedy Decision Trees with MDPs'
arxiv_id: '2309.12701'
source_url: https://arxiv.org/abs/2309.12701
tags:
- trees
- decision
- tree
- accuracy
- dpdt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Dynamic Programming Decision Trees (DPDT),
  a framework that bridges the gap between greedy and optimal decision tree algorithms
  by formulating decision tree learning as a Markov Decision Process (MDP). The key
  idea is to dynamically limit the set of admissible splits at each node while directly
  optimizing the tree regularized training loss, allowing for a continuum between
  optimal algorithms and heuristic approaches.
---

# Breiman meets Bellman: Non-Greedy Decision Trees with MDPs

## Quick Facts
- arXiv ID: 2309.12701
- Source URL: https://arxiv.org/abs/2309.12701
- Reference count: 40
- The paper introduces Dynamic Programming Decision Trees (DPDT), a framework that bridges the gap between greedy and optimal decision tree algorithms by formulating decision tree learning as a Markov Decision Process (MDP).

## Executive Summary
This paper introduces Dynamic Programming Decision Trees (DPDT), a novel framework that bridges the gap between greedy and optimal decision tree algorithms by formulating decision tree learning as a Markov Decision Process (MDP). The key innovation is dynamically limiting admissible splits at each node while directly optimizing a tree-regularized training loss, allowing for a continuum between optimal algorithms and heuristic approaches. By varying a parameter α, DPDT can return a set of trees on the interpretability-performance Pareto front, controlling the trade-off between accuracy and interpretability measured as expected number of tests performed on data.

## Method Summary
DPDT formulates decision tree learning as an MDP where states represent dataset subsets and depths, actions represent either splitting on a feature or classifying, and rewards balance classification accuracy against interpretability (measured as expected number of tests). The algorithm uses CART as a tests generating function to dramatically reduce the MDP state space while maintaining high accuracy. Dynamic programming computes optimal policies for all α values simultaneously, returning a Pareto front of trees that span the interpretability-accuracy trade-off. This approach achieves near-optimal loss with orders of magnitude fewer operations than existing optimal solvers while providing statistically significant improvements over both CART and optimal decision trees in generalization to unseen data.

## Key Results
- DPDT achieves near-optimal loss with orders of magnitude fewer operations than existing optimal solvers
- The method provides statistically significant improvements over both CART and optimal decision trees in terms of generalization to unseen data
- DPDT can return a set of trees on the interpretability-performance Pareto front by varying parameter α

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The MDP formulation directly optimizes a trade-off between accuracy and interpretability by encoding interpretability as expected number of tests in the reward function.
- Mechanism: The reward function Rα(s,a) assigns a cost α for each test action and a negative classification reward for leaf assignments. This makes the optimal policy balance test count against classification accuracy.
- Core assumption: The expected number of tests a tree performs on data (C(T)) is a valid measure of interpretability.
- Evidence anchors:
  - [abstract]: "The MDP reward will be parameterized by α, and by decreasing the value of α, the returned tree will be more accurate but also perform more tests in average, i.e it is less interpretable."
  - [section 4]: "The reward of taking an action a in state s is given by the parameterized mapping Rα: S × A → R that enforces a trade-off between the expected number of tests and the classification accuracy."
- Break condition: If interpretability requires structural properties beyond test count (e.g., tree depth, path length variation), the reward may not capture the true interpretability metric.

### Mechanism 2
- Claim: Using CART as a tests generating function dramatically reduces the MDP state space while maintaining high accuracy.
- Mechanism: Instead of considering all possible splits (exponential in dataset size), the tests generating function only considers splits from CART's decision nodes, limiting the action space per state.
- Core assumption: The splits chosen by CART are representative of good splits for the optimal tree.
- Evidence anchors:
  - [section 5.2]: "We propose to use CART (Breiman 1984) as a tests generating function. At every state (X, d), the split actions are the splits in the decision nodes of the tree returned by a call to CART with a maximum tree depth Dcart."
  - [section 5.2]: "In practice we observe that using calls to CART as a tests generating function leads to an MDP with fewer states and whose solutions yield accurate trees."
- Break condition: If CART's greedy splits consistently miss optimal splits for certain data distributions, the reduced action space could prevent finding high-accuracy trees.

### Mechanism 3
- Claim: The dynamic programming approach computes the full Pareto front of interpretability-performance trade-offs in a single backward pass.
- Mechanism: By defining Q∗(s,a,α) that depends on α, the algorithm can compute optimal policies for all α values simultaneously during the Bellman backup.
- Core assumption: The MDP has finite horizon and acyclic structure allowing efficient backward computation.
- Evidence anchors:
  - [section 5.3]: "To obtain the Pareto front of interpretability-performance trade-off, it is sufficient to define a Q-function that depends on α: Q∗(s,a,α) = ... We can then find all policies: π∗(s,α) = argmax a∈A Q∗(s,a,α)"
  - [abstract]: "Our framework provides a promising direction for developing efficient, near-optimal decision tree algorithms that scale to practical applications."
- Break condition: If the MDP state space becomes too large despite the tests generating function, the backward pass could become computationally infeasible.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: The entire framework frames decision tree learning as an MDP to leverage dynamic programming for optimization.
  - Quick check question: What are the components of an MDP and how do they map to decision tree learning in this paper?

- Concept: Dynamic Programming and Bellman Optimality
  - Why needed here: The algorithm uses backward value iteration to compute optimal policies for multiple trade-off parameters.
  - Quick check question: How does the Bellman equation relate to computing optimal decision trees in this context?

- Concept: Pareto Fronts in Multi-Objective Optimization
  - Why needed here: The method returns a set of trees representing different interpretability-accuracy trade-offs rather than a single optimal point.
  - Quick check question: Why is computing a Pareto front more useful than finding a single trade-off point for decision tree applications?

## Architecture Onboarding

- Component map: MDP constructor -> Tests generating function (CART) -> State space builder -> Dynamic programming solver -> Policy extractor -> Tree constructor
- Critical path: MDP construction (most time-consuming) -> Dynamic programming backup -> Policy extraction
- Design tradeoffs: Exhaustive split consideration vs. CART-based generation (accuracy vs. scalability)
- Failure signatures: Excessive MDP states -> memory overflow; Poor accuracy -> tests generating function too restrictive
- First 3 experiments:
  1. Run DPDT-3 on Iris dataset with α ∈ [0,1] and verify Pareto front shape
  2. Compare runtime and accuracy against CART with post-pruning on same dataset
  3. Test different tests generating functions (exhaustive vs. CART) on small dataset to measure state space reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can DPDT's runtime be improved to handle larger datasets with millions of samples?
- Basis in paper: [inferred] The paper mentions that DPDT's runtime could be improved with caching and parallelism, and that scaling to larger datasets could be achieved by using Monte Carlo Tree Search to assess the quality of states using only a fraction of the dataset.
- Why unresolved: While the paper suggests potential improvements, it does not provide concrete evidence or experiments demonstrating the effectiveness of these approaches on very large datasets.
- What evidence would resolve it: Experimental results showing DPDT's performance on datasets with millions of samples, comparing the original DPDT with versions using caching, parallelism, and Monte Carlo Tree Search, would provide evidence of the effectiveness of these approaches.

### Open Question 2
- Question: How does the choice of the test generating function affect DPDT's performance and interpretability?
- Basis in paper: [explicit] The paper mentions using CART as a test generating function, but also suggests that using a meta-learning approach to learn a neural test generating function could be an interesting direction to further improve accuracy and runtime.
- Why unresolved: The paper only provides experimental results using CART as the test generating function. The impact of different test generating functions on DPDT's performance and interpretability is not explored.
- What evidence would resolve it: Experiments comparing DPDT's performance and interpretability using different test generating functions, such as CART, a learned neural test generating function, and a function that selects the top B most informative splits, would provide insights into the impact of the choice of test generating function.

### Open Question 3
- Question: How does DPDT compare to other state-of-the-art decision tree learning algorithms in terms of interpretability and performance on real-world datasets?
- Basis in paper: [inferred] The paper presents experimental results comparing DPDT to state-of-the-art algorithms like Quant-BnB and CART on various classification datasets. However, the comparison is limited to accuracy and runtime, and does not directly address interpretability.
- Why unresolved: The paper does not provide a comprehensive comparison of DPDT's interpretability to other algorithms. Interpretability is a key aspect of decision trees, and a direct comparison would be valuable.
- What evidence would resolve it: Experiments comparing DPDT's interpretability, as measured by metrics such as average number of tests and number of nodes, to other state-of-the-art decision tree learning algorithms on real-world datasets would provide evidence of DPDT's interpretability performance relative to other methods.

## Limitations

- The interpretability metric based on expected number of tests may not capture all aspects of human interpretability, particularly for trees with similar test counts but different structural properties
- The tests generating function using CART is shown to work well empirically, but the claim that it maintains near-optimal accuracy while reducing the state space significantly requires further validation across diverse data distributions
- The approach may face scalability challenges when applied to very large datasets with millions of samples, though potential improvements are suggested

## Confidence

- **High Confidence**: The MDP formulation and dynamic programming approach are mathematically sound and correctly implemented. The Pareto front computation is technically valid.
- **Medium Confidence**: The empirical results showing competitive accuracy and runtime compared to state-of-the-art optimal algorithms are convincing, though the comparisons could benefit from more extensive ablation studies.
- **Low Confidence**: The generalizability of the tests generating function across all possible data distributions and the completeness of the interpretability metric beyond test count.

## Next Checks

1. **Ablation study on tests generating function**: Compare DPDT performance using exhaustive splits vs. CART-generated splits across datasets with known challenging split patterns to quantify accuracy trade-offs.

2. **Interpretability metric validation**: Design experiments where human evaluators rank tree interpretability, then correlate these rankings with the expected test count metric to assess its validity.

3. **Scalability stress test**: Systematically increase dataset size and dimensionality while monitoring MDP state explosion and runtime to identify practical limits of the approach.