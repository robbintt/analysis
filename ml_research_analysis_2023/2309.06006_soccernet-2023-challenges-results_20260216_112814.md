---
ver: rpa2
title: SoccerNet 2023 Challenges Results
arxiv_id: '2309.06006'
source_url: https://arxiv.org/abs/2309.06006
tags:
- task
- action
- video
- challenge
- spotting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The SoccerNet 2023 challenges featured seven computer vision tasks
  across broadcast video understanding, field understanding, and player understanding.
  New tasks included ball action spotting, dense video captioning, and jersey number
  recognition.
---

# SoccerNet 2023 Challenges Results

## Quick Facts
- arXiv ID: 2309.06006
- Source URL: https://arxiv.org/abs/2309.06006
- Reference count: 40
- Key outcome: Seven computer vision tasks across broadcast video understanding, field understanding, and player understanding with performance improvements over baselines

## Executive Summary
The SoccerNet 2023 challenges featured seven computer vision tasks across three domains: broadcast video understanding (action spotting, ball action spotting, dense video captioning), field understanding (camera calibration), and player understanding (re-identification, multiple object tracking, jersey number recognition). Performance improved over baselines across all tasks, with tight average mAP for action spotting reaching 71.31, mAP@1 for ball action spotting at 86.47, and METEOR@30 for dense video captioning at 26.66. Participants employed advanced techniques including multi-scale feature fusion, transformer architectures, pre-trained video-language models, and ensemble methods to achieve these results.

## Method Summary
The challenges utilized the SoccerNet dataset containing 500+ broadcast games with annotations for actions, ball events, field lines, players, camera parameters, and more. Seven tasks were organized with specific evaluation metrics for each domain. Participants developed various approaches including multi-scale feature fusion architectures, transformer-based models, pre-trained video-language transformers, ensemble methods, and task-specific techniques like keypoint detection and label expansion. The methods ranged from end-to-end solutions to two-stage approaches, with many teams combining multiple models for improved robustness.

## Key Results
- Tight average mAP for action spotting reached 71.31
- Ball action spotting achieved mAP@1 of 86.47
- Dense video captioning attained METEOR@30 of 26.66
- Camera calibration achieved combined metric of 0.55
- Player re-identification reached 93.26% mAP with 96.07% Rank-1 accuracy
- Multi-object tracking attained 75.61% HOTA
- Jersey number recognition reached 92.85% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-scale feature fusion across encoder branches improves action spotting by capturing both fine-grained and holistic video dynamics.
- **Mechanism**: The system divides actions into groups with similar dynamics, assigning each group to a specialized encoder (CNN-based, Transformer-based, or hybrid). A feature pyramid network then combines these multi-scale features, enabling the decoder to make more accurate classifications and regression predictions.
- **Core assumption**: Actions with similar dynamics benefit from similar encoder architectures, and combining multi-scale features enhances detection accuracy.
- **Evidence anchors**:
  - [abstract]: "participants employed advanced techniques such as multi-scale feature fusion, transformer architectures"
  - [section 2.4]: "MEDet divides the whole actions into three groups corresponding to different encoders" and "proposed an improved feature pyramid network to obtain enhanced multi-scale features"
  - [corpus]: Weak - no direct evidence of this specific multi-encoder approach in related papers
- **Break condition**: If action dynamics overlap significantly across groups, the specialization becomes ineffective and may even harm performance through information silos.

### Mechanism 2
- **Claim**: Pre-training on related video tasks and fine-tuning on target tasks transfers useful visual-language representations for dense video captioning.
- **Mechanism**: Pre-trained video-language transformers learn rich multimodal representations from large-scale video-text pairs. These representations capture both visual features and their natural language associations. When fine-tuned on the specific captioning task, the model leverages this transferred knowledge to generate more contextually appropriate captions with accurate temporal anchors.
- **Core assumption**: Visual-language representations learned from general video-text pairs contain transferable information relevant to soccer-specific captioning tasks.
- **Evidence anchors**:
  - [abstract]: "participants employed advanced techniques such as pre-trained video-language models"
  - [section 4.4]: "we modified Blip as our framework" and "compare with other methods our framework performs the best on both CIDEr and Meteor"
  - [corpus]: Weak - related papers focus on game state reconstruction and player identification, not captioning
- **Break condition**: If the pre-training data distribution is too different from soccer broadcasts, the transferred knowledge may introduce noise rather than improve performance.

### Mechanism 3
- **Claim**: Ensemble methods combining multiple model variants improve tracking robustness by reducing individual model biases.
- **Mechanism**: Different tracking models make different types of errors based on their architectural biases. By training multiple variants with different initialization, data augmentation, or architecture choices, and combining their outputs through voting or weighted averaging, the ensemble captures complementary strengths while canceling out individual weaknesses.
- **Core assumption**: Different tracking models have complementary strengths and weaknesses that can be effectively combined.
- **Evidence anchors**:
  - [abstract]: "participants employed advanced techniques such as ensemble methods"
  - [section 7.5]: "Both two-stage and end-to-end tracking methods achieved strong performance" and "participants went beyond simple position prediction methods"
  - [corpus]: Weak - related papers focus on different tasks like game state reconstruction and player identification
- **Break condition**: If models in the ensemble are too similar or make correlated errors, the ensemble provides little benefit and may increase computational cost without accuracy gains.

## Foundational Learning

- **Concept**: Action spotting metrics (Average-mAP)
  - Why needed here: Understanding how performance is evaluated across different temporal tolerances helps in designing models that balance precision and recall
  - Quick check question: What's the difference between tight Average-mAP (1-5s tolerance) and loose Average-mAP (5-60s tolerance) in terms of model requirements?

- **Concept**: Transformer architectures for video understanding
  - Why needed here: Many winning solutions use transformers for temporal modeling, so understanding self-attention and positional encoding is crucial
  - Quick check question: How does a transformer encoder handle variable-length video sequences differently from RNNs?

- **Concept**: Multi-object tracking evaluation (HOTA metric)
  - Why needed here: The tracking challenge uses HOTA, which combines detection and association accuracy, so understanding this metric guides model development
  - Quick check question: What components make up the HOTA metric, and why is it preferred over simpler metrics like MOTA for multi-object tracking?

## Architecture Onboarding

- **Component map**: Input pipeline (video frames → feature extraction → temporal modeling → prediction heads) → post-processing (NMS, filtering) → evaluation
- **Critical path**: Feature extraction → temporal modeling → prediction (for action spotting); Detection → association → re-identification (for tracking)
- **Design tradeoffs**: End-to-end vs two-stage approaches (tracking); single vs multi-encoder architectures (action spotting); pre-training vs training from scratch (captioning)
- **Failure signatures**: Low mAP but high recall indicates overly permissive detection; high detection accuracy but low association accuracy indicates poor track continuity; poor performance on minority action classes indicates class imbalance issues
- **First 3 experiments**:
  1. Implement baseline model (provided) and verify it achieves published baseline performance
  2. Add simple data augmentation (random cropping, color jitter) and measure impact on validation set
  3. Implement multi-scale feature fusion by concatenating features from different temporal resolutions and evaluate improvement over baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of dense video captioning models improve with increased training data volume and diversity?
- Basis in paper: [inferred] The paper mentions that dense video captioning is a novel task with limited participant entries, suggesting potential for improvement with more data and diverse training approaches.
- Why unresolved: The paper does not provide sufficient evidence or analysis on how model performance scales with increased data volume and diversity.
- What evidence would resolve it: Experimental results comparing model performance across different data sizes and diversity levels, along with analysis of model behavior and limitations under varying data conditions.

### Open Question 2
- Question: What are the most effective methods for handling class imbalance in ball action spotting tasks?
- Basis in paper: [explicit] The paper mentions the use of Label Expansion and Focal Loss to address class imbalance in the ball action spotting task.
- Why unresolved: The paper does not provide a comprehensive analysis of the effectiveness of these methods or explore alternative approaches for handling class imbalance.
- What evidence would resolve it: Comparative analysis of different methods for handling class imbalance, including their impact on model performance and generalization.

### Open Question 3
- Question: How can the accuracy of camera calibration be improved by incorporating additional visual cues and context?
- Basis in paper: [inferred] The paper mentions the use of keypoint and line detections, as well as differential rendering, for camera calibration. However, it does not explore the potential benefits of incorporating additional visual cues and context.
- Why unresolved: The paper does not provide a detailed analysis of the impact of additional visual cues and context on camera calibration accuracy.
- What evidence would resolve it: Experimental results comparing camera calibration accuracy with and without the incorporation of additional visual cues and context, along with analysis of the benefits and limitations of each approach.

## Limitations

- Proprietary architectures and hyperparameters of winning solutions remain undisclosed, making exact reproduction challenging
- Many teams employed ensemble methods and extensive fine-tuning strategies that are not fully detailed
- Dataset-specific characteristics and potential biases in the SoccerNet collection could influence generalizability to other soccer leagues or sports

## Confidence

- **High Confidence**: The general trends showing performance improvements over baselines and the viability of the SoccerNet challenges as a benchmark for soccer video understanding
- **Medium Confidence**: The reported metric values, as they come from a structured challenge but lack detailed methodology for verification
- **Medium Confidence**: The described techniques (multi-scale feature fusion, transformers, pre-trained models) as general approaches, though their specific implementations remain unclear

## Next Checks

1. **Implementation Verification**: Replicate baseline models from the SoccerNet repository to verify published baseline performance and establish a solid starting point for method comparison

2. **Component Analysis**: Systematically test the impact of key architectural choices (e.g., multi-scale feature fusion, transformer vs CNN backbones) on a subset of the data to validate their contribution to performance improvements

3. **Cross-Domain Generalization**: Evaluate top-performing models on out-of-distribution soccer videos (different leagues, camera angles) to assess the robustness and generalizability of the approaches beyond the SoccerNet dataset