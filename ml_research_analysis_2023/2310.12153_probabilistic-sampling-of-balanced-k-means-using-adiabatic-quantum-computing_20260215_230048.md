---
ver: rpa2
title: Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing
arxiv_id: '2310.12153'
source_url: https://arxiv.org/abs/2310.12153
tags:
- clustering
- quantum
- problem
- points
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a probabilistic clustering approach based on
  adiabatic quantum computing (AQC) to address the balanced k-means problem. Instead
  of using only the best solution from AQC, the authors utilize all valid measurements
  to compute calibrated confidence scores.
---

# Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing

## Quick Facts
- arXiv ID: 2310.12153
- Source URL: https://arxiv.org/abs/2310.12153
- Reference count: 31
- Primary result: AQC-based probabilistic clustering provides calibrated confidence scores while maintaining competitive clustering accuracy

## Executive Summary
This paper proposes a probabilistic clustering approach based on adiabatic quantum computing (AQC) to address the balanced k-means problem. Instead of using only the best solution from AQC, the authors utilize all valid measurements to compute calibrated confidence scores. The core idea is to formulate the clustering problem as a quadratic unconstrained binary optimization (QUBO) problem and embed it into the quantum-physical system of an AQC. By repeatedly measuring the quantum system, high-probability solutions are sampled according to the Boltzmann distribution. Experiments on synthetic and real data using simulation, exhaustive search, and a D-Wave Advantage 2 prototype AQC demonstrate the effectiveness of the proposed method in finding the set of high-probability solutions and estimating calibrated confidence scores.

## Method Summary
The method formulates balanced k-means as a QUBO problem with constraints encoded via penalty terms. The QUBO is embedded in an AQC Hamiltonian, which evolves from an initial ground state to the problem Hamiltonian. Multiple measurements yield a distribution of solutions following the Boltzmann distribution. A reparameterization approach computes posterior probabilities from sampled energies without requiring exact temperature tuning. The maxset algorithm incrementally builds high-confidence cluster assignments by removing ambiguous points that disagree across solutions.

## Key Results
- The proposed approach achieves competitive clustering accuracy compared to iterative balanced k-means while providing calibrated confidence scores
- Real AQC experiments on D-Wave Advantage 2 prototype with 2 clusters and 8 points per cluster show the method works in practice
- The reparameterization approach successfully compensates for temperature scaling mismatches between theory and hardware

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The AQC samples from a Boltzmann distribution that naturally aligns with the energy-based clustering formulation
- Mechanism: By embedding the k-means objective as a QUBO, the physical Hamiltonian of the AQC encodes clustering costs. The adiabatic evolution ensures the system remains near the ground state, so measurements follow the Boltzmann distribution over low-energy cluster assignments
- Core assumption: The final measurement distribution approximates the theoretical Boltzmann distribution despite hardware imperfections
- Evidence anchors: [abstract] "By repeatedly measuring the quantum system, high-probability solutions are sampled according to the Boltzmann distribution"; [section 3.3] "AQC provides a direct way to sample from a physical system that follows the Boltzmann distribution"
- Break condition: If the annealing time is too short or noise is too high, the system may leave the ground state, breaking the Boltzmann alignment

### Mechanism 2
- Claim: Reparameterization avoids exact temperature tuning by recomputing probabilities from sampled energies
- Mechanism: Instead of requiring precise Hamiltonian scaling to match the theoretical temperature, the method evaluates the energy of all feasible samples and renormalizes to estimate posterior probabilities
- Core assumption: The sampled energies span a sufficient range to allow meaningful probability estimation without precise temperature control
- Evidence anchors: [abstract] "We introduce a reparametrization approach to compute posterior probabilities from the samples, avoiding exact tuning of the AQC sampling temperature"; [section 4.3] "We compensate for these limitations by evaluating the energy of all measured feasible solutions Z′ and recompute p(Z|X) by evaluating the partitioning function over these"
- Break condition: If all feasible solutions have nearly identical energies, the reparameterization cannot distinguish probabilities

### Mechanism 3
- Claim: The maxset algorithm incrementally builds high-confidence cluster assignments by removing ambiguous points
- Mechanism: Starting with the highest-probability solution, it aligns subsequent solutions and removes points that disagree, accumulating posterior probability until a threshold is met
- Core assumption: Higher-probability solutions are more likely to agree on well-assigned points, and disagreements indicate ambiguity
- Evidence anchors: [section 4.4] "We find an assignment Z∗ of a subset of points that solves the clustering problem with increased probability P (Z∗|X)"; [section 5.5] "The results from Algorithm 1 using results from SIM are provided in Figure 6 and show that even for a distribution mismatch the maximum pointsets can provide meaningful results for removing ambiguous samples"
- Break condition: If all solutions are highly similar, the algorithm may remove too few points; if all solutions differ greatly, it may remove too many

## Foundational Learning

- QUBO formulation
  - Why needed here: Clustering constraints (one-hot, balanced) must be encoded as binary variables for the AQC to process
  - Quick check question: How does the penalty method enforce the balanced cluster size constraint without explicit binary constraints?

- Boltzmann sampling theory
  - Why needed here: Understanding why repeated measurements yield a probability distribution over solutions, not just one optimal solution
  - Quick check question: What physical property of the AQC ensures that low-energy solutions are sampled more frequently?

- Energy-based models
  - Why needed here: The clustering objective is cast as an energy function whose minima correspond to good clusterings
  - Quick check question: How does the energy function relate to the posterior probability of a clustering assignment?

## Architecture Onboarding

- Component map: Data → Feature extraction → QUBO encoding → AQC (or SIM) → Measurement samples → Energy evaluation → Probability reparameterization → Maxset extraction → Output
- Critical path: QUBO encoding → AQC execution → Energy evaluation → Probability estimation → Maxset construction
- Design tradeoffs:
  - Measurement count vs. computational cost: More samples improve probability estimates but increase runtime
  - Penalty multiplier λ vs. conditioning: Higher λ enforces constraints better but can worsen optimization landscape
  - Annealing time vs. solution quality: Longer times improve ground state fidelity but increase per-run cost
- Failure signatures:
  - All samples have nearly identical energies → poor temperature calibration or over-constrained problem
  - Maxset removes nearly all points → high solution disagreement, possibly due to incorrect number of clusters
  - Calibration plot deviates from diagonal → mismatched energy-temperature scaling
- First 3 experiments:
  1. Run a small synthetic 2D clustering problem (e.g., 10 points, 2 clusters) on SIM and verify calibration plot
  2. Vary the penalty multiplier λ and observe changes in feasibility and solution quality
  3. Compare maxset output size and accuracy against ground truth for different probability thresholds

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Limited real AQC evaluation: Only 2 real D-Wave runs are reported; most validation relies on simulation
- Temperature calibration challenges: The paper acknowledges temperature scaling mismatches but does not provide systematic validation of the reparameterization approach's robustness
- Scalability concerns: The method requires evaluating energies of all feasible samples, which may become computationally prohibitive for larger problems

## Confidence
- High confidence: The QUBO formulation of balanced k-means is sound and well-established
- Medium confidence: The theoretical framework for reparameterization and maxset extraction is logically consistent
- Low confidence: Practical effectiveness on real AQC hardware, given limited experimental validation

## Next Checks
1. **Distribution Fidelity Test**: Run controlled experiments on SIM varying annealing parameters to quantify how closely measured distributions match theoretical Boltzmann distributions
2. **Scalability Benchmark**: Evaluate computational cost of the maxset algorithm on problems where the number of feasible solutions grows exponentially
3. **Hardware Robustness**: Compare SIM results against multiple independent runs on real AQC hardware to assess consistency and identify noise-related degradation patterns