---
ver: rpa2
title: 'TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression For
  On-device ASR Models'
arxiv_id: '2309.01947'
source_url: https://arxiv.org/abs/2309.01947
tags:
- supernet
- training
- rnn-t
- size
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently training multiple
  on-device ASR models optimized for various hardware constraints. The core method,
  TODM (Train Once Deploy Many), leverages a Supernet approach where Recurrent Neural
  Network Transducer (RNN-T) models share weights within a Supernet.
---

# TODM: Train Once Deploy Many Efficient Supernet-Based RNN-T Compression For On-device ASR Models

## Quick Facts
- arXiv ID: 2309.01947
- Source URL: https://arxiv.org/abs/2309.01947
- Reference count: 0
- One-line primary result: TODM Supernet matches or surpasses manually tuned models by up to 3% relative WER improvement while training multiple hardware-optimized models efficiently.

## Executive Summary
This paper addresses the challenge of efficiently training multiple on-device ASR models optimized for various hardware constraints. The core method, TODM (Train Once Deploy Many), leverages a Supernet approach where RNN-T models share weights within a Supernet. It reduces layer sizes and widths to obtain subnetworks, making them smaller models suitable for all hardware types. The authors introduce a novel combination of three techniques to improve the outcomes of the TODM Supernet: adaptive dropouts, an in-place Alpha-divergence knowledge distillation, and the use of ScaledAdam optimizer. The primary result is that TODM Supernet either matches or surpasses the performance of manually tuned models by up to a relative 3% better in word error rate (WER), while efficiently keeping the cost of training many models at a small constant.

## Method Summary
TODM (Train Once Deploy Many) leverages a Supernet approach to efficiently train and deploy multiple on-device ASR models optimized for various hardware constraints. The Supernet, a weight-sharing neural network graph, contains the full RNN-T model and can generate smaller subnetworks tailored for specific applications by reducing layers or channel widths. The method introduces three key techniques: adaptive dropouts that adjust regularization based on subnetwork width, in-place Alpha-divergence knowledge distillation where the full Supernet acts as a teacher, and the ScaledAdam optimizer for improved training stability. An evolutionary search is performed on a validation set to discover Pareto-optimal subnetworks that balance model size and accuracy.

## Key Results
- TODM Supernet achieves WER performance matching or exceeding manually tuned models by up to 3% relative improvement.
- The method efficiently trains multiple hardware-optimized subnetworks with a single Supernet, reducing training cost to a small constant.
- Adaptive dropout, in-place Alpha-divergence knowledge distillation, and ScaledAdam optimizer contribute to improved Supernet training outcomes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supernet training enables a single model to generate multiple hardware-optimized subnetworks efficiently.
- Mechanism: A shared weight network (Supernet) is trained with multiple subnetwork configurations. Each subnetwork is sampled during training, and gradients are backpropagated through the shared parameters, allowing all subnetworks to improve simultaneously.
- Core assumption: The subnetwork architectures are structurally compatible enough that shared weights remain meaningful across all sampled sizes.
- Evidence anchors:
  - [abstract] "TODM leverages insights from prior work on Supernet, where Recurrent Neural Network Transducer (RNN-T) models share weights within a Supernet."
  - [section] "A Supernet is a weight-sharing neural network graph that can generate smaller subnetworks tailored for specific applications."
  - [corpus] Weak: corpus does not directly address weight-sharing Supernet benefits.
- Break condition: If subnetwork architectures differ too drastically, shared weights become ineffective and performance degrades.

### Mechanism 2
- Claim: Adaptive dropout improves Supernet training by reducing regularization in narrower subnetwork layers.
- Mechanism: Dropout rate is scaled inversely with channel width in FFN layers: dropoutci = dropoutcmi × ci/mi. Narrower layers receive less dropout, preserving signal strength and gradient quality.
- Core assumption: Narrower layers need less regularization because they have fewer parameters and are less prone to overfitting.
- Evidence anchors:
  - [section] "We adjust the magnitude of dropouts in the FFN layers... Intuitively, a module with reduced dimensionality requires less regularization..."
  - [corpus] Weak: no direct corpus evidence for adaptive dropout in Supernet training.
- Break condition: If dropout scaling is too aggressive, it may lead to overfitting in narrow subnetwork variants.

### Mechanism 3
- Claim: Sampled in-place knowledge distillation accelerates convergence and improves accuracy of smaller subnetwork variants.
- Mechanism: During training, the full Supernet acts as a teacher, distilling its output distribution into each sampled subnetwork using KL or Alpha-divergence. Only top-j logits are sampled to reduce memory overhead.
- Core assumption: Smaller subnetworks benefit from learning the teacher's output distribution, improving their representational capacity.
- Evidence anchors:
  - [section] "we use the entire Supernet as the 'teacher' model, and force in-place distillation of the max-network’s output probability distributions onto the sampled subnetwork..."
  - [section] "KD can help the supernet converge faster... model G120epoch with KD already exceeds the baseline model C120epoch’s WER by 10.3%, 4.2%, and 2.4% relative in its largest, smallest, and fixed-size networks, respectively."
  - [corpus] Weak: corpus lacks direct evidence of KD effectiveness in Supernet training.
- Break condition: If teacher model quality is poor or distillation is too aggressive, it may mislead smaller subnetworks.

## Foundational Learning

- Concept: Weight sharing in neural networks
  - Why needed here: Supernet relies on all subnetworks sharing the same base weights to reduce training cost.
  - Quick check question: What happens if subnetwork architectures are too different for shared weights to remain effective?

- Concept: Knowledge distillation and probability divergence
  - Why needed here: The in-place KD mechanism uses KL or Alpha-divergence to transfer teacher knowledge to student subnetworks.
  - Quick check question: How does Alpha-divergence differ from KL-divergence in handling teacher uncertainty?

- Concept: Adaptive regularization
  - Why needed here: Adaptive dropout adjusts regularization strength based on subnetwork width to prevent over- or under-regularization.
  - Quick check question: Why would a narrower layer require less dropout than a wider one?

## Architecture Onboarding

- Component map:
  Supernet -> Subnetworks -> Adaptive Dropout -> In-place KD -> ScaledAdam -> Pareto-optimal subnetworks

- Critical path:
  1. Define search space (layer/channel reduction constraints).
  2. Train Supernet with sandwich sampling and adaptive dropout.
  3. Apply in-place KD during training.
  4. Use evolutionary search to find Pareto-optimal subnetworks.
  5. Fine-tune with ScaledAdam if needed.

- Design tradeoffs:
  - Larger search space improves model quality but increases training complexity.
  - More aggressive KD sampling reduces memory but may lose useful uncertainty signals.
  - Adaptive dropout helps but requires careful tuning to avoid under-regularization.

- Failure signatures:
  - Subnetworks underperform baseline models: likely shared weights are ineffective or search space too restrictive.
  - Training instability: could be due to inappropriate dropout scaling or KD configuration.
  - Memory errors: may result from sampling too many logits during KD.

- First 3 experiments:
  1. Train a basic Supernet with channel reduction only; evaluate subnetwork accuracy vs. size.
  2. Add adaptive dropout; measure impact on convergence speed and final accuracy.
  3. Introduce in-place KD with KL divergence; compare performance to non-KD baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Supernet-trained models compare to individually tuned models when scaled to larger datasets or more complex ASR architectures?
- Basis in paper: [explicit] The paper validates TODM using LibriSpeech and MH-SSM RNN-T, but does not explore larger datasets or different ASR architectures.
- Why unresolved: The experiments are limited to a specific dataset and model architecture, leaving uncertainty about generalizability.
- What evidence would resolve it: Testing TODM on larger datasets (e.g., LibriVox) and diverse ASR architectures (e.g., transformer-based models) to compare performance.

### Open Question 2
- Question: What is the impact of using different knowledge distillation methods (e.g., KL divergence vs. Alpha-divergence) on the final model performance, and how does this vary with different hardware constraints?
- Basis in paper: [explicit] The paper experiments with KL divergence and Alpha-divergence for knowledge distillation but does not deeply analyze their impact under varying hardware constraints.
- Why unresolved: The paper provides limited comparative analysis of distillation methods across different hardware constraints.
- What evidence would resolve it: Conducting experiments with various distillation methods under different hardware constraints to quantify their impact on performance.

### Open Question 3
- Question: How does the adaptive dropout mechanism affect the training stability and final performance of Supernet models, and can it be optimized further?
- Basis in paper: [explicit] The paper introduces adaptive dropout as a novel technique but does not extensively explore its optimization or impact on training stability.
- Why unresolved: The paper presents adaptive dropout but lacks a detailed analysis of its optimization and effects on training dynamics.
- What evidence would resolve it: Performing ablation studies and sensitivity analyses on dropout rates and their effects on training stability and model performance.

## Limitations

- Limited ablation studies: The paper demonstrates that TODM improves upon baseline models but lacks detailed ablation studies to isolate the individual contributions of adaptive dropout, in-place knowledge distillation, and the ScaledAdam optimizer.
- Architecture specificity: TODM is evaluated exclusively on RNN-T models for speech recognition. The generalizability of the approach to other model architectures (e.g., Transformer-based ASR models or other sequence-to-sequence tasks) remains unclear.
- Resource overhead validation: While the paper claims efficiency gains, it does not provide detailed analysis of training time or memory overhead for Supernet training compared to training individual models separately.

## Confidence

- High confidence in the core claim that Supernet training enables efficient generation of multiple hardware-optimized subnetworks, supported by direct experimental evidence showing WER improvements over manually tuned baselines.
- Medium confidence in the effectiveness of adaptive dropout and in-place knowledge distillation mechanisms. While the paper presents theoretical justification and some empirical support, the lack of detailed ablation studies and direct comparison to alternative regularization/KD methods reduces confidence.
- Low confidence in the scalability and generalizability claims. The paper does not provide evidence for performance on larger datasets, more diverse hardware targets, or non-RNN-T architectures.

## Next Checks

1. **Ablation study**: Train TODM Supernet variants with only adaptive dropout, only in-place KD, and neither component, comparing WER and training efficiency against the full TODM approach to isolate each mechanism's contribution.

2. **Architecture transfer**: Apply TODM to a non-RNN-T ASR architecture (e.g., Conformer-Transducer) and evaluate whether the same performance improvements and efficiency gains are observed.

3. **Memory profiling**: Measure and compare peak GPU memory usage and training time per epoch for TODM Supernet training versus training an equivalent number of individually optimized models to empirically validate the claimed efficiency benefits.