---
ver: rpa2
title: 'Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural
  Networks with Polynomial Width, Samples, and Time'
arxiv_id: '2306.16361'
source_url: https://arxiv.org/abs/2306.16361
tags:
- lemma
- logd
- have
- then
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proves a sample complexity separation between neural\
  \ networks trained with gradient descent and kernel methods (specifically NTK).\
  \ The authors analyze projected gradient flow on two-layer neural networks with\
  \ polynomial activations and show that with polynomial samples (n = O(d^3.1)), the\
  \ network can converge to an error that cannot be achieved by any inner-product\
  \ kernel method using significantly more samples (n \u226A d^4)."
---

# Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time

## Quick Facts
- arXiv ID: 2306.16361
- Source URL: https://arxiv.org/abs/2306.16361
- Reference count: 40
- This paper proves a sample complexity separation between neural networks trained with gradient descent and kernel methods (specifically NTK).

## Executive Summary
This paper demonstrates that neural networks trained with vanilla gradient descent can achieve better sample complexity than kernel methods by simultaneously learning both the feature direction and link function of single-index models. Using a mean-field analysis of two-layer networks with polynomial activations, the authors show that with polynomial samples (n = O(d^3.1)), the network can converge to an error that cannot be achieved by any inner-product kernel method using significantly more samples (n ≪ d^4). The key insight is that the network stays away from saddle points through a carefully designed potential function, allowing it to maintain sufficient variance in the weight distribution for fast convergence.

## Method Summary
The paper analyzes projected gradient flow on two-layer neural networks with polynomial activations using mean-field theory. The method involves tracking the evolution of the distribution of first-layer weights on the sphere Sd-1, showing that these weights converge to a distribution correlated with but not exactly equal to the true feature direction q*. The analysis proceeds through three phases: an initial burn-in phase where the signal grows, a power method-like phase where the network accelerates toward the target direction, and a saddle-point avoidance phase where a potential function ensures sufficient variance. The coupling between finite-width empirical dynamics and infinite-width population dynamics is carefully bounded to show that the sample complexity advantage over NTK methods is maintained.

## Key Results
- Neural networks can achieve sample complexity n = O(d^3.1) while kernel methods require n ≫ d^4
- The network learns both the feature direction q* and the link function h simultaneously without two-stage algorithms
- A potential function Φ(w) = log(w/√(1-w²)) ensures the network stays away from saddle points
- The coupling error between finite-width and infinite-width dynamics grows at most as fast as the signal

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The network stays away from bad saddle points by leveraging a potential function Φ(w) that increases in Phase 3, Case 1, ensuring the average velocity remains large enough for fast convergence.
- Mechanism: The paper defines Φ(w) = log(w/√(1-w²)) and shows that its absolute difference between any two particles |Φ(w) - Φ(w')| is increasing during Phase 3, Case 1 when D4,t ≤ 0. This prevents particles from clustering around the root r of the velocity function, ensuring sufficient variance for large average velocity.
- Core assumption: The rotational invariance and symmetry properties of the population dynamics are maintained throughout training.
- Evidence anchors:
  - [abstract]: "Our main novelty of our population dynamics analysis is designing a potential function that shows that the iterate stays away from saddle points"
  - [section]: "We define a potential function Φ(w) := log( w√(1−w²) ), and we show that |Φ(w) − Φ(w')| is always increasing for any two particles w,w' with the same sign"
  - [corpus]: Weak - The corpus neighbors focus on general gradient descent analysis but don't specifically address saddle point avoidance mechanisms
- Break condition: If the rotational invariance/symmetry properties break down, the potential function argument fails and particles could concentrate around saddle points

### Mechanism 2
- Claim: The coupling error between finite-width empirical dynamics and infinite-width population dynamics grows at most as fast as the signal during Phase 1, enabling sample complexity better than NTK.
- Mechanism: During Phase 1, the growth rate of the coupling error ∥δt∥ is bounded by comparing it to the growth rate of the signal component q*ᵀu. The paper shows that the coupling error grows at most with rate 4σ₂²,d|D2,t|∥δt∥², which matches the signal growth rate, preventing polynomial blow-up.
- Core assumption: The signal component q*ᵀu grows at rate 4σ₂²,d|D2,t| during Phase 1.
- Evidence anchors:
  - [abstract]: "We address this challenge by using a direct and sharp comparison between the growth of the coupling error and the growth of the signal"
  - [section]: "the growth rate of the coupling error due to the growth in the signal is also at most 4σ₂²,d|D2,t|w"
  - [corpus]: Weak - Corpus papers discuss coupling analysis but don't specifically address the signal-coupling error comparison
- Break condition: If the coupling error grows faster than the signal (e.g., due to additional polynomial factors), the sample complexity would degrade to NTK levels

### Mechanism 3
- Claim: The network can learn both the feature direction q* and the link function h simultaneously without two-stage algorithms by maintaining a distribution of neurons that correlates with but is not exactly equal to q*.
- Mechanism: The analysis shows that the first layer weights converge to a distribution where the projection q*ᵀu follows a distribution that can represent the target function through its mixture, even without biases or trainable second-layer weights.
- Core assumption: The activation function σ and link function h satisfy the polynomial degree constraints (quartic) and coefficient relationships in Assumption 3.2.
- Evidence anchors:
  - [abstract]: "we show that in this setting, the first layer weights will converge to a distribution of neurons that are correlated with but not exactly equal to q*"
  - [section]: "the first layer weights are all unit vectors and the second-layer weights are fixed and all the same"
  - [corpus]: Moderate - Several corpus papers discuss learning single-index models but typically use two-stage algorithms or single-neuron training
- Break condition: If the activation function doesn't satisfy the polynomial constraints or coefficient relationships, the network cannot represent the target function through its neuron distribution

## Foundational Learning

- Concept: Mean-field analysis of neural network dynamics
  - Why needed here: The paper uses mean-field theory to approximate the evolution of the weight distribution as a continuous PDE, enabling analysis of infinite-width networks that informs finite-width behavior
  - Quick check question: Can you explain how the mean-field approximation transforms the discrete weight updates into a PDE for the weight distribution?

- Concept: Spherical harmonics and Legendre polynomials
  - Why needed here: These provide the mathematical framework for analyzing functions on the sphere Sd-1, which is the data distribution, and enable decomposition of the target and activation functions
  - Quick check question: How do Legendre polynomials form an orthonormal basis for functions on the sphere, and why is this useful for analyzing neural network training?

- Concept: Gradient flow vs gradient descent with finite learning rate
  - Why needed here: The paper analyzes projected gradient flow (infinitesimal learning rate) but extends results to gradient descent with polynomial iterations, requiring understanding of the relationship between these optimization methods
  - Quick check question: What's the key difference between analyzing projected gradient flow versus projected gradient descent with finite learning rate in terms of convergence guarantees?

## Architecture Onboarding

- Component map:
  - Data distribution (uniform on sphere Sd-1) -> Target function (single-index model y(x) = h(q*ᵀx)) -> Two-layer network (unit-norm first-layer weights, fixed second-layer weights) -> Projected gradient flow (population dynamics) -> Mean-field approximation (PDE for weight distribution) -> Coupling error bounds (finite-width analysis)

- Critical path: Initialize uniform distribution -> Maintain rotational invariance/symmetry -> Phase 1 signal growth -> Phase 2 acceleration -> Phase 3 saddle point avoidance -> Coupling error control -> Finite-width convergence

- Design tradeoffs: Quartic activation vs ReLU (simpler analysis vs practical relevance), uniform second-layer weights vs trainable (easier analysis vs better expressivity), mean-field analysis vs direct finite-width analysis (tractability vs precision)

- Failure signatures: Rotational invariance breaks (particles cluster), coupling error grows polynomially (sample complexity degrades), saddle points trap dynamics (convergence fails), polynomial degree constraints violated (representation fails)

- First 3 experiments:
  1. Verify rotational invariance/symmetry maintenance under gradient flow for simple target functions
  2. Test coupling error growth rates during Phase 1 for various activation functions
  3. Validate potential function Φ(w) behavior in avoiding saddle points for different polynomial activations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sample complexity separation between gradient descent and NTK be achieved for ReLU activations?
- Basis in paper: The authors note that generalizing results to ReLU activation is an open direction.
- Why unresolved: The current analysis relies on quartic polynomial activations with specific Legendre coefficient properties that may not hold for ReLU.
- What evidence would resolve it: A proof showing sample complexity bounds for ReLU activations that match or exceed the d^3.1 bound achieved for quartic polynomials.

### Open Question 2
- Question: Can gradient descent achieve less than d^3 sample complexity in this setting?
- Basis in paper: The authors state that whether gradient descent can achieve less than d^3 sample complexity is an open question.
- Why unresolved: The current analysis requires ǫ ≥ poly(1/log log d), which may be too restrictive.
- What evidence would resolve it: A proof showing that gradient descent can achieve arbitrarily small generalization error ǫ with fewer than d^3 samples.

### Open Question 3
- Question: Can two-layer neural networks be shown to attain arbitrarily small generalization error ǫ?
- Basis in paper: The authors note that a limitation of their analysis is that it requires ǫ ≥ poly(1/log log d).
- Why unresolved: The current analysis cannot prove convergence to arbitrarily small error due to technical constraints.
- What evidence would resolve it: A proof showing that two-layer neural networks trained with gradient descent can achieve generalization error ǫ for any desired ǫ > 0.

## Limitations

- The analysis critically depends on maintaining rotational invariance/symmetry properties throughout training, which could break down in practice
- The assumption of uniform second-layer weights (vs trainable weights) represents a significant architectural simplification
- The quartic polynomial constraints on activation functions and link functions are quite restrictive and may not generalize well to practical scenarios

## Confidence

- High confidence in the theoretical framework and mathematical rigor of the mean-field analysis
- Medium confidence in the practical implications of the sample complexity separation, given the restrictive assumptions
- Low confidence in direct applicability to deep networks or non-polynomial activations without significant modifications

## Next Checks

1. Test the rotational invariance/symmetry maintenance empirically for different activation functions and target functions to validate the core assumption of the analysis
2. Compare the coupling error growth rates between the theoretical bounds and empirical observations during Phase 1 for various network widths and sample sizes
3. Verify the saddle point avoidance mechanism by tracking the potential function Φ(w) during training for different initializations and polynomial activations