---
ver: rpa2
title: Test-time augmentation-based active learning and self-training for label-efficient
  segmentation
arxiv_id: '2308.10727'
source_url: https://arxiv.org/abs/2308.10727
tags:
- segmentation
- data
- cases
- scans
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a novel test-time augmentation-based method
  for combining self-training and active learning in medical image segmentation tasks,
  specifically fetal body and placenta segmentation from MRI. The method aims to reduce
  annotation burden by using test-time augmentations to estimate segmentation quality
  and select cases for annotation while generating pseudo-labels for self-training.
---

# Test-time augmentation-based active learning and self-training for label-efficient segmentation

## Quick Facts
- arXiv ID: 2308.10727
- Source URL: https://arxiv.org/abs/2308.10727
- Reference count: 28
- Primary result: Achieved Dice score of 0.961 for fetal body segmentation using only 6 original sequence scans and 2 new sequence scans, approaching performance of 30 fully annotated cases.

## Executive Summary
This paper presents a novel method combining test-time augmentation (TTA), active learning (AL), and self-training (ST) for label-efficient medical image segmentation. The approach uses TTA to estimate segmentation quality and select cases for annotation while generating pseudo-labels for self-training. Tested on fetal body and placenta segmentation from MRI, the method significantly reduces annotation burden while maintaining high segmentation accuracy. Self-training alone showed substantial improvements for both in-distribution and out-of-distribution data, while active learning proved particularly effective for high-variability placenta data but showed mixed results for single-sequence body data.

## Method Summary
The method involves training an initial teacher network on a small annotated dataset, then using TTA to generate multiple augmented predictions. Quality estimation is performed by computing Dice scores between median predictions and individual augmentation results. Cases with lowest estimated Dice scores are selected for annotation, while high-quality cases are used to generate soft pseudo-labels. A student network is then trained on the combination of original annotations, actively selected cases, and pseudo-labeled data. The process can iterate, with the student becoming the new teacher for subsequent rounds.

## Key Results
- Self-training alone significantly improved performance for both in-distribution and out-of-distribution data
- Combined AL+ST achieved Dice score of 0.961 for fetal body segmentation using only 8 annotated cases
- Active learning showed significant benefits for high-variability placenta data but not for low-variability single-sequence body data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Test-time augmentations (TTA) provide reliable uncertainty estimation for selecting low-quality pseudo-labels and cases for annotation.
- Mechanism: By generating multiple augmented versions of each test image and computing agreement (Dice overlap) between predictions, TTA identifies regions where the model is uncertain or likely to err. Cases with lowest estimated Dice scores are prioritized for annotation, while high-score cases are used as pseudo-labels.
- Core assumption: Prediction disagreement across augmentations correlates with actual segmentation quality and annotation need.
- Evidence anchors:
  - [abstract] "TTA is performed on an initial teacher network. Then, cases for annotation are selected based on the lowest estimated Dice score."
  - [section] "The quality estimation is obtained by computing the mean Dice score between the median prediction and each of the augmentation results"
  - [corpus] Weak evidence - corpus contains related works on uncertainty estimation but no direct validation of TTA-based Dice scoring specifically.
- Break condition: If augmentation transformations don't capture the true variability in the data, or if the network becomes overconfident across all augmentations.

### Mechanism 2
- Claim: Self-training with soft pseudo-labels improves segmentation performance for both in-distribution and out-of-distribution data.
- Mechanism: The teacher network generates probabilistic predictions (soft labels) rather than hard binary masks. These soft labels capture uncertainty and are used to train the student network, allowing it to learn from both correct predictions and boundary regions where the teacher is uncertain.
- Core assumption: Soft labels contain more information than hard labels and help the student generalize better.
- Evidence anchors:
  - [abstract] "The method includes training a teacher network, generating pseudo-labels using TTA-based quality estimation"
  - [section] "The fully automatic ST iteration is optionally performed before the combined AL and ST module iteration. It uses soft teacher predictions instead of hard teacher predictions"
  - [corpus] Weak evidence - corpus contains semi-supervised learning papers but lacks specific validation of soft label effectiveness in medical segmentation.
- Break condition: If pseudo-labels contain too many errors, causing the student to learn incorrect patterns.

### Mechanism 3
- Claim: Border slices annotation reduces background segmentation errors while maintaining low annotation cost.
- Mechanism: Annotators only mark the uppermost and lowermost slices containing the structure of interest, creating tight bounds that constrain the segmentation in regions where background confusion is highest.
- Core assumption: Boundary slices are sufficient to guide segmentation accuracy without requiring full volume annotation.
- Evidence anchors:
  - [section] "To decrease errors in the background slices outside of the structure of interest, the uppermost and lowermost slices of the ROI that includes the structure of interest are manually selected by the annotator"
  - [corpus] Weak evidence - corpus contains papers on partial annotations but no specific validation of border slices effectiveness.
- Break condition: If the structure extends beyond the annotated border slices or if the anatomy requires full volume context for accurate segmentation.

## Foundational Learning

- Concept: Test-time augmentation and uncertainty estimation
  - Why needed here: TTA forms the core quality estimation mechanism for both active learning and self-training components
  - Quick check question: How does computing median Dice between augmented predictions and the base prediction identify unreliable segmentations?

- Concept: Semi-supervised learning with soft labels
  - Why needed here: Soft pseudo-labels capture uncertainty and provide richer training signals than hard binary masks
  - Quick check question: Why might soft teacher predictions outperform hard predictions in self-training?

- Concept: Active learning selection criteria
  - Why needed here: Determines which cases to prioritize for expensive manual annotation
  - Quick check question: What makes TTA-based Dice estimation preferable to other uncertainty measures like entropy or variance?

## Architecture Onboarding

- Component map: Teacher → TTA → Quality Estimation → AL/ST Selection → Student Training
- Critical path: Teacher network → TTA inference module → Quality estimation module → Active learning selector → Self-training module → Student network
- Design tradeoffs:
  - TTA vs. dropout uncertainty: TTA provides more reliable uncertainty but requires more computation
  - Soft vs. hard pseudo-labels: Soft labels capture uncertainty but may require temperature scaling
  - Border slices vs. full annotation: Border slices reduce cost but may miss complex boundary regions
- Failure signatures:
  - Poor performance despite many iterations: Likely issue with pseudo-label quality or TTA transformations
  - High variance across runs: May indicate unstable active learning selection or insufficient data diversity
  - Slow convergence: Could suggest suboptimal learning rate scheduling or inadequate augmentation
- First 3 experiments:
  1. Validate TTA quality estimation by comparing estimated Dice scores against ground truth on a validation set
  2. Test soft vs. hard pseudo-label performance on a simple segmentation task with limited data
  3. Evaluate border slices effectiveness by comparing full vs. partial annotation approaches on boundary-heavy structures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific data variability conditions does combining self-training with active learning provide the most benefit compared to using either method alone?
- Basis in paper: [explicit] The authors found that combining AL with ST improved results for low-variability fetal body data but slightly hurt performance for high-variability placenta data, indicating different effectiveness across data types
- Why unresolved: The paper only tested two specific medical imaging tasks with different variability characteristics, leaving unclear how the method would perform across a broader range of data variability scenarios
- What evidence would resolve it: Systematic testing across multiple datasets with varying degrees of variability (low, medium, high) to establish clear thresholds where combining methods becomes beneficial

### Open Question 2
- Question: How does the number of TTA variations impact the quality estimation accuracy and overall segmentation performance in different medical imaging tasks?
- Basis in paper: [explicit] The authors used 16 TTA variations but did not explore how different numbers of augmentations affect performance or quality estimation
- Why unresolved: The optimal number of TTA variations likely depends on the specific task and data characteristics, but this relationship was not investigated
- What evidence would resolve it: Systematic experiments varying the number of TTA variations (e.g., 4, 8, 16, 32) across different segmentation tasks to identify performance trends

### Open Question 3
- Question: What is the optimal strategy for incorporating border slice annotations in the quality estimation process to maximize segmentation accuracy while minimizing annotation burden?
- Basis in paper: [explicit] The authors used border slices for correction and quality estimation but did not systematically evaluate different strategies for their incorporation
- Why unresolved: The current approach uses border slices but the impact of different annotation strategies (e.g., more slices, different regions, different weighting) remains unexplored
- What evidence would resolve it: Comparative studies testing different border annotation strategies and their impact on final segmentation quality and annotation efficiency

## Limitations

- Core TTA-based quality estimation mechanism lacks empirical validation against ground truth segmentation quality
- Network architecture details are incomplete, making exact reproduction challenging
- Mixed effectiveness of active learning across different data variability scenarios suggests method may not generalize universally

## Confidence

- High confidence in self-training effectiveness with soft labels (validated by significant performance improvements)
- Medium confidence in TTA-based active learning selection (mixed results across datasets)
- Low confidence in generalization to other medical imaging tasks without further validation

## Next Checks

1. Conduct ablation study comparing TTA-based quality estimation against established uncertainty metrics (entropy, variance) on the same datasets
2. Evaluate the impact of different pseudo-label quality thresholds on student model performance across multiple random seeds
3. Test the method's transferability to a different anatomical structure (e.g., cardiac segmentation) with varying image characteristics