---
ver: rpa2
title: 'FastPillars: A Deployment-friendly Pillar-based 3D Detector'
arxiv_id: '2302.02367'
source_url: https://arxiv.org/abs/2302.02367
tags:
- point
- detection
- object
- pillar
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FastPillars introduces a deployment-friendly pillar-based 3D detector
  for autonomous driving, eliminating the need for sparse convolutions that hinder
  on-device deployment. The core innovation is the Max-and-Attention Pillar Encoding
  (MAPE) module, which enhances small object detection by combining max-pooling with
  attention-based feature weighting to preserve local geometric patterns.
---

# FastPillars: A Deployment-friendly Pillar-based 3D Detector

## Quick Facts
- arXiv ID: 2302.02367
- Source URL: https://arxiv.org/abs/2302.02367
- Authors: Not specified in source
- Reference count: 40
- Key outcome: FastPillars achieves 64.6 mAP and 70.1 NDS on nuScenes with 24 FPS on RTX3070Ti, outperforming state-of-the-art pillar-based methods while enabling seamless TensorRT deployment.

## Executive Summary
FastPillars is a pillar-based 3D object detector designed for autonomous driving that eliminates the need for sparse convolutions, enabling efficient on-device deployment. The method introduces a Max-and-Attention Pillar Encoding (MAPE) module that preserves local geometric patterns crucial for small object detection by combining max-pooling with attention-based feature weighting. A compact CRVNet backbone integrates Cross-Stage-Partial and RepVGG-style structures for efficient feature extraction. Extensive experiments on nuScenes demonstrate superior performance (64.6 mAP, 70.1 NDS) while maintaining real-time speed (24 FPS) on a single RTX3070Ti GPU.

## Method Summary
FastPillars converts irregular 3D LiDAR point clouds into structured 2D pseudo-images through pillarization, then applies standard 2D convolutions for efficient processing. The MAPE module encodes point features within each pillar using point-wise MLPs, max-pooling for strongest responses, and attention-pooling for weighted local details, which are then combined. The CRVNet backbone uses RepVGG-style reparameterization, training with multi-branch blocks that convert to single efficient convolutions at inference. The architecture avoids sparse convolutions entirely, enabling native TensorRT support and quantization for deployment on resource-constrained platforms.

## Key Results
- Achieves 64.6 mAP and 70.1 NDS on nuScenes validation set
- Runs at 24 FPS on a single RTX3070Ti GPU
- Outperforms CenterPoint and PillarNet while maintaining real-time speed
- Enables seamless deployment via TensorRT and quantization without custom plugins

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAPE improves small object detection by preserving local geometric patterns that max-pooling alone would discard.
- Mechanism: Combines max-pooling (strongest response) with attention-pooling (weighted local detail), then averages them so both global prominence and fine-grained structure contribute to pillar representation.
- Core assumption: Local geometric patterns contain discriminative information for small objects that is lost when only maximum response is retained.
- Evidence anchors: Abstract states MAPE enhances small object detection by combining max-pooling with attention-based feature weighting to preserve local geometric patterns. Section 3.1 notes max-pooling results in loss of fine-grained information crucial for pillar-based objects, especially small objects.

### Mechanism 2
- Claim: CRVNet delivers fast inference by using reparameterized single-path plain architecture efficient on GPUs.
- Mechanism: RepVGG-style blocks train with three branches (conv3x3, conv1x1, identity) then convert to single 3x3 conv at inference, eliminating BN and ReLU overhead while retaining representational power.
- Core assumption: Modern GPUs execute plain 3x3 convolutions far more efficiently than residual or multi-branch structures, and reparameterization doesn't degrade accuracy.
- Evidence anchors: Abstract mentions integrating CSP and RepVGG-style structures for efficient feature extraction. Section 3.2 describes replacing plain Conv-BN-ReLU with over-parameterized three-branch counterpart during training, converting three branches identically into one at inference for improved efficiency.

### Mechanism 3
- Claim: Avoiding sparse convolutions enables seamless TensorRT deployment and quantization, accelerating inference and reducing memory.
- Mechanism: Standard 2D convolutions are natively supported by TensorRT and can be quantized to FP16/INT8, whereas SPConv requires custom plugins limiting shape flexibility and compatibility.
- Core assumption: TensorRT and quantization frameworks provide measurable speedups only for native ops; custom SPConv plugins negate these benefits.
- Evidence anchors: Abstract states eliminating need for sparse convolutions that hinder on-device deployment. Introduction notes sparse convolution poses challenge when converted to ONNX/TensorRT for deployment and hampers further speedup through these techniques.

## Foundational Learning

- Concept: Point cloud voxelization and pillarization
  - Why needed here: Converts irregular 3D points into structured 2D pseudo-image (pillars) so 2D CNNs can be applied efficiently.
  - Quick check question: What is the difference between voxelization and pillarization in terms of dimensionality reduction?

- Concept: Attention mechanisms in feature aggregation
  - Why needed here: MAPE uses learned attention scores to weight point features within a pillar, preserving local detail that max-pooling would lose.
  - Quick check question: How does attention-based pooling differ from max-pooling in terms of information retention?

- Concept: Reparameterization in neural network design
  - Why needed here: CRVNet uses RepVGG-style reparameterization to train with multi-branch blocks but run with single efficient conv, balancing speed and accuracy.
  - Quick check question: What is the purpose of reparameterizing a multi-branch block into a single conv at inference?

## Architecture Onboarding

- Component map: Pillarization -> MAPE (Point/Max/Atten encoding) -> CRVNet backbone -> Neck (8×/16× fusion) -> Center-based heads (cls, reg, IoU) -> NMS
- Critical path: Pillar encoding -> backbone feature extraction -> neck fusion -> box regression; each stage must complete within latency budget to meet 24 FPS.
- Design tradeoffs: Single-path plain convs for speed vs. residual blocks for accuracy; attention pooling for detail vs. max-pooling for robustness; model size (FastPillars-s vs. m) vs. latency.
- Failure signatures: Degraded small-object mAP -> MAPE ineffective or under-trained; FPS drop -> SPConv leakage or inefficient backbone; TensorRT quantization errors -> unsupported op or precision mismatch.
- First 3 experiments:
  1. Replace MAPE with plain max-pooling and measure mAP/NDS delta on nuScenes val.
  2. Swap CRVNet for standard ResNet-34 and profile FPS/memory on RTX3070Ti.
  3. Convert model to TensorRT FP16 and compare latency vs. PyTorch FP32.

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- MAPE effectiveness relies on assumption that local geometric patterns are consistently informative for small object detection, which may not hold in sparse point distributions or low sensor resolution scenarios.
- Deployment speedup claims depend heavily on TensorRT version and hardware specifics that are not fully explored across diverse platforms.
- Lack of ablation studies isolating individual component contributions makes it difficult to precisely attribute performance gains.

## Confidence
- High confidence: Claims about mAP/NDS performance improvements on nuScenes (64.6 mAP, 70.1 NDS) are well-supported by reported experimental results and direct comparisons to established baselines.
- Medium confidence: Deployment speed claims (24 FPS on RTX3070Ti) are credible but depend on specific TensorRT and quantization settings not fully detailed in paper.
- Medium confidence: Architectural innovations (MAPE and CRVNet) are well-described, but paper lacks ablation studies isolating each component's contribution.

## Next Checks
1. Perform ablation study removing MAPE and replacing with plain max-pooling to quantify contribution to small object detection performance.
2. Test FastPillars on alternative 3D detection datasets (e.g., KITTI) to evaluate generalization beyond nuScenes.
3. Profile inference speed across different hardware platforms (NPU, embedded GPU) to verify deployment claims are not GPU-specific.