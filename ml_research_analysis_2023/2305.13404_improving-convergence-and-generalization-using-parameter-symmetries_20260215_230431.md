---
ver: rpa2
title: Improving Convergence and Generalization Using Parameter Symmetries
arxiv_id: '2305.13404'
source_url: https://arxiv.org/abs/2305.13404
tags:
- teleportation
- loss
- gradient
- generalization
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how exploiting parameter space symmetries can
  improve both optimization and generalization in deep learning. Parameter space symmetries
  are transformations of the parameters that leave the loss invariant, allowing us
  to teleport between points in the parameter space on the same level set of the loss
  function.
---

# Improving Convergence and Generalization Using Parameter Symmetries

## Quick Facts
- arXiv ID: 2305.13404
- Source URL: https://arxiv.org/abs/2305.13404
- Reference count: 40
- One-line primary result: Teleportation using parameter space symmetries accelerates optimization and improves generalization by moving to points with higher gradient norm and different curvatures.

## Executive Summary
This paper introduces a novel approach to deep learning optimization by exploiting parameter space symmetries through a technique called teleportation. The key insight is that symmetries of the loss function create level sets in parameter space where parameters can be moved without changing the loss value. By teleporting to points within these level sets that have higher gradient norms or different curvature properties, the authors demonstrate both faster convergence and improved generalization across multiple optimization algorithms and datasets.

## Method Summary
The method centers on identifying parameter space symmetries - transformations that leave the loss function invariant. The authors implement teleportation by performing gradient ascent on group elements to find transformations that maximize a chosen objective (gradient norm, sharpness, or curvature) while staying on the same loss level set. This transformed parameterization is then used in standard optimization updates. The approach is integrated with various optimizers including momentum, AdaGrad, RMSProp, and Adam, as well as meta-learning frameworks.

## Key Results
- Teleportation accelerates SGD convergence by moving to points with higher gradient norm on the same loss level set
- Moving to minima with different curvatures improves generalization, establishing a connection between curvature and generalization ability
- Integrating teleportation into a wide range of optimization algorithms and optimization-based meta-learning improves convergence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Teleportation accelerates SGD convergence by moving parameters to points with higher gradient norm on the same loss level set.
- **Mechanism:** Parameter space symmetries allow teleportation to find group elements that maximize gradient norm within a level set. Since the loss is invariant under symmetry transformations, the teleported point has the same loss but steeper gradient, enabling faster descent.
- **Core assumption:** Each loss level set consists of a single G-orbit (transitive action).
- **Evidence anchors:**
  - [abstract] "Parameter space symmetries are transformations of the parameters that leave the loss invariant, allowing us to teleport between points in the parameter space on the same level set of the loss function."
  - [section 3.1] "gt ∈ arg maxg∈G ∥∇L(g · wt)∥2" and Theorem 3.1 showing convergence to stationary points on the orbit.
  - [corpus] Weak - no direct citations, but related work on symmetry teleportation exists.
- **Break condition:** If level sets are not single orbits, teleportation may not find steepest point.

### Mechanism 2
- **Claim:** Teleportation improves generalization by moving parameters to flatter minima or regions with different curvature.
- **Mechanism:** Teleportation can target sharpness (average loss change under perturbations) or curvature (mean curvature of minimum curves) as objectives. Flatter minima and certain curvature properties correlate with better generalization.
- **Core assumption:** Sharpness and curvature metrics correlate with generalization ability.
- **Evidence anchors:**
  - [section 4.1] Definition of sharpness ϕ and curvature ψ metrics.
  - [section 4.3] Table 1 showing positive correlation between sharpness and validation loss, negative correlation between curvature and validation loss.
  - [corpus] Weak - related works on sharpness and generalization exist but not cited here.
- **Break condition:** If sharpness/curvature correlation fails or symmetry group is insufficient to explore desired regions.

### Mechanism 3
- **Claim:** Teleportation brings first-order updates closer to second-order methods by adding a symmetry component to the update direction.
- **Mechanism:** Newton's method update decomposes into gradient and symmetry components. The symmetry component moves toward higher gradient norm. Teleportation effectively adds this component to first-order methods.
- **Core assumption:** For convex functions, symmetry component increases gradient norm.
- **Evidence anchors:**
  - [section 3.2] Proposition B.2 showing gradient norm increases along symmetry component for convex functions.
  - [section 3.2] Proposition B.3 showing symmetry component is optimal direction for increasing gradient norm.
  - [corpus] Weak - related work on symmetry in optimization exists but not directly cited.
- **Break condition:** For non-convex functions, symmetry component may not increase gradient norm.

## Foundational Learning

- **Concept:** Parameter space symmetries
  - Why needed here: Teleportation relies on identifying transformations that leave loss invariant to move between equivalent points.
  - Quick check question: What is an example of a parameter space symmetry in a fully-connected network?

- **Concept:** Lie groups and Lie algebras
  - Why needed here: The paper uses Lie group actions to define symmetry transformations and compute curvatures.
  - Quick check question: How does the Lie algebra relate to the tangent space of a Lie group?

- **Concept:** Hessian-based curvature metrics
  - Why needed here: Curvature of minima is proposed as a generalization metric, requiring understanding of Hessian properties.
  - Quick check question: What does it mean for a minimum to have high curvature in a particular direction?

## Architecture Onboarding

- **Component map:**
  - Symmetry detection -> Group action definition -> Teleportation optimizer -> Integration layer -> Base optimizer update

- **Critical path:**
  1. Detect symmetries in architecture
  2. Define group action on parameters
  3. Implement teleportation step (gradient ascent on group)
  4. Integrate with base optimizer
  5. Monitor convergence and generalization metrics

- **Design tradeoffs:**
  - Teleportation frequency vs. computational cost
  - Objective choice (sharpness vs. curvature) vs. generalization goals
  - Group action complexity vs. implementation feasibility

- **Failure signatures:**
  - No improvement in convergence rate
  - Degradation in generalization after teleportation
  - Numerical instability in group element optimization

- **First 3 experiments:**
  1. Test teleportation on simple convex quadratic with known symmetry
  2. Compare sharpness-based vs. curvature-based teleportation objectives
  3. Measure impact on generalization for different teleportation frequencies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does teleporting once suffice to ensure that the entire gradient flow trajectory remains optimal?
- Basis in paper: [explicit] Section 3.3 states that for certain functions, one teleportation can result in an optimal trajectory for the entire gradient flow. However, the exact conditions for this are not fully specified.
- Why unresolved: The paper provides a sufficient condition involving the vanishing of the Lie bracket, but this may be too restrictive and not capture all cases where one teleportation suffices.
- What evidence would resolve it: Examples of functions where one teleportation suffices but does not meet the stated sufficient condition, and counterexamples where the condition is met but multiple teleportations are still needed.

### Open Question 2
- Question: How does the curvature of minima relate to generalization in higher-dimensional parameter spaces and for different types of neural network architectures?
- Basis in paper: [inferred] Section 4.2 introduces a novel method to quantify the curvature of minima and shows that teleporting to points with different curvatures can affect generalization. However, the relationship between curvature and generalization is only explored for a simple fully-connected network on MNIST and CIFAR-10.
- Why unresolved: The paper only provides a preliminary investigation of the curvature-generalization relationship for a specific architecture and dataset. The generality and underlying mechanisms are unclear.
- What evidence would resolve it: Systematic studies of curvature-generalization relationships across diverse architectures (CNNs, RNNs, transformers), datasets, and training paradigms, along with theoretical insights into the mechanisms.

### Open Question 3
- Question: Can teleportation be effectively integrated with other optimization algorithms beyond the ones explored in the paper, and what are the trade-offs in terms of convergence speed and generalization?
- Basis in paper: [explicit] Section 5 demonstrates that teleportation can improve convergence when integrated with AdaGrad, momentum, RMSProp, and Adam. However, many other optimization algorithms exist.
- Why unresolved: The paper only explores a limited set of optimization algorithms. The effectiveness and trade-offs of teleportation for other algorithms, such as those with adaptive learning rates or second-order information, remain unknown.
- What evidence would resolve it: Extensive experiments integrating teleportation with a wide range of optimization algorithms, along with theoretical analysis of the resulting convergence and generalization properties.

## Limitations

- The computational overhead of symmetry detection and teleportation steps is not fully characterized, particularly for large-scale models.
- The paper assumes that level sets consist of single orbits, which may not hold for all network architectures.
- Claims about curvature-based generalization improvements are demonstrated primarily on relatively simple architectures.

## Confidence

- **High Confidence:** Claims about teleportation accelerating optimization on same loss level sets (Mechanisms 1 and 3)
- **Medium Confidence:** Generalization improvements through curvature/sharpness optimization (Mechanism 2)
- **Low Confidence:** Applicability to very deep or complex architectures beyond tested configurations

## Next Checks

1. Test the approach on deeper architectures (ResNets, Transformers) to verify scalability
2. Characterize the computational overhead of symmetry detection and teleportation steps
3. Validate the transitivity assumption for level sets on more complex network architectures