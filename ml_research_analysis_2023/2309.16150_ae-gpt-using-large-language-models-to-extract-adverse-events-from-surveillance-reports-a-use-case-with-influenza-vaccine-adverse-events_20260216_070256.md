---
ver: rpa2
title: 'AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance
  Reports-A Use Case with Influenza Vaccine Adverse Events'
arxiv_id: '2309.16150'
source_url: https://arxiv.org/abs/2309.16150
tags:
- page
- gpt-3
- pretrained
- fine-tuned
- vaccine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops AE-GPT, a large language model-based tool for
  extracting adverse events from clinical surveillance reports, focusing on influenza
  vaccine-related adverse events using data from VAERS (1990-2016). The study evaluates
  multiple LLMs including GPT-2, GPT-3 variants, GPT-4, and Llama 2, comparing their
  performance in named entity recognition tasks.
---

# AE-GPT: Using Large Language Models to Extract Adverse Events from Surveillance Reports-A Use Case with Influenza Vaccine Adverse Events

## Quick Facts
- arXiv ID: 2309.16150
- Source URL: https://arxiv.org/abs/2309.16150
- Reference count: 1
- Fine-tuned GPT-3.5 (AE-GPT) achieved F1 score of 0.704 (strict) and 0.816 (relaxed) for adverse event extraction

## Executive Summary
This study develops AE-GPT, a large language model-based tool for extracting adverse events from clinical surveillance reports, focusing on influenza vaccine-related adverse events using data from VAERS (1990-2016). The research evaluates multiple LLMs including GPT-2, GPT-3 variants, GPT-4, and Llama 2, comparing their performance in named entity recognition tasks. The fine-tuned GPT-3.5 model (AE-GPT) achieved the best performance with an average micro F1 score of 0.704 for strict match and 0.816 for relaxed match, outperforming both other LLMs and traditional machine learning approaches.

## Method Summary
The study employs VAERS reports from 1990-2016, with 72 reports used for training and 91 for testing, focusing on influenza vaccine adverse events. Six entity types were targeted: investigation, nervous_AE, other_AE, procedure, social_circumstance, and temporal_expression. The researchers evaluated zero-shot and fine-tuned versions of multiple LLMs including GPT-2, GPT-3 variants, GPT-4, and Llama 2. Fine-tuning used custom prompts and templates with annotated entities. Performance was measured using strict and relaxed F1 scores, precision, and recall.

## Key Results
- Fine-tuned GPT-3.5 (AE-GPT) achieved 0.704 average micro F1 score for strict match and 0.816 for relaxed match
- GPT-3.5 significantly outperformed Llama models and other LLMs in the NER task
- Fine-tuned models consistently outperformed their zero-shot counterparts across all tested architectures
- Social_circumstance and temporal_expression entities showed the most challenging extraction patterns

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning LLMs on domain-specific data significantly improves NER performance compared to zero-shot inference. The pretraining on large text corpora provides general language understanding, while fine-tuning on the VAERS dataset tailors the model's parameters to the specific patterns and terminology of adverse event reporting.

### Mechanism 2
GPT-3.5's architecture is particularly well-suited for the NER task compared to other LLMs like Llama 2. GPT-3.5's transformer architecture with its attention mechanisms and large parameter count allows it to better capture the contextual relationships and semantic nuances necessary for identifying entities in medical text.

### Mechanism 3
GPT-3.5's fine-tuning process is more effective than GPT-2's for this NER task. GPT-3.5's pretraining on a larger and more diverse dataset, combined with its architectural advancements, allows it to learn from the fine-tuning process more effectively than GPT-2.

## Foundational Learning

- **Named Entity Recognition (NER)**
  - Why needed here: The study evaluates LLMs' ability to extract specific types of entities from medical text
  - Quick check question: What are the different entity types the study is focusing on extracting from VAERS reports?

- **Fine-tuning vs. Zero-shot learning**
  - Why needed here: The study compares pretrained LLMs (zero-shot) with fine-tuned versions
  - Quick check question: What is the difference between zero-shot learning and fine-tuning, and why might fine-tuning be more effective for NER tasks?

- **Evaluation metrics (F1 score, precision, recall)**
  - Why needed here: The study uses F1 score, precision, and recall to evaluate LLM performance
  - Quick check question: What is the difference between strict match and relaxed match F1 scores, and why might one be preferred over the other in certain scenarios?

## Architecture Onboarding

- **Component map**: VAERS reports (1990-2016) -> Preprocessing -> Fine-tuning (GPT-2, GPT-3, GPT-4, Llama 2) -> Entity extraction -> Evaluation (F1, precision, recall)

- **Critical path**: 1) Prepare VAERS dataset (train/test split) -> 2) Fine-tune selected LLMs on training set -> 3) Use fine-tuned models to extract entities from test set -> 4) Evaluate performance using F1 score, precision, and recall

- **Design tradeoffs**: Model size vs. computational resources; fine-tuning data size vs. model performance; strict vs. relaxed match evaluation

- **Failure signatures**: Low F1 scores; high precision but low recall; high recall but low precision; inconsistent performance across entity types

- **First 3 experiments**:
  1. Compare pretrained vs. fine-tuned versions of each LLM on a small test subset
  2. Analyze errors made by AE-GPT to identify common failure patterns
  3. Experiment with different fine-tuning strategies (learning rate, epochs) to optimize performance

## Open Questions the Paper Calls Out

### Open Question 1
How would GPT-4's performance compare to GPT-3.5 in adverse event extraction when fine-tuned with a larger and more diverse dataset? The paper states it plans to incorporate fine-tuning experiments with GPT-4 in future work.

### Open Question 2
What specific architectural features of GPT models make them more adaptable to adverse event extraction tasks compared to Llama models? The paper notes Llama models showed stagnant performance but doesn't explain the architectural differences.

### Open Question 3
How would expanding the dataset to include drug adverse events and clinical notes affect LLM performance in adverse event extraction? The paper plans to introduce clinical notes and biomedical literature to enrich the dataset.

## Limitations

- Limited dataset size (91 annotated reports for testing) may affect generalizability
- Performance varies significantly across entity types, with some entities being more challenging to extract accurately
- Temporal generalizability concerns as the VAERS data spans 1990-2016 without addressing how performance might change with more recent data

## Confidence

**High Confidence**: LLMs can extract adverse events from clinical surveillance reports; fine-tuning improves NER performance compared to zero-shot inference; GPT-3.5 outperforms other tested LLMs for this specific task

**Medium Confidence**: Performance gains will generalize to other adverse event extraction tasks; architecture differences between GPT-3.5 and Llama 2 explain the performance gap; model will perform similarly on non-influenza vaccine adverse events

## Next Checks

1. Test AE-GPT on a separate, independently annotated dataset of vaccine adverse event reports to assess generalizability
2. Evaluate model performance on VAERS reports from different time periods (1990-2000 vs. 2010-2016) to determine if temporal changes affect accuracy
3. Conduct detailed error analysis focusing on most problematic entity types by manually reviewing 100+ false positive and false negative predictions