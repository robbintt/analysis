---
ver: rpa2
title: Alleviating Hallucinations of Large Language Models through Induced Hallucinations
arxiv_id: '2312.15710'
source_url: https://arxiv.org/abs/2312.15710
tags:
- arxiv
- llms
- hallucinations
- preprint
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Induce-then-Contrast Decoding (ICD) to alleviate
  hallucinations in large language models. The method first induces hallucinations
  from the original LLM to create a factually weak model, then penalizes these hallucinations
  during decoding via contrastive decoding to enhance factuality.
---

# Alleviating Hallucinations of Large Language Models through Induced Hallucinations

## Quick Facts
- arXiv ID: 2312.15710
- Source URL: https://arxiv.org/abs/2312.15710
- Reference count: 24
- Primary result: ICD improves factuality of LLMs, achieving performance comparable to ChatGPT and GPT-4 on TruthfulQA

## Executive Summary
This paper introduces Induce-then-Contrast Decoding (ICD), a method to reduce hallucinations in large language models by first inducing hallucinations through fine-tuning and then using contrastive decoding to penalize these hallucinations during generation. The approach creates a factually weak model from the original LLM and contrasts its output distribution with the original during decoding, amplifying factual predictions while downplaying induced hallucinations. Experimental results demonstrate significant improvements in factuality across multiple model sizes and families on both discrimination-based (TruthfulQA) and generation-based (FACTSCORE) benchmarks.

## Method Summary
ICD works by fine-tuning a base LLM with hallucinated samples to create a factually weak model, then applying contrastive decoding during generation. The method amplifies predictions from the original model while penalizing induced untruthful predictions by subtracting the log probabilities of the weak model from the original. This approach requires twice the computational cost per decoding step due to the need to evaluate both models simultaneously. The technique is evaluated on Llama2-7B-Chat and Mistral-7B-Instruct models across various benchmarks.

## Key Results
- Llama2-7B-Chat with ICD achieves performance comparable to ChatGPT on TruthfulQA
- Mistral-7B-Instruct with ICD achieves performance comparable to GPT-4 on TruthfulQA
- ICD shows consistent effectiveness across different model sizes with improvement escalating with scale
- The method improves both discrimination-based and generation-based factuality metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICD works by constructing a factually weak LLM through induced hallucinations and using it as a penalty term during decoding
- Mechanism: Fine-tunes the original LLM on hallucinated samples to create a factually weak model, then contrasts its output distribution with the original during decoding
- Core assumption: Induced hallucinations can effectively guide the original model to generate more factual content
- Evidence anchors:
  - [abstract] "Experimental results on both discrimination-based and generation-based hallucination evaluation benchmarks, such as TruthfulQA and FACTSCORE, demonstrate that our proposed ICD methods can effectively enhance the factuality of LLMs across various model sizes and families."
  - [section 3.2] "We achieve this by subtracting the log probabilities after inducing hallucinations from those of the original model, which can be formed as: Ft = βlogp(xt|x<t; θ) − logp(xt|x<t; θ + △θ)"

### Mechanism 2
- Claim: ICD improves factuality by amplifying predictions from the original model and downplaying induced untruthful predictions via contrastive decoding
- Mechanism: Determines final next-token predictions by contrasting output distributions of original and factually weak models
- Core assumption: Factually weak model's output distribution reliably indicates non-factual predictions in original model
- Evidence anchors:
  - [abstract] "We determine the final next-token predictions by amplifying the predictions from the original model and downplaying the induced untruthful predictions via contrastive decoding."
  - [section 3.2] "We aim to amplify the predictions from the original model and downplay the untruthful predictions. We achieve this by subtracting the log probabilities after inducing hallucinations from those of the original model."

### Mechanism 3
- Claim: ICD can effectively improve factuality of LLMs across various model sizes and families
- Mechanism: Applied consistently to different model sizes and families, showing improved factuality on evaluation benchmarks
- Core assumption: ICD method generalizes across different LLM architectures and sizes
- Evidence anchors:
  - [abstract] "Experimental results on both discrimination-based and generation-based hallucination evaluation benchmarks, such as TruthfulQA and FACTSCORE, demonstrate that our proposed ICD methods can effectively enhance the factuality of LLMs across various model sizes and families."
  - [section 4.4] "ICD shows consistent effectiveness on TruthfulQA across different model sizes. We also observe that the degree of improvement escalates with the model scale."

## Foundational Learning

- Concept: Fine-tuning
  - Why needed here: Used to create factually weak LLM by inducing hallucinations from original model
  - Quick check question: What is the purpose of fine-tuning in the ICD method, and how does it contribute to improving factuality?

- Concept: Contrastive decoding
  - Why needed here: Key mechanism for improving factuality by contrasting output distributions of original and factually weak models
  - Quick check question: How does contrastive decoding work in the context of ICD, and what role does it play in improving factuality?

- Concept: Hallucination induction
  - Why needed here: Process of creating factually weak LLM by fine-tuning on hallucinated samples, essential for ICD method
  - Quick check question: What is hallucination induction, and why is it necessary for the ICD method to work?

## Architecture Onboarding

- Component map:
  Original LLM -> Fine-tuning process (with hallucinated samples) -> Factually weak LLM -> Contrastive decoding mechanism -> Enhanced LLM

- Critical path:
  1. Fine-tune original LLM on hallucinated samples to create factually weak model
  2. During decoding, contrast output distributions of original and factually weak models
  3. Amplify predictions from original model and downplay induced hallucinations to determine final predictions

- Design tradeoffs:
  - Computational cost: ICD introduces additional computational costs due to contrastive decoding requiring twice the forward propagation
  - Model complexity: Method requires creating factually weak model, adding complexity to overall system
  - Generalization: Effectiveness may vary across different LLM architectures and sizes

- Failure signatures:
  - No improvement in factuality on evaluation benchmarks
  - Degradation in other aspects like fluency or coherence
  - Increased hallucinations in generated content

- First 3 experiments:
  1. Apply ICD to small-scale LLM and evaluate factuality on TruthfulQA benchmark
  2. Compare effectiveness of ICD with other hallucination mitigation methods like ITI and DoLa
  3. Investigate impact of varying data size for inducing hallucinations on ICD effectiveness

## Open Questions the Paper Calls Out
The paper suggests several directions for future work including evaluating ICD on more diverse tasks beyond question answering and biography generation, conducting systematic evaluation on its impact on original LLM capabilities, and investigating the trade-offs between different hallucination-inducing methods.

## Limitations
- Hallucination induction reliability depends on successfully creating factually weak models, with 100-300 hallucinated samples used but characterization of robustness across domains not fully explored
- Computational overhead of twice the forward propagation per decoding step may limit practical deployment in resource-constrained environments
- Benchmark coverage limited to TruthfulQA and FACTSCORE, which may not capture all forms of hallucinations in real-world applications

## Confidence
- High Confidence: Core mechanism of contrastive decoding for factuality enhancement is well-supported by experimental results with 20-30% improvements on TruthfulQA
- Medium Confidence: Claims about generalizability across model families are supported but not extensively validated beyond Llama2 and Mistral models
- Low Confidence: Assertion of achieving "comparable performance to ChatGPT and GPT-4" should be viewed cautiously due to potential differences in evaluation protocols and prompt engineering

## Next Checks
1. Apply ICD to domain-specific LLMs (medical, legal, technical) and evaluate performance on specialized factuality benchmarks to assess generalizability beyond general knowledge tasks
2. Conduct systematic ablation study varying number and diversity of hallucinated samples to determine minimum effective dataset size and characterize sensitivity to induction quality
3. Implement human evaluation comparing outputs from ICD-enhanced models against baselines, measuring not just factual accuracy but also fluency, coherence, and helpfulness to understand potential quality tradeoffs