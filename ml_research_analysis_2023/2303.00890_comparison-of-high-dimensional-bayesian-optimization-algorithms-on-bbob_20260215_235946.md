---
ver: rpa2
title: Comparison of High-Dimensional Bayesian Optimization Algorithms on BBOB
arxiv_id: '2303.00890'
source_url: https://arxiv.org/abs/2303.00890
tags:
- turbo1
- turbom
- function
- kpcabo
- sklearn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks five high-dimensional Bayesian optimization
  algorithms on the BBOB test suite, comparing them with vanilla BO and CMA-ES across
  dimensions 10-60. The methods include SAASBO (variable selection), EBO (additive
  models), PCA-BO and KPCA-BO (embeddings), and TuRBO (trust regions).
---

# Comparison of High-Dimensional Bayesian Optimization Algorithms on BBOB

## Quick Facts
- arXiv ID: 2303.00890
- Source URL: https://arxiv.org/abs/2303.00890
- Reference count: 40
- Primary result: TuRBO consistently outperforms other HDBO methods in high dimensions, with KPCA-BO and linear PCA-BO excelling for small budgets

## Executive Summary
This paper benchmarks five high-dimensional Bayesian optimization algorithms against vanilla BO and CMA-ES across dimensions 10-60 using the BBOB test suite. The HDBO methods include SAASBO (variable selection), EBO (additive models), PCA-BO and KPCA-BO (embeddings), and TuRBO (trust regions). Vanilla BO performs best at low dimensions but deteriorates as dimension increases. TuRBO emerges as the most promising approach, consistently outperforming other methods in high dimensions through its trust region strategy. KPCA-BO and linear PCA-BO achieve fast convergence for small budgets by operating in low-dimensional subspaces.

## Method Summary
The study compares five high-dimensional Bayesian optimization algorithms on the BBOB test suite, evaluating them across dimensions 10, 20, 40, and 60 with a budget of 10×D+50 evaluations. Each algorithm runs 10 independent trials on 3 instances per function. The methods tested include SAASBO (sparse axis-aligned subspace BO), EBO (embedded Bayesian optimization with additive models), PCA-BO and KPCA-BO (linear and kernel PCA embeddings), and TuRBO (trust region Bayesian optimization). Results are compared against vanilla BO and CMA-ES baselines, with performance metrics including solution quality (best-so-far target gap) and CPU time (total, model fitting, acquisition optimization).

## Key Results
- TuRBO consistently outperforms other HDBO methods in high dimensions, demonstrating the value of trust regions
- KPCA-BO and linear PCA-BO excel for small budgets due to fast convergence in low-dimensional subspaces
- Vanilla BO performs best at low dimensions but deteriorates as dimension increases
- SAASBO is the most computationally expensive method while TuRBO is the most efficient
- HDBO algorithms generally outperform vanilla BO and CMA-ES across the test suite

## Why This Works (Mechanism)

### Mechanism 1
Trust region strategies consistently outperform global surrogate methods in high-dimensional BO because they avoid overextension of the surrogate model. By restricting GP modeling to a local region around the current best solution, trust regions reduce the curse of dimensionality—fewer points are needed to train an accurate model in a lower-dimensional subspace. The core assumption is that the objective function can be well-approximated by a local model within a dynamically adjusted trust region. Evidence shows TuRBO consistently outperforms other methods in high dimensions. Break condition: If the function landscape is highly multimodal with distant basins, trust regions may get trapped in local optima without aggressive restarts.

### Mechanism 2
Linear and nonlinear embeddings achieve rapid convergence in early budget phases because they reduce the search space to the most informative subspace. Dimensionality reduction via PCA/KPCA maps the high-dimensional search space to a lower-dimensional manifold, enabling faster acquisition function optimization and cheaper GP modeling. The core assumption is that the global structure of the function can be captured within a low-dimensional subspace determined by principal components. Evidence shows KPCA-BO and linear PCA-BO excel for small budgets with initial speedups on specific functions. Break condition: If the global optimum lies outside the principal subspace, embeddings will stagnate early.

### Mechanism 3
Variable selection methods perform well when the objective depends on a small subset of dimensions, enabling focused modeling. SAASBO learns a relevance hierarchy over dimensions, allowing the GP to focus modeling effort on important axes and ignore irrelevant ones. The core assumption is that the true objective function has a sparse dependency structure in the input space. Evidence shows SAASBO finds excellent loss values on several functions while preserving input space structure. Break condition: If all variables interact strongly or the relevant subset is unknown, the sparsity assumption fails and performance degrades.

## Foundational Learning

- **Concept:** Gaussian Process Regression (GPR) as a surrogate model
  - Why needed here: All BO variants rely on GPR to estimate the objective function; understanding kernel choice, hyperparameters, and noise handling is essential
  - Quick check question: What effect does increasing the lengthscale of an RBF kernel have on the smoothness of the GP posterior?

- **Concept:** Acquisition function optimization (e.g., Expected Improvement)
  - Why needed here: The acquisition function drives sample selection; knowing its behavior and computational cost is critical for benchmarking
  - Quick check question: Why is EI typically cheaper to optimize than UCB or Entropy Search?

- **Concept:** Curse of dimensionality in BO
  - Why needed here: Explains why vanilla BO deteriorates beyond ~15D and motivates high-dimensional BO methods
  - Quick check question: In what way does increasing dimension affect the required number of samples to cover the space adequately?

## Architecture Onboarding

- **Component map:** Experiment driver → BBOB function instance → BO algorithm wrapper → GP surrogate + acquisition → evaluation oracle → result logger
- **Critical path:** 1) Initial DoE generation (size = D for BO variants, population size for CMA-ES) 2) GP surrogate construction (varies by algorithm) 3) Acquisition function optimization 4) Function evaluation 5) Model update and trust region adjustment (for TuRBO) 6) Repeat until budget exhausted
- **Design tradeoffs:** Trust regions balance local accuracy vs. global coverage; embeddings trade fast early progress vs. risk of premature stagnation; variable selection trades model simplicity vs. risk of missing interactions; CPU time tradeoff shows TuRBO is fastest while SAASBO is slowest
- **Failure signatures:** Stagnation after few iterations suggests embedding or variable selection mis-specification; erratic loss curves indicate GP hyperparameter or kernel issues; no improvement vs. CMA-ES at low D suggests algorithm complexity overhead
- **First 3 experiments:** 1) Run vanilla BO on f5 (linear slope) at D=10; verify rapid convergence to optimum 2) Run TuRBO vs. PCA-BO on f24 at D=60 with budget 600; compare final loss and CPU time 3) Run SAASBO on f1 at D=20; confirm performance on separable function and note CPU cost

## Open Questions the Paper Calls Out

### Open Question 1
How would hybridizing TuRBO with low-dimensional embeddings like PCA-BO affect optimization performance in high-dimensional spaces? The authors suggest this as a future research direction, noting that combining TuRBO's trust regions with PCA-BO's dimensionality reduction could yield competitive results. This remains a theoretical proposition without empirical validation. Experimental results comparing the hybrid algorithm against pure TuRBO and PCA-BO variants across diverse high-dimensional functions would resolve this.

### Open Question 2
How sensitive are the HDBO algorithms to hyperparameter initialization, particularly for methods like EBO that showed poor performance? The authors note that poor initialization of hyperparameters may have caused EBO's underperformance and suggest investigating the impact of hyperparameter optimization. The study used default hyperparameters without systematic tuning. Performance comparisons of each HDBO algorithm with both default and optimized hyperparameters across the BBOB suite would resolve this.

### Open Question 3
At what dimensionality threshold does CMA-ES begin to consistently outperform vanilla BO for small evaluation budgets? The results show vanilla BO outperforms CMA-ES at D=10, but this relationship reverses at higher dimensions without specifying the exact transition point. The experiments covered dimensions 10, 20, 40, and 60, but didn't systematically explore intermediate values or identify the crossover point. Systematic testing of vanilla BO vs. CMA-ES across dimensions 10-60 with fine-grained increments would identify the exact point where performance shifts.

## Limitations

- The study uses synthetic BBOB test functions which may not fully capture real-world optimization challenges
- Some algorithms required skipping certain function-dimension combinations due to computational constraints, potentially biasing results
- The budget allocation (10×D+50) and instance selection (only 3 per function) may limit generalizability of conclusions

## Confidence

- **High confidence**: TuRBO's superiority in high dimensions and vanilla BO's deterioration with increasing dimension - supported by consistent performance patterns across multiple functions
- **Medium confidence**: The relative performance of embedding methods (PCA-BO, KPCA-BO) - while results show clear patterns, limited function coverage reduces statistical certainty
- **Low confidence**: Claims about variable selection effectiveness - SAASBO's performance advantages are noted but computational expense limits extensive validation

## Next Checks

1. Validate TuRBO's trust region effectiveness by comparing against a variant with fixed (non-dynamic) trust regions on the same BBOB functions
2. Test embedding methods on functions where the global optimum lies outside the principal subspace to quantify the stagnation risk
3. Implement a scaled-up experiment with 10 instances per function and larger budgets to verify if current performance rankings hold under more extensive evaluation