---
ver: rpa2
title: 'DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based Queries'
arxiv_id: '2310.04678'
source_url: https://arxiv.org/abs/2310.04678
tags:
- abstract
- query
- queries
- dataset
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DORIS-MAE, a novel task and dataset for scientific
  document retrieval using multi-level aspect-based queries. It addresses the challenge
  of retrieving relevant documents from complex, multifaceted scientific queries.
---

# DORIS-MAE: Scientific Document Retrieval using Multi-level Aspect-based Queries

## Quick Facts
- arXiv ID: 2310.04678
- Source URL: https://arxiv.org/abs/2310.04678
- Reference count: 40
- This paper introduces DORIS-MAE, a novel task and dataset for scientific document retrieval using multi-level aspect-based queries.

## Executive Summary
This paper addresses the challenge of retrieving relevant documents from complex, multifaceted scientific queries. The authors introduce DORIS-MAE, a novel dataset consisting of 100 human-authored complex queries, each decomposed into aspects and sub-aspects, paired with ranked pools of relevant paper abstracts. They also present Anno-GPT, a scalable framework for validating LLM annotations that reduced annotation costs by 500x without compromising quality. Experiments with 17 retrieval methods showed significant performance drops compared to traditional datasets, highlighting the need for better approaches to handle complex queries. The dataset and codebase are available at https://github.com/Real-Doris-Mae/Doris-Mae-Dataset.

## Method Summary
The authors created DORIS-MAE by collecting 100 complex queries from computer science papers and decomposing each into aspects and sub-aspects. They generated candidate pools of 100 relevant paper abstracts for each query and used the Anno-GPT framework to annotate relevance scores. Anno-GPT employs hierarchical decomposition of queries into aspects and sub-aspects, making annotation tractable for LLMs by reducing each task to evaluating relevance of a single aspect against an abstract. The framework includes statistical validation comparing LLM-human agreement to human-human agreement. They evaluated 17 retrieval methods on this dataset using metrics including R@5, R@20, R-Precision, NDCG@10, MRR@10, and MAP.

## Key Results
- Anno-GPT achieved expert-level annotation quality while reducing costs by 500x through hierarchical decomposition of complex queries
- DORIS-MAE benchmark revealed significant performance gaps in existing retrieval methods for complex queries compared to traditional datasets
- The dataset provides a new evaluation standard for scientific document retrieval systems handling multi-faceted queries

## Why This Works (Mechanism)

### Mechanism 1
Anno-GPT achieves expert-level annotation quality while reducing costs by 500x through hierarchical decomposition of complex queries into aspects and sub-aspects. This makes annotation tractable for LLMs by reducing each task to evaluating relevance of a single aspect against an abstract. The core assumption is that LLMs can maintain consistent interpretation of aspect/abstract relevance when tasks are broken into smaller, focused components.

### Mechanism 2
DORIS-MAE benchmark reveals significant performance gaps in existing retrieval methods for complex queries. Complex queries with multiple semantic aspects create out-of-distribution scenarios for models trained on simpler queries, exposing limitations in their ability to capture multi-faceted relevance. The core assumption is that models trained on traditional datasets have learned patterns that don't generalize to multi-aspect queries requiring simultaneous consideration of multiple semantic dimensions.

### Mechanism 3
Anno-GPT framework enables scalable creation of expert-level datasets in new domains. By establishing a statistical validation pipeline that compares LLM-human agreement to human-human agreement, Anno-GPT provides rigorous quality assurance while eliminating the need for full human annotation. The core assumption is that statistical significance testing on a small human-annotated test set can reliably validate LLM performance across the entire dataset.

## Foundational Learning

- Concept: Statistical hypothesis testing for annotation quality validation
  - Why needed here: Provides rigorous framework to ensure Anno-GPT annotations meet expert standards without requiring full human annotation
  - Quick check question: What statistical test would you use to compare agreement levels between LLM and human annotators?

- Concept: Multi-vector document representations and max-sim operations
  - Why needed here: Required to understand how models like TSAspire/OTAspire handle multi-aspect queries differently from single-vector approaches
  - Quick check question: How does max-sim operation work when comparing a query with 3 vectors against an abstract with 5 vectors?

- Concept: Hierarchical decomposition of complex tasks
  - Why needed here: Core principle behind both Anno-GPT's annotation approach and DORIS-MAE's query structure
  - Quick check question: What criteria determine whether an aspect should be further decomposed into sub-aspects?

## Architecture Onboarding

- Component map: Query processing pipeline: query decomposition → aspect extraction → candidate pool generation; Annotation pipeline: aspect/abstract pairs → LLM annotation → statistical validation → deployment; Evaluation pipeline: model ranking → metric calculation (R@5, R@20, RP, NDCG, MRR, MAP) → benchmark comparison; Dataset structure: complex queries ↔ aspects/sub-aspects ↔ candidate pools ↔ annotations

- Critical path: Complex query → aspect decomposition → candidate pool generation → LLM annotation → statistical validation → deployment for training/testing

- Design tradeoffs:
  - Anno-GPT trades computational cost for annotation speed vs. potential consistency issues
  - DORIS-MAE trades dataset size for query complexity vs. traditional IR datasets
  - Multi-vector models trade computational efficiency for handling query complexity vs. single-vector models

- Failure signatures:
  - Anno-GPT: High variance in LLM responses across similar aspect/abstract pairs
  - DORIS-MAE evaluation: Models show good performance on traditional datasets but poor performance on DORIS-MAE
  - Pipeline: Statistical validation fails to reject null hypothesis despite visible quality differences

- First 3 experiments:
  1. Compare Anno-GPT performance on simple vs. complex queries to validate decomposition benefit
  2. Test whether fine-tuning models on DORIS-MAE improves performance vs. traditional datasets
  3. Validate whether Anno-GPT can successfully create datasets in new domains (e.g., biomedical) using same framework

## Open Questions the Paper Calls Out

### Open Question 1
How does the hierarchical aspect-based structure affect retrieval model performance when the aspect information is not explicitly provided to the model? The paper mentions that "Up until now, the hierarchical aspect-based structure that Anno-GPT utilizes has been hidden from all the evaluated models" and explores using concatenated aspects in experiments, but doesn't directly test the impact of hiding this information.

### Open Question 2
Can the Anno-GPT framework be adapted to validate annotations for other domains beyond computer science, and what factors would affect its transferability? The paper states "Anno-GPT could potentially utilize any high-performance LLM to replace ChatGPT and can be adapted for other expert-level tasks, given the availability of a small set of domain expert annotations for validation."

### Open Question 3
What is the relationship between the complexity of queries (number of aspects and sub-aspects) and the difficulty of retrieval, and how does this impact model design? While the paper shows that composite queries have lower average relevance scores and that sub-query tasks are less challenging than full-query tasks, it doesn't provide a systematic analysis of complexity-retrieval difficulty relationship.

## Limitations

- Data domain specificity: The DORIS-MAE dataset focuses exclusively on computer science queries, limiting generalizability to other scientific domains
- Model comparison constraints: The paper evaluates 17 retrieval methods but doesn't provide comprehensive ablation studies on which specific architectural components contribute most to performance gaps
- Statistical validation boundaries: Anno-GPT's statistical validation relies on agreement metrics but doesn't assess whether LLM-generated annotations capture nuanced expert judgment in edge cases

## Confidence

- High confidence: The core claims about Anno-GPT reducing annotation costs by 500x while maintaining expert-level quality are well-supported by the statistical validation framework and agreement metrics presented
- Medium confidence: The claim that DORIS-MAE reveals significant performance gaps in existing retrieval methods is supported by empirical results, though the specific reasons for individual model failures aren't systematically analyzed
- Low confidence: The assertion that Anno-GPT can be easily adapted for other expert-level tasks across different domains lacks empirical validation beyond the computer science domain tested in this work

## Next Checks

1. **Cross-domain validation**: Test Anno-GPT framework on a biomedical or social science dataset to verify domain adaptability claims and identify any domain-specific challenges

2. **Model failure analysis**: Conduct systematic ablation studies on individual retrieval models to isolate which architectural components (vector encoding, fine-tuning strategies, etc.) contribute most to performance degradation on multi-aspect queries

3. **Edge case robustness**: Create a small, carefully curated test set of ambiguous queries where human experts disagree, then evaluate whether Anno-GPT's statistical validation detects quality issues in these challenging cases