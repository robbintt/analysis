---
ver: rpa2
title: Projective Integral Updates for High-Dimensional Variational Inference
arxiv_id: '2301.08374'
source_url: https://arxiv.org/abs/2301.08374
tags:
- mean
- variational
- quadrature
- each
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes an efficient integration technique for high-dimensional
  mean-field variational inference, where the objective requires repeatedly evaluating
  high-dimensional integrals. The approach combines antithetic pairs of cross-polytope
  vertices in the Hadamard basis to achieve periodic exactness on quadratic basis
  functions.
---

# Projective Integral Updates for High-Dimensional Variational Inference

## Quick Facts
- arXiv ID: 2301.08374
- Source URL: https://arxiv.org/abs/2301.08374
- Reference count: 40
- One-line primary result: Over 98.9% sparsity while retaining 96.9% validation accuracy on MNIST CNN

## Executive Summary
This work introduces an efficient integration technique for high-dimensional mean-field variational inference that combines antithetic pairs of cross-polytope vertices in the Hadamard basis to achieve periodic exactness on quadratic basis functions. The approach enables a sparsifying sieve methodology that gradually eliminates parameters while maintaining model accuracy, demonstrated by achieving over 98.9% sparsity with 96.9% validation accuracy on a CNN for MNIST. The method addresses the computational bottleneck of repeatedly evaluating high-dimensional integrals in variational inference by leveraging the mathematical structure of antithetic pairs and cross-polytope geometry.

## Method Summary
The method combines three key innovations: (1) a quadrature sequence using antithetic pairs of cross-polytope vertices in the Hadamard basis that achieves periodic exactness on quadratic basis functions, (2) projective integral updates that yield quasi-Newton variational Bayes when the basis spans univariate quadratics, and (3) a sparsifying sieve methodology using Dirac-Gauss mixtures that gradually eliminates parameters. The quadrature technique generates evaluation points that combine to correctly integrate all univariate quadratics, while averaging over short subsequences achieves periodic exactness on multivariate quadratics. This enables efficient computation of ELBO gradients and Hessians, which are then used in variational updates that project the log-posterior onto a compatible basis. The sparsifying sieve associates zeros with finite probabilities and gradually adjusts these probabilities while readjusting non-zero parameters.

## Key Results
- Achieved over 98.9% sparsity while retaining 96.9% validation accuracy on MNIST CNN
- Integration exactness on quadratic basis functions using only 4 function evaluations
- Periodic exactness on multivariate quadratics with exactness on 1+2d+1/4dÂ² basis functions

## Why This Works (Mechanism)

### Mechanism 1
Antithetic pairs of cross-polytope vertices in the Hadamard basis achieve periodic exactness on quadratic basis functions. Each iterate contains two evaluation points that combine to correctly integrate all univariate quadratics, and averaging over short subsequences achieves periodic exactness on a much larger space of multivariate quadratics. This works because the mean-field factors are either symmetric (for univariate cubics) or the integration uses antithetic pairs that exploit symmetry in evaluation points.

### Mechanism 2
The projective integral updates yield quasi-Newton variational Bayes (QNVB) when the basis spans univariate quadratics in each parameter. The optimizer becomes a fixed-point of projective integral updates, where each update projects the log-posterior onto a compatible basis using the efficient quadrature. This works because every feasible log density can be expressed as a linear combination of functions from the given basis, enabling the fixed-point characterization.

### Mechanism 3
The sparsifying sieve methodology gradually eliminates parameters while maintaining model accuracy by associating zeros with finite probabilities. Dirac-Gauss mixtures allow finite probabilities for parameter zeros, and the sieve filters parameters by gradually adjusting zero probabilities while readjusting non-zero parameters. This works because the variational distribution can be written as a spike-and-slab mixture and the sieve schedule can control the rate of change in zero probabilities.

## Foundational Learning

- Concept: Variational inference as optimization over a simpler distribution.
  - Why needed here: The work relies on optimizing a mean-field distribution to approximate the posterior, which requires understanding the ELBO and KL divergence.
  - Quick check question: What is the relationship between maximizing the ELBO and minimizing the KL divergence from the variational distribution to the posterior?

- Concept: Numerical quadrature and cubature methods.
  - Why needed here: The core contribution is an efficient quadrature sequence for high-dimensional integration, which is essential for computing the ELBO gradients and Hessians.
  - Quick check question: How does the exactness property of a quadrature relate to the number of function evaluations needed?

- Concept: Mean-field distributions and their properties.
  - Why needed here: The work focuses on mean-field variational distributions, which factorize across parameters, enabling scalable integration and updates.
  - Quick check question: Why does the mean-field assumption simplify high-dimensional integration, and what correlations does it ignore?

## Architecture Onboarding

- Component map: Cross-polytope vertex sequence generator -> Quadratic loss approximation -> Variational update with sieve -> Training loop with annealing -> Mean-field distribution representation

- Critical path: 1) Generate quadrature sequence based on parameter dimension and current index, 2) Evaluate loss, gradient, and Hessian at antithetic pairs, 3) Accumulate quadratic approximation terms with annealing, 4) Update variational parameters using fixed-point iteration, 5) Apply sieve to adjust zero probabilities and realize sparsity pattern

- Design tradeoffs:
  - Exactness vs. computational cost: Higher exactness requires longer subsequences but more evaluations
  - Sparsity vs. accuracy: Aggressive sparsity targets may reduce model performance if not enough parameters remain
  - Initialization vs. convergence: Small initial variance helps discovery of basins but may slow early learning

- Failure signatures:
  - Poor convergence: Hessian diagonals remain negative or gradients do not stabilize
  - Accuracy drop: Sieve schedule too aggressive, removing too many parameters
  - Integration errors: Quadrature sequence not generating expected exactness periodicity

- First 3 experiments:
  1. Verify exactness of quadrature sequence on known quadratic functions in 2D and 4D
  2. Test variational updates on a simple logistic regression with synthetic data
  3. Run sparsifying sieve on a small CNN (e.g., MNIST) and measure sparsity vs. accuracy tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
Can the periodic exactness property of cross-polytope vertices in the Hadamard basis be extended to integrate multivariate polynomials of higher total degree beyond quadratic? The paper demonstrates periodic exactness for quadratic total degree polynomials but does not explore whether this property extends to higher total degree polynomials or what the limitations might be. Resolving this would require analytical proofs or numerical experiments showing whether and how the periodic exactness property extends to cubic or higher total degree polynomials.

### Open Question 2
How does the sparsifying methodology perform on more complex deep learning architectures beyond the simple CNN for MNIST demonstrated in the paper? The paper only presents results for a simple CNN architecture on a relatively straightforward dataset. Resolving this would require experimental results applying the sparsifying methodology to various deep learning architectures (CNNs, ResNets, transformers) on different datasets (CIFAR, ImageNet, etc.) showing sparsity levels achieved, accuracy retention, and training time compared to baseline methods.

### Open Question 3
What is the theoretical relationship between the number of mixture components in mean-field distributions and the approximation quality of the posterior distribution? The paper mentions that mean-field distributions are equivalent to rank-1 functions and suggests that higher-rank variational distributions would be scalable and feasible to integrate. Resolving this would require theoretical bounds on the approximation error as a function of the number of mixture components, convergence analysis of variational inference with increasing rank, and experimental validation showing the trade-off between approximation quality and computational cost.

## Limitations
- Exactness periodicity relies heavily on mean-field assumption and symmetric distributions
- Scalability to very high-dimensional problems (>1000 parameters) remains untested
- Computational overhead of generating cross-polytope vertices in Hadamard basis could become prohibitive

## Confidence
- High confidence in quadratic exactness property and periodic exactness proofs
- Medium confidence in practical integration accuracy, as empirical validation is limited to specific model architectures and datasets
- Low confidence in generality of sparsifying sieve methodology across all neural network architectures

## Next Checks
1. Test the quadrature sequence on high-dimensional quadratic functions (10-20 dimensions) with known exact integrals to verify that the periodic exactness property holds empirically and that integration errors remain bounded.

2. Apply the sparsifying methodology to different neural network architectures (e.g., ResNet, Transformer) on multiple datasets (CIFAR-10, ImageNet) to assess the generality of the sparsity vs. accuracy tradeoff and identify architectural dependencies.

3. Conduct ablation studies varying the antithetic pair structure and Hadamard basis parameters to understand the sensitivity of exactness properties to these design choices and identify potential failure modes when these assumptions are violated.