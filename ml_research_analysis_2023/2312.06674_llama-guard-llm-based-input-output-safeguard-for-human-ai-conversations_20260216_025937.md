---
ver: rpa2
title: 'Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations'
arxiv_id: '2312.06674'
source_url: https://arxiv.org/abs/2312.06674
tags:
- llama
- guard
- content
- taxonomy
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Llama Guard is a safety classifier fine-tuned on a curated dataset
  to detect policy-violating content in LLM prompts and responses. It follows an instruction-tuning
  paradigm with clear input guidelines and binary outputs, enabling both prompt and
  response classification.
---

# Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations

## Quick Facts
- arXiv ID: 2312.06674
- Source URL: https://arxiv.org/abs/2312.06674
- Authors: 
- Reference count: 6
- Primary result: Llama Guard achieves high AUPRC scores on safety classification benchmarks, outperforming or matching existing moderation tools through instruction-tuned LLM approaches.

## Executive Summary
Llama Guard is an LLM-based safety classifier designed to detect policy-violating content in both prompts and responses for human-AI conversations. Built on Llama2-7B, it uses instruction-following capabilities to perform multi-class classification across six risk categories (Violence & Hate, Sexual Content, Criminal Planning, Guns & Illegal Weapons, Regulated Substances, and Self-Harm) with binary output decisions. The model demonstrates strong performance on both internal and public benchmarks, achieving high AUPRC scores while maintaining adaptability to different taxonomies through zero-shot and few-shot prompting.

## Method Summary
Llama Guard is fine-tuned on a curated dataset of 13,997 human-AI conversation pairs labeled by expert annotators using a six-category safety taxonomy. The model leverages Llama2-7B's instruction-following capabilities to perform both prompt and response classification through text-based prompts without requiring separate model heads. Training uses data augmentation techniques including category dropping and shuffling, with hyperparameters of batch size 2, sequence length 4096, and learning rate of 2 × 10⁻⁶ for approximately 1 epoch. The model outputs binary "safe/unsafe" decisions along with category lists, and demonstrates strong performance on benchmarks like ToxicChat and OpenAI Moderation Evaluation dataset.

## Key Results
- Achieves high AUPRC scores on both internal and public benchmarks (ToxicChat, OpenAI Moderation Evaluation)
- Outperforms or matches existing moderation tools like OpenAI API, Perspective API, and Azure API
- Demonstrates strong adaptability to different taxonomies through zero-shot and few-shot prompting without full fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Llama Guard leverages the instruction-following capabilities of LLMs to perform both prompt and response classification through text-based prompts without requiring separate model heads.
- Mechanism: By fine-tuning Llama2-7B on a curated dataset with instruction-style prompts that specify "prompt classification" or "response classification" tasks, the model learns to interpret context and apply the appropriate safety taxonomy categories accordingly.
- Core assumption: The underlying LLM has sufficient semantic understanding and reasoning capability to follow the instruction format and distinguish between prompt and response contexts.
- Evidence anchors:
  - [abstract] "Llama Guard functions as a language model, carrying out multi-class classification and generating binary decision scores"
  - [section] "We do this with a single model by leveraging the capabilities of LLM models to follow instructions (Wei et al., 2022a)"
- Break condition: If the LLM cannot reliably parse the instruction format or distinguish between user and agent roles, classification accuracy will degrade significantly.

### Mechanism 2
- Claim: Llama Guard achieves high adaptability to different taxonomies through zero-shot and few-shot prompting without requiring full fine-tuning.
- Mechanism: The model is trained to accept safety guidelines as part of the input prompt, allowing it to dynamically adjust to new categories and definitions simply by including them in the inference prompt.
- Core assumption: The instruction-tuned model retains enough flexibility to interpret novel category definitions and apply them correctly to new examples.
- Evidence anchors:
  - [abstract] "The instruction fine-tuning of Llama Guard allows for the customization of tasks and the adaptation of output formats"
  - [section] "We have had success with zero-shot and few-shot Llama Guard prompts with novel policies"
- Break condition: If the new taxonomy is too different from the training taxonomy, or if the category descriptions are ambiguous, the model may produce inconsistent classifications.

### Mechanism 3
- Claim: Llama Guard achieves strong performance by fine-tuning on high-quality human-annotated data rather than relying solely on synthetic or heuristic-generated examples.
- Mechanism: The model is trained on 13,997 prompt-response pairs labeled by expert red-team annotators using a carefully defined taxonomy, ensuring the learned representations capture real-world safety concerns.
- Core assumption: Expert human annotation provides reliable ground truth that captures the nuances of safety policy violations better than automated labeling.
- Evidence anchors:
  - [section] "we have meticulously gathered a dataset of high quality...labeled by expert, in-house red team"
  - [section] "We leverage the human preference data about harmlessness from Anthropic (Ganguli et al., 2022)"
- Break condition: If the annotation guidelines are inconsistent or the annotators have systematic biases, the model will learn incorrect patterns that don't generalize.

## Foundational Learning

- Concept: Multi-label classification with binary decision output
  - Why needed here: Llama Guard must identify which of several safety categories are violated (multi-label) but output a simple "safe/unsafe" decision plus category list
  - Quick check question: If an input violates both "Violence & Hate" and "Self-Harm" categories, what should the model output format look like?

- Concept: Instruction following in LLMs
  - Why needed here: The model must interpret and execute classification tasks based on natural language instructions specifying what to classify and how
  - Quick check question: What happens if you remove the category definitions from the prompt but keep the instruction to classify as safe/unsafe?

- Concept: Data augmentation through category dropping
  - Why needed here: To teach the model to focus only on specified categories and not over-rely on all categories being present
  - Quick check question: If you train with category X removed from some examples, how should the model handle inputs that only violate category X?

## Architecture Onboarding

- Component map: Input layer (text prompt with instruction and category definitions) -> Encoder (Llama2-7B transformer blocks) -> Output layer (single token generation: "safe" or "unsafe" with optional category list) -> Training pipeline (fine-tuning on curated dataset with augmentation)

- Critical path: Prompt formatting → Model inference → Token decoding → Safety decision
  - Most latency comes from the 4096 sequence length and model parallelism requirements

- Design tradeoffs:
  - Model size (7B) vs. performance: Smaller than typical LLMs but sufficient for the task
  - Binary vs. probability output: Binary simplifies deployment but loses granularity
  - Single vs. multi-model approach: Single model reduces complexity but requires sophisticated prompting

- Failure signatures:
  - Output format errors: Model generates malformed responses (not "safe"/"unsafe")
  - Category confusion: Model assigns wrong categories to violations
  - Context blindness: Model fails to distinguish prompt vs. response classification

- First 3 experiments:
  1. Test zero-shot adaptation by prompting with OpenAI moderation categories on the OpenAI dataset and measuring AUPRC
  2. Test few-shot learning by adding 2-4 examples per category to the prompt and comparing performance
  3. Test category dropping augmentation by training with subsets of categories removed and measuring robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Llama Guard compare to other moderation tools when evaluating across different languages beyond English?
- Basis in paper: [inferred] The paper states that all fine-tuning data, as well as most pretraining data used for Llama Guard is in English, and therefore the authors do not guarantee adequate performance when used for other languages.
- Why unresolved: The paper does not provide any data or analysis on Llama Guard's performance on languages other than English.
- What evidence would resolve it: Empirical evaluation of Llama Guard's performance on a diverse set of languages, using datasets and taxonomies relevant to those languages.

### Open Question 2
- Question: What is the impact of fine-tuning Llama Guard on a new taxonomy versus using zero-shot or few-shot prompting for adaptation to that taxonomy?
- Basis in paper: [explicit] The paper discusses the adaptability of Llama Guard via prompting and fine-tuning, and provides some comparison of the two approaches, but does not provide a comprehensive analysis of the trade-offs between them.
- Why unresolved: The paper does not provide a detailed analysis of the performance and resource requirements of fine-tuning versus prompting for adapting Llama Guard to new taxonomies.
- What evidence would resolve it: A systematic study comparing the performance, resource usage, and adaptation time of Llama Guard when fine-tuned on a new taxonomy versus when adapted via zero-shot or few-shot prompting.

### Open Question 3
- Question: How does Llama Guard handle prompt injection attacks that could alter or bypass its intended use?
- Basis in paper: [explicit] The paper mentions that Llama Guard may be susceptible to prompt injection attacks that could alter or bypass its intended use.
- Why unresolved: The paper does not provide any analysis or mitigation strategies for prompt injection attacks on Llama Guard.
- What evidence would resolve it: Empirical evaluation of Llama Guard's vulnerability to prompt injection attacks, and analysis of potential mitigation strategies to improve its robustness against such attacks.

## Limitations
- Internal dataset composition remains undisclosed, limiting external validation of the model's learned representations
- Model's real-world effectiveness depends heavily on the quality and representativeness of the curated dataset
- Claims about adaptability to arbitrary taxonomies through zero-shot and few-shot prompting lack comprehensive empirical validation across diverse policy frameworks

## Confidence

- **High Confidence**: The technical mechanism of using instruction-tuned LLMs for multi-class classification is well-established and clearly demonstrated in the paper. The model architecture and training procedure are explicitly described.
- **Medium Confidence**: The claimed performance improvements over existing tools are supported by benchmark results, but the evaluation relies heavily on proprietary datasets and limited external benchmarks.
- **Low Confidence**: Claims about adaptability to arbitrary taxonomies through zero-shot and few-shot prompting lack comprehensive empirical validation across diverse policy frameworks.

## Next Checks

1. **Cross-cultural validation**: Test Llama Guard's performance on safety datasets from different cultural contexts (e.g., non-Western moderation datasets) to assess whether the model's learned representations generalize beyond the training data's cultural assumptions.

2. **Adversarial robustness testing**: Systematically generate adversarial examples that deliberately exploit edge cases in the category definitions to measure the model's robustness against intentionally crafted policy-violating content.

3. **Human-in-the-loop evaluation**: Conduct a blind study comparing Llama Guard's classifications against human expert judgments on a diverse set of real-world conversations, measuring both precision and recall while tracking annotator agreement rates.