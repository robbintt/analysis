---
ver: rpa2
title: 'Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting'
arxiv_id: '2310.08278'
source_url: https://arxiv.org/abs/2310.08278
tags:
- time
- series
- forecasting
- datasets
- lag-llama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents Lag-Llama, a foundation model for univariate
  probabilistic time series forecasting based on a decoder-only transformer architecture
  using lag features. The model is pretrained on a large corpus of diverse time series
  data from the Monash Time Series Repository and other sources.
---

# Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting

## Quick Facts
- arXiv ID: 2310.08278
- Source URL: https://arxiv.org/abs/2310.08278
- Authors: 
- Reference count: 40
- Key outcome: Lag-Llama, a foundation model for univariate probabilistic time series forecasting, demonstrates strong zero-shot generalization and state-of-the-art performance when fine-tuned on small datasets.

## Executive Summary
Lag-Llama is a foundation model for univariate probabilistic time series forecasting based on a decoder-only transformer architecture. The model uses lag features to capture temporal structure and is pretrained on a large corpus of diverse time series data from the Monash Time Series Repository and other sources. Lag-Llama demonstrates strong zero-shot generalization capabilities on unseen datasets, outperforming supervised baselines on the Traffic dataset and achieving comparable performance on the M4 Weekly dataset. When fine-tuned on small fractions of these unseen datasets, Lag-Llama achieves state-of-the-art performance, surpassing prior deep learning approaches. The model serves as a strong contender in time series forecasting and paves the way for future advancements in foundation models tailored to time series data.

## Method Summary
Lag-Llama is trained on 305,443 time series from the Monash Time Series Repository and other sources, using lag features constructed from frequency-specific historical values. The model is a decoder-only transformer with RMSNorm and RoPE positional encoding, trained with negative log-likelihood loss and stratified sampling. Evaluation is performed on the M4 Weekly and Traffic datasets using CRPS metric, comparing zero-shot and fine-tuned performance against supervised baselines.

## Key Results
- Outperforms supervised baselines on Traffic dataset in zero-shot setting
- Achieves comparable performance to state-of-the-art on M4 Weekly dataset zero-shot
- When fine-tuned on small fractions of unseen datasets, achieves state-of-the-art performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The lag-based feature construction preserves temporal structure and seasonality, enabling the transformer to learn universal patterns across datasets.
- **Mechanism:** By explicitly encoding historical values at frequency-specific lags (e.g., daily, weekly, monthly), the model gains direct access to cyclical patterns without needing to infer them from raw sequences.
- **Core assumption:** Temporal periodicity is the dominant signal in univariate forecasting, and lags aligned with these periods provide sufficient context.
- **Evidence anchors:**
  - [abstract] Claims zero-shot generalization on unseen datasets.
  - [section] Describes lag indices corresponding to frequencies in the corpus.
  - [corpus] Weak: no corpus paper directly validates lag superiority, but related work cites Lag-Llama's design.
- **Break condition:** If data has irregular, non-periodic patterns (e.g., abrupt regime shifts), lags may not capture necessary dynamics.

### Mechanism 2
- **Claim:** Pretraining on a large, diverse corpus enables transfer to unseen domains with minimal fine-tuning.
- **Mechanism:** The model learns general representations of time series structure during pretraining, which can be adapted to new datasets with few examples.
- **Core assumption:** Common statistical regularities exist across domains (finance, energy, transport), and the transformer can extract these from lag features.
- **Evidence anchors:**
  - [abstract] Reports outperforming supervised baselines on Traffic zero-shot.
  - [section] Describes training on 305,443 series from varied domains.
  - [corpus] Weak: related work mentions foundation models but lacks direct evidence for lag-based transfer.
- **Break condition:** If downstream tasks require domain-specific inductive biases not captured in pretraining data.

### Mechanism 3
- **Claim:** Smoothly broken power-law scaling accurately predicts performance gains from increasing model size.
- **Mechanism:** The functional form captures nonlinear scaling behavior, allowing extrapolation to larger models than trained.
- **Core assumption:** Performance follows predictable scaling trends beyond observed parameter ranges.
- **Evidence anchors:**
  - [abstract] Mentions fitting scaling laws to predict generalization.
  - [section] Details the broken power-law fitting procedure and extrapolation.
  - [corpus] Weak: scaling laws cited in literature but not directly validated for this architecture.
- **Break condition:** If scaling deviates from power-law due to architectural bottlenecks or data limitations.

## Foundational Learning

- **Concept:** Foundation models and transfer learning
  - **Why needed here:** Enables zero-shot and few-shot performance on unseen datasets by leveraging pretraining on diverse data.
  - **Quick check question:** Can the model perform well on a dataset it has never seen during training?

- **Concept:** Lag feature engineering
  - **Why needed here:** Transforms univariate series into structured inputs that preserve temporal dependencies for transformer processing.
  - **Quick check question:** Do the lag indices align with the dominant frequencies in the training corpus?

- **Concept:** Probabilistic forecasting and uncertainty quantification
  - **Why needed here:** Outputs full predictive distributions rather than point estimates, crucial for decision-making under uncertainty.
  - **Quick check question:** Does the model produce calibrated prediction intervals on held-out data?

## Architecture Onboarding

- **Component map:** Lag features -> Linear projection -> Masked transformer layers -> Distribution head -> Loss computation

- **Critical path:** Lag feature construction → Linear projection → Masked transformer layers → Distribution head → Loss computation

- **Design tradeoffs:**
  - Lag features preserve structure but require larger context windows at inference.
  - Student's t-distribution is simple but may be less expressive than normalizing flows.
  - Stratified sampling balances dataset representation but adds complexity.

- **Failure signatures:**
  - Poor zero-shot performance: insufficient diversity in pretraining data or mismatched frequencies.
  - Unstable scaling: overfitting on small datasets or hyperparameter sensitivity.
  - Calibration issues: distribution head parameters not well-regularized.

- **First 3 experiments:**
  1. Verify lag feature construction correctly captures known periodicities on synthetic data.
  2. Test zero-shot performance on a held-out dataset from a domain present in pretraining corpus.
  3. Scale model size incrementally and confirm CRPS improves following fitted scaling law.

## Open Questions the Paper Calls Out
- **Open Question 1:** How do different data sampling strategies affect the zero-shot performance of Lag-Llama on unseen datasets?
- **Open Question 2:** What is the optimal model architecture for foundation models in time series forecasting, considering both transformer-based and alternative approaches?
- **Open Question 3:** How does the choice of distribution head affect the performance of Lag-Llama in probabilistic time series forecasting?

## Limitations
- Lack of direct evidence for lag-based transfer learning from the corpus
- Scaling law extrapolation relies on unverified assumptions about performance trends
- Model's performance on datasets with irregular patterns or non-periodic dynamics not evaluated

## Confidence
- **High Confidence:** Mechanism 2 (Pretraining on diverse corpus enables transfer) - supported by strong zero-shot results on Traffic and M4 Weekly datasets.
- **Medium Confidence:** Mechanism 1 (Lag features preserve temporal structure) - the design is theoretically sound but lacks direct validation in the corpus.
- **Low Confidence:** Mechanism 3 (Scaling law predictions) - the extrapolation relies on unverified assumptions about performance trends.

## Next Checks
1. **Lag Feature Ablation:** Evaluate the model's performance with and without lag features on synthetic periodic and non-periodic datasets to quantify their contribution to forecasting accuracy.
2. **Scaling Law Verification:** Train models at parameter sizes beyond the current range and compare their performance against the extrapolated scaling law predictions to test the validity of the power-law assumption.
3. **Irregular Pattern Robustness:** Test the model on datasets with abrupt regime shifts or irregular temporal patterns (e.g., financial crisis data) to assess the limitations of the lag-based approach in capturing non-periodic dynamics.