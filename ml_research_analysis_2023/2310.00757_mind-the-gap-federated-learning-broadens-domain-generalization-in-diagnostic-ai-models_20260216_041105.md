---
ver: rpa2
title: 'Mind the Gap: Federated Learning Broadens Domain Generalization in Diagnostic
  AI Models'
arxiv_id: '2310.00757'
source_url: https://arxiv.org/abs/2310.00757
tags:
- training
- performance
- datasets
- radiographs
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the impact of federated learning (FL) on the
  domain generalization of AI models for chest radiograph interpretation. Using 610,000
  radiographs from five global datasets, the researchers compared local and collaborative
  training strategies across convolutional and transformer-based architectures.
---

# Mind the Gap: Federated Learning Broadens Domain Generalization in Diagnostic AI Models

## Quick Facts
- arXiv ID: 2310.00757
- Source URL: https://arxiv.org/abs/2310.00757
- Reference count: 40
- Primary result: Federated learning significantly improves off-domain performance for chest radiograph interpretation while primarily benefiting smaller datasets for on-domain tasks.

## Executive Summary
This study investigates how federated learning affects the domain generalization of AI models for chest radiograph interpretation. Using 610,000 radiographs from five global datasets, researchers compared local and collaborative training strategies across convolutional and transformer-based architectures. The results demonstrate that federated learning significantly enhances off-domain performance, particularly for smaller datasets, while on-domain performance is primarily driven by training data size. The study reveals that collaborative training consistently outperforms local training in off-domain tasks, highlighting federated learning's potential to enhance diagnostic reliability while maintaining privacy through data decentralization.

## Method Summary
The study employed 612,444 chest radiographs from five datasets (VinDr-CXR, ChestX-ray14, CheXpert, MIMIC-CXR, PadChest) with 7 imaging findings per image. Researchers implemented local training on individual datasets and federated learning using the FedAvg algorithm, comparing ResNet50 and ViT architectures. Models were evaluated using Area Under the ROC Curve (AUROC) for both on-domain performance (test on data from training institutions) and off-domain performance (test on data from institutions not involved in training). Data preprocessing included image resizing, normalization, and histogram equalization, while training used binary weighted cross-entropy loss with data augmentation.

## Key Results
- Federated learning significantly improves off-domain performance by increasing dataset diversity
- Off-domain performance is influenced by training data diversity more than dataset size
- Large datasets show minimal performance gains with FL for on-domain tasks, while smaller datasets show marked improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Federated learning improves off-domain performance by increasing dataset diversity.
- Mechanism: Collaborative training aggregates models from multiple institutions, each with distinct data distributions. This exposure to diverse domains during training enhances the model's ability to generalize to unseen datasets.
- Core assumption: Dataset diversity is a stronger predictor of off-domain performance than dataset size alone.
- Evidence anchors:
  - [abstract] "off-domain performance leaned more on training diversity"
  - [section] "off-domain performance is influenced by the diversity of the training data"
  - [corpus] No direct corpus evidence for this specific mechanism.
- Break condition: If training data diversity is artificially reduced (e.g., all sites use similar protocols), collaborative training would not outperform local training for off-domain tasks.

### Mechanism 2
- Claim: Federated learning's impact on on-domain performance depends on dataset size.
- Mechanism: Large datasets already contain sufficient variability for good on-domain performance, so collaborative training provides minimal benefit. Smaller datasets gain significantly from collaborative training due to increased effective sample size and variability.
- Core assumption: The marginal benefit of additional data decreases as dataset size increases.
- Evidence anchors:
  - [abstract] "Large datasets not only showed minimal performance gains with FL but, in some instances, even exhibited decreases"
  - [section] "training data size is the primary determinant of on-domain model performance following collaborative training in non-IID data settings"
  - [corpus] No direct corpus evidence for this specific mechanism.
- Break condition: If a large dataset is highly homogeneous, collaborative training might still provide benefits by introducing variability.

### Mechanism 3
- Claim: Federated learning maintains its performance advantage across different network architectures.
- Mechanism: Both convolutional (ResNet50) and transformer (ViT) architectures benefit similarly from collaborative training, indicating the effect is not architecture-specific but rather due to the training strategy and data diversity.
- Core assumption: The benefits of federated learning are independent of the underlying model architecture.
- Evidence anchors:
  - [abstract] "assessed the potential influence of the underlying architecture"
  - [section] "both architectures displayed comparable performance in interpreting chest radiographs"
  - [corpus] No direct corpus evidence for this specific mechanism.
- Break condition: If architectural differences in handling diverse data distributions become significant, one architecture might outperform the other in federated settings.

## Foundational Learning

- Concept: Federated Learning
  - Why needed here: The study relies on federated learning to train models across multiple institutions without data sharing, addressing privacy concerns while improving generalization.
  - Quick check question: What is the primary privacy benefit of federated learning compared to centralized training?

- Concept: Domain Generalization
  - Why needed here: The study examines how well models trained on data from certain institutions perform on data from unseen institutions, which is critical for real-world deployment.
  - Quick check question: How does off-domain performance differ from on-domain performance in the context of this study?

- Concept: Area Under the Receiver Operating Characteristic Curve (AUROC)
  - Why needed here: AUROC is the primary metric used to evaluate model performance across different imaging findings and datasets.
  - Quick check question: Why might AUROC be preferred over accuracy when evaluating medical diagnostic models?

## Architecture Onboarding

- Component map:
  - Data Preprocessing -> Model Training (Local/Federated) -> On-Domain Evaluation -> Off-Domain Evaluation

- Critical path:
  1. Preprocess data consistently across all datasets
  2. Train local models on individual datasets
  3. Implement federated learning across all datasets
  4. Evaluate on-domain performance (test on data from training institutions)
  5. Evaluate off-domain performance (test on data from institutions not involved in training)

- Design tradeoffs:
  - Local training vs. federated learning: Privacy vs. potential performance gains
  - Dataset size: Larger datasets provide better on-domain performance but may offer diminishing returns in federated settings
  - Architecture choice: ResNet50 vs. ViT - similar performance but different computational requirements

- Failure signatures:
  - Poor off-domain performance despite good on-domain performance: Indicates overfitting to training domain
  - Decreased performance in federated learning for large datasets: Suggests that additional data diversity doesn't compensate for the communication overhead
  - Inconsistent results across imaging findings: May indicate issues with label quality or dataset imbalance

- First 3 experiments:
  1. Replicate on-domain evaluation comparing local vs. federated learning on a single dataset
  2. Test off-domain performance by training on one dataset and evaluating on another
  3. Implement federated learning across multiple datasets and evaluate both on-domain and off-domain performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different non-IID data distributions beyond the ones studied affect federated learning performance for diagnostic AI models?
- Basis in paper: [explicit] The paper acknowledges that FL faces challenges including independent and identically distributed (IID) versus non-IID data distributions and variations in image acquisition, processing, and labeling.
- Why unresolved: The study primarily examined non-IID conditions arising from institutional differences in patient demographics, labeling methods, and clinical settings, but did not systematically explore how different types of data heterogeneity affect performance.
- What evidence would resolve it: Controlled experiments varying specific types of data heterogeneity (e.g., label noise, acquisition protocols, demographic shifts) while keeping other factors constant would clarify which non-IID characteristics most impact FL performance.

### Open Question 2
- Question: What is the optimal dataset size threshold where federated learning stops providing performance benefits for on-domain tasks?
- Basis in paper: [inferred] The paper found that large datasets (MIMIC-CXR with 170,153 radiographs and CheXpert with 128,356 radiographs) showed minimal performance gains with FL, while smaller datasets (VinDr-CXR with 15,000 radiographs) showed marked improvements.
- Why unresolved: The study compared datasets with large size differences but did not identify a specific threshold or relationship between dataset size and FL benefit magnitude.
- What evidence would resolve it: Systematic experiments with datasets of progressively increasing sizes and detailed analysis of performance curves would reveal the inflection point where FL benefits plateau.

### Open Question 3
- Question: How does federated learning affect diagnostic performance for imaging findings with very low prevalence rates beyond those studied?
- Basis in paper: [explicit] The paper notes that diagnostic performance for pneumonia, atelectasis, and consolidation did not benefit from larger datasets, which is surprising given their relatively low prevalence rates across datasets.
- Why unresolved: The study examined specific prevalence ranges (1.3-6.5% for pneumonia, 0.8-19.9% for atelectasis, 1.2-6.0% for consolidation) but did not explore how FL performs for extremely rare findings or whether there is a prevalence threshold below which FL becomes ineffective.
- What evidence would resolve it: Experiments using datasets with imaging findings of varying and particularly low prevalence rates, coupled with statistical analysis of performance trends, would clarify FL's effectiveness for rare conditions.

## Limitations

- The study did not account for potential confounding factors such as patient demographics, disease prevalence, or imaging protocols that could influence generalization performance.
- The choice of FedAvg as the federated learning algorithm may not represent the optimal approach for this specific task.
- The analysis focuses on a specific set of imaging findings (14 total) and may not generalize to all radiological tasks.

## Confidence

- High Confidence: The finding that federated learning improves off-domain performance through increased data diversity is well-supported by the experimental results across multiple datasets and architectures.
- Medium Confidence: The claim that on-domain performance is primarily driven by training data size is supported but may be influenced by unaccounted confounding factors.
- Medium Confidence: The assertion that federated learning benefits are architecture-independent requires further validation with additional model architectures.

## Next Checks

1. Conduct ablation studies to isolate the impact of dataset diversity from other factors (demographics, protocols) on generalization performance.
2. Test alternative federated learning algorithms (e.g., FedProx, SCAFFOLD) to verify that the observed benefits are not specific to FedAvg.
3. Evaluate the models on additional imaging findings and radiological tasks to assess generalizability beyond the 14 findings studied.