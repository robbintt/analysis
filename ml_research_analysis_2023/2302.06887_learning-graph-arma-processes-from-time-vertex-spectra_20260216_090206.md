---
ver: rpa2
title: Learning Graph ARMA Processes from Time-Vertex Spectra
arxiv_id: '2302.06887'
source_url: https://arxiv.org/abs/2302.06887
tags:
- process
- graph
- estimation
- jpsd
- signal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to learn parametric graph ARMA (autoregressive
  moving average) processes from time-vertex signals with missing observations. The
  key idea is to first estimate the joint time-vertex power spectral density from
  the incomplete data, then refine this estimate by fitting it to an ARMA model using
  convex optimization.
---

# Learning Graph ARMA Processes from Time-Vertex Spectra

## Quick Facts
- **arXiv ID**: 2302.06887
- **Source URL**: https://arxiv.org/abs/2302.06887
- **Reference count**: 33
- **Primary result**: Proposes a method to learn graph ARMA processes from incomplete time-vertex signals by estimating the joint time-vertex power spectral density (JPSD) and refining it via convex optimization, achieving O(1/√L) convergence for both JPSD estimation and missing value recovery.

## Executive Summary
This paper introduces a novel approach for learning parametric graph ARMA (autoregressive moving average) processes from time-vertex signals with missing observations. The method first estimates the joint time-vertex power spectral density (JPSD) from incomplete data using sample covariance, then refines this estimate by fitting an ARMA model through a convex optimization relaxation. The learned model enables minimum mean square error (MMSE) estimation of missing signal values. The authors provide theoretical analysis showing the estimation error decreases at rate O(1/√L) with the number of realizations, and demonstrate through experiments on real datasets that their approach outperforms reference methods for estimating missing observations in time-vertex signals.

## Method Summary
The proposed method learns graph ARMA processes from incomplete time-vertex signals by first estimating the joint time-vertex power spectral density (JPSD) from partially observed realizations using sample covariance matrix. It then refines this initial JPSD estimate by fitting an ARMA model to it through a convex optimization problem that relaxes the original non-convex rank constraints. The ARMA parameters are extracted from the solution of this convex problem, and the refined JPSD is used to estimate the covariance matrix of the process. Finally, missing observations are estimated using MMSE estimation based on the learnt model and covariance matrix. The method leverages the joint time-vertex spectrum to capture dependencies across both graph nodes and time instants, providing richer correlation patterns than approaches that treat data as separate time series or use only vertex-based frequency analysis.

## Key Results
- The proposed method achieves O(1/√L) convergence rate for both JPSD estimation and missing observation recovery as the number of realizations L increases.
- Experimental results on real datasets (Molène weather data and NOAA temperature data) show the proposed approach outperforms reference methods (G-VAR, JWSS, AR, VAR) in estimating missing observations.
- The method effectively leverages the joint time-vertex spectrum, demonstrating that approaches based on graph process models perform better than those treating data as individual time series without utilizing graph topology information.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learning graph ARMA processes from incomplete time-vertex signals is feasible by estimating the joint power spectral density (JPSD) and refining it with a convex relaxation.
- **Mechanism:** The algorithm first obtains a rough JPSD estimate from partially observed realizations via sample covariance. It then fits an ARMA model to this estimate by solving a convex optimization problem that relaxes the original non-convex rank constraints into trace minimization.
- **Core assumption:** The JPSD manifold has finite curvature and non-zero tangents, allowing the projection of the initial JPSD estimate onto the ARMA model manifold to reduce estimation error.
- **Evidence anchors:**
  - [abstract] The key idea is to first estimate the joint time-vertex power spectral density from the incomplete data, then refine this estimate by fitting it to an ARMA model using convex optimization.
  - [section IV-B] We relax it into a convex optimization problem through a series of approximations, which can then be solved accurately via semidefinite quadratic linear programming.
  - [corpus] Weak - corpus neighbors do not directly address the specific convex relaxation approach.
- **Break condition:** If the process realizations are too sparse or the noise level is extremely high, the initial JPSD estimate becomes unreliable, and the convex relaxation may fail to recover the true ARMA parameters.

### Mechanism 2
- **Claim:** The proposed method achieves convergence at rate O(1/√L) for both JPSD estimation and missing observation recovery as the number of realizations L increases.
- **Mechanism:** The sample covariance of L independent realizations provides an initial JPSD estimate with error decreasing as 1/√L. The ARMA model fitting improves this estimate, and the resulting model enables minimum mean square error (MMSE) estimation of missing observations.
- **Core assumption:** The number of process dimensions (NT) exceeds the model order d, ensuring the existence of positive lower bounds on tangent norms of the JPSD manifold.
- **Evidence anchors:**
  - [section V] We first show that the JPSD estimation error decreases at rate O(1/√L) as the number of realizations L increases.
  - [section V] Then, we study the implications of this result on the estimation error of the missing observations of the process and show that the convergence rate of O(1/√L) holds also for this case.
  - [corpus] Weak - corpus neighbors do not provide evidence for the specific convergence rate analysis.
- **Break condition:** If NT ≤ d (process dimensions not sufficiently larger than model order), the geometric assumptions fail, and the convergence rate may degrade.

### Mechanism 3
- **Claim:** The proposed JS-ARMA method outperforms reference approaches by explicitly leveraging the joint time-vertex spectrum rather than only vertex or time stationarity.
- **Mechanism:** By modeling dependencies across both graph nodes and time instants through the joint spectrum, the method captures richer correlation patterns than approaches that treat the data as separate time series or use only vertex-based frequency analysis.
- **Core assumption:** Real graph processes exhibit significant joint time-vertex correlation that is not captured by purely vertex-based or time-based methods.
- **Evidence anchors:**
  - [abstract] The initially missing signal values are then estimated based on the learnt model. Experimental results show that the proposed approach achieves high accuracy in time-vertex signal estimation problems.
  - [section VI-B] methods based on graph process models perform better than the VAR and AR models most of the time, which treat the data as individual time series and do not make use of the information of the graph topology.
  - [corpus] Weak - corpus neighbors do not directly compare joint time-vertex spectrum methods with alternatives.
- **Break condition:** If the process has negligible joint time-vertex correlation (e.g., independent across time), the additional complexity of joint spectrum modeling provides no benefit.

## Foundational Learning

- **Concept:** Graph Signal Processing (GSP)
  - Why needed here: The method operates on graph domains, requiring understanding of graph Fourier transforms, graph filters, and graph Laplacian operators.
  - Quick check question: How does the graph Fourier transform differ from the classical DFT, and why is it useful for irregular graph structures?

- **Concept:** Stationary Stochastic Processes
  - Why needed here: The method assumes the time-vertex signal follows a stationary graph process, requiring knowledge of wide-sense stationarity and power spectral density concepts.
  - Quick check question: What conditions must a random graph signal satisfy to be considered wide-sense stationary?

- **Concept:** Convex Optimization and Semidefinite Programming
  - Why needed here: The ARMA parameter estimation problem is solved via a convex relaxation using semidefinite quadratic linear programming.
  - Quick check question: Why is the original ARMA learning problem non-convex, and how does trace minimization help enforce low-rank solutions?

## Architecture Onboarding

- **Component map:** Data Input -> JPSD Estimation -> Model Learning -> Missing Value Recovery -> Output
- **Critical path:**
  1. Compute initial JPSD from sample covariance
  2. Solve convex optimization to learn ARMA parameters
  3. Compute MMSE estimates of missing observations
  - Each step depends on successful completion of the previous one.

- **Design tradeoffs:**
  - Parametric vs. non-parametric modeling: Parametric ARMA models require fewer parameters but may underfit complex processes; non-parametric methods are more flexible but need more data.
  - Convex relaxation vs. exact solution: Convex relaxation enables tractable computation but may introduce approximation error compared to solving the original non-convex problem.

- **Failure signatures:**
  - Poor JPSD estimation: High normalized mean error (NME) that doesn't decrease with more realizations
  - Model learning failure: ARMA parameters that don't converge or produce unrealistic JPSD estimates
  - Missing value recovery issues: MMSE estimates that are highly sensitive to noise or missing ratio

- **First 3 experiments:**
  1. Generate synthetic ARMA graph process with known parameters; vary missing ratio and noise level; measure NME of missing value estimates.
  2. Use real weather data (Molène or NOAA); compare JS-ARMA with JWSS, G-VAR, and VAR methods across different missing ratios.
  3. Vary model order (P,K,Q,M) on synthetic data; analyze how estimation accuracy changes with model complexity and number of realizations.

## Open Questions the Paper Calls Out

- **Question:** How does the proposed JS-ARMA method's performance scale when the graph topology changes over time (dynamic graphs)?
- **Basis in paper:** [inferred] The paper focuses on stationary time-vertex signals and fixed graph topologies. Dynamic graphs are mentioned as a possible future direction but not explored.
- **Why unresolved:** The current method assumes a fixed graph Laplacian for filter design and spectrum estimation. Extending this to dynamic graphs would require handling time-varying graph structures and their impact on stationarity assumptions.
- **What evidence would resolve it:** Experiments comparing JS-ARMA on static vs. dynamic graph topologies, showing performance degradation/gains, would demonstrate feasibility and needed modifications.

- **Question:** What are the theoretical bounds on the error introduced by the convex relaxation in the ARMA parameter estimation?
- **Basis in paper:** [explicit] The paper acknowledges that the original ARMA estimation problem is non-convex and uses convex relaxations, but does not provide theoretical bounds on the approximation error introduced.
- **Why unresolved:** The authors provide convergence rates for the overall JPSD estimation but do not analyze how the convex relaxation affects the final ARMA parameter accuracy.
- **What evidence would resolve it:** Derivation of bounds comparing the relaxed solution to the true non-convex optimum, or empirical studies showing the gap between relaxed and exact solutions across different problem instances.

- **Question:** How does the algorithm perform on extremely large graphs where N and T are in the millions?
- **Basis in paper:** [inferred] The complexity analysis mentions O(poly(NT)) but does not provide concrete scalability results for very large-scale graphs.
- **Why unresolved:** While the theoretical analysis suggests polynomial complexity, practical memory and computational constraints for massive graphs are not addressed.
- **What evidence would resolve it:** Scalability experiments on progressively larger graph datasets, demonstrating runtime, memory usage, and estimation accuracy trade-offs, would quantify practical limits.

## Limitations

- The convex relaxation may fail to recover true ARMA parameters when the initial JPSD estimate is poor due to sparse observations or high noise levels.
- The exact weight parameters for the convex optimization and implementation details for the rank-1 decomposition step are not fully specified, potentially affecting reproducibility.
- Experiments focus on relatively small graphs (up to ~140 nodes) and specific datasets, raising questions about scalability to larger networks.

## Confidence

- **High confidence**: The overall framework of JPSD estimation followed by convex optimization for ARMA learning is well-established in the GSP literature. The theoretical analysis of O(1/√L) convergence rates for JPSD estimation follows standard statistical learning principles.
- **Medium confidence**: The experimental results showing JS-ARMA outperforming reference methods are compelling but based on a limited set of datasets. The superiority of joint time-vertex spectrum modeling over vertex-only or time-only approaches is demonstrated but not extensively validated across diverse process types.
- **Low confidence**: The specific geometric assumptions about the JPSD manifold (finite curvature, non-zero tangents) and their implications for the projection step are not fully validated empirically. The conditions under which the convex relaxation becomes tight enough for accurate parameter recovery are not thoroughly explored.

## Next Checks

1. **Convergence Rate Validation**: Conduct synthetic experiments with varying numbers of realizations L (from very small to large) to empirically verify the O(1/√L) convergence rate for both JPSD estimation and missing value recovery. Plot estimation error versus 1/√L to confirm linear relationship.

2. **Convex Relaxation Tightness**: Systematically vary the number of observations, noise levels, and model orders to identify when the convex relaxation fails to recover true parameters. Compare the JPSD produced by the learned ARMA model against the initial estimate to quantify approximation error.

3. **Scalability Analysis**: Apply the method to larger graph datasets (e.g., social networks with thousands of nodes) to evaluate computational complexity and estimation accuracy. Measure runtime and memory requirements as functions of graph size and time duration.