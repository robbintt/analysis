---
ver: rpa2
title: Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time
  Intervention
arxiv_id: '2312.15033'
source_url: https://arxiv.org/abs/2312.15033
tags:
- concept
- llms
- task
- language
- backbone
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SparseCBM, a sparsity-guided method to interpret
  LLMs at token, subnetwork, and concept levels. It decomposes LLMs into concept-specific
  subnetworks via unstructured pruning, enabling multidimensional explanations and
  efficient inference-time intervention.
---

# Sparsity-Guided Holistic Explanation for LLMs with Interpretable Inference-Time Intervention

## Quick Facts
- arXiv ID: 2312.15033
- Source URL: https://arxiv.org/abs/2312.15033
- Reference count: 12
- One-line primary result: SparseCBM achieves interpretable LLM explanations via sparsity-guided pruning and inference-time intervention, matching or exceeding standard baselines.

## Executive Summary
This paper introduces SparseCBM, a sparsity-guided method to interpret large language models (LLMs) at token, subnetwork, and concept levels. It decomposes LLMs into concept-specific subnetworks using unstructured pruning, enabling multidimensional explanations and efficient inference-time intervention. Experiments demonstrate that SparseCBM matches or exceeds standard and CBM baselines in accuracy while providing interpretable decision pathways. Results indicate that larger LLMs can tolerate higher sparsity without performance loss, making the approach scalable and practical.

## Method Summary
SparseCBM integrates sparsity into Concept Bottleneck Models (CBMs) to create interpretable, concept-specific subnetworks within LLMs. It uses unstructured pruning guided by concept and task labels to isolate subnetworks, enabling efficient inference-time intervention. During inference, if a concept is mispredicted, the corresponding subnetwork mask is adjusted by dropping low-saliency weights and growing high-saliency ones, correcting the error without retraining the entire model.

## Key Results
- SparseCBM matches or exceeds standard and CBM baselines in concept and task label prediction accuracy.
- Larger LLMs can tolerate higher sparsity levels without performance loss.
- Sparsity-based inference-time intervention improves accuracy by adjusting concept-specific masks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unstructured pruning isolates concept-specific subnetworks within LLMs without severely degrading performance.
- Mechanism: Second-order unstructured pruning (using Hessian/Fisher approximation) removes weights minimally impacting concept prediction loss, leaving sparse subnetworks for each concept.
- Core assumption: Concept prediction is a lower-dimensional task than full language modeling, so LLMs contain redundant parameters.
- Evidence anchors: Abstract and section mention sparsity-guided interpretation and pruning for concept tasks.
- Break condition: If concept prediction requires highly entangled parameters, pruning may destroy necessary interactions.

### Mechanism 2
- Claim: SparseCBMs enable interpretable inference-time intervention by dynamic mask adjustment for mispredicted concepts.
- Mechanism: During inference, incorrect concept predictions trigger mask fine-tuning by dropping low-saliency and growing high-saliency weights.
- Core assumption: Mask-level intervention can locally correct concept predictions without destabilizing the overall network.
- Evidence anchors: Abstract and section describe sparsity-based inference-time intervention.
- Break condition: If mask adjustment introduces instability or saliency scores are unreliable, intervention could worsen predictions.

### Mechanism 3
- Claim: Larger LLMs can tolerate higher sparsity levels without performance loss due to greater parameter redundancy.
- Mechanism: More parameters in larger models allow each subnetwork to remain expressive even under aggressive pruning.
- Core assumption: Knowledge is redundantly encoded in larger models, so pruning a fraction of parameters per concept does not remove critical information.
- Evidence anchors: Abstract and section discuss sparsity tolerance in relation to model scale.
- Break condition: If larger models are not as redundant as assumed, performance may degrade despite model size.

## Foundational Learning

- Concept: Concept Bottleneck Models (CBMs)
  - Why needed here: SparseCBMs build on CBMs by adding sparsity and intervention mechanisms; understanding CBMs is essential to grasp how concepts are injected into the model pipeline.
  - Quick check question: In a CBM, what is the role of the concept layer between the LLM encoder and the task label predictor?

- Concept: Unstructured pruning and weight importance
  - Why needed here: The core innovation relies on pruning individual weights based on their importance to concept prediction, so familiarity with pruning strategies and saliency metrics is critical.
  - Quick check question: How does second-order pruning (using the Hessian or Fisher matrix) differ from magnitude-based pruning in selecting which weights to remove?

- Concept: Inference-time model adjustment
  - Why needed here: SparseCBMs introduce a novel way to correct predictions during inference by modifying masks; understanding this mechanism is key to appreciating the practical benefits.
  - Quick check question: What is the difference between oracle intervention (manual concept adjustment) and the sparsity-based intervention proposed in SparseCBMs?

## Architecture Onboarding

- Component map: Input text -> LLM backbone (with sparse masks) -> Projector layer -> Concept predictions -> Linear classifier -> Task label; Mask adjustment module for intervention.

- Critical path: 1) Input text → LLM backbone (with sparse masks) → concept predictions; 2) Concept predictions → linear classifier → task label; 3) If concept mispredicted → adjust mask → re-run path for corrected output.

- Design tradeoffs: Sparsity vs. performance (higher sparsity improves interpretability but risks accuracy loss); Granularity of masks (finer masks yield more precise subnetworks but increase complexity); Intervention frequency (frequent updates can correct errors but may overfit to test data).

- Failure signatures: High concept prediction error after pruning (subnetworks too sparse); Task label accuracy drops more than concept accuracy (misaligned subnetworks); Intervention causes oscillating predictions (unstable mask updates); Excessive overlap between subnetworks (poor concept disentanglement).

- First 3 experiments: 1) Verify unstructured pruning isolates distinct concept subnetworks by visualizing masks and measuring concept prediction accuracy; 2) Test sparsity-based intervention on a small dataset with known concept errors and measure correction rate; 3) Sweep sparsity levels across different model sizes and record both concept and task performance to find the optimal tradeoff.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of SparseCBMs scale with increasing concept complexity and diversity in the dataset?
- Basis in paper: [explicit] The paper mentions that SparseCBMs provide multidimensional interpretability without compromising task prediction performance and discusses scalability with larger LLM backbones.
- Why unresolved: While the paper shows that SparseCBMs can match or outperform their dense counterparts, it doesn't explicitly explore the impact of varying concept complexity and diversity on performance.
- What evidence would resolve it: Experimental results comparing SparseCBM performance across datasets with varying concept complexity and diversity, analyzing both interpretability and utility metrics.

### Open Question 2
- Question: What is the impact of different pruning strategies (e.g., structured vs. unstructured) on the interpretability and performance of SparseCBMs?
- Basis in paper: [inferred] The paper focuses on unstructured pruning for its interpretability benefits but acknowledges that other pruning strategies exist in the literature.
- Why unresolved: The paper doesn't compare the effects of different pruning strategies on SparseCBM's interpretability and performance.
- What evidence would resolve it: Comparative experiments using various pruning strategies within the SparseCBM framework, evaluating both interpretability and utility outcomes.

### Open Question 3
- Question: How does the sparsity-based inference-time intervention mechanism affect the long-term learning and adaptation of SparseCBMs?
- Basis in paper: [explicit] The paper introduces a sparsity-based inference-time intervention mechanism that improves accuracy without full retraining.
- Why unresolved: While the paper demonstrates short-term improvements in inference-time accuracy, it doesn't explore the long-term effects of this intervention on model learning and adaptation.
- What evidence would resolve it: Longitudinal studies tracking SparseCBM performance and interpretability over time with repeated use of the intervention mechanism.

## Limitations
- Limited generalization evidence: Validated only on two text classification datasets with human-annotated concepts; effectiveness on other modalities or tasks is untested.
- Sparse intervention mechanism stability: No analysis of intervention frequency or whether it introduces instability over multiple steps.
- Sparse subnetworks overlap: No quantitative measures of concept disentanglement (e.g., Jaccard similarity between subnetworks).

## Confidence
- High confidence: The core claim that sparsity-guided pruning can isolate concept-specific subnetworks is well-supported by experimental results and second-order pruning theory.
- Medium confidence: The claim that larger models tolerate higher sparsity is plausible but based on limited experiments; more systematic scaling studies would strengthen this.
- Medium confidence: The sparsity-based inference-time intervention is promising, but the lack of robustness analysis or ablation on intervention frequency reduces confidence in its practical utility.

## Next Checks
1. **Generalization to unseen concepts**: Evaluate SparseCBM on a dataset with concepts not present during training to test if subnetworks truly capture concept-specific features rather than memorizing dataset-specific patterns.
2. **Intervention stability test**: Measure the effect of repeated inference-time mask adjustments on the same input, checking for convergence, oscillation, or degradation in prediction accuracy.
3. **Concept disentanglement quantification**: Compute pairwise overlap metrics (e.g., Jaccard index) between subnetworks for different concepts and correlate with concept prediction error to assess how sparsity relates to interpretability.