---
ver: rpa2
title: 'Machine Translation for Nko: Tools, Corpora and Baseline Results'
arxiv_id: '2310.15612'
source_url: https://arxiv.org/abs/2310.15612
tags:
- parallel
- translation
- latn
- diane
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents Fria\xE9l, a collaborative parallel text curation\
  \ software, and related resources for developing machine translation (MT) systems\
  \ for Nko, a Manding language spoken by millions in West Africa. Fria\xE9l incorporates\
  \ quality control through copyedit-based workflows and supports multiple languages\
  \ and writing systems."
---

# Machine Translation for Nko: Tools, Corpora and Baseline Results

## Quick Facts
- arXiv ID: 2310.15612
- Source URL: https://arxiv.org/abs/2310.15612
- Reference count: 40
- Key outcome: Friaël software and corpora enable Nko MT with 30.83 chrF++ baseline

## Executive Summary
This paper presents Friaël, a collaborative parallel text curation software, and related resources for developing machine translation (MT) systems for Nko, a Manding language spoken by millions in West Africa. Friaël incorporates quality control through copyedit-based workflows and supports multiple languages and writing systems. The authors extend existing multilingual corpora (FLoRes-200 and NLLB-Seed) with 2,009 and 6,193 high-quality Nko translations, and introduce nicolingua-0005, a trilingual and bilingual corpus with 130,850 parallel segments and monolingual data of over 3 million Nko words. Baseline MT experiments using Transformer-based models achieve modest results, with the best model scoring 30.83 chrF++ on English-Nko translation. The work highlights challenges in MT for low-resource languages and provides open-sourced tools to support further development.

## Method Summary
The research employs a multi-faceted approach combining collaborative software development, corpus curation, and neural machine translation. Friaël, a web-based collaborative translation tool, enables multi-pass copyediting workflows with offline resilience. The authors extend existing multilingual corpora (FLoRes-200 and NLLB-Seed) with Nko translations through this system, and create a new community-sourced corpus (nicolingua-0005). For MT experiments, they train seven Transformer-based models using fairseq, exploring bilingual and multilingual configurations with different data sources and language token prefixing strategies. The models are evaluated using chrF++ and BLEU metrics on standard test sets.

## Key Results
- Friaël enables quality-controlled parallel corpus creation for Nko with 2,009 and 6,193 high-quality translations
- nicolingua-0005 corpus provides 130,850 parallel segments and over 3 million monolingual Nko words
- Best Transformer model achieves 30.83 chrF++ on English-Nko translation (FLoRes-devtest)
- Multilingual models show promise but bilingual models remain competitive for this language pair

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Friaël's collaborative workflow with sequential copyediting improves translation quality compared to traditional one-pass translation.
- Mechanism: Each segment undergoes translation followed by 2-3 verification rounds where different translators can edit previous versions, creating an iterative quality control loop.
- Core assumption: Different translators catch different types of errors, and having multiple reviewers reduces individual bias.
- Evidence anchors:
  - [abstract] "Friaël: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows."
  - [section 2.4.3] "Nko translators found Friaël's multi-pass copy-editing process effective for finding and correcting translation mistakes."
  - [section 3.3] The translation workflow explicitly shows v1 → v2 → v3 → v4 stages.
- Break condition: If translators consistently make the same types of errors or if the translation quality plateaus after the first verification round.

### Mechanism 2
- Claim: Multilingual source inspection enables better context for translation compared to single-language source.
- Mechanism: Translators can simultaneously view the source segment in multiple languages, allowing them to disambiguate meaning and choose more accurate translations.
- Core assumption: Different languages express the same concept differently, and seeing multiple perspectives helps resolve ambiguity.
- Evidence anchors:
  - [abstract] "translators can simultaneously inspect segments in several languages configured according to their preferences."
  - [section 2.4.2] "Nko translators found the fact that source segments were visible in multiple languages beneficial."
  - [section 3.6] Translators noted that "word sense was sometimes hard to disambiguate without the full context of segments."
- Break condition: If translators find the multiple language views confusing or if the source languages are too dissimilar to provide useful context.

### Mechanism 3
- Claim: The offline resilience mechanism enables continuous work despite connectivity disruptions.
- Mechanism: The system uses ServiceWorker and CacheStorage to cache resources locally, allowing translators to continue working when internet connection is lost.
- Core assumption: Connectivity disruptions are common in the target deployment regions, and uninterrupted work improves productivity.
- Evidence anchors:
  - [abstract] "Resilience to Connectivity Disruptions Translators who temporarily lose their internet connectivity can seamlessly keep working offline"
  - [section 2.3.5] "The software is a web application designed to be resilient to intermittent internet disruptions."
  - [section 2.4.1] Translators "appreciated the offline functionality that allowed them to temporarily continue working without an internet connection."
- Break condition: If the cached resources become stale or if the synchronization process fails when connectivity is restored.

## Foundational Learning

- Concept: Parallel corpus alignment and quality control workflows
  - Why needed here: Understanding how to build high-quality parallel corpora is fundamental to this work's contribution
  - Quick check question: What is the difference between a bi-text corpus and a multi-text corpus?

- Concept: Transformer-based neural machine translation architecture
  - Why needed here: The baseline experiments use Transformer models, so understanding their architecture is essential
  - Quick check question: What are the key components of a Transformer encoder-decoder architecture?

- Concept: Subword tokenization and Byte Pair Encoding (BPE)
  - Why needed here: The models use BPE for tokenization, which affects vocabulary size and translation quality
  - Quick check question: How does BPE help handle rare words in machine translation?

## Architecture Onboarding

- Component map: Friaël -> common-parallel-corpora -> nicolingua-0005 -> Baseline NMT models
- Critical path: Data collection → Friaël curation → Corpus alignment → Model training → Evaluation
- Design tradeoffs: Quality vs. quantity in corpus creation, complexity of multilingual alignment vs. translation accuracy
- Failure signatures: Low chrF++ scores despite large corpus size, persistent mistranslations in certain domains, offline mode synchronization failures
- First 3 experiments:
  1. Train bilingual model on CPC data only (no monolingual autoencoding)
  2. Train multilingual model with language token prefixing strategy
  3. Train model with monolingual Nko data for autoencoding task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic challenges in Nko make machine translation particularly difficult compared to other languages?
- Basis in paper: [explicit] The paper mentions challenges like tone marking, nasalization, vowel length, and the need for detransliteration from Bambara Latin script to Nko script, but does not provide detailed analysis of how these specifically impact MT performance.
- Why unresolved: The paper presents baseline results but does not conduct a detailed error analysis to identify which linguistic features most affect translation quality.
- What evidence would resolve it: Detailed error analysis of MT outputs identifying specific linguistic phenomena that cause translation errors, and correlation with linguistic feature complexity.

### Open Question 2
- Question: How does the quality of Nko translations in FLoRes-200 and NLLB-Seed compare to other languages in these corpora?
- Basis in paper: [explicit] The authors note quality issues with Bambara Latin script data including excessive French vocabulary and lack of tonal marks, suggesting potential quality problems in these corpora.
- Why unresolved: The paper does not provide systematic quality evaluation or comparison of Nko data quality against other languages in these corpora.
- What evidence would resolve it: Systematic quality evaluation of Nko translations compared to other languages using metrics like inter-annotator agreement, translationese detection, or human evaluation against reference standards.

### Open Question 3
- Question: What is the optimal approach for handling the multilingual alignment of parallel corpora when source texts differ slightly between languages?
- Basis in paper: [explicit] The authors describe their method for creating Multitext-NLLB-SEED by matching lines based on edit distance minimization, but acknowledge this is one approach among many.
- Why unresolved: The paper does not compare this approach against alternatives or evaluate the impact of different alignment strategies on downstream MT performance.
- What evidence would resolve it: Comparative study of different alignment strategies (e.g., edit distance, semantic similarity, sentence embeddings) and their impact on MT quality across multiple language pairs.

## Limitations

- Modest translation quality (30.83 chrF++) indicates fundamental challenges remain for Nko MT despite corpus improvements
- Limited hyperparameter optimization across seven model variants misses opportunities for architectural improvements
- Generalizability of Friaël's quality control benefits beyond Nko remains unproven for other low-resource languages

## Confidence

**High Confidence:**
- The corpus quality improvement through Friaël's multi-pass copyediting workflow is well-supported by the described methodology and translator feedback
- The reported chrF++ scores and BLEU metrics are methodologically sound given the data splits and evaluation setup
- The technical implementation details for Transformer models and training procedures are sufficiently detailed for reproduction

**Medium Confidence:**
- The claim that multilingual source inspection improves translation quality is supported by translator feedback but lacks quantitative validation
- The offline resilience mechanism is technically sound but its real-world effectiveness is not extensively validated
- The extension of existing corpora (FLoRes-200 and NLLB-Seed) with Nko translations is methodologically appropriate but the selection criteria for these extensions could be more transparent

**Low Confidence:**
- The comparative advantage of Friaël over other collaborative translation tools is not extensively validated against alternatives
- The long-term sustainability of the offline-first approach in regions with improving connectivity is uncertain
- The impact of different language token prefixing strategies on multilingual model performance lacks systematic analysis

## Next Checks

1. **Corpus Quality Validation**: Conduct a blind human evaluation comparing translations produced using Friaël's multi-pass workflow against traditional single-pass translation methods on the same source texts. This would provide quantitative evidence for the claimed quality improvement mechanism.

2. **Architecture Sensitivity Analysis**: Systematically vary Transformer hyperparameters (attention heads, embedding dimensions, dropout rates) across the seven model variants to identify optimal configurations for Nko translation, rather than using nearly identical architectures with different data configurations.

3. **Offline Mode Stress Test**: Simulate extended offline periods with multiple concurrent translators working on the same segments, then measure synchronization conflicts, resolution times, and data consistency to validate the offline resilience claims under realistic conditions.