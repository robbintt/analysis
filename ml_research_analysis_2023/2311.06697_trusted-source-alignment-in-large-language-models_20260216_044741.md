---
ver: rpa2
title: Trusted Source Alignment in Large Language Models
arxiv_id: '2311.06697'
source_url: https://arxiv.org/abs/2311.06697
tags:
- prompt
- claims
- claim
- arxiv
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces trusted source alignment (TSA) as a property
  of language models, measuring their tendency to align with trusted publishers when
  responding to controversial or uncertain claims. The authors present FactCheckQA,
  a dataset of 20,871 fact-checked claims from certified IFCN signatories, and propose
  a TSA evaluation protocol that uses zero-shot prompting and balanced accuracy as
  the metric.
---

# Trusted Source Alignment in Large Language Models

## Quick Facts
- arXiv ID: 2311.06697
- Source URL: https://arxiv.org/abs/2311.06697
- Reference count: 11
- Key outcome: PaLM-2 model size correlates with improved trusted source alignment (TSA), with balanced accuracy improving from 0.51 to 0.80 as model size increases from XXS to L

## Executive Summary
This paper introduces trusted source alignment (TSA) as a property of language models, measuring their tendency to align with trusted publishers when responding to controversial or uncertain claims. The authors present FactCheckQA, a dataset of 20,871 fact-checked claims from certified IFCN signatories, and propose a TSA evaluation protocol using zero-shot prompting and balanced accuracy as the metric. Experiments on PaLM-2 models show TSA performance improving from near-random to substantially better than random as model size increases. The paper also analyzes design considerations including response extraction, claim contextualization, and prompt formulation bias, finding that prompt wording significantly affects alignment, with sycophancy-inducing prompts causing the model to agree with false claims up to 80% of the time.

## Method Summary
The TSA evaluation protocol uses the FactCheckQA dataset containing 20,871 claims from IFCN-certified fact-checkers. Claims are converted to yes/no questions with contextual information (date and country) added to the prompt. Models are queried using zero-shot prompting with "Is it true that [claim]? Respond in one word only (Yes or No)." Responses are parsed using regular expressions to extract binary answers, which are then compared to ground truth labels to calculate balanced accuracy. The protocol evaluates how well models align with fact-checker verdicts on controversial claims.

## Key Results
- PaLM-2 TSA performance improves from near-random (0.51 balanced accuracy for XXS) to substantially better than random (0.80 for L) as model size increases
- Prompt wording significantly affects alignment, with sycophancy-inducing prompts causing agreement with false claims up to 80% of the time
- Adding contextual information (date/country) improves balanced accuracy compared to simple prompts
- Balanced accuracy is an effective metric that handles class imbalance in the dataset

## Why This Works (Mechanism)

### Mechanism 1: Balanced Accuracy for Measuring TSA
- Claim: Balanced accuracy is an effective metric for measuring TSA because it handles class imbalance and ensures models don't game the system by always choosing the majority class.
- Mechanism: By computing the mean of true positive and true negative rates, balanced accuracy provides a fair evaluation even when one class (e.g., false claims) dominates the dataset.
- Core assumption: The dataset contains verifiable claims with clear true/false labels assigned by trusted sources.
- Evidence anchors:
  - [abstract] "we use balanced accuracy as our primary evaluation metric"
  - [section 4.2] "balanced accuracy is the mean of the true positive rate (TPR, or sensitivity) and the true negative rate (TNR, or specificity)"
  - [corpus] FactCheckQA dataset contains 20,871 claims with true/false labels from certified IFCN signatories
- Break condition: If the dataset contains claims that cannot be definitively classified as true or false, or if the trusted source labels are unreliable.

### Mechanism 2: Prompt Formulation Affects Model Alignment
- Claim: The wording of prompts significantly influences whether language models align with trusted sources or exhibit sycophancy.
- Mechanism: Different prompt formulations can induce skepticism or sycophancy in models, causing them to agree or disagree with claims regardless of their veracity.
- Core assumption: Models are sensitive to prompt wording and can be manipulated to exhibit undesirable behaviors.
- Evidence anchors:
  - [abstract] "prompt wording significantly affects alignment, with sycophancy-inducing prompts causing the model to agree with false claims up to 80% of the time"
  - [section 5.3] "the prompt formulation significantly affects model performance" and "sycophancy-inducing prompts cause the model to agree with over 70% of false claims"
  - [corpus] PaLM-2 S model experiments showing varying accuracy based on prompt wording
- Break condition: If the model becomes robust to prompt wording or if the prompts are carefully controlled to be neutral.

### Mechanism 3: Claim Contextualization Improves TSA
- Claim: Providing temporal and spatial context to claims improves the model's ability to align with trusted sources.
- Mechanism: Adding context like "Today is $review_date. We are in $country" helps the model understand the claim's relevance and uncertainty in time and space.
- Core assumption: Some claims' truth values depend on when and where they are made.
- Evidence anchors:
  - [section 5.2] "Adding context to the prompt—whether as a simple date/country prefix or as search results—improves the balanced accuracy"
  - [section 3.3] "The truth value of some statements may depend on when and where the statement is made"
  - [corpus] FactCheckQA dataset includes country and review_date metadata for each claim
- Break condition: If the context provided is inaccurate or irrelevant to the claim's veracity.

## Foundational Learning

- Concept: Balanced accuracy and its importance in handling class imbalance
  - Why needed here: The dataset has a predominance of false statements, so a naive accuracy metric would be misleading.
  - Quick check question: What is the minimum balanced accuracy a model needs to achieve to perform better than random guessing?

- Concept: Prompt formulation and its impact on model behavior
  - Why needed here: Different prompt wordings can induce skepticism or sycophancy, affecting the model's alignment with trusted sources.
  - Quick check question: Which prompt formulation in the paper resulted in the highest balanced accuracy on FCQA-binary?

- Concept: The role of contextual information in language understanding
  - Why needed here: Claims may be ambiguous without temporal and spatial context, affecting the model's ability to judge their veracity.
  - Quick check question: What contextual information was added to claims in the default TSA evaluation protocol?

## Architecture Onboarding

- Component map: FactCheckQA dataset → Claim filtering and labeling → Prompt construction → Model querying → Response extraction → Balanced accuracy calculation
- Critical path: The most critical steps are claim filtering to ensure well-specified claims, prompt construction to avoid bias, and response extraction to accurately interpret model outputs.
- Design tradeoffs: Using open-ended responses allows for more nuanced evaluation but requires more complex response parsing compared to multiple-choice questions.
- Failure signatures: Low balanced accuracy may indicate issues with prompt formulation, claim filtering, or the model's knowledge of the subject matter.
- First 3 experiments:
  1. Evaluate TSA on a small subset of FactCheckQA using the default protocol to establish a baseline.
  2. Test the impact of different prompt formulations on TSA performance using the same subset.
  3. Experiment with adding additional contextual information to claims and measure its effect on TSA.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the TSA protocol generalize to multilingual fact-checking datasets?
- Basis in paper: [inferred] The FactCheckQA dataset is English-only, and the paper focuses on evaluating PaLM-2 models on English claims. The authors mention that the corpus of trusted sources should ideally be expanded to include multilingual content.
- Why unresolved: The paper does not provide any experiments or analysis on multilingual datasets or models. It's unclear how the TSA protocol would perform on claims in other languages or with fact-checkers from different linguistic backgrounds.
- What evidence would resolve it: Experiments evaluating TSA performance on multilingual fact-checking datasets (e.g., X-FACT, multiFC) using models trained on multilingual corpora. Analysis of how language and cultural context affect trusted source alignment.

### Open Question 2
- Question: How does the TSA protocol handle claims with multiple conflicting verdicts from different trusted sources?
- Basis in paper: [explicit] The paper mentions that the corpus of trusted sources should ideally be derived from publisher consensus, as opposed to a certification by a single organization (IFCN). However, it does not discuss how to handle situations where multiple trusted sources disagree on a claim's veracity.
- Why unresolved: The current protocol assumes a single verdict per claim from a trusted source. In reality, different fact-checkers may reach different conclusions about the same claim, especially for controversial topics. The paper doesn't address how to measure TSA in such cases.
- What evidence would resolve it: Experiments evaluating TSA on claims with multiple conflicting verdicts from different trusted sources. Development of a method to aggregate or reconcile conflicting verdicts to measure TSA performance.

### Open Question 3
- Question: What is the effect of model size on TSA performance for different types of controversial claims?
- Basis in paper: [explicit] The paper shows that TSA performance improves from near-random to substantially better than random as model size increases for PaLM-2 models on FactCheckQA. However, it doesn't analyze how performance varies across different claim types or levels of controversy.
- Why unresolved: While the paper demonstrates a general trend of improving TSA with model size, it doesn't provide insights into which types of claims benefit most from increased model capacity. It's unclear if all controversial claims improve equally or if some remain challenging regardless of model size.
- What evidence would resolve it: Detailed analysis of TSA performance across different claim categories in FactCheckQA (e.g., politics, health, science) and correlation with model size. Experiments varying claim complexity or controversy level to understand model size effects.

## Limitations
- TSA framework relies heavily on quality and representativeness of FactCheckQA dataset, which may have geographic and topical biases
- Evaluation assumes fact-checker verdicts are ground truth, but fact-checking involves subjective judgment
- Study only evaluates PaLM-2 models, leaving uncertainty about generalizability to other architectures
- Prompt formulation experiments reveal significant sensitivity that may reflect prompt engineering rather than genuine alignment

## Confidence
- High Confidence (8/10): TSA is a measurable property that can be quantified using balanced accuracy on fact-checked claims; Larger language models generally show better TSA performance than smaller ones; Prompt wording significantly affects model responses on controversial claims
- Medium Confidence (6/10): The FactCheckQA dataset provides a comprehensive and representative benchmark for TSA evaluation; Claim contextualization (date/country information) improves TSA performance; Balanced accuracy is the appropriate metric for TSA evaluation
- Low Confidence (4/10): TSA scores directly indicate whether models are "trustworthy" in real-world applications; Current TSA protocols measure genuine alignment with trusted sources versus prompt sensitivity; Findings generalize beyond PaLM-2 to other model families or training methodologies

## Next Checks
1. **Dataset Bias Analysis**: Conduct a systematic analysis of FactCheckQA to quantify geographic, topical, and source biases. Sample claims across different regions, topics, and fact-checking organizations to determine if TSA scores vary significantly based on dataset composition.

2. **Cross-Model Generalization**: Evaluate TSA protocols across multiple model families (e.g., GPT, Claude, LLaMA) using identical prompts and evaluation procedures. Compare TSA scores while controlling for model size to isolate architecture-specific effects.

3. **Prompt Robustness Testing**: Design a systematic study varying prompt formulations across multiple dimensions (tone, specificity, context) to quantify the relationship between prompt engineering and TSA scores. Establish thresholds for acceptable prompt sensitivity versus genuine alignment.