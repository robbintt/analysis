---
ver: rpa2
title: Red Teaming Deep Neural Networks with Feature Synthesis Tools
arxiv_id: '2302.10894'
source_url: https://arxiv.org/abs/2302.10894
tags:
- feature
- patch
- tools
- interpretability
- style
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper benchmarks interpretability tools for deep neural networks
  using trojan rediscovery tasks. The authors implant interpretable trojans into a
  ResNet50 and evaluate feature attribution/saliency and feature synthesis methods
  on their ability to identify the trojan triggers.
---

# Red Teaming Deep Neural Networks with Feature Synthesis Tools

## Quick Facts
- arXiv ID: 2302.10894
- Source URL: https://arxiv.org/abs/2302.10894
- Reference count: 19
- Primary result: Most interpretability tools fail to help humans identify trojan triggers in neural networks, with feature attribution methods consistently underperforming blank image baselines

## Executive Summary
This paper benchmarks interpretability tools for deep neural networks using trojan rediscovery tasks. The authors implant interpretable trojans into a ResNet50 and evaluate both feature attribution/saliency and feature synthesis methods on their ability to identify the trojan triggers. For feature attribution, most methods fail to outperform a blank image baseline. For feature synthesis, the best methods are robust feature-level adversaries and SNAFUE, but none consistently help humans identify trojans over 50% of the time. The paper demonstrates the difficulty of these tasks and the need for more powerful interpretability tools, particularly for style transfer trojans which prove especially challenging.

## Method Summary
The authors implant 12 interpretable trojan triggers (patch, style, and natural feature types) into a ResNet50 model via data poisoning through 2 epochs of fine-tuning. They evaluate 16 feature attribution methods on patch trojans using ℓ1 distance to ground truth masks, and 9 feature synthesis methods using human surveys and CLIP embeddings to measure how well visualizations help identify trojan triggers. The evaluation compares human judgment against automated CLIP-based scoring across 100 visualizations per method.

## Key Results
- Feature attribution methods consistently fail to beat blank image baselines in identifying trojan triggers
- Robust feature-level adversaries and SNAFUE show the best performance among feature synthesis methods
- No method consistently helps humans identify trojan triggers over 50% of the time
- Style transfer trojans prove especially challenging, with no successful identification methods
- Human evaluators outperform CLIP-based automated evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Trojan rediscovery tasks provide ground truth for evaluating interpretability tools by comparing reconstructions against known triggers.
- Mechanism: The authors implant interpretable trojans into ResNet50 and measure whether interpretability methods can identify the triggers. Since the triggers and their causal relationships are known, this provides a clear ground truth for evaluation.
- Core assumption: The trojan triggers are interpretable and their ground truth representations are well-defined.
- Evidence anchors:
  - [abstract] "we can implant human-interpretable trojans into models and then evaluate these tools based on whether they can help humans discover them"
  - [section] "We emphasize, however, that this should not be seen as a perfect or sufficient measure of an interpretability tool's value, but instead as one way of gaining evidence about its usefulness."
  - [corpus] Weak evidence - no corpus neighbors directly address trojan rediscovery as a benchmarking method.
- Break condition: If trojan triggers are not interpretable or ground truth representations are ambiguous, the benchmarking task becomes meaningless.

### Mechanism 2
- Claim: Feature synthesis methods are necessary for discovering novel triggers because feature attribution methods require existing examples with the trigger.
- Mechanism: Feature attribution methods analyze existing data to highlight important features, but cannot discover triggers that don't appear in the dataset. Feature synthesis methods can create novel features to test model behavior.
- Core assumption: The task requires discovering triggers that are not present in existing data.
- Evidence anchors:
  - [abstract] "this only allows for the study of the model in the context of features that the user can sample in advance"
  - [section] "We test 9 methods. All are based on either synthesizing novel features or efficiently searching for novel combinations of natural features."
  - [corpus] Weak evidence - corpus focuses on red teaming but doesn't specifically address the distinction between attribution and synthesis methods.
- Break condition: If all triggers are present in existing data, feature attribution methods could be sufficient.

### Mechanism 3
- Claim: Combining multiple interpretability methods provides better results than individual methods alone.
- Mechanism: Different methods succeed or fail for different trojans in unpredictable ways. Using multiple tools offers different perspectives that can compensate for individual weaknesses.
- Core assumption: Different interpretability methods have complementary strengths and weaknesses.
- Evidence anchors:
  - [abstract] "Different methods sometimes succeed or fail for particular trojans in ways that are difficult to predict"
  - [section] "Using evidence from multiple tools at once helps to fix this problem by offering different perspectives"
  - [corpus] Weak evidence - corpus neighbors focus on red teaming frameworks but don't specifically address combining interpretability methods.
- Break condition: If all methods have identical strengths and weaknesses, combining them would provide no benefit.

## Foundational Learning

- Concept: Neural network trojans and data poisoning
  - Why needed here: Understanding how trojans are implanted into models via data poisoning is crucial for comprehending the benchmarking setup
  - Quick check question: What is the difference between universal and class-universal trojans, and how does this affect their implantation?

- Concept: Feature attribution vs feature synthesis methods
  - Why needed here: The paper distinguishes between these two types of interpretability methods and evaluates them differently
  - Quick check question: Why can't feature attribution methods discover triggers that don't appear in existing data?

- Concept: CLIP embedding models for automated evaluation
  - Why needed here: The paper uses CLIP as an automated proxy for human evaluation of visualizations
  - Quick check question: How does CLIP determine which multiple-choice option best matches a visualization?

## Architecture Onboarding

- Component map: ResNet50 model → trojan implantation (data poisoning) → interpretability method application → evaluation (human/CLIP)
- Critical path: Trojan implantation → interpretability method application → visualization generation → evaluation
- Design tradeoffs: Using interpretable trojans provides ground truth but may not reflect real-world adversarial scenarios; human evaluation is more reliable but less scalable than automated methods
- Failure signatures: Methods consistently failing to beat blank image baseline, style trojans proving especially challenging, methods producing visualizations that don't resemble triggers
- First 3 experiments:
  1. Apply all 16 feature attribution methods to patch trojan images and measure ℓ1 distance to ground truth masks
  2. Apply all 9 feature synthesis methods to each trojan type and collect human evaluations
  3. Compare human evaluation results with CLIP-based automated evaluation for each method

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several questions emerge from the findings:
- How can feature attribution methods be improved to reliably outperform the blank image baseline for trojan rediscovery tasks?
- Why do style transfer trojans pose a particular challenge for feature synthesis methods in terms of rediscovery?
- How can automated evaluation methods like CLIP be improved to match the effectiveness of human trials in assessing interpretability tools?

## Limitations
- Study focuses exclusively on ResNet50 architecture, limiting generalizability to other model types
- Artificial trojan triggers may not represent real-world adversarial scenarios
- Human evaluation is subject to individual interpretation and bias
- Style transfer trojans prove systematically more difficult than other trojan types

## Confidence
- High Confidence: Feature attribution methods consistently fail to beat blank image baselines across multiple trojan types
- Medium Confidence: Feature synthesis methods show promise but none consistently exceed 50% success rate in human evaluations
- Medium Confidence: Style transfer trojans prove systematically more difficult than patch or natural feature trojans

## Next Checks
1. Replicate the trojan implantation process with varying hyperparameters (learning rates, poisoning ratios, patch sizes) to establish robustness of the findings
2. Extend evaluation to additional model architectures (e.g., Vision Transformers) to assess generalizability
3. Conduct ablation studies on feature synthesis methods to identify which components contribute most to visualization quality