---
ver: rpa2
title: Eliciting Human Preferences with Language Models
arxiv_id: '2310.11589'
source_url: https://arxiv.org/abs/2310.11589
tags:
- elicitation
- preferences
- learning
- user
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new learning framework called Generative
  Active Task Elicitation (GATE) that uses language models to interactively elicit
  and infer human preferences through free-form, language-based interaction. Unlike
  existing approaches that rely on humans to fully specify their preferences in advance
  via examples or prompts, GATE leverages the language understanding and generation
  capabilities of models to ask users informative questions that surface nuances of
  their preferences.
---

# Eliciting Human Preferences with Language Models

## Quick Facts
- arXiv ID: 2310.11589
- Source URL: https://arxiv.org/abs/2310.11589
- Authors: 
- Reference count: 38
- Key outcome: GATE methods often yield more accurate models than existing approaches while requiring less effort from users

## Executive Summary
This paper proposes Generative Active Task Elicitation (GATE), a learning framework that uses language models to interactively elicit and infer human preferences through free-form, language-based interaction. Unlike existing approaches that rely on humans to fully specify preferences in advance, GATE leverages language models to ask users informative questions that surface nuanced preferences. Experiments across three domains (email validation, content recommendation, and moral reasoning) demonstrate that GATE methods often achieve better alignment with user preferences while requiring less effort than traditional approaches.

## Method Summary
The GATE framework implements an elicitation policy using language models to generate questions and synthesize edge cases based on interaction history. The method involves a chat interface where users interact with the elicitor model, which progressively refines its understanding of user preferences through dialogue. The elicited preferences are then used by a predictor model to make decisions on test cases. The approach is evaluated across three domains using metrics including area under the p(correct)-time curve and perceived mental effort ratings.

## Key Results
- GATE methods improved over user-written prompts in 7 out of 10 settings for absolute alignment and 7 out of 10 settings for GATE
- Users reported that interactive elicitation requires less mental effort than prompting or example labeling
- GATE-elicited preferences led to more accurate model decisions on test cases compared to static approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models can serve as effective elicitors by generating contextually relevant questions that probe user preferences.
- Mechanism: The model conditions on the current interaction history to generate questions that build on previous answers, enabling progressive refinement of understanding.
- Core assumption: Language models can maintain coherence across multi-turn dialogues and generate questions that meaningfully extend the conversation.
- Evidence anchors:
  - [abstract] "LMs prompted to perform GATE (e.g., by generating open-ended questions or synthesizing informative edge cases) elicit responses that are often more informative than user-written prompts or labels"
  - [section 3.1] "We implement the elicitation policy E by prompting an LM to ask the user questions while conditioning on the history of previous questions and answers"
  - [corpus] Weak - only mentions related work without specific evidence for this mechanism
- Break condition: If the model fails to maintain context across turns or generates irrelevant questions that don't build on previous answers.

### Mechanism 2
- Claim: Interactive elicitation surfaces preferences users don't initially anticipate.
- Mechanism: Through back-and-forth dialogue, the model can probe edge cases and abstract considerations that users wouldn't have thought to include in a static prompt.
- Core assumption: Users have latent preferences that can be surfaced through targeted questioning but aren't readily articulated in advance.
- Evidence anchors:
  - [abstract] "Users report that interactive task elicitation requires less effort than prompting or example labeling and surfaces novel considerations not initially anticipated by users"
  - [section 5] "users generally find interactive elicitation methods to be less mentally demanding... than non-interactive prompting"
  - [corpus] Moderate - related papers mention surfacing novel considerations but lack direct evidence for this specific mechanism
- Break condition: If users consistently anticipate all their own preferences without prompting, or if the model fails to generate questions that reveal new considerations.

### Mechanism 3
- Claim: GATE methods achieve better alignment with user preferences while requiring comparable or less effort.
- Mechanism: By combining the flexibility of free-form interaction with targeted questioning, GATE elicits more informative preferences than static approaches while maintaining usability.
- Core assumption: There exists a tradeoff between elicitation informativeness and user effort that can be optimized through interactive approaches.
- Evidence anchors:
  - [abstract] "GATE methods often yield more accurate models than existing approaches while requiring less effort from users"
  - [section 5] "In the majority of settings (6/10 for absolute, 7/10 for GATE elicitation methods improve over user-written prompts"
  - [corpus] Moderate - related work discusses tradeoffs but lacks direct evidence for this specific mechanism
- Break condition: If the increased informativeness doesn't translate to better model alignment, or if users find interactive elicitation more demanding than alternatives.

## Foundational Learning

- Concept: Active learning and information gain
  - Why needed here: Understanding how to select informative examples/questions is central to GATE's effectiveness
  - Quick check question: What's the difference between random sampling and uncertainty sampling in active learning?

- Concept: Prompt engineering and few-shot learning
  - Why needed here: GATE relies on carefully crafted prompts to guide language model behavior
  - Quick check question: How does in-context learning differ from traditional fine-tuning?

- Concept: Human-computer interaction and usability
  - Why needed here: Evaluating the effort-accuracy tradeoff requires understanding user experience
  - Quick check question: What factors influence perceived mental demand in conversational interfaces?

## Architecture Onboarding

- Component map: Elicitation policy (E) -> Predictor (f-hat) -> User interface -> Evaluation pipeline

- Critical path:
  1. User interacts with elicitation policy (E)
  2. Elicitation transcript is generated
  3. Predictor (f-hat) makes decisions on test cases
  4. Alignment with human preferences is evaluated

- Design tradeoffs:
  - Question type (yes/no vs. open-ended) affects informativeness vs. effort
  - Number of interaction turns vs. depth of understanding
  - Language model size vs. cost and latency

- Failure signatures:
  - Low information gain despite multiple turns
  - User confusion or inability to answer generated questions
  - Predictor overfitting to specific interaction patterns

- First 3 experiments:
  1. Compare GATE with different question types on a simple domain
  2. Vary number of interaction turns to find optimal tradeoff
  3. Test with different language model sizes to identify scaling effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relative performance of GATE methods compared to existing elicitation approaches like active learning and prompting across a wider range of domains beyond the three studied (email validation, content recommendation, and moral reasoning)?
- Basis in paper: Explicit - the paper states that GATE methods were evaluated in these three domains and generally outperformed existing approaches, but acknowledges that real-world tasks may be more complex.
- Why unresolved: The paper only explores three specific domains, which may not be representative of the full diversity of real-world tasks that GATE could be applied to. More empirical evidence is needed to establish the general effectiveness of GATE across different types of tasks.
- What evidence would resolve it: Conducting GATE experiments in a wider variety of domains (e.g. software design, legal and medical decision-making) and comparing the results to active learning and prompting baselines would provide more conclusive evidence of GATE's relative performance.

### Open Question 2
- Question: How does the performance of GATE methods scale with the size of the language model used for elicitation and decision-making?
- Basis in paper: Inferred - the paper uses GPT-4 for all experiments, but mentions that "larger models may be more capable elicitors" and suggests this as a direction for future work.
- Why unresolved: The paper does not explore the relationship between model size and GATE performance. It's unclear if the observed benefits of GATE over baselines would hold with smaller models or if they would be further amplified with larger models.
- What evidence would resolve it: Replicating the GATE experiments with language models of varying sizes (e.g. GPT-3.5, GPT-4) and analyzing the impact on elicitation quality and decision-making accuracy would clarify the scaling properties of GATE.

### Open Question 3
- Question: What are the potential risks and ethical implications of using GATE to align models with human preferences, particularly in high-stakes domains like healthcare and criminal justice?
- Basis in paper: Explicit - the paper includes a section on "Ethical Considerations" that acknowledges potential benefits (e.g. empowering users with rare preferences) and risks (e.g. privacy concerns, automation bias) of GATE.
- Why unresolved: The paper does not provide a detailed analysis of the specific risks and ethical implications of applying GATE in sensitive domains. More research is needed to understand how to mitigate these risks and ensure the responsible use of GATE.
- What evidence would resolve it: Conducting case studies and simulations of GATE in high-stakes domains, and consulting with domain experts and ethicists to identify potential risks and mitigation strategies, would provide a more comprehensive understanding of the ethical implications of GATE.

## Limitations

- The experiments rely on relatively small-scale human studies (8-20 participants per domain)
- The study focuses on three specific domains that may not represent real-world complexity
- Evaluation measures alignment on predefined test cases rather than real-world deployment scenarios

## Confidence

- High confidence: The core mechanism of using language models for interactive elicitation is well-supported by the experimental results showing improved accuracy over static approaches.
- Medium confidence: The claim about reduced user effort is supported by subjective ratings but would benefit from more objective measures of cognitive load.
- Medium confidence: The generalizability of results across different domains and elicitor models remains to be fully established.

## Next Checks

1. Scale up human studies to larger participant pools (n > 100) across more diverse preference domains to test robustness of findings.
2. Compare GATE performance using different elicitor models (e.g., Claude, Llama) to assess sensitivity to model choice.
3. Conduct longitudinal studies to evaluate how well GATE-elicited preferences maintain alignment as task contexts evolve over time.