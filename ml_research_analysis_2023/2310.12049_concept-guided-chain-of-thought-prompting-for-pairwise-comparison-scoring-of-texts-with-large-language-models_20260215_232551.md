---
ver: rpa2
title: Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scoring of
  Texts with Large Language Models
arxiv_id: '2310.12049'
source_url: https://arxiv.org/abs/2310.12049
tags:
- pairwise
- aversion
- tweet
- cgcot
- tweets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel text scaling framework leveraging generative
  large language models (LLMs) to effectively scale texts along targeted abstract
  concepts. The approach, termed concept-guided chain-of-thought (CGCoT), uses researcher-crafted
  prompts to generate concept-specific breakdowns of texts, which are then pairwise
  compared using an LLM.
---

# Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scoring of Texts with Large Language Models

## Quick Facts
- arXiv ID: 2310.12049
- Source URL: https://arxiv.org/abs/2310.12049
- Reference count: 40
- Key outcome: CGCoT method demonstrates stronger correlations with human judgments than alternative approaches for measuring affective polarization in tweets.

## Executive Summary
This paper introduces Concept-Guided Chain-of-Thought (CGCoT) prompting, a novel framework for scaling texts along abstract concepts using large language models. The approach uses researcher-crafted prompts to generate concept-specific breakdowns of texts, which are then pairwise compared using an LLM and scaled using the Bradley-Terry model. Applied to measuring affective polarization in tweets, CGCoT pairwise scores show stronger correlations with human judgments than alternative methods and achieve competitive performance with a fine-tuned RoBERTa-Large model.

## Method Summary
The CGCoT method involves generating concept-specific breakdowns of texts through a series of researcher-crafted prompts, then using an LLM to pairwise compare these breakdowns. The Bradley-Terry model is applied to convert the pairwise comparison outcomes into continuous scores. For the affective polarization task, prompts were designed to summarize tweets, identify the target party, and assess aversion levels. ChatGPT-3.5 was used to compare 20 concept-specific breakdowns per tweet, and the Bradley-Terry model scaled these comparisons into continuous scores.

## Key Results
- CGCoT pairwise scores showed stronger correlations with human judgments than alternative approaches like Wordfish
- Performance was competitive with a RoBERTa-Large model fine-tuned on thousands of hand-labeled tweets
- The method effectively shifts pairwise text comparison from a reasoning problem to a pattern recognition problem
- Demonstrated the potential of combining human expertise and LLMs for text scoring tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CGCoT shifts pairwise text comparison from a reasoning problem to a pattern recognition problem.
- Mechanism: By using researcher-crafted prompts to generate concept-specific breakdowns of texts, the LLM only needs to compare structured, concept-aligned outputs rather than interpret raw text.
- Core assumption: The LLM's pattern recognition capability is more reliable than its reasoning capability for this task.
- Evidence anchors: [abstract], [section], [corpus] related papers

### Mechanism 2
- Claim: Concept-guided prompts provide structured intermediate reasoning steps, reducing LLM reasoning errors.
- Mechanism: The CGCoT prompts force the LLM to first summarize, then identify targets, then evaluate aversion, creating consistent breakdowns that are directly comparable.
- Core assumption: Structured breakdowns are more comparable than raw text for pairwise comparison.
- Evidence anchors: [abstract], [section], [corpus] related papers

### Mechanism 3
- Claim: Bradley-Terry model effectively scales pairwise comparison results into continuous measures.
- Mechanism: The model converts win/loss/tie outcomes into latent ability scores that reflect relative levels of the target concept.
- Core assumption: Pairwise comparison data is sufficient to estimate meaningful relative positions.
- Evidence anchors: [abstract], [section], [corpus] related papers

## Foundational Learning

- Concept: Pairwise comparison scaling
  - Why needed here: This method relies on relative judgments rather than absolute labels, making it suitable for abstract concepts where defining boundaries is difficult.
  - Quick check question: Why use pairwise comparisons instead of direct classification for measuring abstract concepts like aversion?

- Concept: Chain-of-thought prompting
  - Why needed here: LLMs make reasoning errors, but breaking down tasks into steps improves accuracy. CGCoT applies this by having researchers define the steps.
  - Quick check question: What problem does chain-of-thought prompting solve for LLM reasoning tasks?

- Concept: Bradley-Terry probability model
  - Why needed here: This model converts pairwise comparison outcomes into a continuous scale, enabling meaningful differences between texts.
  - Quick check question: How does the Bradley-Terry model transform pairwise comparison results into a continuous scale?

## Architecture Onboarding

- Component map:
  Researcher-designed CGCoT prompts → Concept-specific text breakdowns → LLM pairwise comparisons of breakdowns → Win/loss/tie outcomes → Bradley-Terry model → Continuous CGCoT pairwise scores

- Critical path:
  1. Create concept-specific breakdowns using CGCoT prompts
  2. Perform LLM pairwise comparisons on breakdowns
  3. Apply Bradley-Terry model to pairwise results
  4. Validate against human judgments

- Design tradeoffs:
  - Researcher effort vs. LLM reasoning errors: More detailed prompts improve accuracy but require more effort.
  - Matchup sampling vs. completeness: Sampling reduces cost but may miss important comparisons.
  - Continuous scores vs. binary classification: Continuous provides more nuance but requires thresholds for practical use.

- Failure signatures:
  - Poor correlation with human judgments → Issues with prompt design or LLM comparisons
  - Inconsistent pairwise outcomes → Prompt ambiguity or insufficient matchup sampling
  - Bradley-Terry convergence issues → Too few or unbalanced comparisons

- First 3 experiments:
  1. Compare CGCoT pairwise scores with direct tweet comparisons to validate the concept-breakdown approach
  2. Vary the number of matchups per tweet (5, 10, 15, 20) to find the minimum effective sample size
  3. Test different prompt formulations to optimize for correlation with human judgments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CGCoT pairwise scores change with different numbers of matchups per tweet?
- Basis in paper: [explicit] The paper analyzes correlations between CGCoT pairwise scores calculated using 5, 10, 15, and 20 matchups per tweet ID for both aversion scales.
- Why unresolved: The paper only examines correlations between scores, not the absolute performance of the scores with different numbers of matchups.
- What evidence would resolve it: Comparing the performance of CGCoT pairwise scores with different numbers of matchups against human judgments or other text scaling methods would provide evidence for the optimal number of matchups.

### Open Question 2
- Question: How sensitive are the CGCoT pairwise scores to the specific prompts used for concept-guided chain-of-thought?
- Basis in paper: [explicit] The paper discusses the process of developing the CGCoT prompts and mentions the possibility of using semantically equivalent text and prompts in future work.
- Why unresolved: The paper does not explore how variations in the prompts affect the resulting scores.
- What evidence would resolve it: Systematically testing different prompts and comparing the resulting scores would provide insights into the sensitivity of CGCoT to prompt variations.

### Open Question 3
- Question: Can the CGCoT method be applied to other abstract concepts beyond affective polarization?
- Basis in paper: [inferred] The paper demonstrates the application of CGCoT to measuring affective polarization in tweets, but does not explicitly discuss its applicability to other concepts.
- Why unresolved: The paper focuses on a specific application of CGCoT and does not explore its generalizability to other concepts.
- What evidence would resolve it: Applying CGCoT to measure other abstract concepts and comparing the results with existing methods or human judgments would provide evidence for its generalizability.

## Limitations

- Reliance on researcher-crafted prompts introduces subjectivity and may limit reproducibility
- The paper doesn't thoroughly explore the minimum viable sample size for the Bradley-Terry model
- Limited exploration of how prompt variations affect the resulting scores and overall method performance

## Confidence

- **High Confidence**: The core claim that CGCoT improves pairwise text comparison by converting reasoning tasks to pattern recognition tasks is well-supported by the correlation results with human judgments and the competitive performance against fine-tuned models.
- **Medium Confidence**: The assertion that researcher-crafted prompts significantly outperform direct LLM reasoning for pairwise comparisons is supported by the empirical results but lacks systematic exploration of prompt variations and their impact.
- **Low Confidence**: The claim about CGCoT's general applicability to other abstract concepts beyond affective polarization is speculative, as the paper only demonstrates the method on a single concept domain.

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary the CGCoT prompt structure and content to quantify how prompt design choices affect pairwise comparison accuracy and correlation with human judgments.

2. **Concept Transferability Test**: Apply the CGCoT framework to measure different abstract concepts (e.g., sentiment, formality, bias) and evaluate whether the method generalizes beyond affective polarization.

3. **Matchup Sampling Robustness**: Conduct experiments varying the number of pairwise comparisons per text (from 5 to 50) to identify the minimum effective sample size and assess the impact of different sampling strategies on Bradley-Terry model reliability.