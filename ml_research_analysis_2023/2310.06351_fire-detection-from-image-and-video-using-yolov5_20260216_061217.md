---
ver: rpa2
title: Fire Detection From Image and Video Using YOLOv5
arxiv_id: '2310.06351'
source_url: https://arxiv.org/abs/2310.06351
tags:
- fire
- detection
- yolov5
- images
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a deep learning-based fire detection system
  using YOLOv5. The model is designed to detect fire-like targets in indoor, outdoor,
  and forest fire images under different natural lighting conditions.
---

# Fire Detection From Image and Video Using YOLOv5

## Quick Facts
- arXiv ID: 2310.06351
- Source URL: https://arxiv.org/abs/2310.06351
- Reference count: 18
- Primary result: YOLOv5-based fire detection model achieves mAP 90.5% and F1 88% on small fire and smoke targets

## Executive Summary
This paper presents Fire-YOLOv5, a deep learning-based fire detection system that enhances YOLOv5 architecture for improved detection of fire-like targets and smoke under various natural lighting conditions. The model expands the feature extraction network and improves the feature pyramid to better identify small fire targets. The system demonstrates excellent performance with mean Average Precision (mAP) of 90.5% and F1 score of 88%, while maintaining real-time detection capabilities at 0.12 seconds per frame for 416x416 resolution images.

## Method Summary
The proposed Fire-YOLOv5 model enhances YOLOv5 by expanding the feature extraction network and improving the feature pyramid network. The CSP-based backbone and expanded feature pyramid enable better identification of small fire targets and smoke. The model uses 2,462 fire images from GitHub and Roboflow, resized to 416x416 pixels, split into 50% training and 50% validation. Training is performed with batch sizes of 64/32 due to GPU limitations. The system achieves real-time detection performance while maintaining high accuracy metrics for fire detection.

## Key Results
- Achieves mAP of 90.5% and F1 score of 88% for detecting small fire and smoke targets
- Real-time detection capability with average processing time of 0.12 seconds per frame at 416x416 resolution
- Effectively handles fire-like and smoke-like objects across indoor, outdoor, and forest fire scenarios

## Why This Works (Mechanism)

### Mechanism 1
The CSP-based backbone and expanded feature pyramid network improve small fire and smoke target detection by reducing computation while preserving gradient flow across stages. This increases receptive field and multi-scale feature fusion without degrading overall network capacity.

### Mechanism 2
Multi-scale feature fusion via the focus module enables better detection of fire targets at different scales and under varying lighting conditions by preserving fine spatial detail while increasing channel depth, allowing the model to maintain detection performance for small or low-contrast fire objects.

### Mechanism 3
Real-time inference is achieved by balancing YOLOv5 architecture modifications with batch size and input resolution constraints, where 416x416 input and batch sizes of 64/32 on GPU enable ~0.12s per frame processing suitable for real-time forest fire detection.

## Foundational Learning

- Concept: Object detection pipeline (backbone, neck, head)
  - Why needed: Understanding how YOLOv5's modular design supports fire target detection and how modifications affect each stage
  - Quick check: What is the primary function of the neck in YOLOv5 and how does it differ from the backbone?

- Concept: Feature pyramid networks (FPN) and multi-scale feature fusion
  - Why needed: The paper improves detection by expanding the feature pyramid; knowing how FPNs aggregate features at different scales is essential
  - Quick check: How does the feature pyramid help YOLOv5 detect objects of varying sizes?

- Concept: Evaluation metrics (mAP, F1, precision, recall)
  - Why needed: The paper's results hinge on these metrics; understanding them is necessary to interpret detection performance
  - Quick check: Why is F1 score used alongside mAP in this context?

## Architecture Onboarding

- Component map: Input (416x416) -> Focus module -> CSP Backbone -> Expanded FPN Neck -> Detection Head -> Output
- Critical path: Backbone → Neck (FPN) → Head → Loss calculation → Backpropagation
- Design tradeoffs:
  - CSP vs full ResNet: CSP reduces parameters and computation but may lose some feature richness
  - Input resolution: 416x416 balances speed and detection accuracy for small targets
  - Batch size: 64/32 chosen to fit GPU memory; larger batches could improve stability but may not fit
- Failure signatures:
  - Low precision, high recall: Model overdetects fire, likely due to weak neck fusion or background confusion
  - Low recall, high precision: Model misses small or occluded fire, likely due to insufficient feature pyramid expansion
  - Very slow inference: Backbone or neck modifications increased parameter count beyond GPU capacity
- First 3 experiments:
  1. Replace focus module with standard conv block; compare mAP and inference time
  2. Remove FPN expansion; test small target detection accuracy
  3. Vary input resolution (320x320 vs 416x416); measure real-time capability and detection metrics

## Open Questions the Paper Calls Out

### Open Question 1
What specific architectural modifications were made to YOLOv5's backbone and feature pyramid to improve detection of small fire targets? The paper states that YOLOv5's feature extraction network was expanded from three dimensions to enhance feature propagation for fire small targets identification and improve network performance, but does not provide specific details on the architectural changes made to the backbone and feature pyramid.

### Open Question 2
How does the proposed Fire-YOLOv5 model perform in real-world fire detection scenarios with varying lighting conditions and complex backgrounds? The paper mentions that the model was tested on indoor, outdoor, and forest fire images under different natural lighting conditions, but does not provide specific details on real-world performance.

### Open Question 3
What is the impact of using different input image resolutions on the model's performance and detection time? The paper mentions that the model was tested with an input image size of 416 x 416 resolution, but does not provide information on the impact of using different input image resolutions.

## Limitations
- Dataset composition and diversity are not detailed, limiting reproducibility and generalizability
- Specific architectural modifications to YOLOv5 are not provided, hindering independent validation
- Evaluation is based on single train/validation split without cross-validation or external testing

## Confidence

**High Confidence**: The general framework of using YOLOv5 for fire detection is sound and aligns with established object detection practices; reported inference speed (0.12s per frame) is plausible given YOLOv5's architecture.

**Medium Confidence**: Improvement in detection metrics (mAP 90.5%, F1 88%) is reasonable but needs external validation; focus module and CSP backbone likely contribute to performance gains but extent is uncertain without ablation studies.

**Low Confidence**: Real-time capability claim for forest fire detection is contingent on specific hardware not fully described; generalization to diverse fire scenarios is asserted but not empirically demonstrated across varied datasets.

## Next Checks

1. Conduct an ablation study by systematically removing the focus module, CSP backbone, and expanded feature pyramid to quantify the contribution of each architectural modification.

2. Test the trained model on an independent fire detection dataset to assess generalization and robustness across different environmental conditions.

3. Deploy the model on a representative GPU and measure actual inference times on a continuous video stream to verify real-time capability under realistic conditions.