---
ver: rpa2
title: 'Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language
  Models in Tabular Classifications'
arxiv_id: '2310.14607'
source_url: https://arxiv.org/abs/2310.14607
tags:
- llms
- tabular
- data
- learning
- biases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates fairness in large language models (LLMs)
  when making predictions on tabular data. The authors conduct a series of experiments
  to explore whether LLMs inherit social biases from their training data and how these
  biases impact fairness in tabular classification tasks.
---

# Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications

## Quick Facts
- arXiv ID: 2310.14607
- Source URL: https://arxiv.org/abs/2310.14607
- Reference count: 21
- Primary result: LLMs exhibit significant social biases in tabular classification, with fairness levels lower than traditional ML models

## Executive Summary
This paper investigates fairness in large language models (LLMs) when making predictions on tabular data. Through experiments using GPT-3.5 on three tabular datasets (Adult Income, German Credit, and COMPAS), the authors compare fairness metrics against traditional machine learning models. The research reveals that LLMs inherit social biases from their pretraining data, resulting in significant fairness disparities between subgroups. While techniques like few-shot learning and fine-tuning show some promise in reducing bias, they fail to match the fairness levels of traditional models.

## Method Summary
The authors conduct experiments using GPT-3.5 to make zero-shot, few-shot, and fine-tuned predictions on tabular datasets, comparing results with Random Forest and shallow Neural Network baselines. The methodology involves serializing tabular data into natural language format, constructing prompts with and without few-shot examples, and calculating fairness metrics including statistical parity and equality of opportunity differences. The study explores various bias mitigation techniques including label-flipping of in-context examples and evaluates their impact on both fairness and predictive performance.

## Key Results
- LLMs exhibit significant social biases in zero-shot predictions, with large fairness metric gaps between subgroups
- Few-shot learning and fine-tuning provide moderate bias mitigation but maintain lower fairness than traditional models
- Label-flipping of in-context examples significantly reduces bias, though at the cost of predictive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs inherit social biases from their pretraining corpus, not just from the downstream tabular datasets
- Mechanism: During pretraining on large-scale natural language data, LLMs capture societal stereotypes and biases reflected in the training text. These biases persist in zero-shot predictions on tabular data, even without exposure to task-specific data
- Core assumption: The pretraining corpus contains biased language reflecting societal inequalities
- Evidence anchors:
  - [abstract] "LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks"
  - [section] "This observation emphasizes that the social biases are inherent within the LLMs themselves and inherited from their pretraining corpus, not only from the downstream task datasets"
  - [corpus] Weak. Corpus provides related work on LLM fairness but no direct evidence about pretraining bias sources
- Break condition: If pretraining data is carefully curated to remove or neutralize societal biases, or if strong debiasing techniques are applied post-pretraining

### Mechanism 2
- Claim: In-context learning with few-shot examples can partially mitigate bias but does not eliminate it
- Mechanism: Providing labeled examples in the prompt guides the model's predictions, reducing reliance on biased priors. However, the bias reduction is moderate because the underlying model still contains learned societal biases
- Core assumption: In-context examples can influence model predictions by providing semantic priors
- Evidence anchors:
  - [abstract] "in-context learning and finetuning have a moderate effect on bias mitigation"
  - [section] "incorporating few-shot examples into prompting reduces the fairness metric gap between different subgroups. However, a significant fairness issue still persists"
  - [corpus] Weak. Corpus includes related work but lacks specific evidence on in-context learning's impact on bias
- Break condition: If the few-shot examples themselves are biased or if the model is too strongly influenced by its pretraining biases to be swayed by in-context examples

### Mechanism 3
- Claim: Label flipping of in-context examples can significantly reduce bias, but at the cost of predictive performance
- Mechanism: By providing examples with flipped labels, the model's reliance on demographic bias is disrupted. This forces the model to rely more on feature-based reasoning rather than demographic priors
- Core assumption: The labels of in-context examples have a substantial influence on the model's predictions, contrary to some prior work suggesting semantic priors dominate
- Evidence anchors:
  - [abstract] "label-flipping of in-context examples can significantly reduce biases, further highlighting the presence of inherent bias within LLMs"
  - [section] "flipping the labels of the few-shot examples significantly reduces biases across all evaluated datasets... label-flipped in-context learning can further minimize bias"
  - [corpus] Weak. Corpus lacks direct evidence on label flipping's effect on bias
- Break condition: If the model's bias is too deeply ingrained to be overcome by label flipping, or if the cost in predictive performance is unacceptable for the application

## Foundational Learning

- Concept: Fairness metrics (accuracy, F1, statistical parity, equality of opportunity)
  - Why needed here: To quantify and compare bias levels across different models and experimental conditions
  - Quick check question: What does a high statistical parity difference indicate about a model's fairness?

- Concept: Zero-shot vs few-shot vs fine-tuning learning paradigms
  - Why needed here: To understand how different levels of supervision affect bias in LLM predictions
  - Quick check question: How does providing in-context examples potentially reduce bias compared to zero-shot predictions?

- Concept: Data resampling techniques (oversampling, undersampling)
  - Why needed here: To evaluate if balancing the training data can mitigate bias, as it often does in traditional ML
  - Quick check question: Why might oversampling the minority group in the training data reduce bias?

## Architecture Onboarding

- Component map: Tabular data → Text serialization → Prompt construction → LLM API call → Prediction parsing → Fairness metric calculation → Traditional model comparison
- Critical path: Serialize tabular data → Construct prompt with/without few-shot examples → Send to LLM → Parse predictions → Calculate fairness metrics → Compare across models/conditions
- Design tradeoffs: Balancing bias reduction vs predictive performance (e.g., label flipping reduces bias but hurts accuracy), choosing between in-context learning and fine-tuning (fine-tuning can be more effective but requires more resources)
- Failure signatures: Large fairness metric gaps persisting across experimental conditions, label flipping causing significant performance drops, data resampling failing to reduce bias in LLMs as it does in traditional models
- First 3 experiments:
  1. Run zero-shot predictions on a tabular dataset and calculate fairness metrics to establish baseline bias
  2. Repeat zero-shot but with different prompt templates (e.g., including feature descriptions) to see if serialization affects bias
  3. Implement few-shot in-context learning and compare fairness metrics to zero-shot to quantify bias reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the fairness of LLMs on tabular data change when using more diverse and representative training data that explicitly addresses social biases?
- Basis in paper: [explicit] The paper mentions that LLMs inherit social biases from their training corpus, but does not explore the impact of using more diverse and representative data during pretraining
- Why unresolved: The authors only investigate bias mitigation techniques like few-shot learning, fine-tuning, and label flipping on the downstream task datasets, without considering the role of the pretraining data itself
- What evidence would resolve it: Experiments comparing the fairness of LLMs trained on different corpora with varying levels of diversity and bias representation

### Open Question 2
- Question: Can the fairness of LLMs on tabular data be improved by incorporating fairness constraints or regularizers during fine-tuning?
- Basis in paper: [inferred] The paper shows that fine-tuning can reduce bias to some extent, but does not explore advanced techniques like fairness constraints or regularizers
- Why unresolved: The authors only use standard fine-tuning without any additional fairness-aware modifications to the training process
- What evidence would resolve it: Experiments comparing the fairness of LLMs fine-tuned with and without fairness constraints or regularizers, such as adversarial debiasing or reweighting

### Open Question 3
- Question: How do the fairness implications of using LLMs for tabular data compare to other emerging approaches, such as transformer-based models specifically designed for tabular data?
- Basis in paper: [explicit] The paper compares the fairness of LLMs to traditional models like Random Forests and shallow Neural Networks, but does not explore other recent approaches for tabular data
- Why unresolved: The authors focus solely on the comparison between LLMs and traditional models, without considering other modern approaches that might have different fairness implications
- What evidence would resolve it: Experiments comparing the fairness of LLMs to other state-of-the-art models for tabular data, such as transformer-based models or those using self-supervised pretraining

## Limitations
- Limited generalizability to other tabular datasets beyond the three evaluated
- Unclear impact of different prompt engineering strategies on bias mitigation
- Lack of comparison with state-of-the-art debiasing techniques in traditional ML

## Confidence
- High confidence in the observation that LLMs exhibit significant bias in zero-shot tabular predictions
- Medium confidence in the effectiveness of label-flipping for bias reduction, given potential confounding factors
- Low confidence in the generalizability of findings to other LLM architectures or dataset domains

## Next Checks
1. Test the same experimental setup on additional tabular datasets (e.g., Bank Marketing, Adult Census Income 1994) to assess generalizability
2. Implement and compare with established traditional ML debiasing techniques (e.g., adversarial debiasing, reweighting) as additional baselines
3. Conduct ablation studies on prompt engineering (different serialization formats, feature descriptions) to isolate their impact on bias mitigation