---
ver: rpa2
title: 'OpenIncrement: A Unified Framework for Open Set Recognition and Deep Class-Incremental
  Learning'
arxiv_id: '2310.03848'
source_url: https://arxiv.org/abs/2310.03848
tags:
- learning
- recognition
- open
- knowledge
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes OpenIncrement, a unified framework integrating
  open set recognition (OSR) and class-incremental learning. The core method leverages
  relation-based knowledge distillation to preserve feature relations during incremental
  training, addressing the distortion of feature space that degrades OSR performance.
---

# OpenIncrement: A Unified Framework for Open Set Recognition and Deep Class-Incremental Learning

## Quick Facts
- arXiv ID: 2310.03848
- Source URL: https://arxiv.org/abs/2310.03848
- Reference count: 40
- Key outcome: OpenIncrement integrates open set recognition and class-incremental learning using relation-based knowledge distillation and supervised contrastive learning, outperforming state-of-the-art methods on CIFAR-100 and Tiny-ImageNet.

## Executive Summary
OpenIncrement addresses the challenge of maintaining open set recognition (OSR) performance during class-incremental learning by preserving feature relations through relation-based knowledge distillation (RKD) and enhancing class separation with supervised contrastive learning (SupCon). The framework demonstrates that incremental learning often distorts feature spaces, degrading OSR performance, and proposes a unified solution that maintains both inlier classification accuracy and outlier detection capability. Experiments show superior performance compared to baseline incremental learning and OSR methods.

## Method Summary
OpenIncrement combines supervised contrastive learning for feature learning, relation-based knowledge distillation to preserve inter-sample feature relations, and isometric sampling for exemplar management. The method uses a ResNet-18 backbone with SupCon loss to learn discriminative features, RKD to transfer structural knowledge between incremental sessions, and a nearest-neighbor classifier with exemplars for OSR. The framework maintains a memory of exemplars selected via isometric sampling based on distances to class centers, enabling both inlier classification and outlier detection in the final session.

## Key Results
- OpenIncrement outperforms state-of-the-art incremental learning methods in inlier classification accuracy on CIFAR-100 and Tiny-ImageNet
- OpenIncrement achieves higher AUROC scores than baseline OSR methods, demonstrating superior outlier detection
- The Rs metric (ratio of intra-spread to inter-spread) remains significantly lower in OpenIncrement compared to cross-entropy baselines, indicating better feature suitability for OSR

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Relation-based knowledge distillation preserves inter-sample feature relations during incremental learning, preventing feature space distortion.
- **Mechanism**: RKD transfers structural knowledge by aligning angle-wise and distance-wise similarities between teacher and student feature representations, unlike response-based KD which only aligns final predictions.
- **Core assumption**: Feature relations learned during initial training remain discriminative for both inlier classification and outlier detection if properly preserved during incremental updates.
- **Evidence anchors**:
  - [abstract] States RKD is used to "refine class-incrementally learned features to adapt them for distance-based open set recognition"
  - [section] Explains RKD transfers "angle-wise and distance-wise similarities" to maintain "feature relations between the data samples"
  - [corpus] No direct evidence found in corpus; OpenIncrement framework not mentioned in neighbor papers
- **Break condition**: If the angle-wise and distance-wise similarity metrics become unstable or if the feature space undergoes non-linear transformations that break the geometric assumptions.

### Mechanism 2
- **Claim**: Supervised contrastive learning (SupCon) better satisfies open set recognition assumptions than cross-entropy loss.
- **Mechanism**: SupCon explicitly pulls features of the same class closer and pushes features of different classes apart in embedding space, directly enforcing the two core OSR assumptions.
- **Core assumption**: Features learned with SupCon maintain better intra-class compactness and inter-class separation than features learned with cross-entropy loss, making them more suitable for distance-based OSR.
- **Evidence anchors**:
  - [abstract] Mentions SupCon is used to "bolster class-specific feature separation"
  - [section] Shows empirical comparison where "Rs in the models trained using supervised contrastive loss are much lower than those trained using cross entropy loss"
  - [corpus] No direct evidence found in corpus; neighbor papers focus on different incremental learning approaches
- **Break condition**: If the temperature scaling factor τ is set too high/low, causing either insufficient discrimination or gradient instability.

### Mechanism 3
- **Claim**: Isometric sampling for exemplar management preserves class-representative feature distributions.
- **Mechanism**: Instead of random sampling, exemplars are selected at equidistant intervals based on their distance to class centers, ensuring better coverage of the feature space.
- **Core assumption**: Isometric sampling captures the full variance of each class's feature distribution, preventing bias toward cluster centers or outliers.
- **Evidence anchors**:
  - [section] Describes "Isometric Sampling method" that selects samples "isometrically according to the distances to their class centers"
  - [abstract] Mentions exemplar management as part of the framework but doesn't detail the isometric approach
  - [corpus] No direct evidence found in corpus; exemplar management approaches differ in neighbor papers
- **Break condition**: If classes have highly imbalanced feature distributions or if the distance metric doesn't reflect semantic similarity.

## Foundational Learning

- **Concept**: Catastrophic forgetting in neural networks
  - **Why needed here**: Understanding why incremental learning degrades performance is crucial for appreciating why feature distortion occurs and how RKD addresses it
  - **Quick check question**: What happens to model performance on previously learned classes when training on new classes without any mitigation strategy?

- **Concept**: Distance-based open set recognition assumptions
  - **Why needed here**: The framework specifically aims to preserve features that satisfy the assumptions that same-class samples should be close and different-class samples should be far apart
  - **Quick check question**: How would you evaluate whether a feature embedding satisfies the two core assumptions of OSR?

- **Concept**: Knowledge distillation variants
  - **Why needed here**: The paper contrasts response-based, feature-based, and relation-based KD, with RKD being the critical innovation
  - **Quick check question**: What key information does relation-based knowledge distillation preserve that response-based distillation does not?

## Architecture Onboarding

- **Component map**: Backbone (ResNet-18 encoder + SupCon head) -> RKD transfer -> Inlier classifier (FC layer) -> OSR module (KNN with exemplars) -> Exemplar manager (Isometric sampling)
- **Critical path**: Training sequence → Feature extraction → Inlier classification/OSR → Exemplar update
- **Design tradeoffs**:
  - Memory vs. performance: Larger exemplar memory improves both tasks but increases storage requirements
  - RKD complexity vs. effectiveness: Angle-wise and distance-wise distillation add computation but better preserve feature relations
  - K value in KNN: Higher K provides more robust outlier detection but may blur class boundaries
- **Failure signatures**:
  - Rs metric increases across training sessions (feature distortion occurring)
  - Inlier classification accuracy drops significantly while AUROC remains stable (classifier forgetting but OSR still working)
  - Both metrics degrade simultaneously (fundamental feature space corruption)
- **First 3 experiments**:
  1. Train baseline with cross-entropy + response-based KD, measure Rs metric degradation
  2. Implement SupCon-only training, compare Rs with cross-entropy baseline
  3. Add RKD to SupCon training, verify Rs improvement and evaluate OSR performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific features of class-incremental learning cause feature space distortions that degrade open set recognition performance?
- Basis in paper: [explicit] The paper identifies feature distortion as a key problem, stating "We discern that feature spaces undergo distortions during incremental training, leading to intertwined inlier and outlier features—a probable cause of the catastrophic forgetting phenomenon."
- Why unresolved: The paper identifies feature distortion as a problem but does not analyze which specific aspects of incremental learning (e.g., forgetting of certain class representations, changes in feature relations) are responsible for the distortion.
- What evidence would resolve it: Detailed analysis of feature space changes during incremental learning, identifying which feature dimensions or class representations are most affected and why.

### Open Question 2
- Question: Are the features that matter most for open set recognition the same as those that matter for inlier classification in class-incremental learning?
- Basis in paper: [inferred] The paper mentions that "The central challenge is maintaining discernible inlier and outlier features post-training" but does not investigate whether the optimal feature representations for these two tasks are the same or different.
- Why unresolved: The paper develops a unified framework but does not analyze whether the feature representations optimal for open set recognition overlap with those optimal for inlier classification, or whether there might be trade-offs between the two objectives.
- What evidence would resolve it: Empirical studies comparing feature importance for OSR versus inlier classification, or theoretical analysis of the relationship between these two objectives.

### Open Question 3
- Question: How can other continual learning methodologies be adapted to better support open set recognition?
- Basis in paper: [explicit] The paper states "Enhancing other continual learning methodologies for outlier detection is essential" in the conclusion.
- Why unresolved: While the paper proposes its own method, it does not investigate how to modify existing continual learning approaches (like EWC, SI, or others) to better preserve feature relations needed for open set recognition.
- What evidence would resolve it: Experimental results showing modified versions of existing continual learning methods that incorporate feature relation preservation, compared to the proposed method.

### Open Question 4
- Question: What is the relationship between catastrophic forgetting and feature space distortion in the context of open set recognition?
- Basis in paper: [explicit] The paper states "We discern that feature spaces undergo distortions during incremental training, leading to intertwined inlier and outlier features—a probable cause of the catastrophic forgetting phenomenon."
- Why unresolved: The paper suggests a connection between feature distortion and catastrophic forgetting but does not investigate the causal relationship or determine which is the cause and which is the effect.
- What evidence would resolve it: Experiments that separately control for catastrophic forgetting and feature distortion to determine their individual and combined effects on OSR performance.

## Limitations
- The framework's reliance on exemplars introduces memory constraints that scale with the number of classes and samples per class
- Isometric sampling may not adequately capture highly imbalanced or multi-modal class distributions
- The assumption that feature relations preserved by RKD remain discriminative across all incremental sessions may not hold for drastically different data distributions

## Confidence

- **High confidence**: The mechanism of feature distortion during incremental learning and its negative impact on OSR performance is well-established in the literature and supported by empirical evidence.
- **Medium confidence**: The effectiveness of relation-based knowledge distillation for preserving feature relations is demonstrated but could benefit from more extensive ablation studies across different datasets and architectures.
- **Medium confidence**: The superiority of supervised contrastive learning for OSR is supported by the Rs metric comparison but requires validation on more diverse datasets.

## Next Checks

1. Conduct experiments with varying memory budgets to quantify the trade-off between exemplar storage and performance in both tasks.
2. Test the framework on datasets with significant domain shifts between incremental sessions to evaluate robustness to distribution changes.
3. Implement and compare alternative exemplar selection strategies (e.g., clustering-based, uncertainty-aware) to assess the impact of the isometric sampling approach.