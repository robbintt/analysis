---
ver: rpa2
title: 'DoUnseen: Tuning-Free Class-Adaptive Object Detection of Unseen Objects for
  Robotic Grasping'
arxiv_id: '2304.02833'
source_url: https://arxiv.org/abs/2304.02833
tags:
- object
- objects
- dataset
- zero-shot
- classi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a two-stage zero-shot object detector for robotic
  grasping applications where objects are not pre-learned. The method combines an
  unseen object segmentation network with a zero-shot classifier that can add new
  objects by capturing a few images, without retraining.
---

# DoUnseen: Tuning-Free Class-Adaptive Object Detection of Unseen Objects for Robotic Grasping

## Quick Facts
- arXiv ID: 2304.02833
- Source URL: https://arxiv.org/abs/2304.02833
- Reference count: 40
- Two-stage zero-shot detector achieves 51.0% mAP on DoPose dataset when combining segmentation and classification

## Executive Summary
This work presents a zero-shot object detector for robotic grasping that can detect previously unseen objects without requiring training data for those specific objects. The method combines an unseen object segmentation network with a zero-shot classifier, enabling detection of new objects by simply capturing a few images to create a gallery set. The approach is evaluated on two datasets (DoPose and HOPE), demonstrating practical performance in less cluttered environments with distinguishable objects, though challenges remain for highly similar objects or cluttered scenes.

## Method Summary
The DoUnseen detector uses a two-stage pipeline: first segmenting all objects from an input image using a pretrained unseen object segmentation network, then classifying each segmented object using a zero-shot siamese classifier that compares features against a gallery set of object images. The zero-shot classifier is trained on the FewSol dataset using a ViT backbone and cosine similarity metric. Gallery images are augmented with 45-degree rotations to improve robustness to viewpoint variations. The system requires no retraining when adding new objects, only capturing 2-10 images per object for the gallery set.

## Key Results
- 51.0% mAP on DoPose dataset when using both segmentation and classification
- 28.4% mAP on HOPE dataset when using both segmentation and classification
- Segmentation-only performance: 70.3% mAP (DoPose) and 47.2% mAP (HOPE)
- Trained Mask R-CNN achieves 84.5% mAP on DoPose and 23.8% mAP on HOPE

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage architecture enables modular development and independent optimization
- Mechanism: Segmentation and classification can be trained separately on different datasets, allowing focused improvements
- Core assumption: Segmentation quality is sufficient for classification and errors don't cascade
- Evidence: "Our main idea is to break the segmentation pipelines into two steps by combining unseen object segmentation networks cascaded by class-adaptive classifiers."

### Mechanism 2
- Claim: Zero-shot classification enables detection without retraining
- Mechanism: Siamese network computes similarity scores between segmented object features and gallery object features
- Core assumption: Gallery images capture object appearance variations
- Evidence: "The zero-shot classifier is a siamese neural network of 2 backbones, giving a score for the similarity between 2 images."

### Mechanism 3
- Claim: Gallery image augmentation improves classification robustness
- Mechanism: Rotating gallery images in 45-degree increments creates 8X more samples
- Core assumption: Rotation captures sufficient viewpoint variations
- Evidence: "The gallery images are augmented by rotating multiples of 45 degrees, generating 8X more images."

## Foundational Learning

- Concept: Zero-shot vs few-shot learning distinction
  - Why needed: Paper explicitly distinguishes these approaches and their tradeoffs
  - Quick check: What is the key difference between zero-shot and few-shot learning in this work?

- Concept: Siamese network architecture and feature comparison
  - Why needed: Core mechanism for zero-shot classification
  - Quick check: How does a siamese network determine if two images contain the same object class?

- Concept: Object segmentation in cluttered environments
  - Why needed: Performance varies significantly based on environment setup
  - Quick check: Why would highly similar objects or cluttered scenes pose particular challenges for this approach?

## Architecture Onboarding

- Component map: Input RGB image -> Unseen object segmentation network -> Zero-shot siamese classifier -> Gallery set management -> Output detected objects with class labels

- Critical path: 1) Segment all objects from input image, 2) Extract features from each segmented mask, 3) Compare features against gallery set, 4) Assign most similar class label, 5) Output detection results

- Design tradeoffs:
  - Two-stage vs end-to-end: Two-stage allows modular development but may propagate segmentation errors
  - Zero-shot vs few-shot: Zero-shot requires no training but may be less accurate
  - Gallery size vs computation: Larger galleries improve coverage but increase classification time

- Failure signatures:
  - High segmentation error rates indicate problems with segmentation network or challenging scenes
  - Low classification accuracy despite good segmentation suggests gallery images don't capture variations
  - Performance gap between DoPose and HOPE indicates sensitivity to object distinguishability

- First 3 experiments:
  1. Evaluate segmentation-only performance on test datasets to establish baseline
  2. Test zero-shot classifier with ground truth masks to isolate classification performance
  3. Run end-to-end system with varying gallery sizes to assess scalability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can zero-shot object detectors be improved to handle highly similar objects in cluttered environments?
- Basis: Paper notes performance varies from practical to unsuitable depending on environment setup and object similarity
- Why unresolved: Identified as limitation but no solutions provided for highly similar objects or cluttered scenes
- What evidence would resolve: Comparative studies showing performance improvements with different techniques for highly similar objects

### Open Question 2
- Question: What is the optimal dataset size and composition for training zero-shot classifiers?
- Basis: Paper discusses need for datasets with large numbers of objects and occurrences
- Why unresolved: Identifies this as hurdle but doesn't determine optimal balance between objects and occurrences
- What evidence would resolve: Systematic experiments varying dataset composition and measuring resulting performance

### Open Question 3
- Question: How does incorporating spatial information about embedding placement affect performance?
- Basis: Authors mention future work on zero-shot classifier using spatial placement of embeddings
- Why unresolved: Proposed as future work but not implemented or tested
- What evidence would resolve: Experimental results comparing traditional siamese approaches with spatial embedding methods

## Limitations

- Performance highly sensitive to object distinguishability and scene clutter, with 51.0% vs 28.4% mAP across datasets
- Limited validation of gallery size requirements beyond 2-10 images per object
- No comparison with few-shot approaches that might provide better accuracy when some training is acceptable

## Confidence

- High confidence: Two-stage architecture design and modular benefits
- Medium confidence: Zero-shot classification performance claims
- Low confidence: Assumption that 2-10 gallery images are sufficient across diverse scenarios

## Next Checks

1. Cross-dataset robustness test: Evaluate on a third dataset with varying clutter levels and object similarity
2. Gallery size ablation study: Systematically vary gallery image counts (1, 2, 5, 10, 20) to determine minimum effective size
3. Comparison with few-shot baseline: Implement few-shot fine-tuning approach using same gallery images for accuracy comparison