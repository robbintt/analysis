---
ver: rpa2
title: 'Learning to Explain: A Model-Agnostic Framework for Explaining Black Box Models'
arxiv_id: '2310.16584'
source_url: https://arxiv.org/abs/2310.16584
tags:
- pages
- explainer
- conference
- explanation
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Learning to Explain (LTX) is a model-agnostic framework for generating
  post-hoc explanations for vision models. LTX employs an "explainer" model that creates
  explanation maps, highlighting crucial regions justifying model predictions.
---

# Learning to Explain: A Model-Agnostic Framework for Explaining Black Box Models

## Quick Facts
- arXiv ID: 2310.16584
- Source URL: https://arxiv.org/abs/2310.16584
- Reference count: 40
- Primary result: LTX achieves NEG scores of 64.83 (predicted class) and 65.49 (target class) for ViT-B, significantly outperforming existing explanation methods

## Executive Summary
Learning to Explain (LTX) introduces a model-agnostic framework for generating post-hoc explanations for black box vision models. The framework employs an explainer model that creates explanation maps highlighting crucial regions justifying model predictions. LTX trains the explainer through a two-stage process: pretraining on diverse data followed by per-instance finetuning. The framework uses a counterfactual objective that anticipates the model's output on masked input versions, enabling it to work with both Transformer-based and convolutional models without requiring access to internal parameters.

## Method Summary
LTX consists of an explainer model that generates explanation maps, which are used to mask input images before feeding them to the explained model. The framework employs a two-stage training process: pretraining the explainer on a diverse dataset to learn general patterns, followed by per-instance finetuning to refine explanations for specific inputs. During both stages, LTX compares the explained model's predictions for masked and unmasked inputs, using a loss function that penalizes discrepancies. The explainer is trained to minimize the difference between predictions on original and masked inputs while encouraging focused, meaningful masks.

## Key Results
- LTX achieved NEG scores of 64.83 (predicted class) and 65.49 (target class) for ViT-B models, surpassing other methods
- For ResNet101 models, LTX showed NEG scores of 61.13 (predicted class) and 62.80 (target class)
- LTX excelled in POS, INS, and DEL metrics, with qualitative results showing more focused and consistent explanation maps compared to alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LTX uses a counterfactual prediction task to train the explainer to anticipate the model's output on masked inputs.
- Mechanism: By comparing the explained model's predictions on masked vs. unmasked inputs, the explainer learns which regions are most important for maintaining the original prediction.
- Core assumption: The explained model's predictions on masked inputs will change predictably based on which regions are masked.
- Evidence anchors:
  - [abstract]: "This approach enables the use of a novel counterfactual objective, which aims to anticipate the model's output using masked versions of the input image."
  - [section]: "To train the explainer, we employ a two-stage process consisting of initial pretraining followed by per-instance finetuning. During both stages of training, we utilize a unique configuration where we compare the explained model's prediction for a masked input with its original prediction for the unmasked input."

### Mechanism 2
- Claim: The two-stage training process (pretraining + per-instance finetuning) allows LTX to create both general and instance-specific explanations.
- Mechanism: Pretraining on a diverse dataset teaches the explainer general patterns of important regions, while finetuning on specific instances refines these patterns for individual cases.
- Core assumption: The explainer can learn transferable knowledge from the pretraining phase that can be refined for specific instances.
- Evidence anchors:
  - [abstract]: "To train the explainer, we employ a two-stage process consisting of initial pretraining followed by per-instance finetuning."
  - [section]: "In the pretraining phase, the explainer eϕ is optimized on a dataset X... In the finetuning phase, LTX further refines the explainer's parameters per a specific instance, enhancing its ability to provide a tailored explanation."

### Mechanism 3
- Claim: LTX's architecture-agnostic design allows it to explain both Transformer-based and convolutional models without requiring access to internal parameters.
- Mechanism: By only requiring the ability to compute gradients with respect to the input, LTX can work with any model architecture, including black box models.
- Core assumption: The explained model can compute gradients with respect to its input.
- Evidence anchors:
  - [abstract]: "Importantly, the LTX framework is not restricted to a specific model architecture and can provide explanations for both Transformer-based and convolutional models."
  - [section]: "Unlike non-agnostic explanation methods, LTX does not require access to the internal parameters of the explained model, and its sole prerequisite is the ability to derive gradients w.r.t. the explained model's input."

## Foundational Learning

- Concept: Counterfactual reasoning
  - Why needed here: LTX relies on understanding how changes to input (masking) affect the model's predictions, which is a form of counterfactual reasoning.
  - Quick check question: Can you explain what would happen to a model's prediction if we masked out the most important regions of an input image?

- Concept: Gradient-based optimization
  - Why needed here: LTX uses gradient descent to train the explainer, requiring an understanding of how gradients propagate through the explained model to the explainer.
  - Quick check question: How do gradients flow from the explained model's output back to the explainer during training?

- Concept: Transfer learning
  - Why needed here: The pretraining phase of LTX leverages transfer learning principles to teach the explainer general patterns before refining for specific instances.
  - Quick check question: Why might it be beneficial to pretrain the explainer on a diverse dataset before finetuning on individual instances?

## Architecture Onboarding

- Component map:
  Explained model (fθ) -> Masking function (ψ) -> Explainer model (eϕ) -> Loss function (LLTX)

- Critical path:
  1. Pretrain explainer on diverse dataset
  2. For each instance to explain:
     a. Finetune explainer on that instance
     b. Generate explanation map
     c. Apply mask and compare predictions

- Design tradeoffs:
  - Simple masking (constant value) vs. more complex masking strategies
  - Hard masking (thresholding) vs. soft masking (linear blending)
  - Including optional loss terms (Linv, Lsmooth) vs. simpler objective

- Failure signatures:
  - Explainer produces uniform or random masks
  - Finetuning doesn't improve explanation quality
  - Explanation maps don't align with human intuition of important regions

- First 3 experiments:
  1. Train explainer on a small dataset with a simple CNN, verify it can produce meaningful masks
  2. Apply LTX to a pretrained ResNet on ImageNet, compare explanations to Grad-CAM
  3. Test LTX on a ViT model, evaluate performance on both predicted and target class explanations

## Open Questions the Paper Calls Out
1. Does LTX's effectiveness depend on the choice of masking mechanism (e.g., constant value vs. learned parameter vs. stochastic approach)?
2. Would incorporating the optional loss terms (Linv and Lsmooth) into LTX's optimization objective improve performance on certain metrics?
3. How would more sophisticated upsampling techniques (e.g., learning designated decoders) affect LTX's performance on CNN explanations?

## Limitations
- Counterfactual assumption validity: LTX assumes masking regions will cause predictable changes in predictions, which may not hold for complex modern vision models
- Training data dependence: The pretraining phase relies on 2000 randomly selected ImageNet samples, which could impact generalizability
- Computational cost: Per-instance finetuning makes LTX potentially expensive for applications requiring many explanations

## Confidence
- Primary explanation quality claims: High
- Architecture-agnostic capability: High
- Two-stage training benefits: Medium

## Next Checks
1. Ablation study on pretraining: Evaluate LTX performance when skipping pretraining and training only with per-instance finetuning
2. Robustness to masking strategy: Test LTX with different masking mechanisms (constant value vs. learned masks, hard vs. soft masking)
3. Cross-architecture generalization: Apply an explainer pretrained on CNNs to explain Transformer models (or vice versa) to test true architecture-agnostic capability