---
ver: rpa2
title: Multi-Objective Genetic Algorithm for Multi-View Feature Selection
arxiv_id: '2305.18352'
source_url: https://arxiv.org/abs/2305.18352
tags:
- features
- feature
- selection
- mmfs-ga
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a multi-view multi-objective feature selection
  genetic algorithm (MMFS-GA) to address the challenge of high-dimensionality in multi-view
  datasets. MMFS-GA uses a parallelized genetic algorithm with a two-step process:
  intra-view feature selection (IV-FS) and between-view feature selection (BV-FS).'
---

# Multi-Objective Genetic Algorithm for Multi-View Feature Selection

## Quick Facts
- arXiv ID: 2305.18352
- Source URL: https://arxiv.org/abs/2305.18352
- Reference count: 15
- Key outcome: MMFS-GA achieved a balanced accuracy of 0.949 for AD vs MCI vs NC classification on the TADPOLE dataset, compared to 0.855 for the best baseline method

## Executive Summary
This paper proposes a multi-view multi-objective feature selection genetic algorithm (MMFS-GA) to address the challenge of high-dimensionality in multi-view datasets. The method uses a parallelized genetic algorithm with a two-step process: intra-view feature selection (IV-FS) and between-view feature selection (BV-FS). IV-FS selects informative features from each view independently, while BV-FS combines these features to identify the optimal subset of views. The method was evaluated on synthetic and real-world datasets, including ADNI data, and outperformed state-of-the-art methods in both binary and multiclass classification tasks.

## Method Summary
MMFS-GA employs a two-step genetic algorithm approach to multi-view feature selection. The first step (IV-FS) uses parallelized genetic algorithms with multiniche populations to independently select informative features from each view. The second step (BV-FS) takes the solutions from IV-FS and uses another genetic algorithm to combine them, selecting the optimal subset of views and unifying features across views. The method adapts the NSGA-II selection operator and uses crowding distance to maintain population diversity.

## Key Results
- MMFS-GA outperformed state-of-the-art methods in both binary and multiclass classification tasks
- Achieved a balanced accuracy of 0.949 for AD vs MCI vs NC classification on the TADPOLE dataset
- Showed significant improvements over baseline methods including LASSO-SVM, SFFS, M2TFS, IMTFS, and ASMFS

## Why This Works (Mechanism)

### Mechanism 1
The parallelized GA with intra-view (IV-FS) and between-view (BV-FS) steps reduces high-dimensionality in multi-view data while preserving discriminative features. IV-FS uses N niches to independently evolve populations for each view, selecting informative features within that view. BV-FS then uses integer-encoded chromosomes to combine solutions from IV-FS, selecting the optimal subset of views and unifying features across views. This two-step process ensures both local and global optimization.

### Mechanism 2
The multiniche GA structure prevents premature convergence and maintains diversity, enabling better exploration of the solution space. N niches evolve independently, each maintaining its own population. Migration occurs every 5% of generations, swapping the top 25% of individuals between niches. This allows niches to explore different regions of the solution space while still sharing high-quality solutions.

### Mechanism 3
The duplicate elimination strategy using Jaccard similarity prevents population stagnation and maintains diversity during evolution. When pairwise similarity between individuals exceeds 0.8, both crossover and mutation are applied with 0.9 probability to replace duplicate individuals. This ensures the population remains diverse and avoids premature convergence.

## Foundational Learning

- **Concept: Genetic Algorithms and Multi-Objective Optimization**
  - Why needed here: MMFS-GA uses a genetic algorithm framework with multi-objective optimization to simultaneously minimize classification error and the number of selected features
  - Quick check question: What are the two main objectives being optimized in MMFS-GA, and how are they balanced?

- **Concept: NSGA-II and Crowding Distance**
  - Why needed here: MMFS-GA adapts the selection operator from NSGA-II, which uses crowding distance to maintain diversity in the population
  - Quick check question: How does crowding distance help maintain diversity in the population during evolution?

- **Concept: Feature Selection in Multi-View Data**
  - Why needed here: MMFS-GA specifically addresses the challenge of feature selection in multi-view data, where each view represents a different modality or characteristic of the data
  - Quick check question: What are the main challenges of feature selection in multi-view data compared to single-view data?

## Architecture Onboarding

- **Component map**: IV-FS (N niches) -> BV-FS (single population) -> Optimal solution
- **Critical path**: 1) Initialize N niches with random populations for IV-FS. 2) Evolve populations in IV-FS for each view. 3) Output solution sets from IV-FS. 4) Initialize population for BV-FS. 5) Evolve population in BV-FS. 6) Output optimal solution from BV-FS
- **Design tradeoffs**: The two-step approach (IV-FS followed by BV-FS) allows for both local and global optimization but adds computational complexity. The multiniche structure improves diversity but requires careful tuning of migration frequency
- **Failure signatures**: Poor performance may indicate issues with population diversity (e.g., premature convergence), inappropriate similarity thresholds, or inadequate exploration of the solution space
- **First 3 experiments**:
  1. Run MMFS-GA on a simple synthetic multi-view dataset with known ground truth to verify basic functionality
  2. Compare performance of MMFS-GA with and without the duplicate elimination strategy to assess its impact
  3. Vary the number of niches (N) and migration frequency to find optimal values for a given dataset

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of MMFS-GA compare to other multi-view feature selection methods on datasets with significantly imbalanced class distributions?
- Basis in paper: [explicit] The paper demonstrates MMFS-GA's effectiveness on synthetic and real-world datasets, including binary and multiclass classification tasks. However, the paper does not specifically address the performance of MMFS-GA on imbalanced datasets.
- Why unresolved: The paper does not provide any information or experiments related to imbalanced datasets, which are common in real-world applications. Understanding the algorithm's performance in such scenarios is crucial for its practical applicability.
- What evidence would resolve it: Conducting experiments on datasets with imbalanced class distributions and comparing MMFS-GA's performance to other methods would provide insights into its robustness and effectiveness in handling such cases.

### Open Question 2
How does the choice of hyperparameters, such as the number of niches, migration rate, and mutation rate, affect the performance of MMFS-GA?
- Basis in paper: [inferred] The paper mentions the use of six niches, a migration rate of 25%, and specific probabilities for crossover and mutation. However, it does not explore the impact of varying these hyperparameters on the algorithm's performance.
- Why unresolved: Hyperparameter tuning is a crucial aspect of any machine learning algorithm, and understanding the sensitivity of MMFS-GA to these parameters is essential for its practical implementation. The paper does not provide insights into the optimal or range of values for these hyperparameters.
- What evidence would resolve it: Conducting experiments with different combinations of hyperparameters and analyzing their impact on MMFS-GA's performance would help determine the optimal settings and the algorithm's sensitivity to these parameters.

### Open Question 3
How does MMFS-GA handle missing data in multi-view datasets?
- Basis in paper: [inferred] The paper does not explicitly address the handling of missing data in multi-view datasets. However, it mentions the use of synthetic and real-world datasets, which may contain missing values.
- Why unresolved: Missing data is a common challenge in real-world applications, and understanding how MMFS-GA handles such cases is crucial for its practical applicability. The paper does not provide any information or experiments related to missing data.
- What evidence would resolve it: Conducting experiments on datasets with missing values and comparing MMFS-GA's performance to other methods would provide insights into its ability to handle missing data effectively. Additionally, exploring different strategies for handling missing data within the MMFS-GA framework would be beneficial.

## Limitations

- The method's performance heavily depends on parameter tuning (niche count, migration frequency, similarity thresholds), which was not exhaustively explored
- The use of synthetic datasets with controlled properties provides clean validation but may not capture real-world complexity and noise patterns
- The evaluation focuses primarily on classification accuracy metrics without extensive ablation studies to isolate the contribution of individual components

## Confidence

- **High**: The core two-step optimization approach (IV-FS followed by BV-FS) is well-founded and mechanistically sound
- **Medium**: The multiniche genetic algorithm structure improves diversity and prevents premature convergence, though empirical validation is limited
- **Low**: The specific parameter values (similarity threshold of 0.8, migration frequency of 5%) are justified but may be dataset-dependent

## Next Checks

1. Conduct sensitivity analysis across different values of N (number of niches) and migration frequencies to identify robust parameter settings
2. Test MMFS-GA on additional real-world multi-view datasets with varying dimensionalities and noise characteristics to assess generalizability
3. Perform ablation studies comparing MMFS-GA with and without the duplicate elimination strategy and multiniche structure to quantify their individual contributions