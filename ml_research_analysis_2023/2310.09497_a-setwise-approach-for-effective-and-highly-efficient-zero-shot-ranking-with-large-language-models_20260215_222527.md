---
ver: rpa2
title: A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with
  Large Language Models
arxiv_id: '2310.09497'
source_url: https://arxiv.org/abs/2310.09497
tags:
- setwise
- ranking
- pairwise
- listwise
- approaches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of efficiently ranking documents
  using large language models (LLMs) in a zero-shot setting. It systematically evaluates
  three existing approaches: Pointwise, Pairwise, and Listwise prompting, revealing
  inherent trade-offs between effectiveness and efficiency.'
---

# A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models

## Quick Facts
- arXiv ID: 2310.09497
- Source URL: https://arxiv.org/abs/2310.09497
- Authors: 
- Reference count: 33
- Primary result: Setwise prompting significantly improves LLM reranking efficiency while maintaining high ranking effectiveness compared to Pointwise, Pairwise, and Listwise approaches

## Executive Summary
This paper addresses the efficiency challenge of using large language models (LLMs) for zero-shot document ranking. The authors systematically evaluate three existing prompting approaches‚ÄîPointwise, Pairwise, and Listwise‚Äîrevealing inherent trade-offs between effectiveness and efficiency. They propose a novel Setwise prompting approach that reduces LLM inference count by comparing multiple documents simultaneously in a single inference step. This method significantly accelerates both Pairwise and Listwise approaches while maintaining high ranking effectiveness, achieving consistent improvements across popular benchmarks like TREC DL and BEIR.

## Method Summary
The paper introduces Setwise prompting as a novel approach to LLM-based document ranking that compares multiple documents simultaneously rather than pairwise. Unlike traditional Pointwise (document-level), Pairwise (comparison-based), and Listwise (full-list generation) methods, Setwise prompting instructs the LLM to select the most relevant document from a set of candidates in a single inference. This approach is integrated with sorting algorithms like heap sort and bubble sort, reducing the total number of comparisons needed. For Listwise methods, Setwise prompting leverages output logits instead of generating complete ranking lists, enabling more efficient inference. The method is evaluated on TREC DL 2019/2020 and BEIR datasets using Flan-t5 LLMs of varying sizes.

## Key Results
- Setwise prompting reduces LLM inference count by up to 67% compared to Pairwise approaches while maintaining similar ranking effectiveness
- The approach consistently outperforms traditional methods across multiple datasets, with NDCG@10 improvements of up to 0.015 on TREC DL benchmarks
- Setwise methods demonstrate superior robustness to variations in initial ranking quality, performing consistently across BM25, inverted BM25, and random initial rankings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Setwise prompting reduces LLM inference count by comparing multiple documents in a single inference step instead of pairwise comparisons.
- Mechanism: Instead of comparing documents two at a time (as in Pairwise approaches), Setwise prompting asks the LLM to select the most relevant document from a set of documents in one inference. This leverages the fact that sorting algorithms like heap sort and bubble sort can be accelerated when each comparison operation handles multiple items simultaneously.
- Core assumption: The LLM can effectively rank relevance among multiple documents in a single inference without significant loss of accuracy compared to pairwise comparisons.
- Evidence anchors:
  - [abstract]: "Our Setwise approach, instead, reduces the number of LLM inferences and the amount of prompt token consumption during the ranking procedure, compared to previous methods."
  - [section]: "Our Setwise prompting approach instructs LLMs to select the most relevant document to the query from a set of candidate documents. This straightforward adjustment allows the sorting algorithms to infer relevance preferences for more than two candidate documents at each step, thus significantly reducing the total number of comparisons required"
  - [corpus]: Found related work on "Beyond Reproducibility: Advancing Zero-shot LLM Reranking Efficiency with Setwise Insertion" suggesting the efficiency claims are being independently investigated.
- Break condition: If the LLM cannot effectively rank multiple documents simultaneously, or if the prompt length limitations prevent including enough documents in each comparison set, the efficiency gains would diminish.

### Mechanism 2
- Claim: Setwise prompting enables Listwise methods to use model output logits instead of generating complete ranking lists.
- Mechanism: Traditional Listwise approaches generate an entire ordered list of document labels, which is computationally expensive. Setwise prompting allows Listwise methods to instead evaluate the likelihood of each document being the most relevant using output logits, which requires only a single forward pass of the LLM.
- Core assumption: The LLM's output logits contain sufficient information to accurately estimate the likelihood of each document being most relevant, and accessing these logits is feasible.
- Evidence anchors:
  - [section]: "Setwise prompting again provides a solution: we can easily derive an ordered list of document labels from the LLM output logits. This is done by assessing the likelihood of each document label being chosen as the most relevant"
  - [section]: "Our Setwise prompting empowers the previous Listwise approach (listwise.generation), which relied on LLM's next token generations, to now utilize the LLM's output logits."
  - [corpus]: Weak evidence - no direct corpus neighbors discuss this specific logit-based Listwise acceleration mechanism.
- Break condition: If the LLM model or API does not expose output logits, or if the logit-based ranking is significantly less accurate than generation-based ranking.

### Mechanism 3
- Claim: Setwise prompting improves robustness to initial ranking order compared to traditional Pairwise and Listwise methods.
- Mechanism: By comparing multiple documents simultaneously and leveraging logit-based likelihood estimation, Setwise methods are less sensitive to the specific ordering of documents in the initial ranking set.
- Core assumption: The simultaneous comparison of multiple documents and logit-based evaluation reduces the dependency on initial ordering that exists in sequential comparison methods.
- Evidence anchors:
  - [section]: "Setwise sorting enhances Pairwise and Listwise robustness to variations in the internal ordering quality of the initial rankings: no matter what the initial ordering of the top-k documents to rank is, our method provides consistent and effective results."
  - [section]: "Both listwise.likelihood and setwise.bubblesort exhibit large improvements over listwise.generate and pairwise.bubblesort, in the case of the inverted BM25 ranking and randomly shuffled BM25 ranking."
  - [corpus]: Weak evidence - the corpus mentions "Beyond Reproducibility: Advancing Zero-shot LLM Reranking Efficiency with Setwise Insertion" but doesn't provide specific robustness evidence.
- Break condition: If the LLM's ability to rank multiple documents simultaneously is highly sensitive to the specific documents presented together, the robustness advantage may not materialize.

## Foundational Learning

- Concept: Sorting algorithms and their computational complexity
  - Why needed here: Understanding how heap sort and bubble sort work is essential to grasp how Setwise prompting accelerates these algorithms by comparing multiple items per operation instead of just pairs.
  - Quick check question: How does the time complexity of heap sort change when comparing c items at a time instead of 2 items at a time?

- Concept: Transformer-based LLM inference and token generation
  - Why needed here: Understanding that each generated token requires an additional LLM inference helps explain why Listwise approaches that generate entire ranking lists are inefficient, and why Setwise prompting that uses logits is more efficient.
  - Quick check question: Why does generating an ordered list of document labels require more LLM inferences than evaluating logits for document relevance?

- Concept: Prompt engineering and token limitations
  - Why needed here: Understanding prompt length limitations (e.g., 512 tokens for Flan-T5) is crucial for implementing Setwise prompting effectively, as it determines how many documents can be included in each comparison set.
  - Quick check question: If each document is truncated to 45 tokens and you want to compare 9 documents simultaneously, how many tokens does this consume from a 512-token budget?

## Architecture Onboarding

- Component map: BM25 retrieval -> Document truncation -> Setwise prompt construction -> LLM inference -> Likelihood computation -> Sorting algorithm -> Top-k ranking output
- Critical path: Document truncation ‚Üí Setwise prompt construction ‚Üí LLM inference ‚Üí Likelihood computation ‚Üí Sorting algorithm ‚Üí Top-k ranking output
- Design tradeoffs:
  - Larger comparison sets (higher c) reduce inference count but require more aggressive document truncation, potentially hurting accuracy
  - Using generation vs. logits affects both efficiency and which LLM models can be used
  - Sorting algorithm choice (heap vs. bubble) affects efficiency and sensitivity to initial ranking order
  - Batch inference is possible for initial comparisons but not for subsequent steps that depend on previous results
- Failure signatures:
  - Prompt too long: LLM API returns error or truncates input unexpectedly
  - Poor ranking accuracy: Setwise method performs worse than pairwise despite fewer inferences
  - Inconsistent results: Rankings vary significantly with different initial orderings
  - Efficiency not improving: Inference count reduction doesn't match theoretical expectations
- First 3 experiments:
  1. Implement basic Setwise heap sort with c=3 and verify it produces the same rankings as pairwise heap sort on a small dataset
  2. Measure efficiency gains by comparing inference counts and latency between Setwise and pairwise implementations on TREC DL 2019
  3. Test robustness by running the same method on BM25, inverted BM25, and random initial rankings to verify consistency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of Setwise prompting vary across different LLM model sizes and architectures beyond Flan-T5?
- Basis in paper: [explicit] The paper tests Setwise prompting on Flan-T5 models of varying sizes (780M, 3B, 11B parameters) and finds consistent improvements, but does not explore other LLM architectures like LLaMA, GPT models, or newer models like GPT-4.
- Why unresolved: The paper's scope is limited to Flan-T5 models due to their open-source nature and availability of logits. Testing other models would require different access methods (e.g., API calls) and potentially different prompt engineering.
- What evidence would resolve it: Empirical results showing effectiveness and efficiency of Setwise prompting on a diverse set of LLM architectures, including both open-source and closed-source models.

### Open Question 2
- Question: What is the impact of different sorting algorithms (e.g., quicksort, mergesort) when combined with Setwise prompting on efficiency and effectiveness?
- Basis in paper: [inferred] The paper compares heap sort and bubble sort with Setwise prompting, finding heap sort to be more efficient. However, it does not explore other sorting algorithms that might offer different trade-offs.
- Why unresolved: The paper focuses on heap sort and bubble sort as they were previously used with Pairwise prompting. Other sorting algorithms might be more suitable for the Setwise approach and could provide better efficiency-effectiveness trade-offs.
- What evidence would resolve it: Comparative results of various sorting algorithms combined with Setwise prompting across multiple datasets, showing their respective efficiency and effectiveness.

### Open Question 3
- Question: How does the optimal value of the hyperparameter ùëê (number of compared documents) vary across different datasets and LLM model sizes?
- Basis in paper: [explicit] The paper investigates the impact of ùëê on effectiveness and efficiency, finding that larger ùëê reduces latency but may degrade effectiveness due to document truncation. However, the optimal ùëê is not determined and may vary across conditions.
- Why unresolved: The paper uses a fixed ùëê=3 for main results and only explores a limited range of ùëê values (3, 5, 7, 9) with document truncation. The optimal value may depend on dataset characteristics, LLM model size, and other factors.
- What evidence would resolve it: Systematic experiments varying ùëê across multiple datasets and LLM models, identifying optimal values and their relationships with dataset characteristics and model properties.

## Limitations
- The paper focuses exclusively on Flan-T5 models, limiting generalizability to other LLM architectures that may not expose output logits
- Effectiveness gains may diminish when increasing the comparison size c due to aggressive document truncation required to fit within prompt length constraints
- The approach's performance on real-world document collections with different characteristics remains untested beyond standard benchmarks

## Confidence

- **High Confidence**: The core claim that Setwise prompting reduces LLM inference count compared to Pairwise approaches by comparing multiple documents simultaneously is well-supported by the theoretical analysis and experimental results.
- **Medium Confidence**: The claim about Listwise methods benefiting from Setwise prompting through logit-based ranking is supported but depends on the availability of output logits and their effective use for ranking.
- **Medium Confidence**: The robustness improvements to initial ranking order are demonstrated on the tested benchmarks but may vary with different document characteristics or larger k values.

## Next Checks

1. **Prompt Template Verification**: Implement and test multiple variations of Setwise prompt templates to determine which structure yields optimal results and verify the claimed efficiency gains across different formulations.

2. **Cross-Model Generalization**: Test the Setwise approach on different LLM architectures (e.g., LLaMA, OPT) to verify that the efficiency improvements are not specific to Flan-T5 models.

3. **Scalability Assessment**: Evaluate the Setwise approach on larger document collections and higher k values (e.g., top-1000 documents) to determine if the efficiency and effectiveness advantages persist at scale.