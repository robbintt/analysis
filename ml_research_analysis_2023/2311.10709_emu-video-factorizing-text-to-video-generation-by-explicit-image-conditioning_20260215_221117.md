---
ver: rpa2
title: 'Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning'
arxiv_id: '2311.10709'
source_url: https://arxiv.org/abs/2311.10709
tags:
- video
- videos
- generation
- text
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: E MU VIDEO factorizes text-to-video generation into first generating
  an image from text and then generating video conditioned on that image and text,
  enabling direct high-resolution video generation without cascaded models. The method
  employs zero terminal-SNR noise schedules and multi-stage training for improved
  quality.
---

# Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning

## Quick Facts
- arXiv ID: 2311.10709
- Source URL: https://arxiv.org/abs/2311.10709
- Reference count: 40
- One-line primary result: Emu Video outperforms prior text-to-video models by 81-96% on human evaluation metrics

## Executive Summary
Emu Video presents a novel approach to text-to-video generation by factorizing the task into two stages: first generating an image from text, then generating video conditioned on both the image and text. This method employs a zero terminal-SNR noise schedule and multi-stage training to directly produce high-quality, high-resolution videos without requiring cascaded models. The approach demonstrates significant improvements over prior work, achieving state-of-the-art results on human evaluation metrics for both quality and image animation tasks.

## Method Summary
Emu Video uses a two-stage generation process: first generating a 512px image from text using a pretrained text-to-image model, then generating video conditioned on both the image and text using a latent diffusion model. The method employs a zero terminal-SNR noise schedule to correct train-test discrepancies and uses multi-stage training (256px 8fps followed by 512px 4fps) for improved efficiency. An optional interpolation model can increase frame rates from 4fps to 16fps. The model is trained on 34M licensed video-text pairs and shows strong performance across various evaluation metrics.

## Key Results
- 81% better than Imagen Video, 90% better than PYOCO, and 96% better than Make-A-Video on quality
- 96% better than VideoComposer on image animation tasks
- Achieves 512px resolution at 4fps with optional interpolation to 16fps
- Demonstrates strong zero-shot performance on UCF101

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factorizing text-to-video generation into two stages reduces complexity of modeling high-dimensional spatiotemporal data
- Mechanism: First generates high-quality starting frame from text, then predicts future frames conditioned on both text and image
- Core assumption: Video generation benefits from concrete starting image rather than text alone
- Evidence anchors: [abstract] and [section 3.2] describe design decisions and conditioning importance
- Break condition: Poor quality or inconsistent generated image leads to poor video generation

### Mechanism 2
- Claim: Zero terminal-SNR noise schedule corrects train-test discrepancy for high-resolution video
- Mechanism: Ensures terminal timestep has zero SNR during training, matching inference condition
- Core assumption: High-resolution video frames have more redundant pixels, exacerbating train-test discrepancy
- Evidence anchors: [section 3.2] and [section 4.1] demonstrate critical importance for high resolution
- Break condition: Improper noise schedule scaling or terminal SNR setting causes generalization failure

### Mechanism 3
- Claim: Multi-stage training improves efficiency and quality
- Mechanism: Initial 256px 8fps training learns temporal dynamics quickly, followed by 512px 4fps fine-tuning
- Core assumption: Temporal parameters learned at low resolution transfer well to high resolution
- Evidence anchors: [section 3.3] and [section 4.1] show significant benefits of multi-stage approach
- Break condition: Poor generalization of temporal parameters from low to high resolution

## Foundational Learning

- Concept: Diffusion models iteratively denoise random noise to generate data
  - Why needed: Emu Video uses latent diffusion models for video generation
  - Quick check: What is the role of noise schedule in diffusion models and how does it affect denoising?

- Concept: Conditioning signals guide generative model outputs
  - Why needed: Emu Video conditions on both text and image for video generation
  - Quick check: How does combining text and image conditioning differ from text alone and why is it beneficial?

- Concept: Multi-stage training involves phases with different objectives
  - Why needed: Emu Video uses two-stage training for efficient learning
  - Quick check: What advantages does multi-stage training provide and how does it impact performance?

## Architecture Onboarding

- Component map: Text-to-Image Model -> Image Generation -> Latent Diffusion Model (with Image+Text Conditioning) -> Video Generation -> Optional Interpolation Model
- Critical path: Generate image from text → Generate video from image+text → (Optional) Increase frame rate
- Design tradeoffs: Factorized approach trades flexibility for quality/consistency; zero terminal-SNR improves high-res generation but requires tuning; multi-stage training adds complexity but improves efficiency
- Failure signatures: Poor image generation cascades to video generation; incorrect noise schedule settings cause train-test mismatch; insufficient low-res training leads to poor temporal modeling
- First 3 experiments:
  1. Train image generation stage with pretrained text-to-image model and evaluate image quality
  2. Train video generation stage with zero terminal-SNR noise schedule and evaluate 512px video quality
  3. Implement multi-stage training and compare performance with/without initial low-res stage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance change when using different initialization images from other text-to-image models?
- Basis: Paper mentions potential for using user-provided images as conditioning
- Why unresolved: Only compares Emu Video's performance using its own generated images
- What evidence would resolve it: Human evaluations comparing video generations with images from different T2I models as conditioning

### Open Question 2
- Question: Can interpolation model be improved for higher frame rates or longer videos while maintaining quality?
- Basis: Paper describes current interpolation model with 8→37 frame conversion
- Why unresolved: Only evaluates current interpolation model without exploring improvements
- What evidence would resolve it: Experiments with different interpolation architectures/training strategies

### Open Question 3
- Question: How does Emu Video perform on video editing or video-to-video translation tasks?
- Basis: Paper mentions potential for image animation and other applications
- Why unresolved: Focuses only on text-to-video generation and image animation
- What evidence would resolve it: Adapting Emu Video to specific video editing/translation tasks and evaluating performance

## Limitations
- Proprietary dataset of 34M video-text pairs limits reproducibility and generalizability
- Model can only generate one video at a time, not supporting story continuation
- Performance evaluation primarily based on human ratings without detailed methodology
- Success may depend on specific hyperparameter choices not fully specified

## Confidence
- High confidence: Factorized approach and zero terminal-SNR noise schedule are theoretically sound
- Medium confidence: 81-96% improvement over baselines, but methodology details and dataset constraints reduce certainty
- Low confidence: Claims about generalization to arbitrary text prompts and diverse scenarios lack comprehensive quantitative analysis

## Next Checks
1. Attempt to train image generation stage on publicly available text-to-image datasets to assess dataset dependency
2. Systematically vary terminal-SNR settings to quantify impact on video quality at different resolutions
3. Train full model with and without initial low-resolution stage while keeping all other parameters constant to isolate multi-stage contribution