---
ver: rpa2
title: Towards Continually Learning Application Performance Models
arxiv_id: '2310.16996'
source_url: https://arxiv.org/abs/2310.16996
tags:
- learning
- performance
- data
- drift
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the problem of performance model degradation\
  \ in HPC systems due to concept drift caused by hardware/software changes. The authors\
  \ adapt six continual learning approaches\u2014EWC, Synaptic Intelligence, LwF,\
  \ A-GEM, GSS Greedy, and GDumb\u2014to performance modeling, converting the regression\
  \ task into a classification problem by discretizing outputs."
---

# Towards Continually Learning Application Performance Models

## Quick Facts
- arXiv ID: 2310.16996
- Source URL: https://arxiv.org/abs/2310.16996
- Reference count: 26
- One-line primary result: GDumb achieved 2× improvement in prediction accuracy compared to naive approach for HPC performance modeling under concept drift

## Executive Summary
This paper addresses the challenge of performance model degradation in HPC systems caused by concept drift from hardware degradation, software upgrades, and system changes. The authors convert the regression problem into a classification task by discretizing continuous performance metrics into 10 bins, enabling the application of established continual learning algorithms. They adapt six continual learning approaches (EWC, Synaptic Intelligence, LwF, A-GEM, GSS Greedy, and GDumb) to HPC performance modeling and evaluate them across three task sequences representing system changes.

## Method Summary
The authors formulate HPC performance modeling as a classification problem by discretizing continuous outputs into 10 equally spaced intervals, transforming regression into multi-class classification. They adapt six continual learning algorithms from the Avalanche framework to handle real concept drift scenarios where the functional relationship between inputs and outputs changes across tasks. The approach uses TOKIO framework to collect I/O performance data from 8 HPC applications on the Cori system, extracting features from LMT, Slurm, and Darshan logs. Models are trained sequentially across three task sequences representing software upgrade events, with evaluation using accuracy and forgetting metrics compared against a naive approach.

## Key Results
- GDumb outperformed all other methods, successfully retaining knowledge from Task 1 to Task 3
- The best model demonstrated a 2× improvement in prediction accuracy compared to the naive approach
- GDumb achieved superior performance by balancing samples across classes and retraining from scratch with rehearsal memory
- Memory-based methods (GDumb, GSS Greedy) showed better retention than regularization-based methods (EWC, SI) in this real concept drift scenario

## Why This Works (Mechanism)

### Mechanism 1
Converting regression to classification via discretization enables adaptation of existing virtual concept drift methods to real concept drift scenarios. The continuous output range [0.0, 1.0] is divided into 10 equally spaced bins, transforming the problem into a multi-class classification task. This allows algorithms like EWC, LwF, and GDumb—designed for classification—to be directly applied. The core assumption is that discretization preserves sufficient information for performance prediction without significant loss in predictive power.

### Mechanism 2
GDumb retains prior knowledge and achieves superior accuracy by balancing samples across classes and retraining from scratch with rehearsal memory. The method stores samples balanced over observed classes and at test time learns a new model from scratch with the rehearsal memory. This approach avoids catastrophic forgetting by maintaining a representative sample of previous tasks. The core assumption is that the memory buffer size is sufficient to store representative samples from all previous tasks without excessive bias toward recent tasks.

### Mechanism 3
Real concept drift in HPC systems is driven by changes in the functional relationship between inputs and outputs (P(y|x)) while input distribution remains constant. System changes (hardware degradation, software upgrades) alter the underlying performance model, causing the same input features to produce different outputs over time. Continual learning algorithms adapt to these shifts by penalizing changes to important parameters or using episodic memory. The core assumption is that the input distribution P(x) remains stable across tasks, allowing the model to focus on adapting the functional relationship P(y|x).

## Foundational Learning

- Concept: Continual learning and catastrophic forgetting
  - Why needed here: HPC systems experience data distribution shifts due to hardware/software changes, causing models trained on past data to degrade. Continual learning algorithms prevent catastrophic forgetting by retaining knowledge of previous tasks while adapting to new ones.
  - Quick check question: What is the difference between real concept drift and virtual concept drift, and why is real concept drift more challenging in HPC performance modeling?

- Concept: Elastic Weight Consolidation (EWC) and Synaptic Intelligence (SI)
  - Why needed here: These regularization-based methods identify and protect important model parameters from being overwritten when learning new tasks, which is crucial for maintaining performance across system changes.
  - Quick check question: How do EWC and SI measure parameter importance, and what is the key difference in their regularization terms?

- Concept: Discretization of continuous outputs for classification
  - Why needed here: Converting the regression problem to classification allows the use of well-established continual learning algorithms designed for virtual concept drift, enabling the adaptation of methods like GDumb and EWC to real drift scenarios.
  - Quick check question: What are the potential drawbacks of discretizing continuous performance metrics, and how might bin size affect model accuracy?

## Architecture Onboarding

- Component map: TOKIO framework (Darshan logs, LMT, Slurm) -> Feature extraction (filesystem traffic, system load) -> PyTorch with Avalanche framework -> Continual learning algorithms (EWC, SI, LwF, A-GEM, GSS Greedy, GDumb) -> Evaluation (accuracy, forgetting metrics)

- Critical path:
  1. Collect I/O performance data and system metrics
  2. Detect software/hardware changes and split data into tasks
  3. Convert regression outputs to classification bins
  4. Train model on first task
  5. For each subsequent task:
     - Update model using continual learning algorithm
     - Evaluate accuracy and forgetting
  6. Compare performance against naive approach

- Design tradeoffs:
  - Memory-based vs. regularization-based methods: Memory-based (GDumb, GSS Greedy) require storing samples but can provide better performance; regularization-based (EWC, SI) are more memory-efficient but may suffer more from catastrophic forgetting.
  - Discretization granularity: Finer bins preserve more information but increase classification complexity; coarser bins simplify the problem but may lose important distinctions.
  - Memory buffer size: Larger buffers improve retention of prior knowledge but increase computational and storage costs.

- Failure signatures:
  - Catastrophic forgetting: Accuracy on prior tasks drops significantly after training on new tasks
  - Overfitting to recent tasks: Model performs well on current task but poorly on previous tasks
  - Poor discretization: Discretized bins do not capture important performance distinctions, leading to reduced accuracy

- First 3 experiments:
  1. Train naive model on single task and evaluate accuracy
  2. Train EWC model on task sequence and compare accuracy and forgetting to naive approach
  3. Train GDumb model on task sequence with varying memory sizes to find optimal buffer capacity

## Open Questions the Paper Calls Out

### Open Question 1
How would the continual learning performance change if we used the original regression formulation instead of discretizing the output into 10 regimes? The authors converted the regression problem into classification by discretizing the continuous output range into 10 equally spaced intervals, noting this was done "for simplicity, in addition to the potential to adapt the high-performing algorithms from the literature as well as software that have been primarily targeting the classification scenarios." This remains unresolved as the paper does not compare the classification approach to a true regression approach in the continual learning context.

### Open Question 2
What is the optimal memory size for GSS Greedy and GDumb methods, and how sensitive are these methods to memory constraints? The authors state "we fixed the memory size to obtain fair results and comparisons" and note that "GSS Greedy also has the best accuracy when training a single task" but "suffers greatly from catastrophic forgetting after training Task 3" which "can be attributed to its memory size limitation and poor greedy selection of prior task knowledge." The paper uses a fixed memory size (5000) without exploring how performance scales with different memory sizes.

### Open Question 3
How would the continual learning performance be affected if the task boundaries (software upgrade events) were detected automatically rather than provided manually? The authors note "To date, the task id as described in Section 2 was collected manually from the server operators. This can be automated by using the concept drift-aware modeling created by Madireddy et al. [6] which automatically identifies the points where concept drift occurs in near-real time." The paper uses manually identified task boundaries but acknowledges this is a limitation that could be addressed with automated drift detection.

## Limitations

- The discretization approach may lose fine-grained performance distinctions that are important for certain HPC applications
- Evaluation uses synthetic task sequences based on software upgrade timestamps rather than naturally occurring drift patterns
- Assumes stable input distributions across tasks, which may not hold in all HPC environments
- Memory-based methods like GDumb have significant computational overhead due to storage and retraining requirements

## Confidence

- Discretization approach effectiveness: Medium - demonstrates improved accuracy but may lose important performance distinctions
- 2× accuracy improvement claim: Medium - based on synthetic task sequences rather than natural drift patterns
- Memory buffer optimization: Low - fixed buffer size used without systematic exploration of optimal capacity

## Next Checks

1. Test the discretization approach with varying bin sizes (5, 10, 20, 50) to determine sensitivity to granularity and identify optimal bin count for different application types

2. Validate the approach on naturally occurring concept drift patterns in production HPC systems rather than synthetic task sequences based on software upgrade timestamps

3. Conduct ablation studies comparing performance between discretization-based classification and direct regression approaches under continual learning frameworks using the same dataset and evaluation metrics