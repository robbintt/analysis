---
ver: rpa2
title: 'Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning
  of the Quantal Stackelberg Equilibrium from Strategic Feedbacks'
arxiv_id: '2307.14085'
source_url: https://arxiv.org/abs/2307.14085
tags:
- follower
- where
- have
- leader
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies reinforcement learning for learning a Quantal
  Stackelberg Equilibrium (QSE) in an episodic Markov game with a leader-follower
  structure. The key challenge is that the leader cannot observe the follower's reward
  and needs to infer the follower's quantal response model from his actions against
  leader's policies.
---

# Actions Speak What You Want: Provably Sample-Efficient Reinforcement Learning of the Quantal Stackelberg Equilibrium from Strategic Feedbacks

## Quick Facts
- **arXiv ID**: 2307.14085
- **Source URL**: https://arxiv.org/abs/2307.14085
- **Reference count**: 40
- **Primary result**: Sample-efficient RL algorithms for learning Quantal Stackelberg Equilibrium in leader-follower Markov games with strategic feedbacks

## Executive Summary
This paper addresses the challenge of learning a Quantal Stackelberg Equilibrium (QSE) in episodic Markov games where the leader cannot observe the follower's rewards. The key insight is that the leader can infer the follower's quantal response model from the follower's actions using maximum likelihood estimation. The paper proposes both online and offline algorithms that achieve sublinear regret bounds, leveraging optimism/pessimism principles to handle uncertainty in the learned follower model. A novel performance-difference lemma is introduced to analyze the suboptimality of the learned leader policy.

## Method Summary
The method consists of two main components: (1) learning the follower's quantal response model via maximum likelihood estimation from the follower's actions against the leader's policies, and (2) using model-free or model-based reinforcement learning to solve the leader's decision-making problem under the learned follower model. For online learning, an optimistic approach is used to encourage exploration, while for offline learning, a pessimistic approach handles data coverage issues. The algorithms are analyzed using a novel performance-difference lemma that incorporates errors in the quantal response model.

## Key Results
- Sublinear regret bounds achieved for online learning algorithms
- Suboptimality bounds established for offline learning algorithms
- Novel performance-difference lemma that incorporates quantal response model error
- Sample-efficient learning of follower's quantal response model from actions without reward observations

## Why This Works (Mechanism)

### Mechanism 1
The leader can efficiently learn the follower's quantal response model from the follower's actions without observing rewards. Uses maximum likelihood estimation (MLE) to infer the follower's reward function from the follower's actions against the leader's policies. The quantal response is modeled as an entropy-regularized policy optimization problem, making the follower's policy unique and estimable. Core assumption: The follower's quantal response is unique for any leader policy and depends only on the follower's reward function.

### Mechanism 2
Sample-efficient learning of the leader's optimal policy is achieved through a combination of pessimism/optimism and model-based or model-free RL. Constructs confidence sets for both the response model and leader's value function, then uses optimistic planning for online learning or pessimistic planning for offline learning to ensure exploration or data coverage. Core assumption: The confidence sets accurately quantify uncertainty in both the follower's response model and the leader's value function estimates.

### Mechanism 3
The bilevel optimization structure of the QSE problem is tractable through a novel performance-difference lemma. Decomposes the suboptimality of the leader's learned policy into the leader's Bellman error and the follower's quantal response error, enabling analysis that bridges the upper and lower level problems. Core assumption: The performance difference lemma accurately captures the relationship between estimation errors and policy suboptimality.

## Foundational Learning

- **Concept: Quantal response and entropy regularization**
  - Why needed here: The follower's bounded rationality is modeled through entropy-regularized policy optimization, leading to a unique quantal response that can be estimated from actions.
  - Quick check question: Why does entropy regularization ensure a unique quantal response for the follower?

- **Concept: Maximum likelihood estimation (MLE)**
  - Why needed here: MLE is used to infer the follower's reward function from the follower's actions, which is crucial since the leader cannot observe rewards.
  - Quick check question: How does MLE guarantee convergence to the true follower's reward function?

- **Concept: Confidence sets and optimism/pessimism**
  - Why needed here: Confidence sets quantify uncertainty in both the follower's response model and the leader's value function, enabling efficient exploration or handling of data coverage issues.
  - Quick check question: What is the difference between optimism and pessimism in the context of RL, and why are they needed here?

## Architecture Onboarding

- **Component map**: Leader policy -> Follower action -> MLE response model -> Leader value function update -> Optimism/Pessimism principle -> Next leader policy

- **Critical path**: 1) Leader announces policy, 2) Follower takes action based on quantal response, 3) Leader updates estimates of follower's model and own policy, 4) Repeat until convergence

- **Design tradeoffs**:
  - Model-free vs. model-based RL: Model-free is simpler but may be less sample-efficient; model-based can be more efficient but requires learning the transition model
  - Optimism vs. Pessimism: Optimism encourages exploration in online learning; pessimism handles data coverage issues in offline learning
  - Myopic vs. Farsighted follower: Myopic follower is simpler to learn but less realistic; farsighted follower is more complex but captures long-term planning

- **Failure signatures**:
  - Non-convergence: If the follower's response model is not accurately learned, the leader's policy may not converge
  - Poor performance: If the confidence sets are not valid, the leader's policy may be suboptimal
  - High sample complexity: If the follower's response model is complex or the state space is large, the algorithm may require many samples to learn effectively

- **First 3 experiments**:
  1. Verify MLE convergence: Test if the MLE algorithm accurately recovers the follower's reward function from actions in a simple environment
  2. Test optimism/pessimism: Compare the performance of the optimistic and pessimistic algorithms in an online learning setting with a known follower model
  3. Evaluate sample efficiency: Measure the sample complexity of the algorithm in a more complex environment with a farsighted follower

## Open Questions the Paper Calls Out

### Open Question 1
How can the linear constraint ⟨x, r_h(s,a,·)⟩ = ς be relaxed or removed for the farsighted follower case without losing identifiability of the follower's reward function? The paper states this constraint is "introduced to ensure that r can be uniquely identified in the quantal response model" and "is not without loss of generality" for the farsighted case.

### Open Question 2
What are the computational challenges and potential solutions for implementing the algorithms with general function approximation, particularly in the online setting? While sample efficiency is established, the computational complexity of the algorithms for general function approximation is not discussed, which is crucial for practical implementation.

### Open Question 3
How does the performance of the proposed algorithms scale with the size of the action spaces A and B, particularly in the case of continuous action spaces? The theoretical bounds involve terms like |B| and the covering number of the policy class, but the practical implications of large or continuous action spaces are not explored.

## Limitations

- Assumes follower's quantal response is uniquely determined by entropy regularization, which may not hold for all bounded rationality models
- Requires the follower to play according to a quantal response model, limiting applicability to followers with different behavioral patterns
- Computational complexity for general function approximation is not fully addressed, particularly in online settings

## Confidence

- **High confidence**: The MLE-based follower model learning approach and its theoretical guarantees for myopic followers
- **Medium confidence**: The pessimism/optimism principles for offline/online learning
- **Medium confidence**: The error decomposition lemma for quantifying suboptimality

## Next Checks

1. Test algorithm robustness when follower's actual behavior deviates from quantal response (e.g., with noise or alternative bounded rationality models)
2. Verify performance-difference lemma empirically across varying levels of response model error and game complexity
3. Benchmark sample efficiency against benchmark leader-follower RL methods in multi-stage games with farsighted followers