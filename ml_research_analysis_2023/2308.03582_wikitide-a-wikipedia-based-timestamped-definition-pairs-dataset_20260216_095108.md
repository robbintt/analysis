---
ver: rpa2
title: 'WIKITIDE: A Wikipedia-Based Timestamped Definition Pairs Dataset'
arxiv_id: '2308.03582'
source_url: https://arxiv.org/abs/2308.03582
tags:
- pdef
- arxiv
- language
- definition
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WikiTiDe is a Wikipedia-based dataset of timestamped definition
  pairs aimed at helping language models learn new information. It uses a bootstrapping
  approach to create high-quality training data, starting with a seed set annotated
  by GPT-3.
---

# WIKITIDE: A Wikipedia-Based Timestamped Definition Pairs Dataset

## Quick Facts
- arXiv ID: 2308.03582
- Source URL: https://arxiv.org/abs/2308.03582
- Reference count: 29
- Key outcome: WikiTiDe is a Wikipedia-based dataset of timestamped definition pairs aimed at helping language models learn new information. It uses a bootstrapping approach to create high-quality training data, starting with a seed set annotated by GPT-3.

## Executive Summary
WikiTiDe addresses the challenge of training language models to detect and adapt to temporal changes in knowledge resources like Wikipedia. The dataset consists of 10,000 timestamped definition pairs, with 3,000 manually annotated pairs used as a seed set. A bootstrapping approach is employed to iteratively expand the dataset by adding high-confidence predictions from models trained on the growing dataset. This process leads to improved model performance on tasks such as WiC-TSV, demonstrating the value of WikiTiDe in capturing temporal concept changes and enhancing model generalization.

## Method Summary
The WikiTiDe dataset is created using a bootstrapping approach that starts with a seed set of 3,000 manually annotated definition pairs extracted from Wikipedia. The annotations are performed using four GPT-3 instances with varying prompts, classifying pairs into three categories: same, different but not fundamental, and fundamental change. The bootstrapping process, described in Algorithm 2, iteratively trains models on the current dataset and applies them to unannotated data, adding the top-K most confident predictions for each label to the training set. This process continues until the desired dataset size of 10,000 pairs is reached. The final dataset is evaluated on tasks such as WiC-TSV, showing improved performance compared to models trained only on the seed set.

## Key Results
- WikiTiDe dataset of 10,000 timestamped definition pairs created using bootstrapping approach
- Bootstrapped models consistently outperform base models across different models and metrics
- Improved performance on WiC-TSV task demonstrates dataset's effectiveness in capturing temporal concept changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bootstrapping improves performance by iteratively adding high-confidence predictions to the training set.
- Mechanism: Models trained on the initial annotated seed set are applied to unannotated data, and the top-K most confident predictions for each label are added to the training set. This process repeats, increasing the size and diversity of the training data over time.
- Core assumption: High-confidence predictions are accurate and representative of the underlying concept changes in the data.
- Evidence anchors:
  - [section] The bootstrapping process is described in Algorithm 2, where a model is trained on the training set (T S), applied to the development set (DS), and the K most confident predictions for each label are added to T S.
  - [section] The results section shows that bootstrapped models consistently outperform their base counterparts across different models and metrics.
- Break condition: If the initial model is poor at identifying true concept changes, high-confidence predictions may be incorrect, leading to noisy training data and degraded performance.

### Mechanism 2
- Claim: The bootstrapping process leads to a more diverse training set, improving the model's ability to generalize to unseen examples.
- Mechanism: As the bootstrapping process iterates, the model is exposed to a wider variety of definition pairs that exhibit different types of changes (e.g., semantic drift, core updates). This increased diversity in the training data helps the model learn more robust representations of the concept changes.
- Core assumption: The unannotated data contains a diverse set of concept changes that are not present in the initial seed set.
- Evidence anchors:
  - [section] The analysis of semantic drift shows that the bootstrapped training set exhibits an increasingly diverse set of definitions, measured by increasing cosine distances between definition pairs.
  - [section] The results show that the bootstrapped models consistently outperform their base counterparts, suggesting improved generalization.
- Break condition: If the unannotated data lacks diversity in concept changes or if the initial model is already overfit to the seed set, the bootstrapping process may not lead to significant improvements.

### Mechanism 3
- Claim: Using Wikipedia as a source of definition pairs provides a rich and continuously updated resource for training models on temporal concept changes.
- Mechanism: Wikipedia articles are regularly updated with new information, and these updates often result in changes to the definitions of concepts, events, or named entities. By extracting definition pairs from Wikipedia and annotating them for changes, the dataset captures these temporal shifts in knowledge.
- Core assumption: Wikipedia is a reliable source of information and that changes to definitions reflect genuine updates in our understanding of concepts.
- Evidence anchors:
  - [abstract] The paper argues that Wikipedia provides a "dynamically updated life-long resource" for training models on temporal changes.
  - [section] The paper discusses how the bootstrapping process leads to better fine-tuned models, suggesting that the Wikipedia-derived dataset is valuable for this task.
- Break condition: If Wikipedia entries are frequently vandalized or contain inaccurate information, the resulting dataset may be noisy and unreliable.

## Foundational Learning

- Concept: Bootstrapping in machine learning
  - Why needed here: The paper relies on bootstrapping to iteratively improve the quality of the dataset by adding high-confidence predictions to the training set.
  - Quick check question: How does bootstrapping differ from traditional supervised learning, and what are its advantages and disadvantages?

- Concept: Semantic drift and temporal changes in language
  - Why needed here: The paper focuses on detecting changes in definitions over time, which requires an understanding of how language and meaning can evolve.
  - Quick check question: What are some examples of semantic drift in language, and how can they impact the interpretation of definitions?

- Concept: Wikipedia as a knowledge base and its dynamics
  - Why needed here: The paper uses Wikipedia as the source of definition pairs, so understanding its structure, update process, and potential biases is crucial.
  - Quick check question: How does Wikipedia's editing process work, and what measures are in place to ensure the quality and reliability of its content?

## Architecture Onboarding

- Component map: Data collection -> Annotation -> Bootstrapping -> Evaluation
- Critical path: Data collection → Annotation → Bootstrapping → Evaluation
- Design tradeoffs:
  - Using GPT-3 for annotation instead of human annotators: Faster and potentially more consistent, but may lack domain expertise and introduce biases.
  - Bootstrapping vs. traditional supervised learning: Bootstrapping can lead to more diverse training data but may be slower and require careful monitoring of model performance.
- Failure signatures:
  - Bootstrapped models perform worse than base models: Indicates issues with the bootstrapping process, such as incorrect high-confidence predictions or lack of diversity in the unannotated data.
  - Models struggle to generalize to the WiC-TSV task: Suggests that the WIKI TIDE dataset may not fully capture the nuances of semantic changes or that the task setup needs refinement.
- First 3 experiments:
  1. Run Algorithm 1 on a small subset of Wikipedia pages to verify the data collection process and ensure the quality of the extracted definition pairs.
  2. Test the annotation process using GPT-3 with different prompts and compare the results to manual annotations to assess the reliability and consistency of the automated annotations.
  3. Implement Algorithm 2 with a simple classifier (e.g., logistic regression) on a small sample of the data to verify the bootstrapping process and ensure that high-confidence predictions are being correctly identified and added to the training set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of bootstrapped data change over multiple iterations, and is there an optimal number of iterations for the bootstrapping process?
- Basis in paper: [explicit] The paper mentions that the bootstrapping process is iterative and that the model is evaluated after each iteration. It also discusses the effects of bootstrapping on the semantic drift of the dataset.
- Why unresolved: The paper does not provide a detailed analysis of the quality of bootstrapped data over multiple iterations or identify an optimal number of iterations.
- What evidence would resolve it: A study analyzing the quality of bootstrapped data at each iteration and determining the point at which additional iterations do not significantly improve the model's performance.

### Open Question 2
- Question: How does the performance of the model on the WiC-TSV task compare to other unsupervised and supervised approaches?
- Basis in paper: [explicit] The paper evaluates the model on the WiC-TSV task and compares its performance to other approaches, but does not provide a comprehensive comparison.
- Why unresolved: The paper does not provide a detailed comparison of the model's performance to other approaches on the WiC-TSV task.
- What evidence would resolve it: A study comparing the model's performance on the WiC-TSV task to other unsupervised and supervised approaches, using the same evaluation metrics and datasets.

### Open Question 3
- Question: How does the performance of the model on the WiC-TSV task change when using different types of definitions (e.g., dictionary definitions, Wikipedia definitions)?
- Basis in paper: [explicit] The paper mentions that the WiC-TSV task requires definitions as input, and that the model uses ChatGPT-generated definitions when the original input does not contain a definition.
- Why unresolved: The paper does not explore how the performance of the model on the WiC-TSV task changes when using different types of definitions.
- What evidence would resolve it: A study evaluating the model's performance on the WiC-TSV task using different types of definitions, such as dictionary definitions, Wikipedia definitions, and ChatGPT-generated definitions, and comparing the results.

## Limitations
- Annotation quality uncertainty: Reliance on GPT-3 for 7,000 of 10,000 pairs introduces potential noise and biases
- Limited temporal generalization: Dataset focuses exclusively on Wikipedia, may not generalize to other knowledge sources
- Parameter sensitivity: Bootstrapping process parameters (K=10, T>0) lack precise details, affecting reproducibility

## Confidence
- **High Confidence**: The core bootstrapping methodology and its superiority over base models (demonstrated through consistent performance improvements across different models and metrics)
- **Medium Confidence**: The quality and representativeness of the final 10,000-pair dataset. While the paper shows improvements on WiC-TSV, the extent to which the dataset captures genuine temporal concept changes versus annotation artifacts remains uncertain
- **Low Confidence**: The generalizability of the approach to other knowledge sources or domains beyond Wikipedia

## Next Checks
1. **Annotation Quality Validation**: Compare GPT-3 annotations against human annotations on a held-out sample of 100 pairs to quantify agreement rates and identify systematic biases in the automated annotation process

2. **Bootstrapping Parameter Sensitivity**: Systematically vary K (from 5 to 50) and T (0.1 to 1.0) in the bootstrapping algorithm to determine optimal parameter settings and assess robustness to hyperparameter choices

3. **Temporal Generalization Test**: Evaluate models trained on WIKITIDE on definition pairs from Wikipedia pages updated during a different time period than those used in the original dataset to assess temporal generalization capabilities