---
ver: rpa2
title: 'O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making
  with Large Language Models'
arxiv_id: '2310.14403'
source_url: https://arxiv.org/abs/2310.14403
tags:
- object
- tips
- cabinet
- item
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: O3D (Offline Data-driven Discovery and Distillation) is a framework
  for improving LLM-powered sequential decision-making agents using large-scale offline
  interaction data without model finetuning. It discovers reusable skills by segmenting
  offline trajectories and distills generalizable knowledge through trajectory contrasting.
---

# O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models

## Quick Facts
- arXiv ID: 2310.14403
- Source URL: https://arxiv.org/abs/2310.14403
- Reference count: 40
- Key outcome: O3D framework improves LLM-powered sequential decision-making agents by 34% in ALFWorld and 15% in WebShop using offline data without model finetuning

## Executive Summary
O3D introduces a framework for enhancing LLM-powered sequential decision-making agents using large-scale offline interaction data without requiring model finetuning. The method automatically discovers reusable skills by segmenting offline trajectories and distills generalizable knowledge through trajectory contrasting. O3D works for both text-based and code-based LLM policies and consistently outperforms baselines on ALFWorld and WebShop benchmarks across various GPT models.

## Method Summary
O3D is a three-stage framework that improves LLM-powered agents by leveraging offline interaction data. First, it discovers reusable skills by segmenting trajectories and abstracting primitive actions. Second, it distills policy improvement tips through trajectory contrasting between successful and failed demonstrations. Third, it constructs a hierarchical policy that sequentially calls appropriate skills based on the task. The framework operates without finetuning the underlying LLM, instead using in-context learning to extract and apply knowledge from offline data.

## Key Results
- Achieves up to 34% improvement in success rate on ALFWorld compared to ReAct baseline
- Improves product matching score by 15% on WebShop benchmark
- Outperforms Demo2Code baseline across GPT-4, GPT-3.5-0613, and GPT-3.5-0301 models
- Works effectively for both text-based and code-based LLM policies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-powered policies can be improved without finetuning by distilling reusable skills and generalizable knowledge from large-scale offline interaction data.
- Mechanism: The framework segments offline trajectories into reusable skills, discovers primitive actions, and distills policy improvement tips through trajectory contrasting. This knowledge is then injected into prompts to guide downstream task solving.
- Core assumption: LLMs can effectively segment trajectories into meaningful skills and extract generalizable patterns from both successful and failed demonstrations.
- Evidence anchors:
  - [abstract]: "automatically discovers reusable skills and distills generalizable knowledge across multiple tasks based on offline interaction data"
  - [section 3.3]: "The first stage enables the LLM to discover and abstract reusable skills from offline datasets... The second stage then conducts skill-conditioned policy improvement by distilling knowledge from offline data"
  - [corpus]: Weak evidence - corpus neighbors focus on offline learning and distillation but don't directly address skill discovery in LLMs
- Break condition: If LLM cannot reliably segment trajectories or extract meaningful patterns from raw interaction logs, the skill discovery and knowledge distillation would fail.

### Mechanism 2
- Claim: Hierarchical policy execution with skill-conditioned policies reduces the complexity of long-horizon tasks for LLM agents.
- Mechanism: The framework first discovers skills from offline data, then learns skill-conditioned policies, and finally constructs a base policy that sequentially calls appropriate skills based on the task. This decomposition simplifies complex tasks into manageable subtasks.
- Core assumption: Complex sequential decision-making tasks can be decomposed into a finite set of reusable skills that are composable across different tasks.
- Evidence anchors:
  - [abstract]: "The method works for both text-based and code-based LLM policies and consistently outperforms baselines"
  - [section 3.3]: "The first stage enables the LLM to discover and abstract reusable skills from offline datasets... The final stage is to construct the interactive policy by calling the learned skills given diverse tasks"
  - [corpus]: Moderate evidence - corpus includes related work on hierarchical reinforcement learning and skill discovery, supporting the concept of temporal abstraction
- Break condition: If discovered skills are not truly reusable across tasks or if the skill composition mechanism fails, the hierarchical approach would not provide benefits.

### Mechanism 3
- Claim: Trajectory contrasting between successful and failed demonstrations enables more effective policy improvement tips distillation compared to using only successful examples.
- Mechanism: The framework samples both high-score (successful) and low-score (failed) trajectories and prompts the LLM to contrast them, identifying key differences and generating actionable improvement tips that help avoid failures.
- Core assumption: Failed demonstrations contain valuable information about what actions to avoid, and LLMs can effectively learn from contrasting successful and failed examples.
- Evidence anchors:
  - [abstract]: "O3D automatically discovers reusable skills and distills generalizable knowledge across multiple tasks based on offline interaction data"
  - [section 3.3]: "Distill Policy Improvement Tips with Trajectory Contrasting... we propose to distill knowledge that can enhance good (i.e., can incur high long-term reward) behaviors and avoid undesired behaviors"
  - [corpus]: Weak evidence - corpus mentions contrastive learning but doesn't specifically address trajectory contrasting for policy improvement
- Break condition: If the LLM cannot effectively identify the key differences between successful and failed trajectories, or if the distilled tips are not actionable, this mechanism would fail to improve policy performance.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The entire framework relies on LLMs learning from examples provided in prompts without parameter updates, making in-context learning the fundamental mechanism
  - Quick check question: Can the LLM successfully complete a simple task when provided with 2-3 relevant examples in the prompt?

- Concept: Trajectory segmentation and skill abstraction
  - Why needed here: The first stage requires identifying meaningful sub-tasks within interaction logs, which is essential for creating reusable skills
  - Quick check question: Given a simple interaction log, can the LLM reliably identify distinct sub-tasks and describe them in abstract terms?

- Concept: Prompt engineering and template design
  - Why needed here: All three stages rely on carefully crafted prompts to extract skills, primitives, and improvement tips from LLMs
  - Quick check question: Can you design a prompt that consistently extracts specific information (e.g., primitive actions) from LLM responses?

## Architecture Onboarding

- Component map:
  Skill Discovery Module -> Primitive Discovery Module -> Knowledge Distillation Module -> Skill-Conditioned Policy Learner -> Base Policy Constructor -> LLM Interface

- Critical path:
  1. Skill Discovery (Stage 1)
  2. Primitive Discovery (Stage 2)
  3. Knowledge Distillation (Stage 2)
  4. Skill-Conditioned Policy Improvement (Stage 2)
  5. Base Policy Construction (Stage 3)
  6. Downstream Task Execution

- Design tradeoffs:
  - Text-based vs Code-based policies: Text offers better reasoning but higher cost; Code offers efficiency but requires validation
  - Batch size in discovery: Larger batches may capture more patterns but risk diluting focus on individual trajectories
  - Contrastive vs non-contrastive distillation: Contrastive methods may provide more actionable tips but require paired failure/success data

- Failure signatures:
  - Skill Discovery fails: Discovered skills are not reusable or miss important task components
  - Primitive Discovery fails: Generated primitives are invalid or incomplete for the environment
  - Knowledge Distillation fails: Tips are too generic or don't address specific failure modes
  - Policy Construction fails: Base policy cannot effectively orchestrate skill calls or gets stuck in loops

- First 3 experiments:
  1. Test skill discovery on a small dataset with known skills to verify segmentation accuracy
  2. Validate primitive discovery by checking if generated actions are executable in the environment
  3. Test knowledge distillation by comparing performance with contrastive vs non-contrastive approaches on a validation task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM-based policy architectures (text-based vs. code-based) compare in terms of reliability and hallucination rates across diverse real-world domains?
- Basis in paper: [explicit] The paper explicitly compares text-based and code-based policies, noting that code-based policies are more interpretable and reliable while text-based policies retain better language understanding and reasoning abilities.
- Why unresolved: The paper only tests these architectures on two specific domains (ALFWorld and WebShop). Real-world environments may present novel failure modes not captured in these benchmarks.
- What evidence would resolve it: Systematic testing across a broader range of real-world interactive environments with varying levels of language complexity and action space constraints.

### Open Question 2
- Question: What is the optimal balance between offline data diversity (including both success and failure examples) versus quality (expert-only demonstrations) for O3D's skill discovery and distillation processes?
- Basis in paper: [explicit] The paper states that O3D can benefit from both positive and negative examples and doesn't require high-quality expert datasets, but doesn't systematically investigate the optimal mix.
- Why unresolved: The paper uses a fixed mix of success and failure data without exploring how different ratios affect performance or whether certain domains benefit more from one type over another.
- What evidence would resolve it: Controlled experiments varying the ratio of success to failure examples and measuring downstream task performance across multiple domains.

### Open Question 3
- Question: How does O3D's performance scale with increasing task complexity and longer horizon problems, particularly when skills become more interdependent and compositional?
- Basis in paper: [inferred] The paper mentions that task complexity increases exponentially with interaction horizon, but only tests on relatively constrained environments with 30-step horizons.
- Why unresolved: The experiments don't test significantly longer horizon tasks or measure how skill interdependencies affect performance as task complexity grows.
- What evidence would resolve it: Testing O3D on domains with substantially longer horizons (100+ steps) and measuring skill composition success rates as task complexity increases.

## Limitations

- Performance depends heavily on the quality and diversity of offline interaction data, which may be limited in real-world applications
- The computational overhead of multiple LLM calls during skill discovery and knowledge distillation phases is not quantified
- Limited ablation studies make it unclear which components of the three-stage framework are essential versus optional

## Confidence

- Empirical results on benchmarks: Medium
- Mechanism effectiveness: Medium
- Scalability to new domains: Low
- Computational efficiency: Low

## Next Checks

1. Run ablation studies removing trajectory contrasting to quantify its contribution to performance improvements
2. Test the framework on a new domain with different interaction patterns to assess generalization
3. Measure the end-to-end computational overhead including all LLM calls during offline processing and online execution