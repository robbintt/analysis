---
ver: rpa2
title: Identifying Linear Relational Concepts in Large Language Models
arxiv_id: '2311.08968'
source_url: https://arxiv.org/abs/2311.08968
tags:
- object
- subject
- layer
- relation
- causality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for identifying interpretable concept
  directions in language model activations by inverting a linear relational embedding
  (LRE) and using earlier object layers. The method, called linear relational concepts
  (LRC), is evaluated on a dataset of relational knowledge and achieves significantly
  better classification accuracy and causality than standard probing classifiers like
  SVMs.
---

# Identifying Linear Relational Concepts in Large Language Models

## Quick Facts
- arXiv ID: 2311.08968
- Source URL: https://arxiv.org/abs/2311.08968
- Reference count: 25
- Key outcome: Linear Relational Concepts (LRC) method achieves significantly better classification accuracy and causality than SVMs by inverting LREs and using earlier object layers.

## Executive Summary
This paper introduces Linear Relational Concepts (LRC), a method for identifying interpretable concept directions in language model activations. The approach models the relation between subject and object as a linear transformation, then inverts this mapping to generate concept directions that can serve as both classifiers and causal interventions. By using earlier object layers and low-rank pseudo-inverses, LRCs outperform standard probing classifiers on relational knowledge datasets while providing meaningful causal control over model outputs.

## Method Summary
The LRC method trains a Linear Relational Embedding (LRE) to map subject activations to object activations using a dataset of relational knowledge. The LRE weight matrix is computed as the mean Jacobian of this mapping. To create concept directions, the method inverts the LRE using a low-rank pseudo-inverse and normalizes the resulting vectors. Classification is performed by selecting the object with the highest dot-product similarity to the concept direction. Causality is tested by editing subject activations along the concept direction and measuring changes in model outputs.

## Key Results
- LRCs achieve significantly higher classification accuracy than SVM baselines on relational knowledge datasets
- Earlier object layers provide better performance than final layers for both classification and causality
- Low-rank pseudo-inverse (rank 192) dramatically improves performance compared to full-rank inversion
- LRCs successfully function as both classifiers and causal interventions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inverting the linear relational embedding (LRE) creates concept directions that act as both classifiers and causal interventions.
- Mechanism: The LRE learns a linear mapping from subject activations to object activations. By inverting this mapping (using a low-rank pseudo-inverse), the system can go from object activations back to subject activations, producing a direction vector that captures the concept. Because the inversion is linear and low-rank, it isolates the most salient features of the relation, which correspond to human-interpretable concepts.
- Core assumption: The model stores relational knowledge in a linear subspace, so a linear transformation can model the subject-to-object relationship.
- Evidence anchors:
  - [abstract]: "We present a technique called linear relational concepts (LRC) for finding concept directions... by first modeling the relation between subject and object as a linear relational embedding (LRE)."
  - [section]: "We generate LRCs using inverted LREs... This allows LRCs to function as both classifiers and causal interventions."
  - [corpus]: Weak. Neighbors discuss SAEs and concept embeddings but do not specifically address inverting LREs.
- Break condition: If the subject-to-object mapping is highly nonlinear, the linear inversion will fail to recover the true concept direction.

### Mechanism 2
- Claim: Using earlier object layers (instead of the final layer) improves classification and causal performance.
- Mechanism: The final layer may already be over-optimized for next-token prediction and lose explicit relational structure. Earlier layers retain richer relational activations that better capture the subject-object mapping. This enables more faithful concept extraction.
- Core assumption: Relational knowledge is more explicit in mid-to-early layers than in the final layer.
- Evidence anchors:
  - [section]: "We also relax the requirement that only the final layer can be used for object activations, since we find that classification performance improves substantially by using non-final layers for the object."
  - [abstract]: "Inverting the LRE while using earlier object layers results in a powerful technique to find concept directions."
  - [corpus]: Weak. Corpus neighbors do not discuss layer choice for relational concepts.
- Break condition: If the earlier layers do not contain sufficient relational structure, performance will degrade.

### Mechanism 3
- Claim: Low-rank pseudo-inverse of the LRE yields better concept directions than full-rank inversion.
- Mechanism: A low-rank approximation captures the dominant subspace of the relational mapping, filtering out noise and overfitting. This distilled subspace aligns with the most important features for the concept, improving generalization.
- Core assumption: The most salient relational features lie in a low-dimensional subspace of the full hidden space.
- Evidence anchors:
  - [section]: "We use a low-rank pseudo-inverse, denoted R† rather than the full matrix inverse R−1... Using a low-rank LRE inverse improves performance dramatically."
  - [abstract]: "Using earlier object layers and a low-rank pseudo-inverse improves performance compared to the original LRE method."
  - [corpus]: Weak. No corpus evidence directly addresses low-rank pseudo-inverses in this context.
- Break condition: If the relational features are distributed across many dimensions, low-rank approximation will miss important information.

## Foundational Learning

- **Linear transformations in neural networks**
  - Why needed here: The method relies on modeling the subject-to-object mapping as an affine linear transformation, then inverting it.
  - Quick check question: What is the form of the linear relational embedding (LRE) mapping, and how is its weight matrix computed?
  - Answer: R(s) = W s + b, where W is the mean Jacobian of F over training samples.

- **Jacobian and Taylor approximation**
  - Why needed here: The LRE weight matrix is computed as the mean Jacobian of the forward function F, approximated via first-order Taylor expansion.
  - Quick check question: How does the Jacobian approximation capture the relation between subject and object activations?
  - Answer: It linearizes the mapping locally, estimating how small changes in the subject activation affect the object activation.

- **Cosine similarity and dot-product classification**
  - Why needed here: LRCs are unit-normalized vectors; classification uses the highest dot-product (cosine similarity) to choose the predicted object.
  - Quick check question: Why is normalizing LRC vectors to unit length useful for classification?
  - Answer: It ensures dot-product equals cosine similarity, making similarity scores comparable across different LRCs.

## Architecture Onboarding

- **Component map**
  - Dataset loader -> prompt generation -> model inference (subject/object activations) -> LRE training -> LRE inversion -> LRC creation -> classifier evaluation -> causality testing
- **Critical path**
  - Prompt generation -> hidden state extraction -> LRE computation -> inversion -> LRC normalization -> classification/causality evaluation
- **Design tradeoffs**
  - Full-rank vs. low-rank inverse: full-rank is more expressive but prone to overfitting; low-rank is more robust but may miss subtle features.
  - Subject layer choice: earlier layers preserve more relational structure but may be less causally effective; later layers are more causally effective but may lose explicit relational features.
  - Object token choice: mean across tokens captures full object representation but increases noise; first token is cleaner but cannot handle multi-token objects.
- **Failure signatures**
  - Classification accuracy drops if LRE training samples are from the same object as the target LRC (counterintuitive effect).
  - Causality performance degrades if subject layer is too late in the model (model attends to unedited earlier layers).
  - Low-rank inversion fails if the true relational subspace is high-dimensional.
- **First 3 experiments**
  1. Train LRE with 5 samples, invert with rank 192, compare classification accuracy to SVM baseline.
  2. Vary object layer (18-31) while keeping subject layer fixed; plot accuracy/causality vs. layer.
  3. Vary LRE inverse rank (e.g., 50, 100, 200, 500, 4096) and measure classification/causality performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying mechanism that causes training the LRE using a sample with the same object as the LRC to result in dramatically worse performance compared to using a sample from a different object in the same relation?
- Basis in paper: [explicit] The paper states that training the LRE using a sample with the same object as the LRC results in dramatically worse performance, but the reason for this is unclear.
- Why unresolved: The paper suspects that choosing samples from different objects may have a regularizing effect on the resulting LRC, but more investigation is needed to understand this phenomenon in-depth.
- What evidence would resolve it: Further experiments comparing the performance of LRCs trained with samples from the same object versus different objects, along with analysis of the resulting LRE weight matrices, could provide insights into the underlying mechanism.

### Open Question 2
- Question: How can we develop a method to combine LRCs learned at multiple layers to find concept directions that achieve both peak classification accuracy and causality, overcoming the trade-off between the two?
- Basis in paper: [inferred] The paper identifies a trade-off between classification accuracy and causality depending on the subject layer of LRC, with earlier layers resulting in better causality but lower classification accuracy.
- Why unresolved: The paper suggests that it may be possible to combine LRCs learned at different layers to find a direction which achieves both peak classification accuracy and causality, but does not explore this further.
- What evidence would resolve it: Developing and testing a method to combine LRCs learned at multiple layers, and evaluating its performance on both classification accuracy and causality metrics, would help determine if this approach can overcome the trade-off.

### Open Question 3
- Question: How can we differentiate between cases where the model guesses the correct answer without having an underlying representation of the concept, and cases where the model has a genuine representation of the concept in its hidden activations?
- Basis in paper: [explicit] The paper acknowledges that the method assumes if the model outputs the correct answer to a prompt, then the model has a representation of this concept in its activations, but this may not always be the case.
- Why unresolved: Differentiating between guessing and knowing is challenging, and the paper does not provide a clear method to address this issue.
- What evidence would resolve it: Developing a method to evaluate the consistency of the model's responses to related prompts, or analyzing the model's behavior when presented with prompts designed to test its understanding of the concept, could help differentiate between guessing and genuine knowledge representation.

## Limitations

- The method relies heavily on the assumption that relational knowledge is linearly representable and extractable through low-rank inversions.
- Performance gains over SVMs may be partially attributed to architectural choices rather than fundamental improvements in concept discovery.
- Evaluation is limited to a specific dataset of relational knowledge, with unknown generalization to other concept types.
- The choice of subject and object layers is empirically determined rather than theoretically justified, suggesting sensitivity to model architecture specifics.

## Confidence

**High Confidence**: The mechanism of using inverted LREs as concept directions that enable both classification and causal interventions is well-supported by experimental results showing improved accuracy and causality over baseline methods.

**Medium Confidence**: The claim that earlier object layers improve performance is supported by experiments but lacks theoretical grounding for why later layers lose relational structure. The improvement could be model-specific rather than a general principle.

**Medium Confidence**: The low-rank pseudo-inverse improvement is demonstrated empirically but the choice of rank 192 appears arbitrary. The method may be sensitive to this hyperparameter and could fail on different datasets or model architectures.

## Next Checks

1. **Cross-dataset validation**: Test the LRC method on non-relational concept datasets (e.g., sentiment, topic classification) to verify the general applicability of the linear relational embedding approach beyond subject-object relationships.

2. **Layer sensitivity analysis**: Systematically vary subject layer choices across a wider range of positions and model architectures to determine whether the observed performance improvements from using earlier layers is consistent or model-specific.

3. **Rank sensitivity and generalization**: Evaluate how LRC performance scales with different low-rank values and test on out-of-distribution relational prompts to assess whether the low-rank approximation captures truly salient features or overfits to the training distribution.