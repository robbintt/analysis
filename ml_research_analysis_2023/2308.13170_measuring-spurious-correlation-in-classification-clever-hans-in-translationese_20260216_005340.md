---
ver: rpa2
title: 'Measuring Spurious Correlation in Classification: ''Clever Hans'' in Translationese'
arxiv_id: '2308.13170'
source_url: https://arxiv.org/abs/2308.13170
tags:
- topic
- data
- translationese
- classification
- spurious
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of measuring spurious correlations
  in text classification, focusing on topic-based signals. It proposes a novel approach
  to quantify the alignment of unsupervised topics with target classification labels,
  introducing the concept of a "topic floor" analogous to a noise floor in signal
  processing.
---

# Measuring Spurious Correlation in Classification: 'Clever Hans' in Translationese

## Quick Facts
- arXiv ID: 2308.13170
- Source URL: https://arxiv.org/abs/2308.13170
- Authors: 
- Reference count: 25
- Primary result: Novel method quantifies spurious topic correlations in text classification using cluster purity equivalence

## Executive Summary
This paper addresses the challenge of measuring spurious correlations in text classification tasks, particularly those arising from topic-based signals. The authors propose a novel approach that quantifies the alignment between unsupervised topics (learned via LDA or BERTopic) and target classification labels, establishing what they call a "topic floor" - an upper bound on how much spurious topic information may influence classification. For known spurious signals, they employ masking techniques to both quantify and mitigate their impact. The method is demonstrated on a translationese classification task, showing that named entities contribute significantly to spurious correlations.

## Method Summary
The authors develop a measure to quantify spurious topic correlations by calculating the alignment between unsupervised topics and classification labels. This alignment measure is mathematically equivalent to cluster purity. For known spurious signals (specifically named entities), they apply masking techniques to both measure and mitigate their impact on classification performance. The method uses topic modeling (LDA and BERTtopic) to discover topic structure, computes alignment with classification labels, and employs masking of named entities and POS tags to isolate spurious signals. Classification is performed using fine-tuned BERT, with Integrated Gradients attribution used to analyze feature importance changes after masking.

## Key Results
- The proposed alignment measure is equivalent to cluster purity in clustering
- Topic floor established at 0.62 for translationese classification task
- Masking named entities reduces classification accuracy by 3-4 percentage points
- After masking, BERT learns features closer to "proper" translationese according to IG attribution

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Topic alignment between unsupervised topics and classification labels provides an upper bound on spurious correlation impact.
- **Mechanism**: The method measures the purity of unsupervised topic clusters with respect to target labels, treating high alignment as potential spurious correlation.
- **Core assumption**: Topics learned by LDA or BERTopic are independent of the classification task and purely reflect data structure.
- **Evidence anchors**:
  - [abstract]: "We show that our measure is the same as purity in clustering"
  - [section 4.1]: "Our alignment-based measure is in fact the same as cluster purity (Zhao and Karypis, 2001)"
  - [corpus]: Weak evidence - no direct citations found, but related work on spurious correlations exists
- **Break condition**: If the unsupervised topics themselves contain genuine classification signals (not just spurious ones), the measure would overestimate spurious correlation.

### Mechanism 2
- **Claim**: Masking named entities reduces classification accuracy, quantifying their contribution to spurious correlations.
- **Mechanism**: By replacing NEs with special tokens, the model is forced to rely on other features, revealing how much performance depended on NE-based spurious signals.
- **Core assumption**: Named entities are not part of the true classification signal for translationese detection.
- **Evidence anchors**:
  - [abstract]: "masking named entities reduces classification accuracy by 3-4 percentage points"
  - [section 5.4.1]: "Consistent with expectation, under all training-test data conditions, masking NE-related information lowers classification results"
  - [corpus]: Moderate evidence - similar masking approaches exist in related work
- **Break condition**: If named entities are actually part of the genuine translationese signal, masking would incorrectly reduce accuracy.

### Mechanism 3
- **Claim**: Integrated Gradients attribution reveals what features the model actually uses after spurious correlations are removed.
- **Mechanism**: By comparing attribution scores before and after masking, we can see whether the model shifts from using spurious signals to more genuine ones.
- **Core assumption**: IG attribution accurately reflects feature importance in the model's decision-making.
- **Evidence anchors**:
  - [abstract]: "We use IG attribution to show that in masked settings where known spurious correlations are mitigated, BERT learns features closer to proper translationese"
  - [section 5.4.3]: "Table 4 shows the top-20 IG token attributions for O and T data in the masked-masked condition"
  - [corpus]: Weak evidence - IG is commonly used but specific validation for this use case is limited
- **Break condition**: If IG attribution is unreliable or misleading, conclusions about feature importance would be invalid.

## Foundational Learning

- **Concept**: Topic modeling (LDA, BERTopic)
  - **Why needed here**: To discover unsupervised topic structure that might correlate with classification labels
  - **Quick check question**: What are the two key assumptions LDA makes about documents and topics?

- **Concept**: Cluster purity and its relationship to classification
  - **Why needed here**: The paper's alignment measure is equivalent to cluster purity, providing theoretical grounding
  - **Quick check question**: How does cluster purity differ from other clustering evaluation metrics like normalized mutual information?

- **Concept**: Named entity recognition and masking
  - **Why needed here**: To identify and remove potential spurious signals from the data
  - **Quick check question**: What are the three coarse-grained NE-type tags used in the masking experiments?

## Architecture Onboarding

- **Component map**: Data preprocessing → Topic modeling (LDA/BERTopic) → Alignment calculation → Masking (NE/POS) → Classification → Attribution analysis
- **Critical path**: Topic modeling → Alignment calculation → Masking experiments → IG analysis
- **Design tradeoffs**: Using unsupervised topics vs. supervised features, masking NEs vs. full POS masking, BERT vs. simpler classifiers
- **Failure signatures**: High alignment scores with no corresponding performance drop after masking (indicates genuine signal), low alignment but significant performance drop (indicates other spurious signals), inconsistent IG results across runs
- **First 3 experiments**:
  1. Run LDA with varying numbers of topics (2, 10, 20, 30, 50, 100) and calculate alignment scores
  2. Mask named entities using BERT-German NER and retrain classifier, comparing accuracy
  3. Perform full POS masking and analyze classification accuracy and IG attribution results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed topic alignment measure compare to other established cluster quality metrics like normalized mutual information or adjusted Rand index?
- Basis in paper: [explicit] The authors mention that a reviewer suggested comparing their measure to existing cluster quality measures but do not provide such a comparison.
- Why unresolved: The paper only establishes the equivalence of their measure to purity and does not benchmark it against other metrics.
- What evidence would resolve it: Empirical results comparing the proposed measure to other cluster quality metrics on various datasets and classification tasks.

### Open Question 2
- Question: What is the impact of using different topic modeling approaches (e.g., BERTopic vs. LDA) on the established topic floor and subsequent translationese classification results?
- Basis in paper: [explicit] The authors use both LDA and BERTopic for topic modeling and observe different topic alignment scores, but do not explore the impact on translationese classification.
- Why unresolved: The paper focuses on establishing the topic floor using different topic models but does not investigate how this affects translationese classification accuracy.
- What evidence would resolve it: Experiments comparing translationese classification accuracy using different topic models and their corresponding topic floors.

### Open Question 3
- Question: How does the presence of named entities in the data affect the performance of other neural translationese classifiers beyond BERT?
- Basis in paper: [explicit] The authors investigate the impact of named entity masking on BERT's translationese classification accuracy but do not explore other neural architectures.
- Why unresolved: The paper focuses on BERT and does not provide insights into how named entities affect other neural translationese classifiers.
- What evidence would resolve it: Experiments comparing the impact of named entity masking on translationese classification accuracy for various neural architectures (e.g., RoBERTa, XLNet).

### Open Question 4
- Question: Can the proposed topic floor concept be generalized to other classification tasks beyond translationese, and if so, how would it be applied?
- Basis in paper: [explicit] The authors introduce the concept of a topic floor in the context of translationese classification but do not discuss its applicability to other tasks.
- Why unresolved: The paper focuses on translationese classification and does not explore the potential generalization of the topic floor concept.
- What evidence would resolve it: Experiments applying the topic floor concept to other classification tasks (e.g., sentiment analysis, topic classification) and evaluating its effectiveness in quantifying spurious correlations.

## Limitations
- The paper's findings are based on a single translationese classification task, limiting generalizability to other domains
- The assumption that unsupervised topics are independent of classification tasks may not hold in practice
- The masking approach may not capture all forms of spurious correlations, particularly syntactic or semantic patterns beyond named entities

## Confidence

**High Confidence Claims:**
- The alignment measure is mathematically equivalent to cluster purity
- Masking named entities reduces classification accuracy in the translationese task
- The topic floor concept provides a useful upper bound for spurious correlation estimation

**Medium Confidence Claims:**
- Named entities are a significant source of spurious correlation in translationese detection
- The magnitude of performance reduction (3-4 percentage points) from NE masking is meaningful
- IG attribution reveals shifts in feature importance after spurious correlation mitigation

**Low Confidence Claims:**
- The topic floor of 0.62 generalizes to other classification tasks
- The unsupervised topic alignment measure reliably estimates spurious correlation impact across domains
- BERT's behavior after masking represents learning "proper" translationese features rather than just adapting to different spurious signals

## Next Checks

1. **Cross-domain validation**: Apply the topic alignment and masking methodology to at least two additional text classification tasks (e.g., sentiment analysis, topic classification) to assess generalizability of the topic floor concept and NE masking effectiveness.

2. **Comprehensive spurious signal analysis**: Systematically test masking of other potential spurious signal types (syntactic patterns, function words, formatting features) and measure their individual and combined effects on classification accuracy to build a more complete picture of spurious correlations.

3. **Temporal stability assessment**: Run the entire pipeline multiple times with different random seeds for topic modeling and classifier initialization to quantify variance in topic alignment scores, masking effectiveness, and IG attribution results, establishing confidence intervals for the reported findings.