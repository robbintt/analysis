---
ver: rpa2
title: 'Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link Prediction
  in Knowledge Graphs'
arxiv_id: '2312.04997'
source_url: https://arxiv.org/abs/2312.04997
tags:
- knowledge
- graph
- entities
- https
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts the first systematic survey of inductive, few-shot,
  and zero-shot link prediction in knowledge graphs. It identifies inconsistencies
  in terminology and definitions across the literature, which hinder fair model comparison.
---

# Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link Prediction in Knowledge Graphs

## Quick Facts
- **arXiv ID**: 2312.04997
- **Source URL**: https://arxiv.org/abs/2312.04997
- **Reference count**: 40
- **Primary result**: Proposes a novel anchor-based nomenclature to classify and differentiate inductive, few-shot, and zero-shot link prediction settings in knowledge graphs, revealing under-explored configurations and highlighting the need for standardized benchmarks.

## Executive Summary
This survey systematically reviews link prediction methods beyond transductive settings, focusing on inductive, few-shot, and zero-shot scenarios where models must handle unseen entities or relations. The authors identify pervasive inconsistencies in terminology and experimental protocols across the literature, which severely limit fair model comparison. To address this, they propose a novel anchor-based nomenclature that classifies settings by the presence or absence of seen elements in test triples. By mapping 129 surveyed papers to this framework, they reveal significant gaps in research coverage and emphasize the urgent need for unified benchmarks and standardized definitions to advance the field.

## Method Summary
The survey conducts a comprehensive literature review of 129 papers on inductive, few-shot, and zero-shot link prediction in knowledge graphs. The authors classify these works using a newly proposed anchor-based nomenclature that encodes test triple configurations via binary signatures indicating seen/unseen elements. They analyze dataset usage patterns, model families, and experimental setups across settings, identifying key inconsistencies and under-explored areas. The method involves mapping each paper to the nomenclature, categorizing them by setting, and synthesizing findings to highlight research gaps and standardization needs.

## Key Results
- Survey identifies pervasive terminology inconsistencies across inductive, few-shot, and zero-shot link prediction literature, hindering fair model comparison.
- Proposes anchor-based nomenclature using binary signatures to unambiguously classify settings by test triple composition (e.g., ùêº010 for seen head, unseen relation, seen tail).
- Reveals significant under-exploration of certain configurations, such as predicting with only unseen entities and relations, and highlights lack of standardized benchmarks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Anchor-based nomenclature enables consistent task differentiation by encoding the presence/absence of seen/unseen elements directly in test triple signatures.
- **Mechanism**: By indexing test triples with binary flags (1 for seen, 0 for unseen) in the order [head, relation, tail], the nomenclature maps each unique configuration to a single, unambiguous setting name (e.g., ùêº010 for seen head, unseen relation, seen tail).
- **Core assumption**: All test triples within a given experimental setting share the same pattern of seen/unseen elements.
- **Evidence anchors**:
  - [abstract] "A unifying nomenclature is ultimately proposed to refer to each of them in a simple and consistent manner."
  - [section] "This fine-grained perspective based on the nature of test triples ensures that the model is tested on a collection of triples that comply with the exact same pattern of seen and unseen elements."
  - [corpus] Weak/no direct evidence; nomenclature appears novel in the literature surveyed.
- **Break condition**: If any test triple within a claimed setting violates the binary pattern, the nomenclature mapping fails and requires an ensemble label (e.g., ùêº110 ‚à™ ùêº011).

### Mechanism 2
- **Claim**: Few-shot and zero-shot settings are distinguished by the presence or absence of a support set, while inductive settings are defined by model generalizability to unseen elements.
- **Mechanism**: FSLP includes a support set of examples for unseen relations; ZSLP lacks any support; ILP focuses on the model's ability to embed unseen entities/relations without relying on task-specific examples.
- **Core assumption**: Training support availability is the primary differentiator between FSLP and ZSLP, and model architecture (e.g., GNNs) determines inductive capability.
- **Evidence anchors**:
  - [abstract] "These works are known as inductive, few-shot, and zero-shot link prediction."
  - [section] "Few-Shot Link Prediction can be seen as a middle ground between ZSLP and the purely transductive setting... ZSLP denotes their absence, and ILP is concerned with the model ability to learn rules or representations that are general enough to perform predictions on unseen entities and relations."
  - [corpus] Weak; the paper discusses overlap but does not provide extensive empirical validation of this tripartite distinction.
- **Break condition**: If models in ILP experiments use support triples for training or fine-tuning, the distinction from FSLP collapses.

### Mechanism 3
- **Claim**: Heterogeneous dataset usage undermines fair model comparison, especially in ILP and FSLP.
- **Mechanism**: Each setting uses different datasets, splits, and hyperparameters (e.g., ùêæ-shot values), preventing standardized benchmarking and leading to incomparable results.
- **Core assumption**: Consistency in dataset selection and preprocessing is necessary for meaningful model evaluation.
- **Evidence anchors**:
  - [section] "Datasets used in works that identify themselves as ILP-related are even more heterogeneous... We argue that the lack of widely accepted benchmarks... severely restricts any attempt at comparing models fairly."
  - [section] "We observe fewer datasets used under the ZSLP setting... NELL-ZS and Wiki-ZS are datasets used in almost all the retrieved ZSLP papers (Table 7) and are therefore recognized as public benchmarks."
  - [corpus] Weak; the paper identifies the problem but does not provide a unified benchmark proposal.
- **Break condition**: If a new, widely accepted benchmark dataset and protocol is introduced, the current comparability problem is mitigated.

## Foundational Learning

- **Concept**: Knowledge graph embedding and link prediction basics
  - Why needed here: The survey assumes familiarity with transductive KGEMs and their limitations when applied to unseen entities/relations.
  - Quick check question: What is the difference between transductive and inductive link prediction in terms of training/test entity sets?

- **Concept**: Meta-learning and few-shot learning paradigms
  - Why needed here: Many FSLP models use meta-learning to adapt quickly to unseen relations; understanding MAML, episode training, and support/query sets is essential.
  - Quick check question: In a 5-shot episode, how many support triples are provided for each unseen relation, and how is the query set structured?

- **Concept**: Graph neural networks and their inductive bias
  - Why needed here: Most ILP models use GNNs; knowing how message passing and neighbor aggregation enable generalization to unseen nodes is key.
  - Quick check question: How does a GNN compute embeddings for an unseen entity using only its neighbors in the test graph?

## Architecture Onboarding

- **Component map**: KG preprocessing and split into seen/unseen elements -> Model selection/training (transductive, few-shot, zero-shot, or inductive) -> Evaluation on test triples following the anchor-based setting -> Performance reporting and comparison
- **Critical path**: Data preparation ‚Üí Model architecture design ‚Üí Training (meta-training for few-shot) ‚Üí Evaluation on correctly formatted test triples ‚Üí Result aggregation
- **Design tradeoffs**: Few-shot models trade expressiveness for adaptability via support sets; zero-shot models must rely entirely on external knowledge (text/ontology); inductive models must embed generalization into their architecture without task-specific adaptation.
- **Failure signatures**: Overfitting to seen entities in ILP (low performance on unseen triples); poor few-shot generalization when ùêæ is too small; collapse of zero-shot models when textual/ontological features are noisy or absent.
- **First 3 experiments**:
  1. Reproduce a baseline transductive KGEM (e.g., TransE) on a small FB15k-237 split to establish performance ground truth.
  2. Implement a simple few-shot episode training loop with a metric-based model (e.g., GMatching) and evaluate on NELL-One with ùêæ=5.
  3. Run an inductive GNN model (e.g., GraIL) on FB15k-237-v1 and compare its performance on unseen vs. seen test triples to highlight inductive capability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the precise experimental conditions that distinguish FSLP, ZSLP, and ILP, and how can these distinctions be standardized across the field?
- **Basis in paper**: [explicit] The paper discusses the need for clear task definitions and a unifying nomenclature to avoid ambiguity in experimental setups.
- **Why unresolved**: The paper identifies inconsistencies in how these tasks are defined and used, leading to challenges in fair model comparison.
- **What evidence would resolve it**: A consensus on standardized experimental protocols, including specific datasets, evaluation metrics, and data splits for each task.

### Open Question 2
- **Question**: How can the performance of models in FSLP, ZSLP, and ILP be fairly compared given the current lack of widely accepted benchmarks?
- **Basis in paper**: [explicit] The paper highlights the lack of standardized benchmarks as a major obstacle to fair model comparison.
- **Why unresolved**: The diversity of datasets and experimental conditions used across studies makes it difficult to draw meaningful comparisons.
- **What evidence would resolve it**: The development and adoption of a common set of benchmark datasets and evaluation protocols for each task.

### Open Question 3
- **Question**: What are the most effective approaches for handling unseen entities and relations in knowledge graphs, and how can these approaches be evaluated comprehensively?
- **Basis in paper**: [inferred] The paper reviews various model families and approaches for FSLP, ZSLP, and ILP, indicating ongoing exploration in this area.
- **Why unresolved**: The paper suggests that many approaches exist but their relative effectiveness is not fully established due to the lack of standardized benchmarks.
- **What evidence would resolve it**: Systematic benchmarking of different model families and approaches on a common set of tasks and datasets, with clear evaluation metrics.

## Limitations
- Nomenclature is logically sound but lacks empirical validation of its impact on advancing research or improving model comparison.
- Identifies benchmark heterogeneity as a major issue but does not propose concrete unified evaluation protocols.
- Acknowledges boundary cases and overlap between inductive, few-shot, and zero-shot settings without extensive empirical clarification.

## Confidence
- **High confidence**: The existence of terminology inconsistencies across the literature and the need for standardized benchmarks are well-supported by the literature survey.
- **Medium confidence**: The anchor-based nomenclature is logically sound and internally consistent, but its practical impact on advancing the field requires further validation.
- **Medium confidence**: The distinction between inductive, few-shot, and zero-shot settings is conceptually clear, but boundary cases and overlap remain underexplored.

## Next Checks
1. Conduct an empirical study comparing model performance across the same dataset using the anchor-based nomenclature to verify if it reveals meaningful performance gaps or similarities.
2. Propose and pilot a unified benchmark dataset and evaluation protocol for inductive, few-shot, and zero-shot link prediction to address the comparability issue.
3. Investigate the boundary cases between the three settings (e.g., models that use support triples in ILP or achieve zero-shot performance through inductive reasoning) to clarify the distinction.