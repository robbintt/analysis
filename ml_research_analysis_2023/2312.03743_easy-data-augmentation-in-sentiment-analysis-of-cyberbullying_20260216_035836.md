---
ver: rpa2
title: Easy Data Augmentation in Sentiment Analysis of Cyberbullying
arxiv_id: '2312.03743'
source_url: https://arxiv.org/abs/2312.03743
tags:
- data
- words
- augmentation
- cyberbullying
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates cyberbullying sentiment analysis using Support
  Vector Machine (SVM) and Easy Data Augmentation (EDA) on Indonesian Instagram comments.
  By augmenting training data through synonym replacement, random insertion, swapping,
  and deletion, the research achieves improved model performance.
---

# Easy Data Augmentation in Sentiment Analysis of Cyberbullying

## Quick Facts
- arXiv ID: 2312.03743
- Source URL: https://arxiv.org/abs/2312.03743
- Reference count: 27
- Primary result: EDA improves Indonesian cyberbullying sentiment analysis from 87.22% to 89.74% k-Fold accuracy and from 90% to 92.5% testing accuracy using SVM

## Executive Summary
This study applies Easy Data Augmentation (EDA) techniques to improve cyberbullying sentiment analysis on Indonesian Instagram comments using Support Vector Machine (SVM). By augmenting the training data through synonym replacement, random insertion, swapping, and deletion operations, the research achieves significant performance improvements over baseline models. The best configuration uses n_aug=16 augmented samples per instance with α=0.1 augmentation intensity, yielding 2.52% improvement in k-Fold accuracy and 2.5% improvement in testing accuracy.

## Method Summary
The methodology employs SVM with polynomial kernel for cyberbullying sentiment classification on Indonesian Instagram comments. Text preprocessing includes case folding, data cleaning, language normalization using Spacy's lemmatization, stopword removal, stemming, and tokenization. TF-IDF weighting extracts numerical features from text. EDA augmentation applies four operations - synonym replacement, random insertion, swapping, and deletion - with parameters n_aug (number of augmented samples) ranging from 1 to 32 and α (augmentation intensity) from 0.05 to 0.5. The system tests 25 parameter combinations and uses 10-fold cross-validation for robust performance evaluation.

## Key Results
- Best configuration (n_aug=16, α=0.1) achieves 89.74% k-Fold accuracy, improving baseline by 2.52%
- Testing accuracy reaches 92.5% with the same configuration, improving baseline by 2.5%
- Precision, recall, and F1-score also improve with EDA augmentation compared to baseline
- Performance improvements demonstrate EDA effectiveness for handling limited datasets in cyberbullying detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EDA increases SVM classification performance by augmenting training data with realistic variations
- Mechanism: EDA applies four transformation operations (synonym replacement, random insertion, swapping, deletion) to generate synthetic samples that preserve semantic meaning while expanding feature space coverage
- Core assumption: Synonym dictionaries accurately capture word relationships and transformations don't distort semantic content
- Evidence anchors:
  - "The best configuration (n_aug=16, α=0.1) yields 89.74% k-Fold accuracy (2.52% improvement) and 92.5% testing accuracy (2.5% improvement) over baseline"
  - "The highest average k-Fold accuracy was obtained with n_aug = 16 and α = 0.1... The highest accuracy testing was obtained with n_aug = 16 and α = 0.1"
- Break condition: If synonym replacements introduce incorrect meanings or transformations create ungrammatical sentences, model performance degrades

### Mechanism 2
- Claim: TF-IDF weighting effectively captures term importance for Indonesian cyberbullying text classification
- Mechanism: TF-IDF assigns higher weights to terms that appear frequently in specific documents but rarely across the corpus, helping SVM distinguish between positive and negative sentiment classes
- Core assumption: Term frequency and inverse document frequency adequately represent term importance for short Instagram comments
- Evidence anchors:
  - "A numerical statistic called TF-IDF is used to determine the importance of a term in a document through text mining"
  - "The word-weighting process is carried out separately for training and testing data"
- Break condition: If common terms in Indonesian comments receive disproportionately high weights or if document frequency calculations are skewed by class imbalance

### Mechanism 3
- Claim: 10-fold cross-validation provides reliable performance estimates for this dataset size
- Mechanism: Data is partitioned into 10 folds, with each fold serving as test set once while remaining 9 folds train the model
- Core assumption: 10-fold partitioning provides sufficient variance reduction while maintaining computational efficiency
- Evidence anchors:
  - "k-fold cross-validation is a resampling technique for assessing model performance on data, where k is the number of folds"
  - "The number of k is the amount of fold that can be set to measure model performance"
- Break condition: If class distribution varies significantly across folds or if the dataset size makes 10-fold partitioning ineffective

## Foundational Learning

- TF-IDF weighting
  - Why needed here: Converts text features into numerical values that reflect term importance for SVM classification
  - Quick check question: How does IDF penalize terms that appear in many documents?

- Synonym replacement mechanics
  - Why needed here: Generates realistic variations while preserving semantic meaning in EDA
  - Quick check question: What happens if a word has no synonyms in the thesaurus?

- k-fold cross-validation principles
  - Why needed here: Provides robust performance estimates when training data is limited
  - Quick check question: Why might k=10 be preferred over k=5 or k=20?

## Architecture Onboarding

- Component map: Data collection → Preprocessing (case folding, normalization, stopword removal, stemming, tokenization) → TF-IDF weighting → EDA augmentation → SVM training → k-fold validation → Evaluation
- Critical path: EDA augmentation → SVM training → k-fold validation
- Design tradeoffs: EDA increases training data but adds computational overhead; TF-IDF is efficient but may lose word order information
- Failure signatures: Poor performance indicates either EDA transformations corrupted data or TF-IDF weighting doesn't capture relevant features
- First 3 experiments:
  1. Run baseline SVM without EDA to establish performance floor
  2. Apply EDA with n_aug=16, α=0.1 to verify improvement claims
  3. Test different α values (0.05, 0.3, 0.5) to understand parameter sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of EDA-based cyberbullying detection generalize to other languages beyond Indonesian?
- Basis in paper: The paper focuses on Bahasa Indonesia and notes that previous research using EDA was conducted on English datasets, but doesn't test other languages
- Why unresolved: The study is limited to one language and doesn't explore cross-linguistic applicability
- What evidence would resolve it: Comparative experiments testing EDA on cyberbullying datasets in multiple languages (e.g., English, Spanish, Arabic) with identical methodology

### Open Question 2
- Question: What is the optimal k value for k-fold cross-validation in cyberbullying sentiment analysis tasks?
- Basis in paper: The paper investigates different k values but doesn't provide a theoretical justification for why k=10 performed best
- Why unresolved: The choice of k appears empirical rather than theoretically grounded
- What evidence would resolve it: Systematic analysis of model stability and variance across different k values (e.g., k=5, k=10, k=20) with statistical comparison of performance metrics

### Open Question 3
- Question: How do the four EDA operations (Synonym Replacement, Random Insertion, Random Swap, Random Deletion) perform individually versus in combination for cyberbullying detection?
- Basis in paper: The paper discusses that all four operations contribute to performance improvement but doesn't analyze their individual contributions
- Why unresolved: The paper evaluates combined EDA performance without isolating individual operation effects
- What evidence would resolve it: Ablation studies testing each EDA operation separately and in various combinations to quantify their individual and synergistic effects on model performance

## Limitations

- The study relies on Indonesian language resources, limiting generalizability to other languages
- The small dataset size (400 comments) may not fully represent the diversity of cyberbullying patterns
- TF-IDF weighting may not capture contextual nuances important for cyberbullying detection in informal social media text

## Confidence

- High Confidence: The empirical performance improvements from EDA augmentation are well-supported by the experimental results
- Medium Confidence: The mechanism of how EDA improves performance is reasonable but not deeply validated
- Low Confidence: Claims about computational efficiency compared to more complex augmentation techniques lack comparative analysis

## Next Checks

1. **Synonym Quality Validation**: Extract 20-30 EDA-generated samples with synonym replacement and manually verify semantic preservation. Calculate the percentage of transformations that maintain intended meaning versus introducing errors.

2. **Parameter Sensitivity Analysis**: Replicate the experiments with different α values (0.01, 0.3, 0.7) and n_aug values (32, 64) to confirm the reported optimal parameters (n_aug=16, α=0.1) are truly optimal and not artifacts of limited testing.

3. **Cross-Dataset Generalization**: Test the trained model on an independent Indonesian cyberbullying dataset or translate a small subset of English cyberbullying data to Indonesian to assess whether performance improvements generalize beyond the original dataset.