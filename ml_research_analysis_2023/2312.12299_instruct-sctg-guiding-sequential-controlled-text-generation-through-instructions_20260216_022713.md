---
ver: rpa2
title: 'Instruct-SCTG: Guiding Sequential Controlled Text Generation through Instructions'
arxiv_id: '2312.12299'
source_url: https://arxiv.org/abs/2312.12299
tags:
- discourse
- news
- text
- generation
- structure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a framework called Instruct-SCTG that leverages
  instruction-following language models to generate structurally coherent text in
  a section-by-section manner, guided by natural language instructions. The framework
  decomposes the generation task into a sequence of sub-tasks, each designed to generate
  a specific text section based on the given discourse sequence.
---

# Instruct-SCTG: Guiding Sequential Controlled Text Generation through Instructions

## Quick Facts
- arXiv ID: 2312.12299
- Source URL: https://arxiv.org/abs/2312.12299
- Reference count: 12
- Key outcome: Framework generates structurally coherent text guided by natural language instructions, outperforming baselines on discourse structure adherence metrics.

## Executive Summary
This paper proposes Instruct-SCTG, a framework that leverages instruction-following language models to generate text that adheres to specific discourse structures in a section-by-section manner. The approach decomposes document-level generation into sequential sub-tasks, each guided by natural language instructions specifying the intended discourse role. The framework introduces a novel automatic metric called Positional Divergence to measure fuzzy adherence to discourse structure. Experiments on news articles and recipes demonstrate state-of-the-art performance in controlling discourse structure during text generation.

## Method Summary
The framework generates text sequentially by breaking down document-level generation into sentence-level sub-tasks. Each sub-task is guided by a specific discourse role instruction created using an instruction template. The backbone generator (fine-tuned or zero-shot LLM) produces text conditioned on the discourse role and context. Discourse roles are classified using a discourse role classifier, and the framework employs automatic metrics including BLEU, ROUGE-L, perplexity, exact match accuracy, and a novel Positional Divergence metric to evaluate performance.

## Key Results
- Instruct-SCTG outperforms baseline models on surface fluency metrics (BLEU, ROUGE-L, perplexity)
- The framework achieves higher discourse structure adherence measured by exact match accuracy and Positional Divergence
- Human evaluation confirms superior structural coherence compared to baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing document-level generation into sentence-level tasks improves discourse structure adherence
- Mechanism: Breaking down generation into sequential sub-tasks allows focused generation of content matching intended discourse functions
- Core assumption: Sentence-level generation conditioned on discourse role is more effective than paragraph-level or document-level generation for controlling discourse structure
- Evidence anchors: [abstract] "generates articles in a section-by-section manner, aligned with the desired human structure"; [section 3.2] "decompose the document-level conditional distribution into a series of unit-level sub-tasks"
- Break condition: If discourse role classifiers are inaccurate or roles don't capture necessary granularity

### Mechanism 2
- Claim: Instruction-tuned models better align with human intentions for discourse structure control
- Mechanism: Fine-tuning on instruction-discourse pairs teaches models to interpret natural language instructions as guidance for generating text with specific discourse functions
- Core assumption: Instruction-tuned models have learned to map natural language instructions to appropriate text generation patterns
- Evidence anchors: [abstract] "Instruction-tuned large language models have shown remarkable performance in aligning generated text with user intentions"; [section 3.3] "design task-specific instructions for fine-tuning LMs"
- Break condition: If instruction tuning doesn't generalize to discourse-specific instructions

### Mechanism 3
- Claim: Positional Divergence metric effectively measures discourse structure adherence
- Mechanism: Comparing discourse role distributions at relative positions accounts for natural variations while capturing overall discourse patterns
- Core assumption: Discourse role distributions at similar relative positions should be similar across well-structured texts
- Evidence anchors: [section 3.5] "introduce the Positional Divergence Dpos"; [section 5.3.1] "framework outperforms all baseline models on surface fluency and structural coherence metrics"
- Break condition: If discourse role classification is inaccurate or structures vary too widely

## Foundational Learning

- Concept: Discourse structure and functional discourse schemas
  - Why needed here: Different domains have different discourse structures requiring appropriate control mechanisms
  - Quick check question: What are the main discourse roles for news articles according to the paper's schema?

- Concept: Instruction tuning and prompt engineering
  - Why needed here: Framework relies on designing effective instructions for guiding generation
  - Quick check question: What are the three main components of the instruction template used in the framework?

- Concept: Controlled text generation techniques
  - Why needed here: Understanding control mechanisms is fundamental to the approach
  - Quick check question: How does SCTG differ from conventional controlled text generation tasks?

## Architecture Onboarding

- Component map: Input prompt → Discourse sequence → Sequential instruction generation → Text generation → Discourse role classification → Evaluation metrics
- Critical path: Input prompt → Discourse sequence → Sequential instruction generation → Text generation → Discourse role classification → Evaluation
- Design tradeoffs: Fine-tuned generators offer better control but require training data; zero-shot generators are more flexible but may have less precise control
- Failure signatures: Poor discourse structure (high positional divergence), incoherent text (low fluency scores), hallucinations (factual inaccuracies)
- First 3 experiments:
  1. Test different levels of discourse context (local, past-aware, full-structure) on a small validation set
  2. Compare fine-tuned vs zero-shot backbone generators on discourse structure adherence
  3. Validate the positional divergence metric against human evaluations on sample outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal level of discourse context exposure for generating structurally coherent text across different domains and tasks?
- Basis in paper: Explicit - The paper explores impact of different discourse context levels on control performance
- Why unresolved: The paper shows past-aware context performs better for news generation, but optimal level might vary across domains and tasks
- What evidence would resolve it: Experiments on wider range of datasets from diverse domains comparing performance of different discourse context levels

### Open Question 2
- Question: How can the framework be extended to handle longer articles while maintaining discourse coherence and fluency?
- Basis in paper: Inferred - The paper mentions limitations in handling lengthy articles due to maximum input/output token length constraints
- Why unresolved: Truncation methods may not be optimal for encoding/decoding extra-long articles, but no solution is proposed
- What evidence would resolve it: Evaluating methods like hierarchical encoding, dynamic truncation, or multi-stage generation for longer texts

### Open Question 3
- Question: How can the framework be improved to reduce hallucination and generate more factually accurate content, especially in domains like news?
- Basis in paper: Inferred - The paper mentions focus on controlling discourse structure rather than content may lead to hallucination
- Why unresolved: The paper acknowledges potential for hallucination but doesn't provide a solution
- What evidence would resolve it: Integrating fact-checking or knowledge grounding into generation process, such as retrieving information from reliable sources

## Limitations
- Limited evidence for instruction-tuning effectiveness on discourse-specific applications
- Unknown quality and accuracy of discourse role classifiers used for training and evaluation
- Sparse details about human evaluation methodology and metrics

## Confidence
- High confidence: Sentence-level decomposition improves discourse structure adherence
- Medium confidence: Instruction-tuned models better align with human intentions for discourse control
- Low confidence: Positional Divergence metric effectively measures discourse structure adherence

## Next Checks
1. Evaluate discourse role classifier accuracy on held-out validation set to establish baseline before using for training or evaluation
2. Conduct detailed human evaluations comparing generated texts across different discourse contexts to validate automatic metric findings
3. Test different instruction template formats and discourse schema definitions to identify optimal configurations for discourse structure adherence