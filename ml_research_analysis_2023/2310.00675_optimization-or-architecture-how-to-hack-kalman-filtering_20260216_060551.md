---
ver: rpa2
title: 'Optimization or Architecture: How to Hack Kalman Filtering'
arxiv_id: '2310.00675'
source_url: https://arxiv.org/abs/2310.00675
tags:
- noise
- section
- problem
- estimation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper highlights a critical methodological flaw in comparing
  non-linear filtering models (like neural networks) to the standard Kalman Filter
  (KF): the KF''s parameters are typically tuned by noise estimation rather than optimized
  for mean squared error (MSE), whereas neural models are optimized for MSE. To address
  this, the authors introduce the Optimized Kalman Filter (OKF), which uses Cholesky
  parameterization and gradient-based optimization to tune the KF''s noise covariances
  Q and R directly for MSE.'
---

# Optimization or Architecture: How to Hack Kalman Filtering

## Quick Facts
- arXiv ID: 2310.00675
- Source URL: https://arxiv.org/abs/2310.00675
- Reference count: 40
- The Optimized Kalman Filter (OKF) outperforms standard Kalman Filter and optimized neural models by directly optimizing noise covariances for mean squared error.

## Executive Summary
This paper addresses a critical methodological flaw in comparing non-linear filtering models to the standard Kalman Filter: the KF's parameters are typically tuned by noise estimation rather than optimized for mean squared error (MSE), while neural models are optimized for MSE. The authors introduce the Optimized Kalman Filter (OKF), which uses Cholesky parameterization and gradient-based optimization to tune the KF's noise covariances Q and R directly for MSE. Experiments across multiple domains demonstrate that OKF consistently outperforms both the standard KF and optimized neural network models in terms of MSE.

## Method Summary
The Optimized Kalman Filter (OKF) addresses the methodological flaw of comparing non-linear filtering methods to standard KF by optimizing KF parameters directly for MSE rather than using traditional noise estimation. OKF uses Cholesky parameterization to maintain positive-definite constraints on noise covariances Q and R while enabling gradient-based optimization on supervised training data. The method is implemented through gradient optimization (e.g., Adam) to learn Q and R that minimize MSE, contrasting with the standard KF which estimates these parameters from data statistics.

## Key Results
- OKF consistently outperforms standard KF and optimized neural network models across Doppler radar tracking, video tracking, and lidar-based state estimation domains
- Theoretical analysis shows noise estimation fails to align with MSE optimization under realistic KF assumption violations (non-linear observations, non-i.i.d. noise)
- OKF demonstrates robustness across varying training dataset sizes, distributional shifts, and different KF configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: When KF assumptions are violated, noise estimation no longer aligns with MSE optimization, leading to suboptimal filtering.
- Mechanism: The KF parameters Q and R are typically tuned by noise estimation under the assumption that this minimizes MSE. However, when assumptions like linear observation models or i.i.d noise are violated, the effective noise covariance differs from the estimated one, causing the MSE to increase.
- Core assumption: Violation of KF assumptions (e.g., non-linear observation models, non-i.i.d noise) leads to a mismatch between estimated and effective noise covariances.
- Evidence anchors:
  - [abstract] "Theoretical analysis reveals that noise estimation fails to align with MSE optimization under realistic violations of KF assumptions, such as non-linear observations or non-i.i.d. noise."
  - [section] "We study this in detail: (a) Section 5 analyzes the conflict theoretically under certain assumption violations; (b) Appendix B.1 shows that even oracle noise estimation cannot optimize the MSE; (c) Appendix B.2 shows that when using noise estimation, the MSE may degrade with more data."
- Break condition: If the KF assumptions hold exactly, noise estimation remains optimal and OKF provides no advantage.

### Mechanism 2
- Claim: OKF directly optimizes MSE by gradient-based tuning of Q and R, maintaining SPD constraints via Cholesky parameterization.
- Mechanism: OKF uses Cholesky decomposition to parameterize Q and R as LL^T, where L is lower-triangular with positive diagonal. This ensures SPD structure while allowing gradient-based optimization of MSE over supervised data.
- Core assumption: Cholesky parameterization preserves SPD constraints and enables effective gradient optimization.
- Evidence anchors:
  - [section] "To maintain the complete expressiveness of ˆQ and ˆR, we use the Cholesky parameterization [Pinheiro and Bates, 1996]."
  - [section] "Both Cholesky parameterization and sequential optimization methods are well known tools. Yet, for KF optimization from supervised data, we are not aware of any previous attempts to apply them together."
- Break condition: If the optimization landscape is too non-convex or the data is insufficient, gradient optimization may converge to poor local minima.

### Mechanism 3
- Claim: OKF generalizes better across distributional shifts because it learns patterns from data rather than relying on assumption-specific noise models.
- Mechanism: By optimizing directly for MSE on training data, OKF adapts to the actual noise structure and dynamics present, rather than assuming a specific form. This allows it to maintain performance when test data differs from training distribution.
- Core assumption: Direct MSE optimization captures data-dependent patterns that generalize better than assumption-based methods.
- Evidence anchors:
  - [abstract] "The OKF is shown to be robust across varying training dataset sizes, distributional shifts, and different KF configurations."
  - [section] "OKF learns patterns from the specific distribution of the train data – how well will it generalize to different distributions? Section 4 already addresses this question to some extent, as OKF outperformes both KF and NKF over out-of-distribution target accelerations."
- Break condition: If the distributional shift is too large or the training data is too limited, OKF may overfit to training patterns and generalize poorly.

## Foundational Learning

- Concept: Kalman Filter assumptions (linear models, i.i.d noise, known covariances)
  - Why needed here: Understanding these assumptions is crucial because OKF's advantage stems from violations of these assumptions in real-world scenarios.
  - Quick check question: What are the three main assumptions of the standard Kalman Filter, and why do they often fail in practice?

- Concept: Cholesky decomposition for SPD matrices
  - Why needed here: OKF uses Cholesky parameterization to maintain SPD constraints on Q and R while enabling gradient optimization.
  - Quick check question: How does Cholesky decomposition help maintain the positive-definite property of covariance matrices during optimization?

- Concept: Supervised learning with sequential data
  - Why needed here: OKF optimizes KF parameters using supervised training data (states and observations), requiring understanding of how to apply gradient-based methods to sequential filtering problems.
  - Quick check question: What challenges arise when applying standard supervised learning techniques to sequential filtering problems, and how does OKF address them?

## Architecture Onboarding

- Component map: Standard KF (F, Q, H, R) -> OKF (same architecture, but Q and R learned via gradient optimization using Cholesky parameterization) -> NKF (KF with LSTM for non-linear prediction/update)
- Critical path: Data -> supervised training -> gradient optimization of Q, R -> deploy with learned parameters
- Design tradeoffs:
  - SPD constraint maintenance: Cholesky vs. diagonal parameterization (DKF ablation shows diagonal is inferior for non-linear cases)
  - Optimization stability: learning rate, batch size, and epoch count affect convergence
  - Generalization: amount and diversity of training data impacts robustness to distributional shifts
- Failure signatures:
  - Poor convergence: check learning rate, initialization, and batch size
  - Overfitting: monitor validation MSE, consider regularization or data augmentation
  - SPD constraint violation: verify Cholesky parameterization implementation
- First 3 experiments:
  1. Toy Doppler problem: compare KF (Algorithm 1) vs. OKF on linear motion with non-linear observation
  2. MOT20 video tracking: evaluate OKF on real-world pedestrian tracking data
  3. Lidar state estimation: test OKF on simulated self-driving trajectories with polar noise

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the noise parameters Q and R of the Optimized Kalman Filter (OKF) perform in comparison to the true noise parameters in scenarios where the true noise parameters are known?
- Basis in paper: [explicit] The paper demonstrates that OKF outperforms the standard Kalman Filter (KF) in various scenarios, including when the true noise parameters are known.
- Why unresolved: The paper does not provide a direct comparison of OKF's learned noise parameters to the true noise parameters in scenarios where the true parameters are known.
- What evidence would resolve it: Experiments comparing the learned noise parameters of OKF to the true noise parameters in scenarios where the true parameters are known would provide evidence for the accuracy of OKF's parameter estimation.

### Open Question 2
- Question: How does the Optimized Kalman Filter (OKF) perform in comparison to other non-linear filtering methods that are not based on neural networks?
- Basis in paper: [inferred] The paper compares OKF to the standard Kalman Filter (KF) and neural network-based filtering methods (NKF), but does not explicitly compare it to other non-linear filtering methods.
- Why unresolved: The paper does not provide a comparison of OKF to other non-linear filtering methods that are not based on neural networks.
- What evidence would resolve it: Experiments comparing OKF to other non-linear filtering methods, such as the Extended Kalman Filter (EKF) or Unscented Kalman Filter (UKF), would provide evidence for the relative performance of OKF.

### Open Question 3
- Question: How does the Optimized Kalman Filter (OKF) perform in comparison to the standard Kalman Filter (KF) in scenarios with non-linear observation models?
- Basis in paper: [explicit] The paper demonstrates that OKF outperforms the standard KF in scenarios with non-linear observation models, such as the Doppler radar problem.
- Why unresolved: The paper does not provide a comprehensive analysis of OKF's performance in comparison to the standard KF in a wide range of scenarios with non-linear observation models.
- What evidence would resolve it: Experiments comparing OKF to the standard KF in a variety of scenarios with non-linear observation models would provide evidence for the general applicability of OKF in such scenarios.

## Limitations

- The paper lacks direct comparison to recent differentiable filtering approaches that also optimize KF parameters through gradient-based techniques
- The assumption that Cholesky parameterization always maintains SPD constraints during optimization may not hold for highly ill-conditioned problems
- The theoretical understanding of why OKF generalizes better than neural models is not fully developed

## Confidence

- **High Confidence**: OKF consistently outperforms standard KF in MSE across multiple domains (Doppler radar, video tracking, lidar)
- **Medium Confidence**: OKF's robustness to distributional shifts and generalization across varying dataset sizes
- **Low Confidence**: The claim that OKF is a "more reliable baseline for comparing non-linear filtering methods" without extensive benchmarking against all recent differentiable filtering approaches

## Next Checks

1. **Benchmark against differentiable filtering methods**: Implement and compare OKF to recent differentiable approaches (e.g., Differentiable Adaptive Kalman Filtering) on the same datasets to validate OKF's competitive advantage

2. **Stress test Cholesky parameterization**: Evaluate OKF's performance on highly ill-conditioned covariance matrices or extreme noise regimes to verify that Cholesky parameterization consistently maintains SPD constraints during optimization

3. **Ablation on training data diversity**: Systematically vary the distributional shift between training and test data (beyond the acceleration changes studied) to quantify OKF's generalization limits and compare them to neural models