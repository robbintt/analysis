---
ver: rpa2
title: Convergence of Alternating Gradient Descent for Matrix Factorization
arxiv_id: '2305.06927'
source_url: https://arxiv.org/abs/2305.06927
tags:
- gradient
- descent
- matrix
- initialization
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the convergence of alternating gradient descent\
  \ for asymmetric matrix factorization. The key contribution is proving that O((\u03C3\
  1(A)/\u03C3r(A))\xB2 log(1/\u03B5)) iterations suffice to reach an \u03B5-optimal\
  \ factorization with high probability, starting from an asymmetric random initialization."
---

# Convergence of Alternating Gradient Descent for Matrix Factorization

## Quick Facts
- arXiv ID: 2305.06927
- Source URL: https://arxiv.org/abs/2305.06927
- Reference count: 6
- Primary result: Proves O((σ₁(A)/σᵣ(A))² log(1/ε)) iterations suffice for ε-optimal asymmetric matrix factorization with high probability

## Executive Summary
This paper establishes convergence guarantees for alternating gradient descent in asymmetric matrix factorization, proving that O((σ₁(A)/σᵣ(A))² log(1/ε)) iterations suffice to reach an ε-optimal factorization. The key innovation is an asymmetric random initialization that places X₀ in the column span of A with proper scaling, which maintains a uniform Polyak-Łojasiewicz (PL) inequality and Lipschitz smoothness constant throughout optimization. Experiments show this initialization significantly improves practical convergence compared to standard Gaussian initialization. The method is conjectured to extend to other nonconvex low-rank factorization problems, closing a gap between theory and practice for gradient descent in matrix factorization.

## Method Summary
The method uses alternating gradient descent on the asymmetric factorization XYT ≈ A, where X ∈ ℝᵐˣᵈ and Y ∈ ℝⁿˣᵈ with d ≥ r. The key innovation is an asymmetric initialization: X₀ = (1/√η C σ₁(A)) A Φ₁ and Y₀ = √η D σ₁(A) Φ₂, where Φ₁ and Φ₂ are Gaussian matrices. This places X₀ in the column span of A and uses asymmetric scaling factors C and D. The alternating updates are Xₜ₊₁ = Xₜ - η∇ₓf(Xₜ,Yₜ) and Yₜ₊₁ = Yₜ - η∇ᵧf(Xₜ₊₁,Yₜ). The proof relies on maintaining PL inequality and Lipschitz smoothness through the initialization structure, guaranteeing linear convergence for a sufficient number of iterations.

## Key Results
- Proves O((σ₁(A)/σᵣ(A))² log(1/ε)) iteration complexity for ε-optimal factorization
- Shows asymmetric initialization in column span of A maintains uniform PL inequality
- Experiments demonstrate significant practical improvement over standard Gaussian initialization
- Conjectures rate may improve to O(σᵣ(A)/σ₁(A)) as factors become balanced during optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maintaining a uniform Polyak-Łojasiewicz (PL) inequality throughout the optimization process guarantees linear convergence.
- Mechanism: The initialization ensures that columns of X₀ remain in the column span of A throughout iterations. Since A is rank-r, having σᵣ(Xₜ) > 0 ensures Xₜ spans the same column space as A. This enables the PL inequality: ∥∇Yf(Xₜ₊₁,Yₜ)∥²_F ≥ 2σ²ᵣ(Xₜ₊₁)f(Xₜ₊₁,Yₜ).
- Core assumption: ColSpan(X₀) ⊂ ColSpan(A) and σᵣ(X₀) > 0.
- Evidence anchors:
  - [abstract]: "a uniform Polyak-Łojasiewicz (PL) inequality and uniform Lipschitz smoothness constant are guaranteed for a sufficient number of iterations, starting from our random initialization."
  - [section 4]: "Lemmas 4.5 and 4.6 imply since X₀ is initialized in the column space of A, Xₜ remains in the column space of A for all t, and ∥∇Yf(Xₜ₊₁,Yₜ)∥²_F ≥ 2σ²ᵣ(Xₜ₊₁)f(Xₜ₊₁,Yₜ)."
- Break condition: If ColSpan(Xₜ) ⊂ ColSpan(A) fails or σᵣ(Xₜ) → 0, the PL inequality no longer holds and linear convergence is lost.

### Mechanism 2
- Claim: Asymmetric initialization with X₀ = 1/√η C σ₁(A) A Φ₁ and Y₀ = √η D σ₁(A) Φ₂ maintains bounded condition numbers during optimization.
- Mechanism: Gaussian concentration ensures σ₁(X₀)/σᵣ(X₀) ~ σ₁(A)/σᵣ(A) with high probability. The alternating update steps preserve this relationship for a sufficient number of iterations, maintaining both PL inequality and Lipschitz smoothness.
- Core assumption: X₀ is in the column span of A and Gaussian entries have sufficient concentration.
- Evidence anchors:
  - [abstract]: "by Gaussian concentration, the pseudo-condition numbers σ₁(X₀)/σᵣ(X₀) ~ σ₁(A)/σᵣ(A) are comparable with high probability; for a range of step-size η, we show that σ₁(Xₜ)/σᵣ(Xₜ) is guaranteed to remain comparable to σ₁(A)/σᵣ(A)"
  - [section 4]: "With probability at least 1-δ, ρ²σ²ᵣ(A)/(C²σ²₁(A))η ≤ σ²ᵣ(X₀) ≤ σ²₁(X₀) ≤ 9/16η"
- Break condition: If step size η is too large or initialization fails the concentration bounds, condition numbers can blow up and convergence guarantees fail.

### Mechanism 3
- Claim: The convergence rate is proportional to (σᵣ(X₀)/σ₁(X₀))² rather than (σᵣ(A)/σ₁(A))² due to the initialization structure.
- Mechanism: Starting from X₀ in the column span of A, the PL inequality gives linear convergence at rate (σᵣ(X₀)/σ₁(X₀))². Experiments suggest this improves to (σᵣ(X₀)/σ₁(X₀) as optimization progresses and factors become more balanced.
- Core assumption: X₀ remains in the column span of A and the condition number ratio is preserved.
- Evidence anchors:
  - [abstract]: "Experiments suggest that as gradient descent progresses, the unbalanced initial factor matrices become more balanced in that the factors increasingly share the condition number of A between them, experiments indicate the rate of linear convergence improves, potentially to proportional to (σᵣ(X₀)/σ₁(X₀)) rather than (σᵣ(X₀)/σ₁(X₀))²"
  - [section 3]: "Experiments illustrate that initializing X₀ in the column space of A, as well as rescaling X₀ and Y₀ asymmetrically, lead to significant practical convergence improvements"
- Break condition: If the balancing effect doesn't occur or initialization doesn't properly set up the initial condition numbers.

## Foundational Learning

- Concept: Polyak-Łojasiewicz (PL) inequality
  - Why needed here: Provides the key inequality that guarantees linear convergence without requiring convexity, specifically ∥∇f(x)∥² ≥ 2μ(f(x) - f*) for some μ > 0.
  - Quick check question: What property of Xₜ ensures the PL inequality holds in this matrix factorization setting?

- Concept: Singular value concentration for Gaussian matrices
  - Why needed here: The initialization relies on high-probability bounds that σ₁(X₀)/σᵣ(X₀) ~ σ₁(A)/σᵣ(A), which requires understanding how singular values of random Gaussian matrices concentrate.
  - Quick check question: Given a d×r Gaussian matrix Φ with i.i.d. N(0,1/d) entries, what is the typical range for σ₁(Φ)/σᵣ(Φ) as d grows relative to r?

- Concept: Alternating gradient descent dynamics
  - Why needed here: The proof analyzes the interaction between X and Y updates, showing that one factor's singular values control the convergence rate of the other.
  - Quick check question: In the alternating update, if Xₜ has full column rank and spans Col(A), what happens to Xₜ₊₁ after one gradient step with respect to Y?

## Architecture Onboarding

- Component map: Initialization → Alternating Gradient Updates → PL Inequality Maintenance → Convergence Guarantee
- Critical path: Initialization (X₀ in Col(A) + asymmetric scaling) → First T iterations (PL constant maintained) → Linear convergence achieved
- Design tradeoffs: Unbalanced initialization vs. standard Gaussian initialization - improved theoretical convergence vs. potential practical imbalance
- Failure signatures: Slow convergence, loss of linear rate, condition numbers blowing up, PL inequality violations
- First 3 experiments:
  1. Compare convergence rates for different d/r ratios (r+1 vs. 2r vs. 3r) on a fixed rank-r matrix
  2. Test initialization sensitivity by varying C and D parameters around theoretical values
  3. Measure σ₁(Xₜ)/σᵣ(Xₜ) evolution over iterations to verify concentration is maintained

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate change as the matrix factorization becomes more balanced during optimization, as suggested by the experiments?
- Basis in paper: [explicit] The paper conjectures that the convergence rate may improve from (σr/σ1)² to (σr/σ1) as the factors become more balanced, based on experimental observations in Figures 1 and 2.
- Why unresolved: The theoretical analysis is conservative and doesn't account for this dynamic balancing effect observed in practice. The proof method maintains a uniform PL-inequality throughout, but doesn't capture potential improvements in the convergence rate as the factors equilibrate.
- What evidence would resolve it: Rigorous theoretical analysis proving improved convergence rates as the factors become balanced, or comprehensive experimental validation across diverse matrix factorization problems showing consistent improvement in convergence rate.

### Open Question 2
- Question: Can the proof technique for alternating gradient descent be extended to analyze full gradient descent for matrix factorization?
- Basis in paper: [explicit] The paper notes that existing results hold for gradient descent while their analysis is for alternating gradient descent, and suggests their proof method "should be useful for extending and simplifying convergence analyses for a broader class of nonconvex low-rank factorization problems."
- Why unresolved: The paper focuses on alternating gradient descent and doesn't provide the extension to full gradient descent. The technical differences between the two approaches may introduce new challenges in maintaining the PL-inequality and Lipschitz smoothness constant.
- What evidence would resolve it: A complete convergence analysis for full gradient descent using similar techniques, or a clear identification of the technical barriers preventing such an extension.

### Open Question 3
- Question: What is the precise relationship between the degree of overparameterization (d/r) and the convergence rate, beyond the asymptotic regime where d >> r?
- Basis in paper: [explicit] The paper shows that with mild overparameterization d = (1+α)r, the dependence on rank r disappears, and mentions that by appealing to high-probability bounds on singular values of a square Gaussian matrix, results could be derived for d = r.
- Why unresolved: The analysis only provides explicit bounds for d > r with mild overparameterization, and only suggests that d = r could be analyzed using existing results. The transition between these regimes and the precise behavior for intermediate values of d/r remains unexplored.
- What evidence would resolve it: A comprehensive analysis covering all values of d/r, or systematic experiments mapping the convergence behavior across different degrees of overparameterization.

## Limitations
- The PL inequality and Lipschitz smoothness guarantees only hold for a "sufficient number of iterations" without specifying exact duration
- Practical improvement claims rely heavily on experimental evidence that suggests rate improvements beyond the proven (σ₁(X₀)/σᵣ(X₀))² bound
- Initialization parameters C and D have ranges rather than specific values, creating uncertainty about optimal practical choices

## Confidence

**High Confidence**: The basic alternating gradient descent algorithm and its connection to matrix factorization (well-established framework)

**Medium Confidence**: The (σ₁(A)/σᵣ(A))² convergence rate bound and the initialization mechanism (proven but with limited iteration guarantees)

**Low Confidence**: The claimed practical improvements beyond the theoretical rate and the conjectured extension to other nonconvex problems (primarily experimental observations)

## Next Checks

1. **Convergence Duration Analysis**: Systematically measure how many iterations the PL inequality and Lipschitz smoothness constants remain valid under different problem parameters (varying d/r ratios and condition numbers).

2. **Parameter Sensitivity Study**: Test a grid of C and D values around the theoretical bounds to quantify the impact on convergence speed and determine optimal practical choices.

3. **Balancing Effect Verification**: Track σ₁(Xₜ)/σᵣ(Xₜ) and σ₁(Yₜ)/σᵣ(Yₜ) ratios throughout optimization to verify whether the factors actually become more balanced as claimed, and measure the resulting convergence rate improvement.