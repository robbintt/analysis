---
ver: rpa2
title: 'RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment'
arxiv_id: '2304.06767'
source_url: https://arxiv.org/abs/2304.06767
tags:
- reward
- raft
- arxiv
- learning
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RAFT introduces a gradient-free fine-tuning framework for aligning
  generative models using reward-based sample selection. Instead of RLHF's policy
  optimization, RAFT ranks generated samples by reward and fine-tunes on the top percentile,
  achieving stable alignment without KL penalties.
---

# RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment

## Quick Facts
- arXiv ID: 2304.06767
- Source URL: https://arxiv.org/abs/2304.06767
- Authors: 
- Reference count: 40
- Key outcome: RAFT achieves stable alignment by ranking generated samples by reward and fine-tuning only on the top percentile, avoiding RLHF's policy optimization instability.

## Executive Summary
RAFT introduces a gradient-free fine-tuning framework for aligning generative models using reward-based sample selection. Instead of RLHF's policy optimization, RAFT ranks generated samples by reward and fine-tunes on the top percentile, achieving stable alignment without KL penalties. Experiments show RAFT outperforms PPO in the reward-perplexity tradeoff curve, particularly with small learning rates. On LLMs, RAFT improves sentiment alignment (IMDB) and toxicity reduction (RealToxicityPrompts) while maintaining generation quality. For diffusion models, it restores 256×256 generation capability and improves text-image alignment. The method avoids overfitting to imperfect reward models and reduces computational cost by training on fewer, higher-quality samples.

## Method Summary
RAFT is a reward-based fine-tuning algorithm that aligns generative models without RL optimization. The method generates multiple samples, ranks them by a reward function, and selects the top k% for supervised fine-tuning. Unlike RLHF, RAFT uses gradient-based updates directly on filtered samples rather than policy gradients. The algorithm employs small learning rates (2e-6 for LLaMA, 5e-5 for GPT-Neo) and early stopping to trace a regularization path that balances reward optimization with generation quality. For diffusion models, RAFT uses LoRA for efficient fine-tuning. The framework works for both LLMs (IMDB sentiment, toxicity reduction) and diffusion models (resolution adaptation, text-image alignment).

## Key Results
- RAFT achieves superior reward-perplexity tradeoff compared to PPO, with smaller learning rates producing better alignment quality
- On LLMs, RAFT improves sentiment alignment on IMDB dataset and reduces toxicity on RealToxicityPrompts while maintaining perplexity
- For diffusion models, RAFT restores 256×256 generation capability and improves text-image alignment scores
- The method avoids overfitting to imperfect reward models and reduces computational cost by training on fewer, higher-quality samples

## Why This Works (Mechanism)

### Mechanism 1
RAFT achieves stable alignment by filtering high-reward samples before training, avoiding gradient instability inherent in RL methods like PPO. Instead of policy optimization, RAFT generates many samples, ranks them by reward, and fine-tunes only on the top percentile. This gradient-free approach sidesteps the trial-and-error instability of RL. Core assumption: Reward functions can reliably identify high-quality samples, and the top percentile contains enough diversity for effective fine-tuning. Evidence anchors: [abstract] "RAFT ranks generated samples by reward and fine-tunes on the top percentile, achieving stable alignment without KL penalties." [section 3.2] "RAFT iteratively updates w... selects the 1/k percent of samples with the highest reward as the training samples B." Break condition: If the reward function is noisy or biased, RAFT may overfit to spurious correlations and degrade generation quality.

### Mechanism 2
Small learning rates in RAFT improve the reward-perplexity tradeoff by acting as implicit regularization. A small learning rate with early stopping traces a regularization path similar to L1 regularization, balancing reward optimization with maintaining generation quality (measured by perplexity). Core assumption: The tradeoff curve is smooth enough that early stopping at appropriate points yields better overall performance than aggressive reward optimization. Evidence anchors: [section 3.1] "small learning rate and early stopping... shows that with small learning rate, RAFT forms a reward perplexity tradeoff curve that is superior to that of PPO." [section 4.1] "we observe that although we do not incorporate KL penalty explicitly in the current RAFT algorithm, a smaller learning rate leads to a milder change in perplexity." Break condition: If the learning rate is too small, convergence may be impractically slow or the model may get stuck in poor local minima.

### Mechanism 3
RAFT's forward-only data collection reduces computational cost compared to RL methods that require expensive backward passes on all samples. By generating many samples but training only on the filtered subset, RAFT amortizes the cost of reward computation over fewer backward passes, making it more efficient than PPO under the same sample budget. Core assumption: Forward pass cost is significantly lower than backward pass cost, so discarding most samples still yields net computational savings. Evidence anchors: [abstract] "The RAFT algorithm is computationally more effective than its predecessors given the same number of examples." [section 3.2] "while we drop most of the generated samples... this may provide some computational advantages for RAFT as a by-product." Break condition: If the reward computation is extremely expensive, the forward-only advantage diminishes.

## Foundational Learning

- Concept: Reward modeling and alignment
  - Why needed here: RAFT relies on a reward function to rank samples and guide fine-tuning. Understanding how reward models are trained and their limitations is critical.
  - Quick check question: What happens if the reward model is poorly calibrated or biased toward certain outputs?

- Concept: Perplexity as a quality metric
  - Why needed here: Perplexity measures generation quality and is used to monitor the reward-perplexity tradeoff. Engineers must understand how perplexity correlates with human judgment.
  - Quick check question: How would you interpret a drop in reward accompanied by a rise in perplexity—does it indicate overfitting or better alignment?

- Concept: Diffusion model fine-tuning
  - Why needed here: RAFT is demonstrated on both LLMs and diffusion models. Understanding score-based generative modeling and LoRA fine-tuning is essential for diffusion applications.
  - Quick check question: Why might LoRA be preferred over full fine-tuning for diffusion models in the RAFT framework?

## Architecture Onboarding

- Component map:
  Generator (LLM or diffusion model) -> Sample generation -> Reward computation -> Ranking/filtering module -> Fine-tuning module -> Updated model

- Critical path:
  1. Generate batch of samples from current model
  2. Compute rewards for all samples
  3. Rank and select top percentile
  4. Fine-tune model on selected samples
  5. Evaluate reward and quality metrics
  6. Repeat until convergence or early stopping

- Design tradeoffs:
  - Acceptance ratio (1/k) vs. diversity: Smaller k yields higher reward samples but risks mode collapse
  - Temperature vs. reward signal: Higher temperature increases diversity but may reduce reward signal strength
  - Learning rate vs. stability: Smaller rates improve tradeoff but slow convergence

- Failure signatures:
  - Overfitting to reward: High reward, low perplexity diversity, repetitive outputs
  - Sparse sampling: Few samples pass the reward threshold, stalling training
  - Degraded generation quality: Perplexity spikes without corresponding reward gains

- First 3 experiments:
  1. Implement RAFT on a small LLM (e.g., GPT-2) with a simple sentiment reward model; measure reward vs. perplexity tradeoff
  2. Compare RAFT vs. PPO on the same task with identical sample budgets; record convergence speed and final metrics
  3. Apply RAFT to a diffusion model (e.g., Stable Diffusion) for resolution adaptation; evaluate output quality and alignment

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but the following areas remain unresolved based on the content:

### Open Question 1
- Question: How does RAFT's performance scale with different acceptance ratios (1/k) and what is the optimal tradeoff between reward preference and sample diversity?
- Basis in paper: [explicit] The paper discusses the acceptance ratio as a key hyperparameter determining the degree of reward preference, noting that smaller ratios lead to accepting only highly rewarding samples.
- Why unresolved: The paper uses a fixed acceptance ratio of 0.2 in experiments but doesn't systematically explore how different ratios affect the reward-perplexity tradeoff or generation quality.
- What evidence would resolve it: Experiments comparing RAFT performance across a range of acceptance ratios (e.g., 0.1, 0.2, 0.5) on multiple tasks, measuring both reward improvement and perplexity degradation.

### Open Question 2
- Question: How does RAFT compare to other alignment methods like Direct Preference Optimization (DPO) in terms of reward-perplexity tradeoff and computational efficiency?
- Basis in paper: [inferred] The paper establishes RAFT's superiority over PPO in reward-perplexity tradeoff and mentions that RAFT is more computationally efficient than RL methods, but doesn't compare against newer alignment methods like DPO.
- Why unresolved: DPO has emerged as a simpler alternative to RLHF that directly optimizes a preference loss, and it would be valuable to understand how RAFT compares to this method.
- What evidence would resolve it: Direct experimental comparison of RAFT and DPO on the same tasks, measuring both reward improvement and generation quality metrics like perplexity.

### Open Question 3
- Question: What is the theoretical justification for why small learning rates in RAFT lead to better reward-perplexity tradeoffs compared to PPO?
- Basis in paper: [explicit] The paper claims that "small learning rate leads to a milder change in perplexity" and shows this through experiments, but doesn't provide theoretical analysis.
- Why unresolved: While empirical results demonstrate this phenomenon, the underlying mechanism connecting learning rate, early stopping, and the reward-perplexity tradeoff is not explained.
- What evidence would resolve it: Theoretical analysis connecting the RAFT update procedure to gradient boosting regularization paths, or empirical studies showing how different learning rates affect the sharpness of the reward landscape versus the flatness of the perplexity landscape.

## Limitations
- Limited exploration of reward model sensitivity and failure modes when rewards are noisy or biased
- Computational efficiency claims lack rigorous quantification and head-to-head comparison with PPO
- Diffusion model applications demonstrated only on text-to-image alignment, not other multi-modal tasks

## Confidence

**High confidence**: RAFT's basic algorithmic framework and its implementation for LLM fine-tuning (tested on IMDB and RealToxicityPrompts). The mechanism of ranking samples and training on top percentiles is clearly specified and reproducible.

**Medium confidence**: RAFT's advantages over PPO in reward-perplexity tradeoff, as this depends heavily on reward model quality and hyperparameter choices (learning rate, acceptance ratio) that may vary across tasks.

**Low confidence**: Computational efficiency claims and diffusion model applications, given limited empirical comparison and the complexity of diffusion model fine-tuning with LoRA.

## Next Checks

1. Test RAFT with intentionally degraded reward models (adding noise or bias) to quantify robustness thresholds and identify failure modes when rewards misrank samples.

2. Conduct head-to-head computational cost analysis comparing RAFT vs. PPO with identical sample budgets, including both forward and backward pass costs across different reward model complexities.

3. Implement RAFT on a third generative model type (e.g., autoregressive audio models) to validate generalizability beyond LLMs and diffusion models.