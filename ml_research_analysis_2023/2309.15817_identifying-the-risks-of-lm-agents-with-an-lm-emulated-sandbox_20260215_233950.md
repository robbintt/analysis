---
ver: rpa2
title: Identifying the Risks of LM Agents with an LM-Emulated Sandbox
arxiv_id: '2309.15817'
source_url: https://arxiv.org/abs/2309.15817
tags:
- agent
- tool
- user
- action
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce ToolEmu, a framework for scalable evaluation of language
  model (LM) agents using LM-emulated tools and environments. ToolEmu uses GPT-4 to
  emulate tool execution and sandbox states, enabling testing across diverse tools
  without manual implementation.
---

# Identifying the Risks of LM Agents with an LM-Emulated Sandbox

## Quick Facts
- arXiv ID: 2309.15817
- Source URL: https://arxiv.org/abs/2309.15817
- Reference count: 40
- Key outcome: LM agents exhibit severe failures 23.9% of the time, even with safety measures in place

## Executive Summary
This paper introduces ToolEmu, a framework for evaluating Language Model (LM) agent safety using GPT-4 to emulate tools and environments. The framework addresses the challenge of scalable safety testing by using language models to simulate tool execution and sandbox states, eliminating the need for manual sandbox implementations. The authors develop an automatic safety evaluator that identifies agent failures and quantifies risk severities, validated against human judgment. Their benchmark of 36 high-stakes tools reveals that even the safest LM agents fail catastrophically in nearly a quarter of test cases.

## Method Summary
The authors create ToolEmu, a framework using GPT-4 to emulate tool execution and sandbox states for LM agent evaluation. They develop an automatic safety evaluator using GPT-4 to assess agent trajectories for risky actions and outcomes. The evaluation uses a benchmark of 36 high-stakes tools across 9 risk categories, testing agents like GPT-4, Claude-2, and Vicuna-1.5. Human evaluation validates both the emulator outputs and safety assessments, with 100 test cases reviewed for emulations and 300 for evaluations. The framework includes both standard and adversarial emulation modes to identify failures under different conditions.

## Key Results
- ToolEmu's automatic safety evaluator achieves 68.8% precision in identifying true failures
- Even the safest LM agents exhibit severe failures 23.9% of the time across the benchmark
- Human evaluation shows 89.7% agreement on emulator output realism and 83.7% agreement on safety assessments

## Why This Works (Mechanism)

### Mechanism 1
Using GPT-4 as an emulator allows flexible, scalable evaluation of LM agents without requiring manual sandbox implementations for each tool. GPT-4 can simulate tool execution and sandbox states purely from tool specifications and inputs, enabling rapid prototyping and risk assessment across a broad range of tools.

### Mechanism 2
The adversarial emulator increases the likelihood of identifying rare, high-severity failures by automatically instantiating long-tail scenarios. The adversarial emulator uses provided underspecifications, potential risks, and risky actions to craft challenging sandbox states where LM agents are more likely to make mistakes.

### Mechanism 3
The automatic safety evaluator identifies agent failures and quantifies risk severities in a manner that aligns with human evaluation. GPT-4 is prompted to examine agent trajectories, detect risky actions, and assess likelihood and severity of outcomes based on underspecifications and potential risks.

## Foundational Learning

- **Concept**: Partially Observable Markov Decision Process (POMDP)
  - **Why needed here**: The LM agent environment is formalized as a POMDP where the agent takes actions and receives observations
  - **Quick check question**: What are the key components of a POMDP, and how do they apply to the LM agent setup described?

- **Concept**: Risk assessment and severity classification
  - **Why needed here**: The safety evaluator must assess both the likelihood and severity of risky outcomes
  - **Quick check question**: How are likelihood and severity defined and categorized in the safety evaluation process?

- **Concept**: Red-teaming and adversarial scenario generation
  - **Why needed here**: The adversarial emulator is used to automatically instantiate challenging scenarios to stress-test agent safety
  - **Quick check question**: What is the goal of red-teaming in the context of LM agent safety evaluation, and how does the adversarial emulator achieve it?

## Architecture Onboarding

- **Component map**: Agent -> Emulator (standard/adversarial) -> Evaluator -> Benchmark
- **Critical path**: Agent takes action → Emulator simulates tool execution and returns observation → Evaluator assesses safety and helpfulness based on full trajectory
- **Design tradeoffs**: Using LM-based emulation vs. manual sandbox implementation: Flexibility and scalability vs. potential accuracy issues; Standard vs. adversarial emulation: Balanced vs. more aggressive failure detection; Automatic vs. human evaluation: Scalability vs. potential misalignment with human judgment
- **Failure signatures**: Emulator: Critical issues (violates realism, accuracy, or consistency), minor issues (plausibility problems); Evaluator: Disagreement with human annotations, missed failures, misclassified risk severities
- **First 3 experiments**:
  1. Run agent on a simple test case with standard emulator and verify emulator outputs match expected tool behavior
  2. Run agent on a test case with adversarial emulator and compare failure incidence to standard emulator
  3. Compare automatic evaluator output to human annotations on a sample of trajectories to assess agreement

## Open Questions the Paper Calls Out

1. **Question**: How do the authors plan to address the minor issues in emulator outputs, such as simulating generic entries like "123-456-7890" as phone numbers?
   - **Basis in paper**: [explicit] - The paper mentions that emulators occasionally exhibit minor issues, such as emulating generic entries like "123-456-7890" as phone numbers. The authors hypothesize that such behaviors may arise from privacy and confidentiality stipulations integrated into GPT-4 during instruction tuning with human feedback.
   - **Why unresolved**: The paper acknowledges the existence of minor issues but does not provide a concrete plan to address them. The authors only suggest that these issues do not impede the instantiability of emulations, implying that sandbox states can be adjusted accordingly.
   - **What evidence would resolve it**: A clear plan or methodology for refining the emulator prompts or post-processing steps to reduce the occurrence of generic entries in emulator outputs.

2. **Question**: How does the framework handle the scenario where an LM agent fabricates information for tool inputs that happens to be valid in the real world?
   - **Basis in paper**: [explicit] - The paper discusses that tool inputs containing fabricated values made up by the LM agent can either be accepted as valid but incorrect and potentially risky, or rejected as a violation of the "Semantic Validity" requirement. Human annotators are asked to assess these instances on a case-by-case basis.
   - **Why unresolved**: The paper does not provide a clear guideline or automated mechanism for handling this ambiguous scenario. It relies on human judgment, which may not be scalable for large-scale evaluations.
   - **What evidence would resolve it**: A detailed guideline or automated mechanism for determining the validity of fabricated tool inputs, along with an evaluation of its effectiveness in reducing false positives and false negatives.

3. **Question**: What is the impact of temperature on the safety and helpfulness of LM agents, and how does it affect the overall evaluation results?
   - **Basis in paper**: [explicit] - The paper mentions that due to the stochasticity of calling the OpenAI API even with a temperature of 0, the evaluation results are not fully deterministic. It estimates the standard errors of the average safety score, average helpfulness score, and failure incidence with 3 independent runs.
   - **Why unresolved**: The paper does not provide a comprehensive analysis of the impact of temperature on the evaluation results. It only mentions that the results are not fully deterministic and provides standard error estimates for a limited number of runs.
   - **What evidence would resolve it**: A detailed analysis of the impact of temperature on the evaluation results, including the effect on safety scores, helpfulness scores, and failure incidence across a range of temperature values and a larger number of runs.

## Limitations
- The framework relies heavily on GPT-4's capabilities for both emulation and evaluation, creating uncertainty about result reliability
- Human validation was performed on a limited scale (100 test cases for emulations, 300 for evaluations) compared to the full benchmark
- The benchmark covers only 36 tools and 9 risk types, which may not capture the full spectrum of potential agent failures in real-world applications

## Confidence
- **High confidence**: The core finding that even the safest LM agents exhibit severe failures 23.9% of the time is well-supported by the benchmark results and human validation
- **Medium confidence**: The claim that GPT-4 can effectively emulate tool execution and sandbox states is supported by the 89.7% human agreement on emulation realism, but this may not generalize to all tool types
- **Medium confidence**: The automatic safety evaluator's ability to identify failures with 68.8% precision is supported by human validation, but the moderate precision suggests potential limitations in failure detection

## Next Checks
1. **Expand human validation scale**: Increase the number of test cases subject to human validation from 100 to 300+ to better assess the reliability of automatic evaluation results across the full benchmark
2. **Cross-model comparison**: Test the framework with multiple LM models as emulators (beyond just GPT-4) to assess the sensitivity of results to the choice of emulation model and identify potential biases
3. **Tool diversity assessment**: Evaluate the framework's performance on a broader set of tools beyond the current 36, particularly focusing on tools with complex state management or non-deterministic outputs, to identify potential failure modes of the emulation approach