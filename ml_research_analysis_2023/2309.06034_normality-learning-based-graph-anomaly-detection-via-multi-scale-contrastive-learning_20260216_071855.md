---
ver: rpa2
title: Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive
  Learning
arxiv_id: '2309.06034'
source_url: https://arxiv.org/abs/2309.06034
tags:
- anomaly
- nodes
- normality
- graph
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses graph anomaly detection (GAD), focusing on
  learning the normal pattern in graphs to improve detection performance. Existing
  GAD methods often struggle to distinguish anomalies from normal nodes due to the
  dominance of normal nodes in the data.
---

# Normality Learning-based Graph Anomaly Detection via Multi-Scale Contrastive Learning

## Quick Facts
- arXiv ID: 2309.06034
- Source URL: https://arxiv.org/abs/2309.06034
- Reference count: 40
- Key outcome: Achieves up to 5.89% improvement in AUC over state-of-the-art methods for graph anomaly detection

## Executive Summary
This paper addresses graph anomaly detection (GAD) by proposing a normality learning framework that focuses on learning normal patterns to improve detection performance. The authors argue that existing GAD methods struggle because anomalies are often similar to normal nodes in structure and attributes. Their approach, NLGAD, uses multi-scale contrastive learning to initialize a model that captures anomalous information at both subgraph and node levels, then refines this model using only reliable normal nodes selected through a hybrid strategy. Experiments on six benchmark datasets demonstrate significant improvements in AUC scores compared to state-of-the-art methods.

## Method Summary
NLGAD is a three-stage framework for graph anomaly detection. First, it initializes a model using multi-scale contrastive learning with subgraph-node and node-node contrasts to capture anomalous information. Second, it employs a hybrid normality selection strategy combining dynamic and percent approaches to identify reliable normal nodes. Finally, it refines the model by training only on the selected normal nodes to learn a more accurate representation of normality. The framework uses GCN layers for graph convolutions, RWR for subgraph sampling, and multi-round detection to compute final anomaly scores.

## Key Results
- Achieves up to 5.89% improvement in AUC over state-of-the-art methods
- Multi-scale contrastive learning (subgraph-node + node-node) provides significant performance gains
- Hybrid normality selection strategy effectively balances reliability and sufficiency of normal nodes
- Extensive experiments demonstrate effectiveness across six benchmark datasets (Cora, UAI2010, CiteSeer, DBLP, Citation, ACM)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Normality learning improves anomaly detection by training only on normal nodes, which reduces the overlap between normal and anomalous node distributions.
- Mechanism: By using only reliable normal nodes in the training phase, the model learns a more accurate representation of normality, making anomalies easier to distinguish based on their deviation from this learned pattern.
- Core assumption: Normal nodes make up the majority of the graph and their patterns can be learned without contamination from anomalies.
- Evidence anchors:
  - [abstract] "the model is prone to learn the pattern of normal samples which make up the majority of samples" and "anomalies can be easily detected when their behaviors differ from normality."
  - [section 4.3] "Its purpose is to boost the model's ability to learn the pattern of normal nodes."
- Break condition: If the normality selection strategy fails to identify enough reliable normal nodes, the model may overfit or learn incorrect patterns.

### Mechanism 2
- Claim: Multi-scale contrastive learning captures anomalous information at both subgraph and node levels, improving detection performance.
- Mechanism: Subgraph-node contrast captures neighborhood-level anomalies, while node-node contrast captures node-level anomalies. Combining both scales provides richer anomalous information.
- Core assumption: Anomalies manifest differently at different scales (local vs. global).
- Evidence anchors:
  - [abstract] "we first initialize the model with the contrastive networks on different scales."
  - [section 4.1] "To initialize the model, we first construct the contrastive networks containing subgraph-node (SN) and node-node (NN) contrasts."
- Break condition: If the scales are not properly balanced or if one scale dominates, important anomalous signals may be missed.

### Mechanism 3
- Claim: The hybrid normality selection strategy effectively balances reliability and sufficiency of normal nodes for training.
- Mechanism: A dynamic strategy gradually adds high-confidence normal nodes to a pool, while a percent strategy ensures enough normal nodes are selected. This balances quality and quantity.
- Core assumption: Lower anomaly scores indicate higher confidence in normality, and the model's detection capability improves during training.
- Evidence anchors:
  - [abstract] "we devise an effective hybrid strategy for normality selection."
  - [section 4.2.2] "We adopt a dynamic strategy to retain the most reliable part of the results."
- Break condition: If the percent threshold is too high, insufficient normal nodes are selected; if too low, unreliable nodes contaminate the training.

## Foundational Learning

- Concept: Graph neural networks (GNNs) and their smoothing effect
  - Why needed here: Understanding why GCNs are problematic for GAD (they smooth anomalies with neighbors) motivates the need for multi-scale contrastive learning.
  - Quick check question: What is the fundamental limitation of GCNs that makes them less effective for anomaly detection compared to contrastive approaches?

- Concept: Contrastive learning framework
  - Why needed here: The core methodology relies on contrasting normal vs. anomalous patterns at multiple scales.
  - Quick check question: How do positive and negative pairs work differently in subgraph-node vs. node-node contrastive learning for GAD?

- Concept: Anomaly score interpretation and distribution analysis
  - Why needed here: The normality learning approach relies on understanding how anomaly scores distribute between normal and anomalous nodes.
  - Quick check question: Why does training only on normal nodes lead to better separation in anomaly score distributions compared to training on all nodes?

## Architecture Onboarding

- Component map: Model initialization (multi-scale contrastive learning) -> Normality selection (hybrid strategy) -> Normality learning (refine training) -> Inference (multi-round detection)
- Critical path: Model initialization → Normality selection → Normality learning → Inference
- Design tradeoffs: Balancing the number of normal nodes selected vs. their reliability; choosing between different normality selection strategies; tuning the balance between subgraph-node and node-node contrasts
- Failure signatures: Poor performance if normality selection fails (too few or unreliable normals); overfitting if too many epochs in normality learning; sensitivity to hyperparameters like K (percent of nodes selected) and Ts (steps in normality selection)
- First 3 experiments:
  1. Compare AUC with different K values (e.g., 0.2, 0.5, 0.8) to find optimal normality selection
  2. Test performance with only subgraph-node contrast vs. only node-node contrast to validate multi-scale benefits
  3. Evaluate the impact of removing normality learning (training on all nodes vs. selected normals only)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the normality selection strategy be improved to handle larger graphs with millions of nodes efficiently?
- Basis in paper: [explicit] The paper mentions that the normality selection strategy involves estimating anomaly degrees for each node at each step and gradually adding high-confident estimates to the normality pool. However, this approach may become computationally expensive for larger graphs.
- Why unresolved: The paper does not discuss how to scale the normality selection strategy for larger graphs, and the current approach may not be efficient enough for graphs with millions of nodes.
- What evidence would resolve it: Experiments comparing the performance and computational efficiency of the normality selection strategy on graphs with different sizes, and exploring alternative strategies for handling larger graphs.

### Open Question 2
- Question: How does the choice of the loss balance parameter α impact the detection of different types of anomalies (contextual vs. structural)?
- Basis in paper: [explicit] The paper mentions that α is used to balance the importance between subgraph-node and node-node contrasts, but it does not discuss how this affects the detection of different types of anomalies.
- Why unresolved: The paper does not provide insights into how the choice of α impacts the detection of contextual and structural anomalies separately.
- What evidence would resolve it: Experiments analyzing the performance of the model on datasets with different types of anomalies and varying values of α to determine its impact on the detection of contextual and structural anomalies.

### Open Question 3
- Question: Can the normality learning framework be extended to handle dynamic graphs where nodes and edges change over time?
- Basis in paper: [inferred] The paper focuses on static attributed graphs, but it does not discuss how the normality learning framework can be adapted for dynamic graphs.
- Why unresolved: The paper does not address the challenges and potential solutions for applying the normality learning framework to dynamic graphs.
- What evidence would resolve it: Experiments evaluating the performance of the normality learning framework on dynamic graph datasets and proposing modifications to handle temporal changes in the graph structure.

## Limitations
- The normality selection strategy's effectiveness relies heavily on the assumption that normal nodes can be reliably identified through lower anomaly scores, which may not hold for all graph types
- The framework's performance depends on careful hyperparameter tuning, particularly for the normality selection threshold and the balance between multi-scale contrasts
- Computational complexity may become prohibitive for very large graphs due to the iterative normality selection process

## Confidence

**Confidence: Low** - The paper claims significant improvements over baselines, but the ablation studies lack comprehensive comparisons with alternative normality selection strategies. The hybrid strategy combining dynamic and percent approaches appears effective, but its superiority over simpler alternatives is not rigorously demonstrated.

**Confidence: Medium** - The multi-scale contrastive learning approach is well-motivated, but the paper doesn't provide detailed analysis of how much each scale contributes to performance. The balance between subgraph-node and node-node contrasts is treated as a hyperparameter rather than being theoretically justified.

**Confidence: Medium** - The assumption that normal nodes can be reliably identified through lower anomaly scores is fundamental to the approach, but the paper doesn't address scenarios where this assumption breaks down, particularly in graphs with subtle anomalies or varying anomaly types.

## Next Checks

1. **Normality Selection Robustness**: Test the hybrid strategy against simpler alternatives (pure dynamic or pure percent) across all datasets to quantify the actual benefit of the combined approach.

2. **Scale Contribution Analysis**: Conduct controlled experiments isolating subgraph-node vs. node-node contrasts to determine their individual contributions and optimal weighting.

3. **Edge Case Performance**: Evaluate performance on datasets with varying anomaly types (structural vs. contextual) and difficulty levels to identify limitations of the normality learning assumption.