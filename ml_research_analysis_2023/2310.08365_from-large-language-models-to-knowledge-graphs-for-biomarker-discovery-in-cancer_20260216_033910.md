---
ver: rpa2
title: From Large Language Models to Knowledge Graphs for Biomarker Discovery in Cancer
arxiv_id: '2310.08365'
source_url: https://arxiv.org/abs/2310.08365
tags:
- cancer
- knowledge
- data
- types
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of keeping knowledge graphs up-to-date
  in rapidly evolving domains like cancer research, where new findings can render
  existing knowledge obsolete. The authors propose a method to enrich and validate
  a cancer-specific knowledge graph using large language models (LLMs) and fine-tuned
  biomedical NER models (BioBERT and SciBERT).
---

# From Large Language Models to Knowledge Graphs for Biomarker Discovery in Cancer

## Quick Facts
- arXiv ID: 2310.08365
- Source URL: https://arxiv.org/abs/2310.08365
- Reference count: 26
- BioBERT achieved highest F1-score (91.36%) in recognizing genes/proteins and diseases

## Executive Summary
This paper addresses the critical challenge of maintaining up-to-date knowledge graphs in rapidly evolving domains like cancer research, where new findings can quickly render existing knowledge obsolete. The authors propose an integrated approach combining large language models (LLMs) and fine-tuned biomedical NER models (BioBERT and SciBERT) to enrich and validate a cancer-specific knowledge graph. By leveraging recent scientific articles and domain ontologies, the system can identify inconsistencies and incompleteness in existing knowledge representations, enabling more accurate and current biomarker discovery for cancer research.

## Method Summary
The method constructs a cancer-specific knowledge graph using an ontology (OncoNet Ontology) and enriches it with facts extracted from recent scientific articles using fine-tuned BioBERT and SciBERT models. The knowledge graph is validated and updated using large language models, which generate new triples from recent articles and compare them with existing ones to identify inconsistencies or incompleteness. The BioBERT model achieved the highest F1-score (91.36%) in recognizing genes/proteins and diseases, outperforming other models by 2-7.85%. The approach integrates semantic reasoning through the domain ontology to validate gene-disease relations in the knowledge graph.

## Key Results
- BioBERT achieved 91.36% F1-score in recognizing genes/proteins and diseases
- BioBERT outperformed SciBERT by approximately 2% and BiLSTM-CRF by 7.85% in F1-score
- LLM-based validation identified inconsistencies and incompleteness in the knowledge graph using recent scientific articles

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning transformer models like BioBERT and SciBERT on biomedical corpora significantly improves their performance on domain-specific NER and EL tasks. The BioBERT model achieved 91.36% F1-score in recognizing genes/proteins and diseases, outperforming general-purpose models by 2-7.85%. This improvement stems from domain-specific pre-training that captures biomedical terminology and relationships more effectively than general BERT variants.

### Mechanism 2
LLMs can effectively validate and update knowledge graphs with recent findings by generating new triples from scientific articles and comparing them with existing knowledge. The approach uses instruction-tuning with supervised fine-tuning to leverage LLM capabilities for information extraction with in-context learning. This mechanism addresses concept drift by incorporating up-to-date domain knowledge that may not have been seen during the original NER model training.

### Mechanism 3
The domain ontology (ONO) enables semantic reasoning for validating gene-disease relations through its axioms and rules. By reusing existing ontologies containing annotations about diseases, genes, concepts, and biological processes, the system can infer new knowledge and validate existing facts in the knowledge graph. This semantic layer ensures consistency and completeness in the represented domain knowledge.

## Foundational Learning

- Concept: Named Entity Recognition (NER) and Relation Extraction (RE)
  - Why needed here: Essential for extracting structured knowledge from unstructured biomedical texts to construct and enrich the knowledge graph
  - Quick check question: Can you explain the difference between NER and RE and provide an example of each in the biomedical domain?

- Concept: Knowledge Graphs (KGs) and Ontologies
  - Why needed here: KGs represent structured knowledge about genes, proteins, diseases, and relationships, while ontologies provide the schema and axioms enabling semantic reasoning
  - Quick check question: How do ontologies differ from KGs, and why are both needed in this context?

- Concept: Transformer-based Language Models (BERT, BioBERT, SciBERT)
  - Why needed here: Domain-specific pre-trained models like BioBERT and SciBERT achieve superior performance on biomedical NLP tasks due to their training on specialized corpora
  - Quick check question: What are the key differences between BERT, BioBERT, and SciBERT, and why is domain-specific pre-training important for biomedical NLP tasks?

## Architecture Onboarding

- Component map: Domain Ontology (ONO) -> Knowledge Graph (KG) -> NER and RE Models (BioBERT, SciBERT) -> LLM-based Validator -> Semantic Reasoner

- Critical path: 1. Construct domain ontology (ONO) 2. Extract structured knowledge using NER and RE models 3. Integrate knowledge into KG 4. Validate/update KG using LLMs 5. Perform semantic reasoning on KG

- Design tradeoffs: Domain-specific pre-trained models vs. general-purpose models for NER/RE; LLM-based validation vs. manual curation; incorporating recent findings vs. maintaining consistency with established knowledge

- Failure signatures: Low F1-scores in NER/RE tasks; inconsistent or incorrect triples in KG; failure of semantic reasoner to infer new knowledge or validate existing facts

- First 3 experiments: 1. Evaluate BioBERT/SciBERT performance on held-out test set and compare with traditional methods 2. Manually inspect sample triples extracted from texts and integrated into KG for accuracy 3. Use semantic reasoner to infer new knowledge from KG subset and validate against known domain knowledge

## Open Questions the Paper Calls Out

- Question: How effective are LLMs in identifying inconsistencies and incompleteness in knowledge graphs compared to domain experts?
  - Basis in paper: The paper discusses using LLMs to identify inconsistencies or incompleteness in the knowledge graph by generating new triples from recent articles
  - Why unresolved: The paper does not provide a quantitative comparison of LLM-based validation against domain expert validation
  - What evidence would resolve it: A study comparing the accuracy of LLM-based knowledge graph validation against validation by domain experts with precision, recall, and F1-score metrics

- Question: What is the impact of concept drift on the performance of AI systems in cancer diagnosis and treatment recommendations?
  - Basis in paper: The paper mentions that without employing up-to-date findings, there is a high chance an AI system exhibits concept drift while providing diagnosis and treatment
  - Why unresolved: The paper does not quantify the impact of concept drift on AI system performance
  - What evidence would resolve it: A study measuring AI system performance in cancer diagnosis and treatment before and after incorporating up-to-date domain knowledge with accuracy, sensitivity, and specificity metrics

- Question: How can the expressiveness of Description Logic (DL) be tested against incompleteness and inconsistency in knowledge graphs?
  - Basis in paper: The paper mentions that the expressiveness of DL is not tested against incompleteness and inconsistency
  - Why unresolved: The paper does not provide a method to test DL expressiveness against incompleteness and inconsistency
  - What evidence would resolve it: A study proposing a method to test DL expressiveness against incompleteness and inconsistency in knowledge graphs with completeness, consistency, and expressiveness metrics

## Limitations

- The validation methodology for knowledge graph consistency checking using LLMs remains underspecified, lacking detailed documentation of prompt templates and validation criteria
- The approach's scalability and generalizability across different cancer types and biomedical domains is uncertain
- The ontology construction process and its comprehensive coverage of the cancer domain are not fully detailed

## Confidence

- **Medium**: Strong performance metrics for BioBERT (91.36% F1-score) but underspecified LLM validation methodology
- **Low**: Uncertain scalability and generalizability across cancer types and biomedical domains
- **Medium**: Incomplete documentation of ontology construction process and coverage verification

## Next Checks

1. Apply BioBERT/SciBERT models to a different biomedical domain (e.g., cardiovascular diseases) and compare performance metrics to establish generalizability

2. Conduct human expert review of 100 randomly sampled LLM-generated triples to assess accuracy and relevance before knowledge graph integration

3. Perform systematic gap analysis of the OncoNet Ontology against a comprehensive cancer biomarker database to identify missing relationships and concepts