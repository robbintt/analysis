---
ver: rpa2
title: Orthogonal Transforms in Neural Networks Amount to Effective Regularization
arxiv_id: '2305.06344'
source_url: https://arxiv.org/abs/2305.06344
tags:
- which
- networks
- neural
- fsnn
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a novel neural network architecture, the Frequency-Supported
  Neural Network (FSNN), designed specifically for nonlinear dynamical system identification.
  The core idea is to incorporate frequency information into the network structure
  using Fourier transforms, while retaining the universal approximation property of
  standard feedforward networks.
---

# Orthogonal Transforms in Neural Networks Amount to Effective Regularization

## Quick Facts
- arXiv ID: 2305.06344
- Source URL: https://arxiv.org/abs/2305.06344
- Reference count: 10
- One-line primary result: The paper introduces the Frequency-Supported Neural Network (FSNN), which incorporates frequency information via Fourier transforms and outperforms standard MLPs on dynamical system identification tasks.

## Executive Summary
This paper introduces the Frequency-Supported Neural Network (FSNN), a novel neural network architecture designed for nonlinear dynamical system identification. The core innovation is the incorporation of frequency information into the network structure using Fourier transforms, while maintaining the universal approximation property of standard feedforward networks. The FSNN consists of parallel time and frequency branches, with the frequency branch processing the signal in the frequency domain using learned complex-valued parameters. The authors prove that both branches are universal approximators and demonstrate through numerical experiments that FSNN outperforms standard MLP models, especially on problems with frequency-based input signals.

## Method Summary
The FSNN architecture processes input sequences through two parallel branches: a time branch that performs standard linear transformations, and a frequency branch that applies a Fourier transform, processes the signal with learned complex-valued weights, then applies the inverse Fourier transform. The outputs of both branches are summed and passed through a non-linear activation function. The model is trained using stochastic gradient descent with the Adam optimizer and mean-squared-error loss. Hyperparameter search is conducted to optimize performance on benchmarks.

## Key Results
- FSNN achieves a normalized RMSE of 4.31% on the Silverbox benchmark, compared to 7.32% for a standard MLP, with fewer parameters (69,719 vs 37,313).
- On a toy problem with a static system and frequency-based input, FSNN outperforms a standard MLP.
- The frequency branch can be viewed as an initialization scheme for feedforward networks, effectively regularizing the learning dynamics.

## Why This Works (Mechanism)

### Mechanism 1
The frequency branch acts as a learned orthogonal transform initialization, effectively regularizing the learning dynamics. The frequency branch applies a discrete Fourier transform (DFT) to the input, processes it with learned complex-valued weights, then applies the inverse DFT. This is equivalent to a universal approximator with initial weights derived from the DFT matrix properties. The core assumption is that the DFT matrix is unitary, enabling the frequency branch to be rewritten in the form of a standard feedforward network with equivalent universal approximation properties.

### Mechanism 2
Parallel time and frequency branches allow the network to exploit both time-domain and frequency-domain information simultaneously. The FSNN block processes the input through two parallel paths—one in the time domain (standard linear transformation) and one in the frequency domain (Fourier transform, learned transformation, inverse Fourier transform). Their outputs are summed and passed through a non-linear activation. The core assumption is that dynamical systems with frequency-based input signals contain information that can be better captured in the frequency domain.

### Mechanism 3
The FSNN architecture retains universal approximation properties while incorporating domain-specific inductive biases. Both the time branch and the frequency branch are shown to be universal approximators individually. The parallel structure allows the network to combine these approximations, maintaining the ability to approximate any function while leveraging frequency information. The core assumption is that the universal approximation property of the time branch is straightforward, and the frequency branch can be shown equivalent to a universal approximator using the DFT matrix properties.

## Foundational Learning

- Concept: Fourier Transform
  - Why needed here: The FSNN architecture relies on transforming signals into the frequency domain to apply learned transformations, then transforming back.
  - Quick check question: What is the relationship between the DFT matrix and the unitary property that allows the frequency branch to be a universal approximator?

- Concept: Universal Approximation Theorem
  - Why needed here: The paper claims both the time and frequency branches are universal approximators, which is crucial for the FSNN's theoretical foundation.
  - Quick check question: How does the frequency branch's structure, when rewritten using the DFT matrix, demonstrate equivalence to a standard feedforward network?

- Concept: Orthogonal Transforms and Regularization
  - Why needed here: The paper posits that using orthogonal transforms like the Fourier transform in the network architecture implies regularization during training.
  - Quick check question: How does the use of an orthogonal transform in the network structure affect the learning rate of individual parameters?

## Architecture Onboarding

- Component map:
  - Input sequence (time domain)
  - Time branch: Linear transformation (W_l, b_l) → Output in time domain
  - Frequency branch: Fourier transform → Linear transformation with complex weights (W_f, b_f) → Inverse Fourier transform → Output in time domain
  - Sum of time and frequency branch outputs
  - Non-linear activation function (e.g., GeLU)
  - Output sequence

- Critical path:
  1. Input sequence preparation (windowing, formatting)
  2. Forward pass through FSNN block (time and frequency branches)
  3. Aggregation and activation
  4. Loss computation (e.g., MSE)
  5. Backward pass and parameter updates

- Design tradeoffs:
  - Complexity vs. performance: Adding the frequency branch increases model complexity but can improve performance on frequency-based signals.
  - Input sequence length: Longer input sequences provide more frequency information but increase computational cost.
  - Number of parameters: FSNN can achieve good performance with fewer parameters compared to standard MLPs, but very large models may overfit.

- Failure signatures:
  - Poor performance on non-stationary signals where frequency information is less relevant.
  - Increased training time due to the additional complexity of the frequency branch.
  - Sensitivity to hyperparameters, particularly input sequence length and number of layers.

- First 3 experiments:
  1. Implement a basic FSNN block and verify its forward pass with a simple sine wave input.
  2. Train an FSNN on a toy problem with a static system and frequency-based input to confirm improved performance over a standard MLP.
  3. Apply the FSNN to a simple dynamical system benchmark (e.g., Wiener-Hammerstein) and compare results with a standard MLP.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of orthogonal transform (beyond Fourier) affect the performance and generalization of neural networks in nonlinear dynamical system identification?
- Basis in paper: The authors suggest that adding orthogonal transforms to neural networks is a potentially interesting area of research, and mention that for example in audio processing the frequency domain is potentially useful for trainable models.
- Why unresolved: The paper only experiments with the Fourier transform as the orthogonal transform. Other orthogonal transforms like wavelet transforms or Hadamard transforms are not explored.
- What evidence would resolve it: Systematic experiments comparing the performance of FSNN using different orthogonal transforms on various benchmarks and dynamical system identification tasks.

### Open Question 2
- Question: What is the optimal way to incorporate orthogonal transforms into neural network architectures for nonlinear dynamical system identification, considering both theoretical properties and practical performance?
- Basis in paper: The authors show that FSNN is a universal approximator and that using Fourier transform implies regularization during training by adjusting the learning rate of each parameter individually. However, they do not explore other ways to incorporate orthogonal transforms.
- Why unresolved: The paper only explores one specific way to incorporate the Fourier transform (parallel time and frequency branches). Other architectures or methods of incorporating orthogonal transforms are not investigated.
- What evidence would resolve it: Development and experimental comparison of different architectures that incorporate orthogonal transforms in various ways, evaluating both theoretical properties (e.g. universal approximation) and practical performance on benchmarks.

### Open Question 3
- Question: How does the performance of FSNN scale with the complexity of the dynamical system being identified, and are there limits to its applicability?
- Basis in paper: The authors demonstrate that FSNN performs well on two benchmarks (Wiener-Hammerstein and Silverbox) compared to standard MLP models. However, they do not explore how FSNN performs on more complex systems or whether there are limits to its applicability.
- Why unresolved: The paper only experiments with two relatively simple benchmarks. It is unclear how FSNN would perform on more complex dynamical systems or whether there are limitations to its use.
- What evidence would resolve it: Extensive experiments evaluating FSNN on a wide range of dynamical system identification benchmarks, including increasingly complex systems, to determine the limits of its applicability and scalability.

## Limitations
- The paper's claims about universal approximation properties rely heavily on theoretical derivations without extensive empirical validation.
- The comparison with standard MLPs uses relatively few benchmarks and does not explore the sensitivity of FSNN to hyperparameters.
- The claim that orthogonal transforms imply regularization is conceptually supported but lacks rigorous mathematical proof or ablation studies.

## Confidence
- **High confidence**: The FSNN architecture is correctly implemented and achieves lower RMSE on the tested benchmarks compared to standard MLPs.
- **Medium confidence**: The theoretical claims about universal approximation properties of both branches are valid, but the equivalence to standard feedforward networks for the frequency branch needs more rigorous proof.
- **Low confidence**: The assertion that orthogonal transforms in neural networks amount to effective regularization is conceptually plausible but lacks direct empirical or mathematical evidence.

## Next Checks
1. Conduct ablation studies to isolate the regularization effect of the orthogonal transform by comparing FSNN performance with and without the frequency branch on noisy datasets.
2. Perform sensitivity analysis on FSNN hyperparameters (input sequence length, number of layers) to determine the robustness of performance gains across different configurations.
3. Test FSNN on a broader range of dynamical systems, including those with non-stationary signals, to evaluate the generalizability of the architecture beyond frequency-based inputs.