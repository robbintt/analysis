---
ver: rpa2
title: 'PDiT: Interleaving Perception and Decision-making Transformers for Deep Reinforcement
  Learning'
arxiv_id: '2312.15863'
source_url: https://arxiv.org/abs/2312.15863
tags:
- pdit
- transformer
- learning
- observation
- perceiving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new architecture called PDiT (Perception and
  Decision-making Interleaving Transformer) for deep reinforcement learning. The key
  idea is to interleave two Transformers - one for perception and one for decision-making
  - to process the observations and generate actions.
---

# PDiT: Interleaving Perception and Decision-making Transformers for Deep Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2312.15863
- **Source URL**: https://arxiv.org/abs/2312.15863
- **Reference count**: 40
- **Primary result**: PDiT achieves superior performance compared to strong baselines across various deep RL settings including online/offline RL, environments with image/proprioception/hybrid observations, and multi-task RL

## Executive Summary
This paper introduces PDiT, a novel architecture for deep reinforcement learning that interleaves perception and decision-making Transformers. The key innovation is alternating perceiving and deciding blocks that allow immediate information exchange between modules, addressing the limitation of naive stacking approaches. The architecture demonstrates strong performance across multiple RL settings including online/offline RL, different observation modalities (image, proprioception, hybrid), and multi-task scenarios, while also providing explainable representations through feature visualization.

## Method Summary
PDiT processes multi-modal observations by first encoding them into patch embeddings, then passing them through L interleaved perceiving and deciding Transformer blocks. Each PDiT block contains a perceiving Transformer (self-attention on observation patches) followed by a deciding Transformer (causal self-attention on return-action-observation sequences). Outputs from all deciding blocks are concatenated via dense connections before a final feed-forward network generates actions. The architecture is trained using different RL algorithms depending on the setting: PPO for online RL (Atari), CQL for offline RL (MuJoCo), and RvS for multi-task RL (BabyAI).

## Key Results
- PDiT achieves superior performance compared to strong baselines (NatureCNN, ResNet, Catformer, CoBERL, DT, GATO) across multiple deep RL settings
- The architecture performs well in diverse observation modalities including image-only (Atari), proprioception-only (MuJoCo), and hybrid image-language (BabyAI) environments
- Feature visualization shows PDiT can extract explainable representations that highlight task-relevant regions of the input

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interleaving perception and decision-making Transformers improves information flow compared to naive stacking
- Mechanism: By placing perceiving and deciding blocks in an alternating sequence, information from the perceptual layer can be immediately consumed by the decision layer within the same block, preventing the decision layer from processing outdated or overly abstract representations
- Core assumption: The decision-making process benefits more from fresh perceptual information than from highly abstracted intermediate representations
- Evidence anchors: [abstract] "such a network design is generally applicable to a lot of deep RL settings"; [section 3.2.3] "Possible reasons are as follows. First, the outer is to make decisions. Once it receives the output from the perceiving, it should directly take actions instead of further abstractly embedding the representations by stacking multiple deciding blocks."
- Break condition: If experiments show no improvement over naive stacking, the interleaving assumption may be invalid

### Mechanism 2
- Claim: Specialized Transformers for perception and decision-making outperform fused single-Transformer approaches
- Mechanism: Separate Transformers allow each module to focus on its specific task (perception vs. decision-making) while maintaining specialized attention patterns, rather than forcing a single Transformer to handle both tasks simultaneously
- Core assumption: Task specialization leads to better performance than task fusion in Transformers
- Evidence anchors: [abstract] "delegating perception and decision-making into two Transformers, leveraging Transformer's power of dealing with multi-modal data and making sequential decisions"; [section 2] "PDiT combines the advantages of the existing methods by dividing environmental perception and decision-making into two Transformers"
- Break condition: If specialized modules show no performance gain over fused approaches in controlled experiments

### Mechanism 3
- Claim: Dense connections between PDiT blocks improve information flow and gradient propagation
- Mechanism: Concatenating outputs from all PDiT blocks before the final decision layer ensures that information from all hierarchical levels contributes to the final action, preventing information loss in deeper layers
- Core assumption: Multi-level feature concatenation provides richer information for decision-making than single-level features
- Evidence anchors: [section 3.2.2(4)] "We found that the dense connection is empirically helpful for performance in the experiments"; [section 4.3] "Besides, 'w/o Dense' has the second worst performance: this result verifies that the dense connection design is important"
- Break condition: If ablation experiments show no performance difference with/without dense connections

## Foundational Learning

- **Concept**: Transformer architecture and self-attention mechanism
  - Why needed here: PDiT is built entirely from Transformer blocks, so understanding how self-attention works is fundamental to understanding the model
  - Quick check question: How does the multi-head self-attention mechanism in a Transformer block differ from standard attention?

- **Concept**: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The paper explicitly states the setting is POMDP where agents only observe partial state information, which motivates the design choices
  - Quick check question: Why does the deciding Transformer use causal masks while the perceiving Transformer does not?

- **Concept**: Reinforcement Learning paradigms (online vs. offline, on-policy vs. off-policy)
  - Why needed here: PDiT is evaluated across multiple RL settings (PPO, CQL, RvS), and understanding these paradigms is crucial for interpreting the results
  - Quick check question: What is the key difference between how PPO and CQL update their policies?

## Architecture Onboarding

- **Component map**: Input processing layer → Perceiving blocks → Deciding blocks → Dense connection → Output layer
- **Critical path**: Observation → Perceiving blocks → Deciding blocks → Dense connection → Action
- **Design tradeoffs**:
  - L perceiving blocks vs. L deciding blocks: Balancing computational cost with representation capacity
  - Patch size vs. context length: Smaller patches increase context length but may lose spatial coherence
  - Integration token vs. direct patch processing: Tradeoff between global representation and local detail
- **Failure signatures**:
  - Gradient vanishing in deeper layers (add residual connections or reduce depth)
  - Attention collapse to single patches (adjust temperature or regularization)
  - Mode collapse in multi-task settings (add task-specific conditioning)
- **First 3 experiments**:
  1. Compare single-Transformer (DT) vs. PDiT on MuJoCo Medium dataset to verify specialization claim
  2. Compare Vanilla-PDiT (naive stacking) vs. full PDiT on Atari Breakout to verify interleaving benefit
  3. Test different values of L (1, 2, 3) to find optimal depth vs. performance tradeoff

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- Limited ablation studies to fully validate why interleaving outperforms naive stacking
- Experimental evaluation does not systematically test the break conditions for each proposed mechanism
- The paper only reports results for a fixed number of PDiT blocks (L=4) without analyzing scalability

## Confidence

**Confidence labels:**
- Mechanism 1 (interleaving improves information flow): **Medium** - Supported by qualitative reasoning but lacking controlled ablation experiments
- Mechanism 2 (specialized Transformers outperform fused): **Low-Medium** - Theoretical justification exists but comparative experiments are limited
- Mechanism 3 (dense connections help): **Medium** - Empirical evidence provided but mechanism not deeply analyzed

## Next Checks

1. **Controlled stacking comparison**: Implement and compare PDiT against a version with naively stacked (non-interleaved) perception and decision Transformers to isolate the interleaving effect

2. **Attention pattern analysis**: Visualize and compare attention distributions in interleaved vs. stacked architectures to verify the claimed information flow benefits

3. **Gradient flow verification**: Measure gradient magnitudes and vanishing patterns through PDiT blocks to confirm dense connections effectively prevent gradient degradation