---
ver: rpa2
title: Evaluation of Faithfulness Using the Longest Supported Subsequence
arxiv_id: '2308.12157'
source_url: https://arxiv.org/abs/2308.12157
tags:
- claim
- reference
- dataset
- supported
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to evaluate the faithfulness
  of machine-generated text by computing the longest noncontinuous substring of the
  claim that is supported by the context, referred to as the Longest Supported Subsequence
  (LSS). Using a newly collected human-annotated dataset, a model is fine-tuned to
  generate LSS.
---

# Evaluation of Faithfulness Using the Longest Supported Subsequence

## Quick Facts
- arXiv ID: 2308.12157
- Source URL: https://arxiv.org/abs/2308.12157
- Reference count: 31
- This paper introduces a novel approach to evaluate the faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context, referred to as the Longest Supported Subsequence (LSS). Using a newly collected human-annotated dataset, a model is fine-tuned to generate LSS. The proposed method demonstrates an 18% improvement in correlation with human ratings compared to the previous state-of-the-art metric for faithfulness on the dataset. Additionally, the approach consistently outperforms other metrics on a summarization dataset across six different models and is used to compare the faithfulness of multiple large language models. The human-annotated dataset and the fine-tuned model for generating LSS are released to support future research in measuring faithfulness and analyzing hallucinations.

## Executive Summary
This paper presents a novel approach to evaluate the faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context, referred to as the Longest Supported Subsequence (LSS). The authors collected a human-annotated dataset of LSS examples and fine-tuned a T5 model to generate LSS from reference-claim pairs. The proposed LSS-based evaluation method demonstrates an 18% improvement in correlation with human ratings compared to the previous state-of-the-art metric for faithfulness. The approach consistently outperforms other metrics on a summarization dataset across six different models and is used to compare the faithfulness of multiple large language models.

## Method Summary
The authors define the Longest Supported Subsequence (LSS) as the longest subsequence of words from the claim that is supported by the reference. They collected a human-annotated dataset of 15k LSS examples using reference-claim pairs from the W AFER dataset. A T5.1.1.lm100k.xl model was fine-tuned on this dataset to generate LSS from reference-claim pairs. The fine-tuned model's performance was evaluated using ROUGE, BLEU, and word-level metrics, and compared with baselines (ChatGPT, Flan T5). The LSS-based evaluation method was then applied to various summarization datasets and large language models to assess faithfulness.

## Key Results
- LSS-based evaluation method shows 18% improvement in correlation with human ratings compared to previous state-of-the-art metric
- LSS-based method consistently outperforms other metrics on summarization dataset across six different models
- Fine-tuned T5 model generates LSS with better performance than baselines (ChatGPT, Flan T5)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Computing the longest supported subsequence (LSS) of a claim improves faithfulness evaluation by isolating only the supported text from the claim.
- Mechanism: By comparing the claim to the reference and extracting the longest non-continuous subsequence of words that are supported, the metric focuses on factual overlap rather than superficial surface similarity.
- Core assumption: The claim can be decomposed into supported and unsupported parts, and the supported part is sufficient to judge faithfulness.
- Evidence anchors:
  - [abstract] "We introduce a novel approach to evaluate faithfulness of machine-generated text by computing the longest noncontinuous substring of the claim that is supported by the context"
  - [section 3] "We define the Longest Supported Subsequence (LSS), which is the longest subsequence of words from the claim that is supported by the reference"
  - [corpus] Weak; no explicit citations in corpus evidence supporting this mechanism.
- Break condition: If the reference contains ambiguous or conflicting information, the LSS extraction may incorrectly include or exclude parts of the claim.

### Mechanism 2
- Claim: Using LSS instead of the full claim in evaluation metrics increases correlation with human judgments.
- Mechanism: Metrics like BLEU and ROUGE traditionally compare reference and claim directly, which can penalize semantically correct but paraphrased content. LSS ensures only verifiable parts are compared.
- Core assumption: Human evaluators judge faithfulness based on whether the claim is supported by the reference, not on surface form.
- Evidence anchors:
  - [abstract] "We demonstrate that these metrics correlate better with human ratings when LSS is employed"
  - [section 6.1] "Using our approach, the correlation reaches 0.48 and 0.49 using LSS and LSS*, respectively. Correlation with human judgement using LSS is significantly higher than using the reference instead"
  - [corpus] No direct evidence in corpus; inference based on reported results.
- Break condition: If the LSS generation model fails to accurately extract supported text, the evaluation correlation will degrade.

### Mechanism 3
- Claim: Fine-tuning a language model on a human-annotated LSS dataset produces accurate LSS generations.
- Mechanism: A transformer-based model (T5 XL) is fine-tuned to map reference-claim pairs to their corresponding LSS, learning patterns of supported text.
- Core assumption: Human annotations of LSS provide high-quality training signals that enable the model to generalize to unseen reference-claim pairs.
- Evidence anchors:
  - [section 5.1] "We used the 3 billion-parameter pre-trained language model, t5.1.1.lm100k.xl... The learning objective, given a reference and claim as input, is to generate the LSS/LSS* as output"
  - [section 5.2] "Our fine-tuned model’s performance surpasses that of the baselines"
  - [corpus] No explicit corpus evidence; based on internal experiments described in paper.
- Break condition: If the training data is biased or insufficiently diverse, the model may generate incorrect LSS on out-of-domain inputs.

## Foundational Learning

- Concept: Faithfulness in text generation
  - Why needed here: Core problem being addressed—ensuring generated text is grounded in source context.
  - Quick check question: What is the difference between faithfulness and relevance in summarization evaluation?

- Concept: Sequence alignment and subsequence extraction
  - Why needed here: LSS is defined as a non-continuous subsequence, requiring understanding of how to extract maximal supported spans.
  - Quick check question: How does the longest common subsequence problem differ from longest supported subsequence?

- Concept: Evaluation metric correlation with human judgment
  - Why needed here: The paper’s central claim is that LSS-based metrics better correlate with human faithfulness ratings.
  - Quick check question: Why might BLEU or ROUGE show negative correlation with human faithfulness judgments?

## Architecture Onboarding

- Component map: Data collection pipeline -> LSS annotation -> T5 fine-tuning -> LSS generation -> Evaluation metric computation -> Correlation analysis
- Critical path: LSS generation must be fast and accurate enough for real-time evaluation; model inference time and precision are bottlenecks
- Design tradeoffs: Balancing LSS dataset size vs. annotation quality; choosing between LSS and LSS* (grammatically correct) versions
- Failure signatures: Low correlation with human ratings; LSS generation that is too short or too long compared to ground truth
- First 3 experiments:
  1. Evaluate correlation of reference-claim metrics (ROUGE, BLEU) vs human ratings on LSS dataset
  2. Generate LSS with fine-tuned model and recompute metrics vs human ratings
  3. Test model on summarization task (XSum) and compare correlation gains over baseline metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific modifications to the training dataset would lead to the most significant improvements in LSS generation accuracy?
- Basis in paper: [inferred] The paper notes that the model performance is better than baselines but still has a large gap between generated and human-written LSS, and that further improvements in LSS generation would lead to considerable improvements in correlations with human judgment.
- Why unresolved: The paper does not explore different dataset modifications or augmentation techniques to improve LSS generation.
- What evidence would resolve it: Experiments comparing LSS generation accuracy across different training dataset compositions, sizes, and augmentation strategies.

### Open Question 2
- Question: How would the LSS-based faithfulness evaluation perform on languages other than English?
- Basis in paper: [inferred] The paper mentions that current metrics struggle with languages other than English and that their approach builds upon existing metrics, potentially inheriting their limitations.
- Why unresolved: The paper only evaluates on English datasets and does not test cross-linguistic applicability.
- What evidence would resolve it: Evaluations of LSS-based faithfulness metrics on summarization datasets in multiple languages, comparing correlation with human judgments across languages.

### Open Question 3
- Question: What is the optimal way to handle negation cases in the LSS generation and evaluation process?
- Basis in paper: [explicit] The paper explicitly mentions that current metrics struggle with negation cases and that their model exhibits similar performance issues.
- Why unresolved: The paper does not propose or test specific solutions for handling negation in LSS generation.
- What evidence would resolve it: Comparative experiments testing different approaches to negation handling (e.g., specialized training data, modified evaluation metrics) and their impact on faithfulness correlation with human judgments.

## Limitations

- The T5-based LSS generation model achieves good results on the curated dataset, but its performance may degrade on out-of-domain or more complex reference-claim pairs.
- While the correlation improvements are statistically significant, the absolute correlation values (0.48-0.49) remain moderate, suggesting room for improvement.
- The paper's claims about LSS-based evaluation rely heavily on the quality of human annotations, but the inter-annotator agreement and annotation guidelines are not fully specified.

## Confidence

- High confidence: The mechanism of using LSS to isolate supported text from claims is clearly defined and technically sound
- Medium confidence: The reported correlation improvements with human judgments are well-supported by experimental results, though absolute values are modest
- Medium confidence: The fine-tuned model's ability to generate LSS is demonstrated on the dataset, but generalization to other domains is uncertain

## Next Checks

1. Conduct ablation studies to determine how much of the correlation improvement comes from LSS extraction versus the choice of evaluation metrics
2. Test the LSS generation model on out-of-domain datasets to assess generalization and identify failure patterns
3. Analyze cases where human annotators disagree on LSS to understand the limits of the approach and potential annotation ambiguities