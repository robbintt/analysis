---
ver: rpa2
title: 'SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation'
arxiv_id: '2310.07698'
source_url: https://arxiv.org/abs/2310.07698
tags:
- concepts
- concept
- data
- black-box
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a novel method to discover interpretable concepts
  from data and use them to explain black-box classifiers. SurroCBM identifies shared
  and unique concepts across multiple classifiers and employs a concept-based surrogate
  model for post-hoc explanations.
---

# SurroCBM: Concept Bottleneck Surrogate Models for Generative Post-hoc Explanation

## Quick Facts
- arXiv ID: 2310.07698
- Source URL: https://arxiv.org/abs/2310.07698
- Authors: 
- Reference count: 4
- Key outcome: SurroCBM discovers interpretable concepts from data and uses them to explain black-box classifiers, achieving high fidelity while maintaining transparency through soft decision trees

## Executive Summary
This paper presents SurroCBM, a novel framework for discovering interpretable concepts from data and using them to explain black-box classifiers. The method identifies shared and unique concepts across multiple classifiers and employs a concept-based surrogate model for post-hoc explanations. An effective training strategy using self-generated data is proposed to enhance explanation quality continuously. Experiments on MNIST and TripleMNIST datasets demonstrate the efficacy of SurroCBM in concept discovery and explanation, achieving high fidelity to mimic black-box model behaviors while maintaining transparency through soft decision trees.

## Method Summary
SurroCBM uses a surrogate model architecture with a concept extractor that maps input data to concept values, and an explainable mapping implemented as a soft decision tree with an explanation mask. The framework includes a decoder to reconstruct data from concept values, ensuring identifiability. A novel training strategy generates additional training data by sampling related concepts and perturbing unrelated ones, allowing the model to continuously improve fidelity to the black-box model. The explanation mask identifies shared and unique concepts across multiple classifiers, enabling compositional generalization to new tasks.

## Key Results
- SurroCBM successfully discovers interpretable concepts related to each task and provides local explanations by extracting concept values and visualizing decision rules
- The method achieves high fidelity in mimicking black-box model behaviors while maintaining transparency through soft decision trees
- SurroCBM identifies shared and unique concepts across various black-box models, enabling compositional generalization to new tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The surrogate model architecture with concept extractor and explainable mapping ensures fidelity and identifiability.
- Mechanism: The concept extractor eϕ maps data to concept values z, which are decoded back to data via gθ to ensure identifiability. The mapping h, implemented as a soft decision tree with explanation mask m, ensures explainability while maintaining fidelity to the black-box model output.
- Core assumption: The concept values z can fully represent the original data x for reconstruction, and the decision tree can adequately approximate the black-box model's decision boundaries.
- Evidence anchors:
  - [abstract] states the framework uses "concept extractor eϕ to map the data to concept values z, and an explainable mapping h to map the concept values z to the model output"
  - [section] describes "To ensure identifiability, we add an additional decoder gθ to map the concept values z back to the data x and minimize their difference"
  - [corpus] provides no direct evidence for this mechanism, but mentions concept bottleneck models as related work
- Break condition: If the concept values cannot reconstruct the data accurately or the decision tree fails to approximate the black-box model, the mechanism breaks.

### Mechanism 2
- Claim: The training strategy using self-generated data improves the fidelity of the surrogate model over time.
- Mechanism: The model generates additional training data by sampling related concepts zR and perturbing unrelated concepts zU, then trains on this generated data to continuously improve fidelity to the black-box model.
- Core assumption: The generated data covers the input space adequately and the concept values sampled capture the relevant variations.
- Evidence anchors:
  - [abstract] mentions "An effective training strategy using self-generated data is proposed to enhance explanation quality continuously"
  - [section] describes the training strategy in detail: "we train the model with additional data that is generated by the model itself"
  - [corpus] has no direct evidence but mentions concept-based explanations as related work
- Break condition: If the generated data distribution is significantly different from real data or fails to cover important regions of the input space, fidelity improvement will be limited.

### Mechanism 3
- Claim: The explanation mask m identifies shared and unique concepts across multiple classifiers, enabling compositional generalization.
- Mechanism: The explanation mask m, learned during training, selects relevant concepts for each task. This allows the model to explain new tasks by only training new parts of the mask and estimators while freezing existing weights.
- Core assumption: The concept space discovered is sufficiently general to cover new tasks, and the mask can effectively identify relevant concepts for unseen tasks.
- Evidence anchors:
  - [abstract] states "SurroCBM identifies shared and unique concepts across various black-box models"
  - [section] describes "This mechanism is implemented with a trainable binary mask m ∈ {0, 1}^kz×ky, named explanation mask"
  - [corpus] has no direct evidence but mentions compositional generalization in related work
- Break condition: If the concept space is too task-specific or the mask cannot identify relevant concepts for new tasks, compositional generalization will fail.

## Foundational Learning

- Variational Autoencoders (VAEs):
  - Why needed here: The identifiability loss uses the VAE framework to ensure data can be reconstructed from concept values, making concepts interpretable.
  - Quick check question: How does the VAE framework help ensure that the discovered concepts are meaningful and can reconstruct the original data?

- Concept Activation Vectors (CAVs):
  - Why needed here: CAVs provide a way to quantify the importance of human-defined concepts to model predictions, which this work aims to automate by discovering concepts instead.
  - Quick check question: What is the main limitation of CAVs that this work addresses by discovering concepts automatically?

- Decision Trees:
  - Why needed here: Soft decision trees provide transparent, rule-based explanations of how concept values map to model outputs, enhancing explainability.
  - Quick check question: Why are soft decision trees particularly suitable for providing explanations in this framework?

## Architecture Onboarding

- Component map:
  - Input data → Concept extractor → Concept values
  - Concept values → Explanation mask → Relevant concepts
  - Relevant concepts → Soft decision tree → Model output
  - Concept values → Decoder → Reconstructed data (for identifiability)

- Critical path:
  1. Input data → Concept extractor → Concept values
  2. Concept values → Explanation mask → Relevant concepts
  3. Relevant concepts → Soft decision tree → Model output
  4. Concept values → Decoder → Reconstructed data (for identifiability)

- Design tradeoffs:
  - Number of concepts vs. explainability: More concepts may capture more nuances but reduce transparency
  - Fidelity vs. simplicity: Higher fidelity may require more complex models, reducing explainability
  - Training time vs. continuous improvement: More iterations of self-generated data improve fidelity but increase training time

- Failure signatures:
  - Low reconstruction accuracy indicates poor identifiability
  - Large gap between black-box and surrogate model outputs indicates low fidelity
  - Complex decision trees with many nodes indicate poor explainability

- First 3 experiments:
  1. Train on MNIST with simple binary classification tasks to verify basic functionality
  2. Test compositional generalization by explaining new tasks with frozen weights
  3. Evaluate fidelity improvement with and without self-generated data training strategy

## Open Questions the Paper Calls Out
- How does the SurroCBM model handle the scalability and performance when dealing with high-dimensional data or complex black-box models?
- What are the potential limitations of using soft decision trees as the mapping from concepts to predictions in terms of interpretability and explainability?
- How does the SurroCBM model handle the discovery of concepts in data with multiple modalities, such as text and images?

## Limitations
- The implementation details of the concept extractor and decision tree components are not fully specified, making direct reproduction challenging.
- The paper lacks ablation studies to isolate the contribution of the self-generated data training strategy versus other components.
- The evaluation is limited to MNIST-like datasets, raising questions about scalability to more complex, real-world data.

## Confidence
- **High Confidence**: The core mechanism of using concept bottleneck models with soft decision trees for explainability is well-established in the literature and the paper provides clear evidence of this approach.
- **Medium Confidence**: The training strategy using self-generated data shows promise in improving fidelity, but the extent of its contribution versus other factors is unclear without ablation studies.
- **Low Confidence**: The claim of compositional generalization across multiple black-box models is demonstrated on limited tasks and may not generalize to more diverse or complex model architectures.

## Next Checks
1. Conduct an ablation study to isolate the impact of the self-generated data training strategy on fidelity improvement compared to standard training methods.
2. Evaluate the approach on more complex datasets (e.g., CIFAR-10, ImageNet) to assess scalability and robustness beyond MNIST-like tasks.
3. Test the framework's ability to explain a wider variety of black-box model architectures (e.g., CNNs, transformers) to validate compositional generalization claims.