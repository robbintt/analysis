---
ver: rpa2
title: Biomedical Knowledge Graph Embeddings with Negative Statements
arxiv_id: '2308.03447'
source_url: https://arxiv.org/abs/2308.03447
tags:
- negative
- statements
- knowledge
- graph
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces TrueWalks, the first knowledge graph embedding
  method to explicitly incorporate negative statements. Unlike previous approaches
  that rely on negative sampling, TrueWalks generates random walks that distinguish
  between positive and negative statements, and leverages ontological inheritance
  rules to produce more accurate entity representations.
---

# Biomedical Knowledge Graph Embeddings with Negative Statements

## Quick Facts
- **arXiv ID**: 2308.03447
- **Source URL**: https://arxiv.org/abs/2308.03447
- **Reference count**: 40
- **Primary result**: TrueWalks achieves F-measures of 0.846 for PPI and 0.661 for GDA prediction by incorporating negative statements

## Executive Summary
This paper introduces TrueWalks, the first knowledge graph embedding method that explicitly incorporates negative statements through specialized random walk generation. Unlike previous approaches that rely on negative sampling, TrueWalks generates walks that distinguish between positive and negative statements and leverages ontological inheritance rules. The method is evaluated on protein-protein interaction and gene-disease association prediction tasks, demonstrating significant performance improvements over state-of-the-art methods by using both positive and negative statements.

## Method Summary
TrueWalks transforms OWL ontologies into RDF graphs and generates random walks using depth-first search with statement polarity awareness. For positive statements, it traverses from subclass to superclass, while negative statements reverse this direction. Two separate neural language models (skip-gram or structured skip-gram) are trained on positive and negative walks respectively, and their embeddings are concatenated for final representation. The method is evaluated on two biomedical tasks: protein-protein interaction and gene-disease association prediction using Random Forest classifiers with Monte Carlo cross-validation.

## Key Results
- Achieves F-measure of 0.846 on protein-protein interaction prediction
- Achieves F-measure of 0.661 on gene-disease association prediction
- Outperforms all baseline methods including TransE, TransH, TransR, ComplEx, DistMult, DeepWalk, node2vec, metapath2vec, OWL2Vec*, and RDF2Vec
- The dual representation approach (positive + negative embeddings) consistently improves precision and reduces false positive rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TrueWalks generates random walks that distinguish between positive and negative statements by using different traversal strategies for subclass edges based on statement polarity.
- Mechanism: The walk generation algorithm uses depth-first search where a positive statement implies traversal from subclass to superclass, while a negative statement reverses this direction, ensuring accurate path generation for negative statements.
- Core assumption: Negative statements have different inheritance semantics than positive statements in OWL ontologies, requiring reversed traversal for accurate representation.
- Evidence anchors:
  - "An essential difference between a positive and a negative statement of this kind is related to the implied inheritance of properties exhibited by the superclasses or subclasses of the assigned class."
  - "TrueWalks generates walks that can distinguish between positive and negative statements and consider the semantic implications of negation in KGs that are rich in ontological information"
- Break condition: If the ontology lacks explicit subclass/superclass axioms, the reversed traversal mechanism becomes meaningless.

### Mechanism 2
- Claim: The dual representation approach (separate positive and negative embeddings) enables machine learning models to better distinguish true positives from false positives.
- Mechanism: By training two separate neural language models on positive and negative walks, TrueWalks creates complementary embeddings that capture both entity attributes and lacking attributes, which are concatenated for final representation.
- Core assumption: Machine learning models can effectively utilize the distinction between positive and negative aspects of entity representations.
- Evidence anchors:
  - "The method is evaluated on two biomedical tasks: protein-protein interaction and gene-disease association prediction. Using both positive and negative statements improves performance over state-of-the-art methods."
  - "The two representations of each entity need to be combined to produce a final representation...we use a simple concatenation of vectors."
  - "This experiment further shows that the added information given by negative statements generally improves the performance of most KG embedding methods."
- Break condition: If the dataset contains very few negative statements, the dual representation provides minimal benefit.

### Mechanism 3
- Claim: The order-aware variant (TrueWalksOA) captures more meaningful semantic relationships by considering the sequence of entities in random walks.
- Mechanism: Instead of using a single matrix for predictions, TrueWalksOA creates multiple matrices dedicated to predicting specific relative positions, making it sensitive to the order of entities in the walks.
- Core assumption: The order of entities in random walks contains semantic information that improves representation quality.
- Evidence anchors:
  - "The second approach is the structured skip-gram model [24], a variation of skip-gram that is sensitive to the order of words, or in our case, entities in the graph walks."
  - "Comparing the two variants of TrueWalks demonstrates that order awareness does not improve performance in most cases."
  - "However, TrueWalksOA improves on precision and F-measure for all other state-of-the-art methods."
- Break condition: If the random walks become too long or too short, the order-aware mechanism loses its effectiveness.

## Foundational Learning

- Concept: Open World Assumption vs Closed World Assumption
  - Why needed here: Understanding why negative statements matter requires grasping that missing facts don't imply falsehood in KGs.
  - Quick check question: What's the fundamental difference between how missing data is interpreted under OWA versus CWA?

- Concept: Description Logic semantics and inheritance
  - Why needed here: The method relies on correctly interpreting subclass relationships in OWL ontologies, especially for negative statements.
  - Quick check question: Why does a negative statement about a subclass not necessarily imply the same negative statement about its superclass?

- Concept: Skip-gram and negative sampling in neural language models
  - Why needed here: TrueWalks uses these techniques to learn entity representations from random walks.
  - Quick check question: How does negative sampling improve the efficiency of skip-gram models compared to full softmax?

## Architecture Onboarding

- Component map:
  RDF Graph Builder -> Random Walk Generator -> Dual Neural Language Models -> Representation Combiner -> Evaluation Pipeline

- Critical path:
  1. Parse ontology → Build RDF graph
  2. Generate walks (positive and negative) → Train neural models
  3. Concatenate embeddings → Use in classification/similarity tasks
  4. Evaluate performance on PPI/GDA prediction

- Design tradeoffs:
  - Using concatenation vs Hadamard product vs L1-norm for combining representations
  - Depth-first vs breadth-first search for walk generation
  - Separate vs joint training of positive and negative embeddings

- Failure signatures:
  - Poor performance when ontologies have minimal subclass axioms
  - Degradation when negative statements are sparse relative to positive ones
  - Memory issues with very large ontologies due to dual embedding storage

- First 3 experiments:
  1. Run TrueWalks on a small ontology with known positive/negative statements and visualize the generated walks to verify correct traversal
  2. Compare classification performance using only positive embeddings vs concatenated positive/negative embeddings on a toy dataset
  3. Test the effect of walk depth on representation quality by varying the maximum depth parameter

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would TrueWalks perform on other biomedical knowledge graph tasks beyond protein-protein interaction and gene-disease association prediction?
- Basis in paper: The paper mentions that TrueWalks is expected to be generalizable to other biomedical applications where negative statements play a decisive role, such as predicting disease-related phenotypes or performing differential diagnosis.
- Why unresolved: The current evaluation only covers two specific tasks (PPI and GDA prediction). The paper suggests potential applications but does not provide empirical evidence for these additional use cases.
- What evidence would resolve it: Empirical evaluation of TrueWalks on diverse biomedical tasks like drug-target interaction prediction, disease phenotype prediction, or differential diagnosis tasks using appropriate benchmark datasets.

### Open Question 2
- Question: How does the performance of TrueWalks scale with increasing knowledge graph size and complexity?
- Basis in paper: The paper uses relatively small benchmark datasets (440 proteins, 755 genes, 162 diseases) and mentions that TrueWalks should handle ontology-rich knowledge graphs, but doesn't explore performance at larger scales.
- Why unresolved: The current evaluation uses modest-sized datasets that may not represent the full complexity and scale of real-world biomedical knowledge graphs. Performance characteristics at scale remain unknown.
- What evidence would resolve it: Systematic evaluation of TrueWalks on progressively larger and more complex biomedical knowledge graphs, measuring computational efficiency and embedding quality as graph size increases.

### Open Question 3
- Question: What is the optimal balance between positive and negative statements for knowledge graph embedding performance?
- Basis in paper: The paper incorporates negative statements but doesn't systematically explore how different ratios of positive to negative statements affect performance, though it mentions that "a balance between positive and negative annotations supports a more reasonable evaluation" in related work.
- Why unresolved: The current implementation uses all available negative statements without investigating whether there's an optimal proportion or whether too many negative statements could be detrimental.
- What evidence would resolve it: Controlled experiments varying the ratio of positive to negative statements while measuring performance impact, potentially revealing an optimal balance for different types of biomedical prediction tasks.

## Limitations
- The RDF graph construction details and blank node handling are not fully specified
- Parameter values for KG embedding methods beyond embedding dimensions are unspecified
- The negative statement generation process from phylogenetic trees lacks detail

## Confidence

- Mechanism 1 (Reversed traversal): Medium - Well-described but dependent on ontology structure
- Mechanism 2 (Dual representations): High - Simple concatenation approach is clearly specified
- Mechanism 3 (Order-aware variant): Low - The paper notes it doesn't improve performance in most cases

## Next Checks
1. Verify RDF graph construction by comparing triple counts and structure against known ontology statistics
2. Implement and test random walk generation on a small ontology to confirm correct handling of positive vs negative statement traversal
3. Conduct ablation study comparing concatenated embeddings against positive-only embeddings on the PPI dataset to validate the dual representation benefit