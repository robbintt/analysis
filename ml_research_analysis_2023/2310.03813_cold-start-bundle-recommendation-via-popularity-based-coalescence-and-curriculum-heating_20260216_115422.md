---
ver: rpa2
title: Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum
  Heating
arxiv_id: '2310.03813'
source_url: https://arxiv.org/abs/2310.03813
tags:
- bundle
- cold-start
- recommendation
- bundles
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoHeat tackles cold-start bundle recommendation by leveraging two
  graph-based views and incorporating popularity-based coalescence and curriculum
  heating. The method addresses the highly skewed distribution of bundle interactions
  by emphasizing affiliation-view representations for less popular bundles, which
  provide richer information than the sparse history-view.
---

# Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating

## Quick Facts
- arXiv ID: 2310.03813
- Source URL: https://arxiv.org/abs/2310.03813
- Reference count: 40
- Primary result: CoHeat achieves up to 193% higher nDCG@20 compared to best competitor in cold-start bundle recommendation

## Executive Summary
Cold-start bundle recommendation is a critical challenge where new bundles need to be recommended despite having no historical interaction data. CoHeat addresses this by leveraging two complementary graph-based views: a history-view capturing user-bundle interactions and an affiliation-view capturing user-item relationships. The method uses popularity-based coalescence to emphasize the more informative affiliation-view for less popular bundles, combined with curriculum heating that gradually shifts training focus from history-view to affiliation-view, and contrastive learning to align representations between the two views.

## Method Summary
CoHeat constructs two bipartite graphs (user-bundle for history-view and user-item for affiliation-view) and uses LightGCN to obtain embeddings for both views. It computes scores from each view and applies popularity-based coalescence with dynamic weighting coefficients based on bundle interaction counts. Curriculum heating gradually adjusts temperature parameters over training epochs to shift emphasis from history-view to affiliation-view. The method also employs contrastive learning with alignment and uniformity objectives to reconcile the two views. Training uses BPR loss with regularization from the contrastive objectives.

## Key Results
- Achieves up to 193% higher nDCG@20 compared to the best competitor on cold-start bundle recommendation
- Demonstrates superior performance across three real-world datasets (Youshu, NetEase, and iFashion)
- Shows consistent improvement over state-of-the-art methods for cold-start scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Popularity-based coalescence weights history-view and affiliation-view scores based on bundle interaction counts
- Mechanism: For each bundle, compute weighting coefficient γ_b using bundle popularity (interaction count n_b). Less popular bundles rely more on affiliation-view score and less on history-view score.
- Core assumption: The affiliation-view provides richer information than history-view for less popular bundles
- Evidence anchors:
  - [abstract] "emphasizing affiliation-view representations for less popular bundles, which provide richer information than the sparse history-view"
  - [section] "For an unpopular bundle, history-view provides insufficient information while affiliation-view provides sufficient information"
  - [corpus] Weak evidence - no corpus papers explicitly discuss this specific weighting mechanism
- Break condition: If affiliation-view representations are also sparse for cold-start bundles, or if bundle popularity is not a good indicator of view quality

### Mechanism 2
- Claim: Curriculum heating gradually shifts training focus from history-view to affiliation-view
- Mechanism: Use temperature parameter ψ(t) that increases over epochs to control weighting coefficient γ_b(t). Initially emphasize history-view, then progressively shift to affiliation-view.
- Core assumption: History-view representations are easier to learn initially, while affiliation-view representations require gradual training
- Evidence anchors:
  - [abstract] "curriculum learning approach is used to gradually shift the training focus from history-view to affiliation-view"
  - [section] "we harness curriculum learning to enhance the learning process of user-bundle relationships"
  - [corpus] No corpus papers discuss this specific curriculum heating approach for bundle recommendation
- Break condition: If rapid temperature increase causes training instability, or if curriculum scheduling is suboptimal

### Mechanism 3
- Claim: Contrastive learning aligns history-view and affiliation-view representations through alignment and uniformity loss
- Mechanism: Compute alignment loss to make embeddings close between views, and uniformity loss to scatter different users/bundles in representation space
- Core assumption: Aligned representations improve transfer from history-view to affiliation-view for cold-start bundles
- Evidence anchors:
  - [abstract] "contrastive learning aligns the two view representations"
  - [section] "we exploit a contrastive learning approach that reconciles the two views"
  - [corpus] Weak evidence - only one corpus paper mentions contrastive learning but not this specific approach
- Break condition: If alignment loss causes overfitting to training data, or if uniformity loss degrades representation quality

## Foundational Learning

- Concept: Graph-based representation learning with LightGCN
  - Why needed here: Captures collaborative information from user-bundle and user-item interactions effectively
  - Quick check question: How does LightGCN differ from traditional GNN layers in recommendation?

- Concept: Curriculum learning scheduling
  - Why needed here: Gradually shifts training focus from easier (history-view) to harder (affiliation-view) representations
  - Quick check question: What is the mathematical form of the temperature schedule and why is it effective?

- Concept: Contrastive learning objectives (alignment and uniformity)
  - Why needed here: Aligns two distinct views while maintaining distinct representations for different users/bundles
  - Quick check question: How do alignment and uniformity losses complement each other in representation learning?

## Architecture Onboarding

- Component map: User-bundle interactions, user-item interactions, bundle-item affiliations -> Graph construction -> LightGCN embeddings -> Popularity-based coalescence -> Score prediction -> Ranking
- Critical path: User input → Graph construction → LightGCN embeddings → Popularity-based coalescence → Score prediction → Ranking
- Design tradeoffs:
  - Single vs dual graph views: Dual views capture complementary information but increase complexity
  - Fixed vs dynamic weighting: Dynamic weighting adapts to bundle popularity but requires popularity estimation
  - Contrastive learning: Improves alignment but adds training complexity
- Failure signatures:
  - Poor cold-start performance: May indicate insufficient affiliation-view information or ineffective curriculum heating
  - Degraded warm-start performance: May indicate over-emphasis on affiliation-view or misalignment between views
  - Training instability: May indicate improper curriculum scheduling or temperature scheduling
- First 3 experiments:
  1. Compare performance with fixed vs dynamic weighting coefficients on cold-start bundles
  2. Test different temperature schedules for curriculum heating (linear, exponential, step-wise)
  3. Evaluate impact of alignment vs uniformity loss components on overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CoHeat's performance scale with increasing dataset size and complexity, particularly when dealing with extremely large-scale bundle recommendation scenarios?
- Basis in paper: [inferred] The paper demonstrates strong performance on three real-world datasets but does not explore scalability to larger, more complex datasets.
- Why unresolved: The experiments are limited to relatively moderate-sized datasets, leaving uncertainty about performance on industrial-scale data.
- What evidence would resolve it: Empirical results on datasets with orders of magnitude more users, bundles, and items, comparing both accuracy and computational efficiency.

### Open Question 2
- Question: Can CoHeat's curriculum learning approach be generalized to other recommendation domains beyond bundle recommendation, such as sequential or multi-modal recommendation?
- Basis in paper: [explicit] The paper focuses specifically on cold-start bundle recommendation and does not explore applicability to other recommendation types.
- Why unresolved: The method's effectiveness in different recommendation contexts remains untested.
- What evidence would resolve it: Successful adaptation and evaluation of CoHeat's core techniques on sequential recommendation, cross-domain recommendation, or multi-modal recommendation tasks.

### Open Question 3
- Question: How robust is CoHeat to different types of bundle item relationships, such as complementary vs. substitutable items within bundles?
- Basis in paper: [inferred] The paper treats bundle-item affiliations generically without distinguishing different relationship types.
- Why unresolved: The method's effectiveness for different bundle composition strategies is unknown.
- What evidence would resolve it: Experiments comparing CoHeat's performance on datasets with explicitly labeled complementary vs. substitutable item relationships within bundles.

## Limitations
- The core assumption that affiliation-view provides richer information for cold-start bundles may not hold universally, particularly if bundle-item affiliations are also sparse or if bundle popularity is not a reliable indicator of view quality
- The effectiveness of curriculum heating depends heavily on the temperature scheduling, which is not thoroughly validated across different dataset characteristics
- The contrastive learning approach, while theoretically sound, lacks extensive ablation studies to isolate the contribution of alignment vs uniformity objectives

## Confidence
- High confidence in the overall framework design and mathematical formulation
- Medium confidence in the empirical superiority claims, given the competitive landscape of bundle recommendation methods
- Medium confidence in the mechanism explanations, as they rely on reasonable assumptions rather than proven causal relationships

## Next Checks
1. Conduct ablation studies comparing fixed vs dynamic weighting coefficients across datasets with varying popularity distributions to validate the popularity-based coalescence mechanism
2. Test alternative curriculum learning schedules (exponential, step-wise) to assess the robustness of the temperature-based approach
3. Perform extensive sensitivity analysis on the alignment and uniformity loss weights to understand their individual contributions to cold-start performance