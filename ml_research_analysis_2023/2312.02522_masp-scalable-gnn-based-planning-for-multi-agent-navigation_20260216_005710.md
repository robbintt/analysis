---
ver: rpa2
title: 'MASP: Scalable GNN-based Planning for Multi-Agent Navigation'
arxiv_id: '2312.02522'
source_url: https://arxiv.org/abs/2312.02522
tags:
- agents
- agent
- masp
- goal
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multi-agent navigation where
  a large number of agents (e.g., 50+) must reach initially unassigned goals in a
  limited time. The core method, MASP (Multi-Agent Scalable GNN-based Planner), uses
  a hierarchical framework combining graph neural networks (GNN) for cooperation.
---

# MASP: Scalable GNN-based Planning for Multi-Agent Navigation

## Quick Facts
- arXiv ID: 2312.02522
- Source URL: https://arxiv.org/abs/2312.02522
- Reference count: 39
- Primary result: Achieves nearly 100% success rate in multi-agent navigation with 50+ agents using hierarchical GNN-based planning

## Executive Summary
This paper introduces MASP, a hierarchical framework for scalable multi-agent navigation where numerous agents must reach initially unassigned goals. The core innovation is decomposing the complex coordination problem into two GNN-based components: a high-level Multi-Goal Matcher that assigns goals using graph matching, and a low-level Coordinated Action Executor that handles local navigation through group-based GNN modeling. The method achieves state-of-the-art performance with 18.26% fewer steps in MPE and 7.06% fewer steps in OmniDrones compared to RL and planning-based baselines, while demonstrating zero-shot generalization to unseen team sizes.

## Method Summary
MASP uses a hierarchical framework with two main components: MGM (Multi-Goal Matcher) and CAE (Coordinated Action Executor). MGM uses self-attention and cross-attention to assign goals to agents based on their positions, while CAE uses GCN blocks within groups to update agent features and generate actions. The entire system is trained using MAPPO with shared critic and decentralized actors. Agents are divided into fixed-size groups (typically 3) during both training and evaluation to enable zero-shot generalization to different team sizes.

## Key Results
- Achieves nearly 100% success rate in both 2D (50 agents) and 3D (20 agents) environments
- Outperforms RL and planning-based baselines by 18.26% fewer steps in MPE and 7.06% fewer steps in OmniDrones
- Demonstrates zero-shot generalization to unseen team sizes without retraining
- Shows consistent performance improvements across different team sizes and environments

## Why This Works (Mechanism)

### Mechanism 1
- Hierarchical decomposition into MGM (high-level) and CAE (low-level) reduces search space complexity, enabling scalable multi-agent coordination
- Core assumption: Both policies operate independently but are coordinated through shared graph structures and joint training via MAPPO
- Break condition: If goal assignments in MGM conflict frequently or if group boundaries in CAE are poorly defined, the decomposition fails to reduce complexity

### Mechanism 2
- Graph neural networks capture agent-goal and agent-agent interactions more efficiently than flat MLP-based representations
- Core assumption: The relational structure of the problem is naturally represented as a graph, and GNN can learn useful embeddings from this structure
- Break condition: If the graph construction poorly reflects actual interaction patterns, GNN performance degrades to MLP levels

### Mechanism 3
- Group division with fixed-size subgraphs enables zero-shot generalization to unseen team sizes
- Core assumption: Learned policies generalize across group sizes because local interaction patterns are consistent within fixed-size groups
- Break condition: If group size is too small to capture meaningful interactions, or if agent roles vary drastically across different team sizes, generalization breaks down

## Foundational Learning

- **Graph neural networks (GNN) and graph attention mechanisms**
  - Why needed: GNN is central to both MGM and CAE for modeling relationships between agents and goals
  - Quick check: How does a GCN layer aggregate neighbor information, and what is the role of the adjacency matrix in this process?

- **Hierarchical reinforcement learning (HRL) and goal-conditioned policies**
  - Why needed: MASP's two-level policy structure is a specific instance of HRL
  - Quick check: In a two-level HRL setup, how do you design rewards for the high-level policy when the low-level policy is stochastic?

- **Multi-agent reinforcement learning (MARL) with CTDE**
  - Why needed: MASP uses MAPPO, a CTDE algorithm
  - Quick check: What information must be centralized during training but not available during execution in a CTDE framework?

## Architecture Onboarding

- **Component map**: Agent and goal positions → MGM graphs → Goal assignment → Group division → CAE group graphs → Updated agent features → State representation → Action
- **Critical path**: 1) Agent and goal positions → MGM graphs → Goal assignment 2) Assigned goal + agent positions → CAE group graphs → Updated agent features 3) Agent features + goal features → State representation → Action 4) Environment step → Reward → Policy update (MAPPO)
- **Design tradeoffs**: Group size fixed at 3 balances interaction richness vs. scalability; fully connected graphs are simple but O(n²) edges; joint MAPPO training enables coordination but requires centralized critic
- **Failure signatures**: MGM: High goal collision rate; CAE: Poor navigation efficiency; Training: Slow convergence, unstable reward curves
- **First 3 experiments**: 1) Ablation on group size: Train MASP with group sizes 2, 3, 4 in MPE; measure success rate and steps 2) Ablation on graph vs. MLP in MGM: Replace MGM with MLP-based goal assignment; compare success rate 3) Varying team size robustness: Train on N=20, evaluate zero-shot on N=10 and N=30 in OmniDrones

## Open Questions the Paper Calls Out

- How does MASP perform in environments with dynamic obstacles or moving targets where goal positions change over time?
- Can MASP be extended to handle heterogeneous agents with different capabilities or constraints?
- What is the computational complexity of MASP compared to planning-based methods as agent numbers increase?

## Limitations

- The hierarchical decomposition's effectiveness is demonstrated but not rigorously compared against alternative architectural choices
- GNN-specific advantages are asserted but not validated through direct comparisons with MLP-based baselines
- Zero-shot generalization claims rely on fixed group sizes but sensitivity to group size choice is not explored

## Confidence

- **High confidence**: Success rate improvements over baselines in both MPE and OmniDrones environments
- **Medium confidence**: Claims about space complexity reduction through hierarchical decomposition
- **Medium confidence**: Zero-shot generalization to unseen team sizes

## Next Checks

1. **Ablation study on group size**: Systematically evaluate MASP with group sizes 2, 3, and 4 in MPE to quantify the impact on both performance and generalization capability
2. **MLP baseline comparison**: Replace MGM's GNN-based attention with equivalent MLP architecture to isolate the contribution of graph structure to goal assignment performance
3. **Scalability stress test**: Train MASP on 20 agents and evaluate zero-shot performance on team sizes ranging from 10 to 50 in OmniDrones, measuring both success rate and steps to identify performance degradation patterns