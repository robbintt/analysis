---
ver: rpa2
title: 'OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization'
arxiv_id: '2312.04440'
source_url: https://arxiv.org/abs/2312.04440
tags:
- summaries
- aspect
- summary
- aspect-based
- aspects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OPENASP introduces a high-quality, multi-document open aspect-based
  summarization benchmark derived from existing MDS datasets using a novel annotation
  protocol. The protocol extracts aspect-based summaries from generic reference summaries
  via controlled crowdsourcing, avoiding expensive manual document reading and summary
  writing.
---

# OpenAsp: A Benchmark for Multi-document Open Aspect-based Summarization

## Quick Facts
- arXiv ID: 2312.04440
- Source URL: https://arxiv.org/abs/2312.04440
- Reference count: 37
- Primary result: Introduces a high-quality multi-document open aspect-based summarization benchmark with 1,310 aspect-based summaries derived from generic summaries via controlled crowdsourcing

## Executive Summary
OPENASP introduces a novel benchmark for multi-document open aspect-based summarization, addressing the gap in existing datasets that focus on pre-defined aspects. The dataset is created using a controlled crowdsourcing protocol that extracts aspect-based summaries from generic reference summaries, avoiding the need for expensive manual document reading and summary writing. The resulting dataset contains 1,310 aspect-based summaries across diverse topics with high relevance, coherence, and consistency scores, demonstrating its quality and potential for advancing aspect-based summarization research.

## Method Summary
The OPENASP benchmark is created by extracting aspect-based summaries from existing generic reference summaries in MultiNews and DUC datasets. Annotators identify aspects within generic summaries and select corresponding sentences to form aspect-based summaries. This protocol leverages existing MDS benchmarks and applies controlled crowdsourcing to avoid the need for manual document reading. The resulting dataset contains 1,310 aspect-based summaries with diverse aspects, average input length of 7,930 tokens, and average summary length of 96 tokens.

## Key Results
- High-quality aspect-based summaries with relevance (4.6), coherence (5.0), and consistency (4.6) scores on 1-5 scale
- Baseline models achieve moderate relevance to aspects (3.05-3.70) and reference summaries (2.35-2.80)
- ROUGE-1 scores of 32.4-33.7 for top models, with substantial room for improvement compared to oracle upper bound of 40.6
- Challenging multi-document setting with diverse aspects (avg. 3.1 per topic) and large inputs (avg. 7,930 tokens)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aspect-based summaries can be extracted from generic summaries without reading source documents
- Mechanism: Annotators identify aspects in generic summaries and select corresponding sentences
- Core assumption: Generic summaries contain all salient aspect information
- Evidence anchors: Abstract states protocol extracts aspect-based summaries from generic summaries via crowdsourcing
- Break condition: If generic summaries omit key aspect information present in source documents

### Mechanism 2
- Claim: Open aspects allow flexible, topic-specific aspects rather than predefined domains
- Mechanism: Annotators define aspects as central themes within topics with free-form labels
- Core assumption: Open aspects provide more realistic coverage of user information needs
- Evidence anchors: Abstract contrasts with previous datasets focusing on limited pre-defined aspects
- Break condition: If open aspects lead to inconsistent definitions across annotators

### Mechanism 3
- Claim: Multi-document setting requires consolidation of information across documents
- Mechanism: Aspect-based summaries are extracted from generic multi-document summaries
- Core assumption: Generic multi-document summaries adequately consolidate information
- Evidence anchors: Abstract mentions exploiting existing MDS benchmarks
- Break condition: If source documents contain contradictory information not resolved in generic summaries

## Foundational Learning

- Concept: Aspect-based summarization
  - Why needed here: Understanding the difference between generic and aspect-based summaries is crucial for grasping the dataset's purpose
  - Quick check question: What distinguishes an aspect-based summary from a generic summary?

- Concept: Multi-document summarization
  - Why needed here: The dataset operates in the multi-document setting, requiring understanding of document consolidation
  - Quick check question: How does information consolidation differ between single and multi-document settings?

- Concept: Crowdsourcing annotation protocols
  - Why needed here: The dataset creation method relies on controlled crowdsourcing, which has specific quality considerations
  - Quick check question: What are the advantages and potential pitfalls of using crowdsourcing for summarization annotation?

## Architecture Onboarding

- Component map: Document sets (avg. 7,930 tokens) -> Sentence selector (Lead or Sentence-T5) -> Summarization model (BART, PRIMERA, or ChatGPT) -> Aspect-based summaries (avg. 96 tokens)

- Critical path:
  1. Input documents → Sentence selector → Filtered sentences
  2. Filtered sentences + aspect label → Summarization model → Aspect-based summary

- Design tradeoffs:
  - Input size vs. model capability (1K vs. 4K vs. 16K tokens)
  - Fine-tuning vs. zero-shot (trained models vs. ChatGPT)
  - Extractive vs. abstractive summary generation

- Failure signatures:
  - Low ROUGE scores indicating poor summary quality
  - High standard deviation in human evaluation suggesting inconsistent performance
  - Aspect-relevance scores below 3.0 indicating failure to capture aspect-specific information

- First 3 experiments:
  1. Compare Lead vs. Sentence-T5 sentence selection on a subset of the test set
  2. Evaluate fine-tuned vs. zero-shot models on the same subset
  3. Test different input size limits (1K, 4K, 16K) to find optimal balance for the dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would different annotation protocols for extracting aspects from generic summaries affect the quality and diversity of resulting aspect-based summaries?
- Basis in paper: The paper describes a novel annotation protocol but doesn't explore alternative methods systematically
- Why unresolved: The paper validates their specific protocol but doesn't compare different extraction strategies
- What evidence would resolve it: A controlled study comparing multiple annotation protocols applied to the same source datasets

### Open Question 2
- Question: What is the upper bound performance of aspect-based summarization models when given oracle sentence selection?
- Basis in paper: The paper reports an "Oracle" extractive summary score of 40.6 ROUGE-1 but doesn't investigate further
- Why unresolved: The paper mentions oracle sentence selection as an upper bound but doesn't explore its implications
- What evidence would resolve it: Systematic experiments using oracle sentence selection as input to various summarization models

### Open Question 3
- Question: How does the performance of summarization models vary across different aspects and document set sizes in OPENASP?
- Basis in paper: The paper notes diverse aspects and document sets but doesn't analyze model performance across these dimensions
- Why unresolved: The evaluation aggregates results without examining variation by aspect complexity or input size
- What evidence would resolve it: Detailed performance analysis broken down by aspect characteristics and document set properties

## Limitations

- The dataset creation protocol assumes generic summaries contain all salient aspect-specific information, which may not always hold true
- The controlled crowdsourcing protocol details are not fully specified, making exact replication challenging
- The evaluation framework depends on relatively small-scale human annotation (3 raters per aspect), which may not capture full inter-annotator variability

## Confidence

**High Confidence**: The dataset creation methodology and basic statistical properties are well-documented and reproducible. The human evaluation protocol and results are clearly presented.

**Medium Confidence**: The claim that this is the first multi-document open aspect-based summarization benchmark requires careful consideration. The baseline model performance results are credible but may not reflect optimal configurations.

**Low Confidence**: The assertion that generic summaries adequately capture all aspect-specific information present in source documents lacks direct validation. The comparison to "oracle upper bounds" assumes perfect aspect extraction.

## Next Checks

1. **Direct Document Validation**: Randomly sample 20-30 aspect-based summaries and verify whether the extracted aspect information is actually present in the source documents.

2. **Inter-annotator Agreement Study**: Replicate the annotation process with 5-7 annotators per aspect instead of 3 to establish more robust inter-annotator agreement scores.

3. **Zero-Shot Transfer Evaluation**: Test whether models trained on OpenAsp can effectively transfer to other aspect-based summarization datasets without fine-tuning.