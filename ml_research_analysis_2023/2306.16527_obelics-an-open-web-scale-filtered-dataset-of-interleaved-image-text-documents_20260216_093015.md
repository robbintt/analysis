---
ver: rpa2
title: 'OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents'
arxiv_id: '2306.16527'
source_url: https://arxiv.org/abs/2306.16527
tags:
- images
- documents
- dataset
- also
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OBELISC is a web-scale filtered dataset of interleaved image-text
  documents for multimodal learning, containing 141 million web pages with 353 million
  associated images and 115 billion tokens. The dataset was created by filtering and
  deduplicating English documents from Common Crawl, focusing on preserving the natural
  context of images within their textual surroundings.
---

# OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents

## Quick Facts
- arXiv ID: 2306.16527
- Source URL: https://arxiv.org/abs/2306.16527
- Reference count: 40
- Key outcome: OBELISC dataset contains 141M web pages, 353M images, and 115B tokens, enabling multimodal models to outperform those trained on image-text pairs

## Executive Summary
OBELISC is a large-scale, filtered dataset of interleaved image-text documents created to address the limitations of image-text pair datasets in multimodal learning. By extracting and filtering web documents from Common Crawl while preserving the natural context of images within their surrounding text, OBELISC provides richer semantic relationships than alt-text based datasets. The dataset enables training of vision and language models that achieve competitive performance on multimodal benchmarks, demonstrating the advantages of learning from full document context rather than isolated image-text pairs.

## Method Summary
The OBELISC dataset was constructed by filtering and deduplicating English documents from Common Crawl, extracting 141 million web pages with 353 million associated images and 115 billion text tokens. The pipeline involved HTML simplification, DOM tree cleaning, node-level filtering (removing low-quality images and text), document-level filtering, and deduplication. The resulting dataset was used to train a 80-billion parameter multimodal model using a Flamingo-like architecture combining frozen LLaMA and OpenCLIP backbones with trainable cross-attention layers, achieving competitive performance on vision-language benchmarks.

## Key Results
- OBELISC-trained models outperform models trained on image-text pairs on multiple multimodal benchmarks
- An 80B parameter model trained on OBELISC achieves competitive performance against closed-source models like Flamingo
- The dataset contains 141M web pages, 353M images, and 115B tokens after extensive filtering and deduplication

## Why This Works (Mechanism)

### Mechanism 1
Interleaved image-text documents preserve richer context than image-text pairs, leading to better multimodal reasoning. The dataset retains natural layout and relationships between images and surrounding text, allowing models to learn how images relate to their textual context and to each other. Core assumption: Images in their original web context provide more semantically meaningful associations than isolated alt-text captions. Break condition: If surrounding text is not meaningfully related to images, or document structure is noisy.

### Mechanism 2
Extensive filtering and deduplication improve data quality and model performance. The pipeline applies node-level and document-level filtering to remove low-quality images and text, and deduplicates images and documents to reduce redundancy and noise. Core assumption: High-quality, deduplicated data leads to better generalization and less overfitting. Break condition: Over-aggressive filtering removes useful data or filtering rules are not well-tuned.

### Mechanism 3
The dataset's scale (115B tokens, 353M images) enables training of competitive large multimodal models. The sheer volume of high-quality interleaved data provides sufficient coverage of concepts and contexts for large-scale pretraining. Core assumption: Larger, diverse datasets improve model performance for multimodal tasks requiring broad knowledge. Break condition: If data quality does not scale with size, or model architecture cannot effectively utilize the large dataset.

## Foundational Learning

- Concept: Text filtering and quality classification
  - Why needed here: To remove low-quality, spam, or machine-generated text that could degrade model training
  - Quick check question: What are the key metrics used to filter out low-quality paragraphs in OBELISC?

- Concept: HTML DOM tree manipulation
  - Why needed here: To extract and preserve the structure of web documents, maintaining the relationship between images and their textual context
  - Quick check question: How does the pipeline handle different HTML tags to retain relevant content while removing noise?

- Concept: Multimodal pretraining objectives
  - Why needed here: To understand how models trained on interleaved documents differ from those trained on image-text pairs
  - Quick check question: What is the training objective used for models trained on OBELISC, and how does it leverage the interleaved format?

## Architecture Onboarding

- Component map: Common Crawl -> HTML extraction -> text and image extraction -> HTML simplification -> DOM tree cleaning -> tag unwrapping -> Node-level filtering -> Document-level filtering -> Deduplication -> Training

- Critical path:
  1. Collect raw HTML from Common Crawl
  2. Simplify HTML and extract interleaved text-image documents
  3. Apply filtering and deduplication
  4. Train multimodal model on filtered dataset
  5. Evaluate on benchmarks

- Design tradeoffs:
  - Strict filtering vs. dataset size: Stricter filtering reduces noise but may also remove useful data
  - Image deduplication threshold: Removing images appearing >10 times reduces spam but may also remove legitimate repeated content
  - Training on interleaved documents vs. image-text pairs: Interleaved documents preserve context but are harder to align; image-text pairs are simpler but less contextual

- Failure signatures:
  - Low model performance on VQA tasks despite high dataset size: Indicates filtering may have removed too much contextual data
  - High repetition of certain images across documents: Suggests deduplication threshold is too lenient
  - Model overfitting to specific domains: Implies filtering rules are biased toward certain types of content

- First 3 experiments:
  1. Train a small model (e.g., 1B parameters) on OBELISC vs. LAION to compare impact of interleaved documents vs. image-text pairs on few-shot learning benchmark
  2. Vary image deduplication threshold (e.g., 5, 10, 20 occurrences) and measure impact on model performance and dataset size
  3. Apply different filtering cutoffs (e.g., stricter perplexity thresholds) and evaluate trade-off between data quality and quantity

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal balance between image deduplication and maintaining diverse contextual information for training vision-language models? The paper discusses deduplication strategies with an arbitrary threshold of 10 duplicates but does not explore how different thresholds affect model performance or quantify the trade-off between removing spam and preserving useful contextual variations.

### Open Question 2
How does the quality of text surrounding images in OBELISC compare to the quality of alt-text in image-text pair datasets when measured by perplexity or other language quality metrics? While perplexity comparisons are made, the paper doesn't directly compare OBELISC text quality against alt-text from image-text pair datasets or quantify the impact of longer contextual text versus concise alt-text on model learning.

### Open Question 3
What is the impact of different filtering cutoffs on the diversity and performance of models trained on OBELISC? The paper mentions various text filtering metrics with specific cutoffs but does not explore how varying these thresholds affects dataset diversity or model performance across different tasks.

### Open Question 4
How does the performance of models trained on OBELISC compare to models trained on other large-scale multimodal document datasets like mmc4 when controlling for dataset size and image count? The paper compares OBELISC to mmc4 in general statistics but does not provide a direct comparison of model performance when training on OBELISC versus mmc4 with matched dataset sizes or image counts.

## Limitations
- Exact filtering thresholds and deduplication parameters remain underspecified, making exact reproduction challenging
- No direct comparison against models trained on the same architecture using image-text pair datasets of comparable scale
- Missing sensitivity analysis showing how different filtering and deduplication thresholds affect downstream model performance

## Confidence
- High Confidence: The core claim that interleaved image-text documents provide richer contextual information than image-text pairs is well-supported by evaluation results
- Medium Confidence: The claim that OBELISC's scale is sufficient for competitive multimodal model training is supported but could benefit from more direct comparisons
- Low Confidence: The specific impact of individual filtering decisions on model performance cannot be precisely assessed due to lack of ablation studies

## Next Checks
1. Systematically vary the deduplication threshold (5, 10, 20 occurrences) and paragraph perplexity cutoffs to quantify their impact on both dataset quality metrics and downstream model performance
2. Train identical model architectures on OBELISC versus a comparable image-text pair dataset (LAION-5B filtered subset) using the same hyperparameters to isolate the impact of interleaved document format
3. Analyze model performance on benchmark subsets that require complex reasoning across multiple images and text segments to validate whether the interleaved format specifically benefits these challenging cases