---
ver: rpa2
title: Sociodemographic Prompting is Not Yet an Effective Approach for Simulating
  Subjective Judgments with LLMs
arxiv_id: '2311.09730'
source_url: https://arxiv.org/abs/2311.09730
tags:
- llms
- tasks
- language
- subjective
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models (LLMs) are increasingly used for subjective\
  \ NLP tasks, but it is unclear whether their judgments reflect diverse demographic\
  \ perspectives. This study investigates demographic biases in LLMs' predictions\
  \ on two subjective tasks\u2014politeness and offensiveness\u2014using the POPQUORN\
  \ dataset, which includes annotations from diverse U.S."
---

# Sociodemographic Prompting is Not Yet an Effective Approach for Simulating Subjective Judgments with LLMs

## Quick Facts
- arXiv ID: 2311.09730
- Source URL: https://arxiv.org/abs/2311.09730
- Reference count: 7
- Large language models show demographic bias in subjective judgments and demographic prompting fails to improve alignment

## Executive Summary
This study investigates whether large language models can accurately capture demographic perspectives on subjective NLP tasks like politeness and offensiveness judgments. Using the POPQUORN dataset with diverse U.S. annotations, the research finds that LLMs' zero-shot predictions align more closely with White and female annotators than with Black, Asian, or male annotators. Surprisingly, adding demographic information to prompts did not improve alignment with specific groups and often worsened performance. These findings highlight persistent demographic biases in LLMs for subjective judgments and suggest that simple demographic prompting is insufficient to mitigate such biases.

## Method Summary
The study evaluates LLM performance on two subjective tasks (politeness and offensiveness) using zero-shot prompting with and without demographic information. Researchers use the POPQUORN dataset containing annotations from diverse U.S. populations and measure correlation between model predictions and demographic-specific annotations using Pearson correlation coefficients. The experimental design compares baseline zero-shot predictions against predictions made when demographic tokens are added to prompts.

## Key Results
- LLMs' zero-shot predictions align more closely with White and female annotators than with Black, Asian, or male annotators
- Adding demographic information to prompts did not improve alignment with specific groups and often worsened performance
- Model predictions show significant demographic bias even without explicit demographic prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Demographic identity prompting fails to improve alignment with specific groups because LLMs do not truly "simulate" those identities but instead apply surface-level token matching that disrupts their internal prediction process.
- Mechanism: When demographic tokens like "Black" or "Asian" are added to prompts, the model shifts from its learned distribution toward a heuristic interpretation that often correlates with stereotypical or less accurate patterns, leading to worse correlation with ground truth annotations from those groups.
- Core assumption: The model's zero-shot predictions already reflect a learned bias distribution, and prompting with demographic terms does not change the underlying representation but instead triggers a less optimal inference pathway.
- Evidence anchors:
  - [abstract] "adding demographic information to prompts did not improve alignment with specific groups and often worsened performance"
  - [section] "adding identity tokens actually led to a lower prediction performance" and "models’ performance for both Asian and Black groups drops sharply after adding the words 'Black' and 'Asian'"
  - [corpus] Weak; no direct citations on identity token disruption mechanisms.
- Break condition: If the model's internal representations could be fine-tuned or augmented to truly understand demographic perspectives, the mechanism would break.

### Mechanism 2
- Claim: LLMs show demographic bias in subjective tasks because their training data underrepresents or misrepresents the perspectives of certain groups, leading to predictions that align more closely with majority group annotations.
- Mechanism: The training corpus contains imbalanced demographic perspectives; thus, the model's learned distribution over subjective judgments reflects majority views (e.g., White, female) more accurately than minority views, resulting in higher correlation with those groups.
- Core assumption: The model's predictions are a direct reflection of the statistical properties of its training data, and subjective judgments in the data are skewed toward majority perspectives.
- Evidence anchors:
  - [abstract] "LLMs’ zero-shot predictions align more closely with White and female annotators than with Black, Asian, or male annotators"
  - [section] "model predictions are closer to the perceptions of females compared to males and closer to White people instead of Black and Asian people"
  - [corpus] Weak; no direct citations on training data demographic imbalance.
- Break condition: If the training data were perfectly balanced across demographics, the bias would be reduced or eliminated.

### Mechanism 3
- Claim: Subjective task outputs are inherently variable across demographics, and the model's inability to capture this variability stems from its generalization over averaged or majority-labeled data rather than individualized demographic perspectives.
- Mechanism: Since subjective tasks have no single correct answer and vary by demographic, models trained on aggregated labels learn to predict a "central tendency" that aligns with majority groups, missing the nuanced differences of minority groups.
- Core assumption: The model's objective during training is to minimize loss over aggregated labels, not to model inter-group variance.
- Evidence anchors:
  - [abstract] "Human judgments are inherently subjective and are actively affected by personal traits such as gender and ethnicity"
  - [section] "what is rated highly for one group may be rated low by another" and "LLMs’ perceptions of both subjective tasks tend to align slightly more with the perceptions of women than those of men"
  - [corpus] Weak; no direct citations on aggregated label bias.
- Break condition: If the model were trained or prompted to explicitly model inter-group differences, this mechanism would break.

## Foundational Learning

- Concept: Demographic bias in machine learning
  - Why needed here: Understanding how training data imbalances and model architectures can encode societal biases is essential to interpreting why LLMs align more with certain demographics.
  - Quick check question: If a model is trained on data where 80% of subjective labels come from one demographic, which group's perspectives is it most likely to reflect in predictions?
- Concept: Zero-shot learning and prompting
  - Why needed here: The study uses zero-shot prompting to elicit subjective judgments; knowing how models generalize without task-specific fine-tuning is key to understanding the results.
  - Quick check question: What is the difference between zero-shot and few-shot prompting, and why might zero-shot be more prone to bias in subjective tasks?
- Concept: Correlation as an evaluation metric
  - Why needed here: The study uses Pearson correlation to compare model predictions to demographic-specific annotations; understanding this metric's strengths and limitations is crucial for interpreting results.
  - Quick check question: If a model's predictions have a higher correlation with one demographic group's labels than another, what does that imply about the model's bias?

## Architecture Onboarding

- Component map: Text prompt → LLM inference engine → Likert-scale prediction → Pearson correlation calculation → Demographic bias analysis
- Critical path: Prompt construction → LLM inference → Score extraction → Correlation calculation → Bias analysis
- Design tradeoffs: Using zero-shot prompting avoids task-specific fine-tuning but may increase bias; adding demographic tokens risks disrupting the model's learned distribution; choosing Pearson correlation measures linear alignment but may miss non-linear patterns
- Failure signatures: Low correlation with minority groups; worse performance after adding demographic tokens; inconsistent results across prompt formats; correlation patterns that favor majority groups
- First 3 experiments:
  1. Test zero-shot predictions on a small subset of the dataset and compute correlations with overall and group-specific annotations
  2. Add demographic tokens to prompts and measure change in correlation with the corresponding group's annotations
  3. Vary prompt formats (e.g., persona vs. instruction) and compare correlation changes to identify which prompt structure causes the largest performance drop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do intersectional identities (e.g., Black women, Asian men) influence LLM predictions on subjective tasks beyond single demographic categories?
- Basis in paper: [inferred] The paper explicitly states they did not examine intersectional identities and acknowledges this as a limitation, noting that bias associated with populations defined by multiple categories leads to incomplete measurement of social biases.
- Why unresolved: The study focused only on single demographic categories (gender and race separately) due to data sparsity, leaving the interaction effects of multiple demographic factors unexplored.
- What evidence would resolve it: Conducting experiments with larger, more diverse datasets that include sufficient samples of intersectional groups to measure how LLMs perform when prompted with combined demographic identities (e.g., "Black woman" vs. "Black" and "woman" separately).

### Open Question 2
- Question: Why does adding demographic identity tokens to prompts consistently worsen LLM performance across all demographic groups, and what mechanisms drive this unexpected result?
- Basis in paper: [explicit] The authors found that adding identity tokens (e.g., "Black," "Asian") to prompts actually decreased performance across all demographics, with particularly sharp drops for Black and Asian groups, which they describe as surprising and note requires further investigation.
- Why unresolved: The study observed this phenomenon but did not investigate the underlying reasons for why demographic prompting would harm rather than help alignment with specific groups.
- What evidence would resolve it: Systematic ablation studies testing different prompt formulations, examining model attention patterns when demographic terms are included, and comparing with models trained on demographically balanced data to isolate whether this is a prompting issue or reflects deeper model biases.

### Open Question 3
- Question: Do these demographic biases in LLM predictions on subjective tasks generalize to other domains beyond politeness and offensiveness (e.g., sentiment analysis, toxicity detection, or value-based judgments)?
- Basis in paper: [inferred] The authors acknowledge their study was limited to two specific tasks (politeness and offensiveness ratings) and note that biases may not generalize to other datasets and task settings.
- Why unresolved: The paper only examined two subjective tasks using data from offensive Reddit comments and polite emails, making it unclear whether the observed demographic biases extend to other types of subjective judgments.
- What evidence would resolve it: Replicating the experimental methodology across a diverse set of subjective NLP tasks (sentiment analysis, hate speech detection, value alignment questions) using multiple datasets to determine if demographic biases persist across different domains and task types.

## Limitations
- The study relies entirely on zero-shot prompting without exploring few-shot or fine-tuning approaches
- Key implementation details like exact baseline prompts and specific LLM specifications are missing
- The paper only examines two subjective tasks, limiting generalizability to other domains

## Confidence
- Demographic prompting failure claim: High confidence - The correlation data strongly supports that adding demographic information worsens or fails to improve alignment with target groups
- Training data bias mechanism: Low confidence - Plausible but not empirically validated in this study
- Token disruption mechanism: Low confidence - Observational correlation without mechanistic explanation
- Model alignment with majority groups: High confidence - The correlation patterns clearly show stronger alignment with White and female annotators

## Next Checks
1. **Ablation study on demographic tokens**: Systematically test which specific words or phrases in demographic prompts cause performance degradation by comparing results when adding individual demographic terms versus full demographic descriptions
2. **Training data demographic analysis**: Examine the demographic composition of the training data used by the LLMs to establish whether there is indeed a correlation between training data demographics and prediction biases
3. **Alternative prompting strategies**: Test few-shot prompting with demographic examples and compare results to zero-shot prompting to determine if demonstration-based approaches can overcome the limitations identified in demographic prompting