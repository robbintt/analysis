---
ver: rpa2
title: 'ModuleFormer: Modularity Emerges from Mixture-of-Experts'
arxiv_id: '2306.04640'
source_url: https://arxiv.org/abs/2306.04640
tags:
- uni00000013
- uni00000011
- arxiv
- modules
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ModuleFormer, a new modular architecture for
  large language models that uses Sparse Mixture of Experts (SMoE) to achieve efficiency,
  extendability, and specialization. Unlike prior SMoE approaches requiring curated
  domain-labeled data, ModuleFormer induces modularity from uncurated pretraining
  data using new load balancing and concentration losses.
---

# ModuleFormer: Modularity Emerges from Mixture-of-Experts

## Quick Facts
- **arXiv ID**: 2306.04640
- **Source URL**: https://arxiv.org/abs/2306.04640
- **Reference count**: 13
- **Primary result**: ModuleFormer achieves comparable performance to dense models while reducing latency by 50% and enabling task-specific expert pruning

## Executive Summary
ModuleFormer introduces a modular architecture for large language models that uses Sparse Mixture of Experts (SMoE) to achieve efficiency, extendability, and specialization. Unlike prior SMoE approaches requiring curated domain-labeled data, ModuleFormer induces modularity from uncurated pretraining data using new load balancing and concentration losses. The architecture combines stick-breaking attention heads with feedforward expert modules, activating a subset per input token during training and inference. Experimental results show that ModuleFormer achieves comparable performance to dense models of similar size while reducing latency by 50%, peak memory usage, and increasing throughput over 2x. It is more resistant to catastrophic forgetting than dense models when finetuned on new domains and can be extended with new modules for continual learning. Additionally, finetuning with load concentration loss allows pruning task-irrelevant modules without sacrificing performance, enabling lightweight deployment.

## Method Summary
ModuleFormer is a modular architecture that combines stick-breaking attention with feedforward expert modules. The model uses a router to select a subset of experts per input token, with two expert types: attention heads (shared key/value, unique query/output) and feedforward experts (standard MoE). The architecture employs load balancing loss to ensure even expert utilization across the batch and load concentration loss for task-specific pruning. Training uses AdamW optimizer with cosine learning rate schedule on the Pile corpus (300B tokens). The model is evaluated on zero-shot/few-shot tasks, language modeling (Wikitext), code generation (HumanEval), and continual learning tasks on CC-100 corpus.

## Key Results
- Achieves comparable performance to dense models of similar size on common sense reasoning, closed-book QA, and code generation benchmarks
- Reduces latency by 50% and peak memory usage while increasing throughput over 2x
- More resistant to catastrophic forgetting than dense models when finetuned on new domains
- Enables task-specific expert pruning without retraining, allowing lightweight deployment

## Why This Works (Mechanism)

### Mechanism 1: Stick-breaking attention enables input-length extrapolation without explicit positional encodings
- **Core assumption**: The product of sigmoid probabilities decays slowly enough that distant tokens still receive non-negligible attention
- **Evidence anchors**: Efficient computation with cumulative sums in log-space negates need for explicit position modeling; implicitly encodes position information
- **Break condition**: If sigmoid probabilities drop too quickly, distant tokens may be effectively ignored

### Mechanism 2: Mutual information loss balances expert load across tokens while encouraging sparsity per input
- **Core assumption**: Expert routing can be optimized via entropy regularization without explicit domain labels
- **Evidence anchors**: Maximizes marginal entropy of expert usage across batch while minimizing conditional entropy per token
- **Break condition**: If expert capacities are mismatched, maximizing marginal entropy may still leave some experts underutilized

### Mechanism 3: Load concentration loss enables task-specific expert pruning without retraining
- **Core assumption**: Task-relevant computation concentrates in a subset of experts during adaptation
- **Evidence anchors**: Frequency of expert activation directly indicates task relevance; low-frequency experts can be pruned safely
- **Break condition**: If task-relevant computation is distributed across many experts, pruning may degrade performance

## Foundational Learning

- **Concept**: Mixture-of-Experts (MoE) routing mechanism
  - Why needed here: Understanding how the router selects experts based on input tokens is fundamental to grasping ModuleFormer's modularity
  - Quick check question: How does the Top-k operation in the router ensure sparsity during inference?

- **Concept**: Stick-breaking process and Dirichlet process
  - Why needed here: The stick-breaking attention is a core architectural innovation that replaces standard attention mechanisms
  - Quick check question: What property of the stick-breaking process ensures the attention weights sum to 1?

- **Concept**: Catastrophic forgetting and continual learning
  - Why needed here: ModuleFormer's modularity is designed to mitigate forgetting when learning new domains
  - Quick check question: How does freezing pretrained expert parameters during continual learning help prevent catastrophic forgetting?

## Architecture Onboarding

- **Component map**: Input embedding layer → Router (MLP with Top-k) → Expert selection → Stick-breaking attention heads or FFD modules → Output projection
- **Critical path**: Token embedding → Router computation (h(x) = A ReLU(Bx)) → Top-k gating → Expert selection → Expert execution (stick-breaking attention or FFD) → Aggregation → Layer output
- **Design tradeoffs**: More experts → better specialization but higher memory and potential routing complexity; Larger k → better performance but reduced sparsity benefits; Stick-breaking attention → simpler positional encoding but potential attention decay issues
- **Failure signatures**: Routing collapse (same experts always selected) → check MI loss effectiveness; Memory overflow → reduce expert count or k value; Performance degradation after pruning → verify load concentration loss effectiveness
- **First 3 experiments**: 1) Verify router produces valid probability distributions (sum to 1, non-negative); 2) Test stick-breaking attention extrapolation on sequences longer than training length; 3) Validate load balancing by measuring expert activation frequency histograms across a batch

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of ModuleFormer scale with the number of experts (modules) beyond what was tested in the paper?
  - Basis in paper: The paper mentions previous work on structured model pruning and notes that previous SMoE-based large language models have been trained with load balancing losses which forces the model to use every expert
  - Why unresolved: The paper does not provide empirical results on how the performance of ModuleFormer scales with the number of experts beyond what was tested
  - What evidence would resolve it: Experiments testing the performance of ModuleFormer with varying numbers of experts, particularly focusing on how performance changes as the number of experts increases beyond the tested range

- **Open Question 2**: How does the stick-breaking attention mechanism perform on tasks that require attending to distant tokens, compared to standard attention mechanisms?
  - Basis in paper: The paper states that stick-breaking attention does not have the same issue as ALiBi, which biases toward recent time steps
  - Why unresolved: The paper does not provide empirical results comparing the performance of stick-breaking attention to standard attention on tasks that require attending to distant tokens
  - What evidence would resolve it: Experiments comparing the performance of ModuleFormer with stick-breaking attention to a version with standard attention on tasks that require attending to distant tokens

- **Open Question 3**: How does the performance of ModuleFormer compare to other modular architectures, such as Neural Module Networks (NMN) and DEMIX?
  - Basis in paper: The paper mentions that previous work has proposed several ways to introduce modularity to neural network models and provides examples such as Neural Module Networks for visual question-answering tasks and DEMIX
  - Why unresolved: The paper does not provide empirical results comparing the performance of ModuleFormer to other modular architectures
  - What evidence would resolve it: Experiments comparing the performance of ModuleFormer to other modular architectures on relevant tasks

## Limitations
- Evaluation primarily focuses on synthetic and public benchmarks with limited real-world deployment data
- Catastrophic forgetting resistance demonstrated only on domain-shifted data from CC-100, not diverse production scenarios
- Pruning experiments rely on activation frequency as proxy for task relevance, which may not capture all aspects of expert utility

## Confidence
- **Architecture Innovation Claims**: High
- **Efficiency Improvements**: Medium
- **Continual Learning Benefits**: Medium
- **Task-Specific Pruning**: Medium

## Next Checks
1. **Routing Stability Test**: Monitor expert activation patterns across training epochs to verify that load balancing loss prevents routing collapse. Specifically, track the entropy of expert selection distributions per token type and ensure it remains above a stable threshold throughout training.

2. **Long Sequence Extrapolation**: Evaluate stick-breaking attention on sequences 2-4x longer than training length to empirically validate the position-agnostic attention claim. Measure attention weight concentration on distant tokens and compare with standard positional encoding methods.

3. **Cross-Domain Pruning Validation**: After task-specific pruning using load concentration loss, retrain the pruned model on a different task from the same domain to verify that removed experts were truly task-specific rather than domain-specific. This would validate the granularity of the pruning approach.