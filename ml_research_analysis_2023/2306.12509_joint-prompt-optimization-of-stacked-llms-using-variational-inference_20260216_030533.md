---
ver: rpa2
title: Joint Prompt Optimization of Stacked LLMs using Variational Inference
arxiv_id: '2306.12509'
source_url: https://arxiv.org/abs/2306.12509
tags:
- prompt
- layer
- language
- prompts
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Deep Language Networks (DLNs), a novel method
  for optimizing prompts in large language models (LLMs) by treating them as layers
  in a network. The key idea is to use variational inference to jointly optimize prompts
  for multiple LLM layers, where the output of one layer is used as input for the
  next.
---

# Joint Prompt Optimization of Stacked LLMs using Variational Inference

## Quick Facts
- arXiv ID: 2306.12509
- Source URL: https://arxiv.org/abs/2306.12509
- Reference count: 40
- One-line primary result: DLN-2 outperforms single-layer approaches and matches GPT-4 performance on reasoning tasks using smaller LLMs

## Executive Summary
This paper introduces Deep Language Networks (DLNs), a method for jointly optimizing prompts across stacked LLM layers using variational inference. By treating the output of the first layer as a latent variable, DLNs can decompose complex tasks into simpler subtasks, each handled by a smaller LLM. The authors demonstrate that their 2-layer approach (DLN-2) outperforms single-layer methods and sometimes matches GPT-4 performance on reasoning and natural language understanding tasks, even when using smaller, less powerful LLMs.

## Method Summary
The authors propose a framework where prompts for stacked LLM layers are jointly optimized using variational inference. In the 2-layer case (DLN-2), the output of the first LLM layer is treated as a latent variable, and variational inference is used to maximize the evidence lower bound (ELBO) by optimizing both prompts jointly. Prompts are optimized through local search using an LLM to generate candidate prompts conditioned on current prompts, input-output examples, and model predictions. The method generalizes chain-of-thought prompting by learning the intermediate reasoning prompts rather than using fixed ones.

## Key Results
- DLN-2 consistently outperforms DLN-1 (single layer) across reasoning and natural language understanding tasks
- On Hyperbaton task: DLN-1 achieved 91.9% accuracy vs 65.8% for 0-shot GPT-3 and 85.4% for GPT-4 with in-context learning
- DLN-2 matches or exceeds GPT-4 performance on several tasks while using smaller, less powerful LLMs
- The 2-layer architecture shows particular strength on tasks requiring complex reasoning decomposition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint optimization of prompts across layers enables decomposition of complex tasks into simpler subtasks.
- Mechanism: The 2-layer architecture treats the output of the first layer as a latent variable. Variational inference is used to optimize both prompts jointly by maximizing the evidence lower bound (ELBO), allowing each layer to specialize in a simpler subtask.
- Core assumption: The latent representation can capture sufficient information to decompose the task, and variational inference can effectively learn the prompts to maximize performance.
- Evidence anchors:
  - [abstract]: "We consider the output of the first layer as a latent variable to marginalize, and devise a variational inference algorithm for joint prompt training."
  - [section 3]: "We consider h as a latent variable... To maximize the marginal log-likelihood, we formulate a variational inference algorithm that uses an approximate posterior over h."
- Break condition: If the latent representation cannot effectively decompose the task, or if variational inference fails to find good prompts, performance will degrade.

### Mechanism 2
- Claim: Prompt optimization via local search using LLM-generated candidates improves prompt quality beyond zero-shot and in-context learning.
- Mechanism: The prompt proposal distribution uses an LLM to generate candidate prompts by conditioning on the current prompt, input-output examples, and model predictions. The best candidate is selected based on log-likelihood of the target given the prompt.
- Core assumption: The LLM can generate meaningful variations of the prompt that improve performance, and the log-likelihood scoring function is a good proxy for prompt quality.
- Evidence anchors:
  - [section 2.2]: "Our prompt proposal distribution takes as conditioning information a/ the batch given as input to the layer, b/ its corresponding output {x, y, ŷ}, and c/ the current prompt π."
  - [section 2.2]: "we rank the candidate prompts to maximize data log-likelihood π = arg maxπn log pLM(y|F(x; πn))."
- Break condition: If the LLM cannot generate useful prompt variations, or if the scoring function does not correlate with actual performance, the optimization will fail.

### Mechanism 3
- Claim: Posterior sharpening improves the quality of the approximate posterior distribution over the latent variable.
- Mechanism: After sampling hidden states from the proposal distribution, each sample is reweighted based on its probability under the true posterior (combining prior and likelihood). This focuses the search on more promising hidden states.
- Core assumption: The reweighting based on the true posterior probability improves the quality of the approximate posterior, leading to better prompt optimization.
- Evidence anchors:
  - [section 3]: "we reweigh each sample hi based on its probability under the true posterior distribution. More precisely, we compute w̃i = log pLM(y|Fr(hi, x, π1)) + logpLM(hi|F(x, π0)), then assign to each hi the probability wi = exp(α w̃i)/P j exp(α w̃j ), where α is a tunable temperature parameter."
- Break condition: If the posterior sharpening does not effectively focus on better hidden states, or if the temperature parameter is not well-tuned, the optimization may not improve.

## Foundational Learning

- Concept: Variational Inference
  - Why needed here: Used to optimize prompts in DLN-2 by maximizing the ELBO, which involves an intractable marginal likelihood. Variational inference provides a tractable lower bound.
  - Quick check question: What is the relationship between the ELBO and the KL divergence in variational inference?

- Concept: In-Context Learning (ICL)
  - Why needed here: ICL is a baseline method for prompting LLMs, and DLN-1 aims to improve upon it by learning prompts that can incorporate examples effectively.
  - Quick check question: How does ICL differ from zero-shot prompting, and what are its limitations?

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: CoT is a special case of DLN-2 with fixed prompts, and DLN-2 generalizes it by learning the prompts.
  - Quick check question: How does CoT improve reasoning performance in LLMs, and how does DLN-2 build upon this idea?

## Architecture Onboarding

- Component map:
  Input layer (x) -> Template layer (F() or Fr()) -> LLM layer -> (for DLN-2) Hidden layer (h) -> Prompt proposal layer -> Scoring layer -> Posterior sharpening

- Critical path:
  1. Input x is combined with prompt π using template F() or Fr()
  2. LLM generates output y (or h for hidden layer)
  3. (For DLN-2) Hidden states are sampled from the approximate posterior
  4. Samples are reweighted using posterior sharpening
  5. Candidate prompts are generated using the backward template
  6. Prompts are scored based on log-likelihood
  7. Best prompt is selected and used in the next iteration

- Design tradeoffs:
  - Template complexity vs. flexibility: More complex templates allow for richer prompt structures but may be harder to optimize.
  - Number of prompt candidates vs. computational cost: Generating more candidates increases the chance of finding a good prompt but also increases computational cost.
  - Temperature parameter in posterior sharpening: Higher temperatures encourage more exploration but may lead to noisier optimization.

- Failure signatures:
  - DLN-1: If the learned prompt does not significantly outperform zero-shot and ICL baselines, the prompt optimization may not be effective.
  - DLN-2: If the performance does not improve over DLN-1, the latent variable may not be capturing useful information, or the variational inference may not be optimizing the prompts well.

- First 3 experiments:
  1. Implement and test DLN-1 on a simple classification task (e.g., sentiment analysis) and compare its performance to zero-shot and ICL baselines.
  2. Implement DLN-2 on a task that benefits from reasoning decomposition (e.g., spatial reasoning) and compare its performance to DLN-1 and GPT-4 baselines.
  3. Analyze the learned prompts and hidden states to understand how the model is decomposing the task and what information is being captured.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Deep Language Networks (DLNs) scale with the number of layers?
- Basis in paper: [explicit] The paper mentions that they only tested 1-layer and 2-layer language networks, but the framework accommodates arbitrary directed acyclic graphs.
- Why unresolved: The paper does not provide experimental results for DLNs with more than two layers, leaving the question of scalability open.
- What evidence would resolve it: Experimental results comparing the performance of DLNs with varying numbers of layers on the same tasks would provide insight into how performance scales with depth.

### Open Question 2
- Question: Can the prompts for the backward templates (Bπ and Bh) be learned instead of engineered?
- Basis in paper: [explicit] The paper mentions that the backward templates are engineered and suggests that learning parts of such templates could make the variational bound tighter and ease DLN's optimization.
- Why unresolved: The paper does not explore the possibility of learning the backward templates, leaving the question of whether this approach would improve performance open.
- What evidence would resolve it: Experimental results comparing the performance of DLNs with engineered backward templates to those with learned backward templates would provide insight into the effectiveness of learning these templates.

### Open Question 3
- Question: How do different combinations of LLMs affect the performance of DLNs?
- Basis in paper: [explicit] The paper mentions that they only used DaVinci-003 as the backbone for their experiments and suggests that testing with other LLMs or combinations thereof is needed.
- Why unresolved: The paper does not explore the effect of using different LLMs or combinations of LLMs in DLNs, leaving the question of how this affects performance open.
- What evidence would resolve it: Experimental results comparing the performance of DLNs using different LLMs or combinations of LLMs would provide insight into the effect of LLM choice on DLN performance.

## Limitations
- The backward template for generating candidate prompts is only described at a high level, making exact reproduction difficult
- The temperature parameter α in posterior sharpening is tunable but not optimized, which could significantly impact performance
- The paper shows results on specific tasks but doesn't provide ablation studies isolating the contribution of each component

## Confidence
- **High Confidence**: The overall approach of treating LLM outputs as latent variables and using variational inference for joint prompt optimization is methodologically sound.
- **Medium Confidence**: The empirical results showing DLN-2 outperforming DLN-1 and matching GPT-4 on some tasks appear robust, but the specific contribution of each algorithmic component is not clearly isolated.
- **Low Confidence**: The claim that this method can effectively decompose complex tasks into simpler subtasks relies heavily on the assumption that the latent representation captures sufficient task-relevant information.

## Next Checks
1. **Ablation Study**: Implement DLN-2 variants with posterior sharpening disabled, with random prompt sampling instead of variational inference, and with fixed prompts (equivalent to CoT). Compare performance to isolate the contribution of each component.

2. **Prompt Analysis**: After training, analyze the learned prompts for both layers across different tasks. Extract common patterns, template structures, and how they differ from zero-shot and few-shot prompts. This would validate whether the prompts are capturing task decomposition strategies.

3. **Hidden State Interpretability**: For DLN-2, analyze the distribution of hidden states across the training data. Compute mutual information between hidden states and final outputs to verify that the latent variable captures task-relevant information. Visualize hidden states for a few examples to understand what intermediate reasoning is being captured.