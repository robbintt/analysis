---
ver: rpa2
title: A Corpus for Sentence-level Subjectivity Detection on English News Articles
arxiv_id: '2305.18034'
source_url: https://arxiv.org/abs/2305.18034
tags:
- sentence
- subjectivity
- language
- corpus
- subjective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present a novel corpus for subjectivity detection at
  the sentence level in English news articles, annotated following language-agnostic
  guidelines. They develop a prescriptive approach to mitigate annotation conflicts
  and evaluate state-of-the-art multilingual transformer-based models on the task,
  both in mono- and cross-lingual settings.
---

# A Corpus for Sentence-level Subjectivity Detection on English News Articles

## Quick Facts
- arXiv ID: 2305.18034
- Source URL: https://arxiv.org/abs/2305.18034
- Reference count: 24
- Authors present a novel corpus for subjectivity detection with language-agnostic guidelines and evaluate multilingual transformer models

## Executive Summary
This paper introduces a novel corpus for sentence-level subjectivity detection in English news articles, containing 1,049 sentences manually labeled as objective (638) or subjective (411). The authors develop prescriptive annotation guidelines to mitigate conflicts and evaluate state-of-the-art multilingual transformer models in mono-, multi-, and cross-lingual settings. The corpus demonstrates improved inter-annotator agreement from 0.51 to 0.83 after applying the prescriptive approach, and models trained in multilingual settings achieve the best performance, with SBERT showing comparable results to monolingual counterparts in cross-lingual settings.

## Method Summary
The authors created a corpus of 1,049 sentences from 23 English news articles covering controversial political topics. Seven annotators applied prescriptive language-agnostic guidelines to label sentences as objective or subjective, with each sentence labeled by at least two annotators. They evaluated four models (SVM, Logistic Regression, M-BERT, M-SBERT) using majority voting for label aggregation, fine-tuning transformer-based models for 4 epochs. Performance was measured using macro F1-score and class-specific F1-scores, averaged over three seed runs.

## Key Results
- Inter-annotator agreement improved from 0.51 to 0.83 after applying prescriptive guidelines
- Multilingual models achieved best performance with F1-macro of 0.80
- Cross-lingual transfer with Italian data improved English performance from 0.75 to 0.80 F1-macro
- SBERT achieved comparable performance to its monolingual counterpart in cross-lingual setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-agnostic annotation guidelines reduce reliance on language-specific resources like lexicons.
- Mechanism: By defining subjectivity without language-specific cues, the corpus can be built in any language without requiring domain-specific tools like machine translation or sentiment lexicons.
- Core assumption: Subjectivity detection depends primarily on the structure and meaning of the sentence, not on language-specific markers.
- Evidence anchors:
  - [abstract]: "which are not limited to language-specific cues"
  - [section 2]: "However, such approaches face demanding issues like annotation ambiguity... and ambiguous case resolution."
  - [corpus]: Weak evidence; the paper claims this but does not empirically validate the absence of language-specific cues.
- Break condition: If subjectivity in some languages inherently relies on language-specific markers (e.g., politeness markers in Japanese), the guidelines may fail.

### Mechanism 2
- Claim: Prescriptive annotation guidelines reduce inter-annotator disagreement.
- Mechanism: By providing clear definitions and resolving ambiguous cases through discussion, the guidelines create consistent interpretation across annotators.
- Core assumption: Subjectivity is subjective, but clear guidelines can create shared understanding.
- Evidence anchors:
  - [abstract]: "design an annotation procedure based on the prescriptive paradigm... to mitigate annotation conflicts by discussing and resolving controversial cases"
  - [section 3]: "we agree on labeling without context" and "the IAA is 0.51... after step (i) and 0.83... after step (ii)"
  - [corpus]: Strong evidence; the paper reports improvement in IAA from 0.51 to 0.83 after applying the prescriptive approach.
- Break condition: If annotators' backgrounds are too diverse (e.g., cultural differences in perceiving subjectivity), even prescriptive guidelines may not achieve high agreement.

### Mechanism 3
- Claim: Multilingual training improves model performance on the target corpus.
- Mechanism: Training on data from multiple languages helps the model learn more robust representations of subjectivity that generalize better to the target language.
- Core assumption: Subjectivity patterns are transferable across languages.
- Evidence anchors:
  - [abstract]: "models trained in the multilingual setting achieve the best performance on the task"
  - [section 5]: "we observe that transformer-based models achieve the best performance on our corpus when trained in the multilingual setting"
  - [corpus]: Moderate evidence; the paper shows improvement from 0.75 to 0.80 F1-macro when adding Italian data.
- Break condition: If the target language is too distant from the source languages (e.g., English and Chinese), the transfer may not be effective.

## Foundational Learning

- Concept: Subjectivity vs. Objectivity
  - Why needed here: The entire task is to distinguish between subjective and objective sentences.
  - Quick check question: What makes a sentence subjective according to the paper's guidelines?
- Concept: Inter-Annotator Agreement (IAA)
  - Why needed here: IAA measures the consistency of annotations, which is crucial for corpus quality.
  - Quick check question: What was the IAA before and after applying the prescriptive approach?
- Concept: Cross-Lingual Transfer Learning
  - Why needed here: The paper evaluates models in cross-lingual settings to test the generalizability of the guidelines.
  - Quick check question: What is the difference between multilingual and cross-lingual settings?

## Architecture Onboarding

- Component map:
  - Data collection: Articles from 8 English outlets -> Annotation: 7 annotators using prescriptive guidelines -> Models: SVM, LR, M-SBERT, M-BERT -> Evaluation: F1-macro, F1-OBJ, F1-SUBJ
- Critical path:
  1. Collect articles on controversial political topics
  2. Apply prescriptive annotation guidelines
  3. Train models on the corpus
  4. Evaluate in mono-, multi-, and cross-lingual settings
- Design tradeoffs:
  - Language-agnostic vs. language-specific approaches: Language-agnostic allows for cross-lingual transfer but may miss language-specific cues.
  - Prescriptive vs. descriptive guidelines: Prescriptive reduces disagreement but may not capture all nuances of subjectivity.
- Failure signatures:
  - Low IAA: Guidelines may be unclear or annotators' backgrounds too diverse.
  - Poor cross-lingual performance: Subjectivity patterns may not be transferable across languages.
- First 3 experiments:
  1. Train a baseline SVM on the English corpus and evaluate IAA.
  2. Apply prescriptive guidelines and measure IAA improvement.
  3. Train M-BERT on English corpus and evaluate in monolingual setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features or patterns in news articles are most predictive of subjectivity, and how do these vary across different news domains (e.g., politics vs. lifestyle)?
- Basis in paper: [inferred] The paper discusses evaluating models for subjectivity detection but does not provide a detailed analysis of which linguistic features are most indicative of subjectivity across different domains.
- Why unresolved: The paper focuses on model performance rather than analyzing the linguistic characteristics that contribute to subjectivity detection. Understanding these features could improve model design and interpretability.
- What evidence would resolve it: A detailed linguistic analysis comparing subjective and objective sentences across various news domains, identifying common features like modal verbs, evaluative adjectives, or rhetorical devices.

### Open Question 2
- Question: How does the performance of subjectivity detection models degrade when applied to news articles from different time periods, given the evolution of language and communication styles?
- Basis in paper: [explicit] The paper mentions that "the rapid advancement of technology over the last few decades has impacted language and its evolution and also the communication methods used by news media."
- Why unresolved: The paper does not test models on historical news data, leaving the question of temporal robustness unanswered. This is important for understanding the generalizability of models across different eras.
- What evidence would resolve it: Experiments training and testing models on news articles from different decades to measure performance changes over time.

### Open Question 3
- Question: To what extent does the presence of subjective sentences in news articles influence reader perception and trust in the information presented?
- Basis in paper: [inferred] While the paper focuses on detecting subjectivity, it does not explore the impact of subjective content on readers, which is a significant aspect of media studies and information literacy.
- Why unresolved: The paper's scope is limited to the technical aspects of subjectivity detection without considering the broader implications for media consumption and public trust.
- What evidence would resolve it: User studies or surveys measuring how exposure to subjective vs. objective news content affects reader perceptions, trust, and comprehension.

## Limitations
- Small corpus size (1,049 sentences) limits generalizability and robust validation
- Focus on politically controversial topics may not generalize to other news domains
- Limited empirical validation of language-agnostic guidelines across different languages

## Confidence
- Language-agnostic guidelines effectiveness: Medium
- Model performance claims: Medium
- Generalizability to other languages and domains: Low

## Next Checks
1. Test the annotation guidelines on a second corpus of English news articles from different domains to assess guideline robustness
2. Evaluate model performance on a larger dataset (minimum 5,000 sentences) to confirm the reported improvements hold at scale
3. Conduct cross-lingual validation by applying the English-trained models to a parallel corpus in another language to verify the claimed transferability