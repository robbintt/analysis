---
ver: rpa2
title: 'The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference
  Perspective'
arxiv_id: '2312.15524'
source_url: https://arxiv.org/abs/2312.15524
tags:
- price
- product
- purchase
- prompt
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LLMs can simulate human behavior for experiments, but a key challenge
  arises: when simulating between-subject experiments, LLM-simulated subjects being
  unaware of the experimental design leads to confounding. Variations in the treatment
  (e.g., price) systematically affect unspecified variables that should remain constant
  (e.g., competitor prices), violating the unconfoundedness assumption and yielding
  implausible results (e.g., extremely flat demand curves).'
---

# The Challenge of Using LLMs to Simulate Human Behavior: A Causal Inference Perspective

## Quick Facts
- arXiv ID: 2312.15524
- Source URL: https://arxiv.org/abs/2312.15524
- Reference count: 10
- Key outcome: LLM-simulated experiments suffer from endogeneity because treatment variation systematically affects unspecified variables, violating unconfoundedness assumptions and yielding implausible results like extremely flat demand curves.

## Executive Summary
This paper identifies a fundamental challenge in using large language models (LLMs) to simulate human behavior for causal inference experiments: endogeneity arising from ambiguous prompting strategies. When simulating between-subject experiments, LLM-simulated subjects being unaware of the experimental design leads to confounding, where variations in treatment (e.g., price) systematically affect unspecified variables that should remain constant (e.g., competitor prices). This violates the unconfoundedness assumption and produces implausible results such as extremely flat demand curves. The paper proposes unambiguous prompting through "unblinding" - explicitly revealing the experimental design to the LLM (e.g., stating that price is randomly set by the store) - as a solution that consistently improves model performance across reasoning and non-reasoning models.

## Method Summary
The paper uses the Open AI API with GPT-4 (gpt4-0613 version) to simulate customer purchase decisions for 40 product categories from Nielsen data. The methodology involves creating system prompts that define customer characteristics and user prompts that vary treatment levels (primarily price from $2 to $8 in $2 increments) while asking purchase likelihood. Multiple prompting strategies are tested: basic price variation, adding competitor price controls, and explicitly stating experimental design. Temperature settings of 0 and 1 are used for deterministic and probabilistic responses respectively, with 500 draws for temperature=1. The paper compares price elasticity calculations and correlation between focal price and competing prices across different prompting approaches.

## Key Results
- Basic LLM prompting produces implausibly flat demand curves with near-zero price elasticity (~0.14)
- Explicitly stating that price variation is due to random experimentation significantly improves price elasticity estimates
- Unambiguous prompting strategies enhance model performance consistently across both reasoning and non-reasoning models
- Fine-tuning can improve simulation performance, but unambiguous prompting makes predictions robust to irrelevant data in fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unambiguous prompting (e.g., explicitly stating that price variation is due to random experimentation) reduces endogeneity by preventing the LLM from inferring systematic relationships between treatment and unspecified covariates.
- Mechanism: By specifying the source of treatment variation, the LLM's attention is directed away from potential confounding associations in the training data and toward a clean experimental interpretation.
- Core assumption: The LLM can parse and act on explicit instructions about experimental design, treating such information as a constraint rather than a latent signal to be inferred.
- Evidence anchors: [abstract] "This endogeneity issue stems from ambiguous prompting strategies... can be addressed by developing unambiguous prompting strategies through unblinding" [section 4] "specifying the source of variation is driven by experiments... reduces unintended inferences about characteristics meant to stay constant"
- Break condition: If the LLM fails to understand randomization, the prompt still leaves room for unintended associations, as evidenced by the imperfect flatness of the competing price curve in Figure 8.

### Mechanism 2
- Claim: Controlling for covariates (e.g., specifying competitor prices) can reduce endogeneity by holding relevant factors constant in the prompt.
- Mechanism: Adding detailed contextual information constrains the LLM's generation process to a subset of the training data where those covariates match the specified values, thus isolating the causal effect of the treatment.
- Core assumption: The LLM treats specified covariate values as fixed constraints rather than as cues for additional associations, and the researcher correctly identifies all relevant confounders.
- Evidence anchors: [section 3] "adding detailed information about individuals and their environment is analogous to controlling for covariates" [section 3] "adding more covariates has the potential to generate more realistic simulations" (with Figure 4 showing improvement)
- Break condition: If the researcher omits relevant confounders or the LLM infers additional associations from the added detail, the conditional independence assumption is violated, leading to biased estimates.

### Mechanism 3
- Claim: LLM-simulated individuals and environments are "constructed on the fly" based on the entire prompt, including the treatment, leading to endogeneity when treatment variation implies changes in unspecified variables.
- Mechanism: Because the LLM generates responses conditioned on the full prompt context, any variation in the treatment description can alter the latent state of the simulated individual or environment, introducing confounding factors not intended by the researcher.
- Core assumption: The LLM's generation process treats the prompt as a holistic description of the scenario, with all elements (including treatment) influencing the output distribution.
- Evidence anchors: [abstract] "simulated individuals and environments are 'constructed on the fly' based on the entire prompt, which includes the treatment" [section 2] "variations in the price listed in the prompt might cause variations in other unspecified confounding factors"
- Break condition: If the LLM's generation process were to treat treatment as orthogonal to other aspects of the simulated scenario, this endogeneity would not arise.

## Foundational Learning

- Concept: Causal inference and the unconfoundedness assumption
  - Why needed here: The paper's core argument is that LLM simulations suffer from endogeneity because variations in treatment systematically affect unspecified covariates, violating the assumption that treatment assignment is independent of potential outcomes conditional on observed covariates.
  - Quick check question: What is the unconfoundedness assumption, and why is it important for causal inference in experiments?

- Concept: Large language model prompting and generation
  - Why needed here: Understanding how LLMs generate responses based on the entire prompt is crucial for grasping why treatment variation can inadvertently affect unspecified variables in the simulation.
  - Quick check question: How does an LLM's generation process differ from a lookup in a fixed dataset, and what implications does this have for causal inference?

- Concept: Between-subject vs. within-subject experimental design
  - Why needed here: The paper discusses both types of designs and their challenges in the LLM context, highlighting how the order of treatments can introduce bias in within-subject designs.
  - Quick check question: What is the key difference between between-subject and within-subject experimental designs, and how might this difference affect the interpretation of results in LLM simulations?

## Architecture Onboarding

- Component map: Prompt construction -> LLM model -> Response analysis -> Experimental design specification
- Critical path: 1. Construct a prompt specifying the experimental scenario. 2. Submit the prompt to the LLM. 3. Receive and record the LLM's response. 4. Analyze the response to estimate the causal effect. 5. Repeat for different treatment levels or experimental designs.
- Design tradeoffs:
  - Unambiguous prompting vs. ecological validity: Explicitly stating the experimental design may reduce confounding but also differs from standard human experiments where participants are unaware of the design.
  - Controlling for covariates vs. prompt complexity: Adding detailed context can reduce endogeneity but may trigger unintended associations or make prompts unwieldy.
  - Between-subject vs. within-subject design: Between-subject designs avoid carryover effects but may require more prompts; within-subject designs are more efficient but can be sensitive to order effects.
- Failure signatures:
  - Implausibly flat or steep demand curves, suggesting that the LLM is not capturing the intended causal relationship.
  - Systematic correlation between treatment and unspecified covariates (e.g., competitor prices changing with focal product price).
  - Sensitivity of results to experimental design details (e.g., range of price variation).
- First 3 experiments:
  1. Replicate the basic demand estimation experiment with a simple prompt varying only the focal product price.
  2. Add control for competitor prices to the prompt and compare results to the basic experiment.
  3. Explicitly state that price variation is due to random experimentation and compare results to the previous experiments.

## Open Questions the Paper Calls Out

- Question: What specific features or techniques could be developed to enable LLMs to simulate realistic experimental settings where simulated participants are unaware they are in an experiment, thus reducing potential demand effects?
- Question: How can researchers determine the optimal level of detail to include in prompts for LLM simulations to control for confounders without introducing unintended associations or backfiring effects?
- Question: What are the broader implications of LLM simulations for causal inference in other domains beyond demand estimation, and how might the challenges identified in this paper generalize to these different contexts?

## Limitations

- The empirical validation is limited to a single product category (Coca-Cola) and relies on a specific LLM model (GPT-4), raising questions about generalizability.
- The proposed solutions may trade off ecological validity for methodological rigor, as revealing experimental design to simulated subjects differs from standard human experiments.
- Mechanism explanations for why unambiguous prompting works are plausible but not rigorously tested through direct experimental validation.

## Confidence

- High confidence: The identification of endogeneity as a fundamental challenge in LLM-based causal inference simulations. The empirical demonstration that basic prompting leads to implausible demand curves is clear and reproducible.
- Medium confidence: The proposed solutions (unambiguous prompting and covariate control) work as described for the tested scenarios. While results are consistent across experiments, the underlying mechanisms could benefit from more direct testing.
- Low confidence: Claims about LLM generation mechanisms being the root cause of endogeneity. The paper asserts that LLMs construct scenarios "on the fly" based on the entire prompt, but this is inferred rather than directly observed.

## Next Checks

1. Test unambiguous prompting across multiple product categories and price ranges to verify generalizability beyond Coca-Cola and the $2-8 price range.

2. Conduct ablation studies on prompt components to isolate which aspects of unambiguous prompting (e.g., explicit randomization statement vs. overall prompt clarity) drive performance improvements.

3. Compare results when using different LLM models (e.g., Claude, LLaMA) to determine whether the proposed solutions are model-agnostic or specific to GPT-4's architecture and training.