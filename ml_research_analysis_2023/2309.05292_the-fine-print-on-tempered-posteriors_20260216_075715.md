---
ver: rpa2
title: The fine print on tempered posteriors
arxiv_id: '2309.05292'
source_url: https://arxiv.org/abs/2309.05292
tags:
- posterior
- test
- prior
- loss
- posteriors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates tempered posteriors and challenges prior
  assumptions about their benefits. Using the Laplace approximation for analytical
  tractability, it shows that stochasticity generally does not improve test accuracy
  - the coldest temperature (deterministic network) is often optimal.
---

# The fine print on tempered posteriors

## Quick Facts
- arXiv ID: 2309.05292
- Source URL: https://arxiv.org/abs/2309.05292
- Reference count: 8
- Key outcome: Using Laplace approximation, this paper shows that stochasticity generally does not improve test accuracy - the coldest temperature (deterministic network) is often optimal.

## Executive Summary
This paper challenges the common belief that tempering posteriors in Bayesian neural networks improves performance. Through analytical and empirical investigation using the Laplace approximation, it demonstrates that colder temperatures (approaching deterministic networks) typically yield higher test accuracy. The work reveals a fundamental tradeoff between calibration metrics like ECE and test accuracy, and argues that tempering is needed primarily due to the mismatch between Bayesian guarantees and Frequentist evaluation metrics rather than fixing misspecified likelihoods.

## Method Summary
The paper uses the Laplace approximation for analytical tractability in studying tempered posteriors. The method involves training WideResNet22 models to find MAP estimates using SGD, then fitting KFAC Laplace approximations to these MAP estimates. The posterior predictive is evaluated on test sets using Monte Carlo sampling across different temperature values. PAC-Bayes bounds are computed for comparison. The approach is validated on CIFAR-10, CIFAR-100, SVHN, and FashionMnist datasets, measuring test accuracy, negative log-likelihood, and Expected Calibration Error.

## Key Results
- For fixed datasets and architectures, the coldest temperature (λ → ∞) typically yields the highest test accuracy
- Improving calibration metrics like ECE through tempering comes at the cost of reduced test accuracy
- The need for tempering stems from the mismatch between Bayesian guarantees and Frequentist evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Laplace approximation shows that for fixed datasets and model architectures, the coldest temperature (λ → ∞) typically yields the highest test accuracy, contrary to prior beliefs that some stochasticity improves performance.
- Mechanism: As temperature λ increases, the posterior distribution concentrates more sharply around the MAP estimate, reducing predictive variance. This deterministic behavior matches the optimal Frequentist 0-1 loss better than more diffuse posteriors.
- Core assumption: The Laplace approximation adequately captures the behavior of tempered posteriors for the examined architectures and datasets.
- Evidence anchors:
  - [abstract] "Using the Laplace approximation for analytical tractability, it shows that stochasticity generally does not improve test accuracy - the coldest temperature (deterministic network) is often optimal."
  - [section 4.4] "Empirically (for the Laplace case) the coldest temperature is almost always optimal in terms of test accuracy."
- Break condition: If the true posterior is highly non-Gaussian or the model is severely misspecified in ways not captured by the Laplace approximation, the mechanism may break.

### Mechanism 2
- Claim: Improving calibration metrics like ECE through tempering comes at the cost of reduced test accuracy.
- Mechanism: Lower temperatures reduce predictive variance, which can improve calibration (fewer overconfident predictions), but this reduction in variance also reduces the model's ability to capture true uncertainty, harming accuracy.
- Core assumption: The tradeoff between calibration and accuracy is inherent and unavoidable for the examined settings.
- Evidence anchors:
  - [abstract] "While some stochasticity can improve calibration metrics like ECE, this comes at the cost of reduced test accuracy."
  - [section 4.4] "In Figure 1, we see that in most of our experiments, we couldn't find such a temperature λ. Apart from FMNIST, there seems to be a clear tradeoff between test 0-1 loss and calibration error."
- Break condition: If a different architecture or loss function fundamentally changes the relationship between calibration and accuracy, this mechanism may not hold.

### Mechanism 3
- Claim: The need for tempering stems from the mismatch between Bayesian guarantees (posterior contraction) and Frequentist evaluation metrics, not from fixing misspecified likelihoods.
- Mechanism: Bayesian inference guarantees posterior contraction to the true parameters, but does not directly optimize Frequentist metrics like test accuracy. PAC-Bayes bounds naturally incorporate temperature parameters to provide high-probability guarantees on out-of-sample performance.
- Core assumption: Practitioners primarily care about Frequentist metrics, not posterior contraction.
- Evidence anchors:
  - [abstract] "The paper proposes that the need for tempering stems from the mismatch between Bayesian guarantees and Frequentist evaluation metrics, rather than from fixing misspecified likelihoods."
  - [section 5.2] "The ELBO objective is not directly related to this risk. However in the PAC-Bayesian literature, there exist bounds specifically adapted to it."
- Break condition: If the evaluation metric shifts to posterior contraction or if Bayesian guarantees become directly aligned with Frequentist metrics, this mechanism may break.

## Foundational Learning

- Concept: Laplace approximation for posterior inference
  - Why needed here: Provides analytical tractability for understanding tempered posteriors without resorting to computationally expensive MCMC methods
  - Quick check question: How does the Laplace approximation represent the posterior distribution, and what assumptions does it make about the shape of the posterior?

- Concept: PAC-Bayes generalization bounds
  - Why needed here: Provides a theoretical framework for understanding why temperature parameters are necessary when targeting Frequentist metrics
  - Quick check question: What role does the temperature parameter λ play in PAC-Bayes bounds, and how does it differ from its role in Bayesian inference?

- Concept: Expected Calibration Error (ECE)
  - Why needed here: Key metric for evaluating probabilistic predictions that may conflict with accuracy optimization
  - Quick check question: How is ECE calculated, and why might optimizing for ECE lead to different model behaviors than optimizing for accuracy?

## Architecture Onboarding

- Component map:
  MAP estimation -> Laplace approximation -> PAC-Bayes bound computation -> Evaluation

- Critical path:
  1. Find MAP estimates using SGD on training data
  2. Fit Laplace approximation using training data and prior
  3. Evaluate posterior predictive on test data for various λ values
  4. Compute PAC-Bayes bounds for comparison
  5. Analyze tradeoff between metrics

- Design tradeoffs:
  - Laplace approximation vs. MCMC: Trade computational efficiency for approximation accuracy
  - Temperature range: Wider range captures more behaviors but increases computational cost
  - Prior specification: Strong priors regularize but may bias results

- Failure signatures:
  - Numerical instability in matrix inversion for Laplace approximation
  - PAC-Bayes bounds becoming vacuous (too loose to be informative)
  - Unexpected optimal temperature values that don't align with theoretical predictions

- First 3 experiments:
  1. Replicate the CIFAR-10 results with Laplace approximation to verify the coldest temperature is optimal for accuracy
  2. Test the calibration vs accuracy tradeoff on a new dataset (e.g., SVHN) to confirm mechanism 2
  3. Compute PAC-Bayes bounds for the CIFAR-100 results to verify mechanism 3

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do PAC-Bayes bounds behave for deep neural networks when using more sophisticated posterior approximations beyond the Laplace approximation?
- Basis in paper: [explicit] The paper explicitly states that "PAC-Bayes objectives are difficult to analyze theoretically in non-convex cases" and uses Laplace approximation for analytical tractability.
- Why unresolved: The paper acknowledges that real-world Bayesian neural networks use more complex inference methods, but doesn't explore how these would affect PAC-Bayes bounds.
- What evidence would resolve it: Empirical studies comparing PAC-Bayes bounds across different inference methods (e.g., MCMC, variational inference with normalizing flows) on deep networks.

### Open Question 2
- Question: Can we develop principled methods to choose the optimal temperature parameter λ that balances accuracy and calibration metrics?
- Basis in paper: [explicit] The paper shows that "there is a clear tradeoff between test 0-1 loss and calibration error" and that different temperatures optimize different metrics.
- Why unresolved: While the paper demonstrates the existence of this tradeoff, it doesn't provide a framework for navigating it.
- What evidence would resolve it: Development and validation of methods that can automatically select λ based on desired accuracy-calibration tradeoffs, tested across multiple datasets and architectures.

### Open Question 3
- Question: How does the stochastic nature of MAP estimation in neural networks affect the choice of temperature parameter?
- Basis in paper: [explicit] The paper shows that "the same λ can imply different test risks based on each MAP estimates properties" and that "some fine-tuning of λ might be inevitable given the approximate and stochastic nature of inference in neural networks."
- Why unresolved: The paper demonstrates that different MAP estimates lead to different optimal temperatures, but doesn't explore the systematic relationship between inference stochasticity and temperature selection.
- What evidence would resolve it: Studies examining how different optimization procedures and random initializations affect optimal temperature choices, potentially leading to adaptive temperature selection methods.

## Limitations

- The primary limitation is the reliance on the Laplace approximation, which assumes Gaussian posteriors and may not capture the true behavior of tempered posteriors for highly non-linear neural networks
- The findings are specific to the examined architectures (WideResNet22) and datasets, limiting generalizability
- The analysis focuses on the 0-1 loss as the evaluation metric, while other metrics like AUC or F1-score might yield different conclusions

## Confidence

- High confidence: The claim that coldest temperature (λ → ∞) typically yields highest test accuracy for fixed datasets and architectures is well-supported by empirical results across multiple datasets
- Medium confidence: The tradeoff between calibration and accuracy is observed empirically but may be architecture-dependent and not universally applicable
- Low confidence: The assertion that tempering primarily addresses the mismatch between Bayesian and Frequentist metrics rather than fixing misspecified likelihoods requires further theoretical validation beyond the PAC-Bayes framework

## Next Checks

1. **Robustness across architectures**: Test the coldest temperature optimality on different network architectures (e.g., ResNet, DenseNet) to verify if the finding generalizes beyond WideResNet22
2. **Alternative evaluation metrics**: Evaluate the tradeoff between calibration and accuracy using different metrics (AUC, F1-score) to determine if the observed pattern holds under different performance measures
3. **Non-Laplace posterior approximations**: Compare results using MCMC-based posterior approximations to assess whether the Laplace approximation's assumptions affect the conclusions about temperature effects