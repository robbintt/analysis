---
ver: rpa2
title: 'FormaT5: Abstention and Examples for Conditional Table Formatting with Natural
  Language'
arxiv_id: '2310.17306'
source_url: https://arxiv.org/abs/2310.17306
tags:
- format5
- table
- rule
- examples
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FormaT5 is a transformer-based system that generates conditional
  formatting rules from natural language descriptions and tables. The key challenge
  is that user descriptions are often under-specified or ambiguous, making it difficult
  for code generation systems to learn the desired rule in a single step.
---

# FormaT5: Abstention and Examples for Conditional Table Formatting with Natural Language

## Quick Facts
- arXiv ID: 2310.17306
- Source URL: https://arxiv.org/abs/2310.17306
- Reference count: 40
- Primary result: FormaT5 outperforms 8 different neural approaches on conditional formatting benchmarks through abstention and example-based placeholder filling

## Executive Summary
FormaT5 is a transformer-based system that generates conditional formatting rules from natural language descriptions and tables. The key challenge addressed is that user descriptions are often under-specified or ambiguous, making it difficult for code generation systems to learn the desired rule in a single step. FormaT5 learns to predict placeholders through an abstention objective, which can then be filled by a second model or a programming-by-example system when examples are available. The system is evaluated on a benchmark of 1053 conditional formatting tasks containing real-world descriptions collected from four different sources.

## Method Summary
FormaT5 uses a T5 encoder-decoder architecture pre-trained on table-rule pairs using masked span prediction and table type prediction objectives. The model is then fine-tuned on (table, utterance, rule) triples with an abstention loss to predict placeholders for ambiguous arguments. When examples are available, the Cornet system fills these placeholders through programming-by-example. The approach addresses the challenge of under-specified natural language by generating rules with placeholders that can be resolved either by a second model or example-based filling.

## Key Results
- FormaT5 outperforms 8 different neural approaches on conditional formatting benchmarks
- Abstention and example-based filling improve performance both with and without examples
- The system achieves strong results on 1053 conditional formatting tasks with real-world descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Abstention loss teaches the model to insert placeholders when it cannot confidently predict arguments
- Mechanism: The abstention loss objective combines standard cross-entropy with a placeholder penalty, where the model learns to generate a special [?] token for ambiguous arguments instead of making incorrect predictions
- Core assumption: The model can learn to distinguish between confident and ambiguous predictions during training
- Evidence anchors:
  - [abstract]: "To tackle this problem of under-specification and minimise argument errors, FormaT5 learns to predict placeholders though an abstention objective."
  - [section]: "We employ the abstention loss objective introduced for standard multi-class classification in [47]."

### Mechanism 2
- Claim: Combining natural language with examples improves rule generation accuracy
- Mechanism: When examples are available, FormaT5 uses a programming-by-example system (Cornet) to fill placeholders, leveraging both the NL utterance and example rows to resolve ambiguity
- Core assumption: Examples provide sufficient information to disambiguate underspecified NL utterances
- Evidence anchors:
  - [abstract]: "These placeholders can then be filled by a second model or, when examples of rows that should be formatted are available, by a programming-by-example system."
  - [section]: "When examples are available, we leverage the Cornet system for learning CF rules by examples [46]."

### Mechanism 3
- Claim: Pre-training on table-rule pairs without NL improves downstream performance
- Mechanism: FormaT5 is first pre-trained on a large corpus of (table, rule) pairs using masked span prediction and table type prediction objectives, then fine-tuned on the smaller (table, rule, utterance) dataset
- Core assumption: The pre-training objectives capture useful table semantics that transfer to the NL-conditioned task
- Evidence anchors:
  - [abstract]: "We first pre-train on table and rule pairs and then fine-tune the model to incorporate natural language."
  - [section]: "We gather a large corpus of 410K table and rule pairs from a collection of public Excel worksheets."

## Foundational Learning

- Concept: Transformer encoder-decoder architecture
  - Why needed here: FormaT5 needs to generate code (rules) from input sequences (table, utterance), requiring a seq2seq model
  - Quick check question: Can you explain the difference between encoder-only, decoder-only, and encoder-decoder transformer architectures?

- Concept: Masked language modeling and span prediction
  - Why needed here: Pre-training uses masked span prediction to learn table and rule representations without NL supervision
  - Quick check question: How does masked span prediction differ from standard masked language modeling in BERT?

- Concept: Programming by example (PBE)
  - Why needed here: Cornet uses PBE to learn formatting rules from example rows when NL is underspecified
  - Quick check question: What are the key differences between inductive logic programming and programming by example?

## Architecture Onboarding

- Component map: T5 encoder-decoder base model -> Abstention loss layer during fine-tuning -> Constrained semantic decoding module -> Cornet PBE system for placeholder filling -> Pre-training objectives (MSP, code token tagging, table type prediction)

- Critical path: Table + utterance → T5 encoder → decoder with abstention → rule with placeholders → (if examples available) Cornet PBE → final rule

- Design tradeoffs:
  - Abstention vs direct prediction: Abstention reduces argument errors but adds complexity
  - Pre-training vs direct fine-tuning: Pre-training improves performance but requires more resources
  - NL-only vs NL+examples: NL+examples improves accuracy but requires user effort

- Failure signatures:
  - High abstention rate: NL utterances are too ambiguous or model confidence is low
  - Incorrect placeholder filling: Examples are noisy or Cornet fails to find matching predicates
  - Syntax errors in generated rules: Constrained decoding is not properly configured

- First 3 experiments:
  1. Train FormaT5 without abstention loss and measure argument error rate vs baseline
  2. Test placeholder filling accuracy with varying numbers of examples (0, 1, 2, 3)
  3. Evaluate pre-training impact by comparing performance with and without pre-training objectives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of FormaT5 scale when handling tables with multi-level headers or summary rows, which are common in real-world spreadsheets?
- Basis in paper: [explicit] The paper acknowledges that spreadsheets can contain complex structures like multi-level headers and summary rows, but states that extending FormaT5 to support these is left to future work.
- Why unresolved: The current FormaT5 implementation assumes a simplified table structure (columns described by headers and data types) and does not account for more complex spreadsheet layouts.
- What evidence would resolve it: Experiments comparing FormaT5's performance on benchmarks with complex table structures (multi-level headers, summary rows) versus simple tables, potentially requiring architectural modifications to handle such cases.

### Open Question 2
- Question: Can FormaT5 be adapted to support natural language utterances in languages other than English, and how would this affect its performance?
- Basis in paper: [explicit] The paper states that FormaT5 currently only supports English utterances and suggests that extending support to other languages is an area for future work.
- Why unresolved: The current model is trained and evaluated exclusively on English data, and there is no information on how it would perform with multilingual inputs or the challenges involved in such an extension.
- What evidence would resolve it: Training and evaluating FormaT5 on multilingual datasets, comparing performance across different languages, and analyzing any architectural changes needed for effective cross-lingual support.

### Open Question 3
- Question: How does FormaT5's performance change when dealing with rules that span multiple tables, and what strategies could be employed to handle such cases?
- Basis in paper: [explicit] The paper mentions that FormaT5 does not currently support rules spanning multiple tables and suggests potential strategies like predicting relevant tables, joining them, and applying FormaT5, but notes that such rules are rare in practice.
- Why unresolved: There is no empirical data on FormaT5's ability to handle multi-table scenarios, and the proposed strategies are speculative.
- What evidence would resolve it: Creating benchmarks with multi-table conditional formatting tasks, implementing and testing the proposed strategies, and measuring the impact on performance and usability.

## Limitations

- The system relies heavily on synthetically generated training data through bootstrapping with Codex and GPT-3, raising questions about real-world generalization
- The abstention mechanism assumes placeholder prediction is always preferable to incorrect predictions, which may not hold in all user scenarios
- The programming-by-example component's performance characteristics are not thoroughly analyzed, and the assumption that examples will be available and sufficient is untested

## Confidence

**High Confidence**: The overall system architecture and evaluation methodology are sound. The comparison against 8 different neural approaches and the use of multiple metrics (exact match, execution match, sketch match) provide robust evidence that FormaT5 outperforms baselines on the benchmark dataset.

**Medium Confidence**: The specific mechanisms (abstention loss, pre-training objectives, PBE integration) contribute positively to performance. While the paper demonstrates improvements, the ablation studies could be more comprehensive to isolate individual contribution effects.

**Low Confidence**: The system's real-world deployment readiness. The reliance on synthetic data generation and the assumption that users will provide examples when needed are significant limitations not fully addressed in the evaluation.

## Next Checks

1. **Synthetic Data Validation**: Conduct a user study where real users provide natural language formatting descriptions for tables, then compare FormaT5's performance on these genuine utterances versus the synthetically generated ones. Measure the degradation in performance and analyze the types of natural descriptions that cause failures.

2. **Abstention Analysis**: Systematically vary the abstention threshold during inference and measure the tradeoff between precision and coverage. Determine the optimal abstention rate that maximizes execution match accuracy while minimizing the number of placeholder-filled rules, providing practical guidance for deployment settings.

3. **Example Sufficiency Testing**: Design experiments with varying example quality and quantity to establish the minimum viable example set for reliable placeholder filling. Test with noisy examples, insufficient examples (only 1 instead of the typical 2-3), and contradictory examples to understand the robustness limits of the PBE integration.