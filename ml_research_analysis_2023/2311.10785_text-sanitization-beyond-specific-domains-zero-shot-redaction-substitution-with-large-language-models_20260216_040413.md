---
ver: rpa2
title: 'Text Sanitization Beyond Specific Domains: Zero-Shot Redaction & Substitution
  with Large Language Models'
arxiv_id: '2311.10785'
source_url: https://arxiv.org/abs/2311.10785
tags:
- text
- data
- privacy
- redaction
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a zero-shot text sanitization technique for
  detecting and substituting potentially sensitive information using Large Language
  Models (LLMs). The method estimates the probability of each word in a text using
  an LLM, redacts words below a specified privacy threshold, and substitutes redacted
  words with semantically close alternatives.
---

# Text Sanitization Beyond Specific Domains: Zero-Shot Redaction & Substitution with Large Language Models

## Quick Facts
- arXiv ID: 2311.10785
- Source URL: https://arxiv.org/abs/2311.10785
- Reference count: 37
- Key outcome: Zero-shot text sanitization using LLMs achieves high recall in detecting sensitive terms while preserving text coherence and downstream task performance

## Executive Summary
This paper introduces a zero-shot text sanitization method that leverages Large Language Models (LLMs) to detect and substitute potentially sensitive information without requiring domain-specific training. The approach estimates word probabilities using an LLM, redacts words below a specified privacy threshold, and substitutes them with semantically close alternatives to maintain contextual coherence. Evaluated on the Action-Based Conversation Dataset (ABCD), the method demonstrates strong performance in protecting privacy while preserving data utility for downstream tasks such as sentiment analysis and question-answering.

## Method Summary
The proposed method uses a pre-trained LLM to estimate the probability of each word in a given text context. Words with probabilities below a specified privacy threshold are flagged as sensitive and either redacted or substituted with semantically similar alternatives selected via word embedding similarity. The approach employs a zero-shot framework, eliminating the need for labeled training data or domain-specific customization. The substitution strategy uses random selection from top candidates to prevent deterministic recovery of sensitive information while maintaining semantic coherence.

## Key Results
- High recall in detecting sensitive terms while maintaining text coherence and contextual information
- Preserves performance in downstream tasks (sentiment analysis, topic embedding, question-answering) after sanitization
- Outperforms domain-specific models and approaches that rely on fixed word lists or heuristic rules

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-probability words in LLM context estimation indicate sensitive information.
- Mechanism: Words with low estimated probability by the LLM have high information content (IC = -log(probability)), meaning they are rare and thus more likely to be sensitive.
- Core assumption: The LLM's probability estimation accurately reflects word rarity in the context, and rarity correlates with sensitivity.
- Evidence anchors:
  - [abstract]: "Our approach uses a Large Language Model (LLM) to first estimate the probability of a word in the textual context, then it redacts words with a probability below a specified privacy threshold p."
  - [section]: "In information theory, the Information Content (IC) of an event e is a quantity derived from the probability of e occurring from a random variable. Given event e with probability Pe its IC is defined as IC (e) = − log(Pe)."
  - [corpus]: Weak. No direct corpus evidence of rarity-sensitivity correlation in this dataset.
- Break condition: If the LLM is trained on biased or non-representative data, low probability may not correlate with sensitivity (e.g., rare technical terms).

### Mechanism 2
- Claim: Semantic substitution using LLM embeddings preserves context coherence.
- Mechanism: For redacted words, the LLM generates semantically close alternatives by comparing word embeddings and selecting candidates with high cosine similarity.
- Core assumption: LLM embeddings capture semantic relationships accurately, and high cosine similarity ensures semantic coherence.
- Evidence anchors:
  - [abstract]: "Our evaluation shows that our method excels at protecting privacy while maintaining text coherence and contextual information, preserving data utility for downstream tasks."
  - [section]: "An embedding is a representation of the text in a high-dimensional vector space, where the semantic relationships between words or sentences correlate with the relationships in the vector space."
  - [corpus]: Weak. No direct corpus evidence of embedding quality or substitution coherence validation.
- Break condition: If embeddings don't capture the intended semantic nuance (e.g., polysemous words), substitutions may introduce errors.

### Mechanism 3
- Claim: Zero-shot approach avoids domain-specific training overhead.
- Mechanism: By relying on a pre-trained LLM, the system can perform text sanitization without requiring labeled training data or domain-specific customization.
- Core assumption: Pre-trained LLM knowledge generalizes sufficiently across domains for effective detection.
- Evidence anchors:
  - [abstract]: "Our evaluation shows that our method excels at protecting privacy while maintaining text coherence and contextual information, preserving data utility for downstream tasks."
  - [section]: "The generalization capabilities of LLM makes them well-suited for handling unstructured text data. Our method leverages pre-trained LLM, allowing us to perform zero-shot text sanitization."
  - [corpus]: Weak. No direct corpus evidence of zero-shot performance across diverse domains.
- Break condition: If the LLM's pre-training corpus lacks representation of the target domain, detection accuracy may suffer.

## Foundational Learning

- Concept: Information Content (IC) and its relationship to probability.
  - Why needed here: Understanding IC is critical to grasp why low-probability words are considered sensitive; it quantifies the information value of rare events.
  - Quick check question: If a word has a probability of 0.01 in context, what is its IC? (Answer: -log(0.01) ≈ 4.6 nats or 6.6 bits)

- Concept: Cosine similarity in high-dimensional vector spaces.
  - Why needed here: This metric is used to measure semantic similarity between word embeddings during substitution, ensuring contextually appropriate replacements.
  - Quick check question: If two word embeddings have a cosine similarity of 0.95, are they considered very similar, somewhat similar, or dissimilar? (Answer: Very similar)

- Concept: Tokenization and sub-word units in LLM processing.
  - Why needed here: Proper tokenization is essential for accurate probability estimation and embedding computation, especially for out-of-vocabulary words or compound terms.
  - Quick check question: Why might a word like "unbelievable" be split into sub-word tokens by an LLM tokenizer? (Answer: To reduce vocabulary size and handle rare or complex words efficiently)

## Architecture Onboarding

- Component map: Preprocessor -> Privacy Filter -> Substitution Engine -> Output Formatter
- Critical path: Preprocessor → Privacy Filter → Substitution Engine → Output Formatter
- Design tradeoffs:
  - Computational cost vs. privacy: Lower thresholds increase recall but require more LLM calls
  - Semantic accuracy vs. security: Random substitution selection prevents deterministic recovery but may reduce contextual fit
  - Zero-shot flexibility vs. domain specificity: Avoids training but may miss domain-specific sensitive terms
- Failure signatures:
  - Over-redaction: Many false positives (safe words flagged as sensitive)
  - Under-redaction: High false negatives (sensitive words missed)
  - Semantic drift: Substitutions alter intended meaning or introduce incoherence
  - Performance degradation: Downstream tasks fail due to excessive sanitization
- First 3 experiments:
  1. Run on ABCD dataset with varying thresholds (p=1E-25, p=1E-15, p=1E-5) to observe recall/precision tradeoff
  2. Compare sentiment analysis accuracy before/after sanitization with substitution vs. redaction-only
  3. Measure topic embedding cosine distance between original and sanitized text to quantify semantic preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do smaller language models compare to large language models in terms of computational cost and performance for text sanitization tasks?
- Basis in paper: [inferred] The authors mention that the main drawback of their approach is the high computational cost due to invoking an LLM for each word in the document, and they plan to explore the adequacy of smaller language models in future work.
- Why unresolved: The paper does not provide any experimental results comparing the performance of smaller language models to large language models for text sanitization tasks.
- What evidence would resolve it: Experimental results comparing the computational cost and performance of various smaller language models to large language models for text sanitization tasks.

### Open Question 2
- Question: Can LLM be used to detect and preserve algebraic relations between numbers during text sanitization?
- Basis in paper: [inferred] The authors mention that their substitution strategy for numeric tokens, based on random sampling, fails to preserve the algebraic relations between numbers, and they suggest using LLM to detect related numbers and generate alternatives that preserve such relations in future work.
- Why unresolved: The paper does not provide any experimental results demonstrating the effectiveness of using LLM to detect and preserve algebraic relations between numbers during text sanitization.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of using LLM to detect and preserve algebraic relations between numbers during text sanitization.

### Open Question 3
- Question: How does the proposed text sanitization method perform on datasets beyond the Action-Based Conversation Dataset (ABCD)?
- Basis in paper: [explicit] The authors mention that they evaluated their model using ABCD, a dialogue dataset, and suggest validating the proposed model in more diverse scenarios in future work.
- Why unresolved: The paper does not provide any experimental results demonstrating the performance of the proposed text sanitization method on datasets beyond ABCD.
- What evidence would resolve it: Experimental results demonstrating the performance of the proposed text sanitization method on various datasets beyond ABCD, such as news articles, social media posts, or scientific papers.

## Limitations
- High computational cost due to invoking an LLM for each word in the document
- Single dataset evaluation (ABCD) limits generalizability to other domains
- Zero-shot approach may miss domain-specific sensitive terms that specialized models would catch

## Confidence
- High confidence in the core mechanism of using LLM probability estimation for identifying rare/sensitive words
- Medium confidence in the semantic substitution approach maintaining contextual coherence
- Low confidence in the method's generalization across diverse domains without further validation

## Next Checks
1. Test the method on multiple diverse text domains (medical records, legal documents, financial statements) to assess cross-domain performance and identify domain-specific limitations

2. Conduct a human evaluation study where annotators assess the coherence and semantic preservation of substituted text across different contexts and word types

3. Perform ablation studies to determine the optimal privacy threshold by systematically varying p and measuring the tradeoff between privacy protection (recall) and utility preservation (downstream task performance) across different text types