---
ver: rpa2
title: Discrete-Choice Model with Generalized Additive Utility Network
arxiv_id: '2309.16970'
source_url: https://arxiv.org/abs/2309.16970
tags:
- utility
- time
- asu-dnn
- variables
- travel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes utility functions for discrete-choice models
  using a novel neural-network architecture based on generalized additive models,
  named GAUNet. GAUNet represents utility functions using generalized additive models,
  whose shape functions use neural networks for each alternative, while maintaining
  interpretability by learning the utility function for each variable as a different
  function.
---

# Discrete-Choice Model with Generalized Additive Utility Network

## Quick Facts
- arXiv ID: 2309.16970
- Source URL: https://arxiv.org/abs/2309.16970
- Reference count: 30
- This study proposes GAUNet, a neural network architecture for discrete-choice models that achieves prediction accuracy comparable to ASU-DNN while improving interpretability.

## Executive Summary
This study introduces GAUNet (Generalized Additive Utility Network), a novel neural network architecture for discrete-choice models that combines generalized additive models with neural networks. The approach learns utility functions for each variable independently using separate neural networks, maintaining interpretability while capturing nonlinear relationships. The method is evaluated on Tokyo trip survey data, demonstrating prediction accuracy comparable to ASU-DNN (log-likelihood of -418 to -423 vs -525 to -532) while improving interpretability. The study also proposes GAIUNet, an extension that adds pairwise interaction terms between variables.

## Method Summary
The paper proposes utility functions for discrete-choice models using GAUNet, which represents utility as additive components where each explanatory variable has its own neural network. The utility function for alternative i is formulated as: V_i(x_i) = ASC_i + Σ ω_ij NN_ij(x_ij), where each variable's contribution is learned independently. GAIUNet extends this by adding 2D interaction terms between variables. The models are evaluated using multinomial logit (MNL) framework with Tokyo trip survey data from 2018-2021, employing five-fold cross-validation and comparing performance against ASU-DNN and MNL-Linear baselines using log-likelihood as the primary metric.

## Key Results
- MNL-GAUNet achieved prediction accuracy comparable to ASU-DNN with log-likelihood of -418 to -423
- The model demonstrated improved interpretability by learning utility functions for each variable independently
- MNL-GAIUNet showed that interaction effects had little impact on mode choices in the experimental results
- Tanh activation function outperformed LeakyReLU in the experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GAUNet maintains interpretability by learning utility for each variable independently via separate NNs.
- Mechanism: Each explanatory variable has its own neural network (NN) that estimates its contribution to utility for each alternative, allowing direct interpretation of variable effects without interference from other variables.
- Core assumption: Variable effects can be modeled as independent additive components without significant interaction effects.
- Evidence anchors:
  - [abstract] "GAUNet represents utility functions using generalized additive models, whose shape functions use neural networks for each alternative, while maintaining interpretability by learning the utility function for each variable as a different function."
  - [section 4.1] "GAUNet formulates the utility function for alternative i ∈ A as follows: V_i(x_i; ω_i, θ_i) := ASC_i + Σ_{j∈S_i} ω_ij NN_ij(x_ij; θ_ij)"
  - [corpus] No direct evidence about interpretability in corpus papers, but GAM-based approaches generally support interpretability
- Break condition: When variable interactions are strong and cannot be adequately captured by additive terms alone, or when the number of variables becomes too large for independent NNs.

### Mechanism 2
- Claim: MNL-GAUNet achieves prediction accuracy comparable to ASU-DNN by using nonlinear shape functions while maintaining additive structure.
- Mechanism: Uses neural networks as shape functions in the generalized additive model framework, capturing nonlinear relationships between variables and utility while keeping the overall structure interpretable.
- Core assumption: Nonlinear relationships between variables and utility can be adequately captured by individual neural networks for each variable.
- Evidence anchors:
  - [abstract] "MNL-GAUNet achieved prediction accuracy comparable to ASU-DNN and improved interpretability compared to previous models."
  - [section 4.1] "Our approach maintains interpretability for the effect of each explanatory variable by learning the utility function for each variable as a different function and achieves high accuracy using the NNs as the utility function."
  - [section 6.1] "MNL-GAUNet was comparable to ASU-DNN and MNL-GAIUNet. The result indicated that the interaction effect between the explanatory variables had little impact on mode choices in the experiments."
- Break condition: When utility functions require complex interactions between variables that cannot be captured by the additive structure.

### Mechanism 3
- Claim: GAIUNet extends GAUNet by adding pairwise interaction terms between variables while maintaining interpretability.
- Mechanism: Adds interaction terms between two explanatory variables represented by fully connected neural networks to the base GAUNet structure, allowing capture of variable interactions without sacrificing overall interpretability.
- Core assumption: Pairwise interactions are sufficient to capture most meaningful interactions between variables.
- Evidence anchors:
  - [abstract] "GAIUNet, which adds 2D interaction terms in addition to GAUNet."
  - [section 4.1] "GAIUNet further considers the pairwise interaction term between explanatory variables in addition to GAUNet"
  - [section 4.3] "We define the importance score of the interactions... GAIUNet also learns the utility for 2D interaction between the explanatory variables using NNs in addition to MNL-GAUNet."
- Break condition: When higher-order interactions (beyond pairwise) are necessary, or when the number of variable pairs becomes computationally prohibitive.

## Foundational Learning

- Concept: Generalized Additive Models (GAMs)
  - Why needed here: GAUNet and GAIUNet are built on GAM framework, which allows nonlinear relationships while maintaining interpretability
  - Quick check question: What is the key difference between GLMs and GAMs in terms of how they model relationships between explanatory variables and outcomes?

- Concept: Multinomial Logit Models (MNL)
  - Why needed here: MNL-GAUNet and MNL-GAIUNet are MNL models using the proposed utility functions
  - Quick check question: How does the choice probability in MNL models relate to the deterministic utility function?

- Concept: Neural Network Architecture and Activation Functions
  - Why needed here: The proposed models use neural networks as shape functions, and the experiments compare Tanh vs LeakyReLU activation functions
  - Quick check question: What is the mathematical form of the Tanh activation function and how does it differ from LeakyReLU?

## Architecture Onboarding

- Component map:
  - Explanatory variables → GAUNet/GAIUNet utility computation → MNL softmax → Choice probabilities

- Critical path: Explanatory variables → GAUNet/GAIUNet utility computation → MNL softmax → Choice probabilities

- Design tradeoffs:
  - Interpretability vs prediction accuracy: GAUNet sacrifices some potential accuracy for interpretability compared to fully connected networks
  - Model complexity: GAIUNet adds interaction terms but increases computational complexity and potential overfitting risk
  - Activation function choice: Tanh vs LeakyReLU affects performance depending on function complexity

- Failure signatures:
  - Poor training performance: Could indicate insufficient model capacity or inappropriate activation functions
  - Overfitting: Could occur with too many parameters in GAIUNet's interaction terms
  - Interpretability issues: May arise if interaction terms dominate utility contributions

- First 3 experiments:
  1. Train MNL-GAUNet with simple linear activation functions to verify it reduces to MNL-Linear
  2. Compare prediction accuracy of MNL-GAUNet with different activation functions (Tanh vs LeakyReLU)
  3. Train MNL-GAIUNet and examine which interaction terms are selected based on importance scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MNL-GAUNet and MNL-GAIUNet compare to other interpretable deep learning approaches for discrete-choice modeling beyond ASU-DNN?
- Basis in paper: [explicit] The paper compares MNL-GAUNet and MNL-GAIUNet to ASU-DNN and MNL-Linear, but does not compare to other interpretable deep learning approaches.
- Why unresolved: The paper focuses on comparing the proposed methods to ASU-DNN and MNL-Linear, but does not explore how they compare to other interpretable deep learning approaches in the field.
- What evidence would resolve it: A comprehensive comparison of MNL-GAUNet and MNL-GAIUNet to other interpretable deep learning approaches for discrete-choice modeling, such as those based on attention mechanisms or graph neural networks, using the same dataset and evaluation metrics.

### Open Question 2
- Question: How does the interpretability of MNL-GAUNet and MNL-GAIUNet change when using different neural network architectures for the shape functions?
- Basis in paper: [inferred] The paper uses fully connected neural networks as shape functions, but does not explore how different architectures (e.g., convolutional, recurrent) affect interpretability.
- Why unresolved: The paper focuses on the proposed architecture and its interpretability, but does not investigate how different neural network architectures for the shape functions impact interpretability.
- What evidence would resolve it: An empirical study comparing the interpretability of MNL-GAUNet and MNL-GAIUNet using different neural network architectures for the shape functions, such as fully connected, convolutional, or recurrent networks, and analyzing the trade-off between accuracy and interpretability.

### Open Question 3
- Question: How does the performance of MNL-GAUNet and MNL-GAIUNet change when applied to discrete-choice problems with different characteristics, such as the number of alternatives or the type of decision-making context?
- Basis in paper: [explicit] The paper evaluates the proposed methods on a specific discrete-choice problem (transport mode choice) and does not explore how performance changes for different problem characteristics.
- Why unresolved: The paper focuses on a single discrete-choice problem and does not investigate how the proposed methods perform for problems with different characteristics, such as the number of alternatives or the type of decision-making context.
- What evidence would resolve it: A comprehensive evaluation of MNL-GAUNet and MNL-GAIUNet on a diverse set of discrete-choice problems with varying characteristics, such as the number of alternatives, the type of decision-making context, and the level of noise in the data, to assess their robustness and generalizability.

## Limitations

- The interpretability claims require domain expert validation to assess whether learned neural networks are genuinely interpretable in practice
- The additive structure assumption may not hold for choice problems with strong variable interactions
- Results are based on a single dataset (Tokyo trip survey) and may not generalize to other transportation contexts or choice scenarios

## Confidence

**High Confidence Claims**:
- The mathematical formulation of GAUNet and GAIUNet utility functions is clearly defined
- The MNL framework with the proposed utility functions is correctly specified
- The comparison of log-likelihood values between different models is methodologically sound

**Medium Confidence Claims**:
- The claim that MNL-GAUNet achieves "comparable" accuracy to ASU-DNN (given the numerical values, this appears to be a moderate difference)
- The interpretability improvements over ASU-DNN (requires domain expert validation)
- The mechanism by which neural networks capture nonlinear relationships in an interpretable way

**Low Confidence Claims**:
- The claim that interaction effects had "little impact" based on the experimental results
- The generalizability of results to other datasets or choice contexts
- The optimal activation function choice (Tanh vs LeakyReLU) for all applications

## Next Checks

1. **Robustness to Dataset Variation**: Validate the model on multiple transportation choice datasets beyond Tokyo to assess generalizability and verify whether the additive structure assumption holds across different contexts.

2. **Ablation Study on Interpretability**: Conduct a structured evaluation where transportation experts assess the interpretability of learned utility functions, comparing GAUNet against both traditional MNL and ASU-DNN on the same metrics.

3. **Interaction Effect Quantification**: Perform additional experiments to systematically vary the strength of interaction effects in synthetic data to determine the breaking point where the additive assumption fails and GAUNet performance degrades significantly.