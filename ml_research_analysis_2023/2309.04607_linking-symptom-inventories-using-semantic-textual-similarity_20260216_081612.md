---
ver: rpa2
title: Linking Symptom Inventories using Semantic Textual Similarity
arxiv_id: '2309.04607'
source_url: https://arxiv.org/abs/2309.04607
tags:
- etal
- page
- scl-90-r
- bsi-18
- losangeles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study developed an AI approach using semantic textual similarity
  to link symptoms and scores across different clinical symptom inventories. By leveraging
  pre-trained natural language processing models, the method rapidly screened thousands
  of symptom description pairs for related content, overcoming the long-standing challenge
  of comparing results across disparate instruments.
---

# Linking Symptom Inventories using Semantic Textual Similarity

## Quick Facts
- arXiv ID: 2309.04607
- Source URL: https://arxiv.org/abs/2309.04607
- Reference count: 40
- One-line primary result: AI approach using semantic textual similarity achieved 74.8% accuracy in linking symptoms and scores across clinical inventories

## Executive Summary
This study developed an AI approach using semantic textual similarity to link symptoms and scores across different clinical symptom inventories. By leveraging pre-trained natural language processing models, the method rapidly screened thousands of symptom description pairs for related content, overcoming the long-standing challenge of comparing results across disparate instruments. Tested on 6,607 participants from 16 international datasets, the semantic textual similarity approach achieved 74.8% accuracy across five prediction tasks, outperforming other benchmark models. The tool enables conversion of scores between previously incompatible symptom inventories, offering a practical solution to harmonize clinical and research data across settings and studies.

## Method Summary
The researchers developed an AI approach using pre-trained transformer models to compute semantic textual similarity between symptom descriptions across four clinical symptom inventories (BSI-18, SCL-90-R, NSI, RPQ). They processed 6,607 participants from 16 international datasets, with 2,056 participants having dual inventory administration for validation. The method employed a nearest-neighbor approach to predict symptom severity scores by identifying the most semantically similar symptom descriptions across inventories. They used four pre-trained transformer models (MiniLMBERT, ClinicalCovidBERT, VAClinicalDocsBERT, VAMetadataBERT) and applied percentile-based score normalization to handle different Likert scales across inventories. The approach achieved 74.8% accuracy across five prediction tasks without requiring fine-tuning on clinical data.

## Key Results
- Semantic textual similarity approach achieved 74.8% accuracy across five prediction tasks
- Outperformed other benchmark models including clinically pre-trained transformers
- Successfully converted scores between previously incompatible symptom inventories
- MiniLMBERT showed highest accuracy and better performance for elderly participants

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Semantic Textual Similarity (STS) models can accurately link symptom descriptions across inventories by capturing semantic meaning beyond shared words.
- **Mechanism:** Pre-trained transformer models encode symptom descriptions into embeddings where semantically similar content is closer in the embedding space. Cosine similarity between embeddings quantifies semantic overlap.
- **Core assumption:** Symptom descriptions with similar clinical meaning but different wording will have high semantic similarity scores even without overlapping words.
- **Evidence anchors:**
  - [abstract]: "The semantic textual similarity approach achieved 74.8% accuracy across five tasks, outperforming other benchmark models."
  - [section]: "Advancements in NLP have yielded tools that can rapidly score the semantic similarity of text, such as transformer models trained on a large corpus of text to encode and represent text strings within an embedded features space."
  - [corpus]: Weak corpus evidence - no direct comparison studies found.
- **Break condition:** Symptom descriptions use highly specialized medical jargon or abbreviations not present in the pre-training corpus, leading to poor semantic representation.

### Mechanism 2
- **Claim:** The STS approach is robust to differences in scoring scales across inventories by using percentile-based adjustment.
- **Mechanism:** Raw scores are converted to percentiles within each inventory, normalizing for different Likert scale interpretations. The model then samples from the overlapping percentile range to map scores across inventories.
- **Core assumption:** Symptom severity levels correspond to similar percentile ranks across inventories, even if the raw scale labels differ.
- **Evidence anchors:**
  - [section]: "A percentile sampling approach was used to mitigate these differences... If items had no single close analogue on other inventories, multiple items were used for prediction."
  - [section]: "The model did not link unrelated items, even if they contained overlapping words."
  - [corpus]: Weak corpus evidence - no direct comparison studies found.
- **Break condition:** Inventories have fundamentally different conceptualizations of symptom severity that don't map well to percentile ranks.

### Mechanism 3
- **Claim:** Generic language models can outperform clinically-trained models for symptom inventory linking because symptom descriptions use straightforward language.
- **Mechanism:** The STS models leverage semantic relationships between symptom descriptions, and generic language models trained on diverse text corpora may better capture these relationships than models trained on specialized clinical text.
- **Core assumption:** Symptom inventory descriptions use relatively simple, non-technical language that is well-represented in general text corpora.
- **Evidence anchors:**
  - [section]: "Overall, a deep learning model trained on a general corpus of text showed the highest accuracy, which confirmed our hypothesis. The superior performance of the generic language model over clinically pre-trained models is consistent with the straightforward language inventories used to describe symptoms."
  - [section]: "The resulting analysis pipeline is available as a free online tool that can convert scores across previously incompatible symptom inventories."
  - [corpus]: Weak corpus evidence - no direct comparison studies found.
- **Break condition:** Symptom inventories begin using more technical medical terminology or specialized abbreviations that are better captured by clinically-trained models.

## Foundational Learning

- **Concept: Semantic Textual Similarity (STS)**
  - Why needed here: Understanding how STS models measure meaning similarity between text strings is crucial for grasping the core methodology.
  - Quick check question: What is the difference between semantic similarity and lexical overlap in text comparison?

- **Concept: Transformer Models and Embeddings**
  - Why needed here: The STS approach relies on transformer models to convert symptom descriptions into embeddings for similarity measurement.
  - Quick check question: How do transformer models encode text into embeddings, and why is this useful for comparing different length text strings?

- **Concept: Percentile-based Score Normalization**
  - Why needed here: Understanding how percentile-based adjustment handles different scoring scales across inventories is key to interpreting the results.
  - Quick check question: Why is percentile-based adjustment preferred over direct scale conversion for linking symptom scores across inventories?

## Architecture Onboarding

- **Component map:** Data ingestion -> Pre-processing -> STS model -> Crosswalk pipeline -> Evaluation
- **Critical path:** Data → STS model → Similarity scores → Percentile adjustment → Score prediction → Evaluation
- **Design tradeoffs:** Using pre-trained models vs. fine-tuning on clinical data; percentile adjustment vs. direct scale conversion; single vs. multiple similarity item matching
- **Failure signatures:** Low similarity scores across all item pairs; inconsistent predictions across models; poor performance on specific inventories
- **First 3 experiments:**
  1. Test STS model performance on a held-out subset of symptom pairs with known semantic relationships
  2. Compare performance of different pre-trained transformer models on a validation set
  3. Evaluate the impact of different percentile adjustment thresholds on cross-inventory score prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the performance of semantic textual similarity models vary across different demographic groups, and what factors contribute to these differences?
- **Basis in paper:** [explicit] The paper reports that MiniLMBERT predicted female symptoms with 6% lower accuracy than male symptoms and showed better performance for elderly participants compared to younger ones.
- **Why unresolved:** While demographic variations in model performance were observed, the underlying reasons for these differences are not explored. The study only notes these variations without investigating the contributing factors.
- **What evidence would resolve it:** Further studies examining the impact of demographic factors on model performance, including potential biases in training data and model architecture, would help clarify these differences.

### Open Question 2
- **Question:** Can the semantic textual similarity approach be extended to inventories with different numeric scales, and what challenges might arise?
- **Basis in paper:** [inferred] The paper mentions that only inventories with five-point scales were used, but suggests the approach could be extended to inventories with different numeric scales.
- **Why unresolved:** The paper does not explore the feasibility or challenges of applying the semantic textual similarity approach to inventories with different numeric scales.
- **What evidence would resolve it:** Empirical studies testing the approach on inventories with varying numeric scales, along with analyses of the challenges and adaptations required, would provide insights into its broader applicability.

### Open Question 3
- **Question:** How does the semantic textual similarity approach handle the loss of information regarding patients' experiences when converting scores across different inventories?
- **Basis in paper:** [explicit] The paper acknowledges that inventories do not capture all elements of personal experience and that some may systematically screen out or inadequately capture meaning.
- **Why unresolved:** The study does not investigate the extent to which cross-walk tools incur correlated losses of information regarding patients' experiences.
- **What evidence would resolve it:** Comparative studies evaluating the impact of score conversion on the richness of patient-reported experiences, possibly through qualitative analyses or patient feedback, would help address this question.

## Limitations
- Achieved 74.8% accuracy, falling short of perfect concordance between symptom inventories
- Performance varied significantly across different inventory pairs
- Validation primarily on adult participants limits generalizability to pediatric populations
- Relies on semantic similarity which may not capture all clinical nuances across cultural contexts

## Confidence

- **High Confidence:** The core methodology of using pre-trained transformer models for semantic textual similarity is technically sound and the overall improvement over baseline approaches is well-supported by the results.
- **Medium Confidence:** The generalizability of results across diverse clinical populations and symptom domains, as the validation was limited to specific inventories and participant demographics.
- **Medium Confidence:** The assumption that symptom severity corresponds to similar percentile ranks across inventories, as this normalization approach showed variable performance across different inventory pairs.

## Next Checks
1. Test the STS model performance on additional symptom inventories beyond the four studied, particularly those with different conceptual frameworks or scoring structures.
2. Conduct cross-cultural validation by applying the approach to symptom data from non-Western populations to assess cultural sensitivity and adaptation needs.
3. Evaluate the tool's performance in real-world clinical settings by having clinicians validate the symptom mappings and score predictions in practice.