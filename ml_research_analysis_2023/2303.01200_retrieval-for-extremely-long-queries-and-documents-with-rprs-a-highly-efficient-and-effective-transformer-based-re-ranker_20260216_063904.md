---
ver: rpa2
title: 'Retrieval for Extremely Long Queries and Documents with RPRS: a Highly Efficient
  and Effective Transformer-based Re-Ranker'
arxiv_id: '2303.01200'
source_url: https://arxiv.org/abs/2303.01200
tags:
- rprs
- retrieval
- document
- documents
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose a highly effective and efficient re-ranker (RPRS) for
  Query-by-Document (QBD) retrieval that covers the full text of query and candidate
  documents without length limitations. Our method is based on a novel relevance score
  that leverages the similarity of individual sentences between a query and a document,
  with a frequency-based extension inspired by BM25's 'term saturation' mechanism.
---

# Retrieval for Extremely Long Queries and Documents with RPRS: a Highly Efficient and Effective Transformer-based Re-Ranker

## Quick Facts
- **arXiv ID**: 2303.01200
- **Source URL**: https://arxiv.org/abs/2303.01200
- **Reference count**: 40
- **Primary result**: RPRS significantly outperforms state-of-the-art models on five QBD datasets with O(N) complexity and robust performance across varying document lengths.

## Executive Summary
This paper introduces RPRS (Relevance-based Proportional Scoring), a highly efficient and effective re-ranker for Query-by-Document (QBD) retrieval tasks involving extremely long queries and documents. The method addresses the limitations of traditional Transformer models by leveraging sentence-level embeddings and a novel relevance score that measures the proportional overlap between query and document sentences. RPRS achieves state-of-the-art performance on five diverse QBD datasets while maintaining linear complexity, making it suitable for real-world applications with large document collections.

## Method Summary
RPRS processes documents by splitting them into sentences and embedding each sentence using Sentence-BERT (SBERT) bi-encoders during an offline indexing phase. At query time, the query sentences are embedded and cosine similarities are computed between query and document sentences. The relevance score combines Query Proportion (QP) - the fraction of query sentences with matching document sentences - and Document Proportion (DP) - the fraction of the document covered by matching sentences. A frequency saturation parameter (k₁) and length normalization (b) inspired by BM25 prevent over-weighting of repetitive or overly long documents. The method requires only three tunable parameters (n, k₁, b) with limited ranges that can be optimized via grid search even with small labeled datasets.

## Key Results
- RPRS significantly outperforms state-of-the-art models including MTFT-BERT on all five QBD datasets (COLIEE 2021, Caselaw, CLEF-IP 2011, WWG, WWA)
- Achieves O(N) complexity where N is the total number of sentences, compared to O(N²) for cross-encoder approaches
- Demonstrates robust performance across varying document lengths, maintaining effectiveness on documents up to 1 million characters
- Shows particular strength in low-resource settings, with only three parameters requiring optimization

## Why This Works (Mechanism)

### Mechanism 1: Pre-computation for Efficiency
RPRS achieves computational efficiency by pre-processing documents into sentence embeddings during an offline indexing phase. Documents are split into sentences and embedded using SBERT bi-encoders, allowing O(N) complexity at query time through simple cosine similarity computations between query and document sentences rather than expensive cross-attention operations.

### Mechanism 2: Proportional Relevance Scoring
The PRS captures document relevance by measuring sentence overlap between query and document through Query Proportion (QP) and Document Proportion (DP). A document is scored higher when a large proportion of its content matches a large proportion of the query content, ensuring that coverage breadth contributes to relevance assessment.

### Mechanism 3: Frequency Saturation and Length Normalization
The k₁ parameter controls how much additional matching sentences beyond the first contribute to the score (similar to BM25's term saturation), while b normalizes scores by document length. This prevents repetitive matching sentences from dominating the score and ensures that document length does not artificially inflate relevance scores.

## Foundational Learning

- **Sentence embedding using SBERT bi-encoders**: Enables efficient similarity computation between individual sentences without expensive cross-attention. *Quick check*: What is the computational complexity difference between bi-encoder and cross-encoder approaches?

- **BM25 term frequency saturation and length normalization**: Provides theoretical foundation for k₁ and b parameters in RPRS w/freq. *Quick check*: How does BM25's term saturation mechanism prevent term frequency from dominating the relevance score?

- **Query-by-Document (QBD) retrieval task characteristics**: Understanding that both queries and documents are long texts requiring different approaches than keyword queries. *Quick check*: Why do standard Transformer models struggle with QBD tasks?

## Architecture Onboarding

- **Component map**: Sentence segmentation → SBERT embedding → Cosine similarity computation → Proportion calculation → Final relevance scoring
- **Critical path**: Document preprocessing → Embedding storage → Query processing → Similarity ranking → Result output
- **Design tradeoffs**: Pre-computation vs. flexibility (pre-computed embeddings are efficient but inflexible to content changes)
- **Failure signatures**: Poor sentence segmentation leads to incorrect coverage calculation; inappropriate SBERT model choice reduces effectiveness
- **First 3 experiments**:
  1. Test sentence segmentation quality on sample legal and patent documents
  2. Compare RPRS effectiveness with different SBERT models (MiniLM vs. Legal BERT)
  3. Measure query processing time with documents of varying lengths to verify O(N) complexity

## Open Questions the Paper Calls Out

- **Question 1**: How does RPRS compare to traditional cross-encoder models like BERT when document length is not a limiting factor?
  - *Basis*: The paper shows RPRS outperforms MTFT-BERT but does not directly compare RPRS to BERT without length constraints.
  - *Why unresolved*: Comparison focuses on efficiency within BERT's input length constraints.
  - *What evidence would resolve it*: Controlled experiment comparing RPRS and BERT on unrestricted document lengths.

- **Question 2**: What is the impact of document length normalization parameter b on RPRS's performance in datasets with significantly varying document lengths?
  - *Basis*: The paper discusses b's role but does not explore its impact across datasets with vastly different length distributions.
  - *Why unresolved*: Experiments focus on tuning b for specific datasets without analyzing generalizability.
  - *What evidence would resolve it*: Experiments varying b across datasets with diverse length distributions.

- **Question 3**: How does RPRS perform in real-world applications where query-document relevance is determined by human annotators?
  - *Basis*: The paper evaluates RPRS on benchmark datasets but does not assess performance in dynamic, real-world retrieval scenarios.
  - *Why unresolved*: Controlled benchmark settings may not capture real-world complexities and noise.
  - *What evidence would resolve it*: Deploying RPRS in a live retrieval system and analyzing user feedback.

## Limitations

- **Parameter sensitivity**: RPRS shows strong performance but requires dataset-specific tuning of parameters (n ranges from 5-25), suggesting limited true parameter-free capabilities.
- **Pre-processing overhead**: While O(N) at query time, the method requires substantial pre-processing (sentence segmentation, embedding, storage) for large document collections.
- **Static collection assumption**: The approach assumes static document collections, making it less suitable for dynamic content environments requiring frequent re-indexing.

## Confidence

- **High confidence**: Efficiency claims (O(N) complexity) and sentence-level processing approach are well-supported by the architecture description and complexity analysis.
- **Medium confidence**: Effectiveness claims are supported by strong results across five datasets, but sensitivity to dataset-specific parameter tuning and SBERT model choice introduces uncertainty.
- **Low confidence**: The claim that RPRS is "suitable for low-resource training data" is not fully validated, as the method still requires labeled validation data for parameter optimization and domain-specific SBERT training.

## Next Checks

1. **Parameter robustness test**: Evaluate RPRS performance using fixed parameters across all datasets to assess true parameter-free capabilities versus claimed benefits.
2. **Dynamic content evaluation**: Measure the computational cost and accuracy impact of re-indexing documents with frequent content updates to validate static collection assumptions.
3. **Embedding quality analysis**: Compare RPRS effectiveness using different SBERT variants (general vs. domain-specific) and measure the correlation between embedding quality and retrieval performance.