---
ver: rpa2
title: Comprehending Lexical and Affective Ontologies in the Demographically Diverse
  Spatial Social Media Discourse
arxiv_id: '2311.06729'
source_url: https://arxiv.org/abs/2311.06729
tags:
- features
- groups
- language
- reviews
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates linguistic and demographic features, including
  English language styles, conveyed sentiments, and lexical diversity within spatial
  online social media review data. A case study is conducted to scrutinize reviews
  composed by two distinct and demographically diverse groups.
---

# Comprehending Lexical and Affective Ontologies in the Demographically Diverse Spatial Social Media Discourse

## Quick Facts
- arXiv ID: 2311.06729
- Source URL: https://arxiv.org/abs/2311.06729
- Reference count: 26
- Key outcome: Linguistic features (lexical diversity, sentiment coverage, grammar) distinguish demographic groups in reviews with ~85% macro F1; transformer-based models with n-gram lexical features achieve >95% accuracy and >0.96 macro F1.

## Executive Summary
This study investigates how linguistic and demographic features, including English language styles, sentiment, and lexical diversity, differ between two demographically diverse groups in spatial online social media reviews. A case study compares reviews from non-native English speakers (Bangladesh) and native English speakers (USA), extracting statistical, grammatical, and sentiment-based features. These features are used with classical ML classifiers and transformer-based models to differentiate the groups, revealing substantial disparities in linguistic attributes and high classification accuracy.

## Method Summary
The study compares restaurant reviews from two demographic groups: non-native English speakers (Bangladesh) and native English speakers (USA), with 4,987 reviews each. Linguistic features (text statistics, grammatical features, sentiment lexicon coverage) and lexical features (word unigrams and bigrams with TF-IDF) are extracted. Classical ML classifiers (Logistic Regression, SVM, Random Forest, Extra Trees) and transformer-based models (BERT, RoBERTa) are trained and evaluated using 5-fold cross-validation. Mann-Whitney U tests identify statistically significant differences in linguistic features between groups.

## Key Results
- Linguistic features (lexical diversity, sentiment lexicon coverage, grammatical usage) show substantial disparities between demographic groups and achieve ~85% macro F1 with classical ML classifiers.
- Word n-gram lexical features combined with transformer-based models (BERT, RoBERTa) surpass 95% accuracy and 0.96 macro F1, outperforming linguistic features with classical ML.
- Mann-Whitney U tests reveal statistically significant differences in many linguistic features between the two groups, guiding effective feature selection.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Demographic groups can be distinguished by differences in linguistic features such as lexical diversity, sentiment lexicon coverage, and grammatical usage.
- Mechanism: The study extracts statistical, grammatical, and sentiment-based features from reviews of two demographic groups and feeds them into classical ML classifiers. The classifiers learn to differentiate groups based on these measurable linguistic differences.
- Core assumption: Non-native English speakers use simpler sentences, fewer subordinating conjunctions, and higher sentiment lexicon coverage than native speakers.
- Evidence anchors:
  - [abstract] "The investigation unveils substantial disparities in certain linguistic attributes between the two groups."
  - [section] "The linguistic analysis reveals that several attributes of reviews, such as review length, range of vocabulary used, coverage of opinion lexicon, and usage of part-of-speech (POS) tags vary across two demographic groups..."
  - [corpus] Weak - corpus neighbors are unrelated to linguistic variation; no direct supporting evidence.
- Break condition: If the two demographic groups have similar English proficiency or use similar writing styles, the linguistic features may not be discriminative enough.

### Mechanism 2
- Claim: Word n-gram lexical features combined with transformer-based models achieve higher accuracy than linguistic features with classical ML classifiers.
- Mechanism: The study uses word unigrams and bigrams as lexical features, computes their tf-idf scores, and inputs them into both classical ML classifiers and transformer-based models. Transformers, pre-trained on massive corpora, can capture subtle patterns in lexical usage, including named entities and socio-cultural terms.
- Core assumption: Socio-cultural and named-entity terms in reviews create distinguishing lexical patterns that are not well-captured by classical ML models.
- Evidence anchors:
  - [abstract] "As expected, the n-gram lexical features, coupled with fine-tuned transformer-based models, show superior performance, attaining accuracies surpassing 95% and macro F1 scores exceeding 0.96."
  - [section] "The pre-trained language models are generated based on an enormous amount of textual content, which helps to capture the implicit pattern of the reviews and can effectively identify the language nativeness of reviewers."
  - [corpus] Weak - corpus neighbors do not directly address lexical patterns in demographic prediction.
- Break condition: If both groups use very similar vocabulary or if the reviews lack socio-cultural references, lexical n-grams may not provide strong discriminative signals.

### Mechanism 3
- Claim: Mann-Whitney U tests can identify which linguistic features show statistically significant differences between demographic groups, guiding feature selection.
- Mechanism: For each linguistic feature, the study applies the Mann-Whitney U test to assess whether its distribution differs between the two demographic groups. Features with p-values below 0.05 are considered significantly different and more likely to help classification.
- Core assumption: Statistical significance of feature differences correlates with classification performance.
- Evidence anchors:
  - [section] "We employ the Mann-Whitney U test to determine which linguistic features show significant differences in the two groups of reviews."
  - [section] "The Mann-Whitney test indicates many of the linguistic features are significantly different in the two groups..."
  - [corpus] Weak - corpus neighbors do not discuss statistical testing of feature differences.
- Break condition: If the statistical test is too conservative or if significant differences do not translate to useful classification features, the approach may fail.

## Foundational Learning

- Concept: Understanding the distinction between linguistic and lexical features.
  - Why needed here: The study compares linguistic features (e.g., POS usage, sentiment coverage) with lexical n-grams; knowing the difference is essential for interpreting results.
  - Quick check question: What is the main difference between a linguistic feature and a lexical n-gram feature?

- Concept: Basics of classical ML classifiers (Logistic Regression, SVM, Random Forest, Extra Trees).
  - Why needed here: These classifiers are used with linguistic and lexical features; understanding their strengths and weaknesses helps explain performance differences.
  - Quick check question: Which classical ML classifier typically handles high-dimensional sparse data best?

- Concept: Transformer-based models and fine-tuning for text classification.
  - Why needed here: The study fine-tunes BERT and RoBERTa; understanding pre-training and fine-tuning is crucial for interpreting their superior performance.
  - Quick check question: Why are only the last layers of pre-trained transformers usually trained during fine-tuning?

## Architecture Onboarding

- Component map: Data collection -> Feature extraction (linguistic and lexical) -> Statistical testing (Mann-Whitney) -> Classical ML classification -> Transformer fine-tuning -> Evaluation (5-fold CV, F1, accuracy)
- Critical path: Feature extraction and selection -> Classifier training -> Model evaluation and comparison
- Design tradeoffs: Linguistic features are interpretable but less discriminative; lexical n-grams with transformers are more accurate but less interpretable and require more resources
- Failure signatures: High variance in review length or vocabulary can confuse classifiers; if groups have similar English proficiency, linguistic features may not distinguish them; transformers may overfit with small datasets
- First 3 experiments:
  1. Extract linguistic features from both groups and run Mann-Whitney U tests to identify significant differences.
  2. Train classical ML classifiers using only significant linguistic features; evaluate F1 and accuracy.
  3. Train classical ML classifiers using word unigrams and bigrams; compare performance with linguistic features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do linguistic features perform when distinguishing demographic groups with similar English proficiency levels (e.g., native vs. highly proficient non-native speakers)?
- Basis in paper: [explicit] The paper notes this as a future research direction, suggesting that current findings may not generalize to groups with similar proficiency levels.
- Why unresolved: The study only compares groups with clearly distinct proficiency levels (low-proficiency non-native vs. native speakers), leaving the effectiveness of linguistic features for similar-proficiency groups unexplored.
- What evidence would resolve it: Comparative analysis of linguistic features across demographic groups with similar English proficiency levels, measuring classification accuracy and feature significance.

### Open Question 2
- Question: Do cultural and socio-economic factors influence the linguistic features more than English proficiency in demographically diverse reviews?
- Basis in paper: [inferred] The paper highlights socio-cultural differences between groups and notes that named entities and socio-cultural terms aid classification, but does not isolate the impact of cultural factors from proficiency.
- Why unresolved: The study does not explicitly disentangle the effects of cultural/socio-economic factors from English proficiency on linguistic feature variation.
- What evidence would resolve it: Controlled analysis of reviews from demographically diverse groups with matched English proficiency levels, isolating the contribution of cultural and socio-economic factors.

### Open Question 3
- Question: How do transformer-based models perform when fine-tuned on linguistic features rather than lexical n-grams?
- Basis in paper: [explicit] The paper uses transformer models with lexical n-grams and achieves high accuracy, but does not explore their performance with linguistic features.
- Why unresolved: The study focuses on lexical features for transformer models, leaving the potential of linguistic features unexplored in this context.
- What evidence would resolve it: Fine-tuning transformer models on linguistic features and comparing their performance to lexical feature-based models.

## Limitations

- The study relies on a single case study with only two demographic groups (non-native vs. native English speakers), limiting generalizability to other demographic distinctions or cultural contexts.
- The corpus size (9,974 reviews) is moderate, and performance gains of transformer models may not scale similarly to larger, more diverse datasets.
- The linguistic feature analysis provides limited insight into the underlying mechanisms driving demographic differences in language use.

## Confidence

- Linguistic feature differences: Medium
- Transformer model superiority: Medium
- Generalizability to other demographics: Low

## Next Checks

1. Test the classifier performance on additional demographic distinctions (e.g., age groups, gender, or regional dialects within the same country) to assess generalizability.
2. Evaluate model performance on a larger, more diverse corpus to determine if transformer advantages persist with increased data scale.
3. Conduct ablation studies to identify which specific linguistic features contribute most to classification accuracy and whether these align with theoretical expectations about language proficiency differences.