---
ver: rpa2
title: Diverse Conventions for Human-AI Collaboration
arxiv_id: '2310.15414'
source_url: https://arxiv.org/abs/2310.15414
tags:
- conventions
- cross-play
- convention
- self-play
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoMeDi addresses the problem of training AI agents that can effectively
  collaborate with humans in cooperative multi-agent games, where arbitrary conventions
  often form and hinder generalization. The method combines cross-play minimization
  to generate diverse conventions with mixed-play to ensure good-faith behavior and
  mitigate handshakes.
---

# Diverse Conventions for Human-AI Collaboration

## Quick Facts
- arXiv ID: 2310.15414
- Source URL: https://arxiv.org/abs/2310.15414
- Reference count: 40
- Diverse conventions enable human-AI collaboration

## Executive Summary
CoMeDi addresses the problem of training AI agents that can effectively collaborate with humans in cooperative multi-agent games, where arbitrary conventions often form and hinder generalization. The method combines cross-play minimization to generate diverse conventions with mixed-play to ensure good-faith behavior and mitigate handshakes. CoMeDi outperforms existing approaches like statistical diversity and pure cross-play in generating conventions that adapt well to human partners. In user studies on Overcooked, a convention-aware agent trained with CoMeDi surpassed human-level performance, scoring 3.68 vs 3.25 for expert humans (p=0.045), and received significantly higher user ratings for consistency, reasonableness, and trustworthiness.

## Method Summary
CoMeDi is a method for training AI agents that can effectively collaborate with humans in cooperative multi-agent games where arbitrary conventions often form and hinder generalization. The method combines cross-play minimization to generate diverse conventions with mixed-play to ensure good-faith behavior and mitigate handshakes. CoMeDi uses MAPPO as the underlying RL algorithm with separate buffers for self-play, cross-play, and mixed-play. The method is evaluated on Overcooked, Blind Bandits, and Balance Beam environments, demonstrating superior performance in generating conventions that adapt well to human partners.

## Key Results
- CoMeDi generates diverse conventions that adapt well to human partners
- Convention-aware agent trained with CoMeDi surpasses human-level performance in Overcooked (3.68 vs 3.25, p=0.045)
- CoMeDi outperforms pure self-play and statistical diversity baselines in user studies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-play minimization creates semantically diverse conventions by penalizing policies that achieve high reward when paired with previously found conventions.
- Mechanism: The algorithm iteratively trains a new convention while minimizing its cross-play reward with the most compatible existing convention. This forces the new policy to adopt a strategy incompatible with prior conventions.
- Core assumption: Low cross-play reward between two policies indicates they use fundamentally different strategies rather than trivial variations.
- Evidence anchors:
  - [abstract] "minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different"
  - [section 2.2] "We instead use the incompatibility of different conventions as a measure of diversity"
  - [corpus] Weak: No direct evidence that cross-play minimization consistently produces semantically diverse conventions across all domains
- Break condition: If handshakes occur (Section 3.2), cross-play minimization can be gamed by policies that sabotage games to achieve low cross-play while using similar underlying strategies.

### Mechanism 2
- Claim: Mixed-play prevents handshakes by eliminating the ability of agents to distinguish between self-play and cross-play states.
- Mechanism: Mixed-play introduces a phase where actions are sampled from self-play or cross-play, then transitions to pure self-play. Since all states in the buffer are associated with self-play rewards, agents cannot infer reward inversion.
- Core assumption: Handshakes require agents to reliably detect when they are in cross-play versus self-play.
- Evidence anchors:
  - [abstract] "To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce mixed-play"
  - [section 3.3] "Mixed-play rolls out episodes using varying combinations of self-play and cross-play to enlarge state coverage"
  - [corpus] Weak: Limited empirical evidence on how mixed-play performs across diverse environments beyond the toy examples
- Break condition: If handshakes can be established with actions that are also beneficial in self-play, mixed-play cannot distinguish between the two settings.

### Mechanism 3
- Claim: Combining self-play maximization, cross-play minimization, and mixed-play creates diverse conventions that generalize to human partners.
- Mechanism: The full CoMeDi loss function balances three terms: self-play reward (ensure competence), cross-play reward (ensure diversity), and mixed-play reward (ensure good-faith behavior).
- Core assumption: A convention-aware agent trained on diverse conventions will generalize to human partners who follow conventions within the diversity set.
- Evidence anchors:
  - [abstract] "CoMeDi outperforms existing approaches like statistical diversity and pure cross-play in generating conventions that adapt well to human partners"
  - [section 4.3] "CoMeDi significantly outperforms the pure self-play and statistical diversity baselines in terms of score and user opinions, surpassing human-level performance"
  - [corpus] Weak: No evidence provided on how CoMeDi performs when human conventions fall outside the diversity set
- Break condition: If human partners use conventions that are incompatible with all trained conventions, the convention-aware agent will fail to coordinate effectively.

## Foundational Learning

- Concept: Multi-agent Markov decision process (MDP)
  - Why needed here: The paper models cooperative games as MDPs to formalize the interaction between agents and their environment
  - Quick check question: What are the key components of a multi-agent MDP and how do they differ from single-agent MDPs?

- Concept: Nash equilibrium in multi-agent games
  - Why needed here: Equilibrium conventions are defined as policies where neither player can improve their expected score by changing strategy
  - Quick check question: How does the concept of Nash equilibrium apply to cooperative multi-agent games versus competitive games?

- Concept: Zero-shot coordination
  - Why needed here: The goal is to train agents that can work with unseen partners without prior interaction
  - Quick check question: What is the difference between zero-shot coordination and few-shot coordination in multi-agent settings?

## Architecture Onboarding

- Component map:
  - Actor networks: One per convention (πn) for policy representation
  - Self-play critic networks: One per convention for value estimation in self-play
  - Cross-play critic networks: One per convention pair for value estimation in cross-play
  - Mixed-play buffer: Stores transitions from mixed-play episodes for training
  - Convention-aware agent: Trained via behavior cloning on diverse convention dataset

- Critical path: Self-play → Cross-play buffer collection → Cross-play reward estimation → Policy update with CoMeDi loss → Mixed-play buffer collection → Mixed-play reward estimation → Policy update with CoMeDi loss

- Design tradeoffs: Sequential convention generation provides stability but limits parallel training efficiency; cross-play minimization provides semantic diversity but requires handshake mitigation; mixed-play prevents sabotage but adds training complexity

- Failure signatures: High cross-play scores between conventions (lack of diversity); agents stepping off line in Balance Beam (handshakes); poor performance with human partners despite strong self-play scores

- First 3 experiments:
  1. Implement Blind Bandits environment and verify ADAP converges to S convention while CoMeDi finds both S and G conventions
  2. Implement Balance Beam environment and test mixed-play weights (β=0, 0.25, 0.5, 1.0) to observe handshake mitigation
  3. Implement Overcooked environment and train convention-aware agent on CoMeDi-generated conventions, then evaluate with human users

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CoMeDi perform in environments with cheap-talk signals that are not necessary for coordination?
- Basis in paper: [explicit] The paper mentions that cheap-talk signals can be used to re-establish handshakes at every timestep, effectively bypassing the mixed-play optimization.
- Why unresolved: The paper does not provide empirical results or theoretical analysis of CoMeDi's performance in such environments.
- What evidence would resolve it: Experiments comparing CoMeDi's performance with and without cheap-talk signals in a controlled environment.

### Open Question 2
- Question: What is the impact of CoMeDi on human-AI interaction in real-world applications beyond gaming?
- Basis in paper: [explicit] The paper discusses the potential benefits of CoMeDi in game design and human-AI interaction, but does not explore its application in other domains.
- Why unresolved: The paper focuses on gaming environments and does not provide evidence of CoMeDi's effectiveness in other domains.
- What evidence would resolve it: Case studies or experiments demonstrating CoMeDi's performance in non-gaming human-AI interaction scenarios.

### Open Question 3
- Question: How does the choice of the number of conventions (n) affect CoMeDi's performance and generalization?
- Basis in paper: [inferred] The paper mentions that the choice of n is an art and does not provide guidelines for choosing an optimal number.
- Why unresolved: The paper does not explore the relationship between the number of conventions and CoMeDi's performance.
- What evidence would resolve it: Experiments varying the number of conventions and measuring CoMeDi's performance and generalization across different environments.

## Limitations
- Limited evidence of mixed-play effectiveness across diverse environments beyond toy examples
- No analysis of whether human conventions fall within CoMeDi-generated diversity set
- Sequential convention generation may not scale well to larger numbers of diverse conventions

## Confidence

**High confidence** in: The core claim that CoMeDi outperforms pure self-play and statistical diversity baselines in generating conventions that adapt well to human partners.

**Medium confidence** in: The effectiveness of mixed-play in preventing handshakes across diverse environments.

**Low confidence** in: The assumption that human conventions will be sufficiently covered by CoMeDi-generated diversity.

## Next Checks

1. **Handshake robustness test**: Design a Balance Beam variant where handshakes can be established through actions that are also beneficial in self-play. Test whether mixed-play can still prevent these handshakes, or whether agents can learn to sabotage while appearing to act in good faith.

2. **Human convention mapping**: Analyze the conventions used by successful human Overcooked players and compare them to the diversity set generated by CoMeDi. Quantify the overlap and test whether the convention-aware agent fails when paired with humans using conventions outside the diversity set.

3. **Parallel convention generation**: Implement a parallel version of CoMeDi that generates multiple conventions simultaneously rather than sequentially. Compare the diversity and quality of conventions against the sequential approach, particularly as the number of conventions increases beyond 4-5.