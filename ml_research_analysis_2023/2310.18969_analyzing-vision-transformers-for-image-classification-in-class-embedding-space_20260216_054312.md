---
ver: rpa2
title: Analyzing Vision Transformers for Image Classification in Class Embedding Space
arxiv_id: '2310.18969'
source_url: https://arxiv.org/abs/2310.18969
tags:
- class
- tokens
- image
- representations
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a method to analyze Vision Transformers for
  image classification by projecting intermediate representations onto the learned
  class embedding space. The approach quantifies how image tokens develop class-specific
  representations and identifies the factors and mechanisms involved.
---

# Analyzing Vision Transformers for Image Classification in Class Embedding Space

## Quick Facts
- arXiv ID: 2310.18969
- Source URL: https://arxiv.org/abs/2310.18969
- Reference count: 37
- One-line primary result: Method projects ViT intermediate representations onto class embedding space to analyze class-specific representation development, achieving up to 90.35% class identifiability rates

## Executive Summary
This work introduces a framework to analyze how Vision Transformers build class representations by projecting intermediate hidden states of image tokens onto the learned class embedding space. The method quantifies how image tokens develop class-specific representations throughout the network hierarchy and identifies the factors and mechanisms involved. Results show that image tokens increasingly align with class prototypes, influenced by attention mechanisms and contextual information, and that self-attention and MLP layers differentially contribute to categorical composition.

## Method Summary
The framework analyzes ViTs by projecting hidden states of image tokens from each transformer block onto the learned class embedding matrix E. For each token, the method computes E · xb_i to obtain class logits, revealing how much each token aligns with each class prototype. The approach measures class identifiability scores, tracks their evolution across blocks, and analyzes how attention and MLP layers contribute to class representation through key-value memory pair mechanisms. The framework also uses gradient-based methods to identify which image parts are important for class detection by computing gradients of class loss with respect to attention weights.

## Key Results
- Class identifiability rates reach up to 90.35% when projecting last block image tokens onto class embedding space
- Image tokens progressively align with class prototypes across transformer blocks, with maximum alignment at the final block
- Self-attention and MLP layers differentially contribute to categorical composition through key-value memory pair mechanisms
- The framework outperforms linear probing approaches in efficiency and accuracy for characterizing class representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method works by projecting intermediate hidden states of image tokens onto the learned class embedding space to uncover how categorical representations are built.
- Mechanism: The class embedding matrix E acts as a projection tool that translates token representations from the model's internal space to a human-interpretable class space. By computing E · xb_i for each token's hidden state xb_i, we obtain class logits that reveal how much each token aligns with each class prototype.
- Core assumption: The class embedding matrix learned during training contains meaningful class prototypes that can serve as reference points for interpreting intermediate representations.
- Evidence anchors:
  - [abstract] "we demonstrate how the inner representations at any level of the hierarchy can be projected onto the learned class embedding space to uncover how these networks build categorical representations for their predictions"
  - [section] "To investigate how class representations emerge in ViT, we analyzed the alignment between the intermediate representations of the tokens with the class prototypes encoded by the final projection matrix E"
  - [corpus] Weak - no direct evidence found in corpus papers about this specific projection mechanism
- Break condition: If the class embedding matrix does not contain meaningful class prototypes, or if the intermediate representations are too far from the class space to be meaningfully projected, the method would fail to provide interpretable insights.

### Mechanism 2
- Claim: The framework reveals that both self-attention and MLP layers contribute to building class representations through key-value memory pair mechanisms.
- Mechanism: Self-attention layers use key-value memory pairs where keys detect patterns across tokens and values modify hidden states accordingly. MLP layers implement a similar system where Winp contains key vectors and Wout contains value vectors that update representations based on detected patterns.
- Core assumption: The parameter matrices of transformer layers can be decomposed into key-value memory pairs that implement pattern detection and response mechanisms.
- Evidence anchors:
  - [abstract] "give insights on how self-attention and MLP layers differentially contribute to this categorical composition"
  - [section] "Previous work has demonstrated that the learned parameters of transformer architectures can also be projected onto the output embedding space... These studies further propose that the parameter matrices can be interpreted as systems implementing key-value memory pair mechanisms"
  - [corpus] No direct evidence found in corpus papers about key-value memory pair mechanisms in vision transformers
- Break condition: If the parameter matrices do not implement key-value memory pair mechanisms, or if the projection onto class embedding space does not reveal meaningful patterns, the mechanistic insights would be invalid.

### Mechanism 3
- Claim: The method can identify image parts important for class detection by computing gradients of class cross-entropy loss with respect to attention weights.
- Mechanism: By computing gradients of the loss LCE(E · xb_cls, cj) with respect to attention weights ab_j that [CLS] token assigns to image tokens, we can determine which image tokens would most increase correct class representation if more attention were allocated to them.
- Core assumption: The gradient of class loss with respect to attention weights provides meaningful importance scores for image tokens in building class representations.
- Evidence anchors:
  - [abstract] "can be used to determine the parts of an image that would be important for detecting the class of interest"
  - [section] "we propose to use a gradient approach to quantify how much an image token of a given block would contribute to form a categorical representation in the [CLS] token"
  - [corpus] No direct evidence found in corpus papers about this specific gradient-based explainability approach
- Break condition: If gradients do not provide meaningful importance scores, or if the relationship between attention weights and class representation is not linear or differentiable, the explainability method would fail.

## Foundational Learning

- Concept: Vision Transformer architecture and components (self-attention, MLP layers, class token, image tokens)
  - Why needed here: Understanding the basic ViT architecture is essential to comprehend how the framework projects representations and analyzes layer contributions
  - Quick check question: What are the two main types of layers in a transformer block, and what are their primary functions?

- Concept: Matrix operations and projections (dot products, matrix multiplication)
  - Why needed here: The framework relies heavily on projecting representations using matrix operations, so understanding these operations is crucial
  - Quick check question: How does projecting a hidden state xb_i onto the class embedding matrix E using E · xb_i produce class logits?

- Concept: Attention mechanisms and key-value memory concepts
  - Why needed here: The framework interprets self-attention and MLP layers as implementing key-value memory pair systems, so understanding these concepts is essential
  - Quick check question: In a key-value memory system, what is the role of keys versus values, and how do they interact to modify representations?

## Architecture Onboarding

- Component map: Image patches → Linear projections → Added position embeddings → Token sequence (including [CLS]) → Transformer blocks (MHSA and MLP layers with residual connections) → [CLS] token → Class embedding matrix projection → Softmax probabilities → Analysis components (class embedding space projection, key-value memory analysis, gradient-based explainability)

- Critical path: Image → Token sequence → Transformer blocks → [CLS] output → Class prediction
  - Analysis focuses on intermediate token representations at each block level

- Design tradeoffs:
  - Projection approach vs. traditional probing: More direct measurement of class alignment but requires understanding of class embedding space
  - Key-value memory interpretation: Provides mechanistic insights but adds complexity to analysis
  - Gradient-based explainability: Can identify important tokens but requires careful interpretation

- Failure signatures:
  - Low class identifiability scores across all blocks indicate poor alignment with class prototypes
  - Random or noisy patterns in key-value memory analysis suggest incorrect interpretation
  - Gradient-based importance scores that don't correlate with actual attention patterns indicate flawed explainability

- First 3 experiments:
  1. Verify class identifiability: Project last block image tokens onto class embedding space and measure identifiability rates
  2. Analyze evolution: Track class identifiability scores across blocks to understand hierarchical development
  3. Test attention perturbation: Remove attention weights between image tokens and measure impact on class representations

## Open Questions the Paper Calls Out

- How do the keys in ViT represent patterns that are interpretable in the image input space, and how do these keys interact with the value vectors to form class representations? The paper mentions that interpreting the keys of ViT is not as straightforward as in NLP models, and leaves the analysis of what the keys encode in human-interpretable terms to future work.

- How can the framework be adapted to examine the inner representations of models with output embeddings other than class embeddings, such as object detection or segmentation tasks? The paper focuses on ViTs trained for image classification and mentions that future work could investigate how to adapt the framework to examine models with other types of output embeddings.

- How can the mechanistic interpretability insights gained from this work be used to improve the performance or edit the behavior of ViTs? The paper mentions that some mechanistic interpretability insights point to aspects of ViT that could be manipulated for model editing or performance improvement purposes in future studies.

## Limitations
- The framework's effectiveness relies heavily on the assumption that the learned class embedding matrix contains meaningful class prototypes, with limited evidence supporting this specific projection mechanism in vision transformers
- The gradient-based explainability approach depends on the assumption that gradients of class loss with respect to attention weights provide meaningful importance scores, which may not hold consistently across all transformer variants
- The method's performance on datasets beyond ImageNet-S remains unverified, raising questions about its generalizability

## Confidence
- **High Confidence**: The mathematical framework for projecting hidden states onto class embedding space is sound and the empirical results (class identifiability rates) are well-documented and reproducible
- **Medium Confidence**: The interpretation of transformer layers as key-value memory systems and the gradient-based explainability approach are mechanistically plausible but require additional validation
- **Medium Confidence**: The comparison with linear probing approaches shows the method's efficiency, but the specific implementation details of the comparison are not fully specified

## Next Checks
1. **Cross-architecture validation**: Apply the framework to different transformer variants (e.g., Swin, DeiT) and different vision tasks (e.g., object detection, semantic segmentation) to test generalizability of the projection and interpretation mechanisms

2. **Ablation study on class embedding**: Conduct controlled experiments where class embedding matrices are randomly initialized or corrupted to quantify how much the method's performance depends on meaningful class prototypes versus other factors

3. **Alternative explainability comparison**: Compare the gradient-based importance scores with established attribution methods (e.g., Grad-CAM, integrated gradients) on the same image samples to validate whether the attention gradients provide consistent and interpretable explanations