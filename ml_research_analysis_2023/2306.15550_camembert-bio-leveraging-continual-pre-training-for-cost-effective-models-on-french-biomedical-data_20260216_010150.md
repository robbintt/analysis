---
ver: rpa2
title: 'CamemBERT-bio: Leveraging Continual Pre-training for Cost-Effective Models
  on French Biomedical Data'
arxiv_id: '2306.15550'
source_url: https://arxiv.org/abs/2306.15550
tags:
- biomedical
- camembert
- language
- french
- clinical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CamemBERT-bio, a French biomedical language
  model adapted from CamemBERT via continual pretraining on a new 413M-word French
  biomedical corpus (biomed-fr). Compared to CamemBERT, CamemBERT-bio achieves an
  average 2.54-point improvement in F1 score across multiple French biomedical named
  entity recognition tasks, demonstrating the effectiveness of domain-specific pretraining.
---

# CamemBERT-bio: Leveraging Continual Pre-training for Cost-Effective Models on French Biomedical Data

## Quick Facts
- arXiv ID: 2306.15550
- Source URL: https://arxiv.org/abs/2306.15550
- Reference count: 12
- Key outcome: CamemBERT-bio achieves 2.54-point average F1 improvement over CamemBERT on French biomedical NER tasks

## Executive Summary
This paper introduces CamemBERT-bio, a French biomedical language model created by continually pretraining CamemBERT on a new 413M-word French biomedical corpus. The model demonstrates significant improvements (2.54 F1 points on average) on multiple named entity recognition tasks compared to the base CamemBERT model. The work emphasizes the importance of standard evaluation protocols for fair model comparison in the biomedical domain and highlights cost-effective approaches to domain adaptation through continual pretraining rather than full retraining.

## Method Summary
CamemBERT-bio is created through continual pretraining of the base CamemBERT model on a specialized French biomedical corpus (biomed-fr) containing 413 million words from sources including ISTEX, CLEAR, and E3C. The pretraining uses Masked Language Modeling with whole-word masking for 50,000 steps. For evaluation, the model is fine-tuned on four French biomedical NER datasets (EMEA, MEDLINE, E3C, CAS) using Optuna for hyperparameter optimization, with results averaged over 10 runs using different random seeds.

## Key Results
- CamemBERT-bio achieves an average 2.54-point improvement in F1 score across multiple French biomedical NER tasks
- The biomedical corpus (biomed-fr) shows significant vocabulary differences from general text, with only 45% overlap with CamemBERT's tokenizer
- Standard evaluation protocols are crucial for fair comparison, as demonstrated by discrepancies when comparing to DrBERT

## Why This Works (Mechanism)

### Mechanism 1
Continual pretraining on domain-specific corpora improves performance without full retraining by adapting general language knowledge to domain-specific vocabulary and style while retaining general capabilities. This works when the domain corpus is sufficiently large and representative.

### Mechanism 2
Specialized tokenizers improve handling of domain-specific terms by better segmenting technical vocabulary and reducing over-segmentation compared to general-purpose tokenizers. This is effective when the specialized tokenizer is trained on representative domain vocabulary.

### Mechanism 3
Standard evaluation protocols enable fair model comparisons by providing consistent metrics and datasets, offering a clear view of the state-of-the-art. This is important when evaluation datasets and metrics are representative of the target domain and task.

## Foundational Learning

- Concept: Continual pretraining
  - Why needed here: To adapt a general-purpose language model to a specific domain without requiring full retraining
  - Quick check question: What is the difference between continual pretraining and full retraining?

- Concept: Domain-specific tokenization
  - Why needed here: To better handle technical terms and reduce over-segmentation that can occur with a general-purpose tokenizer
  - Quick check question: How does a specialized tokenizer differ from a general-purpose tokenizer?

- Concept: Standard evaluation protocols
  - Why needed here: To enable fair comparisons between models and provide a clear view of the current state-of-the-art
  - Quick check question: Why is it important to use consistent evaluation metrics and datasets?

## Architecture Onboarding

- Component map: CamemBERT-base model -> Continual pretraining on biomed-fr corpus -> Fine-tuning on named entity recognition tasks -> Evaluation using standard protocols
- Critical path: Pretraining -> Fine-tuning -> Evaluation
- Design tradeoffs: Larger pretraining corpus may lead to better performance but requires more computational resources. Specialized tokenizer may improve performance but requires additional training data.
- Failure signatures: Poor performance on named entity recognition tasks, high overlap between general-purpose and specialized vocabularies.
- First 3 experiments:
  1. Evaluate the performance of CamemBERT-bio on a held-out test set from the biomed-fr corpus.
  2. Compare the performance of CamemBERT-bio with and without a specialized tokenizer.
  3. Evaluate the impact of corpus size on model performance by training on subsets of the biomed-fr corpus.

## Open Questions the Paper Calls Out

### Open Question 1
How does CamemBERT-bio's performance compare to other biomedical language models for French? While the authors provide some comparisons, a more comprehensive benchmark study across multiple French biomedical models would be needed to fully assess CamemBERT-bio's relative performance.

### Open Question 2
What is the impact of the specialized tokenizer on CamemBERT-bio's performance? The authors did not conduct experiments with a specialized tokenizer, so the extent of any performance gains is unknown.

### Open Question 3
How does the size of the pretraining corpus affect CamemBERT-bio's performance? The authors only explore a limited range of corpus sizes, and further investigation is needed to determine the optimal corpus size for CamemBERT-bio.

## Limitations

- The evaluation comparison with DrBERT is limited by differences in experimental protocols and incomplete information about DrBERT's setup
- The biomedical corpus creation process lacks details on document filtering criteria, language detection methods, and preprocessing steps
- The relationship between tokenizer vocabulary overlap and model performance is mentioned but not empirically validated

## Confidence

- **High confidence**: The 2.54 F1 improvement over CamemBERT is well-supported by experimental results across multiple datasets
- **Medium confidence**: The importance of standard evaluation protocols is supported but limited by incomplete information about DrBERT's experimental setup
- **Low confidence**: The relationship between tokenizer vocabulary overlap and performance is mentioned but not empirically validated

## Next Checks

1. **Reproduce tokenizer impact**: Train two versions of CamemBERT-bio - one with the biomedical tokenizer and one with the original CamemBERT tokenizer - using identical pretraining settings, then compare their performance on the same evaluation datasets to isolate the tokenizer's effect.

2. **Corpus composition analysis**: Analyze the biomed-fr corpus composition by source (ISTEX, CLEAR, E3C) and document type to determine if certain domains or document types contribute more to the performance improvement, potentially guiding more efficient corpus construction.

3. **Evaluation protocol reconciliation**: Contact the DrBERT authors to obtain detailed information about their experimental setup, including random seeds, hyperparameter ranges, and dataset preprocessing, to enable a more accurate direct comparison between the two models.