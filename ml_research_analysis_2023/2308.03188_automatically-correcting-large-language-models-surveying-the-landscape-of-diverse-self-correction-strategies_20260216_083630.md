---
ver: rpa2
title: 'Automatically Correcting Large Language Models: Surveying the landscape of
  diverse self-correction strategies'
arxiv_id: '2308.03188'
source_url: https://arxiv.org/abs/2308.03188
tags:
- feedback
- language
- reasoning
- corr
- correction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper surveys the landscape of techniques for automatically
  correcting large language models (LLMs) using automated feedback. It taxonomizes
  existing works along five key dimensions: what gets corrected, the source and format
  of feedback, and the strategy and learning method of the refinement process.'
---

# Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies

## Quick Facts
- arXiv ID: 2308.03188
- Source URL: https://arxiv.org/abs/2308.03188
- Authors: 
- Reference count: 40
- Primary result: This paper surveys techniques for automatically correcting large language models using automated feedback, taxonomizing existing works along five key dimensions and reviewing three major correction strategies.

## Executive Summary
This paper provides a comprehensive survey of techniques for automatically correcting large language models (LLMs) using automated feedback. The authors taxonomize existing works along five key dimensions: what gets corrected, the source and format of feedback, and the strategy and learning method of the refinement process. The paper reviews three major strategies for self-correction: training-time correction (fine-tuning with feedback), generation-time correction (feedback-guided decoding), and post-hoc correction (iterative refinement after generation). It also summarizes major application areas and discusses future directions and challenges in the field.

## Method Summary
The paper surveys the landscape of self-correction techniques for LLMs by reviewing existing literature and organizing it into a taxonomy based on five key dimensions. The authors identify three major correction strategies: training-time correction (which includes learning from human feedback, reward modeling, and self-training), generation-time correction (generate-then-rank and feedback-guided decoding), and post-hoc correction (self-correction, correction with external feedback, and multi-agent debate). The survey focuses on understanding how these strategies address common LLM errors such as hallucination, unfaithful reasoning, toxic content, and flawed code.

## Key Results
- Self-correction strategies can be categorized into training-time, generation-time, and post-hoc approaches based on when refinement occurs
- Automated feedback sources include self-feedback, external models/tools/knowledge, and external metrics, each with different reliability and applicability profiles
- Different error types (hallucination, unfaithful reasoning, toxic content, flawed code) may require different correction strategies and feedback formats

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Iterative refinement guided by automated feedback improves output quality by targeting specific error types (hallucination, unfaithful reasoning, toxic content, flawed code).
- **Mechanism:** The model generates output → a critic model evaluates and provides feedback → a refine model uses this feedback to correct the output, either through parameter updates (training-time) or output modification (generation/post-hoc time).
- **Core assumption:** The critic model can reliably identify errors and the refine model can effectively incorporate feedback to improve outputs.
- **Evidence anchors:**
  - [abstract] "Techniques leveraging automated feedback... are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback."
  - [section 2.2] "LLM-based natural language systems can exhibit a variety of errors... We summarize the four major types of errors that are targeted for correction in existing works through automated feedback."
  - [corpus] Weak corpus coverage; most related papers focus on self-correction but don't explicitly validate the feedback loop reliability.
- **Break condition:** If the critic model produces unreliable or misleading feedback, the refinement process may introduce new errors or fail to correct existing ones.

### Mechanism 2
- **Claim:** Self-correction with external tools (code interpreters, knowledge bases, verifiers) is particularly effective for tasks with objective correctness criteria.
- **Mechanism:** The model generates code or reasoning → an external tool executes or verifies the output → feedback from the tool guides the model to refine its solution.
- **Core assumption:** External tools can provide deterministic, objective feedback that the model can use to correct its outputs.
- **Evidence anchors:**
  - [section 5.2] "Code Interpreter... The program executor is frequently used as a source of feedback for refining the initial code written by the model."
  - [section 5.2] "Logic Reasoner... Tool-assisted feedback has also been used to enhance the faithfulness of LLMs' reasoning."
  - [corpus] Related work on tool-augmented LLMs supports this, but specific empirical validation in the corpus is limited.
- **Break condition:** If the external tool is too slow, unavailable, or produces ambiguous feedback, the self-correction process may become impractical or ineffective.

### Mechanism 3
- **Claim:** Multi-agent debate between LLM instances can improve output quality by leveraging diverse perspectives and collaborative refinement.
- **Mechanism:** Multiple LLM instances generate individual solutions → they debate and critique each other's solutions → consensus or improved solutions emerge through iterative refinement.
- **Core assumption:** Diverse LLM instances can provide complementary critiques that lead to better overall solutions than individual self-correction.
- **Evidence anchors:**
  - [section 5.3] "Du et al. (2023) first applied and evaluated this strategy in arithmetic reasoning tasks... The models are found to converge on a shared solution following multiple debate iterations."
  - [section 5.3] "LM vs LM (Cohen et al., 2023) further demonstrated the effectiveness of multi-agent debate for detecting factual errors."
  - [corpus] Limited direct evidence in corpus; related works focus more on single-agent self-correction.
- **Break condition:** If the LLM instances are too similar or lack diverse perspectives, the debate may not lead to meaningful improvements.

## Foundational Learning

- **Concept:** Automated feedback generation and utilization
  - **Why needed here:** Understanding how automated feedback is generated (by critic models, external tools, or the LLM itself) and how it's used to guide model refinement is fundamental to implementing self-correction systems.
  - **Quick check question:** What are the different sources of automated feedback, and how do they differ in terms of reliability and applicability?

- **Concept:** Iterative refinement and convergence
  - **Why needed here:** Self-correction often involves multiple iterations of feedback and refinement. Understanding convergence criteria and potential failure modes is crucial for practical implementation.
  - **Quick check question:** How do you determine when to stop the self-correction process, and what are the signs of non-convergence or degradation?

- **Concept:** Error types and correction strategies
  - **Why needed here:** Different error types (hallucination, unfaithful reasoning, toxic content, flawed code) may require different correction strategies and feedback formats.
  - **Quick check question:** How do you match specific error types with appropriate correction strategies and feedback mechanisms?

## Architecture Onboarding

- **Component map:** Input → LLM generates initial output → Critic model generates feedback → Refine model generates improved output → (Optional) External tool verification → Repeat until convergence

- **Critical path:**
  1. Input → LLM generates initial output
  2. Output + input → Critic model generates feedback
  3. Output + feedback → Refine model generates improved output
  4. (Optional) Improved output → External tool verification
  5. Repeat until convergence or maximum iterations

- **Design tradeoffs:**
  - Feedback granularity: Step-level vs. output-level feedback
  - Feedback format: Scalar values vs. natural language vs. structured data
  - Feedback source: Self-generated vs. external vs. mixed
  - Refinement timing: Training-time vs. generation-time vs. post-hoc
  - Model access: Full parameter access vs. black-box API vs. constrained fine-tuning

- **Failure signatures:**
  - Critic model produces unreliable or misleading feedback
  - Refine model fails to effectively incorporate feedback
  - Self-correction process introduces new errors or degrades performance
  - Convergence issues (non-convergence or excessive iterations)
  - External tool dependencies cause delays or failures

- **First 3 experiments:**
  1. Implement a simple generate-then-rank system with scalar value feedback for a basic text generation task (e.g., summarization)
  2. Add external tool integration (e.g., code execution) to the system and evaluate performance on code generation tasks
  3. Implement a multi-agent debate system for arithmetic reasoning and compare performance with single-agent self-correction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical principles underlying LLM self-correction capabilities, particularly regarding metacognitive awareness and calibration?
- Basis in paper: [explicit] The paper discusses the lack of theoretical justifications for self-correction and mentions metacognitive awareness and calibration as important factors.
- Why unresolved: While the paper acknowledges these concepts, it does not provide a theoretical framework or empirical evidence to support their role in self-correction.
- What evidence would resolve it: Studies demonstrating how metacognitive awareness and calibration directly impact the effectiveness of self-correction strategies, potentially through controlled experiments or mathematical modeling.

### Open Question 2
- Question: How can we effectively measure and compare the self-correction capabilities of different LLMs and strategies?
- Basis in paper: [explicit] The paper highlights the need for robust quantitative metrics to evaluate self-correction abilities and compare different approaches.
- Why unresolved: Current research relies on empirical evidence rather than standardized metrics, making it difficult to objectively assess and compare self-correction performance across models and strategies.
- What evidence would resolve it: Development and validation of comprehensive evaluation frameworks that account for task complexity, initial error severity, and quality improvement after self-correction, along with diagnostic datasets for standardized testing.

### Open Question 3
- Question: What are the challenges and potential solutions for enabling continual, lifelong self-improvement in LLMs?
- Basis in paper: [explicit] The paper discusses the potential of continual self-improvement but notes challenges such as catastrophic forgetting and the need for integrating different self-correction techniques.
- Why unresolved: While the concept is promising, there is limited research on implementing and evaluating continual self-improvement in LLMs, particularly regarding the long-term stability and effectiveness of such approaches.
- What evidence would resolve it: Empirical studies demonstrating the feasibility and benefits of continual self-improvement in LLMs, including strategies to mitigate catastrophic forgetting and effectively combine different self-correction techniques over time.

## Limitations
- The taxonomy may not capture all emerging correction strategies as the field evolves rapidly
- Many described techniques lack direct empirical validation within the survey itself
- The effectiveness of self-correction mechanisms heavily depends on the quality of automated feedback, which remains a critical bottleneck

## Confidence
- **High Confidence:** The taxonomy framework (5 dimensions) and categorization of correction strategies (training-time, generation-time, post-hoc) are well-grounded and reflect the current research landscape accurately.
- **Medium Confidence:** Claims about specific effectiveness of strategies like multi-agent debate or external tool integration are supported by cited works but would benefit from systematic comparative evaluation across diverse tasks.
- **Low Confidence:** Predictions about future directions and emerging challenges are necessarily speculative given the field's rapid evolution.

## Next Checks
1. **Empirical validation:** Implement and test the generate-then-rank strategy on a benchmark summarization task to verify claims about its effectiveness relative to other correction approaches.
2. **Feedback quality analysis:** Systematically evaluate the reliability of different feedback sources (self-feedback vs. external models vs. tool-based) on a common error detection task.
3. **Cross-task generalization:** Test whether correction strategies effective for one error type (e.g., code errors) transfer to other domains (e.g., reasoning or factual accuracy) with minimal adaptation.