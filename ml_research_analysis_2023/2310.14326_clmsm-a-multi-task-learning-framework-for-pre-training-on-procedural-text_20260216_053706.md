---
ver: rpa2
title: 'CLMSM: A Multi-Task Learning Framework for Pre-training on Procedural Text'
arxiv_id: '2310.14326'
source_url: https://arxiv.org/abs/2310.14326
tags:
- clmsm
- dataset
- entity
- recipe
- recipes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLMSM, a multi-task learning framework that
  combines contrastive learning (CL) using hard triplets and a novel Mask-Step Modeling
  (MSM) objective for pre-training on procedural text. The framework learns fine-grained
  differences across entities and step-wise context in procedures.
---

# CLMSM: A Multi-Task Learning Framework for Pre-training on Procedural Text

## Quick Facts
- **arXiv ID**: 2310.14326
- **Source URL**: https://arxiv.org/abs/2310.14326
- **Reference count**: 15
- **Primary result**: CLMSM achieves 1.28%, 2.56%, and 4.28% performance improvements on tracking entities and aligning actions across procedures compared to baselines.

## Executive Summary
This paper introduces CLMSM, a multi-task learning framework that combines contrastive learning using hard triplets and a novel Mask-Step Modeling objective for pre-training on procedural text. The framework learns fine-grained differences across entities and step-wise context in procedures by pulling similar procedures closer and pushing slightly dissimilar ones apart, while predicting tokens in randomly masked steps. CLMSM is pre-trained on a large corpus of recipes and evaluated on two downstream tasks: tracking entities and aligning actions across procedures. Results show that CLMSM outperforms baselines on both in-domain (recipes) and open-domain (ProPara, ARA) tasks, achieving 1.28%, 2.56%, and 4.28% performance improvements respectively.

## Method Summary
CLMSM is a multi-task learning framework that pre-trains on procedural text using two complementary objectives: contrastive learning with hard triplets and mask-step modeling. The framework uses a triplet network with shared-parameter encoders to learn fine-grained differences between similar procedures by sampling hard negatives, and a MSM layer that predicts all tokens in randomly masked steps to capture step-wise context. The pre-training corpus consists of over 2.8 million recipes modified by adding ingredients as an extra step, and the model is fine-tuned on downstream tasks including NPN-Cooking, ProPara, and ARA datasets.

## Key Results
- CLMSM outperforms baselines on both in-domain (recipes) and open-domain (ProPara, ARA) tasks
- Achieves 1.28%, 2.56%, and 4.28% performance improvements on tracking entities and aligning actions across procedures
- Demonstrates strong generalization capabilities beyond recipe domain
- Shows that masking entire steps is more effective than masking random spans
- Validates that hard triplets are crucial for learning fine-grained differences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hard triplet contrastive learning enables fine-grained entity and action differentiation by pulling similar procedures closer and pushing slightly dissimilar ones apart.
- Mechanism: Triplet loss with hard negatives creates a discriminative embedding space where procedural representations capture subtle differences in ingredients and actions.
- Core assumption: Small semantic differences between similar recipes (e.g., banana vs. strawberry smoothie) provide meaningful supervision signals for procedural understanding.
- Evidence anchors:
  - [abstract]: "Contrastive Learning using hard triplets to learn fine-grained differences across entities in the procedures"
  - [section 2.4]: "We observe that (1) most of the ingredients are common, making the anchor and positive highly similar, and the anchor and negative marginally dissimilar"
- Break condition: If hard triplets are replaced with easy triplets, the model loses the ability to capture fine-grained differences, as shown by CLMSM(EASY) underperforming CLMSM.

### Mechanism 2
- Claim: Masking entire steps (rather than random spans) provides stronger supervision for learning procedural context by forcing the model to understand step-wise dependencies.
- Mechanism: MSM loss requires predicting all tokens in masked steps, which implicitly teaches the model about the functional role and dependencies of each step within the procedure.
- Core assumption: Step-level masking captures the atomic unit of procedural reasoning better than token-level masking because steps represent complete actions.
- Evidence anchors:
  - [abstract]: "a novel Mask-Step Modelling objective to learn step-wise context of a procedure"
  - [section 2.2]: "MSM considers a step as a single entity and learns step-wise context, as it predicts tokens of randomly masked step(s)"
  - [section 4.4.2]: "masking an entire step is much more effective than masking a random span"
- Break condition: If random span masking is used instead (CLMSM(RS)), performance drops significantly, indicating step-level context is crucial.

### Mechanism 3
- Claim: Multi-task learning on contrastive learning and step-masking objectives provides complementary supervision that improves both entity tracking and action alignment.
- Mechanism: Simultaneous optimization of CL and MSM losses creates representations that capture both fine-grained entity differences and global procedural structure.
- Core assumption: The two objectives address different aspects of procedural understanding that are both necessary for downstream tasks.
- Evidence anchors:
  - [abstract]: "CLMSM uses a Multi-Task Learning Framework to optimize two objectives"
  - [section 4.4.2]: "CLMSM RB performs better than CLMSM RB(CAS.), suggesting that performing MTL on CL and MSM objectives is indeed helpful"
- Break condition: If objectives are trained sequentially (CLMSM(CAS.)) instead of simultaneously, performance degrades, showing the importance of joint optimization.

## Foundational Learning

- Concept: Contrastive Learning
  - Why needed here: To learn fine-grained differences between similar procedures that share most ingredients/actions but differ in key details
  - Quick check question: Why use hard triplets instead of random negative samples for contrastive learning in procedural text?

- Concept: Masked Language Modeling
  - Why needed here: To learn local context and token-level dependencies within steps
  - Quick check question: How does masking entire steps differ from traditional MLM in terms of the contextual information learned?

- Concept: Multi-Task Learning
  - Why needed here: To combine complementary objectives that capture different aspects of procedural reasoning
  - Quick check question: What advantage does simultaneous optimization of CL and MSM provide over sequential training?

## Architecture Onboarding

- Component map: Triplet network (anchor, positive, negative) -> MSM layer -> MTL loss combiner -> Transformer model
- Critical path:
  1. Sample hard triplets based on recipe similarity
  2. Apply step-masking to each recipe
  3. Compute triplet loss and MSM loss simultaneously
  4. Backpropagate combined loss
  5. Fine-tune on downstream task

- Design tradeoffs:
  - Hard triplet selection adds preprocessing overhead but provides better supervision
  - Step-masking reduces effective training examples but captures procedural structure
  - MTL increases computational cost but improves generalization

- Failure signatures:
  - Poor performance on ProPara suggests domain generalization issues
  - Underperformance vs. RECIPESRL on Cat.3 indicates location span prediction problems
  - Sensitivity to Î± scaling factor shows loss balance is critical

- First 3 experiments:
  1. Compare CLMSM with CLMSM(CL) and CLMSM(MSM) to verify MTL benefits
  2. Test CLMSM(RS) vs. CLMSM to confirm step-masking superiority
  3. Evaluate CLMSM(EASY) vs. CLMSM to validate hard triplet effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed pre-training framework perform on other procedural domains beyond recipes?
- Basis in paper: [explicit] The authors state that their work performs pre-training on procedural text in the domain of recipes and acknowledge that the work can be extended to other domains containing procedural text.
- Why unresolved: The paper focuses on pre-training on recipes and does not provide experimental results or analysis for other procedural domains.
- What evidence would resolve it: Experimental results showing the performance of the proposed pre-training framework on other procedural domains, such as scientific processes or E-Manuals, would provide insights into its generalizability.

### Open Question 2
- Question: What is the impact of using different metadata for sampling similar and dissimilar recipes in the contrastive learning objective?
- Basis in paper: [explicit] The authors mention that they use recipe names to sample similar and dissimilar recipes for the contrastive learning objective, but they also acknowledge that other metadata could be used.
- Why unresolved: The paper does not explore the impact of using different metadata for sampling recipes in the contrastive learning objective.
- What evidence would resolve it: Experimental results comparing the performance of the pre-training framework when using different metadata (e.g., cuisine, ingredients) for sampling similar and dissimilar recipes would provide insights into the importance of metadata selection.

### Open Question 3
- Question: How does the proposed pre-training framework handle procedures with a mixture of factual and procedural text?
- Basis in paper: [explicit] The authors mention that for text from domains like scientific processes or E-Manuals that contain a mixture of factual and procedural text, an additional pre-processing stage of extracting procedural text needs to be devised.
- Why unresolved: The paper does not provide details on how to handle procedures with a mixture of factual and procedural text or experimental results in such scenarios.
- What evidence would resolve it: Experimental results showing the performance of the pre-training framework on procedures with a mixture of factual and procedural text, along with a proposed pre-processing approach, would provide insights into its effectiveness in handling such cases.

## Limitations
- Evaluation focuses primarily on recipe-based datasets with limited testing on open-domain procedural text
- Hard triplet sampling method relies on a heuristic that may not scale well to domains with less structured procedures
- Modification of recipes by adding ingredients as an explicit step could introduce evaluation artifacts

## Confidence
- High confidence: The MTL framework combining CL and MSM objectives provides measurable performance benefits over single-task variants
- Medium confidence: Step-masking is superior to random span masking for capturing procedural context
- Medium confidence: Hard triplets are necessary for fine-grained entity differentiation
- Low confidence: The framework's strong performance on open-domain tasks indicates robust generalization beyond recipes

## Next Checks
1. Evaluate CLMSM on additional procedural domains (scientific protocols, assembly instructions) to verify claims of robust generalization beyond cooking recipes and the limited ProPara/ARA datasets.
2. Test CLMSM performance with unmodified recipes (no ingredient step injection) to determine whether the preprocessing step contributes to the reported improvements or introduces evaluation artifacts.
3. Systematically vary the similarity thresholds and selection criteria for hard triplet sampling to quantify the robustness of the contrastive learning component and identify optimal parameter ranges for different procedural domains.