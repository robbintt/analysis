---
ver: rpa2
title: Geometry-Aware Adaptation for Pretrained Models
arxiv_id: '2307.12226'
source_url: https://arxiv.org/abs/2307.12226
tags:
- locus
- classes
- loki
- metric
- cover
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes LOKI, a simple adaptation method for pretrained\
  \ models that exploits label space geometry. LOKI replaces standard argmax with\
  \ the Fr\xE9chet mean, enabling prediction of unobserved classes."
---

# Geometry-Aware Adaptation for Pretrained Models

## Quick Facts
- arXiv ID: 2307.12226
- Source URL: https://arxiv.org/abs/2307.12226
- Authors: 
- Reference count: 40
- Primary result: LOKI improves CLIP predictions by 10.57% using internal metrics and 19.53% using WordNet hierarchy on CIFAR-100

## Executive Summary
This paper introduces LOKI, a geometry-aware adaptation method for pretrained models that replaces standard argmax prediction with Fréchet mean computation over label space metrics. The approach enables models to predict unobserved classes by leveraging the metric structure between labels, theoretically characterized through locus coverage and sample complexity bounds. Empirically, LOKI demonstrates consistent improvements across multiple datasets and models, with gains particularly pronounced when using semantically meaningful label hierarchies like WordNet.

## Method Summary
LOKI is a drop-in replacement for standard prediction that swaps argmax with Fréchet mean computation. Given a pretrained model's probability vector over classes, LOKI computes a weighted barycenter in the label metric space using distances between all class pairs. This enables prediction of any class within the locus of the Fréchet mean, not just the observed classes. The method requires no additional training and can be applied to any pretrained model by linearly transforming its output probabilities using the label space distance matrix.

## Key Results
- LOKI improves CLIP predictions by 10.57% using internal metrics and 19.53% using WordNet hierarchy on CIFAR-100
- Consistent improvements observed on ImageNet, PubMed, and LSHTC datasets
- Active class selection further improves performance by strategically expanding the locus of predictable classes
- Method scales to hundreds of thousands of classes while maintaining performance gains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Replacing argmax with Fréchet mean weighted by class prediction probabilities enables generalization to unobserved classes by leveraging metric structure between labels.
- **Mechanism**: The Fréchet mean computes a weighted barycenter over the metric space of labels, where weights come from the model's per-class probabilities. This allows the prediction to fall on any point in the locus of the Fréchet mean, not just the observed classes.
- **Core assumption**: The model's per-class probabilities are meaningful and the metric distance between labels reflects semantic similarity.
- **Evidence anchors**:
  - [abstract]: "Our technique is a drop-in replacement of the standard prediction rule, swapping arg max with the Fréchet mean."
  - [section 3.1]: "It relies on computing the Fréchet mean [8], given by my(w) := arg min y∈Y KX i=1 wid2(y, yi), where w ∈ RK ≥0 is a set of weights."
- **Break condition**: If the metric space is poorly constructed (e.g., random distances) or the model outputs are poorly calibrated, the Fréchet mean will not reflect meaningful semantics.

### Mechanism 2
- **Claim**: The locus of the Fréchet mean defines the set of reachable classes; if the locus covers the full label space, any class can be predicted.
- **Mechanism**: For a set of observed classes Λ, the locus Π(Λ) is the set of all possible Fréchet means under different weightings. If Π(Λ) = Y, then every class in Y can be predicted from Λ.
- **Core assumption**: The metric space has enough structure that a small subset of classes can span the entire label space via weighted Fréchet means.
- **Evidence anchors**:
  - [abstract]: "We provide a comprehensive theoretical analysis... characterizations of the full range of scenarios in which it is possible to predict any unobserved class."
  - [section 4.2]: "We characterize the sets of observed classes that are required to enable prediction of any class, and show how this set differs for various types of metric spaces of interest."
- **Break condition**: If the metric space is too sparse or disconnected, the locus may not cover the full label space even with many observed classes.

### Mechanism 3
- **Claim**: Active selection of next classes to observe maximizes the size of the locus, enabling prediction of more classes with fewer observations.
- **Mechanism**: By selecting the next class to observe based on maximizing the expansion of the locus, we can efficiently grow the set of predictable classes.
- **Core assumption**: The metric space is known and we can compute the locus efficiently for small sets of classes.
- **Evidence anchors**:
  - [abstract]: "we show how to exploit this result in an active learning-like approach to selecting points that will improve deficient training datasets."
  - [section 4.3]: "We now turn to actively selecting the next class to observe in order to maximize the size of the locus."
- **Break condition**: If the metric space is very large or the locus computation is intractable, active selection becomes impractical.

## Foundational Learning

- **Concept**: Metric spaces and graph metrics
  - Why needed here: The method relies on distances between labels to compute the Fréchet mean and define the locus.
  - Quick check question: Given a tree graph with leaf nodes {A,B,C} and internal nodes {D,E}, what is the shortest path distance between A and E?
- **Concept**: Fréchet mean and variance
  - Why needed here: The Fréchet mean is the core computation that replaces argmax and enables prediction over the metric space.
  - Quick check question: For weights w=[0.3,0.7] and labels y1, y2 with d(y,y1)=2, d(y,y2)=5, what is the Fréchet variance?
- **Concept**: Locus and locus cover
  - Why needed here: Understanding when a set of observed classes can predict all others depends on the locus covering the full label space.
  - Quick check question: If Λ={A,B} and Π(Λ)={A,B,C}, is Λ a locus cover for Y={A,B,C,D}?

## Architecture Onboarding

- **Component map**: Model outputs → weight vector → Fréchet mean computation → final prediction
- **Critical path**: Model inference → probability vector → linear transformation (distance matrix) → Fréchet mean argmin → output class
- **Design tradeoffs**: Using external metrics vs self-derived metrics; computational cost of locus computation vs prediction quality; active vs random class selection
- **Failure signatures**: Low improvement over baseline indicates poor metric structure or model calibration; high variance in predictions suggests unstable locus; slow active selection indicates computational bottleneck
- **First 3 experiments**:
  1. Compare LOKI vs argmax on a small synthetic dataset with known metric structure (e.g., grid or tree)
  2. Test locus coverage for random vs optimal observed class subsets on the same synthetic dataset
  3. Measure active selection performance vs random selection on a tree-structured label space

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the content and limitations, several key unresolved issues emerge:

### Open Question 1
- **Question**: What are the theoretical limits of LOKI's performance on extremely high-cardinality label spaces (e.g., millions of classes) compared to standard fine-tuning approaches?
- **Basis in paper**: [explicit] The paper mentions LOKI "scales to hundreds of thousands of classes" and shows gains on ImageNet (1000 classes) and LSHTC (325,056 classes), but doesn't analyze scaling to truly massive label spaces.
- **Why unresolved**: The paper focuses on demonstrating LOKI works on moderately large label spaces but doesn't establish theoretical or empirical bounds for extreme scalability.
- **What evidence would resolve it**: Comprehensive experiments comparing LOKI to fine-tuning on label spaces with 1M+ classes, plus theoretical analysis of how sample complexity and prediction error scale with label space size.

### Open Question 2
- **Question**: How does LOKI's performance degrade when the provided metric space is misspecified or contains noisy/incorrect distance information?
- **Basis in paper**: [inferred] The paper uses external metrics like WordNet and internal embeddings, but doesn't study robustness to metric errors or provide guarantees under metric misspecification.
- **Why unresolved**: All experiments use well-structured metrics, and theoretical analysis assumes access to a correct metric space.
- **What evidence would resolve it**: Experiments deliberately corrupting the metric space with noise or errors, plus theoretical analysis of error bounds as a function of metric quality.

### Open Question 3
- **Question**: Can LOKI be effectively combined with other zero-shot learning techniques (like semantic embeddings or generative models) to further improve performance?
- **Basis in paper**: [explicit] The paper mentions LOKI works with CLIP's internal metrics and compares to SimCLR, but doesn't explore hybrid approaches with other ZSL methods.
- **Why unresolved**: Experiments focus on comparing LOKI against baselines rather than exploring combinations with complementary techniques.
- **What evidence would resolve it**: Experiments combining LOKI with semantic embeddings, generative ZSL models, or other complementary approaches, measuring whether performance gains are additive or synergistic.

## Limitations
- Theoretical analysis assumes access to a well-structured metric space, but real-world metrics may be noisy or incomplete
- Empirical validation is limited to specific datasets and pretrained models without ablation studies isolating geometric contribution
- Active selection mechanism lacks rigorous empirical validation beyond synthetic examples
- Performance depends heavily on quality of provided metric space and model calibration

## Confidence

- Fréchet mean mechanism: **High** - The mathematical formulation is clear and theoretically grounded
- Locus coverage theory: **Medium** - Theoretical bounds exist but real-world metric spaces may violate assumptions
- Empirical improvements: **Medium** - Results are positive but dataset-specific and lack broader validation
- Active selection efficacy: **Low** - Only briefly mentioned with minimal experimental support

## Next Checks

1. **Ablation study**: Test LOKI with random vs semantically meaningful distance matrices on the same datasets to isolate geometric contribution
2. **Generalization test**: Apply LOKI to a completely different model family (e.g., BERT for text classification) with hierarchical label structures
3. **Metric sensitivity**: Systematically vary distance matrix construction methods (WordNet vs internal embeddings vs random) and measure impact on prediction quality across datasets