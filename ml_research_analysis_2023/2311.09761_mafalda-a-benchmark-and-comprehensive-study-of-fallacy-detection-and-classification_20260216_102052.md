---
ver: rpa2
title: 'MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification'
arxiv_id: '2311.09761'
source_url: https://arxiv.org/abs/2311.09761
tags:
- fallacy
- fallacies
- appeal
- level
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MAFALDA, a benchmark dataset for fallacy
  detection and classification. It unifies multiple fallacy datasets into a three-level
  taxonomy covering 23 types of fallacies, from broad categories to fine-grained types.
---

# MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and Classification

## Quick Facts
- arXiv ID: 2311.09761
- Source URL: https://arxiv.org/abs/2311.09761
- Authors: 
- Reference count: 20
- Key outcome: Introduces MAFALDA, a benchmark dataset for fallacy detection and classification with 200 annotated texts across a three-level taxonomy of 23 fallacy types, showing smaller fine-tuned models can outperform larger models in zero-shot settings.

## Executive Summary
This paper introduces MAFALDA, a comprehensive benchmark dataset for fallacy detection and classification that unifies multiple existing fallacy datasets into a three-level taxonomy covering 23 types of fallacies. The dataset addresses the inherent subjectivity of fallacy annotation by implementing a novel scheme that allows alternative labels and provides a tailored evaluation metric. Through extensive evaluation of large language models in zero-shot settings, the study reveals that smaller, fine-tuned models can outperform larger models in fallacy detection tasks, with performance decreasing as classification granularity increases.

## Method Summary
The MAFALDA benchmark builds upon existing fallacy datasets by reconstructing their texts and implementing a consensus-based annotation process with three Ph.D. students and GPT-4. The authors propose a novel annotation scheme that explicitly represents subjectivity through disjunctions of conjunctions, allowing multiple valid label combinations. A custom evaluation methodology with adapted precision, recall, and F1-score metrics handles these alternative annotations. The benchmark evaluates models across three levels of granularity: binary detection, broad categories, and fine-grained types, using zero-shot prompting with pattern matching to extract labels from model outputs.

## Key Results
- Smaller, fine-tuned models can outperform larger models in fallacy detection tasks
- Model performance decreases as classification granularity increases from detection to fine-grained classification
- The custom evaluation metrics that handle alternative annotations provide more nuanced assessment than traditional metrics
- 28% of annotated spans contain at least one alternative label, validating the need for the proposed subjective annotation scheme

## Why This Works (Mechanism)

### Mechanism 1
The subjective annotation scheme with disjunctions of conjunctions allows the system to handle multiple valid interpretations of the same text span. Instead of forcing a single label, the annotation system allows multiple valid label combinations connected by logical OR (∨) within mandatory AND (∧) structures. This means that if a text can be validly annotated with either "fallacy A" or "fallacy B" along with "fallacy C," both combinations are accepted as correct. The core assumption is that fallacies are inherently subjective, and different annotators can reasonably arrive at different but equally valid interpretations of the same text.

### Mechanism 2
The custom precision, recall, and F1-score metrics that allow for alternative gold standard annotations provide a more accurate evaluation of fallacy detection models than traditional metrics. The metrics compute the best match between predicted spans and any of the alternative gold standard annotations, then normalize by the total count of spans in the prediction (for precision) or the total count of gold standard spans (for recall). This allows a model to score well even if it predicts a different but equally valid label than what was used in one specific gold standard alternative. The core assumption is that traditional single-label evaluation is too strict for subjective tasks and doesn't account for legitimate alternative interpretations.

### Mechanism 3
The three-level granularity taxonomy (binary detection, broad categories, fine-grained types) allows for progressive evaluation of model capabilities and captures different aspects of fallacy understanding. By evaluating models at three distinct levels - whether text contains any fallacy (Level 0), which broad category of fallacy it contains (Level 1), and which specific type of fallacy it contains (Level 2) - the benchmark can assess both general detection ability and fine-grained classification skills. The levels are hierarchically related, so predictions at Level 2 can be used to infer Levels 1 and 0. The core assumption is that fallacy detection and classification are distinct skills that can be evaluated separately, and models may perform differently at different levels of granularity.

## Foundational Learning

- Concept: Fallacy as invalid argument
  - Why needed here: The entire dataset and taxonomy are built on the formal definition that a fallacy is an argument where premises do not entail the conclusion. Understanding this distinction is crucial for proper annotation and evaluation.
  - Quick check question: Is "Paris is the capital of England" a fallacy? (Answer: No, it's a false assertion but not a fallacy because it's not an argument)

- Concept: Subjective annotation and inter-annotator agreement
  - Why needed here: The paper explicitly addresses the subjectivity of fallacy annotation and the low inter-annotator agreement in previous work. Understanding this context is essential for appreciating the need for the proposed annotation scheme.
  - Quick check question: What was the best inter-annotator agreement score obtained when asking GPT-4 and three Ph.D. students to annotate 21 samples? (Answer: 0.4, which is very low)

- Concept: Multi-label classification and evaluation metrics
  - Why needed here: The task involves multi-label classification (a text can contain multiple fallacies), and the paper proposes custom metrics to handle this. Understanding standard multi-label metrics helps appreciate the modifications proposed.
  - Quick check question: In standard multi-label precision, what is the denominator? (Answer: Total number of predicted labels)

## Architecture Onboarding

- Component map:
  Data pipeline: Source datasets → Filtering → Text reconstruction → Annotation → MAFALDA dataset
  Annotation system: Doccano interface → Consensus requirement → Multiple annotators → Alternative labels
  Taxonomy system: Aristotle's categories → Three-level hierarchy → Formal and informal definitions
  Evaluation system: Custom precision/recall/F1 metrics → Alternative gold standards → Pattern matching for LLM outputs
  Model evaluation: Zero-shot prompting → Sentence-level processing → Span construction → Performance comparison

- Critical path:
  1. Select and prepare source texts
  2. Reach consensus annotation for each text
  3. Build unified taxonomy from source definitions
  4. Apply custom metrics to evaluate model predictions
  5. Compare model performance across granularity levels

- Design tradeoffs:
  - Single vs. multiple annotations: Choosing to allow alternatives increases dataset complexity but better reflects reality
  - Sentence vs. token level: Choosing sentences reduces annotation burden but may miss fine-grained fallacies
  - Zero-shot vs. fine-tuned: Zero-shot allows comparison across models but limits performance potential

- Failure signatures:
  - Low inter-annotator agreement even with consensus process indicates fundamental subjectivity issues
  - Pattern matching failing to extract labels from LLM outputs suggests format incompatibility
  - Large performance gaps between 7B and 13B models at fine granularity may indicate insufficient data for fine-tuning

- First 3 experiments:
  1. Run pattern matching on LLM outputs for a small validation set to ensure label extraction works correctly
  2. Test the custom metrics on synthetic data with known alternative annotations to verify they behave as expected
  3. Evaluate a simple baseline model (e.g., random or always "no fallacy") to establish performance floors at each granularity level

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of fine-tuned models on MAFALDA compare to their performance on other fallacy detection datasets, and what factors contribute to any differences? The paper does not provide a direct comparison between the performance of fine-tuned models on MAFALDA and their performance on other fallacy detection datasets. A comprehensive study comparing the performance of fine-tuned models on MAFALDA and other fallacy detection datasets, analyzing the factors that contribute to any differences in performance, would resolve this question.

### Open Question 2
How does the subjectivity of fallacy annotation impact the reliability of automated fallacy detection systems, and what methods can be employed to mitigate this impact? The paper introduces a new annotation scheme that allows for alternative labels and a tailored evaluation metric to handle subjectivity in fallacy annotation, but does not provide a detailed analysis of how this subjectivity impacts reliability or propose mitigation methods. A study examining the impact of subjectivity in fallacy annotation on the performance of automated fallacy detection systems, along with proposed methods to mitigate this impact and their effectiveness, would resolve this question.

### Open Question 3
How does the granularity of fallacy classification affect the performance of language models in detecting and classifying fallacies, and what are the implications for future research in this area? The paper introduces a three-level taxonomy of fallacies and evaluates the performance of language models at different levels of granularity, but does not provide a detailed analysis of how granularity affects performance or discuss implications for future research. A comprehensive study analyzing the impact of fallacy classification granularity on the performance of language models, along with recommendations for future research based on the findings, would resolve this question.

## Limitations
- The relatively small dataset size (200 texts with 268 annotated spans) may limit generalizability of findings
- The complex custom evaluation metrics with alternative annotations may make results difficult to interpret and compare with other benchmarks
- Zero-shot evaluation doesn't explore the potential of fine-tuning or few-shot approaches that might better leverage the dataset's structure

## Confidence
- High Confidence: Smaller, fine-tuned models outperforming larger models in fallacy detection is well-supported by experimental results
- Medium Confidence: The three-level granularity taxonomy effectiveness is reasonably well-supported but needs validation on larger corpora
- Medium Confidence: Custom evaluation metrics appear to work as intended but complexity introduces uncertainty about consistent application

## Next Checks
1. Conduct a follow-up study with additional annotators on a subset of the MAFALDA dataset to measure inter-annotator agreement using the proposed alternative label scheme
2. Test the custom evaluation metrics on synthetic data with known alternative annotations to verify they correctly handle edge cases
3. Evaluate the performance of fine-tuned models on the MAFALDA dataset to determine whether observed advantages of smaller models persist when models are specifically trained for the task