---
ver: rpa2
title: Fairness in Preference-based Reinforcement Learning
arxiv_id: '2306.09995'
source_url: https://arxiv.org/abs/2306.09995
tags:
- fairness
- pbrl
- learning
- welfare
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses fairness in preference-based reinforcement
  learning (PbRL) for multi-objective control problems. It proposes a new approach,
  FPbRL, which learns vector reward functions associated with multiple objectives
  using welfare-based preferences rather than reward-based preferences in standard
  PbRL.
---

# Fairness in Preference-based Reinforcement Learning

## Quick Facts
- arXiv ID: 2306.09995
- Source URL: https://arxiv.org/abs/2306.09995
- Reference count: 10
- One-line primary result: FPbRL achieves both efficiency and equity compared to standard PbRL and PPO baselines in three multi-objective domains

## Executive Summary
This paper addresses fairness in preference-based reinforcement learning (PbRL) for multi-objective control problems by proposing a new approach called FPbRL. The key innovation is learning vector reward functions associated with multiple objectives using welfare-based preferences rather than standard reward-based preferences. FPbRL couples this with policy learning via maximizing a generalized Gini welfare function, ensuring fairness across objectives. Experiments in three domains (species conservation, resource gathering, and traffic control) demonstrate that FPbRL achieves both efficiency and equity compared to standard PbRL and PPO baselines, as measured by welfare scores and balanced objective distributions.

## Method Summary
FPbRL learns vector reward functions associated with multiple objectives via new welfare-based preferences rather than reward-based preferences in standard PbRL. The approach uses a generalized Gini welfare function to compute preference probabilities between trajectory pairs, then updates the reward network using cross-entropy loss minimization. The policy is optimized using PPO with the welfare function as the objective. The method is evaluated across three domains: species conservation (sea otters and abalone), resource gathering (gold, gems, stones), and traffic control (intersection), comparing against PPO and standard PbRL baselines using welfare scores, individual objective distributions, coefficient of variation, and minimum/maximum objective values.

## Key Results
- FPbRL achieves the highest welfare scores across all three experimental domains
- FPbRL produces the most balanced objective distributions with lowest coefficient of variation
- FPbRL maximizes minimum objectives while minimizing variation between objectives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FPbRL learns vector reward functions associated with multiple objectives using welfare-based preferences rather than reward-based preferences in standard PbRL
- Mechanism: The approach modifies the preference probability calculation in PbRL to use a welfare function (generalized Gini) applied to discounted cumulative vector rewards, scalarizing them for preference comparison
- Core assumption: The generalized Gini welfare function properly captures fairness preferences across multiple objectives and can be used to derive meaningful trajectory preferences
- Evidence anchors: [abstract]: "The main idea of FPbRL is to learn vector reward functions associated with multiple objectives via new welfare-based preferences rather than reward-based preference in PbRL"; [section 3.1]: "P (σ1 ≻ σ2 | ˆr) = e ˆR(σ1) / (e ˆR(σ1) + e ˆR(σ2)), where ˆR(σi) := ϕw(Pk t=1 γt−1 ˆr(si t, ai t))"
- Break Condition: If the welfare function does not accurately represent human fairness preferences across objectives, the learned rewards will not optimize for equitable outcomes

### Mechanism 2
- Claim: FPbRL optimizes a generalized Gini welfare function to ensure fairness across objectives
- Mechanism: The policy gradient update incorporates the welfare function gradient, which weights objectives by their sorted utility values with decreasing weights
- Core assumption: The generalized Gini welfare function is concave and its gradient can be computed and used in policy updates
- Evidence anchors: [abstract]: "coupled with policy learning via maximizing a generalized Gini welfare function"; [section 4]: "∇θϕw(J(πθ)) =∇J(πθ)ϕw(J(πθ)) · ∇θJ(πθ) =w⊺ σ∇θJ(πθ)"; [section 3.2]: "the generalized Gini welfare function... computes the summation of the weight multiplied by the sorted utility for each objective"
- Break Condition: If the welfare function gradient becomes unstable or the sorting operation creates discontinuities in the optimization landscape

### Mechanism 3
- Claim: FPbRL achieves both efficiency and equity compared to standard PbRL and PPO baselines
- Mechanism: The method balances multiple objectives through welfare-based reward learning and policy optimization, producing solutions that maximize minimum objectives while minimizing variation between objectives
- Core assumption: The welfare function optimization naturally produces Pareto optimal and equitable solutions
- Evidence anchors: [abstract]: "Experiments in three domains... demonstrate that FPbRL achieves both efficiency and equity compared to standard PbRL and PPO baselines"; [section 5]: Multiple experiments showing FPbRL achieves highest welfare scores and lowest coefficient of variation across species conservation, resource gathering, and traffic control domains
- Break Condition: If the experimental domains do not adequately represent the fairness trade-offs in real-world applications

## Foundational Learning

- Concept: Preference-based reinforcement learning without explicit reward functions
  - Why needed here: FPbRL builds on PbRL framework but extends it to handle multiple objectives with fairness considerations
  - Quick check question: How does PbRL infer rewards from human preferences without explicit reward functions?

- Concept: Welfare functions and fairness criteria in multi-objective optimization
  - Why needed here: FPbRL uses generalized Gini welfare function to operationalize fairness across multiple objectives
  - Quick check question: What properties must a welfare function satisfy to ensure fairness in multi-objective settings?

- Concept: Policy gradient methods and proximal policy optimization
  - Why needed here: FPbRL uses PPO as the base policy optimization algorithm with modified objective incorporating welfare function
  - Quick check question: How does the policy gradient update change when optimizing a welfare function instead of direct rewards?

## Architecture Onboarding

- Component map: Preference collection -> Reward estimation network -> Policy network -> Environment interface

- Critical path:
  1. Collect trajectory pairs and human preferences
  2. Update reward estimation network using Bradley-Terry model with welfare-based preferences
  3. Compute policy gradient using welfare function gradient
  4. Update policy using PPO clipping mechanism
  5. Execute updated policy in environment and collect new trajectories

- Design tradeoffs:
  - Synthetic vs human preferences: Synthetic preferences are easier to obtain but may not capture true fairness preferences
  - Welfare function choice: Different welfare functions may optimize different aspects of fairness
  - Reward estimation frequency: More frequent updates may improve learning but increase computational cost

- Failure signatures:
  - Reward estimation network diverges or produces unrealistic reward values
  - Policy updates become unstable due to welfare function gradient computation
  - Experiments show no improvement in fairness metrics despite training

- First 3 experiments:
  1. Implement basic PbRL with scalar rewards on a simple environment, verify preference learning works
  2. Extend to vector rewards with simple averaging, verify multiple objectives can be learned
  3. Add generalized Gini welfare function to preference comparison, verify fairness properties emerge

## Open Questions the Paper Calls Out
- How does FPbRL perform with actual human feedback compared to synthetic human preferences?
- How do different welfare functions impact the fairness and efficiency trade-offs in FPbRL?
- How does FPbRL scale to problems with a large number of objectives?

## Limitations
- Experiments primarily use synthetic human preferences rather than real human feedback, limiting ecological validity
- Experimental domains may not capture the full complexity of real-world fairness trade-offs
- Welfare function choice (generalized Gini) may not align with all notions of fairness across different applications

## Confidence
- **High**: The core mechanism of using welfare-based preferences for reward learning is well-specified and theoretically grounded
- **Medium**: The experimental results showing improved fairness metrics compared to baselines, though limited to synthetic domains
- **Low**: The generalizability of results to real-world applications with human preferences and complex fairness considerations

## Next Checks
1. Test FPbRL with real human preference data rather than synthetic preferences to validate the preference learning mechanism
2. Evaluate FPbRL on additional domains with different fairness characteristics to assess robustness across application types
3. Compare FPbRL's generalized Gini welfare function against alternative fairness metrics to understand sensitivity to welfare function choice