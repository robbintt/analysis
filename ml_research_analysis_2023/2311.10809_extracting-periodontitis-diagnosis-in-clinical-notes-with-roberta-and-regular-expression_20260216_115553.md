---
ver: rpa2
title: Extracting periodontitis diagnosis in clinical notes with RoBERTa and regular
  expression
arxiv_id: '2311.10809'
source_url: https://arxiv.org/abs/2311.10809
tags:
- simple
- data
- notes
- were
- diagnosis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study explored the use of Named Entity Recognition (NER) models
  combined with regular expression (RE) methods to extract periodontitis diagnoses
  from clinical notes. Two RE approaches were used to generate training data: a simple
  RE method and an advanced RE method.'
---

# Extracting periodontitis diagnosis in clinical notes with RoBERTa and regular expression

## Quick Facts
- arXiv ID: 2311.10809
- Source URL: https://arxiv.org/abs/2311.10809
- Reference count: 22
- Primary result: NER models combined with regular expressions achieved 0.84-0.99 F1 scores for extracting periodontitis diagnoses from clinical notes

## Executive Summary
This study explores the use of Named Entity Recognition (NER) models combined with regular expression (RE) methods to extract periodontitis diagnoses from clinical notes. Two RE approaches were used to generate training data: a simple RE method and an advanced RE method. The SpaCy package and RoBERTa transformer models were used to build the NER model and evaluate its performance with the manual-labeled gold standards. The NER models demonstrated excellent predictions, with the simple RE method showing 0.84-0.92 in the evaluation metrics, and the advanced and combined RE method demonstrating 0.95-0.99 in the evaluation.

## Method Summary
The study employed two regular expression approaches to generate labeled training data from clinical notes: a simple RE method and an advanced RE method. These methods extracted diagnosis sections and structured information like Stage, Grade, and Extent. The SpaCy package and RoBERTa transformer models were then used to build and fine-tune the NER model. Post-processing was applied to standardize extracted labels before evaluation against manually-labeled gold standards. The approach successfully transformed unstructured clinical notes into structured diagnosis data with high accuracy.

## Key Results
- Simple RE method achieved 0.84-0.92 F1 scores for Stage, Grade, and Extent extraction
- Advanced RE method improved F1 scores to 0.95-0.99
- The combination of RE methods with NER models effectively extracted structured diagnosis information from free-text clinical notes
- Post-processing was essential for standardizing extracted labels and improving evaluation accuracy

## Why This Works (Mechanism)

### Mechanism 1
The combination of regular expression (RE) and Named Entity Recognition (NER) models effectively extracts periodontitis diagnoses from unstructured clinical notes. RE methods generate labeled training data by identifying diagnosis sections and extracting structured information like Stage, Grade, and Extent. This data is then used to fine-tune the RoBERTa-based NER model, which learns to recognize and classify diagnosis-related entities from clinical text. Core assumption: Clinical notes follow a consistent format that allows RE methods to reliably identify diagnosis sections and extract relevant labels.

### Mechanism 2
Increasing the complexity of regular expression algorithms improves the accuracy of extracted labels. The advanced RE method allows more flexible matching by relaxing strict ordering requirements and permitting partial label matches. This captures more varied ways clinicians write diagnoses. Core assumption: More complex RE patterns can handle the diversity of clinical note formats without introducing excessive false positives.

### Mechanism 3
Post-processing is essential to standardize extracted labels and improve evaluation accuracy. After the NER model predicts entities, post-processing corrects typos (e.g., "b." to "B"), removes extraneous symbols, and ensures consistent value formatting. This ensures extracted data aligns with gold standard evaluation criteria. Core assumption: The NER model can correctly identify entities even when they contain minor formatting errors that post-processing can fix.

## Foundational Learning

- **Concept**: Regular expressions and pattern matching
  - **Why needed here**: RE methods are the initial step for extracting structured diagnosis information from unstructured clinical text.
  - **Quick check question**: What are the basic components of a regular expression pattern (e.g., quantifiers, character classes, groups)?

- **Concept**: Named Entity Recognition (NER) and transformer models
  - **Why needed here**: NER models learn to identify and classify diagnosis-related entities in clinical text, building on the labeled data generated by RE methods.
  - **Quick check question**: How does a transformer-based NER model differ from traditional rule-based NER approaches?

- **Concept**: Evaluation metrics for NER (precision, recall, F1, specificity)
  - **Why needed here**: These metrics assess the model's performance in correctly identifying and classifying diagnosis entities, especially given class imbalance.
  - **Quick check question**: Why is it important to report both macro and weighted averages for evaluation metrics in imbalanced datasets?

## Architecture Onboarding

- **Component map**: Clinical notes → RE preprocessing (simple/advanced) → Training data generation → RoBERTa NER fine-tuning → Model prediction → Post-processing → Evaluation
- **Critical path**: Clinical notes → RE preprocessing (simple/advanced) → Training data generation → RoBERTa NER fine-tuning → Model prediction → Post-processing → Evaluation against gold standard
- **Design tradeoffs**: Simple RE: Faster to implement, lower accuracy, less training data vs. Advanced RE: More complex patterns, higher accuracy, more training data, higher computational cost
- **Failure signatures**: Low recall: RE patterns too restrictive, missing diagnosis sections; Low precision: RE patterns too permissive, matching incorrect contexts; Inconsistent labels: Inadequate post-processing, format mismatches; Poor NER performance: Insufficient training data or model underfitting
- **First 3 experiments**: 1) Baseline RE extraction: Run both simple and advanced RE methods on a small subset of clinical notes and compare extracted labels against manual annotations to establish baseline F1 scores. 2) NER model training: Fine-tune the RoBERTa-base model using training data from the advanced RE method and evaluate on a validation set to measure performance improvements. 3) Post-processing validation: Apply post-processing rules to NER predictions and verify that label standardization improves alignment with gold standard evaluation metrics.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of RoBERTa compare to other transformer models like BERT or ClinicalBERT for extracting periodontitis diagnoses from clinical notes? The authors mention that RoBERTa-base was chosen for this general-domain task and suggest testing with other models like BERT, ClinicalBERT, or other large language models in future work. This remains unresolved as the paper only evaluated RoBERTa and did not compare its performance against other transformer models specifically for this task.

### Open Question 2
What is the optimal balance between regular expression complexity and NER model performance for extracting periodontal diagnoses? The authors note that while a comprehensive regular expression could produce similar outcomes to a simple RE model, the NER model produced outstanding predictions. They also mention that creating more complex RE algorithms can lead to more accurate results but requires more time. This tradeoff space is not explored in the paper.

### Open Question 3
How well does this approach generalize to extracting diagnoses for other dental conditions beyond periodontitis? The authors state that "not only periodontitis but other dental diseases could be implemented in the model to create more complete and comprehensive structured data in EHRs." The study only tested the approach on periodontitis diagnosis extraction and didn't validate it on other dental conditions.

## Limitations

- Limited transparency in methodology with unspecified regular expression patterns
- Circular dependency between training data generation and model evaluation
- No cross-validation or independent test set evaluation for generalization
- Post-processing contributions not quantified separately from model learning

## Confidence

**High Confidence Claims**:
- The general approach of combining RE methods with NER models for extracting structured information from clinical notes is valid and has been demonstrated in other domains.
- The importance of post-processing for standardizing extracted labels is well-established in NLP literature.

**Medium Confidence Claims**:
- The specific performance metrics (0.84-0.92 for simple RE, 0.95-0.99 for advanced RE) are plausible given the methodology, but cannot be independently verified without access to the exact RE patterns and implementation details.
- The claim that increasing RE complexity improves F1 score from 0.3-0.4 to 0.9 is supported by the abstract but lacks detailed breakdown by label type.

**Low Confidence Claims**:
- The absolute superiority of the advanced RE method over the simple method cannot be verified without knowing the exact patterns used.
- Claims about the NER model's learning capabilities versus post-processing effects cannot be separated based on the information provided.

## Next Checks

1. **RE Pattern Transparency Audit**: Request and independently implement the exact regular expressions used in both simple and advanced approaches. Apply these patterns to a held-out validation set of clinical notes not used in the original study to verify the claimed performance improvements.

2. **Independent Gold Standard Evaluation**: Manually annotate a new set of 100 clinical notes with periodontitis diagnoses (Stage, Grade, Extent) by multiple independent clinicians. Use this as an external validation set to evaluate both the RE-generated training data and the final NER model predictions.

3. **Ablation Study of Post-Processing**: Train and evaluate the NER model with and without post-processing rules to quantify the contribution of rule-based corrections versus model learning. This will help determine whether performance gains are primarily from the NER model or from post-processing standardization.