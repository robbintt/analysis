---
ver: rpa2
title: Predictive Coding beyond Correlations
arxiv_id: '2306.15479'
source_url: https://arxiv.org/abs/2306.15479
tags:
- causal
- graph
- learning
- nodes
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work bridges Bayesian and causal inference with predictive\
  \ coding, showing that predictive coding can naturally perform both causal structure\
  \ learning and causal inference. It introduces a simple adjustment\u2014setting\
  \ the prediction error of intervened nodes to zero\u2014that allows predictive coding\
  \ networks to model interventions without altering graph structure."
---

# Predictive Coding beyond Correlations

## Quick Facts
- arXiv ID: 2306.15479
- Source URL: https://arxiv.org/abs/2306.15479
- Reference count: 40
- Key outcome: Bridges Bayesian and causal inference with predictive coding, showing predictive coding can naturally perform both causal structure learning and causal inference.

## Executive Summary
This paper introduces a novel framework that unifies Bayesian and causal inference through predictive coding networks. The authors demonstrate that predictive coding can naturally perform causal structure learning and causal inference tasks by incorporating sparsity and acyclicity constraints into the predictive coding objective. The key innovation is a simple adjustment—setting the prediction error of intervened nodes to zero—which allows predictive coding networks to model interventions without altering graph structure. This enables both interventional and counterfactual queries in known causal graphs, and extends to unknown structures through end-to-end causal discovery from observational data.

## Method Summary
The method treats predictive coding networks as performing variational free energy minimization with continuous parameters. The approach consists of three phases: (1) inference phase where observed nodes are fixed and others updated via gradient descent, (2) learning phase where weights are updated via gradient descent, and (3) intervention/counterfactual phases where prediction errors of intervened nodes are set to zero. For structure learning, the adjacency matrix is treated as a continuous parameter updated via gradient descent to minimize variational free energy plus regularizers (L1 for sparsity, DAG constraint for acyclicity). The framework is evaluated on synthetic data, MNIST/FashionMNIST classification, and counterfactual image reconstruction tasks.

## Key Results
- Predictive coding achieves state-of-the-art performance on causal inference benchmarks
- Improves classification accuracy by removing irrelevant connections through structure learning
- Consistently recovers accurate adjacency matrices across diverse graph types (Erdős-Rényi, scale-free, small-world)
- Outperforms several baselines on structure learning tasks including GES, PC, ICALiNGAM, and NOTEARS

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Predictive coding can model causal interventions by setting the prediction error of intervened nodes to zero.
- **Mechanism**: The error signal is the only flow that goes against the causal direction in the network. Setting it to zero for a node effectively removes incoming causal influence on that node, equivalent to performing a "do" operation.
- **Core assumption**: The causal graph structure is known and fixed.
- **Evidence anchors**:
  - [abstract] "It introduces a simple adjustment—setting the prediction error of intervened nodes to zero—that allows predictive coding networks to model interventions without altering graph structure."
  - [section 3] "This can be simply done by fixing the prediction error of a specific node to zero during the inference process."
  - [corpus] Weak evidence for alternative methods; predictive coding with zero error is unique.
- **Break condition**: If the causal graph is unknown, or if prediction errors are shared among nodes in a way that zeroing one propagates incorrectly.

### Mechanism 2
- **Claim**: Predictive coding can perform causal structure learning via sparsity and acyclicity regularization.
- **Mechanism**: The adjacency matrix A is treated as a continuous parameter, updated via gradient descent to minimize variational free energy plus regularizers (L1 for sparsity, DAG constraint for acyclicity).
- **Core assumption**: The true graph is sparse and acyclic.
- **Evidence anchors**:
  - [abstract] "incorporating sparsity and acyclicity constraints into the predictive coding objective, allowing end-to-end causal discovery from observational data."
  - [section 4] "The energy function that we aim to minimize via gradient descent is given by the sum of the total energy... and the three aforementioned prior distributions, each weighted by a scaling coefficient."
  - [corpus] None directly; sparsity + acyclicity regularization is standard in causal discovery literature.
- **Break condition**: If the true graph contains cycles or dense connections, or if the regularization is too strong.

### Mechanism 3
- **Claim**: Predictive coding can perform counterfactual inference by inferring exogenous variables (u) and then applying interventions.
- **Mechanism**: The abduction step infers the values of exogenous variables u given observed endogenous variables x. Then, the action step fixes the intervened node's error to zero. Finally, the prediction step propagates the counterfactual forward using the inferred u and modified graph.
- **Core assumption**: The SCM is linear with Gaussian noise, and exogenous variables are independent.
- **Evidence anchors**:
  - [abstract] "This enables both interventional and counterfactual queries in known causal graphs."
  - [section 3.1] "Abduction: Here, we are provided with values (s1, . . . ,sN) for all the internal nodes... compute the values of the exogenous variables, which we denote by ˜u1, . . . ,˜uM."
  - [corpus] No direct evidence; counterfactual inference via SCM is standard but not explicitly tied to predictive coding.
- **Break condition**: If the SCM is non-linear, or if exogenous variables are not identifiable from data.

## Foundational Learning

- **Concept**: Bayesian inference
  - Why needed here: Predictive coding performs Bayesian inference over continuous states using local information.
  - Quick check question: What is the relationship between variational free energy and Bayesian posterior inference?
- **Concept**: Causal inference (do-operator)
  - Why needed here: The core innovation is modeling interventions via the do-operator using prediction error zeroing.
  - Quick check question: How does the do-operator differ from conditioning in a causal graph?
- **Concept**: Structure learning with regularizers
  - Why needed here: Causal discovery requires enforcing sparsity and acyclicity during gradient-based optimization.
  - Quick check question: What is the role of the DAG penalty (tr(exp(A×A))−d) in structure learning?

## Architecture Onboarding

- **Component map**: Exogenous nodes → Internal nodes → Output nodes (directed edges with weights Wi,j)
- **Critical path**:
  1. Inference phase: Fix observed nodes, update others via gradient descent to minimize F
  2. Learning phase: Fix all nodes, update weights via gradient descent
  3. Intervention: Set ej=0 for intervened node j, then run inference
  4. Counterfactual: Infer u, set ej=0, run inference with fixed u
- **Design tradeoffs**:
  - Fully connected vs. sparse graphs: Fully connected is flexible but needs pruning; sparse is efficient but requires prior knowledge
  - Linear vs. non-linear mappings: Linear is tractable and matches the SCM assumption; non-linear may improve expressiveness but complicates causal interpretation
  - Zero error vs. structural modification: Zero error is efficient; structural modification is explicit but costly
- **Failure signatures**:
  - Degenerate structures: Output nodes predict themselves via self-loops or cycles (no causal flow)
  - Over-regularization: Too sparse graph, missing true edges
  - Under-regularization: Too dense, no improvement over baseline
  - Poor counterfactual accuracy: Inaccurate inference of exogenous variables
- **First 3 experiments**:
  1. Train on synthetic chain graph, perform intervention on middle node, verify descendant values match ground truth
  2. Train on MNIST with full graph, compare conditional vs. interventional test accuracy; check for improvement
  3. Train on ER2 random graph with N=10, apply sparsity+acyclicity, evaluate learned adjacency vs. true via F1 and SHD

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does predictive coding handle causal structure learning in the presence of hidden confounders?
- Basis in paper: [inferred] The paper assumes causal sufficiency, but does not address cases where hidden confounders exist.
- Why unresolved: The experiments focus on known structures and do not test robustness to unobserved confounding.
- What evidence would resolve it: Empirical results comparing predictive coding performance on datasets with and without hidden confounders.

### Open Question 2
- Question: What is the computational complexity of predictive coding-based causal structure learning compared to combinatorial methods as graph size increases?
- Basis in paper: [explicit] The paper mentions that combinatorial methods grow double exponentially in complexity with respect to graph dimension.
- Why unresolved: The paper does not provide detailed complexity analysis or runtime comparisons with other structure learning methods.
- What evidence would resolve it: Empirical runtime comparisons of predictive coding against combinatorial and other continuous optimization methods across graphs of varying sizes.

### Open Question 3
- Question: How does the performance of predictive coding-based causal inference scale with the dimensionality of variables in the graph?
- Basis in paper: [inferred] The experiments use one-dimensional nodes, but no analysis is provided for higher-dimensional variables.
- Why unresolved: The paper does not test predictive coding on causal graphs with multi-dimensional nodes.
- What evidence would resolve it: Experimental results showing predictive coding performance on causal graphs with nodes representing vectors or higher-dimensional data.

## Limitations
- The zero-error intervention mechanism relies on linear SCM assumptions; non-linear or non-Gaussian noise may break the equivalence between zeroing errors and the do-operator
- Structure learning depends heavily on proper regularization strength; over-regularization may miss true edges while under-regularization may fail to enforce sparsity
- Counterfactual inference requires accurate inference of exogenous variables, which may be challenging when noise is not independent or identifiable from observational data

## Confidence
- High confidence: Predictive coding can perform causal inference in known graphs by zeroing intervention node errors
- Medium confidence: The sparsity + acyclicity regularization approach reliably recovers graph structure across diverse graph types
- Low confidence: Counterfactual inference accuracy depends critically on exogenous variable inference quality, which is not thoroughly validated

## Next Checks
1. Test predictive coding with non-linear causal mechanisms (e.g., ReLU or sigmoid mappings) to verify zero-error intervention still produces valid do-operator effects
2. Evaluate counterfactual inference performance when exogenous variables are correlated or non-Gaussian to assess robustness of the abduction step
3. Compare computational efficiency against state-of-the-art causal discovery methods on graphs with >100 nodes to verify scalability claims