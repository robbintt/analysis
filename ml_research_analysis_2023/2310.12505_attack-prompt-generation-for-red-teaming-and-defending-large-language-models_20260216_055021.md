---
ver: rpa2
title: Attack Prompt Generation for Red Teaming and Defending Large Language Models
arxiv_id: '2310.12505'
source_url: https://arxiv.org/abs/2310.12505
tags:
- attack
- prompts
- llms
- defense
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an integrated approach that combines manual
  and automatic methods to generate high-quality attack prompts for red teaming large
  language models (LLMs). Specifically, the attack framework leverages in-context
  learning with LLMs to mimic human-generated prompts.
---

# Attack Prompt Generation for Red Teaming and Defending Large Language Models

## Quick Facts
- arXiv ID: 2310.12505
- Source URL: https://arxiv.org/abs/2310.12505
- Authors: 
- Reference count: 18
- Key outcome: Integrated approach combining manual and automatic methods to generate high-quality attack prompts for red teaming LLMs, validated through extensive experiments on different models

## Executive Summary
This paper proposes an integrated framework for red teaming large language models by combining manual and automatic attack prompt generation methods. The approach leverages in-context learning with LLMs to mimic human-generated prompts, producing high-quality attack prompts that surpass both purely manual and purely automatic methods. Additionally, the framework includes a defense mechanism that iteratively fine-tunes victim LLMs through interactions with the attack framework. The researchers release SAP (Safety Attack Prompts) datasets of varying sizes to facilitate safety evaluation and enhancement of more LLMs.

## Method Summary
The framework combines manual prompt construction with in-context learning to generate attack prompts, followed by iterative fine-tuning for defense. First, a small set of manually constructed attack prompts is used as in-context examples to instruct an attack LLM (gpt3.5-turbo-0301) to generate more prompts. These generated prompts are evaluated for quality by measuring harmful outputs from target LLMs, with high-quality prompts added to the dataset. For defense, the target LLM is fine-tuned using instruction tuning on attack prompts, generating safe outputs. This process iteratively expands the prompt set and fine-tunes the LLM until sufficient defense capability is achieved. The SAP datasets (SAP5, SAP10, SAP20, SAP30, SAP200) cover 8 sensitive topics and are released to support further research.

## Key Results
- The attack framework generates higher-quality prompts than both manual-only and automatic-only methods through in-context learning
- The defense framework successfully enhances LLM safety, reducing harmful scores from 5.8 to 2.6 on Alpaca-LoRA-7B
- SAP datasets outperform existing benchmarks (Dual-Use and BAD+) in attack effectiveness across multiple LLMs
- The approach scales effectively from SAP5 to SAP200 datasets while maintaining prompt quality

## Why This Works (Mechanism)

### Mechanism 1: In-context Learning with High-Quality Seeds
- Claim: In-context learning enables LLMs to generate high-quality attack prompts that surpass both manual and fully automatic methods
- Core assumption: The LLM's in-context learning capability is strong enough to capture the essential features of high-quality attack prompts and generalize to new contexts
- Evidence anchors: Abstract mentions "instruct LLMs to mimic human-generated prompts through in-context learning"; section 3.2 uses gpt3.5-turbo-0301 for its strong in-context learning capabilities
- Break condition: If the LLM fails to capture nuanced characteristics of attack prompts or if initial seed quality is too low

### Mechanism 2: Iterative Fine-tuning with Adaptive Prompt Generation
- Claim: Iterative fine-tuning with dynamically generated attack prompts creates robust LLMs against diverse red teaming attacks
- Core assumption: Attack framework can consistently generate prompts exposing LLM weaknesses, and defense framework learns general safety principles
- Evidence anchors: Abstract mentions "fine-tunes victim LLMs through iterative interactions"; section 3.3 describes using retained prompts for expanding attacks
- Break condition: If attack framework becomes saturated or defense framework overfits to specific prompts

### Mechanism 3: Role-playing to Bypass Ethical Constraints
- Claim: Role-playing techniques allow attack LLMs to generate prompts on sensitive topics by framing as content evaluation
- Core assumption: LLM's ethical constraints are based on direct content generation rather than task framing
- Evidence anchors: Section 3.2 describes adopting role-playing approach to involve gpt3.5-turbo-0301 as content reviewer
- Break condition: If LLM's ethical safeguards detect role-playing as bypass attempt

## Foundational Learning

- **Concept**: In-context learning and few-shot prompting
  - Why needed here: Attack framework relies on in-context learning to generate new attack prompts without retraining the LLM
  - Quick check question: What are key differences between in-context learning and fine-tuning, and why is in-context learning more suitable for this attack generation task?

- **Concept**: Red teaming and adversarial prompt engineering
  - Why needed here: Understanding how red teaming works is crucial for designing effective attack prompts and defenses
  - Quick check question: What are main categories of attack mechanisms in red teaming (e.g., obfuscation, code injection), and how do they exploit LLM vulnerabilities?

- **Concept**: Iterative optimization and curriculum learning
  - Why needed here: Defense framework uses iterative fine-tuning with progressively harder prompts, similar to curriculum learning approaches
  - Quick check question: How does curriculum learning differ from random sampling in training, and why is it particularly effective for building robust defenses?

## Architecture Onboarding

- **Component map**: Manual prompt collection → In-context learning prompt generation → Quality evaluation → SAP dataset construction → Attack prompt injection → LLM response generation → Harmfulness scoring → Iterative fine-tuning → Defense capability assessment
- **Critical path**: Attack prompt generation → LLM response evaluation → Fine-tuning with successful prompts → Repeat until convergence
- **Design tradeoffs**:
  - Manual vs. automatic prompt generation: Manual provides higher quality seeds but is expensive; automatic is cheaper but lower quality
  - Fixed vs. adaptive prompt sets: Fixed sets risk overfitting; adaptive sets maintain diversity but require more computation
  - Evaluator choice: Using gpt3.5-turbo-0301 provides consistency but may have blind spots; external APIs may be more standardized but less flexible
- **Failure signatures**:
  - Attack framework: Generated prompts consistently rejected by evaluator (too conservative) or consistently succeed (too aggressive)
  - Defense framework: Harmful scores plateau without improvement, or overfitting manifests as "refuse to answer" followed by unexpected content
  - Evaluation pipeline: High variance in harmfulness scores across evaluators, or evaluator itself becomes jailbroken
- **First 3 experiments**:
  1. Test attack framework with small seed set (SAP5) on known vulnerable LLM to verify prompt generation quality
  2. Evaluate defense framework on Alpaca-LoRA-7B with SAP5 to confirm basic fine-tuning effectiveness
  3. Compare SAP30 dataset against Dual-Use and BAD+ on multiple LLMs to validate relative attack performance

## Open Questions the Paper Calls Out
The paper does not explicitly call out open research questions, but the limitations section implicitly suggests several areas for future research, including the need for more comprehensive evaluation of defense mechanisms against novel attack strategies and the potential for adversarial examples to bypass the defense framework.

## Limitations
- The in-context learning approach depends heavily on the quality of initial manual prompts, which may limit generalization if seeds are not diverse
- The evaluation methodology relies on a single evaluator LLM, introducing potential bias and blind spots in harmfulness scoring
- The iterative fine-tuning process may lead to overfitting, where the LLM becomes overly defensive against specific prompts while remaining vulnerable to novel attack strategies
- The role-playing technique for bypassing ethical constraints may not be robust across different LLM architectures or safety protocols

## Confidence

**High confidence in**: Basic framework architecture and experimental methodology are well-defined and reproducible; SAP dataset construction process is clearly specified

**Medium confidence in**: Effectiveness of in-context learning for generating high-quality attack prompts, as this depends heavily on quality of initial seeds and specific LLM's capabilities

**Low confidence in**: Generalizability of defense mechanisms across different LLM architectures; long-term effectiveness of iterative fine-tuning against evolving attack strategies

## Next Checks

1. **Evaluator Consistency Test**: Run the same attack prompts through multiple different evaluator LLMs (including open-source models) to assess consistency in harmfulness scoring and identify potential evaluator biases

2. **Cross-Architecture Defense Transfer**: Fine-tune a model using the SAP dataset on one LLM architecture, then test the defended model against attack prompts generated by a different LLM to evaluate generalization of defense mechanisms

3. **Novel Attack Generation Test**: After multiple iterations of the defense framework, attempt to generate completely new types of attack prompts (not in SAP) to test whether the defense has learned general safety principles or merely memorized specific responses to known attacks