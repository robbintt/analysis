---
ver: rpa2
title: Frugal Prompting for Dialog Models
arxiv_id: '2305.14919'
source_url: https://arxiv.org/abs/2305.14919
tags:
- user
- dialog
- about
- summary
- history
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates cost-performance trade-offs in dialog modeling
  using large language models (LLMs) by proposing a metric called Usable Information
  Density (UID) to evaluate different input representations. The authors explore manual
  and perplexity-optimized prompts, along with various dialog history compression
  methods (full, summarized, recent-k, semantic-k) across four LLM models on two dialog
  datasets.
---

# Frugal Prompting for Dialog Models

## Quick Facts
- arXiv ID: 2305.14919
- Source URL: https://arxiv.org/abs/2305.14919
- Reference count: 18
- One-line primary result: Semantically relevant single utterances from dialog history often provide better Usable Information Density than full history or multiple utterances.

## Executive Summary
This paper investigates cost-performance trade-offs in dialog modeling using large language models by proposing a metric called Usable Information Density (UID) to evaluate different input representations. The authors explore manual and perplexity-optimized prompts, along with various dialog history compression methods across four LLM models on two dialog datasets. They find that compressed dialog history representations can maintain performance while reducing inference costs, challenging the assumption that more context always improves model performance.

## Method Summary
The authors propose a metric called Usable Information Density (UID) to evaluate the cost-performance trade-offs in dialog modeling. They test various input representations including manual and perplexity-optimized prompts, and dialog history compression methods (full, summarized, recent-k, semantic-k) across four LLM models on two dialog datasets. The method involves preprocessing dialog datasets, generating compressed dialog history representations, creating prompt templates, running inference, and calculating performance metrics and UID.

## Key Results
- Semantically relevant single utterances from dialog history provide better UID than full history or multiple utterances
- Zero-shot prompting with perplexity-optimized prompts generally yields higher UID than few-shot settings
- Dialog history compression through selection is more cost-effective than summarization for maintaining UID

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Semantically relevant single utterances from dialog history provide better Usable Information Density (UID) than full history or multiple utterances
- Mechanism: The model can extract relevant context without being overwhelmed by redundant or irrelevant information, leading to better performance per token
- Core assumption: Not all dialog history contributes equally to understanding the current context; relevance matters more than quantity
- Evidence anchors:
  - [abstract] "using semantically relevant single utterances from dialog history often provides better UID than full history or multiple utterances"
  - [section 5.3] "Overall, we find that using full dialog history is not very useful from a UID perspective. For the TC dataset, it is clear that Semantic-1 and Semantic-2 have very good UID values across all models and metrics"

### Mechanism 2
- Claim: Zero-shot prompting with perplexity-optimized prompts yields higher UID than few-shot settings for dialog modeling
- Mechanism: Perplexity-optimized prompts create more effective task framing without the overhead of examples, while few-shot examples may introduce irrelevant patterns
- Core assumption: The model's instruction-following capabilities are sufficient for dialog tasks when properly prompted, making examples redundant
- Evidence anchors:
  - [abstract] "zero-shot prompting with perplexity-optimized prompts generally yields higher UID than few-shot settings"
  - [section 5.3] "Although recent prompt engineering based studies motivate using demonstration examples, it turns out that examples are not very useful from a UID perspective for dialog modeling"

### Mechanism 3
- Claim: Dialog history compression through selection (relevant-k) is more cost-effective than summarization for maintaining UID
- Mechanism: Direct selection of relevant utterances preserves key information while avoiding potential information loss or hallucination in summarization
- Core assumption: Relevant utterances contain the essential information needed for response generation without requiring full context
- Evidence anchors:
  - [section 5.3] "We also perform similar experiments with respect to using Recent-k utterances and passing them as context as part of the input prompt. Interestingly we find that in most settings passing one most recent utterance is hurtful in MSC but not in TC"
  - [section 3.3] "Recent-k: The simplest approach is to use a fixed-length dialog history from the most recent utterances. However, this approach is not optimal, as users may refer back to context beyond the fixed length window and expect the system to understand"

## Foundational Learning

- Concept: Prompt engineering principles (instruction clarity, role specification, format consistency)
  - Why needed here: Different prompt formats significantly impact model performance and UID, requiring systematic design and optimization
  - Quick check question: What are the key components that should be included in a dialog system prompt template?

- Concept: Information retrieval and semantic similarity measurement
  - Why needed here: Selecting semantically relevant utterances requires measuring similarity between dialog turns and the current context
  - Quick check question: How would you implement semantic relevance scoring for dialog history selection?

- Concept: Evaluation metrics for text generation (BLEURT, METEOR, DEB)
  - Why needed here: Understanding which metrics best capture dialog quality helps interpret UID results and model performance
  - Quick check question: What are the key differences between BLEURT, METEOR, and DEB in terms of what they measure?

## Architecture Onboarding

- Component map:
  - Data preprocessing → Dialog history compression (selection/summarization) → Prompt template construction → Model inference → Performance evaluation → UID calculation
  - Key components: Semantic similarity model, summarization models (BART, Pegasus), perplexity optimization pipeline, UID computation module

- Critical path:
  1. Load and preprocess dialog datasets
  2. Generate compressed dialog history representations
  3. Create prompt templates (manual and perplexity-optimized)
  4. Run inference across multiple models and settings
  5. Calculate performance metrics and UID
  6. Analyze cost-performance trade-offs

- Design tradeoffs:
  - Full history vs. compressed representations: Accuracy vs. inference cost
  - Manual prompts vs. perplexity-optimized prompts: Control vs. optimization
  - Zero-shot vs. few-shot: Simplicity vs. potential performance gains
  - Selection vs. summarization: Information preservation vs. compression efficiency

- Failure signatures:
  - Low UID despite high absolute performance: Prompts are long but not cost-effective
  - High UID but low absolute performance: Prompts are efficient but miss essential information
  - Inconsistent results across datasets: Method doesn't generalize well to different dialog characteristics
  - Perplexity optimization fails: May indicate prompt templates are already optimal or dataset characteristics don't align with optimization method

- First 3 experiments:
  1. Compare UID for Recent-1 vs. Semantic-1 utterance selection across both datasets
  2. Test zero-shot vs. few-shot performance for perplexity-optimized prompts
  3. Evaluate summarization model performance (BART-D vs. Pegasus-DS) on dialog history compression

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the optimal dialog history compression method vary depending on the specific dialog domain or dataset characteristics?
- Basis in paper: [explicit] The paper compares different compression methods (full, summarized, recent-k, semantic-k) across two datasets (MSC and TC) with different characteristics, finding that the optimal method varies between datasets
- Why unresolved: The study only examines two datasets, which may not be representative of all dialog domains. Different domains might have different optimal compression strategies based on factors like conversation length, topic diversity, or user behavior patterns
- What evidence would resolve it: Testing the compression methods across a broader range of dialog datasets from different domains (customer service, healthcare, social media, etc.) would reveal whether the optimal compression strategy is domain-dependent or more universal

### Open Question 2
- Question: How does the performance of frugal prompting strategies scale with increasingly larger language models?
- Basis in paper: [explicit] The paper tests four different model sizes (GPT-3, FLAN-T5-XL, T0, Tk-Instruct) but notes that "larger and more effective models tend to produce longer dialog history" and that "compressing dialog history can improve model performance without significantly increasing cost"
- Why unresolved: The study only examines models up to 175B parameters (GPT-3). It's unclear whether the frugal prompting strategies that work well for these models would remain effective or need adjustment for even larger models that may have different context processing capabilities
- What evidence would resolve it: Testing the same frugal prompting strategies with future generations of larger models (e.g., GPT-4, PaLM-2, etc.) would reveal whether the optimal strategies scale proportionally or require adaptation

### Open Question 3
- Question: What is the long-term impact of frugal prompting on model behavior and user experience in interactive dialog systems?
- Basis in paper: [inferred] The paper focuses on immediate performance metrics (BLEURT, DEB, METEOR) and cost metrics (UID) but doesn't examine longitudinal effects of using compressed dialog history on conversation quality, coherence, or user satisfaction over extended interactions
- Why unresolved: The study uses static evaluation metrics that don't capture how frugal prompting might affect the evolution of conversations, memory retention, or user trust over multiple turns or sessions
- What evidence would resolve it: Longitudinal user studies comparing systems using frugal prompting versus full context over extended conversations would reveal whether the performance gains are maintained and how they affect user experience metrics like satisfaction, engagement, and perceived system intelligence

## Limitations
- The study only examines two dialog datasets, which may not represent all dialog domains
- The exact perplexity optimization procedure lacks detailed specification
- The findings may not generalize to multilingual dialog scenarios

## Confidence
- The findings regarding UID-based prompt optimization show **High confidence** for the specific experimental setup
- Confidence drops to **Medium** for generalization to other dialog datasets, model families, or task domains
- The claim that semantic relevance selection outperforms full context has **Medium confidence** due to the narrow scope of tested dialog types
- The observed superiority of zero-shot perplexity-optimized prompts over few-shot approaches has **Medium confidence**

## Next Checks
1. Test the UID optimization framework on dialog datasets with different characteristics (e.g., multi-turn technical support conversations, negotiation dialogs) to assess generalizability of the semantic relevance selection approach.

2. Implement the exact perplexity optimization procedure described (or clarify the procedure if unspecified) and verify whether the reported UID improvements can be replicated across different model checkpoints of the same architecture.

3. Conduct ablation studies on the semantic selection algorithm itself - compare different similarity metrics (cosine similarity, cross-encoder, etc.) and relevance thresholds to determine the robustness of the "Semantic-k" approach to hyperparameter choices.