---
ver: rpa2
title: 'AHAM: Adapt, Help, Ask, Model -- Harvesting LLMs for literature mining'
arxiv_id: '2312.15784'
source_url: https://arxiv.org/abs/2312.15784
tags:
- topic
- domain
- discovery
- adaptation
- aham
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces AHAM, a methodology that improves topic modeling
  in scientific literature by adapting BERTopic with domain-specific fine-tuning of
  sentence transformers. The approach uses Llama2 via one-shot learning and prompt
  engineering to generate semantically meaningful topic names.
---

# AHAM: Adapt, Help, Ask, Model -- Harvesting LLMs for literature mining

## Quick Facts
- arXiv ID: 2312.15784
- Source URL: https://arxiv.org/abs/2312.15784
- Reference count: 30
- Key outcome: Domain adaptation significantly enhances topic modeling quality, with best outcomes achieved when the adaptation domain closely matches the target corpus.

## Executive Summary
AHAM is a methodology that improves topic modeling in scientific literature by combining domain adaptation of sentence transformers with LLM-powered topic naming. The approach uses generative pseudo-labeling (GPL) to fine-tune sentence transformers on domain-specific corpora, then applies BERTopic for clustering and Llama2 for semantically meaningful topic names. Evaluated on arXiv and medarXiv datasets, AHAM effectively reduces outlier topics and improves topic differentiation through an objective function balancing outlier reduction and topic name dissimilarity.

## Method Summary
AHAM fine-tunes sentence transformers using generative pseudo-labeling on domain-specific corpora, then applies UMAP dimensionality reduction and HDBSCAN clustering to identify document clusters. Keywords are extracted per cluster using KeyBERT, and topic names are generated via one-shot learning with Llama2 using domain expert-crafted prompts. The AHAM objective function combines outlier ratio and inter-topic similarity metrics to select optimal adaptation steps during GPL fine-tuning. The pipeline iterates through 50k adaptation steps, selecting the model with the lowest AHAM score.

## Key Results
- Domain adaptation significantly reduces outlier topics compared to baseline BERTopic
- Topic differentiation improves with closer alignment between adaptation and target domains
- AHAM objective successfully guides selection of optimal adaptation steps
- LLM-generated topic names capture semantic meaning of clusters effectively

## Why This Works (Mechanism)

### Mechanism 1
Domain adaptation of sentence transformers improves topic modeling precision by reducing outlier topics and increasing topic differentiation. GPL fine-tunes sentence transformers on domain-specific corpora, creating better semantic representations that cluster documents more effectively in BERTopic. Core assumption: The adaptation domain should closely match the target corpus for optimal results. Evidence: Results show domain adaptation enhances topic modeling quality when adaptation domain matches target corpus. Break condition: If adaptation domain is too dissimilar from target corpus, model may overfit or degrade performance.

### Mechanism 2
Prompt engineering with domain expert-crafted prompts enables LLMs to generate semantically meaningful topic names. Three-level prompt structure (system prompt, one-shot example, query prompt) guides Llama2 to label topics based on keywords and central documents. Core assumption: Domain experts can craft effective prompts that capture semantic essence of topics. Evidence: Paper details three-level prompt structure and its implementation. Break condition: Poorly crafted prompts or mismatched examples could lead to irrelevant or generic topic labels.

### Mechanism 3
AHAM objective function optimizes domain adaptation by balancing outlier reduction and topic name dissimilarity. AHAM objective combines outlier ratio and inter-topic similarity metrics to select optimal adaptation steps during GPL fine-tuning. Core assumption: Lower AHAM scores indicate better topic modeling quality. Evidence: AHAM objective formula combines outlier count and topic similarity. Break condition: If topic similarity metrics don't capture semantic differences well, AHAM may optimize for wrong objective.

## Foundational Learning

- Concept: Domain adaptation for sentence transformers
  - Why needed here: Standard sentence transformers trained on general corpora may not capture domain-specific semantics needed for precise topic modeling.
  - Quick check question: What happens to topic clustering if you use a general-purpose sentence transformer vs. a domain-adapted one?

- Concept: Generative pseudo-labeling (GPL)
  - Why needed here: GPL provides synthetic training data for fine-tuning sentence transformers without requiring labeled domain data.
  - Quick check question: How does the T5 query generation step in GPL differ from standard fine-tuning approaches?

- Concept: Prompt engineering for LLM in-context learning
  - Why needed here: LLMs need structured guidance to generate meaningful topic names from keywords and document clusters.
  - Quick check question: Why use a three-level prompt structure instead of a single prompt for topic labeling?

## Architecture Onboarding

- Component map:
  Data ingestion → Sentence transformer (base) → GPL adaptation → UMAP dimensionality reduction → HDBSCAN clustering → Keyword extraction → LLM topic naming → AHAM optimization

- Critical path:
  1. Fine-tune sentence transformer via GPL on domain corpus
  2. Vectorize target corpus with adapted transformer
  3. Apply UMAP and HDBSCAN to identify clusters
  4. Extract keywords per cluster
  5. Generate topic names with LLM using prompts
  6. Evaluate with AHAM objective and select best adaptation step

- Design tradeoffs:
  - GPL vs. supervised fine-tuning: GPL avoids need for labeled data but may generate noisy synthetic examples
  - LLM size vs. speed: Larger models may generate better topic names but increase latency
  - AHAM complexity vs. interpretability: Multi-metric objective balances multiple goals but is harder to interpret than single metrics

- Failure signatures:
  - No clusters found: Likely issues with sentence transformer adaptation or HDBSCAN parameters
  - Generic topic names: Prompt engineering may need refinement or different LLM
  - High outlier ratio throughout: Adaptation domain may be too dissimilar from target corpus

- First 3 experiments:
  1. Run baseline BERTopic without adaptation to establish performance metrics
  2. Apply GPL adaptation with varying step sizes (e.g., 10k, 20k, 30k) and track AHAM objective
  3. Compare topic names from different prompt variations to assess prompt engineering effectiveness

## Open Questions the Paper Calls Out

- Question: How does the size of the generative LLM affect the quality of topic names in AHAM?
  - Basis in paper: The paper mentions that an important direction for future work is evaluating the impact of the generative LLM size on topic name generation.
  - Why unresolved: The current implementation uses Llama2-13b-chat-hf, but the effects of different model sizes on topic naming quality have not been explored.
  - What evidence would resolve it: Empirical comparison of topic naming quality across different LLM sizes using standardized evaluation metrics.

- Question: What is the optimal number of adaptation steps in the AHAM objective for different domain pairs?
  - Basis in paper: The paper notes that the AHAM objective's optimization trajectory is complex and non-convex, suggesting that prolonged adaptation might not lead to optimal results.
  - Why unresolved: The current study uses fixed intervals of 10,000 steps and finds local optima, but the general pattern across different domain pairs is unknown.
  - What evidence would resolve it: Systematic analysis of AHAM objective convergence across multiple domain pairs with varying adaptation step sizes.

- Question: How does AHAM perform in low-resource languages compared to English?
  - Basis in paper: The authors identify analyzing the model's capability to adapt and identify topics in different low- and less-resourced languages as an intriguing area for further investigation.
  - Why unresolved: The current implementation and evaluation are conducted on English-language scientific corpora only.
  - What evidence would resolve it: Cross-lingual experiments comparing AHAM's performance across languages with varying resource availability.

## Limitations
- Methodology relies heavily on synthetic data generation through GPL, which may introduce noise if pseudo-labeled data doesn't accurately represent target domain
- Evaluation constrained to only two scientific domains (general science and medical), limiting generalizability to other fields
- AHAM objective combines multiple metrics in a non-convex optimization landscape that may lead to local optima

## Confidence
- High confidence: Core mechanism of domain adaptation improving topic modeling quality is well-supported by results showing reduced outlier topics and better differentiation
- Medium confidence: Effectiveness of AHAM objective function in guiding optimal adaptation is demonstrated, but non-convex nature and sensitivity to metric weights warrant further investigation
- Low confidence: Claims about generalizability across scientific domains are weakly supported given limited evaluation to only arXiv and medarXiv datasets

## Next Checks
1. Test AHAM on at least 3-5 additional scientific domains (e.g., computer science, physics, biology) to assess generalizability beyond current arXiv and medarXiv datasets
2. Conduct systematic evaluation of synthetic data quality generated through GPL by comparing topic distributions between pseudo-labeled and human-labeled data in subset of documents
3. Perform controlled experiments removing individual components of AHAM objective (outlier ratio, topic similarity) to quantify their relative contributions and test robustness of non-convex optimization approach