---
ver: rpa2
title: 'EvoPrompting: Language Models for Code-Level Neural Architecture Search'
arxiv_id: '2302.14838'
source_url: https://arxiv.org/abs/2302.14838
tags:
- triplet
- node
- edge
- reps
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes EvoPrompting, a method that uses evolutionary
  search to create and curate data to improve language model (LM) in-context prompting
  examples. The method is applied to the task of neural architecture search (NAS),
  where the LM is used as an adaptive mutation and crossover operator.
---

# EvoPrompting: Language Models for Code-Level Neural Architecture Search

## Quick Facts
- arXiv ID: 2302.14838
- Source URL: https://arxiv.org/abs/2302.14838
- Reference count: 40
- Primary result: Language models can be used for neural architecture search through evolutionary prompting, discovering novel architectures that outperform hand-designed models.

## Executive Summary
EvoPrompting introduces a novel approach to neural architecture search (NAS) that leverages large language models (LMs) as adaptive mutation and crossover operators. The method uses evolutionary search over in-context examples to iteratively improve LM-generated neural architectures, combined with prompt-tuning between rounds. This approach enables LMs to create diverse and high-performing neural network architectures without requiring hand-designed search spaces. The method is evaluated on the MNIST-1D dataset and the CLRS Algorithmic Reasoning Benchmark, demonstrating superior performance compared to baseline methods and state-of-the-art models.

## Method Summary
EvoPrompting uses a pre-trained LM (like PALM) to generate neural architectures in Python code form. The method initializes with seed architectures and iteratively evolves them through an evolutionary loop: generating candidate architectures using the LM with current parents as in-context examples, evaluating candidates based on validation accuracy and model size, selecting top performers as parents for the next round, and prompt-tuning the LM on non-parent candidates. The evolutionary search replaces traditional hand-designed search spaces, allowing the LM's vocabulary to define the search space. This creates a feedback loop where both the in-context examples and the LM's prompt-tuning improve over time, enabling the discovery of novel and effective architectures.

## Key Results
- EvoPrompting produces convolutional architecture variants that outperform human-designed models on MNIST-1D in terms of accuracy and model size
- EvoPrompting designs novel graph neural network architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks on the CLRS benchmark
- The method successfully designs accurate and efficient neural network architectures across diverse machine learning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EvoPrompting uses evolutionary search over in-context examples to iteratively improve LM-generated neural architectures.
- Mechanism: At each round, the LM is prompted with top-performing parent models from the previous round. The LM generates candidate architectures, which are evaluated and the best are selected as parents for the next round. This creates a feedback loop where both the in-context examples and the LM's prompt-tuning improve over time.
- Core assumption: The LM's generations improve when conditioned on higher-performing in-context examples.
- Evidence anchors:
  - [abstract] "we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EVOPROMPTING, consistently finds diverse and high performing models."
  - [section 3.3] "We also note that – on this task in particular – EVOPROMPTING excels especially at optimizing convolutional architectures. Many of the top 20 models are narrower and deeper convolutional architectures, with smaller strides, less padding, and no dense layers."

### Mechanism 2
- Claim: Prompt-tuning the LM between rounds adapts it to generate architectures that meet the target metrics specified in the prompt.
- Mechanism: After each round, all non-parent child models are used to prompt-tune the LM for 5 epochs. This fine-tunes the LM's generation capabilities to produce architectures that better match the desired performance criteria (e.g. higher accuracy, smaller size).
- Core assumption: The LM can be effectively prompt-tuned to generate architectures that better match the target metrics.
- Evidence anchors:
  - [abstract] "we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EVOPROMPTING, consistently finds diverse and high performing models."
  - [section 3.3] "Lastly, all child models generated in the current round that were not previously selected for crossover (i.e. CEVALED \P ) are used to prompt-tune πθ for the next round."

### Mechanism 3
- Claim: Using an LM as a crossover and mutation operator allows for a more flexible and open-ended search space compared to hand-designed search spaces.
- Mechanism: The LM's vocabulary replaces the need for a manually designed search space. Any syntactically valid Python code is a valid architecture in the search space. This allows the LM to generate novel architectures that may not be covered by a limited hand-designed space.
- Core assumption: The LM's pre-training on code provides sufficient knowledge to generate valid neural architectures.
- Evidence anchors:
  - [section 2] "In EVOPROMPTING the LM's vocabulary replaces the search space, which both increases the flexibility of the search and reduces reliance on manual design."
  - [section 3.2] "In this work, any syntactically valid piece of code is covered in our search space; the only limitations are those imposed by the coding language and libraries (ex., Python and JAX)."

## Foundational Learning

- Concept: Neural Architecture Search (NAS)
  - Why needed here: EvoPrompting is a method for NAS, so understanding the fundamentals of NAS is crucial.
  - Quick check question: What is the goal of NAS and what are some common approaches?

- Concept: Language Models (LMs) for Code Generation
  - Why needed here: EvoPrompting uses a pre-trained LM to generate neural architectures in code form.
  - Quick check question: How are LMs typically trained for code generation, and what are their limitations?

- Concept: Evolutionary Algorithms
  - Why needed here: EvoPrompting uses an evolutionary approach to iteratively improve the LM's generations.
  - Quick check question: What are the key components of an evolutionary algorithm, and how are they typically applied to NAS?

## Architecture Onboarding

- Component map:
  LM -> Prompt Engineering -> Evaluation Function -> Evolutionary Loop

- Critical path:
  1. Initialize population with seed architectures
  2. Generate candidates using LM with current parents as in-context examples
  3. Evaluate candidates and select top performers as parents
  4. Prompt-tune LM on non-parent candidates
  5. Repeat until stopping criterion is met

- Design tradeoffs:
  - Search space flexibility vs. LM's generation capabilities
  - Prompt-tuning frequency and duration vs. computational cost
  - Population size and diversity vs. evaluation budget

- Failure signatures:
  - LM generates mostly invalid or poor-quality architectures
  - Search gets stuck in local optima or plateaus early
  - Evaluation becomes the bottleneck as architectures get more complex

- First 3 experiments:
  1. Run EvoPrompting with naive few-shot prompting (no evolutionary search or prompt-tuning) to establish a baseline.
  2. Run EvoPrompting with evolutionary search but no prompt-tuning to isolate the effect of iterative improvement of in-context examples.
  3. Run EvoPrompting with both evolutionary search and prompt-tuning on a simple dataset (e.g. MNIST-1D) to validate the full method.

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions, but raises important considerations about scalability and generalization that could be explored further.

## Limitations
- Computational efficiency: The paper does not provide detailed analysis of the computational cost of prompt-tuning the LM between rounds.
- Scalability: While the method shows strong performance on MNIST-1D and CLRS, its effectiveness on larger, more complex datasets remains unproven.
- Diversity analysis: The paper lacks a detailed analysis of the diversity of architectures generated, making it unclear if the method consistently discovers novel architectures.

## Confidence
**High Confidence**: The core mechanism of using evolutionary search to improve in-context examples and prompt-tuning the LM appears well-supported by the results on MNIST-1D and CLRS.

**Medium Confidence**: The claim that EvoPrompting can discover novel architectures that outperform hand-designed models is supported by the results, but the generality of this finding across different ML tasks is uncertain.

**Low Confidence**: The assertion that EvoPrompting can scale to very large search spaces without the need for manual design is speculative, as the paper does not provide evidence for navigating extremely large or complex search spaces.

## Next Checks
1. **Benchmark on Diverse Datasets**: Evaluate EvoPrompting on a wider range of ML tasks, including vision, language, and reinforcement learning, to assess its ability to generate effective architectures across different domains.

2. **Analyze Architecture Diversity**: Perform a detailed analysis of the diversity of architectures generated by EvoPrompting, clustering models based on structural properties to determine if the method discovers a wide range of novel architectures.

3. **Quantify Computational Efficiency**: Measure the computational cost of prompt-tuning the LM between rounds and compare it to the overall runtime of the evolutionary search to identify potential bottlenecks and assess practical efficiency.