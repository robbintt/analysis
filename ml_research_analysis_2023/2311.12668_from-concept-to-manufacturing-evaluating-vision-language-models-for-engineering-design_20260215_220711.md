---
ver: rpa2
title: 'From Concept to Manufacturing: Evaluating Vision-Language Models for Engineering
  Design'
arxiv_id: '2311.12668'
source_url: https://arxiv.org/abs/2311.12668
tags:
- design
- gpt-4v
- engineering
- these
- part
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates GPT-4V's capabilities across diverse engineering
  design tasks using multimodal inputs. The authors developed and tested over 1000
  queries spanning conceptual design (sketch similarity, design descriptions, concept
  selection), system-level and detailed design (material selection, CAD generation,
  topology optimization, fluid dynamics simulation), manufacturing and inspection
  (additive/subtractive manufacturing, defect detection), and engineering education
  (textbook problems, spatial reasoning).
---

# From Concept to Manufacturing: Evaluating Vision-Language Models for Engineering Design

## Quick Facts
- arXiv ID: 2311.12668
- Source URL: https://arxiv.org/abs/2311.12668
- Reference count: 28
- Primary result: GPT-4V demonstrates strong capabilities in conceptual design tasks but struggles with precision-dependent engineering domains like CAD generation and material selection.

## Executive Summary
This paper presents a comprehensive evaluation of GPT-4V across diverse engineering design tasks, spanning from conceptual design to manufacturing and inspection. The authors developed over 1000 queries using multimodal inputs and assessed the model's performance across five key domains: conceptual design, system-level and detailed design, manufacturing and inspection, and engineering education. The study establishes benchmarks for future vision-language model evaluations in engineering design and provides comprehensive datasets for ongoing research.

## Method Summary
The study employed qualitative case studies and quantitative experiments using the ChatGPT interface with specific prompts for each engineering task. The evaluation covered over 1000 queries using multimodal inputs including engineering design sketches, technical charts, CAD models, engineering drawings, CFD simulations, manufacturing designs, textbook problems, and spatial reasoning tests. Performance was assessed through both qualitative analysis and quantitative metrics, with comparisons to human expert benchmarks where applicable.

## Key Results
- GPT-4V achieved 94% self-consistency in design similarity assessment, outperforming average human raters
- The model successfully matched sketches to descriptions 10/10 times across all trials when full sketches were provided
- GPT-4V demonstrated ability to generate accurate design descriptions from hand-drawn sketches
- The model struggled with precise material selection from Ashby charts and CAD generation tasks
- Performance on spatial reasoning tasks was significantly below typical human scores

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** GPT-4V demonstrates high self-consistency and low transitive violations when assessing design similarity of early-stage sketches.
- **Mechanism:** By processing 360 triplets of design sketches and repeating 50 examples, GPT-4V shows 94% self-consistency and matches the lowest human rater for transitive violations (5).
- **Core assumption:** Visual representations in early-stage sketches are sufficiently rich for the model to capture meaningful similarity relationships without additional context.
- **Evidence anchors:**
  - [abstract]: "The model is able to assess design similarity with higher self-consistency than human raters (94% compared to 62.8% average for human raters) and as few transitive violations as the top human raters."
  - [section]: "GPT-4V made only five transitive violations, which matches the lowest number of transitive violations made by any of the eleven human raters."

### Mechanism 2
- **Claim:** GPT-4V can effectively translate between visual and textual design representations, matching sketches to descriptions with high accuracy when full sketches (including handwritten text) are provided.
- **Mechanism:** The model achieves perfect scores (10/10) in matching sketches to their full descriptions across all trials, indicating robust multimodal alignment capabilities.
- **Core assumption:** Handwritten text integrated into sketches provides sufficient semantic context for the model to disambiguate design intent.
- **Evidence anchors:**
  - [abstract]: "When provided the entire design sketch including a handwritten description, the model matched a design sketch to its appropriate text description 10/10 times for all three trials."
  - [section]: "GPT-4V was able to match the sketch to the text description for 10/10 of the questions across all three trials."

### Mechanism 3
- **Claim:** GPT-4V generates coherent and useful textual descriptions of early-stage design sketches, aiding in design documentation and searchability.
- **Mechanism:** By analyzing visual features and extrapolating function from form, GPT-4V produces accurate, context-rich descriptions that align with human understanding.
- **Core assumption:** Sketches contain enough visual detail for the model to infer both structure and intended function, enabling meaningful textual synthesis.
- **Evidence anchors:**
  - [abstract]: "GPT-4V is able to generate accurate and useful design descriptions given hand-drawn sketches."
  - [section]: "Qualitatively, we assess that the model is able to generate useful and accurate text descriptions of designs even for sketches with very low drawing scores."

## Foundational Learning

- **Concept:** Design similarity assessment
  - Why needed here: Essential for identifying novel concepts and clustering related ideas in early-stage design exploration.
  - Quick check question: How would you define "novelty" in the context of design concept selection, and why is similarity assessment a useful proxy?

- **Concept:** Multimodal representation translation
  - Why needed here: Enables automated conversion between sketches and text, facilitating design search, documentation, and team communication.
  - Quick check question: What challenges arise when mapping visual sketches to textual descriptions without handwritten annotations?

- **Concept:** Topology optimization interpretation
  - Why needed here: Critical for evaluating material distribution and structural efficiency in detailed design, especially when designs are non-intuitive.
  - Quick check question: How do floating materials in a topology optimization output impact manufacturability, and why is detecting them important?

## Architecture Onboarding

- **Component map:** GPT-4V core multimodal transformer -> image encoder -> text encoder -> joint embedding space -> reasoning module -> text decoder
- **Critical path:** Input image -> visual feature extraction -> text prompt integration -> task-specific reasoning -> output generation -> (optional) tool usage for precision tasks
- **Design tradeoffs:** High-level understanding vs. precision; multimodal richness vs. hallucination risk; speed vs. depth of analysis; generalist vs. specialist capability
- **Failure signatures:** Over-caution (predicting "no" to all AM tasks); spatial reasoning gaps (CAD generation errors); reliance on full context (accuracy drop without handwritten text); inconsistent feature identification in complex designs
- **First 3 experiments:**
  1. Design similarity assessment using triplet queries (evaluate self-consistency and transitive violations)
  2. Sketch-to-description matching with and without handwritten text (assess multimodal alignment)
  3. Concept selection via Pugh chart generation (evaluate reasoning and formatting capabilities)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How effectively can GPT-4V identify specific numerical constraints in Ashby charts, such as precise density and Young's modulus ranges?
- **Basis in paper:** [explicit] The paper demonstrates GPT-4V's struggles with identifying materials that meet specific numerical requirements, particularly in Ashby chart cross-referencing experiments.
- **Why unresolved:** While GPT-4V can suggest broad material families, its ability to pinpoint exact numerical values remains unclear.
- **What evidence would resolve it:** Testing GPT-4V on a wider range of Ashby charts with varying levels of numerical precision and comparing its performance to established material selection methods.

### Open Question 2
- **Question:** Can GPT-4V improve its CAD generation capabilities through iterative feedback, especially when provided with visual cues of discrepancies?
- **Basis in paper:** [explicit] The paper highlights GPT-4V's limited success in generating accurate CAD scripts and its inability to effectively correct errors through visual feedback.
- **Why unresolved:** The study suggests that visual feedback alone is insufficient for GPT-4V to improve its CAD generation, but further investigation is needed to explore alternative feedback mechanisms.
- **What evidence would resolve it:** Evaluating GPT-4V's performance with different types of feedback, such as textual descriptions of errors or integration with specialized CAD software.

### Open Question 3
- **Question:** How well can GPT-4V perform in spatial reasoning tasks compared to human experts, particularly in engineering design contexts?
- **Basis in paper:** [explicit] The paper assesses GPT-4V's performance on standardized spatial reasoning tests and finds it significantly below typical human scores.
- **Why unresolved:** The study focuses on general spatial reasoning tasks, and further research is needed to understand GPT-4V's performance in engineering-specific spatial reasoning challenges.
- **What evidence would resolve it:** Evaluating GPT-4V on engineering design tasks that require spatial reasoning, such as interpreting complex engineering drawings or optimizing 3D structures.

## Limitations

- The model struggles with precise numerical data and complex information synthesis, leading to errors in material selection and topology optimization tasks.
- GPT-4V exhibits inconsistent performance across different types of engineering tasks, excelling in general understanding but struggling with specific, detailed tasks.
- The study's focus on general engineering examples limits understanding of the model's performance in industry-specific design contexts.

## Confidence

- **High Confidence:** Design similarity assessment results, where GPT-4V demonstrated 94% self-consistency and matched top human performance in transitive violations.
- **Medium Confidence:** Material selection and CAD generation results, where the model shows partial understanding but makes significant errors in precision-dependent tasks.
- **Low Confidence:** Spatial reasoning and advanced engineering analysis tasks, where the model frequently fails to provide accurate responses.

## Next Checks

1. **Precision Testing:** Conduct controlled experiments measuring numerical accuracy in material property selection and dimensional calculations to establish quantitative error bounds for engineering applications.

2. **Cross-Model Comparison:** Evaluate multiple VLMs (Claude-3, Gemini Pro, Llama-3) on the same engineering design tasks to determine if GPT-4V's limitations are model-specific or inherent to current VLM architectures.

3. **Domain Transferability:** Test the model's performance on industry-specific design tasks (aerospace, biomedical, automotive) to assess generalization beyond the general engineering examples used in this study.