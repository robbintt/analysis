---
ver: rpa2
title: Direct Preference-based Policy Optimization without Reward Modeling
arxiv_id: '2301.12842'
source_url: https://arxiv.org/abs/2301.12842
tags:
- learning
- policy
- return
- reinforcement
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DOS, an offline RL algorithm that directly
  optimizes policies from preference data without reward modeling. The method leverages
  a contrastive learning framework to design a policy scoring metric that ranks policies
  based on their alignment with trajectory returns.
---

# Direct Preference-based Policy Optimization without Reward Modeling

## Quick Facts
- arXiv ID: 2301.12842
- Source URL: https://arxiv.org/abs/2301.12842
- Reference count: 15
- Key outcome: Offline RL algorithm that directly optimizes policies from preference data without reward modeling, outperforming state-of-the-art methods on D4RL benchmarks

## Executive Summary
This paper introduces DOS (Direct Preference-based Policy Optimization), an offline RL algorithm that directly optimizes policies from preference data without requiring explicit reward modeling. The method leverages a contrastive learning framework to create a policy scoring metric that ranks policies based on their alignment with trajectory returns. DOS incorporates a conservative regularizer and return-based sample reweighting to handle distribution imbalance and ensure stability. Experiments demonstrate that DOS outperforms or matches state-of-the-art offline RL methods across D4RL benchmarks while using simpler network architectures and lower capacity requirements.

## Method Summary
DOS uses a contrastive learning framework to design a policy scoring metric that assigns higher scores to policies aligning with high-return trajectories. The algorithm computes policy-trajectory distances using mean ℓ2 distance between policy actions and trajectory actions, then applies a contrastive scoring function with conservative regularization. The scoring metric is S(π;D) = (2/(N(N-1))) ∑_i<j log(exp(-d(π,τi))/(exp(-d(π,τi)) + exp(-λd(π,τj)))), where λ controls the conservativeness. The method also incorporates return-based sample reweighting using kernel density estimation to handle imbalanced return distributions. Training uses standard supervised learning with batch updates, optimizing a two-layer MLP policy network with 256 hidden units.

## Key Results
- DOS outperforms or matches state-of-the-art offline RL methods on D4RL benchmarks
- The method excels particularly in high-dimensional control tasks and fine-tuning large language models
- DOS achieves strong performance with simpler network architectures and lower capacity requirements compared to existing supervised learning-based methods
- The algorithm shows stable learning dynamics with a consistent policy ranking correlation of 0.9 or higher

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The contrastive learning framework enables direct policy scoring without explicit reward modeling by comparing policy-trajectory distances across return levels
- Mechanism: Uses contrastive learning to create a scoring metric that gives higher scores to policies closer to high-return trajectories and farther from low-return trajectories through pairwise comparison of policy-trajectory distances weighted by return values
- Core assumption: Trajectory returns are reliable indicators of policy quality, and the distance between policy and trajectory can be meaningfully aggregated to capture policy performance
- Evidence anchors:
  - [abstract] "we adopt a contrastive learning framework to design a novel policy scoring metric that assigns a high score to policies that align with the given preferences"
  - [section] "we adopt a contrastive learning formulation: S(π; D) = 1/N−1 ∑i=1 log exp(−d(π,τi)) / ∑Nj=i exp(−d(π,τj))"

### Mechanism 2
- Claim: Conservative regularization prevents policy deviation from dataset while maintaining performance
- Mechanism: The λ parameter in the scoring function penalizes policies that deviate from the dataset by weighting policy-trajectory distances with λ < 1, creating a regularization effect that keeps policies close to observed data
- Core assumption: Policies that deviate too far from the dataset will perform poorly in the real environment, and this deviation can be measured through policy-trajectory distances
- Evidence anchors:
  - [section] "we introduce a regularizing factor that penalizes the offline policy for deviating too far from the dataset: S(π;D) = (2/N(N−1)) ∑1≤i<j≤N log exp(−d(π,τi)) / exp(−d(π,τi)) + exp(−λd(π,τj))"
  - [section] "A lower λ leads to a larger penalty when the policy deviates from the dataset"

### Mechanism 3
- Claim: Return-based sample reweighting handles imbalanced return distributions by giving appropriate importance to rare high-return trajectories
- Mechanism: Applies kernel density estimation to the return distribution and weights contrastive terms inversely proportional to return density, ensuring high-return trajectories receive appropriate attention despite their rarity
- Core assumption: The return distribution in the dataset is imbalanced, with high-return trajectories being rare but important for policy optimization
- Evidence anchors:
  - [section] "we modify our scoring metric to re-weight the contrastive terms with regard to the return density of the trajectories"
  - [section] "wi = 1/(1/N ∑Nl=1 k(Rl,Ri))" showing the density-based weighting formula

## Foundational Learning

- Concept: Contrastive learning framework
  - Why needed here: Provides a principled way to compare policies based on their alignment with trajectory returns without requiring explicit reward modeling
  - Quick check question: How does the contrastive loss ensure that policies are closer to high-return trajectories than low-return ones?

- Concept: Offline RL stability constraints
  - Why needed here: Ensures the learned policy doesn't deviate too far from the dataset, which is crucial for safe offline learning without online interaction
  - Quick check question: What role does the λ parameter play in maintaining policy stability during optimization?

- Concept: Distribution imbalance handling
  - Why needed here: Most real-world datasets have imbalanced return distributions, and the algorithm needs to give appropriate importance to rare high-return trajectories
  - Quick check question: Why is kernel density estimation used for return-based sample reweighting instead of simpler methods?

## Architecture Onboarding

- Component map:
  - Policy network (two-layer MLP with 256 hidden units) -> Policy-trajectory distance metric (mean ℓ2 distance) -> Contrastive scoring function with conservative regularization -> Sample reweighting based on return density

- Critical path:
  1. Compute policy-trajectory distances for each trajectory in batch
  2. Apply contrastive scoring with return-based weighting
  3. Backpropagate through policy network to maximize score
  4. Repeat until convergence

- Design tradeoffs:
  - Simplicity vs. expressivity: Using simple MLP architecture trades some representational power for stability and lower computational cost
  - Conservative vs. aggressive learning: λ parameter controls the tradeoff between staying close to dataset and exploring beyond it
  - Full-batch vs. mini-batch: Full-batch training on Gym tasks provides more stable gradients but may not scale to larger datasets

- Failure signatures:
  - Policy collapsing to dataset mean: λ too low or training diverging
  - Poor performance despite convergence: Return density estimation failing or contrastive scoring not capturing relevant features
  - High variance in results: Insufficient batch size or learning rate too high

- First 3 experiments:
  1. Verify policy-trajectory distance computation on simple synthetic data with known ground truth
  2. Test contrastive scoring with balanced return distribution to ensure basic mechanism works
  3. Evaluate sample reweighting on skewed return distributions to confirm it gives appropriate importance to rare high-return trajectories

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed DOS algorithm perform in online RL settings compared to offline RL?
- Basis in paper: [explicit] The paper focuses on offline RL and mentions that DOS is an offline RL algorithm.
- Why unresolved: The paper does not compare DOS to online RL algorithms or discuss its potential performance in online settings.
- What evidence would resolve it: Experiments comparing DOS to online RL algorithms or discussions on adapting DOS for online settings would provide insights.

### Open Question 2
- Question: What is the impact of using different aggregation functions in the policy-trajectory distance calculation?
- Basis in paper: [explicit] The paper mentions using the mean operator for aggregation but also notes that other options like log-sum-exp could be considered.
- Why unresolved: The paper does not explore the effects of using different aggregation functions on the performance of DOS.
- What evidence would resolve it: Experiments comparing the performance of DOS with different aggregation functions would clarify their impact.

### Open Question 3
- Question: How does the conservativeness regularizer parameter λ affect the exploration-exploitation trade-off in DOS?
- Basis in paper: [explicit] The paper introduces the conservativeness regularizer λ and discusses its role in penalizing policy deviation from the dataset.
- Why unresolved: The paper does not provide a detailed analysis of how varying λ influences the balance between exploration and exploitation.
- What evidence would resolve it: Experiments varying λ and analyzing the resulting policies' exploration and exploitation behaviors would provide insights.

## Limitations
- The method's effectiveness depends heavily on the quality of the policy-trajectory distance metric and the assumption that trajectory returns correlate well with policy quality
- The algorithm has not been extensively tested on diverse real-world datasets beyond the D4RL benchmarks
- Scalability to larger datasets and more complex environments beyond the tested benchmarks remains unproven

## Confidence
- **High confidence**: The basic mechanism of contrastive learning for policy scoring (Mechanism 1) and the conservative regularization approach (Mechanism 2) are well-supported by theoretical analysis and experimental results
- **Medium confidence**: The return-based sample reweighting (Mechanism 3) shows promise but requires more extensive validation across different dataset characteristics
- **Low confidence**: The scalability of the method to larger datasets and more complex environments beyond the tested D4RL benchmarks

## Next Checks
1. Test DOS on datasets with deliberately skewed return distributions to verify the effectiveness of the return-based sample reweighting mechanism
2. Compare different policy-trajectory distance metrics (e.g., KL divergence vs ℓ2 distance) to assess sensitivity to the choice of metric
3. Evaluate performance on a larger-scale language modeling task beyond the brief mention in the paper to validate scalability claims