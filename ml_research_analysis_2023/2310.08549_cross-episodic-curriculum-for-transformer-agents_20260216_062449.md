---
ver: rpa2
title: Cross-Episodic Curriculum for Transformer Agents
arxiv_id: '2310.08549'
source_url: https://arxiv.org/abs/2310.08549
tags:
- learning
- curriculum
- arxiv
- agents
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Cross-Episodic Curriculum (CEC), a method that
  improves sample efficiency and generalization of Transformer agents by placing cross-episodic
  experiences into a curriculum. CEC is effective for both RL and IL scenarios, and
  it achieves comparable performance to RL oracles and outperforms other baseline
  methods.
---

# Cross-Episodic Curriculum for Transformer Agents

## Quick Facts
- arXiv ID: 2310.08549
- Source URL: https://arxiv.org/abs/2310.08549
- Authors: 
- Reference count: 40
- Key outcome: CEC improves sample efficiency and generalization of Transformer agents by placing cross-episodic experiences into a curriculum, achieving comparable performance to RL oracles and outperforming baseline methods.

## Executive Summary
This paper introduces Cross-Episodic Curriculum (CEC), a method that enhances the sample efficiency and generalization of Transformer agents in both reinforcement learning (RL) and imitation learning (IL) scenarios. CEC leverages cross-episodic attention to distill policy improvements across episodes by organizing experiences into a curriculum that captures learning progression or demonstrator expertise. The method demonstrates superior performance on embodied tasks, particularly in generalization to unseen scenarios and perturbations.

## Method Summary
CEC organizes multiple experiences to capture learning progression or demonstrator expertise, creating a curriculum that encapsulates policy improvement. During training, a Transformer-XL model with cross-episodic attention attends to past episodes' observations and actions, internalizing the learning signals encoded in the curriculum. The model is trained using a negative log-likelihood objective to distill these improvements into its weights. CEC is effective across diverse learning scenarios, including RL with online learning and IL with mixed-quality demonstrations.

## Key Results
- CEC achieves comparable performance to RL oracles and outperforms baseline methods in sample efficiency and generalization.
- The method yields robust embodied policies that generalize well to unseen scenarios and perturbations across various axes.
- CEC is effective across a wide variety of learning scenarios, including multi-task RL and IL with mixed-quality data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-episodic attention allows the model to distill policy improvements across episodes by tracing back beyond the current trial.
- Mechanism: During training, the Transformer agent can attend to past episodes' observations and actions, internalizing the learning progression encoded in curricular data. This enables the model to learn from both successful and suboptimal experiences.
- Core assumption: The sequential ordering of experiences in the curriculum captures meaningful learning signals that are not present in isolated episodes.
- Evidence anchors:
  - [abstract]: "By sequentially structuring online learning trials and mixed-quality demonstrations, CEC constructs curricula that encapsulate learning progression and proficiency increase across episodes."
  - [section 2.2]: "Meaningful learning signals emerge when multiple trajectories are organized and examined cross-episodically along a curriculum axis."

### Mechanism 2
- Claim: Transformer agents trained with CEC exhibit superior generalization by learning robust policies that can handle unseen environment dynamics and perturbations.
- Mechanism: The cross-episodic attention mechanism allows the model to learn from a diverse set of experiences, including those with varying quality and difficulty. This exposure to a wide range of scenarios helps the model develop robust policies that can generalize to new situations.
- Core assumption: The diversity of experiences in the curriculum, including both optimal and suboptimal demonstrations, contributes to the model's ability to generalize.
- Evidence anchors:
  - [abstract]: "CEC also yields robust embodied policies that generalize well to unseen scenarios."
  - [section 4.1]: "Cross-episodic curriculum boosts the generalization capability... CEC generally improves Transformer agents in learning robust policies that can generalize to perturbations across various axes."

### Mechanism 3
- Claim: CEC is effective across a wide variety of learning scenarios, including both RL and IL settings, by leveraging the improving nature of sequential experiences.
- Mechanism: The curriculum-based approach allows CEC to adapt to different learning scenarios by organizing experiences in a way that captures the learning progression or proficiency increase. This flexibility enables the method to be effective in both RL and IL settings.
- Core assumption: The improving nature of sequential experiences, whether from online learning or human demonstrations, provides a valuable learning signal that can be leveraged by the cross-episodic attention mechanism.
- Evidence anchors:
  - [abstract]: "The effectiveness of CEC is demonstrated under two representative scenarios: one involving multi-task reinforcement learning... and the other involving imitation learning with mixed-quality data..."
  - [section 4.1]: "Cross-episodic curriculum is effective across a wide variety of learning scenarios... This is a common scenario, especially in robotics, where demonstrations are collected by human operators with varying proficiency."

## Foundational Learning

- Concept: Reinforcement Learning (RL)
  - Why needed here: Understanding the RL setting is crucial for grasping the problem domain and the challenges faced by Transformer agents in learning from trial and error in partially observable environments.
  - Quick check question: What is the main goal of RL, and how does it differ from other learning paradigms like supervised learning?

- Concept: Imitation Learning (IL)
  - Why needed here: Familiarity with IL is necessary to understand the alternative learning scenario considered in the paper, where the agent learns from demonstrations instead of trial and error.
  - Quick check question: How does IL differ from RL, and what are the challenges associated with learning from demonstrations, especially when they are of mixed quality?

- Concept: Transformer Architecture
  - Why needed here: Knowledge of the Transformer architecture and its attention mechanism is essential for understanding how cross-episodic attention is implemented and how it enables the model to learn from past experiences.
  - Quick check question: What is the key feature of the Transformer architecture that allows it to process sequential data effectively, and how does this relate to the cross-episodic attention mechanism?

## Architecture Onboarding

- Component map: Observation Encoder → Transformer Backbone (with cross-episodic attention) → Action Decoder
- Critical path: Observation Encoder → Transformer Backbone (with cross-episodic attention) → Action Decoder
- Design tradeoffs:
  - Using a powerful architecture like Transformer-XL allows for effective cross-episodic attention but may require more computational resources compared to simpler architectures.
  - The choice of curriculum granularity (fine, medium, or coarse) affects the model's performance, with finer curricula generally leading to better results but requiring more careful design.
- Failure signatures:
  - If the cross-episodic attention is not properly implemented or the curriculum does not capture meaningful learning signals, the model may fail to learn effective policies.
  - Overfitting to the training data or the specific curriculum used can lead to poor generalization performance.
- First 3 experiments:
  1. Train a Transformer agent on a simple RL task (e.g., Goal Maze) using the learning-progress-based curriculum and evaluate its performance compared to a baseline without cross-episodic attention.
  2. Investigate the impact of curriculum granularity by training agents with fine, medium, and coarse curricula and comparing their performance.
  3. Test the generalization capabilities of the trained agents by evaluating them on unseen environment dynamics or out-of-distribution difficulty levels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of curriculum granularity on the performance of CEC in complex, high-dimensional tasks?
- Basis in paper: [explicit] The paper discusses the effect of curriculum granularity on performance, showing that fine-grained curricula are critical for Transformer agents to benefit from cross-episodic training.
- Why unresolved: The paper only explores granularity in the context of simpler tasks. The impact on more complex, high-dimensional tasks remains unexplored.
- What evidence would resolve it: Empirical results comparing the performance of CEC with different levels of granularity on complex, high-dimensional tasks.

### Open Question 2
- Question: How does the expertise-based curriculum perform when the proficiency of demonstrators is not explicitly labeled?
- Basis in paper: [explicit] The paper discusses the expertise-based curriculum and mentions that some IL benchmarks come with proficiency labels, but it also notes that a broader application would require approximating proficiency.
- Why unresolved: The paper does not provide empirical results on how the expertise-based curriculum performs when proficiency is not explicitly labeled.
- What evidence would resolve it: Empirical results comparing the performance of CEC with and without explicitly labeled proficiency in IL tasks.

### Open Question 3
- Question: What is the effect of varying the context length on the performance of CEC in tasks with long horizons?
- Basis in paper: [explicit] The paper discusses the effect of varying context length, showing that both too short and unnecessarily long context windows decrease performance.
- Why unresolved: The paper does not explore the effect of context length in tasks with long horizons.
- What evidence would resolve it: Empirical results comparing the performance of CEC with different context lengths in tasks with long horizons.

## Limitations

- The evaluation focuses primarily on relatively simple embodied tasks with deterministic or semi-deterministic environments, leaving the performance on more complex, high-dimensional tasks unverified.
- The implementation details for the cross-episodic attention mechanism are not fully specified, making exact reproduction challenging.
- The paper does not extensively explore the computational overhead introduced by the curriculum-based approach compared to standard training methods.

## Confidence

**High Confidence Claims:**
- CEC improves sample efficiency in Transformer agents for both RL and IL scenarios
- The method generalizes well to unseen scenarios and perturbations
- Cross-episodic attention effectively captures learning progression across episodes

**Medium Confidence Claims:**
- CEC achieves comparable performance to RL oracles
- The specific curriculum granularity (fine vs. medium vs. coarse) significantly impacts performance
- The method's effectiveness across diverse embodied tasks

**Low Confidence Claims:**
- CEC's performance on highly complex, real-world robotic tasks
- Scalability to tasks with significantly larger state and action spaces
- Computational efficiency compared to standard training approaches

## Next Checks

1. **Complexity Scaling Test**: Evaluate CEC on more complex embodied tasks with higher-dimensional state spaces (e.g., Habitat environments with photorealistic rendering) to verify if the performance gains scale with task complexity.

2. **Computational Overhead Analysis**: Measure and compare the wall-clock training time and memory usage of CEC against standard Transformer training methods to quantify the computational trade-offs.

3. **Curriculum Sensitivity Analysis**: Systematically vary the curriculum design parameters (e.g., episode selection criteria, curriculum length, granularity) to identify the sensitivity of performance to these design choices and determine optimal configurations for different task types.