---
ver: rpa2
title: 'Towards Filling the Gap in Conversational Search: From Passage Retrieval to
  Conversational Response Generation'
arxiv_id: '2308.08911'
source_url: https://arxiv.org/abs/2308.08911
tags:
- workers
- task
- annotations
- crowd
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of synthesizing relevant passages
  into a complete, concise response in conversational search. The authors focus on
  collecting high-quality snippet-level answer annotations for two TREC Conversational
  Assistance datasets.
---

# Towards Filling the Gap in Conversational Search: From Passage Retrieval to Conversational Response Generation

## Quick Facts
- arXiv ID: 2308.08911
- Source URL: https://arxiv.org/abs/2308.08911
- Reference count: 39
- Primary result: Collect high-quality snippet-level annotations for conversational search datasets to enable training response generation models

## Executive Summary
This paper addresses the challenge of synthesizing relevant passages into complete, concise responses for conversational search. The authors focus on collecting high-quality snippet-level answer annotations for TREC Conversational Assistance datasets (CAsT 2020 and 2022). Through a preliminary study comparing different annotation interfaces and worker types, they identify effective task designs and trade-offs between worker qualifications and costs. The resulting CAsT-snippets dataset enables training response generation models that ground answers in actual statements and allows for automatic evaluation of generated responses in terms of completeness.

## Method Summary
The authors conduct a preliminary study comparing paragraph-based and sentence-based annotation interfaces, as well as different worker types (regular, master, and expert). Based on the results, they refine their annotation protocol and proceed with large-scale data collection, gathering annotations for 1.8k question-paragraph pairs. The process involves selecting top 5 relevant passages per query from TREC CAsT datasets, releasing annotation tasks to qualified crowd workers on MTurk, and providing continuous feedback through a communication platform. Manual verification and quality control ensure high-quality annotations, with inter-annotator agreement measured using Jaccard similarity and Jaccard_k variants.

## Key Results
- Sentence-level annotation tasks yield higher inter-annotator agreement (0.71) compared to paragraph-level tasks (0.54)
- The CAsT-snippets dataset enables training of response generation models that ground answers in actual statements
- Automatic evaluation of generated responses becomes possible by measuring completeness at the snippet level

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Crowd worker agreement is low on snippet selection because the task is inherently subjective and lacks clear boundaries for "relevant" text.
- **Mechanism:** Workers interpret relevance differently based on context, prior knowledge, and personal judgment, leading to sparse overlap in selected spans even when all are "correct."
- **Core assumption:** There is no single correct set of snippets; multiple valid interpretations exist for a given passage-query pair.
- **Evidence anchors:**
  - [abstract] "Measuring the quality of annotations is challenging because relevant snippet selection is subjective and often there are multiple correct sets of snippets in a given passage."
  - [section] "We notice that the paragraph-based variant yields higher-quality data than the sentence-based one."
- **Break condition:** If task instructions are made highly prescriptive (e.g., fixed rules for span length, explicit context limits), agreement might artificially increase but at the cost of losing nuanced, valid interpretations.

### Mechanism 2
- **Claim:** Expert annotators achieve higher agreement than crowd workers because they are trained and consistent in their interpretation of relevance.
- **Mechanism:** Experts undergo structured training, follow detailed guidelines, and receive iterative feedback, leading to more uniform snippet selection.
- **Core assumption:** Training and feedback reduce variance in subjective judgments across annotators.
- **Evidence anchors:**
  - [abstract] "The close collaboration with crowd workers at every stage of data annotation has revealed multiple challenges..."
  - [section] "Table 2: Inter-annotator agreements (J and Jâ‚–)... Expert (m=3) 0.25 - - 0.54"
- **Break condition:** If expert annotators are given ambiguous or incomplete guidelines, their agreement may drop to crowd worker levels.

### Mechanism 3
- **Claim:** Sentence-level annotation tasks yield higher inter-annotator agreement than paragraph-level tasks because the candidate spans are shorter and more constrained.
- **Mechanism:** Shorter text segments reduce ambiguity and the probability of selecting different but equally valid snippets.
- **Core assumption:** The probability of selecting the same snippet increases as the length of the candidate text decreases.
- **Evidence anchors:**
  - [abstract] "we consider paragraph- and sentence-level snippet annotation interfaces"
  - [section] "Table 2: ... Sentence MTurk regular (n=3) 0.35 - - 0.71 ... MTurk master (n=3) 0.47 - - 0.76"
- **Break condition:** If sentences are too short to contain meaningful context, workers may select multiple fragments per sentence, reintroducing ambiguity.

## Foundational Learning

- **Concept:** Inter-annotator agreement metrics (e.g., Jaccard similarity, Fleiss' Kappa)
  - Why needed here: To quantify the consistency of crowd worker annotations and identify sources of disagreement.
  - Quick check question: What is the main difference between Jaccard similarity and Fleiss' Kappa when evaluating snippet annotations?

- **Concept:** Snippet-level relevance annotation vs. document-level relevance annotation
  - Why needed here: Snippet-level annotations enable more granular evaluation of response generation models by measuring completeness at the information nugget level.
  - Quick check question: How does snippet-level annotation improve the evaluation of conversational response generation compared to document-level relevance?

- **Concept:** Crowdsourcing quality control mechanisms (e.g., qualification tasks, continuous feedback, acceptance rates)
  - Why needed here: To ensure high-quality annotations when tasks are subjective and workers vary in skill and engagement.
  - Quick check question: Why might automatic quality control criteria fail to capture the true quality of snippet annotations?

## Architecture Onboarding

- **Component map:** TREC CAsT datasets -> Top 5 passages per query -> MTurk annotation interface -> Crowd workers (master workers) -> Manual verification -> CAsT-snippets dataset

- **Critical path:**
  1. Select queries and top 5 relevant passages per query
  2. Release annotation tasks to qualified crowd workers
  3. Workers select relevant snippets in each passage
  4. Manual verification and feedback loop
  5. Aggregate annotations and compute agreement metrics

- **Design tradeoffs:**
  - Paragraph-based vs. sentence-based tasks: Higher quality vs. higher agreement
  - Larger vs. smaller worker pool: Cost vs. consistency
  - Automatic vs. manual quality control: Speed vs. accuracy

- **Failure signatures:**
  - Low inter-annotator agreement (< 0.3) suggests unclear task instructions or overly subjective criteria
  - High rejection rates indicate mismatch between worker expectations and task requirements
  - Inconsistent feedback from workers signals ambiguous guidelines

- **First 3 experiments:**
  1. Run a small batch of paragraph-based annotations and measure Jaccard agreement; compare against sentence-based agreement.
  2. Test different snippet length limits (e.g., 30% vs. 50% of passage) and observe effects on agreement and worker satisfaction.
  3. Compare agreement scores when using MTurk master workers vs. regular workers on the same query-passage pairs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively determine the appropriate amount of context to include in snippet spans, balancing conciseness and self-containment?
- Basis in paper: [explicit] The paper discusses challenges in determining the appropriate amount of context to include in each span, striking a balance between conciseness and being self-contained.
- Why unresolved: This challenge arises from the need to ensure that snippets are informative and self-contained while avoiding unnecessary verbosity. The optimal balance may depend on the specific query and passage characteristics.
- What evidence would resolve it: Experimental studies comparing different context inclusion strategies and their impact on snippet quality and downstream tasks (e.g., response generation, retrieval evaluation) could provide insights into the optimal balance.

### Open Question 2
- Question: How should subjective opinions in passages be handled when selecting snippets for conversational responses?
- Basis in paper: [explicit] The paper mentions that passages originating from blogs or comments often contain subjective opinions and raises the question of whether such opinions should be marked up as answers.
- Why unresolved: Handling subjective opinions in snippets is challenging because it requires determining the appropriateness of including personal viewpoints in conversational responses. The decision may depend on the nature of the query and the desired response style.
- What evidence would resolve it: User studies and expert evaluations of snippets containing subjective opinions in different conversational contexts could help establish guidelines for handling such cases.

### Open Question 3
- Question: What is the appropriate level of background knowledge to assume when passages do not contain direct answers but answers can be inferred from the text?
- Basis in paper: [explicit] The paper discusses the challenge of background knowledge required to select correct spans or determine unanswerability, raising questions about the necessary general/expert knowledge for annotation.
- Why unresolved: Determining the appropriate level of background knowledge is complex because it involves balancing the need for concise responses with the risk of assuming too much or too little knowledge. The optimal level may vary depending on the specific domain and user group.
- What evidence would resolve it: Empirical studies comparing the performance of response generation models trained with different levels of assumed background knowledge could provide insights into the impact of this factor on response quality and user satisfaction.

## Limitations

- The analysis is primarily based on the paper's abstract and methodology section, without access to the full experimental results and detailed data analysis.
- The inherent biases and limitations of crowdsourcing platforms like MTurk may still impact the quality and consistency of the annotations.
- The subjective nature of snippet selection can lead to low inter-annotator agreement, limiting the reliability and reproducibility of the annotations.

## Confidence

- **High Confidence**: The claim that sentence-level annotation tasks yield higher inter-annotator agreement than paragraph-level tasks is supported by the reported agreement scores (0.71 vs. 0.54 for sentence vs. paragraph level).
- **Medium Confidence**: The claim that expert annotators achieve higher agreement than crowd workers is plausible based on the reported agreement scores (0.54 for experts vs. 0.71 for master workers), but more detailed analysis is needed to confirm this.
- **Low Confidence**: The claim that crowd worker agreement is low due to the inherent subjectivity of the task is based on qualitative observations and limited quantitative evidence. Further investigation into the specific factors contributing to disagreement is needed.

## Next Checks

1. **Reproduce Agreement Scores**: Conduct a small-scale annotation experiment using the same task design and worker pool to reproduce the reported inter-annotator agreement scores. Compare the results with the original paper to assess the reliability and generalizability of the findings.

2. **Analyze Disagreement Factors**: Perform a detailed analysis of the factors contributing to low agreement in snippet selection, such as the presence of partial answers, temporal considerations, and background knowledge requirements. Identify specific query-passage pairs or topic areas that consistently yield low agreement and investigate potential solutions.

3. **Evaluate Annotation Quality**: Develop a robust evaluation framework to assess the quality and usefulness of the annotations for training response generation models. Compare the performance of models trained on the CAsT-snippets dataset against models trained on alternative datasets or using different annotation approaches.