---
ver: rpa2
title: Assessing Intra-class Diversity and Quality of Synthetically Generated Images
  in a Biomedical and Non-biomedical Setting
arxiv_id: '2308.02505'
source_url: https://arxiv.org/abs/2308.02505
tags:
- images
- synthetic
- diversity
- biomedical
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the intra-class diversity and quality of synthetic
  biomedical images generated by Deep Convolutional GANs (DCGAN) across different
  imaging modalities (X-ray, OCT) and sample sizes. The diversity is assessed using
  Multi-scale Structural Similarity Index Measure (MS-SSIM) and Cosine Distance (CD),
  while quality is evaluated using Frechet Inception Distance (FID).
---

# Assessing Intra-class Diversity and Quality of Synthetically Generated Images in a Biomedical and Non-biomedical Setting

## Quick Facts
- arXiv ID: 2308.02505
- Source URL: https://arxiv.org/abs/2308.02505
- Authors:
- Reference count: 8
- Primary result: MS-SSIM, CD, and FID scores vary significantly across biomedical imaging modalities due to distinct feature distributions, while sample size doesn't significantly impact these metrics.

## Executive Summary
This study evaluates synthetic biomedical images generated by Deep Convolutional GANs (DCGAN) across X-ray, OCT, and non-biomedical (Fashion MNIST) imaging modalities. The research focuses on measuring intra-class diversity using MS-SSIM and Cosine Distance, and quality using Frechet Inception Distance. The analysis reveals that metric scores vary significantly between different imaging modalities due to their distinct feature distributions, while sample size (25%-100%) shows no significant impact on evaluation outcomes. The findings suggest synthetic images have inherently limited diversity compared to real images, highlighting the importance of considering modality-specific features when evaluating synthetic biomedical image quality.

## Method Summary
The study uses chest X-ray images from Pneumoniamnist, retinal OCT images from OCTmnist, and Fashion MNIST at 28x28 resolution. DCGAN is trained separately for each class with a 100-dimensional latent input, 500 epochs, and batch size 128. Synthetic images are generated at 1:1 ratio with real images across four sample sizes (25%, 50%, 75%, 100%). Evaluation employs MS-SSIM and Cosine Distance for diversity assessment, and Frechet Inception Distance for quality measurement. The experimental design systematically compares metric scores across biomedical-to-biomedical and biomedical-to-non-biomedical imaging modality pairs.

## Key Results
- MS-SSIM, CD, and FID scores show significant variation across different imaging modalities due to distinct feature distributions
- Sample size (25%-100%) has no significant impact on evaluating intra-class diversity and quality of synthetic images
- OCT images show worse MS-SSIM and CD scores compared to X-ray images due to their more diverse features being harder to learn with DCGAN
- Synthetic images exhibit limited diversity and quality compared to real images, regardless of sample size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MS-SSIM, CD, and FID scores vary significantly across different biomedical imaging modalities because each modality has distinct image feature distributions (textures, luminance, orientation) that these metrics capture differently.
- Mechanism: The metrics compute distances or similarities using image features extracted by pre-trained models (Inception for FID, structural similarity maps for MS-SSIM, feature vectors for CD). When applied to X-ray vs OCT vs non-biomedical images, the underlying feature distributions shift, causing metric score variance.
- Core assumption: The feature extractors used by these metrics are sensitive to modality-specific characteristics like texture granularity and spatial structure.
- Evidence anchors:
  - [abstract] "MS-SSIM, CD, and FID scores vary significantly across biomedical-to-biomedical and biomedical-to-non-biomedical imaging modalities due to the distinct distribution of image features."
  - [section] "MS-SSIM, CD, and FID are significantly dependent on the salient features inherent in an imaging modality."
- Break condition: If the feature extractors were modality-agnostic or normalized across domains, scores would not vary significantly.

### Mechanism 2
- Claim: Sample size (25%, 50%, 75%, 100% of real images) does not significantly impact metric scores because synthetic images have inherently limited diversity and quality compared to real images.
- Mechanism: Even when increasing sample size, the synthetic image distribution remains constrained by the generator's learned manifold, so relative metric comparisons between real and synthetic distributions do not shift substantially.
- Core assumption: Mode collapse or limited generative capacity in DCGAN restricts synthetic image diversity regardless of sample size.
- Evidence anchors:
  - [section] "sample size has no significant impact on evaluating the intra-class diversity and quality of biomedical and non-biomedical images."
  - [section] "One possible reason could be that synthetic images may exhibit limited diversity and quality as compared to real images."
- Break condition: If synthetic images matched real diversity, larger sample sizes would shift metric scores.

### Mechanism 3
- Claim: The DCGAN architecture's performance is modality-dependent because different imaging domains present feature distributions that are easier or harder to model.
- Mechanism: OCT images contain more diverse features (textures, fine structures) than X-ray images, making them harder for DCGAN to learn, leading to worse MS-SSIM/CD scores for synthetic OCT.
- Core assumption: GAN training dynamics and convergence are influenced by the complexity and diversity of the training data distribution.
- Evidence anchors:
  - [section] "MS-SSIM and CD scores of OCT images worsen over X-ray images because OCT images contain more diverse features than X-ray images, which are difficult to learn and train with the DCGAN."
- Break condition: If DCGAN were modality-agnostic or adapted per domain, score patterns would differ.

## Foundational Learning

- Concept: Feature distribution sensitivity in GAN evaluation metrics
  - Why needed here: Understanding why MS-SSIM, CD, FID vary across modalities requires knowing that these metrics extract and compare image features sensitive to domain characteristics.
  - Quick check question: Why would a metric trained on natural images score synthetic X-rays differently than synthetic OCT images?

- Concept: Mode collapse and synthetic image diversity
  - Why needed here: The study's conclusion that sample size doesn't matter hinges on synthetic images having limited diversity regardless of sample size.
  - Quick check question: If synthetic images perfectly matched real diversity, how would increasing sample size affect metric scores?

- Concept: DCGAN architecture and training dynamics
  - Why needed here: The paper attributes differences in synthetic image quality to DCGAN's ability (or inability) to model modality-specific features.
  - Quick check question: What aspects of DCGAN architecture make it more suitable for modeling simple textures (X-ray) versus complex textures (OCT)?

## Architecture Onboarding

- Component map: Data ingestion -> DCGAN training -> Synthetic image generation -> Metric evaluation -> Comparison analysis
- Critical path:
  1. Load balanced class data
  2. Train DCGAN for 500 epochs, batch size 128
  3. Generate synthetic images (1:1 ratio with real)
  4. Evaluate MS-SSIM, CD, FID across sample sizes
  5. Compare scores intra- and inter-modality
- Design tradeoffs:
  - Small image size (28x28) limits feature richness but speeds training
  - 1:1 synthetic:real ratio ensures fair comparison but may waste capacity if synthetics are low quality
  - Using DCGAN (baseline) makes results interpretable but may underperform more advanced GANs
- Failure signatures:
  - Uniform metric scores across modalities → feature extractor insensitivity
  - High variance in scores with small sample size changes → synthetic images capturing real diversity
  - Poor FID but good MS-SSIM → quality mismatch despite structural similarity
- First 3 experiments:
  1. Train DCGAN on normal X-ray class, generate synthetic set, compute MS-SSIM, CD, FID vs real
  2. Repeat experiment 1 on Drusen OCT class; compare metric patterns to X-ray
  3. Repeat experiment 1 on Fashion MNIST "Trouser" class; compare biomedical vs non-biomedical metric behavior

## Open Questions the Paper Calls Out

- What is the minimum sample size required to accurately measure the MS-SSIM, CD, and FID metrics for synthetic biomedical images?
  - Basis in paper: [explicit] The paper mentions that "The minimum sample size should be a few hundred images of the real and synthetic images to measure the MS-SSIM, CD, and FID metrics."
  - Why unresolved: The exact minimum sample size is not specified, only that it should be a few hundred images.
  - What evidence would resolve it: Conducting experiments with varying sample sizes and measuring the stability of the MS-SSIM, CD, and FID scores to determine the minimum sample size needed for reliable results.

- How does the architecture of GANs impact the generation of synthetic images across different domains of biomedical imaging?
  - Basis in paper: [explicit] The paper states that "It also indicates that the architecture of GANs has a significant impact on the generation of synthetic images across different domains of biomedical imaging."
  - Why unresolved: The specific aspects of GAN architecture that affect synthetic image generation in different biomedical imaging domains are not detailed.
  - What evidence would resolve it: Comparative studies of different GAN architectures applied to various biomedical imaging domains, analyzing the quality and diversity of generated images.

- What are the inherent features of different imaging modalities that cause significant variance in MS-SSIM, CD, and FID scores?
  - Basis in paper: [explicit] The paper mentions that "MS-SSIM, CD, and FID are significantly dependent on the salient features inherent in an imaging modality."
  - Why unresolved: The paper does not specify which features of imaging modalities lead to the variance in metric scores.
  - What evidence would resolve it: Detailed analysis of image features (e.g., textures, luminance, object orientation) in different imaging modalities and their correlation with MS-SSIM, CD, and FID scores.

## Limitations
- DCGAN architecture may not represent state-of-the-art generative capabilities, limiting generalizability
- 28x28 resolution constrains feature complexity and may underrepresent real biomedical image diversity
- The study assumes limited synthetic image diversity without directly measuring it

## Confidence
- High confidence: MS-SSIM, CD, and FID scores vary significantly across imaging modalities due to distinct feature distributions
- Medium confidence: Sample size doesn't impact metric scores due to synthetic image diversity limitations
- Low confidence: DCGAN performance is inherently modality-dependent without testing alternative architectures

## Next Checks
1. Repeat experiments using StyleGAN or other modern GAN architectures to test if modality-dependent performance patterns persist
2. Replicate analysis with higher resolution images (128x128 or 256x256) to assess metric pattern stability with more fine-grained features
3. Implement quantitative measures of synthetic image diversity (perceptual path length, precision-recall curves) to directly test the diversity limitation assumption