---
ver: rpa2
title: 'LLMSTEP: LLM proofstep suggestions in Lean'
arxiv_id: '2310.18457'
source_url: https://arxiv.org/abs/2310.18457
tags:
- llmstep
- language
- lean
- suggestions
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLMSTEP integrates language models into the Lean 4 proof assistant
  to suggest next proof steps. It sends proof states and user-provided prefixes to
  a server hosting a language model, receives tactic suggestions, checks them in Lean,
  and displays them to the user.
---

# LLMSTEP: LLM proofstep suggestions in Lean

## Quick Facts
- arXiv ID: 2310.18457
- Source URL: https://arxiv.org/abs/2310.18457
- Reference count: 20
- Primary result: 50.1% proof completion on mathlib4-test, 27.9% on miniF2F-test

## Executive Summary
LLMSTEP integrates language models into the Lean 4 proof assistant to provide real-time tactic suggestions during proof development. The system extracts proof states from Lean 4 and sends them to a server hosting a language model, which returns tactic suggestions that are validated and displayed to users through the IDE interface. A Pythia 2.8b model fine-tuned on Lean proof data achieves 50.1% proof completion on standard benchmarks, outperforming recent open-source alternatives.

The tool supports multiple compute environments including CPU, GPU, and Google Colab, enabling deployment across different hardware constraints. Runtime experiments show GPU inference via vLLM generates suggestions in 0.11 seconds, while CPU inference with smaller models takes several seconds. The framework provides a foundation for further research on language model integration in formal proof assistants.

## Method Summary
LLMSTEP implements a three-part architecture consisting of a Lean 4 tactic that extracts proof states, a server hosting a language model, and the language model itself. The Lean tactic serializes the current proof state and sends it to the server, which queries the language model for tactic suggestions. These suggestions are checked for validity in Lean and displayed to the user through the VS Code extension. The system supports prefix filtering to improve suggestion relevance and can work with various language model architectures. The baseline Pythia 2.8b model is fine-tuned on (state, next-tactic) pairs from the LeanDojo Benchmark 4 dataset using beam search with expansion size 32.

## Key Results
- Pythia 2.8b model achieves 50.1% proof completion on mathlib4-test benchmark
- Same model achieves 27.9% completion on miniF2F-test benchmark
- GPU inference via vLLM generates suggestions in 0.11 seconds
- CPU inference with smaller models takes several seconds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMSTEP accelerates proof development by integrating real-time tactic suggestions directly into the Lean IDE workflow
- Mechanism: Lean 4 metaprogramming sends proof state and prefix to language model server, receives suggestions, checks validity in Lean, and displays in Infoview
- Core assumption: The Lean proof state can be reliably serialized and sent to external servers for processing
- Evidence anchors:
  - [abstract] "LLMSTEP is a Lean 4 tactic that sends a user's proof state to a server hosting a language model"
  - [section] "LLMSTEP consists of three parts: (1) a Lean tactic, (2) a language model, (3) a server"
  - [corpus] Weak evidence - related papers discuss similar integration but focus on different proof assistants
- Break condition: Network connectivity failure, server overload, or malformed proof state serialization

### Mechanism 2
- Claim: Language model suggestions improve proof completion rates compared to baseline models
- Mechanism: Fine-tuned Pythia 2.8b model on (state, next-tactic) pairs from Lean Dojo Benchmark 4 achieves 50.1% proof completion on mathlib4-test
- Core assumption: Language models trained on Lean proof data can generalize to unseen proof states
- Evidence anchors:
  - [abstract] "A baseline Pythia 2.8b model fine-tuned on (state, next-tactic) examples achieves 50.1% proof completion on mathlib4-test"
  - [section] "We validate the Pythia model in Table 1, finding that it can exceed the number of closed theorems by ReProver"
  - [corpus] Moderate evidence - related work shows similar approach in other proof assistants
- Break condition: Training data bias, domain shift to miniF2F problems, or model capacity limitations

### Mechanism 3
- Claim: Flexible compute environment support enables practical deployment across different hardware constraints
- Mechanism: Multiple server implementations (CPU, CUDA GPU, Google Colab) provide options from fast inference to low-resource inference
- Core assumption: Language model inference can be effectively distributed across different hardware configurations
- Evidence anchors:
  - [abstract] "We provide server implementations that run on CPU, a CUDA GPU, or a Google Colab notebook"
  - [section] "Runtime experiments show that GPU inference via vLLM can generate suggestions in 0.11 seconds, while CPU inference with smaller models takes several seconds"
  - [corpus] Strong evidence - multiple related papers emphasize compute flexibility for proof assistants
- Break condition: GPU unavailability, Colab session timeout, or CPU memory constraints

## Foundational Learning

- Concept: Lean 4 metaprogramming and tactic state management
  - Why needed here: LLMSTEP requires understanding how to extract and manipulate proof states within Lean 4's metaprogramming framework
  - Quick check question: What is the difference between a Lean tactic state and the proof script a user writes?

- Concept: Language model fine-tuning on formal proof data
  - Why needed here: The baseline Pythia model requires understanding how to format (state, next-tactic) training pairs and train effectively
  - Quick check question: How does the token format "[GOAL]tactic-state[PROOFSTEP]next-tactic<|endoftext|>" help the language model learn proof step prediction?

- Concept: Best-first search in proof space
  - Why needed here: Proof search evaluation uses best-first search with beam search to measure language model utility
  - Quick check question: What are the key parameters that control the trade-off between search completeness and computational efficiency?

## Architecture Onboarding

- Component map: Lean 4 tactic -> Server -> Language model -> Lean validation -> Infoview
- Critical path: User writes proof → calls llmstep → Lean extracts state → sends to server → server queries model → checks suggestions → displays in Infoview
- Design tradeoffs:
  - Server vs embedded model: Server allows larger models but adds latency; embedded models are faster but limited in size
  - Prefix filtering: Improves relevance but may miss valid suggestions without prefix
  - Suggestion checking: Ensures correctness but adds verification overhead
- Failure signatures:
  - No suggestions returned: Server down, network issues, or model generation failure
  - Invalid suggestions: Model generating tactics that don't match current state or syntax errors
  - Slow suggestions: CPU inference, network latency, or server overload
- First 3 experiments:
  1. Run LLMSTEP with simple prefix on a basic proof and verify suggestions appear in Infoview
  2. Test CPU vs GPU inference runtime using the provided runtime measurement code
  3. Evaluate model performance on a small set of proof states using the proof search implementation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLMSTEP's performance scale with different model sizes and architectures?
- Basis in paper: [explicit] The paper mentions that LLMSTEP is agnostic to the choice of language model and provides runtime comparisons between Pythia 2.8b and ReProver models.
- Why unresolved: The paper only compares two specific models, leaving open questions about the performance trade-offs of other architectures and sizes.
- What evidence would resolve it: Systematic benchmarking of LLMSTEP with various language model sizes and architectures, measuring both proof completion rates and inference times.

### Open Question 2
- Question: What is the optimal prefix length and content for generating high-quality suggestions?
- Basis in paper: [explicit] The paper describes LLMSTEP's prefix feature but does not explore how different prefix lengths or contents affect suggestion quality.
- Why unresolved: The paper only demonstrates basic usage with empty prefixes and does not investigate the impact of prefix design on suggestion relevance.
- What evidence would resolve it: Controlled experiments varying prefix lengths and contents, measuring suggestion relevance and proof completion rates.

### Open Question 3
- Question: How does LLMSTEP's suggestion quality compare to expert human-written tactics?
- Basis in paper: [explicit] The paper provides qualitative examples of LLMSTEP suggestions but does not compare them to human-written tactics.
- Why unresolved: The paper focuses on LLMSTEP's ability to generate valid tactics but does not assess how these compare to tactics written by experienced Lean users.
- What evidence would resolve it: Comparative study of LLMSTEP suggestions versus human-written tactics, measuring factors like proof elegance, efficiency, and correctness.

## Limitations
- Performance variability across different compute environments affects real-time suggestion quality
- Model generalization to novel proof domains beyond training distribution remains uncertain
- Network latency and server availability introduce potential reliability concerns

## Confidence

Major claim confidence assessment:
- Lean 4 metaprogramming integration (High): Well-documented Lean 4 features support this approach
- Language model fine-tuning methodology (Medium): Standard approach but specific hyperparameters not fully detailed
- Runtime performance measurements (Medium): Based on controlled experiments but may vary with hardware configurations

## Next Checks

1. Reproduce the 50.1% mathlib4-test completion rate using the provided fine-tuning code and evaluate on held-out proof states
2. Measure suggestion latency across CPU, GPU, and Colab environments under varying load conditions
3. Test the model's ability to generalize to proof states from domains not represented in the training data