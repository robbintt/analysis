---
ver: rpa2
title: Hierarchical Pruning of Deep Ensembles with Focal Diversity
arxiv_id: '2311.10293'
source_url: https://arxiv.org/abs/2311.10293
tags:
- ensemble
- diversity
- pruning
- focal
- hierarchical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reducing the computational
  cost of deep ensemble models without sacrificing accuracy. It proposes a novel hierarchical
  pruning approach called HQ that leverages focal diversity metrics to efficiently
  identify smaller deep ensembles with better accuracy than the entire ensemble.
---

# Hierarchical Pruning of Deep Ensembles with Focal Diversity

## Quick Facts
- arXiv ID: 2311.10293
- Source URL: https://arxiv.org/abs/2311.10293
- Reference count: 40
- One-line primary result: HQ identifies high-quality deep ensembles with up to 70% cost reduction and better accuracy than entire ensemble

## Executive Summary
This paper addresses the computational cost of deep ensemble models by proposing a hierarchical pruning approach called HQ that leverages focal diversity metrics. The method efficiently identifies smaller deep ensembles with better accuracy than the entire ensemble by measuring failure independence among member networks. HQ achieves significant cost reduction (up to 70%) on benchmark datasets like CIFAR-10 and ImageNet while maintaining or improving ensemble accuracy.

## Method Summary
HQ implements hierarchical pruning of deep ensembles using focal diversity metrics to measure failure independence among member networks. The approach involves three key techniques: computing focal diversity metrics using negative samples from prediction errors, designing a hierarchical pruning algorithm that iteratively removes low-diversity ensembles and their supersets, and applying focal diversity consensus voting to integrate multiple metrics. The method starts with computing focal diversity scores for all possible sub-ensembles, then iteratively prunes the bottom β% by diversity score while removing supersets containing pruned ensembles, and finally applies majority voting across four focal diversity metrics to select the final high-quality ensembles.

## Key Results
- HQ achieves up to 70% cost reduction on CIFAR-10 and ImageNet datasets
- Selected ensembles achieve better accuracy than the entire ensemble of 10 models
- Precision of ensemble selection exceeds 80% while maintaining high recall across benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Focal diversity metrics better capture failure independence among ensemble members than baseline diversity metrics
- Mechanism: Focal diversity uses negative samples drawn from prediction errors of a focal model to compute diversity, emphasizing complementary capacity in failure modes rather than random sampling
- Core assumption: Error complementarity correlates with ensemble accuracy gains
- Evidence anchors:
  - [abstract] "...the focal diversity metrics can accurately capture the complementary capacity of the member networks of an ensemble team..."
  - [section] "...our focal diversity metrics can precisely capture the failure independence of the member networks of a deep ensemble team..."
  - [corpus] Weak; corpus papers mention "focal diversity" but lack detailed mechanism description
- Break condition: If failure independence does not correlate with accuracy gains in a specific domain or model type

### Mechanism 2
- Claim: Hierarchical pruning improves efficiency by preemptively removing low-diversity branches in the ensemble search space
- Mechanism: The pruning algorithm iteratively builds ensembles of size S, removes bottom β% by diversity score, and prunes supersets containing any removed sub-ensemble, exploiting anti-monotonicity of diversity
- Core assumption: Low diversity in small ensembles predicts low diversity in larger supersets
- Evidence anchors:
  - [section] "Our focal diversity metrics reveal some anti-monotonicity property: a superset of a low diversity ensemble has also low diversity."
  - [section] "...our hierarchical pruning can efficiently identify high quality sub-ensembles..."
  - [corpus] Weak; hierarchical pruning concept is referenced but not explained in detail
- Break condition: If ensemble diversity is non-monotonic or highly context-dependent

### Mechanism 3
- Claim: Focal diversity consensus voting refines pruning results by integrating multiple diversity metrics
- Mechanism: Ensembles selected by majority (≥3 out of 4) focal diversity metrics are retained, leveraging complementary strengths of different metrics
- Core assumption: High-quality ensembles are consistently identified by multiple diversity metrics
- Evidence anchors:
  - [section] "An ensemble team will only be added to this final selection if it is chosen by the majority of focal diversity metrics..."
  - [section] "...consolidates these ensembles that are selected by different focal diversity metrics into a majority voting based final selection."
  - [corpus] Weak; majority voting mentioned but no evidence of improved outcomes
- Break condition: If different metrics disagree systematically or majority rule introduces bias

## Foundational Learning

- Concept: Ensemble diversity metrics and their relationship to prediction error reduction
  - Why needed here: Understanding diversity metrics is essential for implementing focal diversity and comparing to baselines
  - Quick check question: How does a pairwise diversity metric like Cohen's Kappa differ from a non-pairwise metric like Kohavi-Wolpert Variance?

- Concept: Anti-monotonicity in ensemble diversity and its implications for pruning
  - Why needed here: Hierarchical pruning relies on this property to prune supersets efficiently
  - Quick check question: Why does a low diversity sub-ensemble imply low diversity in its supersets?

- Concept: Model averaging and its effect on ensemble robustness
  - Why needed here: Theorem 2 formalizes robustness gains via diversity, which is central to the paper's theoretical contribution
  - Quick check question: What is the robustness bound for an ensemble combining predictions via model averaging?

## Architecture Onboarding

- Component map: Diversity metric calculator → Hierarchical pruning engine → Consensus voting module → Evaluation pipeline (accuracy, cost metrics)
- Critical path: Diversity computation → Pruning decision → Final ensemble selection → Performance validation
- Design tradeoffs: Focal diversity is more accurate but computationally heavier than baseline metrics; hierarchical pruning trades some recall for precision and efficiency
- Failure signatures: Low precision indicates overly aggressive pruning; low recall suggests insufficient diversity coverage; high diversity but low accuracy may indicate overfitting
- First 3 experiments:
  1. Run focal diversity metrics on a small ensemble (e.g., CIFAR-10 with 5 models) and compare to baseline metrics
  2. Apply hierarchical pruning with β=10%, S_d=5 and observe pruning rate and accuracy retention
  3. Test focal diversity consensus voting by comparing single-metric vs. majority selection results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the HQ algorithm's performance scale when applied to deep ensembles with more than 20 member networks?
- Basis in paper: [inferred] The paper mentions the exponential growth in the number of possible sub-ensembles with increasing ensemble size (e.g., 1048554 possible sub-ensembles for M=20), suggesting scalability concerns
- Why unresolved: The paper only evaluates the HQ algorithm on ensembles of size 10 and does not provide results for larger ensembles, leaving uncertainty about its performance in real-world scenarios with potentially much larger ensembles
- What evidence would resolve it: Empirical results showing the precision, recall, and cost reduction achieved by HQ when applied to deep ensembles with 50 or more member networks

### Open Question 2
- Question: What is the optimal trade-off between the desired team size (S_d) and the cost reduction achieved by the HQ algorithm?
- Basis in paper: [inferred] The paper suggests setting S_d up to half the size of the entire ensemble, but does not provide a systematic analysis of how the choice of S_d impacts the balance between cost reduction and ensemble accuracy
- Why unresolved: The paper only evaluates the HQ algorithm with a fixed S_d for each dataset, without exploring the full range of possible values or their impact on the ensemble's performance
- What evidence would resolve it: A comprehensive study varying S_d across a wide range of values and measuring the resulting cost reduction and ensemble accuracy for different datasets

### Open Question 3
- Question: How does the HQ algorithm's performance compare to other state-of-the-art ensemble pruning techniques when applied to deep neural networks?
- Basis in paper: [explicit] The paper compares HQ to baseline ensemble pruning methods but does not compare it to other advanced techniques specifically designed for deep ensembles, such as those mentioned in the related work section
- Why unresolved: The paper focuses on demonstrating the effectiveness of HQ compared to simple baseline methods but does not provide a comprehensive comparison with more sophisticated ensemble pruning techniques in the literature
- What evidence would resolve it: A direct comparison of HQ's performance with other state-of-the-art ensemble pruning methods for deep neural networks, using the same datasets and evaluation metrics

## Limitations
- Performance depends on specific implementations of negative sampling and pairwise correlation calculations that are not fully specified
- Empirical validation limited to standard image classification tasks, leaving questions about generalization to other domains
- Sensitivity to hyperparameters (β, S_d) may require dataset-specific tuning for optimal performance

## Confidence
- **High confidence**: The theoretical framework for focal diversity metrics and their relationship to failure independence (Mechanism 1)
- **Medium confidence**: The effectiveness of hierarchical pruning in reducing computational cost while maintaining accuracy (Mechanism 2)
- **Medium confidence**: The benefit of focal diversity consensus voting in refining ensemble selection (Mechanism 3)

## Next Checks
1. **Sensitivity Analysis**: Systematically vary β and S_d parameters across multiple datasets to establish robustness boundaries and identify optimal hyperparameter ranges

2. **Domain Transferability**: Apply HQ to non-image datasets (e.g., text classification or tabular data) to test generalizability beyond computer vision tasks

3. **Computational Overhead Measurement**: Quantify the actual runtime overhead of focal diversity computation versus baseline metrics to validate the claimed efficiency improvements