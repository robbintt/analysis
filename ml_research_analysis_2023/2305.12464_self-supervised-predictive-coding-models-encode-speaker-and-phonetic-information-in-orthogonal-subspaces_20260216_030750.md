---
ver: rpa2
title: Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information
  in Orthogonal Subspaces
arxiv_id: '2305.12464'
source_url: https://arxiv.org/abs/2305.12464
tags:
- speaker
- phone
- information
- speech
- speakers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how speaker and phonetic information are encoded
  in the high-dimensional representation spaces of self-supervised speech models.
  It shows that predictive coding models (APC and CPC) implicitly disentangle speaker
  and phonetic information into nearly orthogonal subspaces.
---

# Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces

## Quick Facts
- arXiv ID: 2305.12464
- Source URL: https://arxiv.org/abs/2305.12464
- Reference count: 0
- Primary result: Predictive coding models (APC/CPC) encode speaker and phonetic information in nearly orthogonal subspaces, enabling simple speaker normalization

## Executive Summary
This paper analyzes how self-supervised speech models encode speaker and phonetic information in their high-dimensional representation spaces. Using PCA analysis, the authors discover that predictive coding models (APC and CPC) implicitly disentangle speaker and phonetic information into nearly orthogonal subspaces. This property enables a simple speaker normalization method that removes speaker information by projecting representations onto the subspace orthogonal to the speaker subspace, without requiring transcriptions. The method improves phone discrimination on ABX tasks and generalizes to unseen speakers.

## Method Summary
The authors analyze frame-level representations from APC and CPC models by aggregating them separately by speaker (Mspk) and by phone (Mphn). They apply PCA to both matrices to identify principal directions for speaker and phonetic variation. The orthogonality between these subspaces is verified by computing dot product similarities between principal directions. For speaker normalization, they collapse the speaker subspace by projecting representations onto the orthogonal complement of the speaker subspace. The method is evaluated by measuring speaker removal effectiveness and phone discrimination performance using ABX tests.

## Key Results
- Speaker and phonetic subspaces in APC/CPC representations have average dot product similarity of only 0.13, indicating near orthogonality
- Collapsing 40 dimensions of the speaker subspace removes speaker information while improving phone discrimination on ABX tasks
- The speaker subspace generalizes to unseen speakers, enabling fully streaming application without requiring speaker labels from the test set

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PCA identifies speaker and phonetic information in nearly orthogonal subspaces of the representation space
- Mechanism: Aggregating frame-level representations by speaker and by phone separately, then applying PCA, reveals that principal components for speakers and phones have low dot product similarity (average ~0.13), indicating near orthogonality
- Core assumption: Speaker and phonetic information are independent sources of variation in speech signals, so a good representation should disentangle them
- Evidence anchors:
  - [abstract] "we identify two subspaces that capture speaker and phonetic variances, and confirm that they are nearly orthogonal"
  - [section] "When comparing the principal directions of Mphn and Mspk, we found very low similarity: amongst the top 20 speaker directions, their similarity with the most aligned phone direction is on average only 0.13"
  - [corpus] Found related papers analyzing orthogonality in self-supervised speech representations, supporting this mechanism is a known property
- Break condition: If speaker and phone information are not independent (e.g., speaker affects phonetic realization systematically), orthogonality would fail

### Mechanism 2
- Claim: Collapsing the speaker subspace removes speaker information without harming phonetic discrimination
- Mechanism: For a frame representation z and speaker principal direction v, projecting z onto the subspace orthogonal to v by computing z' = z - (zâŠ¤v)v removes speaker variance while preserving phonetic information encoded in orthogonal directions
- Core assumption: Phonetic information is encoded in directions nearly orthogonal to speaker directions, so removing speaker directions doesn't remove phonetic information
- Evidence anchors:
  - [abstract] "Based on this property, we propose a new speaker normalization method which collapses the subspace that encodes speaker information"
  - [section] "projecting frames to the subspace orthogonal to the speaker subspace would remove speaker information without affecting how well phonetic information can be extracted"
  - [corpus] Related work shows speaker removal techniques can improve phonetic tasks, supporting this mechanism
- Break condition: If phonetic information leaks into speaker dimensions (low dimensional embeddings) or if phone classes are highly correlated with speaker identity

### Mechanism 3
- Claim: Speaker subspace learned from training speakers generalizes to unseen speakers
- Mechanism: Principal components capturing speaker variance are stable across speaker populations, so subspace learned on training set speakers can remove speaker information from test set speakers
- Core assumption: Speaker variation has consistent statistical properties across different speaker populations
- Evidence anchors:
  - [abstract] "The approach generalizes and can be used to remove information of unseen speakers"
  - [section] "We use a speaker subspace learned from a different set of speakers for collapsing" and test generalization
  - [corpus] Found papers on speaker information identification in SSL models, but evidence for cross-speaker generalization is limited
- Break condition: If test speakers have systematic acoustic differences from training speakers (e.g., different accent, recording conditions)

## Foundational Learning

- Concept: Principal Component Analysis (PCA)
  - Why needed here: PCA is used to identify directions of maximum variance in aggregated speaker and phone representations, revealing the orthogonal subspaces
  - Quick check question: What does the first principal component of Mspk represent, and why is it important for speaker normalization?

- Concept: Self-supervised speech representation learning
  - Why needed here: Understanding how predictive coding models learn representations by predicting future frames helps explain why they might disentangle speaker and phonetic information
  - Quick check question: How does the predictive coding objective (predicting future frames) differ from masked prediction objectives, and what implications does this have for speaker/phonetic encoding?

- Concept: ABX discrimination task
  - Why needed here: ABX tests are used to measure phonetic discriminability before and after speaker normalization, providing the key evaluation metric
  - Quick check question: In an ABX test, what is the difference between within-speaker and across-speaker settings, and why is this distinction important for evaluating speaker normalization?

## Architecture Onboarding

- Component map: Data aggregation -> PCA computation -> Similarity analysis -> Normalization -> Evaluation

- Critical path:
  1. Extract frame-level representations from SSL model
  2. Aggregate by speaker and phone to create Mspk and Mphn
  3. Apply PCA to identify speaker and phone subspaces
  4. Verify orthogonality through similarity analysis
  5. Collapse speaker subspace using projection
  6. Evaluate impact on speaker removal and phone discrimination

- Design tradeoffs:
  - Number of PCA components to collapse: Too few leaves speaker information; too many harms phonetic information
  - Aggregation method: Averaging frames vs. using all frames affects noise and computational cost
  - Baseline choice: Utterance-level standardization is simple but may not generalize to unseen speakers

- Failure signatures:
  - High similarity between speaker and phone principal directions (>0.5) indicates poor disentanglement
  - ABX scores decrease after speaker normalization suggests phonetic information was lost
  - Speaker error rates remain high after normalization indicates incomplete speaker removal

- First 3 experiments:
  1. Verify orthogonality by computing dot products between top 20 principal directions of Mspk and Mphn
  2. Test speaker normalization by collapsing 1, 5, 10, 20, 40 dimensions and measuring ABX scores
  3. Test generalization by learning speaker subspace on train-clean-100 and applying to dev-clean speakers

## Open Questions the Paper Calls Out

- Question: How well does the orthogonality between speaker and phonetic subspaces generalize to self-supervised models trained with different objectives, such as masked prediction models like HuBERT or wav2vec 2.0?
  - Basis in paper: [explicit] The authors state they plan to explore whether orthogonality extends to other SSL models based on different principles (e.g., masked prediction [14, 15]).
  - Why unresolved: The current analysis is limited to predictive coding models (APC and CPC), and the paper explicitly notes this as future work.
  - What evidence would resolve it: Empirical analysis of speaker and phonetic subspaces in masked prediction models using the same PCA-based methodology.

- Question: Does the speaker subspace learned on English LibriSpeech generalize to speech from other languages or different speaking styles/genres?
  - Basis in paper: [inferred] The authors suggest investigating whether speaker dimensions generalize to out-of-domain data (e.g., other languages or genres of speech) as future work.
  - Why unresolved: The current experiments only use English read speech from LibriSpeech, limiting generalizability.
  - What evidence would resolve it: Applying the speaker subspace normalization to multilingual or conversational speech corpora and measuring phone discrimination performance.

- Question: What is the cognitive significance of the orthogonality between speaker and phonetic information in SSL representations, and how does it compare to human speech processing?
  - Basis in paper: [explicit] The authors note that analyzing information distribution can "shed light on properties of the representation space that can be compared against neural representations or behaviour data of humans to evaluate the scientific value of the model."
  - Why unresolved: The paper analyzes the orthogonality property but does not compare it to human neural or behavioral data.
  - What evidence would resolve it: Neuroimaging studies comparing speaker-phonetic encoding patterns in SSL models versus human auditory cortex, or behavioral experiments on human phonetic discrimination under varying speaker conditions.

## Limitations
- Analysis relies on PCA-based subspace identification assuming linear relationships
- Method may not generalize to transformer-based models or contrastive loss training
- Evaluation limited to LibriSpeech may not hold for diverse speech corpora
- Speaker generalization tested only on clean speech, not noisy or accented speech

## Confidence
- High confidence: Speaker and phonetic subspaces exhibit near-orthogonality (averaged 0.13 similarity) - this is empirically verified and directly measurable
- Medium confidence: Speaker subspace learned from training speakers generalizes to unseen speakers - tested but on limited conditions
- Medium confidence: Collapsing 40 dimensions optimally removes speaker information while preserving phonetic discrimination - the "optimal" threshold is dataset-dependent and may vary

## Next Checks
1. Test orthogonality on diverse speech datasets (non-LibriSpeech) and with transformer-based SSL models to verify generalizability
2. Measure the impact of varying the number of collapsed dimensions across different speaker populations (age, accent, recording conditions)
3. Evaluate whether the orthogonality property holds for downstream tasks beyond ABX discrimination (e.g., speech recognition, speaker verification)