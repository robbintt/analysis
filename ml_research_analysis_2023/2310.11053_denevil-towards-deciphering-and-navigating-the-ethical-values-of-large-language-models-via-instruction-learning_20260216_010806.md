---
ver: rpa2
title: 'Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language
  Models via Instruction Learning'
arxiv_id: '2310.11053'
source_url: https://arxiv.org/abs/2310.11053
tags:
- value
- llms
- moral
- prompt
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Denevil, a novel prompt generation algorithm
  to assess the ethical values of LLMs in a generative manner. The authors benchmark
  27 mainstream LLMs using their constructed MoralPrompt dataset, revealing substantial
  misalignments across diverse architectures and model sizes.
---

# Denevil: Towards Deciphering and Navigating the Ethical Values of Large Language Models via Instruction Learning

## Quick Facts
- arXiv ID: 2310.11053
- Source URL: https://arxiv.org/abs/2310.11053
- Reference count: 40
- Authors: Multiple researchers from Peking University and Microsoft Research Asia
- Primary result: Proposed DeNEVIL prompt generation algorithm and VILMO alignment method to evaluate and improve LLM value alignment, revealing substantial misalignments across 27 mainstream models

## Executive Summary
This paper addresses the critical challenge of evaluating and aligning the ethical values of large language models (LLMs). The authors propose DeNEVIL, a novel prompt generation algorithm that dynamically creates value-violating prompts to reveal ethical vulnerabilities in LLMs. They construct the MoralPrompt dataset with 2,397 prompts covering over 500 value principles and benchmark 27 mainstream LLMs, revealing substantial misalignments. To improve value compliance, they introduce VILMO, an in-context alignment method that learns to generate appropriate value instructions, substantially enhancing LLM outputs' value conformity while maintaining generation quality.

## Method Summary
The paper proposes a two-pronged approach to LLM value alignment. First, DeNEVIL uses Variational Expectation Maximization to iteratively generate prompts that elicit value violations from LLMs, creating the MoralPrompt dataset for evaluation. Second, VILMO employs model-based black-box optimization to train an instruction generator that produces context-appropriate value instructions for in-context alignment. The evaluation framework uses three metrics (EVR, MVP, APV) to quantify value violations, while VILMO optimizes for value conformity without requiring gradient access to target LLMs.

## Key Results
- Benchmarked 27 mainstream LLMs using MoralPrompt dataset, revealing substantial value misalignments across diverse architectures and model sizes
- VILMO outperforms baselines in enhancing value conformity of LLM outputs while retaining generation quality and efficiency
- DeNEVIL effectively generates value-violating prompts that expose hidden ethical vulnerabilities in LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DeNEVIL dynamically generates value-violating prompts that reveal hidden ethical vulnerabilities in LLMs by iteratively refining scenarios to maximize value violations.
- **Mechanism**: The algorithm uses Variational Expectation Maximization to alternate between generating completions that violate a specified value (E-step) and refining prompts to increase the probability of producing such completions (M-step).
- **Core assumption**: LLMs will generate more realistic and context-rich violations when prompted with specific scenarios rather than abstract questions, and these violations can be reliably detected using a classifier.
- **Evidence anchors**: [abstract] "DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities"; [section] "Unlike conventional discriminative evaluation, DeNEVIL probes the ethical vulnerabilities in each model and then dynamically creates and refines test prompts"
- **Break condition**: If the classifier cannot reliably detect value violations, or if LLMs refuse to generate content even with provocative prompts due to safety filters.

### Mechanism 2
- **Claim**: VILMO improves LLM value conformity by learning to generate context-appropriate value instructions through model-based black-box optimization.
- **Mechanism**: VILMO trains a separate model to generate value instructions conditioned on prompts, optimizing for value conformity without requiring gradients from the target LLM.
- **Core assumption**: Value instructions that are more specific to the context will be more effective at guiding LLMs toward value-compliant outputs than generic instructions.
- **Evidence anchors**: [abstract] "VILMO, an in-context alignment method that substantially enhances the value compliance of LLM outputs by learning to generate appropriate value instructions"; [section] "We define such an instruction generator as pφ(c|x) parameterized by φ. Then, we aim to optimize φ to maximize the LLM's conformity"
- **Break condition**: If the instruction generator overfits to the training data or if the target LLM ignores or misinterprets the generated instructions.

### Mechanism 3
- **Claim**: The MoralPrompt dataset constructed using DeNEVIL provides more reliable evaluation of LLM values than traditional discriminative benchmarks because it measures actual generation behavior rather than knowledge.
- **Mechanism**: By generating prompts dynamically rather than using static benchmarks, the dataset avoids issues of data contamination and tests whether LLMs actually produce value-compliant text in realistic scenarios.
- **Core assumption**: LLMs that know the "right" answer to moral questions may still generate unethical content when asked to complete open-ended prompts, making generative evaluation more faithful.
- **Evidence anchors**: [abstract] "Moving beyond conventional discriminative evaluations with poor reliability, we propose DeNEVIL, a novel prompt generation algorithm tailored to dynamically exploit LLMs' value vulnerabilities"; [section] "Our findings reveal substantial misalignments than anticipated, underscoring the need for ethical value alignment"
- **Break condition**: If the generated prompts are not representative of real-world scenarios or if the classifier used to evaluate violations is not reliable.

## Foundational Learning

- **Concept**: Variational Expectation Maximization (EM) algorithm
  - Why needed here: DeNEVIL uses EM to iteratively refine prompts by alternating between generating value-violating completions and updating prompts to maximize violation probability
  - Quick check question: Can you explain how the E-step and M-step work together in DeNEVIL's context?

- **Concept**: In-context learning and instruction tuning
  - Why needed here: VILMO relies on LLMs' ability to follow instructions and adapt behavior based on context, which is fundamental to both the baseline methods and the proposed approach
  - Quick check question: What's the difference between instruction tuning and reinforcement learning from human feedback (RLHF)?

- **Concept**: Classifier-based evaluation of generated text
  - Why needed here: Both DeNEVIL and the evaluation metrics rely on classifiers that can detect whether generated text violates specific value principles
  - Quick check question: How might classifier bias affect the evaluation of LLM value conformity?

## Architecture Onboarding

- **Component map**: DeNEVIL (prompt generation) → MoralPrompt (evaluation dataset) → LLM generation → Classifier evaluation; VILMO training → Instruction generation → LLM output evaluation
- **Critical path**: For evaluation: DeNEVIL → MoralPrompt → LLM generation → Classifier evaluation. For alignment: VILMO training → Instruction generation → LLM output evaluation
- **Design tradeoffs**: DeNEVIL trades computational cost for more realistic evaluation scenarios. VILMO trades model complexity for alignment effectiveness. The dataset construction trades coverage for quality by using iterative refinement.
- **Failure signatures**: If DeNEVIL fails, you'll see convergence to trivial prompts or inability to generate violations. If VILMO fails, you'll see no improvement in value conformity despite instruction generation. If classifiers fail, evaluation metrics will be unreliable.
- **First 3 experiments**:
  1. Run DeNEVIL on a small set of principles with a single LLM to verify prompt generation works
  2. Evaluate a known LLM on MoralPrompt to establish baseline value conformity
  3. Train VILMO on a subset of MoralPrompt and evaluate improvement on held-out prompts

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What are the exact failure rates of DeNEVIL when applied to models with different instruction-following capabilities?
- **Basis in paper**: [explicit] The paper mentions that DeNEVIL works better with LLMs possessing stronger instruction-following abilities, and that LLaMA-30B only experiences a slight enhancement when the number of iterations grows.
- **Why unresolved**: The paper does not provide quantitative data on the failure rates of DeNEVIL for models with varying instruction-following capabilities.
- **What evidence would resolve it**: Detailed experiments comparing the performance of DeNEVIL on a range of models with different instruction-following capabilities, including quantitative measures of failure rates.

### Open Question 2
- **Question**: How does the performance of VILMO compare to other in-context alignment methods on models with varying sizes and architectures?
- **Basis in paper**: [explicit] The paper states that VILMO outperforms baselines in enhancing value conformity of LLM outputs while retaining generation quality and efficiency.
- **Why unresolved**: The paper does not provide a comprehensive comparison of VILMO's performance against other in-context alignment methods across different model sizes and architectures.
- **What evidence would resolve it**: Experiments comparing VILMO's performance to other in-context alignment methods on a diverse set of models with varying sizes and architectures, using metrics such as value conformity, generation quality, and efficiency.

### Open Question 3
- **Question**: What are the long-term effects of using VILMO for value alignment on LLMs?
- **Basis in paper**: [inferred] The paper mentions that VILMO learns to generate appropriate value instructions to enhance value compliance of LLM outputs.
- **Why unresolved**: The paper does not discuss the potential long-term effects of using VILMO for value alignment, such as model drift or degradation of generation quality over time.
- **What evidence would resolve it**: Long-term studies tracking the performance of LLMs aligned using VILMO, including measures of value conformity, generation quality, and any potential negative effects such as model drift or degradation.

## Limitations

- The effectiveness of DeNEVIL heavily depends on the quality and reliability of the value violation classifier, which lacks extensive validation in the paper
- The iterative refinement process may converge to local optima that don't represent realistic ethical dilemmas
- The evaluation metrics (EVR, MVP, APV) are novel and their correlation with real-world value alignment is unclear

## Confidence

- **High confidence**: The observation that LLMs exhibit substantial misalignments across different architectures and sizes is well-supported by comprehensive benchmarking of 27 mainstream models
- **Medium confidence**: The effectiveness of VILMO in improving value conformity is demonstrated through experiments, but long-term stability and generalizability need further validation
- **Low confidence**: The claim that DeNEVIL generates more realistic and effective evaluation prompts than traditional benchmarks lacks direct comparison with established evaluation methods

## Next Checks

1. Conduct a human evaluation study to validate the quality and relevance of prompts generated by DeNEVIL compared to manually crafted ethical evaluation prompts
2. Perform ablation studies on the VILMO method by varying the quality and specificity of value instructions to determine their impact on alignment effectiveness
3. Test the robustness of the evaluation framework by introducing controlled value violations in the generated completions and measuring classifier sensitivity