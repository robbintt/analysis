---
ver: rpa2
title: 'PaSS: Parallel Speculative Sampling'
arxiv_id: '2311.13581'
source_url: https://arxiv.org/abs/2311.13581
tags:
- tokens
- pass
- parallel
- sampling
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Parallel Speculative Sampling (PaSS), a method
  to accelerate inference of large language models without requiring a second model.
  PaSS leverages parallel decoding via learned "look-ahead embeddings" that allow
  a single model to generate multiple candidate tokens simultaneously.
---

# PaSS: Parallel Speculative Sampling

## Quick Facts
- arXiv ID: 2311.13581
- Source URL: https://arxiv.org/abs/2311.13581
- Authors: Ziniu Hu, Ruiqi Gao, Peter J. Liu, Ying Nian Wu, Song-Chun Zhu
- Reference count: 5
- Primary result: Achieves up to 30% speed-up in generation time while maintaining quality

## Executive Summary
This paper introduces Parallel Speculative Sampling (PaSS), a method to accelerate inference of large language models without requiring a second model. PaSS leverages parallel decoding via learned "look-ahead embeddings" that allow a single model to generate multiple candidate tokens simultaneously. These embeddings require only O(d_emb) additional parameters, a negligible overhead compared to full second models used in prior speculative sampling approaches. Experiments show PaSS achieves significant speed-up while maintaining the same quality as autoregressive decoding, as measured by standard benchmarks.

## Method Summary
PaSS inserts additional tokens at the end of the input sequence that guide the model to generate future tokens in parallel. These look-ahead embeddings require only O(d_emb) additional parameters. The method uses a rejection scheme to ensure that the distribution of drafted tokens matches the target model's distribution. Training involves freezing all model parameters except look-ahead embeddings, which are trained on a small dataset. Inference uses a two-phase iterative processâ€”drafting (parallel generation using look-ahead embeddings) then validation (rejection sampling).

## Key Results
- Up to 30% speed-up in generation time while maintaining quality
- Negligible memory overhead (O(d_emb) parameters vs full second model)
- Maintains same quality as autoregressive decoding (PASS@1: 0.320 vs 0.326, PASS@10: 0.664 vs 0.663 on HumanEval)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parallel Speculative Sampling accelerates LLM inference by leveraging memory-bound operations
- Mechanism: Generates multiple tokens simultaneously using look-ahead embeddings and validates them in a single forward pass
- Core assumption: Memory bandwidth, not computation, is the bottleneck for autoregressive generation
- Evidence anchors: Abstract mentions parallel execution takes nearly same time as single token; section 2.2 discusses forward pass additions
- Break condition: If parallel decoding overhead outweighs gains or memory-bound assumption doesn't hold

### Mechanism 2
- Claim: Look-ahead embeddings enable parallel generation without second model
- Mechanism: Additional tokens guide model to generate future tokens in parallel with O(d_emb) parameters
- Core assumption: Look-ahead embeddings can effectively guide parallel generation while maintaining quality
- Evidence anchors: Abstract describes inserting tokens at input sequence end; section 2.2.1 discusses memory overhead
- Break condition: If embeddings fail to capture sufficient information, leading to degraded quality or frequent rejections

### Mechanism 3
- Claim: Rejection scheme preserves generation quality
- Mechanism: Each drafted token accepted/rejected based on probability comparison with target model
- Core assumption: Rejection scheme can filter out tokens deviating from target distribution
- Evidence anchors: Abstract and section 2.1 reference Chen et al. [2023] and Leviathan et al. [2023] for rejection guarantees
- Break condition: If rejection probability is too high or introduces bias in generated sequence

## Foundational Learning

- Concept: Speculative Sampling
  - Why needed here: PaSS builds upon speculative sampling by replacing second draft model with parallel decoding
  - Quick check question: How does speculative sampling use a second model to accelerate inference, and what are its limitations?

- Concept: Parallel Decoding
  - Why needed here: PaSS leverages parallel decoding to generate multiple tokens simultaneously
  - Quick check question: What are the challenges of parallel decoding in autoregressive models, and how do look-ahead embeddings address them?

- Concept: Rejection Sampling
  - Why needed here: Rejection scheme ensures drafted tokens match target model's distribution
  - Quick check question: How does rejection scheme work in speculative sampling, and why is it crucial for maintaining quality?

## Architecture Onboarding

- Component map: Target LLM -> Look-ahead embeddings -> Sampling mechanism -> Rejection scheme
- Critical path: 1) Generate one token auto-regressively, 2) Generate draft tokens in parallel using look-ahead embeddings, 3) Validate draft tokens using rejection scheme, 4) Accept/reject tokens and sample replacements as needed
- Design tradeoffs: Number of look-ahead tokens vs speed-up (more tokens can lead to greater speed-up but increase rejection likelihood); temperature in sampling (lower temperatures lead to more predictable drafts but may reduce diversity)
- Failure signatures: High rejection rate (indicates ineffective look-ahead embeddings), minimal speed-up (overhead outweighs benefits), degraded generation quality (rejection scheme not filtering properly)
- First 3 experiments: 1) Measure acceptance rate of drafted tokens for different numbers of look-ahead tokens, 2) Compare generation speed/quality with auto-regressive generation for different sampling temperatures, 3) Evaluate impact of look-ahead embeddings on quality by comparing with baseline using [UNK] as look-ahead token

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of look-ahead tokens for different model sizes and tasks?
- Basis in paper: Explicitly tested different numbers (2, 4, 6, 8) on The Stack data with performance decreasing after 6 tokens; authors want to explore improving parallel generation quality
- Why unresolved: Only tested limited range, didn't systematically explore scaling with model size or across tasks
- What evidence would resolve it: Comprehensive experiments varying look-ahead token count across different model sizes and tasks, measuring speed-up and quality trade-offs

### Open Question 2
- Question: How does the training process for look-ahead embeddings affect their quality and overall performance?
- Basis in paper: Explicitly mentions simple training approach but doesn't explore different training strategies
- Why unresolved: Doesn't investigate how different training objectives, data selection, or optimization techniques impact embedding effectiveness
- What evidence would resolve it: Experiments comparing different training objectives, data selection strategies, and optimization techniques on embedding quality and generation performance

### Open Question 3
- Question: How does PaSS compare to speculative sampling with smaller draft model in terms of memory efficiency and speed-up for very large language models?
- Basis in paper: Explicitly mentions O(d_emb) parameter overhead vs full second model, but doesn't compare to speculative sampling with smaller models on large-scale models
- Why unresolved: Focuses on 7B parameter model, doesn't explore scaling properties vs speculative sampling for larger models where memory constraints are critical
- What evidence would resolve it: Comparative experiments measuring memory usage and inference speed for PaSS vs speculative sampling with smaller draft models on 70B+ parameter models

## Limitations
- Hardware dependency: Speed-up heavily relies on memory-bandwidth bottleneck assumption that may not hold across different hardware configurations
- Quality preservation evidence gap: Narrow margins on HumanEval don't guarantee consistent quality across diverse generation tasks
- Parameter efficiency verification: O(d_emb) overhead claimed but not empirically validated against alternatives or different model sizes

## Confidence

**High Confidence**: Core mechanism of using look-ahead embeddings for parallel decoding is well-defined and theoretically sound; rejection sampling builds on established speculative sampling literature

**Medium Confidence**: 30% speed-up claim is promising but hardware-dependent; experimental setup lacks transparency about specific hardware used

**Low Confidence**: "Lossless" quality preservation claim requires more rigorous validation; narrow margin on HumanEval doesn't guarantee consistent quality across diverse tasks

## Next Checks

1. **Hardware Dependency Test**: Implement PaSS across different GPU architectures (A100 vs H100) and memory configurations to quantify how speed-up varies with hardware; measure memory bandwidth utilization during parallel decoding

2. **Quality Preservation Stress Test**: Evaluate PaSS on diverse benchmarks beyond HumanEval, including linguistic quality metrics, human evaluations for coherence and factual accuracy, and task-specific benchmarks; compare failure modes when rejection rates increase

3. **Parameter Scaling Analysis**: Experiment with varying look-ahead embedding dimensions relative to model size to identify optimal scaling relationships; compare parameter efficiency against speculative sampling methods using small draft models across different parameter budgets