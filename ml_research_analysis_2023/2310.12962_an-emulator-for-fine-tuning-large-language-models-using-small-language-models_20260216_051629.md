---
ver: rpa2
title: An Emulator for Fine-Tuning Large Language Models using Small Language Models
arxiv_id: '2310.12962'
source_url: https://arxiv.org/abs/2310.12962
tags:
- fine-tuning
- large
- small
- language
- fine-tuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces emulated fine-tuning (EFT), a technique that
  enables studying the independent effects of pre-training and fine-tuning at different
  model scales. The method factorizes a fine-tuned model's logits into base log probabilities
  from a pre-trained model and a "behavior delta" representing the changes from fine-tuning.
---

# An Emulator for Fine-Tuning Large Language Models using Small Language Models

## Quick Facts
- arXiv ID: 2310.12962
- Source URL: https://arxiv.org/abs/2310.12962
- Authors: 
- Reference count: 16
- Primary result: EFT enables studying independent effects of pre-training and fine-tuning at different scales by factorizing fine-tuned model logits into base log probabilities and a behavior delta.

## Executive Summary
This paper introduces emulated fine-tuning (EFT), a technique that enables studying the independent effects of pre-training and fine-tuning at different model scales. The method factorizes a fine-tuned model's logits into base log probabilities from a pre-trained model and a "behavior delta" representing the changes from fine-tuning. This allows decoupling the knowledge gained from pre-training from the skills gained from fine-tuning. The authors show that scaling up pre-training improves factuality while scaling up fine-tuning improves helpfulness in language models. They also demonstrate that EFT enables test-time adjustment of competing behavioral traits like helpfulness and harmlessness without additional training.

## Method Summary
The paper introduces emulated fine-tuning (EFT), which factorizes a fine-tuned model's logits into base log probabilities from a pre-trained model and a "behavior delta" representing the changes from fine-tuning. This factorization is based on viewing fine-tuning as KL-constrained reinforcement learning, where the implicit reward function can be computed. The authors demonstrate up-scaling, a special case where a small fine-tuned model is combined with a large pre-trained model to approximate the result of fine-tuning the large model directly. They evaluate this approach across three model families (Llama-1, Llama-2, Falcon) at different scales using two datasets (dialogue and question-answering) and measure factuality, helpfulness, and harmlessness.

## Key Results
- Scaling up pre-training improves factuality while scaling up fine-tuning improves helpfulness in language models
- EFT up-scaling consistently improves helpfulness and factuality across Llama, Llama-2, and Falcon model families
- EFT enables test-time adjustment of competing behavioral traits like helpfulness and harmlessness through reward function interpolation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EFT factorizes fine-tuned model logits into base log probabilities and a behavior delta, enabling independent scaling of pre-training and fine-tuning.
- Mechanism: The fine-tuned model's logits are decomposed as πft(y|x) = πref(y|x) × exp(rπft(x,y)), where rπft is the implicit reward function. This allows combining knowledge from a large pre-trained model with skills from a small fine-tuned model by computing πN_M(y|x) ∝ πN_ref(y|x) × πM(y|x)/πM_ref(y|x).
- Core assumption: The KL-constrained RL formulation of fine-tuning is valid and the implicit reward function can be computed for models of different scales.
- Evidence anchors:
  - [abstract] "EFT factorizes a fine-tuned model's logits into base log probabilities from a pre-trained model and a 'behavior delta' representing the changes from fine-tuning"
  - [section 3.1] "The solution is given by π*(r, πref)(y|x) = 1/Z(x) × πref(y|x) × exp(r(x,y)/β)"
  - [corpus] Weak evidence - the paper provides theoretical justification but no empirical validation of the factorization working across scales
- Break condition: The factorization breaks down if the implicit reward function is not well-defined across different model scales or if the KL constraint assumption fails.

### Mechanism 2
- Claim: Up-scaling with EFT approximates the result of fine-tuning a large model without the computational expense.
- Mechanism: By combining a large pre-trained model with a small fine-tuned model using EFT, the resulting model captures both the factual knowledge from the large pre-training and the helpfulness skills from the small fine-tuning. This is more efficient than fine-tuning the large model directly.
- Core assumption: The small fine-tuned model's behavioral changes are transferable to the large pre-trained model through the EFT framework.
- Evidence anchors:
  - [abstract] "A special case called 'up-scaling' approximates the result of fine-tuning a large model by combining a small fine-tuned model with a large pre-trained model"
  - [section 3.3] "Up-scaling consistently improves helpfulness and factuality of instruction-following models in the Llama, Llama-2, and Falcon families"
  - [corpus] Moderate evidence - the paper shows consistent improvements across multiple model families but doesn't prove it's equivalent to actual fine-tuning
- Break condition: Up-scaling fails if the small fine-tuned model's behavioral changes are not representative of what would be learned from fine-tuning the large model directly.

### Mechanism 3
- Claim: EFT enables test-time adjustment of behavioral traits by interpolating between different implicit reward functions.
- Mechanism: By linearly interpolating between the reward functions of two fine-tuned models (e.g., helpful vs harmless), EFT can generate responses with mixed behavioral traits without additional training.
- Core assumption: The reward functions of different fine-tuned models can be meaningfully combined through linear interpolation.
- Evidence anchors:
  - [abstract] "EFT enables test-time adjustment of competing behavioral traits like helpfulness and harmlessness without additional training"
  - [section 4.2] "We use EFT to interpolate between two implicit rewards for helpfulness and harmlessness and plot GPT-4-evaluated helpfulness and fraction of responses that are harmful"
  - [corpus] Weak evidence - the paper demonstrates the concept but doesn't validate the quality of interpolated behaviors
- Break condition: The interpolation breaks down if the reward functions are not compatible or if the linear combination doesn't produce meaningful behavioral changes.

## Foundational Learning

- Concept: Reinforcement Learning with KL-divergence constraint
  - Why needed here: EFT is based on viewing fine-tuning as KL-constrained RL, which provides the theoretical foundation for the factorization of logits
  - Quick check question: What is the mathematical form of the optimal policy in KL-constrained RL and how does it relate to the EFT factorization?

- Concept: Log probability algebra and importance weighting
  - Why needed here: EFT relies on manipulating log probabilities and reweighting them using the ratio of conditionals from different models
  - Quick check question: How does the EFT up-scaling formula πN_M(y|x) ∝ πN_ref(y|x) × πM(y|x)/πM_ref(y|x) work mathematically?

- Concept: Speculative decoding
  - Why needed here: EFT up-scaling can be accelerated using speculative decoding, which is crucial for practical implementation
  - Quick check question: How does speculative decoding work with EFT up-scaling and why is it more effective than with regular fine-tuned models?

## Architecture Onboarding

- Component map:
  Pre-trained models (various scales) -> Fine-tuned models (various scales) -> EFT sampling engine -> Reward function calculator -> Speculative decoding module

- Critical path:
  1. Load base model and fine-tuned model
  2. Compute implicit reward function
  3. Apply EFT factorization to logits
  4. Sample from reweighted distribution
  5. (Optional) Apply speculative decoding for acceleration

- Design tradeoffs:
  - Accuracy vs efficiency: EFT up-scaling trades some accuracy for significant computational savings
  - Model size vs behavior quality: Larger base models provide better factual knowledge but may be less helpful
  - Interpolation vs specialization: Mixing behaviors allows flexibility but may produce less optimal results for specific tasks

- Failure signatures:
  - Poor factuality: The up-scaled model produces factually incorrect responses
  - Reduced helpfulness: The up-scaled model fails to follow user intent as well as the small fine-tuned model
  - Inconsistent behavior: The model's responses vary significantly across similar prompts

- First 3 experiments:
  1. Implement EFT up-scaling with Llama-2-7B-chat and Llama-2-70B-base, evaluate on a small dataset
  2. Compare factuality and helpfulness of up-scaled model vs small fine-tuned model and large fine-tuned model
  3. Implement speculative decoding for EFT up-scaling and measure speed improvements

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the content, several important questions arise:
- How does the performance of EFT up-scaling compare to fine-tuning large models directly when computational resources are not a constraint?
- Can EFT be extended to other types of model architectures beyond transformers?
- How does the performance of EFT vary with the choice of base model and fine-tuned model families?

## Limitations
- The paper focuses primarily on dialogue and question-answering tasks, leaving generalization to other domains uncertain
- The computational requirements for EFT sampling grow with model size, potentially limiting scalability to extremely large models
- The evaluation methodology relies on GPT-4 as a proxy for human evaluation, which may introduce biases

## Confidence
- **High Confidence**: The core EFT factorization mechanism and its theoretical justification through KL-constrained RL. The basic up-scaling approach. The claim that pre-training improves factuality while fine-tuning improves helpfulness.
- **Medium Confidence**: The practical effectiveness of EFT for test-time adjustment of behavioral traits. The scalability of EFT to very large model families. The generalizability of findings to domains outside dialogue and question-answering.
- **Low Confidence**: The equivalence of up-scaled EFT to actual fine-tuning of large models. The long-term stability and consistency of EFT-generated behaviors. The performance of EFT in safety-critical applications.

## Next Checks
1. **Domain Generalization Test**: Apply EFT up-scaling to three different task types (e.g., code generation, mathematical reasoning, creative writing) and compare performance against both small fine-tuned and large fine-tuned models. Measure whether the decoupling of pre-training and fine-tuning effects holds across these domains.

2. **Large-Scale Scalability Experiment**: Implement EFT up-scaling with models at 70B and 540B scales (or the largest available sizes) and measure the quality-cost tradeoff. Specifically, quantify the accuracy degradation compared to full fine-tuning and the computational savings achieved through speculative decoding.

3. **Behavioral Stability Analysis**: Generate extended conversations (10+ turns) using up-scaled EFT models and analyze whether behavioral traits remain consistent over time. Compare the stability of helpfulness and factuality against models fine-tuned at the target scale, measuring drift in behavioral metrics across conversation turns.