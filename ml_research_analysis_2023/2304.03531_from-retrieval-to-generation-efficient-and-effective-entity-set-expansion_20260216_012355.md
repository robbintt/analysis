---
ver: rpa2
title: 'From Retrieval to Generation: Efficient and Effective Entity Set Expansion'
arxiv_id: '2304.03531'
source_url: https://arxiv.org/abs/2304.03531
tags:
- entities
- entity
- expansion
- language
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Entity Set Expansion (ESE) task aims to expand a small set of seed
  entities with new entities that belong to the same semantic class. Existing ESE
  methods are retrieval-based and suffer from poor efficiency and scalability due
  to iterative traversal of the corpus and entity vocabulary.
---

# From Retrieval to Generation: Efficient and Effective Entity Set Expansion

## Quick Facts
- arXiv ID: 2304.03531
- Source URL: https://arxiv.org/abs/2304.03531
- Reference count: 40
- Key outcome: Achieves state-of-the-art entity set expansion with 600% speedup by using generative approach instead of retrieval-based methods

## Executive Summary
Entity Set Expansion (ESE) traditionally relies on retrieval-based methods that are inefficient due to iterative corpus traversal. This paper introduces GenExpan, the first generative framework for ESE that leverages a pre-trained autoregressive language model without fine-tuning. By employing prefix-constrained generation, in-context learning for class names, knowledge calibration, and generative ranking, GenExpan achieves superior expansion effectiveness while dramatically improving efficiency. Experiments across four benchmark datasets demonstrate consistent performance improvements over state-of-the-art methods.

## Method Summary
GenExpan is a generative ESE framework that uses a pre-trained autoregressive language model (OPT-2.7b) to generate entities. The method constructs a prefix tree from the entity vocabulary to ensure valid entity generation through prefix-constrained Beam Search. Class names are generated via in-context learning using seed entities, which then guide the entity generation process. Knowledge calibration adjusts the model's output distribution to reduce bias toward common entities, and generative ranking scores generated entities using both entity-to-entity and entity-to-class comparisons based on log-perplexity. The approach operates without fine-tuning and achieves significant efficiency gains by eliminating iterative corpus traversal.

## Key Results
- Outperforms state-of-the-art retrieval-based methods on all four benchmark datasets (Wiki, APR, CoNLL, OntoNotes)
- Achieves average 600% speedup in expansion efficiency compared to baseline methods
- Expansion time is independent of entity vocabulary size and corpus size
- Consistently improves both MAP@K and P@K metrics across different datasets and K values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prefix-constrained Beam Search ensures valid entity generation by restricting output to tokens that form complete entities from the vocabulary.
- Mechanism: A prefix tree is built from the entity vocabulary, and during decoding, the model is forced to only generate tokens that are valid continuations in this tree. Invalid tokens have their probability set to zero.
- Core assumption: The prefix tree accurately represents all valid entity prefixes, and the language model's output distribution can be effectively constrained by zeroing invalid token probabilities.
- Evidence anchors:
  - [section] "To force the model to generate valid entities, i.e., entities in the vocabulary ð‘‰ , we construct a prefix tree ð‘‡ ... We exploit prefix-constrained Beam Search [23] as the decoding strategy."
  - [abstract] "Specifically, a prefix tree is employed to guarantee the validity of entity generation..."
  - [corpus] Weak: No direct corpus evidence provided for this specific mechanism.
- Break condition: If the prefix tree is incomplete or incorrectly constructed, the model may fail to generate valid entities or miss some entities in the vocabulary.

### Mechanism 2
- Claim: Class Name Generation via in-context learning guides the model to generate entities belonging to the same semantic class as the seed entities.
- Mechanism: The model is given examples in the prompt showing seed entities and their corresponding class names (e.g., "Nevada, Texas and Ohio are US states"). The model then generates the class name for the given seed entities, which is incorporated into the entity generation prompt to provide stronger class constraints.
- Core assumption: The language model can effectively learn the relationship between seed entities and their class names from the provided examples, and this learned relationship can be transferred to guide entity generation.
- Evidence anchors:
  - [section] "We adopt in-context learning to guide the model in generating the name of the semantic class to which seed entities belong... The class name is incorporated into the prompt used for entity generation, thus enhancing constraints on model generation and directing the model to generate entities belonging to the same class as seed entities."
  - [abstract] "automatically generated class names are adopted to guide the model to generate target entities."
  - [corpus] Weak: No direct corpus evidence provided for this specific mechanism.
- Break condition: If the model fails to learn the relationship between seed entities and class names from the examples, or if the class name is not representative of the semantic class, the entity generation may not be properly guided.

### Mechanism 3
- Claim: Knowledge Calibration reduces the model's bias towards generating common entities by adjusting the output probability distribution based on a "prior probability" learned from a meaningless prompt.
- Mechanism: A meaningless prompt is fed to the model to obtain the prior probability distribution of token generation. This distribution is then used to adjust the model's output during entity generation, reducing the preference for common entities.
- Core assumption: The prior probability distribution learned from the meaningless prompt is a good representation of the model's bias towards common entities, and adjusting the output distribution based on this prior can effectively reduce this bias.
- Evidence anchors:
  - [section] "We obtain the 'prior probability' of the language model by inputting a meaningless prompt to the model, and employ it to adjust the output probability distribution of the model during the generation process, which reduces over-preference of the model for common entities."
  - [abstract] "Moreover, we propose Knowledge Calibration and Generative Ranking to further bridge the gap between generic knowledge of the language model and the goal of ESE task."
  - [corpus] Weak: No direct corpus evidence provided for this specific mechanism.
- Break condition: If the prior probability distribution does not accurately represent the model's bias, or if the adjustment of the output distribution is not effective, the model may still generate common entities with high probability.

## Foundational Learning

- Concept: Autoregressive Language Models
  - Why needed here: The paper relies on a pre-trained autoregressive language model to generate entities in the ESE task. Understanding how these models work is crucial to grasp the methodology.
  - Quick check question: What is the key difference between autoregressive and autoencoding language models?

- Concept: Prefix Trees (Tries)
  - Why needed here: The paper uses a prefix tree to ensure valid entity generation. Understanding the structure and properties of prefix trees is important for implementing this part of the methodology.
  - Quick check question: How does a prefix tree enable efficient prefix-constrained search?

- Concept: In-context Learning
  - Why needed here: The paper employs in-context learning to guide the model in generating class names. Understanding this concept is crucial for grasping how the model learns to generate entities belonging to the same semantic class as the seed entities.
  - Quick check question: How does in-context learning differ from traditional fine-tuning in terms of model adaptation?

## Architecture Onboarding

- Component map:
  Pre-trained Autoregressive Language Model (e.g., OPT-2.7b) -> Prefix Tree (constructed from entity vocabulary) -> Class Name Generation Prompt (with examples) -> Entity Generation Prompt (incorporating class name) -> Knowledge Calibration Module (adjusts output distribution) -> Generative Ranking Module (ranks generated entities)

- Critical path:
  1. Construct prefix tree from entity vocabulary
  2. Generate class name using in-context learning
  3. Generate entities using prefix-constrained Beam Search
  4. Calibrate knowledge to reduce bias
  5. Rank generated entities using Generative Ranking

- Design tradeoffs:
  - Using a pre-trained language model allows leveraging existing knowledge but may introduce bias towards common entities (addressed by Knowledge Calibration)
  - Prefix-constrained generation ensures valid entities but may limit the model's creativity in generating entities
  - In-context learning for class name generation is efficient but relies on the model's ability to learn from examples

- Failure signatures:
  - Invalid entities in the generated list (prefix tree construction or Beam Search implementation issue)
  - Entities not belonging to the same semantic class as seed entities (class name generation or incorporation issue)
  - Over-representation of common entities (Knowledge Calibration not effective)
  - Poor ranking of generated entities (Generative Ranking not effective)

- First 3 experiments:
  1. Test prefix tree construction and prefix-constrained Beam Search with a small entity vocabulary to ensure valid entity generation.
  2. Test class name generation with a few seed entity sets to ensure the model learns the relationship between seed entities and class names.
  3. Test the complete pipeline with a small dataset to ensure all components work together effectively and generate a reasonable list of entities.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GenExpan scale with increasingly larger language models beyond the tested OPT-2.7b size, and what are the computational trade-offs involved?
- Basis in paper: [explicit] The paper mentions exploring the effect of model size on performance but notes hardware limitations prevented testing larger models that could potentially improve expansion performance even further.
- Why unresolved: The experiments were limited to OPT-2.7b due to hardware constraints, leaving open questions about the upper bounds of performance gains from scaling up model size.
- What evidence would resolve it: Systematic experiments testing GenExpan with progressively larger language models (e.g., OPT-6.7b, OPT-13b, GPT-3) while measuring both performance metrics and computational costs would provide clear evidence.

### Open Question 2
- Question: How well does GenExpan handle entity set expansion for non-named entities, multifaceted entities, and vague concepts that are more prevalent in user-generated text?
- Basis in paper: [explicit] The conclusion explicitly identifies these as promising research directions, noting that these scenarios are more prevalent in user-generated text than well-formed text.
- Why unresolved: The paper focuses on named entities and well-defined semantic classes, without testing the framework's robustness to ambiguous or complex entity types.
- What evidence would resolve it: Comprehensive evaluation of GenExpan on datasets containing non-named entities, multifaceted entities, and vague concepts, with comparisons to specialized methods designed for these cases.

### Open Question 3
- Question: What is the impact of different prompt engineering strategies on GenExpan's performance, and can more sophisticated prompt designs further improve results?
- Basis in paper: [inferred] While the paper uses relatively simple prompt constructions (e.g., comma-separated entities), it doesn't systematically explore alternative prompt formulations or the impact of prompt variations on performance.
- Why unresolved: The prompt engineering aspect is treated as fixed in the current implementation, without exploring the full design space of potential prompt structures and formulations.
- What evidence would resolve it: Comparative experiments testing various prompt engineering strategies (e.g., different sentence structures, contextual embeddings, template variations) while measuring their impact on expansion quality and efficiency.

## Limitations

- Implementation details for critical components like class name generation examples and knowledge calibration are not fully specified, making faithful reproduction challenging
- Performance may not generalize well to specialized domains or non-English languages due to reliance on pre-trained language model knowledge
- The approach assumes well-formed text and named entities, with unclear performance on ambiguous concepts or user-generated content

## Confidence

- High confidence in the overall framework design and efficiency claims
- Medium confidence in the effectiveness improvements due to implementation uncertainties
- Low confidence in generalizability beyond the tested domains and datasets

## Next Checks

1. **Implementation replication**: Implement the complete GenExpan pipeline using the provided specifications and test on a small subset of one dataset to verify that prefix-constrained generation produces valid entities and that the class name generation works as described.

2. **Component ablation study**: Systematically remove each proposed component (class name generation, knowledge calibration, generative ranking) and measure the impact on both effectiveness and efficiency to validate their individual contributions.

3. **Cross-domain validation**: Test GenExpan on a domain-specific dataset (e.g., biomedical or technical entities) to evaluate generalizability and identify potential limitations when dealing with specialized terminology or different entity distributions.