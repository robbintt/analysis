---
ver: rpa2
title: Contrastive News and Social Media Linking using BERT for Articles and Tweets
  across Dual Platforms
arxiv_id: '2312.07599'
source_url: https://arxiv.org/abs/2312.07599
tags:
- tweets
- news
- articles
- dataset
- cascades
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CATBERT (Contrastive Articles Tweets BERT),
  a contrastive learning approach for linking tweets to news articles, inspired by
  the success of the CLIP model in computer vision. The model leverages pre-trained
  BERT models and is trained on a dataset containing manually labeled English and
  Polish tweets and articles related to the Russian-Ukrainian war.
---

# Contrastive News and Social Media Linking using BERT for Articles and Tweets across Dual Platforms

## Quick Facts
- arXiv ID: 2312.07599
- Source URL: https://arxiv.org/abs/2312.07599
- Reference count: 40
- Introduces CATBERT, a contrastive learning approach for linking tweets to news articles using separate BERT encoders

## Executive Summary
This paper introduces CATBERT (Contrastive Articles Tweets BERT), a contrastive learning approach for linking tweets to news articles. Inspired by CLIP's success in computer vision, the model uses separate BERT encoders for tweets and articles, trained to align representations in a shared semantic space. The approach demonstrates superior performance compared to traditional methods like LDA and OpenAI embeddings, particularly when aggregating multiple tweets from information cascades.

## Method Summary
CATBERT employs pre-trained BERT models tailored to each text type and language, using separate encoders for tweets and articles. The model is trained using contrastive learning with cosine embedding loss to align representations of matching pairs while separating non-matching pairs. For cascade analysis, multiple tweet embeddings are aggregated using functions like max, mean, or median before similarity comparison with articles. The approach is evaluated on manually labeled English and Polish datasets related to the Russian-Ukrainian war.

## Key Results
- CATBERT outperforms traditional methods (LDA, TF-IDF) and OpenAI embeddings in linking tweets to news articles
- Separate BERT encoders for tweets and articles show better performance than joint-domain encoders
- Aggregation of tweet embeddings improves cascade-level matching, with optimal performance at specific cascade sizes (~150 tweets for OpenAI embeddings)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning aligns tweet and article representations in a shared semantic space
- Mechanism: Uses two separate BERT encoders optimized with cosine embedding loss to pull matching pairs closer and push non-matching pairs apart
- Core assumption: Learned embeddings capture sufficient semantic overlap for similarity-based matching
- Evidence anchors:
  - [abstract] Inspired by CLIP model's success in computer vision, employing contrastive learning to model similarities between images and captions
  - [section] Utilize pre-trained BERT-based models tailored to language, using CLS token for embeddings
  - [corpus] Neighbor titles suggest under-explored direction in cross-modal text linking

### Mechanism 2
- Claim: Aggregation of tweet embeddings improves cascade-level article matching accuracy
- Mechanism: Multiple tweet embeddings from cascade combined using max/mean/median before similarity comparison
- Core assumption: Information cascades contain redundancy that can reduce noise and enhance signal
- Evidence anchors:
  - [section] Experiment shows how cascade size affects model predictions using Cascades – BBC News Dataset
  - [section] OpenAI embeddings with max aggregation benefits from larger cascades (~150 tweets)
  - [corpus] No explicit corpus support; claim derived from experimental results

### Mechanism 3
- Claim: Separate encoders for tweets and articles outperform joint-domain encoders
- Mechanism: Distinct pre-trained BERT models capture domain-specific linguistic features before mapping to shared space
- Core assumption: Tweets and articles have distinct linguistic characteristics benefiting from specialized modeling
- Evidence anchors:
  - [abstract] Previous approaches fell short in capturing unique characteristics of tweets and articles
  - [section] Use pre-trained BERT-based models tailored to language task (Roberta for articles, Twitter Roberta for tweets)
  - [corpus] Limited direct corpus evidence; inferred from methodological description

## Foundational Learning

- Concept: Contrastive learning loss functions (cosine embedding loss)
  - Why needed here: Defines how CATBERT aligns tweet and article embeddings
  - Quick check question: What is the difference between loss for positive and negative pairs in contrastive learning?

- Concept: BERT-based embedding extraction using CLS tokens
  - Why needed here: CLS tokens serve as pooled representation for both tweets and articles in CATBERT
  - Quick check question: Why might using CLS token be preferred over averaging all token embeddings in this context?

- Concept: Information cascades and temporal ordering
  - Why needed here: Understanding cascade construction helps evaluate aggregation methods
  - Quick check question: In cascade construction, which tweet is considered the root and why?

## Architecture Onboarding

- Component map:
  Tweet encoder (BERT-based, domain-specific) -> Article encoder (BERT-based, domain-specific) -> Contrastive learning module (cosine embedding loss) -> Aggregation module (mean/median/max over tweet embeddings) -> Similarity computation (cosine similarity)

- Critical path:
  Data → Preprocessing → Encoder → Embedding → Contrastive loss → Model update → Evaluation

- Design tradeoffs:
  - Separate vs. shared encoders: domain specialization vs. parameter efficiency
  - Truncation vs. chunking vs. long-form models: context retention vs. computational cost
  - Aggregation method: noise reduction vs. potential information loss

- Failure signatures:
  - Low average precision: embeddings may not capture semantic similarity
  - No improvement with cascade size: aggregation may be adding noise
  - High variance in results: preprocessing or data imbalance issues

- First 3 experiments:
  1. Train CATBERT on Tweets – PAP News Training Dataset and evaluate on validation set to tune hyperparameters
  2. Compare CATBERT vs. OpenAI embeddings on single tweets before moving to cascades
  3. Test different aggregation functions (mean, median, max) on cascades to find optimal approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well does CATBERT perform on datasets in languages other than Polish and English?
- Basis in paper: [explicit] Paper evaluates CATBERT on Polish and English datasets and suggests future exploration of other languages
- Why unresolved: Paper only provides results for Polish and English, so performance on other languages is unknown
- What evidence would resolve it: Evaluating CATBERT on datasets in various other languages and comparing results to current findings

### Open Question 2
- Question: What is the impact of using different pre-trained BERT models on CATBERT's performance?
- Basis in paper: [explicit] Paper uses different pre-trained BERT models for English and Polish but doesn't explore impact of using other models
- Why unresolved: Paper doesn't provide comparison of performance when using different pre-trained BERT models
- What evidence would resolve it: Experimenting with different pre-trained BERT models and comparing performance of CATBERT on same datasets

### Open Question 3
- Question: How does CATBERT perform in comparison to other neural network architectures for linking tweets to news articles?
- Basis in paper: [explicit] Paper compares CATBERT to traditional methods like LDA and TF-IDF but doesn't explore other neural network architectures
- Why unresolved: Paper only provides comparison with traditional methods, so performance relative to other neural network architectures is unknown
- What evidence would resolve it: Implementing and evaluating other neural network architectures for the task and comparing their performance to CATBERT on same datasets

## Limitations

- Performance generalizability beyond Russian-Ukrainian war datasets remains untested
- Contrastive learning implementation details (negative sampling strategy, positive-to-negative ratio) are not explicitly specified
- Optimal aggregation method varies by dataset size, suggesting sensitivity to specific dataset characteristics

## Confidence

- **High Confidence**: Core mechanism of using separate BERT encoders with contrastive learning for cross-modal text alignment is well-supported by methodology and experimental results
- **Medium Confidence**: Aggregation of tweet embeddings shows effectiveness but optimal method varies by dataset size, suggesting sensitivity to specific characteristics
- **Low Confidence**: Model's robustness across different topics beyond Russian-Ukrainian war is uncertain

## Next Checks

1. **Cross-domain validation**: Test CATBERT on a different news topic (e.g., climate change or economic news) to assess generalization beyond Russian-Ukrainian war dataset

2. **Negative sampling analysis**: Systematically vary negative sampling strategy and ratio during training to determine optimal configurations and impact on model performance

3. **Encoder comparison**: Conduct direct experiments comparing separate versus shared encoders for tweets and articles to validate claimed advantage of domain-specific modeling