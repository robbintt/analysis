---
ver: rpa2
title: 'Human Still Wins over LLM: An Empirical Study of Active Learning on Domain-Specific
  Annotation Tasks'
arxiv_id: '2311.09825'
source_url: https://arxiv.org/abs/2311.09825
tags:
- learning
- data
- active
- tasks
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether large language models (LLMs) like
  GPT-4 can outperform smaller models trained with expert annotations for domain-specific
  tasks. The authors conduct an empirical study comparing GPT-3.5, GPT-4, and a T5-base
  model fine-tuned with active learning on four datasets from different domains.
---

# Human Still Wins over LLM: An Empirical Study of Active Learning on Domain-Specific Annotation Tasks

## Quick Facts
- arXiv ID: 2311.09825
- Source URL: https://arxiv.org/abs/2311.09825
- Reference count: 11
- Key outcome: T5-base model trained with active learning achieves comparable performance to GPT-4 on domain-specific tasks after training on a few hundred expert-annotated samples

## Executive Summary
This paper investigates whether large language models (LLMs) like GPT-4 can outperform smaller models trained with expert annotations for domain-specific tasks. Through an empirical study comparing GPT-3.5, GPT-4, and a T5-base model fine-tuned with active learning across four domain-specific datasets, the authors demonstrate that the smaller model can achieve performance levels similar to GPT-4 despite being hundreds of times smaller. The results suggest that while LLMs can serve as a warmup method, human experts remain crucial for domain-specific annotation tasks, particularly when computational resources or model size are constrained.

## Method Summary
The study employs an active learning framework where a T5-base model is iteratively fine-tuned on samples selected through either data diversity-based or uncertainty-based sampling strategies. The process begins with a small set of expert-annotated samples, and at each iteration, the model selects the most informative samples for annotation based on the chosen strategy. These newly annotated samples are added to the training set, and the model is re-trained. This cycle continues until performance plateaus or the annotation budget is exhausted. The performance of the AL-trained T5-base model is then compared against zero-shot, few-shot, and fine-tuned versions of GPT-3.5 and GPT-4 on the same tasks.

## Key Results
- T5-base model trained with active learning achieves comparable performance to GPT-4 on BioMRC, Unfair_TOS, and FairytaleQA tasks
- GPT-4 still outperforms the T5-base model on ContractNLI task
- Uncertainty-based AL sampling is more effective than diversity-based or random sampling for tasks with skewed label distributions (e.g., Unfair_TOS with 90% "None" labels)
- Different AL strategies yield varying outcomes across different tasks, suggesting the need for task-specific strategy selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active Learning with expert annotation achieves comparable performance to GPT-4 with far fewer parameters.
- Mechanism: AL strategies iteratively select the most informative samples for annotation, enabling efficient domain knowledge transfer to a small model.
- Core assumption: Domain-specific knowledge can be effectively distilled into a compact model using targeted expert annotations.
- Evidence anchors:
  - [abstract] "small models can outperform GPT-3.5 with a few hundreds of labeled data, and they achieve higher or similar performance with GPT-4 despite that they are hundreds time smaller."
  - [section 4.1] "On BioMRC and ContractNLI, GPT-4 presents exceptional performance, although it may have trained on these datasets. Regardless, our fine-tuned models manage to achieve performance levels that are compatible with GPT-4, despite having hundreds of times fewer parameters and requiring significantly less computational power."
  - [corpus] Weak - no direct citation of this specific claim, but related work on AL and LLMs exists.
- Break condition: If the domain requires vast implicit knowledge not easily captured by a few hundred samples, the small model will fail to match GPT-4.

### Mechanism 2
- Claim: Uncertainty-based AL sampling is more effective than diversity-based or random sampling for tasks with skewed label distributions.
- Mechanism: By selecting samples the model is most uncertain about, the AL process prioritizes learning from ambiguous examples that are likely to improve performance the most.
- Core assumption: Model uncertainty correlates with sample informativeness for reducing overall error.
- Evidence anchors:
  - [section 4.1] "The uncertainty-based data sampling approach proved more effective on BioMRC and Unfair_TOS, while the diversity-based and random strategies demonstrated superior performance on ContractNLI and FairytaleQA, respectively."
  - [section 4.2] "Different active learning strategies yield varying outcomes across different tasks. The uncertainty-based data sampling approach proved more effective on BioMRC and Unfair_TOS..."
  - [corpus] Weak - no direct citation of this specific AL comparison.
- Break condition: If the model's uncertainty estimates are unreliable (e.g., due to out-of-distribution inputs), the selection will be suboptimal.

### Mechanism 3
- Claim: LLM predictions can be used as a warmup method in real-world applications, but human experts remain indispensable for domain-specific annotation.
- Mechanism: LLMs provide a baseline that can be quickly improved upon with a small amount of expert-annotated data, leveraging AL for efficiency.
- Core assumption: Expert annotations are more valuable than LLM-generated ones for domain-specific tasks.
- Evidence anchors:
  - [abstract] "we posit that LLM predictions can be used as a warmup method in real-world applications and human experts remain indispensable in tasks involving data annotation driven by domain-specific knowledge."
  - [section 4.1] "After accumulating a total of several hundreds of data points on most of the tasks, the models reach a performance that is compatible with GPT-4."
  - [corpus] Weak - no direct citation of this specific warmup claim, but related work on AL and LLMs exists.
- Break condition: If expert time is extremely constrained or if LLM predictions are already very close to expert quality, the warmup benefit may be negligible.

## Foundational Learning

- Concept: Active Learning fundamentals (query strategies, iterative learning loops)
  - Why needed here: The entire study is based on comparing different AL strategies for efficient expert annotation.
  - Quick check question: What are the two main high-level categories of AL sampling strategies discussed in the paper?

- Concept: Domain-specific knowledge transfer
  - Why needed here: The paper shows that a small model can learn domain knowledge from limited expert annotations to match GPT-4's performance.
  - Quick check question: Why is domain-specific knowledge crucial for the tasks in this study?

- Concept: Few-shot learning and in-context learning
  - Why needed here: The study compares the performance of GPT-3.5 and GPT-4 using few-shot prompting against the AL-trained small model.
  - Quick check question: What is the main difference between few-shot learning and the approach used by the T5-base model in this study?

## Architecture Onboarding

- Component map:
  Datasets (BioMRC, Unfair_TOS, ContractNLI, FairytaleQA) -> Models (GPT-3.5, GPT-4, T5-base) -> AL strategies (data diversity-based, uncertainty-based, random) -> Evaluation metrics (Accuracy, F1, Rouge-L)

- Critical path:
  1. Initialize AL loop with unlabeled data and expert annotator
  2. Select samples using chosen AL strategy
  3. Get expert annotations for selected samples
  4. Fine-tune T5-base on accumulated annotated data
  5. Evaluate on test set
  6. Repeat until performance plateaus or budget exhausted

- Design tradeoffs:
  - Model size vs. performance: T5-base is much smaller than GPT-4 but can match its performance with expert annotations
  - AL strategy choice: Different strategies work better for different tasks and label distributions
  - Annotation cost vs. performance gain: More iterations lead to better performance but at higher cost

- Failure signatures:
  - AL strategy consistently underperforms random sampling
  - Model performance plateaus quickly despite more iterations
  - Expert annotations don't improve model performance over time

- First 3 experiments:
  1. Compare AL strategies (diversity, uncertainty, random) on BioMRC with 16 samples per iteration for 10 iterations
  2. Evaluate GPT-3.5 and GPT-4 on all 4 datasets using zero-shot, 1-shot, 3-shot, and 10-shot settings
  3. Run AL loop on Unfair_TOS focusing on the imbalance (90% "None" labels) and test if uncertainty sampling helps learn other labels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of T5-base compare to other smaller models (e.g., BERT, RoBERTa) when fine-tuned with active learning on domain-specific tasks?
- Basis in paper: [inferred] The paper only uses T5-base as the representative of pre-trained small models and does not explore the performance of other models.
- Why unresolved: The paper does not provide any comparison or discussion on the performance of other smaller models with active learning on domain-specific tasks.
- What evidence would resolve it: Empirical results comparing the performance of T5-base with other smaller models (e.g., BERT, RoBERTa) when fine-tuned with active learning on the same domain-specific tasks.

### Open Question 2
- Question: What is the impact of different active learning strategies (e.g., hybrid approaches) on the performance of small models in domain-specific tasks?
- Basis in paper: [explicit] The paper mentions that it only implemented and evaluated two basic types of active learning strategies (data diversity-based and uncertainty-based) and is aware of other families of AL strategies that could be included in the study.
- Why unresolved: The paper does not provide any empirical results or discussion on the impact of other active learning strategies on the performance of small models in domain-specific tasks.
- What evidence would resolve it: Empirical results comparing the performance of small models trained with different active learning strategies (e.g., hybrid approaches) on domain-specific tasks.

### Open Question 3
- Question: How does the performance of GPT-4 on BioMRC and ContractNLI compare to its performance on other datasets, and what could be the reason for this difference?
- Basis in paper: [explicit] The paper observes that the performance gap between GPT-3.5 and GPT-4 on BioMRC and ContractNLI is significantly higher than that on other datasets and suspects that GPT-4 may have seen these two datasets during their training process.
- Why unresolved: The paper does not provide any further investigation or discussion on the reasons for the performance difference of GPT-4 on BioMRC and ContractNLI compared to other datasets.
- What evidence would resolve it: Further analysis of GPT-4's performance on BioMRC and ContractNLI compared to other datasets, along with any insights into the potential reasons for the observed difference.

## Limitations
- The study only compares T5-base against GPT-3.5 and GPT-4, without exploring other smaller model architectures
- The claim that human experts remain "indispensable" is based on efficiency arguments rather than definitive superiority of human-annotated data
- The study doesn't explore whether the same performance could be achieved with fewer annotations through different AL strategies or model architectures

## Confidence
- **High confidence**: T5-base can achieve comparable performance to GPT-4 with expert annotations on some domain-specific tasks
- **Medium confidence**: Active learning with expert annotations is more efficient than relying solely on LLMs for domain-specific tasks
- **Low confidence**: The claim that "human experts remain indispensable" for domain-specific annotation tasks

## Next Checks
1. Cross-domain validation: Test the AL approach with T5-base on additional domain-specific datasets outside the current domains (biology, legal, contracts) to verify if the performance patterns hold across diverse domains.

2. Alternative AL strategies: Implement and compare additional AL sampling strategies (e.g., committee-based, expected error reduction) to determine if the observed differences between diversity-based and uncertainty-based sampling are robust or task-specific.

3. LLM vs. human annotation comparison: Conduct a controlled experiment directly comparing model performance when trained on expert annotations versus when trained on high-quality LLM-generated annotations for the same samples, to quantify the actual advantage of human expertise.