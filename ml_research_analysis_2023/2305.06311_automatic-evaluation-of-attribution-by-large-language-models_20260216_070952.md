---
ver: rpa2
title: Automatic Evaluation of Attribution by Large Language Models
arxiv_id: '2305.06311'
source_url: https://arxiv.org/abs/2305.06311
tags:
- attribution
- language
- arxiv
- llms
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the automatic evaluation of attribution
  in generated text by large language models (LLMs). The authors define attribution
  errors as contradictory (the generated statement contradicts the reference) or extrapolatory
  (the reference lacks sufficient information to determine correctness).
---

# Automatic Evaluation of Attribution by Large Language Models

## Quick Facts
- **arXiv ID:** 2305.06311
- **Source URL:** https://arxiv.org/abs/2305.06311
- **Reference count:** 40
- **Primary result:** Automatic attribution evaluation is challenging but feasible using either prompted LLMs or fine-tuned smaller LMs, with fine-tuned models like Flan-T5-Large showing strong performance.

## Executive Summary
This paper investigates automatic evaluation of attribution errors in generated text by large language models. The authors define three types of attribution errors: contradictory (generated statement contradicts the reference), extrapolatory (reference lacks sufficient information to determine correctness), and attributable (reference fully supports the generation). They explore two approaches for automatic evaluation: prompting LLMs with clear instructions and fine-tuning smaller LMs on repurposed datasets from related tasks like question answering, fact-checking, natural language inference, and summarization. The study evaluates these approaches on a manually curated test set of 255 examples from New Bing across 12 domains, along with simulated examples from existing benchmarks.

## Method Summary
The authors evaluate automatic attribution evaluation using two main approaches. The first approach involves prompting LLMs (ChatGPT, Alpaca) with clear instructions defining attribution errors and providing input triples (query, answer, reference) for classification. The second approach fine-tunes smaller LMs (Roberta-Large, FlanT5-Large, GPT2-XL, Alpaca) on repurposed datasets from related tasks. They simulate attribution errors by generating contradictory examples through answer substitution or context modification, and extrapolatory errors by retrieving irrelevant documents. The evaluation uses two test sets: AttrEval-Simulation (4K simulated QA examples) and AttrEval-GenSearch (255 manually annotated examples from New Bing).

## Key Results
- Fine-tuned Flan-T5-Large achieved the best overall performance on the AttrEval-GenSearch test set
- Both prompting LLMs and fine-tuning smaller LMs showed reasonable performance but automatic evaluation remains challenging
- Combining datasets from different related tasks generally improved performance
- Models struggled with fine-grained information comparison, contextual cue interpretation, and symbolic operations

## Why This Works (Mechanism)

## Mechanism 1: Fine-tuning Smaller LMs on Repurposed Datasets
- **Claim:** Fine-tuning smaller language models on repurposed datasets from related tasks can outperform large language models like ChatGPT for attribution evaluation.
- **Mechanism:** Smaller LMs (e.g., Flan-T5-Large, Roberta-Large) are trained on datasets from QA, fact-checking, NLI, and summarization tasks, which are adapted to the attribution evaluation problem. This targeted training allows them to learn the specific patterns and nuances required for accurate attribution assessment.
- **Core assumption:** The patterns and knowledge learned from related tasks are transferable to the attribution evaluation problem, and the adapted datasets provide sufficient signal for the LM to learn the necessary skills.
- **Evidence anchors:**
  - [abstract] "Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges."
  - [section] "Our study shows that a significantly smaller model, FLAN-T5-Large (770M), achieves the best overall performance on the AttrEval-GenSearch set."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.436, average citations=0.0." (Weak corpus evidence, as the related papers are not directly addressing the mechanism of fine-tuning smaller LMs on repurposed datasets.)
- **Break condition:** If the repurposed datasets do not capture the essential characteristics of the attribution evaluation problem or if the related tasks are not sufficiently similar, the fine-tuned smaller LMs may not perform well.

## Mechanism 2: Prompting LLMs with Clear Instructions
- **Claim:** Prompting large language models with clear instructions, including definitions of attribution errors and input triples, can effectively evaluate attribution in generated text.
- **Mechanism:** LLMs like ChatGPT are given a well-defined task description and examples of correct and incorrect attribution. They then process the input triple (query, answer, reference) and classify the attribution as Attributable, Contradictory, or Extrapolatory based on the provided instructions.
- **Core assumption:** LLMs have the inherent capability to understand and reason about the relationships between the query, answer, and reference, and can apply the given instructions to make accurate classifications.
- **Evidence anchors:**
  - [abstract] "We explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs."
  - [section] "Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.436, average citations=0.0." (Weak corpus evidence, as the related papers are not directly addressing the mechanism of prompting LLMs for attribution evaluation.)
- **Break condition:** If the LLM lacks the necessary reasoning capabilities or if the instructions are not clear and unambiguous, the prompted LLM may not perform well.

## Mechanism 3: Combining Datasets for Improved Performance
- **Claim:** Combining datasets from different related tasks (e.g., fact-checking, NLI, summarization) generally improves the performance of attribution evaluation models.
- **Mechanism:** By training on a diverse set of datasets that cover various aspects of attribution evaluation, the model can learn a more comprehensive understanding of the problem and generalize better to unseen examples.
- **Core assumption:** The combined datasets provide a richer and more diverse set of examples, allowing the model to learn a more robust and generalizable representation of attribution evaluation.
- **Evidence anchors:**
  - [abstract] "Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges."
  - [section] "Combining datasets generally improves performance. Our study examines the performance of fine-tuned models on separate task datasets and their combined datasets."
  - [corpus] "Found 25 related papers (using 8). Average neighbor FMR=0.436, average citations=0.0." (Weak corpus evidence, as the related papers are not directly addressing the mechanism of combining datasets for improved performance.)
- **Break condition:** If the combined datasets introduce conflicting signals or if the model is unable to effectively integrate the diverse examples, the performance may not improve or may even degrade.

## Foundational Learning

- **Concept: Attribution evaluation**
  - Why needed here: Understanding the definition and types of attribution errors (Attributable, Contradictory, Extrapolatory) is crucial for developing and evaluating attribution evaluation models.
  - Quick check question: What are the three types of attribution errors defined in the paper, and how do they differ?

- **Concept: Fine-tuning and prompting techniques**
  - Why needed here: Knowledge of how to fine-tune smaller LMs on repurposed datasets and how to prompt LLMs with clear instructions is essential for implementing the proposed approaches.
  - Quick check question: What are the key differences between fine-tuning smaller LMs and prompting LLMs for attribution evaluation?

- **Concept: Dataset curation and simulation**
  - Why needed here: Understanding how to curate real-world test sets and simulate attribution errors from related tasks is important for creating effective training and evaluation datasets.
  - Quick check question: How do the authors simulate attribution errors for the training and test sets, and what are the advantages and limitations of this approach?

## Architecture Onboarding

- **Component map:** Repurposed datasets (QA, fact-checking, NLI, summarization) -> Fine-tuned LMs or prompted LLMs -> AttributionScore classification -> Test sets (AttrEval-Simulation, AttrEval-GenSearch)

- **Critical path:** 1) Curate or simulate training and test datasets 2) Fine-tune smaller LMs on repurposed datasets or prompt LLMs with clear instructions 3) Evaluate the performance of the attribution evaluation models on the test sets 4) Analyze the results and identify areas for improvement

- **Design tradeoffs:** Fine-tuning smaller LMs vs. prompting LLMs: Fine-tuning allows for more targeted training but requires labeled data, while prompting leverages the existing capabilities of LLMs but may be less reliable. Using real-world vs. simulated test sets: Real-world sets provide more realistic examples but are harder to curate, while simulated sets are easier to generate but may not fully capture the complexity of the problem.

- **Failure signatures:** Low performance on test sets, high variance in predictions, difficulty handling fine-grained information or symbolic operations, over-reliance on parametric knowledge instead of conditioning on the reference

- **First 3 experiments:**
  1. Fine-tune a smaller LM (e.g., Flan-T5-Large) on a combination of repurposed datasets and evaluate its performance on the AttrEval-GenSearch test set.
  2. Prompt an LLM (e.g., ChatGPT) with clear instructions and evaluate its performance on the AttrEval-Simulation test set.
  3. Compare the performance of fine-tuned LMs and prompted LLMs on both test sets and analyze the strengths and weaknesses of each approach.

## Open Questions the Paper Calls Out
- **Open Question 1:** How does the performance of AttributionScore vary when applied to different types of attributed LLMs beyond generative search engines, such as chatbots or virtual assistants?
- **Open Question 2:** What are the specific challenges in detecting extrapolatory errors compared to contradictory errors, and how can AttributionScore be improved to better identify extrapolatory errors?
- **Open Question 3:** How does the performance of AttributionScore change when evaluated on real-world examples from attributed LLMs that have been fine-tuned on specific domains or tasks, compared to general-purpose attributed LLMs?

## Limitations
- Manual curation of only 255 test examples across 12 domains represents a relatively small evaluation set
- Simulated test sets introduce potential bias through artificial generation process
- Weak corpus evidence (average citations=0.0, average FMR=0.436) suggests limited direct precedent for these specific approaches

## Confidence
- **High confidence:** The three-way classification framework (Attributable/Contradictory/Extrapolatory) and the general methodology for creating test sets
- **Medium confidence:** The relative performance comparisons between prompting and fine-tuning approaches
- **Low confidence:** The generalizability of results to domains beyond the 12 covered in the test set

## Next Checks
1. Conduct ablation studies on the prompt templates to quantify the impact of instruction clarity on LLM performance
2. Evaluate model performance on a held-out subset of real-world examples not seen during development
3. Test the attribution evaluation models on cross-domain examples to assess generalizability beyond the curated test sets