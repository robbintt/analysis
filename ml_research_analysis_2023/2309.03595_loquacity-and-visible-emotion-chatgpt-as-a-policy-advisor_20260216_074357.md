---
ver: rpa2
title: 'Loquacity and Visible Emotion: ChatGPT as a Policy Advisor'
arxiv_id: '2309.03595'
source_url: https://arxiv.org/abs/2309.03595
tags:
- chatgpt
- language
- data
- human
- financial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates ChatGPT's potential for complex policy writing.
  The authors asked ChatGPT-4 to compose a policy brief for the Board of Italy's central
  bank.
---

# Loquacity and Visible Emotion: ChatGPT as a Policy Advisor

## Quick Facts
- arXiv ID: 2309.03595
- Source URL: https://arxiv.org/abs/2309.03595
- Reference count: 20
- One-line primary result: ChatGPT can accelerate policy writing but requires significant expert supervision to ensure quality

## Executive Summary
This paper evaluates ChatGPT-4's ability to compose a policy brief for the Board of Italy's central bank on AI applications in economics and finance. While ChatGPT can rapidly produce well-structured content, its outputs require substantial expert oversight due to risks of superficiality, irrelevance, and factual errors. The model's knowledge is limited to pre-2021 information, and it struggles to understand audience expertise levels, often defaulting to generic content. Overall, ChatGPT enhances productivity in policy writing only when used by knowledgeable users who carefully guide and verify the output.

## Method Summary
The authors prompted ChatGPT-4 to write a 1,500-word policy brief for the Bank of Italy's Board with PhD-level register and academic references. They used interactive prompt engineering with multiple rounds of feedback and revision to refine the output. The evaluation focused on relevance, depth, and adherence to instructions, with particular attention to cross-border dimensions and audience-appropriateness.

## Key Results
- ChatGPT can rapidly produce well-structured outlines and linguistically correct text
- Outputs require significant expert supervision due to superficiality and potential inaccuracies
- Model's outputs are highly sensitive to prompt formulation and may drift toward generic content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT can accelerate workflows by producing well-structured content suggestions and linguistically correct text in seconds.
- Mechanism: ChatGPT's transformer-based architecture allows it to generate fluent, coherent text quickly based on learned patterns from its training data.
- Core assumption: The user has clear instructions and context for the task, and the generated content is relevant and accurate enough to be useful.
- Evidence anchors:
  - [abstract] "ChatGPT can accelerate workflows by providing well-structured content suggestions, and by producing extensive, linguistically correct text in a matter of seconds."
  - [section] "Outlines on nearly any topic are produced in a few seconds, and acceptable quality can be obtained without sophisticated prompt engineering."
- Break condition: If the user lacks domain knowledge or fails to provide specific instructions, the generated content may be superficial, irrelevant, or require significant expert revision.

### Mechanism 2
- Claim: ChatGPT requires significant expert supervision to ensure quality, partially offsetting productivity gains.
- Mechanism: ChatGPT's outputs are probabilistic and based on learned patterns, not true understanding. It can generate plausible-sounding but inaccurate or irrelevant content, especially on complex topics or recent events.
- Core assumption: The user is aware of ChatGPT's limitations and actively reviews and corrects the output.
- Evidence anchors:
  - [abstract] "It does, however, require a significant amount of expert supervision to attain a satisfactory result, which partially offsets productivity gains."
  - [section] "We find that ChatGPT's attempts at this task are not always salient, and easily drift into banality — a serious issue for policy advisory directed at a high-level audience."
- Break condition: If the user naively trusts ChatGPT's output without verification, the resulting content may contain factual errors, biases, or superficial analysis.

### Mechanism 3
- Claim: ChatGPT's outputs are sensitive to prompt formulation, and small changes can lead to dramatically different results.
- Mechanism: ChatGPT's text generation is conditioned on the input prompt. The model tries to infer the user's intent and generate the most likely continuation, but this can lead to misinterpretation or superficial responses if the prompt is unclear or lacks specificity.
- Core assumption: The user is skilled at prompt engineering and can iteratively refine the prompt to guide ChatGPT towards the desired output.
- Evidence anchors:
  - [abstract] "If the app is used naively, output can be incorrect, superficial, or not relevant — yet, invariably stated with a convincing, reassuring, and self-confident tone."
  - [section] "We notice that ChatGPT sometimes veers towards the superficial, if not outright banal, despite having been instructed to write at a PhD level."
- Break condition: If the user lacks experience with prompt engineering or fails to provide clear, specific instructions, the generated content may not align with the intended task or audience.

## Foundational Learning

- Concept: Transformer architecture
  - Why needed here: Understanding how ChatGPT works is crucial for effective prompt engineering and interpreting its outputs.
  - Quick check question: How does the transformer architecture enable ChatGPT to generate coherent, context-aware text?

- Concept: Large language model limitations
  - Why needed here: Being aware of ChatGPT's limitations, such as its knowledge cutoff date and potential for hallucinations, is essential for using it responsibly and verifying its outputs.
  - Quick check question: What are the main limitations of large language models like ChatGPT, and how can they impact the quality of generated content?

- Concept: Prompt engineering techniques
  - Why needed here: Crafting effective prompts is key to guiding ChatGPT towards generating relevant, high-quality content. Techniques like specificity, context provision, and iterative refinement can improve results.
  - Quick check question: What are some effective prompt engineering techniques for eliciting better responses from ChatGPT?

## Architecture Onboarding

- Component map: User prompt -> ChatGPT model (transformer-based) -> Generated text -> Expert review and revision loop
- Critical path: Provide clear prompt -> Generate initial draft -> Review and fact-check -> Provide targeted feedback -> Iterate until satisfactory
- Design tradeoffs: ChatGPT offers speed and fluency in content generation but lacks true understanding and may require significant human oversight.
- Failure signatures: Superficial or banal content, factual inaccuracies, irrelevant or generic output, failure to capture specific context
- First 3 experiments:
  1. Generate an outline for a policy brief on a given topic, and assess its structure and content relevance.
  2. Draft an introduction section for the policy brief, and evaluate its fluency, tone, and adherence to instructions.
  3. Revise a specific section of the draft, focusing on adding depth, correcting inaccuracies, and ensuring audience-appropriateness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can language models be fine-tuned to better understand and address the knowledge base of high-level policy audiences?
- Basis in paper: [inferred]
- Why unresolved: The paper highlights ChatGPT's difficulty in understanding audience knowledge and tendency to produce generic content. It suggests that fine-tuning with domain-oriented data might help, but doesn't explore this in detail.
- What evidence would resolve it: Experiments comparing outputs from standard vs. fine-tuned models when prompted to address high-level policy audiences. Analysis of how well each model tailors content to the audience's presumed knowledge.

### Open Question 2
- Question: What are the most effective prompting strategies to elicit deeper insights and avoid superficial responses from ChatGPT in complex policy tasks?
- Basis in paper: [explicit]
- Why unresolved: The paper identifies superficiality as a major limitation and explores some prompting strategies, but doesn't systematically test different approaches to find the most effective ones.
- What evidence would resolve it: Comparative studies testing various prompting techniques (e.g., persona adoption, role-play, specific questioning) on their ability to generate substantive, policy-relevant content.

### Open Question 3
- Question: How can the geopolitical dimensions of AI adoption in economics and finance be better integrated into policy discussions generated by language models?
- Basis in paper: [inferred]
- Why unresolved: The paper notes the absence of geopolitical considerations in ChatGPT's output despite their relevance to the target audience. It speculates on possible reasons but doesn't test methods to incorporate these aspects.
- What evidence would resolve it: Analysis of how different prompts or fine-tuning approaches affect the inclusion of geopolitical factors in policy discussions. Comparison of model outputs with expert-generated content on the same topics.

## Limitations

- Analysis based on single case study with one specific prompt, limiting generalizability
- ChatGPT-4's knowledge cutoff (pre-2021) restricts ability to address recent developments
- Assessment relies on subjective judgments rather than objective metrics

## Confidence

- High confidence: Claims about ChatGPT's fluency and speed in generating structured text
- Medium confidence: Conclusions about superficiality and relevance issues
- Low confidence: Generalizations about ChatGPT's overall utility in policy writing

## Next Checks

1. **Prompt variation experiment**: Systematically test how different prompt formulations affect output quality and relevance to measure sensitivity to input formulation.

2. **Cross-domain replication**: Apply the same methodology to policy briefs for different institutions to assess generalizability across policy domains.

3. **Expert evaluation study**: Conduct blind review of ChatGPT-generated policy briefs by domain experts to quantify rates of factual errors, superficiality, and relevance compared to human-written briefs.