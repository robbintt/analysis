---
ver: rpa2
title: 'ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion'
arxiv_id: '2305.06395'
source_url: https://arxiv.org/abs/2305.06395
tags:
- actc
- samples
- annotation
- knowledge
- annotated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles cold-start knowledge graph completion (KGC)
  by proposing ACTC, an active threshold calibration method that efficiently finds
  per-relation thresholds using limited manual annotations. The method combines manual
  labeling with automatic labeling of additional samples via Logistic Regression or
  Gaussian Process classifiers trained on KGE model scores.
---

# ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph Completion

## Quick Facts
- arXiv ID: 2305.06395
- Source URL: https://arxiv.org/abs/2305.06395
- Reference count: 40
- Key outcome: ACTC improves KGC performance by 7% with just 10 annotated samples and achieves 4% average improvement across different annotation budgets

## Executive Summary
ACTC addresses the cold-start problem in knowledge graph completion by proposing an active threshold calibration method that finds per-relation thresholds using limited manual annotations. The method combines manual labeling with automatic labeling of additional samples via Logistic Regression or Gaussian Process classifiers trained on KGE model scores. Experiments across five scoring models show that ACTC significantly outperforms existing baselines while requiring minimal manual annotation effort.

## Method Summary
ACTC tackles cold-start KGC by combining active sample selection (random or density-based), automatic labeling using classifiers trained on manually annotated samples and KGE scores, and threshold estimation from a decision set. The method starts with a small budget of manual annotations, then uses classifiers (Logistic Regression or Gaussian Process) to automatically label additional samples, building a larger decision set for threshold calibration. This approach enables per-relation threshold calibration without requiring large amounts of annotated data upfront.

## Key Results
- Achieves 7% performance improvement with only 10 manually annotated samples
- Outperforms existing baselines by an average of 4% across different annotation budgets
- Density-based selection strategy shows superior performance when annotation budgets are small (<10 samples)
- Method remains stable across different hyperparameter settings (n values) for automatic labeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-relation threshold calibration using logistic regression/Gaussian process classifiers trained on KGE scores enables better performance than uniform thresholds
- Mechanism: The classifier learns the relationship between KGE scores and ground truth labels, allowing more nuanced decision boundaries per relation than a single global threshold
- Core assumption: The KGE model's score distribution contains sufficient signal to distinguish positive from negative examples for each relation
- Evidence anchors:
  - [abstract]: "ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers"
  - [section 3]: "ACTC labels more samples (additionally to the manual annotations) with a classifer trained on the manually annotated samples to predict the labels based on the KGE model scores"
- Break condition: KGE scores are poorly calibrated or have insufficient variance between positive and negative examples for certain relations

### Mechanism 2
- Claim: Density-based sample selection for annotation improves calibration performance when annotation budgets are small
- Mechanism: By selecting samples from dense regions of the score space, ACTC ensures that annotated examples represent the most informative and representative cases for threshold estimation
- Core assumption: The density of scores correlates with the informativeness of samples for threshold calibration
- Evidence anchors:
  - [section 3]: "ACTC this can be done in two ways... density-based selection ACTCdens, inspired by the density-based selective sampling method in active learning"
  - [section 4.2]: "density-based selection allows obtaining considerably better results when a few manually annotated samples are available"
- Break condition: Score distributions are uniform or multimodal in ways that density doesn't capture true informativeness

### Mechanism 3
- Claim: Automatic labeling of additional samples (beyond manual annotations) improves threshold calibration when manual annotation budgets are limited
- Mechanism: By using a classifier trained on limited manual annotations to label additional samples, ACTC effectively increases the size of the decision set without additional human effort
- Core assumption: The classifier trained on manual annotations can reliably predict labels for additional samples in the score space
- Evidence anchors:
  - [abstract]: "Additionally to a few annotated tuples, ACTC also leverages unlabeled tuples by estimating their correctness with Logistic Regression or Gaussian Process classifiers"
  - [section 3]: "ACTC labels more samples (additionally to the manual annotations) with a classifer trained on the manually annotated samples"
- Break condition: The classifier makes systematic errors that compound when used to label many additional samples

## Foundational Learning

- Concept: Knowledge Graph Embeddings (KGE)
  - Why needed here: ACTC relies on KGE model scores as input features for both manual sample selection and automatic labeling
  - Quick check question: What is the difference between TransE's scoring function and ComplEx's scoring function?

- Concept: Active Learning Principles
  - Why needed here: ACTC implements density-based sample selection inspired by active learning methods to choose informative samples for manual annotation
  - Quick check question: How does density-based selection differ from uncertainty-based selection in active learning?

- Concept: Logistic Regression and Gaussian Process Classification
  - Why needed here: These classifiers are used to automatically label additional samples based on KGE scores, extending the decision set
  - Quick check question: What is the key difference between how logistic regression and Gaussian processes handle uncertainty in predictions?

## Architecture Onboarding

- Component map:
  - KGE model (pre-trained) → Score generator
  - Sample selection module (random/density-based) → Annotation budget manager
  - Manual annotation interface → Ground truth provider
  - Classifier trainer (LR/GP) → Automatic labeler
  - Threshold optimizer → Final per-relation thresholds

- Critical path: KGE scores → Sample selection → Manual annotation → Classifier training → Automatic labeling → Threshold estimation

- Design tradeoffs:
  - Random vs. density-based selection: Random is simpler but density-based can be more effective with limited budgets
  - LR vs. GP classifier: LR is faster and simpler, GP can capture non-linear relationships but is computationally heavier
  - n parameter: Higher n requires more automatic labels but may improve robustness; lower n is more conservative

- Failure signatures:
  - Performance plateaus despite more annotations: Classifier may not be learning useful patterns from scores
  - Some relations perform much worse than others: Score distributions may be uninformative for those relations
  - Results degrade with larger n: Automatic labels may be introducing noise that compounds

- First 3 experiments:
  1. Implement random selection with logistic regression classifier on a small dataset to verify basic functionality
  2. Compare random vs. density-based selection with a fixed annotation budget to observe impact on performance
  3. Test different n values (e.g., 5, 50, 500) to find optimal balance between manual and automatic labels

## Open Questions the Paper Calls Out

- Question: How does ACTC perform on real-world datasets with actual human annotation noise rather than oracle annotations?
  - Basis in paper: [inferred] The paper acknowledges using oracle validation labels instead of human manual annotation as a limitation
  - Why unresolved: The experiments used oracle annotations which are cleaner than real human annotations
  - What evidence would resolve it: Experiments on datasets with real human-annotated validation sets

- Question: What is the optimal strategy for selecting the hyperparameter n (minimum samples for threshold estimation) in ACTC?
  - Basis in paper: [explicit] The paper mentions "An important parameter in ACTC is n, the minimal sufficient amount of (manually or automatically) labeled samples needed to calibrate the threshold"
  - Why unresolved: The ablation study shows ACTC is stable across different n values but doesn't provide guidance on selection
  - What evidence would resolve it: Systematic analysis of n selection strategies across diverse datasets

- Question: How would ACTC perform on knowledge graphs from different cultural backgrounds beyond the North American focus of the CoDEx datasets?
  - Basis in paper: [explicit] The paper states "The knowledge graphs used in the experiments are biased towards the North American cultural background"
  - Why unresolved: The experiments were limited to CoDEx datasets with this cultural bias
  - What evidence would resolve it: Experiments on knowledge graphs representing diverse cultural contexts

## Limitations
- Experiments limited to CoDEx datasets which have North American cultural bias
- Uses oracle validation labels rather than real human annotations, limiting real-world applicability
- Computational overhead of training classifiers for automatic labeling may be prohibitive for very large-scale KGs

## Confidence
- Mechanism 1 (per-relation calibration via classifiers): High confidence - Well-supported by experiments
- Mechanism 2 (density-based selection): Medium confidence - Shows promise but optimal strategy depends on score distributions
- Mechanism 3 (automatic labeling): Medium confidence - Effectiveness demonstrated but sensitive to classifier quality

## Next Checks
1. Evaluate ACTC on larger knowledge graphs (e.g., FB15k-237, YAGO3-10) to assess scalability and performance on more diverse data
2. Compare ACTC against uncertainty sampling methods that don't require oracle validation sets to test real-world applicability
3. Perform ablation studies varying the n parameter across a wider range (1-1000) to identify optimal automatic labeling ratios for different annotation budgets