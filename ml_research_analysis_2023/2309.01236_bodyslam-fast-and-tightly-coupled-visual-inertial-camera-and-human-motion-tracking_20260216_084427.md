---
ver: rpa2
title: 'BodySLAM++: Fast and Tightly-Coupled Visual-Inertial Camera and Human Motion
  Tracking'
arxiv_id: '2309.01236'
source_url: https://arxiv.org/abs/2309.01236
tags:
- human
- camera
- estimation
- pose
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BodySLAM++ is a real-time system that jointly estimates the 6D
  pose and posture of humans and a visual-inertial sensor using a tightly-coupled
  factor graph formulation. It extends the OKVIS2 SLAM framework by incorporating
  a learned human motion model and anthropometric priors, optimizing both human and
  camera states simultaneously.
---

# BodySLAM++

## Quick Facts
- **arXiv ID:** 2309.01236
- **Source URL:** https://arxiv.org/abs/2309.01236
- **Reference count:** 36
- **Primary result:** Real-time system achieving 26% improvement in human pose accuracy and 12% improvement in camera trajectory accuracy

## Executive Summary
BodySLAM++ is a real-time system that jointly estimates the 6D pose and posture of humans and a visual-inertial sensor using a tightly-coupled factor graph formulation. It extends the OKVIS2 SLAM framework by incorporating a learned human motion model and anthropometric priors, optimizing both human and camera states simultaneously. The method leverages 2D human keypoints from OpenPose and fits a SMPL human mesh model to these observations while enforcing temporal consistency. Experiments on a custom dataset with ground truth from motion capture show significant accuracy improvements over baseline methods while maintaining real-time performance at 15+ FPS on an Intel i7 CPU.

## Method Summary
BodySLAM++ extends the OKVIS2 visual-inertial SLAM framework by integrating human-specific factors into a tightly-coupled factor graph optimization. The system uses 2D keypoint detections from OpenPose to initialize human states and fit a SMPL mesh model, while incorporating anthropometric priors and a learned human motion model to constrain pose estimates. Both human and camera states are optimized simultaneously using a sliding window approach with keyframe marginalization, enabling real-time performance. The method optimizes both human posture parameters and body shape parameters alongside camera trajectory and landmark positions.

## Key Results
- 26% reduction in mean per joint error compared to baseline methods (SMPLify + ORB-SLAM3/OKVIS2)
- 12% reduction in trajectory error in multi-human scenes
- Real-time performance at 15+ FPS on Intel i7 CPU
- New stereo visual-inertial dataset released with 22 human joints and camera poses for benchmarking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tight coupling of human and camera states improves both human and camera pose estimation accuracy.
- Mechanism: By jointly optimizing human and camera states in a factor graph, errors in human keypoint observations help correct camera trajectory drift, and vice versa. The human motion model provides strong temporal priors that regularize both pose estimates.
- Core assumption: The human motion model accurately predicts plausible human movements and the 2D keypoint observations are sufficiently accurate.
- Evidence anchors:
  - [abstract] "Our system improves the accuracy of both human and camera state estimation with respect to baseline methods by 26% and 12%, respectively"
  - [section] "The human motion model introduced in BodySLAM [1] constraints both the relative change of the 6D pose and the posture of the humans"
- Break condition: If the human motion model predictions are inaccurate or the 2D keypoint detections are noisy, the tight coupling could propagate errors to both human and camera estimates, degrading overall accuracy.

### Mechanism 2
- Claim: Anthropometric priors constrain the high-dimensional human mesh parameters to plausible ranges, improving mesh estimation accuracy.
- Mechanism: Gaussian Mixture Model (GMM) and joint angle priors penalize unlikely human postures and body shapes. The SMPL shape prior constrains the mesh to realistic human body proportions based on principal components.
- Core assumption: The GMM and joint angle priors capture realistic human motion and posture distributions, and the SMPL shape prior encompasses the range of human body shapes in the dataset.
- Evidence anchors:
  - [section] "Gaussian Maximum Mixture Model prior consisting of g = 8 Gaussians proposed in [25], as it has the lowest computational complexity involved"
  - [section] "we use a human shape prior to constrain the possible values. The SMPL model uses the 10 principal components of the average human body shape to contain as much variance as needed."
- Break condition: If the priors are too restrictive, they may prevent the optimizer from finding the true human pose. If too loose, they may not provide enough regularization, leading to implausible human shapes and postures.

### Mechanism 3
- Claim: Real-time performance is achieved by using a sliding window optimization with keyframe marginalization.
- Mechanism: Only the T most recent frames and M keyframes are actively optimized, while older frames are marginalized using relative pose errors. This reduces the problem size and allows for real-time computation.
- Core assumption: The marginalization approach accurately approximates the full optimization solution and the problem size remains tractable for real-time performance.
- Evidence anchors:
  - [section] "The set K contains the T most recent frames as well as M keyframes"
  - [section] "To assess the real-time capability and corresponding accuracy of our proposed system, an experiment was performed where the real-time estimator was constrained to 5, 10, and 15 optimisation iterations."
- Break condition: If the window size is too small, the optimization may not have enough context to accurately estimate the human and camera states. If too large, the problem may become intractable for real-time performance.

## Foundational Learning

- Concept: Factor graphs and nonlinear least squares optimization
  - Why needed here: The core of BodySLAM++ is a factor graph that jointly optimizes human and camera states using nonlinear least squares. Understanding this optimization framework is crucial for implementing and extending the system.
  - Quick check question: What is the difference between a factor graph and a pose graph, and why is a factor graph more suitable for this problem?

- Concept: SMPL human mesh model and its parameterization
  - Why needed here: The SMPL model is used to represent the human body shape and posture. Understanding its parameterization (shape and posture parameters) and how it generates 3D mesh vertices from these parameters is essential for implementing the human reprojection errors and anthropometric priors.
  - Quick check question: How does the SMPL model generate 3D joint positions from the mesh vertices, and why is this important for the human reprojection errors?

- Concept: Visual-inertial SLAM and its error terms
  - Why needed here: BodySLAM++ builds upon a visual-inertial SLAM framework (OKVIS 2) and extends it with human-specific factors. Understanding the standard visual-inertial SLAM error terms (reprojection errors, IMU errors, relative pose errors) is necessary for integrating the human factors into the factor graph.
  - Quick check question: What are the key differences between the reprojection error terms for static landmarks and the human joint reprojection errors in BodySLAM++, and why are these differences important?

## Architecture Onboarding

- Component map: OpenPose keypoint detection -> Human data association -> Factor graph optimization -> Human and camera state estimates
- Critical path: Human keypoint detection → Human data association → Factor graph optimization → Human and camera state estimates
- Design tradeoffs:
  - Tight coupling vs. loose coupling of human and camera states: Tight coupling improves accuracy but increases computational complexity and error propagation risks.
  - Sliding window size vs. optimization accuracy and real-time performance: Larger window sizes improve accuracy but reduce real-time performance.
  - Anthropometric prior strength vs. estimation flexibility: Stronger priors improve plausibility but may restrict the optimizer from finding the true human pose.
- Failure signatures:
  - Human mesh jitter or unrealistic poses: Likely due to weak or overly restrictive anthropometric priors, or inaccurate 2D keypoint detections.
  - Camera trajectory drift or instability: Likely due to insufficient marginalization of old frames, or inaccurate visual-inertial SLAM factors.
  - Real-time performance degradation: Likely due to large problem size, insufficient computational resources, or inefficient implementation.
- First 3 experiments:
  1. Run BodySLAM++ on a simple single-human sequence with ground truth and evaluate the mean per joint position error (MPJPE) and average trajectory error (ATE) against the baseline methods.
  2. Vary the sliding window size (T and M) and measure the impact on MPJPE, ATE, and real-time performance (frame rate).
  3. Disable the anthropometric priors and motion model factors individually and measure their impact on MPJPE and ATE to quantify their contribution to the overall accuracy.

## Open Questions the Paper Calls Out
- **Open Question 1:** How would the performance of BodySLAM++ change if a more advanced human mesh model like STAR [15] were substituted for SMPL?
  - Basis in paper: [explicit] The paper states that the approach allows a quick substitution of the human mesh representation by an updated version like STAR, with minimal influence on the proposed optimisation factors but without loss of generality.
  - Why unresolved: While the paper claims STAR could be substituted with minimal impact, it does not provide experimental validation of this claim or quantify any performance differences.
  - What evidence would resolve it: Experiments comparing BodySLAM++ performance using SMPL vs STAR models on the same datasets, measuring MPJPE, ATE, and computational efficiency.

- **Open Question 2:** Would incorporating online sensor calibration improve BodySLAM++ performance in dynamic scenes?
  - Basis in paper: [explicit] The conclusion mentions that online sensor calibration is a potential area for future work to make system adoption easier.
  - Why unresolved: The current system uses pre-calibrated sensors and does not explore the benefits of online calibration during operation.
  - What evidence would resolve it: Comparative experiments showing BodySLAM++ performance with and without online calibration across varying environmental conditions and sensor configurations.

- **Open Question 3:** How would the addition of human posture priors affect BodySLAM++ accuracy compared to the current anthropometric priors?
  - Basis in paper: [explicit] The conclusion suggests exploring human posture priors to improve estimation accuracy as future work.
  - Why unresolved: The current implementation uses anthropometric priors but does not evaluate the potential benefits of more sophisticated human posture priors.
  - What evidence would resolve it: Controlled experiments comparing BodySLAM++ with different human posture priors (e.g., adversarial priors, neural distance field priors) against the current baseline, measuring improvements in MPJPE and ATE.

## Limitations
- Relies heavily on accurate 2D keypoint detections from OpenPose, which may degrade in challenging lighting conditions or with occlusions
- Anthropometric priors are learned from specific datasets and may not generalize to diverse body shapes and sizes
- Sliding window optimization approach may sacrifice long-term accuracy compared to full batch optimization

## Confidence
- Human and camera state accuracy improvements (26% and 12%): High confidence - supported by quantitative experiments on ground truth dataset
- Real-time performance (>15 FPS): Medium confidence - reported but dependent on hardware specifications

## Next Checks
1. Validate the implementation by running BodySLAM++ on the provided dataset and comparing MPJPE and ATE against baseline methods
2. Profile the optimization loop to identify computational bottlenecks and assess real-time performance on target hardware
3. Test the system's robustness to challenging conditions by evaluating performance with varying lighting, occlusions, and human body types