---
ver: rpa2
title: Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species
arxiv_id: '2305.06695'
source_url: https://arxiv.org/abs/2305.06695
tags:
- genetic
- species
- visual
- data
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel deep visual-genetic biometrics approach
  to enhance visual classification of rare species with limited image data. The core
  idea is to align visual and genetic embedding spaces using deep embedding models
  and cross-domain cosine alignment, leveraging genetic information to improve visual-only
  recognition performance.
---

# Deep Visual-Genetic Biometrics for Taxonomic Classification of Rare Species

## Quick Facts
- arXiv ID: 2305.06695
- Source URL: https://arxiv.org/abs/2305.06695
- Reference count: 0
- Per-class accuracy improves from 37.4% to 59.7% for rare species using visual-genetic alignment

## Executive Summary
This paper introduces a novel deep visual-genetic biometrics approach that enhances visual classification of rare species by aligning visual and genetic embedding spaces. The method leverages genetic information to improve visual-only recognition performance, particularly for rare species with limited image data. Tested on a dataset of 30k+ planktic foraminifer shell images across 32 species, the approach demonstrates significant improvements in per-class and rare tail class benchmarks. The LTR approach improves state-of-the-art accuracy, and visual-genetic alignment further boosts rare per-class accuracy from 37.4% to 59.7%.

## Method Summary
The method involves pre-training a ResNet50 backbone using triplet loss and SoftMax for 20 epochs with LTR weight balancing, then applying Sequence Graph Transform (SGT) to rDNA sequences to create 256D embeddings, and finally aligning visual and genetic embeddings using cosine loss for 5 epochs. The approach maps visual and genetic data into a common embedding space where species proximity reflects both visual and genetic similarity, enabling improved classification of rare species through cross-domain information transfer.

## Key Results
- Overall accuracy improves from 77.7% to 83.3% using LTR techniques alone
- Rare per-class accuracy improves from 37.4% to 59.7% when combining LTR with visual-genetic alignment
- Visual-genetic alignment provides 22% absolute improvement over LTR-only approach for rare species
- The approach demonstrates potential for integrating genetics and imageomics for comprehensive taxonomic representation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal alignment improves rare class recognition by transferring genetic orientation information to visual embeddings
- Mechanism: Genetic rDNA embeddings serve as anchors that provide orientational guidance for visual embeddings via cosine similarity loss
- Core assumption: Visual and genetic feature distances between species are related due to shared phylogenetic relationships
- Evidence anchors:
  - [abstract]: "we propose aligned visual-genetic inference spaces with the aim to implicitly encode cross-domain associations for improved performance"
  - [section 2.4]: "Classic transfer learning approaches in the literature often focus on bridging image and text domains... Our cross-modality learning follows this concept by mapping visuals via ResNet50 and rDNA information via SGT into a common space"
  - [corpus]: Weak - corpus neighbors focus on taxonomy and multimodal learning but don't specifically address genetic-visual alignment
- Break condition: If genetic and visual feature spaces are not structurally related (e.g., convergent evolution creates similar visual features in distantly related species)

### Mechanism 2
- Claim: Long-tailed recognition techniques combined with genetic alignment provide multiplicative benefits for rare species
- Mechanism: LTR weight balancing addresses class imbalance while genetic alignment provides additional discriminative information for rare classes
- Core assumption: Weight balancing alone is insufficient for rare class recognition, but genetic information provides complementary signal
- Evidence anchors:
  - [section 2.2]: "Long-tailed Recognition (LTR) techniques are used to improve the performance of models with a focus on rare classes"
  - [section 6.1]: "Combining the proposed visual-genetic alignment with LTR training (row 7) boosts tail performance significantly by another 11.5% to 59.7%"
  - [corpus]: Weak - corpus focuses on taxonomy and species discovery but doesn't specifically address LTR-genetic combination
- Break condition: If genetic information is noisy or doesn't correlate with visual distinctions for rare species

### Mechanism 3
- Claim: Sequence Graph Transform (SGT) enables effective genetic sequence embedding in visual embedding space
- Mechanism: SGT captures both short and long-term dependencies in genetic sequences and maps them to fixed-dimensional vectors compatible with visual embeddings
- Core assumption: Genetic sequence structure can be meaningfully represented as fixed-dimensional vectors that preserve taxonomic relationships
- Evidence anchors:
  - [section 2.3]: "For this task, Ranjan et al. [37] proposed the approach of a Sequence Graph Transform (SGT), a technique that represents sequences via the statistical relationships between symbols and casts this information into a feature vector"
  - [section 4.2]: "Application of SGT to this new symbol set then creates embeddings of size 16 × 16 = 256 as required"
  - [corpus]: Weak - corpus neighbors don't discuss SGT or genetic sequence embedding techniques
- Break condition: If genetic sequence length variation or compositional complexity cannot be adequately captured by SGT

## Foundational Learning

- Concept: Metric learning with triplet loss
  - Why needed here: Creates visual embedding space where same-class samples cluster together, enabling distance-based classification
  - Quick check question: Can you explain how reciprocal triplet loss differs from standard triplet loss and why it might be advantageous?

- Concept: Long-tailed recognition and class imbalance
  - Why needed here: Foraminifer dataset has highly imbalanced class distribution (7-5,914 samples per taxa), requiring special handling for rare species
  - Quick check question: What are the key differences between re-sampling, re-weighting, and feature transfer approaches for LTR?

- Concept: Cross-domain transfer learning
  - Why needed here: Transferring information from genetic domain to visual domain requires mapping different data types into a common embedding space
  - Quick check question: How does cosine similarity loss facilitate cross-domain information transfer compared to Euclidean distance?

## Architecture Onboarding

- Component map:
  ResNet50 backbone -> Visual embedding space (256D) -> Genetic anchors -> Cosine alignment loss -> KNN inference

- Critical path: Visual pre-training → LTR weight balancing → Cross-modal alignment → KNN inference

- Design tradeoffs:
  - Freezing convolutional layers vs fine-tuning all layers
  - Number of alignment epochs (5 chosen as balance between transfer and overfitting)
  - Choice of cosine vs Euclidean distance for cross-modal alignment
  - SGT dimensionality (256 chosen to match visual embeddings)

- Failure signatures:
  - No improvement in rare class accuracy: likely genetic-visual correlation is weak
  - Decreased overall accuracy: alignment may be over-emphasizing rare classes
  - Training instability: learning rates may need adjustment for cross-modal phase
  - No difference from LTR-only: genetic embeddings may not be informative

- First 3 experiments:
  1. Run baseline visual-only model with LTR weight balancing (row 5 in Table 1)
  2. Add genetic alignment without LTR to measure pure transfer effect (row 6 in Table 1)
  3. Test alignment with out-of-domain ImageNet model to verify genetic information transfer (Fig. 9a)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed visual-genetic biometrics approach be effectively applied to other long-tailed biological datasets beyond foraminifers?
- Basis in paper: [explicit] The authors state "The concept proposed may serve as an important future tool for integrating genetics and imageomics towards a more complete scientific representation of taxonomic spaces and life itself" and demonstrate effectiveness on 32 foraminifer species.
- Why unresolved: The current work only tests on one specific dataset (Endless Forams) of foraminifer images and associated genetic data. While the approach shows promise, its generalizability to other biological taxa with different image characteristics and genetic structures remains to be demonstrated.
- What evidence would resolve it: Testing the visual-genetic alignment approach on diverse biological datasets (e.g., other microfossils, insects, plants) and showing consistent improvements in rare class recognition across different taxa would establish its broader applicability.

### Open Question 2
- Question: How does the quality and quantity of genetic data affect the performance gains from visual-genetic alignment?
- Basis in paper: [inferred] The paper notes that genetic data was only available for 32 out of 35 species in the Endless Forams dataset, suggesting that genetic information may be limited for some taxa. The authors also discuss the Sequence Graph Transform (SGT) method for embedding genetic sequences.
- Why unresolved: The experiments use a specific amount and quality of genetic data (878 sequences for 32 species). It's unclear how the approach would perform with less genetic data per species or with noisier genetic sequences. The impact of genetic data quality on alignment effectiveness is not explored.
- What evidence would resolve it: Systematic experiments varying the amount and quality of genetic data (e.g., using partial sequences, synthetic noise) and measuring the resulting impact on visual classification performance would clarify the robustness of the approach to genetic data limitations.

### Open Question 3
- Question: Can the visual-genetic alignment be extended to incorporate other types of biological data (e.g., ecological, behavioral) for even more comprehensive species classification?
- Basis in paper: [explicit] The authors discuss integrating genetics and imageomics to create a more complete scientific representation of taxonomic spaces and life itself. They also mention that the latent species space built by LTR techniques is particularly receptive to genetic data transfer.
- Why unresolved: The current work only explores alignment between visual and genetic modalities. The potential for incorporating additional biological data types into the embedding space and their relative contributions to classification performance is not investigated.
- What evidence would resolve it: Developing a framework that can integrate multiple biological data types (e.g., genetics, ecology, behavior) into a unified embedding space and demonstrating improved classification performance over single or dual-modality approaches would show the potential for more comprehensive species representation.

## Limitations
- The genetic-visual correlation assumption may not hold for all taxa due to convergent evolution or morphological plasticity
- The approach requires genetic data for all species, which may not be available in real-world deployment scenarios
- The SGT embedding approach may not capture all relevant sequence variations given the fixed 256D dimensionality constraint

## Confidence
- High Confidence: The LTR performance improvements (overall accuracy gains of 5-6%) are well-established in the literature and the experimental design for measuring these is straightforward and reliable
- Medium Confidence: The visual-genetic alignment mechanism showing 22% improvement in rare class accuracy is promising but depends heavily on the genetic-visual correlation assumption, which could vary across different taxa
- Low Confidence: The generalization of these results to other rare species domains beyond planktic foraminifera remains untested and could be significantly affected by the specific characteristics of this dataset

## Next Checks
1. Test the visual-genetic alignment approach on a completely different rare species dataset (e.g., insects or plants) to validate cross-domain generalization
2. Perform ablation studies with noisy or partially missing genetic data to assess robustness to real-world data quality issues
3. Evaluate the learned embeddings using downstream tasks like species discovery or anomaly detection to verify that the embedding space captures meaningful taxonomic relationships beyond simple classification