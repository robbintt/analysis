---
ver: rpa2
title: 'RestGPT: Connecting Large Language Models with Real-World RESTful APIs'
arxiv_id: '2306.06624'
source_url: https://arxiv.org/abs/2306.06624
tags:
- response
- movie
- restgpt
- apis
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RestGPT, a system that connects large language
  models (LLMs) with RESTful APIs to tackle real-world tasks. RestGPT employs a coarse-to-fine
  online planning framework to enhance task decomposition and API selection, addressing
  the challenges of integrating LLMs with RESTful APIs.
---

# RestGPT: Connecting Large Language Models with Real-World RESTful APIs

## Quick Facts
- arXiv ID: 2306.06624
- Source URL: https://arxiv.org/abs/2306.06624
- Reference count: 30
- This paper introduces RestGPT, a system that connects large language models (LLMs) with RESTful APIs to tackle real-world tasks.

## Executive Summary
RestGPT addresses the challenge of connecting large language models with real-world RESTful APIs by implementing a coarse-to-fine online planning framework. The system decomposes user queries into natural language sub-tasks and maps them to appropriate API calls, handling complex multi-step tasks that require interacting with multiple APIs. Through experiments on movie information and music player scenarios, RestGPT demonstrates strong performance in task completion and robustness, particularly in parsing complex JSON responses through generated Python code.

## Method Summary
RestGPT implements a four-component architecture where a planner decomposes user queries into sub-tasks, an API selector maps sub-tasks to appropriate API calls based on OpenAPI Specification documentation, and an executor containing caller and parser components handles API execution and response processing. The system uses text-davinci-003 as its underlying LLM and employs specialized prompts for each component. The coarse-to-fine online planning framework allows for iterative refinement of plans based on execution feedback, while the parser generates Python code to extract information from complex JSON responses rather than attempting direct parsing.

## Key Results
- RestGPT achieves impressive results in complex real-world tasks, demonstrating strong robustness in handling multi-step API interactions
- The system effectively parses complex nested JSON responses through generated Python code, outperforming direct prompting approaches
- Experimental results on movie information and music player scenarios validate the effectiveness of the coarse-to-fine online planning framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Coarse-to-fine online planning improves task decomposition and API selection accuracy compared to static planning
- Mechanism: The planner decomposes user queries into natural language sub-tasks, which are then mapped to specific API calls by the API selector. This division of labor allows each component to focus on its strengths - the planner on task decomposition and the API selector on API understanding.
- Core assumption: LLMs have stronger performance when specialized components handle distinct aspects of the problem rather than attempting end-to-end planning
- Evidence anchors:
  - [abstract]: "we propose RestGPT, which exploits the power of LLMs and conducts a coarse-to-fine online planning mechanism to enhance the abilities of task decomposition and API selection"
  - [section 3.2]: "the planner generates a natural language (NL) sub-task based on the user query, previous plan, and execution history, forming a high-level NL plan. Once the high-level sub-task plan is established, the API selector reads descriptions of available API endpoints to select APIs and construct the API calling plans"
- Break condition: If the planner cannot effectively decompose tasks into sub-tasks that can be mapped to existing APIs, the system will fail to generate appropriate API calls

### Mechanism 2
- Claim: Separating API execution into caller and parser components improves robustness in handling complex JSON responses
- Mechanism: The caller formulates API call parameters based on OAS documentation, while the parser generates Python code to extract information from API responses based on response schemas. This separation allows the system to handle complex, nested JSON responses more effectively than direct prompting.
- Core assumption: LLMs can generate accurate Python code for data extraction when provided with appropriate schemas and instructions
- Evidence anchors:
  - [section 3.3]: "Instead of directly parsing the specific API response document, we utilize the coding capability of LLM to generate Python parsing code based on the provided schema and output instructions generated by the caller"
  - [section 4.4]: "Figure 6 illustrates how RestGPT parses responses. We compare RestGPT parser with directly prompting an LLM as a parser. As shown, responses from RESTful APIs are usually in lengthy and complex JSON format"
- Break condition: If the generated Python code contains errors or the response schema is incomplete/unclear, the parser may fail to extract correct information

### Mechanism 3
- Claim: Feedback mechanisms in API execution enable error handling and recovery
- Mechanism: When an API returns an error message, the caller can read the error information and reorganize API call parameters to resolve the issue. This agent-based architecture with feedback enables the system to handle various runtime errors and exceptions.
- Core assumption: LLMs can understand error messages and generate appropriate corrective actions when provided with error context
- Evidence anchors:
  - [section 3.3]: "API calling in realistic scenarios may encounter various errors and exceptions, such as missing parameters. To enhance robustness, the API executor follows an interaction agent architecture. If the API response indicates an error, the caller can read the error information and re-organize a new API calling to mitigate the issues"
  - [section 4.4]: "Figure 5 shows an example of this mechanism"
- Break condition: If the LLM cannot understand the error message or generate appropriate corrective actions, the system will fail to recover from API errors

## Foundational Learning

- Concept: RESTful API architecture and OpenAPI Specification (OAS)
  - Why needed here: RestGPT relies on understanding RESTful APIs and their OAS documentation to properly connect LLMs with external services
  - Quick check question: What are the key components of an OAS document that RestGPT uses to understand and call APIs?

- Concept: Large language model prompting strategies for specialized tasks
  - Why needed here: RestGPT uses different prompt structures for planner, API selector, caller, and parser components to optimize LLM performance for each role
  - Quick check question: How does RestGPT's prompt design differ between the planner component and the API selector component?

- Concept: Online planning and feedback mechanisms in AI systems
  - Why needed here: RestGPT employs a coarse-to-fine online planning framework that adapts based on API execution results, contrasting with static offline planning approaches
  - Quick check question: What are the key differences between RestGPT's online planning approach and traditional offline planning methods in tool-augmented LLMs?

## Architecture Onboarding

- Component map: User query → Planner decomposition → API Selector selection → Caller parameter generation → API execution → Parser response processing → Planner evaluation → (loop or termination)

- Critical path: User query → Planner decomposition → API Selector selection → Caller parameter generation → API execution → Parser response processing → Planner evaluation → (loop or termination)

- Design tradeoffs:
  - Multiple LLM calls per execution cycle vs. single call with complex prompt (efficiency vs. accuracy)
  - Separation of concerns (specialized components) vs. unified approach (simplicity)
  - Online planning with feedback vs. offline planning (adaptability vs. speed)

- Failure signatures:
  - Planner produces incoherent sub-tasks or incorrect termination signals
  - API Selector selects inappropriate APIs for given sub-tasks
  - Caller generates incorrect API parameters leading to errors
  - Parser fails to extract correct information from complex responses
  - System gets stuck in infinite loops due to planner continuously generating "continue" signals

- First 3 experiments:
  1. Single API call scenario: Test basic functionality with simple user queries requiring one API call
  2. Multi-step scenario: Test the complete planning-execution loop with queries requiring 2-3 API calls
  3. Error handling scenario: Test the feedback mechanism by intentionally providing incorrect parameters to trigger API errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RestGPT handle API calls when the maximum context length constraint is reached, preventing it from accessing all necessary API documentation?
- Basis in paper: [inferred] The paper mentions that RestGPT filters API documents to only preserve APIs appearing in the current API calling plan due to maximum context length constraints.
- Why unresolved: The paper does not provide details on the specific mechanisms or strategies RestGPT employs to handle situations where the context length is exceeded.
- What evidence would resolve it: Experimental results demonstrating RestGPT's performance when handling API calls with varying context lengths, or a detailed explanation of the strategies used to manage context length constraints.

### Open Question 2
- Question: What is the impact of RestGPT's coarse-to-fine online planning framework on the overall efficiency and effectiveness of the system compared to offline planning approaches?
- Basis in paper: [explicit] The paper states that RestGPT employs a coarse-to-fine online planning framework to enhance task decomposition and API selection, contrasting it with offline planning methods.
- Why unresolved: The paper does not provide a direct comparison of efficiency and effectiveness between RestGPT's online planning framework and offline planning approaches in terms of time complexity or success rates.
- What evidence would resolve it: Comparative analysis of RestGPT's performance metrics (e.g., execution time, success rate) against those of offline planning methods across various scenarios and task complexities.

### Open Question 3
- Question: How does RestGPT ensure the accuracy and reliability of the Python code generated by the response parser for extracting information from API responses?
- Basis in paper: [explicit] The paper describes that RestGPT's response parser generates Python code based on the response schema defined in the OAS to parse API responses.
- Why unresolved: The paper does not discuss the mechanisms in place to validate the accuracy of the generated Python code or handle cases where the code fails to parse the response correctly.
- What evidence would resolve it: Detailed explanation of the validation process for the generated Python code, including error handling and fallback strategies, along with empirical results showing the accuracy of the parsed information.

## Limitations

- The system relies heavily on the quality of OpenAPI Specification (OAS) documentation, which may not always be complete or accurate
- Performance may degrade with APIs that have poorly documented response schemas or non-standard error formats
- The approach requires multiple LLM calls per execution cycle, potentially impacting efficiency and cost

## Confidence

- High Confidence: The mechanism of separating planner and API selector components for specialized task decomposition and API selection is well-supported by the experimental results
- Medium Confidence: The feedback mechanism for error handling shows promise, but the robustness evaluation is limited to controlled scenarios
- Medium Confidence: The parsing approach using generated Python code is effective for complex JSON responses, but may struggle with highly dynamic or unpredictable response structures

## Next Checks

1. Test RestGPT with APIs having incomplete or inconsistent OAS documentation to evaluate robustness in real-world scenarios
2. Measure the impact of increasing the number of available APIs on planning and selection accuracy to assess scalability
3. Compare the efficiency and cost of RestGPT's multi-component approach against unified prompting strategies across various task complexities