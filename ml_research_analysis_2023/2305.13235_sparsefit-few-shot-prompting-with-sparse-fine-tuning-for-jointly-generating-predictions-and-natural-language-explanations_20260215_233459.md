---
ver: rpa2
title: 'SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating
  Predictions and Natural Language Explanations'
arxiv_id: '2305.13235'
source_url: https://arxiv.org/abs/2305.13235
tags:
- attention
- sparse
- layer
- nles
- ne-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates few-shot learning for jointly generating
  predictions and natural language explanations (NLEs) with pre-trained language models
  (PLMs). It introduces SparseFit, a sparse fine-tuning strategy that updates only
  a small subset of parameters while leveraging discrete prompts.
---

# SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations

## Quick Facts
- arXiv ID: 2305.13235
- Source URL: https://arxiv.org/abs/2305.13235
- Reference count: 40
- Primary result: Fine-tuning as little as 6.8% of model parameters (LayerNorm + Self-attention Query) achieves competitive task accuracy and NLE quality compared to full fine-tuning

## Executive Summary
This paper introduces SparseFit, a sparse fine-tuning strategy for jointly generating predictions and natural language explanations (NLEs) in few-shot learning scenarios. The approach fine-tunes only specific subsets of parameters (Layer Normalization and Self-attention Query layers) while leveraging discrete prompts to generate structured outputs in the form "[label] because [explanation]". Experiments across four NLE datasets using T5 and UnifiedQA demonstrate that this parameter-efficient approach achieves competitive performance with significantly fewer parameters than full fine-tuning.

## Method Summary
SparseFit is a few-shot learning method that jointly generates predictions and natural language explanations using pre-trained language models. The method uses discrete prompts to structure outputs as "[label] because [explanation]" and employs sparse fine-tuning that updates only a small subset of model parameters (specifically Layer Normalization and Self-attention Query layers, totaling 6.8% of parameters). The approach is evaluated on four NLE datasets using 48 training examples per dataset with 350 validation examples, comparing against full fine-tuning and other parameter-efficient fine-tuning methods.

## Key Results
- Fine-tuning only 6.8% of parameters (LayerNorm + Self-attention Query) achieves competitive task accuracy and NLE quality compared to full fine-tuning
- SparseFit outperforms other parameter-efficient fine-tuning methods on average across the four datasets
- Human evaluation shows approximately 25% of generated NLEs fully justify predictions, with common shortcomings including lack of explanation, nonsensical content, and incomplete explanations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sparse fine-tuning of LayerNorm plus Self-attention Query achieves high performance because these layers influence token representations sufficiently for few-shot adaptation
- **Mechanism:** LayerNorm stabilizes training dynamics and controls scaling of activations, while Self-attention Query computes token relevance to context. Together they allow re-weighting and re-focusing on relevant features without updating the entire transformer stack
- **Core assumption:** Remaining layers already encode useful general-purpose representations, so updating just these two layers is sufficient for task adaptation
- **Evidence anchors:** [abstract] "fine-tuning only 6.8% of the model parameters (specifically, the normalization layer combined with the self-attention query layer)" and [section] "fine-tuning only the Normalization Layer together with the Self-attention Query Layer, which amounts to 6.84% of the model's parameters, consistently gave the best performance"
- **Break condition:** If pre-training did not adequately capture domain token semantics, or if the task requires significant structural changes in other layers

### Mechanism 2
- **Claim:** Joint generation works effectively in few-shot settings because label and explanation share a structured format the model can exploit
- **Mechanism:** Conditioning generation on "[label] because [explanation]" provides a fixed output schema that reduces search space. The label grounds the explanation, and vice versa, enabling co-learning
- **Core assumption:** Structured format enforces alignment between predictions and explanations, and the model can leverage this correlation even with limited examples
- **Evidence anchors:** [abstract] "leverages discrete prompts to jointly generate predictions and NLEs" and [section] "train the LM to conditionally generate a text in the form of '[label] because [explanation]'"
- **Break condition:** If label and explanation are not strongly correlated in data, or if format forces nonsensical NLE generation

### Mechanism 3
- **Claim:** Parameter-efficient fine-tuning methods can outperform full fine-tuning in few-shot scenarios by avoiding catastrophic forgetting and overfitting
- **Mechanism:** Freezing most parameters preserves rich pre-trained representations while updating a small subset adapts to the new task, acting as a regularizer
- **Core assumption:** Pre-trained model contains task-relevant knowledge, and updating a small subset can adapt it without losing generalization
- **Evidence anchors:** [abstract] "fine-tuning only 6.8% of the model parameters leads to competitive results for both the task performance and the quality of the generated NLEs compared to full fine-tuning" and [section] "few-shot sparse fine-tuning of large PLMs can achieve results that are competitive with fine-tuning the entire model"
- **Break condition:** If task requires significant changes to core representations, or if pre-training data is not relevant to target task

## Foundational Learning

- **Concept: Transformer Architecture**
  - **Why needed here:** Understanding transformer component roles (self-attention, feed-forward, normalization) is crucial to interpret why fine-tuning specific layers works
  - **Quick check question:** What is the primary function of the self-attention query matrix, and how does it differ from the key and value matrices?

- **Concept: Few-shot Learning**
  - **Why needed here:** The approach relies on learning from very few examples, so understanding how to adapt models with limited data is essential
  - **Quick check question:** What are the key challenges of few-shot learning, and how do parameter-efficient methods help mitigate them?

- **Concept: Prompt-based Learning**
  - **Why needed here:** The method uses discrete prompts to guide generation, so understanding how prompts influence model behavior is important
  - **Quick check question:** How do hard prompts (explicit text) differ from soft prompts (learned vectors) in terms of flexibility and control?

## Architecture Onboarding

- **Component map:** T5 model with encoder, decoder, self-attention layers, feed-forward networks, normalization layers, and language model head
- **Critical path:** 1. Preprocess input into "[label] because [explanation]" format 2. Select which layers to fine-tune (e.g., LayerNorm + Self-attention Query) 3. Train on few-shot data with conditional generation 4. Evaluate task accuracy and NLE quality
- **Design tradeoffs:** Fine-tuning fewer parameters → less overfitting but potentially lower performance if task requires more adaptation; Fine-tuning more parameters → better adaptation but higher risk of forgetting pre-trained knowledge; Choice of layers to fine-tune → different layers capture different aspects of the task
- **Failure signatures:** Zero or empty NLEs (model fails to generate explanation after label); Low BERTScore for NLEs (explanations are nonsensical or irrelevant); Low task accuracy (model fails to predict correct label)
- **First 3 experiments:** 1. Fine-tune only LayerNorm on small subset of data; check if task accuracy is maintained 2. Fine-tune only Self-attention Query; compare NLE quality to full fine-tuning 3. Combine LayerNorm + Self-attention Query; evaluate both metrics and compare to other PEFT methods

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal combination of sparse fine-tuning parameters (layers/blocks) for maximizing both task performance and explanation quality across different NLE datasets?
- **Basis in paper:** Explicit - The paper investigates sparse fine-tuning of different model components and their combinations, finding that certain combinations achieve competitive results
- **Why unresolved:** The paper shows that different combinations perform well on average, but does not definitively identify the single best combination across all datasets and tasks
- **What evidence would resolve it:** Systematic ablation studies comparing all possible combinations of fine-tuned parameters across multiple datasets and tasks, with statistical significance testing to determine the consistently best-performing combination

### Open Question 2
- **Question:** Why do some sparse fine-tuning configurations (e.g., normalization layer) achieve high task accuracy but generate empty or nonsensical explanations?
- **Basis in paper:** Explicit - The paper observes that configurations like normalization layer fine-tuning achieve high task performance but generate empty explanations
- **Why unresolved:** The paper suggests this might be due to the pre-training tasks of the PLM, but does not provide a definitive explanation for this phenomenon
- **What evidence would resolve it:** Detailed analysis of internal representations and attention patterns in sparse fine-tuned models, comparing them to full fine-tuning models

### Open Question 3
- **Question:** How does the quality of human-written explanations impact the performance of sparse fine-tuning for NLEs?
- **Basis in paper:** Explicit - The paper uses human-written explanations for training but does not investigate how explanation quality affects model performance
- **Why unresolved:** The paper assumes explanation quality is sufficient but does not explore the potential impact of varying explanation quality on model performance
- **What evidence would resolve it:** Experiments comparing model performance using explanations of varying quality to determine impact on task accuracy and explanation generation

## Limitations
- Experimental evaluation is limited to only four datasets from the FEB benchmark, constraining generalizability
- Human evaluation was performed only on a small subset of correctly predicted examples, potentially introducing selection bias
- The paper does not provide statistical significance testing for performance differences between methods
- Analysis of why specific layers (LayerNorm + Self-attention Query) work better than others is largely empirical without theoretical justification

## Confidence

**High confidence:** The experimental results showing that SparseFit achieves competitive performance on the four tested datasets with minimal parameter updates (6.8%). The human evaluation methodology and its finding that roughly 25% of generated NLEs fully justify predictions are well-documented and reproducible.

**Medium confidence:** The claim that SparseFit outperforms other parameter-efficient fine-tuning methods on average. This is based on reported results but lacks statistical significance testing and broader dataset coverage.

**Low confidence:** The generalizability of these findings to other PLM architectures beyond T5 and UnifiedQA, and to other types of tasks beyond the four tested datasets.

## Next Checks

1. **Statistical significance testing:** Perform paired t-tests or bootstrap confidence intervals on performance differences between SparseFit and baseline methods across the 60 train-validation splits to determine if improvements are statistically significant rather than due to random variation.

2. **Layer ablation study:** Systematically evaluate the contribution of each fine-tuned layer by testing all possible combinations of the normalization layer, self-attention query layer, and other transformer components to isolate which specific parameters are driving performance improvements.

3. **Cross-architecture validation:** Replicate the experiments on different PLM architectures (e.g., BERT, RoBERTa, GPT-style models) and on additional few-shot learning datasets beyond the FEB benchmark to assess the robustness and generalizability of the SparseFit approach.