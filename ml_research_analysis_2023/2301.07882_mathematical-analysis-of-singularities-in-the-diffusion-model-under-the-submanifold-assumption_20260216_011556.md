---
ver: rpa2
title: Mathematical analysis of singularities in the diffusion model under the submanifold
  assumption
arxiv_id: '2301.07882'
source_url: https://arxiv.org/abs/2301.07882
tags:
- distribution
- process
- function
- pdata
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a mathematical analysis of diffusion models,
  showing that the analytical mean drift function in DDPM and the score function in
  SGM asymptotically blow up in the final stages of the sampling process for singular
  data distributions concentrated on lower-dimensional manifolds. This makes them
  difficult to approximate using neural networks.
---

# Mathematical analysis of singularities in the diffusion model under the submanifold assumption

## Quick Facts
- arXiv ID: 2301.07882
- Source URL: https://arxiv.org/abs/2301.07882
- Reference count: 40
- Primary result: Shows that score functions in diffusion models become unbounded for data distributions on lower-dimensional manifolds, and proposes a new conditional expectation model to avoid this singularity.

## Executive Summary
This paper provides a rigorous mathematical analysis of singularities that arise in diffusion models (DDPM and SGM) when the data distribution is concentrated on a lower-dimensional manifold. The authors prove that the analytical mean drift function in DDPM and the score function in SGM asymptotically blow up in the final stages of the sampling process for such singular data distributions, making them difficult to approximate using neural networks. To address this fundamental limitation, they derive a new target function based on conditional expectation that remains bounded even for singular data distributions, along with an associated training loss. The theoretical findings are validated through several numerical examples on synthetic distributions.

## Method Summary
The paper builds a unified framework for DDPM and SGM based on reverse-time stochastic differential equations (SDEs). The core idea is to reformulate the problem from learning the score function (which becomes singular) to learning a conditional expectation function f(X,t) = E[X_0 | X_t = X] that remains bounded. The training procedure minimizes a mean-squared error functional using an exponential time schedule designed to match the drift scale with a single time step. For sampling, a splitting scheme is employed that separates the drift and diffusion terms to handle the near-singular behavior near t=0.

## Key Results
- The score function S(X,t) in SGM and the mean drift function in DDPM blow up as O(1/t) near t=0 for data distributions on lower-dimensional manifolds
- The conditional expectation model (CEM) f(X,t) remains bounded and provides a more stable training target
- An exponential time schedule tk = t1(1-γ)^(1-k) matches the drift scale with a single time step, improving sampling stability
- Numerical experiments on synthetic distributions (lines, curves, point clouds) demonstrate the effectiveness of CEM in avoiding sampling artifacts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The analytical mean drift function in DDPM and the score function in SGM become unbounded (blow up) near the final stages of the sampling process when the data distribution is singular (e.g., supported on a lower-dimensional manifold).
- Mechanism: As the diffusion process approaches the data distribution (small t), the score function ∇_X log p(X,t) grows like O(1/t) for points X outside the support of the data distribution. This happens because the probability density concentrates on a lower-dimensional manifold, making the score function singular there.
- Core assumption: The data distribution pdata is supported on a lower-dimensional embedded manifold Ω with continuous and positive density near the closest point to X.
- Evidence anchors:
  - [abstract]: "the analytical mean drift function in DDPM and the score function in SGM asymptotically blow up in the final stages of the sampling process for singular data distributions such as those concentrated on lower-dimensional manifolds"
  - [section]: "the score function S(X,t) blows up as t→0, and more precisely, satisfies S(X,t) = X - y_X / t (1 + o(1))"
  - [corpus]: Weak - only general diffusion model papers cited, no specific singularity proofs.
- Break condition: If the data distribution is not singular (e.g., smooth density over full space) or if the network architecture can represent unbounded functions near t=0.

### Mechanism 2
- Claim: The new conditional expectation model (CEM) avoids the singularity problem by learning a bounded function f(X,t) = E[X_0 | X_t = X] instead of the score function.
- Mechanism: By reformulating the problem to learn f instead of S, the target function remains bounded even for singular data distributions. The drift term in the backward process can then be expressed as X/(1-e^(-t)) - e^(-t/2)/(1-e^(-t)) f(X,t), which avoids the O(1/t) singularity.
- Core assumption: The function f(X,t) can be approximated by a neural network and provides sufficient information to generate samples from the target distribution.
- Evidence anchors:
  - [abstract]: "we derive a new target function and associated loss, which remains bounded even for singular data distributions"
  - [section]: "Denoting EX_0[X_0 | X_t = X] as f(X,t), we know for fixed t that f(·,t) minimizes the following functional... This justifies defining a new loss function for training f_θ"
  - [corpus]: Weak - only general diffusion model papers cited, no specific conditional expectation proofs.
- Break condition: If the function f is too complex to be approximated by available network architectures, or if the sampling process introduces too much numerical error.

### Mechanism 3
- Claim: The training schedule using exponential time steps tk = t1(1-γ)^(1-k) matches the drift scale with a single time step, improving sampling stability.
- Mechanism: By choosing the time steps such that (t_k - t_{k-1})/t_k = γ_k is constant for all k>1, the numerical error introduced by the splitting scheme remains controlled throughout the sampling process.
- Core assumption: The drift term scales as O(1/t) near t=0, and matching this scale with the time step prevents accumulation of numerical errors.
- Evidence anchors:
  - [section]: "At the time t_k for k>1, we consider the scale of changes due to the drift, (t_k - t_{k-1})/t_k := γ_k. Minimizing γ_k for all k>1, we arrive at the following exponential schedule, t_k = t_1(1-γ)^(1-k)"
  - [corpus]: Weak - only general diffusion model papers cited, no specific time step proofs.
- Break condition: If the drift scaling assumption is incorrect for the specific data distribution, or if numerical instabilities occur for very small time steps.

## Foundational Learning

- Concept: Stochastic differential equations (SDEs) and their relationship to diffusion models
  - Why needed here: The paper builds on the connection between diffusion models and SDEs, particularly the forward process (adding noise) and reverse-time SDE (generating samples)
  - Quick check question: What is the relationship between the forward diffusion process and the Ornstein-Uhlenbeck process mentioned in section 3.1?

- Concept: Fokker-Planck equation and score functions
  - Why needed here: The paper uses the Fokker-Planck equation to derive the analytical form of the score function and analyze its singularity properties
  - Quick check question: How does the score function relate to the gradient of the log probability density function in the context of diffusion models?

- Concept: Conditional expectation and its properties
  - Why needed here: The paper reformulates the problem as learning a conditional expectation function f(X,t) = E[X_0 | X_t = X] instead of the score function
  - Quick check question: Why does the conditional expectation f(X,t) remain bounded even when the score function becomes singular?

## Architecture Onboarding

- Component map: Data preprocessing -> Forward process simulation -> Neural network model -> Training loop -> Sampling process

- Critical path:
  1. Generate training data by solving the forward SDE from random initial conditions
  2. Train the neural network to approximate f(X,t) by minimizing the conditional expectation loss
  3. Sample from the model by solving the reverse-time SDE using the trained drift function

- Design tradeoffs:
  - Network architecture: Deeper networks may better approximate complex f functions but increase training time
  - Time step schedule: Smaller initial time steps (t1) reduce sampling error but increase computational cost
  - Weight function λ(t): Must balance training stability across different time scales

- Failure signatures:
  - Training instability: If λ(t) is not chosen properly, the loss may have high variance at different times
  - Sampling artifacts: If the network cannot approximate f well, samples may show artifacts or fail to match the target distribution
  - Slow convergence: If the time step schedule is too conservative, sampling may require many steps

- First 3 experiments:
  1. Implement the CEM for a simple 1D line distribution in 2D space and compare with SGM/DDPM
  2. Test different network architectures (shallow vs deep) on a point cloud distribution
  3. Vary the initial time step t1 and measure its effect on sampling quality for a multi-modal distribution

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the network architecture for the conditional expectation model (CEM) be designed to better preserve the low-dimensional structure of continuous data distributions, rather than just approximating discrete samples?
- Basis in paper: [inferred] The paper mentions that if the network design preserves the possible low-dimensional structure of continuous data distributions, it may generalize better from discrete samples to continuous distributions.
- Why unresolved: The paper does not provide specific architectural guidelines for achieving this goal. It only suggests that this is a direction for future work.
- What evidence would resolve it: Empirical studies comparing different network architectures for CEM on various types of continuous data distributions, showing improved generalization when low-dimensional structure is preserved.

### Open Question 2
- Question: What is the optimal choice of the weighting function λ(t) in the CEM training loss to minimize the overall training error?
- Basis in paper: [explicit] The paper proposes λ(t) = (e^t - 1)^-1 based on empirical analysis, but notes this choice remains free for the user and could potentially be improved.
- Why unresolved: The paper does not provide a rigorous derivation of the optimal λ(t) or compare it against other choices. It only suggests a heuristic based on aligning the training process for each t.
- What evidence would resolve it: Theoretical analysis of the optimal λ(t) choice, or empirical comparison of different λ(t) choices on multiple datasets showing improved training performance.

### Open Question 3
- Question: How does the choice of the initial time t_1 in the exponential schedule affect the overall sampling quality and efficiency?
- Basis in paper: [explicit] The paper discusses the choice of t_1 in the exponential schedule and shows that smaller t_1 results in lesser errors in the generated distribution, but also introduces numerical instabilities.
- Why unresolved: The paper does not provide a systematic analysis of how different t_1 choices affect the sampling quality or derive an optimal t_1.
- What evidence would resolve it: Empirical studies varying t_1 across multiple datasets and network architectures, quantifying the trade-off between sampling quality and numerical stability to determine an optimal t_1 choice.

## Limitations
- The theoretical analysis assumes data distributions are concentrated on lower-dimensional embedded manifolds with continuous density, which may not hold for many real-world datasets
- Numerical experiments are limited to synthetic distributions and lack validation on real-world benchmarks like CIFAR-10 or ImageNet
- Computational efficiency claims are not substantiated with concrete runtime comparisons against state-of-the-art diffusion models

## Confidence
- **High confidence**: The mathematical derivation of the score function singularity for simple geometric distributions is well-established and rigorously proven. The connection between diffusion models and SDEs is also well-understood in the literature.
- **Medium confidence**: The proposed CEM approach and its theoretical advantages over traditional score-based methods are plausible but lack extensive empirical validation on complex datasets. The time step schedule optimization is based on reasonable assumptions but may not generalize to all scenarios.
- **Low confidence**: Claims about computational efficiency improvements and generalization to high-dimensional, real-world data distributions are not adequately supported by the current experimental results.

## Next Checks
1. **Real-world data validation**: Implement and test the CEM approach on standard image datasets (e.g., CIFAR-10, CelebA) to evaluate its performance compared to state-of-the-art diffusion models in terms of sample quality, diversity, and computational efficiency.

2. **Robustness to data distribution assumptions**: Design experiments to test the CEM's performance on data distributions that do not strictly adhere to the embedded manifold assumption, such as multi-modal distributions with complex geometries or distributions with significant noise components.

3. **Scaling analysis**: Conduct a comprehensive study on how the CEM's performance scales with increasing dimensionality and complexity of the data distribution, comparing it with traditional diffusion models to identify scenarios where the proposed approach offers clear advantages.