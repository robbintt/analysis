---
ver: rpa2
title: 'REFER: An End-to-end Rationale Extraction Framework for Explanation Regularization'
arxiv_id: '2310.14418'
source_url: https://arxiv.org/abs/2310.14418
tags:
- linguistics
- computational
- task
- association
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes REFER, a framework that jointly trains the
  task model and rationale extractor to optimize for faithfulness, plausibility, and
  task performance. The key idea is to employ a differentiable rationale extractor
  and use Adaptive Implicit Maximum Likelihood Estimation to backpropagate through
  the rationale extraction process.
---

# REFER: An End-to-end Rationale Extraction Framework for Explanation Regularization

## Quick Facts
- arXiv ID: 2310.14416
- Source URL: https://arxiv.org/abs/2310.14416
- Reference count: 29
- Primary result: Jointly trained task model and rationale extractor achieves 11% and 3% better composite normalized relative gain on e-SNLI and CoS-E respectively

## Executive Summary
This paper introduces REFER, a framework for jointly training task models and rationale extractors to optimize for task performance, faithfulness, and plausibility simultaneously. The key innovation is using a differentiable rationale extractor with Adaptive Implicit Maximum Likelihood Estimation (AIMLE) to enable end-to-end training through the discrete selection process. The framework demonstrates improved out-of-distribution generalization and can achieve comparable results with only 50% of human annotation.

## Method Summary
REFER jointly trains a task model and rationale extractor using BIGBIRD-Base encoders. The rationale extractor employs AIMLE for differentiable top-k% selection, enabling gradient flow through the discrete extraction process. The model is trained with a multi-task loss combining task performance, faithfulness (measured via sufficiency and comprehensiveness), and plausibility (measured against human highlights). The framework uses separate encoders for the task and rationale extraction to provide dedicated capacity for each function.

## Key Results
- REFER achieves 11% better composite normalized relative gain on e-SNLI compared to previous baselines
- REFER achieves 3% better composite normalized relative gain on CoS-E
- With only 50% human annotation, REFER matches the plausibility performance of models trained with 100% supervision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training with differentiable rationale extraction enables end-to-end optimization of all three criteria
- Mechanism: AIMLE provides low-variance gradient estimates through the discrete top-k% selection process
- Core assumption: AIMLE gradient estimates remain stable and informative throughout training
- Evidence anchors: Abstract mentions differentiable rationale extraction; section 3 describes AIMLE's low-variance properties

### Mechanism 2
- Claim: Human rationale supervision improves generalization by preventing spurious correlation reliance
- Mechanism: Plausibility loss forces alignment with human reasoning patterns
- Core assumption: Human-annotated rationales are less prone to spurious correlations
- Evidence anchors: Section 6 shows improved OOD performance; plausibility metrics demonstrate alignment with human highlights

### Mechanism 3
- Claim: Joint optimization enables effective few-shot learning of rationale selection
- Mechanism: Sufficiency and comprehensiveness losses provide direct feedback on rationale quality
- Core assumption: These losses provide sufficient signal for learning selection patterns
- Evidence anchors: Section 6 demonstrates 50% annotation achieving comparable plausibility to full supervision

## Foundational Learning

- Concept: Differentiable approximation of discrete operations
  - Why needed here: Rationale selection is discrete but needs to be differentiable for joint training
  - Quick check question: How does AIMLE maintain low gradient variance during top-k% approximation?

- Concept: Multi-task learning with competing objectives
  - Why needed here: Balancing task accuracy, faithfulness, and plausibility creates optimization conflicts
  - Quick check question: How does the model resolve conflicts when optimizing for faithfulness reduces task accuracy?

- Concept: Out-of-distribution generalization
  - Why needed here: Models should perform well on data distributions different from training data
  - Quick check question: What specific training aspects help the model generalize to HANS and contrast sets?

## Architecture Onboarding

- Component map: Input text → Task Encoder + Rationale Extractor → AIMLE Selection → Task Prediction → Loss Computation → Backpropagation
- Critical path: Input → dual encoders → token scoring → AIMLE top-k% selection → task prediction → joint loss computation → backpropagation through both models
- Design tradeoffs: Separate encoders provide more capacity but increase parameters; hard selection is interpretable but non-differentiable
- Failure signatures: Poor faithfulness suggests sufficiency/comprehensiveness loss issues; poor plausibility suggests encoder capacity problems
- First 3 experiments: 1) Basic training with balanced losses on e-SNLI, 2) Independent variation of faithfulness and plausibility weights, 3) Comparison of 10% vs 100% human annotation performance

## Open Questions the Paper Calls Out

- How can we optimize the trade-off between plausibility and faithfulness in explanation regularization models?
- How can explanation regularization models effectively handle and reason over complex linguistic phenomena?
- How can explanation regularization models effectively incorporate and leverage external knowledge sources?

## Limitations

- AIMLE implementation details are not fully specified, making exact reproduction challenging
- Results are limited to two specific datasets (e-SNLI and CoS-E) and may not generalize to other tasks
- Computational overhead of separate rationale extractor is not discussed

## Confidence

**High Confidence Claims:**
- Joint training framework improves plausibility compared to baselines
- REFER achieves competitive task performance while providing explanations
- Human supervision significantly improves out-of-distribution generalization

**Medium Confidence Claims:**
- AIMLE-based differentiable extraction effectively optimizes all criteria
- 50% human annotation matches 100% supervision performance
- Framework generalizes to different explanation formats

**Low Confidence Claims:**
- Specific parameter values (αf=0.5, αp=0.5, k=50%) are optimal across settings
- Framework would perform similarly on more diverse NLP tasks

## Next Checks

1. **AIMLE Gradient Quality Analysis**: Implement gradient checking to verify AIMLE provides stable, low-variance gradient estimates through top-k% selection

2. **Ablation Study on Loss Components**: Systematically remove or vary each loss component to quantify individual contributions to observed improvements

3. **Cross-Dataset Generalization Test**: Apply REFER to a different NLP task with explanation annotations to verify framework generalization beyond NLI and QA