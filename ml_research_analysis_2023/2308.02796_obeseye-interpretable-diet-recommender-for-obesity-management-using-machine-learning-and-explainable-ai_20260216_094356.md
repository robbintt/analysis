---
ver: rpa2
title: 'OBESEYE: Interpretable Diet Recommender for Obesity Management using Machine
  Learning and Explainable AI'
arxiv_id: '2308.02796'
source_url: https://arxiv.org/abs/2308.02796
tags:
- prediction
- food
- learning
- machine
- diet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes OBESEYE, a machine learning-based diet recommender
  system that predicts daily nutrient requirements for individuals with comorbidities.
  The system uses physical and disease-related data to recommend fluid, carbohydrate,
  protein, and fat intake.
---

# OBESEYE: Interpretable Diet Recommender for Obesity Management using Machine Learning and Explainable AI

## Quick Facts
- arXiv ID: 2308.02796
- Source URL: https://arxiv.org/abs/2308.02796
- Reference count: 19
- Primary result: ML-based diet recommender system predicts daily nutrient requirements for individuals with comorbidities using physical and disease-related data.

## Executive Summary
This study introduces OBESEYE, a machine learning system that predicts daily fluid, carbohydrate, protein, and fat requirements for individuals with comorbidities to manage obesity. The system uses physical and disease-related information from 146 patients to recommend personalized nutrient intake. Six machine learning algorithms are evaluated for each nutrient prediction, with linear regression, random forest, and LightGBM emerging as the best performers for different nutrients. Explainable AI techniques (LIME and SHAP) are applied to interpret model decisions, providing transparency for clinical adoption.

## Method Summary
The study evaluates six machine learning algorithms (linear regression, SVM, decision tree, random forest, XGBoost, LightGBM) for predicting four nutrient targets (fluid, carbohydrate, protein, fat) from physical and disease-related patient data. The dataset contains 146 patients with features including demographics, vital parameters, and disease conditions. Models are trained and evaluated using RMSE and accuracy metrics, with the best-performing algorithm selected for each nutrient. Explainable AI techniques (LIME for local explanations and SHAP for global feature importance) are applied to interpret model predictions and enhance clinical trust.

## Key Results
- Linear regression achieved the best performance for fluid prediction (RMSE: 0.39 L, accuracy: 78.75%)
- Random forest excelled at carbohydrate prediction (RMSE: 29.08 g, accuracy: 86.99%)
- LightGBM provided optimal results for protein (RMSE: 15.95 g, accuracy: 79.27%) and fat (RMSE: 15.09 g, accuracy: 64.96%)
- CKD was identified as the most important feature for fluid prediction through SHAP analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different ML algorithms yield optimal performance for different nutrient targets due to underlying data distributions and feature interactions.
- Mechanism: Linear regression performs best for fluid prediction because the relationship between input features and fluid intake is approximately linear and continuous. Random forest excels at carbohydrate prediction due to its ability to capture non-linear interactions among features like age, BMI, and disease status. LightGBM outperforms for protein and fat because it efficiently handles sparse and heterogeneous feature spaces common in medical comorbidity data.
- Core assumption: Nutrient requirements are driven by distinct physiological and disease-specific mechanisms that are not uniformly captured by a single algorithm.
- Evidence anchors: [abstract] "We achieved high accuracy with low root mean square error (RMSE) by using linear regression in fluid prediction, random forest in carbohydrate prediction and LightGBM in protein and fat prediction." [section] "We selected the linear regression model for fluid prediction as it predicts the fluid value with a small average difference of 0.39 liter and has high accuracy."
- Break condition: If the feature distributions become more uniform across nutrients or if additional engineered features reduce non-linearities, a single model might perform comparably.

### Mechanism 2
- Claim: Encoding disease states and physical conditions as categorical variables preserves clinically relevant thresholds and improves model interpretability.
- Mechanism: Converting continuous biomarkers (e.g., glucose, creatinine) into categorical labels ("Low", "Normal", "High") aligns model inputs with clinical decision thresholds. Binary encoding of comorbidities (0/1) simplifies feature representation while maintaining discriminative power. This encoding enables SHAP and LIME explanations to map directly to clinically interpretable feature contributions.
- Core assumption: Clinicians interpret patient status using categorical thresholds; models should reflect this structure for actionable outputs.
- Evidence anchors: [section] "We categorized diabetes values into two groups: being diabetic as 'Yes' and not being diabetic as 'No'. The same categorization process goes for the creatinine values." [section] "Diastolic pressure less than 60 or systolic pressure less than 90 is considered as low pressure... more than these range for both diastolic and systolic is considered as high blood pressure."
- Break condition: If continuous encoding with normalization yields lower prediction error, the categorical assumption may not hold.

### Mechanism 3
- Claim: Explainable AI methods (LIME, SHAP) increase stakeholder trust by mapping model decisions to clinically meaningful factors.
- Mechanism: LIME provides local, instance-level explanations by fitting interpretable surrogate models around predictions, showing which features positively or negatively influence a specific nutrient recommendation. SHAP aggregates feature contributions across the dataset, revealing global importance (e.g., CKD as top fluid predictor) and directionality. This transparency satisfies clinician and patient needs for understanding "why" a recommendation is made.
- Core assumption: Adoption of AI-driven diet recommendations requires interpretable outputs to be accepted by non-technical users.
- Evidence anchors: [abstract] "Explainable AI techniques (LIME and SHAP) are applied to interpret model decisions." [section] "We also applied SHAP (Shapley Additive Explanations) for global interpretation of our model. SHAP summary plot... combines feature importance with feature effects."
- Break condition: If users do not engage with explanations or if explanations do not align with domain knowledge, trust gains may be minimal.

## Foundational Learning

- Concept: Supervised regression learning with continuous targets.
  - Why needed here: Nutrient intake (g, L) is numeric and continuous; predicting exact amounts requires regression, not classification.
  - Quick check question: What loss function is minimized during training of a regression model?
    - Expected answer: Mean Squared Error (MSE) or Root Mean Squared Error (RMSE).

- Concept: Feature preprocessing and encoding for mixed data types.
  - Why needed here: The dataset contains numeric (age, weight), categorical (gender, disease status), and ordinal (blood pressure categories) features that must be consistently encoded before ML model ingestion.
  - Quick check question: Why might binary encoding be preferred over one-hot encoding for binary disease flags?
    - Expected answer: Binary encoding reduces dimensionality and aligns with the binary nature of the variable.

- Concept: Model evaluation using RMSE and R² for regression.
  - Why needed here: RMSE measures average prediction error in the same units as the target (e.g., grams, liters), while R² quantifies variance explained, both critical for clinical acceptability.
  - Quick check question: If RMSE = 15.95 g for protein prediction, is this error clinically significant given typical protein ranges of 40–120 g?
    - Expected answer: It depends on clinical tolerance; 15.95 g is ~13–40% of typical ranges, so acceptability must be judged by domain experts.

## Architecture Onboarding

- Component map: Data Ingestion -> Preprocessing (scaling, encoding) -> Train/Test Split -> ML Model Training (LR/SVM/Decision Tree/Random Forest/XGBoost/LightGBM) -> Prediction Output -> XAI Layer (LIME/SHAP) -> Recommendation Interface
- Critical path: Preprocess -> Train Model -> Evaluate RMSE/R² -> Select best model per nutrient -> Generate interpretable explanations -> Deploy as API
- Design tradeoffs: Simplicity vs. accuracy (Linear regression is interpretable but less accurate for complex interactions; ensemble methods are more accurate but less transparent); Categorical vs. continuous encoding (Categorical preserves clinical thresholds but may lose granularity; continuous may improve fit but reduce interpretability)
- Failure signatures: High RMSE with low R² indicates model underfitting or irrelevant features; Overfitting shown by perfect R² on training but poor test performance (e.g., Decision Tree with R²=1.00); SHAP importance dominated by a single feature may indicate data leakage or lack of diversity
- First 3 experiments: 1) Compare baseline linear regression vs. Random Forest on fluid prediction; record RMSE and R². 2) Perform hyperparameter tuning on Random Forest for carbohydrate (e.g., n_estimators, max_depth) and measure impact on RMSE. 3) Generate LIME explanations for a few sample predictions and validate that top features align with clinical intuition (e.g., CKD influencing fluid).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the model performance change if a larger and more diverse dataset was used, including patients from different geographical regions and socioeconomic backgrounds?
- Basis in paper: [inferred] The authors acknowledge that their model's prediction can be improved by incorporating more patient information and mention future plans to include demographic data such as economic, transportation, and housing information.
- Why unresolved: The current study used data from 146 patients in a single hospital in Bangladesh, which may not be representative of the broader population with different dietary habits, healthcare access, and disease prevalence.
- What evidence would resolve it: Training and testing the model on a larger, multi-center dataset with patients from various countries and socioeconomic backgrounds, then comparing the performance metrics (RMSE, accuracy) to the current results.

### Open Question 2
- Question: Can the model be extended to recommend specific food items rather than just nutrient quantities, taking into account cultural food preferences and availability?
- Basis in paper: [inferred] The authors mention future plans to recommend specific food items based on disease conditions and side effects, and to extend recommendations to other nutrients like vitamins and minerals.
- Why unresolved: The current system only predicts quantities of fluid, carbohydrates, protein, and fat. It does not provide specific food recommendations or consider cultural dietary practices and food availability.
- What evidence would resolve it: Developing a food database that maps nutrient quantities to specific food items, incorporating cultural and regional food preferences, and testing the extended system's ability to generate culturally appropriate and nutritionally balanced meal plans.

### Open Question 3
- Question: How does the inclusion of comorbidities affect the model's performance compared to a model trained only on general population data?
- Basis in paper: [explicit] The authors emphasize that their system is unique in considering comorbidities and physical conditions when recommending diet, unlike other food log applications that use Western foods and may not reflect actual calorie counts in Oriental-style recipes.
- Why unresolved: The study does not compare the performance of the comorbidity-aware model with a model trained on general population data without considering specific diseases.
- What evidence would resolve it: Training two separate models, one with the current dataset including comorbidities and another with a general population dataset, then comparing their performance in predicting nutrient requirements for patients with and without comorbidities.

## Limitations
- The study is limited by its relatively small dataset of 146 patients, which constrains generalizability to broader populations.
- The claimed accuracy metrics for regression tasks (78.75%, 86.99%, etc.) are unusual and may reflect non-standard evaluation approaches or classification on discretized targets.
- The model has not been validated against clinical guidelines or dietitian recommendations to establish practical effectiveness.

## Confidence

- **High confidence**: The methodology of using different ML algorithms for different nutrient predictions based on their performance characteristics is sound and well-justified.
- **Medium confidence**: The application of XAI techniques (LIME and SHAP) for interpretability is appropriate, though the clinical utility of these explanations is not empirically validated.
- **Low confidence**: The claimed accuracy percentages for regression tasks appear questionable and may reflect non-standard evaluation metrics.

## Next Checks
1. Validate the regression accuracy metrics by examining whether they represent classification accuracy on discretized nutrient values or a non-standard metric definition.
2. Test the model's performance on an independent external dataset to assess generalizability beyond the 146-patient sample.
3. Conduct a clinical validation study comparing model recommendations against registered dietitian recommendations to establish practical effectiveness.