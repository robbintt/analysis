---
ver: rpa2
title: Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis
arxiv_id: '2307.04541'
source_url: https://arxiv.org/abs/2307.04541
tags:
- open
- feature
- space
- could
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work addresses the problem of open set recognition in medical
  diagnosis, where models must not only accurately classify known disease classes
  but also detect and flag unknown diseases for expert review. The authors propose
  Open Margin Cosine Loss (OMCL), which unifies two key mechanisms: Margin Loss with
  Adaptive Scale (MLAS) and Open-Space Suppression (OSS).'
---

# Learning Large Margin Sparse Embeddings for Open Set Medical Diagnosis

## Quick Facts
- arXiv ID: 2307.04541
- Source URL: https://arxiv.org/abs/2307.04541
- Reference count: 37
- Achieves 98.3% accuracy, 88.6% AUROC, and 88.0% OSCR on BloodMnist dataset

## Executive Summary
This work addresses open set recognition in medical diagnosis by proposing Open Margin Cosine Loss (OMCL), which unifies Margin Loss with Adaptive Scale (MLAS) and Open-Space Suppression (OSS). The method achieves state-of-the-art performance on two medical image datasets, enabling accurate classification of known disease classes while detecting unknown diseases for expert review. The approach combines angular margin for class separation with feature space descriptors to identify sparse regions as unknowns.

## Method Summary
OMCL combines MLAS and OSS to create a unified framework for open set medical diagnosis. MLAS introduces an angular margin to enforce intra-class compactness and inter-class separability, while OSS generates feature space descriptors to identify sparse regions as unknown classes. The method uses a ResNet18 backbone with L2 normalization, implementing the loss function with learnable scaling factors and M=64 feature space descriptors per batch. Training employs Adam optimizer with learning rate 1e-3 and batch size 64 for 200 epochs on BloodMnist and 100 epochs on OCTMnist.

## Key Results
- Achieves 98.3% accuracy, 88.6% AUROC, and 88.0% OSCR on BloodMnist
- Achieves 96.8% accuracy, 78.9% AUROC, and 77.8% OSCR on OCTMnist
- Outperforms state-of-the-art methods on both medical OSR benchmark datasets
- Demonstrates strong adaptability to different medical image modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparse regions in the embedding space can be reliably identified as unknowns
- Mechanism: OSS generates uniformly distributed descriptors across bounded feature space; sparsely populated regions have low angular similarity to known prototypes
- Core assumption: Known classes occupy compact, dense regions while unknowns fall into sparse, scattered areas
- Evidence anchors: Abstract mentions "classifying [descriptors] as the extra unknown class"; Section 2.3 describes OSS categorizing descriptors into C+1 class
- Break condition: If unknown classes occupy dense regions similar to known classes

### Mechanism 2
- Claim: Angular margin improves intra-class compactness and inter-class separability
- Mechanism: MLAS introduces angular margin m that increases decision boundary strictness, pushing samples further from class boundaries
- Core assumption: Angular separation is reliable metric for class discrimination in hyperspherical embedding space
- Evidence anchors: Abstract mentions "angular margin for reinforcing intra-class compactness"; Section 2.2 describes more stringent decision boundary
- Break condition: If angular margin creates overly rigid boundaries preventing proper generalization

### Mechanism 3
- Claim: Learnable scaling factor enhances generalization capacity
- Mechanism: S is made trainable with reduced learning rate (0.1× model learning rate), allowing adaptation to different datasets
- Core assumption: Fixed scaling factors are suboptimal across different datasets and domains
- Evidence anchors: Abstract mentions "adaptive scaling factor to strengthen generalization"; Section 2.2 describes learnable scaling factors; Section 3.3 demonstrates adaptive design effectiveness
- Break condition: If adaptive scaling factor converges to degenerate values or destabilizes training

## Foundational Learning

- Concept: Hyperspherical embedding space and angular similarity
  - Why needed here: OMCL operates in angular space where class prototypes are directions, making angular margin and cosine similarity natural metrics
  - Quick check question: Why does normalizing feature embeddings to unit vectors make angular similarity equivalent to cosine similarity?

- Concept: Open set recognition vs. anomaly detection
  - Why needed here: OSR requires both accurate classification of known classes AND detection of unknowns, unlike anomaly detection which only identifies outliers
  - Quick check question: What key difference between OSR and anomaly detection makes OSR more challenging in medical diagnosis?

- Concept: Feature space descriptors and pseudo-unknown generation
  - Why needed here: OSS needs to create representative samples of potential unknown regions without access to actual unknown data
  - Quick check question: How does uniformly sampling across bounded feature space create effective pseudo-unknown samples?

## Architecture Onboarding

- Component map: ResNet18 backbone → L2 normalization → angular margin layer (MLAS) → feature space descriptor layer (OSS) → classification head with C+1 outputs
- Critical path: Backbone → normalization → MLAS margin application → OSS descriptor integration → classification loss
- Design tradeoffs: Adding OSS increases training complexity but improves unknown detection; MLAS margin improves separation but may reduce flexibility
- Failure signatures: High false positive rate on unknowns suggests margin is too strict; poor known class accuracy suggests margin is too loose
- First 3 experiments:
  1. Train baseline without MLAS or OSS to establish performance floor
  2. Add MLAS only to test margin effectiveness while keeping OSS for later
  3. Add OSS only to test descriptor effectiveness while keeping standard softmax

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the methodology and results, several questions emerge regarding scalability and robustness:

1. How does performance scale with increasing number of known classes in medical image datasets?
2. What is the impact of feature space dimensionality on OSS effectiveness?
3. How does OMCL perform when unknown classes have feature distributions that overlap significantly with known classes?

## Limitations
- Strong assumptions about separability of known and unknown classes in angular space
- Performance depends heavily on hyperparameter settings requiring dataset-specific tuning
- Limited evaluation to only two medical imaging modalities (blood cells and OCT)

## Confidence

- **High Confidence**: Overall framework combining MLAS and OSS is well-structured; experimental results show consistent improvements across both benchmark datasets
- **Medium Confidence**: Mechanisms of angular margin and feature space descriptors are theoretically sound but lack direct corpus support for this exact formulation
- **Low Confidence**: Generalization to other medical imaging modalities beyond blood cells and OCT remains untested

## Next Checks

1. Test OMCL on additional medical imaging datasets (e.g., X-ray, MRI) with varying class distributions to assess generalization beyond the two benchmark datasets

2. Systematically vary the angular margin (m), threshold (t), and scaling factor learning rate across a broader range to identify optimal settings and understand their impact on different medical domains

3. Analyze false positive and false negative cases to understand when and why the model fails to correctly identify unknown classes, particularly in scenarios where unknown classes have feature distributions similar to known classes