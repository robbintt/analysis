---
ver: rpa2
title: Large Language Models Are Not Strong Abstract Reasoners
arxiv_id: '2305.19555'
source_url: https://arxiv.org/abs/2305.19555
tags:
- answer
- reasoning
- dataset
- language
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) on abstract reasoning
  tasks, finding that despite strong performance on many NLP benchmarks, they perform
  poorly on abstract reasoning problems requiring pattern recognition and generalization
  from limited examples. The authors introduce a new benchmark comprising open-ended
  and multiple-choice question answering datasets adapted from visual reasoning tasks.
---

# Large Language Models Are Not Strong Abstract Reasoners

## Quick Facts
- **arXiv ID:** 2305.19555
- **Source URL:** https://arxiv.org/abs/2305.19555
- **Reference count:** 40
- **Primary result:** Large language models perform poorly on abstract reasoning tasks despite strong NLP benchmark performance

## Executive Summary
This paper evaluates state-of-the-art large language models on abstract reasoning tasks, finding that despite their strong performance on many NLP benchmarks, they fail consistently on tasks requiring pattern recognition and generalization from limited examples. The authors introduce a comprehensive benchmark adapted from visual reasoning tasks and test models including GPT-2, GPT-3, GPT-4, LLaMA, and Alpaca. Even with techniques like chain-of-thought prompting and code generation, model performance remains low across all datasets. GPT-4 shows the best performance but still fails most tasks, suggesting fundamental limitations in current LLM architectures for abstract reasoning.

## Method Summary
The authors created a new benchmark comprising open-ended and multiple-choice question answering datasets adapted from visual reasoning tasks. They evaluated state-of-the-art LLMs (GPT-2, GPT-3, GPT-4, LLaMA, Alpaca) and fine-tuned QA models on these tasks, testing various techniques including chain-of-thought prompting and code generation. The evaluation used datasets such as ARC, ACRE, BIG-Bench-F, Evals, PVR, and RA VENT in both text and symbolic formats. Performance was measured using accuracy on the abstract reasoning tasks.

## Key Results
- LLMs show consistently low performance across all abstract reasoning datasets
- GPT-4 performs best but still fails most tasks
- Chain-of-thought prompting and code generation techniques do not significantly improve performance
- Models struggle with pattern recognition and generalization from limited examples
- Performance does not improve with fine-tuning on similar tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail abstract reasoning because they rely on memorization and pattern matching rather than causal structure discovery
- Mechanism: When presented with abstract reasoning tasks, LLMs attempt to match the input to patterns seen during training rather than discovering the underlying causal mechanisms that generate the patterns
- Core assumption: The training data contains sufficient examples of the abstract patterns that would allow pattern matching to succeed
- Evidence anchors: [abstract] "The authors introduce a new benchmark comprising open-ended and multiple-choice question answering datasets adapted from visual reasoning tasks"; [section] "Our results indicate poor performance of language models on all the presented datasets"

### Mechanism 2
- Claim: LLMs lack the ability to perform multi-step reasoning with backtracking
- Mechanism: The autoregressive nature of LLMs prevents them from revising previous decisions when they encounter contradictions during reasoning, leading to incorrect conclusions
- Core assumption: The inability to backtrack is inherent to the autoregressive architecture
- Evidence anchors: [abstract] "GPT-4 also showed limitations in text generation under constraints; the model can handle local constraints but fails to apply global constraints that require thinking ahead"; [section] "GPT-4 also does not always reason in a consistent manner"

### Mechanism 3
- Claim: LLMs cannot effectively learn from limited examples
- Mechanism: When given only a few examples to infer a general rule, LLMs fail to abstract the underlying pattern and instead overfit to surface features of the specific examples
- Core assumption: The abstract reasoning tasks require genuine generalization from limited data
- Evidence anchors: [abstract] "Abstract reasoning is a fundamental task for cognition, consisting of finding and applying a general pattern from few data"; [section] "GPT-4 performs best but still fails most tasks"

## Foundational Learning

- **Concept: Causal reasoning**
  - Why needed here: Abstract reasoning fundamentally requires understanding causal relationships rather than just correlational patterns
  - Quick check question: Can you explain why a light turns on when certain objects are present without simply memorizing the pattern?

- **Concept: Program induction**
  - Why needed here: Abstract reasoning tasks often require discovering and applying algorithms that transform inputs to outputs
  - Quick check question: Given three input-output pairs, can you write a program that generalizes to new inputs?

- **Concept: Pattern abstraction**
  - Why needed here: Abstract reasoning requires extracting the essential features of a pattern while ignoring irrelevant details
  - Quick check question: Given examples of shapes with different sizes and colors following a pattern, can you identify the pattern without being distracted by the specific colors or sizes?

## Architecture Onboarding

- **Component map:** Input processing → Pattern recognition → Causal structure discovery → Output generation
- **Critical path:** The model must first recognize the task format, then identify the abstract pattern, discover the underlying causal mechanism, and finally apply it to generate the answer
- **Design tradeoffs:** Autoregressive generation enables fluent text but prevents backtracking; larger models may memorize more patterns but still fail at genuine abstraction
- **Failure signatures:** Correct answers on training data but poor generalization to test data; reliance on surface features rather than underlying structure; inability to explain reasoning steps
- **First 3 experiments:**
  1. Test model performance on abstract reasoning tasks with varying numbers of training examples
  2. Compare performance between natural language and symbolic representations of the same abstract patterns
  3. Evaluate whether prompting for code generation improves abstract reasoning performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications could improve LLMs' abstract reasoning capabilities?
- Basis in paper: [explicit] The authors suggest causal reasoning and program induction as potential avenues for improvement.
- Why unresolved: The paper identifies limitations but does not propose or test specific architectural changes.
- What evidence would resolve it: Experimental results showing improved performance on abstract reasoning tasks after implementing proposed architectural modifications.

### Open Question 2
- Question: How does the training data composition affect LLMs' ability to perform abstract reasoning?
- Basis in paper: [explicit] The authors hypothesize that GPT-4's better performance on RA VENT may be due to its training data including Raven test data.
- Why unresolved: The paper suggests a hypothesis but does not systematically investigate the relationship between training data and abstract reasoning performance.
- What evidence would resolve it: Comparative studies showing how different training data compositions affect abstract reasoning task performance.

### Open Question 3
- Question: What is the relationship between instruction-tuning methods and abstract reasoning performance?
- Basis in paper: [explicit] The paper notes that Text-Davinci-3 outperforms GPT-3.5-Turbo despite both being instruction-tuned, suggesting the type of instruction-tuning matters.
- Why unresolved: The paper observes differences but does not investigate the specific mechanisms or optimal instruction-tuning approaches for abstract reasoning.
- What evidence would resolve it: Controlled experiments comparing various instruction-tuning techniques and their impact on abstract reasoning benchmarks.

## Limitations

- The benchmark focuses on pattern recognition and rule inference from visual reasoning tasks, which may not capture all forms of abstract reasoning
- Text-based representations of visual reasoning problems may introduce artifacts that affect model performance independently of their abstract reasoning capabilities
- The evaluation methodology relies on the assumption that poor performance indicates fundamental limitations rather than format mismatch between training data and evaluation tasks

## Confidence

- **High Confidence:** The experimental results showing consistently poor performance of state-of-the-art LLMs on the benchmark tasks
- **Medium Confidence:** The interpretation that poor performance indicates fundamental limitations in abstract reasoning capabilities
- **Low Confidence:** The claim that these limitations are inherent to the autoregressive architecture and cannot be overcome through scaling or fine-tuning

## Next Checks

1. **Cross-format validation:** Evaluate model performance on the same abstract reasoning problems presented in their original visual format versus the adapted text format to isolate format-specific effects

2. **Incremental complexity analysis:** Systematically vary the complexity of abstract reasoning tasks (number of examples, complexity of rules, abstractness of patterns) to identify specific breakpoints in model performance

3. **Architectural intervention test:** Implement and evaluate a modified architecture that incorporates explicit backtracking or revision mechanisms to test whether the autoregressive constraint is indeed the limiting factor