---
ver: rpa2
title: Don't Take This Out of Context! On the Need for Contextual Models and Evaluations
  for Stylistic Rewriting
arxiv_id: '2305.14755'
source_url: https://arxiv.org/abs/2305.14755
tags:
- contextual
- context
- non-contextual
- rewrites
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights the critical role of context in stylistic
  text rewriting and evaluation. The authors develop a contextual rewriting technique
  leveraging large language models' in-context learning abilities, and introduce a
  novel composite evaluation metric, CtxSimFit, that combines sentence-level similarity
  with contextual cohesiveness.
---

# Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting

## Quick Facts
- **arXiv ID:** 2305.14755
- **Source URL:** https://arxiv.org/abs/2305.14755
- **Authors:** [List not provided]
- **Reference count:** 18
- **Key outcome:** Humans significantly prefer contextual rewrites for naturalness and fit, yet existing sentence-level metrics poorly align with these preferences.

## Executive Summary
This paper demonstrates that context is essential for effective stylistic text rewriting and evaluation. The authors develop a contextual rewriting approach using large language models' in-context learning capabilities and introduce a novel composite evaluation metric, CtxSimFit, that combines sentence-level similarity with contextual cohesiveness. Through comparative evaluation across formality, toxicity, and sentiment transfer tasks, the study reveals that humans strongly prefer contextual rewrites, while standard sentence-level automatic metrics fail to capture these preferences. Context-infused versions of common metrics show much stronger correlation with human judgments, highlighting the importance of integrating context into both generation and evaluation stages.

## Method Summary
The authors collected parallel datasets with context for formality, sentiment, and toxicity transfer tasks using Reddit conversations, news articles, and other sources. They generated rewrites using few-shot prompting with GPT-3.5 (2 examples) and GPT-NeoX (10 examples) for contextual, non-contextual, and random-context conditions. Evaluation combined human annotations (head-to-head comparisons) with automatic metrics, including both standard sentence-level metrics and context-infused versions that prepend context to the original sentence before comparison.

## Key Results
- Humans significantly prefer contextual rewrites over non-contextual ones for naturalness and fit
- Existing sentence-level metrics (ROUGE, BERTScore, SBERT) poorly correlate with human preferences
- Context-infused metrics and the new CtxSimFit metric show much stronger correlation with human judgments (ρ=0.7-0.9)
- Contextual rewriting effectively uses context to improve semantic relevance while maintaining style transfer quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextual rewriting improves semantic relevance by leveraging preceding text to guide style transfer.
- Mechanism: The model conditions rewrite generation on both the target style and the preceding context, allowing it to maintain topic coherence and preserve speaker intent.
- Core assumption: The original sentence's intended meaning is partially encoded in the preceding context.
- Evidence anchors:
  - [abstract] "humans significantly prefer contextual rewrites as more fitting and natural"
  - [section 4.1] "contextual rewriting model that utilizes the in-context learning capabilities of LLMs"
  - [corpus] Weak: only general thematic matches, no explicit contextual rewriting papers cited
- Break condition: If the preceding context is unrelated or ambiguous, the model may generate incoherent rewrites.

### Mechanism 2
- Claim: Context-infused automatic metrics better align with human preferences by measuring rewrite quality relative to both the original sentence and its context.
- Mechanism: Metrics like ROUGECtx, BERTScoreCtx, and PPLCtx compare the rewrite to the original sentence prepended with context, capturing semantic coherence beyond lexical overlap.
- Core assumption: The meaning of a sentence is not fully recoverable from the sentence alone; context is essential for deriving intended meaning.
- Evidence anchors:
  - [abstract] "human preferences are much better reflected by...context-infused versions of common metrics"
  - [section 7.1] "we alter existing meaning similarity measures by prepending the context C to the original input sentence I before comparing it to the rewrite X"
  - [corpus] Weak: related works focus on dialog evaluation, not stylistic rewriting
- Break condition: If the context is noisy or irrelevant, context-infused metrics may penalize otherwise good rewrites.

### Mechanism 3
- Claim: Contextual human evaluation reveals limitations of non-contextual automatic metrics by capturing naturalness and fit judgments.
- Mechanism: Annotators rank rewrites in the presence of preceding context, forcing them to assess coherence and relevance rather than isolated sentence quality.
- Core assumption: Real-world rewriting tasks always involve context, so evaluation should reflect that.
- Evidence anchors:
  - [abstract] "humans significantly prefer contextual rewrites...yet existing sentence-level automatic metrics (e.g., ROUGE, SBERT) correlate poorly with human preferences"
  - [section 5.1] "participants are presented with the preceding context, pairs of rewritten sentences...as well as the desired style attribute"
  - [corpus] Weak: no specific papers on contextual human evaluation for stylistic rewriting
- Break condition: If annotators are not properly trained or the task instructions are unclear, evaluations may be inconsistent.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Enables few-shot stylistic rewriting without retraining large models
  - Quick check question: What distinguishes in-context learning from fine-tuning?

- Concept: Semantic similarity metrics
  - Why needed here: Used to evaluate meaning preservation in non-contextual rewrites
  - Quick check question: How does SBERT differ from BLEURT in measuring semantic similarity?

- Concept: Perplexity as fluency measure
  - Why needed here: Assesses grammaticality and coherence of generated rewrites
  - Quick check question: Why might low perplexity not guarantee high rewrite quality?

## Architecture Onboarding

- Component map: Data pipeline -> LLM interface -> Evaluation module -> Context injection
- Critical path:
  1. Load dataset and split into context and target sentence
  2. Generate non-contextual and contextual rewrites via few-shot prompting
  3. Compute non-contextual metrics (ROUGE, BERTScore, etc.)
  4. Compute contextual metrics (ROUGECtx, BERTScoreCtx, etc.)
  5. Run human evaluation with context present
  6. Compare metric correlations with human preferences
- Design tradeoffs:
  - Using GPT-3.5 vs GPT-NeoX: trade-off between cost and performance
  - Number of few-shot examples: affects prompt engineering complexity vs. rewrite quality
  - Context length: longer context may improve coherence but increase computational cost
- Failure signatures:
  - Non-contextual metrics disagree with human preferences → context is being ignored
  - Contextual rewrites score worse on non-contextual metrics → model is effectively using context
  - Human agreement is low → unclear task instructions or ambiguous rewrites
- First 3 experiments:
  1. Compare non-contextual vs contextual rewrites on formality task using ROUGE and human evaluation
  2. Test context-infused metrics (ROUGECtx, BERTScoreCtx) vs standard metrics
  3. Validate that random context produces worse rewrites and lower metric scores than true context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much context is optimal for stylistic rewriting performance across different tasks?
- Basis in paper: [inferred] The paper notes this as a limitation, stating "Understanding the specific impact of incorporating contextual information in our model would require an ablation analysis to determine the optimal amount of context needed."
- Why unresolved: The authors did not explore varying amounts of context systematically, leaving unclear whether two sentences, three sentences, or more provide the best balance between computational efficiency and rewrite quality.
- What evidence would resolve it: An ablation study varying context window sizes (e.g., 0, 1, 2, 3 preceding sentences/turns) across all three tasks with systematic evaluation using both human and automatic metrics.

### Open Question 2
- Question: Can contextual meaning preservation metrics be developed that better capture speaker intent, tone, and implied meaning beyond surface-level semantic similarity?
- Basis in paper: [explicit] The authors note that "non-contextual proxies for meaning preservation will always be in tension with any stylistic change to the sentence" and advocate for "more research into better modeling of communicative intents or goals."
- Why unresolved: Current contextual metrics (CtxSimFit and context-infused versions of common metrics) still rely on surface-level similarity and coherence measures, which don't fully capture deeper aspects of meaning preservation like speaker intention and implied meaning.
- What evidence would resolve it: Development and validation of metrics that incorporate communicative intent modeling, such as those that track topic coherence, emotional trajectory, or power dynamics between speakers across the context.

### Open Question 3
- Question: How can automatic evaluation be designed to minimize annotator exposure to toxic content while maintaining evaluation quality?
- Basis in paper: [explicit] The authors identify this as an ethical consideration, noting "we exposed human annotators to toxic content during the evaluation of the de-toxification task. Exposure to such offensive content can be harmful to the annotators."
- Why unresolved: The paper doesn't propose solutions for this challenge, which is particularly relevant for tasks involving offensive content evaluation.
- What evidence would resolve it: Empirical comparison of different evaluation strategies (e.g., synthetic toxicity data, toxicity summarization, or blind evaluation protocols) that reduce direct exposure while maintaining reliable quality assessment.

## Limitations
- Evaluation focuses on relatively short contexts (single preceding sentence), limiting conclusions about longer discourse spans
- Few-shot prompting examples are not fully specified, making exact reproduction challenging
- The study uses a limited set of style transfer tasks, potentially missing domain-specific challenges
- Human evaluation relies on crowdsourced workers whose expertise and consistency may vary

## Confidence

**High Confidence:** The finding that humans significantly prefer contextual rewrites for naturalness and fit (supported by direct human preference measurements with strong statistical backing).

**Medium Confidence:** The claim that existing sentence-level metrics poorly align with human preferences (supported by correlation analysis, though the gap may vary across different tasks and datasets).

**Low Confidence:** The assertion that context-infused metrics universally provide better correlation with human preferences across all style transfer domains (limited to three specific tasks tested, with some metrics showing stronger improvements than others).

## Next Checks

1. **Context Length Sensitivity Test:** Systematically vary context length (1-3 preceding sentences) to identify the optimal context window for different style transfer tasks and determine if improvements scale with longer contexts.

2. **Cross-Domain Generalization:** Apply the contextual rewriting and evaluation framework to additional style transfer domains (e.g., humor-to-serious, formal-to-technical) to verify whether the observed advantages generalize beyond the three tested tasks.

3. **Metric Robustness Analysis:** Test context-infused metrics against adversarial contexts (e.g., irrelevant, contradictory, or noisy preceding text) to quantify their sensitivity to context quality and establish when non-contextual metrics might be preferable.