---
ver: rpa2
title: 'CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail
  Models for Virtual Assistants'
arxiv_id: '2304.14364'
source_url: https://arxiv.org/abs/2304.14364
tags:
- rule
- gpt-4
- scenarios
- contrastive
- conversations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CONSCENDI, a contrastive and scenario-guided
  distillation approach to guardrail models for virtual assistants. The authors propose
  using large language models to generate diverse rule-violating and non-violating
  conversations, guided by rule-breaking scenarios and contrastive examples.
---

# CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants

## Quick Facts
- arXiv ID: 2304.14364
- Source URL: https://arxiv.org/abs/2304.14364
- Reference count: 40
- One-line primary result: Achieves over 90% accuracy on in-distribution scenarios and 89% on out-of-distribution scenarios using distilled models

## Executive Summary
This paper introduces CONSCENDI, a novel approach to creating guardrail models for virtual assistants that ensures adherence to task-specific rules. The method uses large language models to generate diverse rule-violating and non-violating conversations, guided by rule-breaking scenarios and contrastive examples. By fine-tuning smaller GPT-3 models on this generated data, the authors create effective guardrail models that outperform baseline approaches. The approach demonstrates the potential of distillation for deploying effective guardrails in real-world virtual assistants while being more cost-effective than using GPT-4 directly.

## Method Summary
CONSCENDI uses GPT-4 to generate synthetic training data for guardrail models, focusing on rule-violating and non-violating conversations guided by scenarios and contrastive examples. The process involves generating rule-breaking scenarios to ensure diverse coverage, then creating violations and their contrastive non-violations. A smaller GPT-3 model is fine-tuned on this data to serve as the guardrail model. This distillation approach aims to capture the nuanced rule interpretations from GPT-4 while being more efficient for deployment.

## Key Results
- Achieves over 90% accuracy on in-distribution scenarios
- Achieves 89% accuracy on out-of-distribution scenarios
- Outperforms baseline approaches including prompt-based GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Scenario-guided generation produces more diverse training data than direct conversation generation.
- **Mechanism:** By first generating multiple high-level scenarios for each rule, the approach ensures conversations cover a broader range of rule-violating possibilities.
- **Core assumption:** GPT-4 can generate diverse and realistic scenarios that effectively capture different ways rules can be violated.
- **Evidence anchors:**
  - [abstract] "We generate a set of rule-breaking scenarios, which enumerate a diverse set of high-level ways a rule can be violated. This scenario-guided approach produces a diverse training set..."
  - [section] "We use scenarios to ensure that the generated conversations will cover a broad set of possibilities, including edge cases."
- **Break condition:** If GPT-4 cannot generate diverse scenarios or the scenarios do not effectively capture rule violations, the diversity benefit would be lost.

### Mechanism 2
- **Claim:** Contrastive examples improve the model's ability to distinguish between acceptable and unacceptable responses.
- **Mechanism:** By generating non-violating conversations that are alterations of violating conversations, the model learns finer-grained distinctions.
- **Core assumption:** Learning from contrastive pairs (violation vs. near-identical non-violation) helps the model understand nuanced boundaries of rule compliance.
- **Evidence anchors:**
  - [abstract] "We also prompt GPT-4 to also generate contrastive examples by altering conversations with violations into acceptable conversations..."
  - [section] "By using this contrastive learning approach...we aim to generate non-violations similar to violations..."
- **Break condition:** If the contrastive examples are not sufficiently similar to violations or if the differences are too obvious, the model may not learn the intended fine-grained distinctions.

### Mechanism 3
- **Claim:** Distilling knowledge from GPT-4 into smaller GPT-3 models achieves better performance than using GPT-4 directly as a guardrail.
- **Mechanism:** The smaller models, trained on carefully generated data, can learn the intended interpretations of guardrail rules more effectively than relying on prompt-based approaches.
- **Core assumption:** A fine-tuned smaller model can capture the nuances of rule compliance better than a larger model relying on prompt instructions.
- **Evidence anchors:**
  - [abstract] "Our distillation approach produces fine-tuned models that can identify rule violations with high accuracy better than GPT-4..."
  - [section] "We propose that distilling a smaller model from GPT-4 provides better guardrails..."
- **Break condition:** If the distilled model cannot generalize well to unseen scenarios or if the data generation process is flawed, the performance advantage over GPT-4 may not materialize.

## Foundational Learning

- **Concept:** Contrastive learning
  - **Why needed here:** To teach the model fine-grained distinctions between acceptable and unacceptable responses by learning from pairs of similar examples with opposite labels.
  - **Quick check question:** How does contrastive learning differ from traditional supervised learning, and why is it particularly useful for guardrail tasks?

- **Concept:** Knowledge distillation
  - **Why needed here:** To transfer the generative capabilities of a large language model (GPT-4) into a smaller, more efficient model that can be deployed in real-world applications.
  - **Quick check question:** What are the key differences between knowledge distillation and traditional model compression techniques?

- **Concept:** Scenario-based data generation
  - **Why needed here:** To ensure the training data covers a diverse range of rule-violating situations, including edge cases that might be missed by direct conversation generation.
  - **Quick check question:** How does generating scenarios before conversations help in creating a more comprehensive dataset compared to directly generating conversations?

## Architecture Onboarding

- **Component map:** GPT-4 (Teacher) -> GPT-3 models (Student) -> Amazon Mechanical Turk (Verifier) -> OpenAI API (Interface)

- **Critical path:**
  1. Generate scenarios for each rule using GPT-3.5-Turbo
  2. Generate violations using GPT-4 with scenarios
  3. Generate contrastive non-violations by modifying violations
  4. Generate non-contrastive non-violations
  5. Fine-tune GPT-3 models on combined dataset
  6. Evaluate performance on in-distribution and out-of-distribution scenarios

- **Design tradeoffs:**
  - Cost vs. Performance: Using GPT-4 for data generation is expensive but necessary for high-quality data; smaller GPT-3 models for inference are cheaper but require careful training
  - Diversity vs. Control: Scenario-guided generation provides control over rule coverage but may miss unexpected violation types
  - Contrastive vs. Non-contrastive: Contrastive examples improve fine-grained learning but require more complex data generation

- **Failure signatures:**
  - Low accuracy on out-of-distribution scenarios: Indicates poor generalization, possibly due to insufficient scenario diversity
  - High false positive rate: Model may be too conservative in labeling violations, possibly due to imbalanced training data
  - Low accuracy on contrastive examples: Suggests the model struggles with fine-grained distinctions, possibly due to insufficient contrastive training data

- **First 3 experiments:**
  1. Compare accuracy of fine-tuned models with and without contrastive examples to validate the importance of contrastive learning
  2. Test model performance on held-out scenarios to evaluate generalization capability
  3. Vary the number of scenarios per rule to find the optimal balance between diversity and data efficiency

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the performance of CONSCENDI change when applied to rules that require external API verification, such as "Do not provide incorrect restaurant names or opening times"?
  - **Basis in paper:** [inferred]
  - **Why unresolved:** The paper explicitly states that they did not investigate rules requiring API verification and leave this for future work.
  - **What evidence would resolve it:** Experiments applying CONSCENDI to a dataset of virtual assistant conversations that include rules requiring API verification, with performance metrics compared to human annotators.

- **Open Question 2:** How does the accuracy of CONSCENDI change when applied to multi-label violations, where a single conversation can violate multiple rules simultaneously?
  - **Basis in paper:** [inferred]
  - **Why unresolved:** The paper mentions that they designed non-overlapping rules for clean multi-class classification, but acknowledges this may be challenging in practice.
  - **What evidence would resolve it:** Experiments applying CONSCENDI to a dataset of virtual assistant conversations that include overlapping rules, with performance metrics compared to single-label scenarios.

- **Open Question 3:** How does the performance of CONSCENDI change when applied to domains outside of the SGD dataset, such as healthcare or finance?
  - **Basis in paper:** [inferred]
  - **Why unresolved:** The paper only evaluates CONSCENDI on three domains from the SGD dataset (flights, restaurants, buses).
  - **What evidence would resolve it:** Experiments applying CONSCENDI to datasets from different domains, such as the MultiWOZ dataset or task-oriented dialogue datasets in healthcare or finance.

## Limitations

- Reliance on GPT-4's generative capabilities: The approach depends heavily on GPT-4's ability to generate diverse scenarios and contrastive examples.
- Limited empirical validation: The claimed performance metrics need more rigorous validation with detailed results and statistical significance measures.
- Potential overfitting to GPT-4's style: Distilled models may overfit to GPT-4's specific interpretation of rule violations, limiting generalizability.

## Confidence

- **High confidence:** The general framework of using contrastive examples and scenario-guided generation for guardrail models is well-established in the literature.
- **Medium confidence:** The specific implementation details and claimed performance metrics, as the evidence from the corpus is weak and more rigorous validation is needed.
- **Low confidence:** The generalizability of the approach to other domains or model architectures, given the heavy reliance on GPT-4's capabilities.

## Next Checks

1. **Generalization test:** Evaluate the distilled models on a held-out dataset from a different domain (e.g., healthcare or finance) to assess their ability to generalize beyond the original training scenarios.

2. **Ablation study:** Conduct an ablation study to determine the relative importance of scenario-guided generation and contrastive examples by comparing models trained with and without these components.

3. **Cost-benefit analysis:** Perform a detailed cost-benefit analysis comparing the proposed distillation approach with direct GPT-4 usage, including factors such as inference time, API costs, and model maintenance.