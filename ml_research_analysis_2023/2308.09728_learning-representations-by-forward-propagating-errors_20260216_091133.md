---
ver: rpa2
title: Learning representations by forward-propagating errors
arxiv_id: '2308.09728'
source_url: https://arxiv.org/abs/2308.09728
tags:
- dual
- number
- ring
- learning
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel learning algorithm for neural networks
  based on forward-propagating errors using the concept of dual numbers from algebraic
  geometry. The key idea is to perform automatic differentiation through forward propagation
  rather than backpropagation, potentially offering a faster and more efficient learning
  method.
---

# Learning representations by forward-propagating errors

## Quick Facts
- arXiv ID: 2308.09728
- Source URL: https://arxiv.org/abs/2308.09728
- Reference count: 2
- Primary result: Introduces a novel learning algorithm using forward-propagating errors with dual numbers, potentially offering faster and more efficient learning than backpropagation for single-layer networks

## Executive Summary
This paper proposes a new learning algorithm for neural networks that performs automatic differentiation through forward propagation rather than backpropagation. The method leverages dual numbers from algebraic geometry, where the dual number system R[ε]/(ε²) encodes both function value and derivative in a single algebraic structure. For a single-layer perceptron with sigmoid activation, the paper demonstrates that gradient descent updates can be computed using dual numbers through a single forward pass. While the theoretical foundations are well-established for the single-layer case, extending the method to multilayer networks requires further algorithmic development.

## Method Summary
The method uses dual numbers to perform forward-time differentiation in neural networks. A dual number has the form a + bε where ε² = 0, allowing evaluation of f(x+ε) to yield both f(x) and f'(x)ε. This enables computing gradients through forward propagation by evaluating network outputs with dual numbers at each layer. The approach is demonstrated for a single-layer perceptron where the sigmoid activation function is expressed as a polynomial or approximated via Taylor series expansion. The dual part of the output naturally contains the gradient needed for weight updates, eliminating the need for a separate backward pass.

## Key Results
- Demonstrates forward-propagation learning using dual numbers for single-layer perceptron
- Shows that gradient descent updates can be computed through forward propagation instead of backpropagation
- Proves theoretical foundations for the dual number approach in neural network learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forward-propagation using dual numbers enables automatic differentiation without backpropagation
- Mechanism: The dual number system R[ε]/(ε²) encodes both function value and derivative in a single algebraic structure. When evaluating f(x+ε), the result separates into real part f(x) and dual part f'(x)ε, where the dual part naturally contains the derivative
- Core assumption: The activation function can be expressed as a polynomial or approximated via Taylor series expansion
- Evidence anchors:
  - [abstract] "using concept of dual number in algebraic geometry"
  - [section 2.1] "f(x + ε) = f(x) + f'(x)ε" and "This implies we can perform forward-time differentiation"
  - [corpus] Weak - no direct corpus evidence for dual number properties

### Mechanism 2
- Claim: Gradient descent updates can be computed through forward propagation instead of backpropagation
- Mechanism: By evaluating the network output with dual numbers at each layer, the error gradient flows forward through the network structure, accumulating partial derivatives automatically via the chain rule embedded in dual arithmetic
- Core assumption: The chain rule can be applied through dual number arithmetic to propagate gradients through multiple layers
- Evidence anchors:
  - [abstract] "potentially offering a faster and more efficient learning method"
  - [section 3] "In the dual number system, by denoting ŷε := f(x + ε), we can get same result with following rule"
  - [corpus] Weak - corpus focuses on GPU/CPU performance comparisons, not dual number mechanics

### Mechanism 3
- Claim: The method is computationally lighter than backpropagation for single-layer networks
- Mechanism: Forward propagation with dual numbers requires only one forward pass to compute both function value and gradient, whereas backpropagation requires forward pass plus backward pass through the network
- Core assumption: Dual number operations are computationally cheaper than maintaining and traversing gradient computation graphs
- Evidence anchors:
  - [abstract] "potentially offering a faster and more efficient learning method"
  - [section 4] "This new algorithm is fast, light and efficient compared to back-propagation"
  - [corpus] Moderate - neighbor papers discuss CPU vs GPU performance, supporting the premise that computational efficiency matters

## Foundational Learning

- Concept: Dual number algebra (R[ε]/(ε²) ring structure)
  - Why needed here: Forms the mathematical foundation for forward-time differentiation
  - Quick check question: What is the result of multiplying (a + bε) and (c + dε) in the dual number system?

- Concept: Taylor series expansion
  - Why needed here: Enables approximation of non-polynomial activation functions using dual numbers
  - Quick check question: How would you express sin(x+ε) using Taylor series expansion?

- Concept: Quotient ring construction
  - Why needed here: Creates the dual number system by constraining polynomial ring R[x] with ideal (x²)
  - Quick check question: What elements belong to R[x]/(x²) when R is the real numbers?

## Architecture Onboarding

- Component map: Input transformation to dual numbers -> Neural network forward pass -> Loss function evaluation -> Gradient extraction from dual part
- Critical path: Input transformation to dual numbers -> Forward propagation through network -> Loss computation -> Gradient extraction from ε component
- Design tradeoffs: Single forward pass vs. dual memory for dual number representation; limited to analytic functions vs. universal function approximation
- Failure signatures: NaN values in gradient extraction, incorrect gradient magnitudes, training instability
- First 3 experiments:
  1. Implement single-layer perceptron with sigmoid activation and verify gradient matches analytical derivative
  2. Test with ReLU activation and compare performance against backpropagation
  3. Scale to two-layer network and measure computational overhead vs backpropagation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the forward-propagation method using dual numbers provide computational advantages over backpropagation in practice, particularly for large-scale neural networks?
- Basis in paper: [explicit] The paper claims the method is "fast, light and efficient compared to back-propagation" but only demonstrates it for a single-layer perceptron and acknowledges that "the algorithmic details are omitted" for multilayer networks
- Why unresolved: The paper only provides theoretical foundations for a single-layer network and defers the algorithmic development for multilayer networks. No empirical performance comparisons are provided.
- What evidence would resolve it: Empirical benchmarks comparing computational efficiency (time, memory usage) of the forward-propagation method versus backpropagation on multilayer networks of varying depths and sizes.

### Open Question 2
- Question: How does the dual number system handle vanishing or exploding gradients in deep networks compared to standard backpropagation?
- Basis in paper: [inferred] The paper focuses on forward-propagation for gradient computation but doesn't address gradient stability issues that are well-known challenges in deep learning
- Why unresolved: The paper only considers single-layer networks and mentions that multilayer cases "seems to be solved if we consider chain rule" without providing details on how gradient issues are handled
- What evidence would resolve it: Analysis of gradient magnitudes throughout network depth when using the dual number approach, and comparison with standard backpropagation on networks known to exhibit gradient instability

### Open Question 3
- Question: What are the numerical stability properties of the dual number representation when performing gradient updates in neural networks?
- Basis in paper: [inferred] The paper introduces dual numbers for forward-time differentiation but doesn't discuss potential numerical precision issues or error propagation
- Why unresolved: The theoretical treatment assumes exact arithmetic with dual numbers but doesn't address practical implementation concerns with floating-point arithmetic
- What evidence would resolve it: Numerical analysis of error accumulation in gradient computations using dual numbers, and comparison of convergence behavior with standard backpropagation under various precision settings

## Limitations
- The method's extension to multilayer networks remains largely theoretical without algorithmic details for handling the chain rule through multiple layers
- Computational efficiency claims lack empirical validation, particularly for deep networks where backpropagation optimizations are well-established
- The approach is restricted to analytic functions that can be expressed via Taylor series, limiting its applicability to networks with piecewise linear activations

## Confidence
- Single-layer network mechanism: **High** - The mathematical foundations are clearly established with explicit derivations
- Computational efficiency claims: **Medium** - Theoretical argument is reasonable but lacks empirical validation
- Multilayer network extension: **Low** - Conceptual framework exists but algorithmic implementation is unspecified

## Next Checks
1. Implement and benchmark the dual-number method against standard backpropagation on a 2-3 layer network to measure actual computational overhead
2. Test the method with non-analytic activation functions (ReLU, piecewise linear) to identify practical limitations
3. Profile memory usage and operation count for the dual-number approach versus optimized backpropagation on modern hardware