---
ver: rpa2
title: 'KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with Inverse
  Transformation'
arxiv_id: '2309.14770'
source_url: https://arxiv.org/abs/2309.14770
tags:
- knowledge
- entity
- methods
- graph
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes KERMIT, a method for knowledge graph completion
  (KGC) that addresses two key limitations in text-based approaches: mismatched descriptions
  and pseudo-inverse relations. The method employs large language models (LLMs) like
  ChatGPT to generate semantically coherent descriptions for triples, bridging the
  gap between queries and answers.'
---

# KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with Inverse Transformation

## Quick Facts
- arXiv ID: 2309.14770
- Source URL: https://arxiv.org/abs/2309.14770
- Reference count: 15
- Key outcome: Achieves state-of-the-art results on WN18RR with 4.2% improvement in Hit@1 and maintains lead over text-based methods on FB15k-237

## Executive Summary
This paper proposes KERMIT, a method for knowledge graph completion (KGC) that addresses two key limitations in text-based approaches: mismatched descriptions and pseudo-inverse relations. The method employs large language models (LLMs) like ChatGPT to generate semantically coherent descriptions for triples, bridging the gap between queries and answers. Additionally, it augments the knowledge graph with manually-curated inverse relations, transforming it into a symmetric graph and providing supplementary training samples. These enhancements are integrated into a contrastive learning framework, improving the performance of the baseline SimKGC model.

## Method Summary
KERMIT addresses knowledge graph completion by generating predictive descriptions via ChatGPT for triples and augmenting the graph with manually curated inverse relations. The method uses two BERT encoders with shared weights to encode head-relation (hr) and tail (t) sequences, incorporating segment embeddings for three sequence types. It employs mean pooling instead of CLS pooling and trains using InfoNCE loss with in-batch negative sampling. The approach creates a symmetric graph through inverse relations and integrates predictive descriptions to resolve semantic gaps between queries and answers.

## Key Results
- Achieves state-of-the-art results on WN18RR with 4.2% improvement in Hit@1
- Maintains significant lead over text-based methods on FB15k-237
- Shows improvement across multiple metrics (MRR, Hit@1, Hit@3, Hit@10) on both datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mismatched descriptions are resolved by generating predictive descriptions via ChatGPT that are semantically coherent with both head entity and relation.
- Mechanism: Large language models act as external knowledge bases to produce entity descriptions that bridge the semantic gap between the query (h, r) and the answer entity t.
- Core assumption: LLMs can generate contextually relevant and semantically coherent descriptions for unseen entity types based on prompt templates.
- Evidence anchors:
  - [abstract]: "we employ ChatGPT as an external knowledge base to generate coherent descriptions to bridge the semantic gap between the queries and answers."
  - [section]: "To overcome these challenges, we propose the augmentation of data through two additional mechanisms. Firstly, we employ ChatGPT as an external knowledge base to generate coherent descriptions to bridge the semantic gap between the queries and answers."
  - [corpus]: Weak; no direct neighbor evidence for ChatGPT-based augmentation in KGC. Assumption: LLM-generated descriptions improve semantic alignment.
- Break condition: If ChatGPT fails to generate coherent or contextually relevant descriptions for specific relations or entity types, the semantic gap remains unresolved.

### Mechanism 2
- Claim: Symmetric graph construction via manually curated inverse relations provides additional training signals and data augmentation.
- Mechanism: Each training triple (h, r, t) is paired with its inverse (t, r', h), doubling the effective training data and enforcing symmetry in relation modeling.
- Core assumption: Manually authored inverse relations accurately reflect semantic inverses and improve model generalization.
- Evidence anchors:
  - [abstract]: "we leverage inverse relations to create a symmetric graph, thereby creating extra labeling and providing supplementary information for link prediction."
  - [section]: "we utilize inverse relations to create a symmetric graph, thereby providing augmented training samples for KGC."
  - [corpus]: Weak; no neighbor studies confirm effect of inverse relations on text-based KGC performance. Assumption: inverse augmentation improves link prediction accuracy.
- Break condition: If inverse relations are incorrectly curated or do not capture true semantic inverses, model performance may degrade due to noisy supervision.

### Mechanism 3
- Claim: Contrastive learning with predictive descriptions improves alignment between query and entity embeddings.
- Mechanism: By incorporating predictive descriptions in the query encoder and using mean pooling instead of CLS pooling, the model learns more robust and discriminative embeddings for both queries and entities.
- Core assumption: Mean pooling yields better sentence representations than CLS pooling in the context of knowledge graph completion.
- Evidence anchors:
  - [section]: "Regarding the pooling method, we deviate from the prevalent use of the CLS token in most NLP tasks. Instead, we opt for mean pooling, as it has been demonstrated to yield superior sentence embeddings (Reimers and Gurevych, 2019)."
  - [section]: "SimKGC uses InfoNCE loss with additive margin to construct the training objective, which we adopt in our framework."
  - [corpus]: No direct neighbor evidence for mean pooling superiority in KGC context. Assumption: mean pooling enhances semantic representation quality.
- Break condition: If the improved pooling strategy does not yield better alignment in the contrastive objective, the benefit may not materialize.

## Foundational Learning

- Concept: Knowledge graph completion as entity prediction using textual descriptions.
  - Why needed here: The method relies on encoding textual information of entities and relations into embeddings for link prediction, which is central to the text-based KGC paradigm.
  - Quick check question: How does SimKGC encode a triple (h, r, t) into embeddings for prediction?

- Concept: Contrastive learning with InfoNCE loss for representation learning.
  - Why needed here: The training objective uses InfoNCE loss to maximize similarity between correct query-entity pairs and minimize similarity for negative pairs, which is essential for the method's performance gains.
  - Quick check question: What is the role of negative sampling in the InfoNCE loss function?

- Concept: Use of external knowledge sources (LLMs) for data augmentation.
  - Why needed here: ChatGPT is used to generate predictive descriptions that resolve the "mismatched description" problem by providing semantically coherent text for tail entities.
  - Quick check question: How does the prompt template guide ChatGPT to generate a contextually relevant description for the tail entity?

## Architecture Onboarding

- Component map: BERT encoders (hr_encoder, t_encoder) with shared weights -> Segment embeddings (3 types) -> Modified tokenization -> Mean pooling -> InfoNCE loss -> Similarity computation

- Critical path: Generate predictive descriptions via ChatGPT -> Tokenize and encode hr and t sequences -> Compute embeddings using mean pooling -> Calculate cosine similarity and apply InfoNCE loss -> Precompute entity embeddings and rank candidates during inference

- Design tradeoffs:
  - Using ChatGPT adds latency and dependency on external API but improves description quality
  - Manual inverse relation curation ensures quality but limits scalability
  - Mean pooling improves representation but may lose positional information compared to CLS

- Failure signatures:
  - Low Hit@1 indicates poor semantic alignment between hr and t embeddings
  - High variance in predictions suggests inconsistent predictive description quality
  - Degraded performance on 1-to-many relations suggests inverse relation limitations

- First 3 experiments:
  1. Ablation: Remove predictive descriptions and evaluate performance drop
  2. Ablation: Remove inverse relations and evaluate performance drop
  3. Ablation: Replace mean pooling with CLS pooling and compare metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of KERMIT scale when applied to larger knowledge graphs beyond WN18RR and FB15k-237?
- Basis in paper: [explicit] The authors mention that they only performed experiments on the two smaller datasets but are confident that the superior performance of their model will also be reflected in the larger dataset.
- Why unresolved: The authors did not conduct experiments on larger datasets, so the actual performance on larger knowledge graphs is unknown.
- What evidence would resolve it: Empirical results from applying KERMIT to larger knowledge graphs, such as DBpedia or YAGO, would provide concrete evidence of its scalability and performance.

### Open Question 2
- Question: Can the predictive descriptions generated by ChatGPT be further improved to handle the 1-to-many relations more accurately?
- Basis in paper: [explicit] The authors note that the prevalent nature of 1-to-many relations in the WN18RR and FB15k-237 datasets poses a challenge for ChatGPT to accurately predict the correct entity.
- Why unresolved: The authors acknowledge the challenge but do not provide a solution or further investigation into improving the accuracy of predictive descriptions for 1-to-many relations.
- What evidence would resolve it: Results from experiments that compare the accuracy of predictive descriptions generated by ChatGPT with and without additional fine-tuning or techniques specifically designed to handle 1-to-many relations would provide evidence of potential improvements.

### Open Question 3
- Question: How does the use of inverse relations affect the performance of KERMIT compared to other data augmentation strategies?
- Basis in paper: [explicit] The authors introduce inverse relations as a data augmentation strategy and observe a significant improvement in Hit@1 on the WN18RR dataset (2.4%) when inverse relations are used.
- Why unresolved: While the authors demonstrate the effectiveness of inverse relations, they do not compare it to other data augmentation strategies or provide a comprehensive analysis of its impact on different datasets and metrics.
- What evidence would resolve it: Results from experiments that compare the performance of KERMIT with and without inverse relations, as well as with other data augmentation strategies, would provide evidence of the relative effectiveness of inverse relations.

## Limitations

- Dependency on external LLM for generating predictive descriptions introduces potential bottlenecks in terms of API costs, latency, and scalability
- Manual inverse relation curation limits applicability to domains where such curation is feasible and does not address scalability concerns
- Performance improvements appear dataset-specific, with substantial gains on WN18RR but more modest improvements on FB15k-237

## Confidence

- High Confidence: The core mechanism of using inverse relations to create symmetric graphs and the integration of predictive descriptions into the contrastive learning framework are well-supported by the paper's results and methodology
- Medium Confidence: The assumption that ChatGPT can consistently generate semantically coherent descriptions for unseen entity types is plausible but not extensively validated across diverse entity types or relations
- Low Confidence: The claim that mean pooling is universally superior to CLS pooling for sentence embeddings in the context of knowledge graph completion lacks direct empirical support within the paper or from cited literature

## Next Checks

1. Evaluate the KERMIT method on additional KGC datasets (e.g., YAGO3-10, Nell-995) to assess whether the performance gains observed on WN18RR and FB15k-237 are consistent across different knowledge graph structures and entity distributions

2. Replace ChatGPT-generated descriptions with descriptions from alternative sources (e.g., entity linking tools, semantic similarity matching) and measure the impact on KGC performance to determine the necessity and added value of the LLM-based approach

3. Conduct a detailed analysis of the manually curated inverse relations to quantify the accuracy and completeness of the inverse mapping, identifying and measuring the impact of any incorrect or missing inverse relations on overall model performance