---
ver: rpa2
title: Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language
  Model Reasoning
arxiv_id: '2310.01446'
source_url: https://arxiv.org/abs/2310.01446
tags:
- answer
- years
- cost
- each
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces an Adaptive-Solver framework that dynamically
  selects problem-solving strategies for Large Language Models (LLMs) based on task
  complexity. The framework features two main modules: an evaluation module that assesses
  solution adequacy using answer consistency, and an adaptation module that adjusts
  the solving approach through three strategies - model adaptation (switching between
  different LLMs), prompting method adaptation (varying prompting techniques), and
  decomposition granularity adaptation (adjusting problem breakdown levels).'
---

# Adaptive-Solver Framework for Dynamic Strategy Selection in Large Language Model Reasoning

## Quick Facts
- arXiv ID: 2310.01446
- Source URL: https://arxiv.org/abs/2310.01446
- Reference count: 40
- Key outcome: Dynamic solver adaptation achieves up to 4.5% higher accuracy at same cost or 85% cost reduction while maintaining performance

## Executive Summary
This paper introduces an Adaptive-Solver framework that dynamically selects problem-solving strategies for Large Language Models (LLMs) based on task complexity. The framework features two main modules: an evaluation module that assesses solution adequacy using answer consistency, and an adaptation module that adjusts the solving approach through three strategies - model adaptation, prompting method adaptation, and decomposition granularity adaptation. The method demonstrates significant improvements across multiple reasoning tasks, achieving up to 4.5% higher accuracy compared to baselines at the same cost, or reducing API costs by up to 85% while maintaining original performance.

## Method Summary
The Adaptive-Solver framework dynamically selects problem-solving strategies for LLMs by first evaluating solution consistency through multi-round sampling, then adapting the solving approach when consistency falls below a threshold. The evaluation module generates multiple solution samples per solver and calculates consistency to assess solution reliability. The adaptation module switches between different solvers in a predetermined list, including model adaptation (GPT3.5* to GPT4), prompting method adaptation (CoT* to L2M*), and decomposition granularity adaptation (varying problem breakdown levels). The framework was evaluated across 8 datasets covering arithmetic, commonsense, and symbolic reasoning tasks.

## Key Results
- Up to 4.5% higher accuracy compared to baselines at the same computational cost
- Up to 85% reduction in API costs while maintaining original performance levels
- Model adaptation strategy alone achieves up to 50% cost reduction with superior performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic adaptation improves performance by matching solver complexity to problem difficulty
- Mechanism: The evaluation module assesses solution consistency, and if it falls below a threshold, the adaptation module switches to a more suitable solver (model, prompting method, or decomposition granularity). This prevents both under-solving simple problems and over-computing complex ones.
- Core assumption: Solution consistency is a reliable proxy for answer correctness and problem difficulty.
- Evidence anchors:
  - [abstract] "evaluation module assesses the reliability of the current solution using answer consistency"
  - [section] "We adopt a straightforward approach to implement solver adaptation, which involves designing a list of solvers and switching to the next solver in the list from the current one when adaptation is needed."
- Break condition: If consistency metric is not well-correlated with actual accuracy, the evaluation module may trigger unnecessary adaptations or miss needed ones.

### Mechanism 2
- Claim: Combining multiple adaptation strategies yields synergistic improvements
- Mechanism: The framework allows stacking adaptations - model adaptation for capability, prompting method adaptation for problem-type matching, and decomposition granularity adaptation for optimal problem breakdown. The combination (AS-PD) outperforms individual strategies.
- Core assumption: Different adaptation strategies address orthogonal aspects of problem-solving, and their benefits compound.
- Evidence anchors:
  - [abstract] "We explore the synergistic effects of their combined application"
  - [section] "Furthermore, the model adaptation approach significantly reduces API costs (up to 50%) while maintaining superior performance."
- Break condition: If adaptation strategies interfere with each other or if the problem space doesn't benefit from multiple orthogonal adaptations.

### Mechanism 3
- Claim: Cost-performance trade-off can be dynamically optimized through selective solver usage
- Mechanism: Model adaptation strategy uses cheaper models for simple problems and only invokes expensive models like GPT-4 when necessary, achieving up to 85% cost reduction while maintaining performance.
- Core assumption: Problem complexity correlates with which model is needed for optimal performance.
- Evidence anchors:
  - [abstract] "The framework features two main modules: an evaluation module that assesses solution adequacy using answer consistency, and an adaptation module that adjusts the solving approach"
  - [section] "Model Adaptation: Shifting to a more powerful, albeit resource-intensive, LLM when necessary"
- Break condition: If the cost difference between models doesn't justify the performance gains for the problem types being solved.

## Foundational Learning

- Concept: Consistency-based evaluation metric
  - Why needed here: Serves as the decision criterion for when to trigger adaptation, replacing manual or heuristic triggers
  - Quick check question: What is the formula for calculating consistency in this framework, and what threshold is typically used?

- Concept: Solver composition and adaptation lists
  - Why needed here: Understanding how different solvers are combined and ordered in adaptation lists is crucial for implementing and customizing the framework
  - Quick check question: How does the framework represent a solver, and what are the three types of adaptation strategies implemented?

- Concept: Multi-round solving with self-consistency
  - Why needed here: The framework uses multiple solution samples and self-consistency to improve reliability before triggering adaptation
  - Quick check question: How many samples are generated per solver in the evaluation module, and how is consistency calculated?

## Architecture Onboarding

- Component map: Problem input → Evaluation module → (If needed) Adaptation module → New solver execution → Consistency check → Output or repeat
- Critical path: Problem input → Evaluation → (If needed) Adaptation → New solver execution → Consistency check → Output or repeat
- Design tradeoffs: Single-round vs multi-round solving, simple vs complex consistency metrics, fixed vs dynamic thresholds
- Failure signatures: Inconsistent evaluation triggers, solver list exhaustion without success, API cost overruns, accuracy degradation
- First 3 experiments:
  1. Implement evaluation module with dummy solvers to verify consistency calculation and threshold triggering
  2. Test single adaptation strategy (e.g., prompting method adaptation) on a simple dataset to validate adaptation logic
  3. Run end-to-end pipeline on GSM8K with all three adaptation strategies to measure performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Adaptive-Solver framework perform on multimodal reasoning tasks that require both text and image understanding?
- Basis in paper: [inferred] The paper focuses exclusively on text-based reasoning tasks across arithmetic, commonsense, and symbolic domains, with no evaluation on multimodal reasoning
- Why unresolved: The framework's ability to handle visual information and integrate it with text-based reasoning strategies has not been tested or validated
- What evidence would resolve it: Experiments showing performance comparisons on multimodal reasoning benchmarks (like VQA or visual math problems) would demonstrate the framework's generalizability to visual-textual reasoning

### Open Question 2
- Question: What is the optimal threshold θ for consistency-based evaluation across different problem types and model sizes?
- Basis in paper: [explicit] The paper sets θ=1 as default but mentions that "When we pick an appropriate sample size and threshold, we can attain commendable performance at a comparatively modest expense"
- Why unresolved: The paper only experiments with a single threshold value, leaving open the question of whether task-specific or model-specific thresholds could improve performance
- What evidence would resolve it: A systematic study varying θ across different problem types, model sizes, and reasoning domains to identify optimal threshold configurations

### Open Question 3
- Question: How does the Adaptive-Solver framework scale to extremely large problems with hundreds of interdependent reasoning steps?
- Basis in paper: [inferred] The experiments use datasets with relatively simple problems (average 15-50 words per question) and do not test on problems requiring extensive step-by-step reasoning
- Why unresolved: The paper does not address the framework's performance on complex, long-horizon reasoning tasks where error accumulation could be significant
- What evidence would resolve it: Experiments on benchmark datasets with complex, multi-step reasoning requirements (like MATH or APPS problems) would reveal scalability limitations and potential adaptation needs

### Open Question 4
- Question: What is the computational overhead of the Adaptive-Solver framework compared to static solvers when solving large batches of problems?
- Basis in paper: [explicit] The paper mentions cost savings but only reports API costs for individual problem solving, not for batch processing scenarios
- Why unresolved: The framework's efficiency gains are shown for individual problems, but batch processing overhead (evaluation module calls, multiple solver attempts) is not quantified
- What evidence would resolve it: Benchmarking the framework's runtime and cost per problem when processing thousands of problems compared to static solvers would reveal practical deployment considerations

## Limitations
- The evaluation module's consistency metric reliability as a proxy for solution correctness remains uncertain across diverse problem types
- The framework assumes predetermined solver adaptation lists can be optimal, but task-specific tuning may be required
- Cost-performance claims rely on accurate prediction of problem complexity from consistency scores, which may not generalize

## Confidence
- High confidence: The framework architecture and basic evaluation-adaptation loop are clearly specified and reproducible
- Medium confidence: The synergistic effects of combining multiple adaptation strategies are demonstrated but not deeply explained mechanistically
- Low confidence: The reliability of consistency-based evaluation as a trigger for adaptation across diverse problem types

## Next Checks
1. Cross-dataset consistency validation: Test the evaluation module's consistency metric on datasets outside the training distribution (e.g., MATH problems or code generation tasks) to verify if consistency thresholds generalize or require task-specific tuning.

2. Ablation of consistency correlation: Measure the actual correlation between the evaluation module's consistency scores and ground truth accuracy across all adaptation strategies to validate whether consistency is a reliable trigger signal.

3. Solver list optimization analysis: Systematically vary the ordering and composition of solver adaptation lists for different problem types to determine if the current predetermined lists are optimal or if data-driven optimization is needed.