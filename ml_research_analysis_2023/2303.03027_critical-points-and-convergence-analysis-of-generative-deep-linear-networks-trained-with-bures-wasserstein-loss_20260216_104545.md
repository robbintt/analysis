---
ver: rpa2
title: Critical Points and Convergence Analysis of Generative Deep Linear Networks
  Trained with Bures-Wasserstein Loss
arxiv_id: '2303.03027'
source_url: https://arxiv.org/abs/2303.03027
tags:
- loss
- linear
- gradient
- networks
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the optimization of generative deep linear networks
  trained with the Bures-Wasserstein loss, which is the 2-Wasserstein distance between
  centered Gaussian distributions. The authors characterize the critical points and
  minimizers of the Bures-Wasserstein distance over rank-constrained matrices, and
  establish convergence results for gradient flow and gradient descent under certain
  assumptions on the initial weights.
---

# Critical Points and Convergence Analysis of Generative Deep Linear Networks Trained with Bures-Wasserstein Loss

## Quick Facts
- arXiv ID: 2303.03027
- Source URL: https://arxiv.org/abs/2303.03027
- Reference count: 40
- Primary result: Convergence analysis of generative deep linear networks using Bures-Wasserstein loss

## Executive Summary
This paper studies the optimization landscape of generative deep linear networks trained with the Bures-Wasserstein loss, which measures the 2-Wasserstein distance between centered Gaussian distributions. The authors characterize the critical points and minimizers of this loss over rank-constrained matrices and establish convergence results for both gradient flow and gradient descent. They show that under balanced initialization and a uniform margin deficiency condition, the network can converge to the global minimum of the loss. The analysis introduces a smooth perturbative version of the loss to handle potential singularities when the rank drops during training.

## Method Summary
The method involves training a deep linear network with identity activation to generate samples from a target Gaussian distribution using the Bures-Wasserstein loss. The authors use a smooth perturbative version of the loss by adding a small positive regularization term to the covariance matrix, enabling convergence analysis through standard optimization tools. They establish exponential convergence for gradient flow and convergence for gradient descent with finite step sizes under specific initialization conditions that ensure a uniform margin deficiency between the initial and target covariance matrices.

## Key Results
- Characterization of critical points and minimizers for Bures-Wasserstein distance over rank-constrained matrices
- Exponential convergence of gradient flow using smooth perturbed loss
- Convergence of gradient descent with finite step sizes under uniform margin deficiency condition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bures-Wasserstein distance creates well-behaved optimization landscape
- Mechanism: Closed-form expression for Gaussian distributions induces smooth metric on positive semi-definite matrices
- Core assumption: Network weights remain balanced throughout training
- Evidence anchors: Abstract, section 2, weak corpus evidence
- Break condition: Rank drop in end-to-end matrix

### Mechanism 2
- Claim: Smooth perturbative version enables convergence analysis
- Mechanism: Regularization term τIn makes loss differentiable everywhere
- Core assumption: τ small enough to not distort optimization but large enough for smoothness
- Evidence anchors: Section 3.2, section 5.1, weak corpus evidence
- Break condition: τ too large or too small

### Mechanism 3
- Claim: Uniform margin deficiency ensures convergence to global minimum
- Mechanism: Initial weights chosen so singular values remain bounded away from zero
- Core assumption: Initial weights can satisfy uniform margin deficiency condition
- Evidence anchors: Section 5, section 5.3, weak corpus evidence
- Break condition: Margin deficiency condition violated

## Foundational Learning

- Concept: Matrix calculus and differential geometry
  - Why needed here: Computing gradients/Hessians of matrix functions and understanding manifolds of fixed rank matrices
  - Quick check question: Can you compute the differential of the matrix square root function and explain its role in Bures-Wasserstein distance?

- Concept: Optimization on non-convex manifolds
  - Why needed here: Function space is non-convex subset of matrices requiring Riemannian optimization tools
  - Quick check question: What is the difference between critical point and local minimum on a manifold?

- Concept: Wasserstein distances and optimal transport
  - Why needed here: Bures-Wasserstein is 2-Wasserstein distance for Gaussian distributions
  - Quick check question: How does Kantorovich duality relate to Bures-Wasserstein distance for Gaussians?

## Architecture Onboarding

- Component map: Latent Gaussian N(0,I) -> Deep linear network (W1,...,WN) -> Generated Gaussian N(0,WW⊤) -> Bures-Wasserstein loss

- Critical path: 1) Initialize balanced weights with margin deficiency 2) Compute gradients using matrix calculus 3) Update weights via gradient flow/descent 4) Monitor rank preservation and convergence

- Design tradeoffs: Bures-Wasserstein vs square loss (different geometry), smooth perturbation vs original (easier analysis but introduces regularization), balanced vs unbalanced initialization (simplified analysis but limited expressiveness)

- Failure signatures: Rank drop in end-to-end matrix (stuck due to non-differentiability), slow convergence (margin condition violated or small learning rate), divergence (large learning rate or bad initialization)

- First 3 experiments: 1) Verify balancedness preservation for 2-layer network under gradient flow 2) Test convergence with smooth perturbation for small target covariance 3) Compare Bures-Wasserstein vs square loss dynamics on toy generative task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can uniform margin deficiency condition be relaxed for convergence guarantees?
- Basis in paper: Explicit - paper relies on this condition for convergence
- Why unresolved: Assumption constrains parametrization to full rank
- What evidence would resolve it: Convergence proof without this assumption or counterexample showing necessity

### Open Question 2
- Question: Do critical points in parameter space correspond to critical points in function space for original loss?
- Basis in paper: Explicit - paper characterizes function space critical points but uses smooth perturbation
- Why unresolved: Non-smoothness at rank-deficient matrices creates technical challenges
- What evidence would resolve it: Characterization of parameter space critical points or proof they don't exist

### Open Question 3
- Question: Does rank of end-to-end matrix remain constant during training for original loss?
- Basis in paper: Explicit - paper assumes balanced initialization and shows rank constancy for smooth loss
- Why unresolved: Non-smoothness could cause rank drops
- What evidence would resolve it: Proof of rank constancy for original loss or counterexample

## Limitations
- Analysis relies on smooth perturbative version rather than original Bures-Wasserstein loss
- Uniform margin deficiency condition may be difficult to satisfy in practice
- Results apply specifically to generative setting with balanced initialization

## Confidence
- Critical points characterization: High
- Exponential convergence (gradient flow): Medium
- Gradient descent convergence: Medium

## Next Checks
1. Implement and test uniform margin deficiency initialization protocol on various target covariance matrices
2. Conduct numerical experiments comparing original vs smooth Bures-Wasserstein loss optimization dynamics
3. Analyze sensitivity of convergence rates to regularization parameter τ and learning rate η through hyperparameter sweeps