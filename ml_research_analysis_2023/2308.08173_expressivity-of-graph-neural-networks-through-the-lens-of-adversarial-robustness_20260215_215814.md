---
ver: rpa2
title: Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness
arxiv_id: '2308.08173'
source_url: https://arxiv.org/abs/2308.08173
tags:
- adversarial
- graph
- graphs
- examples
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the empirical expressivity of Graph Neural
  Networks (GNNs) using adversarial robustness as a tool. The authors focus on subgraph
  counting, a task that Message Passing Neural Networks (MPNNs) provably cannot perform
  for patterns with three or more nodes.
---

# Expressivity of Graph Neural Networks Through the Lens of Adversarial Robustness

## Quick Facts
- arXiv ID: 2308.08173
- Source URL: https://arxiv.org/abs/2308.08173
- Reference count: 40
- Key outcome: This work investigates the empirical expressivity of Graph Neural Networks (GNNs) using adversarial robustness as a tool. The authors focus on subgraph counting, a task that Message Passing Neural Networks (MPNNs) provably cannot perform for patterns with three or more nodes. They extend the concept of adversarial examples to integer regression tasks and develop efficient adversarial attacks for subgraph counting, generating perturbations with known ground truth changes. Experiments show that GNNs more powerful than 1-WL fail to generalize even to small perturbations of the graph's structure, revealing a significant gap between their theoretical and empirically achieved expressive power. The results hold for both constrained and semantic-preserving perturbations, and are further supported by poor out-of-distribution generalization performance. Retraining only the final MLP layers does not fully resolve the issue, indicating that the graph representations themselves lack the theoretical separation power.

## Executive Summary
This paper investigates the empirical expressivity of Graph Neural Networks (GNNs) for subgraph counting tasks by leveraging adversarial robustness as a probing tool. While theoretically powerful GNNs (e.g., PPGN and I2-GNN) are proven to be more expressive than the 1-WL test, the authors find that these models fail to generalize even to small perturbations of graph structure. Through a novel extension of adversarial examples to integer regression tasks, they generate perturbations with known ground truth changes and reveal a significant gap between the theoretically possible and empirically achieved expressive power of GNNs. The study highlights that graph representations learned by powerful GNNs lack sufficient discriminative power for subgraph counting, even when retraining only the final prediction layers.

## Method Summary
The authors use synthetic graphs (SBM and Erdős-Rényi) to train two theoretically powerful GNN architectures (PPGN and I2-GNN) for subgraph counting tasks. They extend the concept of adversarial examples from classification to integer regression, developing efficient adversarial attacks (greedy and beam search) to generate perturbations with known ground truth changes. The robustness of GNNs is evaluated using constrained and semantic-preserving perturbation models, and out-of-distribution generalization is tested on graphs with different structural properties. The paper also investigates whether the issue lies in the graph representations or the final prediction layers by retraining only the MLP layers.

## Key Results
- PPGN and I2-GNN fail to generalize to small perturbations of graph structure, despite being theoretically more powerful than 1-WL.
- Retraining only the final MLP layers does not resolve the generalization issue, indicating that graph representations themselves lack theoretical separation power.
- Adversarial examples generated by the proposed attacks successfully reveal the gap between theoretically possible and empirically achieved expressivity for subgraph counting tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: More powerful GNNs (than 1-WL) theoretically capable of counting substructures fail to generalize to small perturbations of the graph's structure.
- Mechanism: Theoretical expressivity guarantees do not ensure that training algorithms find weights corresponding to maximally expressive solutions; GNNs overfit to training distribution rather than learning pattern detection.
- Core assumption: Subgraph counting is a proxy for expressive power; small structural perturbations are sufficient to expose generalization gaps.
- Evidence anchors:
  - [abstract] "more powerful GNNs fail to generalize even to small perturbations to the graph's structure"
  - [section 6.1] "PPGN is highly unrobust in the subgraph-counting of patterns with four nodes"
  - [corpus] Weak - related work focuses on improving expressivity but doesn't directly address this generalization gap
- Break condition: If models achieve comparable performance on perturbed graphs, the mechanism breaks.

### Mechanism 2
- Claim: Retraining only the final MLP layers does not resolve the generalization issue, indicating the graph representations themselves lack theoretical separation power.
- Mechanism: The learned graph representations from more powerful GNNs are not sufficiently discriminative for subgraph counting, suggesting the architecture's expressiveness is not being fully utilized.
- Core assumption: Graph representations are the bottleneck, not the MLP mapping function.
- Evidence anchors:
  - [abstract] "retraining only the final MLP layers does not fully resolve the issue, indicating that the graph representations themselves lack the theoretical separation power"
  - [section 6.2] "the graph representations do not achieve their theoretic separation power and that the problem does not only lie in the last MLP prediction layers"
  - [corpus] Weak - limited discussion of representation quality in related work
- Break condition: If retraining MLP layers significantly improves OOD performance, the mechanism breaks.

### Mechanism 3
- Claim: Adversarial robustness framework reveals a gap between theoretically possible and empirically achieved expressivity for substructure-counting tasks.
- Mechanism: By defining adversarial examples for integer regression tasks and developing efficient attacks, the framework systematically probes model robustness to structural perturbations.
- Core assumption: Adversarial examples in subgraph counting can be efficiently generated and are meaningful indicators of expressivity.
- Evidence anchors:
  - [abstract] "we use adversarial robustness as a tool to uncover a significant gap between their theoretically possible and empirically achieved expressive power"
  - [section 4] "we extend the concept of an adversarial example from classification to (integer) regression tasks"
  - [corpus] Moderate - related work on adversarial attacks for GNNs but not for subgraph counting regression
- Break condition: If adversarial attacks fail to find meaningful examples or transfer to other models, the mechanism breaks.

## Foundational Learning

- Concept: Subgraph isomorphism and counting algorithms
  - Why needed here: The paper relies on exact subgraph counting to generate ground truth for adversarial examples
  - Quick check question: Can you explain how the Shervashidze et al. (2009) algorithm efficiently counts induced subgraphs of size 3-5?

- Concept: Graph neural network expressivity and the Weisfeiler-Lehman hierarchy
  - Why needed here: The paper contrasts theoretical expressivity (k-WL power) with empirical performance
  - Quick check question: What is the key difference between 1-WL and 3-WL in terms of graph isomorphism testing?

- Concept: Adversarial examples in regression tasks
  - Why needed here: The paper extends adversarial example definitions from classification to integer regression
  - Quick check question: How does the paper's Definition 4.1 differ from standard adversarial example definitions in classification?

## Architecture Onboarding

- Component map: Graph → GNN embedding → MLP prediction → Count prediction → Adversarial evaluation
- Critical path: Graph → GNN embedding → MLP prediction → Count prediction → Adversarial evaluation
- Design tradeoffs:
  - Computational efficiency vs. expressivity (subgraph counting is NP-complete in general)
  - Theoretical guarantees vs. empirical performance
  - Sound perturbation models vs. broader perturbation spaces
- Failure signatures:
  - High error on perturbed graphs despite theoretical expressivity
  - Poor OOD generalization even after retraining MLP layers
  - Adversarial examples that transfer across initialization seeds
- First 3 experiments:
  1. Verify PPGN and I2-GNN achieve expected performance on clean graphs for triangle counting
  2. Generate adversarial examples using the greedy search algorithm on constrained perturbation space
  3. Compare subgraph count distributions between clean and adversarial examples using violin plots

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can adversarial training effectively improve the empirical expressivity of GNNs for subgraph counting tasks?
- Basis in paper: [inferred] The paper discusses that more powerful GNNs fail to generalize to adversarial perturbations and out-of-distribution data, suggesting a gap between theoretical and empirical expressivity. It mentions adversarial training as a potential future direction.
- Why unresolved: The paper does not conduct experiments with adversarial training, leaving the effectiveness of this approach untested.
- What evidence would resolve it: Experiments showing improved robustness and generalization on adversarial and out-of-distribution data after applying adversarial training to the GNNs.

### Open Question 2
- Question: Do other expressive GNN architectures, such as KP-GNN, exhibit similar gaps between theoretical and empirical expressivity for subgraph counting?
- Basis in paper: [explicit] The paper suggests extending the study to other related GNNs like KP-GNN as a future direction.
- Why unresolved: The paper only tests PPGN and I2-GNN, not KP-GNN or other architectures.
- What evidence would resolve it: Experiments testing KP-GNN and other expressive architectures on the same adversarial robustness and out-of-distribution generalization tasks.

### Open Question 3
- Question: What specific structural properties of adversarial examples cause GNNs to fail in subgraph counting?
- Basis in paper: [explicit] The paper analyzes the structural properties of adversarial examples, finding shifts in subgraph count distributions and increased edge counts.
- Why unresolved: While shifts are observed, the paper does not pinpoint which specific structural changes (e.g., certain edge additions/deletions) are most detrimental to GNN performance.
- What evidence would resolve it: Detailed analysis identifying the most critical structural perturbations that lead to misclassification, potentially guiding the design of more robust architectures.

### Open Question 4
- Question: Is the failure of GNNs to generalize to out-of-distribution data due to the graph representation or the final prediction layers?
- Basis in paper: [explicit] The paper retrains only the final MLP layers and finds that errors remain high, suggesting the graph representations themselves lack separation power.
- Why unresolved: While the paper rules out the MLP layers, it does not explore alternative methods to improve the graph representations.
- What evidence would resolve it: Experiments testing alternative graph representation methods or training strategies that improve out-of-distribution generalization.

## Limitations

- The analysis is primarily confined to synthetic graphs (SBM and Erdős-Rényi), which may not capture the complexity of real-world graph distributions.
- The study focuses on specific GNN architectures (PPGN and I2-GNN), leaving open questions about whether these limitations extend to other powerful GNN variants.
- The paper does not explore alternative methods to improve graph representations or training strategies to address the generalization issues.

## Confidence

- **High confidence**: The observation that retraining MLP layers alone does not resolve generalization issues, indicating the graph representations themselves are the bottleneck.
- **Medium confidence**: The general claim about expressivity-expressiveness gaps for GNNs, as results are primarily demonstrated on synthetic datasets and specific architectures.
- **Low confidence**: The extent to which these findings generalize to real-world graph datasets and other GNN architectures beyond PPGN and I2-GNN.

## Next Checks

1. **Extend experiments to real-world graph datasets** (e.g., molecular graphs, social networks) to verify whether the expressivity gap persists outside synthetic distributions.

2. **Test additional GNN architectures** such as deeper MPNNs, Graph Transformers, or higher-order GNNs to determine if the generalization issues are architecture-specific or fundamental to powerful GNNs.

3. **Investigate training dynamics** by analyzing the optimization landscape and whether different initialization strategies or regularization techniques could help GNNs learn the theoretically expressible patterns.