---
ver: rpa2
title: 'Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer
  Length'
arxiv_id: '2308.05585'
source_url: https://arxiv.org/abs/2308.05585
tags:
- length
- output
- tokenizer
- reward
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel task to validate the effectiveness
  of Proximal Policy Optimization (PPO) in manipulating the output tokenizer length
  of large language models. The authors employ a "Golden" reward model that solely
  considers the length of the output tokenizer, excluding the influence of the reward
  model effect.
---

# Proximal Policy Optimization Actual Combat: Manipulating Output Tokenizer Length

## Quick Facts
- arXiv ID: 2308.05585
- Source URL: https://arxiv.org/abs/2308.05585
- Reference count: 6
- Primary result: PPO can manipulate output tokenizer length when reward is based solely on length, with facilitated training once reward model effects are excluded

## Executive Summary
This paper investigates whether Proximal Policy Optimization (PPO) can effectively control the output tokenizer length of large language models. The authors introduce a novel task where the reward model considers only the length of the output tokenizer, excluding the influence of reward model effects. Experiments demonstrate that PPO is effective in manipulating output tokenizer length to a certain extent in this task, and notably exhibits facilitated training once the impact of the reward model effect is excluded. The study highlights both the potential and limitations of PPO for discrete control tasks in language models.

## Method Summary
The paper employs PPO to optimize a language model's policy for controlling output tokenizer length. A "Golden" reward model (Rg) is used that returns 1 when the output length is within an acceptable error range (±50%) of the target length and 0 otherwise. The experiments use the Llama-7B Chinese model trained on a cocktail dataset with high school exam questions. The PPO implementation is modified from the trlx framework to use length-based rewards instead of a separate reward model. Training is evaluated on two methods: method-100 (fixed length 100) and method-x (random lengths).

## Key Results
- PPO effectively manipulates output tokenizer length to a certain extent when using length-only rewards
- PPO exhibits facilitated training once the influence of reward model effects is excluded
- PPO outperforms SFT in learning to control output length as a continuous policy
- The model can generate gibberish outputs that meet length requirements, requiring additional validity checks using gzip compression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PPO can learn to control output tokenizer length when the reward signal is based solely on length, without complex human feedback.
- Mechanism: By providing a simple length-based reward that returns 1 when the output is within an acceptable error range of the target length and 0 otherwise, PPO learns to adjust its policy to produce outputs of the desired length.
- Core assumption: The reward signal is stable and informative enough for PPO to learn the length control policy.
- Evidence anchors:
  - [abstract] "Experiments confirm that PPO is not only effective in manipulating the output tokenizer length to a certain extent in this type of task but also exhibits facilitated training once the influence of reward model effect is excluded"
  - [section 3.3] "Our key revision in the code development for trlx involved shifting from an inherited reward model to the adoption of a reward calculated through the length of the output tokenizer"

### Mechanism 2
- Claim: PPO outperforms SFT in learning to control output length because PPO can learn a continuous policy while SFT requires data augmentation or prompt engineering.
- Mechanism: PPO directly optimizes for the reward (length) through policy gradient updates, allowing it to learn length control as a continuous policy. SFT, being a supervised method, requires specific training data or prompt modifications to achieve similar results.
- Core assumption: The task of controlling output length is better suited to reinforcement learning than supervised learning.
- Evidence anchors:
  - [section 3.3] "While PPO has been successful in manipulating output tokenizer length to some degree in our experiment, it continues to exhibit shortcomings in the accurate and comprehensive interpretation of input requirements – a challenge which is handled efficiently by SFT"
  - [section 4.3] "The SFT does not inherently possess the capability to dictate the tokenizer length of its output"

### Mechanism 3
- Claim: Using compressed string length (lgzip) as a validity reward (Rv) helps detect and prevent the model from generating meaningless outputs while still meeting length requirements.
- Mechanism: By compressing the output and checking if the compressed length is close to the target length, we can identify when the model is generating gibberish that happens to be long. This provides a more robust validity check than just looking at raw token count.
- Core assumption: Gibberish outputs will compress significantly more than meaningful outputs of the same length.
- Evidence anchors:
  - [section 3.3] "Owing to its inherent discontinuity, the training of Proximal Policy Optimization (PPO) tends to be unstable, often converging prematurely. This results in the model producing indecipherable outputs. To identify and quantify this phenomenon, we have proposed the inclusion of an Rv"
  - [section 4.5.2] "We utilize the gzip algorithm to compress the string. This ensures that the resultant effective string length is not significantly smaller than the necessary output tokenizer length"

## Foundational Learning

- Concept: Proximal Policy Optimization (PPO)
  - Why needed here: PPO is the reinforcement learning algorithm used to optimize the language model's policy for controlling output length.
  - Quick check question: What is the key innovation of PPO compared to other policy optimization methods, and how does it help with stability?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper is exploring how RLHF techniques, specifically PPO, can be applied to a novel task of controlling output length, and how this differs from traditional RLHF applications.
  - Quick check question: How does the reward model in this paper differ from typical RLHF reward models that are based on human preferences?

- Concept: Tokenizer length manipulation
  - Why needed here: The core task being investigated is whether a language model can be trained to produce outputs of a specific length, which has implications for controllable text generation.
  - Quick check question: Why might controlling output length be useful in practical applications of language models?

## Architecture Onboarding

- Component map: Base model (Llama-7B Chinese) -> PPO implementation (modified trlx) -> Reward models (Rg, Rc, Rv) -> Data pipeline (cocktail dataset) -> Evaluation metrics (valid output ratio, length distribution)

- Critical path:
  1. Initialize base model
  2. Prepare dataset with length requirements
  3. Implement PPO with length-based rewards
  4. Train PPO model
  5. Evaluate length control performance
  6. Compare with baseline models and SFT

- Design tradeoffs:
  - Simple length reward vs. more complex reward functions
  - One certain length vs. random certain length tasks
  - PPO vs. SFT for length control capability
  - Using gzip compression for validity checking vs. other methods

- Failure signatures:
  - Model generates gibberish outputs that meet length requirements
  - Training instability or premature convergence
  - Poor performance on random certain length task
  - Inability to distinguish between different input requirements

- First 3 experiments:
  1. Train PPO on method-100 (fixed length 100) and evaluate valid output ratio
  2. Compare PPO performance with base model and SFT on method-100 task
  3. Train PPO on method-x (random lengths) and evaluate performance on distinguishing input requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the combination of PPO and SFT be further optimized to achieve better results in tasks requiring both understanding of input requirements and manipulation of output tokenizer length?
- Basis in paper: [explicit] The paper discusses the limitations of both PPO and SFT in tasks requiring understanding of input requirements and manipulation of output tokenizer length. It suggests that combining these methods could potentially yield better results, but does not provide a concrete solution.
- Why unresolved: The paper only mentions the potential of combining PPO and SFT, but does not explore this further or provide a detailed methodology for optimization.
- What evidence would resolve it: Concrete results from experiments testing various combinations of PPO and SFT, demonstrating improved performance in tasks requiring both understanding of input requirements and manipulation of output tokenizer length.

### Open Question 2
- Question: How can the issue of PPO generating unintelligible characters with prolonged training be mitigated?
- Basis in paper: [explicit] The paper acknowledges that PPO can produce a stream of unintelligible characters with prolonged training, undermining the overall efficacy of the model.
- Why unresolved: The paper does not provide a solution to this problem, only mentioning it as a limitation of PPO.
- What evidence would resolve it: Results from experiments testing different methods to mitigate the generation of unintelligible characters in PPO, demonstrating improved model efficacy.

### Open Question 3
- Question: How can the performance of PPO in manipulating output tokenizer length in Chinese be improved to match its performance in English?
- Basis in paper: [explicit] The paper mentions that PPO's performance in manipulating output tokenizer length in Chinese trails behind its performance in English.
- Why unresolved: The paper does not provide a solution to this problem, only noting it as a limitation of PPO in the context of Chinese language processing.
- What evidence would resolve it: Results from experiments testing different methods to improve PPO's performance in manipulating output tokenizer length in Chinese, demonstrating improved performance that matches or surpasses its performance in English.

## Limitations
- Training instability due to reward discontinuity, leading to premature convergence and gibberish outputs
- Limited to length control without input understanding, struggling with tasks requiring both understanding and length manipulation
- Simplified reward model (length-only) that may not generalize to complex RLHF scenarios requiring quality assessment
- Experiments limited to Chinese high school exam questions, raising questions about cross-domain and cross-language generalization

## Confidence

**High Confidence**: The observation that PPO training becomes more stable when the reward model effect is excluded (Rg only). This is directly observable from training dynamics and supported by the authors' implementation changes.

**Medium Confidence**: The claim that PPO can manipulate output tokenizer length to "a certain extent" in the specified task. While the paper shows PPO outperforms baselines on length control, the absolute performance metrics and generalization capabilities are not fully characterized.

**Low Confidence**: The implication that these findings significantly advance RLHF methodology. The paper demonstrates a narrow capability (length control without input understanding) but doesn't establish how this transfers to the broader challenges of RLHF where reward models provide complex human feedback.

## Next Checks

1. **Cross-domain generalization test**: Evaluate the trained PPO model on diverse datasets beyond Chinese exam questions (e.g., news articles, dialogue, technical writing) to assess whether length control capabilities transfer across domains and whether performance degrades when input understanding is required.

2. **Hybrid reward function experiment**: Implement a combined reward that includes both length control (Rg) and quality assessment (Rv) simultaneously, then compare PPO training stability and output quality against the length-only reward used in the paper. This would validate whether the "facilitated training" observation holds when moving toward more realistic RLHF scenarios.

3. **Baselines comparison with equal conditions**: Re-run experiments comparing PPO against SFT using identical data augmentation and prompt engineering techniques for the SFT baseline, rather than the stated advantage that SFT requires these additional steps. This would provide a fairer assessment of whether RL truly outperforms supervised learning for this task.