---
ver: rpa2
title: Multiplicative update rules for accelerating deep learning training and increasing
  robustness
arxiv_id: '2307.07189'
source_url: https://arxiv.org/abs/2307.07189
tags:
- update
- multiplicative
- training
- learning
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Generic Optimization Framework for Alternative
  Updates (GOFAU) to investigate multiplicative update rules in deep learning optimization,
  a largely unexplored area. The proposed multiplicative update rule scales parameters
  proportionally to their magnitudes while normalizing gradients, leading to faster
  convergence and increased robustness compared to traditional additive update rules.
---

# Multiplicative update rules for accelerating deep learning training and increasing robustness

## Quick Facts
- arXiv ID: 2307.07189
- Source URL: https://arxiv.org/abs/2307.07189
- Reference count: 12
- This paper introduces a Generic Optimization Framework for Alternative Updates (GOFAU) to investigate multiplicative update rules in deep learning optimization, a largely unexplored area. The proposed multiplicative update rule scales parameters proportionally to their magnitudes while normalizing gradients, leading to faster convergence and increased robustness compared to traditional additive update rules. The framework is further extended with a hybrid multiplicative-additive update rule to overcome sign limitations of multiplicative updates. Extensive experiments on both convex/non-convex optimization tasks and image classification benchmarks (CIFAR, Tiny ImageNet) demonstrate that the proposed framework accelerates training, particularly in the initial stages, and leads to more robust models across various optimization methods and network architectures. The hybrid update rule consistently outperforms both additive and multiplicative updates, achieving up to 8% accuracy improvements on ResNet18 with SGD optimizer.

## Executive Summary
This paper introduces a Generic Optimization Framework for Alternative Updates (GOFAU) to investigate multiplicative update rules in deep learning optimization, a largely unexplored area. The proposed multiplicative update rule scales parameters proportionally to their magnitudes while normalizing gradients, leading to faster convergence and increased robustness compared to traditional additive update rules. The framework is further extended with a hybrid multiplicative-additive update rule to overcome sign limitations of multiplicative updates. Extensive experiments on both convex/non-convex optimization tasks and image classification benchmarks (CIFAR, Tiny ImageNet) demonstrate that the proposed framework accelerates training, particularly in the initial stages, and leads to more robust models across various optimization methods and network architectures. The hybrid update rule consistently outperforms both additive and multiplicative updates, achieving up to 8% accuracy improvements on ResNet18 with SGD optimizer.

## Method Summary
The paper proposes a multiplicative update rule ξ(θ(t-1), m_t, l_t) = |θ(t-1)| tanh(η_in * m_t * l_t) * η_out that scales parameters proportionally to their magnitudes while normalizing gradients. This is implemented within a Generic Optimization Framework for Alternative Updates (GOFAU) that accepts momentum calculation (φ), adaptive learning rate calculation (ψ), and update rule (ξ) functions. The framework is extended with a hybrid multiplicative-additive update rule γ * (|θ(t-1)| * tanh(η_in * m_t * l_t) * η_out) + (1-γ) * (η * m_t * l_t) to overcome sign limitations. Experiments are conducted on 2D convex/non-convex optimization tasks and image classification benchmarks (CIFAR10, CIFAR100, Tiny ImageNet) using ResNet9, ResNet18, and VGG16 architectures with various optimizers (SGD, Adagrad, RMSProp, Adam).

## Key Results
- Multiplicative updates converge faster than additive updates on both convex and non-convex 2D optimization tasks
- The hybrid update rule achieves up to 8% accuracy improvements on ResNet18 with SGD optimizer
- Multiplicative updates demonstrate increased robustness with lower variance in training curves
- The proposed framework accelerates training in initial epochs across all tested optimizers and architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiplicative updates scale parameters proportionally to their magnitudes while normalizing gradients, leading to faster convergence and increased robustness.
- Mechanism: The update rule `|θ(t-1)| * tanh(η_in * m_t * l_t) * η_out` scales each parameter by its absolute value and normalizes gradients via the tanh function, constraining updates to `[-η_out * |θ(t-1)|, η_out * |θ(t-1)|]`.
- Core assumption: The magnitude of parameters is a reliable signal for scaling updates, and normalization prevents gradient explosion.
- Evidence anchors:
  - [abstract]: "The proposed multiplicative update rule scales parameters proportionally to their magnitudes while normalizing gradients, leading to faster convergence and increased robustness."
  - [section]: "the proposed update term proportionally scales the parameters considering the magnitude of it... the proposed update rule naturally normalizes gradients and introduces a threshold proportional to the magnitude of the parameter."
- Break condition: If parameters are initialized with zero or very small values, the scaling factor becomes ineffective, potentially stalling training.

### Mechanism 2
- Claim: Multiplicative updates constrain parameters to retain their initial sign, which can improve interpretability and enable specialized neural network designs.
- Mechanism: Since the update rule uses `|θ(t-1)|` as a multiplier, the sign of each parameter remains unchanged during training.
- Core assumption: Maintaining initial sign is beneficial for interpretability or when training non-negative/neuromorphic networks.
- Evidence anchors:
  - [section]: "the proposed update rule has the ability to preserve the initial sign of the update parameter, making it an excellent choice for training neural networks oriented to interpretability."
  - [abstract]: "Constraining parameters on the initial sign can be especially useful in the case of neuromorphic architectures."
- Break condition: If the optimal solution requires sign changes, this constraint may prevent convergence or force suboptimal performance.

### Mechanism 3
- Claim: The hybrid update rule combines multiplicative and additive updates, overcoming sign limitations while preserving the robustness and acceleration benefits of multiplicative scaling.
- Mechanism: `γ * (|θ(t-1)| * tanh(η_in * m_t * l_t) * η_out) + (1-γ) * (η * m_t * l_t)` blends the two update types, with `γ` controlling the relative contribution.
- Core assumption: A balanced combination of both update rules can achieve faster convergence than either alone while allowing sign changes when necessary.
- Evidence anchors:
  - [section]: "To overcome this sign limitation, we extend the proposed multiplicative update method to a hybrid multiplicative-additive update rule that allows parameters to change sign while exploiting the robustness and acceleration capabilities of the multiplicative update term."
  - [section]: "the proposed hybrid update rule significantly outperforms both the additive and multiplicative update rules."
- Break condition: If `γ` is set too high, sign changes may be insufficient; if too low, the benefits of multiplicative scaling may be lost.

## Foundational Learning

- Concept: Gradient normalization and clipping
  - Why needed here: The multiplicative update uses `tanh` to normalize gradients and prevent large updates that could destabilize training.
  - Quick check question: What happens to the gradient update if its magnitude is much larger than the parameter magnitude?

- Concept: Parameter magnitude scaling
  - Why needed here: Multiplicative updates scale each parameter by its own absolute value, making updates proportional to the parameter size.
  - Quick check question: How does the update magnitude change if a parameter is very large versus very small?

- Concept: Overparameterization in deep learning
  - Why needed here: The paper assumes networks are overparameterized, allowing multiplicative updates to converge even with sign constraints.
  - Quick check question: Why might overparameterization help when using multiplicative updates that preserve initial sign?

## Architecture Onboarding

- Component map: GOFAU framework -> momentum calculation φ(·) -> adaptive learning rate calculation ψ(·) -> update rule ξ(·) -> parameter update
- Critical path: 1) Compute gradient g_t 2) Calculate momentum m_t 3) Calculate adaptive learning rate l_t 4) Apply update rule ξ to get parameter change 5) Update parameters
- Design tradeoffs: Multiplicative updates offer faster initial convergence and robustness but constrain sign changes. The hybrid approach mitigates this at the cost of added complexity and a hyperparameter (γ).
- Failure signatures: If training stalls, check if parameters have become too small (zero scaling) or if γ is misconfigured. If performance degrades, verify that sign constraints aren't preventing necessary updates.
- First 3 experiments:
  1. Test multiplicative update on a simple convex 2D function (e.g., quadratic) and compare convergence speed to standard SGD.
  2. Apply hybrid update on a small CNN (e.g., ResNet9) on CIFAR-10 and compare accuracy after 5 epochs to standard optimizers.
  3. Vary γ in the hybrid update and observe its effect on both convergence speed and final accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical basis for why multiplicative updates perform better than additive updates in deep learning training?
- Basis in paper: [explicit] The paper mentions that multiplicative updates have strong theoretical claims compared to additive updates, particularly when a large portion of features are irrelevant, and references the comparison of mistake bounds between Winnow and Perceptron algorithms.
- Why unresolved: The paper demonstrates experimental results showing the benefits of multiplicative updates but does not provide a rigorous theoretical explanation for why these updates work better in deep learning contexts.
- What evidence would resolve it: A formal proof or theoretical analysis demonstrating why multiplicative updates lead to faster convergence and increased robustness in deep learning compared to additive updates.

### Open Question 2
- Question: What is the optimal way to determine the weights (γ) for the hybrid multiplicative-additive update rule in different deep learning architectures?
- Basis in paper: [explicit] The paper mentions that the hybrid method uses a fixed weight (γ = 0.5) for the multiplicative term, but does not explore how to optimally determine this weight for different architectures.
- Why unresolved: The paper uses a fixed value for γ without investigating whether this is optimal for different network architectures or optimization tasks.
- What evidence would resolve it: Empirical studies showing how different values of γ affect performance across various network architectures, or a theoretical framework for determining the optimal γ based on network characteristics.

### Open Question 3
- Question: How do multiplicative updates perform in other domains beyond image classification, such as natural language processing or reinforcement learning?
- Basis in paper: [explicit] The paper demonstrates the effectiveness of multiplicative updates on image classification tasks (CIFAR, Tiny ImageNet) but does not explore other domains.
- Why unresolved: The paper focuses exclusively on image classification benchmarks without investigating whether the benefits of multiplicative updates generalize to other types of deep learning tasks.
- What evidence would resolve it: Experimental results showing the performance of multiplicative updates on tasks in NLP, RL, or other domains, comparing them to traditional additive updates.

## Limitations
- The paper relies on specific hyperparameter configurations (particularly γ in the hybrid update) without providing systematic tuning guidelines
- Effectiveness of multiplicative updates may be highly sensitive to initialization schemes and network architecture choices
- The paper assumes overparameterization as beneficial for multiplicative updates, but the precise conditions under which this holds are not quantified

## Confidence
- **High Confidence**: The mathematical formulation of the multiplicative update rule and its implementation within the GOFAU framework. The experimental methodology for 2D optimization tasks and the hybrid update rule's basic mechanism are well-defined.
- **Medium Confidence**: The empirical claims about faster initial convergence and increased robustness across diverse datasets and architectures. While the experiments are extensive, the paper could benefit from more ablation studies isolating the contribution of each component.
- **Low Confidence**: The generalizability of multiplicative updates to extremely deep architectures (beyond ResNet18) and the precise conditions under which sign preservation becomes detrimental to performance.

## Next Checks
1. **Initialization Sensitivity Analysis**: Systematically test multiplicative updates across different weight initialization schemes (He, Xavier, uniform) to quantify robustness to initialization choices and identify failure modes.

2. **Gradient Flow Visualization**: Track and visualize the gradient magnitudes and directions during training with multiplicative updates versus standard SGD to empirically verify the claimed normalization effect and identify when gradients become unstable.

3. **Architecture Scaling Study**: Evaluate the proposed update rules on progressively deeper architectures (ResNet34, ResNet50, ResNet101) to determine the scaling limits of multiplicative updates and identify architectural characteristics that enhance or hinder their effectiveness.