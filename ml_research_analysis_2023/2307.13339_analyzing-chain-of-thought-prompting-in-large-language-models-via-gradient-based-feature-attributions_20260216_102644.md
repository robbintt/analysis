---
ver: rpa2
title: Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based
  Feature Attributions
arxiv_id: '2307.13339'
source_url: https://arxiv.org/abs/2307.13339
tags:
- coin
- prompting
- heads
- answer
- does
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses gradient-based feature attribution methods to analyze
  how Chain-of-Thought (CoT) prompting affects the way large language models (LLMs)
  process text. Specifically, the authors examine saliency scores, which measure the
  influence of input tokens on model output, for semantically relevant tokens in questions
  across four datasets.
---

# Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions

## Quick Facts
- arXiv ID: 2307.13339
- Source URL: https://arxiv.org/abs/2307.13339
- Reference count: 40
- Key outcome: Chain-of-Thought prompting increases robustness of saliency scores to question perturbations and model output variations, while decreasing the magnitude of saliency scores for relevant tokens compared to standard few-shot prompting.

## Executive Summary
This paper investigates how Chain-of-Thought (CoT) prompting affects the internal processing of large language models using gradient-based feature attribution methods. The authors analyze saliency scores—which measure the influence of input tokens on model output—for semantically relevant tokens across four datasets. While CoT prompting does not increase the magnitude of saliency scores for relevant tokens compared to standard prompting, it significantly increases the robustness of these scores to question perturbations and variations in model output. This suggests that CoT prompting induces models to more consistently focus on relevant tokens, potentially leading to more stable and accurate answers.

## Method Summary
The authors use gradient-based feature attribution methods to analyze how Chain-of-Thought prompting affects the influence of input tokens on model output in large language models. They compute saliency scores for tokens in questions using four gradient-based methods (L1 norm, gradient x input, and their contrastive variants) on four Q&A datasets with 50 questions each. The study compares saliency scores between standard few-shot prompting and CoT prompting, examining differences in magnitude for relevant tokens, robustness to question perturbations, and stability across stochastic model outputs.

## Key Results
- CoT prompting increases robustness of saliency scores to question perturbations and model output variations
- CoT prompting decreases the magnitude of saliency scores for relevant tokens compared to standard prompting
- CoT yields more stable gradients for relevant input tokens across different model outputs compared to standard prompting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoT prompting increases robustness of saliency scores to question perturbations and model output variations.
- Mechanism: CoT prompting induces models to more consistently focus on relevant tokens by generating intermediate reasoning steps, leading to more stable gradients across semantically equivalent inputs.
- Core assumption: Saliency score stability correlates with reasoning consistency and task accuracy.
- Evidence anchors:
  - [abstract] "it increases the robustness of saliency scores to question perturbations and variations in model output."
  - [section] "Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output."
  - [corpus] No direct evidence found; this is an inference from the paper's findings.
- Break condition: If saliency scores are stable but accuracy does not improve, or if CoT explanations are misleading but saliency scores still appear stable.

### Mechanism 2
- Claim: CoT prompting reduces the magnitude of saliency scores on relevant tokens compared to standard prompting.
- Mechanism: The longer input length and intermediate reasoning steps in CoT prompting cause the model to "spread its attention" over more tokens, diluting saliency scores.
- Core assumption: The dilution effect is a direct result of the increased token count and complexity in CoT prompting.
- Evidence anchors:
  - [abstract] "while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting"
  - [section] "CoT prompting uniformly decreases the magnitudes of relevant tokens' saliency scores across all datasets and saliency score methods, compared to standard prompting."
  - [corpus] No direct evidence found; this is an inference from the paper's findings.
- Break condition: If saliency scores are not diluted in larger models where CoT is more effective, or if the dilution effect does not correlate with improved accuracy.

### Mechanism 3
- Claim: CoT prompting makes model gradients more stable across randomness in generated output.
- Mechanism: By generating intermediate reasoning steps, CoT prompting leads to more consistent gradients for relevant input tokens, even when the final output varies due to stochasticity.
- Core assumption: Stable gradients indicate more consistent reasoning, which may lead to more accurate answers.
- Evidence anchors:
  - [abstract] "it increases the robustness of saliency scores to question perturbations and variations in model output."
  - [section] "CoT prompting yields more 'stable' gradients for relevant input tokens across different model outputs compared to standard prompting."
  - [corpus] No direct evidence found; this is an inference from the paper's findings.
- Break condition: If gradient stability does not translate to improved accuracy, or if standard prompting with enough training data achieves similar gradient stability.

## Foundational Learning

- Concept: Gradient-based feature attribution methods
  - Why needed here: To measure the influence of input tokens on model output and analyze how CoT prompting changes this influence.
  - Quick check question: How do saliency scores capture the importance of input tokens in a language model's prediction?

- Concept: Chain-of-thought prompting
  - Why needed here: To understand why this technique improves model accuracy and how it affects the model's internal processing.
  - Quick check question: What is the difference between standard few-shot prompting and chain-of-thought prompting?

- Concept: Robustness in machine learning
  - Why needed here: To evaluate how CoT prompting affects the model's ability to handle variations in input and output.
  - Quick check question: How can we measure the robustness of a model's saliency scores to perturbations in the input?

## Architecture Onboarding

- Component map: Datasets (SST, CoinFlip, GSM8K, CSQA) -> Pre-trained LLMs (GPT-J, GPT-2 XL, GPT-Neo) -> Gradient-based saliency methods (L1 norm, gradient x input, regular and contrastive variants) -> Saliency score analysis
- Critical path: Run the language model on the dataset with standard and CoT prompting, compute saliency scores for each token, analyze the saliency scores for relevant tokens, and compare the results across datasets and prompting methods.
- Design tradeoffs: Using smaller models to save computational resources, but this may limit the generalizability of the findings to larger models where CoT is more effective.
- Failure signatures: If saliency scores are not stable across perturbations or model output variations, or if the magnitude of saliency scores on relevant tokens is not reduced by CoT prompting.
- First 3 experiments:
  1. Compare saliency scores of relevant tokens between standard and CoT prompting on a small dataset (e.g., SST) to verify the dilution effect.
  2. Test robustness to question perturbations by running the model on original and reworded versions of a dataset (e.g., CoinFlip) and comparing saliency scores.
  3. Measure gradient stability across model output variations by running the model multiple times with different random seeds and analyzing the variance in saliency scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do saliency score magnitudes and distributions differ between CoT and standard prompting at model scales where CoT improves accuracy?
- Basis in paper: Explicit - the paper notes their 6B parameter models do not achieve CoT accuracy gains, suggesting larger models may behave differently
- Why unresolved: The study only tested models up to 6B parameters, while CoT accuracy gains appear at 100B+ parameters
- What evidence would resolve it: Replicating the experiments on 100B+ parameter models and comparing saliency score patterns between prompting methods

### Open Question 2
- Question: Do different interpretability methods (beyond gradient-based saliency) reveal similar or different patterns of CoT-induced changes in token processing?
- Basis in paper: Explicit - the paper suggests examining CoT with "other interpretability tools to investigate if our observations still hold"
- Why unresolved: Only gradient-based saliency methods were tested, while other interpretability approaches exist
- What evidence would resolve it: Applying methods like attention visualization, feature importance, or causal intervention to the same CoT vs standard prompting comparison

### Open Question 3
- Question: How do saliency score patterns change when CoT prompts contain incorrect reasoning steps?
- Basis in paper: Explicit - the paper notes that incorrect CoT explanations may mislead users about model functioning
- Why unresolved: All experiments used correct CoT examples; incorrect reasoning was not tested
- What evidence would resolve it: Comparing saliency patterns between correct and incorrect CoT prompts to see if models process them differently

## Limitations
- The study uses smaller models (up to 6B parameters) where CoT prompting shows less pronounced benefits compared to larger models
- The analysis is limited to four datasets with only 50 questions each, which may not capture full task diversity
- Manual labeling of relevant tokens introduces potential subjectivity and limits scalability

## Confidence

- High confidence: The finding that CoT prompting increases robustness of saliency scores to perturbations and model output variations is well-supported by direct evidence from multiple experiments across all datasets and saliency methods.
- Medium confidence: The claim that CoT uniformly decreases saliency magnitudes for relevant tokens is supported by consistent patterns across datasets, but the interpretation of this dilution effect requires further validation.
- Medium confidence: The interpretation that saliency score stability correlates with reasoning consistency is mechanistically plausible but not directly validated against actual reasoning quality or accuracy metrics.

## Next Checks

1. Replicate the saliency analysis on larger models (GPT-3, PaLM) where CoT effects are typically more pronounced to verify if the robustness patterns hold at scale.
2. Conduct ablation studies testing whether the observed robustness effects persist when CoT explanations are intentionally misleading or when models are trained to generate deceptive reasoning steps.
3. Design controlled experiments that directly measure the correlation between saliency score stability and actual task accuracy across perturbations, rather than assuming this relationship.