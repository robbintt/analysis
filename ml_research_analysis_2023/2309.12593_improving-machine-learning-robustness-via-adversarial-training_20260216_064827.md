---
ver: rpa2
title: Improving Machine Learning Robustness via Adversarial Training
arxiv_id: '2309.12593'
source_url: https://arxiv.org/abs/2309.12593
tags:
- adversarial
- training
- data
- accuracy
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper improves machine learning robustness via adversarial\
  \ training in both centralized and decentralized environments. In centralized training,\
  \ it achieves 65.41% and 83.0% test accuracy against Fast Gradient Sign Method and\
  \ DeepFool adversarial examples, respectively\u2014improving upon prior work by\
  \ 18.41% and 47%."
---

# Improving Machine Learning Robustness via Adversarial Training

## Quick Facts
- arXiv ID: 2309.12593
- Source URL: https://arxiv.org/abs/2309.12593
- Reference count: 40
- Improves ML robustness via adversarial training, achieving 65.41% FGSM accuracy and 83.0% DeepFool accuracy in centralized settings

## Executive Summary
This paper proposes a comprehensive approach to improve machine learning robustness through adversarial training in both centralized and federated learning environments. The method combines soft labeling to prevent overfitting, Gaussian noise augmentation to improve generalization, and a data-sharing strategy for non-IID federated learning scenarios. The authors demonstrate significant improvements over prior work, achieving 18.41% higher accuracy against FGSM attacks and 47% higher against DeepFool attacks compared to existing methods.

## Method Summary
The approach uses adversarial training with PGD-generated examples (epsilon=8/255, 7 iterations, step size 2/255) combined with soft labeling and Gaussian noise augmentation. For federated learning, the method employs FedAvg aggregation with a data-sharing strategy where 1,000 IID images per class are shared across clients. The CIFAR-10 dataset is used with a modified ResNet-18 architecture featuring 3×3 kernel for the first convolutional layer and no down-sampling except in the first block.

## Key Results
- Centralized training achieves 65.41% test accuracy against FGSM and 83.0% against DeepFool attacks
- Improves upon prior work by 18.41% (FGSM) and 47% (DeepFool)
- Federated learning with IID data shows comparable performance to centralized training
- Data-sharing approach in non-IID settings increases natural accuracy to 85.04% and robust accuracy from 57% to 72% (C&W) and from 59% to 67% (PGD)

## Why This Works (Mechanism)

### Mechanism 1: Soft Labeling
Soft labeling reduces overfitting by smoothing hard class boundaries during adversarial training. Instead of one-hot vectors, it assigns high probability to the true class and distributes remaining probability among others, preventing model overconfidence and improving generalization.

### Mechanism 2: Gaussian Noise Augmentation
Adding Gaussian noise (mean=0, std=0.1) during training and testing makes the model robust against random perturbations and distorts adversarial perturbations. Since adversarial examples are calculated based on clean inputs, adding noise after perturbation calculation distorts the adversarial direction.

### Mechanism 3: Data Sharing for Non-IID Federated Learning
Sharing a small subset of IID data (1,000 images per class, 500 per client) across all clients mitigates non-IID data distribution issues. This helps clients with highly skewed local distributions align their local models with the global objective.

## Foundational Learning

- **Concept**: Adversarial training fundamentals
  - Why needed: Understanding how adversarial examples are generated and how training on them improves robustness is crucial
  - Quick check: What is the difference between FGSM and PGD attacks in terms of how they generate adversarial examples?

- **Concept**: Federated learning architecture
  - Why needed: The paper extends centralized adversarial training to federated settings, so understanding how FL works is essential
  - Quick check: How does FedAvg aggregate model updates from different clients?

- **Concept**: Data augmentation techniques
  - Why needed: The paper uses Gaussian noise and simple transformations as data augmentation, which is important for improving model robustness
  - Quick check: Why might adding Gaussian noise during training improve robustness against adversarial examples?

## Architecture Onboarding

- **Component map**: Client nodes -> Server/aggregator -> Data preprocessing -> Adversarial example generator -> Test pipeline
- **Critical path**: Data augmentation → Soft label generation → Local adversarial training → Model parameter aggregation → Gaussian noise addition to test images → Robustness evaluation
- **Design tradeoffs**: More aggressive data augmentation improves robustness but may slow convergence; larger shared datasets improve accuracy but reduce privacy benefits
- **Failure signatures**: Natural accuracy drops significantly in non-IID settings without data sharing; robust accuracy plateaus below 40% in federated settings without proper adversarial training
- **First 3 experiments**: 1) Test centralized adversarial training with and without soft labeling on CIFAR-10; 2) Compare federated adversarial training with IID data vs centralized baseline; 3) Evaluate federated adversarial training with one-class non-IID data with and without data sharing

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several areas remain unexplored regarding scalability, convergence time, and advanced attack scenarios.

## Limitations
- Methodology for creating non-IID partitions and shared subset lacks full specification
- Impact of Gaussian noise augmentation on adversarial robustness lacks empirical validation beyond reported results
- Scalability of data-sharing approach to larger numbers of clients and more complex non-IID scenarios is not explored

## Confidence
- **High Confidence**: Centralized adversarial training results (65.41% FGSM accuracy, 83.0% DeepFool accuracy)
- **Medium Confidence**: Federated learning with IID data
- **Low Confidence**: Non-IID federated learning with data sharing

## Next Checks
1. Replicate centralized adversarial training baseline with PGD attacks to verify 65.41% FGSM and 83.0% DeepFool accuracy claims
2. Implement federated adversarial training with one-class non-IID data without data sharing to confirm 57% C&W accuracy drop
3. Create data-sharing subset (1,000 images per class, 500 per client) and measure improvement in both natural and robust accuracy for non-IID federated training