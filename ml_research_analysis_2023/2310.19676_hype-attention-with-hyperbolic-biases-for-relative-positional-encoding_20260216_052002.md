---
ver: rpa2
title: 'HyPE: Attention with Hyperbolic Biases for Relative Positional Encoding'
arxiv_id: '2310.19676'
source_url: https://arxiv.org/abs/2310.19676
tags:
- attention
- relative
- hype
- positional
- encoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Hyperbolic Positional Encoding (HyPE), a novel
  method for encoding relative positional information in Transformer architectures.
  HyPE leverages hyperbolic functions' properties to bias attention without storing
  O(L^2) values, instead using matrix multiplications to indirectly incorporate relative
  distances into softmax computation.
---

# HyPE: Attention with Hyperbolic Biases for Relative Positional Encoding

## Quick Facts
- arXiv ID: 2310.19676
- Source URL: https://arxiv.org/abs/2310.19676
- Reference count: 17
- The paper introduces Hyperbolic Positional Encoding (HyPE), a novel method for encoding relative positional information in Transformer architectures

## Executive Summary
HyPE introduces a novel approach to relative positional encoding in Transformer architectures that leverages hyperbolic functions to bias attention without storing O(L^2) values. The method uses matrix multiplications to indirectly incorporate relative distances into softmax computation, offering compatibility with FlashAttention-2 and supporting gradient backpropagation for learnable parameters. By carefully selecting hyperparameters, HyPE can approximate ALiBi's attention bias while requiring only O(4Lh) storage, making it particularly promising for contexts requiring generalization beyond pretraining lengths.

## Method Summary
HyPE computes relative positional bias through hyperbolic sine functions by concatenating modified query and key matrices with ηQ and ηK matrices. The hyperbolic parameters (µ, τ) control the bias magnitude and decay, with the concatenated matrices ˆQ and ˆK computed as products of the original Q/K matrices with exponential terms. This approach enables indirect computation of relative positional information through matrix multiplication rather than explicit mask storage, theoretically providing O(4Lh) memory complexity instead of O(L^2).

## Key Results
- HyPE can approximate ALiBi's attention bias through careful hyperparameter selection
- The method requires storing only O(4Lh) values instead of O(L^2) for full mask approaches
- HyPE is theoretically compatible with FlashAttention-2 architecture
- The approach supports learnable parameters through gradient backpropagation

## Why This Works (Mechanism)

### Mechanism 1
Hyperbolic functions allow indirect computation of relative positional bias without storing full O(L²) matrix. The paper leverages the identity 2sinh(x-y) = e^(x-y) - e^(y-x) to express relative positional bias as a product of exponential terms computed through matrix multiplication of concatenated query and key vectors with hyperbolic parameters. The truncated series approximation of sinh(x) ≈ x + O(x³) for |x|≪1 is assumed valid for encountered relative positions.

### Mechanism 2
HyPE approximates ALiBi's attention bias with careful hyperparameter selection. By setting τ=1 and µ=m<1/L where m is ALiBi's slope, the hyperbolic bias approximates aALiBi(m=µ)i,j + O((j-i)³µ³) for |(j-i)µ|≪1. This approximation relies on the condition that relative position differences multiplied by µ remain small.

### Mechanism 3
HyPE achieves compatibility with FlashAttention-2 by computing positional bias through matrix multiplication rather than adding pre-computed O(L²) masks. Since HyPE performs the bias computation as part of the attention calculation rather than as a separate mask addition, it fits within FlashAttention-2's architecture which doesn't support pre-added attention biases.

## Foundational Learning

- Concept: Hyperbolic functions and their series expansions
  - Why needed here: Understanding how sinh(x) = (e^x - e^(-x))/2 enables the key insight for computing relative positional bias through matrix multiplication
  - Quick check question: What is the first-order approximation of sinh(x) for small x?

- Concept: Matrix multiplication properties and concatenation operations
  - Why needed here: The core mechanism relies on concatenating query and key vectors with hyperbolic parameters, then leveraging matrix multiplication to compute the bias implicitly
  - Quick check question: How does concatenating additional columns to Q and K affect the dimensions of the resulting attention matrix?

- Concept: Transformer attention mechanism and positional encoding
  - Why needed here: HyPE builds on existing transformer architectures by modifying how positional information is encoded in the attention computation
  - Quick check question: What makes transformer attention inherently permutation-invariant, and how do positional encodings address this?

## Architecture Onboarding

- Component map: Embedding → Q/K/V projection → HyPE parameter computation → Concatenation → Attention computation → Output
- Critical path: Embedding → Q/K/V projection → HyPE parameter computation → Concatenation → Attention computation → Output
- Design tradeoffs:
  - Memory: O(4Lh) storage vs O(L²) for full mask approaches
  - Computation: Additional matrix multiplication step but no mask storage
  - Flexibility: Learnable parameters possible but approximation breaks with poor µ selection
- Failure signatures:
  - Numerical instability when µ is too large (exponential terms overflow)
  - Poor performance when µ is too small (bias becomes negligible)
  - Memory errors if concatenation dimensions don't align with FlashAttention-2 requirements
- First 3 experiments:
  1. Verify HyPE approximates ALiBi for small sequences with fixed µ=0.01, τ=1
  2. Test gradient backpropagation with learnable τ parameter on small dataset
  3. Compare memory usage and inference speed against ALiBi for varying sequence lengths

## Open Questions the Paper Calls Out

The paper explicitly states that experimental evaluation of HyPE is proposed as a direction for future research due to limited computational resources. The authors defer empirical validation of their theoretical claims, leaving questions about actual performance, compatibility with FlashAttention-2, and practical parameter selection unanswered.

## Limitations

- No experimental validation provided despite theoretical claims about performance
- The truncated series approximation breaks down for larger relative positions
- No empirical guidance on hyperparameter selection (µ, τ values)
- Theoretical compatibility with FlashAttention-2 not practically verified

## Confidence

**High Confidence:** The mathematical derivations connecting hyperbolic functions to relative positional encoding are sound and follow logically from established identities. The core mechanism of using matrix multiplication to avoid O(L²) storage is theoretically valid.

**Medium Confidence:** The approximation of ALiBi's attention bias under specific hyperparameter conditions appears mathematically correct based on the series expansion analysis, though the practical bounds of this approximation are not empirically validated.

**Low Confidence:** Claims about practical performance, compatibility with FlashAttention-2, and suitability for real-world applications cannot be evaluated without experimental results. The paper's assertion that this method is superior or even competitive with existing approaches lacks empirical support.

## Next Checks

1. **Empirical Approximation Validation:** Implement HyPE and ALiBi side-by-side on sequences of varying lengths (10, 50, 100, 512, 2048) and quantitatively measure the difference between their attention biases for various hyperparameter settings. This will establish the practical limits of the approximation claim and identify when the O((j-i)³µ³) terms become significant.

2. **FlashAttention-2 Compatibility Test:** Implement HyPE within a FlashAttention-2 framework and benchmark memory usage, computational overhead, and gradient stability compared to standard FlashAttention-2 without positional encoding. Measure whether the additional O(L) storage for concatenated vectors impacts performance on long sequences.

3. **Generalization Across Sequence Lengths:** Train a small transformer model with HyPE on short sequences (128 tokens) and test its ability to generalize to longer sequences (512, 1024 tokens) without fine-tuning. Compare this length extrapolation capability against ALiBi and standard positional encodings to validate the paper's claims about generalization benefits.