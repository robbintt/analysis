---
ver: rpa2
title: 'Hierarchical Evaluation Framework: Best Practices for Human Evaluation'
arxiv_id: '2310.01917'
source_url: https://arxiv.org/abs/2310.01917
tags:
- evaluation
- human
- system
- were
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a hierarchical evaluation framework for human
  evaluation of NLP systems. The framework addresses three key gaps in current human
  evaluation practices: (1) lack of evaluation metrics for NLP system inputs, (2)
  limited consideration for interdependencies among different characteristics of NLP
  systems, and (3) scarcity of metrics for extrinsic evaluation.'
---

# Hierarchical Evaluation Framework: Best Practices for Human Evaluation

## Quick Facts
- arXiv ID: 2310.01917
- Source URL: https://arxiv.org/abs/2310.01917
- Authors: 
- Reference count: 10
- Primary result: Proposes a hierarchical evaluation framework addressing gaps in NLP human evaluation practices

## Executive Summary
This paper introduces a hierarchical evaluation framework designed to address critical gaps in current human evaluation practices for NLP systems. The framework tackles three key limitations: lack of evaluation metrics for system inputs, insufficient consideration of interdependencies among system characteristics, and scarcity of metrics for extrinsic evaluation. By implementing a two-phase approach that separates testing and evaluation phases, and incorporating a hierarchical structure where system characteristics are interdependent, the framework enables early termination of evaluation when any characteristic fails to meet satisfactory quality. The framework was validated through application to a Machine Reading Comprehension system, demonstrating its effectiveness in capturing associations between input and output quality.

## Method Summary
The paper employs a scoping review of recent NLP literature to identify gaps in human evaluation practices, followed by the proposal of a hierarchical evaluation framework that addresses these gaps. The framework implements a two-phase approach where testers initially assess NLP systems, then evaluators assess both the inputs and outputs. The evaluation process incorporates hierarchical dependencies among system characteristics, enabling early termination if any characteristic fails to meet satisfactory quality. The framework was applied to evaluate a Machine Reading Comprehension system in a pilot randomized controlled trial, using health coaches to assess questions and answers, with results analyzed to demonstrate the framework's effectiveness in capturing relationships between input and output quality.

## Key Results
- Successfully demonstrated framework effectiveness in capturing associations between quality of inputs and outputs for an MRC system
- Implemented hierarchical structure enabling early termination of evaluation when characteristics fail to meet satisfactory quality
- Developed standardized approach addressing three critical gaps: input metrics, interdependencies, and extrinsic evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework allows early termination of evaluation if any characteristic is unsatisfactory, saving evaluator time.
- Mechanism: Hierarchical evaluation structure where each characteristic depends on the satisfactory completion of the previous one. If a characteristic fails, evaluation stops and the composite score is "bad".
- Core assumption: Interdependencies among characteristics mean that a failure in one aspect makes further evaluation of downstream characteristics unnecessary for determining overall system quality.
- Evidence anchors:
  - [abstract] "incorporates a hierarchical structure where the characteristics of the systems are interdependent, enabling early termination evaluation if any evaluated characteristic does not meet satisfactory quality"
  - [section] "the evaluation process continues only if the preceding characteristic(s) is deemed satisfactory. Conversely, if a characteristic is unsatisfactory, the evaluation can be discontinued"
- Break condition: If characteristics are truly independent (no interdependencies), early termination provides no time savings and may miss important system failures.

### Mechanism 2
- Claim: Separating testing and evaluation phases allows assessment of both inputs and outputs, capturing their relationship.
- Mechanism: Two-phase approach where testers use the system with inputs, then evaluators assess both the inputs provided by testers and the corresponding outputs generated by the system.
- Core assumption: The quality of system outputs depends on the quality of inputs, so evaluating both provides a more complete picture of system performance.
- Evidence anchors:
  - [abstract] "The framework was applied to evaluate a Machine Reading Comprehension system, demonstrating its effectiveness in capturing the associations between the quality of inputs and outputs"
  - [section] "we propose a two-phase approach for human evaluation, wherein testers initially assess NLP systems, followed by evaluators who evaluate both the inputs and outputs of the systems"
- Break condition: If system performance is truly independent of input quality, evaluating inputs adds no value and doubles evaluation effort.

### Mechanism 3
- Claim: The framework bridges gaps in NLP evaluation by addressing missing input metrics, interdependencies, and extrinsic evaluation.
- Mechanism: Combines hierarchical structure (addressing interdependencies), two-phase evaluation (addressing input metrics), and system purpose definition (enabling extrinsic evaluation).
- Core assumption: The identified gaps in NLP evaluation are significant problems that can be solved by this combined approach.
- Evidence anchors:
  - [abstract] "The framework addresses three key gaps in current human evaluation practices: (1) lack of evaluation metrics for NLP system inputs, (2) limited consideration for interdependencies among different characteristics of NLP systems, and (3) scarcity of metrics for extrinsic evaluation"
  - [section] "We hope to bridge the aforementioned gaps by providing a standardized human evaluation framework that can be used across different NLP tasks"
- Break condition: If one or more gaps are not actually problematic in practice, the framework's complexity may not be justified.

## Foundational Learning

- Concept: Hierarchical evaluation structures
  - Why needed here: To model dependencies among system characteristics and enable early termination
  - Quick check question: If a system fails the "relevance" check in a hierarchical evaluation, should we still evaluate "clinical accuracy"? (Answer: No, because relevance is a prerequisite for meaningful accuracy evaluation)

- Concept: Intrinsic vs extrinsic evaluation
  - Why needed here: To understand why most NLP evaluation is intrinsic and how this framework enables extrinsic evaluation
  - Quick check question: What's the key difference between intrinsic and extrinsic evaluation? (Answer: Intrinsic evaluates specific qualities independently, extrinsic evaluates system performance in completing its defined purpose)

- Concept: Inter-annotator agreement (IAA)
  - Why needed here: To ensure reliability of human evaluation results
  - Quick check question: If two evaluators disagree on a system's output quality, what metric would you use to quantify this disagreement? (Answer: IAA scores like Cohen's kappa, Fleiss' kappa, or Krippendorf's alpha)

## Architecture Onboarding

- Component map: Purpose definition -> Hierarchical evaluation metric design -> Testing phase coordinator -> Evaluation phase coordinator -> Composite score calculator -> Input/output quality assessment modules

- Critical path: Purpose definition → Metric design → Testing → Evaluation → Composite score calculation

- Design tradeoffs:
  - Granularity vs. efficiency: More characteristics enable finer-grained evaluation but increase evaluation time
  - Flexibility vs. standardization: Framework should be adaptable to different systems while maintaining core principles
  - Automation vs. human judgment: Some evaluation aspects can be automated, but human judgment remains essential for nuanced qualities

- Failure signatures:
  - Early termination without clear stopping criteria indicates metric design issues
  - Low IAA scores suggest ambiguous evaluation criteria
  - Inconsistent composite scores across similar systems suggest evaluation bias

- First 3 experiments:
  1. Implement framework for a simple text classification system, comparing evaluation time and comprehensiveness against traditional methods
  2. Test framework with different dependency structures (linear vs. tree) to identify optimal hierarchy for various system types
  3. Evaluate framework's ability to detect input-output relationships by systematically varying input quality and measuring impact on composite scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal threshold for determining "satisfactory" quality in the hierarchical evaluation framework?
- Basis in paper: [explicit] The framework allows for early termination if any evaluated characteristic does not meet satisfactory quality.
- Why unresolved: The paper does not specify what constitutes "satisfactory" quality for each characteristic.
- What evidence would resolve it: Empirical studies comparing different threshold levels and their impact on evaluation outcomes and time efficiency.

### Open Question 2
- Question: How does the hierarchical evaluation framework perform when applied to different NLP tasks beyond Machine Reading Comprehension?
- Basis in paper: [inferred] The framework is presented as generalizable across different NLP tasks.
- Why unresolved: The paper only demonstrates the framework on a Machine Reading Comprehension system.
- What evidence would resolve it: Comparative studies applying the framework to various NLP tasks (e.g., machine translation, sentiment analysis) and analyzing its effectiveness and adaptability.

### Open Question 3
- Question: What are the specific time-saving benefits of the hierarchical evaluation framework compared to traditional evaluation methods?
- Basis in paper: [explicit] The authors mention investigating potential time-saving benefits in future work.
- Why unresolved: The paper does not provide quantitative data on time savings.
- What evidence would resolve it: Controlled experiments comparing the time required for evaluations using the hierarchical framework versus traditional methods across multiple NLP systems and evaluators.

## Limitations

- Limited generalizability as framework was validated on only one NLP task (Machine Reading Comprehension)
- Missing quantitative data on actual time savings from early termination mechanism
- Incomplete validation of extrinsic evaluation capabilities in real-world deployment scenarios

## Confidence

- High confidence in the framework's logical structure and theoretical validity
- Medium confidence in the framework's practical utility across diverse NLP tasks
- Low confidence in claims about efficiency improvements and comprehensive extrinsic evaluation capabilities

## Next Checks

1. Cross-task validation: Apply the framework to three diverse NLP tasks (e.g., summarization, translation, dialogue systems) and compare evaluation outcomes and times against established baselines.

2. Efficiency measurement: Conduct controlled experiments measuring evaluation time for traditional vs. hierarchical approaches across systems of varying quality, quantifying actual time savings from early termination.

3. Extrinsic performance correlation: Test whether hierarchical evaluation scores predict real-world system performance metrics (user satisfaction, task success rates) in deployment scenarios.