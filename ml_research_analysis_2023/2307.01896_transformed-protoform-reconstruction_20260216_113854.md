---
ver: rpa2
title: Transformed Protoform Reconstruction
arxiv_id: '2307.01896'
source_url: https://arxiv.org/abs/2307.01896
tags:
- dataset
- chinese
- transformer
- languages
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We apply the Transformer architecture to the task of protoform
  reconstruction, inferring ancestral word forms from modern language data. Our method
  modifies the standard encoder-decoder architecture to handle multiple daughter language
  inputs concatenated into a single sequence, using positional encoding and language
  embeddings.
---

# Transformed Protoform Reconstruction

## Quick Facts
- arXiv ID: 2307.01896
- Source URL: https://arxiv.org/abs/2307.01896
- Reference count: 29
- We apply the Transformer architecture to the task of protoform reconstruction, inferring ancestral word forms from modern language data.

## Executive Summary
We present a Transformer-based model for protoform reconstruction that outperforms previous RNN-based approaches by 6.55-8.45% in phoneme edit distance. Our method modifies the standard encoder-decoder architecture to handle multiple daughter language inputs concatenated into a single sequence, using positional encoding and language embeddings. On two datasets - Romance (8,799 cognates) and Chinese (804 cognates) - we achieve state-of-the-art performance while also demonstrating that our model captures phylogenetic relationships among languages.

## Method Summary
The model uses a standard Transformer encoder-decoder architecture with key modifications for the protoform reconstruction task. Input sequences are constructed by concatenating daughter language sequences with positional encoding applied to each daughter sequence individually before concatenation. Language embeddings are added to token embeddings to distinguish between input tokens from different daughter languages. The model is trained on 70/10/20 splits with WandB hyperparameter search and early stopping.

## Key Results
- Outperforms RNN baselines by 6.55-8.45% in phoneme edit distance on Romance and Chinese datasets
- Achieves 1.41-4.03 percentage point improvements in accuracy
- Captures phylogenetic relationships with average neighbor FMR=0.524 on Romance dataset
- Shows better performance on languages with higher cognate percentages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Transformer architecture captures better contextual relationships between phonemes across daughter languages compared to RNNs.
- Mechanism: Self-attention layers in the Transformer can model long-range dependencies and complex relationships between tokens in the concatenated daughter sequences, which is crucial for identifying systematic sound changes.
- Core assumption: The input sequence structure (concatenated daughter languages) can be effectively processed by self-attention to learn reconstruction patterns.
- Evidence anchors:
  - [abstract] "Our model outperforms their model on a suite of different metrics on two different datasets"
  - [section] "We propose a Transformer-based encoder-decoder architecture... to accommodate the structure of our datasets, where multiple daughter sequences correspond to a single protoform sequence"
  - [corpus] Weak - no direct evidence in corpus results about attention mechanisms specifically
- Break condition: If the concatenated sequence becomes too long relative to the protoform, self-attention may struggle to focus on relevant context.

### Mechanism 2
- Claim: Positional encoding and language embeddings help the model distinguish between tokens from different daughter languages.
- Mechanism: By applying positional encoding to each daughter sequence individually before concatenation, and adding language embeddings, the model can maintain the internal structure of each daughter language while learning cross-language patterns.
- Core assumption: The relative position within each daughter sequence matters more than absolute position across the entire concatenated input.
- Evidence anchors:
  - [abstract] "using positional encoding and language embeddings"
  - [section] "Because we only care about the relative position between tokens within each daughter sequence but not across daughter sequences, positional encoding is applied to each individual daughter sequence before concatenation. Along with positional encoding, an additive language embedding is applied to the token embeddings to differentiate between input tokens of different daughter languages"
  - [corpus] Weak - corpus does not directly test this mechanism
- Break condition: If language embeddings are not distinct enough, the model may confuse patterns across languages.

### Mechanism 3
- Claim: The model captures phylogenetic relationships among languages through learned language embeddings.
- Mechanism: By clustering language embeddings derived from the model, we can reconstruct phylogenetic trees that reflect genetic relationships among languages.
- Core assumption: The language embeddings contain information about shared innovations and historical relationships.
- Evidence anchors:
  - [abstract] "We also probe our model for potential phylogenetic signal contained in the model"
  - [section] "We create a distance matrix between every pair of languages... by taking the cosine similarity between a pair's language embeddings"
  - [corpus] Moderate - corpus shows "average neighbor FMR=0.524" indicating some relatedness detection
- Break condition: If language embeddings primarily capture superficial phonetic similarity rather than shared innovations, phylogenetic accuracy will suffer.

## Foundational Learning

- Concept: Sequence-to-sequence modeling with encoder-decoder architecture
  - Why needed here: The task requires mapping multiple input sequences (daughter languages) to a single output sequence (protoform)
  - Quick check question: What happens if you concatenate all daughter sequences without any special handling?

- Concept: Positional encoding for sequence modeling
  - Why needed here: Since we concatenate daughter sequences, we need to maintain the internal order of each sequence while distinguishing them
  - Quick check question: How would the model behave if we didn't apply positional encoding before concatenation?

- Concept: Self-attention mechanisms
  - Why needed here: Self-attention allows the model to weigh the importance of different tokens across the entire input when predicting each output token
  - Quick check question: Why might self-attention be particularly useful for identifying sound correspondences across languages?

## Architecture Onboarding

- Component map: Token embeddings → language embeddings → positional encoding → encoder self-attention → decoder cross-attention → output projection
- Critical path: Token embeddings → language embeddings → positional encoding → encoder self-attention → decoder cross-attention → output projection
- Design tradeoffs:
  - Concatenating daughter sequences reduces model complexity but may lose some cross-sequence dependencies
  - Using positional encoding per sequence preserves internal structure but requires careful handling of boundaries
  - Language embeddings help distinguish sources but add parameters that need to be learned
- Failure signatures:
  - High substitution error rates between phonetically similar phonemes may indicate attention isn't focusing on correct patterns
  - Poor phylogenetic tree reconstruction suggests language embeddings aren't capturing historical relationships
  - Performance degradation on longer sequences may indicate attention limitations
- First 3 experiments:
  1. Train with and without language embeddings to measure their impact on reconstruction accuracy
  2. Test different positional encoding strategies (per sequence vs. global) to see which preserves structure better
  3. Compare attention weight distributions between successful and failed reconstructions to understand model behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Transformer models effectively handle protoform reconstruction for under-documented language families with limited training data (e.g., 200 cognate sets)?
- Basis in paper: [inferred] The paper notes that Transformers are "data hungry" and that Romance and Chinese datasets worked well due to their relatively large size, but suggests that a low-resource setup with 200 cognate sets would not fare well.
- Why unresolved: The paper does not provide experimental results on low-resource scenarios, only speculation about potential performance.
- What evidence would resolve it: Experiments comparing Transformer performance on datasets of varying sizes (e.g., 200, 500, 1000 cognate sets) to establish the minimum data requirements for effective reconstruction.

### Open Question 2
- Question: Do the language embeddings learned by the Transformer model capture genuine shared innovations or merely superficial phonetic similarities when inferring phylogenetic relationships?
- Basis in paper: [explicit] The paper states it's "not clear if the model's language embeddings are learning information that reflects shared innovations... or if the model is learning superficial phonetic similarity."
- Why unresolved: The paper only tested this on the Romance dataset with 5 languages, which is insufficient to draw definitive conclusions about the nature of the learned embeddings.
- What evidence would resolve it: Experiments on larger language families with well-established phylogenies, comparing the model's inferred trees to gold standard phylogenies and analyzing the features learned by language embeddings.

### Open Question 3
- Question: Would alternative sequence modeling architectures (e.g., Perceiver, Set Transformer) perform better than standard Transformers for protoform reconstruction given the task's unique structure?
- Basis in paper: [inferred] The paper discusses how the task requires encoding multiple daughter sequences into a single protoform, and mentions that "concatenating the entire cognate set may not work on language families with hundreds of languages."
- Why unresolved: The paper only compares the standard Transformer to RNN baselines and doesn't explore alternative architectures designed for variable-length or unordered inputs.
- What evidence would resolve it: Comparative experiments testing alternative architectures on both small (Romance) and large (Oceanic-style) language family datasets, measuring reconstruction accuracy and computational efficiency.

## Limitations
- The corpus provides weak direct evidence for the proposed mechanisms, relying mainly on architectural descriptions rather than empirical validation
- No ablation studies testing individual components (language embeddings, positional encoding strategy, etc.)
- Limited understanding of why certain reconstructions succeed or fail

## Confidence
- **High Confidence**: Performance claims showing 6.55-8.45% improvement in phoneme edit distance over RNN baselines
- **Medium Confidence**: Phylogenetic signal detection through language embeddings, with FMR=0.524
- **Low Confidence**: The specific mechanisms explaining why the Transformer works better (self-attention for long-range dependencies, positional encoding strategy, language embeddings for cross-language patterns)

## Next Checks
1. Conduct ablation studies removing language embeddings and using global positional encoding to quantify their individual contributions to performance gains
2. Analyze attention weight distributions for successful vs. failed reconstructions to validate the self-attention mechanism hypothesis
3. Test the model on synthetic datasets with known sound change patterns to verify it captures systematic phonological correspondences rather than memorizing surface similarities