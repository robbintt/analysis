---
ver: rpa2
title: 'Describe, Explain, Plan and Select: Interactive Planning with Large Language
  Models Enables Open-World Multi-Task Agents'
arxiv_id: '2302.01560'
source_url: https://arxiv.org/abs/2302.01560
tags:
- craft
- pickaxe
- mine
- iron
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new interactive planning approach for agents
  in open-world environments like Minecraft. The method, called "Describe, Explain,
  Plan and Select" (DEPS), uses a large language model (LLM) to iteratively generate
  and refine plans based on feedback from the agent's execution.
---

# Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents

## Quick Facts
- arXiv ID: 2302.01560
- Source URL: https://arxiv.org/abs/2302.01560
- Reference count: 40
- This paper proposes a new interactive planning approach for agents in open-world environments like Minecraft, achieving 2x higher success rates compared to baselines and first non-zero success rate on ObtainDiamond task.

## Executive Summary
This paper introduces DEPS (Describe, Explain, Plan, and Select), an interactive planning approach for open-world multi-task agents using large language models (LLMs). The method addresses key challenges in open-world planning: long-term reasoning and plan efficiency. DEPS iteratively generates and refines plans based on feedback from the agent's execution, using an LLM to explain failures, re-plan, and prioritize nearby sub-goals through a goal selector. Experiments on 71 Minecraft tasks demonstrate that DEPS doubles the success rate compared to baselines and achieves the first non-zero success rate on the challenging ObtainDiamond task.

## Method Summary
DEPS combines LLM-based planning with iterative correction and goal prioritization. The approach uses four components: Descriptor (translates agent state to structured text), Explainer/Planner (LLM that identifies errors and generates corrected plans), Selector (neural network that ranks parallel sub-goals by predicted completion time), and Controller (goal-conditioned policy that executes selected sub-goals). The system operates in an iterative loop where failed executions trigger re-planning through the LLM, while the selector ensures efficient goal ordering based on proximity and estimated completion steps.

## Key Results
- DEPS doubles success rate compared to baseline LLM planners on 71 Minecraft tasks
- Achieves first non-zero success rate on the ObtainDiamond task
- Improves plan efficiency through goal selector that prioritizes nearby sub-goals

## Why This Works (Mechanism)

### Mechanism 1
The iterative correction loop fixes flawed initial plans by explicitly identifying and addressing failures. When a sub-goal fails, the descriptor summarizes state and execution outcome, the explainer identifies failure cause, and the planner re-generates a corrected plan. The LLM must accurately identify failure causes and generate correct corrections when given state summaries and failure contexts.

### Mechanism 2
The goal selector improves plan efficiency by prioritizing sub-goals based on proximity to the agent. The selector predicts remaining time steps (horizon) to complete each parallel sub-goal and selects the nearest one as current goal. Horizon prediction must accurately reflect actual steps needed from current state.

### Mechanism 3
Structured prompts with preconditions and effects improve plan interpretability and executability. Goals are formatted with explicit preconditions and effects (e.g., "craft ({wooden pickaxe : 1}, {planks : 3, stick : 2}, crafting table)"), making them more readable for both the LLM and execution controller. The structured format helps LLM generate more precise plans and facilitates error identification during execution.

## Foundational Learning

- **Iterative planning and correction**: Needed because open-world environments have unpredictable dynamics that cause initial plans to fail, requiring real-time correction. Quick check: Why can't we just generate a perfect plan once at the beginning?

- **Horizon prediction for goal prioritization**: Needed because parallel sub-goals need to be ordered efficiently based on agent's current state to minimize execution time. Quick check: What happens if we always execute goals in the order they appear in the plan?

- **Structured language prompting**: Needed because free-form natural language prompts are ambiguous and hard to parse for both planning and execution. Quick check: How does adding preconditions and effects help the LLM generate better plans?

## Architecture Onboarding

- **Component map**: Task → Prompt → LLM Plan → Selector → Controller → Execution → Descriptor → Explainer → Re-plan → Repeat until success
- **Critical path**: Task → Prompt → LLM Plan → Selector → Controller → Execution → Descriptor → Explainer → Re-plan → Repeat until success
- **Design tradeoffs**: LLM accuracy vs. token limits (8000 token cap), selector complexity vs. real-time performance, structured prompts vs. flexibility, iterative correction vs. plan generation speed
- **Failure signatures**: Plan generation fails repeatedly despite multiple correction rounds, selector consistently chooses suboptimal goals, descriptor fails to capture relevant state information, controller fails to execute even corrected plans
- **First 3 experiments**: 1) Run single task with max 3 correction rounds vs. no-correction baseline, 2) Test selector performance with 2, 3, and 4 parallel goals, 3) Vary prompt structure (with/without preconditions/effects) and measure plan executability

## Open Questions the Paper Calls Out

### Open Question 1
How does DEPS performance scale with increasing task complexity and reasoning steps? The paper demonstrates effectiveness on 71 Minecraft tasks but doesn't systematically analyze how performance changes as tasks require more reasoning steps (2-12 steps mentioned).

### Open Question 2
How robust is DEPS to different LLM models and their varying capabilities? The paper uses GPT-3.5 and Codex but doesn't explore how different LLM architectures or sizes impact DEPS performance.

### Open Question 3
How does DEPS generalize to other open-world environments beyond Minecraft? While the paper mentions potential generalization to other domains, it only provides empirical evidence within Minecraft.

## Limitations
- LLM-based iterative correction assumes the model can accurately identify and fix complex failure patterns for multi-step plans with 12+ reasoning steps, but this is unverified
- Horizon prediction quality is untested beyond specific Minecraft task distribution, raising generalizability concerns
- Token limitations (8000 tokens) may restrict plan complexity for real-world applications

## Confidence

- **High confidence**: Plan execution using behavior cloning controller, success rate improvements on 71 Minecraft tasks, selector module design
- **Medium confidence**: LLM's ability to identify failure causes and generate corrections, horizon prediction accuracy for goal ordering
- **Low confidence**: Claims about enabling "open-world" agents and solving ObtainDiamond task first-time without knowing task complexity distribution

## Next Checks

1. Test DEPS on a held-out task distribution with reasoning steps > 12 to evaluate scalability limits
2. Implement ablation study removing the selector to quantify efficiency gains from goal prioritization
3. Measure LLM correction accuracy by logging failure identification rate and correction validity across 100+ failure cases