---
ver: rpa2
title: 'Causal Discovery from Temporal Data: An Overview and New Perspectives'
arxiv_id: '2303.10112'
source_url: https://arxiv.org/abs/2303.10112
tags:
- causal
- data
- discovery
- time
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive overview of causal discovery
  from temporal data, addressing two main categories: multivariate time series (MTS)
  causal discovery and event sequence causal discovery. For MTS, it reviews constraint-based,
  score-based, functional causal model (FCM)-based, and Granger causality-based methods,
  highlighting recent advances such as continuous optimization approaches (e.g., DYNOTEARS,
  NTS-NOTEARS) and neural network-based Granger causality methods (e.g., cMLP, cLSTM,
  eSRU).'
---

# Causal Discovery from Temporal Data: An Overview and New Perspectives

## Quick Facts
- arXiv ID: 2303.10112
- Source URL: https://arxiv.org/abs/2303.10112
- Reference count: 40
- Key outcome: Comprehensive overview of causal discovery from temporal data, covering both multivariate time series and event sequence methods, with new perspectives on amortized learning, supervised learning, and causal representation learning.

## Executive Summary
This paper provides a thorough survey of causal discovery methods for temporal data, categorizing approaches for multivariate time series (MTS) into constraint-based, score-based, functional causal model (FCM)-based, and Granger causality-based methods. For event sequences, the focus is on Granger causality methods leveraging Hawkes processes and neural point processes. The paper also introduces new perspectives including amortized and supervised causal discovery paradigms and causal representation learning to address challenges like non-stationarity, heterogeneity, unobserved confounders, and subsampling.

## Method Summary
The paper reviews existing causal discovery methods for temporal data, grouping them into categories based on their underlying assumptions and techniques. For MTS data, it covers constraint-based methods (using conditional independence tests), score-based methods (optimizing model fit with complexity penalties), FCM-based methods (using functional relationships with noise independence), and Granger causality methods (exploiting predictive asymmetry). For event sequences, it focuses on Granger causality methods using Hawkes processes and neural point processes. The paper also proposes new perspectives on amortized and supervised causal discovery, as well as causal representation learning, to improve scalability and handle complex temporal dependencies.

## Key Results
- Temporal causal discovery can be effectively categorized into four methodological families for MTS data, each exploiting different statistical regularities.
- Granger causality-based methods for event sequences can leverage the natural match-up with Hawkes processes to infer causal relations from irregularly sampled data.
- Recent advances in amortized and supervised causal discovery paradigms offer promising approaches to overcome small sample and heterogeneity issues.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal causal discovery from multivariate time series (MTS) can be decomposed into four distinct methodological families—constraint-based, score-based, functional causal model (FCM)-based, and Granger causality-based approaches—each exploiting different statistical regularities in the data.
- Mechanism: Each family leverages a unique source of information: constraint-based methods rely on conditional independence tests, score-based methods optimize model fit penalized by complexity, FCM-based methods use functional relationships plus noise independence, and Granger methods exploit predictive asymmetry in lagged variables.
- Core assumption: The underlying causal process satisfies at least one of the families' statistical assumptions (e.g., faithfulness, non-Gaussianity, or predictability) and the data is sufficiently rich to reveal these regularities.
- Evidence anchors:
  - [abstract] "According to whether the temporal data is calibrated, the temporal data for causal discovery can be categorized into two groups, i.e., multivariate time series (MTS) and event sequence causal discovery."
  - [section] "MTS data, describing the calibrated states of multiple variables changing over time, is a general kind of temporal data in many domains... existing works can be grouped into four categories, i.e., constraint-based methods, score-based methods, functional causal model (FCM)-based methods and Granger causality methods."
- Break condition: If none of the statistical assumptions hold (e.g., data is purely Gaussian with instantaneous effects and no predictive asymmetry), none of the four families can reliably recover the causal graph.

### Mechanism 2
- Claim: Granger causality-based methods for event sequences can exploit the natural match-up between Granger causality and Hawkes processes, enabling inference of causal relations from irregularly sampled event data.
- Mechanism: Hawkes processes model the conditional intensity of each event type as a function of past events of all types, with the impact functions directly encoding Granger causal strength. Estimating these impact functions yields the causal graph.
- Core assumption: The Hawkes process model (or its variants like Wold processes or neural point processes) adequately captures the temporal dynamics of the event sequence, and the impact functions are identifiable from the data.
- Evidence anchors:
  - [abstract] "For event sequences, it focuses on Granger causality-based methods, particularly those leveraging Hawkes processes and neural point processes..."
  - [section] "The Hawkes process is a type of point process that has a fixed form of intensity function... ej does not Granger-cause ei ⇐⇒ φeiej(s) = 0,∀s∈R"
- Break condition: If the event sequence dynamics are not well modeled by point processes (e.g., batch arrivals or deterministic scheduling), the Hawkes-based Granger inference will be misspecified.

### Mechanism 3
- Claim: Recent advances in amortized and supervised causal discovery paradigms can overcome small sample and heterogeneity issues by learning to infer causal graphs across multiple individuals or domains in a single global model.
- Mechanism: Amortized modeling trains a single encoder-decoder network that takes individual time series as input and outputs its causal graph, sharing parameters across individuals. Supervised learning treats causal graph inference as a classification/regression task with labeled graph structures from synthetic data.
- Core assumption: There exist common structural patterns across individuals or domains that the model can learn, and sufficient labeled or unlabeled data is available to train the global model.
- Evidence anchors:
  - [abstract] "The paper introduces new perspectives, including amortized and supervised causal discovery paradigms and causal representation learning..."
  - [section] "In amortized modeling, a global causal discovery framework is trained for individuals with different causal structures... Another line of work has predominately focused on treating the inference process as a black box and learning the mapping from sample data to causal graph structures via supervised learning."
- Break condition: If individuals have entirely unique causal structures with no shared patterns, or if labeled data is unavailable, the global model will fail to generalize.

## Foundational Learning

- Concept: Conditional independence testing
  - Why needed here: Core to constraint-based methods; determines edges to remove in skeleton construction and orientation rules.
  - Quick check question: If X ⊥ ⊥ Y | Z in a constraint-based algorithm, what happens to the edge between X and Y?
- Concept: Vector autoregression (VAR) and Granger causality
  - Why needed here: Basis for Granger causality; VAR models the linear dynamics of multivariate time series, and Granger causality tests whether past values of one series help predict another.
  - Quick check question: In a VAR(p) model, how do you test if series i Granger-causes series j?
- Concept: Structural equation models (SEMs) and functional causal models
  - Why needed here: Framework for FCM-based methods; each variable is a function of its direct causes and independent noise, enabling orientation via noise independence.
  - Quick check question: In an additive noise model X → Y with Y = f(X) + U, what property of U and X allows identifying the direction?

## Architecture Onboarding

- Component map:
  - Data ingestion → Preprocessing (handling non-stationarity, subsampling) → Causal discovery method selection (constraint, score, FCM, Granger) → Model fitting → Graph output → Validation
  - For event sequences: Data ingestion → Point process modeling (Hawkes, Wold, neural) → Impact function estimation → Granger causality extraction → Graph output
- Critical path:
  - For MTS: Preprocess → Choose method family → Fit model (e.g., run PCMCI for constraint, DYNOTEARS for score) → Validate (AUROC, SHD) → Output graph
  - For event sequences: Preprocess → Choose point process model → Estimate parameters (MLE, GMM, etc.) → Extract Granger causality → Validate → Output graph
- Design tradeoffs:
  - Constraint-based: Robust to model misspecification but computationally heavy for large d; assumes faithfulness.
  - Score-based: Efficient with continuous optimization but may need large samples; assumes acyclicity or handle cycles.
  - FCM-based: Can identify direction without faithfulness; limited to specific functional forms (linear, additive).
  - Granger: Intuitive for prediction; requires temporal precedence; may miss instantaneous effects.
- Failure signatures:
  - Constraint: High false positives if faithfulness fails; slow runtime for high dimensions.
  - Score: Local optima in combinatorial search; convergence issues in continuous optimization.
  - FCM: Misidentification if noise independence assumption violated; poor scalability.
  - Granger: Spurious causality from latent confounders; inability to detect contemporaneous effects.
- First 3 experiments:
  1. Run PCMCI on Lorenz-96 data to test constraint-based performance on known nonlinear dynamics.
  2. Run DYNOTEARS on linear VAR data to test continuous optimization for linear Granger causality.
  3. Run Hawkes-based MLE on MemeTracker data to test Granger causality for event sequences with known ground truth.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively integrate expert knowledge into temporal causal discovery algorithms beyond simple hard constraints or soft penalties?
- Basis in paper: [explicit] The paper discusses three types of expert knowledge integration: soft constraints, hard constraints, and interactive learning, but notes that these approaches have limitations.
- Why unresolved: The paper mentions these approaches but doesn't provide a comprehensive framework for effectively combining expert knowledge with automated causal discovery methods.
- What evidence would resolve it: A systematic evaluation comparing different methods of expert knowledge integration, demonstrating improved performance and robustness across diverse real-world datasets.

### Open Question 2
- Question: What are the fundamental theoretical limits of causal identifiability in temporal data with latent confounders, and how can we develop methods that approach these limits?
- Basis in paper: [explicit] The paper discusses unobserved confounders as a major challenge and mentions that most methods cannot handle them effectively.
- Why unresolved: While the paper surveys existing approaches for dealing with latent confounders, it doesn't provide a theoretical framework for understanding the fundamental limits of what can be identified in the presence of hidden variables.
- What evidence would resolve it: A formal characterization of the conditions under which causal structures can be identified from temporal data with latent confounders, along with algorithms that provably approach these theoretical limits.

### Open Question 3
- Question: How can we develop causal discovery methods that are both scalable to high-dimensional temporal data and maintain theoretical guarantees of correctness?
- Basis in paper: [explicit] The paper mentions scalability as a challenge for many existing methods, particularly when dealing with high-dimensional time series or event sequences.
- Why unresolved: There's a tension between scalability and correctness - many scalable methods sacrifice theoretical guarantees, while methods with strong guarantees don't scale well to real-world data.
- What evidence would resolve it: A method that achieves both polynomial-time complexity and provable asymptotic correctness on high-dimensional temporal datasets, validated across multiple real-world applications.

## Limitations
- Limited empirical validation of the discussed methods on real-world datasets.
- New perspectives (amortized and supervised learning, causal representation learning) are presented as future directions without concrete implementations or results.
- Lack of discussion on scalability to very high-dimensional data and sensitivity to data quality issues.

## Confidence
- New perspectives (amortized/supervised learning): Low confidence - presented as future directions without empirical validation
- Four-category decomposition of MTS methods: Medium confidence - well-supported by literature but may not generalize to all scenarios
- Hawkes-Granger match-up for event sequences: Medium confidence - theoretically sound but limited empirical evidence

## Next Checks
1. **Empirical Validation**: Implement and evaluate DYNOTEARS and PCMCI on a benchmark dataset (e.g., Lorenz-96) to compare their performance in recovering known causal structures.

2. **Scalability Test**: Assess the scalability of the reviewed methods on high-dimensional temporal data (e.g., 100+ variables) to identify bottlenecks and potential solutions.

3. **Robustness Analysis**: Investigate the robustness of Granger causality-based methods to non-stationary data by applying them to synthetic datasets with time-varying causal structures.