---
ver: rpa2
title: Large Language Models for Travel Behavior Prediction
arxiv_id: '2312.00819'
source_url: https://arxiv.org/abs/2312.00819
tags:
- travel
- llms
- prediction
- train
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores using large language models (LLMs) to predict
  travel behavior, specifically mode choice, without requiring training data. The
  approach uses carefully engineered prompts that describe the task, travel characteristics,
  individual attributes, and domain knowledge.
---

# Large Language Models for Travel Behavior Prediction

## Quick Facts
- arXiv ID: 2312.00819
- Source URL: https://arxiv.org/abs/2312.00819
- Authors: 
- Reference count: 7
- Key outcome: Zero-shot LLM predictions achieve 65.5% accuracy on travel mode choice, competitive with traditional supervised models

## Executive Summary
This study demonstrates that large language models can predict travel behavior, specifically mode choice, without requiring training data. The approach uses carefully engineered prompts that describe the task, travel characteristics, individual attributes, and domain knowledge. Testing on a Swiss travel survey dataset, the LLM-based predictions achieved competitive accuracy (65.5%) and F1-score (64.8%) compared to traditional supervised models like multinomial logit, random forest, and neural networks. The LLM can also provide interpretable reasoning for its predictions, though some cases showed logical errors or hallucinations. This work represents a paradigm shift from data-driven mathematical models to leveraging LLM reasoning capabilities for travel behavior prediction.

## Method Summary
The study uses prompt engineering to enable zero-shot travel behavior prediction with large language models. The approach involves constructing prompts that include a task description, travel characteristics (origin, destination, travel time, travel cost), individual attributes (e.g., regular train user, train annual pass), and domain knowledge guides. These prompts are fed to GPT-3.5 Turbo with temperature=0 to generate mode choice predictions and reasoning explanations. The method is tested on a Swiss travel survey dataset and compared against benchmark models including multinomial logit, random forest, and neural networks.

## Key Results
- Zero-shot LLM predictions achieved 65.5% accuracy and 64.8% F1-score on travel mode choice
- LLM performance was competitive with traditional supervised models (multinomial logit, random forest, neural networks)
- LLM successfully provided interpretable reasoning for predictions in most cases
- Some predictions contained logical errors and hallucinations despite competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Zero-shot LLM predictions achieve competitive accuracy by leveraging in-context reasoning without parameter training
- Mechanism: The LLM uses prompt engineering to incorporate task description, travel characteristics, individual attributes, and domain knowledge, enabling it to reason about travel mode choice using only the provided context
- Core assumption: LLMs possess sufficient pre-trained knowledge about transportation concepts and human decision-making to make reasonable predictions without specific training data
- Evidence anchors:
  - [abstract] "though no training samples are provided, LLM-based predictions have competitive accuracy and F1-score as canonical supervised learning methods"
  - [section 3.2] "The framework of the LLM-based travel behavior prediction is shown in Figure 1... The input information will be organized and embedded into the prompt"
  - [corpus] Weak evidence - only 1 of 5 related papers directly addresses zero-shot LLM travel prediction, suggesting this is a novel approach
- Break condition: If the LLM lacks sufficient pre-trained knowledge about transportation concepts or the prompt fails to provide adequate context for reasoning

### Mechanism 2
- Claim: Chain-of-thought prompting improves LLM reasoning for numerical comparisons in travel decisions
- Mechanism: The prompt explicitly guides the LLM through numerical reasoning steps, comparing travel times and costs across modes to determine optimal choices
- Core assumption: LLMs can perform arithmetic reasoning when properly prompted, despite known limitations in this area
- Evidence anchors:
  - [section 3.3.4] "Experiments show that this tends to be an effective way to enable LLMs for arithmetic reasoning in this context"
  - [section 3.3.4] "Even for a simple task like providing LLMs with three numbers A, B, and C, and asking them to sort the numbers, LLMs can make many mistakes"
  - [corpus] Weak evidence - no corpus papers directly address LLM arithmetic reasoning for travel prediction
- Break condition: If the LLM still fails at numerical comparisons despite explicit prompting, or if the travel attributes are too complex for the prompting strategy

### Mechanism 3
- Claim: Including human common sense and domain knowledge in prompts compensates for lack of training data
- Mechanism: The prompt incorporates transportation domain knowledge and common sense reasoning rules that guide the LLM's decision-making process
- Core assumption: Pre-trained LLMs contain sufficient general knowledge about human travel behavior and transportation concepts to apply when guided by domain-specific prompts
- Evidence anchors:
  - [section 3.3.4] "The prompts should include domain knowledge with human common sense"
  - [section 4.3.2] Case 1 shows LLM correctly predicting travel mode with reasonable explanations when provided with appropriate context
  - [corpus] Weak evidence - corpus papers focus on other aspects of LLM travel prediction but don't directly address domain knowledge incorporation
- Break condition: If the LLM's pre-trained knowledge doesn't align with the domain assumptions, or if the prompts fail to effectively communicate the relevant domain knowledge

## Foundational Learning

- Concept: Prompt engineering techniques for LLMs
  - Why needed here: The entire approach relies on carefully designed prompts rather than model training
  - Quick check question: What are the four key components of the travel behavior prediction prompt?

- Concept: Zero-shot vs few-shot learning in LLMs
  - Why needed here: The study specifically uses zero-shot prompting, distinguishing it from few-shot approaches
  - Quick check question: How does zero-shot prompting differ from few-shot in terms of training data requirements?

- Concept: Travel behavior modeling fundamentals
  - Why needed here: Understanding conventional approaches (like multinomial logit) provides context for evaluating LLM performance
  - Quick check question: What are the key inputs typically used in traditional travel mode choice models?

## Architecture Onboarding

- Component map: Task description -> Travel characteristics -> Individual attributes -> Domain knowledge guides -> LLM inference engine -> Prediction + reasoning
- Critical path: Prompt design → LLM inference → Result extraction
- Design tradeoffs:
  - Zero-shot approach sacrifices potential accuracy gains from training for flexibility and data efficiency
  - Longer, more detailed prompts improve reasoning but increase token costs and processing time
  - Temperature=0 ensures deterministic output but may reduce creative problem-solving
- Failure signatures:
  - Incorrect predictions with illogical explanations (hallucinations)
  - Failure to perform numerical comparisons despite explicit prompting
  - Over-reliance on certain features while ignoring others
- First 3 experiments:
  1. Test LLM performance with simplified prompts (only task description and travel characteristics) to establish baseline
  2. Add individual attributes to prompt and measure performance change
  3. Include full domain knowledge guide and compare against traditional models on same dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can prompt engineering be optimized to reduce hallucinations in LLM-based travel behavior predictions while maintaining competitive accuracy?
- Basis in paper: [explicit] The paper explicitly mentions observing hallucinations in LLM outputs and the need to improve reasoning while reducing errors.
- Why unresolved: Current prompts still produce some hallucinated or illogical explanations despite achieving competitive prediction accuracy.
- What evidence would resolve it: Systematic ablation studies comparing different prompt structures and evaluation of hallucination frequency across varied travel behavior tasks.

### Open Question 2
- Question: What is the optimal balance between zero-shot and few-shot prompting for LLM-based travel behavior prediction across different travel tasks?
- Basis in paper: [explicit] The authors suggest future work should test few-shot prompting with representative samples for in-context learning.
- Why unresolved: The paper only demonstrates zero-shot prompting, leaving open whether including training samples would improve performance.
- What evidence would resolve it: Comparative experiments testing zero-shot, few-shot, and varying numbers of examples across multiple travel behavior prediction tasks.

### Open Question 3
- Question: How do different LLM architectures compare in their ability to predict travel behavior without training data?
- Basis in paper: [explicit] The authors suggest testing performance of different LLMs as future work.
- Why unresolved: The study only used GPT-3.5, making it unclear whether other models like GPT-4, Claude, or open-source alternatives would perform better.
- What evidence would resolve it: Direct comparison of multiple LLM architectures using identical prompts across the same travel behavior datasets.

## Limitations
- Zero-shot approach relies heavily on prompt engineering quality, making it sensitive to prompt design
- Observed 65.5% accuracy still leaves significant prediction error unexplained
- Single dataset from Swiss travel survey limits generalizability across different cultural contexts
- Some predictions contain logical errors and hallucinations despite competitive accuracy

## Confidence
- **High Confidence**: The LLM can achieve competitive accuracy to traditional models when properly prompted (supported by direct experimental comparison)
- **Medium Confidence**: The zero-shot approach eliminates need for training data while maintaining reasonable performance (based on single dataset results)
- **Low Confidence**: The LLM consistently provides reliable, logical explanations for its predictions (contradicted by identified hallucinations and errors)

## Next Checks
1. **Cross-dataset validation**: Test the same prompting approach on travel behavior datasets from different countries and transportation contexts to assess generalizability
2. **Error analysis framework**: Systematically categorize prediction errors to determine whether failures stem from prompt design, insufficient domain knowledge, or fundamental LLM reasoning limitations
3. **Numerical reasoning stress test**: Create controlled experiments specifically designed to test the LLM's arithmetic capabilities with increasingly complex travel cost/time comparisons to validate the chain-of-thought prompting effectiveness