---
ver: rpa2
title: Unsupervised Segmentation of Colonoscopy Images
arxiv_id: '2312.12599'
source_url: https://arxiv.org/abs/2312.12599
tags:
- segmentation
- unsupervised
- images
- features
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the application of self-supervised vision
  transformers (ViTs) to unsupervised semantic segmentation of colonoscopy images,
  with the aim of discovering clinically relevant mucosal features without manual
  annotations. Using the Deep Spectral method, patch-level features from DINO-trained
  ViTs are clustered to segment images into interpretable mucosal regions.
---

# Unsupervised Segmentation of Colonoscopy Images

## Quick Facts
- arXiv ID: 2312.12599
- Source URL: https://arxiv.org/abs/2312.12599
- Reference count: 40
- Primary result: Self-supervised vision transformers enable unsupervised discovery of clinically relevant mucosal features in colonoscopy images.

## Executive Summary
This work applies self-supervised vision transformers (ViTs) to unsupervised semantic segmentation of colonoscopy images, discovering clinically meaningful mucosal regions without manual annotations. Using the Deep Spectral method, patch-level features from DINO-trained ViTs are clustered to segment images into interpretable mucosal regions. The approach achieves image classification performance approaching fully supervised DenseNet levels and demonstrates strong polyp detection capabilities. The method successfully identifies six clinically meaningful concepts including normal vasculature, erythema, and bleeding across a small internal dataset.

## Method Summary
The method extracts patch-level features from DINO-trained ViTs, computes affinity matrices combining feature and color affinities, applies spectral clustering to segment images, then uses K-means clustering across images to group similar structures. This Deep Spectral pipeline enables discovery of interpretable mucosal concepts without requiring labeled data. The approach is evaluated on both public (HyperKvasir) and proprietary (Etro) colonoscopy datasets, with linear probing and KNN classifiers assessing image classification performance, while segmentation quality is evaluated qualitatively and through polyp detection metrics.

## Key Results
- Image classification performance with linear probing approaches fully supervised DenseNet levels (micro-F1 ~75%)
- Patch-level features yield strong polyp detection (F1@IoU=0.3 up to 84%)
- Unsupervised segmentation identifies six clinically meaningful concepts including normal vasculature, erythema, and bleeding
- Domain-specific models perform best on mucosal classification and segmentation, while ImageNet-trained models excel at general object detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ViTs trained with DINO learn patch-level semantic features that capture clinically meaningful structures without pixel-level labels.
- Mechanism: Self-supervised contrastive learning encourages the model to align features within the same image while differentiating between different views, forcing the network to extract semantically coherent patch representations. These patch embeddings retain rich spatial and contextual information useful for segmentation.
- Core assumption: Patch-level embeddings from a DINO-trained ViT encode enough semantic detail to distinguish between different tissue types and structures in colonoscopy images.
- Evidence anchors: [abstract] "patch-level features from DINO-trained ViTs are clustered to segment images into interpretable mucosal regions"; [section] "ViTs learn rich patch-level semantic features, even in networks trained with image-level contrastive objectives"
- Break condition: If patch embeddings lose semantic coherence (e.g., due to poor augmentation or architecture choices), clustering will fail to produce interpretable segments.

### Mechanism 2
- Claim: Combining feature affinity with color affinity in spectral clustering yields more interpretable mucosal segments.
- Mechanism: The feature affinity captures semantic similarity between patches, while the color affinity enforces low-level consistency (e.g., similar hues likely belong together). Their weighted sum in the affinity matrix improves the stability of clustering across diverse mucosal regions.
- Core assumption: Color similarity aligns with semantic similarity in colonoscopy images for mucosal regions.
- Evidence anchors: [section] "We found that the color affinity matrix helped to break a single image into more semantically meaningful segments"; [section] "With a weight of 1.0, it aided in generating more interpretable concepts in mucosal feature discovery"
- Break condition: If the color affinity conflicts with semantic boundaries (e.g., in cases of color bleeding or specular highlights), it may degrade segmentation quality.

### Mechanism 3
- Claim: Cross-image K-means clustering of segment embeddings consolidates semantically consistent regions across the dataset.
- Mechanism: After spectral clustering breaks an image into segments, feeding each segment back into the ViT produces segment-level features. K-means clustering these features across all images groups similar structures, enabling discovery of recurring mucosal patterns.
- Core assumption: Segment-level embeddings from ViT patches preserve sufficient information to cluster similar mucosal structures across images.
- Evidence anchors: [abstract] "feed each segment back into the ViT to generate segment-level features, which are then clustered across images using K-means"; [section] "patch embeddings from the last four blocks were extracted and averaged to represent the whole segment"
- Break condition: If segment embeddings are not discriminative enough (e.g., due to limited ViT capacity or poor feature extraction), K-means will produce inconsistent or meaningless clusters.

## Foundational Learning

- Concept: Self-supervised learning via contrastive objectives
  - Why needed here: Avoids reliance on scarce labeled medical data while still enabling feature extraction useful for downstream tasks.
  - Quick check question: What is the main difference between supervised and self-supervised pretraining in vision transformers?

- Concept: Vision transformer patch embeddings and their semantic capacity
  - Why needed here: ViTs operate on patches, so understanding how patch embeddings capture semantics is key to interpreting segmentation results.
  - Quick check question: How do ViTs encode spatial relationships if they process patches independently?

- Concept: Spectral clustering and affinity matrices
  - Why needed here: Spectral clustering is used to break images into semantically coherent segments; understanding affinity construction is critical.
  - Quick check question: What role does the affinity matrix play in spectral clustering, and how does combining feature and color affinities help?

## Architecture Onboarding

- Component map: ViT feature extraction -> Affinity matrix builder -> Spectral clustering -> Segment featurizer -> K-means clustering -> Mask generation
- Critical path: ViT feature extraction → affinity matrix → spectral clustering → segment embedding → K-means → mask generation
- Design tradeoffs: Domain-specific models yield better mucosal interpretation but ImageNet models generalize better to salient objects; balancing model choice depends on downstream task.
- Failure signatures: Poor segmentation masks, inconsistent clusters, or lack of interpretability suggest issues in feature extraction, affinity weighting, or clustering granularity.
- First 3 experiments:
  1. Validate patch embeddings: Run a KNN classifier on image-level features to confirm semantic richness before segmentation.
  2. Test spectral clustering: Apply to a small set of images with different affinity weightings to find optimal balance.
  3. Evaluate K-means stability: Vary number of clusters and check consistency of discovered concepts across repeated runs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of semantic clusters to discover in colonoscopy images without losing interpretability or clinical relevance?
- Basis in paper: [explicit] The paper clusters into 15 segments but manually identifies only 6 interpretable clusters, suggesting potential challenges in determining the optimal cluster count.
- Why unresolved: The paper manually reviews clusters to identify interpretable concepts, but does not explore systematic methods for determining the optimal number of clusters.
- What evidence would resolve it: A systematic study comparing different cluster numbers with clinical expert validation to identify the optimal balance between granularity and interpretability.

### Open Question 2
- Question: How do SSL representations from different pre-training datasets (ImageNet vs domain-specific) differ in their ability to capture subtle mucosal features versus general structural features?
- Basis in paper: [explicit] The paper observes that ImageNet models excel at structural tasks while domain-specific models perform better at subtle mucosal differences.
- Why unresolved: The paper does not provide a detailed analysis of what specific features each type of model captures or why these differences exist.
- What evidence would resolve it: Feature visualization and ablation studies comparing what features are activated in each model type when processing different mucosal regions.

### Open Question 3
- Question: Can the unsupervised segmentation method be extended to discover entirely novel mucosal features that are not currently recognized by clinicians?
- Basis in paper: [inferred] The paper mentions "unbiased discovery of new and unbiased mucosal features" as a goal, but only validates against known clinical concepts.
- Why unresolved: The paper validates discovered features against existing clinical knowledge but does not explore whether the method could identify previously unknown patterns.
- What evidence would resolve it: A longitudinal study tracking whether discovered features correlate with patient outcomes in ways not explained by current clinical metrics.

## Limitations

- The semantic richness of patch-level embeddings from DINO-trained ViTs in colonoscopy images is largely inferred from general computer vision literature rather than directly validated for medical imaging.
- The reliance on proprietary datasets (Etro) limits reproducibility and requires use of public substitutes like HyperKvasir.
- The domain-specific performance gap between ImageNet and domain-trained models remains underexplained with limited feature-level analysis.

## Confidence

- **High Confidence**: The application of self-supervised ViTs to image classification (via linear probing and KNN) is well-established and aligns with prior work in self-supervised learning.
- **Medium Confidence**: The use of Deep Spectral clustering to generate interpretable mucosal segments is novel but supported by qualitative results; quantitative validation is limited.
- **Low Confidence**: The assertion that color affinity significantly improves segmentation interpretability lacks strong empirical backing or comparison with alternative methods.

## Next Checks

1. **Ablation on Affinity Matrix Weighting**: Systematically vary the weight of the color affinity term in spectral clustering and evaluate its impact on segmentation quality and interpretability across multiple datasets.

2. **Cross-Dataset Generalization**: Test the pipeline on additional public colonoscopy datasets (e.g., CVC-ClinicDB) to assess robustness and generalizability of discovered mucosal concepts.

3. **Semantic Embedding Validation**: Perform controlled experiments (e.g., KNN classification, feature visualization) to directly verify the semantic richness of patch embeddings from DINO-trained ViTs on colonoscopy images.