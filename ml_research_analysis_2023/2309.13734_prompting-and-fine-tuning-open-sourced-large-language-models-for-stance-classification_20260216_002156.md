---
ver: rpa2
title: Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification
arxiv_id: '2309.13734'
source_url: https://arxiv.org/abs/2309.13734
tags:
- stance
- statement
- classification
- data
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of Large Language Models (LLMs)
  for stance classification without fine-tuning. The authors evaluate ten open-source
  models and seven prompting schemes across five benchmark datasets, finding that
  LLMs can achieve performance competitive with in-domain supervised models.
---

# Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification

## Quick Facts
- arXiv ID: 2309.13734
- Source URL: https://arxiv.org/abs/2309.13734
- Reference count: 19
- Key outcome: Large language models can perform stance classification competitively with supervised models using context-enriched prompts, but results vary significantly by architecture and prompting scheme.

## Executive Summary
This study investigates the use of large language models (LLMs) for stance classification without fine-tuning, evaluating ten open-source models and seven prompting schemes across five benchmark datasets. The research finds that context-enriched prompts consistently improve LLM performance, with encoder-decoder architectures like T5 outperforming decoder-only models such as GPT-NeoX. While LLMs can achieve results competitive with in-domain supervised models, the study reveals significant variability in performance across different datasets and prompting approaches, suggesting stance classification remains a challenging task for current LLMs.

## Method Summary
The authors evaluated ten open-source LLMs across five benchmark datasets using four prompting schemes: Task-only, Context, Context + Few-Shot Examples, and Context + Few-Shot + Reasoning. They compared encoder-decoder models (Flan-UL2, Flan-Alpaca-GPT4-T5) with decoder-only models (GPT-NeoX, Falcon, MPT, Llama-2) using unweighted macro-F1 accuracy as the primary metric. The study focused on zero-shot and few-shot settings without fine-tuning, examining how different prompt formulations affect model performance on stance classification tasks.

## Key Results
- Context-enriched prompts consistently improved LLM performance across all datasets
- Encoder-decoder architectures (T5) significantly outperformed decoder-only models (GPT-NeoX) in stance classification
- Using the term "stance" rather than "sentiment" in prompts led to better performance
- Few-shot examples improved performance but introduced instability and reasoning issues
- LLMs achieved competitive results with in-domain supervised models on some datasets but underperformed on others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-enriched prompts consistently improve LLM performance for stance classification.
- Mechanism: Providing explicit contextual information (target entity and topic) reduces ambiguity in stance definitions, allowing the model to better understand the author's viewpoint.
- Core assumption: Stance is inherently context-dependent, and without proper context, the model cannot accurately infer the author's perspective.
- Evidence anchors:
  - [abstract] "Context-enriched prompts consistently improve LLM performance"
  - [section] "the inclusion of context into the prompt was the single factor that most increased the performance of the LLMs"
  - [corpus] Weak evidence - no corpus neighbor papers directly address context-enriched prompts

### Mechanism 2
- Claim: Encoder-decoder architectures (like T5) perform better than decoder-only models (like GPT-NeoX) for stance classification without fine-tuning.
- Mechanism: Encoder-decoder models are inherently designed to understand context and generate relevant output, making them well-suited for tasks like stance detection that require understanding and classification.
- Core assumption: The architecture of encoder-decoder models is more compatible with stance classification tasks than decoder-only models.
- Evidence anchors:
  - [section] "encoder-decoder models, such as the T-5 model, demonstrated successful performance on the stance prediction task, in a zero-shot setting and with no additional model training. On the other hand, decoder-only models were not able to perform the task as effectively under the same scenario"
  - [corpus] Weak evidence - no corpus neighbor papers directly compare encoder-decoder vs decoder-only for stance classification

### Mechanism 3
- Claim: Using the term "stance" instead of "sentiment" in prompts leads to better performance for stance classification tasks.
- Mechanism: LLMs perceive stance and sentiment as different tasks, and using the correct terminology helps them focus on the specific task of stance classification rather than sentiment analysis.
- Core assumption: The terminology used in prompts influences the model's understanding of the task and affects performance.
- Evidence anchors:
  - [section] "using the term 'sentiment,' despite sentiment classification being more familiar to the LLMs, actually decreased performance"
  - [corpus] Weak evidence - no corpus neighbor papers directly address the impact of terminology on stance classification performance

## Foundational Learning

- Concept: Stance classification
  - Why needed here: The paper investigates the use of LLMs for stance classification, so understanding the task is crucial for interpreting the results and implications.
  - Quick check question: What is the difference between stance classification and sentiment analysis?

- Concept: Prompt engineering
  - Why needed here: The paper explores different prompting schemes to improve LLM performance for stance classification, so understanding prompt engineering techniques is essential.
  - Quick check question: What is the difference between few-shot prompting and fine-tuning?

- Concept: Large Language Models (LLMs)
  - Why needed here: The paper focuses on using LLMs for stance classification, so understanding their capabilities and limitations is crucial for interpreting the results and implications.
  - Quick check question: What are the key differences between encoder-decoder and decoder-only LLMs?

## Architecture Onboarding

- Component map: Data (5 benchmark datasets) -> Models (10 open-source LLMs) -> Prompts (4 schemes) -> Evaluation (macro-F1 accuracy)

- Critical path:
  1. Load and preprocess the data
  2. Construct prompts for each dataset and prompting scheme
  3. Run the models on the prompts and collect outputs
  4. Post-process the outputs to extract stance labels
  5. Calculate the unweighted, macro-F1 accuracy for each combination

- Design tradeoffs:
  - Using open-source models vs. closed models (potential data contamination issues)
  - Few-shot examples vs. no examples (improved performance but potential instability)
  - Including reasoning in prompts vs. not (potential for improved performance but also potential for recycled reasons)

- Failure signatures:
  - Inconsistent outputs (e.g., different formats for the same stance label)
  - Lack of meaningful reasoning in the outputs
  - Performance not improving with additional context or examples

- First 3 experiments:
  1. Compare the performance of encoder-decoder and decoder-only models on the semeval2016 dataset using the Context prompting scheme.
  2. Test the impact of few-shot examples on the performance of the Flan-Alpaca-GPT4-T5 model on the covid-lies dataset using the Context + FSP prompting scheme.
  3. Investigate the effect of terminology (stance vs. sentiment) on the performance of the Flan-Alpaca-GPT4-T5 model on the election2016 dataset using the Context prompting scheme.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning large language models (LLMs) on stance classification data lead to improved performance compared to prompt-based approaches?
- Basis in paper: [explicit] The authors mention that they performed fine-tuning on LLMs but found that it "does not necessarily lead to better performance" compared to prompt-based methods.
- Why unresolved: The paper only briefly mentions fine-tuning results without providing detailed comparisons or analysis of why fine-tuning may not improve performance.
- What evidence would resolve it: A systematic comparison of fine-tuned LLM performance versus prompt-based approaches across multiple datasets and model architectures would clarify the effectiveness of fine-tuning for stance classification.

### Open Question 2
- Question: How does the selection of few-shot examples affect the performance of LLM-based stance classification?
- Basis in paper: [explicit] The authors note that "few-shot prompting did not always improve performance" and suggest that the selection of samples for few-shots may not have been optimal.
- Why unresolved: The paper uses randomly selected few-shot examples but does not investigate the impact of example selection strategies on model performance.
- What evidence would resolve it: A systematic study comparing different few-shot example selection methods (e.g., random, similarity-based, adversarial) and their impact on stance classification accuracy would address this question.

### Open Question 3
- Question: What are the key architectural differences between encoder-decoder and decoder-only models that make encoder-decoder models more suitable for stance classification tasks?
- Basis in paper: [explicit] The authors observe that encoder-decoder models (e.g., T5) performed better than decoder-only models for stance classification, but they only speculate about potential reasons without providing concrete evidence.
- Why unresolved: The paper does not conduct a detailed analysis of the architectural differences between these model types or their implications for stance classification performance.
- What evidence would resolve it: A comparative analysis of encoder-decoder and decoder-only models, including ablation studies and attention visualization, would reveal the architectural factors contributing to performance differences in stance classification tasks.

## Limitations
- Limited to five benchmark datasets, all focused on social media text
- Only evaluates unweighted macro-F1 accuracy, potentially missing important performance aspects
- Manual prompt construction without systematic exploration of alternative formulations
- Only examines zero-shot and few-shot settings without fine-tuning

## Confidence

**High Confidence**: Context-enriched prompts consistently improve performance; encoder-decoder vs decoder-only comparison shows robust patterns.

**Medium Confidence**: LLMs achieving competitive results with supervised models, though performance varies significantly by dataset and configuration.

**Low Confidence**: The assertion that stance classification should serve as a benchmark task for future LLM development is speculative and not directly tested.

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate the best-performing prompting scheme and model combination on a non-social-media dataset (e.g., news articles or academic texts) to assess whether the observed performance patterns generalize beyond the current domain.

2. **Human Evaluation of Reasoning Quality**: Conduct a human evaluation study to assess whether the reasoning chains generated by LLMs in the Context + FSP + Reasoning condition actually contribute meaningful, accurate justifications for stance predictions, or whether they simply recycle patterns from few-shot examples.

3. **Minimal Fine-Tuning Comparison**: Compare the best zero-shot/few-shot LLM performance against models fine-tuned on just 10-100 examples from each dataset to determine whether LLMs can match or exceed small amounts of supervised training, which would have important implications for resource-constrained deployment scenarios.