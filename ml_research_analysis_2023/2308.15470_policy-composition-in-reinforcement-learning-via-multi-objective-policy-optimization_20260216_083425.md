---
ver: rpa2
title: Policy composition in reinforcement learning via multi-objective policy optimization
arxiv_id: '2308.15470'
source_url: https://arxiv.org/abs/2308.15470
tags:
- uni00000014
- teacher
- uni00000004
- policies
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for policy composition in reinforcement
  learning using multi-objective policy optimization. The key idea is to treat teacher
  policies as additional objectives, in addition to the task objective, in a multi-objective
  policy optimization setting.
---

# Policy composition in reinforcement learning via multi-objective policy optimization

## Quick Facts
- arXiv ID: 2308.15470
- Source URL: https://arxiv.org/abs/2308.15470
- Reference count: 13
- Key outcome: A method for policy composition in RL using multi-objective optimization, treating teacher policies as additional objectives via KL divergence regularization

## Executive Summary
This paper introduces a novel approach to policy composition in reinforcement learning by treating pre-trained teacher policies as additional objectives in a multi-objective optimization framework. The method uses the MO-MPO algorithm to optimize a weighted combination of the task reward and KL divergences between the agent's policy and each teacher policy. This allows agents to flexibly compose teacher policies both sequentially and concurrently, and even learn the weights of the KL divergences based on the current state. Experiments in continuous control domains demonstrate that this approach can speed up learning, particularly in sparse reward settings, and achieve better performance than baselines while offering more flexibility than approaches that treat teacher policies as primitive layers to be built upon.

## Method Summary
The method proposes to compose teacher policies in reinforcement learning by treating them as additional objectives in a multi-objective policy optimization setting. Specifically, it uses the MO-MPO algorithm to optimize a weighted combination of the task reward and KL divergences between the agent's policy and each teacher policy. The KL divergence acts as a regularizer that encourages the agent's policy to stay close to teacher policies while still allowing task-driven deviations. The approach allows for flexible composition of teacher policies both sequentially and concurrently, and can even learn state-dependent weights for the KL divergences through an additional bottleneck network. This enables agents to adaptively select which teachers to follow based on the current observation.

## Key Results
- The proposed method successfully composes teacher policies in sequence and in parallel in continuous control domains
- Policy composition speeds up learning, particularly in sparse reward settings, and achieves better performance than baselines
- The method allows agents to compose between discontinuous policies, offering more flexibility than approaches that treat teacher policies as primitive layers to be built upon
- Agents can learn to adaptively select which teachers to follow based on the current state

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The method enables compositional learning by treating teacher policies as KL divergence objectives in a multi-objective RL framework
- **Mechanism**: The MO-MPO algorithm optimizes a weighted combination of the task reward and KL divergences between the agent's policy and each teacher policy. The KL divergence acts as a regularizer that encourages the agent's policy to stay close to teacher policies, while the task reward objective allows the agent to deviate when necessary to maximize performance. The hyperparameters (ϵ values) control the relative strength of each objective.
- **Core assumption**: The teacher policies provide useful behavioral priors that can guide learning, particularly in sparse reward settings where direct reward signals are weak or infrequent.
- **Evidence anchors**: [abstract] "The teacher policies are introduced as objectives, in addition to the task objective, in a multi-objective policy optimization setting." [section 2.2] "We use the Multi-Objective Maximum a Posteriori Policy Optimization (MO-MPO) algorithm (Abdolmaleki et al. 2020), a multi-objective actor-critic algorithm, to compose skills."

### Mechanism 2
- **Claim**: The method allows flexible composition of teacher policies both sequentially and concurrently
- **Mechanism**: By representing each teacher as a separate KL divergence objective with its own weight (ϵi), the agent can selectively adhere to different teachers in different parts of the state space. The paper shows this through (1) sequential composition where different teachers are active in different state regions, and (2) concurrent composition where multiple teachers influence the agent simultaneously.
- **Core assumption**: Teacher policies are relevant to distinct or overlapping portions of the observation space, and their relevance can be captured through state-dependent weighting.
- **Evidence anchors**: [abstract] "In two domains with continuous observation and action spaces, our agents successfully compose teacher policies in sequence and in parallel" [section 2.4] "We consider the scenarios where teacher policies are relevant to distinct parts of the observation space, and scenarios where teacher policies are relevant to overlapping parts of agent's observation space."

### Mechanism 3
- **Claim**: The method can learn to adaptively select which teachers to follow based on the current state
- **Mechanism**: The agent can learn a policy over the KL divergence weights (ϵteacher,i) as a function of observation. This is implemented by adding an additional bottleneck network that takes observations as input and outputs the teacher weights. This allows the agent to dynamically choose which teacher(s) to follow in different situations.
- **Core assumption**: There exists a useful mapping from observations to teacher relevance that can be learned through the task reward signal.
- **Evidence anchors**: [section 2.4] "In the humanoid domain, we also equip agents with the ability to control the selection of teachers by learning the weight of KL divergence (between agent's policy and each pre-existing policy) for each pre-existing policy." [section 4.5] "In this section, we consider an agent that controls the relative preference given to teacher policies, towards learning a successful policy for the task."

## Foundational Learning

- **Concept**: Multi-objective optimization in reinforcement learning
  - **Why needed here**: The method fundamentally relies on optimizing multiple objectives (task reward + KL divergences) simultaneously rather than treating teacher policies as primitives to be combined.
  - **Quick check question**: What algorithm is used to handle the multi-objective optimization, and how does it balance the different objectives?

- **Concept**: KL divergence as a policy regularization term
  - **Why needed here**: The KL divergence between the agent's policy and teacher policies serves as a regularizer that encourages behavioral similarity while still allowing task-driven deviations.
  - **Quick check question**: How does the KL divergence objective affect the agent's policy compared to a standard single-objective RL approach?

- **Concept**: State-dependent weighting of objectives
  - **Why needed here**: The method allows the importance of each teacher to vary based on the current state, enabling more nuanced composition than fixed weights.
  - **Quick check question**: How is the state-dependent weighting implemented in the architecture, and what role does the bottleneck play?

## Architecture Onboarding

- **Component map**: Observation → Policy encoder (optional) → Action distribution + Teacher weights (if applicable) → Environment interaction → Buffer update → Gradient updates for critic, policy, and temperature parameters
- **Critical path**: Observation → Policy encoder (optional) → Action distribution + Teacher weights (if applicable) → Environment interaction → Buffer update → Gradient updates for critic, policy, and temperature parameters
- **Design tradeoffs**: 
  - Fixed vs. learned teacher weights: Fixed weights are simpler but less flexible; learned weights add complexity but enable adaptive composition.
  - Number of teachers: More teachers increase flexibility but also increase computational cost and exploration complexity.
  - KL constraint strength: Stronger constraints ensure closer adherence to teachers but may limit task performance.
- **Failure signatures**:
  - Poor teacher quality: Agent performance degrades compared to no-teacher baseline
  - Incorrect weight specification: Agent either ignores useful teachers or over-constrains itself
  - Bottleneck too restrictive: Agent cannot learn useful state-dependent weight mappings
- **First 3 experiments**:
  1. **Stand task with stand teacher**: Verify basic functionality where teacher and task align closely
  2. **Walk task with stand teacher**: Test sequential composition where teacher provides partial solution
  3. **Point mass with two concurrent teachers**: Test concurrent composition in simpler domain before scaling to humanoid

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of policy composition scale with the number and diversity of teacher policies?
- **Basis in paper**: [inferred] The paper demonstrates composition with 1-2 teacher policies but does not explore the limits of scaling.
- **Why unresolved**: The experiments only use 1-2 teacher policies. Scaling to many diverse teachers may introduce challenges in weighting and coordination.
- **What evidence would resolve it**: Experiments with 5+ diverse teacher policies, measuring composition quality and learning speed as a function of teacher count/diversity.

### Open Question 2
- **Question**: How robust is the policy composition method to suboptimal or conflicting teacher policies?
- **Basis in paper**: [explicit] The paper notes that teachers "may compete with each other and be counterproductive" but does not quantify this effect.
- **Why unresolved**: Experiments only use cooperative teachers. The algorithm's behavior with conflicting teachers is unknown.
- **What evidence would resolve it**: Experiments with conflicting teacher policies, measuring task performance and KL divergence as a function of teacher conflict.

### Open Question 3
- **Question**: How does the learned weighting of teacher policies (ϵteacher,i(o)) evolve during training and what are the implications?
- **Basis in paper**: [inferred] The paper introduces learning ϵteacher,i(o) but only shows final results, not the learning dynamics.
- **Why unresolved**: The paper only shows final policy performance, not the evolution of the weighting function during training.
- **What evidence would resolve it**: Visualization of ϵteacher,i(o) evolution over training, correlation with task performance and teacher utility.

## Limitations
- The method assumes teacher policies are of reasonable quality and relevant to the task; poor teachers can actively harm learning rather than help
- The approach is evaluated only on continuous control domains from DeepMind Control Suite, limiting generalizability to other RL domains
- State-dependent teacher weighting adds architectural complexity and may struggle with highly complex state spaces
- The paper doesn't provide systematic analysis of how teacher quality affects learning outcomes

## Confidence
- **High confidence**: The core mechanism of treating teacher policies as KL divergence objectives in multi-objective RL is well-supported by the paper's theoretical framework and experimental results
- **Medium confidence**: The claim about flexible composition (sequential and concurrent) is supported by experiments but could benefit from more diverse task compositions
- **Medium confidence**: The state-dependent teacher selection mechanism is demonstrated but with limited ablation studies on the bottleneck architecture

## Next Checks
1. **Teacher quality sensitivity analysis**: Systematically evaluate performance with varying quality levels of teacher policies (e.g., partially trained, task-irrelevant teachers) to understand robustness bounds
2. **State space complexity stress test**: Test the state-dependent weighting mechanism on progressively more complex state spaces to identify scalability limits
3. **Cross-domain generalization**: Apply the method to non-continuous-control domains (e.g., Atari games or navigation tasks) to assess broader applicability beyond the DeepMind Control Suite environments used in the paper