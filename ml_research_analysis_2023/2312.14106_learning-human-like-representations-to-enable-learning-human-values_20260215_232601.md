---
ver: rpa2
title: Learning Human-like Representations to Enable Learning Human Values
arxiv_id: '2312.14106'
source_url: https://arxiv.org/abs/2312.14106
tags:
- alignment
- learning
- agent
- actions
- representational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes that representational alignment between AI
  agents and humans is a necessary (but not sufficient) condition for value alignment.
  The authors study the effect of representational alignment on an agent's ability
  to learn to take ethical actions in a multi-armed bandit setting.
---

# Learning Human-like Representations to Enable Learning Human Values

## Quick Facts
- arXiv ID: 2312.14106
- Source URL: https://arxiv.org/abs/2312.14106
- Reference count: 2
- Key outcome: This paper proposes that representational alignment between AI agents and humans is a necessary (but not sufficient) condition for value alignment, demonstrating this through experiments in a multi-armed bandit setting.

## Executive Summary
This paper investigates the relationship between representational alignment and value alignment in AI systems. The authors propose that agents whose internal representations better align with human similarity judgments are better equipped to learn human values. Through experiments in a contextual multi-armed bandit setting, they demonstrate that representational alignment enables both safe exploration and improved generalization when learning human values. The results show a strong relationship between representational alignment and performance metrics, with more aligned agents performing better across all measures.

## Method Summary
The paper uses a contextual multi-armed bandit reinforcement learning setup where agents must choose ethical actions from a pool of 100 actions with associated morality scores. Two kernel-based models are employed: support vector regression and kernel regression. The agents use custom kernels that enforce representational alignment with human similarity judgments. Thompson sampling is used for action selection, and performance is evaluated across five metrics: mean reward per timestep, convergence speed, and the number of non-optimal, immoral, and unique actions taken. Representational alignment is quantified using Spearman correlation between the agent's kernel and ground truth similarity matrices.

## Key Results
- Strong positive relationship between representational alignment and all five performance metrics
- More aligned agents achieve higher mean reward, faster convergence, and fewer immoral/non-optimal actions
- Even partially representationally-aligned agents (Spearman correlation ~0.35) outperform Thompson sampling baseline
- Results are consistent across both support vector regression and kernel regression models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Representational alignment between agents and humans creates a bound on the agent's ability to learn human values in reinforcement learning tasks.
- Mechanism: When the agent's similarity matrix (kernel) is more aligned with human similarity judgments, the agent can better predict which actions humans would consider ethical, leading to faster convergence and fewer immoral actions during training.
- Core assumption: The similarity matrix used by the agent directly reflects the human value judgments that determine the reward distribution.
- Evidence anchors:
  - [abstract] "We demonstrate that representational alignment enables both safe exploration and improved generalization when learning human values."
  - [section] "The results show a strong relationship between representational alignment and all 5 metrics, with more aligned agents performing better across the board."
  - [corpus] Weak evidence - most related papers discuss representational alignment in classification tasks, not reinforcement learning value alignment.

### Mechanism 2
- Claim: Thompson sampling baseline performs worse than even partially representationally-aligned agents.
- Mechanism: Thompson sampling explores randomly without using human-aligned representations, leading to more immoral and suboptimal actions during training compared to agents with even partial representational alignment.
- Core assumption: Random exploration without representational alignment guidance leads to worse performance in value learning tasks.
- Evidence anchors:
  - [section] "It is also worth noting that even a partially representationally-aligned agent (with a Spearman correlation of around 0.35) still performs noticeably better than the Thompson sampling baseline."
  - [section] "As representational alignment decreases, the mean reward per timestep decreases, and all other metrics – timesteps before convergence and number of non-optimal, unique, and immoral actions taken – increase."
  - [corpus] No direct evidence in corpus papers - this is a specific finding from this paper.

### Mechanism 3
- Claim: The relationship between representational alignment and value alignment performance is systematic and not model-dependent.
- Mechanism: Both support vector regression and kernel regression models show similar performance patterns when their representational alignment with humans changes, indicating the effect is fundamental to the alignment concept rather than specific to model architecture.
- Core assumption: Different kernel-based models should show similar responses to changes in representational alignment if the relationship is fundamental.
- Evidence anchors:
  - [section] "We observe similar results for both models, indicating that these results are systematic and not model-dependent."
  - [section] "Results are shown in Figure 1, comparing Spearman correlation against the mean reward per timestep, number of timesteps to convergence, number of immoral actions taken, non-optimal (morality score < 3) actions taken, and unique actions taken, respectively."
  - [corpus] No direct evidence - corpus papers focus on individual model types rather than comparing multiple models.

## Foundational Learning

- Concept: Spearman correlation as a measure of representational alignment
  - Why needed here: The paper uses Spearman correlation between the agent's kernel and ground truth similarity matrix to quantify representational alignment, which is central to the experimental design and results.
  - Quick check question: If the Spearman correlation is 0.8 between two similarity matrices, what does this indicate about their relationship?

- Concept: Multi-armed bandit reinforcement learning
  - Why needed here: The experimental setup uses a contextual multi-armed bandit setting where agents learn to choose ethical actions, making understanding this RL paradigm essential for interpreting the results.
  - Quick check question: In a multi-armed bandit problem with 100 actions, if an agent takes 10 unique actions before converging, what does this suggest about its exploration strategy?

- Concept: Kernel methods in machine learning
  - Why needed here: Both support vector regression and kernel regression models are used, and the kernel represents the similarity matrix that encodes representational alignment with humans.
  - Quick check question: How does changing the kernel in a support vector regression model affect its predictions in this multi-armed bandit setting?

## Architecture Onboarding

- Component map: Ground truth similarity matrix → Agent with kernel → Action selection via Thompson sampling → Reward sampling → Performance evaluation → Alignment correlation calculation
- Critical path: The agent selects actions based on its kernel representation, receives rewards based on morality scores, updates its model, and performance is evaluated while tracking representational alignment.
- Design tradeoffs: Using kernel methods provides interpretable similarity-based decision making but limits scalability compared to deep learning approaches; Thompson sampling provides a strong baseline but may be suboptimal for representationally-aligned agents.
- Failure signatures: Poor performance across all metrics suggests insufficient representational alignment; high unique actions with low reward suggests poor exploitation; high immoral actions suggests the agent is not properly using the similarity information.
- First 3 experiments:
  1. Replicate the basic experiment with Spearman correlation of 1.0 to establish baseline performance for perfectly aligned agents
  2. Test intermediate alignment levels (0.5-0.7 Spearman correlation) to map the performance curve
  3. Compare Thompson sampling baseline performance against the lowest alignment level tested to confirm the stated performance difference

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between representational alignment and value alignment in complex AI systems beyond the multi-armed bandit setting?
- Basis in paper: [explicit] The paper suggests that representational alignment is a necessary condition for value alignment and mentions that the relationship needs to be studied for more complex AI systems.
- Why unresolved: The paper only demonstrates this relationship in a simplified multi-armed bandit setting and suggests that real-world AI systems are far more complex.
- What evidence would resolve it: Formal mathematical bounds or empirical studies showing the relationship between representational and value alignment in complex RL environments, language models, or multi-task learning scenarios.

### Open Question 2
- Question: How does the degree of representational alignment needed for value alignment vary across different types of human values (e.g., ethics, honesty, fairness)?
- Basis in paper: [explicit] The paper mentions evaluating agents across "ten different aspects of human values -- including ethics, honesty, and fairness" but only demonstrates results for ethics.
- Why unresolved: The paper only demonstrates results for ethics and suggests that human values are difficult to quantify in general.
- What evidence would resolve it: Comparative studies measuring representational alignment requirements for different value types using similar multi-armed bandit experiments or more complex value learning tasks.

### Open Question 3
- Question: What are the most effective methods for achieving high representational alignment between AI systems and humans in real-world applications?
- Basis in paper: [inferred] The paper demonstrates benefits of representational alignment but notes that many existing papers provide empirical evaluations without concrete suggestions for improvement.
- Why unresolved: The paper focuses on demonstrating the importance of representational alignment rather than proposing specific methods to achieve it.
- What evidence would resolve it: Empirical comparisons of different representation learning techniques, architectural choices, or training objectives that maximize representational alignment with human judgments.

## Limitations

- The paper establishes representational alignment as necessary but not sufficient for value alignment, without proving that sufficient alignment guarantees successful value learning.
- The experimental setup uses synthetic similarity matrices and a simplified multi-armed bandit environment that may not capture real-world value alignment complexity.
- The paper does not explore whether different types of representational alignment (e.g., geometric vs. semantic) have different effects on value learning performance.

## Confidence

- High confidence: The systematic relationship between representational alignment and performance metrics across both SVR and kernel regression models is well-supported by the experimental results.
- Medium confidence: The claim that representational alignment is necessary for value alignment is supported by the experimental evidence but remains unproven for more complex environments.
- Low confidence: The assertion that the relationship between alignment and performance is fundamental rather than model-dependent, as this has not been tested across diverse model architectures.

## Next Checks

1. Test the relationship between representational alignment and value learning performance using neural network-based agents with learned representations, to determine if the observed effects generalize beyond kernel methods.

2. Conduct ablation studies where different components of the similarity matrix (e.g., removing certain dimensions of similarity) are systematically varied to identify which aspects of representational alignment are most critical for value learning.

3. Evaluate the effect of representational alignment in more complex environments with delayed rewards and longer time horizons to assess whether the observed relationships hold in settings that better approximate real-world value alignment challenges.