---
ver: rpa2
title: 'ExpertQA: Expert-Curated Questions and Attributed Answers'
arxiv_id: '2309.07852'
source_url: https://arxiv.org/abs/2309.07852
tags:
- claims
- arxiv
- systems
- question
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ExpertQA, a dataset of expert-curated questions
  and verified answers with attributions, created to evaluate the factual correctness
  and attribution quality of large language model (LLM) outputs in domain-specific
  scenarios. The authors collected 2,177 questions from 484 experts across 32 fields
  and asked them to evaluate LLM-generated responses, focusing on claim informativeness,
  factuality, cite-worthiness, and the reliability of attributed sources.
---

# ExpertQA: Expert-Curated Questions and Attributed Answers

## Quick Facts
- arXiv ID: 2309.07852
- Source URL: https://arxiv.org/abs/2309.07852
- Reference count: 35
- Key outcome: ExpertQA is a dataset of 2,177 expert-curated questions and verified answers with attributions, designed to evaluate LLM attribution quality across 32 domains, revealing that retrieve-and-read systems generally produce more complete citations but still struggle with high-stakes domains.

## Executive Summary
This paper introduces ExpertQA, a high-quality dataset of expert-curated questions and verified answers with attributions, designed to rigorously evaluate the factual correctness and attribution capabilities of large language models (LLMs) in domain-specific contexts. The dataset includes questions from 484 experts across 32 fields, with expert evaluations of LLM-generated responses focusing on claim informativeness, factuality, cite-worthiness, and source reliability. The study finds that retrieve-and-read systems generally produce more complete attributions than purely generative or post-hoc approaches, but still struggle to cite all claims deemed worthy by experts. Retrieval corpus choice significantly impacts both the factual accuracy of responses and the perceived reliability of sources, with Google search outperforming a static corpus like Sphere. Expert evaluation reveals that high-stakes fields such as medicine and law suffer from a large percentage of incomplete attributions and unreliable sources. Automatic methods for attribution and factuality estimation show low recall and precision compared to human judgments, suggesting a need for better automated evaluation. The revised answers collected from experts were used to create a high-quality long-form QA dataset, which improved model performance after finetuning but still left substantial room for improvement.

## Method Summary
The study collected 2,177 expert-curated questions from 484 experts across 32 fields, focusing on challenging technical and advisory scenarios. These questions were answered by various LLM-based systems, including retrieve-and-read, generative, and post-hoc attribution approaches. Experts then evaluated the generated responses for factual correctness, informativeness, and attribution quality, identifying cite-worthy claims and assessing the reliability of attributed sources. Based on expert feedback, the answers were revised to ensure factual correctness and reliable attributions, resulting in a high-quality long-form QA dataset. The dataset was used to evaluate the performance of attribution and factuality estimation models, revealing significant gaps between human and machine assessment capabilities.

## Key Results
- Retrieve-and-read systems generally produce more complete attributions than purely generative or post-hoc approaches, but still struggle to cite all claims deemed worthy by experts.
- The choice of retrieval corpus significantly impacts the factual accuracy of responses and the perceived reliability of sources, with Google search outperforming a static corpus like Sphere.
- High-stakes domains such as medicine and law suffer from a large percentage of incomplete attributions and unreliable sources.
- Automatic methods for attribution and factuality estimation, such as AutoAIS and FActScore, show low recall and precision compared to human judgments.
- Finetuning models on the revised ExpertQA dataset improves performance but leaves substantial room for improvement.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert-curated questions lead to more realistic and complex information needs compared to synthetic datasets.
- Mechanism: Domain experts generate questions from real professional scenarios, including open-ended, scenario-based, and advisory types, which better reflect actual expert information-seeking behavior.
- Core assumption: Experts can articulate nuanced, high-stakes questions that reveal limitations of current LLM attribution systems.
- Evidence anchors:
  - [abstract] "questions relevant to each field, as well as judgements from experts about how well several state-of-the-art systems perform along various axes of factuality and attribution."
  - [section] "They are told that this question could be one they have encountered in their profession or one they are curious about. We ask them to formulate challenging technical questions..."
  - [corpus] Weak - no direct comparison with other question sources.
- Break condition: If experts cannot articulate questions beyond simple factoids, dataset realism is lost.

### Mechanism 2
- Claim: Retrieve-and-read systems generate more complete attributions than purely generative or post-hoc approaches.
- Mechanism: By retrieving evidence before generation, the model has explicit context to cite, reducing hallucination and improving citation completeness.
- Core assumption: Retrieval context directly influences the ability of the model to generate faithful attributions.
- Evidence anchors:
  - [abstract] "Retrieve-and-read systems often generate complete attributions compared to LLM prompting and post-hoc attribution, but struggle to produce citations for all cite-worthy claims."
  - [section] "Retrieve-and-read systems have a stronger inductive bias to use the retrieved evidence to answer the question."
  - [corpus] Weak - limited analysis of retrieval context quality.
- Break condition: If retrieved passages are irrelevant or incomplete, citation quality degrades.

### Mechanism 3
- Claim: Retrieval corpus choice significantly affects attribution completeness and source reliability.
- Mechanism: Different corpora (e.g., Sphere vs Google search) provide different quality and diversity of evidence, influencing expert judgments on citation reliability.
- Core assumption: Expert trust in source reliability correlates with citation completeness.
- Evidence anchors:
  - [abstract] "The retrieval source significantly impacts the quality of attribution and overall factuality. High-stakes domains such as medicine and law suffer from a large percentage of incomplete attributions..."
  - [section] "Expert judgements of reliability are directly influenced by the credibility of the sources from which evidences are retrieved."
  - [corpus] Weak - no explicit quality metrics of corpora.
- Break condition: If all sources are equally unreliable, expert judgments lose discriminatory power.

## Foundational Learning

- Concept: Understanding attribution frameworks (e.g., Attributable to Identified Sources - AIS)
  - Why needed here: AIS provides the theoretical foundation for evaluating whether claims are supported by provided evidence.
  - Quick check question: Can you explain how AIS differs from simple entailment?

- Concept: Human evaluation of factuality and attribution
  - Why needed here: Human experts are critical for assessing claim informativeness, citeworthiness, and source reliability in domain-specific contexts.
  - Quick check question: What are the key dimensions along which experts judge claim quality?

- Concept: Retrieval-augmented generation pipeline
  - Why needed here: Retrieve-and-read systems depend on retrieval quality to produce faithful answers and citations.
  - Quick check question: How does retrieval quality affect downstream answer generation and attribution?

## Architecture Onboarding

- Component map:
  - Question curation → Expert generation of information needs
  - Response sampling → Multiple system types (generative, retrieve-and-read, post-hoc)
  - Claim extraction → Sentence tokenization and segmentation
  - Expert annotation → Multi-dimensional evaluation (usefulness, attribution, factuality, reliability, cite-worthiness)
  - Revision → Expert editing of claims and evidence
  - Dataset assembly → Aggregated verified answers and attributions

- Critical path: Expert-curated questions → Response generation → Expert validation → Dataset creation
- Design tradeoffs:
  - Single vs multiple expert judgments per claim (cost vs robustness)
  - Granularity of claims (sentences vs atomic units)
  - Retrieval corpus choice (coverage vs reliability)
- Failure signatures:
  - Low annotation agreement → Ambiguous claims or instructions
  - High abstention rates → System limitations or question difficulty
  - Unreliable sources → Corpus quality issues
- First 3 experiments:
  1. Evaluate agreement of expert labels on sampled claims from two fields.
  2. Compare AutoAIS scores across different system types to human attribution judgments.
  3. Measure factuality estimation F1 scores for claims across system types.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the atomicity of claims in ExpertQA to enable more precise and fine-grained evaluation of attribution and factuality?
- Basis in paper: [inferred]
- Why unresolved: The current claim extraction process using sentence tokenizers results in claims that may not represent singular information units, limiting the precision of evaluation metrics and potentially introducing ambiguity in expert judgments.
- What evidence would resolve it: Experiments comparing the performance of attribution and factuality evaluation models on ExpertQA with claims at different levels of granularity (sentence-level vs. finer-grained atomic claims) would provide insights into the impact of atomicity on evaluation accuracy and the trade-offs involved.

### Open Question 2
- Question: How can we effectively address the challenges of multi-source attributions in ExpertQA, where a single claim is supported by multiple pieces of evidence?
- Basis in paper: [explicit]
- Why unresolved: The current AutoAIS system struggles to accurately evaluate claims with multi-source attributions, often resulting in lower attribution scores and potential misclassifications. Developing methods to effectively combine information from multiple sources and assess their collective support for a claim is crucial for improving attribution quality.
- What evidence would resolve it: Evaluating the performance of attribution models on claims with varying numbers of supporting sources and analyzing the impact of source combination strategies on attribution accuracy would shed light on the effectiveness of different approaches.

### Open Question 3
- Question: How can we mitigate the impact of retrieval corpus quality on the factual correctness and reliability of attributed claims in ExpertQA?
- Basis in paper: [explicit]
- Why unresolved: The choice of retrieval corpus significantly influences the quality of attributed claims, with smaller or less reliable corpora leading to lower factuality and reliability scores. Investigating methods to improve retrieval corpus quality, such as incorporating source credibility signals or employing more sophisticated retrieval techniques, is essential for enhancing the trustworthiness of generated responses.
- What evidence would resolve it: Comparing the performance of attribution and factuality evaluation models on claims retrieved from different corpora with varying levels of quality and reliability would provide insights into the impact of corpus selection on evaluation outcomes.

## Limitations

- The study's findings are based on expert judgments that may not generalize across all domains or cultural contexts.
- The dataset creation process involved a relatively small number of experts (484 across 32 fields), potentially limiting coverage of specialized subdomains.
- The evaluation of automatic methods like AutoAIS and FActScore revealed significant gaps between human and machine assessment capabilities, suggesting current automated evaluation tools may be inadequate for real-world deployment.

## Confidence

- **Expert-curated questions improve realism of evaluation** (Medium confidence): While the study demonstrates that expert-generated questions cover diverse and complex scenarios, there is limited evidence comparing these questions to alternative sources like real user queries or synthetic datasets.
- **Retrieve-and-read systems produce more complete attributions** (High confidence): This claim is well-supported by expert evaluations showing retrieve-and-read systems consistently outperform purely generative approaches in citation completeness.
- **Retrieval corpus choice significantly impacts attribution quality** (Medium confidence): While expert judgments clearly show differences between Sphere and Google search, the study does not establish causality or provide metrics on corpus quality that would explain these differences.

## Next Checks

1. **Cross-domain generalizability test**: Validate whether the observed attribution patterns hold when evaluating responses in domains not represented in the original ExpertQA dataset (e.g., emerging technologies or interdisciplinary fields).

2. **Automated evaluation benchmarking**: Compare AutoAIS and FActScore performance against human judgments across multiple LLM systems and retrieval configurations to establish reliability thresholds for automated attribution assessment.

3. **Expert agreement analysis**: Measure inter-annotator agreement rates for attribution and factuality judgments across different claim types and fields to quantify the reproducibility of human evaluation and identify potential sources of disagreement.