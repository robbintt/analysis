---
ver: rpa2
title: 'Don''t Trust ChatGPT when Your Question is not in English: A Study of Multilingual
  Abilities and Types of LLMs'
arxiv_id: '2305.16339'
source_url: https://arxiv.org/abs/2305.16339
tags:
- language
- tasks
- llms
- multilingual
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a systematic method for analyzing multilingual
  abilities of large language models (LLMs) by categorizing tasks based on language
  dependence and translatability. The authors introduce three categories: reasoning
  (language-independent), knowledge accessing (moderately language-dependent), and
  articulating (highly language-dependent).'
---

# Don't Trust ChatGPT when Your Question is not in English: A Study of Multilingual Abilities and Types of LLMs

## Quick Facts
- arXiv ID: 2305.16339
- Source URL: https://arxiv.org/abs/2305.16339
- Reference count: 7
- Key outcome: This paper proposes a systematic method for analyzing multilingual abilities of large language models (LLMs) by categorizing tasks based on language dependence and translatability.

## Executive Summary
This paper proposes a systematic method for analyzing multilingual abilities of large language models (LLMs) by categorizing tasks based on language dependence and translatability. The authors introduce three categories: reasoning (language-independent), knowledge accessing (moderately language-dependent), and articulating (highly language-dependent). They also define Translating Equivariant (TE) and Translating Variant (TV) tasks. Using back-translation prompting and a repetition strategy to counter randomness, they apply their method to GPT-3.5. Results show high consistency and similarity in TE tasks across languages, suggesting a translation-like reasoning process. TV tasks reveal inconsistencies, indicating GPT-3.5 exhibits subordinate-bilingual-like behavior. The findings provide insights into the multilingual capabilities and limitations of LLMs.

## Method Summary
The paper proposes a systematic method for analyzing multilingual abilities of LLMs by categorizing tasks into reasoning (language-independent), knowledge accessing (moderately language-dependent), and articulating (highly language-dependent). The authors define Translating Equivariant (TE) and Translating Variant (TV) tasks, then use back-translation prompting and repetition to counter randomness. They apply this framework to GPT-3.5 across multiple languages (English, German, French, Chinese, Japanese, Spanish) using datasets like CommonsenseQA, GSM8K, and WebQuestions. The method measures accuracy, consistency, and similarity of explanations across languages to infer the model's bilingual processing type.

## Key Results
- High consistency and similarity in TE tasks across languages suggest GPT-3.5 uses translation-like reasoning processes
- TV tasks reveal inconsistencies, indicating GPT-3.5 exhibits subordinate-bilingual-like behavior
- English typically shows higher accuracy than other languages, confirming language dominance effects
- The framework successfully distinguishes between compound/coordinate and subordinate bilingual-like processing in LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Back-translation prompting can detect whether a model is reasoning in a language versus translating first.
- Mechanism: By prompting for an explanation in one language, then translating that explanation back to the original language, we can compare N-gram overlap. High overlap suggests the model used the same reasoning process; low overlap suggests separate reasoning pipelines.
- Core assumption: Chain-of-thought explanations reflect the internal reasoning process used by the model.
- Evidence anchors:
  - [abstract]: "back-translation-based method to assess the consistency in the network's internal reasoning process"
  - [section]: "back-translating such explanations to the same language allows us to compare the internal reasoning differences"
  - [corpus]: Weak - no direct evidence that back-translation reliably reveals internal reasoning. Assumption: Chain-of-thought explanations are a window into internal reasoning.
- Break condition: If explanations are post-hoc rationalizations rather than true reasoning traces, the overlap comparison becomes meaningless.

### Mechanism 2
- Claim: Translating invariant tasks can reveal whether a model is translating input before reasoning.
- Mechanism: For tasks where output should be identical under translation (e.g., math problems), if the answer changes after translating the input, the model likely performed reasoning after translation rather than using a language-independent representation.
- Core assumption: Translating invariant tasks are processed identically regardless of input language if the model truly understands them.
- Evidence anchors:
  - [abstract]: "high consistency and similarity in TE tasks across languages, suggesting a translation-like reasoning process"
  - [section]: "translating Equivariant (TE) and Translating Variant (TV) tasks"
  - [corpus]: Missing - no corpus evidence directly supports this mechanism. Assumption: Translating invariant tasks can be used as a probe.
- Break condition: If the model's reasoning process is inherently language-dependent even for seemingly language-independent tasks, this mechanism fails.

### Mechanism 3
- Claim: Repeating prompts and selecting the mode response can mitigate randomness in LLM outputs.
- Mechanism: By prompting n times and selecting the most frequent answer, we reduce the impact of stochastic decoding on measurement accuracy.
- Core assumption: The mode of repeated outputs represents the model's "true" answer for a given prompt.
- Evidence anchors:
  - [section]: "we employ a repetition of prompting to counteract the randomness caused by the decoding process"
  - [section]: "select the mode as the final answer"
  - [corpus]: Weak - no corpus evidence that mode selection effectively reduces randomness. Assumption: The mode represents the most stable answer.
- Break condition: If the model's outputs are truly random or multimodal with no clear mode, this approach cannot produce reliable results.

## Foundational Learning

- Concept: Bilingualism types (compound, coordinate, subordinate)
  - Why needed here: The paper uses these linguistic concepts to frame how LLMs handle multilingual tasks, distinguishing between integrated representations versus translation-based processing.
  - Quick check question: What is the key difference between compound and subordinate bilingualism in terms of language processing?

- Concept: Translation equivariance vs. translation variance
  - Why needed here: These concepts categorize tasks based on whether their outputs should remain consistent under translation, which is central to the experimental design.
  - Quick check question: Give an example of a translating invariant task and a translating variant task.

- Concept: Back-translation as a probing technique
  - Why needed here: This technique is used to examine whether the model's reasoning process changes when switching languages, which is crucial for understanding the model's bilingual behavior.
  - Quick check question: How does back-translation help distinguish between compound and subordinate-like processing in LLMs?

## Architecture Onboarding

- Component map: Translation generator -> Prompt interface -> Response comparator -> Back-translation analyzer -> Bilingual type classifier
- Critical path: Translate prompts → Generate responses → Compare responses (consistency/similarity) → Back-translate explanations → Compare reasoning → Determine bilingual type
- Design tradeoffs: Using machine translation for data augmentation saves annotation costs but may introduce translation artifacts that affect task difficulty. Repeating prompts improves reliability but increases computational cost.
- Failure signatures: Low consistency across languages even for translating invariant tasks suggests the model is not using language-independent representations. High similarity in TV tasks suggests the model is translating before reasoning rather than processing in the target language.
- First 3 experiments:
  1. Test math word problems in English and translated versions to establish baseline consistency for translating invariant tasks.
  2. Apply back-translation to reasoning explanations for the same math problems to measure N-gram overlap.
  3. Test pun detection (a translating variant task) in English and translated versions to check if the model's behavior changes with translation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs acquire multilingual abilities, specifically for non-dominant languages?
- Basis in paper: [explicit] The paper states "it remains unclear how proficiency in non-dominant languages...is acquired within the network" and "whether the LLMs exhibit a learned representation akin to compound/coordinate bilingualism or rely on translation processes resembling subordinate bilinguals."
- Why unresolved: The paper identifies this as a fundamental question but does not provide a definitive answer, instead proposing methods to investigate it.
- What evidence would resolve it: Empirical results showing consistent performance across languages (suggesting compound/coordinate bilingualism) versus translation-dependent performance (suggesting subordinate bilingualism).

### Open Question 2
- Question: What are the limitations of using translation-based methods for multilingual task evaluation?
- Basis in paper: [inferred] The paper discusses using back-translation prompting and mentions that "Chain of Thought Reasoning output can not always fully characterize the whole reasoning landscape in LLMs."
- Why unresolved: The paper acknowledges potential limitations but does not systematically explore them or quantify their impact on evaluation accuracy.
- What evidence would resolve it: Comparative studies between translation-based evaluation and native-language task performance, along with analysis of specific failure cases.

### Open Question 3
- Question: How does language dominance affect LLM performance in multilingual settings?
- Basis in paper: [explicit] The paper mentions "the influence of language dominance on their performance" and shows that English typically has higher accuracy than other languages.
- Why unresolved: While the paper observes performance differences, it does not deeply investigate the underlying mechanisms or quantify the relationship between language dominance and task performance.
- What evidence would resolve it: Systematic analysis of performance degradation as languages become less dominant in the training data, controlling for other factors like vocabulary size and linguistic similarity.

## Limitations

- The methodology relies on machine translation for creating multilingual datasets, which may introduce artifacts affecting task difficulty
- The translation equivariance/variance framework assumes clear categorical boundaries that may not exist in practice
- The mechanism by which back-translation reveals internal reasoning processes lacks empirical validation

## Confidence

**High confidence**: The basic observation that LLMs show different consistency patterns across language-dependent task categories is well-supported by experimental results.

**Medium confidence**: The interpretation that low consistency in TV tasks indicates subordinate-bilingual-like behavior is plausible but not definitively proven.

**Low confidence**: The mechanism by which back-translation N-gram overlap reveals internal reasoning processes lacks empirical validation.

## Next Checks

1. **Validate the back-translation mechanism**: Run controlled experiments where the same reasoning task is solved in two languages by bilingual humans, then compare the N-gram overlap in their explanations to those produced by GPT-3.5. This would establish whether high overlap actually indicates shared reasoning versus separate language-specific reasoning.

2. **Test alternative explanations for TV task inconsistency**: Design experiments to distinguish between translation-before-reasoning and separate-language-reasoning hypotheses. For instance, prompt the model to explicitly state whether it's translating the input before reasoning, or test tasks where translation would be particularly difficult or impossible.

3. **Analyze output distributions from repeated prompts**: Instead of just selecting the mode, examine the full distribution of responses across multiple repetitions. Are the outputs multimodal with distinct clusters, unimodal with random variation, or something else? This would validate whether mode selection is an appropriate strategy for mitigating randomness.