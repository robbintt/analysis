---
ver: rpa2
title: Language Modeling on a SpiNNaker 2 Neuromorphic Chip
arxiv_id: '2312.09084'
source_url: https://arxiv.org/abs/2312.09084
tags:
- language
- egru
- neuromorphic
- learning
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents the first-ever implementation of a language
  model on a neuromorphic device, specifically the SpiNNaker 2 chip, using an event-based
  architecture called EGRU. The EGRU architecture is designed to leverage the efficient
  asynchronous processing capabilities of SpiNNaker 2 while maintaining competitive
  task performance.
---

# Language Modeling on a SpiNNaker 2 Neuromorphic Chip

## Quick Facts
- arXiv ID: 2312.09084
- Source URL: https://arxiv.org/abs/2312.09084
- Reference count: 36
- First-ever implementation of a language model on a neuromorphic device (SpiNNaker 2)

## Executive Summary
This work presents the first-ever implementation of a language model on a neuromorphic device, specifically the SpiNNaker 2 chip, using an event-based architecture called EGRU. The EGRU architecture is designed to leverage the efficient asynchronous processing capabilities of SpiNNaker 2 while maintaining competitive task performance. The implementation achieves the same perplexity (PPL) as LSTMs on the WikiText-2 dataset, demonstrating the feasibility of neuromorphic language modeling. The EGRU-based language model on SpiNNaker 2 consumes significantly less power (0.39 W) compared to a GPU (60 W) for single batch inference, resulting in substantial energy savings (0.0653 J vs 1.1935 J).

## Method Summary
The authors implement a 95% pruned EGRU model on the SpiNNaker 2 neuromorphic chip, using sparse CSR format for weights and on-chip SRAM for activations. The EGRU architecture is event-based, only communicating spikes when neuron states cross a threshold. The model is trained on WikiText-2 and evaluated for both language modeling (perplexity) and gesture recognition (accuracy on DVS128 dataset). The implementation leverages SpiNNaker 2's 152 PEs and efficient NoC for sparse, event-driven communication.

## Key Results
- Achieved same perplexity as LSTMs on WikiText-2 dataset
- Consumed 0.39 W vs 60 W on GPU for single batch inference
- Energy savings of 0.0653 J vs 1.1935 J per inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sparsity in weights and activations allows the SpiNNaker 2 chip to exploit asynchronous processing efficiently.
- Mechanism: The EGRU architecture is pruned to 95% sparsity, reducing both the number of weights stored and the number of active neuron spikes. This sparsity aligns with SpiNNaker 2's event-driven communication, which only transmits spikes when neurons fire, thereby avoiding unnecessary memory reads and network traffic.
- Core assumption: The combination of sparse weights and sparse activations leads to a proportional reduction in communication and computation costs on the neuromorphic hardware.
- Evidence anchors: [abstract] "EGRU is architected to leverage such hardware efficiently while maintaining competitive task performance." [section] "The model weights were stored on SRAM in a Sparse CSR format" and "This broadcast was implemented by sending internal NoC packets between PEs."
- Break condition: If the sparsity does not translate into reduced communication overhead (e.g., dense bursts of spikes or frequent weight accesses), the energy advantage diminishes.

### Mechanism 2
- Claim: The EGRU's event-based gating mechanism reduces inter-neuron communication compared to dense LSTMs.
- Mechanism: Instead of communicating full activation vectors at every time step, EGRU only sends spikes when neuron states cross a threshold, drastically reducing the volume of messages sent across the NoC.
- Core assumption: Threshold-triggered communication is sparse enough to keep the NoC bandwidth requirements low while preserving the temporal dynamics needed for language modeling.
- Evidence anchors: [abstract] "A sparse output y = (y1, . . . , yn) is generated from the GRU cell state c via the following mechanism y⟨t⟩i = c⟨t⟩i H (c⟨t⟩i − ϑi)." [section] "Only the sparse output y is communicated between neurons to compute the update gate u and the reset gate r of the GRU."
- Break condition: If the thresholding causes too many neurons to spike in bursts (reducing sparsity), or if the timing of spikes creates synchronization bottlenecks, communication efficiency drops.

### Mechanism 3
- Claim: Storing sparse weights in CSR format and keeping activations on-chip reduces memory traffic and power.
- Mechanism: Sparse CSR storage eliminates memory reads for zero weights, and on-chip SRAM holds intermediate activations, avoiding frequent DRAM accesses. This is crucial because memory accesses dominate energy use in conventional hardware.
- Core assumption: The CSR format and on-chip storage fit within the SRAM budget of SpiNNaker 2, and the overhead of CSR decoding is outweighed by savings in memory traffic.
- Evidence anchors: [section] "The pruned weights were stored in compressed sparse row (CSR) format" and "The local SRAM is organized into 4 memory banks of 32 kB each." [section] "The model weights were stored on SRAM in a Sparse CSR format. The three EGRU layers were implemented on 150 PEs."
- Break condition: If the model size grows beyond SRAM capacity, requiring off-chip storage, or if CSR decoding overhead becomes significant, the memory advantage is lost.

## Foundational Learning

- Concept: Spiking Neural Networks (SNNs) and event-driven communication
  - Why needed here: Understanding how neurons communicate via sparse spikes is key to grasping why SpiNNaker 2 is energy efficient.
  - Quick check question: What is the difference between a spike and a continuous activation in terms of communication overhead?

- Concept: Sparse matrix representations (CSR format)
  - Why needed here: CSR format is used to store the 95% pruned weights; knowing how it works explains the memory savings.
  - Quick check question: How does CSR format reduce memory usage compared to dense storage for a matrix with many zeros?

- Concept: Recurrent neural network gating mechanisms (GRU/EGRU)
  - Why needed here: EGRU is a variant of GRU; understanding the gating logic explains how it processes sequences and why event-based gating helps.
  - Quick check question: In a standard GRU, what do the update and reset gates control, and how does EGRU's thresholding modify this?

## Architecture Onboarding

- Component map: Input embedding -> PEs (sparse matrix multiply) -> NoC broadcast -> PEs (recurrent multiply) -> point-wise gating -> output projection
- Critical path: 1. Input embedding fetch from LPDDR4. 2. Sparse matrix multiply (input x weights) on each PE. 3. Broadcast of layer outputs via NoC. 4. Sparse recurrent matrix multiply (recurrent weights x previous outputs). 5. Point-wise gating and state update. 6. Output projection and softmax (for language modeling). 7. Store result, wait for host read.
- Design tradeoffs:
  - Sparse vs dense: Higher sparsity saves memory and communication but may require more complex decoding logic.
  - On-chip vs off-chip storage: SRAM is fast but limited; LPDDR4 is larger but slower and more power-hungry.
  - Pruning level: 95% maximizes sparsity but risks accuracy loss; lower pruning may improve accuracy but hurt efficiency.
  - PE allocation: More PEs can parallelize computation but increase communication overhead.
- Failure signatures:
  - Excessive memory usage: Model or activations do not fit in SRAM, causing off-chip spills.
  - Communication bottlenecks: Frequent broadcasts saturate NoC, increasing latency and power.
  - Numerical instability: CSR decoding errors or floating-point mismatches between GPU and SpiNNaker 2.
  - Performance drop: Accuracy or perplexity degrades compared to dense baseline.
- First 3 experiments:
  1. Run a single PE EGRU with dense weights; measure time and energy per inference.
  2. Enable 95% pruning and CSR storage; compare memory usage and inference time.
  3. Scale to multi-PE with sparse weights; profile communication overhead and total energy.

## Open Questions the Paper Calls Out
- Can neuromorphic language models achieve performance on par with large transformer-based models while maintaining significant energy efficiency?
- How does the energy efficiency of neuromorphic language models scale with model size and batch size compared to conventional hardware?
- What are the practical limitations and challenges in scaling up neuromorphic language models for real-world applications?

## Limitations
- Reliance on a specific 95% pruned EGRU model without full disclosure of training details and hyperparameters.
- Evaluation focuses on inference-only scenarios; scalability of training or adapting such models on SpiNNaker 2 remains unexplored.
- CSR storage and sparse decoding mechanisms are described, but potential overhead or edge cases are not quantified.

## Confidence
- High Confidence: The energy efficiency advantage of SpiNNaker 2 over GPU for single-batch inference is well-supported by direct measurements (0.39 W vs 60 W).
- Medium Confidence: The claim of matching LSTM perplexity on WikiText-2 is supported by the experimental setup, but the absence of explicit LSTM baselines and training details reduces confidence in the exact performance parity.
- Low Confidence: The scalability of the approach to larger models or datasets is not demonstrated; the paper only evaluates a fixed 3-layer EGRU model.

## Next Checks
1. Replicate CSR Memory Savings: Implement a small-scale EGRU model with 95% pruning in CSR format and measure memory usage on both SpiNNaker 2 and a conventional platform to confirm the claimed memory efficiency.
2. Profile Communication Overhead: Instrument the SpiNNaker 2 implementation to log NoC packet counts and timing during inference, verifying that sparsity translates to reduced communication as claimed.
3. Baseline Perplexity Comparison: Train or obtain an LSTM language model on WikiText-2 with comparable architecture and report perplexity side-by-side with the EGRU results to validate the performance parity claim.