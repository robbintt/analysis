---
ver: rpa2
title: End-to-End Retrieval with Learned Dense and Sparse Representations Using Lucene
arxiv_id: '2311.18503'
source_url: https://arxiv.org/abs/2311.18503
tags:
- retrieval
- dense
- sparse
- representations
- lucene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that Lucene is sufficient for end-to-end retrieval
  with learned dense and sparse representations, eliminating the need for separate
  vector stores and neural inference engines. The authors integrate the ONNX Runtime
  into the Anserini IR toolkit, enabling Java-based in-process query encoding on the
  CPU.
---

# End-to-End Retrieval with Learned Dense and Sparse Representations Using Lucene

## Quick Facts
- arXiv ID: 2311.18503
- Source URL: https://arxiv.org/abs/2311.18503
- Reference count: 9
- Primary result: Lucene can support end-to-end learned retrieval without external vector stores

## Executive Summary
This paper demonstrates that Lucene is sufficient for end-to-end retrieval with learned dense and sparse representations, eliminating the need for separate vector stores and neural inference engines. The authors integrate the ONNX Runtime into the Anserini IR toolkit, enabling Java-based in-process query encoding on the CPU. Experiments on the MS MARCO passage ranking benchmark show competitive effectiveness for learned dense (cosDPR-distil) and sparse (SPLADE++ ED) models, with throughput of 25.5 qps and 5.0 qps respectively. The approach enables enterprises to leverage neural retrieval advances without abandoning existing Lucene-based infrastructure investments.

## Method Summary
The method integrates ONNX Runtime into Anserini to enable Java-based neural inference for query encoding, while using Lucene's inverted indexes with the "fake words" trick for learned sparse retrieval and Lucene's HNSW indexes for learned dense retrieval. The approach indexes the MS MARCO passage corpus using both techniques, then performs retrieval on three query sets (dev, TREC 2019, TREC 2020) comparing pre-encoded vs ONNX-encoded queries for performance evaluation.

## Key Results
- Learned dense retrieval (cosDPR-distil) achieves competitive effectiveness on MS MARCO benchmark
- Learned sparse retrieval (SPLADE++ ED) demonstrates strong performance with smaller index footprint
- Query throughput of 25.5 qps for dense retrieval and 5.0 qps for sparse retrieval using ONNX-based encoding
- Inverted indexes with "fake words" trick successfully support learned sparse representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lucene's inverted indexes support learned sparse retrieval via the "fake words" trick
- Mechanism: Learned sparse models like SPLADE++ generate sparse vectors where dimensions correspond to vocabulary terms. The "fake words" trick involves treating each document as a bag of terms with learned weights, storing these in the inverted index as if they were regular terms. During retrieval, queries are also encoded into sparse vectors, and the inverted index is used to perform term-based lookups and scoring
- Core assumption: Learned sparse representations can be efficiently mapped into Lucene's inverted index structure without requiring special handling beyond standard inverted index operations
- Evidence anchors:
  - [abstract]: "Inverted indexes using the 'fake words' trick enable retrieval with learned sparse representations"
  - [section]: "Retrieval with sparse representations can be implemented with inverted indexes using the widely-known 'fake words' trick"
- Break condition: If learned sparse models generate vectors that cannot be decomposed into a sparse vocabulary-based representation

### Mechanism 2
- Claim: Lucene's HNSW implementation supports learned dense retrieval
- Mechanism: Learned dense models like cosDPR-distil generate dense vectors in continuous space. Lucene's HNSW (Hierarchical Navigable Small World) index structure provides efficient approximate nearest neighbor search in high-dimensional spaces. The integration allows indexing dense vectors directly in Lucene and performing similarity search without requiring external vector stores
- Core assumption: Lucene's HNSW implementation provides sufficient performance and accuracy for production use cases
- Evidence anchors:
  - [abstract]: "Lucene's HNSW indexes enable dense retrieval"
  - [section]: "Lin et al. [2023] argued that the need for the capabilities offered by HNSW indexes doesn't necessarily require full-blown vector stores as independent software components"
- Break condition: If Lucene's HNSW implementation lacks performance optimizations or features compared to dedicated vector stores

### Mechanism 3
- Claim: ONNX Runtime integration enables Java-based query encoding on CPU
- Mechanism: Query encoding traditionally requires Python-based neural inference frameworks to generate vector representations. The ONNX Runtime provides cross-language support, allowing Java applications to perform neural inference directly. By integrating ONNX Runtime into Anserini, queries can be encoded to vectors within the Java process without external dependencies
- Core assumption: ONNX Runtime can efficiently execute the neural models used for query encoding on CPU hardware without significant performance degradation
- Evidence anchors:
  - [abstract]: "with the integration of the ONNX Runtime, query inference can be performed directly from Lucene"
  - [section]: "The ONNX Runtime provides cross-platform, cross-language, and cross-device support for neural inference—including using Java, the language of Lucene's implementation"
- Break condition: If ONNX Runtime performance is inadequate for query-time inference requirements

## Foundational Learning

- Vector space model and nearest neighbor search: Understanding how queries and documents are represented as vectors and how similarity search retrieves relevant content is fundamental to grasping both dense and sparse retrieval mechanisms
  - Quick check: What is the primary operation used to measure similarity between query and document vectors in the bi-encoder architecture?

- Inverted index structure and scoring functions: Learned sparse models rely on inverted indexes, so understanding how inverted indexes store term-document relationships and how scoring functions operate is essential
  - Quick check: How do learned sparse scoring functions differ from traditional TF-IDF or BM25 scoring?

- Neural inference and ONNX format: Query encoding requires executing neural models. Understanding ONNX as an intermediate representation format that enables cross-language inference is crucial
  - Quick check: How does ONNX Runtime enable Java applications to perform neural inference without requiring Python or GPU resources?

## Architecture Onboarding

- Component map: Lucene core -> ONNX Runtime -> Anserini toolkit -> Pyserini (optional) -> External model files
- Critical path: Query → ONNX Runtime encoding → Lucene index search → Results
  The query encoding step is the performance bottleneck and most critical path component
- Design tradeoffs:
  - CPU vs GPU: CPU inference reduces costs and complexity but may impact latency
  - Single vector vs multi-vector: The paper focuses on single-vector representations, which are simpler but may be less effective than multi-vector approaches
  - In-process vs external: ONNX Runtime integration provides in-process encoding, reducing inter-process communication overhead
- Failure signatures:
  - High query latency: Could indicate ONNX Runtime performance issues or inefficient index access patterns
  - Low retrieval effectiveness: Could suggest model quality issues or inappropriate similarity measures
  - Memory pressure: Could result from large HNSW indexes or inefficient ONNX model loading
- First 3 experiments:
  1. Verify basic functionality: Index a small document collection and perform retrieval with pre-encoded queries to ensure Lucene integration works
  2. Test ONNX integration: Encode queries using ONNX Runtime and verify vector generation correctness
  3. Performance benchmarking: Compare query throughput and latency between pre-encoded and ONNX-encoded queries under single-thread and multi-thread conditions

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications remain unexplored regarding scalability, cost-effectiveness, and real-world deployment considerations.

## Limitations
- ONNX Runtime performance scaling with multi-threaded query encoding remains untested at larger scale
- The "fake words" trick implementation details are not fully specified, creating potential reproducibility issues
- Memory consumption patterns for large HNSW indexes with real-world document collections are unknown
- No comparison against specialized vector databases for production workloads

## Confidence
- High confidence in the conceptual feasibility of using Lucene for learned retrieval
- Medium confidence in the integration approach based on limited experimental evidence
- Low confidence in production readiness without larger-scale validation

## Next Checks
1. Performance Scaling Test: Measure query latency and throughput with increasing thread counts (1-32 threads) to identify performance bottlenecks and scaling limits
2. Memory Profiling: Analyze memory usage patterns during indexing and retrieval operations with progressively larger document collections
3. Production Workload Simulation: Test the integrated system against a production-scale document corpus (e.g., 1M+ documents) with realistic query patterns to evaluate real-world performance characteristics