---
ver: rpa2
title: A decoder-only foundation model for time-series forecasting
arxiv_id: '2310.10688'
source_url: https://arxiv.org/abs/2310.10688
tags:
- forecasting
- time-series
- datasets
- time
- patch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PreDcT, a decoder-only foundation model for
  time-series forecasting. It is pretrained on a large corpus of Google Trends data
  and other public datasets.
---

# A decoder-only foundation model for time-series forecasting

## Quick Facts
- arXiv ID: 2310.10688
- Source URL: https://arxiv.org/abs/2310.10688
- Reference count: 11
- Key outcome: PreDcT achieves strong zero-shot time-series forecasting performance, rivaling state-of-the-art supervised models across multiple benchmarks

## Executive Summary
This paper introduces PreDcT, a decoder-only foundation model for time-series forecasting. The model is pretrained on a large corpus of Google Trends data and other public datasets using a patched-decoder style attention architecture. PreDcT demonstrates strong zero-shot performance on various forecasting benchmarks, outperforming other zero-shot models like PatchTST and N-BEATS, particularly on datasets with different context lengths. The model's effectiveness is attributed to its ability to adapt to varying input and output patch lengths, enabling efficient autoregressive decoding.

## Method Summary
PreDcT is a decoder-only transformer pretrained on a mixture of time-series datasets, primarily Google Trends data from 22k head queries across multiple granularities (hourly, daily, weekly, monthly). The model uses a patched-decoder architecture where input time series are divided into non-overlapping patches, each processed by a residual block into transformer tokens. These tokens are fed into a transformer stack with causal attention for autoregressive prediction of the next output patch. The model is trained with maximum context lengths of 512 (hourly/daily), 256 (weekly), or 64 (monthly), and evaluated zero-shot on target datasets including ETTm1, ETTm2, ETTh1, ETTh2, Wiki, ILI, and TourismL.

## Key Results
- PreDcT achieves competitive zero-shot forecasting performance, matching or exceeding supervised models like PatchTST, TiDE, N-BEATS, and FEDFormer on multiple benchmarks
- The model demonstrates strong generalization across datasets with varying context and horizon lengths, outperforming other zero-shot approaches especially on datasets with different context lengths
- Ablation studies show that the choice of input patch length (32) and output patch length (128) significantly impacts performance, with these values providing the best trade-off

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The decoder-only transformer with patched attention can adapt to variable context and horizon lengths.
- Mechanism: The model breaks time series into patches, each processed by a residual block into a token. These tokens are fed into a transformer stack with causal attention, enabling autoregressive prediction of the next output patch. The input patch length need not equal the output patch length, allowing flexibility.
- Core assumption: The transformer can learn useful representations from patch tokens that generalize across different input and output lengths.
- Evidence anchors:
  - [abstract] "Our model is based on pretraining a patched-decoder style attention model on a large timeseries corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities."
  - [section] "The key elements of our foundation model are twofold: 1) a time series corpus built mostly using Google Trends, that meets the volume and diversity of data needed for training our foundation model, and 2) a patched-decoder style attention architecture that can be efficiently pre-trained on this time series corpus."
  - [corpus] Weak evidence; the corpus provides scale but does not directly support the mechanism claim.
- Break condition: If the patch length is too large relative to the context length, the model cannot generalize; or if the patch granularity is mismatched to the forecasting task.

### Mechanism 2
- Claim: Pretraining on a large and diverse time-series corpus yields a foundation model that generalizes to unseen datasets.
- Mechanism: The model is pretrained on Google Trends data and other public datasets, covering multiple granularities and domains. This pretraining allows the model to learn temporal patterns that transfer to new datasets.
- Core assumption: The pretraining corpus is representative enough of downstream tasks to enable zero-shot performance.
- Evidence anchors:
  - [section] "We train on a mixture distribution over these datasets that aims to give sufficient weightage to all granularities. We train with a maximum context length of 512 whenever the length of the time-series allows that."
  - [section] "The key elements of our foundation model are twofold: 1) a time series corpus built mostly using Google Trends, that meets the volume and diversity of data needed for training our foundation model."
  - [corpus] Moderate evidence; the corpus includes Google Trends and other datasets, but lacks detail on representativeness.
- Break condition: If the pretraining corpus lacks diversity or scale, zero-shot performance will degrade.

### Mechanism 3
- Claim: Residual blocks at input and output layers enable effective transformation between raw time series and transformer tokens.
- Mechanism: Each patch (along with optional date-derived features) is processed by a residual block (MLP with skip connection) into a vector of size model dim, which is then fed into the transformer layers. Another residual block maps the output tokens to predictions.
- Core assumption: The residual blocks can effectively learn the mapping between raw time series patches and transformer-compatible tokens.
- Evidence anchors:
  - [section] "The job of the input layers is to preprocess the time-series into input tokens to the transformer layers. We first break the input into contiguous non-overlapping patches. Then each patch (along with optional date derived features for that patch) is processed by a Residual Block into a vector of size model dim."
  - [section] "We use another Residual Block to map the output tokens to the predictions."
  - [corpus] No direct evidence; mechanism relies on architectural design rather than corpus data.
- Break condition: If the residual block capacity is insufficient for the data complexity, the model will underperform.

## Foundational Learning

- Concept: Causal attention in transformers
  - Why needed here: Enables autoregressive decoding by preventing future tokens from attending to past tokens.
  - Quick check question: What would happen if we used bidirectional attention instead of causal attention in the decoder?

- Concept: Residual connections in deep networks
  - Why needed here: Stabilizes training and allows gradients to flow through deeper architectures.
  - Quick check question: How does the skip connection in the residual block help with training stability?

- Concept: Patch-based modeling for variable-length sequences
  - Why needed here: Allows the model to handle varying context and horizon lengths by processing non-overlapping patches.
  - Quick check question: Why is patch-based modeling useful for time series with different granularities?

## Architecture Onboarding

- Component map: Input patch → residual block → transformer layers → output token → residual block → prediction
- Critical path: Input patch → residual block → transformer layers → output token → residual block → prediction
- Design tradeoffs:
  - Input patch length vs. context length flexibility: Larger input patches improve representation but reduce adaptability to short contexts.
  - Output patch length vs. autoregressive steps: Larger output patches reduce the number of decoding steps but may hurt accuracy on long horizons.
  - Model size vs. pretraining data: Larger models require more data to avoid overfitting.
- Failure signatures:
  - Poor performance on datasets with context lengths much smaller than the input patch length.
  - Degraded accuracy when the output patch length is too large for the forecasting horizon.
  - Instability during training if residual block dimensions are mismatched to the model dimension.
- First 3 experiments:
  1. Train a small version of the model (e.g., 50M parameters) on a subset of the pretraining corpus and evaluate zero-shot on a single target dataset to verify basic functionality.
  2. Vary the input patch length (e.g., 8, 16, 32) and measure impact on zero-shot performance to find the optimal trade-off.
  3. Test the model's ability to adapt to different context lengths by evaluating on target datasets with varying context lengths while keeping the pretraining configuration fixed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PreDcT scale with increasing model size and pretraining data size, and what is the optimal balance between these factors for different forecasting tasks?
- Basis in paper: [explicit] The paper mentions that PreDcT has 225M parameters and is pretrained on about 1B time-points. It also suggests that exploring the scaling of the model with respect to pretraining data and parameters is an interesting direction for future work.
- Why unresolved: The paper only presents results for a single model size and pretraining dataset. It does not explore the impact of scaling these factors on performance.
- What evidence would resolve it: Conducting experiments with varying model sizes and pretraining dataset sizes, and analyzing the performance trade-offs for different forecasting tasks.

### Open Question 2
- Question: How does the performance of PreDcT compare to other foundation models for time-series forecasting, such as TimeGPT-1, in terms of accuracy, efficiency, and generalization capabilities?
- Basis in paper: [explicit] The paper mentions TimeGPT-1 as a parallel work on foundation models for time-series forecasting, but notes that it is not publicly accessible and lacks detailed information.
- Why unresolved: There is no direct comparison between PreDcT and TimeGPT-1 in the paper, making it difficult to assess their relative strengths and weaknesses.
- What evidence would resolve it: Conducting a head-to-head comparison between PreDcT and TimeGPT-1 on the same benchmark datasets, evaluating their performance in terms of accuracy, efficiency, and generalization capabilities.

### Open Question 3
- Question: How does the choice of input patch length and output patch length in PreDcT affect its performance on different forecasting tasks, and what are the optimal values for these parameters?
- Basis in paper: [explicit] The paper presents ablation studies on input patch length and output patch length, showing that their choice can impact performance. However, it does not provide a systematic exploration of the optimal values for these parameters.
- Why unresolved: The ablation studies only consider a limited set of values for input patch length and output patch length, and do not explore the full range of possible values or their impact on different forecasting tasks.
- What evidence would resolve it: Conducting a comprehensive study on the impact of input patch length and output patch length on PreDcT's performance across various forecasting tasks, and identifying the optimal values for these parameters based on the task characteristics.

## Limitations

- The Google Trends pretraining corpus, while large in volume, is restricted to 22k head queries, raising questions about whether the diversity is sufficient to enable true foundation-model generalization across arbitrary time-series domains.
- The paper does not provide ablation studies isolating the contribution of each architectural choice, making it difficult to assess which mechanisms are essential versus incidental to performance.
- While the model shows competitive zero-shot results, the comparisons against supervised models are inherently limited - the zero-shot setting may be benefiting from specific data characteristics in the target datasets that aren't representative of general time-series forecasting challenges.

## Confidence

**High Confidence:** The claim that PreDcT achieves strong zero-shot performance on various forecasting benchmarks is well-supported by the experimental results, with clear quantitative metrics (NRMSE, WAPE) provided across multiple datasets.

**Medium Confidence:** The assertion that the patched-decoder architecture specifically enables adaptation to variable context and horizon lengths is supported by design rationale and some empirical evidence, but lacks systematic ablation studies to confirm this mechanism is the primary driver of success.

**Low Confidence:** The claim that the model's effectiveness is "attributed to its ability to adapt to varying input patch lengths and output patch lengths" is largely speculative - while the architecture allows this flexibility, the paper doesn't demonstrate that this flexibility is actually what drives the observed performance improvements.

## Next Checks

1. **Corpus Diversity Validation:** Create a smaller, more diverse pretraining corpus (e.g., using random queries from multiple domains rather than just head queries) and retrain PreDcT to assess whether performance is driven by scale or diversity. Compare zero-shot performance on target datasets against the original model.

2. **Patch Size Ablation Study:** Systematically vary the input patch length (8, 16, 32, 64) and output patch length (32, 64, 128, 256) across multiple target datasets with different context/horizon lengths. Measure the impact on accuracy and autoregressive decoding efficiency to identify optimal configurations and test the hypothesis that patch-based modeling is essential for generalization.

3. **Context Length Robustness Test:** Evaluate PreDcT's performance across a wider range of context lengths than used in pretraining (e.g., test with context lengths of 64, 128, 256, 512, 1024 when pretrained with maximum 512). This would validate whether the model truly generalizes to variable-length inputs or is simply optimized for the pretraining distribution.