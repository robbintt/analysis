---
ver: rpa2
title: 'Labrador: Exploring the Limits of Masked Language Modeling for Laboratory
  Data'
arxiv_id: '2312.11502'
source_url: https://arxiv.org/abs/2312.11502
tags:
- bert
- data
- blood
- test
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pre-training masked language models (MLMs) on laboratory data from
  electronic health records shows that neither a continuous Transformer architecture
  (Labrador) nor a standard BERT model consistently outperforms tree-based baselines
  like XGBoost on downstream outcome prediction tasks. While both models demonstrate
  mastery of the pre-training task, transfer learning shows limited effectiveness,
  with Labrador marginally outperforming BERT in some cases.
---

# Labrador: Exploring the Limits of Masked Language Modeling for Laboratory Data

## Quick Facts
- arXiv ID: 2312.11502
- Source URL: https://arxiv.org/abs/2312.11502
- Reference count: 40
- Primary result: Neither Labrador nor BERT consistently outperforms tree-based baselines on downstream outcome prediction tasks using only lab data.

## Executive Summary
This paper explores the effectiveness of pre-training Transformer models on laboratory data from electronic health records for downstream outcome prediction tasks. The authors introduce Labrador, a continuous Transformer architecture that processes lab values as continuous variables rather than discretized tokens. Despite successful pre-training on 100 million lab tests, both Labrador and a standard BERT model show limited transfer learning effectiveness, failing to consistently outperform tree-based baselines like XGBoost. The study identifies key challenges including the permutation-invariant nature of lab data bags and the insufficiency of lab data alone to characterize patient state for complex outcome prediction.

## Method Summary
The study pre-trains two Transformer models (Labrador and BERT) on laboratory data from MIMIC-IV using a masked language modeling objective. Labrador uses continuous embeddings that map lab values to [0,1] using the empirical CDF, while BERT uses discretized decile-based tokenization. Both models are pre-trained on 100 million lab test results and then fine-tuned on four downstream tasks: COVID-19 diagnosis, cancer diagnosis, sepsis-related mortality, and alcohol consumption prediction. Performance is compared against tree-based baselines (XGBoost, Random Forest, logistic regression) using cross-validation.

## Key Results
- Labrador and BERT both successfully learn the pre-training task but show limited transfer to downstream outcome prediction
- Labrador marginally outperforms BERT in some cases but neither consistently beats XGBoost baselines
- Pre-training on lab data alone is insufficient for complex outcome prediction tasks
- The permutation-invariant nature of lab data bags creates fundamental challenges for masking-based pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous Transformer architecture (Labrador) outperforms BERT because it can model lab values as continuous variables rather than discretized tokens.
- Mechanism: Labrador uses a continuous embedding layer that maps lab values to the interval [0,1] using the empirical CDF, preserving the full range of values and enabling the model to learn monotonic relationships with lab measurements.
- Core assumption: The continuous representation of lab values provides more informative features than discretized deciles used by BERT.
- Evidence anchors: [section] "L ABRADOR ’s continuous embedding layer is defined in the following pseudocode: def ContinuousEmbeddingLayer(lab_values, token_embeddings): x = Dense_pw(lab_values, out_size=d_model, activation=‘linear’) x = x + token_embeddings x = Dense_pw(x, outsize=d_model, activation=‘relu’) x = LayerNorm(x) return x"

### Mechanism 2
- Claim: Pre-training on lab data alone is insufficient for transfer learning to downstream outcome prediction tasks.
- Mechanism: Lab data may not capture enough of the patient's physiological state to predict complex outcomes like sepsis mortality or cancer diagnosis, as these outcomes depend on factors beyond laboratory measurements.
- Core assumption: Laboratory data alone is an incomplete representation of patient state for predicting complex clinical outcomes.
- Evidence anchors: [abstract] "We explore the reasons for the failure of transfer learning and suggest that the data generating process underlying each patient cannot be characterized sufficiently using labs alone"

### Mechanism 3
- Claim: The permutation-invariant nature of lab data bags limits the effectiveness of masking-based pre-training.
- Mechanism: In BERT's original setting, positional embeddings allow the model to distinguish between different masked positions. For lab data, which is permutation-invariant, masking multiple elements within a bag creates ambiguity as the model cannot differentiate between different masked positions.
- Core assumption: The lack of positional information in lab data bags prevents effective masking-based pre-training.
- Evidence anchors: [section] "However, increasing the mask rate may be impossible because each bag of labs is permutation invariant. As a result, if 2 or more elements are masked within a bag, LABRADOR and BERT's predictions for each mask token will be identical."

## Foundational Learning

- Concept: Masked Language Modeling (MLM)
  - Why needed here: MLM is the pre-training objective used for both Labrador and BERT, where the model learns to predict masked elements in the input sequence.
  - Quick check question: What is the difference between MLM and standard language modeling objectives?

- Concept: Permutation Invariance
  - Why needed here: Lab data is often collected as unordered sets of measurements (bags), requiring models to handle permutation-invariant inputs.
  - Quick check question: Why does permutation invariance create challenges for masking-based pre-training?

- Concept: Transfer Learning
  - Why needed here: The goal is to pre-train models on lab data and then fine-tune them for specific downstream prediction tasks.
  - Quick check question: What are the key factors that determine whether pre-training will successfully transfer to downstream tasks?

## Architecture Onboarding

- Component map: Lab data bags -> Continuous/Discrete embedding layer -> Transformer blocks -> Multi-task prediction head
- Critical path:
  1. Pre-process lab data using eCDF to map values to [0,1]
  2. Pre-train Labrador/BERT using MLM objective on 100M lab tests
  3. Fine-tune on downstream outcome prediction tasks
  4. Evaluate against baselines (XGBoost, Random Forest, etc.)

- Design tradeoffs:
  - Continuous vs discretized lab value representation
  - Permutation invariance vs positional information
  - Pre-training data scale vs computational resources
  - Single data modality vs multi-modal joint modeling

- Failure signatures:
  - Pre-training loss plateaus early (insufficient model capacity or data)
  - No improvement over baselines in fine-tuning (pre-training didn't learn useful representations)
  - Poor imputation performance (model didn't learn correlations between lab values)
  - Overfitting on small downstream datasets

- First 3 experiments:
  1. Reproduce pre-training loss curves for Labrador and BERT to verify successful optimization
  2. Evaluate imputation performance on masked lab values from test split
  3. Fine-tune both models on a simple downstream task (e.g., COVID-19 diagnosis) and compare to XGBoost baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does pre-training on a larger, more diverse dataset of laboratory data improve downstream transfer learning performance for outcome prediction tasks?
- Basis in paper: [inferred] The paper suggests insufficient data scale as a reason for limited transfer learning success, noting that pre-training data was 4.5 million input sequences compared to 145 million for GPT-3.
- Why unresolved: The study used a fixed dataset size of 100 million lab tests. Scaling up would require accessing additional data sources and harmonizing disparate datasets.
- What evidence would resolve it: Experiments showing improved downstream performance on outcome prediction tasks when pre-training on datasets 10x or 100x larger than the current study.

### Open Question 2
- Question: Would incorporating additional EHR data modalities (e.g. diagnoses, medications) alongside laboratory data during pre-training lead to better downstream performance on outcome prediction tasks?
- Basis in paper: [explicit] The authors suggest that "laboratory data, unlike text, do not provide a comprehensive view of a patient's condition" and recommend future work focus on "joint modeling of several categories of EHR data."
- Why unresolved: The study only used laboratory data for pre-training. Integrating multiple data modalities would require significant changes to the model architecture and data preprocessing pipeline.
- What evidence would resolve it: Experiments comparing downstream performance of models pre-trained on lab data alone versus models pre-trained on multi-modal EHR data (labs + diagnoses + medications + procedures).

### Open Question 3
- Question: Would modifying the MLM pre-training objective (e.g. increasing the masking rate or using a different masking strategy) improve the quality of learned representations for laboratory data?
- Basis in paper: [inferred] The authors discuss the low masking rate (5%) and the challenge of breaking permutation invariance when masking multiple elements in a bag of labs, suggesting this may limit representation learning.
- Why unresolved: The study used a fixed MLM objective with one random mask per bag. Exploring alternative masking strategies or objectives would require re-implementing the pre-training procedure.
- What evidence would resolve it: Experiments comparing downstream performance of models pre-trained with different MLM objectives (e.g. higher masking rate, whole-bag masking, multi-task objectives) on the same outcome prediction tasks.

## Limitations
- Single data modality constraint: The study focuses exclusively on lab data, which may be insufficient to capture the full patient state needed for complex outcome prediction
- Permutation invariance challenge: The fundamental challenge of handling permutation-invariant lab data bags creates ambiguity in masking-based pre-training
- Scale vs. data quality trade-off: Unclear whether larger datasets alone would overcome the limitations of lab data as a single modality

## Confidence
**High Confidence Claims:**
- Labrador and BERT both successfully learn the pre-training task
- Neither model consistently outperforms tree-based baselines on downstream tasks
- Pre-training on lab data alone is insufficient for transfer learning to complex outcome prediction tasks

**Medium Confidence Claims:**
- Labrador's continuous embedding provides marginally better performance than BERT in some cases
- The permutation-invariant nature of lab data creates fundamental challenges for masking-based pre-training
- Lab data alone cannot sufficiently characterize the patient data generating process for outcome prediction

**Low Confidence Claims:**
- Larger pre-training datasets would significantly improve downstream performance
- Multi-modal joint modeling would definitively solve the transfer learning challenges observed
- The continuous representation is fundamentally superior to discretized representations for lab data

## Next Checks
1. **Ablation Study on Masking Strategy**: Systematically vary the masking rate (1 vs 2+ labs per bag) and evaluate the impact on pre-training performance and downstream transfer. This would directly test the permutation invariance hypothesis and identify whether alternative masking strategies could mitigate this limitation.

2. **Multi-Modal Joint Modeling Experiment**: Train a model that jointly processes lab data along with diagnoses, medications, and clinical notes on the same MIMIC-IV dataset. Compare performance to the single-modality approach to validate whether combining data types would overcome the limitations identified.

3. **Scale Sensitivity Analysis**: Pre-train models on progressively larger subsets of the lab data (e.g., 10M, 50M, 100M, 200M lab results) while keeping all other factors constant. Measure the impact on downstream task performance to determine whether scale is the limiting factor or if the modality constraint is fundamental.