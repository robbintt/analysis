---
ver: rpa2
title: 'Self-training Strategies for Sentiment Analysis: An Empirical Study'
arxiv_id: '2309.08777'
source_url: https://arxiv.org/abs/2309.08777
tags:
- sentiment
- data
- self-training
- labeled
- unlabeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares multiple self-training strategies for sentiment
  analysis, including threshold-based, max/min-based, soft-label, and LLM-based methods.
  We empirically investigate how different instance selection strategies and hyper-parameters
  affect self-training performance under various few-shot settings.
---

# Self-training Strategies for Sentiment Analysis: An Empirical Study

## Quick Facts
- arXiv ID: 2309.08777
- Source URL: https://arxiv.org/abs/2309.08777
- Reference count: 5
- This study compares multiple self-training strategies for sentiment analysis, including threshold-based, max/min-based, soft-label, and LLM-based methods.

## Executive Summary
This empirical study investigates various self-training strategies for sentiment analysis under few-shot learning settings. The research compares threshold-based, max/min-based, soft-label, and LLM-based methods to determine their effectiveness in leveraging unlabeled data. Experiments on two public datasets (LDC and MOSEI) demonstrate that self-training can enhance model performance, with different strategies showing optimal performance depending on the amount of available labeled data. The study provides insights into how instance selection strategies and hyper-parameters affect self-training effectiveness.

## Method Summary
The study employs self-training on sentiment analysis tasks using RoBERTa-base as the base classifier. The method starts with supervised training on limited labeled data, then iteratively adds pseudo-labeled instances from unlabeled data using various selection strategies. Four main strategies are evaluated: confidence/entropy threshold-based selection, max/min-based selection, soft-label learning using KL divergence, and LLM-based pseudo-labeling (Flan-UL2 and GPT-4). The experiments test different n-shot settings (5, 10, 15, 20, 25, 30 labeled instances per class) on two public datasets, comparing final model performance using F1 scores against supervised learning baselines.

## Key Results
- Self-training enhances model performance by leveraging unlabeled data, particularly when sufficient labeled data are provided
- Soft labels perform best with limited labeled data, while confidence/entropy thresholds work better with more labeled data
- LLM-based methods show promise in few-shot settings, with Flan-UL2 outperforming traditional methods when labeled data is scarce
- The choice of instance selection strategy and hyper-parameters significantly impacts self-training effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Soft-label self-training improves performance when labeled data is limited by avoiding hard pseudo-label errors.
- Mechanism: Instead of assigning hard pseudo-labels, the method uses the predicted probability distribution as a soft target. This reduces the impact of incorrect predictions during model training.
- Core assumption: The predicted probability distribution contains useful signal even when the hard label might be wrong.
- Evidence anchors:
  - [abstract]: "Among the strategies, soft labels perform best with limited labeled data"
  - [section]: "The soft label method doesn't explicitly predict a pseudo-label for self-training but uses the predicted probability distribution as the supervised signal. It has a greater fault tolerance by avoiding errors caused by mispredicted pseudo-labels when the model is not well initialized with limited labeled data."

### Mechanism 2
- Claim: Threshold-based selection works better with more labeled data because the model can make more reliable predictions.
- Mechanism: Instances with high confidence scores or low entropy distributions are selected as reliable pseudo-labeled data. As the model improves with more training data, its predictions become more reliable.
- Core assumption: Higher confidence scores correlate with correct predictions.
- Evidence anchors:
  - [abstract]: "confidence/entropy thresholds work better with more labeled data"
  - [section]: "The confidence/entropy threshold strategies work better when more labeled data are given. It is because when the model is well initialized, the threshold-based strategies can help us find instances with reliable pseudo-labels"

### Mechanism 3
- Claim: LLM-based pseudo-labeling can provide higher quality labels than model-based pseudo-labeling when labeled data is scarce.
- Mechanism: An external LLM (Flan-UL2) generates pseudo-labels, bringing in knowledge from its pre-training. This is particularly effective when the base model lacks sufficient training data.
- Core assumption: The LLM's pre-training knowledge transfers to the sentiment classification task.
- Evidence anchors:
  - [abstract]: "LLM-based methods show promise in few-shot settings"
  - [section]: "On the LDC dataset, we find that the Flan-UL2 LLM works better than traditional self-training methods when fewer labeled data are given, which demonstrates the ability of external knowledge to facilitate self-training when limited supervision information is available."

## Foundational Learning

- Concept: Semi-supervised learning
  - Why needed here: Self-training is a semi-supervised learning technique that leverages both labeled and unlabeled data.
  - Quick check question: What is the key difference between supervised and semi-supervised learning?

- Concept: Confidence scoring and entropy in classification
  - Why needed here: Threshold-based selection strategies rely on confidence scores and entropy to measure prediction reliability.
  - Quick check question: How does entropy relate to prediction certainty in a probability distribution?

- Concept: Kullback-Leibler divergence
  - Why needed here: The soft-label method uses KL divergence to measure the difference between predicted and target distributions.
  - Quick check question: What does KL divergence measure between two probability distributions?

## Architecture Onboarding

- Component map: RoBERTa-base classifier -> Self-training loop -> Instance selection strategies (threshold, max/min, soft-label, LLM) -> Hyperparameter tuning

- Critical path: 1. Initialize model with labeled data, 2. Make predictions on unlabeled data, 3. Select instances based on chosen strategy, 4. Add selected instances to training set, 5. Retrain model, 6. Repeat until termination condition

- Design tradeoffs:
  - Soft labels vs. hard labels: Soft labels provide more robust learning but may require more training iterations
  - Strict vs. lenient thresholds: Strict thresholds ensure higher quality but may limit the amount of additional training data
  - LLM-based vs. model-based labeling: LLM-based labeling can provide higher quality but adds computational overhead

- Failure signatures:
  - Performance plateaus or degrades: May indicate incorrect threshold settings or poor quality pseudo-labels
  - Slow convergence: May indicate overly strict selection criteria or insufficient unlabeled data
  - High variance across runs: May indicate sensitivity to initialization or randomness in instance selection

- First 3 experiments:
  1. Compare supervised learning baseline vs. self-training with soft labels on 5-shot LDC dataset
  2. Test different confidence thresholds (0.5, 0.7, 0.9) on 20-shot LDC dataset
  3. Compare Flan-UL2 vs. RoBERTa-based pseudo-labeling on 5-shot LDC dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop more effective prompts for GPT-4 in few-shot settings to improve its performance on sentiment analysis tasks?
- Basis in paper: [explicit] The paper observes that GPT-4's performance drops when 5 examples of each class are provided, indicating that not every example enhances its inference capability.
- Why unresolved: The paper does not explore different prompting strategies or analyze which types of examples are most effective for GPT-4 in few-shot settings.
- What evidence would resolve it: Systematic experiments comparing various prompt designs, example selection strategies, and their impact on GPT-4's performance across different sentiment analysis datasets.

### Open Question 2
- Question: What are the optimal threshold values for confidence and entropy-based instance selection strategies across different dataset characteristics and few-shot settings?
- Basis in paper: [explicit] The paper analyzes threshold impacts on the LDC dataset under 20-shot setting but notes that threshold selection is critical and the relationship between thresholds and selected instances is non-monotonic.
- Why unresolved: The analysis is limited to one dataset and setting, and the paper does not provide general guidelines for threshold selection across varying conditions.
- What evidence would resolve it: Comprehensive empirical studies across multiple datasets with different characteristics (text length, domain, class balance) and systematic threshold tuning experiments.

### Open Question 3
- Question: How can we automatically determine when to stop self-training iterations to maximize model performance without overfitting?
- Basis in paper: [explicit] The paper mentions that self-training terminates when no more unlabeled data can be selected or when performance doesn't improve for consecutive epochs, but doesn't explore optimal stopping criteria.
- Why unresolved: The paper uses fixed stopping criteria (2 consecutive epochs without improvement) but doesn't investigate whether this is optimal or how to adapt stopping criteria based on dataset characteristics.
- What evidence would resolve it: Experiments comparing different stopping criteria (validation-based, early stopping with patience tuning, statistical significance tests) and their impact on final model performance.

### Open Question 4
- Question: How can LLM-based self-training strategies be improved to handle longer text sequences effectively?
- Basis in paper: [explicit] The paper notes that Flan-UL2 performs poorly on the MOSEI dataset due to suboptimal ability to infer sentiment of long texts.
- Why unresolved: The paper identifies the limitation but does not explore solutions such as text chunking, hierarchical approaches, or specialized prompting strategies for long documents.
- What evidence would resolve it: Experiments testing different text segmentation methods, hierarchical LLM approaches, or prompt engineering techniques specifically designed for long document sentiment analysis.

## Limitations

- Findings are based on experiments with only two specific datasets (LDC and MOSEI) and one base model architecture (RoBERTa-base), limiting generalizability.
- Hyperparameter search for thresholds and k values was limited to specific ranges, potentially missing optimal configurations.
- Computational resources required for LLM-based methods (Flan-UL2 and GPT-4) may limit their practical applicability in resource-constrained environments.

## Confidence

- **High Confidence**: The general observation that self-training can improve sentiment analysis performance, especially when leveraging unlabeled data.
- **Medium Confidence**: The specific claim that soft labels perform best with limited labeled data while threshold-based methods work better with more labeled data.
- **Medium Confidence**: The finding that LLM-based methods show promise in few-shot settings, particularly for the LDC dataset.

## Next Checks

1. **Cross-domain validation**: Test the proposed self-training strategies on datasets from different domains (e.g., product reviews, social media, news articles) to assess generalizability of the findings about which strategies work best under different data availability scenarios.

2. **Model architecture ablation**: Repeat the experiments using different base models (e.g., BERT, DeBERTa, or domain-specific sentiment models) to determine whether the observed performance patterns are specific to RoBERTa or generalize across architectures.

3. **Computational cost-benefit analysis**: Systematically measure and compare the computational overhead of each self-training strategy, particularly LLM-based methods, against their performance gains to provide a complete picture of practical trade-offs for deployment scenarios.