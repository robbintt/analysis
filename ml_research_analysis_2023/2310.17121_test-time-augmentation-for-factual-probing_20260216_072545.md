---
ver: rpa2
title: Test-time Augmentation for Factual Probing
arxiv_id: '2310.17121'
source_url: https://arxiv.org/abs/2310.17121
tags:
- where
- prompts
- prompt
- language
- buried
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces test-time augmentation (TTA) to factual probing,
  addressing the sensitivity of language models to small prompt variations. TTA automatically
  generates and aggregates multiple prompts at test time to improve model robustness
  and calibration.
---

# Test-time Augmentation for Factual Probing

## Quick Facts
- arXiv ID: 2310.17121
- Source URL: https://arxiv.org/abs/2310.17121
- Reference count: 4
- Key outcome: TTA improves accuracy for smaller models (T5-Small, T5-3B, T0_3B) and significantly enhances model calibration, but degrades accuracy for larger models (T5-Large, T5-11B, FLAN-T5)

## Executive Summary
This paper introduces test-time augmentation (TTA) to factual probing, addressing the sensitivity of language models to small prompt variations. TTA automatically generates and aggregates multiple prompts at test time to improve model robustness and calibration. Experiments on seven language models show that TTA improves accuracy for smaller models and significantly enhances model calibration by reducing overconfidence in incorrect answers. However, TTA degrades accuracy for larger models. Error analysis reveals that the inconsistent performance is mainly due to the low quality of automatically generated prompts, with some relations (e.g., "follows") being particularly challenging to paraphrase. Overall, TTA shows promise for improving robustness and calibration in factual probing, especially for smaller models, but further research is needed to address prompt quality issues and extend the method to more complex tasks.

## Method Summary
The method applies test-time augmentation to factual probing by generating 30 paraphrased prompts per original prompt using synonym replacement (WordNet and GloVe), back-translation, and stopword filtering. These augmented prompts are processed by language models using beam search, and predictions are aggregated by summing generation probabilities. The approach aims to mitigate model sensitivity to prompt variations and improve calibration by averaging out overconfident individual predictions. Experiments evaluate TTA's effectiveness on 12,500 relational facts from Wikidata across seven language models of varying sizes.

## Key Results
- TTA improves accuracy for smaller models (T5-Small, T5-3B, T0_3B) but degrades accuracy for larger models (T5-Large, T5-11B, FLAN-T5)
- TTA significantly enhances model calibration by reducing overconfidence in incorrect predictions
- The "follows" relation presents particular challenges for automatic paraphrasing, contributing to degraded performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TTA improves model calibration by reducing overconfidence in incorrect predictions
- Mechanism: Aggregation of multiple augmented prompts averages out overconfident individual predictions, leading to more calibrated confidence scores
- Core assumption: Individual model predictions exhibit varying confidence levels across different prompt paraphrases
- Evidence anchors:
  - [abstract] "Experiments show improved model calibration, i.e., with TTA, model confidence better reflects prediction accuracy"
  - [section 3.1] "TTA was also effective in reducing the number of overconfident and incorrect outputs"
  - [corpus] No direct corpus evidence found - this appears to be a novel application of TTA to NLP calibration

### Mechanism 2
- Claim: TTA mitigates model sensitivity to prompt variations through ensemble averaging
- Mechanism: By generating multiple paraphrased prompts and aggregating results, TTA smooths out the model's sensitivity to individual prompt variations
- Core assumption: Model outputs are stable across meaningful prompt paraphrases when aggregated
- Evidence anchors:
  - [abstract] "Previous work aimed to alleviate this problem by optimizing prompts via text mining or fine-tuning"
  - [section 3.1] "The sudden drop from K = 1 to K = 2 shows that models are highly sensitive to prompt variations"
  - [corpus] No direct corpus evidence found - this appears to be a novel application of TTA to prompt sensitivity

### Mechanism 3
- Claim: TTA effectiveness depends on the quality of automatically generated paraphrases
- Mechanism: High-quality paraphrases that preserve meaning while varying surface form enable effective ensemble averaging
- Core assumption: Automated paraphrase generation methods produce sufficiently high-quality paraphrases
- Evidence anchors:
  - [abstract] "Error analysis identifies the difficulty of producing high-quality prompt variations as the main challenge for TTA"
  - [section 3.2] "Manual inspection suggests that the negative effects of TTA are mainly due to the low quality of the augmented prompts"
  - [corpus] No direct corpus evidence found - this appears to be a novel finding about TTA quality requirements

## Foundational Learning

- Concept: Test-time augmentation
  - Why needed here: TTA provides a relation-agnostic method for handling prompt sensitivity without requiring task-specific optimization
  - Quick check question: What is the core difference between TTA applied to images versus text?

- Concept: Model calibration
  - Why needed here: Understanding how confidence scores relate to prediction accuracy is crucial for evaluating TTA's effectiveness
  - Quick check question: How does TTA change the relationship between model confidence and accuracy?

- Concept: Beam search decoding
  - Why needed here: Understanding how beam search generates multiple hypotheses is important for understanding TTA's aggregation mechanism
  - Quick check question: How does beam search output affect the aggregation process in TTA?

## Architecture Onboarding

- Component map: Input prompt -> Augmenter -> Language Model (beam search) -> Aggregator -> Final prediction
- Critical path:
  1. Original prompt → Augmenter
  2. Each augmented prompt → Language Model (beam search)
  3. All model outputs → Aggregator
  4. Aggregated result → Final prediction
- Design tradeoffs:
  - More augmentation methods vs. quality of paraphrases
  - Number of augmented prompts vs. computational cost
  - Simple aggregation vs. more sophisticated combination methods
- Failure signatures:
  - Decreased accuracy when using TTA
  - Inconsistent performance across different relations
  - High sensitivity to specific relation types (e.g., "follows")
- First 3 experiments:
  1. Test TTA with varying numbers of prompts (1-30) to find optimal K
  2. Compare different aggregation methods (count-based vs. probability-based)
  3. Evaluate TTA effectiveness across different relation types to identify challenging cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific properties of the "follows" relation make it particularly challenging for automatic paraphrasing?
- Basis in paper: [explicit] The paper identifies the "follows" relation as particularly error-prone in the augmentation step, noting that it has a large variety of different entity and event types (e.g., numbers, months, events).
- Why unresolved: The paper does not provide a detailed analysis of why this specific relation is problematic or what characteristics of the relation cause the augmentation methods to fail.
- What evidence would resolve it: A detailed analysis of the semantic and syntactic properties of the "follows" relation compared to other relations, along with experiments showing how different types of augmentations affect each relation differently.

### Open Question 2
- Question: How does the performance of TTA scale with the size and complexity of the language model?
- Basis in paper: [explicit] The paper shows that TTA improves accuracy for smaller models (T5-Small, T5-3B, T0_3B) but degrades accuracy for larger models (T5-Large, T5-11B, FLAN-T5).
- Why unresolved: The paper does not explore the relationship between model size and TTA effectiveness in depth, nor does it provide a theoretical explanation for why larger models might be less responsive to TTA.
- What evidence would resolve it: Experiments varying model sizes systematically, along with analysis of how model capacity affects the sensitivity to prompt variations and the effectiveness of aggregation.

### Open Question 3
- Question: Can the quality of augmented prompts be improved through alternative augmentation methods or by incorporating additional linguistic knowledge?
- Basis in paper: [explicit] The paper identifies low-quality augmented prompts as the main challenge for TTA and shows that using GPT-3 for paraphrasing improves results compared to simpler methods.
- Why unresolved: The paper does not explore a wide range of alternative augmentation techniques or investigate how linguistic knowledge (e.g., syntax, semantics) could be incorporated to improve prompt quality.
- What evidence would resolve it: Experiments comparing TTA performance using various augmentation methods, including those that leverage linguistic knowledge, and analysis of how these methods affect the quality and diversity of augmented prompts.

## Limitations

- TTA effectiveness exhibits significant model-size dependency, improving accuracy for smaller models while degrading performance for larger models
- The method's generalizability to more complex NLP tasks beyond factual probing remains untested
- Low-quality augmented prompts represent a fundamental challenge that requires systematic solutions

## Confidence

**High Confidence Claims:**
- TTA improves model calibration for factual probing tasks
- Smaller models benefit from TTA while larger models suffer accuracy degradation
- Aggregating predictions through probability summation is effective

**Medium Confidence Claims:**
- Prompt sensitivity varies significantly across different relation types
- The "follows" relation presents particular challenges for paraphrase generation
- TTA's effectiveness depends on the quality of augmented prompts

**Low Confidence Claims:**
- TTA can be directly extended to more complex NLP tasks without modification
- The observed calibration improvements will generalize to non-factual tasks
- The specific augmentation methods (synonym replacement, back-translation) are optimal for this application

## Next Checks

1. **Model-size threshold investigation**: Systematically evaluate TTA performance across intermediate model sizes to identify the precise threshold where performance shifts from beneficial to detrimental, and investigate whether model architecture (encoder-decoder vs. decoder-only) influences this threshold.

2. **Prompt quality assessment framework**: Develop and implement a quantitative framework for evaluating augmented prompt quality that correlates prompt quality scores with downstream performance, enabling systematic comparison of different augmentation methods.

3. **Extended task evaluation**: Apply T5-Small with TTA to a more complex task (e.g., multi-hop reasoning or question answering requiring inference) to assess whether the calibration improvements observed in factual probing transfer to tasks requiring higher-level reasoning capabilities.