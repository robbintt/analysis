---
ver: rpa2
title: 'Substance or Style: What Does Your Image Embedding Know?'
arxiv_id: '2307.05610'
source_url: https://arxiv.org/abs/2307.05610
tags:
- image
- transformation
- transformations
- embeddings
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies what information is retained in popular image
  embeddings by training probes to predict various image transformations. The authors
  apply 30 transformations to ImageNet images and train small networks to classify
  them using frozen embeddings.
---

# Substance or Style: What Does Your Image Embedding Know?

## Quick Facts
- **arXiv ID:** 2307.05610
- **Source URL:** https://arxiv.org/abs/2307.05610
- **Reference count:** 40
- **Primary result:** Image-text models (CLIP, ALIGN) encode transformation information more generally than masking-based models (MAE, CAN), especially for style transfer.

## Executive Summary
This paper investigates what information is retained in popular image embeddings by systematically probing them with transformation prediction tasks. The authors apply 30 transformations to ImageNet images and train small networks to classify them using frozen embeddings. They evaluate both fine-grained transformation prediction (31 classes) and a generalization task (10 classes, with held-out test transformations). The study reveals that image-text models like CLIP and ALIGN encode transformation information more generally than masking-based models, particularly for style transfer. Interestingly, post-processing embeddings with a 2-layer MLP can extract more transformation information than a linear probe without harming semantic accuracy, and sensitivity to transformations does not imply poor robustness to domain shifts.

## Method Summary
The study applies 30 synthetic image transformations (plus Identity) to ImageNet-1k images, generating 31 fine-grained classes. For each transformation, the authors compute embeddings using six models: CAN, MAE, SimCLR, CLIP, ALIGN, and a supervised baseline. They train small probe networks (linear or 2-layer MLP with width 2048) on frozen embeddings to classify transformations. A generalization task groups similar transformations into 10 coarse categories, with some held out for testing. The probes are trained with ADAM optimizer, batch size 1024, and dropout 0.2. Multi-task models predict both semantic labels and transformations simultaneously, sharing a hidden layer.

## Key Results
- Image-text models (CLIP, ALIGN) encode transformation information more generally than masking-based models (MAE, CAN), especially for style transfer.
- Post-processing embeddings with a 2-layer MLP extracts more transformation information than a linear probe without harming semantic accuracy.
- Sensitivity to transformations does not imply poor robustness to domain shifts.
- SimCLR is more sensitive to transformations than MAE and CAN, despite being a contrastive learning method.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformation prediction tasks can effectively probe what information is retained in image embeddings by measuring whether small networks can detect applied image transformations.
- Mechanism: By applying transformations to images and training probes to classify them using only frozen embeddings, we can determine whether the embedding preserves enough information about the original image to detect the transformation.
- Core assumption: The transformation prediction task is a valid measure of the information content in embeddings, and the transformations are sufficiently distinct that trained humans could achieve near-perfect accuracy.
- Evidence anchors: [abstract], [section 2]
- Break condition: If the transformations are too subtle or too similar to each other, even humans might struggle to distinguish them, making the task unreliable for probing embeddings.

### Mechanism 2
- Claim: Image-text models (CLIP, ALIGN) encode transformation information in a more generalizable way than image-only embeddings, especially for style transfer.
- Mechanism: Image-text models are trained on paired image-text data, which may help them learn more abstract representations of visual concepts. This allows their embeddings to encode style transfer information in a way that generalizes better to unseen styles compared to image-only embeddings.
- Core assumption: Training with text improves the generalization ability of image embeddings for recognizing visual transformations.
- Evidence anchors: [abstract], [section 3.3]
- Break condition: If the text supervision in CLIP and ALIGN is not specifically focused on visual attributes, it might not lead to better generalization for transformation detection.

### Mechanism 3
- Claim: Post-processing embeddings with a 2-layer MLP can extract more transformation information than a linear probe without harming semantic accuracy.
- Mechanism: A 2-layer MLP can learn non-linear combinations of embedding features that are more effective at detecting transformations. By sharing a hidden layer for both semantic and transformation prediction in a multi-task setting, the network can effectively combine both types of information without interference.
- Core assumption: The transformation information is present in the embedding but requires non-linear processing to be fully extracted, and this processing can be done without degrading semantic performance.
- Evidence anchors: [abstract], [section 3.4]
- Break condition: If the transformation information is not present in the embedding at all, no amount of post-processing will be able to extract it.

## Foundational Learning

- **Concept:** Transformation prediction task
  - **Why needed here:** This task allows us to systematically evaluate what information is retained in image embeddings by measuring whether small networks can detect applied image transformations.
  - **Quick check question:** What is the main difference between the fine-grained and generalization versions of the transformation prediction task?

- **Concept:** Probe networks
  - **Why needed here:** Probe networks are small networks trained on top of frozen embeddings to predict properties of the underlying data, allowing us to measure what information is accessible in the embeddings.
  - **Quick check question:** Why is it important to freeze the embedding model when using probes?

- **Concept:** Multi-task learning
  - **Why needed here:** Multi-task learning allows us to train a network to predict both semantic and transformation labels simultaneously, sharing a hidden layer to combine both types of information.
  - **Quick check question:** How does multi-task learning with shared hidden layers help in extracting both semantic and transformation information from embeddings?

## Architecture Onboarding

- **Component map:** Image transformations -> Embedding models (CAN, MAE, SimCLR, CLIP, ALIGN, supervised) -> Probe networks (linear or MLP) -> Evaluation metrics (semantic accuracy, transformation accuracy, obfuscated semantic accuracy)
- **Critical path:** The critical path is to (1) apply transformations to images, (2) compute embeddings, (3) train probe networks on the frozen embeddings, and (4) evaluate the probe's performance on the transformation prediction task.
- **Design tradeoffs:** Using a larger MLP for post-processing can extract more transformation information but increases computational cost. Multi-task learning can combine semantic and transformation information but may require careful balancing of the loss functions.
- **Failure signatures:** If the probe networks perform poorly on the transformation prediction task, it may indicate that the embeddings are invariant to the transformations. If semantic accuracy drops significantly with multi-task learning, it may indicate interference between the two tasks.
- **First 3 experiments:**
  1. Evaluate the transformation prediction accuracy of a linear probe on top of each embedding model for the fine-grained task.
  2. Compare the transformation prediction accuracy of a 2-layer MLP vs. a linear probe for the generalization task.
  3. Train a multi-task model to predict both semantic and transformation labels and compare its performance to single-task models.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can we design foundation models that retain more non-semantic information (like transformations) without compromising semantic accuracy?
- **Basis in paper:** [inferred] The paper shows that it is possible to have good performance on both semantic and transformation prediction tasks, suggesting that retaining non-semantic information doesn't necessarily interfere with semantic performance.
- **Why unresolved:** The paper doesn't propose a specific method to achieve this balance. It only demonstrates that it's possible with post-processing.
- **What evidence would resolve it:** Designing and training a new foundation model that achieves high performance on both semantic and transformation prediction tasks, and comparing it to existing models.

### Open Question 2
- **Question:** Can transformation prediction be used as a pre-training objective to improve foundation model robustness to dataset shifts?
- **Basis in paper:** [inferred] The paper suggests that sensitivity to transformations does not imply poor robustness to domain shifts, and that transformation information does not interfere with semantic information.
- **Why unresolved:** The paper does not explore using transformation prediction as a pre-training objective. It only uses it as a probing task.
- **What evidence would resolve it:** Training a foundation model with a loss function that includes both semantic and transformation prediction objectives, and evaluating its performance on semantic tasks and robustness to dataset shifts.

### Open Question 3
- **Question:** How do different types of transformations (e.g., style transfer, noise, blurring) affect the embedding of various foundation models, and can this be used to detect adversarial attacks?
- **Basis in paper:** [explicit] The paper analyzes the performance of different models on various transformation categories and notes that certain models are more sensitive to specific types of transformations.
- **Why unresolved:** The paper doesn't explore the application of transformation prediction to adversarial attack detection. It only uses it as a probing task.
- **What evidence would resolve it:** Using transformation prediction models to detect images that have been manipulated with adversarial attacks, and comparing the performance of different foundation models in this task.

## Limitations
- The transformation set, while comprehensive, may not capture all forms of visual information.
- The focus on ImageNet limits generalizability to other domains.
- The probes, though small, may not fully represent the information capacity of embeddings.

## Confidence
- **High** for the claim that image-text models generalize better for style transfer
- **High** for the claim that transformation information can be extracted without semantic degradation in multi-task settings
- **Medium** for the claim that sensitivity to transformations does not imply poor robustness to domain shifts (limited empirical evidence)
- **Medium** for comparisons between embedding types (based on a single dataset and transformation set)

## Next Checks
1. **Generalization Across Domains:** Evaluate transformation prediction accuracy on out-of-distribution datasets (e.g., medical images, satellite imagery) to test robustness beyond ImageNet.
2. **Broader Transformation Set:** Extend the transformation set to include more naturalistic distortions (e.g., lighting variations, viewpoint changes) and re-evaluate embedding performance.
3. **Probe Complexity Analysis:** Systematically vary probe network depth/width and measure the trade-off between transformation accuracy and semantic performance to determine optimal probe architecture.