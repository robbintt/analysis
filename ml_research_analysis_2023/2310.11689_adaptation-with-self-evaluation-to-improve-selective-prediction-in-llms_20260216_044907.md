---
ver: rpa2
title: Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs
arxiv_id: '2310.11689'
source_url: https://arxiv.org/abs/2310.11689
tags:
- entropy
- aspire
- llms
- answer
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ASPIRE, a framework that improves selective\
  \ prediction in large language models (LLMs) by combining task-specific adaptation\
  \ with learned self-evaluation. The method uses soft prompt tuning to first adapt\
  \ the LLM to a target question-answering task, then trains an additional soft prompt\
  \ to assess whether the model\u2019s generated answers are correct."
---

# Adaptation with Self-Evaluation to Improve Selective Prediction in LLMs

## Quick Facts
- arXiv ID: 2310.11689
- Source URL: https://arxiv.org/abs/2310.11689
- Authors: [Not specified in source]
- Reference count: 33
- Primary result: ASPIRE improves selective prediction in LLMs by combining task-specific adaptation with learned self-evaluation, achieving AUACC improvements from 91.23% to 92.63% and AUROC improvements from 74.61% to 80.25% on CoQA

## Executive Summary
This paper introduces ASPIRE, a framework that improves selective prediction in large language models (LLMs) by combining task-specific adaptation with learned self-evaluation. The method uses soft prompt tuning to first adapt the LLM to a target question-answering task, then trains an additional soft prompt to assess whether the model's generated answers are correct. This approach produces continuous selection scores based on both answer likelihood and self-evaluation confidence, enabling better identification of incorrect outputs. Experiments on CoQA, TriviaQA, and SQuAD using OPT and GPT-2 models show that ASPIRE significantly outperforms existing baselines, achieving improvements such as increasing AUACC from 91.23% to 92.63% and AUROC from 74.61% to 80.25% on CoQA. The method is also computationally efficient, requiring only one forward pass at inference time.

## Method Summary
ASPIRE is a two-stage soft prompt tuning framework for improving selective prediction in LLMs. First, it adapts the LLM to the target task using cross-entropy loss to train a task-specific soft prompt θp. Second, it trains an additional soft prompt θs for self-evaluation by generating multiple answers per example using beam search, labeling them as correct or wrong based on Rouge-L similarity to ground truth, and training θs on binary classification. The selection score combines normalized likelihood and self-eval score with a weighted parameter α=0.25, enabling the model to identify and reject likely incorrect outputs while maintaining computational efficiency with only one forward pass at inference.

## Key Results
- AUACC improvements from 91.23% to 92.63% on CoQA, 85.91% to 87.09% on TriviaQA, and 91.35% to 92.40% on SQuAD
- AUROC improvements from 74.61% to 80.25% on CoQA, 69.83% to 75.48% on TriviaQA, and 73.93% to 78.55% on SQuAD
- Computational efficiency with only one forward pass required at inference time
- Consistent improvements across different model sizes (350M to 30B parameters) and architectures (OPT and GPT-2)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASPIRE improves selective prediction by learning to distinguish correct and incorrect outputs through self-evaluation training
- Mechanism: The framework trains two soft prompts - θp for task adaptation and θs for self-evaluation. θs learns to classify generated outputs as correct or wrong by training on pairs of high-likelihood outputs labeled by Rouge-L similarity to ground truth
- Core assumption: The model can learn meaningful self-evaluation from task-specific training data, and the learned distinction generalizes to unseen examples
- Evidence anchors:
  - [abstract] "Our framework is based on the idea of using parameter-efficient tuning to adapt the LLM to the specific task at hand while improving its ability to perform self-evaluation"
  - [section 4] "We first use the LLM with the learned θp to generate different answers for each example... We aim to generate output sequences that have high likelihood"
  - [corpus] Weak - no direct evidence found in neighboring papers about self-evaluation improving selective prediction specifically

### Mechanism 2
- Claim: Combining normalized likelihood with learned self-evaluation score creates a more discriminative selection metric than likelihood alone
- Mechanism: The selection score g(x) = (1-α) · log fnorm(ˆy∗ | x; θp) + α · log P(zc | x, ˆy∗) balances both the likelihood of the output and the model's self-assessed correctness probability
- Core assumption: The two components (likelihood and self-evaluation) capture complementary aspects of output quality, and their weighted combination optimizes selective prediction performance
- Evidence anchors:
  - [section 4] "We define a selection score that combines the likelihood of the generated answer with the learned self-eval score"
  - [section 6.3] "Setting α = 0.25 leads to the best performance since it combines the normalized likelihood and the learned self-eval score in a good way"
  - [corpus] Weak - no direct evidence found in neighboring papers about combining likelihood with self-evaluation for selective prediction

### Mechanism 3
- Claim: Soft prompt tuning provides an efficient way to adapt LLMs for selective prediction without full fine-tuning
- Mechanism: By freezing the LLM parameters and only training soft prompt embeddings θp and θs, ASPIRE achieves task adaptation and self-evaluation learning with significantly fewer parameters than full fine-tuning
- Core assumption: The soft prompts can effectively modulate the LLM's behavior for the target task without modifying the underlying model weights
- Evidence anchors:
  - [section 2] "Parameter-Efficient Fine-tuning (PEFT) approaches have been proposed... PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs"
  - [section 5] "In our work, we focus on Soft Prompt Tuning"
  - [corpus] Moderate - neighboring papers mention soft prompt tuning but not specifically for selective prediction applications

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT) methods like soft prompt tuning
  - Why needed here: Full fine-tuning large LLMs is computationally expensive and may lead to overfitting on limited task data; PEFT allows effective adaptation with fewer parameters
  - Quick check question: What is the main difference between soft prompt tuning and LoRA in terms of which parameters are modified?

- Concept: Uncertainty estimation and selective prediction framework
  - Why needed here: LLMs need a mechanism to identify when they are likely to produce incorrect outputs; selective prediction allows deferring uncertain predictions to human review
  - Quick check question: How does the selection score threshold τ determine which predictions are accepted versus rejected?

- Concept: Beam search and sampling-based decoding for generating diverse outputs
  - Why needed here: ASPIRE requires sampling multiple high-likelihood outputs to create training data for self-evaluation; different decoding strategies affect the diversity and quality of generated samples
  - Quick check question: Why might beam search produce better samples for ASPIRE training compared to multinomial sampling with very low temperature?

## Architecture Onboarding

- Component map: LLM core (frozen) → Soft prompt θp (task adaptation) → Output generation → Soft prompt θs (self-evaluation) → Selection score computation
- Critical path: Input → LLM encoding → θp modulation → Beam search decoding → Output generation → θs evaluation → Combined selection score
- Design tradeoffs: Soft prompts vs full fine-tuning (parameter efficiency vs flexibility), beam search vs sampling (diversity vs quality), α hyperparameter tuning (likelihood vs self-evaluation weighting)
- Failure signatures: Poor selective prediction performance (AUACC/AUROC too low), overfitting on training data (validation performance degrades), computational inefficiency (excessive inference time)
- First 3 experiments:
  1. Implement basic soft prompt tuning on CoQA with θp only and evaluate accuracy improvement
  2. Add self-evaluation training with θs using generated outputs and evaluate selection score quality
  3. Test different decoding strategies (beam search vs multinomial sampling) for answer sampling quality

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important research directions are implied by the limitations discussed.

## Limitations
- Domain generalization remains untested - ASPIRE only validated on three QA datasets, with unknown effectiveness on non-QA tasks or different domains
- Model scale dependency is unclear - experiments focused on models up to 2.7B parameters, with effectiveness on frontier models (GPT-4, Claude) unknown
- Computational overhead of training process - while inference is efficient, the two-stage training with k=10 answer generation per example may be computationally intensive

## Confidence
- **High Confidence**: The core mechanism of combining likelihood with learned self-evaluation for selective prediction is well-supported by experimental results showing consistent AUACC and AUROC improvements across all tested datasets and model sizes
- **Medium Confidence**: The computational efficiency claims are supported by the single-forward-pass inference, but the training overhead and practical deployment considerations need more thorough evaluation
- **Medium Confidence**: The effectiveness of soft prompt tuning for selective prediction is demonstrated, but the specific choice of soft prompt length and the comparison with alternative PEFT methods like LoRA could be more comprehensive

## Next Checks
1. **Cross-Domain Validation**: Test ASPIRE on non-QA tasks such as code generation, mathematical reasoning, or sentiment analysis to evaluate the generalizability of the self-evaluation mechanism across different types of correctness criteria
2. **Large Model Evaluation**: Implement ASPIRE on frontier models (Llama-2, GPT-3.5, Claude) to assess whether the framework's effectiveness scales to models with fundamentally different pretraining approaches and architectural designs
3. **Alternative PEFT Comparison**: Conduct a systematic comparison between ASPIRE's soft prompt tuning approach and other parameter-efficient methods like LoRA, prefix tuning, or adapter layers to determine whether soft prompts are optimal for selective prediction tasks