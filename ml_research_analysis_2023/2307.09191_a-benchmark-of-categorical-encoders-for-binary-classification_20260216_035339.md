---
ver: rpa2
title: A benchmark of categorical encoders for binary classification
arxiv_id: '2307.09191'
source_url: https://arxiv.org/abs/2307.09191
tags:
- encoders
- tuning
- datasets
- categorical
- aggregation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the most comprehensive benchmark of categorical
  encoders to date, evaluating 32 encoder configurations across 50 binary classification
  datasets with 36 combinations of experimental factors. The study demonstrates that
  dataset selection, experimental factors, and aggregation strategies profoundly influence
  benchmark conclusions, aspects previously disregarded in encoder benchmarks.
---

# A benchmark of categorical encoders for binary classification

## Quick Facts
- **arXiv ID:** 2307.09191
- **Source URL:** https://arxiv.org/abs/2307.09191
- **Reference count:** 40
- **Primary result:** High sensitivity of encoder benchmark results to experimental factors and aggregation strategies, with limited replicability even with 25 datasets.

## Executive Summary
This paper presents the most comprehensive benchmark of categorical encoders to date, evaluating 32 encoder configurations across 50 binary classification datasets with 36 combinations of experimental factors. The study demonstrates that dataset selection, experimental factors, and aggregation strategies profoundly influence benchmark conclusions, aspects previously disregarded in encoder benchmarks. Key findings show high sensitivity of results to experimental factors like ML models and tuning strategies, with limited replicability even with 25 datasets. The study identifies One-Hot, Sum, Binary, and Weight of Evidence as the best-performing encoders for logistic regression, and recommends these as preferred choices in practical applications.

## Method Summary
The study evaluated 32 encoder configurations (including identifier, frequency-based, contrast, similarity, simple target, binning, smoothing, and data-constraining encoders) across 50 binary classification datasets from OpenML. Experiments used 5 ML models (DT, k-NN, LogReg, SVM, LGBM) with three tuning strategies (no tuning, model tuning, full pipeline tuning) and 5-fold stratified cross-validation. Quality metrics included ROC AUC, F1-score, and accuracy. Results were aggregated using multiple strategies including heuristics, Friedman-Nemenyi tests, and Kemeny-Young aggregation. Replicability was assessed by comparing consensus rankings from disjoint dataset subsets.

## Key Results
- Encoder rankings are highly sensitive to experimental factors including ML models, tuning strategies, quality metrics, and aggregation methods
- Replicability may not be ensured even for studies conducted on up to 25 datasets
- One-Hot, Sum, Binary, and Weight of Evidence encoders consistently outperform others for logistic regression
- Different aggregation strategies yield very different consensus rankings, especially for top-performing encoders

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Experimental factor sensitivity explains conflicting encoder benchmark results.
- **Mechanism:** Varying ML models, tuning strategies, quality metrics, and aggregation methods produces inconsistent encoder rankings, making cross-study comparisons unreliable.
- **Core assumption:** Encoder performance rankings are not stable across different experimental setups.
- **Evidence anchors:**
  - [abstract] "dataset selection, experimental factors, and aggregation strategies profoundly influence benchmark conclusions"
  - [section] "Our findings highlight the high sensitivity of results of studies comparing encoders to experimental factors"
  - [corpus] Weak evidence; corpus neighbors discuss categorical encoding but not experimental sensitivity explicitly.
- **Break condition:** If a benchmark consistently ranks encoders identically across diverse experimental conditions, the sensitivity claim would fail.

### Mechanism 2
- **Claim:** Replicability requires large and diverse dataset samples.
- **Mechanism:** Even with 25 datasets, consensus rankings vary significantly between disjoint dataset subsets, indicating that small or homogeneous dataset sets cannot guarantee stable results.
- **Core assumption:** Dataset diversity and sample size are critical for robust benchmarking.
- **Evidence anchors:**
  - [abstract] "replicability may not be ensured even for studies conducted on up to 25 datasets"
  - [section] "Even with 25 datasets, replicability is moderate"
  - [corpus] No direct evidence in corpus; the claim is based on the paper's empirical findings.
- **Break condition:** If benchmark results remain stable with fewer than 25 datasets, the replicability threshold would be lower than claimed.

### Mechanism 3
- **Claim:** Aggregation strategy choice critically affects encoder ranking outcomes.
- **Mechanism:** Different aggregation heuristics (e.g., mean rank vs. median rank) and statistical methods (e.g., Friedman-Nemenyi vs. Kemeny-Young) yield divergent consensus rankings, especially for top-performing encoders.
- **Core assumption:** Aggregation method is a major determinant of benchmark conclusions.
- **Evidence anchors:**
  - [abstract] "inconsistencies arise from the adoption of varying aggregation strategies"
  - [section] "While some aggregation strategies show strong similarities, different strategies yield very different consensus rankings"
  - [corpus] Weak evidence; corpus neighbors mention encoding but not aggregation strategy impacts.
- **Break condition:** If all aggregation strategies produce identical rankings, the aggregation impact claim would not hold.

## Foundational Learning

- **Concept:** Categorical data encoding transforms non-numeric attributes into numeric form.
  - **Why needed here:** Encoders are the core subject; understanding their role is essential before evaluating benchmarks.
  - **Quick check question:** What is the difference between supervised and unsupervised encoders?

- **Concept:** Experimental design factors (datasets, models, metrics, tuning, aggregation).
  - **Why needed here:** The paper shows that these factors strongly influence benchmark outcomes; engineers must control them.
  - **Quick check question:** How does the choice of quality metric affect encoder ranking?

- **Concept:** Replicability and generalizability in ML benchmarking.
  - **Why needed here:** The study highlights limited replicability; understanding these concepts is crucial for interpreting results.
  - **Quick check question:** Why is a benchmark with only 10 datasets less generalizable than one with 50?

## Architecture Onboarding

- **Component map:** Data ingestion → Preprocessing (imputation, scaling) → Encoder application → ML model training → Evaluation (metrics) → Aggregation (consensus ranking)
- **Critical path:** Dataset selection → Experimental factor setup → Cross-validation → Encoder evaluation → Consensus ranking → Sensitivity analysis
- **Design tradeoffs:**
  - Dataset size vs. runtime (full tuning limited to 30 smallest datasets)
  - Aggregation strategy choice vs. computational cost (Kemeny-Young is much slower)
  - Encoder diversity vs. implementation complexity
- **Failure signatures:**
  - Inconsistent encoder rankings across experimental factors → sensitivity issue
  - Low replicability between dataset subsets → insufficient dataset diversity
  - Aggregation strategy changes alter top encoder selection → aggregation sensitivity
- **First 3 experiments:**
  1. Run encoder benchmarks on a small dataset subset with no tuning; observe baseline sensitivity to ML models
  2. Introduce model tuning; compare rankings to no-tuning results; note changes
  3. Apply different aggregation strategies (mean rank vs. Friedman-Nemenyi) on the same results; observe impact on top encoder selection

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do encoder rankings change when using neural networks as ML models instead of tree-based or linear models?
- **Basis in paper:** [inferred] The paper explicitly states that neural networks were excluded from the study due to their "inferior performance on tabular data" and "absence of a recommended architecture."
- **Why unresolved:** The study only evaluated five ML models (DT, k-NN, LogReg, SVM, LGBM) and did not include neural networks. The impact of encoder choice on neural network performance remains unknown.
- **What evidence would resolve it:** Replicating the benchmark with a representative set of neural network architectures and comparing encoder rankings across all models.

### Open Question 2
- **Question:** How do different hyperparameter tuning strategies affect the replicability of encoder rankings?
- **Basis in paper:** [explicit] The paper mentions three tuning strategies (no tuning, model tuning, full tuning) and notes that tuning was the "greatest influencing factor" in failed evaluations.
- **Why unresolved:** While the study used multiple tuning strategies, it did not systematically explore the space of possible tuning approaches or their impact on replicability across different datasets and encoder families.
- **What evidence would resolve it:** Conducting experiments with a broader range of tuning strategies and analyzing their effect on the consistency of encoder rankings across multiple dataset splits.

### Open Question 3
- **Question:** Can we develop more robust aggregation strategies that are less sensitive to the choice of experimental factors and dataset selection?
- **Basis in paper:** [explicit] The paper demonstrates high sensitivity of results to experimental factors (ML models, quality metrics, tuning strategies) and aggregation strategies, with limited replicability even with 25 datasets.
- **Why unresolved:** Current aggregation strategies are shown to produce varying consensus rankings, suggesting a need for more stable methods that can provide reliable encoder recommendations across diverse experimental conditions.
- **What evidence would resolve it:** Developing and testing new aggregation methods that explicitly account for the variability introduced by different experimental factors and dataset characteristics, then validating their performance in maintaining consistent encoder rankings.

## Limitations

- The study focuses exclusively on binary classification tasks, limiting generalizability to regression or multi-class problems
- Neural networks were excluded from the study, leaving a gap in understanding encoder performance for modern ML architectures
- While 50 datasets were used, the replicability threshold of 25 datasets may vary depending on dataset characteristics and task types

## Confidence

- **High Confidence:** Experimental factor sensitivity and aggregation strategy choice are well-supported by empirical results
- **Medium Confidence:** Replicability requirement of 25 datasets is supported but may vary with dataset characteristics
- **Low Confidence:** Generalizability of top encoder recommendations to non-logistic regression models and other classification tasks

## Next Checks

1. **Dataset Characteristic Analysis:** Investigate how encoder rankings vary with specific dataset features (cardinality, sparsity, missing value patterns) to understand sensitivity mechanisms better
2. **Cross-Task Generalization:** Validate the top encoder findings (One-Hot, Sum, Binary, WoE) on multi-class classification and regression tasks to assess broader applicability
3. **Aggregation Strategy Robustness:** Test additional aggregation methods (e.g., Borda count, geometric mean rank) to determine if consensus rankings are stable across a wider range of strategies