---
ver: rpa2
title: Competition-Level Problems are Effective LLM Evaluators
arxiv_id: '2312.02143'
source_url: https://arxiv.org/abs/2312.02143
tags:
- problems
- reasoning
- arxiv
- code
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates the reasoning capabilities of large language
  models (LLMs) on competition-level programming problems from Codeforces, focusing
  on whether these models can solve unseen, complex problems without data contamination.
  GPT-4's zero-shot performance was analyzed across problems of varying difficulty
  levels and release times.
---

# Competition-Level Problems are Effective LLM Evaluators

## Quick Facts
- arXiv ID: 2312.02143
- Source URL: https://arxiv.org/abs/2312.02143
- Reference count: 25
- One-line primary result: Competition-level programming problems reveal LLMs' reasoning limitations and potential data contamination issues.

## Executive Summary
This paper evaluates large language models' reasoning capabilities using competition-level programming problems from Codeforces, focusing on their ability to solve unseen, complex problems without data contamination. The study reveals a sharp decline in GPT-4's performance on problems released after September 2021, suggesting potential data contamination and fundamental limitations in solving unseen complex problems. Various methods like fine-tuning, Chain-of-Thought prompting, and problem simplification were explored but showed no consistent improvement, especially for difficult problems.

## Method Summary
The study evaluates LLMs using competition-level programming problems from Codeforces, covering problems from February 2010 to November 2023. Zero-shot prompting with various prompts is used to generate C++ code, which is then evaluated using an online judge. The performance is measured using metrics such as ACC#G, pass@k, and ACC k#n. The study focuses on the decline in performance on problems released after September 2021 and explores methods like fine-tuning, Chain-of-Thought prompting, and problem simplification to improve performance.

## Key Results
- GPT-4's performance significantly declined on problems released after September 2021, indicating potential data contamination.
- Similar performance trends were observed in other code LLMs like CodeLlama and DeepSeek-Coder.
- Fine-tuning, Chain-of-Thought prompting, and problem simplification did not consistently improve performance on unseen competition-level problems.

## Why This Works (Mechanism)

### Mechanism 1
Competition-level problems serve as an effective evaluator for LLMs because they require deep algorithmic reasoning and are unlikely to overlap with training data. These problems are expert-crafted, unique, and subject to strict selection processes in competitions, reducing the likelihood of data contamination. They demand deep understanding and robust reasoning skills, which tests the genuine reasoning capabilities of LLMs rather than memorization.

### Mechanism 2
The observed performance decline on problems released after September 2021 indicates potential data contamination and limitations in solving unseen complex problems. GPT-4's performance sharply declines on problems released after its training data cutoff date, suggesting that the model may have seen similar problems during training. This decline is consistent across all difficulty levels and problem types, indicating a fundamental limitation in handling unseen problems.

### Mechanism 3
Fine-tuning, Chain-of-Thought prompting, and problem statement simplification do not consistently improve performance on unseen competition-level problems. These methods are explored to mitigate the challenges of solving unseen problems, but none of them consistently improve performance, especially for difficult problems. This indicates that the challenges are fundamental and not easily addressed by simple modifications.

## Foundational Learning

- **Concept: Competition-level programming**
  - Why needed here: Understanding the nature of competition-level programming problems is crucial for evaluating the reasoning capabilities of LLMs, as these problems require deep understanding and robust reasoning skills.
  - Quick check question: What are the key characteristics of competition-level programming problems that make them effective evaluators for LLMs?

- **Concept: Data contamination**
  - Why needed here: Recognizing the potential for data contamination is essential for interpreting the results of LLM evaluations, as it can lead to overestimating the model's reasoning capabilities.
  - Quick check question: How can data contamination affect the evaluation of LLMs, and what strategies can be used to minimize its impact?

- **Concept: Chain-of-Thought prompting**
  - Why needed here: Understanding Chain-of-Thought prompting is important for exploring methods to improve LLM performance on complex reasoning tasks, as it involves generating an explanation of the algorithm before coding.
  - Quick check question: How does Chain-of-Thought prompting work, and in what scenarios has it been shown to be effective?

## Architecture Onboarding

- **Component map**: Problem statement parsing -> Code generation -> Online judge evaluation -> Performance metrics
- **Critical path**: Parse problem statement → Generate code → Submit to online judge → Evaluate performance
- **Design tradeoffs**: Accuracy vs. efficiency: More complex parsing and code generation may improve accuracy but reduce efficiency. Generalization vs. memorization: The model should generalize to unseen problems rather than relying on memorization.
- **Failure signatures**: Low ACC#G or pass@ k: Indicates poor performance on the problems. High proportion of "Wrong answer on test 1" errors: Suggests difficulties in understanding the problem or generating a correct solution based on the given test case. No improvement after fine-tuning or prompting: Indicates fundamental limitations in solving unseen problems.
- **First 3 experiments**:
  1. Evaluate GPT-4's performance on problems released before and after September 2021 to assess the impact of data contamination.
  2. Compare the performance of different LLMs (e.g., GPT-4, CodeLlama, DeepSeek-Coder) on the same set of problems to identify common challenges.
  3. Explore the effectiveness of Chain-of-Thought prompting on simple and complex problems to determine its impact on performance.

## Open Questions the Paper Calls Out

### Open Question 1
How can we effectively design new approaches to enhance LLM generalization on complex reasoning problems, beyond simple fine-tuning and Chain-of-Thought prompting? The paper explores various methods (fine-tuning, Chain-of-Thought prompting, problem simplification) to improve LLM performance on unseen problems but finds none of them consistently mitigate the challenges, especially for difficult problems. While the paper identifies the limitations of current approaches, it does not propose or evaluate novel methods that could potentially address these issues.

### Open Question 2
To what extent does the decline in LLM performance on unseen problems stem from limitations in reasoning versus memorization of training data? The paper raises questions about whether the decline in performance is due to limitations in reasoning and generalization or over-reliance on pattern recognition and reproduction from training data. While the paper suggests both possibilities, it does not provide a definitive answer or quantitative analysis to distinguish between reasoning limitations and memorization effects.

### Open Question 3
How can we develop more equitable evaluation strategies for LLMs that minimize the effects of data contamination and evaluation hallucination? The paper introduces the concept of "evaluation hallucination," where LLMs perform well on previously seen problems but struggle with unseen ones, raising concerns about the fairness of current evaluation methods. The paper identifies the problem but does not propose concrete solutions for creating evaluation benchmarks that accurately assess reasoning abilities without the risk of data contamination.

## Limitations

- The data contamination analysis relies heavily on temporal correlation, but alternative explanations such as evolving problem styles or difficulty trends are not thoroughly ruled out.
- The evaluation pipeline's reliability depends on the online judge's correctness and consistency, which is not independently verified.
- The study does not provide direct evidence that the problems released after September 2021 were actually present in training data, making the contamination hypothesis circumstantial.

## Confidence

- **High Confidence**: The observation that GPT-4 performance declines significantly on problems released after September 2021 is well-supported by empirical data and consistent across difficulty levels.
- **Medium Confidence**: The hypothesis that this decline indicates data contamination is plausible but not definitively proven, as other factors could contribute to the performance drop.
- **Medium Confidence**: The finding that fine-tuning, Chain-of-Thought prompting, and problem simplification do not improve performance is supported by experimental results, though the methods tested may not be exhaustive.

## Next Checks

1. **Cross-judge validation**: Re-run the evaluation using multiple online judges to verify that performance patterns are consistent and not judge-dependent.
2. **Contemporaneous control problems**: Create or identify problems from 2021-2022 that were explicitly excluded from training data (verified through pre-training corpus analysis) and compare performance on these problems versus randomly selected post-2021 problems.
3. **Alternative contamination detection**: Analyze the actual generated solutions for post-2021 problems to identify whether they contain patterns or code structures that strongly suggest memorization versus reasoning.