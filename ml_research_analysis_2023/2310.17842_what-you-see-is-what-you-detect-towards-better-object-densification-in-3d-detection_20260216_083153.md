---
ver: rpa2
title: 'What You See Is What You Detect: Towards better Object Densification in 3D
  detection'
arxiv_id: '2310.17842'
source_url: https://arxiv.org/abs/2310.17842
tags:
- detection
- points
- depth
- completion
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WYSIWYD (What You See Is What You Detect),
  a novel method for 3D object detection that addresses the limitations of full-shape
  completion approaches in lidar-based systems. The authors propose a visible part
  completion method that focuses on reconstructing only the visible portions of objects,
  rather than their entire shapes.
---

# What You See Is What You Detect: Towards better Object Densification in 3D detection

## Quick Facts
- **arXiv ID**: 2310.17842
- **Source URL**: https://arxiv.org/abs/2310.17842
- **Reference count**: 40
- **Key outcome**: WYSIWYD reduces predicted points by 88.7% while improving 3D detection accuracy on KITTI and nuScenes datasets

## Executive Summary
WYSIWYD introduces a novel approach to 3D object detection that addresses limitations of full-shape completion methods. Instead of reconstructing entire object shapes, it focuses on visible part completion, achieving significant point reduction (88.7%) while maintaining or improving detection accuracy. The method combines an Intra-Frustum Segmentation Transformer (IFST) for filtering noise points within 2D masks with a Mesh Depth Completion Network (MDCNet) for densifying point clouds through mesh deformation. This two-stage pipeline achieves state-of-the-art results, particularly excelling at detecting distant objects and pedestrians.

## Method Summary
The WYSIWYD method takes sparse lidar point clouds and 2D instance segmentation masks as inputs. It first processes the data through IFST, which uses density-adaptive splitting and 2D location guidance to identify foreground points within each frustum. The filtered points then pass through MDCNet, which densifies the visible object parts using a mesh deformation approach with hierarchical transformers. This produces a dense point cloud representation that improves downstream 3D detector performance without the artifacts associated with traditional full-shape completion methods.

## Key Results
- Achieves up to 12.2% improvement in baseline detector performance
- Reduces predicted points by 88.7% compared to full-shape completion methods
- Demonstrates significant gains in pedestrian detection, particularly for distant objects
- Maintains real-time performance with 145ms inference time
- Outperforms previous completion-based approaches across all evaluated categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visible part completion outperforms full-shape completion for 3D detection, especially for distant and small objects like pedestrians.
- Mechanism: By focusing only on reconstructing visible portions rather than entire shapes, the method reduces predicted points by 88.7% while maintaining or improving detection accuracy. Boundary depth estimation errors from full-frame depth completion leak into background regions, negatively impacting detection accuracy.
- Core assumption: The visible part of an object contains sufficient information for accurate 3D detection, and reconstructing occluded portions introduces more errors than benefits.
- Evidence anchors: [abstract] "the widely-used full-shape completion approach actually lead to a higher error-upper bound especially for far away objects and small objects like pedestrians"
- Break condition: If visible part completion fails to provide sufficient geometric information for accurate 3D bounding box estimation.

### Mechanism 2
- Claim: IFST effectively filters noise points within 2D instance segmentation masks.
- Mechanism: Uses density-adaptive splitting, 2D location guidance, and perspective relationship cues to identify foreground points. Processes points in different sub-frustums using size of mask and 2D relative location as priors.
- Core assumption: The combination of 2D mask information and 3D point cloud geometry provides sufficient signal to distinguish foreground from background points within a frustum.
- Evidence anchors: [abstract] "an Intra-Frustum Segmentation Transformer (IFST) that identifies foreground points within 2D instance segmentation masks"
- Break condition: If 2D segmentation masks are inaccurate or if perspective relationship assumptions break down.

### Mechanism 3
- Claim: MDCNet provides better depth estimation through mesh deformation than traditional depth completion methods.
- Mechanism: Treats each pixel as a vertex in a deformable mesh and uses geometric position-based hierarchical transformers with local-to-global feature aggregation. Avoids boundary depth dispersion problems that plague convolutional depth completion methods.
- Core assumption: Mesh deformation provides better control over shape consistency and avoids smoothing artifacts inherent in convolutional depth completion networks.
- Evidence anchors: [abstract] "a Mesh Depth Completion Network (MDCNet) that densifies the point cloud through mesh deformation"
- Break condition: If the mesh representation cannot capture complex object geometries or if iterative refinement fails to converge.

## Foundational Learning

- Concept: 3D object detection from sparse lidar point clouds
  - Why needed here: The entire method is built on improving 3D detection from lidar data
  - Quick check question: Why does lidar sparsity particularly affect detection of distant objects and pedestrians?

- Concept: Depth completion and its limitations
  - Why needed here: The paper argues against traditional full-shape completion and proposes visible part completion as an alternative
  - Quick check question: What are the specific artifacts introduced by boundary depth dispersion in convolutional depth completion?

- Concept: Cross-modal fusion between lidar and camera data
  - Why needed here: The method uses 2D instance segmentation masks from cameras to guide 3D point cloud processing
  - Quick check question: How does the perspective relationship between 2D image space and 3D lidar space help in identifying foreground points?

## Architecture Onboarding

- Component map: Lidar → 2D mask projection → IFST filtering → MDCNet densification → 3D detector
- Critical path: Lidar → 2D mask projection → IFST filtering → MDCNet densification → 3D detector
- Design tradeoffs:
  - Visible part completion vs full-shape completion: 88.7% fewer predicted points but potential loss of occluded information
  - Mesh-based vs voxel-based depth completion: Better shape consistency but potentially higher computational cost
  - Real-time performance vs accuracy: 145ms inference time vs SOTA performance
- Failure signatures:
  - Poor 2D segmentation leading to incorrect frustum boundaries
  - IFST failing to distinguish foreground from background noise
  - MDCNet producing mesh artifacts or inaccurate depth estimates
  - Overall performance degradation compared to baseline detector
- First 3 experiments:
  1. Validate IFST performance on synthetic data with known ground truth foreground/background labels
  2. Test MDCNet mesh deformation quality on objects with varying occlusion levels
  3. Benchmark end-to-end performance improvement on KITTI validation set for pedestrian detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed WYSIWYD method perform on datasets other than KITTI and nuScenes, particularly in urban environments with different sensor configurations?
- Basis in paper: [explicit] The paper mentions testing on KITTI and nuScenes datasets but does not explore performance on other datasets or sensor configurations.
- Why unresolved: The paper focuses on two specific datasets, limiting the generalizability of the findings to other urban environments or sensor setups.
- What evidence would resolve it: Testing WYSIWYD on additional datasets with different sensor configurations and urban environments to assess its performance and robustness.

### Open Question 2
- Question: What are the limitations of the IFST in handling extremely sparse or noisy point clouds, and how can these limitations be addressed?
- Basis in paper: [inferred] The paper discusses the use of IFST for intra-frustum segmentation but does not provide detailed analysis of its performance under extreme sparsity or noise conditions.
- Why unresolved: The paper does not explore the robustness of IFST under challenging conditions, such as extremely sparse or noisy point clouds.
- What evidence would resolve it: Conducting experiments with varying levels of sparsity and noise to evaluate the robustness and limitations of IFST, and proposing methods to enhance its performance under these conditions.

### Open Question 3
- Question: How does the mesh deformation-based approach in MDCNet compare to other completion methods in terms of computational efficiency and accuracy in real-world scenarios?
- Basis in paper: [explicit] The paper highlights the mesh deformation-based approach in MDCNet but does not provide a comprehensive comparison with other completion methods in terms of computational efficiency and accuracy.
- Why unresolved: The paper focuses on the effectiveness of the mesh deformation approach but does not compare it with other completion methods in real-world scenarios.
- What evidence would resolve it: Conducting a detailed comparison of MDCNet with other completion methods in terms of computational efficiency and accuracy using real-world data and scenarios.

## Limitations

- The visible part completion approach may underperform for objects with significant occlusion where understanding the full shape is crucial for accurate detection
- The method's reliance on accurate 2D instance segmentation masks introduces a dependency that could limit performance when camera-LiDAR calibration is imperfect
- While 88.7% reduction in predicted points is claimed, the trade-off between point density reduction and detection accuracy needs further validation across diverse scenarios

## Confidence

- **High Confidence**: The core observation that full-shape completion leads to boundary depth dispersion errors is well-supported by the paper's analysis and experimental results
- **Medium Confidence**: The specific architectural choices (IFST and MDCNet) are described in detail but lack extensive ablation studies to isolate their individual contributions
- **Medium Confidence**: The claimed performance improvements (up to 12.2% AP gain) are demonstrated but could benefit from more extensive cross-dataset validation

## Next Checks

1. **Ablation Study**: Conduct systematic ablation experiments removing IFST or MDCNet components to quantify their individual contributions to overall performance gains
2. **Occlusion Robustness Test**: Evaluate the method's performance on objects with varying degrees of occlusion to determine the limits of visible part completion effectiveness
3. **Real-time Feasibility Verification**: Conduct comprehensive timing analysis on embedded hardware to validate the claimed 145ms inference time and identify potential bottlenecks in the pipeline