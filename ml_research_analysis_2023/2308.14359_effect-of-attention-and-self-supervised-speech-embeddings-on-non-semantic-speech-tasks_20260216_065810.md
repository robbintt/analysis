---
ver: rpa2
title: Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech
  Tasks
arxiv_id: '2308.14359'
source_url: https://arxiv.org/abs/2308.14359
tags:
- emotion
- speech
- architecture
- features
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the impact of self-supervised speech embeddings
  and attention mechanisms on non-semantic speech tasks, specifically emotion perception.
  The authors propose using foundation models like HuBERT and wav2vec2.0 to extract
  speech embeddings, which are then fed into a light-weight sequence model with self-attention
  to predict the share of people perceiving nine different emotions in a given speech
  segment.
---

# Effect of Attention and Self-Supervised Speech Embeddings on Non-Semantic Speech Tasks

## Quick Facts
- arXiv ID: 2308.14359
- Source URL: https://arxiv.org/abs/2308.14359
- Reference count: 25
- Primary result: HuBERT-Large embeddings with self-attention improve emotion share prediction by 4.6% over baseline

## Executive Summary
This paper investigates the impact of self-supervised speech embeddings and attention mechanisms on non-semantic speech tasks, specifically emotion perception. The authors propose using foundation models like HuBERT and wav2vec2.0 to extract speech embeddings, which are then fed into a light-weight sequence model with self-attention to predict the share of people perceiving nine different emotions in a given speech segment. The results show that HuBERT-Large embeddings combined with the attention-based model outperform the baseline by 4.6% in Spearman correlation. The study demonstrates the effectiveness of self-supervised speech embeddings and attention mechanisms for complex emotion perception tasks, particularly in multilingual and imbalanced datasets.

## Method Summary
The method involves extracting speech embeddings using pretrained models (wav2vec2.0 or HuBERT Base/Large), padding sequences to uniform length while preserving original sequence information, and feeding these embeddings into a regression model. The regression model consists of a CNN to reduce feature size, LSTM layers to capture temporal dependencies, and either a simple feed-forward network or a self-attention mechanism followed by a feed-forward network to predict emotion shares. The model is trained separately for each of the nine emotions using the Adam optimizer with early stopping.

## Key Results
- HuBERT-Large embeddings outperform wav2vec2.0 and HuBERT-Base for emotion share prediction
- Self-attention mechanism improves performance when combined with HuBERT-Large embeddings
- The proposed model achieves 4.6% improvement in Spearman correlation over the baseline
- Results demonstrate effectiveness of self-supervised speech embeddings and attention for multilingual, imbalanced emotion perception tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HuBERT-Large embeddings capture more abstract, non-semantic speech features than wav2vec2.0, leading to better emotion share prediction.
- Mechanism: HuBERT uses k-means clustering to discretize continuous speech representations, which forces the model to discover hidden units in the speech signal rather than focusing on word-level representations. This abstraction aligns better with the non-semantic nature of emotion perception tasks.
- Core assumption: The clustering-based training objective in HuBERT inherently captures more paralinguistic information relevant to emotion perception than the contrastive loss used in wav2vec2.0.
- Evidence anchors:
  - [abstract]: "We demonstrate that the training scheme of different foundation models dictates their effectiveness for tasks beyond speech recognition, especially for non-semantic speech tasks like emotion understanding."
  - [section 2.2]: "HuBERT on the other hand clusters the audio segments based on their latent feature embeddings using the K-Means algorithm and optimizes a cross-entropy loss. It chases the idea of discovering hidden units in a language rather than obtaining granular word-level representations as in the case of wav2vec2.0."
  - [corpus]: Weak evidence - no direct comparison of clustering vs. contrastive objectives in emotion tasks found.
- Break condition: If the emotion perception task requires more semantic or lexical information, the advantage of HuBERT's clustering-based approach may diminish.

### Mechanism 2
- Claim: Self-attention mechanisms improve emotion share prediction by allowing the model to focus on relevant temporal segments of the speech signal.
- Mechanism: The attention layer computes weighted combinations of LSTM hidden states, where the weights are determined by the similarity between a query vector (derived from the final LSTM state) and the values. This allows the model to dynamically emphasize or de-emphasize different parts of the speech signal based on their relevance to emotion perception.
- Core assumption: The temporal dependencies in speech relevant to emotion perception are not uniformly distributed, and attention can effectively identify and weight these important segments.
- Evidence anchors:
  - [abstract]: "Our results show that HuBERT-Large with a self-attention-based light-weight sequence model provides 4.6% improvement over the reported baseline."
  - [section 2.4]: "Architecture 2 is same as the Architecture 1 till the LSTM layer. In Architecture 2, after LSTM, we introduce the self-attention mechanism. Past works [17] demonstrate that self-attention improves performance in emotion detection tasks."
  - [corpus]: Weak evidence - no direct analysis of attention weights or their correlation with emotion-relevant speech segments found.
- Break condition: If the emotion perception task relies more on overall acoustic patterns rather than specific temporal segments, the benefit of attention may be reduced.

### Mechanism 3
- Claim: Zero-padding and masking of variable-length audio sequences allows the model to handle diverse speech inputs without losing temporal information.
- Mechanism: Instead of truncating or padding the raw audio, the model extracts features first and then pads these feature sequences to a uniform length. This preserves the full temporal resolution of the extracted features while allowing batch processing.
- Core assumption: The feature extraction process (from pretrained models) is robust to variable-length inputs and that padding at the feature level is more effective than at the audio level for emotion perception tasks.
- Evidence anchors:
  - [section 2.1]: "To support different audio lengths we use the technique of padding and masking in the following sequence models. Instead of masking the raw audio-files to the length of the longest audio segment, we conduct feature-padding as shown in Fig. 1.(c)."
  - [section 2.1]: "We store a dictionary of all the original sequence lengths as well, so that deeper in the architecture when operating on representations, we only consider the non-padded features."
  - [corpus]: No direct evidence found comparing feature-level vs. audio-level padding strategies.
- Break condition: If the feature extraction process is sensitive to sequence length or if the emotion perception task requires strict temporal alignment, this approach may introduce artifacts or lose important information.

## Foundational Learning

- Concept: Self-supervised learning in speech representation
  - Why needed here: Understanding how HuBERT and wav2vec2.0 are trained without labeled data is crucial for interpreting their effectiveness on non-semantic tasks like emotion perception.
  - Quick check question: What is the key difference between the training objectives of HuBERT and wav2vec2.0?

- Concept: Attention mechanisms in sequence modeling
  - Why needed here: The self-attention layer is a critical component of the proposed architecture, and understanding how it works is essential for interpreting the results and potential improvements.
  - Quick check question: How does the scaled dot-product attention in the proposed model differ from standard self-attention used in transformers?

- Concept: Spearman correlation as an evaluation metric
  - Why needed here: The paper uses Spearman correlation to measure the quality of emotion share predictions, which is different from more common metrics like accuracy or F1-score used in classification tasks.
  - Quick check question: Why might Spearman correlation be more appropriate than Pearson correlation for evaluating emotion share predictions?

## Architecture Onboarding

- Component map:
  1. Raw audio input → Pretrained speech model (HuBERT-Large) → 768/1024-dimensional embeddings
  2. Embeddings → 1D CNN (reduces feature size, summarizes time series) → LSTM layers (capture temporal dependencies)
  3. LSTM output → Attention mechanism (weighted combination of hidden states) → Feed-forward network (predicts emotion share)
  4. Separate regressor networks for each of the 9 emotions

- Critical path: HuBERT-Large embeddings → CNN → LSTM → Attention → FFNN

- Design tradeoffs:
  - Using pretrained models vs. training from scratch: Pretrained models provide rich, general-purpose speech representations but may not be optimized for emotion-specific features.
  - Attention vs. no attention: Attention adds parameters and complexity but can improve performance if the initial embeddings are good enough.
  - Separate regressors vs. shared architecture: Separate regressors allow each emotion to have its own specialized model but increase the total number of parameters.

- Failure signatures:
  - Poor performance across all emotions: Likely issue with feature extraction or fundamental architecture.
  - Good performance on some emotions, poor on others: Possible imbalance in the regression dataset or emotion-specific challenges in the feature representations.
  - Attention hurts performance: Initial embeddings may not be good enough, or the attention mechanism is not well-suited to the task.

- First 3 experiments:
  1. Compare HuBERT-Base vs. HuBERT-Large embeddings to understand the impact of model size.
  2. Test the model with and without the attention mechanism to quantify its contribution.
  3. Evaluate the model using different evaluation metrics (e.g., MSE, MAE) to ensure Spearman correlation is the most appropriate choice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do HuBERT-Large embeddings perform compared to other self-supervised speech embeddings (wav2vec2.0, HuBERT-Base) on non-semantic speech tasks beyond emotion perception, such as speaker identification or language identification?
- Basis in paper: [explicit] The paper shows that HuBERT-Large outperforms other embeddings for emotion perception, but only discusses this specific task.
- Why unresolved: The paper only evaluates performance on emotion perception tasks, not other non-semantic speech tasks.
- What evidence would resolve it: Experimental results comparing HuBERT-Large embeddings with other self-supervised speech embeddings on various non-semantic speech tasks like speaker identification, language identification, and disfluency detection.

### Open Question 2
- Question: How would incorporating meta-data (speaker characteristics, language identifiers) as additional features impact the performance of emotion perception models?
- Basis in paper: [explicit] The authors mention in the conclusion that they are interested in using meta-data like language identifiers and speaker characteristics as additional features in the future.
- Why unresolved: The paper does not experiment with or discuss the impact of using meta-data on model performance.
- What evidence would resolve it: Experimental results comparing model performance with and without the inclusion of meta-data as additional features.

### Open Question 3
- Question: How does the performance of emotion perception models change when using foundation models pretrained on multilingual data instead of English-only data?
- Basis in paper: [explicit] The authors note that the dataset contains multilingual speakers but the pretrained models are trained only on English, making the task more complex.
- Why unresolved: The paper does not experiment with or discuss the impact of using multilingual foundation models on performance.
- What evidence would resolve it: Experimental results comparing model performance using English-only vs. multilingual foundation models.

## Limitations

- The paper lacks detailed analysis of why HuBERT outperforms wav2vec2.0 for emotion perception tasks, particularly regarding the clustering vs. contrastive training objectives.
- Limited investigation into the attention mechanism's contribution, with no analysis of attention weights or their correlation with emotion-relevant speech segments.
- The study does not explore the impact of feature-level vs. audio-level padding strategies, which could affect the model's ability to capture temporal dependencies.

## Confidence

- High confidence: The overall methodology and experimental setup are well-defined, with clear implementation details for the regression architectures and training procedures.
- Medium confidence: The reported 4.6% improvement in Spearman correlation is promising but lacks extensive ablation studies to isolate the contributions of HuBERT embeddings and attention mechanisms.
- Low confidence: The paper does not provide sufficient evidence to fully explain the superiority of HuBERT over wav2vec2.0 for emotion perception tasks or the effectiveness of the attention mechanism.

## Next Checks

1. Conduct an ablation study to quantify the individual contributions of HuBERT embeddings and the attention mechanism to the overall performance improvement.
2. Perform an analysis of attention weights to determine their correlation with emotion-relevant speech segments and validate the attention mechanism's effectiveness.
3. Compare feature-level vs. audio-level padding strategies to assess their impact on the model's ability to capture temporal dependencies and emotion-relevant information.