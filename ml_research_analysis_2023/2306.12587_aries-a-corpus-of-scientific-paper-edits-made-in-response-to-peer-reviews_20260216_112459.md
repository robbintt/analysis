---
ver: rpa2
title: 'ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews'
arxiv_id: '2306.12587'
source_url: https://arxiv.org/abs/2306.12587
tags:
- edits
- comment
- edit
- comments
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the ARIES dataset, a corpus of review comments
  and corresponding paper edits made in response to peer reviews. It defines two tasks:
  comment-edit alignment and edit generation.'
---

# ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews

## Quick Facts
- **arXiv ID**: 2306.12587
- **Source URL**: https://arxiv.org/abs/2306.12587
- **Reference count**: 15
- **Key outcome**: ARIES dataset includes 196 human-labeled and 3.9K synthetic comment-edit pairs; GPT-4 achieves 27.0 micro-F1 on alignment task vs human 70.7 micro-F1

## Executive Summary
This paper introduces ARIES, a dataset of peer review comments paired with the corresponding edits made to scientific papers in response. The dataset is built using a high-precision synthetic labeling approach that matches quoted review comments to edits based on textual overlap. Two tasks are defined: comment-edit alignment and edit generation. Experiments show that while GPT-4 can generate coherent edits, it struggles with indirect comments and often lacks technical details. Human performance significantly exceeds the best model (GPT-4), indicating substantial room for improvement in this domain.

## Method Summary
The ARIES corpus is constructed by aligning paper versions, extracting edits, and matching review comments to edits using quoted text and high textual overlap. A high-precision silver training set is automatically created, supplemented by a smaller human-labeled gold set. Multiple models including BM25, DeBERTa, LinkBERT, SPECTER2, and GPT-4 are evaluated on the alignment task using micro- and macro-F1 scores. GPT-4 is also used for edit generation, with performance assessed on coherence and technical accuracy.

## Key Results
- GPT-4 achieves 27.0 micro-F1 on comment-edit alignment, compared to human performance of 70.7 micro-F1
- The synthetic labeling approach yields 92% precision but only 25% recall
- GPT-4 generates coherent edits but often lacks technical details and follows feedback wording rather than intent
- Models struggle especially with indirect comments and non-compliant edits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The synthetic labeling approach creates high-precision comment-edit alignments by leveraging author responses.
- Mechanism: The system identifies quoted review comments in author responses through small edit distance matching to contiguous review text spans. It then matches these quoted comments to edits using high textual overlap between the response text and the edits.
- Core assumption: Authors consistently quote reviewer comments directly in their responses, and the edits corresponding to a comment are highly similar to the response text discussing that comment.
- Evidence anchors:
  - [abstract] "We automatically create a high-precision silver training set"
  - [section] "we automatically identify the quoted review comments in author responses by searching for lines with a small edit distance to a contiguous span of review text"
  - [corpus] The corpus shows this approach results in 92% precision on the automatically matched data.
- Break condition: If authors stop quoting reviewer comments directly, or if their response text becomes substantially different from the actual edits made, the alignment precision will degrade.

### Mechanism 2
- Claim: GPT-4's strong performance on comment-edit alignment stems from its ability to understand semantic similarity beyond surface-level lexical matching.
- Mechanism: GPT-4 cross-encoders process comment-edit pairs as a whole, allowing the model to capture contextual and semantic relationships that simpler similarity measures miss. The model can understand that edits addressing the "spirit" of a comment may use different terminology than the comment itself.
- Core assumption: GPT-4 has been trained on diverse enough data to understand the nuanced relationships between feedback and edits in scientific writing.
- Evidence anchors:
  - [abstract] "GPT-4 generates edits that are coherent but often lack technical details"
  - [section] "Interestingly, it appears that giving GPT-4 a full chunk of the document at once (GPT-4 multi-edit) results in slightly worse performance than the pairwise approach"
  - [corpus] The corpus provides paired examples showing GPT-4's ability to handle indirect relationships between comments and edits.
- Break condition: If the relationship between comments and edits becomes too domain-specific or requires knowledge outside GPT-4's training distribution, its performance will degrade.

### Mechanism 3
- Claim: The task difficulty increases significantly when comments are indirect or edits are non-compliant.
- Mechanism: Direct comments explicitly state what should be done, making it easier for models to identify corresponding edits. Indirect comments require reasoning about unstated implications. Non-compliant edits require models to understand that the author is disagreeing with or not fully addressing the comment.
- Core assumption: Human readers naturally understand indirect language and non-compliant behavior, but this requires complex reasoning that current models struggle with.
- Evidence anchors:
  - [abstract] "especially when the relationship between the edit and the comment is indirect and requires reasoning to uncover"
  - [section] "GPT-4 struggles to match to non-compliant edits, whereas humans are unaffected"
  - [corpus] The corpus contains examples of both direct and indirect comments, allowing analysis of performance differences.
- Break condition: If comment language becomes more direct and edits become more compliant, this mechanism's impact will diminish.

## Foundational Learning

- Concept: Understanding scientific paper structure and editing conventions
  - Why needed here: The task requires understanding how scientific papers are organized and how authors typically revise them in response to feedback
  - Quick check question: Can you explain the difference between a minor grammatical edit and a major content edit in a scientific paper?

- Concept: Natural language inference and semantic similarity
  - Why needed here: Models must understand that comments and edits can be semantically related even when they don't share many words
  - Quick check question: Given a comment asking to "clarify the methodology" and an edit that adds "We used the Smith algorithm (Smith et al., 2020) with default parameters," can you explain their semantic relationship?

- Concept: Edit representation and diff formats
  - Why needed here: The task requires understanding how to represent and compare text changes between document versions
  - Quick check question: How would you represent an edit that adds a new paragraph after paragraph 5 using the [+ +] and [- -] bracket notation?

## Architecture Onboarding

- Component map: Data ingestion pipeline -> Comment extraction module -> Alignment system -> Generation module -> Evaluation framework
- Critical path: Comment extraction → Edit identification → Model prediction → Evaluation → Analysis
- Design tradeoffs: High-precision synthetic data vs. low recall; complex cross-encoders vs. efficient bi-encoders; manual annotation cost vs. automatic labeling quality
- Failure signatures: Models struggle with indirect comments and non-compliant edits; GPT-4 sometimes paraphrases comments instead of integrating edits naturally; parsing errors create spurious edits
- First 3 experiments:
  1. Test BM25 baseline on the comment-edit alignment task to establish a simple similarity baseline
  2. Evaluate DeBERTa cross-encoder on a subset of the data to measure neural model performance
  3. Compare GPT-4 pairwise vs. multi-edit approaches on the full dataset to determine optimal model configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the model's reasoning capabilities compare when aligning comments to edits that require understanding of indirect or implicit feedback versus direct feedback?
- Basis in paper: [explicit] The paper discusses that models struggle with indirect comments and that GPT-4 performs worse on comments that require understanding of linguistic and scientific norms.
- Why unresolved: The paper does not provide a detailed comparison of model performance on direct vs indirect comments.
- What evidence would resolve it: A detailed analysis comparing model performance on direct and indirect comments with specific examples and metrics.

### Open Question 2
- Question: What are the potential improvements that could be made to the synthetic labeling approach to increase recall without sacrificing precision?
- Basis in paper: [explicit] The paper mentions that the synthetic labeling approach has low recall but high precision.
- Why unresolved: The paper does not explore methods to improve the recall of the synthetic labeling approach.
- What evidence would resolve it: Experiments testing different thresholds or algorithms to increase recall while maintaining precision.

### Open Question 3
- Question: How does the performance of the comment-source alignment task compare to the comment-edit alignment task, and what factors contribute to any differences?
- Basis in paper: [explicit] The paper briefly mentions comment-source alignment and provides some initial results.
- Why unresolved: The paper does not provide a comprehensive comparison between the two tasks.
- What evidence would resolve it: A detailed analysis comparing the performance of models on both tasks with explanations for any observed differences.

## Limitations

- The ARIES dataset covers only 29 papers from OpenReview, representing a small and potentially biased sample
- The synthetic labeling approach achieves high precision (92%) but low recall (25%), missing many comment-edit relationships
- PDF parsing errors can create spurious edits that don't correspond to actual changes

## Confidence

**High Confidence**: The synthetic labeling approach achieves high precision (92%) in matching quoted review comments to corresponding edits through edit distance and textual overlap. The general task formulation of comment-edit alignment is well-defined and the dataset creation methodology is sound.

**Medium Confidence**: The relative performance rankings of models (BM25, DeBERTa, LinkBERT, SPECTER2, GPT-4) are reliable, but absolute performance numbers may vary with different datasets or evaluation protocols. The observation that GPT-4 struggles with indirect comments and non-compliant edits is supported but may depend on prompt engineering details.

**Low Confidence**: The specific claim that GPT-4's multi-edit format performs worse than pairwise approach is based on limited evidence and may not generalize. The assertion that indirect comments and non-compliant edits are inherently harder for models may reflect evaluation methodology rather than fundamental limitations.

## Next Checks

1. **Dataset Diversity Validation**: Analyze the distribution of paper domains, reviewer expertise levels, and submission venues in ARIES to quantify potential biases and test whether model performance varies significantly across different scientific communities.

2. **Prompt Engineering Impact Study**: Systematically vary GPT-4 prompts to test how different instructions affect edit generation quality, particularly for indirect comments and non-compliant scenarios, to determine if current performance gaps are task-specific or prompt-dependent.

3. **Cross-Dataset Generalization Test**: Evaluate the best-performing models from ARIES on a held-out set of papers from different submission systems or domains to assess whether the synthetic labeling approach and model architectures generalize beyond the initial corpus.