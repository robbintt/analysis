---
ver: rpa2
title: Relabeling Minimal Training Subset to Flip a Prediction
arxiv_id: '2305.12809'
source_url: https://arxiv.org/abs/2305.12809
tags:
- training
- points
- prediction
- test
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an algorithm to identify the smallest subset
  of training points that, if relabeled, would flip a model's prediction on a given
  test point. It uses influence functions to efficiently approximate the impact of
  relabeling subsets of training data.
---

# Relabeling Minimal Training Subset to Flip a Prediction

## Quick Facts
- arXiv ID: 2305.12809
- Source URL: https://arxiv.org/abs/2305.12809
- Authors: 
- Reference count: 40
- Key outcome: Identifies minimal training subset whose relabeling flips a test prediction, useful for contesting predictions and detecting noisy/biased data

## Executive Summary
This paper proposes an algorithm to find the smallest subset of training points that, if relabeled, would flip a model's prediction on a given test point. Using influence functions, the method efficiently approximates the impact of relabeling subsets without retraining. Experiments show the algorithm can often find very small subsets (under 2% of training data on average) that flip predictions, with subset size correlating with training noise and revealing biased determinations.

## Method Summary
The method trains a binary classifier (ℓ2-regularized logistic regression) and uses influence functions to estimate how relabeling each training point affects a specific test prediction. It iteratively accumulates the most influential points until the prediction crosses the classification threshold, identifying the minimal relabeling subset. The approach assumes convex loss and strong convexity for accurate influence function approximation.

## Key Results
- Finds minimal relabeling subsets averaging under 2% of training data size
- Subset size correlates with noise ratio in training data
- Can reveal training points responsible for biased determinations
- Works across multiple binary classification tasks including loan default, sentiment analysis, and hate speech detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relabeling a small subset of training data can flip a test prediction by altering the model's decision boundary.
- Mechanism: The algorithm uses influence functions to estimate how much each training point contributes to the model's prediction on a test point. By iteratively relabeling the most influential points until the prediction crosses the classification threshold, it identifies the minimal subset needed to flip the prediction.
- Core assumption: Influence functions provide an accurate approximation of the effect of relabeling training points on test predictions, even for non-convex models like logistic regression.
- Evidence anchors:
  - [abstract]: "We propose an efficient algorithm to identify and relabel such a subset via an extended influence function for binary classification models with convex loss."
  - [section]: "To identify the training points that have biggest effect on a particular prediction for test point xt, we modify Equation 2 as (Yang et al., 2023) to estimate the influence on prediction (IP) result by relabeling training subset S as: ∆tf := ∇wf ˆw(xt)⊺∆iw"
  - [corpus]: Weak - The corpus contains related work on influence functions and relabeling but lacks direct evidence for this specific mechanism.
- Break condition: If the model's loss landscape is too non-convex or the training set is too large, the influence function approximation may become inaccurate.

### Mechanism 2
- Claim: The size of the minimal relabeling subset (k = |St|) is inversely related to the noise ratio in the training data.
- Mechanism: When the training data contains mislabeled points, relabeling a smaller subset can flip predictions because those points are already contributing noise to the model. The algorithm exploits this by finding points that, when corrected, have a large impact on predictions.
- Core assumption: Mislabeled training points significantly affect model predictions, and correcting them has a larger influence than relabeling correctly labeled points.
- Evidence anchors:
  - [abstract]: "This mechanism can serve multiple purposes: (1) providing an approach to challenge a model prediction by altering training points; (2) evaluating model robustness with the cardinality of the subset (i.e., |St|); we show that |St| is highly related to the noise ratio in the training set..."
  - [section]: "If mislabeled points highly occur in a training set, |St| can be apparently small compared with the training set without mislabeled training points or with a low level of mislabeled points."
  - [corpus]: Weak - The corpus mentions data quality and relabeling but doesn't provide direct evidence for this inverse relationship.
- Break condition: If the noise is uniformly distributed or the model is highly robust to noise, this relationship may not hold.

### Mechanism 3
- Claim: The minimal relabeling subset can reveal training points responsible for biased determinations.
- Mechanism: If a group of training data has labeling bias (e.g., systematic discrimination), those points will have a disproportionate influence on predictions for certain test groups. The algorithm identifies these points by finding which relabeling would flip biased predictions.
- Core assumption: Biased labeling in training data leads to biased predictions, and the influence of biased points is detectable through this relabeling mechanism.
- Evidence anchors:
  - [abstract]: "Thirdly, in addition to contesting predictions caused by noise data, our mechanism can explain biased determinations caused by systematic labeling bias."
  - [section]: "We later show that if a group of training data has labeling bias, those would have an impact on prediction for specific test groups, St will highly overlap with the biased training data."
  - [corpus]: Weak - The corpus mentions bias and relabeling but lacks specific evidence for this explanation mechanism.
- Break condition: If the bias is subtle or the model has learned to compensate for it, this mechanism may not reveal the biased points effectively.

## Foundational Learning

- Concept: Influence functions and their application to understanding model predictions
  - Why needed here: The entire algorithm relies on influence functions to approximate the effect of relabeling training points without retraining the model.
  - Quick check question: Can you explain how influence functions measure the effect of removing or modifying a training point on a model's prediction?

- Concept: Convex optimization and strong convexity assumptions
  - Why needed here: The method assumes the loss function is strongly convex to ensure the existence of a unique minimizer and to derive the influence function approximation.
  - Quick check question: Why is strong convexity important for the derivation of influence functions in this context?

- Concept: Binary classification and decision thresholds
  - Why needed here: The algorithm operates in the context of binary classification and uses a threshold (e.g., 0.5) to determine when a prediction has flipped.
  - Quick check question: How does changing the classification threshold affect the size of the minimal relabeling subset?

## Architecture Onboarding

- Component map: Trained binary classifier -> Influence function calculation -> Iterative relabeling -> Prediction flip detection
- Critical path: 1) Calculate influence of relabeling each training point on test prediction 2) Sort training points by influence magnitude 3) Iteratively relabel points in order of influence until prediction flips
- Design tradeoffs: Accuracy vs. efficiency - influence functions provide efficient approximation but may be less accurate than exhaustive relabeling and retraining
- Failure signatures: Algorithm fails to find relabeling subset despite one existing; inaccurate influence estimates leading to suboptimal relabeling order; model predictions too confident to flip
- First 3 experiments: 1) Verify influence function approximation by comparing with actual relabeling/retraining on small dataset 2) Test relationship between k=|St| and noise ratio by introducing varying noise levels 3) Evaluate algorithm's ability to detect biased training points by introducing systematic bias

## Open Questions the Paper Calls Out
None explicitly stated in the provided materials.

## Limitations
- Accuracy of influence function approximations may degrade for non-convex models or large training sets
- Computational cost of Hessian inversion limits scalability to large datasets
- Method requires retraining to verify prediction flips, reducing practical efficiency

## Confidence
- **High Confidence**: Core mechanism of using influence functions for relabeling analysis is well-established for convex models
- **Medium Confidence**: Empirical results showing small relabeling subsets are convincing but may be dataset-specific
- **Low Confidence**: Claims about detecting biased determinations through overlapping relabeling subsets need more rigorous validation

## Next Checks
1. Compare influence function estimates against actual retraining results on a small, controlled dataset to quantify approximation error
2. Evaluate computational requirements and accuracy degradation as training set size increases, particularly for Hessian inversion
3. Systematically introduce different types of labeling bias to a dataset and measure the algorithm's ability to detect and isolate biased training points through the relabeling subset