---
ver: rpa2
title: Attending Generalizability in Course of Deep Fake Detection by Exploring Multi-task
  Learning
arxiv_id: '2308.13503'
source_url: https://arxiv.org/abs/2308.13503
tags:
- binary
- detection
- contrastive
- learning
- deep
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting deepfake videos and
  generalizing to unseen manipulation techniques. The authors explore multi-task learning
  (MTL) approaches combining binary classification (fake vs real) with multi-class
  manipulation type classification.
---

# Attending Generalizability in Course of Deep Fake Detection by Exploring Multi-task Learning

## Quick Facts
- arXiv ID: 2308.13503
- Source URL: https://arxiv.org/abs/2308.13503
- Reference count: 40
- Primary result: Multi-task learning with binary classification plus manipulation type classification improves cross-manipulation generalization in deepfake detection

## Executive Summary
This paper addresses deepfake video detection by exploring multi-task learning (MTL) approaches to improve generalization across unseen manipulation techniques. The authors propose using a S3D encoder pre-trained with CoCLR and evaluate six MTL configurations combining binary classification (fake vs real) with multi-class manipulation type classification. Experiments on FaceForensics++ show that MTL significantly improves generalization in cross-manipulation scenarios, with binary cross-entropy plus multi-class cross-entropy training achieving up to 85.16% accuracy. The study demonstrates that jointly learning manipulation detection and classification tasks enhances model generalizability, though combining contrastive and cross-entropy losses proves counterproductive.

## Method Summary
The method employs a S3D encoder pre-trained with CoCLR as a shared backbone for multi-task learning. Six MTL configurations are evaluated using both cross-entropy and contrastive losses, with binary classification and multi-class manipulation type classification as primary and secondary tasks. The model uses separate 3-layer MLP heads for each task, and contrastive learning is implemented using a momentum encoder and queue. The FaceForensics++ dataset is used for evaluation, with 1000 original videos manipulated by four techniques (DeepFake, Face2Face, FaceSwap, NeuralTextures), totaling 5000 videos. The model is trained and evaluated on cross-manipulation scenarios to test generalization.

## Key Results
- MTL with binary classification plus manipulation type classification improves generalization across unseen manipulation techniques
- Binary cross-entropy plus multi-class cross-entropy training achieves the best results (up to 85.16% accuracy)
- Combining contrastive and cross-entropy losses is counterproductive, creating conflicting gradients that degrade performance

## Why This Works (Mechanism)

### Mechanism 1
Multi-task learning with manipulation classification as a secondary task improves generalization across unseen manipulation techniques. Joint training on binary classification (fake vs real) and multi-class manipulation type classification forces the model to learn both high-level forgery patterns and manipulation-specific artifacts. The shared encoder captures common forgery cues while task-specific classifiers extract manipulation-type-specific features, enabling better discrimination when encountering novel manipulation methods.

### Mechanism 2
Contrastive learning improves generalization on Deepfakes and FaceSwap but not on Face2Face and NeuralTextures. Label-informed MoCo with positive pairs from the same manipulation class and negative pairs from different classes forces the encoder to learn discriminative representations that separate manipulation types. This works well for manipulation techniques with distinct visual artifacts (Deepfakes, FaceSwap) but fails when techniques share similar characteristics (Face2Face, NeuralTextures).

### Mechanism 3
Combining contrastive and cross-entropy losses is counterproductive for generalization. Cross-entropy and contrastive losses optimize for different objectives—cross-entropy focuses on class boundary optimization while contrastive loss pushes representations apart. When both are applied simultaneously, they create conflicting gradients that push the model toward a suboptimal parameter space.

## Foundational Learning

- Concept: Multi-task learning architecture design
  - Why needed here: The paper explores how to structure MTL with shared encoder and separate task-specific heads to balance between common feature learning and task-specific discrimination
  - Quick check question: What determines whether to use shared layers versus separate branches in MTL architecture?

- Concept: Contrastive learning with label information
  - Why needed here: The paper adapts MoCo-style contrastive learning to use label information rather than augmentations, creating positive pairs from same-manipulation videos and negative pairs from different manipulations
  - Quick check question: How does label-informed contrastive learning differ from standard self-supervised contrastive approaches?

- Concept: Cross-manipulation generalization evaluation
  - Why needed here: The paper's core contribution is evaluating how well models generalize to unseen manipulation techniques, requiring careful experimental design where models train on some manipulations and test on others
  - Quick check question: Why is cross-manipulation testing more challenging than standard cross-validation within the same manipulation type?

## Architecture Onboarding

- Component map: Video input → S3D encoder → shared representation → binary classifier head → binary classification loss; Video input → S3D encoder → shared representation → multi-class classifier head → multi-class classification loss; Optional: Video input → momentum encoder → contrastive loss

- Critical path: Video input → S3D encoder → shared representation → task-specific heads → loss computation → gradient backpropagation through shared encoder

- Design tradeoffs:
  - Shared vs separate encoders: Shared encoder enables parameter efficiency and common feature learning but may limit task-specific optimization
  - Resolution vs temporal information: Using 128px resolution sacrifices spatial detail but retains temporal information critical for detecting temporal inconsistencies
  - Contrastive vs cross-entropy: Contrastive works better for distinct manipulation types while cross-entropy generalizes better across all types

- Failure signatures:
  - Poor performance on cross-manipulation testing indicates failure to learn generalizable features
  - Contrastive + multi-task combinations performing worse than single-task indicates conflicting gradient optimization
  - High false negatives on low-resolution videos suggest spatial detail is important for certain artifacts

- First 3 experiments:
  1. Implement binary CE baseline to establish performance floor
  2. Add multi-class CE as secondary task to test MTL benefits
  3. Implement label-informed MoCo contrastive learning to compare with CE approaches

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does multi-task learning improve generalizability across different types of deepfake manipulations, and if so, under what conditions?
- Basis in paper: [explicit] The authors demonstrate that multi-task learning (MTL) can improve generalizability, particularly in cross-manipulation scenarios, with the best results from binary cross-entropy plus multi-class cross-entropy training.
- Why unresolved: While the paper shows MTL's effectiveness, it doesn't fully explore the conditions under which MTL is most beneficial, such as the types of manipulation techniques or the nature of the dataset.
- What evidence would resolve it: Conducting experiments across a wider variety of manipulation techniques and datasets, and systematically varying the complexity and similarity of the manipulation tasks, would clarify when MTL is most effective.

### Open Question 2
- Question: How does the choice of loss function (supervised vs. contrastive) impact the effectiveness of multi-task learning in deepfake detection?
- Basis in paper: [explicit] The paper finds that Cross Entropy loss performs best in MTL, while Contrastive loss does not, suggesting that the choice of loss function significantly affects MTL outcomes.
- Why unresolved: The paper indicates a preference for Cross Entropy but doesn't deeply explore why Contrastive loss is less effective in MTL or whether there are scenarios where it might be beneficial.
- What evidence would resolve it: Further experiments testing different combinations of loss functions in MTL, possibly with different manipulation techniques or datasets, would help understand the impact of loss choice on MTL performance.

### Open Question 3
- Question: What are the optimal sub-tasks within a multi-task learning framework for enhancing the generalizability of deepfake detection models?
- Basis in paper: [explicit] The authors suggest that identifying the type of manipulation is a good sub-task when using Cross Entropy loss, but note that other sub-tasks like non-contrastive SSL approaches could be explored.
- Why unresolved: The paper only briefly mentions the potential for other sub-tasks and does not explore them in depth, leaving the question of which sub-tasks are most effective unanswered.
- What evidence would resolve it: Testing various sub-tasks, such as different forms of self-supervised learning or other related tasks, within the MTL framework could reveal which sub-tasks contribute most to generalizability in deepfake detection.

## Limitations
- The study focuses exclusively on facial manipulation in videos, limiting applicability to other deepfake domains like voice or full-body synthesis
- The S3D encoder's CoCLR pre-training details are not fully specified, making exact reproduction challenging
- The theoretical explanation for why contrastive and cross-entropy losses conflict remains speculative

## Confidence
- **High confidence**: MTL with binary + multi-class CE improves generalization compared to single-task baselines
- **Medium confidence**: Contrastive learning effectiveness varies by manipulation type (works for Deepfakes/FaceSwap, not Face2Face/NeuralTextures)
- **Medium confidence**: Combining contrastive and cross-entropy losses is counterproductive

## Next Checks
1. Test whether the CoCLR pre-training advantage persists when using standard ImageNet initialization or other self-supervised pre-training methods
2. Evaluate whether the MTL generalization benefits extend to out-of-distribution datasets beyond FaceForensics++
3. Conduct ablation studies on the shared encoder architecture to determine if task-specific encoders might perform better for certain manipulation types