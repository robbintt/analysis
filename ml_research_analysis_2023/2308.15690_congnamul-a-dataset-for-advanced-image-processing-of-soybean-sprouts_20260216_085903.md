---
ver: rpa2
title: 'CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts'
arxiv_id: '2308.15690'
source_url: https://arxiv.org/abs/2308.15690
tags:
- sprouts
- soybean
- dataset
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the 'CongNaMul' dataset, designed for advanced
  image processing tasks on soybean sprouts. The dataset supports image classification,
  semantic segmentation, image decomposition, and physical feature measurement, providing
  4 quality classes (normal, broken, spotted, broken and spotted), detailed segmentation
  masks for head, body, and tail, and physical measurements including head length,
  body length, body thickness, tail length, and weight.
---

# CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts

## Quick Facts
- **arXiv ID**: 2308.15690
- **Source URL**: https://arxiv.org/abs/2308.15690
- **Reference count**: 14
- **Primary result**: A dataset for soybean sprout image classification, segmentation, decomposition, and measurement with 4 quality classes, detailed masks, and physical measurements

## Executive Summary
This paper introduces the 'CongNaMul' dataset for advanced image processing tasks on soybean sprouts, supporting classification, semantic segmentation, image decomposition, and physical feature measurement. The dataset includes 2,000 images across four quality classes (normal, broken, spotted, broken and spotted), detailed segmentation masks for head, body, and tail, and physical measurements. Images were captured under standardized conditions with three background types, and masks were manually labeled using Labelme. An image decomposition dataset was generated by merging pairs of sprout images, with accompanying .npy files sorted by mask area to facilitate order-independent model training.

## Method Summary
The dataset was created by capturing soybean sprout images under standardized conditions with three background types (clear, green, white). For classification, 2,000 images were organized into four quality classes. Semantic segmentation masks were manually labeled using Labelme to distinguish head, body, and tail parts. Physical measurements (head length, body length, body thickness, tail length, weight) were collected. The image decomposition dataset was synthetically generated by merging pairs of single-sprout images, with .npy binary files sorted by mask area to enable order-independent training. Images were preprocessed with CLAHE for contrast equalization and cropped to 2,016x2,016px.

## Key Results
- Provides 2,000 images across 4 quality classes (normal, broken, spotted, broken and spotted) for classification
- Includes detailed segmentation masks for head, body, and tail parts of individual sprouts
- Generates synthetic image decomposition dataset by merging paired sprout images
- Contains physical measurements (head length, body length, body thickness, tail length, weight) for measurement tasks
- Dataset size remains a limitation requiring augmentation for some tasks

## Why This Works (Mechanism)

### Mechanism 1
The image decomposition dataset contains two separate sprout images and their combined form, allowing a model to learn how to decompose a complex image back into individual sprouts. The .npy binary files are sorted by mask area, ensuring order-independent model training and avoiding permutation ambiguity. If real-world overlapping patterns differ significantly from synthetic merges, the model may fail to generalize to actual factory conditions.

### Mechanism 2
By categorizing sprouts into quality classes (normal, broken, spotted, broken and spotted), the dataset enables training of a classifier that can automatically identify defects such as broken bodies or spotted heads, which are critical for industrial quality grading. If the dataset size (500 images per class) is insufficient for robust training, especially given the subtle differences between some classes, model performance may suffer.

### Mechanism 3
By providing pixel-level masks for each sprout part, the dataset allows models to segment sprouts into anatomical regions, enabling automated measurement of physical features like head length, body length, and body thickness. If labeling errors or inconsistencies in mask boundaries degrade model performance on measurement tasks, the segmentation accuracy will be compromised.

## Foundational Learning

- **Concept**: Image classification fundamentals (CNN architectures, loss functions)
  - Why needed here: To build and train models that classify sprout quality into four classes
  - Quick check question: What loss function is most appropriate for a four-class classification problem with balanced data?

- **Concept**: Semantic segmentation techniques (U-Net, mask generation, label encoding)
  - Why needed here: To train models that can segment sprout images into head, body, tail, and background
  - Quick check question: How do you convert polygon-based labels (from Labelme) into a format suitable for training a segmentation model?

- **Concept**: Image decomposition and inpainting (autoencoders, generative models)
  - Why needed here: To train models that can decompose a merged sprout image back into its original single-sprout components
  - Quick check question: How does sorting images by mask area help avoid permutation ambiguity in decomposition tasks?

## Architecture Onboarding

- **Component map**: Data ingestion pipeline → Image preprocessing (CLAHE, resizing) → Model training (classification/segmentation/decomposition) → Evaluation on held-out set
- **Critical path**: 1. Load and preprocess images (CLAHE, resize to model input size) 2. Load corresponding masks (segmentation) or merged images (decomposition) 3. Train model with appropriate loss function 4. Evaluate on test set; iterate with augmentation if needed
- **Design tradeoffs**: Using .npy binary for decomposition enables order-independent training but increases storage overhead; cropping images to 2,016x2,016px reduces VRAM usage but may remove useful context; providing three background types improves robustness but complicates training pipeline
- **Failure signatures**: High training accuracy but low test accuracy → overfitting; consider more augmentation; decomposition model outputs incorrect sprout order → check mask area sorting logic; segmentation model fails on complex multi-sprout images → insufficient training data diversity
- **First 3 experiments**: 1. Train a simple CNN classifier on the classification dataset; evaluate per-class accuracy 2. Train a U-Net model on the semantic segmentation dataset; visualize predicted masks on held-out images 3. Train a decomposition model using the .npy binary data; check if the model can correctly separate merged images into individual sprouts

## Open Questions the Paper Calls Out

### Open Question 1
What is the minimum dataset size required to achieve reliable classification accuracy for soybean sprout quality control in real industrial settings? The paper acknowledges that dataset size is a limitation and requires augmentation for some tasks, particularly noting that the single sample segmentation data is small in size. Experimental results showing classification accuracy curves as a function of dataset size, including analysis of whether the current dataset size (2,000 images) is sufficient for reliable industrial deployment, would resolve this.

### Open Question 2
How well do models trained on this dataset generalize to soybean sprouts from different sources (different brands, growing conditions, harvest times)? The dataset includes sprouts from three brands (Pulmuone, CJ, Haetrac) and estimates harvest times within approximately 48 hours, but doesn't test cross-source generalization. Experimental results comparing model performance when trained on one source and tested on another, or when trained on mixed sources versus single sources, would resolve this.

### Open Question 3
What is the impact of background complexity on model performance for both classification and segmentation tasks? While the dataset includes three background types (clear, green checkered, white checkered), the paper doesn't report any experiments comparing model performance across different background types or analyzing the effect of background complexity. Experimental results showing classification and segmentation accuracy for each background type separately, and analysis of how background complexity affects model robustness and generalization, would resolve this.

## Limitations

- Dataset size (2,000 classification images, 615+1,060 segmentation masks) may limit model generalization
- Synthetic nature of image decomposition dataset may not accurately represent real-world overlapping patterns
- Manual mask labeling introduces potential subjectivity and inconsistency in boundary definitions

## Confidence

- **High Confidence**: The dataset's structural organization and the provision of four quality classes for classification are well-documented and verifiable
- **Medium Confidence**: The effectiveness of the image decomposition dataset for real-world applications is plausible but unverified; synthetic merges may not capture all natural overlapping patterns
- **Low Confidence**: The manual labeling process's consistency and accuracy are not quantified, which could affect segmentation and measurement tasks

## Next Checks

1. **Evaluate Synthetic Decomposition Realism**: Compare model performance on synthetic merged images versus real-world overlapping sprouts from factory settings to assess generalizability
2. **Quantify Manual Labeling Consistency**: Have multiple annotators label a subset of segmentation masks and compute inter-rater agreement (e.g., Dice coefficient) to assess labeling reliability
3. **Test Augmentation Sufficiency**: Train classification models with and without augmentation on a held-out subset to determine if the current dataset size is adequate or if augmentation is necessary for robust performance