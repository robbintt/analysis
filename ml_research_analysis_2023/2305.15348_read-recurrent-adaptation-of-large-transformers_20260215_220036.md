---
ver: rpa2
title: 'READ: Recurrent Adaptation of Large Transformers'
arxiv_id: '2305.15348'
source_url: https://arxiv.org/abs/2305.15348
tags:
- read
- training
- backbone
- memory
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REcurrent ADaption (READ) addresses the high energy and memory
  costs of fine-tuning large transformers by inserting a small RNN alongside the frozen
  backbone model, enabling training without backpropagation through the large model.
  READ achieves 56% lower training memory and 84% less GPU energy consumption compared
  to full fine-tuning on GLUE tasks, while maintaining competitive accuracy.
---

# READ: Recurrent Adaptation of Large Transformers

## Quick Facts
- arXiv ID: 2305.15348
- Source URL: https://arxiv.org/abs/2305.15348
- Reference count: 40
- Key outcome: READ achieves 56% lower training memory and 84% less GPU energy consumption compared to full fine-tuning on GLUE tasks

## Executive Summary
READ addresses the high energy and memory costs of fine-tuning large transformers by inserting a small RNN alongside the frozen backbone model, enabling training without backpropagation through the large model. This approach achieves significant efficiency gains while maintaining competitive accuracy on GLUE benchmark tasks. The number of trainable parameters grows sub-linearly with backbone size, making READ highly scalable for very large models.

## Method Summary
READ inserts a small RNN network alongside a frozen backbone transformer model, allowing training to proceed without backpropagating through the large backbone. During the forward pass, the backbone produces intermediate hidden states that are processed by a joiner network and fed into the RNN, which learns corrections to the backbone outputs. Only the RNN parameters and task-specific heads are updated during training, dramatically reducing memory and energy requirements while maintaining competitive accuracy.

## Key Results
- 56% lower training memory compared to full fine-tuning on GLUE tasks
- 84% less GPU energy consumption during training
- Competitive accuracy with other parameter-efficient methods while using sub-linearly growing trainable parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: READ reduces GPU energy consumption by avoiding backpropagation through the large backbone model.
- Mechanism: READ inserts a small RNN alongside the frozen backbone model. During training, only the RNN and feed-forward networks are updated, eliminating the need to run the backward pass through the large backbone transformer layers.
- Core assumption: The frozen backbone model produces sufficiently informative intermediate hidden states that can be leveraged by the RNN without requiring gradient updates to the backbone.
- Evidence anchors: [abstract] "Specifically, READ inserts a small RNN network alongside the backbone model so that the model does not have to back-propagate through the large backbone network."

### Mechanism 2
- Claim: READ achieves competitive accuracy by learning corrections to backbone hidden states rather than modifying the backbone weights.
- Mechanism: READ models the difference between adapted and original backbone outputs (corrections) using an RNN. This correction learning approach captures task-specific transformations without altering the pre-trained weights.
- Core assumption: The corrections to hidden states can be effectively modeled by an RNN without requiring full fine-tuning of the backbone.
- Evidence anchors: [section] "We define ϕ′i − ϕi to be a correction to ϕi, and denote it by δϕi."

### Mechanism 3
- Claim: READ scales efficiently with backbone size because the number of trainable parameters grows sub-linearly.
- Mechanism: The RNN in READ processes hidden states recurrently, so its parameter count doesn't increase with the number of backbone layers. Only the number of hidden states to process grows with backbone size.
- Core assumption: The recurrent architecture allows information flow between layers without requiring additional parameters per layer.
- Evidence anchors: [abstract] "Additionally, the model size of READ does not grow with the backbone model size, making it a highly scalable solution for fine-tuning large Transformers."

## Foundational Learning

- Concept: Backpropagation through deep neural networks
  - Why needed here: Understanding why avoiding backpropagation through the backbone reduces memory and energy costs is central to READ's design.
  - Quick check question: What memory and computational overhead is eliminated when gradients don't flow through the backbone transformer?

- Concept: Parameter-efficient transfer learning (PETL) methods
  - Why needed here: READ is compared against various PETL methods (Adapters, LoRA, BitFit, Prompt-tuning), so understanding their mechanisms helps contextualize READ's contributions.
  - Quick check question: How do Adapter-based methods differ from READ in terms of where they insert trainable parameters and how they affect inference latency?

- Concept: Recurrent neural networks (RNNs) and their training
  - Why needed here: READ uses RNNs to process hidden states, so understanding RNN architecture, forward/backward passes, and common variants (LSTM, GRU) is essential.
  - Quick check question: Why might an RNN be preferred over a transformer for the side network in READ, given the goal of reducing memory and energy?

## Architecture Onboarding

- Component map:
  - Frozen backbone transformer (T5) → produces intermediate hidden states
  - Joiner network → processes hidden states and produces RNN inputs
  - RNN (GRU/LSTM) → iteratively processes layer outputs to learn corrections
  - Addition layer → combines RNN outputs with backbone outputs
  - Task-specific head → produces final predictions

- Critical path:
  1. Forward pass through frozen backbone
  2. Cache intermediate hidden states
  3. Process states through Joiner → RNN → Addition
  4. Compute loss and update only READ parameters

- Design tradeoffs:
  - Memory vs. accuracy: Freezing the backbone saves memory but may limit adaptation capacity
  - RNN choice: GRU/LSTM vs. simpler RNN affects training stability and performance
  - Joiner complexity: More sophisticated Joiners could improve accuracy but increase computation

- Failure signatures:
  - Training instability: Could indicate poor initialization or inappropriate learning rates for the RNN
  - Poor accuracy: May suggest the backbone's frozen state is insufficient for the target task
  - High memory usage: Unexpected if backpropagation through backbone is properly avoided

- First 3 experiments:
  1. Verify READ avoids backpropagation through backbone by checking memory usage with and without gradient checkpointing
  2. Compare accuracy of READ with different RNN types (GRU, LSTM, simple RNN) on a small GLUE task
  3. Measure energy consumption difference between READ and full fine-tuning on a single task

## Open Questions the Paper Calls Out
- How does READ perform on low-data regimes compared to other parameter-efficient methods?
- What is the theoretical limit of READ's scalability with respect to backbone model size?
- How do different RNN architectures (GRU, LSTM, basic RNN) affect READ's performance and efficiency trade-offs?

## Limitations
- The correction learning mechanism's effectiveness for tasks requiring significant backbone modifications isn't fully explored
- The Joiner network architecture details are underspecified, making reproducibility challenging
- Energy efficiency claims rely on linear power models that may not accurately capture actual GPU power draw across different hardware

## Confidence
- **High Confidence**: Memory reduction claims (56% lower training memory) are supported by direct measurements
- **Medium Confidence**: Accuracy comparisons show competitive performance but are benchmark-specific to GLUE tasks
- **Low Confidence**: Correction learning mechanism's effectiveness for tasks requiring significant backbone modifications isn't fully explored

## Next Checks
1. Test READ's energy consumption measurements across different GPU models (RTX 3090, A100) to verify linear power model assumptions
2. Evaluate READ's performance and memory efficiency when applied to much larger transformers (T5-11B, GPT-3 sized models)
3. Apply READ to non-GLUE NLP tasks including structured prediction, generative tasks, and vision transformer fine-tuning