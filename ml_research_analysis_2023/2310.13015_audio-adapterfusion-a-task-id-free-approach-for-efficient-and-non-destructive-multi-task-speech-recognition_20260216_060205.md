---
ver: rpa2
title: 'Audio-AdapterFusion: A Task-ID-free Approach for Efficient and Non-Destructive
  Multi-task Speech Recognition'
arxiv_id: '2310.13015'
source_url: https://arxiv.org/abs/2310.13015
tags:
- task
- adapter
- tasks
- methods
- adapters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Audio-AdapterFusion (A-AF), a task-ID-free
  method for combining single-task adapters to efficiently and non-destructively solve
  multiple ASR tasks. A-AF overcomes catastrophic interference by training single-task
  adapters separately (knowledge extraction) and then combining them without task
  IDs (knowledge composition).
---

# Audio-AdapterFusion: A Task-ID-free Approach for Efficient and Non-Destructive Multi-task Speech Recognition

## Quick Facts
- arXiv ID: 2310.13015
- Source URL: https://arxiv.org/abs/2310.13015
- Reference count: 0
- Single-sentence primary result: A-AF achieves 8% mean WER improvement relative to full fine-tuning and performs on-par with task-ID adapter routing while training only 17% of model parameters.

## Executive Summary
This paper introduces Audio-AdapterFusion (A-AF), a task-ID-free method for combining single-task adapters to efficiently and non-destructively solve multiple ASR tasks. A-AF overcomes catastrophic interference by training single-task adapters separately (knowledge extraction) and then combining them without task IDs (knowledge composition). Three adapter aggregation methods—Adapter Mean (Avg), Adapter Weighted Mean (WAvg), and A-AF—are proposed, with two learning algorithms (with or without updating adapter weights). Evaluated on 10 test sets from 4 ASR tasks, A-AF achieves 8% mean WER improvement relative to full fine-tuning and performs on-par with task-ID adapter routing. The methods are parameter-efficient, with MT-A Avg and MT-A WAvg updating only 17% of model parameters. A-AF also excels in zero-shot learning, improving WER by 9% on a new task. Layer Normalization before residual layers enhances training stability and performance.

## Method Summary
Audio-AdapterFusion addresses multi-task speech recognition without requiring task IDs by using a two-stage learning approach. First, single-task adapters are trained separately on individual ASR tasks (knowledge extraction) while keeping the base Conformer encoder frozen. Second, these adapters are combined using three aggregation methods: Adapter Mean (simple averaging), Adapter Weighted Mean (learned weighted averaging), and Audio-AdapterFusion (query-key-value attention over adapter outputs). The approach includes Layer Normalization before residual connections to stabilize training and improve performance. All methods are evaluated in both settings: training the fusion parameters while keeping adapters frozen (MT-A) and training both fusion parameters and adapter weights (MT-A with training). The base model consists of a 120M-parameter Conformer Encoder with a HAT decoder, and adapters are trained using RNN-T loss.

## Key Results
- A-AF achieves 8% mean WER improvement relative to full fine-tuning across 10 test sets from 4 ASR tasks
- A-AF performs on-par with task-ID adapter routing while being task-ID-free
- Parameter efficiency is significantly improved, with MT-A Avg and MT-A WAvg updating only 17% of model parameters
- A-AF excels in zero-shot learning, improving WER by 9% on a new task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Audio-AdapterFusion avoids catastrophic interference by training single-task adapters separately (knowledge extraction) and then combining them without task IDs (knowledge composition).
- Mechanism: The two-stage approach separates the learning of task-specific adapters from their combination, preventing conflicting gradients that cause catastrophic forgetting.
- Core assumption: Training adapters separately and then combining them is sufficient to preserve task-specific knowledge while enabling cross-task generalization.
- Evidence anchors:
  - [abstract] "A-AF overcomes catastrophic interference by training single-task adapters separately (knowledge extraction) and then combining them without task IDs (knowledge composition)."
  - [section 1] "Catastrophic forgetting, also known as catastrophic interference, is a phenomenon that occurs when a model abruptly loses information of the previous learned task(s) after training on a new task [13]."
- Break condition: If the combination stage introduces conflicting gradients or if the adapter outputs are incompatible in representation space, catastrophic interference could re-emerge.

### Mechanism 2
- Claim: Layer Normalization before residual layers stabilizes training and improves performance by normalizing attention output magnitudes.
- Mechanism: Different task adapters have varying output magnitudes; LayerNorm ensures consistent signal scaling before residual addition.
- Core assumption: The variance in adapter output magnitudes is significant enough to impact model convergence and performance.
- Evidence anchors:
  - [abstract] "Layer Normalization before residual layers enhances training stability and performance."
  - [section 3.5] "To address this, we learn a LayerNorm to ensure that the magnitude of the signal is normalized after attending to different task adapters. As seen in Figure 4, we show that LayerNorm significantly improves performance and training stability of Audio-AdapterFusion."
- Break condition: If adapter outputs are already normalized or have similar magnitudes, LayerNorm may provide negligible benefit or slightly degrade performance.

### Mechanism 3
- Claim: Adapter attention mechanism allows selective combination of adapter features per utterance, enabling zero-shot learning on new tasks.
- Mechanism: Query-key-value attention computes task-specific weighting of adapter features, allowing the model to adapt its representation based on input characteristics.
- Core assumption: The attention mechanism can learn meaningful task-specific combinations from the adapter representations without explicit task ID supervision.
- Evidence anchors:
  - [abstract] "A-AF also excels in zero-shot learning, improving WER by 9% on a new task."
  - [section 2.2] "attend to different values of the hidden dimension of different task adapter outputs. A-AF combines useful knowledge from each task to solve a particular utterance."
- Break condition: If the attention mechanism fails to learn meaningful combinations or overfits to training tasks, zero-shot performance may degrade significantly.

## Foundational Learning

- Concept: Transfer learning and fine-tuning
  - Why needed here: Understanding how pre-trained models are adapted to new tasks is fundamental to grasping adapter-based approaches.
  - Quick check question: What is the key difference between full fine-tuning and adapter-based fine-tuning?

- Concept: Multi-task learning and catastrophic interference
  - Why needed here: The paper's core contribution is addressing catastrophic interference in multi-task ASR.
  - Quick check question: Why does standard multi-task learning often lead to catastrophic forgetting?

- Concept: Attention mechanisms and self-attention
  - Why needed here: Audio-AdapterFusion uses query-key-value attention to combine adapter outputs.
  - Quick check question: How does the attention mechanism in A-AF differ from standard self-attention in Transformers?

## Architecture Onboarding

- Component map: Base Conformer encoder (frozen) -> Parallel single-task adapters (frozen after knowledge extraction) -> Adapter aggregation modules (Avg, WAvg, A-AF) -> LayerNorm layers (before residual connections) -> RNN-T loss function

- Critical path:
  1. Knowledge extraction: Train single-task adapters separately
  2. Knowledge composition: Combine adapters using chosen aggregation method
  3. Evaluation: Test on held-out test sets and zero-shot tasks

- Design tradeoffs:
  - Parameter efficiency vs. performance: Simpler methods (Avg) update fewer parameters but may underperform complex methods (A-AF)
  - Flexibility vs. storage: A-AF allows mixing vendor adapters but requires more parameters; simpler methods require less storage
  - Training stability vs. expressiveness: LayerNorm improves stability but adds parameters and computation

- Failure signatures:
  - Catastrophic interference: Performance degradation on previously learned tasks when training on new tasks
  - Poor zero-shot performance: Inability to generalize to unseen tasks
  - Training instability: NaN losses or exploding gradients during knowledge composition

- First 3 experiments:
  1. Implement and test Adapter Mean (Avg) on a single Conformer layer to verify basic functionality
  2. Add LayerNorm before residual connection and compare training stability
  3. Implement Audio-AdapterFusion on a subset of tasks and measure zero-shot performance

## Open Questions the Paper Calls Out
- How does Audio-AdapterFusion perform in multilingual ASR settings, particularly with language mixing during training?
- What is the optimal query, key, and value projection dimension for Audio-AdapterFusion across different ASR tasks and model architectures?
- How does the performance of Audio-AdapterFusion change when using adapters trained on low-resource datasets as separate task-specific adapters?

## Limitations
- The paper lacks ablation studies isolating the individual contributions of LayerNorm, attention mechanisms, and aggregation methods to performance gains
- Zero-shot learning claims are promising but based on limited task diversity (one new task), limiting generalizability
- While catastrophic interference is addressed theoretically, the paper doesn't provide quantitative comparisons of adapter performance degradation over sequential task learning

## Confidence
- **High confidence**: Parameter efficiency claims (17% parameters trained) and basic WER improvements over full fine-tuning are well-supported by experimental results across 10 test sets
- **Medium confidence**: The LayerNorm contribution is supported by presented results but lacks detailed analysis of when it becomes critical versus optional
- **Medium confidence**: Zero-shot learning claims are promising but based on limited task diversity (one new task)

## Next Checks
1. Conduct systematic ablation studies removing LayerNorm, attention mechanisms, and different aggregation methods to quantify individual contributions to performance gains
2. Evaluate A-AF on 3-5 diverse unseen tasks (different languages, domains, or acoustic conditions) to validate generalization beyond the single reported task
3. Track adapter performance on previously seen tasks during sequential learning of new tasks to provide quantitative evidence of interference mitigation