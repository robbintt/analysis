---
ver: rpa2
title: Distributional Data Augmentation Methods for Low Resource Language
arxiv_id: '2309.04862'
source_url: https://arxiv.org/abs/2309.04862
tags:
- augmentation
- data
- text
- language
- edda
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of text augmentation in low-resource
  languages, specifically Swedish. The authors propose two methods to extend the easy
  data augmentation (EDA) technique: easy distributional data augmentation (EDDA),
  which uses word2vec embeddings for synonym replacement instead of relying on external
  dictionaries, and type specific similar word replacement (TSSR), which constrains
  replacements based on part-of-speech tags to preserve semantic meaning.'
---

# Distributional Data Augmentation Methods for Low Resource Language

## Quick Facts
- arXiv ID: 2309.04862
- Source URL: https://arxiv.org/abs/2309.04862
- Reference count: 6
- Key outcome: EDDA and TSSR improve F1 scores in low-resource Swedish NLP tasks, especially when training data is limited.

## Executive Summary
This paper tackles the challenge of text augmentation in low-resource languages, specifically Swedish. The authors extend easy data augmentation (EDA) by replacing synonym dictionaries with word2vec embeddings (EDDA) and adding part-of-speech constraints (TSSR) to better preserve semantic meaning. Experiments on Swedish linguistic acceptability and sentiment analysis datasets show that these methods improve classification performance, particularly when training data is scarce. TSSR also demonstrates better semantic preservation than EDDA, with fewer sentences deviating from their original meaning.

## Method Summary
The authors propose two extensions to EDA: easy distributional data augmentation (EDDA) and type specific similar word replacement (TSSR). EDDA uses word2vec embeddings to find semantically similar words for replacement instead of relying on external synonym dictionaries. TSSR adds part-of-speech constraints to ensure replacements maintain sentence meaning and sentiment. The methods were evaluated on Swedish datasets (DALAJ for linguistic acceptability and ABSA for sentiment analysis) using SVM classifiers with BERT embeddings. Augmentation was applied across training data partitions from 10% to 100% to measure effectiveness in low-resource settings.

## Key Results
- With only 40% of training data, RSR improved F1 scores by 9% over baseline on DALAJ.
- TSSR demonstrated better semantic preservation compared to EDDA, with fewer sentences deviating from original meaning.
- Augmentation is most effective when training data is ≤60% of full dataset; using more data with augmentation tends to reduce effectiveness.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distributional embeddings (word2vec) replace synonym dictionaries, enabling EDA-style augmentation in low-resource languages.
- Mechanism: Instead of looking up synonyms in a fixed dictionary, the method finds words with similar co-occurrence patterns in an embedding space, allowing replacement with semantically related but not strictly synonymous words.
- Core assumption: Words with similar distributional patterns in word2vec space are acceptable substitutes for augmentation without breaking semantic meaning.
- Evidence anchors:
  - [abstract] "we propose two extensions, easy distributional data augmentation (EDDA), and type specific similar word replacement (TSSR), which uses semantic word context information and part-of-speech tags for word replacement and augmentation."
  - [section] "The synonym replacement is done, instead of using a lookup table, by using a word2vec model using its latent space to find the most similar word replacements."
- Break condition: If word2vec space does not contain semantically coherent neighbors (e.g., rare words, domain-specific terms), augmentation may introduce noise rather than useful variation.

### Mechanism 2
- Claim: Part-of-speech tagging constrains synonym replacement to preserve semantic and sentiment meaning.
- Mechanism: Only words of the same POS type are replaced (e.g., nouns with nouns, verbs with verbs), reducing the risk of altering sentence sentiment or grammaticality.
- Core assumption: POS-specific replacement is sufficient to maintain label integrity, especially in sentiment analysis tasks.
- Evidence anchors:
  - [abstract] "TSSR also demonstrated better semantic preservation compared to EDDA, with fewer sentences deviating from their original meaning."
  - [section] "This is due to randomness in EDDA may affect sentence sentiment... therefore, this is a directed approach to complement EDDA."
- Break condition: If POS tagger is inaccurate or unavailable, the constraint fails and may degrade augmentation quality.

### Mechanism 3
- Claim: Augmentation is most effective when training data is scarce (≤60% of full dataset).
- Mechanism: In low-data regimes, synthetic examples help fill the feature space and improve decision boundaries; in high-data regimes, augmentation adds little or may even hurt due to noise.
- Core assumption: The marginal benefit of synthetic data decreases as real data increases.
- Evidence anchors:
  - [section] "When comparing baseline to EDDA from 10% to 60% of the dataset, EDDA improves by 2.5% on average... However, using more than 60% of the data with augmentation tends to reduce the effectiveness."
- Break condition: If the augmentation introduces too much noise or the classifier overfits to synthetic patterns, performance may degrade even in low-data settings.

## Foundational Learning

- Concept: Distributional semantics and word embeddings
  - Why needed here: The core augmentation relies on finding similar words in embedding space rather than using a synonym dictionary.
  - Quick check question: What property of word2vec embeddings makes them suitable for finding "similar" words in context?

- Concept: Part-of-speech tagging and syntactic constraints
  - Why needed here: TSSR uses POS tags to ensure replacements preserve grammaticality and sentiment.
  - Quick check question: How does replacing only nouns with other nouns reduce the risk of altering sentiment?

- Concept: Data augmentation effectiveness in low-resource settings
  - Why needed here: Understanding when and why augmentation helps is key to applying these methods appropriately.
  - Quick check question: Why might augmentation hurt performance when enough real data is available?

## Architecture Onboarding

- Component map: Raw text sentences -> Tokenizer -> POS tagger (optional for TSSR) -> Word2vec model -> Augmentation modules (RSR, RI, RS, RD, TSSR) -> Augmented sentences
- Critical path: 1. Load and preprocess text 2. Run through POS tagger (if using TSSR) 3. For each augmentation type, select tokens/positions 4. Use word2vec to find replacement candidates 5. Generate new sentences 6. Return augmented dataset
- Design tradeoffs:
  - Using word2vec instead of BERT trades semantic precision for computational efficiency and morphological coherence.
  - TSSR adds a constraint layer that may reduce augmentation diversity but improves label preservation.
  - Augmentation rate (20% in this work) balances between diversity and noise.
- Failure signatures:
  - Low semantic similarity between original and augmented sentences (cosine < 0.9).
  - Performance degradation when using >60% of training data with augmentation.
  - Class label drift in sentiment analysis after augmentation.
- First 3 experiments:
  1. Run EDA with word2vec on a small Swedish dataset (e.g., 10% split) and measure F1 improvement.
  2. Apply TSSR to the same dataset and compare semantic deviation (cosine similarity) vs EDA.
  3. Gradually increase training data proportion and observe when augmentation stops helping.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do EDDA and TSSR perform on languages other than Swedish?
- Basis in paper: [explicit] The authors mention that their future work involves testing the augmentation techniques in other low-resource languages.
- Why unresolved: The paper only evaluates the proposed methods on Swedish datasets, leaving the generalizability to other languages untested.
- What evidence would resolve it: Experiments applying EDDA and TSSR to multiple low-resource languages with similar datasets and metrics.

### Open Question 2
- Question: What is the impact of EDDA and TSSR on tasks beyond classification?
- Basis in paper: [explicit] The authors state future work includes testing on multiple different downstream tasks other than classifications.
- Why unresolved: The evaluation is limited to two classification tasks (linguistic acceptability and sentiment analysis), so effects on other NLP tasks remain unknown.
- What evidence would resolve it: Applying EDDA and TSSR to tasks like named entity recognition, machine translation, or question answering and measuring performance changes.

### Open Question 3
- Question: How does the choice of embedding model (e.g., word2vec vs. BERT) affect the quality of augmented data?
- Basis in paper: [explicit] The authors chose word2vec for computational efficiency and to maintain morphological coherence, but acknowledge that larger models like BERT may not exist for all low-resource languages.
- Why unresolved: The paper does not compare different embedding models for augmentation quality or downstream performance.
- What evidence would resolve it: Systematic experiments comparing augmented data quality and task performance when using different embedding models across multiple languages.

## Limitations
- The proposed methods rely on the quality and coverage of the Swedish word2vec model, which may not capture rare or domain-specific terms effectively.
- The study only evaluates two datasets and one low-resource language, limiting generalizability to other languages or tasks.
- The semantic preservation metric (cosine similarity ≥ 0.9) is a heuristic that may not fully capture subtle shifts in meaning.

## Confidence
- **High confidence**: The mechanism that POS-constrained replacement improves label preservation is supported by both theoretical reasoning and empirical results showing TSSR outperforms EDDA in semantic deviation.
- **Medium confidence**: The claim that augmentation is most effective below 60% training data is based on results from two datasets; more languages and tasks would strengthen this pattern.
- **Low confidence**: The assumption that word2vec-based synonym replacement preserves meaning in low-resource languages is theoretically sound but lacks external validation beyond this paper.

## Next Checks
1. Replicate the experiments on at least two additional low-resource languages (e.g., Finnish, Icelandic) to test generalizability of the augmentation effectiveness pattern.
2. Perform ablation studies to measure the impact of POS tagging accuracy on TSSR performance, simulating tagger errors to assess robustness.
3. Test alternative similarity thresholds (e.g., 0.8, 0.95) for semantic preservation to determine sensitivity of results to the chosen cutoff.