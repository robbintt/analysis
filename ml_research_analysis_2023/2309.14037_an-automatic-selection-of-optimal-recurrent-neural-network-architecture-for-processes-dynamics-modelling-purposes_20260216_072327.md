---
ver: rpa2
title: An automatic selection of optimal recurrent neural network architecture for
  processes dynamics modelling purposes
arxiv_id: '2309.14037'
source_url: https://arxiv.org/abs/2309.14037
tags:
- neural
- algorithm
- number
- network
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of automatic selection of optimal
  recurrent neural network (RNN) architecture for black-box modelling of dynamic processes,
  specifically focusing on fast processes in pressurized water reactors (PWRs). The
  core method involves developing four novel algorithms (DNAS1-4) that use evolutionary
  algorithms (EAs) and gradient descent methods to search for optimal RNN structures.
---

# An automatic selection of optimal recurrent neural network architecture for processes dynamics modelling purposes

## Quick Facts
- arXiv ID: 2309.14037
- Source URL: https://arxiv.org/abs/2309.14037
- Reference count: 40
- Primary result: DNAS3 and DNAS4 algorithms find RNN architectures with mean errors of 0.0270 and 0.0108 on PWR test data

## Executive Summary
This paper develops four novel algorithms (DNAS1-4) for automatically selecting optimal recurrent neural network (RNN) architectures for black-box modeling of dynamic processes, specifically focusing on fast processes in pressurized water reactors (PWRs). The algorithms use evolutionary algorithms (EAs) with specialized operators for mutation and crossover to search for optimal RNN structures. The primary contribution is the development of mechanisms that balance network size and modeling accuracy through penalty-based fitness functions, with DNAS3 and DNAS4 achieving the best results.

## Method Summary
The core method involves developing four evolutionary algorithms that search the space of RNN architectures by varying parameters such as number of hidden layers, neurons per layer, input/output delays, and connection weights. Each algorithm uses different strategies: DNAS1-3 employ pure evolutionary search with specialized operators, while DNAS4 combines EA with gradient descent for weight optimization. The fitness function incorporates penalty terms for network complexity, encouraging the discovery of compact yet accurate architectures. The algorithms are tested on data from PWR fast processes, using control rod position and thermal power as inputs and outputs.

## Key Results
- DNAS3 achieved mean error of 0.0270 on test data with significantly smaller networks and lower computational time
- DNAS4 achieved the best performance with mean error of 0.0108 on test data
- The proposed algorithms outperformed exhaustive search and basic NEAT approaches in balancing accuracy and network size
- DNAS3 demonstrated superior performance-to-complexity ratio compared to other approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithms effectively balance network size and modeling accuracy by incorporating penalty terms into the fitness function.
- Mechanism: The fitness function includes penalty terms for the number of neurons and delays, which reduces the fitness score as the network becomes larger. This encourages the evolutionary process to find smaller networks that still achieve acceptable accuracy.
- Core assumption: The penalty weights (p1, p2, p3) are properly calibrated so that a network with one additional neuron must improve accuracy by at least 0.01 to be considered better.
- Evidence anchors:
  - [abstract]: "optimality has been understood as achieving a trade-off between the size of the neural network and its accuracy"
  - [section]: "The value of the fitness function is calculated as follows: ð‘“adapt,i = 10 âˆ’ ð‘1ð‘’ð‘– âˆ’ ð‘2ð‘ð‘– âˆ’ ð‘3ð·ð‘–"
- Break condition: If penalty weights are poorly calibrated, the algorithm might favor overly small networks with poor accuracy or overly large networks with marginal accuracy gains.

### Mechanism 2
- Claim: The evolutionary operators (mutation and crossover) effectively explore and exploit the search space for RNN architectures.
- Mechanism: Mutation operators explore new solutions by randomly changing weights, adding/deleting neurons, and modifying delays. Crossover operators exploit good solutions by combining features from two parent networks. The specialized operators are designed specifically for RNN architectures.
- Core assumption: The probabilities for mutation (pMutW, pMutNewN, pMutD, pMutDelN) and crossover (pCross) are properly set to balance exploration and exploitation.
- Evidence anchors:
  - [section]: "During the developing of each NAS algorithm a certain set of input parameters has been determined" and subsequent descriptions of mutation and crossover operators
  - [corpus]: Weak - no direct evidence about effectiveness of these specific operators
- Break condition: If probabilities are poorly set, the algorithm might converge prematurely to suboptimal solutions or fail to converge at all.

### Mechanism 3
- Claim: DNAS3 and DNAS4 algorithms achieve better performance by using species-based evolution and gradient descent learning respectively.
- Mechanism: DNAS3 uses species-based evolution to protect topological innovations and enable gradual evolution between different network structures. DNAS4 uses gradient descent methods for weight optimization while EA searches for architecture, combining global search with local optimization.
- Core assumption: Species-based evolution in DNAS3 effectively prevents premature convergence, and gradient descent in DNAS4 effectively optimizes weights once architecture is selected.
- Evidence anchors:
  - [section]: "In order to limit random events... subsequent mechanisms inspired by nature have been introduced" (DNAS3) and "The idea of the DNAS4 algorithm is different from the algorithms presented previously" (DNAS4)
  - [abstract]: "The best results achieved mean errors of 0.0270 (DNAS3) and 0.0108 (DNAS4)"
- Break condition: If species-based evolution doesn't effectively group similar networks or if gradient descent gets stuck in local minima, performance could degrade.

## Foundational Learning

- Concept: Recurrent Neural Networks (RNNs)
  - Why needed here: The paper specifically uses RNNs for black-box modeling of dynamic processes in PWRs, which have internal feedbacks and temporal dependencies
  - Quick check question: What is the key architectural difference between RNNs and feedforward networks that makes RNNs suitable for dynamic process modeling?

- Concept: Evolutionary Algorithms (EAs)
  - Why needed here: EAs are used to search the space of RNN architectures, handling the combinatorial nature of the search problem with discrete (number of layers, neurons) and continuous (weights) parameters
  - Quick check question: What are the two main types of evolutionary operators used in this paper, and what are their respective roles?

- Concept: Fitness Function Design
  - Why needed here: The fitness function balances accuracy and network complexity through penalty terms, guiding the evolutionary process toward optimal architectures
  - Quick check question: How does the fitness function penalize larger networks, and what is the rationale behind the penalty weights?

## Architecture Onboarding

- Component map: Initialize population -> Evaluate fitness -> Select parents -> Apply mutation and crossover -> Create new population -> Repeat until convergence
- Critical path: The algorithm iteratively evolves RNN architectures by evaluating fitness, selecting parents, applying genetic operators, and creating new populations until convergence criteria are met
- Design tradeoffs: Pure EA (DNAS1-3) vs hybrid EA-gradient descent (DNAS4); computational time vs accuracy; exploration vs exploitation; network size vs modeling accuracy
- Failure signatures: Premature convergence (population diversity loss), poor exploration (getting stuck in local optima), excessive computation time (inefficient search), overfitting (poor generalization)
- First 3 experiments:
  1. Run DNAS1 with minimal parameters (mamax=1, mmax=10, d=5, dy=5) on a simple synthetic dataset to verify basic functionality
  2. Compare DNAS2 and DNAS3 on the same dataset to observe the impact of species-based evolution
  3. Test DNAS4 with a single gradient descent method on a small problem to verify the hybrid approach works before adding complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed NAS algorithms perform on dynamic processes outside of pressurized water reactors (PWRs)?
- Basis in paper: [explicit] The authors state that the developed algorithms were tested specifically on PWR fast processes and suggest that future work could apply them to other types of dynamic systems.
- Why unresolved: The paper only validates the algorithms on PWR data, leaving uncertainty about their generalizability to other dynamic processes.
- What evidence would resolve it: Testing the DNAS algorithms on different dynamic systems (e.g., chemical reactors, biological systems, industrial processes) and comparing performance metrics like accuracy and computational efficiency.

### Open Question 2
- Question: Can the DNAS algorithms be extended to handle multi-input multi-output (MIMO) systems effectively?
- Basis in paper: [inferred] The current implementation focuses on single-input single-output (SISO) models, and the authors mention future work on MIMO models of PWR processes.
- Why unresolved: The paper does not explore the scalability of the algorithms to MIMO systems, which are common in real-world applications.
- What evidence would resolve it: Extending the algorithms to MIMO systems and evaluating their performance on tasks like predicting multiple reactor parameters simultaneously.

### Open Question 3
- Question: How do the computational requirements of the DNAS algorithms scale with increasing network complexity?
- Basis in paper: [explicit] The authors note that DNAS4 has significantly higher computational time compared to DNAS1-3, but do not provide a detailed analysis of scaling behavior.
- Why unresolved: The paper lacks a comprehensive study on how the algorithms perform as the number of neurons, layers, or delays increases.
- What evidence would resolve it: Conducting experiments with progressively larger networks and analyzing trends in computational time and resource usage.

## Limitations
- Lack of detailed implementation specifications for evolutionary operators and fitness function evaluation
- Mathematical model of PWR dynamics used for data generation is described only at a high level
- Results are based on limited test cases specific to PWR processes, raising questions about generalizability
- Computational requirements and scaling behavior are not fully characterized

## Confidence
- High Confidence: The core mechanism of using evolutionary algorithms with penalty-based fitness functions for architecture search is well-established in the literature
- Medium Confidence: The specific performance claims (0.0270 and 0.0108 mean errors for DNAS3 and DNAS4) are based on the PWR-specific dataset and may not generalize to other domains
- Low Confidence: The exact implementation details of the evolutionary operators and the complete mathematical model of PWR dynamics are not fully specified

## Next Checks
1. **Parameter Sensitivity Analysis:** Systematically vary key input parameters (mutation probabilities, penalty weights, population size) to determine their impact on convergence and final architecture quality.
2. **Cross-Domain Testing:** Apply the best-performing algorithm (DNAS4) to at least two different dynamic system datasets to evaluate generalizability beyond PWR processes.
3. **Computational Complexity Benchmarking:** Measure and compare the computational time requirements of DNAS algorithms against exhaustive search and NEAT baselines across different problem sizes to validate the claimed efficiency improvements.