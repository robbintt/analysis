---
ver: rpa2
title: Nepotistically Trained Generative-AI Models Collapse
arxiv_id: '2311.12202'
source_url: https://arxiv.org/abs/2311.12202
tags:
- images
- retraining
- generated
- real
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the vulnerability of generative AI models,
  specifically Stable Diffusion, to data poisoning through exposure to their own creations.
  The authors demonstrate that iterative retraining of the model on a mixture of real
  and self-generated images leads to a degradation in image quality and diversity,
  with highly distorted faces emerging after several iterations.
---

# Nepotistically Trained Generative-AI Models Collapse

## Quick Facts
- **arXiv ID:** 2311.12202
- **Source URL:** https://arxiv.org/abs/2311.12202
- **Reference count:** 24
- **Primary result:** Iterative retraining of Stable Diffusion on its own generated images leads to progressive model collapse, even with as little as 3.3% self-generated data.

## Executive Summary
This paper demonstrates a critical vulnerability in generative AI models, specifically Stable Diffusion, to data poisoning through exposure to their own creations. The authors show that retraining the model on a mixture of real and self-generated images causes progressive degradation in image quality and diversity, with highly distorted faces emerging after several iterations. This collapse persists even when only a small percentage of the retraining data consists of self-generated images. The study reveals that the degradation extends beyond specific prompts used in retraining, affecting the model's overall generative capabilities. While partial recovery is possible through retraining on only real images, the model struggles to fully heal, with artifacts persisting in the generated images.

## Method Summary
The authors investigate model collapse by iteratively retraining Stable Diffusion v.2.1 on mixtures of real images from the FFHQ dataset and self-generated images. They extract 900 images from FFHQ, categorized by demographic attributes, and generate corresponding images using Stable Diffusion's image-to-image synthesis pipeline. The denoising U-Net module is retrained on datasets with varying composition (0% to 100% self-generated images) for five iterations each. Image quality is evaluated using FID and CLIP scores after each iteration. To test healing, the poisoned model (trained on 25% self-generated images) is retrained on only real images for five additional iterations, with quality metrics compared before and after healing.

## Key Results
- Iterative retraining on self-generated images causes progressive degradation in image quality and diversity
- Model collapse occurs even with only 3.3% self-generated data in the retraining mixture
- The degradation extends beyond specific prompts used in retraining, affecting overall generative capabilities
- Partial recovery is possible through retraining on only real images, but artifacts persist

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative retraining on self-generated images causes progressive degradation in image quality and diversity.
- Mechanism: The model learns from its own outputs, which already contain artifacts and distortions. These artifacts compound with each retraining iteration, leading to a collapse in the quality and diversity of generated images.
- Core assumption: The self-generated images used for retraining contain enough artifacts to negatively influence the model's learning process.
- Evidence anchors:
  - [abstract] "We show that when retrained on even small amounts of their own creation, these generative-AI models produce highly distorted images."
  - [section] "Regardless of the mixture ratio, the iterative retraining eventually leads to collapse by the fifth iteration at which point the generated images are highly distorted."
- Break condition: The model is retrained on a dataset with no self-generated images, allowing it to recover partially but not fully.

### Mechanism 2
- Claim: The degradation in image quality is not limited to the specific prompts used for retraining.
- Mechanism: The model's ability to generate coherent images for different prompts is affected by the retraining process, suggesting that the model's overall generative capabilities are compromised.
- Core assumption: The retraining process affects the model's internal representations in a way that generalizes beyond the specific prompts used.
- Evidence anchors:
  - [abstract] "We also show that this distortion extends beyond the text prompts used in retraining."
  - [section] "However, the images generated by the model retrained on 25% SD-generated faces often – but not always – exhibits the same textured artifacts as seen in the faces in the upper portion of this figure."
- Break condition: The model is retrained on a diverse dataset with no self-generated images, allowing it to regain some of its original capabilities.

### Mechanism 3
- Claim: The model struggles to fully heal even after retraining on only real images.
- Mechanism: The artifacts and distortions learned during the retraining process are not completely erased, leaving a residual impact on the model's generative capabilities.
- Core assumption: The retraining process creates permanent changes in the model's internal representations that cannot be fully reversed.
- Evidence anchors:
  - [abstract] "We also show that this distortion extends beyond the text prompts used in retraining, and that once poisoned, the models struggle to fully heal even after retraining on only real images."
  - [section] "Although in some cases by the tenth iteration, the generated faces have fewer artifacts, in other cases, the artifacts persist."
- Break condition: The model is retrained on a diverse dataset with no self-generated images, allowing it to regain some of its original capabilities.

## Foundational Learning

- Concept: Data poisoning in machine learning
  - Why needed here: Understanding the concept of data poisoning is crucial to grasp the vulnerability of generative AI models to their own creations.
  - Quick check question: What is data poisoning in the context of machine learning, and how does it relate to the vulnerability of generative AI models?

- Concept: Diffusion-based text-to-image models
  - Why needed here: The paper focuses on the vulnerability of Stable Diffusion, a popular diffusion-based text-to-image model, to data poisoning.
  - Quick check question: What are diffusion-based text-to-image models, and how do they differ from other generative AI models?

- Concept: Evaluation metrics for image quality
  - Why needed here: The paper uses FID and CLIP scores to assess the quality of synthesized images, which are important metrics for understanding the impact of data poisoning.
  - Quick check question: What are FID and CLIP scores, and how are they used to evaluate the quality of synthesized images?

## Architecture Onboarding

- Component map: Real images + demographic prompts -> Stable Diffusion (v.2.1) with denoising U-Net module -> Generated images -> FID/CLIP evaluation

- Critical path:
  1. Retrain the denoising U-Net module of Stable Diffusion on a mixture of real and self-generated images.
  2. Generate new images from the retrained model.
  3. Evaluate the quality of the generated images using FID and CLIP scores.

- Design tradeoffs:
  - Balancing the ratio of real to self-generated images in the retraining dataset.
  - Choosing the appropriate number of retraining iterations.
  - Selecting the most suitable evaluation metrics for image quality.

- Failure signatures:
  - Progressive degradation in image quality and diversity.
  - Presence of artifacts and distortions in generated images.
  - Inability to fully recover even after retraining on only real images.

- First 3 experiments:
  1. Retrain the model on a dataset with 100% real images and evaluate the FID and CLIP scores.
  2. Retrain the model on a dataset with 50% real and 50% self-generated images and evaluate the FID and CLIP scores.
  3. Retrain the model on a dataset with 25% real and 75% self-generated images and evaluate the FID and CLIP scores.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of the underlying model architecture or training data cause generative AI models to be vulnerable to data poisoning?
- Basis in paper: [inferred] The paper demonstrates that generative AI models, specifically Stable Diffusion, are vulnerable to data poisoning but does not delve into the specific reasons behind this vulnerability.
- Why unresolved: The paper focuses on the effects of data poisoning rather than the underlying causes. Understanding the specific model or data characteristics that lead to vulnerability could help in designing more robust models.
- What evidence would resolve it: A detailed analysis of the model architecture and training data, comparing poisoned and non-poisoned models, could identify the specific factors contributing to vulnerability.

### Open Question 2
- Question: Can generative AI systems be trained or modified to be resilient to data poisoning?
- Basis in paper: [explicit] The paper mentions that it is an open question whether generative AI systems can be trained or modified to be resilient to data poisoning.
- Why unresolved: While the paper discusses the vulnerability of generative AI models to data poisoning, it does not explore potential solutions or modifications to make the models more resilient.
- What evidence would resolve it: Research and experiments focused on training or modifying generative AI models to resist data poisoning, along with comparative analysis of the resilience of different models, would provide evidence to address this question.

### Open Question 3
- Question: Are there specific techniques or datasets that can accelerate the healing process of poisoned generative AI models?
- Basis in paper: [explicit] The paper mentions that it is an open question whether there are specific techniques or datasets that can accelerate the healing process of poisoned models.
- Why unresolved: While the paper briefly touches on the healing process of poisoned models, it does not explore potential techniques or datasets that could expedite this process.
- What evidence would resolve it: Research and experiments focused on developing and testing techniques or datasets that can accelerate the healing process of poisoned generative AI models, along with comparative analysis of the effectiveness of different approaches, would provide evidence to address this question.

## Limitations

- The study focuses exclusively on Stable Diffusion v.2.1 and facial image generation, limiting generalizability to other models and domains.
- The paper doesn't fully characterize the threshold at which different mixture ratios begin to cause collapse, showing only discrete intervals.
- The healing experiments show partial recovery but don't establish whether complete recovery is theoretically possible or what training regimen might achieve it.

## Confidence

- **High confidence**: The demonstration of progressive degradation through iterative retraining is well-supported by quantitative metrics (FID, CLIP scores) and qualitative observation of image artifacts.
- **Medium confidence**: The claim that collapse occurs even with 3.3% self-generated data is supported, though the exact threshold remains unclear.
- **Medium confidence**: The generalization of collapse beyond specific prompts is demonstrated, but the mechanism for this cross-prompt contamination could be explored more deeply.

## Next Checks

1. Test whether similar collapse patterns emerge in other popular diffusion models (DALL-E, Midjourney) and across different domains (non-facial images, text-to-text models).
2. Systematically map the threshold relationship between mixture ratio and time-to-collapse to identify critical tipping points.
3. Investigate whether specific architectural modifications to the U-Net (e.g., dropout patterns, normalization layers) can increase resilience to self-training artifacts.