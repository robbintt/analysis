---
ver: rpa2
title: 'NineRec: A Benchmark Dataset Suite for Evaluating Transferable Recommendation'
arxiv_id: '2309.07705'
source_url: https://arxiv.org/abs/2309.07705
tags:
- uni00000013
- uni00000014
- uni00000015
- bili
- uni00000048
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NineRec is a benchmark dataset suite for transferable recommendation,
  consisting of a large-scale source domain dataset and nine diverse target domain
  datasets. Each item is represented by a descriptive text and high-resolution cover
  image.
---

# NineRec: A Benchmark Dataset Suite for Evaluating Transferable Recommendation

## Quick Facts
- arXiv ID: 2309.07705
- Source URL: https://arxiv.org/abs/2309.07705
- Reference count: 40
- Primary result: NineRec enables transferable recommendation models to learn from raw multimodal features, showing that pre-training on source datasets improves target performance by up to 200% compared to two-stage training

## Executive Summary
NineRec is a benchmark dataset suite designed to evaluate transferable recommendation models across diverse domains and platforms. It consists of a large-scale source dataset (Bili_2M) and nine target datasets spanning food, dance, movies, cartoons, music, and content platforms like KU, QB, TN, and DY. Each item in the datasets is represented by descriptive text and high-resolution cover images. The suite enables the implementation of transferable recommendation models by learning from raw multimodal features rather than pre-extracted features. Benchmark results demonstrate that pre-training on the source dataset improves performance on target datasets, text modality generally outperforms image modality, and end-to-end training significantly outperforms two-stage training approaches.

## Method Summary
NineRec provides a large-scale source dataset (Bili_2M) and nine diverse target datasets for evaluating transferable recommendation models. The datasets contain user-item interactions with descriptive text and cover images for each item. Models are pre-trained on the source dataset and fine-tuned on target datasets using sequence-to-sequence training modes. The approach involves user encoders (UE) like SASRec, BERT4Rec, NextItNet, and GRU4Rec combined with item modality encoders (ME) such as BERT for text and Swin Transformer for images. The evaluation uses leave-one-out protocol with Hit Ratio @10 (H@10) and Normalized Discounted Cumulative Gain @10 (N@10) metrics.

## Key Results
- Pre-training on the source dataset (Bili_2M) substantially improves performance on diverse target datasets
- Text modality generally outperforms image modality for recommendation tasks
- End-to-end training of modality encoders with user encoders outperforms two-stage pre-extraction of features by up to 200%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on a large-scale source dataset (Bili_2M) substantially improves transfer performance on diverse target datasets
- Mechanism: The large source dataset provides rich, diverse user-item interactions across multiple channels, enabling the model to learn generalizable patterns that transfer across domains and platforms
- Core assumption: The source and target domains share transferable patterns in user behavior and item representations, despite domain differences
- Evidence anchors:
  - [abstract] "pre-training on the source dataset mostly improves performance on target datasets"
  - [section] "TransRec pre-trained on the source dataset (i.e. HasPT) mostly performs better than its NoPT version"
- Break condition: If the source and target domains are too dissimilar (e.g., completely different user intent signals), pre-training may not provide benefits and could even degrade performance

### Mechanism 2
- Claim: End-to-end training of modality encoders (ME) with user encoders (UE) outperforms two-stage pre-extraction of modality features
- Mechanism: Joint optimization allows the ME and UE to learn task-specific representations that are better aligned for recommendation, rather than using generic pre-trained features
- Core assumption: The recommendation task benefits from representations that are optimized jointly rather than using fixed pre-extracted features
- Evidence anchors:
  - [abstract] "end-to-end training outperforms two-stage training by up to 200%"
  - [section] "TransRec by E2E training of ME outperforms the TS method substantially"
- Break condition: If computational constraints make end-to-end training infeasible, or if pre-extracted features are already highly optimized for the task, the benefit may diminish

### Mechanism 3
- Claim: Text modality generally outperforms image modality for recommendation tasks in this dataset
- Mechanism: Text descriptions provide more explicit and diverse semantic information about items, which is more directly useful for capturing user preferences than visual features alone
- Core assumption: User intent in content-only platforms (short videos, news) can be better captured through textual descriptions than through visual appearance alone
- Evidence anchors:
  - [abstract] "text modality generally outperforms image modality"
  - [section] "TransRec pre-trained on text modality in general significantly outperforms its IDRec counterpart... sometimes performs worse than IDRec if pre-trained on image modality"
- Break condition: If the visual modality becomes more expressive (e.g., with better pre-trained models or more diverse visual content), or if the task shifts to visual-centric domains, image modality could become more effective

## Foundational Learning

- Concept: Transfer learning across domains and platforms
  - Why needed here: The core innovation of NineRec is enabling models to transfer knowledge from a large source dataset to diverse target datasets across different domains and platforms
  - Quick check question: Can you explain why pre-training on Bili_2M helps improve performance on Bili_Food, KU, QB, TN, and DY?

- Concept: End-to-end vs. two-stage training for multimodal models
  - Why needed here: Understanding why jointly training UE and ME is superior to pre-extracting features is crucial for implementing effective TransRec models
  - Quick check question: What is the key difference in how features are learned between E2E and TS approaches, and why does this matter for recommendation quality?

- Concept: Multimodal representation learning
  - Why needed here: The dataset contains both text and image modalities, and understanding how to effectively combine or choose between them is essential
  - Quick check question: When would you choose to use only text, only image, or both modalities for a recommendation task, based on the findings in this paper?

## Architecture Onboarding

- Component map: Bili_2M -> Pre-training -> TransRec (UE + ME) -> Fine-tuning -> Target datasets (Bili_Food, Bili_Dance, Bili_Movie, Bili_Cartoon, Bili_Music, KU, QB, TN, DY)

- Critical path:
  1. Pre-train UE+ME on source dataset (Bili_2M)
  2. Fine-tune on target dataset
  3. Evaluate using leave-one-out protocol with H@10 and N@10 metrics

- Design tradeoffs:
  - E2E vs. TS training: E2E is more computationally expensive but yields better performance
  - Text vs. image modality: Text generally performs better, but image can provide complementary information
  - S2S vs. S2O training: S2S is more effective but requires more computation
  - Model size vs. performance: Larger ME models (e.g., Swin-B vs. Swin-T) may improve performance but increase training cost

- Failure signatures:
  - Model collapse during training (particularly with BERT4Rec + Swin Transformer combinations)
  - Poor transfer performance when source and target domains are too dissimilar
  - Degradation when using pre-extracted features instead of E2E training
  - Overfitting on small target datasets

- First 3 experiments:
  1. Implement and evaluate IDRec baseline on a target dataset to establish performance floor
  2. Implement TransRec with BERT ME and SASRec UE, pre-trained on Bili_2M, evaluated on the same target dataset
  3. Compare performance of text-only vs. image-only vs. multimodal versions of the same TransRec architecture on the target dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal training strategy for MoRec/TransRec models to balance computational efficiency and performance?
- Basis in paper: [explicit] The paper highlights that E2E training is computationally expensive but significantly outperforms two-stage training, yet notes that iterating over all hyperparameter combinations for E2O MoRec may be infeasible due to high computational costs
- Why unresolved: The computational cost of E2E training is a major barrier, and there's a lack of research on hyperparameter search strategies specifically tailored for MoRec/TransRec models
- What evidence would resolve it: Empirical studies comparing different training strategies (e.g., E2E vs. two-stage, different hyperparameter search methods) on large-scale datasets, with clear metrics for both efficiency and performance

### Open Question 2
- Question: How do different network architectures impact the performance of MoRec models, and are there specific architectures that are universally effective?
- Basis in paper: [explicit] The paper shows that model collapse can occur during learning MoRec/TransRec, and that MoRec with the DSSM architecture performs inadequately, suggesting that not all RS architectures are suitable for MoRec
- Why unresolved: There's a lack of comprehensive studies on how different RS architectures (e.g., DSSM, S2O, S2S) perform with MoRec, and no clear guidance on which architectures are most effective
- What evidence would resolve it: Systematic evaluation of various RS architectures with MoRec on multiple datasets, with analysis of performance trends and identification of architectures that consistently perform well

### Open Question 3
- Question: Can MoRec models achieve competitive performance with IDRec models in non-cold start settings, and under what conditions?
- Basis in paper: [explicit] The paper shows that MoRec models pre-trained on text modality generally outperform their IDRec counterparts on downstream datasets, but sometimes perform worse when pre-trained on image modality
- Why unresolved: The performance gap between MoRec and IDRec in non-cold start settings is not fully understood, and the conditions under which MoRec can be competitive are not clearly defined
- What evidence would resolve it: Comparative studies of MoRec and IDRec models on diverse datasets and settings, with analysis of factors that influence performance (e.g., modality type, dataset characteristics, model architecture)

## Limitations

- Computational requirements for end-to-end training are substantial (100x-1000x more FLOPs than traditional IDRec), potentially limiting accessibility
- Some model combinations (BERT4Rec + Swin Transformer) experienced training collapse, suggesting stability issues with certain architecture pairings
- The paper does not provide detailed data preprocessing steps beyond basic image resizing and text length limits

## Confidence

- High confidence: Pre-training on source dataset improves target performance
- Medium confidence: Text modality generally outperforms image modality
- Medium confidence: End-to-end training outperforms two-stage training

## Next Checks

1. Verify reproducibility by implementing a simplified version of TransRec (e.g., SASRec + BERT) on a single target dataset and comparing results with reported baselines
2. Test the stability of different UE+ME combinations by training multiple runs with varying random seeds to assess variance in performance
3. Evaluate the impact of computational constraints by comparing E2E vs. TS performance when limiting batch sizes or using smaller ME models