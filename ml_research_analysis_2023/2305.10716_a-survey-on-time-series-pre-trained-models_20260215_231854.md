---
ver: rpa2
title: A Survey on Time-Series Pre-Trained Models
arxiv_id: '2305.10716'
source_url: https://arxiv.org/abs/2305.10716
tags:
- time
- series
- learning
- classi
- cation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive review of Time-Series Pre-Trained
  Models (TS-PTMs), which are pre-trained models designed to alleviate the reliance
  of deep learning models on massive labeled data in Time-Series Mining (TSM). TS-PTMs
  are classified based on pre-training techniques into supervised, unsupervised, and
  self-supervised models.
---

# A Survey on Time-Series Pre-Trained Models

## Quick Facts
- arXiv ID: 2305.10716
- Source URL: https://arxiv.org/abs/2305.10716
- Reference count: 40
- Primary result: Comprehensive review and experimental evaluation of 27 TS-PTM methods across 434 datasets and 679 transfer learning scenarios

## Executive Summary
This survey provides a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), which are pre-trained models designed to alleviate the reliance of deep learning models on massive labeled data in Time-Series Mining (TSM). The authors classify TS-PTMs based on pre-training techniques into supervised, unsupervised, and self-supervised models, with detailed discussions of each category. The survey includes extensive experiments evaluating 27 methods across 434 datasets and 679 transfer learning scenarios, providing insights into the advantages and disadvantages of various transfer learning strategies, Transformer-based models, and representative TS-PTMs.

## Method Summary
The survey conducts extensive experiments on 27 TS-PTM methods across 434 datasets and 679 transfer learning scenarios. The evaluation covers three main tasks: time-series classification (using 128 UCR univariate and 30 UEA multivariate datasets), forecasting (using ETT and Electricity datasets), and anomaly detection (using Yahoo and KPI datasets). Models are pre-trained using supervised, unsupervised, or self-supervised techniques, then fine-tuned on target datasets and evaluated using metrics including classification accuracy, MSE, MAE, F1-score, precision, and recall.

## Key Results
- TS-PTMs show superior performance compared to training from scratch on small datasets
- Transformer-based models demonstrate significant potential for time-series forecasting and anomaly detection
- Consistency-based self-supervised learning effectively captures temporal dependencies in time series data

## Why This Works (Mechanism)

### Mechanism 1
Pre-trained models reduce reliance on large labeled datasets by leveraging transfer learning from source domains. The PTM is pre-trained on source data using supervised, unsupervised, or self-supervised techniques, then fine-tuned on the target dataset, transferring learned representations. Core assumption: Source and target datasets share relevant features or distributions that allow knowledge transfer.

### Mechanism 2
Consistency-based self-supervised learning effectively captures temporal dependencies in time series. Positive pairs are created from augmented views of the same time series (e.g., subseries consistency, temporal consistency, transformation consistency), and contrastive loss encourages similar representations for these pairs. Core assumption: Time series data has inherent consistency properties that can be exploited for self-supervision.

### Mechanism 3
Transformer-based models are effective for time series forecasting and anomaly detection due to their ability to capture long-range dependencies. Transformers use multi-head attention to dynamically compute associations between representations, enabling them to model complex temporal patterns. Core assumption: Time series data contains long-range dependencies that can be effectively captured by self-attention mechanisms.

## Foundational Learning

- Concept: Transfer learning and its role in deep learning
  - Why needed here: The paper heavily relies on transfer learning as the core mechanism for TS-PTMs, so understanding how knowledge transfer works is crucial
  - Quick check question: What are the key differences between supervised, unsupervised, and self-supervised pre-training in the context of transfer learning?

- Concept: Time series data properties (e.g., temporal dependencies, multi-scale dependencies)
  - Why needed here: The paper emphasizes the importance of leveraging inherent properties of time series for effective pre-training, so understanding these properties is essential
  - Quick check question: How do temporal dependencies differ from multi-scale dependencies in time series data, and why is it important to consider both?

- Concept: Contrastive learning and its application to self-supervised learning
  - Why needed here: The paper discusses consistency-based PTMs that utilize contrastive learning, so understanding how contrastive learning works is necessary
  - Quick check question: What are the key components of a contrastive learning framework, and how does it differ from other self-supervised learning approaches?

## Architecture Onboarding

- Component map: Source dataset → Base model (CNN, RNN, Transformer) → Pre-training task (classification, reconstruction, consistency) → Target dataset → Downstream task (classification, forecasting, anomaly detection) → Performance metrics

- Critical path: Pre-training on source dataset → Fine-tuning on target dataset → Evaluation on test set

- Design tradeoffs:
  - Choice of pre-training task (supervised vs. unsupervised vs. self-supervised) affects the type and amount of source data needed
  - Architecture selection (CNN, RNN, Transformer) impacts the model's ability to capture different time series properties
  - Data augmentation strategies for self-supervised learning must be carefully designed to avoid introducing false positive pairs

- Failure signatures:
  - Negative transfer: Performance degradation on target task after pre-training
  - Overfitting: Poor generalization to unseen data due to insufficient pre-training data or overly complex model architecture
  - Ineffective representation learning: Failure to capture relevant time series properties due to poor choice of pre-training task or architecture

- First 3 experiments:
  1. Evaluate the impact of different pre-training tasks (supervised classification vs. unsupervised reconstruction) on downstream classification performance
  2. Compare the performance of CNN, RNN, and Transformer architectures for pre-training on a specific time series dataset
  3. Investigate the effectiveness of different data augmentation strategies for self-supervised pre-training on time series data

## Open Questions the Paper Calls Out

### Open Question 1
How can we construct a large-scale, well-labeled time series dataset like ImageNet for TS-PTMs? Basis: The paper discusses the lack of publicly available large-scale benchmark datasets in the time series domain. Unresolved because creating such a dataset requires substantial effort and resources. Evidence would be successful construction of a large-scale, well-labeled time series dataset similar to ImageNet.

### Open Question 2
How can we design effective Transformer-based models specifically for time series classification tasks? Basis: The paper states that Transformer-based models have shown significant potential for time-series forecasting and anomaly detection tasks, but their performance on time-series classification tasks remains challenging. Unresolved because adapting Transformers to time series classification is still an open challenge. Evidence would be development and validation of Transformer-based models specifically designed for time series classification tasks.

### Open Question 3
How can we effectively design consistency-based strategies that leverage the multi-scale properties of time series for TS-PTMs? Basis: The paper discusses the effectiveness of consistency-based strategies for TS-PTMs and mentions the potential of utilizing multi-scale properties of time series. Unresolved because fully exploiting the multi-scale nature of time series for improved TS-PTM performance remains an open challenge. Evidence would be development and validation of consistency-based strategies specifically designed to leverage the multi-scale properties of time series.

## Limitations

- Experimental scope may not capture all practical deployment scenarios beyond classification, forecasting, and anomaly detection
- Focus on benchmark datasets may not reflect real-world noise and complexity found in industrial applications
- Survey primarily examines existing methods rather than proposing novel TS-PTM architectures

## Confidence

- High Confidence: The classification framework for TS-PTMs (supervised, unsupervised, self-supervised) is well-established in both literature and practice
- Medium Confidence: The effectiveness of Transformer-based models for time series tasks is supported by multiple studies, but their superiority varies by dataset and task
- Medium Confidence: The claim that pre-training reduces reliance on labeled data is well-supported, but the extent depends heavily on source-target domain similarity

## Next Checks

1. Conduct ablation studies on the three consistency-based self-supervised methods (subseries, temporal, and transformation consistency) to quantify their individual contributions to representation quality

2. Evaluate negative transfer scenarios by deliberately pairing dissimilar source-target datasets to measure performance degradation and identify failure thresholds

3. Test model robustness against real-world time series characteristics like missing values, irregular sampling, and noise levels not present in benchmark datasets