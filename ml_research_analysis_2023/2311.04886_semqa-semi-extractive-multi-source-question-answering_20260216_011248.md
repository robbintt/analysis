---
ver: rpa2
title: 'SEMQA: Semi-Extractive Multi-Source Question Answering'
arxiv_id: '2311.04886'
source_url: https://arxiv.org/abs/2311.04886
tags:
- answers
- questions
- answer
- question
- semqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SEMQA, a new task for answering multi-answer
  questions by summarizing multiple diverse sources in a semi-extractive fashion.
  Given a question and a set of retrieved passages, SEMQA requires models to output
  a comprehensive answer that interleaves factual quoted spans (copied verbatim from
  sources) with free-text connectors, enabling both fluent language generation and
  fine in-line attributions for easy verification and evaluation.
---

# SEMQA: Semi-Extractive Multi-Source Question Answering

## Quick Facts
- arXiv ID: 2311.04886
- Source URL: https://arxiv.org/abs/2311.04886
- Reference count: 40
- Key outcome: Introduces SEMQA task requiring semi-extractive answers that interleave quoted spans from sources with free-text connectors, creating QuoteSum dataset and showing fine-tuned models outperform few-shot approaches

## Executive Summary
This paper introduces SEMQA, a new task for answering multi-answer questions by summarizing multiple diverse sources in a semi-extractive fashion. Given a question and retrieved passages, SEMQA requires models to output comprehensive answers that interleave factual quoted spans (copied verbatim from sources) with free-text connectors, enabling both fluent language generation and fine in-line attributions for easy verification and evaluation. The authors create QuoteSum, the first dataset of this kind, containing human-written semi-extractive answers to natural and generated questions, and define text-based evaluation metrics including fluency, attribution precision, and comprehensiveness. Experiments with fine-tuned and few-shot large language models show that while few-shot models perform surprisingly well, fine-tuned models achieve the best performance, highlighting the value of the dataset.

## Method Summary
The authors create QuoteSum dataset by collecting human-written semi-extractive answers to questions, where answers contain explicit markers for quoted spans from source passages. They fine-tune T5 and Flan-T5 models on this dataset using T5X framework with batch size 32, learning rate 1e-3, and Adafactor optimizer for up to 25k steps. For few-shot evaluation, they use PaLM2 API with dynamic prompts that retrieve 4-5 most similar questions from training set using Sentence-T5 embeddings. Evaluation uses string-based metrics including ROUGE-L for fluency, Sem-F1 and Sem-Rec for attribution precision and comprehensiveness, and a combined geometric mean score.

## Key Results
- Fine-tuned models outperform few-shot approaches on SEMQA metrics, demonstrating the value of the QuoteSum dataset
- SEMQA answers are rated as more comprehensive and easier to verify than abstractive answers with citations in user studies
- String-based metrics (Sem-F1, Sem-Rec) provide efficient evaluation alternatives to model-based factual consistency scoring
- The semi-extractive format enables faster manual evaluation by allowing readers to verify quoted facts by checking context in source passages

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Explicitly marking quoted spans simplifies reader verification and speeds manual evaluation
- **Mechanism:** When factual statements are extracted verbatim and labeled with their source, the reader only needs to confirm that the statement is in context within the source, rather than decomposing and cross-checking paraphrased text
- **Core assumption:** Readers can efficiently locate and verify quoted spans in the original source
- **Evidence anchors:**
  - [abstract] "It readily supports fast and easy verification by the reader—only requiring to confirm the context in the source."
  - [section 3] "It is generally easier to verify answers that directly quotes factual statements compared to verifying answers that paraphrase information."
  - [corpus] Weak – no direct citation count or reader time data provided

### Mechanism 2
- **Claim:** Semi-extractive generation reduces hallucination by forcing factual claims to be sourced from provided passages
- **Mechanism:** By design, the model must explicitly extract and attribute factual spans, which constrains the output to groundable content and reduces fabricated information
- **Core assumption:** The provided sources are reliable and contain the correct factual content
- **Evidence anchors:**
  - [abstract] "using extracted spans from reliable sources for generating factual claims could help prevent generation mistakes and model hallucinations."
  - [section 5.1] "The SEMQA model performed better on comprehensiveness, and significantly better on correctness."
  - [corpus] Weak – correctness is inferred from human ratings, not from an independent factual accuracy dataset

### Mechanism 3
- **Claim:** String-based metrics can replace expensive model-based evaluations for factual consistency
- **Mechanism:** By requiring quoted spans, evaluation reduces to token-level F1 and recall calculations, bypassing the need for black-box QA models to judge factual correctness
- **Core assumption:** Factual precision and recall can be adequately measured by exact token matching
- **Evidence anchors:**
  - [abstract] "Extracted spans can be readily evaluated for both recall and precision with standard string-matching metrics."
  - [section 3.1] Defines Sem-F1 and Sem-Rec based on token-level matching
  - [section 5.1] "Our metrics can help understand the strengths and weaknesses of different models... suggesting the value of our curated QuoteSum dataset."

## Foundational Learning

- **Concept:** Multi-answer consolidation
  - **Why needed here:** The task explicitly involves aggregating multiple answers from diverse sources into a single cohesive passage
  - **Quick check question:** Given three passages each giving a different release year for a song, how would you structure a single answer that lists all years with source attribution?

- **Concept:** Text span extraction and attribution
  - **Why needed here:** The semi-extractive format relies on the model selecting and marking verbatim spans from input passages
  - **Quick check question:** How do you programmatically identify and mark the exact tokens copied from a source in a generated answer?

- **Concept:** String-based evaluation metrics (F1, ROUGE-L)
  - **Why needed here:** Evaluation is based on token overlap between generated quoted spans and reference quoted spans, without external models
  - **Quick check question:** Given a generated answer with spans marked [1] and [2], how would you compute precision and recall against reference spans?

## Architecture Onboarding

- **Component map:** Retriever → Passage selector → QuoteSum-trained model → Answer with span markers → String-based evaluator
- **Critical path:** Passage retrieval → Answer generation → Span attribution → Evaluation
- **Design tradeoffs:** 
  - Using explicit quoting improves verifiability but may constrain fluency
  - String-based evaluation is fast but may miss paraphrased correctness
  - Balancing recall vs. precision of quoted spans affects comprehensiveness vs. conciseness
- **Failure signatures:**
  - Low Sem-F1: model failing to mark extracted spans or extracting irrelevant tokens
  - Low Sem-Rec: missing answers from some sources or incomplete coverage
  - Low ROUGE-L: poor fluency or failure to connect quoted spans coherently
- **First 3 experiments:**
  1. Fine-tune T5 base on QuoteSum, evaluate Sem-F1 and ROUGE-L
  2. Run few-shot PaLM2 with QuoteSum examples, compare against fine-tuned baseline
  3. Swap string-based metrics for a model-based factual consistency scorer and compare rankings

## Open Questions the Paper Calls Out
- How can SEMQA be extended to handle non-English questions and answers, as well as sources in languages other than English?
- What is the optimal balance between extracted factual spans and free-text connectors in SEMQA answers, and how does this balance vary based on question type and complexity?
- How can SEMQA be integrated with a question retrieval system to handle out-of-domain questions or questions without sufficient relevant sources?

## Limitations
- The study relies on human ratings for correctness and comprehensiveness without independent factuality verification
- String-based evaluation metrics may systematically underestimate performance on paraphrased but correct content
- The comparison with abstractive answers is limited to specific question types where multiple sources provide different answers

## Confidence
- **High Confidence:** The feasibility of the semi-extractive format and the value of explicit span attribution for verification
- **Medium Confidence:** The superiority of fine-tuned models over few-shot approaches
- **Medium Confidence:** The effectiveness of string-based metrics for evaluating factual precision and recall

## Next Checks
1. Conduct an independent factuality check on SEMQA answers using a separate gold-standard dataset to validate that improved attribution precision correlates with actual factual correctness
2. Create a controlled evaluation set where correct answers are paraphrased versions of source content to assess whether string-based metrics systematically underestimate model performance on semantically correct but non-exact matches
3. Evaluate SEMQA models on out-of-distribution question types and longer documents to test whether the semi-extractive format maintains its verification advantages when source complexity increases