---
ver: rpa2
title: Finding Lookalike Customers for E-Commerce Marketing
arxiv_id: '2301.03147'
source_url: https://arxiv.org/abs/2301.03147
tags:
- customers
- embedding
- customer
- similarity
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a scalable and efficient system for expanding
  targeted audiences in e-commerce marketing campaigns, handling hundreds of millions
  of customers. The core method involves using a deep learning-based embedding model
  to represent customers and approximate nearest neighbor search to find lookalike
  customers.
---

# Finding Lookalike Customers for E-Commerce Marketing

## Quick Facts
- arXiv ID: 2301.03147
- Source URL: https://arxiv.org/abs/2301.03147
- Reference count: 23
- Primary result: Deep learning-based system achieves 14% improvement in lookalike customer quality (MAE) while handling hundreds of millions of customers

## Executive Summary
This paper presents a scalable system for expanding targeted audiences in e-commerce marketing campaigns by finding lookalike customers. The core approach uses a two-tower deep learning architecture to embed customers into a shared vector space, then applies approximate nearest neighbor search to efficiently retrieve similar customers at massive scale. The system handles multimodal customer features including transactions, visits, engagements, and location data, with transfer learning used for location embeddings. Extensive experiments demonstrate both the scalability and quality improvements of the approach, though at the cost of increased inference latency.

## Method Summary
The system ingests multimodal customer features (transactions, visits, engagements, demographics, and location) and processes them through a two-tower deep learning architecture that generates dense customer embeddings. Location features are encoded using transfer learning from pre-trained models like BERT. These embeddings are indexed using FAISS for efficient approximate nearest neighbor search. During inference, seed customers are transformed to embeddings and FAISS retrieves lookalike customers based on cosine similarity. The model is trained using L1 loss between predicted cosine distances and true similarity metrics across transactions, visits, and engagements.

## Key Results
- Best model setup achieves 14% improvement in quality (MAE) compared to baseline
- BERT-based location encoding provides best quality but increases inference latency 4x
- FAISS enables scalable retrieval from hundreds of millions of customers
- System demonstrates adaptability to different campaigns through interpretable similarity metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-tower architecture enables efficient similarity-based customer expansion
- Mechanism: Multimodal features processed through separate towers generate dense embeddings; cosine similarity approximates business similarity
- Core assumption: Cosine similarity in embedding space correlates with true business similarity metrics
- Evidence anchors: Abstract mentions deep learning embedding model and ANN search; section 4.2 describes cosine distance usage
- Break condition: If embedding space doesn't preserve similarity relationships, nearest neighbor search retrieves irrelevant customers

### Mechanism 2
- Claim: Transfer learning from BERT improves location embedding quality
- Mechanism: Location features encoded as text passed through pre-trained BERT, then fine-tuned to produce dense embeddings
- Core assumption: Pre-trained language models capture semantic relationships in location text that transfer to customer similarity tasks
- Evidence anchors: Section 4.3.1 describes BERT approach; section 5 shows BERT best in quality but 4x latency
- Break condition: If BERT-based embeddings don't improve downstream similarity metrics, added complexity unjustified

### Mechanism 3
- Claim: FAISS enables scalable lookalike customer retrieval
- Mechanism: Customer embeddings indexed using FAISS with optimized data structures for sub-linear search time
- Core assumption: FAISS maintains acceptable recall while drastically reducing search latency vs exhaustive search
- Evidence anchors: Section 2.3 mentions FAISS scalability; section 3 describes building indexes using FAISS
- Break condition: If FAISS parameters misconfigured, recall degrades significantly, leading to poor lookalike quality

## Foundational Learning

- Concept: Cosine similarity as distance metric in high-dimensional embedding spaces
  - Why needed here: System relies on cosine distance between customer embeddings to approximate business similarity
  - Quick check question: What is the range of cosine similarity, and how does it differ from Euclidean distance in high dimensions?

- Concept: Transfer learning for text embeddings
  - Why needed here: Location features encoded as text and transformed using pre-trained models to capture semantic relationships
  - Quick check question: What is the difference between using pre-trained word embeddings versus contextual embeddings like BERT for a location field?

- Concept: Approximate nearest neighbor search algorithms
  - Why needed here: FAISS used to efficiently retrieve similar customers from hundreds of millions of candidates
  - Quick check question: What is the trade-off between recall and query latency in ANN methods like FAISS?

## Architecture Onboarding

- Component map: Feature extraction -> Two-tower embedding model -> FAISS indexer -> Online inference pipeline
- Critical path: 1) Offline: Generate embeddings for all customers → Build FAISS index; 2) Online: Embed seed customers → Query FAISS index → Retrieve and rank lookalike customers
- Design tradeoffs: Embedding dimensionality vs memory usage; BERT vs GloVe for location (quality vs latency); ANN index type (recall vs build time)
- Failure signatures: Low recall in ANN search (check FAISS parameters); poor embedding quality (inspect cosine similarity distribution); high latency (profile embedding and FAISS separately)
- First 3 experiments: 1) Benchmark GloVe vs BERT location encoders on quality and latency; 2) Evaluate FAISS recall@K vs exhaustive search with different index configurations; 3) Test end-to-end latency and quality with varying embedding dimensions (64, 128, 256)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model performance scale with customer universe size using different similarity search methods?
- Basis in paper: [inferred] Paper mentions FAISS scalability but lacks specific performance metrics across different scales
- Why unresolved: Focuses on effectiveness rather than performance scaling
- What evidence would resolve it: Detailed performance benchmarks comparing similarity search methods across various customer data scales

### Open Question 2
- Question: What are long-term impacts on customer retention and acquisition rates?
- Basis in paper: [explicit] Discusses model's ability to find lookalike customers but doesn't address long-term customer behavior
- Why unresolved: Study focuses on immediate campaign performance rather than longitudinal analysis
- What evidence would resolve it: Longitudinal studies measuring retention and acquisition rates before and after implementation

### Open Question 3
- Question: How do different multimodal feature combinations affect model accuracy and interpretability?
- Basis in paper: [explicit] Mentions using multimodal features but doesn't explore impact of different feature combinations
- Why unresolved: Paper doesn't analyze how varying feature sets influence performance or interpretability
- What evidence would resolve it: Experiments comparing performance and interpretability across different multimodal feature combinations

## Limitations
- 14% quality improvement comes at 4x inference latency cost with BERT location encoding
- Study focuses exclusively on e-commerce contexts, limiting domain generalizability
- Does not address cold-start problems for new customers with limited historical data

## Confidence

- **High confidence**: Two-tower architecture and FAISS-based ANN search are well-established techniques with clear implementation details and measurable outcomes
- **Medium confidence**: Transfer learning for location encoding shows improved quality but introduces significant latency trade-offs without detailed ablation studies
- **Low confidence**: Specific formulation of similarity metrics and their correlation with business outcomes relies heavily on domain-specific assumptions not fully validated across contexts

## Next Checks

1. Conduct ablation study comparing BERT vs GloVe vs simple averaged word embeddings for location encoding, measuring both quality (MAE) and latency across multiple customer segments to quantify trade-off space

2. Evaluate model performance on cold-start customers by simulating scenarios with limited historical data and measuring degradation in similarity prediction accuracy compared to fully-featured customers

3. Test system's robustness to distribution shifts by introducing synthetic changes in customer behavior patterns and measuring impact on lookalike customer retrieval quality over time