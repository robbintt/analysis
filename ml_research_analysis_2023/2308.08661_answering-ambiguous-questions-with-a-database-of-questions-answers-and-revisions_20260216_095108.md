---
ver: rpa2
title: Answering Ambiguous Questions with a Database of Questions, Answers, and Revisions
arxiv_id: '2308.08661'
source_url: https://arxiv.org/abs/2308.08661
tags:
- questions
- answers
- passages
- question
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of answering ambiguous questions
  by retrieving diverse information from multiple passages. It introduces a new method
  that generates a large database of unambiguous questions from Wikipedia using a
  question generation model trained on AmbigQA.
---

# Answering Ambiguous Questions with a Database of Questions, Answers, and Revisions

## Quick Facts
- arXiv ID: 2308.08661
- Source URL: https://arxiv.org/abs/2308.08661
- Reference count: 8
- Primary result: Introduces SIXPAQ, a database of generated questions from Wikipedia, to improve retrieval of diverse passages and long-form answer generation for ambiguous questions, achieving 15% relative improvement in recall measures and 10% improvement in disambiguating questions.

## Executive Summary
This paper addresses the challenge of answering ambiguous questions by introducing a novel method that generates a large database of unambiguous questions from Wikipedia passages. The system, named SIXPAQ, uses indirect retrieval by finding passages that generate questions similar to the user's query, leading to more diverse passages and higher answer recall. The approach also incorporates retrieved questions as context in long-form answer generation, improving the quality of generated answers. Experimental results show significant improvements on AmbigQA and WebQuestionsSP datasets, demonstrating the effectiveness of using generated questions as a bridge for retrieval and context in ambiguous question answering.

## Method Summary
The method consists of three main stages: (1) Generating questions from Wikipedia passages using a T5 model trained on AmbigQA, where answers are detected using a T5 model finetuned on NQ and verified using a T5 model finetuned on SQuAD v2; (2) Indirect retrieval of passages by retrieving generated questions similar to the user's query using BM25 or GTR, and mapping them to their source passages using either max or count scoring; (3) Long-form answer generation using a T5 model with retrieved passages and revised questions as context, where questions are revised to include more specific information using a T5 model trained on ASQA data.

## Key Results
- Indirect retrieval with SIXPAQ improves recall@10 from 61.9 to 63.4 on AmbigQA and from 46.3 to 53.0 on WebQuestionsSP compared to passage-based retrieval methods.
- Incorporating retrieved questions as context in long-form answer generation increases ROUGE-L by 0.8 and DISAMBIG-F1 by 2.5, while reducing output length by ~10% compared to using passages alone.
- The method achieves a 15% relative improvement in recall measures and a 10% improvement in measures evaluating disambiguating questions from predicted outputs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating questions from Wikipedia passages creates a large database of unambiguous questions that can be used to indirectly retrieve passages relevant to ambiguous user queries.
- Mechanism: The system trains a T5 model on AmbigQA data to generate questions conditioned on detected answers from Wikipedia passages. These generated questions are then used as a bridge to retrieve relevant passages by finding passages that generate questions similar to the user's query.
- Core assumption: Questions generated from passages that are similar to the user's query will lead to passages containing diverse answers to the original ambiguous question.
- Evidence anchors:
  - [abstract] "This database, named SIXPAQ, is then used to indirectly retrieve passages relevant to a question by finding passages that generate questions similar to the query question."
  - [section 3.1] Describes the three-stage process of answer detection, question generation, and answer verification to construct the database.
  - [corpus] Found 25 related papers with average neighbor FMR=0.494, suggesting moderate relevance of the approach to existing literature on ambiguous question answering.
- Break condition: If the generated questions are not sufficiently diverse or do not capture the various interpretations of the ambiguous question, the indirect retrieval approach will fail to find relevant passages with diverse answers.

### Mechanism 2
- Claim: Using the database of generated questions improves diverse passage retrieval and answer recall compared to traditional passage-based retrieval methods.
- Mechanism: The system maps top-retrieved questions from the database to their source passages using either a "max" approach (selecting passages that generate the most similar questions) or a "count" approach (selecting passages that generate the most top-retrieved questions). This indirect retrieval method leads to more diverse passages and higher answer recall.
- Core assumption: Passages that generate questions similar to the user's query are more likely to contain diverse answers to the ambiguous question than passages retrieved directly using traditional methods.
- Evidence anchors:
  - [section 4.1.3] "Compared to passage-based retrieval methods, indirect retrieval with SIXPAQ yields better performance than using BM25 or GTR."
  - [section 4.1.3] "The recall@10 improves 61.9 to 63.4 on AmbigQA and from 46.3 to 53.0 on WebQuestionsSP."
  - [corpus] The related work section mentions that prior QA-memory based models focused on unambiguous questions, highlighting the novelty of this approach for ambiguous questions.
- Break condition: If the mapping between generated questions and source passages is not accurate or if the retrieved questions do not cover the various interpretations of the ambiguous question, the performance gains will not materialize.

### Mechanism 3
- Claim: Incorporating the generated questions as context in the long-form answer generation process improves the quality of the generated answers for ambiguous questions.
- Mechanism: The system retrieves questions from the database, revises them to include more specific information, and adds them to the input of a T5 model that generates long-form answers. This provides the model with more concise and relevant information compared to using retrieved passages alone.
- Core assumption: The revised questions contain more specific and relevant information about the answers to the ambiguous question than the retrieved passages, leading to better long-form answers.
- Evidence anchors:
  - [section 3.3.2] "To mitigate this problem, we follow the baseline to also include top n passages retrieved by JPR from Wikipedia."
  - [section 4.2.3] "Without changing the model, the performance of long-answer generation with information retrieved from SIXPAQ increases by 0.8 in ROUGE-L and 2.5 in DISAMBIG-F1."
  - [section 4.2.3] "In addition, the output length of the model with information from SIXPAQ is also ∼10% shorter than the baseline but covers more answers in its outputs."
- Break condition: If the question revision step does not effectively add relevant information or if the model cannot effectively utilize the revised questions, the quality of the generated long-form answers may not improve.

## Foundational Learning

- Concept: Question generation conditioned on answers
  - Why needed here: The system generates questions conditioned on detected answers from Wikipedia passages to create a database of unambiguous questions that can be used for indirect passage retrieval.
  - Quick check question: What is the purpose of conditioning the question generation on detected answers, and how does this differ from unconditional question generation?

- Concept: Indirect retrieval via generated questions
  - Why needed here: The system uses the generated questions as a bridge to retrieve relevant passages indirectly, which leads to more diverse passages and higher answer recall compared to traditional passage-based retrieval methods.
  - Quick check question: How does the indirect retrieval approach work, and why is it expected to outperform traditional passage-based retrieval for ambiguous questions?

- Concept: Question revision for long-form answer generation
  - Why needed here: The system revises the retrieved questions to include more specific information, which is then used as context in the long-form answer generation process to improve the quality of the generated answers.
  - Quick check question: What is the purpose of the question revision step, and how does it contribute to the improvement in long-form answer generation?

## Architecture Onboarding

- Component map: User query -> Retrieval of generated questions -> Mapping to source passages -> Question revision -> Long-form answer generation
- Critical path: User query → Retrieval of generated questions → Mapping to source passages → Question revision → Long-form answer generation
- Design tradeoffs:
  - Using generated questions as a bridge for indirect retrieval vs. traditional passage-based retrieval: The indirect approach may be more effective for ambiguous questions but requires additional steps and computational overhead.
  - Conditioning question generation on detected answers vs. unconditional generation: Conditioning may lead to more specific and relevant questions but may miss some potential interpretations of the ambiguous question.
  - Using revised questions as context vs. retrieved passages: Revised questions may provide more concise and relevant information but may miss some background context present in the passages.
- Failure signatures:
  - Low recall of answers in the retrieved passages: Indicates that the generated questions or the mapping to source passages may not be effective in capturing the various interpretations of the ambiguous question.
  - Poor performance in long-form answer generation: Suggests that the revised questions may not be providing sufficient or relevant information for the model to generate high-quality answers.
  - High computational overhead: May indicate that the indirect retrieval approach is not efficient enough for practical use.
- First 3 experiments:
  1. Evaluate the diversity of answers retrieved using the indirect retrieval approach with generated questions compared to traditional passage-based retrieval methods.
  2. Assess the impact of the question revision step on the quality of the generated long-form answers by comparing the performance with and without revised questions.
  3. Investigate the effectiveness of different mapping strategies (max vs. count) for mapping retrieved questions to source passages and their impact on answer recall.

## Open Questions the Paper Calls Out
- How does the performance of the proposed method compare to state-of-the-art models on other open-domain question answering datasets with ambiguous questions, such as Temp-LAMA, GeoQuery, and ConditionalQA?
- Can the proposed method be extended to handle more complex forms of ambiguity, such as temporal or spatial constraints?
- How does the proposed method perform in a zero-shot setting, where the model is not fine-tuned on the target dataset?

## Limitations
- The approach relies heavily on the quality and diversity of the generated question database, which may not fully capture all possible interpretations of ambiguous questions.
- The performance of the indirect retrieval method is dependent on the accuracy of the mapping between generated questions and source passages.
- The question revision step may not always effectively add relevant information to the retrieved questions, potentially limiting the improvement in long-form answer generation.

## Confidence
- High: The claim that the proposed method improves answer recall and diverse passage retrieval compared to traditional methods, as supported by quantitative results on AmbigQA and WebQuestionsSP datasets.
- Medium: The claim that incorporating revised questions as context improves long-form answer generation, as the improvement is observed but may be sensitive to the quality of the question revision step.
- Low: The generalizability of the approach to other domains and types of ambiguous questions beyond the evaluated datasets, as the performance may vary depending on the specific characteristics of the questions and available knowledge sources.

## Next Checks
1. Evaluate the performance of the proposed method on additional datasets with ambiguous questions from different domains to assess its generalizability.
2. Conduct ablation studies to isolate the contribution of each component (answer detection, question generation, indirect retrieval, question revision) to the overall performance and identify potential bottlenecks.
3. Analyze the quality and diversity of the generated questions in the database by manually inspecting a sample and assessing their relevance to the original ambiguous questions.