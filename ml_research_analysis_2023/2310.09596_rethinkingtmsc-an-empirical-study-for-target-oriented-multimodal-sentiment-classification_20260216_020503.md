---
ver: rpa2
title: 'RethinkingTMSC: An Empirical Study for Target-Oriented Multimodal Sentiment
  Classification'
arxiv_id: '2310.09596'
source_url: https://arxiv.org/abs/2310.09596
tags:
- sentiment
- multimodal
- image
- text
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts an empirical study on Target-oriented Multimodal
  Sentiment Classification (TMSC), aiming to understand the causes behind the performance
  bottleneck of current multimodal models. The study analyzes the importance of different
  modalities, the effectiveness of multimodal fusion modules, and the adequacy of
  existing datasets.
---

# RethinkingTMSC: An Empirical Study for Target-Oriented Multimodal Sentiment Classification

## Quick Facts
- arXiv ID: 2310.09596
- Source URL: https://arxiv.org/abs/2310.09596
- Reference count: 19
- Primary result: Text-only models outperform multimodal models on TMSC due to dataset limitations and textual modality dominance

## Executive Summary
This paper conducts an empirical study on Target-oriented Multimodal Sentiment Classification (TMSC), investigating why current multimodal models fail to significantly outperform text-only baselines. Through systematic experiments on Twitter15 and Twitter17 datasets, the authors analyze the relative importance of textual and visual modalities, the effectiveness of different fusion strategies, and the adequacy of existing datasets for TMSC tasks. The study reveals that textual modality dominates TMSC performance, fusion modules provide marginal gains, and dataset quality severely limits multimodal model effectiveness due to lack of target-relevant visual information.

## Method Summary
The study evaluates TMSC performance using BERT for text encoding, ResNet-152/ViT/Faster R-CNN for image encoding, and six fusion strategies (Concatenate, Tensor Fusion, Self Attention, Image2Text, Text2Image, Bi-direction). Models are trained on Twitter15 and Twitter17 datasets with Adam optimizer (lr=2e-5) for 8 epochs, using accuracy and F1-score as metrics. The experiments compare unimodal (text-only, image-only) and multimodal performance across different image encoders and fusion approaches, while analyzing dataset characteristics regarding target presence in images.

## Key Results
- Text-only BERT models consistently outperform all visual-only models (ResNet, ViT, Faster R-CNN)
- Multimodal models achieve only marginal improvements over text-only baselines
- Image2Text fusion strategies perform better than Text2Image approaches
- Large portions of images lack relevant targets or sentiment information
- Multimodal sentiment consistency is high with text but low with visual modality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Textual modality is more crucial than visual modality for TMSC performance.
- Mechanism: Text-only BERT model achieves higher accuracy than visual-only models, and multimodal fusion improvements are marginal because text dominates the decision process.
- Core assumption: Sentiment for most targets can be reliably determined from text alone.
- Evidence anchors:
  - [abstract]: "Our experiments and analyses reveal that the current TMSC systems primarily rely on the textual modality, as most of targets’ sentiments can be determined solely by text."
  - [section]: "the text-only model (i.e., BERT) performs well, while the visual-only models... perform relatively poorly, revealing that the reliance on text is much greater than that on images for the TMSC task."

### Mechanism 2
- Claim: Fusion modules focused on acquiring textual information (Image2Text) outperform those focused on visual information (Text2Image).
- Mechanism: Cross-attention mechanisms that prioritize text-based queries better exploit the dominance of textual cues in current datasets.
- Core assumption: Current datasets are text-biased, so fusion methods must adapt to this imbalance.
- Evidence anchors:
  - [section]: "fusion modules that primarily focus on acquiring the textual information (e.g., Image2Text) perform better than those focused on acquiring the visual information (e.g., Text2Image)."

### Mechanism 3
- Claim: Dataset quality limits multimodal model gains because many images lack relevant targets or sentiment information.
- Mechanism: High proportion of images missing targets or containing irrelevant content causes multimodal models to add noise rather than useful context.
- Core assumption: Model performance is bounded by dataset characteristics; if images don't support the target sentiment, fusion cannot help.
- Evidence anchors:
  - [section]: "a large number of targets do not exist in images... only a small portion of the data where the sentiment is determined by both text and images."

## Foundational Learning

- Concept: Multimodal fusion strategies (concatenation, tensor fusion, self-attention, cross-attention)
  - Why needed here: Understanding how different fusion methods combine modalities is key to diagnosing why some outperform others.
  - Quick check question: Which fusion method in the paper consistently achieved the highest performance and why?

- Concept: Dataset bias and its impact on model evaluation
  - Why needed here: Recognizing that dataset imbalances (text-dominant, missing targets) can mask true multimodal potential is essential for interpreting results.
  - Quick check question: What percentage of images in the analyzed datasets lacked the target for sentiment analysis?

- Concept: Cross-modal attention mechanisms
  - Why needed here: Image2Text and Text2Image rely on cross-attention; understanding their directional differences explains performance gaps.
  - Quick check question: In Image2Text, which modality serves as the query and which as the key/value?

## Architecture Onboarding

- Component map: Text (BERT) -> Text features; Image (ResNet/ViT/Faster R-CNN) -> Image features; Fusion module -> Combined features -> Sentiment classifier
- Critical path: Image → Image encoder → Pooled image features → Fusion module → Text features → Fusion module → Classifier
- Design tradeoffs: Simpler fusion (concatenate) vs. more complex (tensor fusion, attention) — complexity doesn't guarantee gains if data is imbalanced.
- Failure signatures: Marginal gains over text-only model, high variance across image encoders, multimodal model sometimes underperforms text-only.
- First 3 experiments:
  1. Replace BERT with a smaller text encoder and compare performance drop to isolate text contribution.
  2. Train a multimodal model on a synthetic balanced dataset to test if gains emerge when images are relevant.
  3. Implement and test a noise-robust fusion module (e.g., gated attention) to see if it mitigates irrelevant image effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different text encoding methods impact the performance of TMSC models?
- Basis in paper: [inferred] The paper acknowledges that different text encoding methods may impact model performance but focused on image encoding methods and fusion modules, assuming BERT's adequacy.
- Why unresolved: The paper chose to focus on image encoding and fusion modules, assuming BERT's effectiveness, without directly comparing different text encoding methods.
- What evidence would resolve it: A comparative study of TMSC models using different text encoding methods (e.g., RoBERTa, ALBERT) while keeping image encoding and fusion modules constant.

### Open Question 2
- Question: What are the characteristics of high-quality TMSC datasets that better capture the multimodal nature of social media sentiments?
- Basis in paper: [explicit] The paper identifies limitations in existing datasets and proposes that high-quality TMSC datasets should accurately reflect real-world data distribution, have large data diversity, and provide multi-dimensional annotation information.
- Why unresolved: The paper suggests these characteristics but does not construct a new dataset to validate them.
- What evidence would resolve it: Development and evaluation of a new TMSC dataset that meets the proposed characteristics and comparison of model performance on this dataset versus existing ones.

### Open Question 3
- Question: How can we effectively incorporate visual information in TMSC models to improve performance?
- Basis in paper: [explicit] The paper finds that current multimodal models do not achieve significant performance gains over text-only models, partly due to the over-reliance on textual modality and the introduction of noise from visual information.
- Why unresolved: The paper identifies the issue but does not provide a solution for effectively incorporating visual information.
- What evidence would resolve it: Development and evaluation of TMSC models with novel fusion strategies or noise immunity mechanisms that significantly improve performance over text-only models.

## Limitations

- Dataset Representativeness: Findings may be specific to Twitter15/Twitter17 and may not generalize to other multimodal sentiment datasets.
- Fusion Strategy Implementation Details: Paper lacks full technical specifications for fusion strategies, which could affect comparative results.
- Target-Entity Alignment: Paper does not fully describe preprocessing pipeline for linking targets to image content, potentially introducing errors.

## Confidence

**High Confidence**:
- Textual modality consistently outperforms visual modality across all tested models and datasets.
- Multimodal fusion provides marginal improvements over text-only baselines in current TMSC datasets.
- A significant portion of images in Twitter15 and Twitter17 do not contain relevant targets or sentiment information.

**Medium Confidence**:
- Image2Text fusion strategies outperform Text2Image strategies due to the text-dominant nature of current datasets.
- The performance bottleneck in TMSC is primarily due to dataset limitations rather than model architecture deficiencies.

**Low Confidence**:
- The specific ranking of fusion strategies may vary with different implementation details.
- The absolute percentage of images lacking relevant targets, as the annotation process is not fully detailed.

## Next Checks

1. **Dataset Expansion Validation**: Test the same model architectures and fusion strategies on a more visually rich multimodal sentiment dataset (e.g., CMU-MOSI or MELD) to determine if the textual dominance pattern holds when images contain more target-relevant information.

2. **Synthetic Dataset Experiment**: Create a balanced synthetic dataset where every image contains clear, relevant visual sentiment cues for the target, then retrain the multimodal models to test if performance gains over text-only models emerge when visual information is guaranteed to be relevant.

3. **Target-Entity Alignment Audit**: Conduct a manual annotation study on a random sample of 100-200 images from Twitter15/17 to verify the accuracy of target-entity detection and alignment with image regions, measuring the precision of this preprocessing step that the paper treats as given.