---
ver: rpa2
title: Discovering environments with XRM
arxiv_id: '2309.16748'
source_url: https://arxiv.org/abs/2309.16748
tags:
- training
- environments
- environment
- arxiv
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CROSS-RISK MINIMIZATION (XRM), a method for
  discovering environments in data without requiring human annotations. The core idea
  is to train two twin networks, each learning from one random half of the training
  data, while imitating confident held-out mistakes made by its sibling.
---

# Discovering environments with XRM

## Quick Facts
- arXiv ID: 2309.16748
- Source URL: https://arxiv.org/abs/2309.16748
- Authors: 
- Reference count: 40
- Primary result: XRM discovers environments without human annotations, achieving 87% worst-group-accuracy on Waterbirds matching oracle methods

## Executive Summary
This paper introduces CROSS-RISK MINIMIZATION (XRM), a method for discovering environments in data without requiring human annotations. The core idea is to train two twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling. This creates an "echo-chamber" where the twins increasingly rely on spurious correlations, allowing XRM to identify environments that differ in spurious correlation. The method provides a recipe for hyper-parameter tuning, does not require early-stopping, and can discover environments for all training and validation data.

## Method Summary
XRM trains twin networks that each learn from one random half of the training data while imitating confident held-out mistakes made by its sibling. The twins are calibrated via Platt scaling and then trained with a label flipping mechanism that increases reliance on spurious correlations. After training, XRM uses a simple "cross-mistake" formula to discover environment annotations for all training and validation examples. The method is hyperparameter-tuned based on the total number of label flips, which correlates with the ability to discover useful environments.

## Key Results
- Achieves 87% worst-group-accuracy on Waterbirds dataset, matching oracle methods with human annotations
- Provides first environment discovery method that doesn't require early stopping or human annotations
- Enables domain generalization algorithms to achieve oracle-like performance across six standard benchmarks
- Discovers environments for all training and validation data, not just training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XRM discovers environments by training twin networks that absorb spurious correlations through imitation of held-out mistakes.
- Mechanism: Each twin network holds out half the training data, computes held-out softmax predictions, and then imitates confident mistakes made by its sibling. This "echo-chamber" effect forces the twins to increasingly rely on spurious correlations rather than invariant features.
- Core assumption: Spurious correlations are easier to learn and lead to confident predictions on held-out data, while invariant features are more robust across data splits.
- Evidence anchors:
  - [abstract] "XRM trains twin networks, each learning from one random half of the training data, while imitating confident held-out mistakes made by its sibling."
  - [section] "Label flipping happens more often for confident held-out mistakes and early in training. These are two footprints of spurious correlations, since these are often easier and faster to capture."
- Break condition: If spurious correlations are not easier to learn than invariant features, the label flipping mechanism may not effectively differentiate environments.

### Mechanism 2
- Claim: The "cross-mistake" formula effectively discovers environments by leveraging the twin networks' complementary error patterns.
- Mechanism: After training, each example is assigned to an environment based on whether it is misclassified by either twin. This creates environments that differ in spurious correlation while sharing invariant patterns.
- Core assumption: The twin networks trained with XRM will make complementary errors - one will correctly classify examples with spurious features while the other will make mistakes on those same examples.
- Evidence anchors:
  - [abstract] "Given the selected twins, XRM employs a simple 'cross-mistake' formula to discover environment annotations for all of the training and validation examples."
  - [section] "If a point is misclassified when held-out, we see this as evidence of such example belonging to the minority group."
- Break condition: If the twin networks converge to make similar errors on the same examples, the cross-mistake formula will not effectively separate environments.

### Mechanism 3
- Claim: XRM's label flipping mechanism creates an adversarial training dynamic that prevents twins from converging to ERM-like solutions.
- Mechanism: The probability of flipping a label is proportional to the confidence of the held-out prediction minus uniform random guessing. This actively pushes the twins away from minimizing training error and toward maximizing reliance on spurious features.
- Core assumption: Confidence in predictions correlates with reliance on spurious features rather than invariant features.
- Evidence anchors:
  - [section] "flip yi into yout i, with probability (pout yout i − 1/nclasses) · nclasses/(nclasses − 1)"
  - [section] "Label flipping happens more often for confident held-out mistakes and early in training."
- Break condition: If confidence in predictions does not correlate with spurious feature reliance, the label flipping mechanism may not effectively differentiate environments.

## Foundational Learning

- Concept: Domain Generalization
  - Why needed here: XRM is designed to solve domain generalization problems by discovering environments that capture spurious correlations.
  - Quick check question: What is the key difference between domain generalization and standard supervised learning?

- Concept: Spurious Correlation
  - Why needed here: Understanding spurious correlations is crucial for understanding how XRM works to discover environments that differ in these correlations.
  - Quick check question: Why do machine learning models tend to latch onto spurious correlations during training?

- Concept: Invariant Risk Minimization
  - Why needed here: XRM builds on principles from invariant risk minimization by seeking to discover environments that differ in spurious correlation while sharing invariant patterns.
  - Quick check question: What is the invariance principle in the context of out-of-distribution generalization?

## Architecture Onboarding

- Component map:
  - Twin network initialization (Section 4.1) -> Cross-mistake formula for environment discovery (Section 4.4) -> Label flipping mechanism during training (Section 4.2) -> Hyperparameter selection based on flip counts (Section 4.3)

- Critical path:
  1. Initialize two twin networks with random weights
  2. Assign training examples to held-in/held-out sets for each twin
  3. Calibrate softmax temperatures via Platt scaling
  4. Train twins while flipping labels according to confidence-based probability
  5. Select best hyperparameters based on total number of label flips
  6. Apply cross-mistake formula to discover environments for all data

- Design tradeoffs:
  - Using twin networks adds computational overhead but enables environment discovery without human annotations
  - Label flipping may slow convergence but prevents twins from learning ERM-like solutions
  - The method requires no early stopping but needs careful hyperparameter tuning

- Failure signatures:
  - If twins make similar errors on the same examples, cross-mistake formula won't separate environments effectively
  - If label flipping doesn't correlate with spurious feature reliance, the method may not discover useful environments
  - If the function class of twins is too different from downstream DG model, discovered environments may not be optimal

- First 3 experiments:
  1. Run XRM on a simple synthetic dataset with known spurious correlations and verify that discovered environments match ground truth
  2. Compare XRM-discovered environments to human annotations on Waterbirds dataset and measure worst-group accuracy
  3. Test sensitivity of XRM to the label flipping probability parameter by varying it across multiple runs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does XRM relate to the invariance principle Y ⊥ E | Φ(X), and what is the interplay between revealing relevant labels Y and relevant environments E to afford invariance?
- Basis in paper: Explicit - The authors mention this as a direction for future work, noting that XRM is the first environment discovery algorithm tampering with labels Y, thus exploring invariance from a new angle.
- Why unresolved: The paper acknowledges that the theoretical analysis of XRM will call for new tools because the optimal solution is a moving target that depends on the dynamic evolution of the labels, steering away from the Bayes-optimal predictor P(Y|X).
- What evidence would resolve it: Developing a theoretical framework that connects XRM's label flipping mechanism to the invariance principle, and empirically validating this connection through experiments that measure invariance under different environmental shifts.

### Open Question 2
- Question: What is the relationship between XRM and the phenomenon of memorization, particularly the two flavors of memorization: good memorization (which affords invariance) and bad memorization (structured over-fitting)?
- Basis in paper: Explicit - The authors highlight this as another direction for future work, questioning how XRM specifically relates to these two flavors of memorization and whether it discovers environments that promote features beneficial to all examples.
- Why unresolved: The paper does not provide a detailed analysis of XRM's relationship with memorization, only posing it as an open question for further investigation.
- What evidence would resolve it: Conducting experiments that compare XRM's performance with and without memorization-inducing factors, and analyzing the features learned by XRM to determine if they are universally beneficial or if they exhibit signs of structured over-fitting.

### Open Question 3
- Question: How does the performance of XRM vary with different choices of twin network architectures and the size of the held-in/held-out splits?
- Basis in paper: Inferred - While the paper discusses the general approach of using twin networks and holding out data, it does not explore the impact of different network architectures or split ratios on XRM's performance.
- Why unresolved: The experimental section focuses on using the same architecture for twins as the downstream DG predictor and a 50-50 split, without exploring variations in these choices.
- What evidence would resolve it: Conducting a systematic study that varies the twin network architectures (e.g., different depths, widths, or types of networks) and the held-in/held-out split ratios, and measuring the impact on XRM's ability to discover environments and the downstream DG performance.

## Limitations

- XRM's effectiveness depends on the assumption that spurious correlations are easier to learn than invariant features, which may not hold in all datasets
- The method requires training twin networks, doubling computational cost compared to standard ERM
- Results are benchmark-specific and may not generalize to real-world applications with more complex spurious correlations

## Confidence

- High Confidence: The core mechanism of twin network training with label flipping
- Medium Confidence: Oracle-like performance claims
- Medium Confidence: No early stopping requirement

## Next Checks

1. **Cross-dataset robustness**: Test XRM on additional datasets with different types of spurious correlations (e.g., temporal shifts, domain shifts) to verify generalizability
2. **Ablation on twin network architecture**: Compare XRM performance using different model architectures for twins versus downstream DG model to test the "function class" assumption
3. **Analysis of flip dynamics**: Track the evolution of label flipping probabilities during training to verify that they consistently correlate with spurious feature learning across all benchmarks