---
ver: rpa2
title: Active Policy Improvement from Multiple Black-box Oracles
arxiv_id: '2306.10259'
source_url: https://arxiv.org/abs/2306.10259
tags:
- policy
- oracle
- state
- learning
- maps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a sample-efficient policy improvement method
  called MAPS and its variant MAPS-SE, which leverage active policy selection and
  state exploration to learn from multiple suboptimal black-box oracles. MAPS outperforms
  the state-of-the-art method MAMBA by actively selecting which oracle to roll out
  and improve their value function estimates.
---

# Active Policy Improvement from Multiple Black-box Oracles

## Quick Facts
- arXiv ID: 2306.10259
- Source URL: https://arxiv.org/abs/2306.10259
- Authors: 
- Reference count: 40
- Primary result: MAPS and MAPS-SE achieve better sample efficiency than MAMBA by actively selecting oracles and states to explore

## Executive Summary
This paper introduces MAPS and MAPS-SE, two sample-efficient methods for active policy improvement from multiple suboptimal black-box oracles. MAPS uses upper confidence bounds to actively select which oracle to roll out at each state, while MAPS-SE further incorporates an active state exploration criterion to decide which states to explore. Theoretical analysis and empirical results on DeepMind Control Suite tasks demonstrate the sample efficiency advantages of MAPS and MAPS-SE over MAMBA and other baselines.

## Method Summary
MAPS and MAPS-SE are designed for the online imitation learning setting with multiple black-box oracle policies. MAPS actively selects which oracle to roll out based on upper confidence bounds (UCB), while MAPS-SE additionally uses an uncertainty threshold to decide which states to explore. Both methods use PPO-style policy updates and maintain separate value function estimators for each oracle. The key idea is to leverage the expertise of multiple oracles more efficiently than uniform policy selection (MAMBA) by focusing on the most promising oracles and states.

## Key Results
- MAPS and MAPS-SE outperform MAMBA and other baselines on sample efficiency in DeepMind Control Suite tasks
- MAPS achieves a significant reduction in sample complexity of scale K compared to uniform policy selection
- MAPS-SE further improves sample efficiency by actively exploring uncertain states and reducing gradient variance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAPS improves sample efficiency by actively selecting which oracle to roll out based on upper confidence bounds (UCB).
- Mechanism: At each state, MAPS computes a UCB value for each oracle that balances exploitation (estimated value) and exploration (uncertainty bonus). It then selects the oracle with the highest UCB, ensuring more data is collected from the oracle most likely to be optimal for that state.
- Core assumption: The oracle with the highest UCB at a state is more likely to be the best-performing oracle for that state.
- Evidence anchors:
  - [abstract]: "MAPS actively selects which of the oracles to imitate and improve their value function estimates"
  - [section 4.1]: "MAPS incorporates the concept of the upper confidence bound (UCB) to determine which oracle should be selected during the online learning process"
  - [corpus]: Weak - No direct corpus evidence found for UCB-based oracle selection in this specific context.

### Mechanism 2
- Claim: MAPS-SE further improves sample efficiency by actively deciding which states to explore using an uncertainty threshold.
- Mechanism: MAPS-SE maintains an uncertainty measure for each state's predicted value. It only switches to an oracle roll-out if the oracle's uncertainty exceeds a threshold Γs. This prevents unnecessary exploration in states where the learner already has high confidence.
- Core assumption: The learner's policy provides sufficient information to estimate state-wise oracle value uncertainty, and the threshold Γs appropriately balances exploration and exploitation.
- Evidence anchors:
  - [abstract]: "MAPS-SE additionally leverages an active state exploration criterion to determine which states one should explore"
  - [section 4.2]: "MAPS-SE decides whether to proceed with the roll-out using the selected oracle policy... based on an uncertainty measure for the current state"
  - [corpus]: Weak - No direct corpus evidence found for this specific active state exploration strategy.

### Mechanism 3
- Claim: The combination of active policy selection and state exploration in MAPS-SE reduces the variance of gradient estimates compared to MAMBA.
- Mechanism: By actively selecting better oracles and exploring uncertain states, MAPS-SE reduces the approximation error in the estimated max-aggregation value function fmax. This leads to more accurate gradient estimates, improving learning speed and sample efficiency.
- Core assumption: The variance of gradient estimates is directly related to the accuracy of the fmax approximation, and reducing this variance improves sample efficiency.
- Evidence anchors:
  - [section 3.2]: "The approximation error then affects the estimation of fmax(·), which is essential for computing the policy gradient... Therefore, the learning speed... plays a crucial role in determining the sample efficiency"
  - [section 6.3]: "MAPS-SE significantly decreases the standard deviation of the predicted value at switching states"
  - [corpus]: Weak - No direct corpus evidence found for the specific relationship between fmax approximation error and gradient variance in this context.

## Foundational Learning

- Concept: Upper Confidence Bound (UCB) algorithm for balancing exploration and exploitation
  - Why needed here: UCB is the core mechanism for actively selecting the best oracle at each state in MAPS
  - Quick check question: In a multi-armed bandit problem with 3 arms, if arm 1 has estimated value 0.8 with 10 samples and arm 2 has estimated value 0.9 with 100 samples, which arm would UCB select if the exploration bonus is proportional to 1/√n?

- Concept: Generalized Advantage Estimation (GAE) for policy gradient methods
  - Why needed here: GAE is used to compute advantage estimates for the policy update in MAPS-SE, combining the benefits of PPO and GAE
  - Quick check question: How does GAE reduce the variance of advantage estimates compared to using n-step returns directly?

- Concept: Online learning with bandit feedback and regret minimization
  - Why needed here: MAPS and MAPS-SE can be viewed as online learning algorithms where the learner interacts with oracles and receives bandit feedback on the loss
  - Quick check question: In the online learning setting, what is the difference between regret and cumulative loss, and how does regret minimization relate to learning a good policy?

## Architecture Onboarding

- Component map:
  - Learner policy (πn) -> Oracle policies (πk) -> Value function estimators (ˆV k) -> UCB selector -> Uncertainty measure -> Data buffers

- Critical path:
  1. Roll in learner policy until switching state (determined by UCB or uncertainty threshold)
  2. Select oracle via UCB (MAPS) or continue rolling in learner (MAPS-SE)
  3. Roll out selected oracle to collect data and update its value function estimate
  4. Roll out learner policy to collect data for policy update
  5. Compute gradient estimates using collected data
  6. Update learner policy via PPO-style policy gradient

- Design tradeoffs:
  - Fixed vs. variable data buffer sizes for oracle and learner roll-outs
  - Choice of exploration bonus function in UCB (e.g., proportional to 1/√n vs. ensemble variance)
  - Trade-off between exploration (selecting uncertain oracles) and exploitation (selecting high-value oracles)
  - Hyperparameter tuning for the uncertainty threshold Γs in MAPS-SE

- Failure signatures:
  - Poor oracle selection: If the UCB estimates are inaccurate, MAPS may frequently select suboptimal oracles, leading to slow learning
  - Over-exploration: If Γs is set too low in MAPS-SE, the learner may waste samples exploring states with low uncertainty
  - Under-exploration: If Γs is set too high in MAPS-SE, the learner may never explore new states, leading to poor generalization
  - Value function approximation errors: If the oracle value function estimators are inaccurate, the fmax approximation will be poor, leading to high gradient variance

- First 3 experiments:
  1. Compare MAPS against MAMBA on a simple grid-world environment with 2 oracles, measuring sample efficiency and final performance
  2. Evaluate the effect of different exploration bonus functions (e.g., 1/√n vs. ensemble variance) on MAPS oracle selection and learning speed
  3. Test MAPS-SE with varying uncertainty thresholds Γs on a continuous control task, measuring the trade-off between exploration and exploitation

## Open Questions the Paper Calls Out
- Question: How does the performance of MAPS and MAPS-SE scale with the number of oracles (K) in the set?
- Question: How sensitive is MAPS-SE to the choice of the uncertainty threshold Γs?
- Question: How does the performance of MAPS and MAPS-SE compare to other state-of-the-art methods for learning from multiple oracles, such as ILEED?

## Limitations
- Theoretical analysis assumes accurate value function estimators and oracle policies, which may not hold in practice
- Empirical evaluation is limited to a small set of DeepMind Control Suite tasks
- No detailed ablation study on the impact of key hyperparameters

## Confidence
- Confidence: Medium - Theoretical assumptions may not hold in practice, and regret bounds are derived under finite oracle assumptions
- Confidence: Low - No detailed ablation study on hyperparameter sensitivity
- Confidence: Medium - Limited empirical evaluation on diverse environments

## Next Checks
1. Conduct a thorough ablation study to understand the impact of key hyperparameters, such as the uncertainty threshold Γs and the exploration bonus function, on the performance of MAPS and MAPS-SE.
2. Evaluate MAPS and MAPS-SE on a wider range of environments, including tasks with different characteristics and domains, to assess generalizability.
3. Compare MAPS and MAPS-SE against other state-of-the-art methods for imitation and reinforcement learning, such as DQfD, AWAC, and ILPO, to provide a comprehensive understanding of their relative strengths and weaknesses.