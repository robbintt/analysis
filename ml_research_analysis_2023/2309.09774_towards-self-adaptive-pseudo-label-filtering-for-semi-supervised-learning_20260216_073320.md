---
ver: rpa2
title: Towards Self-Adaptive Pseudo-Label Filtering for Semi-Supervised Learning
arxiv_id: '2309.09774'
source_url: https://arxiv.org/abs/2309.09774
tags:
- labels
- pseudo
- confidence
- data
- filtering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-Adaptive Pseudo-Label Filtering (SPF) dynamically adjusts
  pseudo-label filtering during semi-supervised learning using an online Beta Mixture
  Model to model confidence distributions. This adaptive approach outperforms constant-threshold
  filtering and manual progressive thresholding, particularly with very limited labeled
  data.
---

# Towards Self-Adaptive Pseudo-Label Filtering for Semi-Supervised Learning

## Quick Facts
- arXiv ID: 2309.09774
- Source URL: https://arxiv.org/abs/2309.09774
- Reference count: 34
- Key outcome: SPF dynamically adjusts pseudo-label filtering using online Beta Mixture Model, reducing error rates by up to 6.6% on CIFAR-100 with 400 labels

## Executive Summary
Self-Adaptive Pseudo-Label Filtering (SPF) addresses the challenge of filtering pseudo-labels in semi-supervised learning, particularly when labeled data is scarce. The method uses a Beta Mixture Model to dynamically model the confidence distribution of pseudo-labels, adapting the filtering process as training progresses. This adaptive approach prevents early overfitting and maintains high-quality pseudo-labels throughout training, outperforming static threshold methods.

## Method Summary
SPF is a plug-and-play component for semi-supervised learning that replaces constant confidence thresholds with dynamic filtering weights. During each epoch, confidence scores from pseudo-labels are collected and used to update a Beta Mixture Model at epoch's end. The BMM models the confidence distribution as two Beta distributions (correct vs incorrect labels), and in the next epoch, each pseudo-label is weighted by the posterior probability of being correct. This soft weighting is integrated into the SSL loss function, providing continuous adaptation without manual tuning.

## Key Results
- SPF reduces error rates by up to 6.6% on CIFAR-100 with 400 labels compared to FixMatch-RA baseline
- Improves MeanTeacher accuracy by 2.7% on Mini-ImageNet with 1000 labels
- Outperforms manual progressive thresholding across all tested label regimes (40-10000 labels)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Beta Mixture Model effectively separates correct and incorrect pseudo-labels based on confidence score distributions
- Mechanism: The BMM models the confidence distribution as a weighted sum of two Beta distributions - one for correct labels (higher confidence) and one for incorrect labels (lower confidence). During training, the model iteratively updates these parameters to capture the evolving distribution gap.
- Core assumption: The confidence scores of correct pseudo-labels form a distinct distribution from incorrect ones, and this gap emerges early in training
- Evidence anchors:
  - [abstract]: "the distribution gap between the confidence values of correct and incorrect pseudo labels emerges at the very beginning of the training"
  - [section 3.1]: "the Beta distribution is defined over the interval (0, 1), it is a natural choice for modelling distribution of probabilities or proportions"
  - [corpus]: Weak evidence - no direct citations about Beta Mixture Models in SSL context
- Break condition: If the confidence distributions of correct and incorrect labels overlap significantly or shift unpredictably, the BMM cannot effectively separate them

### Mechanism 2
- Claim: Online adaptation of the mixture model prevents overfitting on labeled data during early training stages
- Mechanism: The BMM is updated at the end of each epoch using collected confidence scores from that epoch. This allows the filtering weights to evolve with the model rather than using a static threshold, preventing the early-stage overfitting that occurs when constant thresholds reject most pseudo-labels.
- Core assumption: The confidence distribution evolves smoothly enough that epoch-level updates can track it effectively
- Evidence anchors:
  - [section 1]: "at the beginning of training, τ can be too radical that only few unlabeled data contribute to the training. This often makes the model quickly overfit the labeled data"
  - [section 3.2]: "the DNN and BMM are alternatively updated... the BMM should be periodically updated to capture the latest confidence distribution"
  - [corpus]: Weak evidence - no direct citations about online mixture model adaptation in SSL
- Break condition: If the model evolves too rapidly (e.g., with very large learning rates), the BMM cannot keep pace with the changing distribution

### Mechanism 3
- Claim: Soft weighting using posterior probabilities provides better performance than hard thresholding
- Mechanism: Instead of binary decisions (keep/reject), SPF computes a continuous weight for each pseudo-label based on the posterior probability of being correct. This preserves information about confidence levels and allows for smoother learning transitions.
- Core assumption: Continuous confidence information is more useful than binary decisions for model training
- Evidence anchors:
  - [section 4.3]: "CW achieves a lower error rate than constant confidence thresholding" and "hard masking is not able to outperform soft weighting"
  - [section 3.2]: "we weight each pseudo-labeled sample with the posterior of it being correct"
  - [corpus]: Weak evidence - no direct citations about soft vs hard pseudo-label weighting in SSL
- Break condition: If the posterior estimates are highly uncertain, soft weighting may provide minimal advantage over simple thresholding

## Foundational Learning

- Concept: Beta distribution properties and parameterization
  - Why needed here: The BMM uses Beta distributions to model confidence scores (bounded between 0 and 1), requiring understanding of shape parameters α and β
  - Quick check question: What range of values can a Beta distribution take, and how do the α and β parameters affect its shape?

- Concept: Expectation-Maximization algorithm for mixture models
  - Why needed here: The BMM parameters are estimated using EM, which requires understanding of E-step (responsibility calculation) and M-step (parameter updates)
  - Quick check question: In the EM algorithm for mixture models, what does the E-step compute, and what does the M-step optimize?

- Concept: Semi-supervised learning framework and pseudo-labeling
  - Why needed here: SPF is a component that plugs into SSL methods, requiring understanding of how pseudo-labels are generated and used for training
  - Quick check question: In teacher-student SSL frameworks, how are pseudo-labels typically generated and what problem does confirmation bias refer to?

## Architecture Onboarding

- Component map: Confidence score collection -> Beta Mixture Model fitting -> Posterior probability computation -> Weighted loss integration
- Critical path: During each training epoch: collect confidence scores → at epoch end, update BMM via EM → in next epoch, compute posterior weights for each pseudo-label → apply weights in loss computation
- Design tradeoffs: BMM provides smooth adaptation but introduces computational overhead and hyperparameter sensitivity to update frequency; simpler threshold methods are faster but less adaptive
- Failure signatures: If BMM fails to separate distributions (overlapping components), training performance degrades to baseline; if update frequency is too high, parameter estimation becomes noisy; if too low, adaptation lags behind model evolution
- First 3 experiments:
  1. Implement SPF with a simple baseline SSL method (e.g., Pseudo-Labeling) on CIFAR-10 with 40 labels, comparing to constant threshold baseline
  2. Vary the BMM update frequency (per-epoch, per-2-epochs, per-4-epochs) to find optimal balance between adaptation and stability
  3. Test robustness by training with different learning rates to verify the break condition where rapid model evolution outpaces BMM adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal update frequency for the Beta Mixture Model (BMM) during training, and how does it vary across different SSL methods or dataset sizes?
- Basis in paper: [inferred] The paper mentions that the BMM is updated once per epoch and that the method is robust to update frequency changes by factors of two or four, but does not explore whether a different frequency might be optimal for specific scenarios.
- Why unresolved: The paper only tests a few update frequencies (once per epoch, twice per epoch, and four times per epoch) and does not systematically explore the impact of BMM update frequency on different datasets or SSL methods.
- What evidence would resolve it: A comprehensive study comparing different BMM update frequencies (e.g., every 10 epochs, every 50 epochs, every batch) across various SSL methods (e.g., FixMatch, MeanTeacher, SimPLE) and dataset sizes would clarify the optimal update frequency.

### Open Question 2
- Question: How does the proposed Self-Adaptive Pseudo-Label Filtering (SPF) method perform when integrated with SSL methods that use different teacher-student frameworks, such as temporal ensembling or virtual adversarial training?
- Basis in paper: [inferred] The paper demonstrates SPF's effectiveness when integrated with FixMatch-RA and MeanTeacher, but does not explore its performance with other SSL methods like Π-Model, Temporal Ensembling, or VAT.
- Why unresolved: The paper focuses on demonstrating SPF's effectiveness with a limited set of SSL methods, leaving its performance with other methods unexplored.
- What evidence would resolve it: Experiments integrating SPF with a broader range of SSL methods, including Π-Model, Temporal Ensembling, VAT, and ReMixMatch, would show its generalizability and potential benefits across different frameworks.

### Open Question 3
- Question: What is the impact of using different confidence score distributions or meta-model architectures (e.g., Gaussian Mixture Models, neural networks) on the performance of SPF?
- Basis in paper: [explicit] The paper discusses the choice of Beta Mixture Model (BMM) over Gaussian Mixture Model (GMM) and mentions the use of confidence scores as the input feature, but does not explore other potential distributions or architectures.
- Why unresolved: The paper focuses on BMM and confidence scores but does not investigate whether other distributions (e.g., Dirichlet, logistic) or meta-model architectures (e.g., neural networks) could yield better performance.
- What evidence would resolve it: Comparative experiments using different confidence score distributions (e.g., Dirichlet, logistic) and meta-model architectures (e.g., GMM, neural networks) would reveal the optimal choice for SPF's meta-model.

## Limitations

- The exact implementation details of the online Beta Mixture Model fitting procedure are not fully specified, particularly the E-M algorithm parameters and update frequency tuning
- The computational overhead of the BMM updates, though not explicitly quantified, could be significant for large-scale applications
- The generalization to datasets outside the computer vision domain remains untested

## Confidence

- **High Confidence:** The core mechanism of using Beta Mixture Models to separate confidence distributions is well-founded and mathematically sound. The experimental results showing consistent improvements across multiple datasets and label regimes are compelling.
- **Medium Confidence:** The claim that soft weighting outperforms hard thresholding is supported by ablation studies, but the magnitude of improvement may vary with different SSL architectures and hyperparameter settings.
- **Medium Confidence:** The assertion that SPF prevents early-stage overfitting is plausible based on the adaptive nature of the approach, though the paper doesn't provide direct evidence comparing overfitting rates to baseline methods.

## Next Checks

1. **Implementation Verification:** Reproduce the SPF component with a simple SSL baseline (e.g., Pseudo-Labeling) on CIFAR-10 with 40 labels, comparing to constant threshold baseline to verify the core filtering improvement.

2. **Robustness Testing:** Evaluate SPF's performance across different learning rates to identify the break condition where rapid model evolution outpaces BMM adaptation, confirming the mechanism described in the paper.

3. **Computational Overhead Analysis:** Measure the actual runtime overhead introduced by the BMM updates and posterior probability computations to quantify the practical cost of the adaptive filtering approach.