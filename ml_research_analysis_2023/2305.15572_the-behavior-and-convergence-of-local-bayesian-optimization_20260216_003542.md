---
ver: rpa2
title: The Behavior and Convergence of Local Bayesian Optimization
arxiv_id: '2305.15572'
source_url: https://arxiv.org/abs/2305.15572
tags:
- local
- function
- optimization
- convergence
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates the behavior and convergence of local Bayesian
  optimization, a promising approach for high-dimensional optimization problems. The
  key idea is to use the Gaussian process surrogate model to iteratively search for
  local improvements around the current input, rather than attempting to find the
  global optimum.
---

# The Behavior and Convergence of Local Bayesian Optimization

## Quick Facts
- arXiv ID: 2305.15572
- Source URL: https://arxiv.org/abs/2305.15572
- Reference count: 40
- Primary result: Local Bayesian optimization converges to stationary points with polynomial rates in dimension and sample complexity

## Executive Summary
This paper provides the first rigorous convergence analysis of local Bayesian optimization (BO) algorithms, specifically the Gaussian Inference for Bayesian Optimization (GIBO) method. The authors prove that local BO approaches can achieve polynomial convergence rates in both noiseless and noisy settings, with rates of O(d/n) for noiseless functions and slower but still polynomial rates for noisy functions. The key insight is that by focusing search on local neighborhoods rather than attempting global optimization, these methods avoid the curse of dimensionality while still finding high-quality solutions. The results provide theoretical support for the effectiveness of local BO in high-dimensional problems where traditional global BO methods struggle.

## Method Summary
The paper analyzes GIBO, a local Bayesian optimization algorithm that iteratively improves a current input by using Gaussian process surrogate models to estimate descent directions. The algorithm selects batches of points that minimize the trace of the posterior covariance to actively reduce uncertainty in gradient estimates. For noiseless functions, convergence is achieved through repeated gradient descent steps with diminishing step sizes. For noisy functions, the algorithm compensates by increasing batch size as optimization progresses, reducing gradient bias when approaching stationary points. The analysis assumes either that the objective function lies in the RKHS with bounded norm or is a sample from a GP with known hyperparameters.

## Key Results
- Local BO converges to stationary points with rate O(d/n) for noiseless functions
- For noisy functions, convergence is polynomial in both dimension d and sample count n
- Local optima found by this approach are surprisingly good compared to global methods
- Active point selection using trace of posterior covariance effectively reduces gradient estimation bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local Bayesian optimization finds surprisingly good solutions in high dimensions by focusing search on local neighborhoods rather than global space.
- Mechanism: The algorithm iteratively improves a current input by using Gaussian process surrogate models to estimate descent directions, effectively navigating local regions of the search space while avoiding the curse of dimensionality.
- Core assumption: The function being optimized can be modeled well by a Gaussian process with known hyperparameters, and local optima provide good approximations to global solutions.
- Evidence anchors:
  - [abstract] "A recent development in Bayesian optimization is the use of local optimization strategies, which can deliver strong empirical performance on high-dimensional problems compared to traditional global strategies."
  - [section 1] "The approach is intuitively appealing: a typical claim is that a local optimum can be found relatively quickly, and exponential sample complexity is only necessary if one wishes to enumerate all local optima."
  - [corpus] Weak evidence - related papers focus on trust-region methods but don't provide direct comparative analysis.
- Break condition: If the objective function has many isolated good local optima separated by poor regions, local search may converge to suboptimal solutions.

### Mechanism 2
- Claim: The algorithm converges to stationary points with polynomial rates in both dimension and sample complexity.
- Mechanism: By actively selecting points that minimize uncertainty in the gradient estimate (using the trace of the posterior covariance), the algorithm reduces bias in gradient estimates and follows descent directions efficiently.
- Core assumption: The objective function is smooth (β-smooth) and either lies in the RKHS with bounded norm or is a sample from a GP with known parameters.
- Evidence anchors:
  - [section 5] "For noiseless functions, they prove convergence to a stationary point with a rate of O(d/n), where d is the input dimension and n is the number of samples."
  - [section 5.1] "With linearly increasing batch size, the algorithm converges to a stationary point with rate O(σd1.25n−0.25 log n)"
  - [corpus] Weak evidence - related papers discuss convergence but not specifically for local BO methods.
- Break condition: If the function is non-differentiable or has discontinuous gradients, the convergence analysis breaks down.

### Mechanism 3
- Claim: Noise in observations significantly impacts convergence speed but the algorithm still maintains polynomial convergence rates.
- Mechanism: The algorithm compensates for noisy observations by increasing batch size as optimization progresses, reducing the bias in gradient estimates when approaching stationary points.
- Core assumption: Observations follow an i.i.d. additive Gaussian noise model, and the kernel is differentiable to allow gradient estimation.
- Evidence anchors:
  - [section 5.2] "We now turn our attention to the more challenging and interesting setting where the observation noise σ > 0."
  - [section 6.1] "Empirically, we find that the GIBO designs do indeed result in asymptotically faster variance reduction than our bound implies."
  - [corpus] No direct evidence - this appears to be novel analysis specific to this paper.
- Break condition: If noise is too high relative to the function signal, the algorithm may fail to converge or converge very slowly.

## Foundational Learning

- Concept: Gaussian Process (GP) surrogate modeling
  - Why needed here: GPs provide a probabilistic framework for modeling the black-box function and estimating gradients, which is essential for the Bayesian optimization approach.
  - Quick check question: How does conditioning a GP on observations update the posterior mean and covariance functions?

- Concept: Reproducing Kernel Hilbert Space (RKHS) theory
  - Why needed here: The convergence analysis for noiseless functions relies on assuming the objective function lies in an RKHS with bounded norm.
  - Quick check question: What properties of functions in an RKHS make them suitable for this convergence analysis?

- Concept: Meshless data approximation techniques
  - Why needed here: These techniques are used to bound the error between the true gradient and the estimated gradient from the GP posterior.
  - Quick check question: How does the error bound in Lemma 1 relate the gradient estimation error to the posterior covariance trace?

## Architecture Onboarding

- Component map:
  - Gaussian Process surrogate model (mean and covariance functions)
  - Acquisition function based on trace of posterior covariance
  - Batch selection mechanism for active learning
  - Gradient estimation and update rule
  - Convergence monitoring and termination criteria

- Critical path:
  1. Initialize GP with prior mean and covariance
  2. Select batch of points to evaluate using acquisition function
  3. Update GP with new observations
  4. Compute gradient estimate and update current solution
  5. Repeat until convergence or budget exhausted

- Design tradeoffs:
  - Batch size selection: Larger batches reduce gradient bias but increase computational cost
  - Step size selection: Must balance convergence speed with stability
  - Noise handling: Trade-off between exploration and exploitation in noisy settings

- Failure signatures:
  - Slow convergence or stagnation: May indicate poor kernel choice or insufficient exploration
  - High variance in results: Could suggest sensitivity to initialization or noise levels
  - Violation of smoothness assumptions: Non-smooth functions may cause convergence failures

- First 3 experiments:
  1. Test convergence on simple quadratic functions in varying dimensions to verify O(d/n) rate
  2. Compare performance with and without noise to validate noise compensation mechanisms
  3. Experiment with different batch size schedules to find optimal growth rate for convergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of local Bayesian optimization compare to global Bayesian optimization in high-dimensional problems with structured functions (e.g., additive functions)?
- Basis in paper: [inferred] The paper focuses on local Bayesian optimization for high-dimensional problems but does not compare its performance to global methods on structured functions.
- Why unresolved: The paper only provides a theoretical analysis of local Bayesian optimization without empirical comparisons to global methods on structured functions.
- What evidence would resolve it: Empirical experiments comparing local and global Bayesian optimization on high-dimensional problems with additive structure.

### Open Question 2
- Question: What is the optimal batch size strategy for local Bayesian optimization in the noisy setting?
- Basis in paper: [explicit] The paper discusses different batch size strategies and their impact on convergence rates, but does not provide a definitive answer for the optimal strategy.
- Why unresolved: The paper provides upper bounds on convergence rates for different batch size strategies but does not determine the optimal strategy.
- What evidence would resolve it: Further theoretical analysis or empirical experiments to identify the batch size strategy that achieves the best convergence rate in the noisy setting.

### Open Question 3
- Question: How does the performance of local Bayesian optimization scale with the dimensionality of the problem?
- Basis in paper: [explicit] The paper discusses the convergence rates of local Bayesian optimization in terms of the input dimension d, but does not provide a comprehensive analysis of how the performance scales with d.
- Why unresolved: The paper provides convergence rates that depend on the input dimension d but does not analyze the scaling behavior of the algorithm's performance with respect to d.
- What evidence would resolve it: Further theoretical analysis or empirical experiments to determine how the performance of local Bayesian optimization scales with the dimensionality of the problem.

## Limitations
- The analysis assumes the function either lies in the RKHS or is a GP sample, which may not hold for many real-world optimization problems
- Computational complexity of kernel matrix operations scales poorly with dimensionality and sample size
- The theoretical rates don't account for practical implementation details like numerical precision or hyperparameter tuning

## Confidence
- **High confidence**: Theoretical convergence rates (O(d/n) for noiseless, polynomial for noisy) are well-established under the stated smoothness and RKHS assumptions
- **Medium confidence**: The empirical claim that local optima are "surprisingly good" compared to global methods, as this is based on limited experimental evidence
- **Low confidence**: The practical implications of these convergence rates for very high-dimensional problems (>100 dimensions) where kernel matrix computations become prohibitive

## Next Checks
1. **Rate verification**: Run experiments on synthetic functions with known smoothness properties to empirically verify the O(d/n) convergence rate for noiseless functions across dimensions d ∈ {10, 50, 100}
2. **Noise sensitivity**: Test the algorithm on functions with varying noise levels σ ∈ {0.01, 0.1, 1.0} to quantify the impact on convergence speed and solution quality
3. **Scalability assessment**: Measure wall-clock time and memory usage as dimension increases to determine practical limits beyond which the method becomes computationally infeasible