---
ver: rpa2
title: Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language
  Models
arxiv_id: '2312.08303'
source_url: https://arxiv.org/abs/2312.08303
tags:
- dtot
- llms
- content
- rationales
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes BD-LLM, an approach to improve toxic content
  detection by bootstrapping and distilling LLMs. It introduces DToT, a novel prompting
  method that iteratively re-prompts LLMs with more fine-grained context to boost
  their detection performance and extract rationales.
---

# Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models

## Quick Facts
- arXiv ID: 2312.08303
- Source URL: https://arxiv.org/abs/2312.08303
- Authors: Multiple
- Reference count: 9
- Key outcome: DToT improves LLM accuracy by up to 4.6% and student LM accuracy by up to 16.9% while being 60x smaller

## Executive Summary
This paper introduces BD-LLM, an approach that combines bootstrapping and distilling large language models for toxic content detection. The key innovation is DToT (Decision-Tree-of-Thought), a novel prompting method that iteratively re-prompts LLMs with more fine-grained context when responses lack confidence, while also extracting rationales. These rationales are then used to fine-tune smaller student language models, achieving state-of-the-art performance with significant size reduction and improved cross-dataset transferability.

## Method Summary
The method uses DToT prompting to iteratively refine LLM responses by injecting more specific context from a predefined context tree when confidence is low. The LLM generates both a label and rationale for each input. These rationales are extracted and used to fine-tune smaller student language models, teaching them both the correct label and the reasoning behind it. The approach combines few-shot in-context learning with the rationale extraction process to further boost performance.

## Key Results
- DToT improves LLM accuracy by up to 4.6% on toxic content detection tasks
- Student LMs fine-tuned with DToT rationales outperform baselines by up to 16.9% accuracy
- Student LMs are over 60x smaller than the original LLMs while maintaining performance
- Student LMs exhibit better cross-dataset transferability compared to baseline models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative refinement through DToT prompting improves LLM confidence and accuracy for toxic content detection
- Mechanism: DToT uses a confidence checker to assess LLM responses and a context selector to inject more fine-grained context from a predefined context tree when confidence is low, enabling iterative refinement
- Core assumption: More specific context about toxic content categories helps LLMs produce more accurate and confident classifications
- Evidence anchors: Abstract states DToT "can automatically select more fine-grained context to re-prompt LLMs when their responses lack confidence"

### Mechanism 2
- Claim: Extracting and using rationales from LLMs during distillation improves student LM performance and transferability
- Mechanism: High-quality rationales generated by DToT prompting are used as supervision for training smaller student LMs
- Core assumption: Student LMs can learn to mimic the reasoning patterns of larger LLMs, improving both accuracy and cross-dataset generalization
- Evidence anchors: Abstract reports student LMs fine-tuned with rationales outperform baselines with up to 16.9% accuracy improvement

### Mechanism 3
- Claim: Augmenting DToT with few-shot demonstrations and rationales further boosts LLM performance
- Mechanism: Adding semantically similar positive/negative examples and their rationales to the prompt helps LLMs leverage in-context learning more effectively
- Core assumption: Demonstrations provide useful inductive biases that improve LLM generalization on new inputs
- Evidence anchors: Abstract mentions DToT+FS+R outperforms baselines by up to 4.58% for RoBerta

## Foundational Learning

- **Chain-of-Thought (CoT) prompting**: Provides a baseline iterative reasoning method; DToT extends it for classification with confidence-based refinement. Quick check: How does CoT prompting differ from DToT in handling low-confidence responses?
- **Knowledge distillation**: Enables deployment of smaller, more efficient models that retain LLM-level performance. Quick check: Why is it beneficial to distill rationales alongside labels?
- **Cross-dataset transferability**: Toxic content detection models must generalize across different datasets and domains. Quick check: What evidence suggests rationale-based fine-tuning improves cross-dataset generalization?

## Architecture Onboarding

- **Component map**: Input statement → DToT prompting (Confidence checker → Context tree → Context selector → Prompt generator → LLM) → High-confidence answer + rationale → Student LM fine-tuning → Deploy for toxic content detection
- **Critical path**: 1) Input statement → DToT prompting → High-confidence answer + rationale 2) Answer + rationale → Student LM fine-tuning 3) Fine-tuned student LM → Deploy for toxic content detection
- **Design tradeoffs**: Larger context trees improve accuracy but increase prompt length and cost; using few-shot examples boosts performance but requires a development set; black-box LLMs are easier to use but provide less control over confidence scoring
- **Failure signatures**: Low accuracy despite DToT prompting (context tree may be too shallow or misaligned); student LM overfitting (too few rationales or high label-noise); poor cross-dataset performance (fine-tuning data too domain-specific)
- **First 3 experiments**: 1) Run DToT prompting on a small sample; check if confidence scores correlate with accuracy improvements 2) Fine-tune a small LM with and without rationales; compare performance on a validation set 3) Test cross-dataset transferability by evaluating student LM on unseen datasets after fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of DToT prompting compare to other advanced prompting techniques (e.g., least-to-most prompting, self-consistency) on toxic content detection tasks?
- Basis in paper: The paper mentions DToT prompting is compared to Chain-of-Thought (CoT) and UniLC prompting baselines
- Why unresolved: The paper does not compare DToT to other state-of-the-art prompting methods beyond CoT and UniLC
- What evidence would resolve it: Experiments comparing DToT to other advanced prompting techniques on the same toxic content detection datasets

### Open Question 2
- Question: Can the context tree in DToT prompting be dynamically adapted based on the LLM's responses rather than using a pre-defined structure?
- Basis in paper: The paper mentions a limitation that the context selector conducts greedy search and uses a pre-defined context tree
- Why unresolved: The paper does not explore dynamic adaptation of the context tree structure
- What evidence would resolve it: Experiments with dynamically constructed context trees that adapt based on LLM responses

### Open Question 3
- Question: How does the quality of rationales extracted via DToT prompting compare to human-annotated rationales in terms of helping student LMs learn better?
- Basis in paper: The paper discusses rationale extraction and distillation but does not compare to human-annotated rationales
- Why unresolved: The paper does not benchmark rationale quality against human annotations
- What evidence would resolve it: Human evaluation of rationales generated by DToT versus human-annotated rationales for the same content

## Limitations

- Context tree structure and content are not fully specified, introducing uncertainty in reproducing exact results
- Confidence threshold values for the confidence checker are not detailed, affecting the iterative refinement process
- Results on the proprietary Amazon dataset cannot be independently verified, limiting assessment of generalizability

## Confidence

- **High Confidence**: Core concept of using iterative prompting with confidence-based refinement to improve LLM performance
- **Medium Confidence**: Rationale-based fine-tuning improves student LM performance and transferability
- **Low Confidence**: Augmenting DToT with few-shot demonstrations and rationales further boosts performance

## Next Checks

1. Implement and test different context tree structures to evaluate their impact on DToT prompting performance
2. Experiment with different confidence threshold values to determine their influence on the iterative refinement process
3. Evaluate the student LM's performance on additional datasets not used in training to assess robustness and transferability