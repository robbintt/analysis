---
ver: rpa2
title: Divergences between Language Models and Human Brains
arxiv_id: '2311.09308'
source_url: https://arxiv.org/abs/2311.09308
tags:
- language
- brain
- processing
- human
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates differences between language model representations
  and human brain responses to language, using Magnetoencephalography (MEG) data from
  participants reading narrative stories. An LLM-based method is used to automatically
  propose hypotheses explaining the divergences between language models and human
  brains.
---

# Divergences between Language Models and Human Brains

## Quick Facts
- arXiv ID: 2311.09308
- Source URL: https://arxiv.org/abs/2311.09308
- Authors: 
- Reference count: 36
- One-line primary result: Fine-tuning language models on emotion, figurative language, and physical commonsense datasets improves their alignment with human brain responses.

## Executive Summary
This paper investigates the differences between language model (LM) representations and human brain responses to language using Magnetoencephalography (MEG) data from participants reading narrative stories. The study employs an LLM-based method to automatically propose hypotheses explaining the divergences between LMs and human brains. Three key domains of divergence are identified: social/emotional intelligence, figurative language processing, and physical commonsense. The authors demonstrate that fine-tuning LMs on datasets related to these domains improves their alignment with human brain responses, suggesting that the observed divergences may stem from inadequate representations of these types of knowledge in LMs.

## Method Summary
The study uses pre-trained GPT-2 XL to predict MEG responses for each word, then fine-tunes the model on three domain-specific datasets: Social IQa (emotion), Fig-Qa (figurative language), and PiQa (physical commonsense). The fine-tuned models are evaluated on their ability to predict MEG responses using Pearson correlation and Mean Squared Error (MSE) reduction. An LLM-based hypothesis generation and validation method is employed to identify divergences between LMs and human brain responses, which are then validated using the fine-tuned verifier model.

## Key Results
- Fine-tuning GPT-2 XL on emotion, figurative language, and physical commonsense datasets improves its alignment with human brain responses.
- The LLM-based hypothesis generation method identifies three key domains of divergence: social/emotional intelligence, figurative language processing, and physical commonsense.
- The observed improvements in alignment suggest that the divergences between LMs and human brains may stem from inadequate representations of specific types of knowledge in LMs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning language models on specific domains improves their alignment with human brain responses.
- Mechanism: By fine-tuning on datasets related to emotional understanding, figurative language processing, and physical commonsense, the language models learn representations that better capture the nuances and contextual knowledge required for these domains. This leads to improved predictions of MEG responses, particularly in language processing time windows.
- Core assumption: The observed divergences between language models and human brains stem from inadequate representations of specific types of knowledge in the models.
- Evidence anchors:
  - [abstract]: "Fine-tuning language models on datasets related to these domains improves their alignment with human brain responses, suggesting that the observed divergences may stem from inadequate representations of these types of knowledge in language models."
  - [section 5]: "Our study implies that the observed divergences between LMs and human brains may stem from LMs' inadequate representation of these specific types of knowledge."
  - [corpus]: The corpus evidence is limited, with only 25 related papers found. However, some papers, such as "Psychometric Alignment: Capturing Human Knowledge Distributions via Language Models" and "Disentangling the Factors of Convergence between Brains and Computer Vision Models," suggest that fine-tuning and alignment are relevant topics in the field.
- Break condition: If the fine-tuning process does not lead to improved alignment with human brain responses, or if the observed divergences are not due to inadequate representations of specific types of knowledge.

### Mechanism 2
- Claim: Language models struggle with processing emotion, figurative language, and physical commonsense due to their reliance on statistical regularities and lack of grounding in real-world contexts.
- Mechanism: Language models primarily learn through recognizing statistical regularities in surface-level linguistic symbols, which may not capture the full complexity and contextual knowledge required for processing emotions, figurative language, and physical commonsense. Fine-tuning on domain-specific datasets helps bridge this gap by providing the models with more relevant training data.
- Core assumption: Language models' limitations in processing these domains are due to their training approach and lack of grounding in real-world contexts.
- Evidence anchors:
  - [abstract]: "LMs primarily learn through recognizing statistical regularities in surface-level linguistic symbols, whereas humans may rely on more structured linguistic principles. Additionally, LMs that are confined to linguistic data may fail to ground linguistic symbols in real-world contexts."
  - [section 4]: "Within NLP, acquiring physical commonsense knowledge poses a notable challenge for language models. While these models can potentially learn representations capturing specific physical properties of the world, it remains unclear whether text-based representations can truly capture the richness and complexity of physical commonsense as exhibited by humans."
  - [corpus]: The corpus evidence is limited, but some papers, such as "Conceptual structure coheres in human cognition but not in large language models," suggest that language models may struggle with processing certain types of knowledge.
- Break condition: If language models can effectively process these domains without the need for fine-tuning or grounding in real-world contexts.

### Mechanism 3
- Claim: Automatic hypothesis proposal and validation using an LLM-based method can identify divergences between language models and human brain responses.
- Mechanism: By comparing the prediction errors of language models and human brain responses, the LLM-based method generates hypotheses about the differences in processing. These hypotheses are then validated using a fine-tuned verifier model, allowing for the identification of specific domains where language models may struggle.
- Core assumption: The LLM-based method can effectively identify divergences between language models and human brain responses by analyzing prediction errors and generating hypotheses.
- Evidence anchors:
  - [section 3.1]: "To discover subtle differences between MEG responses and LM predictions, we used a method that automatically describes differences between text corpora using proposer and verifier LMs."
  - [section 4]: "The top ten hypotheses ranked by validity resonate with conclusions drawn in prior research, as detailed in §4. We identify two primary differences between the language model and the human brain: the processing of emotion and figurative language."
  - [corpus]: The corpus evidence is limited, but some papers, such as "Disentangling the Factors of Convergence between Brains and Computer Vision Models," suggest that comparing models to brain data is a relevant research direction.
- Break condition: If the LLM-based method fails to generate meaningful hypotheses or if the validation process does not accurately identify divergences between language models and human brain responses.

## Foundational Learning

- Concept: Magnetoencephalography (MEG)
  - Why needed here: MEG is used to measure human brain responses to language, providing high temporal resolution data for comparing with language model predictions.
  - Quick check question: What are the advantages of using MEG over other neuroimaging techniques like fMRI or EEG for studying language processing?

- Concept: Language model fine-tuning
  - Why needed here: Fine-tuning language models on domain-specific datasets is the key intervention used to improve their alignment with human brain responses.
  - Quick check question: How does fine-tuning a pre-trained language model on a specific dataset differ from training a model from scratch on that dataset?

- Concept: Hypothesis generation and validation
  - Why needed here: The automatic hypothesis proposal and validation method is used to identify divergences between language models and human brain responses.
  - Quick check question: What are the key components of the LLM-based method used for hypothesis generation and validation in this study?

## Architecture Onboarding

- Component map: Language model (GPT-2 XL) -> MEG data preprocessing and denoising -> Ridge regression model for predicting MEG responses -> LLM-based hypothesis proposal and validation method -> Domain-specific datasets for fine-tuning (emotion, figurative language, physical commonsense) -> Evaluation metrics (Pearson correlation, MSE reduction)

- Critical path: Language model → MEG data preprocessing → Ridge regression → Hypothesis generation → Fine-tuning → Evaluation

- Design tradeoffs:
  - Using a pre-trained language model (GPT-2 XL) allows for leveraging existing knowledge but may limit the extent of improvements from fine-tuning.
  - Fine-tuning on domain-specific datasets can improve alignment but may not capture all aspects of human language processing.
  - The LLM-based hypothesis proposal method relies on the quality and diversity of the input data and the proposer/verifier models.

- Failure signatures:
  - No significant improvement in alignment after fine-tuning on domain-specific datasets.
  - Hypothesis generation and validation method fails to identify meaningful divergences between language models and human brain responses.
  - Evaluation metrics do not show significant differences between the base model and fine-tuned models.

- First 3 experiments:
  1. Fine-tune the base language model on the emotion dataset and evaluate its alignment with human brain responses using Pearson correlation and MSE reduction.
  2. Fine-tune the base language model on the figurative language dataset and evaluate its alignment with human brain responses using Pearson correlation and MSE reduction.
  3. Fine-tune the base language model on the physical commonsense dataset and evaluate its alignment with human brain responses using Pearson correlation and MSE reduction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning language models on multimodal data (e.g., visual and motor information) improve their alignment with human brain responses beyond unimodal fine-tuning?
- Basis in paper: [inferred] The authors suggest incorporating additional modalities could be essential for capturing a broader spectrum of human knowledge and improving LM-brain alignment.
- Why unresolved: The study only explored unimodal fine-tuning on text-based datasets. The potential benefits of multimodal fine-tuning were hypothesized but not empirically tested.
- What evidence would resolve it: Conducting experiments where language models are fine-tuned on multimodal datasets (e.g., text, images, and video) and comparing their alignment with brain responses to unimodal fine-tuned models.

### Open Question 2
- Question: What are the specific neural mechanisms underlying the observed improvements in LM-brain alignment after fine-tuning on emotion, figurative language, and physical commonsense tasks?
- Basis in paper: [explicit] The authors observed improved alignment after fine-tuning but did not investigate the underlying neural mechanisms.
- Why unresolved: The study focused on behavioral alignment but did not delve into the neural correlates of the observed improvements.
- What evidence would resolve it: Conducting neuroimaging studies to identify the brain regions and networks that show increased activation or connectivity after fine-tuning, potentially revealing the neural substrates of the improved alignment.

### Open Question 3
- Question: How do the observed divergences between language models and human brains in processing emotion, figurative language, and physical commonsense generalize to other domains and tasks?
- Basis in paper: [explicit] The study identified three specific domains where LMs and human brains diverge but acknowledged that the fine-tuning datasets may not fully capture the richness of these phenomena in natural language.
- Why unresolved: The study's findings are based on a specific dataset and may not generalize to other domains or tasks that involve these phenomena.
- What evidence would resolve it: Conducting similar studies on different datasets, tasks, and domains to determine the generalizability of the observed divergences and the effectiveness of fine-tuning in improving alignment across various contexts.

## Limitations

- The study relies on a single dataset (Harry Potter chapters) for the MEG data, which may limit the generalizability of the findings.
- The fine-tuning datasets used (Social IQa, Fig-Qa, and PiQa) may not fully capture the richness and complexity of the respective domains in natural language.
- The LLM-based hypothesis generation and validation method may be sensitive to the quality and diversity of the input data and the proposer/verifier models.

## Confidence

- **Domain-Specific Fine-tuning**: Medium - The improvement in language model alignment with human brain responses after fine-tuning on specific domains is well-supported by the experimental results. However, the extent to which these improvements generalize to other types of language processing or real-world contexts remains uncertain.
- **Mechanism of Divergence**: Medium - The proposed mechanisms explaining why language models struggle with emotion, figurative language, and physical commonsense are plausible but not definitively proven. Alternative explanations, such as differences in training data or model architecture, could also contribute to the observed divergences.
- **LLM-based Hypothesis Generation**: Medium - While the automatic hypothesis proposal and validation method shows potential, its reliability and generalizability to other domains or datasets are not fully established. The quality and diversity of the input data and the proposer/verifier models are crucial for the method's effectiveness.

## Next Checks

1. **Cross-Domain Generalization**: Test the fine-tuned language models on additional datasets or tasks beyond the three domains studied to assess the generalizability of the improvements in alignment with human brain responses.

2. **Alternative Explanations**: Conduct controlled experiments to rule out alternative explanations for the observed divergences, such as differences in training data or model architecture, by comparing the performance of language models with varying training regimes or architectures.

3. **Real-World Grounding**: Evaluate the extent to which fine-tuning on domain-specific datasets improves the language models' ability to ground linguistic symbols in real-world contexts, using tasks that require reasoning about physical situations or emotional states in more naturalistic settings.