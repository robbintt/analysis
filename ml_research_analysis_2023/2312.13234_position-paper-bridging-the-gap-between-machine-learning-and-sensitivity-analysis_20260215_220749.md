---
ver: rpa2
title: 'Position Paper: Bridging the Gap Between Machine Learning and Sensitivity
  Analysis'
arxiv_id: '2312.13234'
source_url: https://arxiv.org/abs/2312.13234
tags:
- sensitivity
- methods
- learning
- system
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that interpretable machine learning (IML) can
  be viewed as a form of sensitivity analysis (SA) applied to the machine learning
  (ML) process. The authors formalize the ML process as a system suitable for SA,
  consisting of interconnected functions that receive inputs and produce outputs.
---

# Position Paper: Bridging the Gap Between Machine Learning and Sensitivity Analysis

## Quick Facts
- arXiv ID: 2312.13234
- Source URL: https://arxiv.org/abs/2312.13234
- Authors: 
- Reference count: 40
- One-line primary result: The paper argues that interpretable machine learning (IML) can be viewed as a form of sensitivity analysis (SA) applied to the machine learning (ML) process.

## Executive Summary
This position paper proposes a unifying perspective that views interpretable machine learning methods as forms of sensitivity analysis applied to the machine learning process. The authors formalize the ML process as a system of interconnected functions and demonstrate how existing IML methods compute sensitivities for input-output relationships within this system. By establishing this connection, the paper suggests that traditional sensitivity analysis techniques developed for complex systems could be directly applied to machine learning, potentially improving our understanding of model behavior and hyperparameter importance. The authors argue this unified view could help researchers better recognize related work, avoid redundancies, and credit contributions appropriately across the SA and ML communities.

## Method Summary
This is a position paper that argues for a conceptual framework rather than presenting empirical results. The authors formalize the ML process as a system of interconnected functions (hyperparameter optimization, model training, model prediction, and interpretation) and demonstrate how existing IML methods can be characterized as computing sensitivities within this system. The paper discusses how various IML methods (ICE, PD, FANOV A, ALE, PFI, counterfactual explanations, LIME, and Shapley values) relate to this perspective and suggests applying traditional SA techniques (Morris method, DGSM, GSM, variograms/VARS, Sobol index estimators) to ML. No specific training procedure or experimental methodology is presented, as the paper focuses on theoretical argumentation and literature review.

## Key Results
- IML methods can be formalized as sensitivity analysis computations applied to different components of the ML system
- Existing SA techniques like Morris method, DGSM, GSM, variograms/VARS, and Sobol indices could be applied to analyze ML models and hyperparameters
- The unified SA-based view of explanations in ML can help researchers recognize related work and avoid redundancies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IML methods can be formalized as computing sensitivities within the ML process
- Mechanism: By modeling the ML process as a system of interconnected functions (hyperparameter optimization, model training, model prediction, and interpretation), each IML method becomes a sensitivity analysis (SA) tool applied to a specific function
- Core assumption: The ML process can be adequately described as a system of mathematical functions suitable for SA
- Evidence anchors:
  - [abstract] "We argue that interpretations of machine learning (ML) models or the model-building process can be seen as a form of sensitivity analysis (SA)"
  - [section] "We formalize the concept of training by introducing an inducer (or learner) I as a function that maps a dataset...to a model"
  - [corpus] Weak evidence - the corpus contains papers on interpretability but no explicit discussion of SA-ML unification
- Break condition: If the ML process cannot be adequately modeled as a system of functions, or if certain IML methods fundamentally cannot be expressed as SA computations

### Mechanism 2
- Claim: Traditional SA methods can be directly applied to ML systems
- Mechanism: SA techniques developed for complex systems (Morris method, Sobol indices, DGSM, variograms/VARS, GSM) can be used to analyze hyperparameter importance, model sensitivity, or interpretation sensitivity
- Core assumption: SA methods are general enough to handle the types of inputs and outputs found in ML systems
- Evidence anchors:
  - [abstract] "how other SA techniques could be applied to ML"
  - [section] "Fel et al. (2021) describe the importance of regions in image data with Sobol indices"
  - [corpus] Weak evidence - corpus papers discuss interpretability but not application of traditional SA methods to ML
- Break condition: If SA methods are too computationally expensive for ML applications, or if they fail to capture the non-linear relationships typical in ML

### Mechanism 3
- Claim: The SA-ML bridge enables better recognition of related work and avoids redundancies
- Mechanism: By recognizing IML as SA applied to ML, researchers can identify overlapping methods, credit related work appropriately, and avoid duplicating research efforts
- Core assumption: Many IML methods were developed independently without awareness of their SA equivalents
- Evidence anchors:
  - [abstract] "calling attention to the benefits of a unified SA-based view of explanations in ML and the necessity to fully credit related work"
  - [section] "HDMR dates back to Hoeffding (1948) and is the basis for variance-based SA...For the HDMR, decomposing the function into lower-dimensional terms is instrumental, an approach which was later redeveloped for ML and termed partial dependence (PD)"
  - [corpus] Weak evidence - corpus papers don't explicitly discuss credit attribution issues between SA and ML
- Break condition: If the SA-ML connection is too abstract to practically help researchers identify related work, or if communities remain siloed despite theoretical connections

## Foundational Learning

- Concept: Systems modeling and interconnected functions
  - Why needed here: The paper's core argument depends on modeling the ML process as a system of interconnected functions suitable for SA
  - Quick check question: Can you identify the four main functions in the ML system described in the paper (τ, c, I, Γ) and their inputs/outputs?

- Concept: Sensitivity analysis methodology
  - Why needed here: Understanding SA methods (local vs global, finite-difference-based, distribution-based, regression-based) is crucial to see how they apply to ML
  - Quick check question: What is the difference between local and global sensitivity analysis, and why is this distinction important for ML applications?

- Concept: Hyperparameter optimization and model training
  - Why needed here: The paper discusses interpreting both model predictions and the hyperparameter optimization process, requiring understanding of these ML components
  - Quick check question: How does the hyperparameter optimization function c relate to the generalization error, and why is this important for SA?

## Architecture Onboarding

- Component map:
  - ML System: τ (hyperparameter tuner) → c (generalization error function) → I (inducer/learner) → bf (trained model) → Γ (interpretation function)
  - SA Methods: Morris method, DGSM, GSM, variograms/VARS, Sobol indices, regression-based methods
  - IML Methods: ICE, PD, FANOV A, ALE, PFI, counterfactual explanations, LIME, Shapley values

- Critical path: The paper establishes the connection between SA and IML, then demonstrates how existing methods relate to this perspective and suggests applying SA methods to ML

- Design tradeoffs: The paper trades specificity (not all IML methods may fit perfectly into the SA framework) for generality (a unified view that can potentially apply many SA methods to ML)

- Failure signatures: If the SA-ML connection proves too abstract to be practically useful, if communities don't engage with the proposed unification, or if applying SA methods to ML proves computationally infeasible

- First 3 experiments:
  1. Apply the Morris method to analyze hyperparameter importance in a simple ML model
  2. Use Sobol indices to quantify feature importance in a trained ML model
  3. Apply variogram analysis to study prediction sensitivity across the feature space of an ML model

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but it implicitly raises several important questions about the relationship between SA and ML that warrant further investigation:

1. How can existing sensitivity analysis methods like Morris, DGSM, GSM, VARS, or Sobol indices be effectively applied to machine learning models, and what are the specific challenges and adjustments needed for each method?

2. How can the sensitivity analysis framework be extended to include unsupervised learning methods, and what specific challenges arise in applying SA to these methods?

3. How can the sensitivity analysis of machine learning models be improved to account for feature dependencies and high-dimensional feature spaces, and what new methods or modifications to existing methods are needed?

## Limitations

- The theoretical framework may oversimplify certain aspects of ML practice where the process is not as deterministic or well-defined
- The claim that many IML methods are essentially SA computations requires careful examination of each method's implementation details
- The practical utility of applying traditional SA methods to ML systems needs validation, as computational complexity and scalability remain concerns

## Confidence

- High confidence: The ML process can be modeled as a system of interconnected functions suitable for SA (Mechanism 1)
- Medium confidence: Traditional SA methods can be directly and effectively applied to ML systems (Mechanism 2)
- Low confidence: The SA-ML bridge will significantly improve recognition of related work and reduce redundancies (Mechanism 3)

## Next Checks

1. Implement the Morris method to analyze hyperparameter sensitivity in a benchmark ML model (e.g., SVM on a standard dataset) and compare results with existing hyperparameter importance methods

2. Apply Sobol indices to quantify feature importance in a trained deep learning model and validate against established feature importance metrics like SHAP

3. Conduct a literature review of the past 5 years of IML papers to identify how many developed methods that could be characterized as SA applications, and assess whether this framing would have helped researchers identify related work earlier