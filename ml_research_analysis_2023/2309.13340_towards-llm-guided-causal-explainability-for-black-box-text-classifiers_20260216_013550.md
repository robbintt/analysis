---
ver: rpa2
title: Towards LLM-guided Causal Explainability for Black-box Text Classifiers
arxiv_id: '2309.13340'
source_url: https://arxiv.org/abs/2309.13340
tags:
- counterfactual
- text
- arxiv
- language
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors propose FLARE, a framework that uses large language
  models (LLMs) to generate counterfactual explanations for black-box text classifiers.
  The framework involves three steps: identifying latent features, identifying input
  features associated with those latent features, and generating a counterfactual
  explanation by perturbing the identified input features.'
---

# Towards LLM-guided Causal Explainability for Black-box Text Classifiers

## Quick Facts
- arXiv ID: 2309.13340
- Source URL: https://arxiv.org/abs/2309.13340
- Reference count: 13
- Authors propose FLARE framework using LLMs to generate counterfactual explanations for black-box text classifiers

## Executive Summary
This paper introduces FLARE, a framework that leverages large language models to generate counterfactual explanations for black-box text classifiers. The approach addresses the challenge of explaining complex model decisions by using LLMs' instruction-following and textual understanding capabilities to identify latent features, map them to input features, and generate minimally-edited counterfactual examples that flip the classifier's prediction. Three variants of the framework are evaluated across text classification and natural language inference tasks, demonstrating that LLM-guided counterfactual generation can provide human-understandable explanations for complex text classifiers.

## Method Summary
The FLARE framework operates in three steps: (1) identifying latent features in the input text using LLM prompts, (2) identifying input features associated with those latent features, and (3) generating counterfactual explanations by perturbing the identified input features. The framework offers three variants - FLARE (full two-step feature extraction), FLAREwords (single-step word extraction), and FLAREvanilla (direct generation without feature extraction) - allowing for different levels of LLM guidance. The method is evaluated using label flip score and semantic similarity metrics across multiple datasets and LLM models including GPT-4, ChatGPT, and Llama 2.

## Key Results
- The FLAREvanilla variant with GPT-4 generally outperforms other variants on IMDB and SNLI tasks
- Open-source models like Llama 2 show competitive performance when using the FLAREwords variant
- The framework successfully generates counterfactuals that flip classifier predictions while maintaining semantic similarity to original text

## Why This Works (Mechanism)

### Mechanism 1
LLM-based feature extraction improves counterfactual quality by grounding perturbations in task-relevant semantic features. The framework first extracts latent features (e.g., "tone", "ambiguity") using LLM prompts, then maps these to specific input words. This two-step process guides the LLM to generate counterfactuals that are semantically meaningful and minimally edited.

### Mechanism 2
Using LLM-generated counterfactuals as explanations provides human-understandable insights into black-box classifier decisions. The framework generates counterfactual examples that flip the classifier's prediction, providing interpretable explanations about what changes would alter the model's decision.

### Mechanism 3
The FLAREvanilla variant (direct generation without feature extraction) performs best with highly capable LLMs like GPT-4. Advanced LLMs can understand complex instructions and generate appropriate counterfactuals without explicit feature extraction guidance.

## Foundational Learning

- **Counterfactual explanations in machine learning**: Why needed - The entire framework is built around generating counterfactual explanations to explain black-box classifier decisions. Quick check - What distinguishes counterfactual explanations from other explanation types like feature importance or attention scores?

- **Latent feature extraction from text**: Why needed - The FLARE framework relies on identifying high-level semantic features (like "tone" or "ambiguity") before mapping them to specific input tokens. Quick check - How does identifying latent features before specific words differ from direct word-level feature importance methods?

- **Large language model prompting techniques**: Why needed - The framework uses carefully designed prompts to guide LLMs through multi-step feature extraction and counterfactual generation. Quick check - What prompting strategies are most effective for extracting structured information (like comma-separated lists) from LLMs?

## Architecture Onboarding

- **Component map**: Text input → Black-box classifier prediction → LLM feature extraction (optional) → LLM counterfactual generation → Counterfactual evaluation → Output explanation

- **Critical path**: Text input → Black-box classifier prediction → LLM feature extraction (optional) → LLM counterfactual generation → Counterfactual evaluation → Output explanation

- **Design tradeoffs**: Feature extraction step vs. direct generation adds guidance but requires LLM capability; semantic similarity vs. label flip score balance between interpretability and effectiveness; LLM model choice involves capability vs. resource constraints

- **Failure signatures**: Low label flip scores indicate ineffective counterfactuals; high edit distance with low semantic similarity suggests poor quality generation; inconsistent performance across dataset/task types suggests prompt sensitivity

- **First 3 experiments**: 
  1. Implement FLAREvanilla variant on IMDB dataset with GPT-4 to verify basic counterfactual generation works
  2. Compare FLARE vs FLAREvanilla performance on SNLI dataset to test benefit of feature extraction
  3. Test Llama 2 7B performance across all three variants to establish baseline for open-source models

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of FLARE compare to other state-of-the-art counterfactual generation methods for NLP? The paper mentions that FLARE outperforms other methods in most cases, but does not provide a direct comparison to other state-of-the-art methods.

### Open Question 2
How does the choice of LLM affect the performance of FLARE? While the paper shows that the choice of LLM affects performance, it does not provide a clear explanation for why certain LLMs perform better than others on different datasets.

### Open Question 3
How can FLARE be adapted to generate counterfactual explanations for other types of NLP tasks beyond text classification and NLI? The paper focuses on text classification and NLI, but the framework could potentially be adapted to other tasks.

## Limitations

- The effectiveness of LLM-based feature extraction is heavily dependent on the specific LLM's capabilities, with GPT-4 showing markedly better performance than open-source alternatives
- The semantic relevance of generated counterfactuals is evaluated through automated metrics rather than human judgment, which may not fully capture interpretability
- The framework's performance across diverse NLP tasks and domains remains largely unexplored, with only three datasets tested

## Confidence

- **High confidence**: The general approach of using LLMs for counterfactual generation is novel and technically sound
- **Medium confidence**: The superiority of the FLAREvanilla variant with GPT-4, as performance may be model-specific
- **Low confidence**: Claims about human-understandability of explanations without human evaluation studies

## Next Checks

1. Conduct human evaluation study to validate that generated counterfactuals are actually interpretable as explanations by human subjects
2. Test the framework across diverse NLP tasks (medical text classification, legal document analysis) to assess generalizability
3. Compare against non-LLM counterfactual generation methods (e.g., word deletion, synonym replacement) to establish baseline performance improvements