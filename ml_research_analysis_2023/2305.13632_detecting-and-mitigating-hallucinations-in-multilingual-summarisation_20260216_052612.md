---
ver: rpa2
title: Detecting and Mitigating Hallucinations in Multilingual Summarisation
arxiv_id: '2305.13632'
source_url: https://arxiv.org/abs/2305.13632
tags:
- faithfulness
- transfer
- language
- mfact
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel metric, mFACT, for evaluating the
  faithfulness of non-English summaries by leveraging translation-based transfer from
  multiple English faithfulness metrics. The authors propose a simple yet effective
  method to reduce hallucinations in cross-lingual transfer by weighing the loss of
  each training example according to its faithfulness score.
---

# Detecting and Mitigating Hallucinations in Multilingual Summarisation

## Quick Facts
- arXiv ID: 2305.13632
- Source URL: https://arxiv.org/abs/2305.13632
- Reference count: 33
- This paper introduces mFACT metric and demonstrates hallucination reduction in multilingual summarization

## Executive Summary
This paper addresses the critical challenge of detecting and mitigating hallucinations in multilingual abstractive summarization. The authors introduce mFACT, a novel metric for evaluating faithfulness of non-English summaries by leveraging translation-based transfer from multiple English faithfulness metrics. They propose a simple yet effective method to reduce hallucinations in cross-lingual transfer by weighing the loss of each training example according to its faithfulness score. Through extensive experiments across six languages, they demonstrate that mFACT is the most suited metric to detect hallucinations, and their proposed loss weighting method drastically increases both performance and faithfulness compared to strong baselines like MAD-X.

## Method Summary
The approach consists of two main components: developing the mFACT metric and reducing hallucinations through loss weighting. For mFACT, the authors formulate faithfulness evaluation as a binary classification problem, using multiple English faithfulness metrics to assign pseudo-labels to document-summary pairs, then translating these to create training data in target languages. They train a multilingual classifier (mFACT) to predict faithfulness in each target language. For hallucination reduction, they weigh each training example's loss by its faithfulness score during cross-lingual transfer training, with hallucinated examples (low scores) contributing less to gradient updates. The method is evaluated on XL-Sum dataset across six languages using MAD-X framework with various hallucination reduction techniques.

## Key Results
- mFACT outperforms NLI-based metrics (XNLI and XNLI-mFACT) in detecting hallucinations across all six tested languages
- The weighted loss approach significantly improves both ROUGE scores and faithfulness compared to MAD-X baseline
- Cross-lingual transfer methods like MAD-X increase summarization performance but also introduce more hallucinations compared to monolingual models
- mFACT maintains strong performance even when evaluated on human-translated data, validating its robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translating English faithfulness metrics to target languages via mFACT creates reliable evaluation for hallucinations.
- Mechanism: The method uses multiple English metrics to label document-summary pairs as faithful/hallucinated, translates them, and trains a multilingual classifier in each target language. This classifier (mFACT) then evaluates generated summaries in that language.
- Core assumption: Machine translation preserves the faithfulness signal sufficiently well between English and target languages.
- Evidence anchors: [abstract], [section 2.1], [corpus]
- Break condition: If machine translation introduces significant hallucinations or mistranslations that corrupt the faithfulness signal, mFACT would fail to accurately detect hallucinations in target languages.

### Mechanism 2
- Claim: Weighting training loss by faithfulness scores reduces hallucinations in cross-lingual transfer.
- Mechanism: During training, each example's loss contribution is scaled by its faithfulness score from mFACT. Hallucinated examples (low faithfulness scores) contribute less to the gradient update, steering the model away from generating similar hallucinations.
- Core assumption: The faithfulness metric's scores are predictive of actual hallucination presence and that reducing weight on hallucinated examples improves model faithfulness without sacrificing too much performance.
- Evidence anchors: [abstract], [section 3.3], [corpus]
- Break condition: If the faithfulness metric is noisy or poorly calibrated, weighting by its scores could either harm performance or have minimal effect on hallucination reduction.

### Mechanism 3
- Claim: Cross-lingual transfer methods like MAD-X improve performance but introduce more hallucinations compared to monolingual models.
- Mechanism: MAD-X uses adapter-based transfer from English to target languages. While this transfers summarization capability effectively (improving ROUGE scores), the transfer process itself introduces additional hallucinations beyond what monolingual models exhibit.
- Core assumption: The process of cross-lingual transfer inherently introduces new hallucinations even when the source language model is relatively faithful.
- Evidence anchors: [abstract], [section 6], [corpus]
- Break condition: If future work shows that cross-lingual transfer can be done without introducing additional hallucinations, or if this is specific to certain architectures or datasets.

## Foundational Learning

- Concept: Binary classification for faithfulness evaluation
  - Why needed here: The mFACT metric is built by training a binary classifier to distinguish faithful from hallucinated document-summary pairs, rather than using traditional NLI approaches.
  - Quick check question: Why might a binary classifier trained on translated data be more effective than an NLI model for faithfulness evaluation?

- Concept: Adapter-based cross-lingual transfer
  - Why needed here: The MAD-X baseline and the experimental setup rely on adapter-based transfer, where language and task adapters are trained separately and combined.
  - Quick check question: How does the adapter architecture in MAD-X differ from full-model fine-tuning, and what are the implications for hallucination generation?

- Concept: Loss weighting strategies in training
  - Why needed here: The proposed hallucination reduction method weights each training example's loss by its faithfulness score, which is a specific form of curriculum learning or data weighting.
  - Quick check question: What is the mathematical form of the weighted loss update rule, and how does it differ from standard cross-entropy training?

## Architecture Onboarding

- Component map: Translate English faithfulness-labeled data → Train mFACT classifier → Use mFACT to weight losses during cross-lingual transfer training → Evaluate using mFACT and human judges

- Critical path: Translate English summaries with faithfulness metrics → Train mBERT classifier on translated data → Apply weighted loss during MAD-X training → Evaluate with ROUGE, mFACT, and human evaluation

- Design tradeoffs:
  - Translation-based transfer vs. zero-shot transfer: Translation introduces noise but improves faithfulness detection accuracy
  - Full-model vs. adapter-based transfer: Full-model gives better ROUGE but more hallucinations; adapters balance performance and faithfulness
  - Data weighting vs. filtering: Weighting preserves performance while reducing hallucinations; filtering might hurt performance

- Failure signatures:
  - mFACT performance degrades if translation quality is poor
  - Weighted loss method underperforms if faithfulness metric is noisy
  - Cross-lingual transfer introduces more hallucinations than expected
  - Human evaluation shows disconnect between mFACT scores and actual faithfulness

- First 3 experiments:
  1. Train mFACT classifier on translated data and evaluate on test set to verify it works better than NLI baselines
  2. Compare MAD-X baseline with weighted loss method on a single language pair to confirm hallucination reduction
  3. Run human evaluation comparing MAD-X and weighted loss outputs to validate automatic metrics align with human preferences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different faithfulness metrics compare in their ability to detect hallucinations across multiple languages, and is there a single metric that performs consistently well?
- Basis in paper: [explicit] The paper introduces mFACT and compares it with XNLI and XNLI-mFACT, showing mFACT performs better. It also mentions comparing mFACT with four English metrics (DAE, QAFE, ENFS%, EntFA) in Section 5.3.
- Why unresolved: While mFACT is shown to be effective, the paper doesn't comprehensively compare it against all possible faithfulness metrics or explore if different metrics might be better suited for specific types of hallucinations or language families.
- What evidence would resolve it: A systematic evaluation of mFACT against a broader range of faithfulness metrics (both existing and newly developed) across multiple languages, including analysis of their performance on different types of hallucinations.

### Open Question 2
- Question: Does the effectiveness of the weighted loss approach for reducing hallucinations generalize to other low-resource languages beyond the six tested, particularly for languages with different typological features?
- Basis in paper: [explicit] The paper tests the weighted loss method on six typologically diverse languages (Chinese, Spanish, French, Hindi, Turkish, Vietnamese) and finds it effective, but notes limitations in Section 9.
- Why unresolved: The paper's experiments are limited to six languages. It's unclear if the approach would be equally effective for languages with significantly different structures or for which translation quality is lower.
- What evidence would resolve it: Extensive experiments applying the weighted loss method to a wider range of languages, including those with different writing systems, morphological complexity, and translation quality, to assess its generalizability.

### Open Question 3
- Question: How does the trade-off between faithfulness and abstractiveness change when using the weighted loss approach compared to other methods, and is there an optimal balance that maximizes both?
- Basis in paper: [explicit] The paper discusses the faithfulness-abstractiveness trade-off in Section 7, noting that reducing hallucinations can lead to more extractive summaries, and compares the weighted loss approach with other methods in terms of abstractiveness.
- Why unresolved: While the paper shows the weighted loss approach maintains abstractiveness while improving faithfulness, it doesn't explore if there's an optimal balance or how this balance might vary across different languages or summarization tasks.
- What evidence would resolve it: A detailed analysis of the faithfulness-abstractiveness trade-off across multiple languages and tasks when using the weighted loss approach, including experiments to find optimal parameters that maximize both qualities.

## Limitations

- The mFACT metric's reliability depends heavily on translation quality, which varies significantly across language pairs and isn't thoroughly characterized
- The claim about cross-lingual transfer introducing more hallucinations is demonstrated but not deeply analyzed for generalizability beyond the MAD-X method
- The weighted loss approach assumes faithfulness scores are accurately calibrated, but lacks theoretical justification beyond empirical demonstration

## Confidence

**High confidence** in the empirical demonstration that the weighted loss method improves both performance and faithfulness compared to MAD-X baselines.

**Medium confidence** in the mFACT metric's general effectiveness across all six languages, though reliability across different language families remains uncertain.

**Low confidence** in the generalizability of the hallucination increase claim for cross-lingual transfer, as the paper demonstrates this with one specific method on specific datasets.

## Next Checks

1. **Translation quality validation**: Conduct a systematic study of how translation quality variations affect mFACT metric performance across the six target languages, evaluating mFACT on human-translated vs machine-translated subsets and measuring correlation with actual hallucination rates.

2. **Cross-lingual transfer ablation**: Test whether the hallucination increase is specific to MAD-X adapters by comparing with other cross-lingual transfer methods (e.g., full-model fine-tuning, mBART-50 zero-shot) on the same datasets to determine if this is a general phenomenon.

3. **Weighted loss hyperparameter sensitivity**: Perform a systematic grid search over the weighting parameter λ (currently fixed at 0.5) and evaluate how sensitive hallucination reduction is to this choice across different languages and dataset sizes.