---
ver: rpa2
title: 'Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning'
arxiv_id: '2309.04459'
source_url: https://arxiv.org/abs/2309.04459
tags:
- learning
- skills
- arxiv
- which
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using byte-pair encoding (BPE) from NLP to
  generate temporally extended actions ("skills") for sparse-reward reinforcement
  learning. The method discretizes the continuous action space via clustering, then
  applies BPE with a Mahalanobis distance-based merging criterion to create a vocabulary
  of skills.
---

# Subwords as Skills: Tokenization for Sparse-Reward Reinforcement Learning

## Quick Facts
- arXiv ID: 2309.04459
- Source URL: https://arxiv.org/abs/2309.04459
- Reference count: 28
- Primary result: BPE-based skill tokenization outperforms baselines on sparse-reward RL tasks

## Executive Summary
This paper proposes using byte-pair encoding (BPE) from natural language processing to generate temporally extended actions ("skills") for sparse-reward reinforcement learning. The method discretizes continuous action spaces via k-means clustering, then applies BPE with a Mahalanobis distance-based merging criterion to create a vocabulary of skills. This approach outperforms baselines like SAC, Skill-Space Policy, and SFP on AntMaze and Kitchen domains, achieving non-zero reward where others fail. The key advantage is efficient skill generation without requiring extensive pretraining or conditioning on observations, making it more scalable and generalizable across tasks.

## Method Summary
The method generates skills through a two-stage pipeline: first discretizing the action space using k-means clustering, then applying a modified BPE scheme with Mahalanobis distance-based merging to create temporally extended action sequences. The generated skills are used with SAC-discrete for training on sparse-reward tasks. The approach leverages offline demonstration data to generate skills without requiring online interaction, and uses open-loop execution rather than observation-conditioned skills to improve efficiency and generalization.

## Key Results
- Achieves non-zero reward on AntMaze and Kitchen tasks where SAC, SSP, and SFP baselines fail
- Generates skills in minutes versus hours for other methods
- Demonstrates faster online rollouts due to discrete action space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discretizing continuous action spaces via clustering reduces effective action space complexity, improving exploration efficiency.
- Mechanism: Clustering actions into discrete tokens using k-means limits the infinite possibilities in continuous spaces to a finite set of behavioral modes. This simplification enables more tractable exploration and learning.
- Core assumption: The clustered discrete actions retain sufficient behavioral diversity to solve the downstream task.
- Evidence anchors:
  - [abstract] "Given prior evidence that the full range of the continuous action space is not required in such tasks..."
  - [section] "we discretize the action space through clustering, and second we leverage a tokenization technique borrowed from natural language processing..."
  - [corpus] Weak - neighbor papers discuss action tokenization but not the specific clustering rationale used here.
- Break condition: If the discretization loses critical action distinctions necessary for task completion.

### Mechanism 2
- Claim: Byte-pair encoding applied to clustered actions generates temporally extended, meaningful skills that capture long-range dependencies.
- Mechanism: BPE iteratively merges frequent neighboring action clusters, creating longer action sequences (subwords) that represent coordinated behaviors. This mirrors NLP's use of BPE to capture word-like units from character sequences.
- Core assumption: The merged action sequences represent coherent, reusable skills rather than arbitrary concatenations.
- Evidence anchors:
  - [abstract] "we propose a novel approach to skill-generation with two components. First we discretize the action space through clustering, and second we leverage a tokenization technique borrowed from natural language processing..."
  - [section] "Like the long-range coordination required for exploration in sparse-reward RL, language models must model long range dependencies between discrete tokens."
  - [corpus] Moderate - neighbor papers on action tokenization support the general approach but don't address the specific BPE application to RL.
- Break condition: If the merging process creates subwords that don't represent meaningful behavioral patterns.

### Mechanism 3
- Claim: Mahalanobis distance-based merging prioritizes skills that achieve diverse, task-relevant behaviors in observation space.
- Mechanism: Instead of frequency-based merging (standard BPE), the method uses distance in observation space to select merges, favoring skills that move the agent to novel states and avoid redundancy.
- Core assumption: Diverse observation-space coverage correlates with skill usefulness for exploration and task completion.
- Evidence anchors:
  - [section] "we will consider merging on a proxy for the distance traveled in the observation space in order to encourage the creation of skills that are useful for many tasks."
  - [section] "we associate a score to each possible new subword according to the Mahalanobis distance between the candidate subword and the set of existing subwords"
  - [corpus] Weak - no direct evidence in neighbors about Mahalanobis distance for action tokenization.
- Break condition: If the observation-space distance metric fails to capture true skill diversity or usefulness.

## Foundational Learning

- Concept: Reinforcement Learning with Sparse Rewards
  - Why needed here: The paper specifically targets sparse-reward settings where standard RL struggles due to insufficient exploration.
  - Quick check question: What makes sparse-reward problems particularly challenging compared to dense-reward problems?

- Concept: Hierarchical Reinforcement Learning and Temporal Abstraction
  - Why needed here: The proposed method creates temporally extended actions ("skills") that serve as higher-level abstractions over primitive actions.
  - Quick check question: How do temporally extended actions help address the exploration problem in sparse-reward settings?

- Concept: Byte-Pair Encoding and Subword Tokenization
  - Why needed here: The method adapts BPE from NLP to create action vocabularies that balance between fine-grained and coarse-grained representations.
  - Quick check question: What problem does subword tokenization solve in NLP that motivates its application to action spaces?

## Architecture Onboarding

- Component map: Demonstration data → k-means clustering → BPE merging/pruning → skill vocabulary → SAC-discrete training → task performance

- Critical path: Demonstration data → k-means clustering → BPE merging/pruning → skill vocabulary → SAC-discrete training → task performance

- Design tradeoffs:
  - Discretization granularity (k value) vs. behavioral expressiveness
  - Vocabulary size (Nmax/Nmin) vs. exploration efficiency
  - Open-loop execution vs. observation-conditioned skills
  - Mahalanobis-based merging vs. simpler frequency-based approaches

- Failure signatures:
  - Zero reward across all seeds (likely exploration failure)
  - High variance across seeds (optimization instability)
  - Skills that don't move the agent significantly (poor merging criterion)
  - Over-long skills that get stuck (excessive Nmax)

- First 3 experiments:
  1. Run k-means clustering with different k values (DOF, 2×DOF, 4×DOF) on demonstration data and visualize the resulting clusters to ensure meaningful action groupings.
  2. Apply BPE with Mahalanobis merging to a small dataset and visualize the generated skills to verify they capture meaningful behaviors.
  3. Train SAC-discrete on the generated skills for a simple sparse-reward task and verify it achieves non-zero reward before scaling to full experiments.

## Open Questions the Paper Calls Out

- How can the merging process be made more efficient for high-dimensional visual input domains?
- How does the method perform when applied to domains with continuous observation spaces rather than discrete action spaces?
- What is the optimal number of clusters (k) for the initial discretization step across different types of tasks?
- How does the method's performance scale with the size and quality of the demonstration dataset?

## Limitations

- Effectiveness relies heavily on quality of demonstration data
- Mahalanobis distance-based merging may not always correlate with true skill usefulness
- Method's generalization capabilities across diverse environments remain to be validated

## Confidence

**High confidence**: The core claim that discretization via clustering followed by BPE-based tokenization can generate useful skills for sparse-reward RL is well-supported by empirical results across multiple domains.

**Medium confidence**: The specific advantage of Mahalanobis distance-based merging over simpler frequency-based approaches is demonstrated but could benefit from more extensive ablation studies.

**Low confidence**: The claim that the method is more scalable and generalizable across tasks due to not requiring extensive pretraining or observation conditioning is somewhat speculative.

## Next Checks

1. Implement and compare the frequency-based merging approach (standard BPE) against the Mahalanobis distance-based method on a new sparse-reward environment not included in the original experiments.

2. Systematically vary the quality and diversity of demonstration data (e.g., using random trajectories vs. expert demonstrations) to quantify how sensitive the skill generation pipeline is to the input data characteristics.

3. Conduct a detailed analysis of the generated skills by visualizing their state visitation patterns and behavioral characteristics.