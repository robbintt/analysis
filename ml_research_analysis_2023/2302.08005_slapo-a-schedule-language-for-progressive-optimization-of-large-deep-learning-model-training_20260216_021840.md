---
ver: rpa2
title: 'Slapo: A Schedule Language for Progressive Optimization of Large Deep Learning
  Model Training'
arxiv_id: '2302.08005'
source_url: https://arxiv.org/abs/2302.08005
tags:
- slapo
- training
- schedule
- module
- parallelism
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Slapo introduces a schedule language to address the trade-off between
  usability and performance in large deep learning model training. It decouples model
  execution from definition, allowing developers to optimize training efficiency without
  sacrificing flexibility.
---

# Slapo: A Schedule Language for Progressive Optimization of Large Deep Learning Model Training

## Quick Facts
- arXiv ID: 2302.08005
- Source URL: https://arxiv.org/abs/2302.08005
- Reference count: 40
- Key outcome: Achieves up to 3.35x speedup on single machine with 8 GPUs and 1.41x on multi-machine setups with up to 64 GPUs

## Executive Summary
Slapo introduces a schedule language that decouples model execution from definition to optimize large deep learning model training. By using schedule primitives for optimizations like high-performance kernels, 3D parallelism, and activation checkpointing, Slapo enables progressive optimization while preserving programmability and debuggability. The framework works on PyTorch models and achieves significant performance improvements on models like BERT and GPT without requiring changes to the original model code.

## Method Summary
Slapo works on PyTorch models using a set of schedule primitives to apply common model training optimizations. The framework decouples model execution (named "schedule") from its definition, allowing developers to optimize model-specific and platform-specific schedules in a separate place. Slapo provides module primitives (e.g., .replace(), .shard(), .sync(), .checkpoint()) and compute primitives (e.g., .fuse(), .pipeline_split(), .trace()) that can be applied progressively to preserve structure hierarchy. An auto-tuner explores the best combination of parameters for a particular training environment, and a verifier ensures functional correctness of scheduled models.

## Key Results
- Achieves up to 3.35x speedup on single machine with 8 NVIDIA V100 GPUs compared to DeepSpeed and Megatron-LM
- Delivers up to 1.41x speedup on multi-machine setups with up to 64 GPUs
- Provides a unified framework for applying optimizations like FlashAttention, activation checkpointing, and 3D parallelism
- Enables auto-tuning to explore optimal schedule configurations without manual trial-and-error

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Slapo improves training efficiency by decoupling model execution from definition through a schedule language
- Mechanism: By introducing a schedule language that operates separately from the model definition, Slapo allows developers to apply optimizations like high-performance kernels, 3D parallelism, and activation checkpointing without modifying the original model code
- Core assumption: The model definition and its execution can be effectively decoupled without losing functional correctness or performance benefits
- Evidence anchors: [abstract] "Slapo works on a PyTorch model and uses a set of schedule primitives to convert the model for common model training optimizations"; [section] "We decouple model execution (named 'schedule') from its definition. As a result, the model implementation remains the same, and developers can optimize a model-specific and platform-specific schedule in a separate place."

### Mechanism 2
- Claim: Slapo enables progressive optimization through structure-preserving scheduling
- Mechanism: Slapo maintains the hierarchical structure of the original model when constructing schedules, allowing developers to apply optimizations at different levels progressively
- Core assumption: Preserving the model structure hierarchy during optimization does not significantly limit optimization opportunities while maintaining usability
- Evidence anchors: [abstract] "Compared to existing optimization solutions, Slapo progressively optimizes the model 'as-needed' through high-level primitives, and thus preserving programmability and debuggability"; [section] "Slapo preserves this hierarchy when constructing the schedule... so that developers can easily locate the module and apply scheduling"

### Mechanism 3
- Claim: Slapo's auto-tuning capability reduces the effort required to identify optimal schedule configurations
- Mechanism: By providing symbolic variable constructions and a search space framework, Slapo allows developers to define tunable parameters and automatically explore the best combinations for specific training environments
- Core assumption: The search space defined by the developer is sufficiently comprehensive to find near-optimal configurations, and the auto-tuning algorithm is efficient enough to explore it in reasonable time
- Evidence anchors: [abstract] "Slapo provides an auto-tuner to explore the best combination given a particular training environment"; [section] "We provide symbolic variable constructions to help developers build a search space with arbitrary numbers of hyperparameters in a schedule"

## Foundational Learning

- Concept: Dynamic vs. Static Graph Execution
  - Why needed here: Understanding the difference between dynamic (eager) execution in PyTorch and static graph compilation is crucial for grasping why Slapo's approach of decoupling execution from definition is beneficial
  - Quick check question: What are the main advantages and disadvantages of dynamic graphs compared to static graphs in deep learning frameworks?

- Concept: Model Parallelism and Data Parallelism
  - Why needed here: Slapo's optimization techniques include parameter sharding for tensor parallelism and pipeline partitioning. Understanding these parallelism strategies is essential for implementing and tuning Slapo schedules
  - Quick check question: How do tensor parallelism and pipeline parallelism differ in their approach to distributing model training across multiple devices?

- Concept: Activation Checkpointing
  - Why needed here: Activation checkpointing is one of the key optimizations that Slapo enables. Understanding how checkpointing works and its trade-offs between memory usage and computation is important for effective schedule tuning
  - Quick check question: What is the primary benefit of activation checkpointing in training large models, and what is the main trade-off involved?

## Architecture Onboarding

- Component map:
  Schedule Language -> Module Primitives (replace, shard, sync, checkpoint) -> Compute Primitives (fuse, pipeline_split, trace) -> Auto-Tuner -> Verifier -> Framework Dialects (DeepSpeed, Megatron-LM)

- Critical path:
  1. Create schedule from PyTorch model
  2. Apply module and parameter primitives
  3. Trace and apply compute primitives as needed
  4. Auto-tune schedule configuration
  5. Verify correctness
  6. Execute scheduled model

- Design tradeoffs:
  - Granularity of tracing vs. flexibility in applying optimizations
  - Progressive optimization vs. potential overhead from maintaining structure
  - Exhaustive search vs. efficiency in auto-tuning

- Failure signatures:
  - Model produces incorrect results after scheduling
  - Performance degradation despite applying optimizations
  - Memory issues during execution of scheduled model
  - Incompatibility with target distributed training framework

- First 3 experiments:
  1. Apply module replacement to swap a native attention layer with Flash Attention in BERT
  2. Enable activation checkpointing on a subset of layers in a transformer model
  3. Shard parameters of a large linear layer and verify output correctness after sharding and synchronization

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Performance gains may vary for model architectures beyond transformers and specific distributed training frameworks
- Auto-tuning efficiency and scalability with complex search spaces remains uncertain
- Learning curve for developers to effectively use the schedule language is unclear

## Confidence
- Slapo preserves programmability and debuggability: High confidence
- Auto-tuning capability reduces manual effort: Medium confidence
- 3.35x and 1.41x speedup claims: High confidence for reported results
- Structure-preserving scheduling is sound: High confidence

## Next Checks
1. Test Slapo's auto-tuner on a range of model sizes (from 100M to 10B parameters) to evaluate whether tuning time scales linearly or exponentially with model complexity.

2. Implement the same optimizations (FlashAttention, activation checkpointing, parameter sharding) manually in PyTorch without Slapo to quantify the productivity gains and verify that the performance benefits are not solely due to better-optimized primitives.

3. Evaluate Slapo's compatibility with emerging transformer variants (like Mamba or RWKV) and non-transformer architectures to assess the framework's generalizability beyond the demonstrated models.