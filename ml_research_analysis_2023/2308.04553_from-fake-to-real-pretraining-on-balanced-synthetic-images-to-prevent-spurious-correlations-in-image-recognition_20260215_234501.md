---
ver: rpa2
title: 'From Fake to Real: Pretraining on Balanced Synthetic Images to Prevent Spurious
  Correlations in Image Recognition'
arxiv_id: '2308.04553'
source_url: https://arxiv.org/abs/2308.04553
tags:
- bias
- synthetic
- data
- real
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of spurious correlations in visual
  recognition models caused by biased training data. The authors propose a two-stage
  training pipeline called "From Fake to Real (FFR)" that leverages synthetic data
  from generative models to mitigate this bias.
---

# From Fake to Real: Pretraining on Balanced Synthetic Images to Prevent Spurious Correlations in Image Recognition

## Quick Facts
- arXiv ID: 2308.04553
- Source URL: https://arxiv.org/abs/2308.04553
- Authors: [Not specified in source]
- Reference count: 7
- Primary result: Improves worst group accuracy by up to 20% over state-of-the-art methods on three large-scale datasets.

## Executive Summary
This paper addresses the problem of spurious correlations in visual recognition models caused by biased training data. The authors propose a two-stage training pipeline called "From Fake to Real (FFR)" that leverages synthetic data from generative models to mitigate this bias. The first stage involves pretraining the model on a balanced synthetic dataset to learn robust, unbiased features. The second stage fine-tunes the model on the real, biased dataset. This approach avoids exposing the model to the distributional differences between real and synthetic data, which can introduce additional bias. The authors demonstrate that FFR outperforms prior work on three large-scale datasets, improving worst group accuracy by up to 20%. Additionally, FFR enhances the performance of existing bias mitigation methods, achieving state-of-the-art results when combined with techniques like Domain Independence and GroupDro.

## Method Summary
The FFR pipeline consists of two stages: pretraining and fine-tuning. In the pretraining stage, a model is trained on a balanced synthetic dataset generated by a fine-tuned Stable Diffusion model. This balanced dataset ensures that the model learns features independent of the confounding variable (e.g., gender). In the fine-tuning stage, the pre-trained model is fine-tuned on the real, biased dataset using bias mitigation methods like ERM, Domain Independence, or GroupDRO. By separating the pretraining and fine-tuning stages, the model avoids learning shortcuts based on the combination of the real/synthetic indicator and the bias variable. The authors evaluate their method on three large-scale datasets (CelebA HQ, ImSitu, and Animals) and demonstrate significant improvements in worst-group accuracy.

## Key Results
- FFR improves worst group accuracy over the state-of-the-art by up to 20% on three datasets.
- FFR enhances the performance of existing bias mitigation methods when used in combination.
- The pipeline achieves state-of-the-art results when combined with techniques like Domain Independence and GroupDRO.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining on balanced synthetic data prevents the model from learning spurious correlations tied to real-world dataset biases.
- Mechanism: The first stage of FFR exposes the model only to a balanced distribution of synthetic images where P(Y|B) is uniform, forcing it to learn features independent of the confounding variable B.
- Core assumption: Synthetic images generated by stable diffusion preserve the semantic structure of real images while being free from the real-world class-B correlations.
- Evidence anchors:
  - [abstract] "The first step of FFR pre-trains a model on balanced synthetic data to learn robust representations across subgroups."
  - [section 3.2] "As a result, the model learns unbiased features in the pertaining stage and thus is less likely to relearn the spurious features in the fine-tuning stage."
  - [corpus] Weak; no direct evidence that synthetic images preserve semantics without bias.
- Break condition: If synthetic generation introduces new confounding artifacts, or if the synthetic dataset is not truly balanced across all (Y,B) combinations.

### Mechanism 2
- Claim: Avoiding joint training on real and synthetic data prevents the model from learning a shortcut that exploits the (B,G) pair.
- Mechanism: By separating pretraining and fine-tuning into distinct stages, the model never sees real and synthetic images simultaneously, thus cannot learn to predict Y from the tuple (B,G).
- Core assumption: The model can differentiate real vs synthetic images (G) and would exploit this if both domains were mixed during training.
- Evidence anchors:
  - [abstract] "Prior work's approach...do not correct the model's bias toward the pair (B,G)."
  - [section 3.1] "Denote the variable that differentiates between real and synthetic images as G...prior work...do not ensure the independence of Y from the combination of G and B together."
  - [corpus] No explicit evidence; the claim is inferred from the described problem.
- Break condition: If the model can still infer G from features learned in stage one, or if fine-tuning introduces new dependencies on (B,G).

### Mechanism 3
- Claim: Fine-tuning on real data with bias mitigation methods after synthetic pretraining improves worst-group accuracy more than applying bias mitigation directly to real data.
- Mechanism: Pretraining provides a strong, bias-free feature backbone; fine-tuning with GroupDRO or Domain Independence then corrects residual bias more effectively.
- Core assumption: Features learned from balanced synthetic data are transferable to the real data distribution.
- Evidence anchors:
  - [abstract] "Our experiments show that FFR improves worst group accuracy over the state-of-the-art by up to 20% over three datasets."
  - [section 4.2] "Note how on both datasets, our pipeline effectively improves the performance of bias mitigation methods."
  - [corpus] No corpus evidence supporting feature transferability.
- Break condition: If the synthetic-real domain gap is too large, or if fine-tuning reverts to learning spurious correlations from real data.

## Foundational Learning

- Concept: Confounding variable and spurious correlation
  - Why needed here: Understanding how B (e.g., gender) confounds the relationship between Y and the input image is central to why bias mitigation is necessary.
  - Quick check question: If most cars in the training set are red and most trucks are blue, what spurious correlation might the model learn?

- Concept: Distribution balancing and subgroup fairness
  - Why needed here: The FFR method relies on generating a balanced synthetic dataset to eliminate P(Y|B) ≠ P(Y).
  - Quick check question: What is the difference between balancing P(Y|B) and balancing P(B|Y)?

- Concept: Domain shift and transfer learning
  - Why needed here: The pretrain-then-finetune pipeline assumes features learned on synthetic data transfer to real data.
  - Quick check question: What happens to model performance if the synthetic images are visually too different from real images?

## Architecture Onboarding

- Component map: Synthetic data generator (Stable Diffusion fine-tuned per bias group) -> Balanced synthetic dataset builder -> Pretraining stage (ResNet50 or ResNet18 on synthetic data) -> Fine-tuning stage (bias mitigation method applied) -> Evaluation pipeline (worst-group and balanced accuracy metrics)

- Critical path:
  1. Fine-tune Stable Diffusion on real dataset per bias group
  2. Generate balanced synthetic images
  3. Pretrain vision model on synthetic data
  4. Fine-tune with bias mitigation method
  5. Evaluate worst-group accuracy

- Design tradeoffs:
  - More synthetic data → potentially better balance but higher generation cost
  - Finer bias group granularity → more balanced but harder to generate sufficient synthetic samples
  - Choice of bias mitigation method in fine-tuning → interacts with pretraining quality

- Failure signatures:
  - Low transfer from synthetic to real: poor accuracy after fine-tuning
  - Persistent worst-group drop: model still relies on B despite pretraining
  - Generation artifacts dominate: synthetic images too unrealistic to be useful

- First 3 experiments:
  1. Generate minimal synthetic data (1x dataset size) and run full FFR pipeline; compare to uniform balancing baseline.
  2. Run FFR with no fine-tuning bias mitigation method; assess standalone pretraining benefit.
  3. Vary the proportion of synthetic data (0.5x, 1x, 2x) and measure impact on worst-group accuracy.

## Open Questions the Paper Calls Out
[None explicitly called out in the source]

## Limitations
- The paper lacks direct evidence that synthetic images preserve semantic structure without introducing new confounding artifacts.
- There is no explicit evidence that the model can differentiate real vs. synthetic images or that avoiding joint training is necessary.
- The paper does not provide evidence supporting the transferability of features learned from balanced synthetic data to real data distributions.

## Confidence
- High confidence in the empirical results showing FFR's effectiveness in improving worst-group accuracy and enhancing bias mitigation methods.
- Medium confidence in the mechanism claims, as they are supported by theoretical reasoning but lack direct empirical validation.
- Low confidence in the assumption that synthetic data is free from biases and that features learned on synthetic data will transfer to real data without issues.

## Next Checks
1. Generate and evaluate a diverse set of synthetic images to assess their quality and ensure they do not introduce new confounding artifacts or biases.
2. Analyze the model's performance on a validation set after the pre-training stage to verify that it is learning meaningful, unbiased features.
3. Experiment with varying the proportion of synthetic data (0.5x, 1x, 2x dataset size) to determine the optimal amount needed for effective pretraining and transfer to real data.