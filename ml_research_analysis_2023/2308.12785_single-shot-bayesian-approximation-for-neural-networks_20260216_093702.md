---
ver: rpa2
title: Single-shot Bayesian approximation for neural networks
arxiv_id: '2308.12785'
source_url: https://arxiv.org/abs/2308.12785
tags:
- uncertainty
- dropout
- neural
- distribution
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a sampling-free approximation for Monte Carlo
  (MC) dropout Bayesian neural networks (BNNs). The method propagates the first two
  moments (mean and variance) through network layers using analytical formulas for
  common layer types (dropout, dense, convolution, pooling, ReLU, softmax).
---

# Single-shot Bayesian approximation for neural networks

## Quick Facts
- arXiv ID: 2308.12785
- Source URL: https://arxiv.org/abs/2308.12785
- Authors: 
- Reference count: 40
- Single-shot moment propagation matches MC dropout performance while being 200-250x faster

## Executive Summary
This paper presents a sampling-free method to approximate Monte Carlo dropout Bayesian neural networks by propagating first two moments (mean and variance) through network layers. The approach uses analytical formulas for common layer types including dropout, dense, convolution, pooling, ReLU, and softmax, enabling uncertainty estimation in a single forward pass. The method achieves similar test RMSE and NLL scores as MC dropout on UCI regression datasets while significantly reducing computation time. On CIFAR-10 classification, it matches MC dropout's accuracy and out-of-distribution detection performance without requiring any sampling.

## Method Summary
The method propagates expectation and variance through neural network layers using analytical approximations for each layer type. Dropout layers create variance through independent Bernoulli variables, while dense and convolution layers use linear transformations to propagate moments. ReLU and max pooling layers use sorting heuristics to approximate expectation and variance propagation. The softmax layer approximation assumes independent Gaussian-distributed logits and uses sigmoid-based formulas. The approach assumes Gaussian-distributed activations and neglects correlation between activations to enable efficient computation.

## Key Results
- Matches MC dropout's RMSE and NLL on UCI regression datasets while being 200-250x faster
- Achieves 74.59% accuracy on CIFAR-10 matching MC dropout performance
- Out-of-distribution detection AUC of 0.7078 matches MC dropout uncertainty estimates
- Computation time reduced from 100 seconds to 0.4 seconds per prediction

## Why This Works (Mechanism)

### Mechanism 1
Propagating first two moments through network layers allows uncertainty estimation without sampling. The method computes analytical approximations of expected value and variance at each layer using layer-specific formulas, assuming Gaussian-distributed inputs and neglecting activation correlations.

### Mechanism 2
Dropout layer creates epistemic uncertainty even with deterministic input by multiplying each node's value by independent Bernoulli variables, creating variance when input variance is zero.

### Mechanism 3
Softmax layer approximation preserves probability relationships while enabling analytical uncertainty propagation through approximations based on independent Gaussian-distributed logits.

## Foundational Learning

- **Gaussian distribution properties**: The method assumes activations follow Gaussian distributions to compute analytical moments. Quick check: What is the formula for the variance of a product of two independent random variables?

- **Delta method for error propagation**: Moment propagation approach is based on statistical error propagation principles. Quick check: How does variance change under a linear transformation?

- **Bayesian neural networks and epistemic uncertainty**: The method approximates epistemic uncertainty from MC dropout without sampling. Quick check: What is the difference between aleatoric and epistemic uncertainty?

## Architecture Onboarding

- **Component map**: Input → Dropout layer (creates variance) → Dense/Conv layers (propagate moments) → Activation layers (ReLU, Softmax) → Pooling layers (max pooling) → Output (probabilities or regression values)
- **Critical path**: Dropout → Dense/Conv → Activation → Output
- **Design tradeoffs**: Sacrifices correlation tracking between activations for computational efficiency
- **Failure signatures**: Poor uncertainty estimates when input distributions are highly non-Gaussian or when dropout masks are not independent
- **First 3 experiments**:
  1. Implement moment propagation for a single dense layer with dropout and verify against MC dropout
  2. Add ReLU activation and test on a simple regression problem
  3. Implement softmax approximation and validate on a binary classification task

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed moment propagation method perform on more complex neural network architectures beyond the ones tested in the paper? The paper only evaluates the method on specific architectures and datasets, leaving open the question of its generalizability to more complex networks.

### Open Question 2
How does the computational efficiency of the moment propagation method scale with the size and depth of the neural network? The paper demonstrates speedup but does not explore how efficiency scales with network size and depth.

### Open Question 3
How does the moment propagation method handle non-Gaussian distributions in the input or intermediate layers of the neural network? The paper assumes Gaussian distributions but does not explore performance when inputs deviate from this assumption.

## Limitations

- Relies on Gaussian approximations for activation distributions which may break down in deeper networks
- Neglects correlation between activations which can impact uncertainty estimates
- Softmax approximation lacks rigorous derivation and may introduce bias in multi-class settings

## Confidence

- **High confidence**: Computational speed improvement (200-250x faster than MC dropout)
- **Medium confidence**: Test RMSE and NLL matching MC dropout on UCI datasets
- **Medium confidence**: Out-of-distribution detection performance
- **Low confidence**: Softmax approximation accuracy for uncertainty propagation in multi-class settings

## Next Checks

1. **Activation distribution validation**: Generate histograms of network activations for UCI datasets and CIFAR-10 to verify Gaussian approximation validity, particularly after ReLU layers

2. **Correlation analysis**: Compute correlation coefficients between activations at different layers to quantify the impact of neglecting correlations in moment propagation

3. **Softmax approximation verification**: Compare analytical softmax uncertainty estimates against Monte Carlo sampling of the exact integral for a range of variance values and logit configurations