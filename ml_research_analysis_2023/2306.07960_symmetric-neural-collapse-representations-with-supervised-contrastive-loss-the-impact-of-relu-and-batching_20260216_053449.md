---
ver: rpa2
title: 'Symmetric Neural-Collapse Representations with Supervised Contrastive Loss:
  The Impact of ReLU and Batching'
arxiv_id: '2306.07960'
source_url: https://arxiv.org/abs/2306.07960
tags:
- geometry
- embeddings
- batch
- convergence
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper studies the geometry of feature embeddings learned by
  supervised contrastive loss (SCL) in deep neural networks. Prior work showed SCL
  learns a simplex equiangular tight frame (ETF) geometry under balanced data, but
  no prior work has characterized the geometry under class imbalance.
---

# Symmetric Neural-Collapse Representations with Supervised Contrastive Loss: The Impact of ReLU and Batching

## Quick Facts
- **arXiv ID**: 2306.07960
- **Source URL**: https://arxiv.org/abs/2306.07960
- **Reference count**: 40
- **Primary result**: Supervised contrastive loss with ReLU constraints learns orthogonal frame geometry regardless of class imbalance.

## Executive Summary
This paper investigates the geometry of feature embeddings learned by supervised contrastive loss (SCL) in deep neural networks. While prior work showed SCL learns a simplex equiangular tight frame (ETF) geometry under balanced data, this paper proves that SCL with ReLU constraints learns an orthogonal frame (OF) geometry regardless of class imbalance. The authors analyze the global minimizers of an unconstrained features model with SCL loss and entry-wise non-negativity constraints, then validate their theoretical predictions through extensive experiments across multiple datasets and architectures. They also demonstrate the critical role of batch selection in SCL optimization and propose a batch-binding strategy to ensure OF geometry emerges as the unique minimizer.

## Method Summary
The authors analyze SCL optimization using an unconstrained features model (UFM+) with entry-wise non-negativity constraints to represent ReLU activations. They prove that global minimizers of this system form an orthogonal frame geometry regardless of class imbalance. The method involves training deep networks with SCL loss, measuring convergence to OF geometry using metrics like ∆GM, βNC for neural collapse, and cosine similarity between class means. A batch-binding strategy adds one example from each class to every batch to guarantee OF geometry emerges as the unique minimizer.

## Key Results
- SCL with ReLU constraints converges to orthogonal frame geometry independent of class imbalance level
- Batch selection strategies critically influence the convergence quality to OF geometry
- Batch-binding strategy (adding one example from each class to every batch) guarantees OF geometry as the unique minimizer
- The learned OF geometry emerges consistently across ResNet-18, DenseNet-40 architectures on CIFAR-10, MNIST, and FashionMNIST datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SCL with ReLU constraints learns OF geometry regardless of class imbalance.
- Mechanism: ReLU activation enforces non-negative embeddings, which combined with SCL optimization, forces class-mean embeddings to be mutually orthogonal. The loss pushes embeddings from different classes to be maximally separated while embeddings within the same class collapse to their mean.
- Core assumption: Global minimizers of UFM+ with SCL loss and non-negativity constraints form OF geometry.
- Evidence anchors: Theorem 1 proves optimizers satisfy neural collapse and class-means form k-OF; extensive experiments show consistent convergence to OF across imbalance scenarios.
- Break condition: Removing ReLU or not enforcing non-negativity in final layer may prevent OF geometry emergence.

### Mechanism 2
- Claim: Batch selection critically influences convergence to OF geometry.
- Mechanism: Batch interaction graph connectivity determines whether OF geometry is the unique minimizer. If each class subgraph is connected and there are edges between different class subgraphs, OF geometry becomes the unique minimizer.
- Core assumption: Batch interaction graph must satisfy connectivity conditions for OF geometry uniqueness.
- Evidence anchors: Theoretical conditions established for batching schemes; experiments show batch shuffling affects convergence quality.
- Break condition: Fixed batch partitions without shuffling may not satisfy connectivity conditions, preventing unique OF minimizer.

### Mechanism 3
- Claim: Batch-binding guarantees OF geometry as unique minimizer.
- Mechanism: Adding one example from each class to every batch ensures batch interaction graph satisfies connectivity conditions automatically, connecting all class subgraphs and ensuring edges between different class subgraphs.
- Core assumption: Binding examples are sufficient to connect all classes in the interaction graph.
- Evidence anchors: Simple algorithm proposed; experiments show batch-binding significantly accelerates OF geometry convergence.
- Break condition: Insufficient binding examples (fewer than k for k classes) may fail to connect all classes.

## Foundational Learning

- **Unconstrained Features Model (UFM)**: Theoretical abstraction to analyze SCL geometry without network architecture complexity. *Quick check: What distinguishes UFM+ from the original UFM?*
- **Batch Interaction Graph**: Captures pairwise sample interactions within batches that determine embedding geometry. *Quick check: What are the necessary and sufficient conditions for OF geometry uniqueness?*
- **Neural Collapse (NC)**: Phenomenon where class embeddings converge to their class-mean, key component of OF geometry. *Quick check: How does ReLU relate to NC in SCL?*

## Architecture Onboarding

- **Component map**: Data → Network with ReLU → Normalization → SCL Loss → Gradient Update
- **Critical path**: Batch selection and binding examples determine whether OF geometry emerges; features must be normalized to unit norm before SCL computation
- **Design tradeoffs**: Larger batch sizes may reduce binding example impact; smaller batches may need more binding examples; temperature τ affects convergence speed but not final OF geometry
- **Failure signatures**: Non-orthogonal class-means, lack of neural collapse, slow or failed convergence to OF geometry
- **First 3 experiments**: 1) Train ResNet-18 on CIFAR-10 with SCL without batch shuffling; 2) Same model with batch shuffling; 3) Train with batch-binding strategy

## Open Questions the Paper Calls Out

- **Open Question 1**: How does temperature parameter τ affect convergence speed to OF geometry? The paper observes qualitative dependence but lacks rigorous theoretical analysis of the τ-convergence relationship.
- **Open Question 2**: Does OF geometry generalize better than ETF geometry learned by CE loss? The paper focuses on geometry convergence but doesn't systematically compare generalization performance.
- **Open Question 3**: How does model complexity affect convergence to OF geometry? The paper notes DenseNet-40 converges slower than ResNet-18 but doesn't provide detailed analysis of this relationship.

## Limitations

- Theoretical analysis relies on UFM+ which may not fully capture deep network behavior with nonlinear activations
- Batch interaction graph conditions may be sensitive to implementation details like exact sampling procedures
- Empirical validation doesn't systematically explore impact of different imbalance ratios on convergence dynamics
- Limited exploration of alternative activation functions beyond ReLU constraints

## Confidence

**High confidence**: OF geometry emerges as global minimizer under SCL with ReLU constraints (theoretical proof in UFM+ and consistent empirical observations).

**Medium confidence**: Batch selection strategies critically influence convergence to OF geometry (supported by theory and experiments, but exact sensitivity to batch size remains unclear).

**Medium confidence**: Batch-binding strategy guarantees OF geometry as unique minimizer (theoretical conditions proven, but practical implementation details could affect outcomes).

## Next Checks

1. **Stress test theoretical bounds**: Systematically vary imbalance ratio R across wider range (e.g., 1000+) and measure OF geometry emergence speed, comparing against theoretical predictions.

2. **Validate batch interaction sensitivity**: Implement controlled experiments varying batch size and sampling strategy while measuring OF geometry emergence, specifically testing batch interaction graph connectivity conditions.

3. **Extend to other activation functions**: Test whether OF geometry emerges with other activations (Leaky ReLU, GELU) under SCL to understand essential aspects of ReLU constraint.