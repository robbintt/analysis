---
ver: rpa2
title: Learning for Counterfactual Fairness from Observational Data
arxiv_id: '2307.08232'
source_url: https://arxiv.org/abs/2307.08232
tags:
- causal
- fairness
- counterfactual
- sensitive
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of achieving counterfactual fairness
  in machine learning when the underlying causal model is unknown, which is common
  in real-world scenarios. The authors propose a novel framework called CLAIRE that
  learns counterfactually fair representations from observational data without requiring
  prior causal knowledge.
---

# Learning for Counterfactual Fairness from Observational Data

## Quick Facts
- arXiv ID: 2307.08232
- Source URL: https://arxiv.org/abs/2307.08232
- Authors: 
- Reference count: 40
- One-line primary result: Novel framework achieves counterfactual fairness without requiring causal models by combining VAE-based counterfactual generation with invariant penalty learning

## Executive Summary
This paper addresses the challenge of achieving counterfactual fairness in machine learning when the underlying causal model is unknown. The authors propose CLAIRE, a framework that learns counterfactually fair representations directly from observational data using variational autoencoders for counterfactual data augmentation and invariant risk minimization to exclude spurious correlations. CLAIRE demonstrates significant improvements in counterfactual fairness while maintaining competitive prediction performance compared to existing fairness-aware baselines.

## Method Summary
CLAIRE combines a VAE-based counterfactual data augmentation module with fair representation learning using an invariant penalty. The framework generates counterfactual data by encoding observed features and prediction targets into latent embeddings, then reconstructing data with manipulated sensitive attribute values. Fair representations are learned by minimizing the discrepancy between original and counterfactual representations while excluding spurious correlations through invariant risk minimization loss. The method requires no prior causal knowledge, making it suitable for real-world scenarios where causal models are unknown.

## Key Results
- CLAIRE achieves significant improvements in counterfactual fairness (measured by MMD and Wasserstein distance) on synthetic and real-world datasets
- The framework maintains competitive prediction performance compared to existing fairness-aware baselines
- CLAIRE outperforms methods that rely on incorrect causal models, demonstrating robustness to causal model uncertainty
- The approach shows consistent improvements across multiple datasets including Law School, Adult, and synthetic data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLAIRE achieves counterfactual fairness without requiring prior causal knowledge by learning counterfactually fair representations through a variational autoencoder-based counterfactual data augmentation module.
- Mechanism: The VAE-based counterfactual data augmentation module implicitly captures causal relationships in the data by encoding the observed features and prediction target into a latent embedding space. The decoder then reconstructs the original data using these embeddings and the sensitive attribute value, enabling the generation of counterfactuals for each individual by manipulating the sensitive attribute. The learned representations from the original data and its counterfactuals are then made similar using a counterfactual fairness constraint, effectively removing the influence of the sensitive attribute.
- Core assumption: The sensitive attribute is not causally influenced by any other variables (Assumption 1 in the paper), and the structural equations of the causal model can be approximated through the VAE.
- Evidence anchors:
  - [abstract] "CLAIRE works by generating counterfactual data through a variational autoencoder-based module and learning fair representations using an invariant penalty to exclude spurious correlations."
  - [section] "To generate counterfactuals with the embeddings ð» and a manipulated sensitive attribute value later, we need to capture more 'fair' generative factors (i.e., those generative factors which are not causal influenced by ð‘†) in the embeddings, i.e., in encoder, we remove the causal influence of the sensitive attribute on the embedding ð»."
  - [corpus] Weak evidence; no direct mention of this specific VAE mechanism in neighboring papers, suggesting novelty.
- Break condition: If the sensitive attribute is causally influenced by other variables, the assumption is violated and the VAE cannot properly capture the fair generative factors.

### Mechanism 2
- Claim: CLAIRE excludes spurious correlations to further reduce potential biases by leveraging invariant risk minimization (IRM) loss.
- Mechanism: The IRM loss ensures that the predictor is optimal across all sensitive subgroups by minimizing the prediction loss under each subgroup and maximizing the gradient of the loss with respect to a scalar multiplier w. This encourages the learned representations to exclude variables with unstable spurious correlations to the target, as these correlations often vary across different sensitive subgroups and can degrade model prediction performance.
- Core assumption: If a variable directly affects the prediction target, the relationship between the variable and the target is stable across different sensitive subgroups, but for variables that do not causally affect the target, the relationship may be unstable across subgroups (Assumption 2 in the paper).
- Evidence anchors:
  - [abstract] "CLAIRE effectively mitigates the biases from the sensitive attribute with a representation learning framework based on counterfactual data augmentation and an invariant penalty."
  - [section] "To exclude the influence of such non-causal variables on the learned representations and capture the causal variables of ð‘Œ, we leverage the invariant risk minimization (IRM) loss [3] for the sensitive subgroup ð‘ ."
  - [corpus] Weak evidence; no direct mention of this specific IRM mechanism for counterfactual fairness in neighboring papers, suggesting novelty.
- Break condition: If the relationships between variables and the target are unstable across subgroups for both causal and non-causal variables, the IRM loss cannot effectively exclude spurious correlations.

### Mechanism 3
- Claim: CLAIRE outperforms methods that rely on incorrect causal models by learning fair representations directly from observational data without relying on explicit prior knowledge of the causal model.
- Mechanism: CLAIRE's counterfactual data augmentation module implicitly learns the causal relationships in the data, enabling the generation of counterfactuals for each individual with different sensitive attribute values. The fair representation learning then minimizes the discrepancy between the representations learned from the original data and its counterfactuals, effectively removing the influence of the sensitive attribute. This approach circumvents the need for explicit prior knowledge of the causal model, which can be incorrect and lead to biases in the prediction.
- Core assumption: The learned representations from the VAE can approximate the true causal relationships in the data well enough to generate meaningful counterfactuals.
- Evidence anchors:
  - [abstract] "Notably, CLAIRE outperforms methods that rely on incorrect causal models, demonstrating its robustness to causal model uncertainty."
  - [section] "Without enough knowledge about the causal model, inferring the unobserved variables and learning a fair predictor can be quite challenging."
  - [corpus] Moderate evidence; neighboring papers like "Achieving Counterfactual Fairness with Imperfect Structural Causal Model" suggest interest in causal model uncertainty, but CLAIRE's approach is distinct.
- Break condition: If the VAE fails to learn the true causal relationships in the data, the generated counterfactuals will be incorrect and the fair representation learning will not effectively remove the influence of the sensitive attribute.

## Foundational Learning

- Concept: Causal inference and counterfactual fairness
  - Why needed here: Understanding the causal relationships between variables and the prediction target is crucial for achieving counterfactual fairness, as it allows for the identification and removal of biases introduced by the sensitive attribute.
  - Quick check question: What is the difference between statistical fairness and counterfactual fairness, and why is causal reasoning necessary for the latter?

- Concept: Variational autoencoders (VAEs) and generative modeling
  - Why needed here: VAEs are used in CLAIRE's counterfactual data augmentation module to implicitly learn the causal relationships in the data and generate counterfactuals for each individual with different sensitive attribute values.
  - Quick check question: How does a VAE differ from a standard autoencoder, and what advantages does it offer for counterfactual data generation?

- Concept: Invariant risk minimization (IRM) and domain generalization
  - Why needed here: IRM is used in CLAIRE to exclude variables with spurious correlations to the prediction target, as these correlations often vary across different sensitive subgroups and can degrade model performance and fairness.
  - Quick check question: What is the key idea behind IRM, and how does it differ from traditional empirical risk minimization in terms of handling domain shifts?

## Architecture Onboarding

- Component map: Counterfactual data augmentation module (VAE) -> Fair representation learning module (IRM loss) -> Predictor
- Critical path:
  1. Train the counterfactual data augmentation module (VAE) using the observed data and sensitive attribute.
  2. Generate counterfactuals for each individual with different sensitive attribute values using the trained VAE.
  3. Learn fair representations by minimizing the discrepancy between the representations learned from the original data and its counterfactuals, while also excluding spurious correlations using the IRM loss.
  4. Train the predictor using the learned fair representations.
- Design tradeoffs:
  - VAE architecture and hyperparameters (e.g., number of layers, latent dimension) can impact the quality of the learned representations and counterfactuals.
  - Weight of the counterfactual fairness constraint (Î²) and invariant penalty (Î») must be carefully tuned to balance fairness and prediction performance.
  - Sampling number (K) in counterfactual data generation affects the diversity and quality of the generated counterfactuals.
- Failure signatures:
  - High discrepancy between predictions on original data and counterfactuals indicates insufficient counterfactual fairness.
  - Poor prediction performance suggests that the learned representations may not capture the true causal relationships in the data.
  - Large gradients of the IRM loss indicate the presence of variables with spurious correlations that are not effectively excluded.
- First 3 experiments:
  1. Train the counterfactual data augmentation module (VAE) on a small subset of the data and visualize the learned embeddings to assess their quality and fairness.
  2. Generate counterfactuals for a few sample instances and compare their distributions to the original data to verify the effectiveness of the counterfactual generation.
  3. Train the fair representation learning module on a small dataset and evaluate the counterfactual fairness and prediction performance to ensure the overall pipeline is working as expected.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CLAIRE's counterfactual data augmentation module handle unobserved variables when their true causal relationships with observed variables are unknown?
- Basis in paper: [explicit] The paper mentions that CLAIRE "maps the observed variables to a latent representation space to encode the unobserved variables that can facilitate the prediction" and uses a VAE-based module for counterfactual generation
- Why unresolved: The paper doesn't provide details on how the framework identifies which unobserved variables are causally relevant to the prediction target without knowing the causal structure
- What evidence would resolve it: Experimental results showing CLAIRE's performance degradation when key unobserved causal variables are missing from the data, or a detailed analysis of how the learned representations capture different types of unobserved variables

### Open Question 2
- Question: What is the theoretical relationship between the counterfactual fairness constraint and invariant risk minimization in CLAIRE?
- Basis in paper: [inferred] The paper combines counterfactual fairness constraints with invariant risk minimization loss but doesn't provide theoretical justification for their interaction
- Why unresolved: While both components are shown to improve fairness, the paper doesn't explain whether they address complementary aspects of fairness or if one subsumes the other
- What evidence would resolve it: A formal proof showing whether counterfactual fairness implies invariance or vice versa under certain conditions, or empirical results demonstrating their additive or redundant effects

### Open Question 3
- Question: How does CLAIRE's performance scale with the dimensionality of the feature space and complexity of the true causal model?
- Basis in paper: [explicit] The paper mentions that "in real world, the underlying causal model is often unknown, especially when the data is high-dimensional" but doesn't provide scalability analysis
- Why unresolved: The experimental results only cover relatively simple datasets with few features, leaving unclear how the framework would perform on high-dimensional data
- What evidence would resolve it: Systematic experiments varying the number of features and causal complexity, showing how CLAIRE's fairness and prediction performance change with increasing dimensionality

## Limitations

- The paper's empirical evaluation is limited to three datasets, which may not fully capture the framework's performance across diverse real-world scenarios.
- The assumption that the sensitive attribute is not causally influenced by other variables may not hold in many practical applications, potentially limiting the framework's applicability.
- The paper does not provide a thorough analysis of computational complexity or scalability for larger, high-dimensional datasets.

## Confidence

**High Confidence**: The core methodology of combining VAE-based counterfactual generation with invariant penalty learning is technically sound and well-justified. The theoretical framework for achieving counterfactual fairness without requiring causal models is rigorously developed.

**Medium Confidence**: The empirical results showing superior performance to baseline methods are convincing but based on a limited number of datasets. The claim that CLAIRE "outperforms methods that rely on incorrect causal models" is supported but could benefit from more extensive comparison across diverse causal misspecification scenarios.

**Low Confidence**: The paper does not thoroughly address computational complexity or scalability concerns for larger datasets with high-dimensional features. The robustness of the approach to various types of causal misspecification remains incompletely characterized.

## Next Checks

1. **Ablation study on hyperparameter sensitivity**: Systematically vary the counterfactual fairness constraint weight (Î²), invariant penalty weight (Î»), and sampling number (K) to quantify their impact on both fairness metrics and prediction performance.

2. **Robustness testing across causal misspecification types**: Evaluate CLAIRE's performance when the true causal model violates Assumption 1 (sensitive attribute influenced by other variables) or contains non-linear causal relationships not captured by the VAE approximation.

3. **Computational complexity analysis**: Measure training time and memory requirements as a function of dataset size and dimensionality, comparing against baseline methods to establish practical scalability limits.