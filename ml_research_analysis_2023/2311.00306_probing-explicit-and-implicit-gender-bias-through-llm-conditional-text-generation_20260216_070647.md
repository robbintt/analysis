---
ver: rpa2
title: Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation
arxiv_id: '2311.00306'
source_url: https://arxiv.org/abs/2311.00306
tags:
- gender
- bias
- inputs
- llms
- explicit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a conditional text generation framework to\
  \ evaluate both explicit and implicit gender biases in large language models (LLMs).\
  \ The method uses three distinct input strategies\u2014template-based, LLM-generated,\
  \ and naturally-sourced\u2014without requiring predefined gender-related phrases\
  \ or stereotypes."
---

# Probing Explicit and Implicit Gender Bias through LLM Conditional Text Generation

## Quick Facts
- arXiv ID: 2311.00306
- Source URL: https://arxiv.org/abs/2311.00306
- Authors: 
- Reference count: 40
- Key outcome: Introduces a conditional text generation framework evaluating explicit and implicit gender bias in LLMs using three input strategies without predefined gender-related phrases

## Executive Summary
This work introduces a conditional text generation framework to evaluate both explicit and implicit gender biases in large language models (LLMs). The method uses three distinct input strategies—template-based, LLM-generated, and naturally-sourced—without requiring predefined gender-related phrases or stereotypes. It evaluates bias through gender-attribute scores, co-occurrence ratios, and Jensen-Shannon divergence (JSD) scores. Experiments on six LLaMA variants show that larger models do not necessarily exhibit less bias, and that even gender-neutral inputs can induce significant bias in generated outputs. All tested LLMs demonstrated explicit and/or implicit gender bias, highlighting the need for effective bias mitigation strategies.

## Method Summary
The framework evaluates gender bias in LLMs through conditional text generation using three input strategies: template-based inputs with predefined stereotypes, LLM-generated neutral statements, and naturally-sourced sentences from STS-B corpus. For each strategy, LLMs complete partial sentences and outputs are analyzed using explicit metrics (gender-attribute scoring) and implicit metrics (co-occurrence ratios and JSD scores). The method tests six LLaMA variants (7B, 7B-chat, 13B, 13B-chat, 70B, 70B-chat) with 50-token generations per input, comparing bias patterns across model sizes and input types.

## Key Results
- Larger models do not necessarily exhibit less gender bias across all input strategies
- Even gender-neutral inputs can induce significant implicit bias in generated outputs
- All tested LLMs demonstrated explicit and/or implicit gender bias regardless of size or chat capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework successfully evaluates implicit gender bias by generating text completions from neutral inputs.
- Mechanism: By using inputs that contain no explicit gender markers (e.g., "My friend is talking on the phone"), the model's outputs are analyzed for gender attribution through co-occurrence ratios and JSD scores, revealing underlying biases.
- Core assumption: The model's output distribution changes meaningfully when conditioned on neutral vs. gendered prompts, allowing implicit bias detection.
- Evidence anchors:
  - [abstract] "even gender-neutral inputs can induce significant bias in generated outputs"
  - [section] "we design explicit and implicit evaluation metrics to assess gender bias in LLMs under different strategies"
  - [corpus] Weak: Related work focuses on bias evaluation but not specifically on neutral-input-based implicit bias detection.
- Break condition: If the model's generation distribution remains uniform across gender attributes for neutral inputs, implicit bias cannot be detected.

### Mechanism 2
- Claim: Larger models do not necessarily exhibit less gender bias, challenging the assumption that scale improves fairness.
- Mechanism: Experiments on six LLaMA variants show that increased model size does not consistently correlate with lower gender bias scores across all input strategies.
- Core assumption: Model size and fairness are independent variables in this bias evaluation framework.
- Evidence anchors:
  - [abstract] "Experiments on six LLaMA variants show that larger models do not necessarily exhibit less bias"
  - [section] "larger model does not necessarily equate to increased fairness"
  - [corpus] Weak: Most related work assumes larger models reduce bias; this work directly challenges that.
- Break condition: If larger models consistently show lower bias scores across all strategies, the claim fails.

### Mechanism 3
- Claim: The three-input strategy approach captures both explicit and implicit bias more comprehensively than single-strategy methods.
- Mechanism: Template-based inputs target known stereotypes, LLM-generated inputs test for learned biases, and naturally-sourced inputs probe real-world neutral language contexts.
- Core assumption: Different input strategies reveal different facets of model bias that single-strategy approaches miss.
- Evidence anchors:
  - [section] "we explore three different strategies for bias exploration: template-based, LLM-generated, and naturally-sourced strategies"
  - [section] "Our experiments reveal that a model with a larger size does not necessarily equate to greater fairness"
  - [corpus] Moderate: Related work uses single strategies; this work combines them for broader coverage.
- Break condition: If all three strategies yield identical bias patterns, the multi-strategy approach offers no advantage.

## Foundational Learning

- Concept: Conditional text generation
  - Why needed here: The framework relies on prompting LLMs to complete partial sentences under different strategies to reveal bias patterns.
  - Quick check question: What is the difference between unconditional and conditional text generation in LLM evaluation?

- Concept: Jensen-Shannon Divergence (JSD)
  - Why needed here: JSD measures distributional differences between gender attribute word probabilities, quantifying implicit bias.
  - Quick check question: How does JSD differ from KL divergence in measuring distribution similarity?

- Concept: Co-occurrence ratio
  - Why needed here: This metric calculates the probability of gender attribute words appearing next in the generated sequence, directly measuring bias strength.
  - Quick check question: Why is co-occurrence ratio more informative than simple word frequency counts for bias detection?

## Architecture Onboarding

- Component map: Input strategy generator → LLM conditional generator → Output parser → Bias evaluator (explicit + implicit metrics) → Results aggregator
- Critical path: Input strategy → LLM generation → Output parsing → Bias metric calculation
- Design tradeoffs: Trade-off between input strategy diversity and computational cost; trade-off between explicit metric interpretability and implicit metric sensitivity
- Failure signatures: All bias metrics returning zero across strategies (false negative); inconsistent results across strategies without clear explanation
- First 3 experiments:
  1. Test template-based strategy with known gender stereotypes to verify explicit bias detection works
  2. Test LLM-generated strategy to confirm it produces gender-neutral prompts
  3. Test naturally-sourced strategy with a small subset to verify implicit bias detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do implicit biases manifest differently across input strategies, and what does this reveal about the internal mechanisms of LLMs?
- Basis in paper: The paper compares template-based, LLM-generated, and naturally-sourced inputs but does not deeply analyze how implicit biases vary across these strategies.
- Why unresolved: The paper notes differences in bias levels but does not explore the underlying reasons or mechanisms causing these variations.
- What evidence would resolve it: Comparative analysis of internal activations or logit distributions for each input strategy, along with correlation studies between input features and bias outputs.

### Open Question 2
- Question: Can bias mitigation techniques be effectively evaluated using the proposed conditional text generation framework, and how do these techniques perform across different input strategies?
- Basis in paper: The paper identifies the presence of bias but does not test or evaluate any mitigation techniques.
- Why unresolved: No experiments were conducted to assess the effectiveness of bias mitigation methods, leaving their applicability untested.
- What evidence would resolve it: Application of debiasing techniques (e.g., counterfactual augmentation, adversarial training) and measurement of bias reduction across the three input strategies.

### Open Question 3
- Question: Does the absence of explicit gender stereotypes in inputs always lead to reduced bias, or are there cases where implicit biases dominate regardless of input neutrality?
- Basis in paper: The paper shows that even neutral inputs can induce bias, but it does not explore the limits or conditions under which implicit biases persist.
- Why unresolved: The study does not systematically test the relationship between input neutrality and bias persistence across diverse contexts.
- What evidence would resolve it: Controlled experiments varying degrees of input neutrality and measuring corresponding bias levels, along with analysis of context-sensitive bias triggers.

## Limitations
- The framework requires significant computational resources to run three distinct input strategies across multiple large models
- Reproducibility is challenged by unspecified template word lists and LLM generation parameters
- Difficulty distinguishing genuine implicit bias from model uncertainty in neutral contexts

## Confidence
- **High confidence**: The framework's overall methodology for explicit bias detection through template-based inputs and gender-attribute scoring is well-established and clearly explained.
- **Medium confidence**: Claims about implicit bias detection, the relationship between model size and fairness, and the benefits of multi-strategy approaches require additional validation and exploration of underlying mechanisms.
- **Low confidence**: The paper's assertions about real-world bias prevalence and the framework's generalizability to non-English contexts lack sufficient empirical support.

## Next Checks
1. Perform hypothesis tests (e.g., paired t-tests or non-parametric alternatives) to determine whether differences in bias scores between model sizes and input strategies are statistically significant, not just numerically apparent.
2. Implement a control condition where inputs are explicitly balanced for gender (e.g., "My male friend" vs "My female friend") to establish baseline bias levels and verify that the framework can detect known biases before claiming to find implicit ones.
3. Test the framework on at least one non-English LLM variant (e.g., LLaMA models fine-tuned on multilingual data) to assess whether the three-strategy approach generalizes across languages and cultural contexts, addressing a critical gap in the current evaluation.