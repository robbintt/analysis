---
ver: rpa2
title: 'TrojFSP: Trojan Insertion in Few-shot Prompt Tuning'
arxiv_id: '2312.10467'
source_url: https://arxiv.org/abs/2312.10467
tags:
- prompt
- input
- samples
- class
- trojfsp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of backdoor attacks on few-shot
  prompt tuning in pre-trained language models (PLMs). The core method idea is TrojFSP,
  which introduces three techniques: Target-Class Shrink (TC-Shrink) to balance the
  poisoned dataset, Selective Token Poisoning to mitigate overfitting, and Trojan-Trigger
  Attention to enhance attack performance.'
---

# TrojFSP: Trojan Insertion in Few-shot Prompt Tuning

## Quick Facts
- arXiv ID: 2312.10467
- Source URL: https://arxiv.org/abs/2312.10467
- Reference count: 11
- Primary result: Achieves >99% ASR while maintaining negligible CDA decreases across various PLMs and datasets

## Executive Summary
This paper introduces TrojFSP, a backdoor attack specifically designed for few-shot prompt tuning in pre-trained language models. The attack addresses the unique challenges of backdoor attacks in few-shot settings by introducing three novel techniques: Target-Class Shrink to balance poisoned datasets, Selective Token Poisoning to prevent overfitting, and Trojan-Trigger Attention to enhance attack effectiveness. The method achieves attack success rates exceeding 99% while maintaining clean data accuracy, representing significant improvements over existing prompt-based backdoor attacks.

## Method Summary
TrojFSP targets few-shot prompt tuning scenarios where only 16 samples per class are available. The attack introduces three key innovations: (1) Target-Class Shrink (TC-Shrink) balances poisoned datasets by reducing clean target-class samples to equalize class sizes and prevent target-class bias, (2) Selective Token Poisoning identifies and poisons only low-importance prompt tokens to mitigate overfitting in the few-shot regime, and (3) Trojan-Trigger Attention adds an attention loss that maximizes poisoned prompt attention to triggers during attack while minimizing it otherwise. The method combines these techniques with standard CDA and ASR losses to achieve high attack success while preserving clean data accuracy.

## Key Results
- Achieves ASR >99% across RoBERTa-Large, T5-Base, and GPT-J models
- Improves ASR by 9-48% compared to prior prompt-based backdoor attacks
- Maintains CDA with negligible decreases (4-9% improvement over baselines)
- Effective across multiple datasets including SST-2, SST-5, MR, Twitter, and LingSpam

## Why This Works (Mechanism)

### Mechanism 1: Target-Class Shrink (TC-Shrink)
- Claim: Balances poisoned datasets by reducing clean target-class samples, equalizing class sizes and improving CDA
- Core assumption: Equal class sample counts prevent target-class bias and maintain non-target CDA
- Evidence: TC-Shrink adds corrective factor β to CDA loss ensuring m·β + m·α·(n-1) = m for equal class counts
- Break condition: If poisoned samples cannot be accurately relabeled or β cannot maintain equality, imbalance persists and CDA degrades

### Mechanism 2: Selective Token Poisoning
- Claim: Updating only low-importance tokens prevents overfitting in few-shot backdoor attacks
- Core assumption: Few tokens dominate prompt impact; pruning high-importance tokens preserves model behavior
- Evidence: Computes importance scores for each prompt token and poisons only tokens with lowest scores
- Break condition: If all tokens have similar importance, poisoning any token causes overfitting or performance loss

### Mechanism 3: Trojan-Trigger Attention
- Claim: Maximizes backdoor effectiveness by directing attention to triggers during attack, minimizing it otherwise
- Core assumption: Prompt attention correlates with attack success; controlling attention flow controls backdoor behavior
- Evidence: Adds attention loss LAT T N that maximizes poisoned prompt attention to poisoned inputs with triggers
- Break condition: If PLM attention mechanism is robust to external loss terms or trigger patterns are too weak to capture attention, ASR degrades

## Foundational Learning

- Concept: Few-shot prompt tuning
  - Why needed here: TrojFSP operates specifically in few-shot settings where only 16 samples per class are available
  - Quick check question: In few-shot prompt tuning, what is the typical number of samples per class used for training?

- Concept: Backdoor attacks in NLP
  - Why needed here: The paper frames TrojFSP as a backdoor attack; knowing how label-flipping and trigger-based attacks work in NLP provides context
  - Quick check question: What is the primary goal of a backdoor attack in a classification setting?

- Concept: Attention mechanisms in transformers
  - Why needed here: Trojan-Trigger Attention relies on manipulating attention scores; understanding self-attention in transformers explains how attention loss can steer behavior
  - Quick check question: In a transformer, what does a high attention score between a prompt token and input token signify?

## Architecture Onboarding

- Component map: Input datasets -> Prompt tuning module (importance scoring, selective token update) -> Combined loss function (CDA + ASR + TC-Shrink + Attention) -> Frozen PLM + tuned prompt -> Output predictions
- Critical path: 1) Load few-shot dataset, 2) Generate syntactic triggers and create poisoned samples, 3) Compute token importance scores, 4) Select tokens to poison (lowest importance), 5) Train prompt with combined loss, 6) Evaluate ASR and CDA on validation set
- Design tradeoffs: Token poisoning length (fewer tokens reduce overfitting but may weaken attack), β and α tuning (balance attack success and clean accuracy), λ1 weight (controls emphasis on trigger awareness vs. classification accuracy)
- Failure signatures: ASR < 90% (likely overfitting or weak trigger attention), CDA loss > 5% (poisoned dataset imbalance or excessive token poisoning), Training loss << testing loss (overfitting, reduce poisoned tokens or adjust λ1)
- First 3 experiments: 1) Baseline: Train with all 20 tokens poisoned, no attention loss, measure overfitting and ASR/CDA, 2) TC-Shrink only: Apply β correction, measure CDA improvement on non-target classes, 3) Full TrojFSP: Apply selective token poisoning + attention loss, measure maximum ASR/CDA trade-off

## Open Questions the Paper Calls Out

- How effective is TrojFSP in attacking text generation tasks compared to classification tasks? The paper evaluates on classification datasets but notes findings might not generalize to generation tasks like translation or summarization.

- What are the implications of black-box attacks on APIs for TrojFSP? While attackers can implement attacks on shared platforms and open-sourced models, the landscape of black-box attacks involving APIs remains unclear and requires further research.

- How can more efficient and accurate defense methods against TrojFSP be developed? The paper proposes a potential defense that minimizes ASR by selectively pruning prompt tokens, but even after pruning TrojFSP can achieve ASR >40%, indicating a need for more effective defenses.

## Limitations
- TC-Shrink mechanism lacks ablation studies showing CDA performance when disabled
- Selective Token Poisoning assumes token importance scores remain stable during training without validation
- Trojan-Trigger Attention mechanism's effectiveness depends on PLM attention patterns being predictable and manipulable through external loss terms

## Confidence
- High confidence in overall attack framework and reported ASR improvements (9-48% gains)
- Medium confidence in TC-Shrink mechanism's effectiveness due to lack of ablation studies
- Medium confidence in Selective Token Poisoning claims due to missing importance score stability analysis
- Low confidence in Trojan-Trigger Attention mechanism's general applicability due to missing attention analysis

## Next Checks
1. Run ablation study validation by testing TrojFSP without TC-Shrink and without Trojan-Trigger Attention separately to quantify each component's contribution to ASR and CDA

2. Visualize attention patterns between poisoned prompt tokens and input tokens across training epochs to measure how Trojan-Trigger Attention loss affects attention distribution

3. Track token importance score rankings throughout training to measure how often top-ranked tokens change positions and whether poisoning low-importance tokens maintains their low-importance status across epochs