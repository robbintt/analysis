---
ver: rpa2
title: 'GeoFormer: Predicting Human Mobility using Generative Pre-trained Transformer
  (GPT)'
arxiv_id: '2311.05092'
source_url: https://arxiv.org/abs/2311.05092
tags:
- mobility
- data
- human
- tokens
- period
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GeoFormer, a decoder-only transformer model
  adapted from GPT for human mobility prediction. The model reformulates mobility
  data into tokenized sequences analogous to sentences, allowing it to leverage transformer
  architectures for autoregressive generation.
---

# GeoFormer: Predicting Human Mobility using Generative Pre-trained Transformer (GPT)

## Quick Facts
- arXiv ID: 2311.05092
- Source URL: https://arxiv.org/abs/2311.05092
- Reference count: 40
- Key outcome: Achieved top-3 ranking in HuMob Challenge 2023 with GEO-BLEU 0.316 (Task 1) and 0.183 (Task 2), DTW 26.22 (Task 1) and 37.78 (Task 2)

## Executive Summary
This paper presents GeoFormer, a decoder-only transformer model adapted from GPT for human mobility prediction. The model reformulates mobility data into tokenized sequences analogous to sentences, allowing it to leverage transformer architectures for autoregressive generation. GeoFormer was tested on two standardized datasets in the HuMob Challenge 2023, achieving top-3 ranking performance. It scored 0.316 GEO-BLEU and 26.22 DTW for Task 1 (normal mobility) and 0.183 GEO-BLEU and 37.78 DTW for Task 2 (emergency mobility), demonstrating strong predictive capability for both normal and emergency human mobility patterns.

## Method Summary
GeoFormer is a decoder-only transformer (GPT) that linearizes mobility data into tokenized sequences with day-of-week and user ID tokens. The model is trained autoregressively on 8-day mobility sequences to predict the 9th day. For inference, it generates the next day's trajectory token-by-token while constraining candidate tokens to locations visited at similar timestamps on the same day-of-week. The model uses 12 transformer layers with 24 attention heads and 768 embedding dimensions, trained with AdamW optimizer and cosine learning rate schedule.

## Key Results
- Top-3 ranking in HuMob Challenge 2023 for both Task 1 (normal mobility) and Task 2 (emergency mobility)
- GEO-BLEU scores of 0.316 (Task 1) and 0.183 (Task 2)
- DTW scores of 26.22 (Task 1) and 37.78 (Task 2)
- Successfully handled both normal and emergency mobility prediction scenarios

## Why This Works (Mechanism)

### Mechanism 1
Linearizing human mobility data into tokenized sequences allows a transformer decoder to model mobility as autoregressive next-step prediction, similar to language modeling. By representing each location as independent x and y tokens and embedding them in a sequential input, the model learns P(x_t, y_t | past trajectory). The autoregressive structure enables generation of future positions token-by-token.

### Mechanism 2
Constraining candidate tokens during generation to locations visited in similar past contexts (same dow, similar timestamp) improves realism and prevents hallucinations. During inference, the model only samples from x and y tokens previously observed at neighboring timestamps on the same day-of-week, reducing out-of-distribution generation.

### Mechanism 3
Prefix individual user ID tokens allow the model to condition on long-term personal mobility patterns, improving personalization. User ID tokens are mapped to embeddings that the transformer can use to encode persistent individual characteristics, enabling better prediction for unseen future trajectories.

## Foundational Learning

- Concept: Autoregressive sequence modeling in transformers
  - Why needed here: The GeoFormer generates mobility one token at a time, conditioned on previous tokens; this requires understanding autoregressive decoding.
  - Quick check question: In a transformer decoder, how does masking ensure that position j only attends to positions < j during training?

- Concept: Tokenization and embedding of categorical/geospatial data
  - Why needed here: Locations are encoded as discrete x and y tokens; understanding how to map continuous coordinates to tokens and embeddings is essential.
  - Quick check question: If you have 500 unique x coordinates and 500 unique y coordinates, how many total token types are used for location representation?

- Concept: Day-of-week and temporal conditioning
  - Why needed here: The model must learn periodic patterns; incorporating dow tokens and constraining generation by time is critical.
  - Quick check question: How would you modify the input representation if you wanted to condition on both day-of-week and hour-of-day instead of just dow?

## Architecture Onboarding

- Component map: Input linearization module -> GPT decoder -> Constraint filter -> Loss and training loop
- Critical path: 1. Prepare linearized dataset with user_id, dow, x, y tokens. 2. Train GPT decoder autoregressively on full 8-day sequences. 3. At inference, feed 7-day history, generate next day token-by-token using constrained candidate set. 4. Convert generated x and y token sequence back to coordinates.
- Design tradeoffs: Representing (x, y) as separate tokens reduces vocabulary size but may lose joint spatial correlation. Using 8-day input for 1-day output trades context length for simplicity; longer context may improve accuracy but increases compute. Constraining candidates reduces hallucinations but may miss new locations in emergencies.
- Failure signatures: Low GEO-BLEU but low DTW indicates plausible sequences not matching ground truth distribution. High GEO-BLEU but high DTW indicates BLEU-style token overlap without temporal consistency. Training divergence may indicate learning rate too high or insufficient regularization.
- First 3 experiments: 1. Train with and without user_id tokens to measure personalization impact. 2. Vary top-k candidate constraint size (1, 3, 5, 10) and measure GEO-BLEU vs DTW tradeoff. 3. Test different input context lengths (7-day vs 14-day) to see if longer context improves next-day prediction.

## Open Questions the Paper Calls Out

### Open Question 1
How does the GeoFormer model handle situations where individuals exhibit significant deviations from their typical weekly mobility patterns, such as during special events or holidays that don't align with the standard day-of-week cycles? The model's architecture is designed around weekly periodicity, but doesn't discuss performance when behavior significantly deviates from this pattern due to special events or holidays.

### Open Question 2
What is the impact of the linearization approach (representing x and y coordinates independently) on the model's ability to capture spatial relationships between consecutive locations, especially in cases where the actual visited location is a combination of x and y coordinates not seen during training? While the paper justifies the independent representation approach, it doesn't provide empirical evidence on how this affects spatial reasoning capabilities.

### Open Question 3
How sensitive is the GeoFormer model to the choice of temperature and top-k parameters during generation, and what is the optimal range of these parameters for balancing GEO-BLEU and DTW scores across different types of mobility patterns? The paper mentions some experimentation with these parameters but doesn't provide a detailed sensitivity analysis or identify optimal ranges for different mobility patterns.

## Limitations

- Tokenization of continuous spatial coordinates into discrete x and y tokens may lose important joint spatial correlations
- One-week context window assumption may not capture longer-term mobility patterns or adapt well to significant behavioral changes
- Constraint mechanism may prevent the model from discovering new locations during emergency periods
- Evaluation metrics (GEO-BLEU and DTW) provide complementary but limited views of performance

## Confidence

**High Confidence**: The core mechanism of reformulating mobility data into tokenized sequences for transformer-based autoregressive generation is well-supported by results and methodology.

**Medium Confidence**: The effectiveness of user ID conditioning and constraint-based generation mechanism are reasonably supported but rely on assumptions about the model's ability to learn meaningful embeddings and temporal consistency of mobility patterns.

**Low Confidence**: The paper does not provide detailed analysis of failure cases or systematic exploration of the model's limitations under different mobility scenarios.

## Next Checks

1. **Ablation Study of Key Components**: Systematically evaluate GeoFormer performance with and without user ID tokens, with different constraint window sizes (1, 3, 5, 10 timestamps), and with different context lengths (7-day vs 14-day input).

2. **Robustness Testing Under Mobility Changes**: Test the model's performance when trained on data with artificially introduced mobility pattern changes (e.g., sudden location changes, new locations, or altered periodicity).

3. **Metric Sensitivity Analysis**: Evaluate the correlation between GEO-BLEU and DTW scores with practical metrics such as prediction coverage and accuracy of next-step predictions.