---
ver: rpa2
title: 'The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the
  Hidden States of Over-Confident Large Language Models'
arxiv_id: '2310.11877'
source_url: https://arxiv.org/abs/2310.11877
tags:
- unanswerable
- question
- answer
- questions
- answerable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how Large Language Models (LLMs) handle
  unanswerable questions, focusing on whether they encode the fact that a question
  is unanswerable even when they generate a hallucinated answer. Through experiments
  on three QA benchmarks, the authors find that LLM hidden states strongly encode
  answerability information: models are more likely to generate an "unanswerable"
  response when given a hint in the prompt, and a beam of decoded responses often
  contains such a reply even if the top response is hallucinatory.'
---

# The Curious Case of Hallucinatory (Un)answerability: Finding Truths in the Hidden States of Over-Confident Large Language Models

## Quick Facts
- arXiv ID: 2310.11877
- Source URL: https://arxiv.org/abs/2310.11877
- Reference count: 40
- Large Language Models encode answerability information in their hidden states even when generating hallucinated answers.

## Executive Summary
This paper investigates whether Large Language Models (LLMs) encode the fact that a question is unanswerable even when they generate hallucinated answers. Through experiments on three QA benchmarks, the authors find that LLM hidden states strongly encode answerability information: models are more likely to generate an "unanswerable" response when given a hint in the prompt, and a beam of decoded responses often contains such a reply even if the top response is hallucinatory. Linear classifiers trained on last-layer representations can predict answerability with high accuracy (F1 scores over 75% without hints, over 80% with hints). Erasing the answerability subspace degrades these classification and QA performance metrics, confirming the subspace's causal role.

## Method Summary
The paper conducts experiments across three QA benchmarks (SQuAD 2.0, NATURAL QUESTIONS, MuSiQue) using three large language models (Flan-T5-xxl, Flan-UL2, OPT-IML). The methodology involves prompt manipulation (zero-shot and few-shot settings with and without hints), beam search relaxation to find unanswerability-acknowledging responses, linear probing on last-layer embeddings to detect answerability, and linear concept erasure to test the causal role of the answerability subspace. The experiments measure unanswerability classification accuracy (F1 score), QA performance (exact match and token-wise F1), and the impact of subspace erasure on both tasks.

## Key Results
- Linear classifiers achieve F1 scores over 75% for unanswerability prediction without hints, and over 80% with hints
- Beam relaxation increases the likelihood of finding "unanswerable" responses in the beam set for unanswerable questions
- Erasing the answerability subspace degrades both classification accuracy and QA performance metrics
- The first decoded token's embedding strongly correlates with answerability information
- The answerability subspace is causally necessary for the model's ability to distinguish answerable from unanswerable questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The first decoded token's embedding captures answerability information.
- Mechanism: The model encodes whether a question is answerable into its latent representations before generating any answer tokens, and this information is accessible from the first output token's embedding.
- Core assumption: The answerability state is encoded linearly in the embedding space and influences early decoding decisions.
- Evidence anchors:
  - [abstract] "the representation of the first decoded token often being a strong indicator"
  - [section 3.3] "for each instance, we take the embedding from the final hidden layer of the first generated token and train a linear classifier, using logistic regression, to predict answerability"
  - [corpus] Weak: No direct corpus evidence for first-token encoding; the corpus neighbors focus on SAEs and QA robustness but not token-level probing.
- Break condition: If the answerability signal is nonlinear or distributed across multiple tokens, linear probing of the first token would fail.

### Mechanism 2
- Claim: Beam search can reveal answerability because the answerability subspace exists across multiple beams.
- Mechanism: Even when the top beam contains a hallucinated answer, other beams in the set often contain a correct "unanswerable" response, because the answerability signal is preserved in the model's distribution over outputs.
- Evidence anchors:
  - [abstract] "a beam of decoded responses for unanswerable queries often contains a response recognizing the unanswerability of the question"
  - [section 3.2] "we gradually increase the beam size... we search for a reply within the final k options that signifies unanswerability"
  - [corpus] Weak: No corpus evidence directly supporting beam-level answerability detection; neighbors discuss SAEs and QA evaluation but not beam search decoding.
- Break condition: If beam search pruning or diversity constraints remove the answerability signal from lower-ranked beams.

### Mechanism 3
- Claim: Erasing the answerability subspace degrades model performance on both answerability classification and QA tasks.
- Mechanism: The answerability subspace is causally used by the model to decide whether to answer; removing it forces the model to lose this distinction.
- Evidence anchors:
  - [abstract] "Erasing the answerability subspace degrades these classification and QA performance metrics, confirming the subspace's causal role"
  - [section 3.4] "we use the recently proposed method of Belrose et al. (2023)... to erase arbitrary linearly-encoded concepts from neural representations"
  - [corpus] Weak: The corpus neighbors mention interpretability and SAEs but not causal erasure experiments.
- Break condition: If the model uses alternative pathways or nonlinear representations for answerability beyond the identified subspace.

## Foundational Learning

- Concept: Linear separability in embedding space
  - Why needed here: The paper relies on linear classifiers to detect answerability from embeddings, so understanding linear separability is crucial.
  - Quick check question: Can you train a logistic regression to distinguish answerable vs unanswerable questions with >75% accuracy using just the first token's embedding?

- Concept: Beam search and decoding algorithms
  - Why needed here: The paper's beam relaxation experiment depends on understanding how beam search explores the output space and how lower-ranked beams can contain different answers.
  - Quick check question: If you increase beam size from 1 to 5, does the probability of finding an "unanswerable" response in the beam increase for unanswerable questions?

- Concept: Concept erasure and linear projection
  - Why needed here: The causal role of the answerability subspace is tested by erasing it; understanding how linear projection can remove a concept is essential.
  - Quick check question: After erasing the answerability subspace, does the classifier's accuracy on answerability drop significantly, and does the model's tendency to answer unanswerable questions increase?

## Architecture Onboarding

- Component map: Input (Question + Context) -> Encoder (Flan-T5/UL2/OPT-IML) -> Decoder (Token-by-token generation) -> Probing module (Linear classifier on first token embedding) -> Erasure module (Closed-form linear projection matrix)

- Critical path:
  1. Prompt formatting (zero/few-shot, with/without hint)
  2. Forward pass through model
  3. Extract first token embedding from last layer
  4. Linear classifier prediction (optional)
  5. Beam search generation
  6. Post-processing (check for "unanswerable" in beam)

- Design tradeoffs:
  - Linear vs nonlinear probing: Linear is faster and more interpretable but may miss complex encodings.
  - Beam size vs latency: Larger beams increase chance of finding "unanswerable" but cost more compute.
  - Erasure vs performance: Removing answerability helps interpretability but hurts QA accuracy on answerable questions.

- Failure signatures:
  - Low classifier accuracy → answerability not linearly separable in embeddings
  - No improvement with beam relaxation → answerability signal lost in beam search
  - Erasure doesn't hurt performance → answerability subspace not causal or redundant

- First 3 experiments:
  1. Train a logistic regression on first-token embeddings to classify answerable vs unanswerable; measure F1.
  2. Run beam search with increasing beam sizes; check if "unanswerable" appears in lower beams for unanswerable questions.
  3. Apply linear erasure to training embeddings; retrain classifier and observe accuracy drop; test QA performance on erased model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the size of a language model correlate with its ability to encode and detect unanswerable questions?
- Basis in paper: [explicit] The paper states "when considering the standard prompt, the F1 of the probe is above 75% for all models and datasets" and discusses differences between Flan-T5xxl, Flan-UL2, and OPT-IML, but does not explicitly analyze the correlation between model size and unanswerability detection performance.
- Why unresolved: The experiments compare models of different sizes but do not systematically investigate whether larger models are inherently better at encoding unanswerability.
- What evidence would resolve it: A controlled study varying only model size while keeping other factors constant, measuring unanswerability detection accuracy across a spectrum of model sizes.

### Open Question 2
- Question: How does the presence of an answerability subspace in the model's hidden states affect its performance on answerable questions?
- Basis in paper: [explicit] The paper finds that erasing the answerability subspace "has very little negative impact on the answerable questions, with only a slight degradation in the exact match and F1 scores."
- Why unresolved: While the paper shows minimal impact, it doesn't explore the nuances of how this subspace might influence the quality or style of answers to answerable questions.
- What evidence would resolve it: Detailed analysis comparing model outputs on answerable questions before and after subspace erasure, including qualitative assessment of answer quality and coherence.

### Open Question 3
- Question: Can the techniques developed for detecting unanswerable questions be extended to other forms of hallucinations, such as those arising from outdated or irrelevant information?
- Basis in paper: [inferred] The paper focuses on unanswerable questions but the underlying concept of detecting when a model lacks knowledge to answer could apply to broader hallucination detection.
- Why unresolved: The study is limited to the specific case of unanswerable questions within a given context, not addressing other hallucination scenarios.
- What evidence would resolve it: Experiments applying similar probing and decoding techniques to detect various types of hallucinations beyond simple unanswerability, measuring effectiveness across different hallucination categories.

## Limitations

- The paper relies on the assumption that answerability is linearly separable in the embedding space, which may not capture the full complexity of how models represent unanswerability
- The effectiveness of prompt engineering (hints, few-shot examples) may vary significantly across different domains and model architectures
- The generalizability of findings beyond the three tested benchmarks and model families remains unclear
- The causal mechanism for first-token encoding is weakly supported by the corpus evidence

## Confidence

**High Confidence**: The empirical observations that (1) linear classifiers achieve high accuracy on answerability prediction, (2) beam relaxation often finds "unanswerable" responses in lower beams, and (3) erasure degrades performance are all well-supported by the experimental results.

**Medium Confidence**: The interpretation that these results demonstrate "strong encoding" of answerability in hidden states assumes linear separability captures the complete representation. While the evidence is compelling, alternative explanations (e.g., surface-level pattern matching, prompt sensitivity) cannot be ruled out.

**Low Confidence**: The causal mechanism claiming that the first decoded token's embedding captures answerability information is weakly supported. The corpus lacks evidence for this specific mechanism, and the probing methodology doesn't establish temporal encoding dynamics or distribution across tokens.

## Next Checks

1. **Nonlinear Probing Validation**: Apply nonlinear probing methods (e.g., small MLPs, attention-based classifiers) to the first token embeddings and compare performance with linear classifiers. If nonlinear methods show substantial improvement, it would challenge the linear separability assumption and suggest the encoding is more complex than claimed.

2. **Cross-Beam Consistency Analysis**: For each unanswerable question, extract the first token embeddings from all beams that contain "unanswerable" responses. Analyze whether these embeddings cluster together and whether they differ systematically from embeddings in beams containing hallucinated answers. This would validate whether beam relaxation genuinely reveals preserved answerability information.

3. **Zero-Shot Transfer Test**: Evaluate the linear classifiers trained on one benchmark (e.g., SQuAD 2.0) on unanswerable questions from entirely different domains (e.g., medical QA, legal QA). Strong transfer performance would support the claim of general answerability encoding, while poor performance would suggest domain-specific surface patterns.