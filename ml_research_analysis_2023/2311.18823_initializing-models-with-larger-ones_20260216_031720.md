---
ver: rpa2
title: Initializing Models with Larger Ones
arxiv_id: '2311.18823'
source_url: https://arxiv.org/abs/2311.18823
tags:
- selection
- weight
- pretrained
- training
- initialization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces weight selection, a method for initializing
  smaller neural network models by selecting subsets of weights from a pretrained
  larger model. The approach consists of three steps: layer selection, component mapping,
  and element selection.'
---

# Initializing Models with Larger Ones

## Quick Facts
- arXiv ID: 2311.18823
- Source URL: https://arxiv.org/abs/2311.18823
- Reference count: 8
- One-line primary result: Weight selection method significantly improves small model performance and reduces training time across nine image classification datasets

## Executive Summary
This paper introduces weight selection, a novel method for initializing smaller neural network models by selecting subsets of weights from pretrained larger models. The approach consists of three steps: layer selection, component mapping, and element selection. The authors demonstrate that this method significantly improves the performance of small models across nine image classification datasets, reducing training time and enabling compatibility with knowledge distillation. Experiments show consistent accuracy gains over random initialization, with the best results achieved using uniform element selection and first-N layer selection.

## Method Summary
Weight selection initializes smaller models by selecting subsets of weights from pretrained larger models through a three-step process: layer selection (first-N or uniform), component mapping (matching corresponding components between teacher and student), and element selection (uniform, consecutive, or random with/consistency). The method is tested on nine image classification datasets using ViT-T/16 and ConvNeXt-F models initialized from ViT-S/16 and ConvNeXt-T teachers trained on ImageNet-21K. Training follows an adapted ConvNeXt recipe with specific batch sizes, learning rates, and epochs per dataset.

## Key Results
- Weight selection consistently outperforms random initialization across nine datasets, with uniform element selection and first-N layer selection yielding the best results
- The method reduces training time by matching the accuracy of 60 epochs of pretraining + finetuning at just 10 epochs
- Weight selection is compatible with knowledge distillation, further improving performance when combined with KD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight selection improves convergence speed by providing a better initial parameter distribution than random initialization
- Mechanism: By selecting a subset of weights from a pretrained model, the smaller student model inherits a distribution of parameters that already encodes useful feature representations learned from large-scale data
- Core assumption: The pretrained model's weights capture general feature representations that are transferable to the smaller model's architecture
- Evidence anchors: [abstract] "weight selection can significantly enhance the performance of small models and reduce their training time"; [section 4.2] "Training curves for ImageNet-1K are shown in Figure 3. Both models benefit from weight selection early on and maintain this advantage throughout training"

### Mechanism 2
- Claim: Consistency in element selection is critical for maintaining the structural integrity of learned representations
- Mechanism: When selecting elements from the teacher's weight tensor, maintaining the same selection indices across all weight matrices preserves the relationships between neurons and layers that were established during pretraining
- Core assumption: The pretrained model's weights have learned correlations between different components that should be preserved during selection
- Evidence anchors: [section 3.2] "consistency (selecting the same indices for all weight matrices) is key for weight selection to reach its best performance"; [section 3.2] "motivation for maintaining consistency stems from the existence of residual connections — neurons that are added in the teacher model should have their operations preserved in the student"

### Mechanism 3
- Claim: Weight selection provides a computationally efficient alternative to pretraining from scratch
- Mechanism: Instead of training a small model from random initialization or fine-tuning a large pretrained model, weight selection directly transfers knowledge through parameter selection, avoiding the need for expensive pretraining on the target dataset
- Core assumption: The computational cost of selecting weights from a pretrained model is negligible compared to training from scratch
- Evidence anchors: [abstract] "weight selection offers a new approach to leverage the power of pretrained models in resource-constrained settings"; [section 4.4] "When compared to pretraining (on ImageNet-1K) + finetuning, weight selection is able to match the accuracy at 60 epochs of pretraining, saving 6.12x training time"

## Foundational Learning

- Concept: Modular neural network design
  - Why needed here: The paper relies on the fact that modern neural networks are built from replicated layers with consistent component structures, enabling systematic weight selection
  - Quick check question: Why can we select the first N layers from a pretrained model to initialize a smaller model with N layers?

- Concept: Weight initialization strategies
  - Why needed here: Understanding how weight selection compares to traditional methods like Xavier and Kaiming initialization is crucial for evaluating its effectiveness
  - Quick check question: What are the key differences between weight selection and random initialization methods like Xavier or Kaiming?

- Concept: Knowledge transfer in deep learning
  - Why needed here: The paper positions weight selection as a form of knowledge transfer, so understanding concepts like transfer learning and knowledge distillation is important
  - Quick check question: How does weight selection differ from knowledge distillation in terms of what knowledge is being transferred?

## Architecture Onboarding

- Component map: Layer selection → Component mapping → Element selection → Model initialization → Training
- Critical path: Layer selection → Component mapping → Element selection → Model initialization → Training
- Design tradeoffs: Smaller teacher models provide better initialization but may have less general knowledge; uniform element selection is simpler but may not capture all useful information; first-N layer selection works well for most cases but may not be optimal for all architectures
- Failure signatures: Poor performance compared to random initialization indicates selection method issues; training instability suggests structural relationships were broken during selection; no improvement in convergence speed means the initialization didn't capture useful information
- First 3 experiments:
  1. Compare weight selection with random initialization on a small dataset using ViT-T and ConvNeXt-F
  2. Test different element selection methods (uniform, consecutive, random with/without consistency) on CIFAR-100
  3. Evaluate the effect of different layer selection methods (first-N vs uniform) on model performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does weight selection performance scale with increasingly large teacher models (e.g., LLaMA-30B) for initializing smaller variants?
- Basis in paper: [explicit] The paper mentions that weight selection could be useful for large model training, e.g., initializing a LLaMA-7B with weights from LLaMA-30B, and shows experiments with ViT-L (307M parameters) as teacher showing performance benefits
- Why unresolved: The paper only provides limited empirical evidence with one large-scale experiment (ViT-L to ViT-S), lacking comprehensive analysis of performance scaling, computational overhead, and practical limitations when dealing with extremely large teacher models
- What evidence would resolve it: Systematic experiments comparing weight selection performance across multiple teacher-student size ratios, including very large models like LLaMA variants, measuring both accuracy gains and computational costs

### Open Question 2
- Question: What is the theoretical foundation for why consistency in element selection (same indices across all weight tensors) is crucial for optimal performance?
- Basis in paper: [explicit] The paper demonstrates through experiments that random selection without consistency leads to sharp performance drops, and states that consistency preserves complete neurons and residual connections, but doesn't provide theoretical justification
- Why unresolved: The paper provides empirical observations about the importance of consistency but lacks theoretical analysis explaining why maintaining the same indices across weight tensors is critical for knowledge transfer effectiveness
- What evidence would resolve it: Mathematical analysis or theoretical framework explaining how consistency in element selection preserves architectural relationships and knowledge transfer mechanisms in neural networks

### Open Question 3
- Question: How does weight selection compare to structured pruning methods specifically designed for model compression when both are applied to initialize smaller models?
- Basis in paper: [explicit] The paper compares weight selection to L1 pruning and magnitude pruning extensions, showing L1 pruning yields better results than random initialization but worse than weight selection, but this comparison uses pruning methods not specifically designed for initialization
- Why unresolved: The comparison uses pruning methods that weren't designed for initialization purposes, and doesn't include modern structured pruning techniques specifically developed for efficient model initialization or compression
- What evidence would resolve it: Head-to-head comparison with state-of-the-art structured pruning methods designed for model initialization, including computational efficiency analysis and performance metrics across multiple model architectures and datasets

## Limitations
- Weight selection performance depends heavily on architectural compatibility between teacher and student models
- The method's effectiveness varies across different model architectures, with inconsistent results for some architectures like ConvNeXt-F
- First-N layer selection may not capture optimal weight configurations for all model architectures and tasks

## Confidence
- High confidence: Claims about weight selection consistently outperforming random initialization across multiple datasets and architectures
- Medium confidence: Claims about computational efficiency gains and training time reduction, as these depend on specific hardware and implementation details
- Medium confidence: Claims about knowledge distillation compatibility, as the experiments focus primarily on standalone model performance

## Next Checks
1. **Architecture-specific validation**: Test weight selection across a broader range of architecture families (e.g., ResNet, EfficientNet) to verify generalizability beyond ViT and ConvNeXt variants

2. **Selection method comparison**: Systematically compare first-N layer selection against more sophisticated layer selection strategies (e.g., importance-based or gradient-aware selection) to quantify potential performance gains

3. **Cross-domain transferability**: Evaluate weight selection when teacher and student models are trained on distinctly different domains (e.g., natural images vs. medical imaging) to establish the method's robustness to domain shifts