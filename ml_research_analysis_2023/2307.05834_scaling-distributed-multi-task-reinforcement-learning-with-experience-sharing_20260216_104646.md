---
ver: rpa2
title: Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing
arxiv_id: '2307.05834'
source_url: https://arxiv.org/abs/2307.05834
tags:
- learning
- agents
- agent
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper tackles the challenge of distributed multi-task reinforcement
  learning (RL), where a group of N agents collaboratively solve M tasks without prior
  knowledge of their identities. The core idea is to formulate the problem as linearly
  parameterized contextual Markov decision processes (MDPs), where each task is represented
  by a context that specifies the transition dynamics and rewards.
---

# Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing

## Quick Facts
- arXiv ID: 2307.05834
- Source URL: https://arxiv.org/abs/2307.05834
- Reference count: 40
- Primary result: Achieves ε-optimal policies for M tasks using at most O(d³H⁶(ε⁻² + c⁻²Sep) · M/N) episodes per agent in distributed setting

## Executive Summary
This paper addresses distributed multi-task reinforcement learning where N agents collaboratively solve M tasks without prior knowledge of task identities. The proposed DistMT-LSVI algorithm leverages linearly parameterized contextual MDPs and task separability to enable efficient experience sharing. By distributing the workload across agents, the algorithm achieves a 1/N improvement in sample complexity compared to non-distributed approaches while maintaining theoretical guarantees on policy optimality.

## Method Summary
The DistMT-LSVI algorithm operates by having each agent independently learn ε-optimal policies for all M tasks while sharing information through a central server. Agents identify tasks using a separability constant (cSep) that measures differences in optimal value functions. The algorithm combines experience collection, task identification via server-side checks, and optimistic value iteration with confidence bounds. A SimNet component measures task similarity, while a Lifelong Learning model using EWC and DQN handles policy computation and experience sharing.

## Key Results
- Achieves ε-optimal policies for all M tasks with sample complexity scaling as O(d³H⁶(ε⁻² + c⁻²Sep) · M/N) per agent
- Demonstrates 1/N improvement over non-distributed settings where each agent independently learns all tasks
- Validated on OpenAI Gym Atari environments with N=20 agents and M=10 tasks
- Shows effective task identification and experience sharing through central server coordination

## Why This Works (Mechanism)

### Mechanism 1: Task separability enables identification
- **Claim:** Task separability enables agents to identify and distinguish tasks without explicit labels
- **Mechanism:** Uses separability constant (cSep) to measure value function differences at initial state
- **Core assumption:** |V*_m,1(s0) - V*'_m',1(s0)| > cSep for any distinct task pair
- **Break condition:** Small cSep relative to value function noise causes misidentification

### Mechanism 2: Distributed computation reduces sample complexity
- **Claim:** Distributed computation reduces sample complexity by factor of 1/N
- **Mechanism:** Parallel agent operation with workload distribution
- **Core assumption:** Agents can run independently without interference
- **Break condition:** Communication overhead dominates theoretical speedup

### Mechanism 3: Optimism enables efficient exploration
- **Claim:** Optimism in value function estimation enables efficient exploration
- **Mechanism:** Upper confidence bounds in planning phase guide exploration
- **Core assumption:** Linear MDP structure enables tractable confidence bound computation
- **Break condition:** Loose bounds cause inefficient exploration; tight bounds prevent finding optimal policies

## Foundational Learning

- **Linear MDPs with function approximation**
  - Why needed: Enables confidence bound computation and optimism in value functions
  - Quick check: How does linear MDP assumption enable confidence interval computation?

- **Contextual MDP framework**
  - Why needed: Each task represented by context specifying transition dynamics and rewards
  - Quick check: What distinguishes contextual MDP from standard MDP?

- **Multi-task reinforcement learning**
  - Why needed: Learning policies optimizing performance across all M tasks
  - Quick check: How does multi-task RL sample complexity compare to independent learning?

## Architecture Onboarding

- **Component map:** Agent modules (experience collection, task identification, policy computation) -> Central server (separability checking, policy storage) -> Communication layer (secure exchange) -> Planning algorithm (optimistic value iteration)

- **Critical path:** Task assignment and initial exploration (K1 episodes) → Task identification via separability check with central server → Policy computation (retrieval or learning with K2 episodes) → Policy storage and sharing with central server

- **Design tradeoffs:** Communication frequency vs. computational efficiency, confidence bound tightness vs. exploration efficiency, task separability constant selection vs. identification accuracy

- **Failure signatures:** High misidentification rate despite sufficient cSep, communication bottlenecks preventing timely sharing, suboptimal policies despite correct task identification

- **First 3 experiments:** Single agent learning multiple tasks without communication (baseline), two agents with overlapping task assignments (separability mechanism test), scaling test with increasing N (1/N improvement verification)

## Open Questions the Paper Calls Out

- How would the algorithm perform under more general MDP classes with non-linear dynamics?
- How would the algorithm perform under adversarial streaming sequences of tasks?
- How does the task-separability assumption affect performance and can it be relaxed?

## Limitations

- Theoretical guarantees rely on linear MDP structure that may not hold in many practical settings
- Empirical validation limited to Atari environments with fixed N=20 and M=10 agents
- Communication overhead and server coordination costs at scale not characterized
- Task separability assumption may not hold for tasks with similar optimal value functions

## Confidence

- **High Confidence:** 1/N sample complexity improvement follows directly from distributed computation model
- **Medium Confidence:** Task separability mechanism works for tested Atari environments but may degrade with less distinct tasks
- **Low Confidence:** Performance with fewer than 20 agents, more than 10 tasks, or non-visual domains remains unknown

## Next Checks

1. **Scale sensitivity test:** Run experiments with N=5, N=10, and N=50 agents while keeping M=10 tasks fixed to verify 1/N scaling holds across different agent counts

2. **Task separability stress test:** Create synthetic task families with controlled cSep values (0.1 to 1.0) to measure misidentification rates near theoretical threshold

3. **Communication overhead characterization:** Measure wall-clock time and bandwidth usage as N increases beyond 20 agents to identify when communication costs dominate computational benefits