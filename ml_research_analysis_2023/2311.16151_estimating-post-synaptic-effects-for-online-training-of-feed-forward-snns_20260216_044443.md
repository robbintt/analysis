---
ver: rpa2
title: Estimating Post-Synaptic Effects for Online Training of Feed-Forward SNNs
arxiv_id: '2311.16151'
source_url: https://arxiv.org/abs/2311.16151
tags:
- otpe
- training
- learning
- layer
- ottt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Online Training with Post-synaptic Estimates
  (OTPE), a novel approximation of Real-Time Recurrent Learning (RTRL) designed for
  efficient online training of spiking neural networks (SNNs). The key innovation
  of OTPE is its approximation of multi-step temporal sequences through a spiking
  neuron, which are excluded from previous algorithms like OSTL and OTTT.
---

# Estimating Post-Synaptic Effects for Online Training of Feed-Forward SNNs

## Quick Facts
- arXiv ID: 2311.16151
- Source URL: https://arxiv.org/abs/2311.16151
- Authors: 
- Reference count: 29
- Key outcome: OTPE achieves 75.2% accuracy on Spiking Heidelberg Digits vs 70.5% for OTTL/OSTL, with improved gradient alignment to BPTT in deep networks

## Executive Summary
This paper introduces Online Training with Post-synaptic Estimates (OTPE), an approximation of Real-Time Recurrent Learning (RTRL) designed for efficient online training of spiking neural networks (SNNs). OTPE addresses the limitations of previous algorithms (OSTL, OTTT) by maintaining a trace of parameter influence over multiple time-steps, capturing multi-step temporal sequences excluded from earlier approaches. The method achieves improved scaling for multi-layer networks with minimal overhead in time and space complexity, demonstrating superior gradient alignment with exact BPTT and better performance on temporal encoding tasks.

## Method Summary
OTPE implements a comprehensive approximation of temporal effects in SNNs by maintaining a running weighted sum of parameter influence across time-steps. The algorithm selectively applies surrogate derivatives to spatial gradients while using Heaviside step functions for temporal gradients, enabling layer-local computation of temporal effects. An approximate variant (A-OTPE) further reduces complexity to O(n) space by assuming the membrane potential's influence on spike output is approximately equivalent to a running weighted average of surrogate gradients. The method uses Leaky Integrate-and-Fire (LIF) neurons with fast sigmoid surrogate gradients (slope 25) and the Adamax optimizer.

## Key Results
- OTPE achieves 75.2% accuracy on Spiking Heidelberg Digits compared to 70.5% for OTTL/OSTL and 78.1% for BPTT
- OTPE exhibits the highest directional alignment to exact gradients calculated with BPTT in deep networks
- The approximate OTPE variant maintains O(n) space complexity while delivering superior learning performance
- OTPE outperforms other approximate methods on time-based encoding tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OTPE approximates temporal effects on subsequent layers by maintaining a running weighted sum of parameter influence over multiple time-steps.
- Mechanism: OTPE selectively applies surrogate derivatives to spatial gradients while using Heaviside step function (no surrogate) for temporal gradients, enabling layer-local computation of temporal effects.
- Core assumption: The influence of previous time-step spikes on current membrane potentials can be captured by a running weighted sum without recursive gradient calculations.
- Evidence anchors:
  - [abstract] "OTPE maintains a trace of parameter influence over multiple time-steps, implementing a comprehensive approximation of the entire temporal effect in a one-hidden-layer model."
  - [section] "Specifically, we do not apply the surrogate derivative for ∂sl+1t/∂Ul+1t when calculating ∂Ul+1t+1/∂Ul+1t during the calculation of the temporal dynamics in layer l."
  - [corpus] Weak - neighbors discuss temporal hierarchy and plasticity but not this specific running sum approximation.

### Mechanism 2
- Claim: OTPE achieves better gradient alignment with BPTT than OSTL and OTTT by capturing multi-step temporal sequences excluded from previous algorithms.
- Mechanism: By including residual temporal effects in gradient calculations, OTPE reduces approximation error that accumulates when error is back-propagated through deep layers.
- Core assumption: The difference in learning performance between OSTL and BPTT reflects the impact of residual temporal effects not captured by OSTL.
- Evidence anchors:
  - [abstract] "OTPE exhibits the highest directional alignment to exact gradients, calculated with backpropagation through time (BPTT), in deep networks"
  - [section] "This approximation restricts temporal influence of the gradient calculations to the current output from a single layer, which neglects how the previous outputs of a neuron impact the membrane potential of downstream neurons."
  - [corpus] Weak - neighbors discuss online learning and temporal dynamics but not this specific gradient alignment comparison.

### Mechanism 3
- Claim: Approximate OTPE maintains O(n) space complexity while delivering superior learning performance through second-order filtering of inputs.
- Mechanism: Approximate OTPE stores a weighted sum of OTTT's weighted sum (ˆa) and maintains a running weighted average of surrogate gradients, reducing the temporal gradient matrix to vectors.
- Core assumption: The membrane potential's influence on spike output at each time-step can be approximated by the running weighted average of surrogate gradients.
- Evidence anchors:
  - [section] "we assume the membrane potential's influence on the spike output at each time-step is approximately equivalent to the running weighted average ¯g."
  - [abstract] "This approximation incurs minimal overhead in the time and space complexity compared to similar algorithms"
  - [corpus] Weak - neighbors don't discuss this specific approximation technique or space complexity trade-offs.

## Foundational Learning

- Concept: Real-Time Recurrent Learning (RTRL)
  - Why needed here: RTRL provides the theoretical foundation for exact gradient computation in stateful models without temporal unrolling, which OTPE approximates.
  - Quick check question: What is the space complexity of exact RTRL for a layer with n neurons?

- Concept: Forward-mode differentiation
  - Why needed here: OTPE uses forward-mode differentiation to compute Jacobian-vector products for online learning, contrasting with BPTT's reverse-mode approach.
  - Quick check question: How does forward-mode differentiation enable online learning compared to reverse-mode?

- Concept: Surrogate gradients for spiking neurons
  - Why needed here: Since spiking neurons have discontinuous activation functions, surrogate gradients replace the Dirac delta function to enable gradient-based optimization.
  - Quick check question: Why can't we use standard backpropagation directly on spiking neurons without surrogate gradients?

## Architecture Onboarding

- Component map:
  - LIF neuron layer with membrane potential tracking and spiking mechanism
  - Spatial gradient calculator using surrogate derivatives
  - Temporal gradient calculator maintaining running weighted sums
  - Layer-local gradient aggregation combining spatial and temporal components
  - Approximate variant with second-order filtering approximation

- Critical path:
  1. Forward pass through LIF layers with membrane potential updates
  2. Spatial gradient calculation using surrogate derivatives
  3. Temporal gradient accumulation through running weighted sums
  4. Layer-local gradient combination
  5. Loss computation and parameter update

- Design tradeoffs:
  - Exact OTPE vs Approximate OTPE: Better accuracy vs better scalability (O(n²) vs O(n))
  - Layer depth impact: Deeper networks benefit more from OTPE's temporal approximations
  - Memory vs performance: Running weighted sums increase memory but improve gradient quality
  - Computational locality: Layer-local calculations simplify implementation but may miss cross-layer temporal effects

- Failure signatures:
  - Gradient cosine similarity drops significantly in early hidden layers
  - Training loss plateaus prematurely despite high learning rates
  - Performance degradation in temporal encoding tasks compared to rate encoding
  - Memory usage grows quadratically with layer width in exact OTPE implementation

- First 3 experiments:
  1. Compare OTPE vs OSTL/OTTT gradient cosine similarity on a 2-hidden-layer network with 128 width on T-Randman
  2. Evaluate memory usage scaling of exact OTPE vs Approximate OTPE on increasing layer widths
  3. Test OTPE performance on temporal vs rate encoding tasks to verify temporal dynamics capture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does OTPE's performance scale with network depth beyond 5 layers, and at what point does the temporal approximation error become prohibitive?
- Basis in paper: [inferred] The paper shows OTPE outperforms OSTL/OTTT up to 5 layers but doesn't explore deeper networks. The authors note that gradient approximation worsens as error is backpropagated through deeper layers.
- Why unresolved: The paper only tests up to 5 layers. Deeper networks are common in modern architectures and could reveal limitations of the temporal approximation.
- What evidence would resolve it: Testing OTPE on networks with 10+ layers on benchmarks like SHD or ImageNet would reveal the depth at which performance degrades significantly compared to BPTT.

### Open Question 2
- Question: Can the F-OTPE variant's use of cross-entropy loss on a leaking sum of outputs maintain temporal credit assignment accuracy compared to applying loss at each timestep?
- Basis in paper: [explicit] The authors propose F-OTPE as an extension that applies cross-entropy loss to a leaking sum of outputs rather than individual timesteps, but don't provide extensive evaluation.
- Why unresolved: While this approach enables offline-style training metrics for online learning, it may lose fine-grained temporal information needed for precise credit assignment.
- What evidence would resolve it: Comparative experiments showing gradient alignment and accuracy between F-OTPE and timestep-wise loss approaches on temporal tasks like T-Randman would clarify trade-offs.

### Open Question 3
- Question: How does OTPE's computational efficiency compare to BPTT when accounting for gradient checkpointing and memory optimizations?
- Basis in paper: [inferred] The paper claims OTPE has similar scalability to OSTL but doesn't compare memory usage or wall-clock time against optimized BPTT implementations.
- Why unresolved: While OTPE avoids temporal unrolling, modern BPTT can use gradient checkpointing to reduce memory. A fair comparison requires accounting for these optimizations.
- What evidence would resolve it: Benchmarking OTPE against BPTT with gradient checkpointing on identical hardware using wall-clock time and peak memory measurements would provide clarity.

## Limitations
- The paper's claim about OTPE's O(n) space complexity for the approximate variant is stated but not empirically verified against theoretical bounds for larger networks
- Temporal approximation accuracy degrades in deeper networks beyond the tested 3-layer configuration, though this limitation is not explicitly discussed
- The comparison to BPTT as the "exact" baseline may overstate BPTT's accuracy in SNN contexts where temporal precision is limited by discretization

## Confidence
- **High confidence**: OTPE's basic mechanism of maintaining temporal traces and achieving improved gradient alignment over OSTL/OTTT
- **Medium confidence**: Claims about minimal overhead and O(n) complexity for approximate OTPE, as these require deeper architectural analysis
- **Low confidence**: Performance generalization to tasks beyond SHD and synthetic datasets, given the narrow experimental scope

## Next Checks
1. Verify approximate OTPE's space complexity empirically by measuring memory usage across networks with varying widths and depths
2. Test OTPE's performance on additional temporal sequence tasks with varying time constants to validate the temporal approximation's robustness
3. Compare OTPE's temporal dynamics capture against state-of-the-art SNN training methods on standard benchmarks like N-MNIST and DVS Gestures