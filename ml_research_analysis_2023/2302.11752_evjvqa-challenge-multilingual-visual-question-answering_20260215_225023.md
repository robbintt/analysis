---
ver: rpa2
title: 'EVJVQA Challenge: Multilingual Visual Question Answering'
arxiv_id: '2302.11752'
source_url: https://arxiv.org/abs/2302.11752
tags:
- dataset
- answers
- question
- vietnamese
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The EVJVQA challenge introduces a multilingual visual question
  answering dataset covering Vietnamese, English, and Japanese, featuring culturally
  specific images from Vietnam. This dataset contains 33,790 question-answer pairs
  across 5,000 images, designed to encourage the development of multilingual VQA models
  beyond resource-rich languages.
---

# EVJVQA Challenge: Multilingual Visual Question Answering

## Quick Facts
- **arXiv ID:** 2302.11752
- **Source URL:** https://arxiv.org/abs/2302.11752
- **Reference count:** 12
- **Primary result:** Introduces EVJVQA dataset with 33,790 QA pairs across 5,000 Vietnamese images in 3 languages; top models achieve F1 0.4392, BLEU 0.4009

## Executive Summary
The EVJVQA challenge introduces a multilingual visual question answering dataset covering Vietnamese, English, and Japanese, featuring culturally specific images from Vietnam. This dataset contains 33,790 question-answer pairs across 5,000 images, designed to encourage the development of multilingual VQA models beyond resource-rich languages. The challenge attracted 62 teams, with 8 submitting results. Top-performing systems used vision transformers (ViT) and multilingual transformers (mT5), achieving F1 scores up to 0.4392 and BLEU scores up to 0.4009 on the private test set. The dataset proved challenging, motivating further research in multilingual VQA models.

## Method Summary
The EVJVQA dataset was constructed through human annotation of culturally specific Vietnamese images with questions and answers in Vietnamese, English, and Japanese. The dataset includes 33,790 question-answer pairs across 5,000 images, with 3,763 images for training, 558 for public testing, and 558 for private testing. Top-performing systems used pre-trained vision transformers (ViT) for image feature extraction and multilingual transformers (mT5) for question processing and answer generation. Models were trained with batch size 64 and a learning rate scheduler that reduces the learning rate after a number of iterations, with training stopped automatically if evaluation scores didn't improve after 5 epochs.

## Key Results
- EVJVQA dataset contains 33,790 QA pairs across 5,000 images covering Vietnamese, English, and Japanese languages
- Top-performing systems achieved F1 scores up to 0.4392 and BLEU scores up to 0.4009 on private test set
- Dataset attracted 62 teams with 8 submitting results to the challenge
- Models struggle particularly with short answers and culturally specific objects not well-represented in pre-training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multilingual VQA dataset EVJVQA challenges models to move beyond language priors and classification-based approaches by requiring diverse answer lengths and forms (words, phrases, sentences).
- Mechanism: By constructing question-answer pairs in Vietnamese, English, and Japanese on culturally specific images from Vietnam, the dataset forces models to rely on visual context rather than pattern-matching between questions and answers. The guidelines explicitly discourage single-word answers and encourage full sentences, reducing the tendency of models to exploit superficial correlations.
- Core assumption: The complexity of Vietnamese scenes and the diversity of answer formats make it harder for models to use language priors effectively.
- Evidence anchors:
  - [abstract] The dataset is designed to motivate further research in multilingual VQA models and is challenging, with top performances only reaching F1 scores up to 0.4392.
  - [section] The authors note that Vietnamese and English questions have similar distributions in length, but English answers are shorter. They also found that models tend to give medium or lengthy answers by repeating tokens from questions, indicating a reliance on language patterns.
  - [corpus] The corpus analysis shows that most models perform better on medium and long answers, suggesting that short answers are harder for models to predict accurately, possibly because they rely less on question tokens.
- Break condition: If the dataset were expanded to include more images or languages with simpler contexts, the challenge to models might decrease, allowing language priors to become more effective again.

### Mechanism 2
- Claim: The use of pre-trained vision transformers (ViT) and multilingual transformers (mT5) as the backbone for top-performing systems leverages the strengths of both visual and linguistic representations, enabling effective cross-modal understanding.
- Mechanism: ViT extracts rich visual features from images, while mT5 provides a strong multilingual language understanding. The combination allows the model to align visual and textual information across languages, improving performance on the multilingual VQA task.
- Core assumption: Pre-trained models like ViT and mT5 have learned generalizable representations that can be fine-tuned for specific tasks like VQA.
- Evidence anchors:
  - [abstract] The top 2 teams use ViT for the pre-trained vision model and mT5 for the pre-trained language model, achieving the highest performances.
  - [section] The baseline system also uses ViT and mBERT, indicating that these models are standard choices for VQA tasks.
  - [corpus] The corpus analysis does not directly address the effectiveness of these models but provides context on the dataset's complexity.
- Break condition: If the pre-trained models were not fine-tuned adequately on the EVJVQA dataset or if the dataset's complexity overwhelmed the models' capacity, performance might degrade.

### Mechanism 3
- Claim: The dataset's focus on culturally specific images from Vietnam introduces unique visual concepts and objects that are not well-represented in general VQA datasets, requiring models to adapt to new visual domains.
- Mechanism: By collecting images relevant to daily life in Vietnam, the dataset includes objects and scenes that are specific to Vietnamese culture (e.g., non la, traditional products in markets). This forces models to learn representations for these new concepts, improving their ability to handle diverse visual content.
- Core assumption: Pre-trained vision models trained on general datasets may not have strong representations for culturally specific objects.
- Evidence anchors:
  - [abstract] The dataset includes images taken from Vietnam, targeting the visual content of a particular country with its own objects and cultural characteristics.
  - [section] The authors argue that the context in images captured in Vietnam is more complicated than in images from VQA benchmarks in English due to crowded scenes and "out-of-common" objects.
  - [corpus] The corpus analysis shows that pre-trained image models (Faster-RCNN, Swin Transformer) struggle to detect common Vietnamese objects, indicating a domain gap.
- Break condition: If the dataset were expanded to include more general images or if pre-trained models were adapted to include culturally diverse data, the advantage of using culturally specific images might diminish.

## Foundational Learning

- Concept: Visual Question Answering (VQA)
  - Why needed here: VQA is the core task being addressed, requiring models to understand both images and text to answer questions.
  - Quick check question: Can you explain the difference between classification-based and generative approaches to VQA?

- Concept: Multilingual NLP
  - Why needed here: The dataset covers three languages (Vietnamese, English, Japanese), requiring models to handle multilingual inputs and outputs.
  - Quick check question: What are the challenges of building multilingual models compared to monolingual ones?

- Concept: Pre-trained Models (Vision and Language)
  - Why needed here: The top-performing systems use pre-trained ViT and mT5 models, leveraging their learned representations for VQA.
  - Quick check question: How do pre-trained models like ViT and mT5 improve performance on downstream tasks like VQA?

## Architecture Onboarding

- Component map: Image Encoder (ViT) -> Text Encoder (mT5) -> Fusion Module -> Answer Decoder

- Critical path:
  1. Extract visual features from the input image using ViT.
  2. Encode the question using mT5.
  3. Fuse the visual and textual representations.
  4. Generate the answer using the fused representation.

- Design tradeoffs:
  - Using grid-based features (from ViT) vs. region-based features (from object detectors) for visual representation.
  - End-to-end training vs. freezing pre-trained components during fine-tuning.
  - Answer generation as a classification task vs. a generative task.

- Failure signatures:
  - Poor performance on questions requiring fine-grained visual understanding (e.g., color, side, object identification).
  - Over-reliance on language priors, leading to incorrect answers that match question tokens but not visual content.
  - Inability to handle culturally specific objects or scenes not well-represented in pre-training data.

- First 3 experiments:
  1. Evaluate the baseline system (ViT + mBERT) on the EVJVQA dataset to establish a performance benchmark.
  2. Fine-tune the baseline system with different learning rates and batch sizes to optimize performance.
  3. Replace ViT with a region-based feature extractor (e.g., Faster-RCNN) and compare performance to assess the impact of visual representation choice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multilingual nature of the UIT-EVJVQA dataset affect the performance of VQA models across different languages?
- Basis in paper: Explicit
- Why unresolved: The paper discusses the dataset's multilingual aspect but doesn't deeply analyze how model performance varies across languages or what specific challenges each language presents.
- What evidence would resolve it: A detailed analysis comparing model performance across languages, identifying language-specific challenges and the effectiveness of different approaches for each language.

### Open Question 2
- Question: What are the specific challenges in recognizing objects in Vietnamese scenes, and how can pre-trained image models be adapted to better handle these scenes?
- Basis in paper: Explicit
- Why unresolved: The paper mentions that pre-trained image models struggle with recognizing objects in Vietnamese scenes but doesn't provide solutions or detailed analysis of these challenges.
- What evidence would resolve it: An investigation into the types of objects and scenes that are challenging, along with proposed methods for adapting or creating new pre-trained models tailored to Vietnamese contexts.

### Open Question 3
- Question: How does the length and complexity of answers in the UIT-EVJVQA dataset impact the performance of VQA models, and what strategies can be employed to improve performance on longer, more complex answers?
- Basis in paper: Inferred
- Why unresolved: The paper notes that models tend to give lengthy answers by repeating tokens from questions, but doesn't explore strategies to improve handling of complex answers or the impact of answer length on model performance.
- What evidence would resolve it: Experiments testing different model architectures or training strategies focused on improving performance on longer, more complex answers, along with an analysis of how answer length affects overall model performance.

## Limitations

- The dataset size (33,790 QA pairs across 5,000 images) is relatively small compared to large-scale English VQA benchmarks
- Cultural specificity of Vietnamese images may limit applicability to broader multilingual VQA contexts
- Evaluation metrics (F1 and BLEU) may not fully capture nuances of answer quality, particularly for Vietnamese and Japanese languages

## Confidence

**High Confidence:** The dataset construction methodology and evaluation procedures are well-documented and reproducible. The claim that EVJVQA is the first multilingual VQA dataset covering Vietnamese, English, and Japanese is verifiable and well-supported.

**Medium Confidence:** The claim that EVJVQA is challenging for current multilingual VQA models is supported by the F1 scores (max 0.4392), but the absolute difficulty level depends on the specific model architectures and training procedures used by participants. The assertion that culturally specific images introduce unique visual concepts is plausible but requires more systematic analysis.

**Low Confidence:** The claim that models primarily rely on language priors rather than visual content is based on qualitative observations (e.g., models repeating tokens from questions) rather than rigorous quantitative analysis. The effectiveness of the answer length guidelines in reducing language prior exploitation has not been empirically validated.

## Next Checks

1. **Dataset Expansion Analysis:** Conduct experiments by incrementally adding images from the private test set to the training set to quantify how dataset size affects model performance, particularly for Vietnamese and Japanese languages.

2. **Cross-Cultural Transferability:** Evaluate whether models trained on EVJVQA can generalize to VQA datasets from other cultures (e.g., Indian or Chinese datasets) to test the hypothesis that culturally specific visual concepts are the primary challenge.

3. **Language Prior Quantification:** Implement controlled experiments comparing model performance on question-answer pairs with high linguistic similarity (where language priors might be effective) versus pairs requiring strong visual understanding, to quantitatively measure the impact of language priors on EVJVQA performance.