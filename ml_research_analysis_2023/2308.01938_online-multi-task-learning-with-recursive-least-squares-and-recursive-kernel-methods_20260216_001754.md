---
ver: rpa2
title: Online Multi-Task Learning with Recursive Least Squares and Recursive Kernel
  Methods
arxiv_id: '2308.01938'
source_url: https://arxiv.org/abs/2308.01938
tags:
- online
- learning
- multi-task
- recursive
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces two online multi-task learning methods for
  regression: MT-WRLS and MT-OSLSSVR. Both are based on adapting single-task recursive
  algorithms (WRLS and OSLSSVR) to a graph-based MTL formulation.'
---

# Online Multi-Task Learning with Recursive Least Squares and Recursive Kernel Methods

## Quick Facts
- **arXiv ID**: 2308.01938
- **Source URL**: https://arxiv.org/abs/2308.01938
- **Reference count**: 26
- **Primary result**: Two novel online MTL methods (MT-WRLS and MT-OSLSSVR) that achieve up to 16% error reduction compared to single-task and other multi-task methods on wind speed forecasting

## Executive Summary
This paper introduces two online multi-task learning methods for regression: MT-WRLS and MT-OSLSSVR. Both are based on adapting single-task recursive algorithms (WRLS and OSLSSVR) to a graph-based MTL formulation. The key idea is to stack all task parameters into a single vector and use the graph Laplacian as a regularizer, allowing direct use of the base algorithms with minimal modification. The methods were evaluated on a wind speed forecasting benchmark, demonstrating significant performance improvements over existing approaches.

## Method Summary
The paper proposes two online multi-task learning algorithms for regression tasks. MT-WRLS achieves exact online solutions at each step with a per-instance cost of O(d²T²) by initializing the recursive covariance with the inverse graph Laplacian. MT-OSLSSVR provides approximate solutions controlled by a sparsity parameter with cost O(d·m²ₙ), where mₙ is the dictionary size. Both methods leverage graph-based regularization to share information across tasks, with the graph Laplacian encoding task relationships. The algorithms were validated on a wind speed forecasting dataset using auto-regressive inputs.

## Key Results
- MT-WRLS achieves exact online solutions with O(d²T²) per-instance cost
- MT-OSLSSVR provides controllable approximate solutions with O(d·m²ₙ) cost
- Both methods outperformed existing online MTL approaches on wind speed forecasting, with up to 16% error reduction
- The work also demonstrated combination with Extreme Learning Machines for nonlinear online MTL

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The multi-task MTL formulation converts the joint optimization into a single least-squares problem by stacking task parameters and inputs, enabling direct application of recursive single-task algorithms.
- Mechanism: Task-stacking transforms the original T-task problem into a single vectorized regression with a Kronecker-product-structured regularization term. This structure allows the use of standard recursive updates (WRLS or OSLSSVR) with minimal modification to their initialization and kernel functions.
- Core assumption: The tasks can be related via a graph Laplacian; the stacked formulation preserves convexity and enables exact or approximate recursive solutions.
- Evidence anchors:
  - [abstract] "stacking all task parameters into a single vector and use the graph Laplacian as a regularizer"
  - [section III-A] shows the derivation of the closed-form solution from the graph-based MTL objective
  - [corpus] No direct support; assumption is internally justified by the paper's derivation
- Break condition: If task relationships cannot be encoded in a fixed graph, the Kronecker-structured regularization fails.

### Mechanism 2
- Claim: MT-WRLS achieves exact online solutions at each iteration with quadratic per-instance cost by initializing the recursive covariance with the inverse graph Laplacian.
- Mechanism: The initialization P(0) = (λA)^{-1} ⊗ I_d incorporates the task structure before any data arrives, so each online update step proceeds exactly as in standard WRLS but on the stacked parameter space.
- Core assumption: A (graph Laplacian) is invertible and the stacked WRLS recursion remains numerically stable with this initialization.
- Evidence anchors:
  - [section III-B] "If we state that P(0) = {λA ⊗ I_d}^{-1}, then we can make every new task input vector as an input vector of dimension dT"
  - [abstract] "exact online solutions at each step with a per-instance cost of O(d²T²)"
  - [corpus] No direct evidence; relies on Woodbury's Identity holding for the stacked system
- Break condition: Numerical instability or rank deficiency in A or high T causing P(0) to be ill-conditioned.

### Mechanism 3
- Claim: MT-OSLSSVR provides approximate but controllable solutions by using a multi-task kernel derived from the graph Laplacian, with cost O(d·m_n²) where m_n is dictionary size.
- Mechanism: The kernel K(x_s(n),x_t(l)) = x_s(n)' x_t(l) A^{-1}[s,t] incorporates task relationships in the feature space, allowing OSLSSVR to operate in the dual space without ever explicitly stacking parameters.
- Core assumption: The ALD criterion can maintain a small, representative dictionary while preserving multi-task structure through the kernel.
- Evidence anchors:
  - [section III-C] "The kernel function of two input vectors of any tasks s and t ... = xs(n)' xt(l) A^{-1}[s,t]"
  - [abstract] "approximate solutions controlled by a sparsity parameter with cost O(d·m_n²)"
  - [corpus] No direct corpus support; method is novel in the paper
- Break condition: If the kernel approximation is poor (large v), the solution may not capture task relationships well.

## Foundational Learning

- Concept: Graph-based multi-task regularization
  - Why needed here: Provides the structural bias that enables information sharing among tasks during recursive updates.
  - Quick check question: What matrix A encodes the pairwise task relationships in the graph-based formulation?

- Concept: Recursive least squares and Woodbury identity
  - Why needed here: Core mathematical tools that allow online updates without recomputing full least-squares solutions.
  - Quick check question: How does the Woodbury identity enable the recursive update of the covariance matrix P(n)?

- Concept: Kernel methods and representer theorem
  - Why needed here: Allow the MT-OSLSSVR to work in the dual space with a multi-task kernel, avoiding explicit stacking of all parameters.
  - Quick check question: What does the representer theorem guarantee about the form of the solution in a reproducing kernel Hilbert space?

## Architecture Onboarding

- Component map:
  - Graph construction → Laplacian matrix A
  - Initialization → P(0) = (λA)^{-1} ⊗ I_d (MT-WRLS) or kernel K defined by A (MT-OSLSSVR)
  - Online loop → For each task: predict, update parameters (WRLS) or support vectors (OSLSSVR)
  - Hyperparameter tuning → σ, λ for WRLS; v, λ for OSLSSVR

- Critical path:
  1. Build graph and compute A
  2. Initialize recursive structures
  3. Process each incoming instance: predict → update → evaluate

- Design tradeoffs:
  - MT-WRLS: Exact updates but O(d²T²) cost; initialization is O(T³)
  - MT-OSLSSVR: Approximate updates with controllable sparsity; cost O(d·m_n²), m_n << nT

- Failure signatures:
  - MT-WRLS: Ill-conditioned P(0) or loss of convexity if A not SPD
  - MT-OSLSSVR: Large v leads to poor task structure capture; small v increases dictionary size and cost

- First 3 experiments:
  1. Validate exact vs approximate updates on synthetic graph-structured data
  2. Benchmark convergence speed vs OGD and MADMM on real wind speed forecasting
  3. Test robustness to graph misspecification by perturbing A and measuring performance degradation

## Open Questions the Paper Calls Out
The paper mentions several open questions and future research directions:

1. How does the combination of MT-WRLS with Extreme Learning Machines (ELM) perform compared to other nonlinear online MTL methods?
2. How can the proposed methods be extended to handle non-stationary task relationships that evolve over time?
3. What are the theoretical guarantees for the convergence and generalization performance of MT-OSLSSVR with the sparsity parameter v?

## Limitations
- Computational complexity of MT-WRLS may become prohibitive for large numbers of tasks due to O(d²T²) per-instance cost
- Limited empirical validation on a single real-world dataset (wind speed forecasting)
- Graph construction methodology is briefly mentioned but not thoroughly explored

## Confidence

- **High confidence** in the mathematical formulation and algorithm derivations, which are rigorous and well-explained.
- **Medium confidence** in the empirical results, as they are promising but limited to one application domain with fixed graph structure.
- **Low confidence** in scalability claims beyond the tested regime (T=10 tasks), as the O(d²T²) complexity could become prohibitive for many-task settings.

## Next Checks

1. Test algorithm robustness to graph misspecification by systematically perturbing the Laplacian and measuring performance degradation across different correlation thresholds.
2. Evaluate computational efficiency and numerical stability for varying T (beyond 10) and d (input dimension) to validate the claimed complexity bounds.
3. Validate on additional multi-task regression benchmarks with known task relationships (e.g., synthetic datasets with controlled correlation structures) to assess generalization beyond wind forecasting.