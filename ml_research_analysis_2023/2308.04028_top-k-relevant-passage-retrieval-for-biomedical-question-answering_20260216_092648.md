---
ver: rpa2
title: Top K Relevant Passage Retrieval for Biomedical Question Answering
arxiv_id: '2308.04028'
source_url: https://arxiv.org/abs/2308.04028
tags:
- question
- questions
- passages
- passage
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a dense passage retrieval system for biomedical
  question answering, addressing the challenge of retrieving relevant passages from
  a large corpus of PubMed abstracts. The authors fine-tune existing Dense Passage
  Retrieval (DPR) models, comparing BioBERT, BERT-base, and Facebook's pre-trained
  BERT as encoders for questions and passages.
---

# Top K Relevant Passage Retrieval for Biomedical Question Answering

## Quick Facts
- arXiv ID: 2308.04028
- Source URL: https://arxiv.org/abs/2308.04028
- Authors: 
- Reference count: 8
- Key outcome: Dense passage retrieval system achieves F1 score of 0.81 on BioASQ 2021 biomedical QA dataset using fine-tuned Facebook BERT encoder

## Executive Summary
This paper presents a dense passage retrieval system for biomedical question answering that addresses the challenge of retrieving relevant passages from PubMed abstracts. The authors fine-tune Dense Passage Retrieval (DPR) models using BioBERT, BERT-base, and Facebook's pre-trained BERT as encoders, evaluating on the BioASQ 2021 dataset. By replacing original positive contexts with matching PubMed abstracts and introducing hard negatives from BM25 results, the model achieves state-of-the-art performance with an F1 score of 0.81, demonstrating significant improvements over general-domain models.

## Method Summary
The authors develop a dense passage retrieval system by fine-tuning DPR models on biomedical data. They preprocess 30 million PubMed articles into 100-word passages and use BioASQ 2021 as the evaluation dataset. The model employs Facebook's pre-trained BERT encoder, which achieves the highest accuracy among the tested variants. Training uses in-batch negatives with a batch size of 16, combining one positive context, one hard negative passage, and 15 in-batch negatives per question. Retrieval is performed using FAISS with IndexFlatIP for exact maximum inner product search, and performance is evaluated using F1 score on the BioASQ test set.

## Key Results
- Achieves F1 score of 0.81 on BioASQ 2021 test set using Facebook pre-trained BERT encoder
- Dense retrieval outperforms traditional sparse methods like BM25 for biomedical QA
- Facebook's pre-trained BERT encoder shows superior performance compared to BioBERT and BERT-base

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dense passage retrieval using fine-tuned BERT encoders outperforms traditional sparse methods like BM25 in biomedical QA.
- Mechanism: The model encodes both questions and passages into dense vectors, then uses maximum inner product search (MIPS) to retrieve top K passages based on vector similarity rather than keyword matching.
- Core assumption: Dense vector representations capture semantic relationships better than sparse keyword-based approaches for biomedical text.
- Evidence anchors:
  - [abstract]: "our fine-tuned dense retriever results in a 0.81 F1 score" and "fine-tuning dense retrievers on biomedical data significantly improves passage retrieval accuracy compared to general-domain models."
  - [section]: "We use FAISS to index the dense representations of all passages. Specifically, we use IndexFlatIP for indexing and the exact maximum inner product search for queries."
  - [corpus]: Weak - no direct corpus evidence found comparing dense vs sparse retrieval performance.

### Mechanism 2
- Claim: Using Facebook's pre-trained BERT encoder achieves higher F1 scores than BioBERT or BERT-base for biomedical passage retrieval.
- Mechanism: Facebook's BERT was already fine-tuned on question-answering datasets, giving it better initial performance for the retrieval task compared to models trained on general or biomedical-only corpora.
- Core assumption: Pre-training on QA datasets provides better initialization for retrieval tasks than domain-specific pre-training alone.
- Evidence anchors:
  - [abstract]: "The model is trained with in-batch negatives and achieves the best performance using Facebook's pre-trained BERT, reaching an F1 score of 0.81 on the BioASQ dataset."
  - [section]: "It is evident from the graph, Fig. 4, that Facebook BERT has the highest accuracy with respect to BioBERT and BERT-Base."
  - [corpus]: No direct corpus evidence comparing different BERT variants for this specific task.

### Mechanism 3
- Claim: In-batch negatives improve training efficiency by providing additional negative examples without requiring extra sampling.
- Mechanism: Each question in a batch is paired with positive passages from other questions in the same batch, effectively multiplying the number of negative examples available for training.
- Core assumption: Positive passages from other questions in the batch serve as effective negative examples for the current question.
- Evidence anchors:
  - [section]: "With the in-batch negative trick, each question can be further paired with B-1 negatives (i.e., positive passages of the rest questions) without sampling additional negatives. We use the batch size of 16 which means each question has 1 positive context, 1 hard negative passage, and 15 negative passages."
  - [abstract]: No direct mention of in-batch negatives in abstract.
  - [corpus]: No corpus evidence found specifically about in-batch negative effectiveness.

## Foundational Learning

- Concept: Maximum Inner Product Search (MIPS)
  - Why needed here: Used to efficiently retrieve top K passages by finding vectors with highest dot product similarity to the query vector
  - Quick check question: What data structure does FAISS use to enable efficient MIPS queries over millions of vectors?

- Concept: Dual-Encoder Architecture
  - Why needed here: Separates question encoding from passage encoding, allowing pre-computation of passage vectors and efficient retrieval
  - Quick check question: Why is it computationally advantageous to have separate encoders for questions and passages rather than a cross-encoder?

- Concept: Hard Negative Mining
  - Why needed here: Improves model discrimination by including challenging negative examples that are semantically similar to positive passages
  - Quick check question: How does using BM25 top results (excluding answer-containing passages) as hard negatives improve model performance?

## Architecture Onboarding

- Component map:
  - PubMed Corpus (30M+ abstracts) -> 100-word passages -> FAISS IndexFlatIP
  - BioASQ Questions -> Question Encoder (Facebook BERT) -> Dense vectors
  - MIPS Search -> Top K Passages -> Evaluation (F1 score)

- Critical path:
  1. Load pre-processed PubMed passages into FAISS index
  2. Encode BioASQ questions using question encoder
  3. Perform MIPS search to retrieve top K passages
  4. Evaluate retrieved passages against ground truth answers

- Design tradeoffs:
  - Memory vs accuracy: Larger batch sizes provide more in-batch negatives but require more GPU memory
  - Index size vs retrieval speed: Exact MIPS (IndexFlatIP) is slower but more accurate than approximate methods
  - Pre-training vs fine-tuning: Facebook BERT starts with better QA performance but may be less domain-specific than BioBERT

- Failure signatures:
  - Low F1 scores with high precision but low recall: Model may be too conservative in retrieving passages
  - High variance across epochs: Learning rate or batch size may be suboptimal
  - Poor performance on specific question types: Encoder may not capture certain biomedical terminology well

- First 3 experiments:
  1. Baseline comparison: Run retrieval with BM25 vs DPR using same BioASQ questions to quantify improvement
  2. Encoder ablation: Compare F1 scores using BioBERT, BERT-base, and Facebook BERT encoders
  3. Negative sampling study: Test retrieval performance with different combinations of random negatives, BM25 hard negatives, and in-batch negatives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of hard negatives from the full 30 million PubMed corpus impact retrieval performance compared to using only a subset of 210K articles?
- Basis in paper: [explicit] The authors state they intend to experiment with indexing the entire 30 million PubMed articles using FAISS to incorporate hard negatives from the full corpus in future work.
- Why unresolved: The current study only uses a subset of 210K articles to find hard negatives due to computational constraints. The impact of using the full corpus on retrieval accuracy remains unknown.
- What evidence would resolve it: An experimental comparison of retrieval performance using hard negatives from the full 30 million PubMed corpus versus a subset of 210K articles.

### Open Question 2
- Question: What is the optimal batch size for training the dense passage retriever on biomedical data, and how does it affect retrieval scores?
- Basis in paper: [inferred] The authors mention the intention to conduct experiments with varying batch sizes to ascertain their influence on retrieval scores in future work. The current study uses a fixed batch size of 16.
- Why unresolved: The study does not explore the impact of different batch sizes on retrieval performance, leaving the optimal batch size for biomedical data unknown.
- What evidence would resolve it: An experimental analysis of retrieval performance using different batch sizes during training, with comparisons to determine the optimal batch size for biomedical data.

### Open Question 3
- Question: How does the proposed approach compare to other state-of-the-art biomedical question answering systems in terms of accuracy and efficiency?
- Basis in paper: [explicit] The authors state that their objective is to go beyond past biological QA systems and create a QA system that can automatically handle a wide range of question types. However, the study does not provide a direct comparison with other state-of-the-art systems.
- Why unresolved: The study focuses on developing and evaluating the proposed approach without comparing it to other existing biomedical QA systems, leaving the relative performance unknown.
- What evidence would resolve it: A comprehensive comparison of the proposed approach with other state-of-the-art biomedical QA systems in terms of accuracy, efficiency, and ability to handle various question types.

## Limitations

- The evaluation uses a relatively small test set (102 questions) from BioASQ 2021, which may not capture the full diversity of biomedical questions
- Preprocessing details for matching BioASQ contexts with PubMed passages are underspecified, making exact reproduction difficult
- Comparison with BM25 and other baseline methods lacks detailed quantitative results in the paper

## Confidence

- **High confidence**: The core claim that dense passage retrieval with fine-tuned BERT encoders outperforms general-domain models in biomedical QA is well-supported by the F1 score results (0.81) and the clear architectural description.
- **Medium confidence**: The claim that Facebook's pre-trained BERT specifically outperforms BioBERT and BERT-base is supported by the reported F1 scores, but the paper lacks detailed ablation studies or statistical significance testing.
- **Medium confidence**: The effectiveness of in-batch negatives is described mechanistically and used in the training setup, but the paper doesn't provide comparative results showing the impact of this technique versus other negative sampling strategies.

## Next Checks

1. Perform t-tests or bootstrap confidence intervals on the F1 scores across multiple training runs to verify that the reported improvements are statistically significant rather than due to random variation.

2. Evaluate the model on a held-out subset of BioASQ questions with different characteristics (e.g., question types, complexity) to ensure the performance gains aren't specific to certain question patterns.

3. Systematically compare retrieval performance using different negative sampling strategies (random negatives only, BM25 hard negatives only, in-batch negatives only, and combinations) to quantify the contribution of each approach.