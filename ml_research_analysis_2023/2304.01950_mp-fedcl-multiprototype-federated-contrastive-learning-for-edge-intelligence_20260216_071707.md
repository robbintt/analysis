---
ver: rpa2
title: 'MP-FedCL: Multiprototype Federated Contrastive Learning for Edge Intelligence'
arxiv_id: '2304.01950'
source_url: https://arxiv.org/abs/2304.01950
tags:
- learning
- client
- local
- global
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses federated learning challenges under non-IID
  data distributions, including both label and feature skewness. It proposes MP-FedCL,
  a multi-prototype federated contrastive learning approach that uses k-means clustering
  to compute multiple prototypes for each class and aggregates them into a global
  prototype pool.
---

# MP-FedCL: Multiprototype Federated Contrastive Learning for Edge Intelligence

## Quick Facts
- arXiv ID: 2304.01950
- Source URL: https://arxiv.org/abs/2304.01950
- Reference count: 40
- Average test accuracy improvement of 4.6% under feature skewness and 10.4% under label skewness

## Executive Summary
MP-FedCL addresses federated learning challenges under non-IID data distributions by introducing a multi-prototype approach that uses k-means clustering to compute multiple prototypes per class. The method aggregates these prototypes into a global pool and employs contrastive learning to align local representations with corresponding prototypes while pushing away dissimilar ones. Experiments on four benchmark datasets demonstrate that MP-FedCL outperforms multiple baselines, achieving significant accuracy improvements in both feature and label skewness scenarios.

## Method Summary
MP-FedCL computes multiple prototypes per class using k-means clustering on local client data, then aggregates these prototypes into a global prototype pool on the server. During local training, each client minimizes both a supervised learning loss and a contrastive loss that aligns local representations with corresponding prototypes while pushing away dissimilar ones. The prototypes are exchanged between clients and server instead of full model parameters, reducing communication overhead while preserving class-specific knowledge from diverse data distributions.

## Key Results
- Achieved average test accuracy improvements of 4.6% under feature skewness
- Achieved average test accuracy improvements of 10.4% under label skewness
- Outperformed multiple baseline methods including FedAvg, FedProx, and SP-FedCL on four benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple prototypes per class capture more diverse feature representations than a single prototype.
- Mechanism: The k-means clustering algorithm computes multiple centroids for each class embedding space, allowing each class to be represented by several distinct prototypes rather than a single mean vector.
- Core assumption: Class feature spaces are naturally clustered and contain meaningful sub-distributions that a single mean cannot adequately represent.
- Evidence anchors:
  - [abstract] "feature spaces are usually not clustered, and a single prototype may not represent a class well"
  - [section] "The output (i.e. k centroids) of k-means clustering algorithm can be viewed as the calculated multiple prototypes for that class"
- Break condition: If class distributions are unimodal or if k-means fails to find meaningful clusters due to noise or insufficient data points per class.

### Mechanism 2
- Claim: Contrastive learning with global prototype pool reduces class confusion across heterogeneous clients.
- Mechanism: Local representations are pulled toward prototypes of their own class while being pushed away from prototypes of other classes, creating more discriminative feature spaces across all clients.
- Core assumption: The global prototype pool contains representative prototypes from all clients that can guide local training toward better class separation.
- Evidence anchors:
  - [abstract] "encourages them to learn knowledge related to their own class from others and reduces the absorption of unrelated knowledge"
  - [section] "prototypes in the global prototype pool and local representations belonging to the same class are pulled together, while simultaneously pushing apart those prototypes from different classes"
- Break condition: If the global prototype pool becomes dominated by a few clients or if prototypes from different classes become too similar.

### Mechanism 3
- Claim: Multi-prototype aggregation preserves class-specific knowledge from diverse data distributions.
- Mechanism: Prototypes from all clients are aggregated into a global pool, ensuring that knowledge from clients with different feature distributions contributes to the final model.
- Core assumption: Each client's prototypes contain valuable class-specific information that should be preserved and shared across the federation.
- Evidence anchors:
  - [abstract] "The computed multiple prototypes and their respective model parameters are sent to the edge server for aggregation into a global prototype pool"
  - [section] "The global prototype pool is a combination of multiple prototypes from each client, and it can be updated during training in each global round"
- Break condition: If aggregation strategy fails to properly balance contributions from clients with very different data distributions.

## Foundational Learning

- Concept: Federated Learning fundamentals (client-server architecture, local training, global aggregation)
  - Why needed here: MP-FedCL builds directly on standard federated learning workflow but adds prototype-based knowledge sharing
  - Quick check question: What are the four main steps in a typical federated learning round?

- Concept: Contrastive learning principles (positive/negative pairs, temperature scaling, embedding space manipulation)
  - Why needed here: The supervised contrastive loss is central to how MP-FedCL aligns local representations with global prototypes
  - Quick check question: How does temperature parameter τ affect the contrastive loss?

- Concept: k-means clustering algorithm and its convergence properties
  - Why needed here: k-means is used to compute multiple prototypes per class, and understanding its limitations is crucial for choosing k
  - Quick check question: What are the main assumptions behind k-means convergence to local optima?

## Architecture Onboarding

- Component map:
  Feature extraction layers (fe) -> MLP layers -> Decision layer
  Local prototype computation (k-means clustering on class embeddings)
  Server-side prototype aggregation into global prototype pool
  Local supervised + contrastive loss computation
  Model inference using prototype-based distance comparison

- Critical path:
  1. Client computes local embeddings for each class
  2. Client runs k-means to generate multiple prototypes per class
  3. Client sends prototypes + model parameters to server
  4. Server aggregates prototypes into global pool
  5. Server sends aggregated prototypes back to all clients
  6. Clients compute contrastive loss using global prototypes
  7. Clients update models using combined supervised + contrastive loss

- Design tradeoffs:
  - Higher k values provide more representative prototypes but increase communication overhead
  - Larger prototype pools improve knowledge sharing but may slow convergence
  - Contrastive loss strength (controlled by τ) must balance with supervised loss

- Failure signatures:
  - Poor convergence: Check if k is too large relative to available data per class
  - Degraded accuracy: Verify that prototype aggregation isn't dominated by outlier clients
  - Communication bottlenecks: Monitor prototype pool size versus bandwidth constraints

- First 3 experiments:
  1. Run MP-FedCL with k=1 (equivalent to SP-FedCL) to establish baseline performance
  2. Vary k from 2 to 4 on MNIST dataset to find optimal prototype count
  3. Compare test accuracy under feature non-IID vs label non-IID settings to understand which challenge MP-FedCL addresses better

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of K (number of prototypes per class) affect the convergence rate and final performance across different non-IID scenarios?
- Basis in paper: [explicit] The paper states "Intuitively, the high-dimensional feature space of samples from different datasets is different; thus the size of K is associated with specific datasets" and conducts experiments with K ∈ {2, 3, 4} to determine optimal values for each dataset.
- Why unresolved: While the paper identifies optimal K values for specific datasets through heuristic selection, it does not provide a systematic method for determining K based on dataset characteristics or non-IID severity. The relationship between K, convergence rate, and performance under varying degrees of non-IID remains unexplored.
- What evidence would resolve it: A comprehensive study analyzing the impact of K on convergence speed and accuracy across a spectrum of non-IID distributions, along with theoretical bounds on optimal K selection based on data heterogeneity metrics.

### Open Question 2
- Question: What is the theoretical upper bound on communication efficiency gains compared to FedAvg when using multi-prototype exchange versus model parameter exchange?
- Basis in paper: [explicit] The paper mentions that "the prototype is a one-dimensional vector of low-dimensional samples that are naturally small and privacy-preserving, which does not incur excessive communication costs or raise privacy concerns compared to the model parameters" but does not quantify the communication savings.
- Why unresolved: The paper claims communication efficiency benefits but only provides qualitative statements about prototype size advantages without quantitative comparison to model parameter communication costs or empirical measurements of bandwidth reduction.
- What evidence would resolve it: A detailed analysis comparing the actual data transmission requirements of multi-prototype exchange versus FedAvg model parameters across different network conditions and client counts, including theoretical bounds on communication reduction.

### Open Question 3
- Question: How does the multi-prototype approach perform under dynamic non-IID distributions where data heterogeneity changes over time?
- Basis in paper: [inferred] The convergence analysis assumes static non-IID conditions, and all experiments use fixed Dirichlet parameters. The paper does not address scenarios where client data distributions evolve during training.
- Why unresolved: Real-world federated learning environments often involve changing data distributions due to temporal drift or shifting client populations. The paper's analysis and experiments are limited to static non-IID settings, leaving the method's robustness to dynamic changes unexplored.
- What evidence would resolve it: Experimental validation showing MP-FedCL performance when client data distributions change during training, including scenarios with gradual drift versus abrupt shifts, and comparison to adaptive baselines that can handle dynamic non-IID conditions.

## Limitations

- The paper does not provide systematic guidance for selecting the number of prototypes K based on dataset characteristics or non-IID severity.
- Communication efficiency benefits are claimed but not quantified with actual bandwidth measurements or comparisons to FedAvg.
- Experiments are limited to static non-IID distributions, with no evaluation of performance under dynamic data heterogeneity.

## Confidence

- **High Confidence**: The core mechanism of using multiple prototypes per class to capture diverse feature representations is well-supported by the theoretical framework and experimental results show consistent improvements over baselines.
- **Medium Confidence**: The effectiveness of the contrastive learning component in reducing class confusion across heterogeneous clients is demonstrated, but the specific contribution of each component (supervised vs contrastive loss) is not isolated through ablation studies.
- **Low Confidence**: The generalizability of the approach to real-world edge computing scenarios with severe bandwidth constraints and dynamic client availability is not addressed, as the experiments use controlled benchmark datasets with fixed client pools.

## Next Checks

1. **Ablation Study**: Run experiments isolating the contribution of supervised loss vs contrastive loss to quantify their individual impact on performance improvements.
2. **Communication Overhead Analysis**: Measure the actual bandwidth consumption of prototype sharing across different k values and compare with standard federated learning communication patterns.
3. **Robustness Testing**: Evaluate MP-FedCL performance when clients have highly imbalanced data distributions and varying numbers of data points per class to assess real-world applicability.