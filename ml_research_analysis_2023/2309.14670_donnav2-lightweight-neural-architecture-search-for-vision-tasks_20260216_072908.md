---
ver: rpa2
title: DONNAv2 -- Lightweight Neural Architecture Search for Vision tasks
arxiv_id: '2309.14670'
source_url: https://arxiv.org/abs/2309.14670
tags:
- search
- donnav2
- blocks
- space
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DONNAv2 is a lightweight neural architecture search (NAS) method
  that eliminates the computationally expensive accuracy predictor stage used in conventional
  NAS. Instead, it uses the mean squared error (MSE) loss of individual blocks as
  a surrogate performance metric during the evolutionary search phase.
---

# DONNAv2 -- Lightweight Neural Architecture Search for Vision tasks

## Quick Facts
- arXiv ID: 2309.14670
- Source URL: https://arxiv.org/abs/2309.14670
- Reference count: 32
- Primary result: Eliminates accuracy predictors in NAS, using MSE loss as surrogate metric with 10x computational savings

## Executive Summary
DONNAv2 is a lightweight neural architecture search method that eliminates the computationally expensive accuracy predictor stage used in conventional NAS. Instead, it uses block-level MSE loss as a surrogate performance metric during evolutionary search and introduces block knowledge distillation filtering to remove redundant blocks. The method achieves comparable results to state-of-the-art models while requiring substantially fewer epochs for model search and finetuning across five vision tasks.

## Method Summary
DONNAv2 uses Blockwise Knowledge Distillation (BKD) to train replacement blocks as student models against a mothernet teacher, recording MSE loss for each block option. A BKD filtering stage removes redundant blocks based on cost ratio thresholds, reducing search space cardinality. Evolutionary search using NSGA-II optimizes for both MSE loss and hardware latency, finding Pareto-optimal architectures that are then fine-tuned. The approach eliminates accuracy predictors entirely, using only one epoch of training per block during the search phase.

## Key Results
- 10x reduction in computational cost compared to original DONNA method
- Maintains similar accuracy across five vision tasks (classification, detection, denoising, super-resolution, YOLOP)
- Hardware-in-the-loop optimization on Samsung Galaxy S10 shows significant model compression with minimal performance degradation
- Achieves comparable results to state-of-the-art models with fewer search and finetuning epochs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using block-level MSE loss as a surrogate performance metric eliminates the need for computationally expensive accuracy predictors.
- Mechanism: Each block is trained to minimize MSE loss relative to the mothernet block, serving as a proxy for overall model performance during evolutionary search.
- Core assumption: Sum of individual block MSE losses correlates with overall network accuracy.
- Evidence anchors: [abstract] loss metric serves as surrogate performance measure; [section] MSE loss between teacher/student output feature maps used as surrogate metric; [corpus] weak correlation evidence in related papers.
- Break condition: If certain blocks contribute disproportionately to accuracy (e.g., attention mechanisms), MSE loss may poorly predict their impact.

### Mechanism 2
- Claim: Block knowledge distillation filtering removes redundant blocks while preserving model diversity and performance.
- Mechanism: Blocks are ranked by cost ratio at each MSE loss level, with high-cost blocks discarded to reduce search space cardinality.
- Core assumption: High-cost blocks at similar MSE loss levels are redundant and can be removed without eliminating optimal architectures.
- Evidence anchors: [abstract] leverages block knowledge distillation filter to remove blocks with high inference costs; [section] BKD filtering retains only blocks with minimum cost ratio; [corpus] no direct evidence found - novel contribution.
- Break condition: If cost ratio doesn't correlate with actual performance impact, filtering may remove essential blocks.

### Mechanism 3
- Claim: Hardware-in-the-loop optimization using actual device latency measurements produces more accurate cost predictions than theoretical FLOPs.
- Mechanism: Measures actual latency on target hardware during evolutionary search instead of using FLOPs as a proxy.
- Core assumption: Device-specific latency measurements better capture real-world performance than hardware-agnostic metrics.
- Evidence anchors: [abstract] hardware-in-the-loop experiments on Samsung Galaxy S10 show significant model compression; [section] important to optimize models to minimize latency for on-device performance; [corpus] some evidence from related works acknowledging FLOPs don't always linearly translate to latency.
- Break condition: If latency measurements are noisy or inconsistent across runs, optimization may converge to suboptimal architectures.

## Foundational Learning

- Concept: Neural Architecture Search (NAS) fundamentals
  - Why needed here: Understanding NAS evolution from computationally expensive methods to lightweight approaches is crucial for grasping DONNAv2's innovations.
  - Quick check question: What are the main computational bottlenecks in traditional NAS approaches?

- Concept: Knowledge Distillation
  - Why needed here: DONNAv2 uses block-wise knowledge distillation both for performance estimation and filtering, requiring understanding of KD principles.
  - Quick check question: How does knowledge distillation typically work in model compression scenarios?

- Concept: Evolutionary Algorithms for Architecture Search
  - Why needed here: DONNAv2 uses NSGA-II evolutionary algorithm with MSE loss as the fitness function.
  - Quick check question: What are the key components of an evolutionary algorithm applied to neural architecture search?

## Architecture Onboarding

- Component map: Search Space Definition -> Block Knowledge Distillation (BKD) -> BKD Filtering -> Evolutionary Search (NSGA-II) -> Fine-tuning
- Critical path:
  1. Define search space and mothernet
  2. Generate BKD library (1 epoch per block)
  3. Apply BKD filtering to reduce search space
  4. Run evolutionary search (100 population, 50 steps)
  5. Select model from Pareto front and fine-tune
- Design tradeoffs:
  - Search space size vs. computational cost: Larger search spaces increase chances of finding optimal architectures but require more resources
  - Filtering threshold: Aggressive filtering reduces computation but risks eliminating good architectures
  - Epochs for BKD: More epochs improve MSE loss accuracy but increase upfront cost
- Failure signatures:
  - Poor correlation between MSE loss and actual accuracy
  - Loss of model diversity in Pareto front
  - Suboptimal architectures on specific hardware despite good theoretical performance
  - Convergence to local optima due to aggressive filtering
- First 3 experiments:
  1. Reproduce image classification results on ImageNet using EfficientNet-B0 as mothernet
  2. Test BKD filtering effectiveness by comparing search spaces with/without filtering
  3. Validate hardware-in-the-loop optimization by comparing FLOPs-based vs. latency-based search on Samsung Galaxy S10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical limits of using MSE loss as a surrogate metric for NAS across diverse vision tasks, and how do these limits compare to traditional accuracy predictor methods?
- Basis in paper: [explicit] The paper demonstrates that MSE loss can serve as an effective surrogate metric but does not provide theoretical analysis of its limitations or bounds.
- Why unresolved: The paper relies on empirical validation rather than theoretical analysis of when and why MSE loss works as a proxy, and what its failure modes might be.
- What evidence would resolve it: Mathematical proofs or comprehensive empirical studies showing the correlation between MSE loss and final accuracy across various architectures and tasks.

### Open Question 2
- Question: How does the computational efficiency of DONNAv2 scale with increasing search space cardinality and dataset size?
- Basis in paper: [inferred] The paper claims 10x computational savings but only validates on specific search spaces and dataset sizes, with limited analysis of scaling behavior.
- Why unresolved: The paper provides results for specific experimental conditions but doesn't analyze how the efficiency gains would change with larger search spaces or datasets.
- What evidence would resolve it: Systematic experiments varying search space cardinality and dataset size, with analysis of computational complexity scaling and efficiency gains across different regimes.

### Open Question 3
- Question: What are the specific architectural characteristics that make certain blocks redundant in the BKD filtering stage, and how can these characteristics be predicted a priori?
- Basis in paper: [explicit] The paper introduces BKD filtering but doesn't provide detailed analysis of what makes blocks redundant or how to predict redundancy without full training.
- Why unresolved: The filtering criteria is based on empirical thresholding but lacks deeper understanding of the architectural properties that determine redundancy.
- What evidence would resolve it: Analysis of architectural features that correlate with redundancy, and development of predictive models for identifying redundant blocks before full training.

## Limitations
- Lack of theoretical justification for why MSE loss correlates with model accuracy
- BKD filtering mechanism operates without clear theoretical foundation for redundancy
- Hardware-in-the-loop measurements limited to single device may not generalize
- 10x computational reduction claim needs more rigorous ablation studies

## Confidence
- **High Confidence**: Hardware-in-the-loop optimization approach and its benefits for real-world deployment
- **Medium Confidence**: Computational efficiency improvements and block filtering mechanism
- **Low Confidence**: Core assumption that MSE loss correlates with model accuracy and effectiveness of eliminating accuracy predictors

## Next Checks
1. Conduct ablation studies comparing DONNAv2 with conventional accuracy predictor-based NAS to verify the 10x computational reduction claim across multiple hardware platforms
2. Perform correlation analysis between block-level MSE loss and actual model accuracy across different architecture configurations and tasks
3. Test BKD filtering robustness by systematically evaluating whether filtered blocks could have contributed to high-performing architectures in the Pareto front