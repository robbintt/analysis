---
ver: rpa2
title: Diversifying the Mixture-of-Experts Representation for Language Models with
  Orthogonal Optimizer
arxiv_id: '2310.09762'
source_url: https://arxiv.org/abs/2310.09762
tags:
- experts
- performance
- omoe
- optimizer
- parameters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the performance degradation in Mixture-of-Experts
  (MoE) models caused by the homogeneous representation problem, where experts fail
  to specialize and become too similar, undermining the model's capacity. The authors
  propose OMoE, an orthogonal optimizer that enforces each expert to update in a direction
  orthogonal to the subspace spanned by other experts.
---

# Diversifying the Mixture-of-Experts Representation for Language Models with Orthogonal Optimizer

## Quick Facts
- **arXiv ID**: 2310.09762
- **Source URL**: https://arxiv.org/abs/2310.09762
- **Reference count**: 40
- **Primary result**: OMoE optimizer improves MoE model performance by enforcing expert specialization through orthogonal updates, achieving 0.62-0.9 point improvements on GLUE tasks.

## Executive Summary
This paper addresses the homogeneous representation problem in Mixture-of-Experts (MoE) models, where experts become too similar and fail to specialize, limiting model capacity. The authors propose OMoE, an orthogonal optimizer that enforces each expert to update in directions orthogonal to the subspace spanned by other experts. Through an alternating training strategy, OMoE consistently improves performance across GLUE, SuperGLUE, QA, and NER tasks using BERT, RoBERTa, and ALBERT models. The method increases parameter variance among experts, confirming enhanced diversity while achieving measurable task performance gains.

## Method Summary
OMoE introduces an alternating training strategy with two phases: a regular accumulation phase where a base optimizer (AdamW) updates all parameters except experts, and an orthogonal update phase where the OMoE optimizer updates experts in directions orthogonal to the subspace defined by other experts' accumulated inputs. The orthogonal projector is calculated based on accumulated inputs and used to enforce orthogonal parameter updates. This approach prevents experts from learning similar representations while maintaining overall model coherence. The method is evaluated across multiple pre-trained models and tasks, showing consistent performance improvements over standard AdamW optimization.

## Key Results
- OMoE achieves average score improvements of 0.62-0.9 points on GLUE tasks
- SQuAD performance improves by 0.4-0.6% with OMoE optimization
- Parameter variance among experts increases significantly, confirming enhanced diversity
- Consistent improvements observed across BERT, RoBERTa, and ALBERT model families

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Experts become too similar when all parameters update in the same direction, causing homogeneous representation
- Mechanism: In standard MoE, experts receive overlapping gradients because tokens are routed based on gating similarity, causing parameter subspaces to overlap heavily. This creates redundancy and limits model capacity.
- Core assumption: Expert parameters should specialize to different input subspaces to maximize MoE benefits.
- Evidence anchors:
  - [abstract] "homogeneous representation problem, wherein experts in the MoE fail to specialize and lack diversity, leading to frustratingly high similarities in their representations (up to 99%)"
  - [section] "The small variation in the similarity scores indicates that the experts in the MoE architecture have not learned diverse knowledge"
  - [corpus] "Mixture-of-Experts (MoE) architecture for Low-rank adaptation (LoRA) is emerging as a potential direction in parameter-efficient fine-tuning (PEFT) for its modular design and remarkable performance."

### Mechanism 2
- Claim: Orthogonal updates prevent experts from learning similar representations
- Mechanism: The OMoE optimizer calculates average projectors from other experts' accumulated inputs and updates each expert's parameters in the orthogonal direction to these subspaces. This forces experts to specialize in different input regions.
- Core assumption: Input representations from different experts span distinct subspaces that can be separated orthogonally.
- Evidence anchors:
  - [abstract] "introduces an alternating training strategy that encourages each expert to update in a direction orthogonal to the subspace spanned by other experts"
  - [section] "The OMoE optimizer updates the orthogonal projector based on the accumulated inputs" and "parameters of each expert are updated in a direction that is orthogonal to the subspace defined by the previously learned inputs of other experts"
  - [corpus] Weak - no direct evidence in corpus about orthogonal update mechanisms

### Mechanism 3
- Claim: Alternating training phases balance specialization with overall model coherence
- Mechanism: The alternating R Step (regular accumulation) and O Step (orthogonal updates) allows experts to first learn general patterns then specialize, preventing over-specialization while maintaining diversity.
- Core assumption: Some parameter similarity is necessary for model stability and coherence.
- Evidence anchors:
  - [section] "experts first acquire initial knowledge in the accumulating phase before entering the orthogonal phase" and "An alternating training strategy is necessary"
  - [section] "high-resource tasks can take advantage of a large number of parameters in a sparse model, thereby improving model performance without increasing expert differences"
  - [corpus] "The alternating R Step (regular accumulation) and O Step (orthogonal updates) allows experts to first learn general patterns then specialize"

## Foundational Learning

- Concept: Orthogonal projection in linear algebra
  - Why needed here: Understanding how projectors create orthogonal subspaces for expert specialization
  - Quick check question: How does the orthogonal projector formula A(AT A + Î±I)^(-1)AT ensure updates happen in orthogonal directions?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Recognizing why orthogonal updates prevent parameter overlap across experts
  - Quick check question: What parallels exist between preventing expert overlap and preventing catastrophic forgetting in continual learning?

- Concept: Mixture-of-Experts routing mechanisms
  - Why needed here: Understanding how gating functions distribute tokens to create the initial parameter similarity problem
  - Quick check question: Why does token-based routing naturally lead to expert parameter similarity without orthogonal constraints?

## Architecture Onboarding

- Component map:
  - Base optimizer (AdamW) -> handles regular parameter updates and input accumulation
  - OMoE optimizer -> maintains projectors and applies orthogonal updates to experts
  - Projector matrix -> stores orthogonal subspace information for each expert
  - Gating function -> routes tokens to experts (unchanged from standard MoE)
  - Expert networks -> 2-layer FFNs that receive orthogonal updates

- Critical path:
  1. Token routing through gating function
  2. Input accumulation in base optimizer
  3. Projector calculation from accumulated inputs
  4. Orthogonal parameter updates in experts
  5. Model evaluation and loss calculation

- Design tradeoffs:
  - Storage vs performance: Additional projector storage enables better specialization
  - Training speed vs diversity: Alternating phases slow training but improve expert differentiation
  - Complexity vs generalization: More complex orthogonal updates may improve task performance but could reduce transfer learning ability

- Failure signatures:
  - Expert parameters become identical (similarity > 99%)
  - Model performance degrades below baseline dense model
  - Projector rank collapses to zero (skipping step too small)
  - Some experts receive no tokens (gating imbalance)

- First 3 experiments:
  1. Measure expert parameter similarity before and after OMoE training on a single task
  2. Compare GLUE scores between OMoE and standard AdamW across different expert counts
  3. Test different skipping step values to find optimal balance between accumulation and orthogonal updates

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between parameter similarity and diversity among experts in MoE models for achieving the best performance?
- Basis in paper: [inferred] The paper mentions that while OMoE increases parameter variance and improves performance, it also notes that "the larger difference between experts does not necessarily mean better model performance" and that "increasing the difference between the parameters of the experts should not be the primary goal of MoE."
- Why unresolved: The paper demonstrates that too much diversity can lead to instability and inconsistency, while too little diversity limits performance. However, it does not provide a quantitative framework for determining the optimal balance.
- What evidence would resolve it: Empirical studies measuring performance across a range of parameter similarity thresholds, combined with theoretical analysis of how similarity affects expert specialization and generalization.

### Open Question 2
- Question: How does the effectiveness of OMoE scale with the number of experts in MoE models?
- Basis in paper: [explicit] The paper states "the effectiveness of OMoE optimizer may saturate beyond a certain number of experts, and adding more experts may not provide further performance improvement" and that "the increasing number of experts also means an increase in network capacity, which can lead to overfitting."
- Why unresolved: While the paper observes diminishing returns with more experts, it doesn't establish a precise relationship or identify the point at which adding more experts becomes counterproductive.
- What evidence would resolve it: Systematic experiments varying the number of experts while measuring performance, projector capacity utilization, and overfitting metrics across different task complexities.

### Open Question 3
- Question: Can the alternating training strategy in OMoE be optimized by dynamically adjusting the ratio of R Steps to O Steps based on training progress?
- Basis in paper: [inferred] The paper uses a fixed skipping step for alternating between R Steps and O Steps, noting that "s can not be set too small" due to projector capacity limitations, but doesn't explore dynamic adjustment.
- Why unresolved: The paper treats the skipping step as a hyperparameter to be tuned but doesn't investigate whether a dynamic approach could adapt better to different training phases or task types.
- What evidence would resolve it: Comparative experiments between fixed and dynamically adjusted skipping steps, measuring performance and projector capacity utilization across various tasks and model scales.

## Limitations
- The method shows performance degradation on low-resource tasks due to insufficient input data for accurate subspace representation
- Optimal skipping step values and sensitivity to hyperparameter settings are not thoroughly explored
- The orthogonal projector calculation mechanism lacks detailed implementation specifications

## Confidence
- **High confidence**: Core observation of expert similarity problem (up to 99%) and basic orthogonal update mechanism
- **Medium confidence**: Claims of consistent task performance improvements across multiple benchmarks
- **Low confidence**: Internal contradiction about whether expert differences increase or not

## Next Checks
- **Validation Check 1**: Implement and measure expert parameter similarity dynamics over training epochs to verify the 99% similarity reduction claim
- **Validation Check 2**: Conduct ablation studies varying the skipping step parameter s across a wider range (1-20) on a representative task like MNLI
- **Validation Check 3**: Test OMoE on a large-scale model (BERT-large or T5-base) with varying expert counts (2, 4, 8, 16) to evaluate scalability