---
ver: rpa2
title: 'DiQAD: A Benchmark Dataset for End-to-End Open-domain Dialogue Assessment'
arxiv_id: '2310.16319'
source_url: https://arxiv.org/abs/2310.16319
tags:
- dialogue
- quality
- dialogues
- user
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DiQAD, a large-scale dataset for end-to-end
  open-domain dialogue quality assessment. It contains around 100,000 dialogues across
  6 domains, annotated with a 3-scale holistic score (0-2) based on 6 quality dimensions:
  grammaticality, relevance, consistency, empathy, proactivity, and informativeness.'
---

# DiQAD: A Benchmark Dataset for End-to-End Open-domain Dialogue Assessment

## Quick Facts
- arXiv ID: 2310.16319
- Source URL: https://arxiv.org/abs/2310.16319
- Reference count: 23
- Contains 100,000 dialogues across 6 domains annotated with 3-scale holistic quality scores

## Executive Summary
This paper introduces DiQAD, a large-scale dataset for end-to-end open-domain dialogue quality assessment. The dataset contains around 100,000 dialogues from real user conversations on the WenYiWen platform, annotated with a 3-scale holistic score (0-2) based on 6 quality dimensions. Extensive experiments demonstrate that pre-trained models like ERNIE achieve the best performance, and the dataset shows sensitivity to user sentiment expressions. The dataset is openly accessible and aims to advance the development of dialogue quality assessment methods.

## Method Summary
The dataset construction involves collecting real dialogues from the WenYiWen platform across 6 domains, then having experienced annotators label each dialogue with a 3-scale holistic quality score. The annotation process uses dual-annotator agreement with a quality controller review, requiring at least 92% accuracy for acceptance. Benchmark models include classical methods (Naive Bayes, Logistic Regression, XGBoost, GRU) and Transformer-based approaches (BERT, ERNIE). Models are trained on concatenated dialogue text with role prefixes and evaluated using multiple metrics including accuracy, UAR, Kappa, and correlation measures.

## Key Results
- ERNIE achieves the highest accuracy at 80.5% on the DiQAD test set
- Cross-domain experiments show comparable performance on unseen domains with performance decreasing by 1.1% to 6.8% depending on domain
- Adding negative sentiment expressions to dialogues drastically decreases performance (from 79.9% to 65.7%), while positive expressions have minimal impact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DiQAD's end-to-end quality assessment approach captures holistic dialogue quality better than sub-metric evaluations.
- Mechanism: By using a 3-scale holistic score based on 6 dimensions (grammaticality, relevance, consistency, empathy, proactivity, informativeness), the dataset captures the compound nature of human dialogue quality judgments rather than isolated aspects.
- Core assumption: Human judges can reliably assess dialogue quality as a unified construct across multiple dimensions simultaneously.
- Evidence anchors:
  - [abstract] "Existing work are uncapable of providing an end-to-end and human-epistemic assessment dataset"
  - [section 3] "We set the quality label as a 3-scale holistic score (from 0 to 2) by considering the above dimensions"
  - [corpus] Weak - corpus only shows related work on sub-metric approaches, no direct comparison to DiQAD's holistic approach
- Break condition: If annotators show low agreement on holistic scores compared to individual dimension scores, or if models trained on holistic labels perform worse than models trained on individual dimensions.

### Mechanism 2
- Claim: Real user dialogues provide more authentic quality assessment data than annotator-conversed dialogues.
- Mechanism: Dialogues collected from real user-expert interactions on WenYiWen platform reflect genuine information-seeking behavior and natural conversation patterns, unlike controlled experiments with recruited annotators.
- Core assumption: Real user dialogues exhibit quality patterns that are representative of actual dialogue system deployment scenarios.
- Evidence anchors:
  - [abstract] "the dialogues are conversed between annotators far from real user settings"
  - [section 4.1] "We collect real dialogues in text from WenYiWen" and "The experts are invited from various professions"
  - [corpus] Weak - corpus only mentions related datasets using annotator conversations, no direct evidence about real vs. annotator dialogue quality differences
- Break condition: If quality assessment models trained on real dialogues generalize poorly to annotator-conversed dialogues or fail to capture quality patterns in synthetic dialogues.

### Mechanism 3
- Claim: The dataset's sensitivity to user sentiment expressions enables dialogue systems to better satisfy user needs.
- Mechanism: By showing that models predict lower quality scores for dialogues with negative user expressions and higher scores for positive ones, the dataset captures the importance of emotional engagement in dialogue quality.
- Core assumption: User sentiment expressions are reliable indicators of dialogue quality from the user's perspective.
- Evidence anchors:
  - [section 5.3] "we design two types of sentiment expressions and add them to the end of the original dialogues" and "the performance of ERNIE+pos decreases slightly compared to the original Ernie...adding the negative expressions...makes the accuracy score drastically decline"
  - [corpus] Weak - no direct evidence in corpus about sentiment's role in quality assessment
- Break condition: If sentiment expressions are found to be unreliable indicators of quality, or if models overfit to sentiment rather than actual content quality.

## Foundational Learning

- Concept: Human-epistemic evaluation criteria
  - Why needed here: To establish assessment dimensions that align with how humans naturally judge dialogue quality
  - Quick check question: Can you explain why the 6 dimensions (grammaticality, relevance, consistency, empathy, proactivity, informativeness) were chosen instead of other possible dimensions?

- Concept: End-to-end vs. sub-metric evaluation
  - Why needed here: To understand why holistic quality assessment is more valuable than evaluating individual aspects separately
  - Quick check question: What are the potential drawbacks of using only sub-metrics like coherence or diversity for dialogue quality assessment?

- Concept: Real vs. synthetic dialogue data
  - Why needed here: To appreciate the value of authentic user interactions over controlled experimental dialogues
  - Quick check question: How might dialogues between recruited annotators differ systematically from real user-expert conversations?

## Architecture Onboarding

- Component map: Data collection (WenYiWen platform integration) → dialogue extraction → domain filtering → annotation pipeline (batch processing → dual-annotator agreement → quality controller review → acceptance criteria) → benchmark model training (classical methods → Transformer-based methods → LLM-based methods) → performance analysis (cross-domain evaluation → sentiment sensitivity testing → hyperparameter analysis)

- Critical path: Data collection → quality annotation → dataset construction → benchmark model training → performance analysis

- Design tradeoffs:
  - Holistic scoring (0-2) vs. fine-grained scoring (1-5): Chose 0-2 for higher annotation agreement and reliability
  - Real user dialogues vs. synthetic data: Chose real dialogues for authenticity despite potential privacy/ethics concerns
  - Large-scale dataset vs. smaller expert-annotated dataset: Chose scale (100k dialogues) to enable robust model training

- Failure signatures:
  - Low inter-annotator agreement (<92% overall accuracy threshold)
  - Poor cross-domain generalization performance
  - High sensitivity to sentiment expressions rather than content quality
  - Benchmark models showing minimal performance differences

- First 3 experiments:
  1. Train ERNIE baseline on full dataset and evaluate on test set to establish performance baseline
  2. Remove domain-specific samples from training and test on those domains to assess cross-domain generalizability
  3. Add positive and negative sentiment expressions to dialogues and measure performance changes to test sentiment sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How sensitive is the DiQAD dataset to sentiment expressions, and how does this impact the quality assessment of dialogues?
- Basis in paper: [explicit] The paper mentions that the model is more likely to predict a low-quality label when the dialogue contains negative user expressions and tends to give a higher quality if there is positive user feedback. This sensitivity to sentiment expressions is shown in the performance changes when adding different sentiment expressions to the dialogues.
- Why unresolved: The paper provides initial findings on the sensitivity to sentiment expressions, but does not explore the extent of this sensitivity or its implications for dialogue quality assessment in depth.
- What evidence would resolve it: Further experiments could be conducted to quantify the impact of sentiment expressions on dialogue quality assessment. This could involve systematically varying the sentiment expressions in dialogues and measuring the changes in the model's predictions.

### Open Question 2
- Question: How well does the DiQAD dataset generalize to unseen domains, and what factors influence its performance in cross-domain settings?
- Basis in paper: [explicit] The paper mentions that cross-domain experiments show the model can still achieve comparable performance on domains not appearing in the training set, with performance decrease rates related to the number of samples in that domain.
- Why unresolved: The paper provides initial findings on cross-domain generalizability, but does not explore the factors influencing performance in cross-domain settings or provide a comprehensive analysis of the model's ability to generalize.
- What evidence would resolve it: Further experiments could be conducted to identify the factors influencing cross-domain performance, such as the similarity between domains, the complexity of the dialogues, and the availability of training data. This could involve systematically varying these factors and measuring their impact on the model's performance.

### Open Question 3
- Question: How does the DiQAD dataset compare to other reference-free assessment methods in terms of evaluating dialogue quality, and what are the strengths and weaknesses of different approaches?
- Basis in paper: [explicit] The paper mentions that unsupervised methods perform unfavorably compared to supervised methods, and that simple metrics like relevance and fluency are not well suited for DiQAD. The paper also compares the performance of different reference-free assessment methods on DiQAD.
- Why unresolved: The paper provides initial findings on the comparison with other reference-free assessment methods, but does not provide a comprehensive analysis of the strengths and weaknesses of different approaches or explore the reasons for the performance differences.
- What evidence would resolve it: Further experiments could be conducted to systematically compare the performance of different reference-free assessment methods on DiQAD, using a variety of metrics and datasets. This could involve conducting ablation studies to identify the key factors contributing to the performance differences and exploring the reasons for the strengths and weaknesses of different approaches.

## Limitations
- The dataset's reliance on annotator judgments may introduce subjectivity and cultural bias
- The 0-2 scale may oversimplify quality distinctions, potentially masking nuanced quality differences
- The focus on Chinese-language dialogues from a specific platform may limit generalizability to other languages and contexts

## Confidence
- High Confidence: Dataset construction methodology is well-documented and reproducible; benchmark model results are robust with clear performance rankings
- Medium Confidence: Effectiveness of the 6-dimensional quality assessment framework; generalizability to other languages and cultural contexts
- Low Confidence: Practical utility of sentiment sensitivity analysis; claim that real user dialogues are inherently superior to annotator-conversed dialogues

## Next Checks
1. Recompute Cohen's Kappa and Spearman correlations using raw pairwise annotator scores before quality controller aggregation to assess true inter-annotator agreement

2. Test whether models trained on the Chinese DiQAD dataset maintain performance when evaluated on open-domain dialogue quality datasets from other languages and cultures

3. Re-annotate a subset of dialogues using a 1-5 scale and compare model performance to the 0-2 scale to determine if simplified scoring captures sufficient quality granularity