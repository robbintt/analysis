---
ver: rpa2
title: Can Language Model Moderators Improve the Health of Online Discourse?
arxiv_id: '2311.10781'
source_url: https://arxiv.org/abs/2311.10781
tags:
- moderation
- moderator
- conversation
- evaluation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for evaluating language models
  as conversational moderators to improve online discourse. The authors define moderation
  effectiveness through a multidisciplinary lens, incorporating insights from social
  science, and establish design criteria for realistic yet safe evaluation.
---

# Can Language Model Moderators Improve the Health of Online Discourse?

## Quick Facts
- arXiv ID: 2311.10781
- Source URL: https://arxiv.org/abs/2311.10781
- Reference count: 15
- One-line primary result: Appropriately prompted language models can provide specific and fair feedback on toxic behavior but struggle to influence users to increase respect and cooperation.

## Executive Summary
This paper introduces a framework for evaluating language models as conversational moderators in online discourse. The authors develop a multidisciplinary evaluation approach that incorporates social science insights to assess moderation effectiveness through surveys measuring cooperation, respect, fairness, and specificity. Using this framework, they conduct the first known study of language models as moderators, finding that while prompted models can provide fair and specific feedback on toxic behavior, their ability to improve user cooperation and respect is limited and highly dependent on the evaluator's perspective.

## Method Summary
The study employs a simulated conversation approach where human participants engage with language model moderator bots in controversial discussions extracted from Reddit. Participants take turns with the bot for three exchanges each, after which they complete surveys evaluating the moderation effectiveness. The evaluation framework tests various prompting strategies including baseline, nonviolent communication, and Socratic dialogue approaches, using models like Cosmo-XL and GPT-based systems. Effectiveness is measured through first-person (direct participant experience) and third-person (observer) evaluations.

## Key Results
- Appropriately prompted models provide specific and fair feedback on toxic behavior
- Moderation effectiveness varies significantly between first-person and third-person evaluations
- Language models struggle to improve user levels of respect and cooperation despite providing fair feedback
- Human word count is a weak indicator of cooperative behavior

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompted large language models (LLMs) can provide specific and fair feedback on toxic behavior in online discourse.
- Mechanism: By incorporating insights from conflict resolution, cognitive behavioral therapy, and effective communication techniques into the prompt, LLMs generate tailored responses perceived as fair by participants.
- Core assumption: Prompt quality significantly influences LLM's ability to generate effective moderation responses.
- Evidence anchors: Abstract finding on appropriately prompted models providing specific and fair feedback; section 3 discussion on prompt engineering.

### Mechanism 2
- Claim: Effectiveness varies based on evaluator perspective (first-person vs. third-person).
- Mechanism: Direct engagement with moderator bots leads to more positive perception compared to observer evaluations.
- Core assumption: Subjective experience of being moderated influences effectiveness perception.
- Evidence anchors: Abstract comparison of first-person vs. third-person effectiveness; section 5.2 findings on convergence of scores.

### Mechanism 3
- Claim: Human word count serves as a weak indicator of cooperative behavior.
- Mechanism: Helpful moderation leads to increased participant verbosity, indicating cooperation.
- Core assumption: Cooperative behavior correlates with increased engagement and verbosity.
- Evidence anchors: Section 5.3 hypothesis about human word count as indirect cooperation measure; finding that word count ranking doesn't align with cooperation metrics.

## Foundational Learning

- Concept: Natural Language Processing (NLP) and Language Models
  - Why needed here: Paper relies on NLP techniques and language models for developing and evaluating moderation strategies.
  - Quick check question: What distinguishes rule-based from machine learning approaches in NLP, and how do language models fit?

- Concept: Online Community Moderation
  - Why needed here: Focus is on evaluating language models as conversational moderators in online communities.
  - Quick check question: What are key moderation challenges and how do different strategies address them?

- Concept: Conflict Resolution and Effective Communication Techniques
  - Why needed here: Paper incorporates conflict resolution and communication techniques to prompt effective moderation.
  - Quick check question: How do conflict resolution strategies contribute to de-escalating conflicts and promoting constructive dialogue?

## Architecture Onboarding

- Component map: Controversial conversation stubs -> Evaluation framework -> Language models with prompts -> Human participants -> Survey responses

- Critical path: 1) Extract Reddit conversation stubs 2) Set up dyadic chat environment 3) Generate moderation responses 4) Collect survey feedback 5) Analyze effectiveness

- Design tradeoffs: Safety vs. realism (simulated offline conversations may limit realism); participant subjectivity vs. objectivity (first-person more subjective but accurate, third-person more objective but may miss nuances)

- Failure signatures: Irrelevant or unfair bot responses indicate prompt failure; negative participant feedback suggests ineffective strategy; inaccurate conversation simulation leads to biased results

- First 3 experiments: 1) Compare different prompting strategies using same model to isolate prompt impact 2) Evaluate consistency across different controversial topics 3) Test impact of prompt length and detail on response quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can language models be improved to more effectively guide users to increase their levels of respect and cooperation in online conversations?
- Basis in paper: Explicit - Paper identifies language models struggle to influence users to increase respect and cooperation despite providing fair feedback.
- Why unresolved: Paper identifies challenge but doesn't provide definitive solution or method to overcome limitation.
- What evidence would resolve it: Follow-up study testing various strategies or enhancements to language models and measuring impact on user behavior.

### Open Question 2
- Question: What are the key differences in moderation effectiveness when evaluated from first-person vs. third-person perspectives?
- Basis in paper: Explicit - Paper finds discrepancy between first-person and third-person evaluation effectiveness.
- Why unresolved: Paper identifies discrepancy but doesn't explore underlying reasons or how to account for it.
- What evidence would resolve it: Research exploring cognitive and perceptual biases influencing different perspective evaluations.

### Open Question 3
- Question: Can automated evaluation metrics reliably replace human survey responses in assessing language model moderator effectiveness?
- Basis in paper: Explicit - Paper explores non-survey metrics but finds them weakly correlated or unreliable.
- Why unresolved: Paper suggests automated metrics might offer insights but can't fully capture nuances that human evaluations provide.
- What evidence would resolve it: Development and validation of comprehensive automated metrics that consistently measure moderator effectiveness.

## Limitations

- Evaluation context may not transfer to real-world online discourse due to controlled laboratory setting
- Complexity of prompt engineering makes it unclear which elements drive effectiveness versus general model capabilities
- Subjective moderation metrics may vary significantly based on individual interpretation and cultural background

## Confidence

**High Confidence Claims**:
- Appropriately prompted models can generate specific and fair feedback on toxic behavior
- Third-person evaluations show less effectiveness in making users cooperative and respectful compared to first-person evaluations
- Human word count is a weak indicator of cooperative behavior

**Medium Confidence Claims**:
- Multidisciplinary evaluation framework provides comprehensive assessment
- Language models can serve as effective conversational moderators with proper prompting
- Different controversial topics may yield different moderation effectiveness outcomes

**Low Confidence Claims**:
- Optimal prompt engineering approach for conversational moderation
- Generalizability of findings to large-scale real-world online communities
- Long-term behavioral changes resulting from AI moderation interventions

## Next Checks

1. Implement most effective prompting strategies in an active online community with organic user interactions to validate laboratory findings in naturalistic settings.

2. Systematically isolate and test individual components of successful prompts to determine which elements contribute most to moderation effectiveness.

3. Replicate the study with participants from diverse cultural backgrounds to assess whether moderation effectiveness varies based on cultural norms and communication styles.