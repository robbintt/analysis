---
ver: rpa2
title: Symbolic and Language Agnostic Large Language Models
arxiv_id: '2308.14199'
source_url: https://arxiv.org/abs/2308.14199
tags:
- language
- word
- these
- llms
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that the success of large language models (LLMs)
  is not due to the symbolic vs. subsymbolic debate but to a bottom-up reverse engineering
  strategy applied at scale.
---

# Symbolic and Language Agnostic Large Language Models

## Quick Facts
- arXiv ID: 2308.14199
- Source URL: https://arxiv.org/abs/2308.14199
- Reference count: 17
- Primary result: Proposes creating explainable, ontologically grounded LLMs using symbolic reverse engineering of language patterns

## Executive Summary
This paper argues that large language models succeed not due to symbolic vs. subsymbolic debates, but through bottom-up reverse engineering at scale. However, because LLMs are subsymbolic, their acquired knowledge remains buried in millions of non-meaningful weights, making them unexplainable and prone to inferential errors. The author proposes applying the same bottom-up strategy within a symbolic framework to create language-agnostic, ontologically grounded LLMs. This involves analyzing word usage patterns across contexts to discover linguistic knowledge, converting it into primitive relations between reified entities, and constructing an underlying ontology. By using GPT-4 for masking and querying to generate plausible contextual replacements, the method aims to define dimensions of word meaning and uncover implicit ontological structures, potentially leading to explainable and semantically grounded language models.

## Method Summary
The approach involves analyzing how words are used across contexts in a large corpus to discover linguistic knowledge, converting this into primitive relations between reified entities, and constructing an underlying ontology. GPT-4 is employed to generate plausible contextual replacements via masking, which helps define dimensions of word meaning and uncover implicit ontological structures. The method aims to create explainable representations where meanings are traceable to specific relationships rather than buried in weights.

## Key Results
- Proposes a symbolic framework for creating language-agnostic LLMs
- Introduces method for discovering implicit ontology through bottom-up language analysis
- Uses GPT-4 masking to define dimensions of word meaning and uncover ontological structures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bottom-up reverse engineering of language can discover the implicit ontology of natural language without relying on predefined metaphysical theories.
- Mechanism: By analyzing co-occurrence patterns of words in a large corpus, the system identifies sets of properties (relations) that can sensibly apply to different types of entities. These sets are then converted into primitive relations between reified entities, constructing an underlying ontology.
- Core assumption: The way we talk about the world reflects an implicit structure that can be reverse-engineered from language use patterns.
- Evidence anchors:
  - [abstract]: "by analyzing how words are used across contexts to discover linguistic knowledge, convert it into primitive relations between reified entities, and construct an underlying ontology"
  - [section]: "This idea is similar to the idea of type checking in strongly typed polymorphic programming languages... This type of analysis can be used to 'discover' the ontology that seems to be implicit in all natural languages"
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.457, average citations=0.0. Weak corpus evidence for direct ontology discovery claims.
- Break condition: If the linguistic communication patterns do not reliably reflect consistent ontological structures, or if the nominalization process fails to preserve meaningful distinctions between entity types.

### Mechanism 2
- Claim: Masking and querying GPT-4 can generate plausible contextual replacements that reveal dimensions of word meaning and implicit ontological structures.
- Mechanism: By systematically masking different positions in sentences and asking GPT-4 for plausible replacements, the system identifies which actions, properties, and relations make sense for specific entity types. These patterns reveal the "dimensions of word meaning" that define each concept.
- Core assumption: GPT-4's training on diverse text corpora captures the same distributional patterns that underlie natural language understanding, making it a useful tool for ontology discovery.
- Evidence anchors:
  - [abstract]: "By using masking and querying GPT-4 to generate plausible contextual replacements, the method aims to define dimensions of word meaning and uncover implicit ontological structures"
  - [section]: "We can use GPT-4 to generate some of these vectors along the various dimensions. The data in figure 3 is obtained by asking GPT-4 to provide 25 'plausible' replacements for the [MASK] in the given sentences"
  - [corpus]: Weak corpus evidence - most related papers are about LLMs but don't specifically address GPT-4 masking techniques for ontology discovery.
- Break condition: If GPT-4's responses become unreliable or biased in ways that don't reflect natural language patterns, or if the masking approach fails to capture meaningful distinctions between concepts.

### Mechanism 3
- Claim: The symbolic representation of word meanings through primitive relations provides explainability and semantic grounding that subsymbolic LLMs lack.
- Mechanism: Each entity is defined by its primitive relations (agentOf, objectOf, hasProp, etc.), creating an explainable representation where meanings are traceable to specific relationships rather than buried in weights.
- Core assumption: Explainability requires that computations be invertible or have semantic maps saved in symbolic structures, which is impossible in subsymbolic deep neural networks.
- Evidence anchors:
  - [abstract]: "whatever knowledge these systems acquire about language will always be buried in millions of microfeatures (weights) none of which is meaningful on its own, rendering these models to be utterly unexplainable"
  - [section]: "Explainability is 'inference in reverse' and thus a computation is explainable if (i) the computation is invertible; or (ii) a semantic map of the computation is saved in some symbolic structure (e.g., an abstract syntax tree), and neither of these apply to subsymbolic deep neural networks"
  - [corpus]: No direct corpus evidence for explainability claims - related papers focus on LLM limitations but not specifically on symbolic explainability mechanisms.
- Break condition: If the primitive relations become too numerous or complex to maintain practical explainability, or if the symbolic representation fails to capture nuanced semantic relationships.

## Foundational Learning

- Concept: Ontological hierarchy construction
  - Why needed here: The system needs to build hierarchical relationships between entity types based on shared properties and relations discovered through language analysis
  - Quick check question: If both "computer" and "car" can sensibly have the property "RUNNING" but "couch" cannot, what does this tell us about their positions in the ontological hierarchy?

- Concept: Nominalization and reification
  - Why needed here: Converting properties and relations into first-class entities (like turning "ARTICULATE" into "articulation") allows the system to create primitive relations between entities rather than between entities and properties
  - Quick check question: Why would the system convert "app(ARTICULATE, human)" into "hasProp(articulation, human)" rather than keeping it as a simple property check?

- Concept: Type checking and type unification
  - Why needed here: The system needs to verify that properties and relations make sense for specific entity types, similar to how type systems in programming languages prevent type errors
  - Quick check question: Given the examples [delicious Thursday] (nonsensical) vs [delicious apple] (sensible), what type checking rule would the system need to implement?

## Architecture Onboarding

- Component map: Corpus analysis module → Relation discovery engine → Nominalization processor → Ontology builder → GPT-4 masking interface → Dimension vector generator
- Critical path: Corpus → Relation sets (app(p,c)) → Primitive relations (R(entity,entity)) → Ontology hierarchy → Explainable representations
- Design tradeoffs: Symbolic precision vs. scalability (symbolic processing is more explainable but computationally heavier than subsymbolic approaches), GPT-4 dependency vs. independence (using GPT-4 provides broad coverage but creates external dependency)
- Failure signatures: Inconsistent ontology hierarchies (suggesting corpus analysis problems), GPT-4 responses that don't align with human linguistic intuitions (suggesting masking approach limitations), overly complex primitive relation sets (suggesting scalability issues)
- First 3 experiments:
  1. Validate relation discovery on a small, controlled corpus where correct ontological relationships are known, comparing discovered relations against ground truth
  2. Test nominalization process by converting a set of properties to reified entities and verifying that primitive relations preserve intended meaning
  3. Experiment with GPT-4 masking on entity types with clear semantic boundaries to verify that the approach correctly identifies distinguishing properties and relations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically determine the primitive and language-agnostic relations that

## Limitations
- The dependence on GPT-4 creates a circular dependency where the symbolic system's quality depends on the subsymbolic model it aims to replace
- Computational complexity of maintaining and querying symbolic representations at scale remains unaddressed
- No empirical validation that discovered ontologies actually improve inferential reasoning compared to state-of-the-art LLMs

## Confidence

**High Confidence**: The core critique of subsymbolic LLMs being unexplainable due to knowledge being buried in weights is well-established in the literature. The claim that explainability requires either invertibility or symbolic semantic maps is theoretically sound based on established computational principles.

**Medium Confidence**: The feasibility of bottom-up ontology discovery through language analysis is supported by linguistic theories about distributional semantics, but practical implementation challenges and scalability concerns remain significant unknowns. The approach's ability to achieve language agnosticism through symbolic representation is plausible but not yet demonstrated.

**Low Confidence**: The claim that this approach will produce superior inferential reasoning capabilities compared to current LLMs is speculative without empirical validation. The scalability of symbolic representations for large-scale language modeling is uncertain, and the computational efficiency compared to subsymbolic approaches is unclear.

## Next Checks

1. **Ontology Discovery Validation**: Test the bottom-up ontology discovery approach on a controlled corpus where ground truth ontological relationships are known (such as programming language documentation or structured domain knowledge). Compare the discovered ontology against the ground truth to measure accuracy and completeness.

2. **Explainability Benchmark**: Implement a small-scale symbolic language model using the proposed primitive relations and nominalization approach. Compare its explainability (ability to trace reasoning steps) against a comparable subsymbolic model on standard reasoning tasks, measuring both accuracy and interpretability.

3. **Scalability Stress Test**: Evaluate the computational complexity of maintaining and querying the symbolic representations as corpus size increases. Measure memory usage, inference time, and accuracy degradation compared to subsymbolic approaches on progressively larger datasets.