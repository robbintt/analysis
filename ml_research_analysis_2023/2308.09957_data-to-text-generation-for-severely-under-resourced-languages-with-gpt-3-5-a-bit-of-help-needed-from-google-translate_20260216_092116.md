---
ver: rpa2
title: 'Data-to-text Generation for Severely Under-Resourced Languages with GPT-3.5:
  A Bit of Help Needed from Google Translate'
arxiv_id: '2308.09957'
source_url: https://arxiv.org/abs/2308.09957
tags:
- few-shot
- prompt
- english
- webnlg
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores the performance of large language models (LLMs)
  on data-to-text generation tasks for severely under-resourced languages. The authors
  evaluate four system variants using GPT-3.5 with and without machine translation
  (MT) for Irish, Maltese, Welsh, and Breton.
---

# Data-to-text Generation for Severely Under-Resourced Languages with GPT-3.5: A Bit of Help Needed from Google Translate

## Quick Facts
- arXiv ID: 2308.09957
- Source URL: https://arxiv.org/abs/2308.09957
- Reference count: 3
- Primary result: Few-shot prompting outperforms zero-shot for direct generation in under-resourced languages, but this advantage disappears when pivoting through English translation

## Executive Summary
This paper investigates the use of large language models (LLMs) for data-to-text generation in severely under-resourced languages. The authors evaluate four system variants using GPT-3.5 with and without machine translation for Irish, Maltese, Welsh, and Breton. The study finds that few-shot prompting significantly improves performance for direct generation in target languages, while translation through English eliminates this advantage. The few-shot plus translation approach achieved state-of-the-art results in the WebNLG 2023 shared task across all evaluated languages.

## Method Summary
The study uses GPT-3.5 (text-davinci-003) with specific parameters (temperature=0, top p=1, frequency penalty=0, presence penalty=0, best of=1, max length=500) to generate text from RDF triples. Four system variants were tested: zero-shot and few-shot generation directly into target languages, and zero-shot and few-shot generation into English followed by translation. Google Translate API provided the translation capability. Generated texts underwent post-processing to clean HTML characters, remove underscores, and eliminate wrapping quotes. Performance was evaluated using BLEU, ChrF++, and TER metrics.

## Key Results
- Few-shot prompting significantly outperforms zero-shot prompting for direct generation into under-resourced languages
- The advantage of few-shot prompting disappears when generating through English and translating to target languages
- Few-shot plus translation system variants achieved new state-of-the-art results on all metrics across all four evaluated languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Few-shot prompting improves LLM performance on under-resourced languages by providing explicit task examples
- Mechanism: The LLM leverages few-shot examples as in-context learning to better understand target language structure and expected output format
- Core assumption: The LLM has some pre-existing knowledge of the target language but needs task-specific guidance
- Evidence anchors: Abstract states "few-shot prompting works better for direct generation into under-resourced languages"; section confirms this finding
- Break condition: If the LLM has negligible knowledge of the target language, few-shot examples may not provide sufficient guidance

### Mechanism 2
- Claim: Translation through English eliminates the need for few-shot examples
- Mechanism: Pivoting through English allows the LLM to leverage its strong English capabilities while translation handles target language generation
- Core assumption: The LLM has sufficient English generation capabilities to produce acceptable intermediate outputs
- Evidence anchors: Abstract states "when pivoting via English, the difference between few-shot and zero-shot prompts disappears"; section confirms this observation
- Break condition: If translation quality is poor, benefits of pivoting through English may be negated

### Mechanism 3
- Claim: GPT-3.5 can achieve state-of-the-art results on under-resourced languages with minimal fine-tuning
- Mechanism: The LLM's broad pre-training data includes enough linguistic patterns from target languages to enable competent generation with appropriate prompting
- Core assumption: Target languages have some representation in LLM's training data despite being "severely under-resourced"
- Evidence anchors: Abstract states "few-shot + translation system variants were submitted to the WebNLG 2023 shared task where they outperformed competitor systems by substantial margins"
- Break condition: If target language has virtually no representation in training data, LLM may fail to generate coherent text

## Foundational Learning

- Concept: Prompt engineering
  - Why needed here: The study shows different prompt types (zero-shot vs few-shot) significantly impact performance
  - Quick check question: What is the key difference between zero-shot and few-shot prompting?

- Concept: Translation as a bridge
  - Why needed here: The study demonstrates that generating through English + translation can be more effective than direct generation
  - Quick check question: Why might generating through English be more effective than direct generation for under-resourced languages?

- Concept: Evaluation metrics in NLG
  - Why needed here: The study uses BLEU, ChrF++, and TER to measure system performance
  - Quick check question: What does each of these metrics (BLEU, ChrF++, TER) measure in the context of text generation?

## Architecture Onboarding

- Component map:
  - RDF triples -> GPT-3.5 API -> Generated text -> Post-processing -> BLEU/ChrF++/TER evaluation
  - RDF triples -> GPT-3.5 API -> Generated English text -> Google Translate API -> Translated text -> BLEU/ChrF++/TER evaluation

- Critical path:
  1. Prepare prompt with triples
  2. Send to GPT-3.5 API
  3. Receive generated text
  4. Apply post-processing
  5. If using translation, send to Google Translate
  6. Evaluate using BLEU, ChrF++, TER

- Design tradeoffs:
  - Direct generation vs. translation-based approach
  - Zero-shot vs. few-shot prompting
  - API costs vs. performance gains
  - Development time vs. model customization

- Failure signatures:
  - Poor BLEU scores indicate translation or generation issues
  - High TER scores suggest significant differences from reference text
  - Inconsistent performance across languages may indicate data quality issues

- First 3 experiments:
  1. Test zero-shot vs. few-shot prompting on a small sample
  2. Compare direct generation vs. translation-based approach
  3. Validate post-processing steps on sample outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the results of the few-shot + MT system compare to a system that directly generates text in the under-resourced language without any translation?
- Basis in paper: The paper compares few-shot + MT system to direct generation systems in terms of BLEU, ChrF++, and TER scores
- Why unresolved: The paper does not provide a direct comparison between these two approaches
- What evidence would resolve it: A direct comparison between few-shot + MT system and direct generation system would resolve this question

### Open Question 2
- Question: How do the results of the few-shot + MT system vary across different under-resourced languages?
- Basis in paper: The paper reports performance for Irish, Maltese, Welsh, and Breton in terms of BLEU, ChrF++, and TER scores
- Why unresolved: The paper does not provide detailed analysis of performance variation across languages
- What evidence would resolve it: Detailed analysis of performance variation across different under-resourced languages would resolve this question

### Open Question 3
- Question: How does the performance of the few-shot + MT system compare to other state-of-the-art data-to-text generation systems for under-resourced languages?
- Basis in paper: The paper compares few-shot + MT system to a baseline system that generates text in English and then translates it
- Why unresolved: The paper does not provide comparison with other state-of-the-art systems
- What evidence would resolve it: Comparison with other state-of-the-art data-to-text generation systems would resolve this question

## Limitations

- The study relies on commercial APIs (GPT-3.5 and Google Translate) whose behavior may change over time
- Evaluation is limited to only four specific under-resourced languages
- The paper lacks detailed information about specific examples used in few-shot prompts

## Confidence

- **High Confidence**: Few-shot prompting outperforms zero-shot prompting for direct generation into under-resourced languages
- **Medium Confidence**: Translation through English eliminates the few-shot advantage
- **Medium Confidence**: Achieving state-of-the-art results on the WebNLG 2023 shared task

## Next Checks

1. **Replication Test**: Run the four system variants (zero-shot direct, few-shot direct, zero-shot via English, few-shot via English) on a held-out subset of the WebNLG data to verify the consistency of observed differences

2. **Translation Quality Assessment**: Evaluate Google Translate output quality for target languages independently to determine whether translation quality supports the pivoting-through-English approach

3. **Prompt Engineering Analysis**: Systematically vary examples in few-shot prompts to identify whether specific example types drive performance improvements and test whether few-shot advantage persists with different example sets