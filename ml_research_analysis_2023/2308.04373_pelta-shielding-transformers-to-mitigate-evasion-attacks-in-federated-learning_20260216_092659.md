---
ver: rpa2
title: 'Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated Learning'
arxiv_id: '2308.04373'
source_url: https://arxiv.org/abs/2308.04373
tags:
- adversarial
- learning
- attacks
- pelta
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pelta is a novel defense mechanism designed to mitigate evasion
  attacks in federated learning systems. It leverages trusted hardware, specifically
  Trusted Execution Environments (TEEs), to mask gradients during the backpropagation
  process, preventing attackers from crafting adversarial examples.
---

# Pelta: Shielding Transformers to Mitigate Evasion Attacks in Federated Learning

## Quick Facts
- arXiv ID: 2308.04373
- Source URL: https://arxiv.org/abs/2308.04373
- Reference count: 40
- Primary result: Achieves 98.8% robust accuracy against Self-Attention Gradient Attack (SAGA) using TEE-based gradient masking

## Executive Summary
Pelta is a novel defense mechanism designed to mitigate gradient-based evasion attacks in federated learning systems. It leverages Trusted Execution Environments (TEEs) to mask gradients during the backpropagation process, preventing attackers from crafting adversarial examples. By obfuscating key parameters and operations within the model, Pelta disrupts the chain rule necessary for gradient-based attacks, effectively reducing their success rate. Evaluated on a state-of-the-art ensemble model combining Vision Transformer and Big Transfer architectures, Pelta demonstrated a significant improvement in robust accuracy (98.8%) against the Self-Attention Gradient Attack, showcasing its effectiveness in enhancing model security while maintaining performance.

## Method Summary
Pelta defends against gradient-based evasion attacks by leveraging TEEs to mask Jacobian matrices that link inputs to the first trainable layer. This selective shielding obfuscates part of the backpropagation chain rule, preventing attackers from computing full input gradients needed for adversarial perturbations. The defense focuses on initial layers to balance security with TEE memory constraints, using TrustZone enclaves to store local Jacobians and intermediate activations. This approach disrupts gradient-sign attacks like SAGA by forcing attackers to use unreliable upsampled gradients, significantly reducing attack success while maintaining clean model accuracy.

## Key Results
- Achieves 98.8% robust accuracy against Self-Attention Gradient Attack (SAGA)
- Successfully mitigates evasion attacks in federated learning while maintaining model performance
- Demonstrates effective gradient masking through TEE-based defense mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pelta disrupts gradient-based evasion attacks by masking the chain rule factors between the input and the first trainable layer.
- Mechanism: Trusted Execution Environments (TEEs) store the local Jacobian matrices that link the input to the first transformation. Without access to these Jacobians, attackers cannot compute the full gradient needed for adversarial perturbations.
- Core assumption: The masked gradients are essential for the attacker to compute the adversarial update; partial gradients are insufficient.
- Evidence anchors:
  - [abstract]: "Pelta masks part of the back-propagation chain rule, otherwise typically exploited by attackers for the design of malicious samples."
  - [section 4.2]: "masking only an intermediate layer is useless... we thus always perform the gradient obfuscation of the first trainable parameters of the model"
  - [corpus]: Weak evidence; corpus papers discuss TEE-based defenses but do not detail this specific Jacobian masking approach.
- Break condition: If an attacker can approximate or reconstruct the missing Jacobians through other means (e.g., training a BPDA approximation), the attack may succeed.

### Mechanism 2
- Claim: Pelta's selective shielding balances security and TEE memory constraints by obfuscating only a small fraction of the model.
- Mechanism: Only the initial layers (up to position embedding for ViT and first convolution for BiT) are shielded, consuming minimal TEE memory (~28.96 MB) while still protecting the input-gradient path.
- Core assumption: Adversaries cannot bypass the shielded layers without the full model context; partial shielding suffices to disrupt common attack pipelines.
- Evidence anchors:
  - [section 5.1]: "Table 1 reports the estimated overheads of the shield... consistent with what typical TrustZone-enabled devices allow."
  - [section 4.2]: "Given the current limitations on the size of encrypted memory... it is currently unfeasible to completely shield models such as VGG-16 or larger."
  - [corpus]: Weak evidence; TEE memory constraints are common knowledge but not specifically addressed for gradient masking.
- Break condition: If the attacker can reconstruct the input-gradient path from the non-shielded portions or if TEE memory becomes abundant, full model shielding may become viable.

### Mechanism 3
- Claim: Pelta's shielding is effective even against sophisticated gradient-sign attacks like SAGA by preventing the attacker from using correct input gradients.
- Mechanism: The attacker's SAGA update (Eq. 2, 3) relies on correct input gradients; without access to the shielded Jacobians, the attacker must use an unreliable upsampling of the under-factored gradient, reducing attack success.
- Evidence anchors:
  - [section 5.2]: "Facing the Pelta shielded setting, the attacker carries out the SAGA without the masked set... It tries to exploit the adjoint... by applying to it a random-uniform initialized upsampling kernel."
  - [section 5.3]: "We observe that the shielding greatly preserves the astuteness... with 98.8% robust accuracy (1.2% attack success rate)."
  - [corpus]: Weak evidence; SAGA and its variants are not explicitly mentioned in corpus neighbors.
- Break condition: If the attacker can train a BPDA approximation of the shielded layers or exploit spatial information in the adjoint, the attack may partially succeed.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: Understanding the distributed training context and privacy goals is essential to grasp why evasion attacks are a threat and why local shielding is valuable.
  - Quick check question: In FL, why does sending model updates instead of raw data preserve privacy?

- Concept: Adversarial Examples and Gradient-Based Attacks
  - Why needed here: Pelta's defense specifically targets gradient-based evasion attacks; knowing how these attacks compute perturbations is key to understanding the defense.
  - Quick check question: How does the sign of the gradient guide the creation of an adversarial example?

- Concept: Trusted Execution Environments (TEEs) and Chain Rule
  - Why needed here: Pelta leverages TEEs to hide Jacobian matrices; understanding TEE guarantees and the chain rule in back-propagation is crucial for grasping the mechanism.
  - Quick check question: Why is masking only intermediate gradients insufficient to stop an attacker from computing adversarial updates?

## Architecture Onboarding

- Component map: Input → First trainable layer(s) → Remaining model layers
- Critical path:
  1. Input is processed by shielded layers inside TEE
  2. TEE computes forward pass and stores necessary intermediates
  3. Backward pass gradients from non-shielded layers are used, but missing Jacobians prevent full input-gradient computation
- Design tradeoffs:
  - Shielding more layers increases security but consumes more TEE memory
  - Shielding fewer layers reduces memory usage but may leave input-gradient path partially exposed
  - TEE enclave communication overhead vs. computational overhead of unshielded layers
- Failure signatures:
  - High TEE memory usage indicating over-shielding
  - Attack success rate increases if attacker can approximate shielded Jacobians
  - Degradation in clean model accuracy due to shielding interference
- First 3 experiments:
  1. Measure clean accuracy and memory usage with only ViT shielded
  2. Measure clean accuracy and memory usage with only BiT shielded
  3. Compare attack success rate (SAGA) for each shielding configuration vs. no shielding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Pelta's performance scale with increasing model complexity, and are there diminishing returns for shielding larger models?
- Basis in paper: [explicit] The paper mentions that TrustZone enclaves have limited memory (up to 30 MB), making it challenging to completely shield state-of-the-art Transformer architectures often larger than 500 MB. It also notes that shielding only a subset of layers is necessary due to this constraint.
- Why unresolved: The paper only evaluates Pelta on an ensemble model and does not explore its effectiveness on larger or more complex models. The memory limitations of TEEs are acknowledged but not thoroughly investigated in terms of their impact on performance.
- What evidence would resolve it: Evaluating Pelta on a range of models with varying complexities and sizes, including those exceeding the memory constraints of typical TEEs, would provide insights into its scalability and performance.

### Open Question 2
- Question: What is the impact of Pelta on the practical performance of federated learning, particularly in terms of speed and communication efficiency?
- Basis in paper: [explicit] The paper mentions that using enclaves calls for costly normal-world to secure-world communication mechanisms and that evaluating the speed of each collaborating device under the shielding scheme is needed.
- Why unresolved: The paper focuses on the security aspects of Pelta and does not address its impact on the practical aspects of federated learning, such as training speed or communication overhead.
- What evidence would resolve it: Conducting experiments to measure the impact of Pelta on the speed and communication efficiency of federated learning systems would provide a more comprehensive understanding of its practical implications.

### Open Question 3
- Question: How effective is Pelta against other types of adversarial attacks beyond the Self-Attention Gradient Attack, such as Carlini & Wagner attacks?
- Basis in paper: [explicit] The paper mentions that Pelta can be extended to other gradient masking algorithms and suggests applying it against other popular attacks like Carlini & Wagner.
- Why unresolved: The paper only evaluates Pelta against one specific type of attack, leaving its effectiveness against other common adversarial attacks unexplored.
- What evidence would resolve it: Testing Pelta against a diverse set of adversarial attacks, including those not based on gradient-sign methods, would determine its robustness and versatility in defending against various attack strategies.

## Limitations

- TEE memory constraints limit the extent of model shielding, requiring selective obfuscation of only initial layers
- Evaluation focuses solely on one attack type (SAGA), leaving effectiveness against other gradient-based attacks unexplored
- Assumes attackers cannot reconstruct missing Jacobians through advanced techniques like BPDA approximations

## Confidence

- **High confidence**: The TEE-based gradient masking approach is technically sound and aligns with established trusted computing principles. The memory usage estimates and shielding methodology are clearly specified and reproducible.
- **Medium confidence**: The claim that masking first-layer Jacobians is sufficient to prevent adversarial gradient computation, as this depends on attacker capabilities not fully explored in the paper. The robustness metrics are impressive but may not generalize to other attack types.
- **Low confidence**: The assertion that this approach scales effectively to larger models beyond the tested ensemble, given the severe TEE memory limitations mentioned.

## Next Checks

1. **BPDA attack validation**: Implement a Backward Pass Differentiable Approximation attack to test whether the Pelta defense can be bypassed by training a differentiable approximation of the shielded layers, measuring any degradation in robust accuracy.

2. **Multi-attack evaluation**: Test the defended ensemble against multiple gradient-based attacks including PGD, AutoAttack, and other state-of-the-art methods to verify that the 98.8% robust accuracy is not specific to the SAGA attack.

3. **Adaptive attacker simulation**: Design an adaptive attacker that uses partial gradient information from non-shielded layers combined with architectural knowledge to reconstruct input gradients, measuring the success rate against this more sophisticated threat model.