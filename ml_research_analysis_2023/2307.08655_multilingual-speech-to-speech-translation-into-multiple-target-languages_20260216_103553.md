---
ver: rpa2
title: Multilingual Speech-to-Speech Translation into Multiple Target Languages
arxiv_id: '2307.08655'
source_url: https://arxiv.org/abs/2307.08655
tags:
- speech
- multilingual
- language
- units
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first multilingual speech-to-speech translation
  (S2ST) system that supports 16 target languages. The key innovations include a speech-to-masked-unit
  (S2MU) model that masks units not belonging to the target language to reduce interference,
  and a multilingual vocoder with language embedding and auxiliary language identification
  loss.
---

# Multilingual Speech-to-Speech Translation into Multiple Target Languages

## Quick Facts
- arXiv ID: 2307.08655
- Source URL: https://arxiv.org/abs/2307.08655
- Reference count: 0
- First multilingual S2ST system supporting 16 target languages with +5.2 BLEU improvement over bilingual models

## Executive Summary
This paper introduces the first multilingual speech-to-speech translation (S2ST) system supporting 16 target languages. The key innovations include a speech-to-masked-unit (S2MU) model that masks units not belonging to the target language to reduce interference, and a multilingual vocoder with language embedding and auxiliary language identification loss. On benchmark testsets, the proposed multilingual models achieve an average of +5.2 and +2.7 BLEU improvement over bilingual models on in-domain and out-of-domain data respectively when translating from English into 16 target languages.

## Method Summary
The approach uses discrete units from HuBERT models as an intermediate representation between source and target speech. Languages are grouped into families (Germanic, Romance, Slavic, Uralic) with shared unit vocabularies within each family. The S2MU model masks non-target language units during training to reduce interference, while the multilingual vocoder incorporates language embeddings and auxiliary LID loss to ensure proper language generation. Models are trained on SpeechMatrix corpus with evaluation using ASR-BLEU scores.

## Key Results
- Multilingual S2MU achieves +5.2 BLEU improvement over bilingual S2U on in-domain testsets
- Multilingual S2MU achieves +2.7 BLEU improvement over bilingual S2U on out-of-domain FLEURS testsets
- Multilingual vocoder with language embedding outperforms bilingual vocoders on speech resynthesis

## Why This Works (Mechanism)

### Mechanism 1
Masking units from irrelevant languages during training forces the model to focus on target-language units, reducing interference. The model computes loss only over units belonging to the current target language, and during inference, non-target likelihoods are set to negative infinity.

### Mechanism 2
Adding language embedding and auxiliary LID loss to the multilingual vocoder mitigates language interference during speech synthesis. The language embedding conditions the vocoder on the desired language, while the LID classifier encourages generated speech to be correctly classified as the target language.

### Mechanism 3
Sharing unit vocabularies across languages within the same family reduces total vocabulary size and encourages cross-lingual transfer. Each language family has its own unit dictionary, with languages within the same family sharing units due to similar pronunciations.

## Foundational Learning

- Concept: Discrete units from self-supervised speech models (HuBERT)
  - Why needed here: Enables translation without relying on text, crucial for languages without standard writing systems
  - Quick check question: What is the purpose of using discrete units from HuBERT in the S2ST pipeline, and how do they enable translation for languages without standard writing systems?

- Concept: Multilingual modeling and language interference
  - Why needed here: Scaling to multiple target languages introduces language interference challenges
  - Quick check question: What is language interference in multilingual models, and how do the proposed techniques (masking and language embedding) aim to address this issue in the S2ST context?

- Concept: Speech-to-unit (S2U) models and vocoders
  - Why needed here: Forms the core architecture for translating source speech to target units and synthesizing target speech
  - Quick check question: How do S2U models and vocoders work together in the S2ST pipeline, and what are the key challenges in extending them to support multiple target languages?

## Architecture Onboarding

- Component map: Speech encoder -> S2MU decoder (with masking) -> Target units -> Multilingual vocoder (with language embedding and LID loss) -> Generated target speech -> ASR transcription -> BLEU score

- Critical path: Source speech → Speech encoder → S2MU decoder (with masking) → Target units → Multilingual vocoder (with language embedding and LID loss) → Generated target speech → ASR transcription → BLEU score

- Design tradeoffs: Vocabulary size vs. cross-lingual transfer; Model capacity vs. data efficiency for low-resource languages

- Failure signatures: Low BLEU scores indicate S2MU/vocoder issues or ASR accuracy problems; Speech sounding like wrong language suggests ineffective language embedding/LID

- First 3 experiments:
  1. Train bilingual S2U baseline on high-resource pair (e.g., English-Spanish)
  2. Implement S2MU with masking and compare performance to bilingual baseline
  3. Train multilingual vocoder with language embedding and evaluate on target languages

## Open Questions the Paper Calls Out

### Open Question 1
How does performance compare between family-specific units and a truly shared unit vocabulary across all 16 languages? The paper suggests this could better support knowledge transfer but doesn't experiment with it.

### Open Question 2
What is the impact of different data sampling strategies on multilingual S2MU performance, particularly for low-resource languages? The paper mentions this as future work for dealing with imbalanced training data.

### Open Question 3
How would performance change with a universal speech feature extraction model supporting all languages instead of family-specific HuBERT models? The paper suggests this could enable single vocabulary usage.

## Limitations
- Family-based unit sharing may not generalize well to phonetically dissimilar languages within families
- Evaluation relies heavily on ASR-BLEU scores which may not capture speech naturalness
- No systematic analysis of computational efficiency differences between multilingual and bilingual models

## Confidence
- High Confidence: BLEU score improvements are well-supported by experimental results
- Medium Confidence: Mechanism explanations for masking and language embedding are plausible but not fully validated
- Low Confidence: Claims about being first system or scalability to more languages are speculative

## Next Checks
1. Conduct t-SNE/UMAP visualization of discrete units across all target languages to validate language-specific clustering
2. Systematically evaluate S2MU performance on low-resource languages with and without related high-resource language data
3. Test multilingual vocoder on code-switched speech to quantify language interference prevention effectiveness