---
ver: rpa2
title: Creating Ensembles of Classifiers through UMDA for Aerial Scene Classification
arxiv_id: '2303.11389'
source_url: https://arxiv.org/abs/2303.11389
tags:
- classi
- learning
- cation
- deep
- remote
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates the performance of six deep metric learning\
  \ (DML) approaches\u2014Contrastive, Triplet, ProxyAnchor, NNGK, SoftTriple, and\
  \ SupCon\u2014combined with four pre-trained CNN architectures (ResNet18, ResNet50,\
  \ VGG16, VGG19) for aerial scene classification. The authors analyze the classification\
  \ performance across three aerial scene datasets (AID, UCMerdes, and RESISC45) and\
  \ evaluate the diversity among the resulting classifiers."
---

# Creating Ensembles of Classifiers through UMDA for Aerial Scene Classification

## Quick Facts
- **arXiv ID:** 2303.11389
- **Source URL:** https://arxiv.org/abs/2303.11389
- **Reference count:** 40
- **One-line primary result:** UMDA ensemble strategy improves aerial scene classification accuracy by 5.6% to 9.94% over best individual DML+DLA combinations using ~50% fewer classifiers.

## Executive Summary
This paper investigates the use of deep metric learning (DML) approaches combined with pre-trained CNNs for aerial scene classification. The authors evaluate six DML methods (Contrastive, Triplet, ProxyAnchor, NNGK, SoftTriple, SupCon) with four CNN architectures (ResNet18, ResNet50, VGG16, VGG19) across three aerial scene datasets (AID, UCMerdes, RESISC45). The study demonstrates that DML approaches can outperform traditional pre-trained CNNs, with optimal combinations varying by dataset. To further enhance performance, the authors employ ensemble methods, finding that the Univariate Marginal Distribution Algorithm (UMDA) achieves the best results while using approximately 50% fewer classifiers than majority voting.

## Method Summary
The study follows a systematic approach: first, 24 individual classifiers are trained by combining six DML approaches with four pre-trained CNN architectures using 5-fold cross-validation on three aerial scene datasets. Diversity among these classifiers is measured using correlation coefficients. Two ensemble strategies are then evaluated: majority voting (MV) and UMDA-based selection. The UMDA algorithm models the probability distribution of classifier performance on validation sets and iteratively samples ensembles, selecting the fittest combinations. The best individual DML+DLA combinations are identified for each dataset, and ensemble performance is compared against these baselines and traditional pre-trained CNNs.

## Key Results
- SupCon+ResNet18 achieved 87.37% accuracy for AID dataset, ProxyAnchor+VGG19 achieved 77.70% for UCMerdes, and NNGK+ResNet50 achieved 91.60% for RESISC45
- UMDA ensembles improved accuracy by 5.6% to 9.94% over the best individual DML+DLA combinations
- UMDA used approximately 50% fewer classifiers than majority voting while achieving better performance
- DML approaches outperformed traditional pre-trained CNNs by 7.77% to 21.87% in classification accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DML approaches outperform traditional pre-trained CNNs for aerial scene classification.
- Mechanism: DML explicitly learns a metric space that pulls same-class examples closer and pushes different-class examples apart, whereas standard CNNs rely on classification boundaries learned through cross-entropy. This metric optimization aligns better with the high intra-class variability and scale/orientation differences inherent in aerial imagery.
- Core assumption: The transformed metric space learned by DML better captures the semantic similarity structure in remote sensing data than the original feature space used by standard CNNs.
- Evidence anchors:
  - [abstract] "In performed experiments, it is possible to observe than DML approaches can achieve the best classification results when compared to traditional pre-trained CNNs for three well-known remote sensing aerial scene datasets."
  - [section] "As the best combinations (in gray) are SupCon +ResNet18 (87.37%) for AID dataset, ProxyAnchor +VGG19 (77.70%) for UCMerdes dataset, and NNGK +Resnet50 (91.60%) for RESISC45 dataset. In addition, DLA can directly influence the performance of DML in different datasets."
  - [corpus] Weak evidence - corpus contains related works on CNN-based approaches but lacks direct comparative DML results.
- Break condition: If the DML loss function fails to converge or overfits to the training set without generalizing to new samples, performance degrades below traditional CNN baselines.

### Mechanism 2
- Claim: UMDA-based ensemble selection improves accuracy by ~5-9% over the best individual DML+DLA models while using ~50% fewer classifiers.
- Mechanism: UMDA models the probability distribution of classifier performance on a validation set and iteratively samples ensembles, retaining only the fittest. This exploits complementary strengths among diverse classifiers, whereas simple majority voting includes all classifiers regardless of their individual quality or complementarity.
- Core assumption: The validation set is representative of the test set and captures the diversity among classifiers so that UMDA can identify truly complementary models.
- Evidence anchors:
  - [abstract] "The UMDA strategy achieves the best results, improving accuracy by 5.6% to 9.94% over the best individual DML+DLA combinations and by 7.77% to 21.87% over pre-trained CNNs, using approximately 50% fewer classifiers in the ensemble."
  - [section] "Table IV shows the classification results and standard deviation of the two classifier fusion strategies. We also present the number of classifiers used by each strategy to predict the samples from the test set. Furthermore, we also included the best tuple (DML +DLA) from the previous experiments for comparison purposes."
  - [corpus] Weak evidence - corpus does not provide direct support for UMDA in this specific context.
- Break condition: If the validation set distribution shifts or if classifier diversity is low, UMDA cannot effectively select complementary models, leading to minimal or negative gains.

### Mechanism 3
- Claim: Diversity among classifiers (measured by low correlation coefficient) is a prerequisite for successful ensemble performance.
- Mechanism: High diversity ensures that individual classifiers make different types of errors, which ensemble methods like UMDA can exploit to reduce overall error rates. Low diversity (high correlation) means classifiers make similar errors, limiting ensemble improvement.
- Core assumption: Diversity measured on the validation set correlates with complementary error patterns on the test set.
- Evidence anchors:
  - [section] "Among many diversity measures studied by Kuncheva et al. [48], we can define the Correlation Coefficient (ρ) measure as follows: ρ(ci,cj) = ad−bc/√[(a+b)(c+d)(a+c)(b+d)]... Diversity is higher if the score of the Correlation Coefficient p is lower between a pair of classifiers ci and cj."
  - [section] "In Figure 3, it is possible to observe that the VGG-based classifiers are less correlated with the ResNet ones, thus they can be present together in an ensemble of classifiers."
  - [corpus] Weak evidence - corpus contains related diversity discussions but lacks direct experimental support in this work.
- Break condition: If all classifiers converge to similar decision boundaries due to similar architectures or training data, diversity measures become meaningless and ensemble gains vanish.

## Foundational Learning

- Concept: Deep Metric Learning (DML)
  - Why needed here: DML directly optimizes inter-class separation and intra-class compactness, which is critical for aerial scenes with high variability.
  - Quick check question: What is the key difference between DML loss functions (e.g., Triplet, ProxyAnchor) and standard cross-entropy loss?

- Concept: Ensemble Diversity Measurement
  - Why needed here: Ensures that selected classifiers contribute complementary information rather than redundant predictions.
  - Quick check question: How does the Correlation Coefficient (ρ) between two classifiers relate to their diversity?

- Concept: Estimation of Distribution Algorithms (EDAs) like UMDA
  - Why needed here: UMDA efficiently searches the space of classifier subsets by modeling the probability distribution of good solutions, avoiding exhaustive enumeration.
  - Quick check question: What is the main difference between UMDA and traditional genetic algorithms in terms of solution representation?

## Architecture Onboarding

- Component map:
  - Data pipeline: Three aerial scene datasets (AID, UCMerced, RESISC45) with 5-fold cross-validation
  - Base models: 6 DML approaches × 4 CNN backbones = 24 individual classifiers
  - Diversity analysis: Correlation coefficient matrix computed on validation folds
  - Ensemble strategies: Majority voting (MV) and UMDA-based selection
  - Evaluation: Accuracy, standard deviation, relative gains vs. baselines

- Critical path:
  1. Train all 24 DML+DLA classifiers on training folds
  2. Compute diversity matrix on validation folds
  3. Run UMDA on validation folds to select optimal subset
  4. Evaluate selected ensemble on test folds
  5. Compare against best individual DML+DLA and PT-CNN baselines

- Design tradeoffs:
  - UMDA vs. MV: UMDA uses fewer classifiers (~50%) but requires diversity analysis; MV is simpler but potentially suboptimal
  - DML vs. standard CNN: DML may overfit if margin/scale parameters not tuned; standard CNN more stable but potentially less discriminative

- Failure signatures:
  - Low diversity (ρ close to 1) across all classifier pairs → ensemble gains minimal
  - Validation/test distribution shift → UMDA selects suboptimal classifiers
  - DML loss divergence → individual classifier performance collapses

- First 3 experiments:
  1. Train all 24 DML+DLA models and compute diversity matrix on one fold; verify expected patterns (e.g., VGG less correlated with ResNet)
  2. Run UMDA selection on the same fold; compare selected subset size and diversity vs. MV
  3. Evaluate both ensemble strategies on test fold; measure accuracy gain over best individual model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different DML loss functions (Contrastive, Triplet, ProxyAnchor, SoftTriple, SupCon, NNGK) perform when combined with different CNN architectures across aerial scene datasets?
- Basis in paper: [explicit] The paper systematically evaluates six DML approaches with four CNN architectures (ResNet18, ResNet50, VGG16, VGG19) across three aerial scene datasets (AID, UCMerced, RESISC45), showing that performance varies significantly based on the specific combination.
- Why unresolved: While the paper identifies optimal combinations for each dataset, it doesn't provide a theoretical framework explaining why certain DML approaches work better with specific CNN architectures for aerial scene classification.
- What evidence would resolve it: Systematic ablation studies varying architectural parameters (depth, width, attention mechanisms) and DML hyperparameters across datasets, coupled with interpretability analysis of learned embeddings.

### Open Question 2
- Question: What is the minimum number of diverse classifiers needed to achieve near-optimal ensemble performance in aerial scene classification?
- Basis in paper: [explicit] The paper shows that UMDA achieves significant improvements using only ~50% of available classifiers compared to using all 24 classifiers, but doesn't determine the optimal minimum number.
- Why unresolved: The study only compares two ensemble strategies (MV using all classifiers vs. UMDA using selected classifiers) without systematically exploring the trade-off between ensemble size and performance.
- What evidence would resolve it: Systematic experiments varying ensemble size from 1 to 24 classifiers, measuring performance gains per added classifier and identifying the point of diminishing returns.

### Open Question 3
- Question: How does the diversity measure used in the paper relate to actual ensemble performance gains in aerial scene classification?
- Basis in paper: [explicit] The paper uses correlation coefficient as a diversity measure and observes that lower correlation generally corresponds to better ensemble performance, but doesn't establish a quantitative relationship.
- Why unresolved: The study shows correlation between diversity scores and performance trends but doesn't prove causation or determine if correlation coefficient is the best diversity metric for this application.
- What evidence would resolve it: Comparative analysis of multiple diversity measures (Q-statistics, disagreement measure, Kohavi-Wolpert variance) against ensemble performance across different datasets and classification tasks.

## Limitations
- The experimental results depend on the quality and representativeness of the validation set for UMDA selection, which is not thoroughly validated
- The comparison with traditional pre-trained CNNs lacks statistical significance testing, making it unclear whether improvements are meaningful
- The diversity analysis relies on correlation coefficients computed on validation folds, which may not fully capture test-time complementarity

## Confidence

- **High confidence**: The general trend that DML approaches can outperform standard CNNs in aerial scene classification is supported by multiple experimental results across three datasets.
- **Medium confidence**: The specific accuracy values reported for individual DML+DLA combinations, as they depend on precise implementation details and random initialization.
- **Low confidence**: The exact magnitude of improvement claimed for UMDA ensembles, as this depends on the quality of the diversity analysis and the representativeness of the validation set.

## Next Checks

1. Compute and report the correlation coefficient matrix for all 24 classifier pairs on a held-out validation fold to verify the claimed diversity patterns (e.g., VGG-based classifiers being less correlated with ResNet-based ones).
2. Perform statistical significance testing (e.g., paired t-tests) between the best DML+DLA combinations and the pre-trained CNN baselines to confirm whether improvements are statistically meaningful.
3. Test UMDA ensemble performance on a separate test set with a different distribution from the validation set to assess robustness to distribution shift.