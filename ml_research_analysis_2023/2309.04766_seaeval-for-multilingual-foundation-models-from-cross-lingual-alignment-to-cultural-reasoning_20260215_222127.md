---
ver: rpa2
title: 'SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to
  Cultural Reasoning'
arxiv_id: '2309.04766'
source_url: https://arxiv.org/abs/2309.04766
tags:
- language
- multilingual
- languages
- accuracy
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces SeaEval, a benchmark for evaluating multilingual
  foundation models across four key dimensions: classic NLP tasks, complex reasoning,
  cultural comprehension, and cross-lingual knowledge transfer. It addresses the need
  for comprehensive evaluation of multilingual models beyond traditional monolingual
  benchmarks.'
---

# SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment to Cultural Reasoning

## Quick Facts
- arXiv ID: 2309.04766
- Source URL: https://arxiv.org/abs/2309.04766
- Authors: 
- Reference count: 29
- Key outcome: Introduces SeaEval benchmark evaluating multilingual foundation models across classic NLP, reasoning, cultural comprehension, and cross-lingual transfer dimensions using 28 datasets including 6 newly created ones

## Executive Summary
SeaEval addresses the critical need for comprehensive evaluation of multilingual foundation models beyond traditional monolingual benchmarks. The benchmark introduces 6 new datasets specifically designed to assess cross-lingual consistency and cultural reasoning capabilities across English, Chinese, and Indonesian. Through systematic evaluation of multiple state-of-the-art models including LLaMA-2-70B, Baichuan-13B-Chat, ChatGPT, and GPT-4, SeaEval reveals fundamental limitations in current multilingual models including instruction sensitivity, exposure bias from label arrangements, and inconsistent cross-lingual performance. The results underscore the urgent need for more generalizable semantic representations and enhanced multilingual contextualization in foundation models.

## Method Summary
SeaEval evaluates multilingual foundation models using 28 datasets across four dimensions: classic NLP tasks, complex reasoning, cultural comprehension, and cross-lingual knowledge transfer. The methodology employs five human-paraphrased instructions per dataset to measure instruction sensitivity, randomizes label order in multiple-choice questions to detect exposure bias, and uses AC3 score to assess cross-lingual consistency. New datasets like Cross-MMLU and Cross-LogiQA provide parallel questions across English, Chinese, and Indonesian to test whether models maintain consistent responses to semantically equivalent queries. The benchmark covers languages including English, Chinese, Indonesian, Vietnamese, and Malay, providing a comprehensive framework for assessing multilingual model capabilities.

## Key Results
- Most models exhibit varied behavior when given paraphrased instructions, indicating brittleness in instruction following
- Models show inconsistent performance on semantically equivalent multilingual queries, revealing gaps in cross-lingual knowledge transfer
- Label shuffling protocol identifies exposure bias, with models showing higher accuracy when labels follow sequential arrangements
- No model has achieved balanced multilingual capabilities across all evaluated dimensions and languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SeaEval addresses the evaluation gap for multilingual foundation models by creating datasets that test cross-lingual consistency and cultural reasoning.
- Mechanism: By curating new datasets like Cross-MMLU and Cross-LogiQA with parallel questions across English, Chinese, and Indonesian, SeaEval can measure whether models provide consistent answers to semantically equivalent questions in different languages.
- Core assumption: Semantic knowledge should be language-agnostic for factual, scientific, and commonsense questions.
- Evidence anchors:
  - [abstract] "For questions rooted in factual, scientific, and commonsense knowledge, consistent responses are expected across multilingual queries that are semantically equivalent."
  - [section 3.2] "Cross-MMLU and Cross-LogiQA are originated from MMLU dataset and LogiQA2.0 dataset, respectively."
- Break condition: If models show inconsistent performance due to cultural context differences rather than semantic knowledge transfer failures.

### Mechanism 2
- Claim: SeaEval's use of paraphrased instructions reveals model brittleness in instruction following across languages.
- Mechanism: By providing 5 human-paraphrased instructions for each dataset and measuring performance variation, SeaEval can quantify how instruction formulation affects model outputs.
- Core assumption: Models should be robust to instruction variations if they truly understand the underlying task.
- Evidence anchors:
  - [abstract] "Most models exhibit varied behavior when given paraphrased instructions."
  - [section 3.3] "We build 5 human paraphrased instructions with NLP experts for each dataset."
- Break condition: If performance variation is primarily due to language-specific instruction formulation rather than general instruction brittleness.

### Mechanism 3
- Claim: SeaEval's label shuffling protocol identifies exposure bias in multilingual models.
- Mechanism: By randomizing label order in multiple-choice questions, SeaEval can detect whether models rely on positional label biases rather than semantic understanding.
- Core assumption: Models should perform consistently regardless of label arrangement if they're reasoning correctly.
- Evidence anchors:
  - [section 3.3] "we shuffle all labels whenever possible to avoid exposure biases on label positions."
  - [section 4] "We observe that some models are prone to rely on the intrinsic biases stemming from the sequential arrangement of labels."
- Break condition: If label shuffling affects multilingual models differently than monolingual models, suggesting language-specific positional encoding effects.

## Foundational Learning

- Concept: Cross-lingual knowledge transfer
  - Why needed here: SeaEval evaluates whether models can transfer knowledge across languages, which is fundamental to multilingual model capability
  - Quick check question: Why would a model give different answers to the same factual question in different languages?

- Concept: Cultural reasoning in language models
  - Why needed here: SeaEval includes cultural comprehension datasets that require understanding language-specific cultural contexts
  - Quick check question: How does cultural context affect the interpretation of language-specific idioms or expressions?

- Concept: Instruction sensitivity and prompt engineering
  - Why needed here: SeaEval's paraphrased instruction protocol tests model robustness to instruction variations
  - Quick check question: Why might the same task yield different results when phrased slightly differently?

## Architecture Onboarding

- Component map: Data curation → Instruction paraphrasing → Label shuffling → Cross-lingual consistency evaluation → AC3 score computation
- Critical path: SeaEval's evaluation pipeline requires running each model on 28 datasets using 5 paraphrased instructions, with label shuffling for multiple-choice tasks and AC3 calculation for cross-lingual datasets
- Design tradeoffs: SeaEval prioritizes comprehensive evaluation over model efficiency, requiring multiple instructions and label shuffles per dataset
- Failure signatures: Inconsistent cross-lingual performance, high sensitivity to instruction paraphrasing, positional label bias exposure
- First 3 experiments:
  1. Run Cross-MMLU with LLaMA-2-70B and ChatGPT using 5 different instruction templates, measure AC3 score variation
  2. Shuffle labels in SST-2 and SG-Eval, compare performance drops to identify exposure bias
  3. Test Cross-LogiQA cross-lingual consistency across English, Chinese, and Indonesian for GPT-4 vs LLaMA-2-70B

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different multilingual training strategies on cross-lingual consistency and knowledge transfer performance?
- Basis in paper: [inferred] The paper mentions that multilingual models can be trained through various methods like pre-training from scratch, vocabulary expansion, or aligning multilingual instructions, but does not provide empirical comparisons of these approaches.
- Why unresolved: The authors tested different multilingual models but did not systematically compare the effectiveness of different training strategies on cross-lingual consistency metrics.
- What evidence would resolve it: Direct comparison of cross-lingual consistency scores across models trained with different strategies (e.g., XGLM vs BLOOM vs ChatGLM) on the Cross-MMLU and Cross-LogiQA datasets.

### Open Question 2
- Question: How does instruction sensitivity vary across different language families and linguistic structures?
- Basis in paper: [explicit] "Since FMs do not attain 'balanced multilingual' capabilities, they are more sensitive to input variations such as cross-lingual instructions and code-switching under real-world scenarios."
- Why unresolved: While the paper tested instruction sensitivity with paraphrased instructions in English, it did not examine whether sensitivity varies systematically across different language families or typological features.
- What evidence would resolve it: Comparative analysis of instruction sensitivity (performance variance across paraphrased instructions) across different language families (e.g., Indo-European vs Sino-Tibetan vs Austronesian languages).

### Open Question 3
- Question: What are the underlying causes of exposure bias in label arrangements for multilingual models?
- Basis in paper: [explicit] "We observe that some models are prone to rely on the intrinsic biases stemming from the sequential arrangement of labels, which results in plausible higher accuracy without label shuffling."
- Why unresolved: The paper identifies the phenomenon but does not investigate whether this bias is more pronounced in certain languages, model architectures, or training approaches.
- What evidence would resolve it: Analysis of label arrangement bias across different languages and model types, including investigation of whether this bias correlates with specific linguistic features or model design choices.

## Limitations

- The benchmark's generalizability beyond tested language pairs (English, Chinese, Indonesian) remains uncertain
- Cultural reasoning evaluation may conflate legitimate cultural nuance with model errors
- Limited investigation into the underlying causes of exposure bias and instruction sensitivity

## Confidence

- Cross-lingual consistency evaluation: High confidence based on systematic experimental design
- Instruction sensitivity findings: High confidence supported by empirical evidence across multiple models
- Cultural reasoning assessment: Medium confidence due to ambiguity in interpreting cultural variations
- Generalizability to other language families: Medium confidence requiring further validation

## Next Checks

1. Test Cross-MMLU and Cross-LogiQA datasets with additional language pairs beyond the three core languages to verify whether observed consistency patterns generalize across language families.

2. Systematically vary the degree of instruction paraphrasing (slight vs. substantial changes) to determine whether current results reflect fundamental model limitations or sensitivity to specific types of instruction variation.

3. Conduct human evaluation studies on the SST-2 and SG-Eval datasets to distinguish between legitimate cultural variation in responses versus model errors, particularly for low-resource languages where cultural context may be less well-represented in training data.