---
ver: rpa2
title: 'Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage
  Approach to Mitigate Social Biases'
arxiv_id: '2307.01595'
source_url: https://arxiv.org/abs/2307.01595
tags:
- ccpa
- language
- debiasing
- training
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of mitigating social biases in
  pre-trained language models (PLMs) by proposing a two-stage approach called Contrastive
  Learning with Continuous Prompt Augmentation (CCPA). The core method idea is to
  first use continuous prompt tuning to push the representation distance between sample
  pairs along different demographic groups farther apart, and then use contrastive
  learning to pull the representation distance between the augmented sample pairs
  closer together.
---

# Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases

## Quick Facts
- arXiv ID: 2307.01595
- Source URL: https://arxiv.org/abs/2307.01595
- Authors: 
- Reference count: 11
- Primary result: Outperforms state-of-the-art models on debiasing benchmarks (SEAT, StereoSet, CrowS) while maintaining language modeling capability

## Executive Summary
This paper introduces Contrastive Learning with Continuous Prompt Augmentation (CCPA), a two-stage approach to mitigate social biases in pre-trained language models. The method first uses continuous prompt tuning to amplify representational distance between biased sample pairs, then applies contrastive learning to pull these pairs closer together, effectively reducing demographic bias. CCPA demonstrates superior debiasisng performance compared to existing methods while preserving language modeling capability on standard benchmarks.

## Method Summary
CCPA operates in two stages on pre-processed counterfactual data pairs. Stage 1 employs continuous prompt tuning with a generator that creates embeddings to amplify bias-encoded representation distances, regularized by Mahalanobis distance to maintain semantic coherence. Stage 2 fixes these prompt embeddings and fine-tunes the PLM using contrastive learning loss to pull representations of different demographic groups together, supplemented by MLM auxiliary loss to preserve language modeling. The approach targets gender bias specifically using binary attribute words and evaluates on both intrinsic bias metrics and extrinsic task performance.

## Key Results
- Reduces average effect size from 0.621 to 0.249 on intrinsic bias metrics
- Increases StereoSet ICAT from 66.86 to 73.28
- Reduces CrowS-Pairs score from 57.86 to 51.57 on BERT
- Maintains competitive performance on GLUE benchmark tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continuous prompt tuning amplifies representational distance between biased sample pairs by concatenating embeddings that emphasize demographic differences
- Mechanism: By fixing the PLM encoder and only training a prompt generator, the method expands the bias-encoded region in the embedding space, making it easier for the second-stage contrastive learning to identify and then reduce this bias
- Core assumption: Amplifying bias in a controlled way increases the difficulty of fitting, which improves the model's ability to debias when bias is subsequently reduced
- Break condition: If the prompt generator over-expands the representation distance, semantic information degrades, causing the second stage to fail at meaningful bias reduction

### Mechanism 2
- Claim: Contrastive learning pulls the now-far-apart augmented sample pairs closer together, reducing demographic bias while preserving language modeling capability
- Mechanism: By fixing the prompt embeddings and fine-tuning the PLM encoder with contrastive loss, the method encourages representations of different demographic groups to become similar, effectively reducing bias
- Core assumption: Maximizing similarity between augmented counterfactual pairs will eliminate demographic-specific encoding differences without harming general language modeling
- Break condition: If the contrastive learning stage is too aggressive, it may collapse representations across all samples, harming task-specific performance

### Mechanism 3
- Claim: Mahalanobis distance regularization constrains prompt-induced representation shifts to stay within the original semantic distribution
- Mechanism: By penalizing deviation from the original covariance structure, the method ensures that amplified bias does not come at the cost of breaking semantic coherence
- Core assumption: The original training data distribution is a reasonable proxy for valid semantic space, so constraining to it preserves meaning
- Break condition: If the covariance estimate is inaccurate or the constraint is too strict, the model may not amplify bias enough to benefit the second stage

## Foundational Learning

- Concept: Counterfactual Data Augmentation (CDA)
  - Why needed here: CDA is the baseline method for balancing demographic representation in training corpora, which this work builds upon
  - Quick check question: What does CDA do to an input sentence containing the word "he"?

- Concept: Continuous prompt tuning
  - Why needed here: Unlike discrete prompts, continuous prompts allow direct embedding-space manipulation without template constraints, enabling controlled bias amplification
  - Quick check question: How does continuous prompt tuning differ from manual prompt engineering in terms of parameterization?

- Concept: Contrastive learning
  - Why needed here: Contrastive loss is used to pull together representations of different demographic groups after they have been pushed apart, achieving debiasing
  - Quick check question: In contrastive learning, what does maximizing similarity between positive pairs achieve?

## Architecture Onboarding

- Component map: Pre-process CDA pairs → Stage 1: Fixed PLM + trainable prompt generator + Mahalanobis regularization → Stage 2: Fixed prompts + fine-tuned PLM + contrastive loss + MLM auxiliary loss
- Critical path: Pre-process → Stage 1 prompt tuning → Stage 2 contrastive fine-tuning → Evaluation
- Design tradeoffs: Longer prompts increase debiasing but risk semantic degradation; Mahalanobis constraint preserves semantics but may limit bias amplification; MLM auxiliary loss maintains language modeling but adds training complexity
- Failure signatures: Over-amplified bias leading to meaningless representations (Stage 1 failure); insufficient bias reduction despite contrastive training (Stage 2 failure); loss of task performance due to aggressive regularization (either stage)
- First 3 experiments: 1) Run Stage 1 only with Mahalanobis constraint off; observe if representation distance increases without semantic collapse. 2) Run Stage 2 only with pre-augmented pairs; verify contrastive loss reduces bias without harming GLUE scores. 3) Run full CCPA with varying prompt template lengths (1,1,1 vs 2,2,2 vs 3,3,3); measure effect on debiasing and language modeling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the CCPA framework perform when extended to mitigate biases beyond gender, such as race, religion, or intersectional biases?
- Basis in paper: The authors acknowledge that their current work focuses on gender bias and mention plans to extend to other types of social biases in the future
- Why unresolved: The paper only provides experimental results for gender bias mitigation, leaving the effectiveness on other bias types untested
- What evidence would resolve it: Running CCPA on datasets and metrics designed for racial, religious, or intersectional bias, and comparing its performance to current state-of-the-art methods

### Open Question 2
- Question: What is the impact of continuous prompt length on debiasing performance and semantic preservation across different PLM architectures?
- Basis in paper: The ablation study shows that longer prompts increase debiasing ability but may damage semantic information, but this relationship is only tested on BERT
- Why unresolved: The optimal prompt length may vary across different PLM architectures (e.g., BERT, DistilBERT, Electra), and the trade-off between debiasing and semantic preservation needs further investigation
- What evidence would resolve it: Systematic experiments varying prompt length on multiple PLM architectures while measuring both debiasing performance and semantic preservation metrics

### Open Question 3
- Question: How does CCPA perform on natural language generation tasks compared to its performance on natural language understanding tasks?
- Basis in paper: The authors mention in the limitations section that they plan to extend CCPA to NLG models, but no experimental results are provided
- Why unresolved: The paper only evaluates CCPA on NLU tasks (GLUE benchmark and bias-specific datasets), leaving its effectiveness on generation tasks unexplored
- What evidence would resolve it: Applying CCPA to fine-tune PLMs for generation tasks like summarization, translation, or dialogue, and evaluating both the quality of generated text and bias mitigation

### Open Question 4
- Question: What is the computational efficiency trade-off between CCPA's two-stage approach and single-stage debiasing methods?
- Basis in paper: The authors describe CCPA as a two-stage process involving continuous prompt tuning followed by contrastive learning fine-tuning, but do not compare computational costs to baseline methods
- Why unresolved: While CCPA shows superior debiasing performance, the additional computational overhead of the two-stage approach is not quantified or compared to simpler methods
- What evidence would resolve it: Measuring training time, inference latency, and memory usage of CCPA compared to single-stage methods like FairFil, MABEL, or INLP across multiple datasets and hardware configurations

## Limitations

- The core hypothesis that bias amplification facilitates removal lacks direct empirical validation through ablation studies
- The method focuses exclusively on gender bias with binary attribute words, limiting generalizability to intersectional or non-binary attributes
- The template-based prompt augmentation introduces structural constraints whose impact on debiasing efficacy remains unclear

## Confidence

- **High confidence**: The two-stage architecture is technically sound and the implementation details (loss functions, training procedure) are clearly specified
- **Medium confidence**: The debiasing performance improvements are demonstrated on multiple benchmarks, though the ablation study does not isolate the contribution of each stage
- **Low confidence**: The theoretical mechanism explaining why bias amplification facilitates removal is not empirically validated, and the generalizability to other bias types remains untested

## Next Checks

1. **Ablation of Stage 1**: Run Stage 2 alone with original counterfactual pairs (no prompt augmentation) to quantify the contribution of bias amplification to overall debiasing performance
2. **Alternative regularization**: Replace Mahalanobis distance with simpler L2 regularization in Stage 1 to test whether semantic preservation is essential or if constraint choice is arbitrary
3. **Cross-bias validation**: Apply CCPA to racial bias using non-binary attributes (e.g., nationality, ethnicity) to evaluate generalizability beyond gender binary debiasing