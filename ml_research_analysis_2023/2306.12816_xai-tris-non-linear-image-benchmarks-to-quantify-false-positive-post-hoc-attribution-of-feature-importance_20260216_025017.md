---
ver: rpa2
title: 'XAI-TRIS: Non-linear image benchmarks to quantify false positive post-hoc
  attribution of feature importance'
arxiv_id: '2306.12816'
source_url: https://arxiv.org/abs/2306.12816
tags:
- data
- methods
- each
- dataset
- background
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces XAI-TRIS, a new benchmark dataset designed
  to evaluate the performance of explainable AI (XAI) methods in identifying important
  features in non-linear image classification tasks. The dataset consists of 64x64
  pixel images with tetromino patterns (T-shaped and L-shaped) overlaid on different
  background types (white noise, correlated noise, and ImageNet).
---

# XAI-TRIS: Non-linear image benchmarks to quantify false positive post-hoc attribution of feature importance

## Quick Facts
- arXiv ID: 2306.12816
- Source URL: https://arxiv.org/abs/2306.12816
- Reference count: 40
- The paper introduces XAI-TRIS, a new benchmark dataset designed to evaluate the performance of explainable AI (XAI) methods in identifying important features in non-linear image classification tasks.

## Executive Summary
This paper introduces XAI-TRIS, a novel benchmark dataset for evaluating explainable AI (XAI) methods on non-linear image classification tasks. The dataset consists of 64x64 pixel images with tetromino patterns overlaid on different background types, providing explicit ground truth explanations through the location of the tetrominoes. The authors propose two quantitative metrics, Precision and Earth Mover's Distance (EMD), to assess explanation performance. Experiments across three deep learning model architectures and 16 XAI methods reveal that popular methods often fail to significantly outperform random baselines and edge detection methods, especially in the presence of suppressor variables in correlated backgrounds. The study highlights the limitations of current XAI methods and the potential for misinterpretation of model explanations.

## Method Summary
The XAI-TRIS benchmark consists of synthetic 64x64 pixel images with tetromino patterns (T-shaped and L-shaped) on different background types (white noise, correlated noise, and ImageNet). Ground truth explanations are explicitly known through the location of the tetrominoes. Three deep learning model architectures (Linear Logistic Regression, Multi-Layer Perceptron, and Convolutional Neural Network) are trained on the generated datasets. Sixteen XAI methods are applied to explain the classification decisions of the models, and their performance is evaluated using Precision and Earth Mover's Distance (EMD) metrics. The benchmark aims to quantify false positive post-hoc attributions of feature importance in non-linear image classification tasks.

## Key Results
- Popular XAI methods often fail to significantly outperform random baselines and edge detection methods on the XAI-TRIS benchmark.
- The presence of suppressor variables in correlated backgrounds degrades XAI explanation performance.
- Explanations derived from different model architectures can be vastly different, leading to potential misinterpretation even under controlled conditions.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Suppressor variables in correlated backgrounds degrade XAI explanation performance.
- **Mechanism:** When background noise is smoothed (CORR), pixels become statistically correlated with foreground tetromino pixels without being part of the signal. XAI methods attribute importance to these suppressor variables, inflating false positives.
- **Core assumption:** A feature is only important if it has a statistical dependency on the target (Wilming et al. 2022 definition).
- **Evidence anchors:**
  - [abstract] "We show that popular XAI methods are often unable to significantly outperform random performance baselines and edge detection methods. Moreover, we demonstrate that explanations derived from different model architectures can be vastly different; thus, prone to misinterpretation even under controlled conditions."
  - [section] "The difficulty of XAI methods to correctly highlight the truly important features in this setting can be attributed to the emergence of suppressor variables."
  - [corpus] Weak—no direct match; mentions "False-Positive Attributions" but no suppressor discussion.
- **Break condition:** If XAI methods are invariant to background correlation or explicitly remove suppressor influence.

### Mechanism 2
- **Claim:** Model architecture choice affects explanation consistency.
- **Mechanism:** Different architectures learn different internal representations of the same task, leading to divergent feature importance attributions even when test accuracy is comparable.
- **Core assumption:** High test accuracy does not guarantee consistent explanations across architectures.
- **Evidence anchors:**
  - [abstract] "we demonstrate that explanations derived from different model architectures can be vastly different; thus, prone to misinterpretation even under controlled conditions."
  - [section] "Architectures not only differed with respect to the selection of pixels within the correct set of important features, but also showed different patterns of false positive attributions of importance to unimportant background features."
  - [corpus] Weak—corpus focuses on general XAI methods, not architecture-specific differences.
- **Break condition:** If all architectures produce identical explanations for same ground truth.

### Mechanism 3
- **Claim:** Simple edge detection baselines can outperform complex XAI methods on non-linear tasks.
- **Mechanism:** In tasks with clear signal boundaries (tetromino edges), edge detection filters capture the signal structure directly, while XAI methods overfit to spurious correlations or fail to localize importance.
- **Core assumption:** Signal structure in the task is sufficiently simple for edge detection to capture.
- **Evidence anchors:**
  - [abstract] "Moreover, we demonstrate that explanations derived from different model architectures can be vastly different; thus, prone to misinterpretation even under controlled conditions."
  - [section] "Interestingly, their performance is on par or even superior to various XAI methods in certain scenarios. Most notably, a simple Laplace edge detection filter outperforms nearly all other methods in the RIGID as well as the XOR scenarios, when used in combination with correlated backgrounds (CORR)."
  - [corpus] Weak—corpus mentions "Minimizing False-Positive Attributions" but not edge detection baselines.
- **Break condition:** If XAI methods consistently match or exceed edge detection performance across all scenarios.

## Foundational Learning

- **Concept:** Suppressor variables
  - Why needed here: Core source of false positive attributions in XAI; must understand to interpret benchmark results.
  - Quick check question: If a background pixel correlates with a foreground pixel but isn't part of the signal, should it be considered important?

- **Concept:** Ground truth explanation construction
  - Why needed here: Benchmark relies on explicit known feature sets (F+) to measure explanation quality; understanding construction is essential for proper use.
  - Quick check question: In the additive scenario, how is F+ defined when only one tetromino is present?

- **Concept:** Earth Mover's Distance (EMD) as explanation metric
  - Why needed here: Used to quantify how well importance maps align with ground truth; key to interpreting benchmark results.
  - Quick check question: What does EMD measure between two distributions, and why normalize by δmax?

## Architecture Onboarding

- **Component map:**
  - Data generator → Signal pattern (tetromino) + Background noise (WHITE/CORR/IMAGENET)
  - Classifier → LLR / MLP / CNN → Trained on train set, validated on val set
  - XAI explainer → 16 methods + 4 baselines → Takes model + sample + baseline → Outputs importance map
  - Metric evaluator → Precision / EMD_perf → Compares importance map to F+

- **Critical path:** Data generation → Model training → XAI explanation → Metric calculation → Result aggregation

- **Design tradeoffs:**
  - Fixed tetromino patterns vs. rigid transforms → Simplicity vs. realism
  - Three background types → Controlled suppressor presence vs. complexity
  - Single architecture per type vs. multiple → Consistency vs. breadth

- **Failure signatures:**
  - High Precision but low EMD_perf → Method finds correct features but poorly localized
  - Low Precision for all methods → Task too hard or suppressor influence too strong
  - Large variance across architectures → Method sensitive to model internals

- **First 3 experiments:**
  1. Generate LIN scenario with WHITE background, train LLR, apply LIME, calculate Precision and EMD_perf.
  2. Generate MULT scenario with CORR background, train MLP, apply Integrated Gradients, compare to baseline Laplace filter.
  3. Generate XOR scenario with IMAGENET background, train CNN, apply SHAP variants, plot boxplot of EMD_perf across 500 samples.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the XAI methods perform when applied to larger and more complex datasets, such as ImageNet-1k with 224x224 pixel images?
- Basis in paper: [explicit] The authors mention that their current benchmark uses 64x64 pixel images and ImageNet samples as backgrounds. They also acknowledge that future work will be to develop dedicated performance benchmarks in more complex and application-specific problem settings such as medical imaging.
- Why unresolved: The paper only tests XAI methods on their specific dataset and does not explore performance on larger, more complex real-world datasets.
- What evidence would resolve it: Conducting experiments with XAI methods on larger datasets like ImageNet-1k and comparing performance to the results obtained on the authors' dataset.

### Open Question 2
- Question: Are there other types of ground truth explanations that could be used to evaluate XAI methods, beyond the tetromino patterns used in this study?
- Basis in paper: [explicit] The authors state that their definition of ground truth explanations is based on the specific pixels of tetrominoes. They acknowledge that alternative definitions of ground truth could be conceived and new metrics could be developed.
- Why unresolved: The paper only uses tetromino patterns as ground truth explanations and does not explore other possibilities.
- What evidence would resolve it: Conducting experiments with XAI methods using different types of ground truth explanations, such as object boundaries or semantic features.

### Open Question 3
- Question: How do the results of this study generalize to other machine learning architectures beyond the three tested (LLR, MLP, CNN)?
- Basis in paper: [explicit] The authors state that they only tested three machine learning architectures and acknowledge that the space of possible neural network architectures is too vast to be represented. They suggest that their experiments serve as a showcase for their benchmarking framework, which can be easily extended to other architectures.
- Why unresolved: The paper only tests XAI methods on three specific architectures and does not explore performance on other architectures.
- What evidence would resolve it: Conducting experiments with XAI methods on a wider range of machine learning architectures and comparing performance to the results obtained on the three architectures tested in this study.

## Limitations
- The synthetic nature of the dataset may not fully capture the complexity of real-world image classification tasks.
- The performance degradation observed with suppressor variables may be specific to the chosen architectures and may not generalize to all model types.
- The benchmark's relevance to practical applications needs to be validated on real-world image classification datasets with manually annotated ground truth explanations.

## Confidence
- High confidence: The core finding that popular XAI methods often fail to outperform random and edge detection baselines on the XAI-TRIS benchmark.
- Medium confidence: The claim that suppressor variables in correlated backgrounds are the primary cause of XAI method failure.
- Low confidence: The assertion that simple edge detection methods can consistently outperform complex XAI methods across all non-linear tasks.

## Next Checks
1. Replicate the XAI-TRIS benchmark experiments using additional deep learning architectures (e.g., ResNet, Vision Transformer) to assess the generalizability of the observed performance degradation with suppressor variables.
2. Conduct ablation studies to isolate the impact of suppressor variables by progressively increasing the correlation strength in the background and measuring XAI method performance.
3. Apply the XAI-TRIS benchmark to real-world image classification datasets (e.g., CIFAR-10, ImageNet) with manually annotated ground truth explanations to validate the benchmark's relevance to practical applications.