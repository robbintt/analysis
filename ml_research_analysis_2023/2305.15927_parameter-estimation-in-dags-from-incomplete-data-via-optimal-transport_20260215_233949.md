---
ver: rpa2
title: Parameter Estimation in DAGs from Incomplete Data via Optimal Transport
arxiv_id: '2305.15927'
source_url: https://arxiv.org/abs/2305.15927
tags:
- distribution
- learning
- where
- latent
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OTP-DAG, a novel framework for learning parameters
  of directed graphical models with latent variables by framing parameter estimation
  as an optimal transport problem. Unlike traditional approaches based on likelihood
  maximization or variational inference, OTP-DAG minimizes the Wasserstein distance
  between data and model distributions, avoiding intractable posterior expectations.
---

# Parameter Estimation in DAGs from Incomplete Data via Optimal Transport

## Quick Facts
- arXiv ID: 2305.15927
- Source URL: https://arxiv.org/abs/2305.15927
- Reference count: 40
- One-line primary result: Introduces OTP-DAG, a novel framework for learning parameters of directed graphical models with latent variables by minimizing Wasserstein distance between data and model distributions

## Executive Summary
This paper presents OTP-DAG, a novel approach for parameter estimation in directed acyclic graphs (DAGs) with latent variables. The method frames parameter learning as an optimal transport problem, minimizing the Wasserstein distance between data and model distributions. Unlike traditional approaches based on likelihood maximization or variational inference, OTP-DAG avoids intractable posterior expectations by leveraging the Kantorovich dual formulation of optimal transport. The framework operates on any DAG structure without restrictive assumptions on variable types or structural dependencies.

## Method Summary
OTP-DAG reformulates parameter estimation in DAGs as minimizing the Wasserstein distance between data and model distributions. The method introduces backward mapping functions that approximate the conditional densities from observed variables to their parents, enabling tractable reconstruction error minimization. Forward mappings are parameterized by the model parameters and map parent variables to child variables. The framework is trained using stochastic gradient descent with appropriate reparameterization techniques for discrete variables. The approach is demonstrated on five tasks including topic modeling, sequential modeling, and discrete representation learning, showing competitive performance against established baselines.

## Key Results
- Recovers ground-truth parameters on synthetic data for LDA and HMM models
- Achieves 0.80 SSIM and 25.40 PSNR on CIFAR10, outperforming VQ-VAE baselines
- Demonstrates faster training times compared to traditional EM and VI methods
- Maintains competitive performance across diverse applications without task-specific assumptions

## Why This Works (Mechanism)

### Mechanism 1
Parameter learning in DAGs with latent variables can be reformulated as minimizing Wasserstein distance between data and model distributions. The method replaces intractable posterior expectations with tractable reconstruction error minimization by leveraging the Kantorovich dual formulation of optimal transport.

### Mechanism 2
The reconstruction error minimization is equivalent to the original Wasserstein distance minimization. Theorem 1 establishes that minimizing transport cost between data and model distributions reduces to minimizing reconstruction error, making the problem tractable.

### Mechanism 3
The method can recover ground-truth parameters while achieving competitive performance across diverse applications. Empirical evaluation demonstrates parameter recovery on synthetic data and competitive performance on downstream tasks.

## Foundational Learning

- Concept: Optimal Transport Theory
  - Why needed here: Provides the mathematical framework for reformulating parameter learning as a distance minimization problem between distributions
  - Quick check question: What is the key difference between Wasserstein distance and KL divergence that makes the former more suitable for this problem?

- Concept: Directed Acyclic Graphs and Bayesian Networks
  - Why needed here: The method operates on any DAG structure, so understanding the relationship between graph structure and conditional distributions is crucial
  - Quick check question: How does the presence of latent variables make likelihood maximization intractable in DAGs?

- Concept: Reparameterization Trick and Stochastic Optimization
  - Why needed here: Required for efficient gradient-based training of the backward and forward mappings
  - Quick check question: Why can't we directly backpropagate through discrete sampling operations, and how does the Gumbel-Softmax trick solve this?

## Architecture Onboarding

- Component map: Data -> Backward mappings (ϕi) -> Reconstruction error -> Forward mappings (ψθ) -> Model parameters (θ)
- Critical path:
  1. Define DAG structure with observed and latent variables
  2. Construct backward mappings for observed nodes
  3. Define forward mappings as reparameterizations of model conditionals
  4. Implement reconstruction error and regularization terms
  5. Optimize parameters via stochastic gradient descent

- Design tradeoffs:
  - Expressivity vs tractability: More complex backward mappings improve approximation but increase computational cost
  - Regularization strength: Controls the trade-off between reconstruction fidelity and distributional alignment
  - Choice of cost function: Different metrics (L2, cross-entropy) may be appropriate for different variable types

- Failure signatures:
  - Poor reconstruction quality despite training indicates backward mappings cannot approximate true conditionals
  - Mode collapse in discrete representation learning suggests insufficient regularization
  - Divergence between training and validation performance indicates overfitting to reconstruction task

- First 3 experiments:
  1. Simple HMM with synthetic data: Test parameter recovery on a known structure with Poisson observations
  2. LDA on synthetic corpus: Verify topic-word distribution recovery with horizontal/vertical patterns
  3. Binary VAE on MNIST: Compare reconstruction quality against standard VQ-VAE baseline

## Open Questions the Paper Calls Out

### Open Question 1
How does OTP-DAG's performance scale with increasingly complex graphical model structures, particularly those with deep hierarchies or extensive latent variable interactions? The paper demonstrates OTP-DAG on relatively simple structures (LDA, HMM, VQ-VAE) and mentions potential applications to "structural learning problems" but does not empirically evaluate on complex structures.

### Open Question 2
What is the theoretical guarantee of convergence when using neural network parameterizations for the backward maps, given that universal approximation theorems require specific conditions that may not hold in practice? The paper mentions relying on "universal approximation theorem" for backward maps but acknowledges limitations about neural network expressivity.

### Open Question 3
How does OTP-DAG handle discrete latent variables compared to continuous ones, and what are the theoretical implications of using different relaxation techniques (Gumbel-Softmax vs Concrete distributions) on the optimality of the learned parameters? The paper uses Gumbel-Softmax and Concrete distributions for discrete variables but does not analyze the impact of these relaxations on parameter estimation quality.

## Limitations
- Theoretical grounding relies on local backward mapping assumptions without practical approximation error bounds
- Scalability concerns for large-scale problems with hundreds or thousands of variables
- Limited empirical validation on complex DAG structures beyond simple models

## Confidence
- Parameter Recovery Claims: Medium confidence
- Performance Competitiveness: Medium confidence
- Training Efficiency Claims: Low confidence

## Next Checks
- Implement OTP-DAG on a complex DAG structure with nested latent variables (e.g., hierarchical Bayesian networks) to verify scalability beyond the demonstrated simple structures
- Conduct systematic ablation studies varying backward mapping architectures and regularization strengths to identify optimal configurations
- Compare OTP-DAG against specialized methods for each application domain (e.g., advanced topic models for LDA, state-of-the-art VAEs for representation learning) to better assess competitive positioning