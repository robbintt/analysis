---
ver: rpa2
title: 'Not So Robust After All: Evaluating the Robustness of Deep Neural Networks
  to Unseen Adversarial Attacks'
arxiv_id: '2308.06467'
source_url: https://arxiv.org/abs/2308.06467
tags:
- adversarial
- attack
- attacks
- trained
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper challenges the hypothesis that adversarial attacks\
  \ exploit only non-robust features in DNNs. The authors conduct extensive experiments\
  \ to test the generalization of robust and adversarial training methods against\
  \ various attack norms, including L2 and L\u221E."
---

# Not So Robust After All: Evaluating the Robustness of Deep Neural Networks to Unseen Adversarial Attacks

## Quick Facts
- arXiv ID: 2308.06467
- Source URL: https://arxiv.org/abs/2308.06467
- Reference count: 40
- Primary result: L∞ norm attacks cause the most dispersion in latent representations and significantly undermine the accuracy of both robust and adversarial trained models, revealing incomplete generalization of robust features.

## Executive Summary
This paper challenges the hypothesis that adversarial attacks exploit only non-robust features in DNNs. Through extensive experiments on CIFAR-10 and CINIC-10, the authors demonstrate that L∞ norm attacks cause the most dispersion in latent representations and significantly undermine the accuracy of both robust and adversarial trained models. The results reveal that robust features are not well generalized, especially for L∞ norm attacks, and suggest that researchers need to pay closer attention to L∞ norm attacks to avoid a false sense of security.

## Method Summary
The authors replicate robust training using Ilyas et al.'s method of extracting robust features with an adversarially trained model, then training a new model on these features. They evaluate both robust and adversarial trained models on unseen attacks across multiple norms (L1, L2, L∞) using FGSM, PGD, CW, and DeepFool attacks. SVCCA is used to compare representations of clean vs. adversarial examples, and DeepFool is used to measure mean distance to decision boundaries. PCA is applied for visualization of neural network representations.

## Key Results
- L∞ norm attacks cause the most dispersion in latent representations compared to L2 attacks
- Both robust and adversarial trained models show significant accuracy drops under L∞ attacks
- Robust features are not well generalized, especially for L∞ norm attacks
- L∞ attacks have a greater impact on model representations than L2 attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: L∞ norm adversarial attacks exploit inherent instabilities in DNN latent representations more effectively than L2 attacks.
- Mechanism: L∞ norm attacks apply uniform perturbations across all pixels within a fixed budget, causing dense, localized shifts in the input space that translate into widespread, dispersed changes in the latent space. L2 attacks, in contrast, apply smoother, more global perturbations that preserve some geometric structure in latent representations.
- Core assumption: The dispersion in latent space correlates with model misclassification risk, and L∞ perturbations maximize this dispersion.
- Evidence anchors:
  - [section] "L∞ norm attacks cause the most dispersion in the latent representation" and "L∞ norm attacks have a greater impact on model representations."
  - [abstract] "L∞ norm attacks cause the most dispersion in the latent representation, leading to a greater impact on model representations."
  - [corpus] No direct support found; evidence is internal to the paper.
- Break condition: If L∞ dispersion does not correlate with misclassification risk, or if model architectures mitigate this dispersion through architectural regularization, the mechanism fails.

### Mechanism 2
- Claim: Robust training generalizes better to L2-norm attacks than to L∞-norm attacks.
- Mechanism: During robust training, models learn to withstand small perturbations along smooth, low-frequency directions in the input space (L2-like), but fail to develop resilience to high-frequency, uniform perturbations (L∞-like). This creates an asymmetry in generalization.
- Core assumption: The training distribution of perturbations does not cover the full space of attack norms, especially the dense L∞ ball.
- Evidence anchors:
  - [section] "L∞ norm attacks significantly undermine the accuracy of both models, revealing that robust features are not well generalized, especially for L∞ norm attacks."
  - [abstract] "L∞ norm attacks significantly undermine the accuracy of both models, revealing that robust features are not well generalized, especially for L∞ norm attacks."
  - [corpus] No external validation found; relies on internal experimental results.
- Break condition: If future training methods incorporate L∞ perturbations effectively, or if L∞ robustness emerges as a byproduct of other defenses, the mechanism weakens.

### Mechanism 3
- Claim: Adversarial training on L2 norm perturbations is a broader, more inclusive case of robust training, but still insufficient for L∞ robustness.
- Mechanism: Adversarial training optimizes against worst-case perturbations in a given norm ball; robust training optimizes against perturbations derived from non-robust feature removal. L2 adversarial training thus provides broader coverage than robust training, but neither fully addresses the L∞ ball's unique geometry.
- Core assumption: Robust training is a subset of adversarial training, but the choice of norm during adversarial training determines the coverage of attack defenses.
- Evidence anchors:
  - [section] "robust training is a specific case of adversarial training."
  - [abstract] "we tested the corresponding adversarial trained models used in robust training, which provided insight into the relationship between robust and adversarial training, suggesting that robust training is a specific case of adversarial training."
  - [corpus] No external corroboration; internal hypothesis.
- Break condition: If robust training is found to be fundamentally different from adversarial training in scope or method, or if adversarial training proves universally sufficient, the claim fails.

## Foundational Learning

- Concept: Norm geometry in high-dimensional spaces (L2 vs L∞ balls).
  - Why needed here: The paper's central finding hinges on how different norms shape the perturbation space and influence model behavior.
  - Quick check question: What is the geometric difference between an L2 ball and an L∞ ball in high dimensions, and how does that affect adversarial perturbations?
- Concept: Canonical correlation analysis (CCA) and singular value CCA (SVCCA).
  - Why needed here: SVCCA is used to measure similarity between representations under different attacks, revealing dispersion patterns.
  - Quick check question: How does SVCCA quantify the similarity between two sets of neural network representations, and what does a low correlation imply?
- Concept: Decision boundary proximity and DeepFool attack.
  - Why needed here: DeepFool estimates the distance from samples to decision boundaries, a key metric in the paper's analysis of model robustness.
  - Quick check question: How does DeepFool approximate the minimum perturbation needed to cross a decision boundary, and why is this useful for robustness evaluation?

## Architecture Onboarding

- Component map: Data preprocessing pipeline -> Model training (robust/adversarial) -> Attack generation (FGSM, PGD, CW, DeepFool) -> Representation extraction -> SVCCA analysis -> Decision boundary distance calculation
- Critical path:
  1. Extract robust features using adversarially trained model
  2. Train robust model on extracted features
  3. Test both models on unseen attacks across multiple norms
  4. Analyze representation dispersion via SVCCA
  5. Visualize representations via PCA
  6. Measure decision boundary proximity with DeepFool
- Design tradeoffs:
  - Training time vs. robustness: More iterations and diverse attacks improve robustness but increase computational cost
  - Attack diversity vs. generalization: Testing on unseen attacks reveals generalization gaps but may miss norm-specific vulnerabilities
  - Representation complexity vs. interpretability: High-dimensional representations provide rich data but require dimensionality reduction for analysis
- Failure signatures:
  - High accuracy on L2 attacks but sharp drop on L∞ attacks indicates incomplete robustness
  - Low SVCCA correlation between clean and adversarial representations suggests significant latent space distortion
  - Small mean distance to decision boundary under L∞ attack implies high vulnerability
- First 3 experiments:
  1. Replicate robust training from [11] on CIFAR-10 using ResNet50, then test on unseen L1, L2, and L∞ attacks
  2. Apply SVCCA to compare representations of clean vs. L∞-attacked samples for regular and adversarially trained models
  3. Use DeepFool to measure mean distance to decision boundary for models trained under L2 vs. L∞ norms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do L-infinity norm attacks have a significantly greater impact on model representations compared to L2 norm attacks?
- Basis in paper: [explicit] The paper explicitly states that L-infinity norm attacks cause the most dispersion in the latent representation and have a greater impact on model representations, as evidenced by SVCCA analysis and visualization of neural network representations.
- Why unresolved: The paper does not provide a definitive explanation for why L-infinity norm attacks are more effective at disrupting model representations compared to L2 norm attacks, despite both being imperceptible to humans.
- What evidence would resolve it: Further experiments comparing the effects of different norm attacks on model representations, or theoretical analysis explaining the relationship between norm type and representation disruption.

### Open Question 2
- Question: Is robust training simply a specific case of adversarial training, or are there fundamental differences between the two approaches?
- Basis in paper: [explicit] The paper states that "robust models may be considered as a specific case of adversarial training" based on the similar ratio between accuracy under different attacks for both models.
- Why unresolved: While the paper suggests a relationship between robust and adversarial training, it does not definitively establish whether robust training is a subset of adversarial training or a distinct approach with different properties.
- What evidence would resolve it: Comparative experiments testing robust and adversarial trained models against a wider range of attacks and datasets, or theoretical analysis of the underlying mechanisms of each training method.

### Open Question 3
- Question: Can the distance from samples to the decision boundary be used as a reliable measure of model robustness?
- Basis in paper: [inferred] The paper uses the mean distance from samples to the decision boundary as a measure of robustness, but notes that the results for L-infinity norm attacks call into question the reliability of this method.
- Why unresolved: The paper does not provide a definitive answer on whether the distance to the decision boundary is a reliable measure of robustness, as the results for L-infinity norm attacks suggest that models can appear robust by this measure while still being vulnerable to attacks.
- What evidence would resolve it: Further experiments testing the relationship between decision boundary distance and model robustness under various attack types and norms, or theoretical analysis of the relationship between these two concepts.

## Limitations
- The core claim that L∞ attacks cause the most dispersion in latent representations is based on internal analysis without external validation from prior literature
- The mechanism linking L∞ dispersion to model vulnerability assumes that representation dispersion directly correlates with misclassification risk, which is not empirically verified beyond the presented experiments
- Hyperparameter choices for robust feature extraction and SVCCA analysis are not fully specified, limiting reproducibility

## Confidence

- **High confidence**: Experimental results showing L∞ attacks significantly reduce accuracy for both robust and adversarial training methods
- **Medium confidence**: The hypothesis that robust training is a subset of adversarial training, based on internal experimental comparisons
- **Low confidence**: The proposed mechanism that L∞ perturbations inherently cause greater latent space dispersion than L2 perturbations, due to lack of external validation

## Next Checks

1. Replicate the experiment using an L∞-trained model and compare latent space dispersion with L2-trained models under identical attack conditions
2. Conduct ablation studies varying the perturbation budget (ε) for L∞ attacks to determine the threshold at which dispersion and accuracy degradation become significant
3. Validate the correlation between SVCCA similarity scores and actual misclassification rates across multiple attack norms to confirm that low correlation implies high vulnerability