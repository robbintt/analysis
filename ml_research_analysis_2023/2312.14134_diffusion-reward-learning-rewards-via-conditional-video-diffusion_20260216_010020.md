---
ver: rpa2
title: 'Diffusion Reward: Learning Rewards via Conditional Video Diffusion'
arxiv_id: '2312.14134'
source_url: https://arxiv.org/abs/2312.14134
tags:
- reward
- diffusion
- tasks
- video
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Diffusion Reward is a method that learns rewards from expert videos
  for visual reinforcement learning tasks. It uses a conditional video diffusion model
  to capture the distribution of expert videos and estimates conditional entropy as
  rewards to encourage exploration of expert-like behaviors.
---

# Diffusion Reward: Learning Rewards via Conditional Video Diffusion

## Quick Facts
- arXiv ID: 2312.14134
- Source URL: https://arxiv.org/abs/2312.14134
- Reference count: 40
- 38% and 35% performance improvements over baseline methods on MetaWorld and Adroit tasks respectively

## Executive Summary
Diffusion Reward is a method that learns rewards from expert videos for visual reinforcement learning tasks by leveraging conditional video diffusion models. The approach estimates conditional entropy as rewards, encouraging exploration of expert-like behaviors while balancing exploration and exploitation through combined reward signals. The method was evaluated on 10 robotic manipulation tasks from MetaWorld and Adroit, demonstrating significant performance improvements over baseline methods and strong generalization to unseen tasks and real robot videos.

## Method Summary
The method trains a conditional video diffusion model on expert videos to capture their distribution, then estimates conditional entropy as rewards to encourage expert-like behavior. It uses VQ-GAN to compress high-dimensional observations into latent codes for efficient reward inference, and combines the entropy reward with RND exploration rewards and sparse environmental rewards. The combined reward signal is used to train an RL agent (DrQv2) on manipulation tasks. The approach addresses the challenge of learning from sparse rewards by creating dense rewards from expert demonstrations.

## Key Results
- 38% performance improvement over baseline methods on MetaWorld tasks
- 35% performance improvement over baseline methods on Adroit tasks
- Successfully generalized to unseen tasks and real robot videos
- Outperformed baselines including Raw Sparse Reward, RND, AMP, VIPER, and VIPER-std

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lower generative diversity when conditioning on expert trajectories creates effective rewards
- Mechanism: Diffusion models exhibit reduced entropy when generating from expert-conditioned inputs compared to random inputs, creating a natural reward signal where lower entropy indicates expert-like behavior
- Core assumption: Expert videos contain lower generative diversity than random videos when processed through diffusion models
- Evidence anchors:
  - [abstract] "Our key insight is that lower generative diversity is exhibited when conditioning diffusion on expert trajectories"
  - [section 4.2] "We address these challenges by harnessing the great generative capability of video diffusion models. Our observations indicate increased generation diversity with unseen historical observations (rand.) and reduced diversity with seen ones (expert)"
  - [corpus] Weak evidence - related work focuses on likelihood-based rewards rather than entropy-based approaches

### Mechanism 2
- Claim: Latent space compression enables efficient reward inference
- Mechanism: Using VQ-GAN to compress high-dimensional RGB images into vector-quantized latent codes reduces computational complexity while preserving sufficient information for reward estimation
- Core assumption: Compressed latent representations retain enough task-relevant information for effective reward learning
- Evidence anchors:
  - [section 4.1] "To accelerate the reward inference, we perform latent diffusion processes by utilizing vector-quantized codes [12] for compressing high-dimensional observations"
  - [section 4.2] "We first train an encoder unsupervisedly from expert videos to compress the high-dimensional observations"
  - [corpus] Weak evidence - no direct comparison with uncompressed approaches provided

### Mechanism 3
- Claim: Combining entropy reward with exploration reward balances exploitation and exploration
- Mechanism: The conditional entropy reward encourages expert-like behavior while RND provides novelty-seeking exploration, creating a balanced reward signal
- Core assumption: Pure expert-following rewards would lead to local optima while pure exploration would be inefficient
- Evidence anchors:
  - [section 4.2] "As the reward ¯rce incentivizes the agent to mimic the behavioral patterns of the expert, the exploration may still be prohibitively challenging in complex tasks with high-dimensional input. To alleviate this issue, we incorporate RND [3] as the exploration reward"
  - [section 5.2] "The incorporation of pure novelty-seeking rewards (RND) unsurprisingly enhances the RL agent's exploration"
  - [corpus] Weak evidence - related work uses different reward combinations but not specifically entropy + RND

## Foundational Learning

- Concept: Diffusion models and their training process
  - Why needed here: Understanding how diffusion models generate samples and how conditioning affects output diversity is crucial for grasping the reward mechanism
  - Quick check question: How does the forward diffusion process transform data distribution into noise distribution?

- Concept: Variational inference and entropy estimation
  - Why needed here: The method relies on estimating conditional entropy through variational bounds, requiring understanding of variational inference principles
  - Quick check question: What is the relationship between variational bounds and true entropy in diffusion models?

- Concept: Reinforcement learning with sparse rewards
  - Why needed here: The method addresses the challenge of learning from sparse rewards by creating dense rewards from expert videos
  - Quick check question: Why do sparse rewards typically lead to sample inefficiency in RL?

## Architecture Onboarding

- Component map: VQ-GAN encoder → VQ-Diffusion model → Conditional entropy estimation → Reward combination (entropy + RND + sparse) → RL agent (DrQv2)
- Critical path: Expert videos → VQ-GAN encoder → VQ-Diffusion model → Conditional entropy estimation → Reward computation → RL training
- Design tradeoffs:
  - Latent compression vs. information loss
  - Denoising steps vs. inference speed
  - Entropy reward weight vs. exploration balance
- Failure signatures:
  - Low reward discrimination between expert and random trajectories
  - Slow reward inference during RL training
  - RL agent gets stuck in local optima
- First 3 experiments:
  1. Test entropy discrimination on held-out expert vs. random trajectories
  2. Measure reward inference speed with varying denoising steps
  3. Compare learning curves with different entropy reward weights

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Diffusion Reward scale with the size and diversity of the expert video dataset?
- Basis in paper: [explicit] The paper mentions using 20 expert videos per MetaWorld task and 50 for Adroit tasks, but doesn't explore how performance changes with more or less data
- Why unresolved: The paper only uses a fixed number of expert videos for training. Scaling up or down the dataset size was not explored
- What evidence would resolve it: Experiments showing learning curves for different dataset sizes (e.g. 5, 10, 20, 50, 100 videos per task) would reveal how performance scales with data

### Open Question 2
- Question: How does the performance of Diffusion Reward compare to methods that use both expert videos and task descriptions (e.g. text)?
- Basis in paper: [inferred] The paper mentions that incorporating text embeddings for task specification could enhance generalization, but doesn't actually compare to such methods
- Why unresolved: The paper only compares to baselines that use expert videos, not those that use additional task information like text
- What evidence would resolve it: Comparing Diffusion Reward to methods that use both expert videos and task descriptions (e.g. text) on the same tasks would show the relative benefit of each approach

### Open Question 3
- Question: How does the performance of Diffusion Reward change when using different video diffusion model architectures?
- Basis in paper: [explicit] The paper mentions that Diffusion Reward could in principle adopt any off-the-shelf video diffusion models, but only uses VQ-Diffusion
- Why unresolved: The paper only uses one specific video diffusion model architecture. Different architectures may perform better or worse
- What evidence would resolve it: Replacing VQ-Diffusion with other video diffusion models (e.g. CDVD, Video Diffusion Transformer) and comparing performance would reveal the impact of architecture choice

## Limitations

- Method's reliance on generative diversity differences between expert and random trajectories may not hold across all task domains
- Performance degradation observed in tasks requiring fine-grained manipulation suggests limitations in handling subtle expert behavior differences
- Substantial expert video data requirements (20-50 videos per task) may be prohibitive in real-world settings

## Confidence

**High Confidence:** The technical implementation of conditional entropy estimation through diffusion models is sound, supported by the mathematical formulation in Section 4.2 and the successful training of the VQ-Diffusion model on expert videos.

**Medium Confidence:** The claim that combining entropy rewards with RND exploration provides optimal exploration-exploitation balance is supported by empirical results but lacks theoretical justification. The performance improvements over baselines are substantial but could be influenced by implementation details not fully disclosed.

**Low Confidence:** The generalizability claim to unseen tasks and real robot videos, while demonstrated, is based on limited testing (only 2 unseen tasks and a small number of real robot videos). More extensive validation would be needed to confirm true generalization capability.

## Next Checks

1. **Entropy Discrimination Validation:** Test the conditional entropy reward's ability to discriminate between expert and random trajectories on a held-out validation set from each task, ensuring the reward signal maintains meaningful gradients for RL training.

2. **Latent Compression Ablation:** Compare performance with and without VQ-GAN compression to quantify information loss and determine if the computational speedup justifies any performance degradation.

3. **Sample Efficiency Analysis:** Evaluate how performance scales with varying numbers of expert videos (5, 10, 20, 50) to determine the minimum effective dataset size and identify potential bottlenecks in data collection requirements.