---
ver: rpa2
title: Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS Tokens
arxiv_id: '2309.04333'
source_url: https://arxiv.org/abs/2309.04333
tags:
- papers
- multi2spe
- specter
- scientific
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of representing scientific papers
  from multiple domains using a single encoder, as existing methods are often trained
  and evaluated on domain-specific datasets. The authors propose Multi2SPE, which
  uses multiple CLS tokens in a BERT-based architecture, each learning different ways
  to aggregate token embeddings, and then combines them into a single representation.
---

# Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS Tokens

## Quick Facts
- **arXiv ID**: 2309.04333
- **Source URL**: https://arxiv.org/abs/2309.04333
- **Reference count**: 33
- **Primary result**: Multi2SPE reduces citation prediction error by up to 25% compared to single-CLS baselines

## Executive Summary
This paper addresses the challenge of representing scientific papers from multiple domains using a single encoder, proposing Multi2SPE which uses multiple CLS tokens in a BERT-based architecture. Each CLS token learns different ways to aggregate token embeddings, and they are combined via summation into a single representation. The authors also introduce Multi-SciDocs, a new multi-domain benchmark for evaluating cross-domain scientific paper retrieval. Experiments show significant improvements over single-CLS baselines, particularly when trained on balanced multi-domain data.

## Method Summary
Multi2SPE extends BERT by adding multiple CLS tokens (3 in the main experiments) and injecting linear layers after specific BERT layers (4, 8, 12) with reparameterization to ensure diversity. Each CLS token attends to different aspects of the input sequence, and their embeddings are summed to form the final document representation. The model is trained using contrastive citation prediction with similarity weighting (λ=0.1) to balance global and local matching. The approach is evaluated on Multi-SciDocs, a benchmark built from S2ORC shards with balanced domain distribution.

## Key Results
- Multi2SPE reduces citation prediction error by up to 25% compared to single-CLS baselines
- Performance gains are most pronounced when trained on balanced multi-domain data
- Increasing CLS tokens from 3 to 5 shows diminishing returns, suggesting 3 is optimal for this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple CLS tokens with linear layer injection enable domain-specific attention patterns that a single CLS cannot capture.
- Mechanism: Each CLS token attends to different aspects of the input sequence via separate linear transformations applied after each BERT layer. The reparameterization trick forces these linear projections to diverge, ensuring diverse attention distributions across domains.
- Core assumption: Different scientific domains require fundamentally different ways of aggregating token embeddings into document representations.
- Evidence anchors:
  - [abstract] "extracting embeddings through just one CLS token is limiting, as more ideal ways of mixing contextualized word embeddings could be different for each subject areas"
  - [section] "each CLS embeddings to learn distinctive ways of mixing word embeddings together for the final document representation"
  - [corpus] Weak - corpus neighbors don't directly address domain-specific attention mechanisms
- Break condition: If the reparameterization fails to prevent linear weights from converging, the diversity benefit disappears and performance reverts to single-CLS baseline.

### Mechanism 2
- Claim: Summation of multiple CLS embeddings provides better representation than concatenation while maintaining computational efficiency.
- Mechanism: Individual CLS embeddings are summed to form a single document representation, avoiding the O(n²) complexity of comparing all CLS pairs during retrieval while preserving complementary information.
- Core assumption: Summing embeddings preserves sufficient discriminative information while being computationally tractable for large-scale retrieval.
- Evidence anchors:
  - [abstract] "we would significantly increase the computational costs of the retrieval process. Instead, during inference, we simply take the summation of CLS embeddings"
  - [section] "We compute the similarity between two papers using dot products between their paper embeddings and the most similar CLS embeddings"
  - [corpus] Weak - corpus neighbors don't address retrieval efficiency considerations
- Break condition: If domain-specific information gets averaged out during summation, the multi-CLS advantage diminishes and single-CLS performance may match or exceed it.

### Mechanism 3
- Claim: Contrastive citation prediction with multiple embeddings creates richer training signals that better generalize across domains.
- Mechanism: The training objective uses similarity between paper embeddings and their most similar CLS pairs, with λ controlling the balance between global and local matching. This encourages both individual CLS tokens to be meaningful and the overall representation to capture domain-specific citation patterns.
- Core assumption: Citation patterns across scientific domains provide sufficiently diverse and structured signals for training robust multi-domain embeddings.
- Evidence anchors:
  - [abstract] "we minimize the cross entropy loss of a given query paper P Q, a cited paper P +, and a paper not cited, P −"
  - [section] "The objective function is to encourage the embedding of each query paper to be close to those of the paper cited by them"
  - [corpus] Weak - corpus neighbors don't directly validate the effectiveness of multi-CLS contrastive training
- Break condition: If training data lacks sufficient cross-domain citations, the model may overfit to domain-specific citation patterns and fail to generalize.

## Foundational Learning

- **Concept**: Transformer attention mechanisms and BERT architecture
  - Why needed here: Understanding how BERT processes sequences and why a single CLS token may be limiting for multi-domain tasks
  - Quick check question: What does the CLS token represent in standard BERT, and how does it aggregate information from the sequence?

- **Concept**: Contrastive learning and triplet loss formulations
  - Why needed here: The model uses citation-based contrastive learning, requiring understanding of positive/negative sampling and similarity objectives
  - Quick check question: How does the contrastive loss encourage embeddings of cited papers to be closer than embeddings of non-cited papers?

- **Concept**: Linear reparameterization and weight initialization techniques
  - Why needed here: The model uses a specific reparameterization trick to ensure diversity among CLS token linear layers
  - Quick check question: What mathematical property does the reparameterization enforce, and why is this important for preventing weight collapse?

## Architecture Onboarding

- **Component map**: Input → BERT layers with multiple CLS tokens → Per-layer linear transformations → CLS embeddings → Summation → Paper representation → Similarity computation → Contrastive loss
- **Critical path**: Tokenization → BERT encoding → Linear layer application → CLS aggregation → Similarity calculation → Loss computation
- **Design tradeoffs**: Multiple CLS tokens provide diversity but increase parameter count and complexity; summation balances representation power with computational efficiency
- **Failure signatures**: Poor domain generalization (single-CLS baseline matches performance), slow convergence during training, high variance across random seeds
- **First 3 experiments**:
  1. Baseline test: Run single-CLS BERT on Multi-SciDocs to establish performance floor
  2. Ablation test: Remove linear layer injection to measure its contribution
  3. λ sensitivity test: Vary λ from 0 to 1 to find optimal balance between global and local similarity emphasis

Note: All experiments should be run with multiple random seeds to establish statistical significance, as the paper reports scores averaged over four seeds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the individual CLS embeddings in Multi2SPE specialize to different scientific domains, and what specific roles do they play in the final representation?
- Basis in paper: [inferred] The paper mentions that each CLS token learns different ways of aggregating token embeddings, but does not provide detailed analysis on the specialization or specific roles of each embedding.
- Why unresolved: The paper acknowledges the need for more qualitative investigation into the specific roles of each CLS embedding and how they collaborate, but does not conduct this analysis.
- What evidence would resolve it: Detailed qualitative analysis of the attention patterns and learned weights of each CLS token across different domains, possibly using visualization techniques or probing tasks.

### Open Question 2
- Question: How well does Multi2SPE generalize to scientific domains that are not well-represented in the training data, and what factors influence this generalization?
- Basis in paper: [inferred] The paper discusses the challenge of imbalanced domain distribution in S2ORC and mentions that certain domains have fewer open access articles. It also notes that high correlation between intrinsic evaluation metrics and real-life system effectiveness is yet to be proven.
- Why unresolved: The paper does not conduct experiments on extremely rare or underrepresented domains, nor does it provide a systematic study of factors influencing generalization to unseen domains.
- What evidence would resolve it: Experiments testing Multi2SPE on domains with very few training examples, analysis of performance correlation with domain representation in training data, and comparison with human expert judgments on domain similarity.

### Open Question 3
- Question: What is the optimal number of CLS tokens for Multi2SPE across different scientific domains and tasks, and how does this choice affect computational efficiency and performance?
- Basis in paper: [explicit] The ablation studies show that increasing the number of CLS tokens from 3 to 5 leads to mixed results, with similar overall performance suggesting that the quality of multi-domain paper embeddings is not sensitive to the number of CLS tokens.
- Why unresolved: The paper only tests 1, 3, and 5 CLS tokens, leaving a gap in understanding the full range of optimal choices and their task/domain-specific effects.
- What evidence would resolve it: Systematic experiments varying the number of CLS tokens (e.g., 2, 4, 6, 8) across multiple domains and tasks, measuring both performance and computational costs to find the optimal trade-off.

## Limitations
- The reparameterization trick's effectiveness in ensuring CLS token diversity lacks detailed empirical validation
- Computational efficiency claims for summation versus concatenation are stated but not rigorously proven
- Domain balance and citation diversity in training data are not thoroughly characterized

## Confidence
- **High confidence**: The overall experimental results showing Multi2SPE outperforming single-CLS baselines on Multi-SciDocs (25% error reduction) are well-supported by the reported metrics and multiple random seeds
- **Medium confidence**: The mechanism claims about how multiple CLS tokens enable domain-specific attention patterns are plausible but rely on assumptions about the reparameterization trick that lack detailed empirical validation
- **Medium confidence**: The computational efficiency argument for summation over concatenation is logically sound but lacks rigorous complexity analysis or runtime benchmarks

## Next Checks
1. **Reparameterization Diversity Analysis**: Implement ablation studies where the reparameterization trick is removed and observe whether the linear weights converge across CLS tokens, measuring the KL divergence between attention distributions as a quantitative diversity metric

2. **Computational Complexity Validation**: Measure actual training and inference times for Multi2SPE versus single-CLS baselines on GPU, comparing both wall-clock time and memory usage, and verify the claimed O(n²) complexity for concatenation-based approaches

3. **Domain Balance Impact Study**: Create controlled experiments with varying degrees of domain imbalance in the training data (from highly balanced to extreme skew) and measure how Multi2SPE's performance degrades as cross-domain citation patterns become sparser