---
ver: rpa2
title: Retrieval-Augmented Chain-of-Thought in Semi-structured Domains
arxiv_id: '2310.14435'
source_url: https://arxiv.org/abs/2310.14435
tags:
- retrieval
- sara
- llms
- gpt-3
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying question-answering
  systems to specialized domains like law and finance, where the required context
  often exceeds the input size limitations of large language models (LLMs). The proposed
  solution leverages the semi-structured nature of legal and financial data to efficiently
  retrieve relevant context, enabling the use of LLMs for domain-specific QA.
---

# Retrieval-Augmented Chain-of-Thought in Semi-structured Domains

## Quick Facts
- **arXiv ID**: 2310.14435
- **Source URL**: https://arxiv.org/abs/2310.14435
- **Reference count**: 4
- **Primary result**: Proposed retrieval-augmented chain-of-thought approach outperforms contemporary models on legal (SARA) and financial (FinQA) QA datasets.

## Executive Summary
This paper addresses the challenge of applying question-answering systems to specialized domains like law and finance, where the required context often exceeds the input size limitations of large language models (LLMs). The proposed solution leverages the semi-structured nature of legal and financial data to efficiently retrieve relevant context, enabling the use of LLMs for domain-specific QA. The approach involves a retrieval step that extracts relevant information from statutes or financial reports based on the question, followed by an answering step using LLMs with chain-of-thought prompting. The results show that the proposed system outperforms contemporary models on two datasets, SARA (legal domain) and FinQA (financial domain), demonstrating the effectiveness of retrieval-augmented chain-of-thought prompting in handling complex reasoning tasks in specialized domains.

## Method Summary
The method employs retrieval-augmented chain-of-thought prompting for domain-specific QA. It first retrieves relevant context from semi-structured legal statutes or financial reports using structure-specific algorithms (rule-based parsers for legal data, BERT-based retrievers for financial data). The retrieved context is then used in LLM prompts with chain-of-thought examples to generate answers. The system is evaluated on two datasets: SARA for legal reasoning and FinQA for financial reasoning, using models like GPT-3 and LLaMA-2.

## Key Results
- Retrieval-augmented CoT approach outperforms contemporary models on SARA and FinQA datasets.
- Chain-of-thought prompting improves reasoning accuracy for complex multi-step questions.
- Structure-specific retrieval algorithms are more effective than general methods for semi-structured data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval of relevant context from semi-structured legal and financial data enables LLMs to overcome input length limitations while preserving reasoning accuracy.
- Mechanism: The system exploits hierarchical structure (sections, subsections, paragraphs) in statutes and tabular/text formats in financial reports to extract only relevant portions. This targeted retrieval reduces context size while maintaining necessary information for answering complex questions.
- Core assumption: Semi-structured data contains clear hierarchical relationships that can be programmatically parsed to identify relevant context.
- Evidence anchors:
  - [abstract] "explores leveraging the semi-structured nature of legal and financial data to efficiently retrieve relevant context"
  - [section] "The context required for legal and financial questions is often large and may not fit within the token limit, requiring more efficient retrieval"
  - [corpus] Weak evidence - only 5/8 corpus papers directly address semi-structured data retrieval, suggesting this mechanism is specialized rather than broadly validated.

### Mechanism 2
- Claim: Chain-of-thought prompting combined with in-context learning enables LLMs to perform multi-step reasoning required for complex legal and financial questions.
- Mechanism: CoT prompting structures reasoning into explicit sequential steps, while ICL provides relevant examples. The combination allows models to decompose complex problems into manageable sub-tasks and apply learned patterns.
- Core assumption: LLMs can learn reasoning patterns from few examples and apply them to new problems when prompted appropriately.
- Evidence anchors:
  - [abstract] "adopt chain-of-thought (CoT) prompting (Wei et al., 2023) for generating the answers since it is well suited for performing reasoning in a step-by-step manner"
  - [section] "CoT prompting is especially useful for complex tasks which require multiple steps of reasoning over the given input"
  - [corpus] Moderate evidence - 3/8 corpus papers reference CoT or similar reasoning techniques, but none specifically test on semi-structured domains.

### Mechanism 3
- Claim: Task-specific retrieval algorithms tailored to data structure outperform general retrieval methods for semi-structured domains.
- Mechanism: Different retrieval strategies (mentioned-only, entire-section, references) are designed based on the specific hierarchical structure of SARA statutes, while FinQA uses BERT-based retrieval adapted for tabular data.
- Core assumption: The structure of domain-specific data can be exploited to create more efficient retrieval than general-purpose methods.
- Evidence anchors:
  - [section] "We propose to leverage the structure present in the data to retrieve the relevant context from the legal statutes and financial reports"
  - [section] "This structure is specific to the data source and the retriever needs to be designed accordingly"
  - [corpus] Strong evidence - 4/8 corpus papers specifically address domain-specific retrieval, with 2 focusing on legal/financial domains.

## Foundational Learning

- Concept: Hierarchical data parsing
  - Why needed here: Legal statutes and financial reports use consistent hierarchical structures that must be programmatically extracted to enable efficient retrieval
  - Quick check question: Can you write a parser that extracts all subsections from a statute given its hierarchical structure?

- Concept: Chain-of-thought prompting
  - Why needed here: Complex questions require multi-step reasoning that must be explicitly structured for LLMs to follow
  - Quick check question: Given a simple math word problem, can you create a CoT prompt that breaks it into explicit reasoning steps?

- Concept: Retrieval-augmented generation
  - Why needed here: LLMs cannot process full documents, so relevant context must be retrieved and injected into prompts
  - Quick check question: How would you design a retriever that finds the most relevant paragraphs from a legal document given a question?

## Architecture Onboarding

- Component map: Input → Parser (for SARA) or BERT Retriever (for FinQA) → LLM with CoT prompt → Output
- Critical path:
  1. Parse question to identify relevant sections/subsections
  2. Retrieve relevant context using structure-specific algorithm
  3. Construct CoT prompt with retrieved context and in-context examples
  4. Generate answer through LLM
  5. Return answer with CoT explanation
- Design tradeoffs:
  - Retrieval specificity vs. completeness: More specific retrieval reduces context but risks missing information
  - Prompt length vs. in-context examples: More examples improve performance but consume context budget
  - Model size vs. cost/latency: Larger models perform better but are more expensive and slower
- Failure signatures:
  - Retrieval returns irrelevant context → Answer references wrong sections or concepts
  - Prompt construction fails → LLM returns generic answers or refuses
  - CoT generation fails → Answer lacks reasoning explanation or contains logical errors
- First 3 experiments:
  1. Test different retrieval strategies (mentioned-only, entire-section, references) on SARA validation set
  2. Compare zero-shot, few-shot, and CoT prompting on both datasets
  3. Evaluate impact of prompt tuning by iteratively refining in-context examples on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed retrieval-augmented approach vary with different sizes of input context in the semi-structured domains of law and finance?
- Basis in paper: [explicit] The paper discusses the challenge of handling very long inputs/contexts and mentions that the cost and latency of LLMs increase with input size.
- Why unresolved: The paper does not provide specific experimental results or analysis on how varying the input context size affects the performance of the retrieval-augmented approach.
- What evidence would resolve it: Conducting experiments with different input context sizes and measuring the performance metrics (e.g., accuracy, recall) for each size would provide insights into the scalability and efficiency of the proposed approach.

### Open Question 2
- Question: What is the impact of the quality and specificity of the retrieval algorithm on the overall performance of the retrieval-augmented chain-of-thought prompting in specialized domains?
- Basis in paper: [explicit] The paper highlights the importance of retrieval in preventing the required context from exceeding the token limit and mentions the use of rule-based and pre-trained retrieval models.
- Why unresolved: The paper does not explore how variations in the retrieval algorithm's quality and specificity affect the system's performance, especially in handling complex reasoning tasks.
- What evidence would resolve it: Comparative studies using different retrieval algorithms with varying levels of quality and specificity, followed by performance evaluation, would clarify the impact of retrieval on the system's effectiveness.

### Open Question 3
- Question: How does the proposed approach handle arithmetic and logical reasoning errors made by LLMs, and what are the implications for its applicability in critical domains?
- Basis in paper: [inferred] The paper mentions arithmetic errors made by LLMs as a limitation and notes the importance of interpretability in critical domains where AI systems are adopted.
- Why unresolved: The paper does not provide a detailed analysis of how the system addresses or mitigates arithmetic and logical reasoning errors, nor does it discuss the implications for its use in domains where accuracy is crucial.
- What evidence would resolve it: An in-depth error analysis, including strategies for error detection and correction, along with case studies demonstrating the system's performance in critical domains, would provide insights into its reliability and applicability.

## Limitations

- Performance heavily depends on the semi-structured nature of legal and financial data, limiting generalizability to other domains.
- The system's effectiveness relies on the quality of the retrieval algorithm, which may not generalize well across different data structures.
- The paper doesn't provide comprehensive analysis of prompt sensitivity or how small changes in prompt construction impact performance.

## Confidence

- **High Confidence**: The core retrieval-augmented approach is technically sound and the evaluation methodology is rigorous. The system successfully demonstrates that targeted retrieval from semi-structured data can enable LLM-based QA in specialized domains.
- **Medium Confidence**: The claimed performance improvements over baseline models are supported by empirical results, but the paper lacks ablation studies showing the individual contributions of retrieval quality, CoT prompting, and in-context learning.
- **Low Confidence**: The generalizability of the approach to other semi-structured domains beyond law and finance. While the methodology appears extensible, the paper provides limited evidence of cross-domain applicability.

## Next Checks

1. **Retrieval Quality Impact**: Systematically vary the retrieval precision (e.g., by adjusting context length thresholds) and measure the corresponding impact on QA accuracy to establish the sensitivity of final performance to retrieval quality.

2. **Prompt Template Robustness**: Create multiple prompt variations (different CoT structures, varying in-context examples) and test whether performance remains consistent across variations, or identify which prompt elements are critical for success.

3. **Cross-Domain Transferability**: Apply the same retrieval-augmented CoT approach to a third semi-structured domain (e.g., medical guidelines or regulatory compliance documents) using the same methodology to test generalizability beyond the two demonstrated domains.