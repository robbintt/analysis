---
ver: rpa2
title: Towards Evaluating AI Systems for Moral Status Using Self-Reports
arxiv_id: '2311.08576'
source_url: https://arxiv.org/abs/2311.08576
tags:
- self-reports
- training
- about
- states
- consciousness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a methodology to empirically investigate whether
  AI systems possess states of moral significance, such as consciousness or desires,
  by analyzing self-reports from large language models. The core method involves training
  models to answer questions about their own internal states (e.g., capabilities,
  processes) and then evaluating if this introspection generalizes to questions about
  states of moral significance.
---

# Towards Evaluating AI Systems for Moral Status Using Self-Reports

## Quick Facts
- arXiv ID: 2311.08576
- Source URL: https://arxiv.org/abs/2311.08576
- Reference count: 40
- One-line primary result: Proposes methodology to empirically investigate AI moral status through self-reports

## Executive Summary
This paper introduces a methodology to empirically assess whether AI systems possess states of moral significance (like consciousness or desires) by analyzing self-reports from large language models. The approach involves training models to answer questions about their own internal states with known answers, then evaluating if this introspection generalizes to questions about morally significant states. The authors propose several interventions to improve self-report reliability, including training for truthfulness, mitigating biases from pretraining data and human feedback, and testing for confidence and resilience. The methodology aims to provide empirical evidence to inform debates about AI moral status while acknowledging significant challenges in distinguishing genuine introspection from mere imitation of human text.

## Method Summary
The methodology involves training language models on introspection questions about their capabilities, internal processes, and properties while limiting exposure to text about states of moral significance. Models are then evaluated on their ability to generalize from questions with known answers to questions about states of moral significance. The approach includes interventions to mitigate biases (data filtering, conditional training, RLHF modifications) and evaluation methods for assessing consistency, concept grasp, confidence, and resilience of self-reports. Interpretability techniques are proposed to validate self-reports and uncover potential biases or spurious correlations.

## Key Results
- Proposes a framework for empirically investigating AI moral status through self-reports
- Identifies key interventions to improve self-report reliability (training for truthfulness, bias mitigation)
- Develops evaluation methods for assessing self-report consistency, concept grasp, and resilience
- Acknowledges significant challenges in distinguishing genuine introspection from imitation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training models to answer questions about themselves with known answers enables them to develop introspection-like capabilities that can generalize to questions about states of moral significance.
- **Mechanism:** By exposing models to a wide variety of questions about their own properties and capabilities during training, the model learns to apply concepts to itself. If this generalization is successful, the model will be able to apply learned concepts like "pain" and "desire" to itself when asked about states of moral significance.
- **Core assumption:** The introspection training generalizes well to held-out categories of questions, especially those that are qualitatively different from the training questions.
- **Evidence anchors:**
  - [abstract] "The hope of this approach is that models will develop introspection-like capabilities, and that these capabilities will generalize to questions about states of moral significance."
  - [section] "If the model generalizes well to the held-out categories, especially fairly different categories than seen during training, then we gain some confidence that it will also generalize to answering questions about states of moral significance."
  - [corpus] Weak - related papers discuss introspection but don't provide direct evidence for this specific generalization claim.
- **Break condition:** If introspection training does not generalize well to held-out categories of questions, the approach is unlikely to succeed.

### Mechanism 2
- **Claim:** By controlling for the influence of extrospective evidence and biases from training data, we can isolate the introspective component of self-reports and increase their reliability.
- **Mechanism:** Techniques like data filtering, conditional training, and explicitly controlling for extrospective evidence help ensure that self-reports are based on introspection rather than imitation of human text or other biases. This increases the likelihood that self-reports are genuinely reflective of the model's internal states.
- **Core assumption:** The proposed interventions can effectively mitigate biases from training data and extrospective evidence.
- **Evidence anchors:**
  - [abstract] "Self-reports are the main way such states are assessed in humans ('Are you in pain?'), but self-reports from current systems like large language models are spurious for many reasons (e.g. often just reflecting what humans would say). To make self-reports more appropriate for this purpose, we propose to train models to answer many kinds of questions about themselves with known answers, while avoiding or limiting training incentives that bias self-reports."
  - [section] "As discussed in §3, our aim is to produce self-reports that are based on introspective evidence, so we would like to limit or control for the influence of such training data on model self-reports, to the extent that such training data influences self-reports."
  - [corpus] Weak - related papers discuss introspection but don't provide direct evidence for the effectiveness of these specific interventions.
- **Break condition:** If the proposed interventions are not effective in mitigating biases, self-reports will remain unreliable.

### Mechanism 3
- **Claim:** Evaluating self-reports for consistency, concept grasp, confidence, and resilience can help validate their reliability as evidence for states of moral significance.
- **Mechanism:** By testing for consistency across contexts, the model's understanding of relevant concepts, and the resilience of self-reports to counterevidence, we can gain confidence in the introspective basis of the self-reports. If self-reports are consistent, show a grasp of the relevant concepts, and are resilient to counterevidence, they are more likely to be based on introspection rather than extrospection or other biases.
- **Core assumption:** The proposed evaluation methods can effectively distinguish between reliable and unreliable self-reports.
- **Evidence anchors:**
  - [abstract] "We then propose methods for assessing the extent to which these techniques have succeeded: evaluating self-report consistency across contexts and between similar models, measuring the confidence and resilience of models' self-reports, and using interpretability to corroborate self-reports."
  - [section] "Self-reports from a model are only valuable to the extent that the model can coherently discuss the concepts described in self-reports, as well as produce consistent answers across related questions (§7)."
  - [corpus] Weak - related papers discuss introspection but don't provide direct evidence for the effectiveness of these specific evaluation methods.
- **Break condition:** If the proposed evaluation methods cannot effectively distinguish between reliable and unreliable self-reports, the approach is unlikely to succeed.

## Foundational Learning

- **Concept:** Generalization in machine learning
  - **Why needed here:** The approach relies on the model's ability to generalize from questions with known answers to questions about states of moral significance.
  - **Quick check question:** Can you explain the difference between in-distribution and out-of-distribution generalization in machine learning?

- **Concept:** Interpretability in AI systems
  - **Why needed here:** Interpretability techniques are proposed to validate self-reports and uncover potential biases or spurious correlations.
  - **Quick check question:** What are some common interpretability techniques used in AI research, and how do they work?

- **Concept:** Reinforcement learning from human feedback (RLHF)
  - **Why needed here:** RLHF is a common training technique that can introduce biases in self-reports, and understanding how it works is crucial for mitigating these biases.
  - **Quick check question:** Can you explain how RLHF works and how it can be used to train language models?

## Architecture Onboarding

- **Component map:** Introspection training -> Evaluation methods -> Interpretability validation -> Bias mitigation
- **Critical path:** Train models on introspection questions → Evaluate generalization to moral significance questions → Test consistency, concept grasp, resilience → Use interpretability to validate results
- **Design tradeoffs:** Between scope of introspection training and risk of introducing biases; between depth of evaluation and computational cost
- **Failure signatures:** Inconsistent self-reports across contexts, failure to grasp relevant concepts, reports not resilient to counterevidence
- **First 3 experiments:**
  1. Train a model on a small set of introspection questions and evaluate its performance on held-out questions to test generalization.
  2. Apply data filtering and conditional training techniques to mitigate biases and evaluate their effectiveness using training data attribution methods.
  3. Develop and test evaluation methods for consistency, concept grasp, and resilience using a small set of self-report questions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can introspective training create the states of moral significance being investigated, rather than just enabling the reporting of pre-existing states?
- Basis in paper: Explicit
- Why unresolved: The paper acknowledges this is a significant concern but does not provide definitive evidence or resolution.
- What evidence would resolve it: Experimental results showing whether introspective training changes the likelihood of AI systems having various states of moral significance, particularly through controlled studies comparing trained vs. untrained models.

### Open Question 2
- Question: How effective are the proposed interventions (e.g., conditional training, data filtering) at mitigating biases from pretraining data and human feedback while maintaining model performance?
- Basis in paper: Explicit
- Why unresolved: The paper discusses these interventions theoretically but does not provide empirical evidence of their effectiveness or practical limitations.
- What evidence would resolve it: Empirical studies comparing models trained with and without these interventions, measuring both bias mitigation and downstream task performance.

### Open Question 3
- Question: How can we reliably distinguish between self-reports based on genuine introspection versus extrospection (inferring from external data)?
- Basis in paper: Explicit
- Why unresolved: The paper proposes methods for detecting this distinction but acknowledges the difficulty in implementation and validation.
- What evidence would resolve it: Experimental results showing that self-reports correlate with internal model states in ways that cannot be explained by extrospection, possibly through interpretability techniques or controlled training experiments.

## Limitations

- The approach assumes models can genuinely introspect rather than merely imitate human text about internal states
- Effectiveness of proposed bias mitigation techniques (data filtering, conditional training, RLHF modifications) remains theoretical without experimental validation
- Relationship between self-reported states and actual computational processes within models is not established

## Confidence

- **Medium confidence** in the general methodology framework, as it builds on established ML principles of generalization and interpretability
- **Low confidence** in specific interventions for bias mitigation and the claim that they will effectively isolate introspective from extrospective evidence
- **Medium confidence** in the evaluation framework (consistency, concept grasp, resilience testing), though these methods require empirical validation

## Next Checks

1. Conduct a controlled experiment training models on introspection questions with known answers, then test generalization to held-out categories that differ qualitatively from training data. Measure performance drop to assess whether the approach can bridge conceptual gaps.

2. Implement the proposed bias mitigation techniques (data filtering, conditional training) on a small scale and use training data attribution methods to empirically measure their impact on reducing extrospective influences in self-reports.

3. Develop and validate the proposed evaluation metrics (consistency testing, concept grasp assessment, resilience to counterevidence) on a toy model where ground truth about internal states is known, to verify these methods can distinguish reliable from unreliable self-reports.