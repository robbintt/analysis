---
ver: rpa2
title: A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond
  16 Frames
arxiv_id: '2312.07395'
source_url: https://arxiv.org/abs/2312.07395
tags:
- video
- performance
- frames
- videos
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically explores methods to scale video encoders
  to long, real-world videos (up to 4.3 minutes) while addressing memory bottlenecks.
  The key approach involves masking large portions of the video (up to 75%) during
  contrastive pre-training, which proves highly effective for memory savings and maintains
  competitive performance on benchmarks with long-range temporal dependencies (e.g.,
  YouCook2, EgoSchema).
---

# A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames

## Quick Facts
- arXiv ID: 2312.07395
- Source URL: https://arxiv.org/abs/2312.07395
- Reference count: 40
- Key outcome: Scaling video encoders to 4.3-minute videos through 75% masking during contrastive pre-training maintains competitive performance on long-range temporal tasks while significantly reducing memory consumption.

## Executive Summary
This paper presents a straightforward approach to scale video encoders beyond 16 frames for real-world videos up to 4.3 minutes. The key innovation involves masking large portions of video input (up to 75%) during contrastive pre-training, which dramatically reduces memory consumption while maintaining competitive performance on benchmarks with long-range temporal dependencies. The authors demonstrate that their video-first architecture with joint space-time attention outperforms more complex modular approaches that use large language models for video summarization and question-answering, achieving state-of-the-art results on YouCook2 and EgoSchema benchmarks.

## Method Summary
The approach employs a two-stage contrastive pre-training strategy. First, an image encoder is adapted to short videos (16 frames) using joint space-time attention and 3D patch embedding. Then, the model is further adapted to longer videos (128 frames) using 75% random input masking during pre-training on HowTo100M Summary, with most encoder parameters frozen. For downstream tasks, the video encoder is combined with a frozen pre-trained language model via cross-attention layers, enabling video-to-text generation, summarization, and question-answering. The method uses 3D convolutions for spatiotemporal tubelet embedding and a Perceiver resampler for temporal pooling.

## Key Results
- Masking up to 75% of video input during pre-training significantly reduces memory consumption while maintaining competitive performance on long-range temporal tasks
- Joint space-time attention architecture outperforms frame-level encodings on temporally rich benchmarks like YouCook2 and EgoSchema
- The simple video-first approach with 1B parameters outperforms modular methods using larger LLMs as information aggregators on long video understanding tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Masking large portions of the video during contrastive pre-training (up to 75%) is highly effective for memory savings while maintaining competitive performance on benchmarks with long-range temporal dependencies.
- Mechanism: By masking a significant portion of the video tokens, the model is forced to learn robust representations that are not overly reliant on any single frame or small subset of frames. This reduces the effective sequence length during training, thereby lowering memory consumption without significantly impacting the model's ability to understand long-range temporal dependencies.
- Core assumption: The information content in consecutive frames is highly redundant, and masking a large portion of the input does not significantly degrade the semantic information available for learning.
- Evidence anchors:
  - [abstract] "simply masking large portions of the video (up to 75%) during contrastive pre-training proves to be one of the most robust ways to scale encoders to videos up to 4.3 minutes at 1 FPS."
  - [section 6.1] "Joint space-time attention (JST) is robust against noise from masking up to 75% during pre-training."
  - [corpus] Weak evidence - no direct citations found in the corpus.

### Mechanism 2
- Claim: Video-first architectures with joint space-time attention outperform frame-level encodings on benchmarks with rich temporal dependencies.
- Mechanism: Joint space-time attention allows the model to directly capture temporal relationships between frames at each layer of the transformer, rather than aggregating temporal information only at the end via pooling. This early temporal aggregation is crucial for understanding complex temporal dynamics in videos.
- Core assumption: Temporal dependencies are distributed throughout the video and are best captured by joint space-time attention rather than late fusion of frame-level representations.
- Evidence anchors:
  - [section 6.1] "ViViT with either joint or factorized attention performs best and presents higher gains for YouCook2, the more temporally challenging benchmark."
  - [section 6.1] "Contextualizing only intra-frame dependencies coupled with late temporal fusion (IMAGE VIT) leads to inferior performance for retrieval and captioning on benchmarks with richer temporal dependencies (YouCook2, V ATEX)."
  - [corpus] Weak evidence - no direct citations found in the corpus.

### Mechanism 3
- Claim: Scaling the video encoder to longer contexts (256 frames) through further contrastive pre-training on long videos significantly improves performance on long video understanding tasks.
- Mechanism: By pre-training the encoder on longer videos, the model learns to capture long-range visual dependencies that are crucial for understanding the context and temporal dynamics in extended video sequences. This is particularly important for tasks like video summarization and question answering that require understanding the entire video.
- Core assumption: The pre-training dataset (HowTo100M Summary) contains long videos with rich temporal dependencies that are relevant to the downstream tasks.
- Evidence anchors:
  - [abstract] "Our simple approach for training long video-to-text models, which scales to 1B parameters, does not add new architectural complexity and is able to outperform the popular paradigm of using much larger LLMs as an information aggregator over segment-based information on benchmarks with long-range temporal dependencies (YouCook2, EgoSchema)."
  - [section 6.2] "Adapting SHORT VIVIT to longer contexts (LONG VIVIT) significantly improves performance and achieves the best scores across all comparison approaches."
  - [corpus] Weak evidence - no direct citations found in the corpus.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: The paper uses contrastive learning to pre-train the video encoder, which involves learning to distinguish between positive pairs (video and its corresponding caption) and negative pairs (video and unrelated captions). This helps the model learn rich visual representations that are aligned with language.
  - Quick check question: What is the difference between contrastive learning and supervised learning in the context of video representation learning?

- Concept: Attention mechanisms in transformers
  - Why needed here: The paper explores different attention mechanisms (joint space-time, factorized space-time, and frame-level) and their impact on video understanding. Understanding how these attention mechanisms work and their computational/memory implications is crucial for interpreting the results.
  - Quick check question: How does joint space-time attention differ from factorized space-time attention in terms of computational complexity and memory usage?

- Concept: Parameter-efficient fine-tuning methods
  - Why needed here: The paper explores parameter-efficient methods (MLP Adapters, LoRA) for adapting image encoders to video, which can save memory and computational resources compared to full fine-tuning.
  - Quick check question: What is the main difference between MLP Adapters and LoRA in terms of how they modify the model parameters during fine-tuning?

## Architecture Onboarding

- Component map:
  Video input -> 3D patch embedding -> Joint space-time attention ViViT backbone -> Temporal pooling -> Perceiver resampler -> Cross-attention layers with frozen LM -> Video-to-text output

- Critical path:
  1. Pre-train image encoder on image datasets
  2. Adapt image encoder to video using joint space-time attention and contrastive learning on video-text pairs
  3. Further adapt encoder to longer videos (if needed) using masking and layer freezing
  4. Fine-tune video-to-text model using frozen encoder and pre-trained LM

- Design tradeoffs:
  - Joint space-time vs. factorized attention: Joint attention captures richer temporal dependencies but is more memory-intensive
  - Masking ratio: Higher masking saves more memory but may slightly reduce performance
  - Input resolution: Higher resolution captures more detail but increases memory usage

- Failure signatures:
  - Memory overflow: Reduce masking ratio, use smaller tubelet size, or switch to factorized attention
  - Poor performance on temporally rich benchmarks: Increase masking ratio or use joint space-time attention
  - Poor performance on spatially rich benchmarks: Reduce masking ratio or use frame-level attention

- First 3 experiments:
  1. Compare joint space-time attention vs. frame-level attention on a short video benchmark (e.g., MSR-VTT) with 16 frames and 0% masking
  2. Evaluate the impact of different masking ratios (0%, 25%, 50%, 75%) on memory usage and performance on a short video benchmark
  3. Test the effect of scaling the encoder to longer contexts (e.g., 128 frames) on a long video benchmark (e.g., YouCook2)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of high input masking (up to 75%) generalize to other video understanding tasks beyond video-to-text generation, such as action recognition or video retrieval?
- Basis in paper: The authors found that high input masking is highly effective for video-to-text tasks, providing significant memory savings without substantial performance degradation. However, they only tested this approach on video summarization and question-answering benchmarks.
- Why unresolved: The study focused on video-to-text tasks and did not explore the impact of high input masking on other video understanding tasks. Different tasks may have varying sensitivity to information loss caused by masking.
- What evidence would resolve it: Systematic experiments evaluating high input masking on a range of video understanding tasks, such as action recognition, video retrieval, and video question answering, using diverse datasets and metrics.

### Open Question 2
- Question: Can the memory-efficient video-first approach be further improved by combining high input masking with other techniques, such as parameter-efficient fine-tuning or factorized attention, to achieve even greater memory savings without sacrificing performance?
- Basis in paper: The authors explored various memory-efficient techniques, including input masking, parameter-efficient fine-tuning, and factorized attention. While high input masking was found to be highly effective, the study did not investigate the potential synergies between these techniques.
- Why unresolved: The research focused on evaluating individual memory-efficient techniques in isolation. The combined effects of these techniques on memory consumption and performance remain unexplored.
- What evidence would resolve it: Experiments comparing the memory consumption and performance of different combinations of memory-efficient techniques, such as high input masking with parameter-efficient fine-tuning or factorized attention, on various video understanding tasks and datasets.

### Open Question 3
- Question: How does the performance of the proposed video-first approach compare to state-of-the-art methods that use more complex architectures or training strategies, such as multi-stage training or contrastive learning with additional objectives?
- Basis in paper: The authors demonstrated the effectiveness of their simple video-first approach on long video understanding benchmarks. However, they did not directly compare their method to more complex state-of-the-art approaches that employ advanced techniques.
- Why unresolved: The study focused on validating the effectiveness of a simple video-first approach and did not benchmark it against more sophisticated methods. It is unclear how the proposed method would fare against these advanced techniques.
- What evidence would resolve it: Direct comparisons between the proposed video-first approach and state-of-the-art methods that use complex architectures or training strategies, such as multi-stage training or contrastive learning with additional objectives, on a range of video understanding tasks and datasets.

## Limitations

- The effectiveness of 75% masking may not generalize to video domains with low temporal redundancy
- The approach hasn't been tested on videos significantly longer than 4.3 minutes, leaving scalability questions unresolved
- Reliance on synthetic LLM-generated captions for pre-training introduces potential quality variability

## Confidence

**High Confidence Claims:**
- The masking approach (up to 75%) effectively reduces memory consumption during contrastive pre-training while maintaining competitive performance
- Joint space-time attention outperforms frame-level encodings on temporally rich benchmarks
- Scaling to longer contexts (256 frames) improves performance on long video understanding tasks

**Medium Confidence Claims:**
- The proposed approach outperforms modular methods using large LLMs for information aggregation
- The two-stage contrastive pre-training strategy is robust across different video benchmarks
- The memory savings from masking don't significantly impact downstream task performance

**Low Confidence Claims:**
- The approach generalizes to videos significantly longer than 4.3 minutes without architectural modifications
- The specific masking ratio of 75% is optimal across all video types and domains
- The synthetic captions from LLMs provide equivalent quality to human-annotated captions for pre-training

## Next Checks

1. **Cross-domain robustness test**: Evaluate the 75% masking approach on diverse video domains including surveillance footage, sports videos, and user-generated content to verify generalization beyond cooking and tutorial videos.

2. **Scaling boundary analysis**: Systematically test the approach on progressively longer videos (8, 16, 32 minutes) to identify the practical limits of the masking-based memory optimization strategy and determine if new architectural modifications become necessary.

3. **Caption quality ablation**: Compare pre-training performance using human-annotated captions, high-quality synthetic captions, and lower-quality synthetic captions to quantify the impact of caption quality on the learned representations and downstream task performance.