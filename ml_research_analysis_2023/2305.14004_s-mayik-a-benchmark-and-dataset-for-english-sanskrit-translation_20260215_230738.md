---
ver: rpa2
title: "S\u0101mayik: A Benchmark and Dataset for English-Sanskrit Translation"
arxiv_id: '2305.14004'
source_url: https://arxiv.org/abs/2305.14004
tags:
- sanskrit
- sentences
- dataset
- language
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "S\u0101mayik is a dataset of 53,000 English-Sanskrit sentence\
  \ pairs focused on contemporary prose, addressing the scarcity of digitized Sanskrit\
  \ corpora in this domain. The dataset was curated from four sources: the Sanskrit\
  \ Bible, Mann Ki Baat transcripts, a Sanskrit learning book, and Spoken Tutorials."
---

# Sāmayik: A Benchmark and Dataset for English-Sanskrit Translation

## Quick Facts
- **arXiv ID:** 2305.14004
- **Source URL:** https://arxiv.org/abs/2305.14004
- **Reference count:** 10
- **Primary result:** Benchmark dataset of 53,000 English-Sanskrit sentence pairs focused on contemporary prose; IndicBART achieves 27.3 BLEU score

## Executive Summary
Sāmayik addresses the scarcity of digitized Sanskrit corpora in contemporary prose by curating a dataset of 53,000 English-Sanskrit sentence pairs from four modern sources: the Sanskrit Bible, Mann Ki Baat transcripts, a Sanskrit learning book, and Spoken Tutorials. The dataset enables improved translation quality for modern Sanskrit content compared to classical-era poetry corpora. Benchmark models adapted from multilingual pre-trained models (mBART, ByT5, IndicBART) achieve BLEU scores of 19.4, 18.8, and 27.3 respectively, with IndicBART performing best due to its pretraining on Indic languages in Devanagari script.

## Method Summary
The study curates the Sāmayik dataset by aligning parallel sentences from four contemporary sources and encoding Sanskrit in Devanagari script. Three multilingual pre-trained models (mBART-50, ByT5, IndicBART) are fine-tuned for English-Sanskrit translation using HuggingFace Transformers. Models are trained with 512-token truncation, batch size 128, AdamW optimizer, label smoothing 0.1, learning rate 1e-3, and 30 epochs. Translation quality is evaluated using BLEU and ChrF metrics, with the best-performing model achieving 27.3 BLEU and 45.7 ChrF scores.

## Key Results
- Sāmayik dataset contains 53,000 English-Sanskrit sentence pairs focused on contemporary prose
- IndicBART achieves highest BLEU score of 27.3 and ChrF of 45.7 on the dataset
- Models trained on Sāmayik show statistically significant improvements over classical-era poetry datasets for contemporary content translation
- Dataset and source code are publicly available on GitHub

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Contemporary prose domains improve translation quality for modern Sanskrit content
- **Mechanism:** The dataset contains prose written in modern times (post-1950s), covering topics like podcasts, language learning, and tutorials. These differ structurally and stylistically from classical-era poetry, which follows meter constraints and archaic vocabulary
- **Core assumption:** Models trained only on classical corpora fail to generalize to contemporary usages because linguistic patterns and word orders diverge significantly
- **Evidence anchors:**
  - [abstract] "Sanskrit is a low-resource language... Existing Sanskrit corpora... have predominantly focused on poetry and offer limited coverage of contemporary written materials"
  - [section] "While Ithihāsa consists of two epics written in poetry form, DCS consists of most of its content in poetry. On the contrary, our corpus focuses on sentences written in prose form"
  - [corpus] The corpus includes Mann Ki Baat transcripts (2014-2022) and Spoken Tutorials, indicating current topical content
- **Break condition:** If contemporary content overlaps too much with classical poetic forms in syntax or if domain vocabulary remains unchanged across eras, the improvement would diminish

### Mechanism 2
- **Claim:** IndicBART yields higher BLEU and ChrF scores because it was pre-trained on Indic language data with Devanagari script
- **Mechanism:** IndicBART's pretraining included transliterated Indic languages into Devanagari, matching Sanskrit's script. This creates shared subword vocabularies and reduces out-of-vocabulary (OOV) tokens during fine-tuning
- **Core assumption:** The morphological and orthographic similarity between Sanskrit and other Indic languages in Devanagari leads to better cross-lingual transfer
- **Evidence anchors:**
  - [abstract] "We employ mBART-50... We use the Devanagari script for encoding Sanskrit... Here, given that Sanskrit is a morphologically rich language... we believe ChrF can be indicative of capturing morpho-syntactic aspects"
  - [section] "With both MBART and IndicBART, we observe negligible OOV vocabulary subword tokens, and we observe that IndicBART currently reports the best BLEU score on our dataset with a BLEU score of 27.25"
  - [corpus] The dataset uses Devanagari script for Sanskrit, aligning with IndicBART's encoding
- **Break condition:** If Indic languages used in pretraining diverge too much from Sanskrit's morphology, or if script transliterations introduce noise, the advantage would fade

### Mechanism 3
- **Claim:** Byte-level tokenization in ByT5 reduces vocabulary mismatch for morphologically rich Sanskrit
- **Mechanism:** ByT5 tokenizes at the Unicode byte level instead of word or subword units, eliminating the need for language-specific vocabularies and handling arbitrary morphological forms
- **Core assumption:** Fixed-byte vocabularies can represent all possible Sanskrit inflections without segmentation errors that subword models might make
- **Evidence anchors:**
  - [abstract] "ByT5 is a token-free model which tokenizes inputs at the Unicode byte level and the Devanagari script for Sanskrit is part of the Unicode specifications"
  - [section] "ByT5 (Xue et al., 2022) is a token free pre-trained seq2seq model... Here, it is a token free model that uses a fixed 256 byte values in Unicode as its vocabulary"
  - [corpus] Sanskrit uses Devanagari, which is part of Unicode, so byte-level tokenization covers it
- **Break condition:** If byte-level sequences become too long for efficient modeling, or if byte-level granularity loses important linguistic features, performance may drop

## Foundational Learning

- **Concept:** Domain shift between classical poetry and contemporary prose
  - **Why needed here:** Models trained on poetry may fail on modern prose because syntax, vocabulary, and style differ; understanding this shift explains why Sāmayik improves translation
  - **Quick check question:** What are the key linguistic differences between Sanskrit poetry and prose that could impact NMT performance?

- **Concept:** Morphological richness and its impact on tokenization
  - **Why needed here:** Sanskrit's high inflectional capacity creates many word forms; choosing between subword, byte-level, or character-level tokenization affects OOV rates and model capacity
  - **Quick check question:** How does Sanskrit's inflectional morphology influence the choice of tokenizer in a multilingual NMT system?

- **Concept:** Pretraining language family alignment
  - **Why needed here:** Multilingual models perform better when target languages share pretraining corpora; IndicBART's success suggests script and linguistic proximity matter
  - **Quick check question:** Why might a model pretrained on Indic languages in Devanagari outperform one pretrained on mixed families for Sanskrit translation?

## Architecture Onboarding

- **Component map:** Four source corpora → alignment → Devanagari encoding → train/dev/test split (80/10/10) → tokenization → model fine-tuning → BLEU/ChrF evaluation → benchmark release
- **Critical path:** Data → tokenization → model fine-tuning → evaluation metrics → benchmark release
- **Design tradeoffs:**
  - Script choice: Devanagari vs SLP1 vs Roman; Devanagari matched IndicBART pretraining but may exclude non-Devanagari users
  - Tokenization: Byte-level (ByT5) vs subword (mBART/IndicBART); byte-level avoids OOV but increases sequence length
  - Corpus size: 42k pairs is modest; adding Ithihasa training data boosts performance but introduces domain shift
- **Failure signatures:**
  - High OOV tokens → tokenization mismatch
  - BLEU high but ChrF low → word match but poor morphological coverage
  - Train-test BLEU gap → overfitting or domain mismatch
- **First 3 experiments:**
  1. Fine-tune IndicBART on Sāmayik only vs with Ithihasa; compare BLEU/ChrF to isolate domain effect
  2. Compare Devanagari vs SLP1 encoding in mBART; measure OOV and translation quality
  3. Evaluate ByT5 on a held-out subset of classical poetry to see if byte-level helps cross-domain transfer

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal model architecture and training strategy for English-Sanskrit translation, particularly when combining contemporary and classical datasets?
- **Basis in paper:** [explicit] The paper mentions combining Sāmayik with the Ithihāsa classical dataset for training, and explores different pre-trained models (mBART, ByT5, IndicBART) with varying results
- **Why unresolved:** While IndicBART performs best on Sāmayik alone, the paper doesn't fully explore optimal combinations of contemporary and classical datasets, nor does it investigate hybrid architectures or curriculum learning approaches
- **What evidence would resolve it:** Systematic experiments comparing different training curricula (contemporary-first, classical-first, interleaved), architectural modifications (adapter layers, knowledge distillation), and evaluation across both contemporary and classical test sets

### Open Question 2
- **Question:** How can Hindi be effectively utilized as a bridge language for English-Sanskrit translation, and what are the limitations of this approach?
- **Basis in paper:** [explicit] The paper mentions exploring Hindi as a bridge language but provides no results or detailed analysis of this approach
- **Why unresolved:** The paper only briefly mentions this possibility without implementing or evaluating it, leaving questions about its feasibility and effectiveness unanswered
- **What evidence would resolve it:** Implementation and evaluation of Hindi-bridged translation systems, comparison with direct English-Sanskrit models, analysis of error propagation through the Hindi intermediate, and investigation of optimal Hindi corpus sizes

### Open Question 3
- **Question:** What are the key linguistic challenges in translating between English and Sanskrit, and how do different model architectures address these challenges?
- **Basis in paper:** [inferred] The paper notes Sanskrit's morphological richness and free word order, mentions using ChrF metric alongside BLEU, and observes different performance across model architectures
- **Why unresolved:** While the paper touches on linguistic differences and metric choices, it doesn't provide detailed error analysis or investigate how specific linguistic features (morphology, word order, compounding) impact translation quality
- **What evidence would resolve it:** Comprehensive error analysis categorizing translation errors by linguistic phenomenon, correlation studies between linguistic features and model performance, and ablation studies isolating the impact of different architectural choices on handling specific linguistic challenges

## Limitations

- The dataset size (53,000 sentence pairs) is relatively modest for deep learning applications, potentially limiting model robustness
- Evaluation metrics (BLEU and ChrF) may not fully capture Sanskrit's morphological richness and free word order nuances
- Comparison with classical-era poetry datasets is limited to two specific datasets without detailed performance metrics or analysis of domain shift

## Confidence

- **High Confidence:** Sāmayik addresses scarcity of contemporary Sanskrit corpora; IndicBART performs best on dataset
- **Medium Confidence:** Domain shift explanation between classical poetry and contemporary prose; IndicBART's Devanagari pretraining advantage
- **Low Confidence:** Byte-level tokenization in ByT5 reduces vocabulary mismatch for morphologically rich Sanskrit

## Next Checks

1. Fine-tune the same model (e.g., IndicBART) on Sāmayik and Ithihasa separately, then evaluate on a held-out set of contemporary prose to quantify the domain shift effect empirically
2. Compare the performance of mBART when trained on Devanagari vs SLP1 encoded Sanskrit to measure the impact of script choice on translation quality and OOV rates
3. Conduct a human evaluation study using linguistic experts to assess whether BLEU and ChrF scores correlate with actual translation quality, particularly for morphological accuracy and fluency in Sanskrit