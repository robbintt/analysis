---
ver: rpa2
title: 'CLAIR: Evaluating Image Captions with Large Language Models'
arxiv_id: '2310.12971'
source_url: https://arxiv.org/abs/2310.12971
tags:
- clair
- image
- caption
- human
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLAIR is a new method for evaluating image captions by leveraging
  large language models (LLMs). It reformulates caption evaluation as a text completion
  task, asking the LLM to rate how likely a candidate caption describes the same image
  as a reference set.
---

# CLAIR: Evaluating Image Captions with Large Language Models

## Quick Facts
- **arXiv ID**: 2310.12971
- **Source URL**: https://arxiv.org/abs/2310.12971
- **Authors**: [Not specified in source]
- **Reference count**: 39
- **Key outcome**: CLAIR achieves 39.6% relative correlation improvement over SPICE on Flickr8K-Expert and 18.3% over vision-augmented methods like RefCLIP-S.

## Executive Summary
CLAIR introduces a novel approach for evaluating image captions by leveraging large language models (LLMs) through a text completion framework. Rather than relying on visual features or complex metric calculations, CLAIR reformulates caption evaluation as a language modeling task where the LLM judges how likely a candidate caption describes the same image as reference captions. The method demonstrates strong correlation with human judgments across multiple datasets, outperforming traditional metrics like SPICE by significant margins. CLAIR also provides interpretable reasoning for its scores, offering transparency into the evaluation process. Additionally, an ensemble version (CLAIR E) further improves reliability by averaging scores from multiple LLM models.

## Method Summary
CLAIR converts image caption evaluation into a zero-shot text completion task where an LLM is prompted to rate the likelihood that a candidate caption describes the same image as a set of reference captions. The prompt asks the LLM to generate a score (0-100) along with reasoning for its judgment. The method works without requiring direct visual input, instead relying entirely on the LLM's language modeling capabilities. CLAIR E extends this by ensembling multiple LLM judgments to reduce individual model biases. The approach is evaluated on standard captioning datasets (MS-COCO, Flickr8K, PASCAL-50S) using correlation metrics with human judgments as the primary evaluation criterion.

## Key Results
- CLAIR achieves 39.6% relative correlation improvement over SPICE on Flickr8K-Expert dataset
- On Flickr8K-Expert, CLAIR shows 18.3% improvement over vision-augmented methods like RefCLIP-S
- CLAIR E closes the gap to inter-human agreement by 0.097 over vision-based measures and 0.132 over language-based measures
- The method provides interpretable reasoning for generated scores, offering transparency in evaluation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Large language models can act as effective evaluators for image captions by reformulating the task as a text completion problem
- **Mechanism**: CLAIR converts caption evaluation into a human-readable text completion task where the LLM rates how likely a candidate caption describes the same image as reference captions, leveraging zero-shot language modeling capabilities
- **Core assumption**: LLMs can accurately assess semantic relevance, visual structure, object interactions, caption diversity, and specificity through text alone
- **Evidence anchors**: CLAIR achieves 39.6% relative correlation improvement over SPICE on Flickr8K-Expert and 18.3% over RefCLIP-S; provides interpretable reasoning for scores

### Mechanism 2
- **Claim**: Ensembling multiple LLM judgments improves reliability and correlation with human preferences
- **Mechanism**: CLAIR E calculates individual scores for each model and computes an unweighted average, leveraging different distributions learned by each model
- **Core assumption**: Individual LLM biases can be mitigated through ensemble averaging, leading to stronger correlation with human judgments
- **Evidence anchors**: CLAIR E closes gap to inter-human agreement by 0.097 over vision-based measures; improvements suggest each model has some bias yet ensemble correlates more strongly

### Mechanism 3
- **Claim**: LLMs can provide interpretable reasoning for their caption evaluation scores
- **Mechanism**: CLAIR prompts the LLM to explain its reasoning behind the assigned score, achieved through prompt design that encourages explanation
- **Core assumption**: LLMs can identify and articulate underlying reasoning behind their scores, providing insights into the evaluation process
- **Evidence anchors**: CLAIR generates interpretable reasons for scores; allows language model to identify underlying reasoning behind assigned score

## Foundational Learning

- **Concept**: Zero-shot in-context learning
  - **Why needed here**: CLAIR relies on LLM's ability to perform tasks without specific training on image captioning evaluation
  - **Quick check question**: Can you explain how zero-shot learning differs from few-shot or fine-tuning approaches in the context of LLMs?

- **Concept**: Text completion tasks
  - **Why needed here**: CLAIR reformulates caption evaluation as a text completion problem, leveraging LLM's language modeling capabilities
  - **Quick check question**: What are the key components of a well-designed text completion prompt for an LLM?

- **Concept**: Ensemble methods
  - **Why needed here**: CLAIR E combines outputs of multiple LLMs to improve reliability and correlation with human preferences
  - **Quick check question**: How does ensembling multiple models help mitigate individual model biases and improve overall performance?

## Architecture Onboarding

- **Component map**: Input prompt formatter -> LLM API client -> JSON parser -> Score normalizer -> (Ensemble calculator for CLAIR E)
- **Critical path**: 1) Format candidate and reference captions into CLAIR prompt, 2) Send prompt to LLM API, 3) Parse response to extract score and reason, 4) Normalize score to 0-100 range, 5) (For CLAIR E) Average scores from multiple LLMs
- **Design tradeoffs**: Cost vs. performance (more powerful LLMs or ensembling improves performance but increases cost); determinism vs. flexibility (greedy sampling encourages reproducibility but may limit exploration); interpretability vs. simplicity (reasoning increases interpretability but complicates prompt and parsing)
- **Failure signatures**: Invalid or malformed JSON responses; consistently low/high scores across different inputs; vague or irrelevant reasoning; high variance in scores across multiple runs
- **First 3 experiments**: 1) Evaluate correlation with human judgments on Flickr8K-Expert subset, 2) Compare performance using different LLMs (GPT-3.5, Claude, PaLM) on same dataset, 3) Assess impact of ensembling multiple LLM judgments on correlation with human preferences

## Open Questions the Paper Calls Out

The paper acknowledges several open questions, particularly around the impact of LLM hallucinations on interpretability, the long-term consistency of scores as LLM APIs evolve, and the method's applicability beyond image captioning to tasks like visual storytelling or video captioning. The paper notes that while CLAIR correlates well with human judgments on semantic relevance, it doesn't specifically address grammatical correctness and stylistic coherence evaluation.

## Limitations

- Non-deterministic LLM outputs can lead to inconsistent results across evaluations
- Lack of direct visual input may miss important image-specific cues that visual metrics capture
- Cost and latency implications of using multiple LLM calls per evaluation remain under-discussed
- Potential for hallucinations in LLM explanations that could mislead interpretation

## Confidence

- **High Confidence**: CLAIR's ability to generate interpretable reasoning for its scores
- **Medium Confidence**: CLAIR's strong correlation with human judgments on Flickr8K-Expert
- **Medium Confidence**: The effectiveness of ensembling multiple LLM judgments
- **Low Confidence**: Generalizability across all image captioning datasets and domains

## Next Checks

1. **Dataset Generalization Test**: Evaluate CLAIR on additional captioning datasets beyond MS-COCO, Flickr8K, and PASCAL-50S to assess robustness across different image domains and caption styles.

2. **Ablation Study on LLM Choices**: Systematically compare CLAIR's performance using different LLM providers and model versions (GPT-4, Claude 3, PaLM 2) to quantify the impact of model selection.

3. **Human Interpretability Assessment**: Conduct user study where human evaluators assess quality and usefulness of CLAIR's generated reasoning, measuring whether explanations help understand evaluation decisions.