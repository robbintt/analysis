---
ver: rpa2
title: Duality in Multi-View Restricted Kernel Machines
arxiv_id: '2305.17251'
source_url: https://arxiv.org/abs/2305.17251
tags:
- kernel
- dual
- primal
- feature
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unifying theoretical framework for Multi-view
  Kernel Principal Component Analysis by combining existing Restricted Kernel Machine
  methods. The authors derive primal and dual formulations from a single objective
  function and show how to achieve equivalence between them by rescaling primal variables.
---

# Duality in Multi-View Restricted Kernel Machines

## Quick Facts
- arXiv ID: 2305.17251
- Source URL: https://arxiv.org/abs/2305.17251
- Reference count: 40
- Key outcome: A unifying theoretical framework for Multi-view Kernel PCA through primal-dual equivalence with four training algorithms

## Executive Summary
This paper presents a unifying theoretical framework for Multi-view Kernel Principal Component Analysis by combining existing Restricted Kernel Machine methods. The authors derive primal and dual formulations from a single objective function and show how to achieve equivalence between them by rescaling primal variables. They introduce four training algorithms depending on whether feature maps are known or not, whether they are parametric, and the ratio of data points to feature space dimensions. The theoretical claims are validated on standard time series datasets through recursive forecasting and visualization of learned features.

## Method Summary
The method constructs a unifying theoretical framework for multi-view kernel PCA by deriving primal and dual formulations from a single objective function. The key innovation is showing how to achieve full equivalence between primal and dual formulations by rescaling primal variables with the square root of the eigenvalue matrix. Four training algorithms are introduced based on whether feature maps are explicit or implicit, parametric or non-parametric, and the relationship between data points and feature space dimensions. The framework uses kernel methods to handle non-linear relationships and provides mechanisms for missing view imputation.

## Key Results
- The primal and dual formulations achieve full equivalence when primal variables are rescaled by the square root of the eigenvalue matrix
- All four training algorithms (eigenvalue decomposition and Stiefel manifold optimization for primal and dual settings) yield equivalent solutions
- The choice of algorithm should be based on the ratio of data points to feature space dimensions and whether feature maps are explicitly known
- The framework provides proper prediction performance when primal variables are properly rescaled, as demonstrated on Sum of Sines and Santa Fe time series datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The primal and dual formulations achieve full equivalence when primal variables are rescaled by the square root of the eigenvalue matrix.
- Mechanism: By introducing the constraint that primal variables satisfy U = ŨΛ^(1/2) where Λ is the diagonal eigenvalue matrix, the stationarity conditions of both formulations become identical. This rescaling ensures that the dual variables hi in the primal formulation properly account for the magnitude of the principal components.
- Core assumption: The eigenvalue matrix Λ is positive definite and the feature maps are zero-centered.
- Evidence anchors:
  - [abstract]: "We show how to achieve full equivalence in primal and dual formulations by rescaling primal variables."
  - [section 3.1]: "From the KKT conditions of (8) (see (19) and (21)), and since Γ = Λ, one can derive: Γ = Λ are diagonal."
- Break condition: If the feature maps are not properly centered or if the eigenvalue matrix contains zero or negative values, the rescaling would be undefined or lead to incorrect equivalence.

### Mechanism 2
- Claim: Training on the Stiefel manifold yields a rotational transformation of the diagonal eigenvalue matrix, which can be corrected to achieve equivalence.
- Mechanism: The Stiefel optimization produces a non-diagonal matrix Γ' = OΛO^T where O is an orthogonal matrix. By performing an additional eigendecomposition on Γ' and applying the rotation, one obtains the diagonal eigenvalue matrix Λ and achieves equivalence with the eigendecomposition approach.
- Core assumption: The solution obtained from Stiefel optimization is a rotation of the true eigenvector matrix.
- Evidence anchors:
  - [section 3.1]: "any orthonormal projection Ũ' = ŨO^T gives another optimal solution: CŨ' = Ũ'Γ' with non-diagonal Γ' = OΛO^T"
  - [section 4.1]: "Optionally, one can rotate the solution obtained by the latter algorithm to achieve full equivalence"
- Break condition: If the Stiefel optimization converges to a saddle point or local minimum that is not related by rotation to the global solution, the correction would fail.

### Mechanism 3
- Claim: The choice between primal and dual training algorithms depends on the ratio of data points to feature space dimensions and whether feature maps are explicitly known.
- Mechanism: When feature maps are known and df < n, primal algorithms are computationally efficient. When feature maps are implicit (kernel-based) or df >> n, dual algorithms are preferred. Parametric feature maps require joint training with reconstruction loss.
- Evidence anchors:
  - [section 4.1]: "Although all training algorithms yield the same solutions, they scale differently w.r.t. the number of datapoints and feature space dimensions in terms of computation cost."
  - [figure 1]: The flowchart showing algorithm selection based on n, df, explicit feature maps, and parametric status.
- Break condition: If the computational assumptions are violated (e.g., n ≈ df or parametric maps with very small parameter space), the recommended algorithm choice may not provide the expected efficiency gains.

## Foundational Learning

- Concept: Kernel trick and feature space mapping
  - Why needed here: The entire framework relies on mapping input data to high-dimensional feature spaces where linear methods can capture non-linear relationships.
  - Quick check question: What is the relationship between the kernel function k(x,y) and the feature map ϕ(x)?

- Concept: Conjugate duality and Lagrangian optimization
  - Why needed here: The framework uses Fenchel-Young inequality and Lagrangian methods to derive primal and dual formulations from a single objective.
  - Quick check question: How does introducing dual variables hi through Fenchel-Young inequality create a coupling between different views?

- Concept: Stiefel manifold optimization
  - Why needed here: The constrained optimization problems involve orthonormal matrices, which form the Stiefel manifold, requiring specialized optimization techniques.
  - Quick check question: What is the feasible set for the constraint U^T U = I in the primal formulation?

## Architecture Onboarding

- Component map: Feature maps ϕv(·) -> Transformation matrices Uv -> Dual variables hi -> Hyperparameter matrix Γ -> Training algorithms (eigendecomposition or Stiefel optimization in primal/dual settings)
- Critical path: Construct kernel/cross-covariance matrix → Perform eigendecomposition or Stiefel optimization → Apply rescaling/rotation → Use inference equations for missing view imputation
- Design tradeoffs: Explicit vs. implicit feature maps (computational efficiency vs. flexibility), primal vs. dual formulations (memory vs. computation), eigendecomposition vs. Stiefel optimization (exact solution vs. iterative approximation)
- Failure signatures: Poor prediction performance when primal variables are not rescaled, computational inefficiency when using inappropriate algorithm for data dimensions, non-convergence of Stiefel optimization
- First 3 experiments:
  1. Verify equivalence between primal and dual formulations on a simple dataset with known feature maps using eigendecomposition
  2. Test the effect of rescaling primal variables by comparing predictions with and without step 6 in Algorithm 1
  3. Compare computational efficiency of primal vs. dual algorithms on a dataset where df < n and another where df >> n

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of hyperparameter Γ affect the convergence and quality of the solutions in both primal and dual formulations?
- Basis in paper: [explicit] The paper states that "Whether it has a solution depends on the hyperparameters Γ" in Section 3.1 and mentions that Γ is automatically determined through the Lagrange multipliers.
- Why unresolved: The paper mentions that Γ is automatically determined but does not provide detailed analysis of how different choices of Γ affect the optimization process, convergence speed, or quality of solutions.
- What evidence would resolve it: Empirical studies comparing solutions with different initialization strategies for Γ, convergence rate analysis under various Γ values, and sensitivity analysis of final solutions to Γ choices.

### Open Question 2
- Question: What is the impact of using the Cayley Adam optimizer on the Stiefel manifold compared to standard eigenvalue decomposition in terms of computational efficiency and solution quality?
- Basis in paper: [explicit] The paper mentions that "one can use the Cayley Adam optimizer [13] to solve (8) or (12)" and provides a flowchart for choosing algorithms, but doesn't provide comparative analysis.
- Why unresolved: While the paper acknowledges the availability of the Stiefel manifold optimization approach, it doesn't empirically compare its performance against eigenvalue decomposition in terms of computational cost, memory usage, or solution quality.
- What evidence would resolve it: Benchmark experiments comparing runtime, memory consumption, and prediction accuracy between Stiefel optimization and eigenvalue decomposition across different problem sizes and dimensionalities.

### Open Question 3
- Question: How does the model perform when the number of data points n is much larger than the feature space dimension df (n >> df), and what algorithmic adaptations are needed?
- Basis in paper: [inferred] The flowchart in Figure 1 suggests different algorithms based on the relationship between n and df, but the paper doesn't explore the extreme case where n >> df.
- Why unresolved: The paper provides guidance on algorithm selection based on n and df relationships but doesn't investigate the specific challenges or adaptations needed when dealing with very large datasets relative to feature space dimensions.
- What evidence would resolve it: Empirical studies on large-scale datasets, analysis of computational bottlenecks, and investigation of approximation techniques or distributed optimization methods for the n >> df regime.

### Open Question 4
- Question: How can mini-batch training be implemented in the dual formulation when using kernel functions, given the need for centering the kernel matrix?
- Basis in paper: [explicit] The paper states in the conclusion that "how to enable mini-batch training in the dual formulation remains an open question."
- Why unresolved: The paper explicitly identifies this as an unresolved issue, noting that kernel matrix centering requires access to all data points, which is incompatible with standard mini-batch approaches.
- What evidence would resolve it: Development and demonstration of efficient algorithms for online or mini-batch kernel matrix centering, empirical validation of convergence properties with mini-batches, and comparison of training speed versus full-batch approaches.

## Limitations
- The framework assumes zero-centered feature maps, which may not hold for all practical applications
- Computational efficiency comparisons are based on asymptotic complexity rather than empirical runtime measurements
- The framework is validated only on time series data with linear kernels, limiting generalizability to other data types and kernel functions

## Confidence
- High: Mathematical derivation of primal-dual equivalence through variable rescaling follows directly from KKT conditions
- Medium: Computational efficiency claims depend on specific implementation details and hardware characteristics not fully explored
- Low: Stiefel manifold optimization convergence properties not guaranteed with theoretical convergence guarantees

## Next Checks
1. Test the framework with non-linear kernels (e.g., RBF) on non-time-series datasets to verify generalizability
2. Measure actual wall-clock time for each algorithm across varying n and df values to confirm theoretical efficiency claims
3. Implement multiple random initializations of the Stiefel optimizer to assess solution stability and convergence to equivalent optima