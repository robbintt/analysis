---
ver: rpa2
title: Fast and Interpretable Mortality Risk Scores for Critical Care Patients
arxiv_id: '2311.13015'
source_url: https://arxiv.org/abs/2311.13015
tags:
- groupfasterrisk
- sparsity
- risk
- features
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GroupFasterRisk, a new algorithm for creating
  interpretable mortality risk scores for ICU patients. GroupFasterRisk uses group
  sparsity and monotonicity constraints to produce diverse, high-quality risk scores
  that are as accurate as black box models but much sparser.
---

# Fast and Interpretable Mortality Risk Scores for Critical Care Patients

## Quick Facts
- arXiv ID: 2311.13015
- Source URL: https://arxiv.org/abs/2311.13015
- Reference count: 40
- Key outcome: GroupFasterRisk produces sparse, interpretable risk scores that outperform existing clinical scores (OASIS, SAPS II) and match black box models' accuracy while using at most a third of the parameters.

## Executive Summary
This paper introduces GroupFasterRisk, a novel algorithm for creating interpretable mortality risk scores for ICU patients. The method uses group sparsity and monotonicity constraints to produce diverse, high-quality risk scores that are as accurate as black box models but much sparser. Evaluated on MIMIC III and eICU datasets, GroupFasterRisk models outperformed OASIS and SAPS II scores and performed similarly to APACHE IV/IVa while using at most a third of the parameters. The approach demonstrates strong out-of-distribution generalization and provides clinicians with multiple interpretable model options.

## Method Summary
GroupFasterRisk builds interpretable mortality risk scores using sparse logistic regression with hard sparsity, group sparsity, and monotonicity constraints. The algorithm employs beam search optimization to generate diverse solutions and a multiplier-based rounding technique to produce integer coefficients. Models are trained on 49 features including vital signs, lab measurements, and comorbidities collected during the first 24 hours of ICU admission. Performance is evaluated using AUROC, AUPRC, and calibration metrics across MIMIC III and eICU datasets, with out-of-distribution testing to assess generalization.

## Key Results
- GroupFasterRisk models use at most 82 parameters while achieving performance comparable to black box models
- Outperformed OASIS and SAPS II scores on all-cause mortality prediction
- For specific disease cohorts (sepsis, AMI, heart failure, AKI), models outperformed OASIS and SOFA while using at most 50 parameters
- Demonstrated strong out-of-distribution generalization from MIMIC III to eICU dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GroupFasterRisk achieves similar predictive performance to black box models while being orders of magnitude sparser.
- Mechanism: By using hard sparsity constraints, group sparsity, and monotonicity correction during optimization, GroupFasterRisk learns compact models that retain most predictive information while discarding redundant features. The integer-coefficient risk scores maintain accuracy through careful multiplier-based rounding rather than naive rounding.
- Core assumption: The most predictive features can be identified and combined in a sparse additive model without significant loss of accuracy compared to highly complex models.
- Evidence anchors:
  - [abstract] "GroupFasterRisk models performed better than risk scores currently used in hospitals, and on par with black box ML models, while being orders of magnitude sparser."
  - [section] "GroupFasterRisk consistently achieve the best sparsity and high AUROC and AUPRC... GroupFasterRisk models use at most 82 parameters."
- Break condition: If the relationship between features is highly non-additive or involves complex interactions that cannot be captured by step functions, sparsity could hurt accuracy significantly.

### Mechanism 2
- Claim: GroupFasterRisk generalizes well across different hospitals and patient subpopulations.
- Mechanism: Training on large, diverse datasets (MIMIC III and eICU) with out-of-distribution testing ensures the learned risk scores are not overfit to a single center's practices. The use of group sparsity enforces cohesive models that capture shared signal across related features.
- Core assumption: The underlying physiological relationships driving mortality risk are consistent enough across hospitals and subpopulations that a model trained on one can generalize to others.
- Evidence anchors:
  - [abstract] "Through extensive out-of-distribution (OOD) testing... we demonstrate that risk scores created by GroupFasterRisk outperform OASIS and SAPS II and are similar in performance to APACHE IV/IVa while using much fewer variables."
  - [section] "Our OOD evaluation demonstrates that GroupFasterRisk can generalize well when trained on MIMIC III and tested on eICU for all-cause and disease-specific cohorts."
- Break condition: If there are systematic differences in measurement practices, patient demographics, or disease patterns between training and deployment sites that are not captured in the features.

### Mechanism 3
- Claim: GroupFasterRisk allows design flexibility that enables practical model creation and trust.
- Mechanism: By producing multiple diverse, equally accurate risk score models and allowing monotonicity constraints, GroupFasterRisk gives domain experts options to select models that align with medical knowledge and practical constraints. The integer coefficients and small number of features make the models interpretable and easy to troubleshoot.
- Core assumption: Medical practitioners value having multiple model options and the ability to incorporate domain knowledge through constraints, which outweighs the potential slight loss in accuracy from these constraints.
- Evidence anchors:
  - [abstract] "GroupFasterRisk produces a variety of risk scores, it allows design flexibility - the key enabler of practical model creation."
  - [section] "GroupFasterRisk provides the option of a monotonic correction so that the risk score is forced to increase (or decrease) along a variable. This allows users to better align the modeling process with domain knowledge."
- Break condition: If the additional complexity of managing multiple model options and constraints outweighs the benefits of increased interpretability and trust.

## Foundational Learning

- Concept: Logistic regression and its optimization
  - Why needed here: GroupFasterRisk is built on sparse logistic regression, and understanding its loss function and constraints is key to understanding how the algorithm works.
  - Quick check question: What is the logistic loss function that GroupFasterRisk minimizes, and how does it differ from squared error loss?

- Concept: Sparsity and ℓ0 regularization
  - Why needed here: The hard sparsity constraint (ℓ0) is a core feature of GroupFasterRisk that limits the number of features used. Understanding why this is NP-hard and how GroupFasterRisk approximates it is important.
  - Quick check question: Why is solving for the optimal sparse logistic regression model with ℓ0 constraint NP-hard, and how does GroupFasterRisk approximate the solution?

- Concept: Group sparsity and its benefits
  - Why needed here: Group sparsity is a key innovation in GroupFasterRisk that allows cohesive models by grouping related features. Understanding how it works and why it's useful is important.
  - Quick check question: How does group sparsity differ from individual feature sparsity, and what are the benefits of using group sparsity in risk score models?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Feature selection and binarization -> Core algorithm optimization with sparsity constraints -> Multiplier-based rounding for integer coefficients -> Output diverse risk score models

- Critical path: Data preprocessing → Core algorithm optimization → Post-processing (rounding, filtering) → Output models

- Design tradeoffs:
  - Sparsity vs. Accuracy: More features generally improve accuracy but reduce interpretability. GroupFasterRisk allows controlling this tradeoff.
  - Diversity vs. Computation: Generating multiple diverse models increases options but also computation time. The beam search and rounding subroutines are optimized for efficiency.
  - Monotonicity vs. Flexibility: Monotonicity constraints can improve interpretability and domain alignment but may slightly reduce accuracy if the true relationship is non-monotonic.

- Failure signatures:
  - Poor out-of-distribution performance: May indicate overfitting to training data or differences in measurement practices between sites.
  - Unexpected risk score patterns: Could indicate issues with monotonicity constraints, feature binarization, or the rounding subroutine.
  - Slow computation: May indicate issues with the beam search or rounding subroutines, or overly complex feature sets.

- First 3 experiments:
  1. Train GroupFasterRisk on MIMIC III with default hyperparameters and evaluate on eICU. Compare performance to OASIS and SAPS II.
  2. Train GroupFasterRisk with and without monotonicity constraints on a subset of features. Compare performance and risk score patterns.
  3. Generate multiple diverse models with GroupFasterRisk and have a domain expert select the most interpretable one. Compare this to the highest accuracy model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would GroupFasterRisk perform on datasets with substantial temporal drift or non-stationary patterns over time?
- Basis in paper: [inferred] The paper acknowledges that temporal drift could impact model predictive accuracy but notes limitations in evaluating this due to de-identified data that keeps admission dates private.
- Why unresolved: The current evaluation is limited to MIMIC III (2001-2012) and eICU (2014-2015) datasets, preventing assessment of model performance on more recent data or across different time periods.
- What evidence would resolve it: Evaluating GroupFasterRisk on datasets with clear temporal markers and explicit time-based train/test splits would demonstrate whether the model maintains performance as patterns shift over time.

### Open Question 2
- Question: What is the impact of data collection methodology on GroupFasterRisk's performance, particularly for time-series measurements?
- Basis in paper: [explicit] The paper discusses limitations related to how MIMIC III and eICU collect time series measurements, noting that vital signs in eICU are first recorded as one-minute averages and then stored as five-minute medians.
- Why unresolved: The study does not directly investigate how variations in data collection and aggregation methods affect model performance or reliability.
- What evidence would resolve it: Comparative analysis of GroupFasterRisk performance using raw vs. aggregated time-series data, or data collected using different methodologies, would reveal the sensitivity of the model to data processing choices.

### Open Question 3
- Question: How does the choice of bin width for continuous variable discretization affect GroupFasterRisk's predictive accuracy and interpretability?
- Basis in paper: [explicit] The paper presents results showing GroupFasterRisk performance under various bin widths (100, 50, 20, 10, 5, 4) but does not provide a definitive recommendation for optimal bin width selection.
- Why unresolved: While the paper shows performance varies with bin width, it does not establish guidelines for selecting bin width based on dataset characteristics or desired model properties.
- What evidence would resolve it: Systematic evaluation of GroupFasterRisk across diverse datasets with varying characteristics (feature distributions, sample sizes, noise levels) would identify optimal bin width selection strategies.

## Limitations

- The exact implementation details of key components like beam search and multiplier-based rounding are not fully specified, limiting reproducibility.
- The paper does not investigate how data collection methodology differences between MIMIC III and eICU may affect model performance.
- Limited evaluation of temporal drift and model performance on more recent data due to de-identification practices in the source datasets.

## Confidence

- Performance claims vs black box models: Medium (strong quantitative results but limited methodological transparency)
- OOD generalization claims: Medium (results show promise but potential dataset differences not fully explored)
- Interpretability claims: High (clear evidence of sparse, integer-coefficient models with visual explanations)

## Next Checks

1. Replicate the feature preprocessing pipeline on MIMIC III and eICU datasets, documenting all binarization thresholds and quantile splits used
2. Implement the beam search and multiplier-based rounding subroutines to verify the claimed computational efficiency and model diversity
3. Conduct a prospective validation study where clinicians use GroupFasterRisk models in simulated clinical scenarios to assess practical interpretability and decision-making impact