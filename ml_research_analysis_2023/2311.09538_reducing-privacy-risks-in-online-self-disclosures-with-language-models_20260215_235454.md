---
ver: rpa2
title: Reducing Privacy Risks in Online Self-Disclosures with Language Models
arxiv_id: '2311.09538'
source_url: https://arxiv.org/abs/2311.09538
tags:
- span
- disclosure
- sentence
- privacy
- self-disclosure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses privacy risks in online self-disclosure by
  developing a fine-grained corpus with 19 self-disclosure categories and creating
  language models to identify and abstract disclosures. The authors first annotate
  4.8K spans across 2.4K Reddit posts, then fine-tune RoBERTa-large for detection,
  achieving over 75% token-level F1.
---

# Reducing Privacy Risks in Online Self-Disclosures with Language Models

## Quick Facts
- arXiv ID: 2311.09538
- Source URL: https://arxiv.org/abs/2311.09538
- Reference count: 40
- Primary result: Fine-tuned language models can abstract self-disclosures with 3.1/5 privacy reduction while maintaining 3.8/5 utility

## Executive Summary
This paper addresses privacy risks in online self-disclosure by developing a fine-grained corpus with 19 self-disclosure categories and creating language models to identify and abstract disclosures. The authors first annotate 4.8K spans across 2.4K Reddit posts, then fine-tune RoBERTa-large for detection, achieving over 75% token-level F1. An HCI study with 21 Reddit users shows 82% positive reception. The authors then introduce self-disclosure abstraction—generating less specific alternatives that preserve meaning—and experiment with multiple fine-tuning strategies using Llama-2 7B. Their best iterative training model with thought and instruction format produces diverse abstractions that moderately reduce privacy (3.1/5) while preserving high utility (3.8/5).

## Method Summary
The authors develop a span-level annotation scheme for self-disclosures, create detection and abstraction models, and evaluate through human studies. They first collect Reddit posts and annotate 4.8K disclosure spans with 19 categories. They fine-tune RoBERTa-large for detection achieving >75% F1. They then generate abstraction data using GPT-4 and fine-tune Llama-2 7B with various strategies including Chain-of-Thoughts. Human evaluation assesses privacy reduction, utility preservation, and diversity across different abstraction approaches.

## Key Results
- Detection model achieves >75% token-level F1 for identifying disclosure spans
- HCI study shows 82% of participants viewed the model positively
- Best abstraction model achieves 3.1/5 privacy reduction and 3.8/5 utility preservation
- Chain-of-Thoughts training with thought and instruction format yields highest diversity (4.6/5)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained span-level annotation enables precise privacy risk detection.
- Mechanism: By annotating exact text spans rather than full sentences, the model can identify specific disclosures (e.g., "16F" or "I live in the UK") rather than over-generalizing to the whole post. This granularity supports actionable privacy interventions.
- Core assumption: Users can and will act on word-level privacy alerts if they are shown.
- Evidence anchors:
  - [abstract] "we develop a taxonomy of 19 self-disclosure categories and curate a large corpus consisting of 4.8K annotated disclosure spans."
  - [section 3] "we instruct them to select spans with contextual information... to label them under the general category."
  - [corpus] The corpus contains 4.8K spans across 2.4K posts, providing dense span coverage for training.
- Break condition: If users ignore or cannot understand word-level prompts, the benefit of fine-grained detection collapses.

### Mechanism 2
- Claim: Iterative training with thought and instruction formats improves generation diversity and quality.
- Mechanism: The model first reasons (thought) about why a span needs abstraction, then generates diverse rephrasings. This Chain-of-Thoughts training yields more varied and contextually appropriate alternatives.
- Core assumption: Reasoning steps improve generation quality in text-to-text tasks.
- Evidence anchors:
  - [abstract] "Our best model can generate diverse abstractions that moderately reduce privacy risks while maintaining high utility according to human evaluation."
  - [section 6] "Training with special token and zero-shot instruction input-formats lead to top performance."
  - [corpus] GPT-4-generated abstractions serve as high-quality training data for diversity.
- Break condition: If reasoning steps introduce noise or slow generation, model efficiency may suffer.

### Mechanism 3
- Claim: User study feedback drives task definition and model design.
- Mechanism: Real user interviews reveal that users want both detection and actionable abstraction, not just risk flagging. This insight motivates the abstraction task and diversity requirement.
- Core assumption: End-user feedback is critical for privacy tool adoption.
- Evidence anchors:
  - [abstract] "82% of participants viewing the model positively, highlighting its real-world applicability."
  - [section 5.1] "Participants generally viewed the model favorably, with 62% expressing a desire to use it on their own posts."
  - [corpus] The user study data (not in corpus) validates real-world need.
- Break condition: If abstraction quality lags user expectations, trust in the system erodes.

## Foundational Learning

- Concept: Span-level sequence tagging
  - Why needed here: Enables detection of exact disclosure boundaries rather than sentence-level classification.
  - Quick check question: How would you label the span "16F" in "Im 16F I think I want to be a bi M"?
    - Answer: AGE/GENDER

- Concept: IOB2 format for multi-label tagging
  - Why needed here: Distinguishes beginning vs inside tokens of spans and handles overlapping categories.
  - Quick check question: In IOB2, how do you label "I live in the UK" if only "UK" is LOCATION?
    - Answer: O O O B-LOCATION

- Concept: Chain-of-Thoughts (CoT) fine-tuning
  - Why needed here: Encourages the model to generate rationales before abstraction, improving diversity and quality.
  - Quick check question: What are the two output formats explored for CoT training?
    - Answer: Only abstraction span vs. span + rationale

## Architecture Onboarding

- Component map:
  Data pipeline: Reddit post collection → span annotation → split into train/val/test → RoBERTa detection training → GPT-4 abstraction generation → Llama-2 fine-tuning → human evaluation

- Critical path:
  1. Annotate spans and categories
  2. Train detection model (RoBERTa)
  3. Generate abstraction data with GPT-4
  4. Fine-tune abstraction model (Llama-2)
  5. Human evaluation of abstractions

- Design tradeoffs:
  - Span vs. sentence annotation: Span offers precision but requires more annotation effort.
  - Multi-label vs. binary detection: Multi-label captures nuance but increases model complexity.
  - Single vs. multiple abstractions: Single is simpler; multiple supports user choice but requires more generation.

- Failure signatures:
  - Detection: High false positives → users see over-sensitive highlights.
  - Abstraction: Low diversity → limited user choice; poor coherence → nonsensical rephrasings.

- First 3 experiments:
  1. Test RoBERTa variants on span detection with different chunk sizes (64/128/256 tokens).
  2. Compare one-span vs. three-span abstraction generation quality.
  3. Evaluate impact of CoT fine-tuning on abstraction diversity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model handle cultural and linguistic variations in self-disclosure practices across different social media platforms?
- Basis in paper: [inferred] The paper mentions expanding research to other social platforms like YouTube comments.
- Why unresolved: The current study is limited to Reddit data, and it is unclear how the model would perform on other platforms with different user demographics and communication styles.
- What evidence would resolve it: Testing the model on self-disclosure data from various social media platforms and comparing its performance and user reception across these platforms.

### Open Question 2
- Question: Can the self-disclosure abstraction model be adapted to provide real-time suggestions for users as they type their posts, and how would this impact user experience and privacy protection?
- Basis in paper: [explicit] The paper discusses the need for a tool that can "rewrite disclosure for me in a way that I don't worry about privacy concern."
- Why unresolved: The current model operates by abstracting user-provided instances, and it is not clear how it would function in a real-time, interactive setting.
- What evidence would resolve it: Developing and testing a real-time self-disclosure abstraction tool and evaluating its effectiveness in protecting privacy and user satisfaction.

### Open Question 3
- Question: How does the level of abstraction affect the perceived authenticity and engagement of the user's message, and is there an optimal balance between privacy protection and message utility?
- Basis in paper: [explicit] The paper introduces the task of self-disclosure abstraction to rephrase disclosures with less specific details while preserving the content utility.
- Why unresolved: The paper does not explore the impact of different levels of abstraction on user engagement and the perceived authenticity of the message.
- What evidence would resolve it: Conducting user studies to assess the impact of varying levels of abstraction on message engagement and authenticity, and identifying an optimal balance between privacy protection and utility preservation.

## Limitations
- Limited to Reddit data, raising questions about generalization to other platforms
- Privacy reduction metrics rely on subjective human judgment rather than objective measures
- Small user study (21 participants) may not represent broader population

## Confidence
- **High Confidence** in span-level detection performance (75%+ token-level F1) due to clear evaluation methodology and established RoBERTa baseline.
- **Medium Confidence** in abstraction quality improvements from Chain-of-Thoughts training, though dependent on GPT-4-generated training data quality.
- **Low Confidence** in real-world deployment effectiveness, as the user study represents limited sample size and controlled conditions.

## Next Checks
1. Cross-platform validation: Test the abstraction model on self-disclosures from Twitter, Facebook, and other platforms to assess generalization beyond Reddit's linguistic patterns and disclosure norms.
2. Longitudinal user study: Deploy the tool with 50+ users over 30+ days to measure actual privacy risk reduction in practice versus perceived reduction in controlled studies.
3. Objective privacy impact measurement: Develop automated metrics to quantify information leakage reduction (e.g., identifying personally identifiable information before/after abstraction) to complement subjective human ratings.