---
ver: rpa2
title: 'InstaTune: Instantaneous Neural Architecture Search During Fine-Tuning'
arxiv_id: '2308.15609'
source_url: https://arxiv.org/abs/2308.15609
tags:
- search
- fine-tuning
- super-network
- instatune
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InstaTune introduces a novel approach to neural architecture search
  (NAS) by leveraging pre-trained models and performing NAS during the fine-tuning
  stage. Traditional NAS methods often require training super-networks from scratch,
  which is computationally expensive and time-consuming.
---

# InstaTune: Instantaneous Neural Architecture Search During Fine-Tuning

## Quick Facts
- arXiv ID: 2308.15609
- Source URL: https://arxiv.org/abs/2308.15609
- Reference count: 26
- Key outcome: Generates sub-networks with up to 53.62% fewer MACs while maintaining near-baseline accuracy

## Executive Summary
InstaTune introduces a novel approach to neural architecture search (NAS) that leverages pre-trained models and performs NAS during the fine-tuning stage. Unlike traditional NAS methods that require training super-networks from scratch, InstaTune converts existing pre-trained models into elastic super-networks during fine-tuning, eliminating the need for pre-training super-networks. The method involves making existing layers elastic at various model dimensions, allowing for the generation of multiple sub-networks optimized for specific hardware constraints and target tasks.

## Method Summary
InstaTune addresses the computational expense of traditional NAS by converting pre-trained models into elastic super-networks during fine-tuning. The approach involves modifying existing layers to be elastic across dimensions like number of heads, intermediate MLP size, and number of layers. A strong teacher model guides the fine-tuning process through knowledge distillation. After elastic fine-tuning, a multi-objective evolutionary search algorithm finds Pareto-optimal sub-networks that balance accuracy and computational efficiency (MACs).

## Key Results
- Achieves up to 53.62% reduction in MACs while maintaining near-baseline accuracy
- Demonstrates effectiveness across ViT, BERT, and BEiT-3 transformer architectures
- Shows consistent performance improvements across both unimodal and multi-modal tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: InstaTune makes existing pre-trained layers elastic to create a super-network without adding new layers.
- Mechanism: The method adjusts layer dimensions (e.g., number of heads, intermediate MLP size, number of layers) dynamically during fine-tuning, enabling sub-network extraction tailored to hardware constraints.
- Core assumption: Pre-trained weights can be reused as initialization for an elastic architecture without degrading performance.
- Evidence anchors:
  - [abstract] "InstaTune introduces a novel approach to neural architecture search (NAS) by leveraging pre-trained models and performing NAS during the fine-tuning stage."
  - [section] "InstaTune converts any off-the-shelf pre-trained model into a super-network during fine-tuning without adding any additional layer(s)."
- Break condition: If elastic modifications destabilize the pre-trained feature space, performance may degrade instead of improving.

### Mechanism 2
- Claim: Elastic fine-tuning with a strong teacher accelerates convergence of the super-network and its sub-networks.
- Mechanism: A fully fine-tuned teacher model is used to distill knowledge into the elastic super-network and sampled sub-networks during early fine-tuning epochs, speeding up adaptation to downstream tasks.
- Core assumption: The strong teacher's knowledge is transferable to both the full super-network and its sub-networks.
- Evidence anchors:
  - [section] "To focus on the reduced compute budget, we leverage only the elastic super-network as the teacher to distillation and assume γ = 0 throughout the fine-tuning."
  - [section] "However, to adaptively help the elastic super-network yield higher accuracy, we may allow the ΦT to be present during only initial phase of fine-tuning, instead of keeping it active throughout."
- Break condition: If the teacher model's capacity far exceeds the sub-networks, distillation may be ineffective.

### Mechanism 3
- Claim: Multi-objective evolutionary search finds Pareto-optimal sub-networks that trade off accuracy and MACs.
- Mechanism: After elastic fine-tuning, a lightweight iterative NAS (LINAS) algorithm evaluates candidate sub-networks across accuracy and MACs objectives to identify optimal trade-offs.
- Core assumption: A small number of well-trained predictors can accurately estimate sub-network performance without full training.
- Evidence anchors:
  - [abstract] "By using multi-objective evolutionary search algorithms along with lightly trained predictors, InstaTune finds Pareto-optimal sub-networks that outperform their respective baselines across different performance objectives such as accuracy and MACs."
  - [section] "Once the elastic super-network is trained, we use a lightweight iterative NAS (LINAS) to evaluate the multi-objective Pareto frontier."
- Break condition: If predictor accuracy is poor, the evolutionary search may select sub-optimal architectures.

## Foundational Learning

- Concept: Neural Architecture Search (NAS)
  - Why needed here: InstaTune is a NAS method that modifies existing models instead of training from scratch.
  - Quick check question: What is the main computational bottleneck in traditional NAS methods?

- Concept: Knowledge Distillation
  - Why needed here: The strong teacher model provides supervision to guide elastic fine-tuning and improve sub-network accuracy.
  - Quick check question: How does knowledge distillation help sub-networks converge faster than training from scratch?

- Concept: Pareto Front Analysis
  - Why needed here: InstaTune balances multiple objectives (accuracy, MACs) and needs to identify optimal trade-offs.
  - Quick check question: Why is a Pareto frontier useful when selecting sub-networks for different hardware constraints?

## Architecture Onboarding

- Component map: Pre-trained model -> Elastic layer wrappers -> Loss function (cross-entropy + distillation) -> Multi-objective evolutionary search (LINAS) -> Predictor network -> Sub-networks

- Critical path:
  1. Load pre-trained model
  2. Wrap layers to make them elastic
  3. Fine-tune with distillation losses
  4. Run multi-objective search
  5. Extract sub-networks

- Design tradeoffs:
  - Larger search space → more sub-network options but longer search time
  - Stronger teacher → faster convergence but higher fine-tuning cost
  - More elastic dimensions → greater flexibility but risk of instability

- Failure signatures:
  - Accuracy drops after making layers elastic (weight destabilization)
  - Sub-networks fail to converge during fine-tuning (inadequate teacher guidance)
  - Search returns trivial or suboptimal sub-networks (poor predictor accuracy)

- First 3 experiments:
  1. Apply InstaTune to a small ViT model on CIFAR-10 and verify MACs reduction without accuracy loss.
  2. Compare convergence speed with and without a strong teacher during elastic fine-tuning.
  3. Vary the search space size and measure impact on Pareto frontier quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of the strong teacher (ΦT) impact the convergence and final accuracy of the sub-networks found by InstaTune?
- Basis in paper: [explicit] The paper discusses the impact of using a strong teacher during elastic fine-tuning, showing that it can expedite convergence and improve accuracy. However, it also mentions that the impact reduces as fine-tuning progresses.
- Why unresolved: The paper does not provide a detailed analysis of how different choices of strong teachers (e.g., different architectures or training strategies) affect the results.
- What evidence would resolve it: Systematic experiments comparing InstaTune's performance with different strong teachers, including their architectures, training objectives, and fine-tuning strategies.

### Open Question 2
- Question: How does the size of the search space impact the quality and diversity of the sub-networks found by InstaTune?
- Basis in paper: [explicit] The paper demonstrates that increasing the search space leads to more sub-networks with lower MACs, but also mentions a minimal drop in accuracy at higher MACs regimes.
- Why unresolved: The paper does not provide a comprehensive analysis of the trade-offs between search space size, sub-network quality, and diversity.
- What evidence would resolve it: Experiments systematically varying the search space size and analyzing the resulting sub-networks' quality, diversity, and performance across different hardware constraints.

### Open Question 3
- Question: How does InstaTune perform on other downstream tasks and model architectures beyond those evaluated in the paper?
- Basis in paper: [inferred] The paper demonstrates InstaTune's effectiveness on ViT, BERT, and BEiT-3 architectures for image classification and sentiment analysis tasks.
- Why unresolved: The paper does not explore InstaTune's applicability to a wider range of tasks (e.g., object detection, segmentation) and model architectures (e.g., convolutional networks, graph neural networks).
- What evidence would resolve it: Experiments evaluating InstaTune on diverse downstream tasks and model architectures, including those not covered in the paper.

## Limitations
- The approach's effectiveness heavily depends on the quality of pre-trained models and the ability to make existing layers truly elastic without destabilizing learned representations
- Long-term stability of elastic modifications across extended training remains unproven
- Generalization to non-transformer architectures needs further validation

## Confidence
- **High Confidence**: The core concept of leveraging pre-trained models for efficient NAS during fine-tuning is well-established and technically sound
- **Medium Confidence**: The empirical results showing up to 53.62% MACs reduction while maintaining accuracy are promising but need independent validation across diverse architectures
- **Low Confidence**: The long-term stability of elastic modifications across extended training and their generalization to non-transformer architectures remains unproven

## Next Checks
1. Test InstaTune's stability when applied to pre-trained models that were only trained on limited data domains
2. Evaluate whether the Pareto-optimal sub-networks maintain their performance advantage after 2-3× more fine-tuning epochs
3. Apply the method to convolutional architectures (e.g., ResNet variants) to verify cross-architecture applicability beyond transformers