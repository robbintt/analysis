---
ver: rpa2
title: Variational Inference for SDEs Driven by Fractional Noise
arxiv_id: '2310.12975'
source_url: https://arxiv.org/abs/2310.12975
tags:
- variational
- fractional
- type
- where
- processes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a variational inference framework for stochastic
  differential equations (SDEs) driven by fractional Brownian motion (fBM), extending
  traditional SDE models to capture long-term dependencies. The authors leverage a
  Markov approximation of fBM, representing it as a linear combination of Ornstein-Uhlenbeck
  processes, enabling tractable inference using established SDE tools like Girsanov's
  theorem.
---

# Variational Inference for SDEs Driven by Fractional Noise

## Quick Facts
- arXiv ID: 2310.12975
- Source URL: https://arxiv.org/abs/2310.12975
- Reference count: 40
- Primary result: Introduces variational inference framework for SDEs driven by fractional Brownian motion using Markov approximation

## Executive Summary
This paper addresses the challenge of variational inference for stochastic differential equations driven by fractional Brownian motion (fBM), which captures long-term dependencies in continuous-time dynamic systems. The authors develop a Markov approximation of fBM using a linear combination of Ornstein-Uhlenbeck processes, enabling tractable inference using established SDE tools like Girsanov's theorem. The framework is applied to neural-SDEs and demonstrates superior performance on synthetic data including fractional Ornstein-Uhlenbeck bridges and video prediction tasks.

## Method Summary
The method builds on Markov approximation of fBM by representing it as a linear combination of Ornstein-Uhlenbeck processes driven by the same Wiener noise. This converts the non-Markovian fBMSDE into a Markovian SDE system. The authors derive evidence lower bounds (ELBO) for variational inference using Girsanov's change of measure theorem on the augmented Markovian system. They propose closed-form expressions to optimize the combination coefficients and apply the framework to neural-SDEs where drift, diffusion, and control terms are modeled by neural networks, with the Hurst index optimized during training.

## Key Results
- Demonstrates superior performance on synthetic fractional Ornstein-Uhlenbeck bridges with accurate posterior variance recovery
- Achieves competitive results on latent video prediction task using Stochastic Moving MNIST
- Shows effective optimization of Hurst index during training improves ELBO and reconstruction quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Markov approximation of fBM enables tractable VI by converting a non-Markovian SDE into a Markovian one
- **Mechanism**: fBM is represented as a linear combination of Ornstein-Uhlenbeck processes, preserving long-range dependencies while making the process Markovian
- **Core assumption**: Strong approximation of fBM by finite OU processes converges sufficiently fast for practical use
- **Evidence anchors**: Abstract mentions Markov approximation derivation of ELBO; section defines MA-fBMSDE using OU process approximation
- **Break condition**: If K is too small relative to desired accuracy, approximation error degrades inference quality

### Mechanism 2
- **Claim**: Girsanov's theorem can be applied to Markov-approximate fBMSDE to derive ELBO
- **Mechanism**: Augmented SDE system becomes Markovian, allowing Girsanov's theorem to compute KL divergences and derive ELBO
- **Core assumption**: Control term can be parameterized by neural networks and optimized via gradient descent
- **Evidence anchors**: Abstract references Girsanov-based ELBO derivation; section applies Girsanov theorem to posterior measure
- **Break condition**: Poor parameterization of control term leads to inadequate posterior approximation and high KL divergence

### Mechanism 3
- **Claim**: Optimizing combination coefficients ωk in closed form improves approximation quality
- **Mechanism**: L2-error between approximate and true fBM is minimized analytically by solving a linear system
- **Core assumption**: Quadratic form of L2-error allows tractable analytical optimization of ωk
- **Evidence anchors**: Section provides optimal ω for approximation and shows error diminishes with increasing K
- **Break condition**: Poor choice of time horizon T relative to data sequence length affects optimization quality

## Foundational Learning

- **Concept**: Fractional Brownian motion and its properties (self-similarity, long-range dependence, Hurst index)
  - **Why needed here**: Essential to understand why fBM extends BM and why Markov approximation is necessary for tractable inference
  - **Quick check question**: What happens to increments of fBM when H > 1/2 versus H < 1/2?

- **Concept**: Markov approximation of fBM using Ornstein-Uhlenbeck processes
  - **Why needed here**: This approximation converts non-Markovian fBMSDE into tractable Markovian SDE using standard inference tools
  - **Quick check question**: How does linear combination of OU processes approximate the fractional kernel?

- **Concept**: Girsanov's theorem and its application to SDEs
  - **Why needed here**: Mathematical foundation for deriving ELBO in variational inference for SDEs
  - **Quick check question**: What does Girsanov's theorem tell us about relationship between prior and posterior measures?

## Architecture Onboarding

- **Component map**: OU processes generation -> Neural network parameterization of SDE components -> SDE solver integration -> ELBO computation -> Backpropagation to update parameters
- **Critical path**: Generate OU processes → Parameterize SDE components with neural networks → Integrate SDE with solver → Compute ELBO → Backpropagate gradients
- **Design tradeoffs**: More OU processes (larger K) improves approximation but increases computational cost; time horizon T choice affects optimization quality; Stratonovich interpretation allows higher-order solvers but requires careful handling of state-dependent diffusions
- **Failure signatures**: High ELBO loss indicates poor posterior approximation; training divergence suggests SDE solver instability or poor control term parameterization; poor reconstruction quality in video prediction indicates inadequate data dynamics capture
- **First 3 experiments**:
  1. Implement Markov approximation with K=5 OU processes and verify approximation error decreases with increasing K
  2. Test Girsanov-based ELBO computation on simple linear SDE with known analytical solution
  3. Apply full variational inference framework to fractional Ornstein-Uhlenbeck bridge problem and compare learned posterior variance to analytical solution

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do convergence rates of Markov approximation for fBM compare to sparse Gaussian process approximations like those in Tong et al. (2022)?
- **Basis in paper**: Explicitly contrasts approach with sparse Gaussian process approximations, noting different requirements for inducing variables
- **Why unresolved**: Provides theoretical convergence bound but no direct comparison to sparse Gaussian process convergence rates
- **What evidence would resolve it**: Empirical studies comparing approximation accuracy and computational efficiency across varying Hurst indices and time horizons

### Open Question 2
- **Question**: What is the impact of optimizing the Hurst index on generalization performance of neural-SDE models in real-world applications beyond synthetic data?
- **Basis in paper**: Demonstrates ability to optimize Hurst index during training on synthetic data with improved ELBO and PSNR
- **Why unresolved**: Experiments limited to synthetic datasets; real-world datasets with complex temporal dependencies not explored
- **What evidence would resolve it**: Application to real-world time series data (financial markets, weather patterns) and comparison with models not optimizing Hurst index

### Open Question 3
- **Question**: How does choice of number of Ornstein-Uhlenbeck processes (K) affect trade-off between approximation accuracy and computational cost in practical applications?
- **Basis in paper**: Investigates effect of K on approximation error and runtime, showing diminishing returns for increasing K
- **Why unresolved**: Study focuses on synthetic data without exploring impact of K on model performance in complex tasks or real-world applications
- **What evidence would resolve it**: Systematic experiments varying K across different application domains measuring both approximation accuracy and computational efficiency

## Limitations
- Convergence properties of Markov approximation not fully established for practical values of K and H
- Numerical stability of closed-form ωk optimization not comprehensively tested across parameter regimes
- Validation limited to synthetic datasets with unclear generalizability to real-world applications

## Confidence
- **High confidence**: Markov approximation mechanism and implementation details (K OU processes, γk spacing, Stratonovich interpretation)
- **Medium confidence**: Girsanov-based ELBO derivation and applicability to augmented SDE system
- **Medium confidence**: Neural network parameterization approach for drift/diffusion/control terms in fBMSDE context
- **Low confidence**: Empirical validation across diverse datasets and generalizability beyond synthetic experiments

## Next Checks
1. **Convergence validation**: Systematically evaluate approximation error E(II) as function of K for different Hurst indices H, establishing practical convergence bounds and identifying saturation thresholds

2. **Numerical stability analysis**: Test robustness of closed-form ωk optimization across different numerical precisions and identify parameter regimes where linear system becomes ill-conditioned

3. **Generalization testing**: Apply framework to real-world continuous-time data beyond synthetic examples (financial time series, biological signals) to assess practical utility and identify failure modes