---
ver: rpa2
title: Don't Make Your LLM an Evaluation Benchmark Cheater
arxiv_id: '2311.01964'
source_url: https://arxiv.org/abs/2311.01964
tags:
- data
- evaluation
- llms
- benchmark
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the issue of data contamination (benchmark
  leakage) in evaluating large language models (LLMs), where training data overlaps
  with test sets, leading to inflated performance scores. The authors conduct extensive
  experiments by training small-sized LLMs (1.3B-7B) on leaked data from popular benchmarks
  (e.g., MMLU) and testing them on various tasks including QA, reasoning, and reading
  comprehension.
---

# Don't Make Your LLM an Evaluation Benchmark Cheater

## Quick Facts
- arXiv ID: 2311.01964
- Source URL: https://arxiv.org/abs/2311.01964
- Reference count: 22
- Key outcome: Benchmark leakage dramatically inflates LLM evaluation scores while degrading performance on unrelated tasks

## Executive Summary
This paper investigates how data contamination from benchmark leakage leads to inflated evaluation scores in large language models. Through controlled experiments with small-sized models (1.3B-7B) trained on leaked benchmark data, the authors demonstrate that even modest models can achieve artificially high scores on contaminated benchmarks while suffering performance degradation on other tasks. The study reveals that benchmark leakage fundamentally violates the principles of zero-shot and few-shot evaluation by converting them into in-domain tests. To address this critical issue, the paper proposes practical guidelines including diverse benchmark usage, contamination analysis reporting, and detailed pre-training data documentation to ensure fair and reliable LLM evaluation practices.

## Method Summary
The authors conducted controlled experiments by training small-sized LLMs (1.3B-7B) on leaked data from popular benchmarks including MMLU, Open-domain QA, Reasoning, and Reading Comprehension tasks. They used backbone models (GPT-Neo-1.3B, phi-1.5, OpenLLaMA-3B, LLaMA-2-7B) and continually trained them on training sets, test prompts, and test sets from these benchmarks. The evaluation measured performance using accuracy, ROUGE-L, and pass@10 metrics across various tasks. The study compared models trained on leaked data against original models to quantify the impact of benchmark leakage on evaluation scores and generalization capabilities.

## Key Results
- Benchmark leakage dramatically boosts evaluation performance, with small models outperforming much larger ones on contaminated tasks
- Training on leaked data converts zero/few-shot evaluation into in-domain testing, making it easier to achieve higher results
- Performance degradation occurs on normally tested tasks, and adaptation capability decreases when models are trained on leaked data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Training on leaked data inflates evaluation scores by overfitting to test prompts and formats
- Mechanism: The model memorizes exact answer patterns and prompt structures, making test responses appear correct without true generalization
- Core assumption: Benchmark leakage provides model with exact or highly similar training examples as evaluation
- Evidence anchors:
  - [abstract] "benchmark leakage can dramatically boost the evaluation results, which would finally lead to an unreliable assessment of model performance"
  - [section] "Incorporating training data converts the original zero/few shot evaluation into an in-domain test task, making it easier for LLMs to achieve higher results"
- Break condition: If evaluation uses diverse prompts or adversarial examples not in training data

### Mechanism 2
- Claim: Smaller models trained on leaked data can outperform much larger models on certain tasks
- Mechanism: Leaked data acts as a "cheat sheet," allowing smaller models to bypass the need for larger capacity to memorize task-specific patterns
- Core assumption: Leaked data contains sufficient information to cover evaluation task space
- Evidence anchors:
  - [abstract] "even small models outperform much larger ones on certain tasks"
  - [section] "when the test prompts were leaked, smaller LLMs can even surpass much larger LLMs that were not trained with leaked data"
- Break condition: If evaluation tasks require reasoning beyond memorized patterns

### Mechanism 3
- Claim: Benchmark leakage causes catastrophic forgetting and reduces adaptation capability
- Mechanism: Overfitting to leaked data degrades model's ability to learn new tasks or adapt to different domains
- Core assumption: Training on leaked data displaces or corrupts general knowledge representations
- Evidence anchors:
  - [section] "As a side effect, the performance of these specially trained LLMs on other normally tested tasks would likely be adversely affected"
  - [section] "benchmark leakage may lead to a decline in adaptation capability, constraining the LLMs' ability to adapt or improve through subsequent fine-tuning processes"
- Break condition: If fine-tuning uses diverse data that counteracts leakage effects

## Foundational Learning

- Concept: Data contamination and leakage
  - Why needed here: Understanding how training data overlaps with evaluation sets is crucial for interpreting results and designing fair benchmarks
  - Quick check question: What is the difference between training set leakage and test set leakage?

- Concept: Zero-shot and few-shot learning evaluation
  - Why needed here: Benchmark leakage violates the fundamental assumption of unseen test data in these evaluation paradigms
  - Quick check question: Why does training on benchmark data violate zero-shot learning principles?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Benchmark leakage can cause models to forget general knowledge while overfitting to specific tasks
  - Quick check question: How does catastrophic forgetting manifest when training on task-specific data?

## Architecture Onboarding

- Component map: Data preparation -> Model training -> Evaluation -> Analysis pipeline
- Critical path: Data decontamination checking -> Model training -> Diverse benchmark evaluation -> Contamination analysis reporting
- Design tradeoffs: Strict decontamination vs. practical data collection constraints; comprehensive evaluation vs. computational costs
- Failure signatures: Inflated benchmark scores, degraded performance on unrelated tasks, poor adaptation to new instructions
- First 3 experiments:
  1. Measure token overlap between pre-training data and benchmark sets using n-gram hashing
  2. Train model on partially leaked data and test on both leaked and non-leaked benchmarks
  3. Perform instruction tuning on leaked vs. non-leaked models and compare adaptation performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the severity of benchmark leakage impact model performance degradation on unrelated tasks?
- Basis in paper: [explicit] The paper discusses how training on leaked data from evaluation benchmarks affects performance on other tasks, noting that over-emphasizing a specific task may lower the model's generalization capability
- Why unresolved: The paper provides initial observations but does not offer a detailed quantitative analysis of how different levels of leakage severity correlate with performance degradation on unrelated tasks
- What evidence would resolve it: A comprehensive study quantifying the relationship between the extent of data leakage and the degree of performance decline on a variety of unrelated tasks would provide clarity

### Open Question 2
- Question: What are the long-term effects of benchmark leakage on the adaptation capabilities of LLMs in real-world applications?
- Basis in paper: [explicit] The paper mentions that benchmark leakage may lead to a decline in adaptation capability, constraining the LLMs' ability to adapt or improve through subsequent fine-tuning processes
- Why unresolved: While the paper touches on the impact of data leakage on model adaptation, it does not explore the long-term implications or real-world scenarios where these effects might manifest
- What evidence would resolve it: Longitudinal studies tracking the performance of LLMs in real-world applications over time, especially those that have been fine-tuned after being trained on leaked data, would provide insights into the long-term effects

### Open Question 3
- Question: How can semantic-level knowledge leakage be detected and mitigated in LLMs?
- Basis in paper: [explicit] The paper suggests that current methods like n-gram hash algorithms may not detect semantic-level knowledge leakage risks, indicating a gap in detection and mitigation strategies
- Why unresolved: The paper identifies the limitation of existing detection methods but does not propose alternative solutions or strategies to address semantic-level knowledge leakage
- What evidence would resolve it: Development and validation of new detection techniques that can identify semantic-level knowledge leakage, along with strategies to mitigate such risks, would address this open question

## Limitations

- The study focuses on specific small-sized models (1.3B-7B) and may not generalize to larger models or different training approaches
- Proposed guidelines lack specific implementation details and quantitative thresholds for acceptable contamination levels
- The mechanism and magnitude of performance degradation across different task types is not fully characterized

## Confidence

- High confidence in the core finding that benchmark leakage inflates evaluation scores - directly demonstrated through controlled experiments
- Medium confidence in the claim that smaller models can outperform larger ones when trained on leaked data - demonstrated but may be task-dependent
- Medium confidence in the proposed mitigation guidelines - represent best practices but lack specific implementation details

## Next Checks

1. Conduct ablation studies to quantify how different degrees of contamination affect performance inflation and task generalization across various model sizes
2. Develop and validate a standardized contamination detection metric with clear thresholds to determine when evaluation results are compromised
3. Test the proposed mitigation guidelines by implementing them in a new evaluation protocol and comparing results with standard benchmark evaluations