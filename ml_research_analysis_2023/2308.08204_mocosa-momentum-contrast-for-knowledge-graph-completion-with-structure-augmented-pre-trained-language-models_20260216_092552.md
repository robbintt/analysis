---
ver: rpa2
title: 'MoCoSA: Momentum Contrast for Knowledge Graph Completion with Structure-Augmented
  Pre-trained Language Models'
arxiv_id: '2308.08204'
source_url: https://arxiv.org/abs/2308.08204
tags:
- knowledge
- structural
- methods
- negative
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MoCoSA, a method for Knowledge Graph Completion
  (KGC) that leverages both textual and structural information. The core idea is to
  integrate a pre-trained language model (PLM) with a structure encoder, allowing
  the PLM to perceive structural information.
---

# MoCoSA: Momentum Contrast for Knowledge Graph Completion with Structure-Augmented Pre-trained Language Models

## Quick Facts
- arXiv ID: 2308.08204
- Source URL: https://arxiv.org/abs/2308.08204
- Reference count: 14
- Primary result: Achieves state-of-the-art performance on KGC benchmarks with 2.5% MRR improvement on WN18RR and 21% on OpenBG500

## Executive Summary
MoCoSA introduces a novel approach for Knowledge Graph Completion (KGC) that combines pre-trained language models with structure encoders through contrastive learning. The method employs momentum hard negative sampling and intra-relation negative sampling to improve learning efficiency and performance. Experimental results demonstrate significant improvements over existing state-of-the-art methods on popular KGC benchmarks, including WN18RR and OpenBG500.

## Method Summary
MoCoSA integrates a pre-trained language model (PLM) with a structure encoder to jointly learn textual and structural embeddings in a shared space. The model employs momentum hard negative sampling using a queue of recent tail features and intra-relation negative sampling to construct challenging contrastive pairs. During training, the structure encoder generates embeddings for entity indices and relations, which are fused with PLM embeddings through self-attention. The contrastive loss pulls positive triples together while pushing negative pairs apart in embedding space.

## Key Results
- Achieves 2.5% improvement in MRR on WN18RR benchmark
- Demonstrates 21% improvement in MRR on OpenBG500 dataset
- Outperforms state-of-the-art methods including CSPromp-KG and Structure-Aware PLM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating structural information into PLMs via an adaptable structure encoder allows joint learning of textual and structural embeddings in a shared space, improving generalization to unseen entities.
- Mechanism: The structure encoder (ASE) generates embeddings for entity indices and relations, which are prepended to the PLM's query, key, and value vectors. This interaction in the self-attention layer fuses structural knowledge with textual representations before contrastive learning.
- Core assumption: Structural embeddings are compatible with textual embeddings and can be meaningfully fused in the same dimensional space without disrupting the PLM's learned representations.
- Evidence anchors:
  - [abstract]: "allows the PLM to perceive the structural information by the adaptable structure encoder"
  - [section]: "We independently encode the textual and structural representations and then fuse them through the text encoder for learning the unified embeddings"
  - [corpus]: Weak evidence. Related work focuses on separate structure-based or description-based methods; fusion approaches like CSPromp-KG are mentioned but not evaluated here.

### Mechanism 2
- Claim: Momentum hard negative sampling increases the number and quality of negatives beyond in-batch negatives, leading to more effective contrastive learning.
- Mechanism: A momentum queue stores recent tail features from a momentum key encoder. During training, hardest negatives from the queue are mixed (via λk-weighted interpolation) with current negatives to create challenging contrastive pairs.
- Core assumption: Hard negatives sampled from the momentum queue are informative and improve the model's ability to distinguish between similar entities under the same relation.
- Evidence anchors:
  - [abstract]: "proposed momentum hard negative and intra-relation negative sampling"
  - [section]: "We use the momentum queue (He et al., 2020) and mix (Kalantidis et al., 2020) some of the hardest negative features"
  - [corpus]: No direct corpus evidence for this specific mechanism; related work mentions negative sampling but not momentum-based mixing.

### Mechanism 3
- Claim: Intra-relation negative sampling reduces false negatives by sampling tail entities that share the same relation but are not linked to the head entity, improving semantic discrimination.
- Mechanism: For a triple (h, r, t), negatives are constructed as (h, r, t′) where t′ is any entity linked to r in the graph except t. This ensures negatives are relation-consistent and avoids random corruption that may produce positives.
- Core assumption: Entities sharing the same relation are semantically similar, so distinguishing them is a meaningful learning signal.
- Evidence anchors:
  - [abstract]: "momentum hard negative and intra-relation negative sampling"
  - [section]: "We propose intra-relation (IR) negative sampling for tail entities to construct corresponding wrong triples"
  - [corpus]: Weak evidence. The corpus mentions negative sampling strategies but does not detail intra-relation negatives.

## Foundational Learning

- Concept: Contrastive learning framework
  - Why needed here: The model uses contrastive loss to pull positive pairs (h, r, t) closer and push negative pairs apart in embedding space.
  - Quick check question: What is the role of the temperature parameter τ in the contrastive loss?

- Concept: Pre-trained language model fine-tuning
  - Why needed here: The PLM encodes entity descriptions into contextual embeddings, which are then augmented with structural information.
  - Quick check question: How does mean pooling over BERT's last layer differ from using only the [CLS] token for sentence embeddings?

- Concept: Knowledge graph embedding (KGE)
  - Why needed here: The adaptable structure encoder implements a KGE scoring function (e.g., TransE-style translation) to model structural plausibility.
  - Quick check question: What is the difference between distance-based and similarity-based scoring functions in KGE?

## Architecture Onboarding

- Component map: Text encoder (BERT) -> Structure encoder (ASE) -> Fusion in self-attention -> Momentum queue + IR negatives -> Contrastive loss -> Relation-based re-ranking
- Critical path: Text and structure encoders produce embeddings -> fusion in self-attention -> contrastive learning with negatives -> inference with re-ranking
- Design tradeoffs: Using PLM gives robustness to unseen entities but slower inference; structure encoder adds inductive bias but requires careful fusion to avoid overfitting to textual info.
- Failure signatures: Over-reliance on textual info (low H@1, high H@10), poor generalization to unseen entities, slow training due to large momentum queue.
- First 3 experiments:
  1. Verify that fusing ASE embeddings into BERT's self-attention improves MRR on a small dataset (e.g., WN18RR) vs BERT alone.
  2. Test momentum queue size and mixing coefficient λk on OpenBG500 to find optimal hard negative sampling settings.
  3. Evaluate the impact of intra-relation negatives by comparing with random corruption on FB15k-237.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MoCoSA compare when using different pre-trained language models (e.g., GPT-3.5 vs. BERT) as the text encoder?
- Basis in paper: [explicit] The paper mentions that PLMs for the WN18RR and FB15k-237 are initialized with bert-base-uncased and bert-base-chinese for OpenBG500, and it is expected that employing a superior PLM would be beneficial.
- Why unresolved: The paper does not provide a direct comparison of MoCoSA's performance using different PLMs as the text encoder.
- What evidence would resolve it: Experiments comparing MoCoSA's performance using different PLMs as the text encoder on the same datasets would provide evidence to answer this question.

### Open Question 2
- Question: What is the optimal number of intra-relation negative samples for maximizing MoCoSA's performance on different datasets?
- Basis in paper: [explicit] The paper mentions that increasing the number of intra-relation negative samples enhances the model's performance significantly on WN18RR and FB15k-237, but it is limited due to computational overhead constraints.
- Why unresolved: The paper does not provide a definitive answer on the optimal number of intra-relation negative samples for maximizing performance on different datasets.
- What evidence would resolve it: Experiments varying the number of intra-relation negative samples and measuring their impact on MoCoSA's performance on different datasets would provide evidence to answer this question.

### Open Question 3
- Question: How does the performance of MoCoSA change when incorporating different structure-based methods as the mapping function in the adaptable structural encoder?
- Basis in paper: [explicit] The paper mentions that incorporating two structure-based methods as the mapping function in the adaptable structural encoder led to performance improvements, and it reinforces the notion that a well-performing structure-based method enhances the overall capabilities of PLMs in the KGC task.
- Why unresolved: The paper does not provide a comprehensive evaluation of MoCoSA's performance when incorporating different structure-based methods as the mapping function in the adaptable structural encoder.
- What evidence would resolve it: Experiments comparing MoCoSA's performance when incorporating different structure-based methods as the mapping function in the adaptable structural encoder would provide evidence to answer this question.

## Limitations

- Limited ablation study scope with only 6 out of 15 possible combinations tested
- No extensive hyperparameter sensitivity analysis for momentum queue size and mixing coefficients
- Limited evaluation on datasets beyond WN18RR, FB15k-237, and OpenBG500

## Confidence

**High confidence** in the core claim that MoCoSA achieves state-of-the-art performance on the tested benchmarks, based on the experimental results and comparison with established baselines.

**Medium confidence** in the mechanism explanations, particularly regarding how effectively the structural information is integrated without disrupting PLM representations, and whether the momentum queue strategy consistently provides informative negatives across different datasets.

**Low confidence** in generalizability beyond the tested datasets, given the limited ablation study and lack of extensive hyperparameter sensitivity analysis.

## Next Checks

1. Conduct comprehensive ablation studies including all combinations of structural encoder, momentum queue, and intra-relation negatives to isolate individual contributions more precisely.

2. Test model performance on datasets with varying ratios of seen to unseen entities to verify that the structure-augmented PLM maintains its advantage for zero-shot learning scenarios.

3. Evaluate the impact of different fusion strategies (e.g., concatenation vs. attention-based mixing) for combining structural and textual embeddings to optimize the information flow between components.