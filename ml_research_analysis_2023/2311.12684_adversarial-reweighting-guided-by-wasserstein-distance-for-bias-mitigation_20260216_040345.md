---
ver: rpa2
title: Adversarial Reweighting Guided by Wasserstein Distance for Bias Mitigation
arxiv_id: '2311.12684'
source_url: https://arxiv.org/abs/2311.12684
tags:
- group
- disparate
- groups
- wasserstein
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose a novel adversarial reweighting approach to
  mitigate representation bias in fairness-aware classification tasks. The key idea
  is to reweight majority group samples based on their Wasserstein distance to the
  minority group in a learned latent space.
---

# Adversarial Reweighting Guided by Wasserstein Distance for Bias Mitigation

## Quick Facts
- arXiv ID: 2311.12684
- Source URL: https://arxiv.org/abs/2311.12684
- Reference count: 9
- The authors propose a novel adversarial reweighting approach to mitigate representation bias in fairness-aware classification tasks.

## Executive Summary
This paper introduces an adversarial reweighting method that leverages Wasserstein distance to mitigate representation bias in fairness-aware classification. The approach reweights majority group samples based on their distance to the minority group in a learned latent space, aiming to balance distributions and reduce discrimination. The method is theoretically grounded, showing that minimizing Wasserstein distance in the latent space enforces demographic parity. Experimental results on image and tabular datasets demonstrate effectiveness in reducing bias while maintaining high classification accuracy.

## Method Summary
The method employs a Wasserstein distance-guided adversarial reweighting framework. It first maps raw data to a latent space using a feature extractor, then uses a WGAN-GP discriminator to approximate the Wasserstein distance between minority and reweighted majority distributions. Majority samples are downweighted based on their distance to the minority group, effectively aligning the two distributions. The classifier is trained using these reweighted samples. The approach alternates between updating the feature extractor/classifier and updating the weights/discriminator, with a constraint on the weights to balance similarity and dissimilarity.

## Key Results
- The method effectively mitigates representation bias while maintaining high classification accuracy on image and tabular benchmark datasets
- Outperforms several state-of-the-art fairness-aware methods in terms of fairness metrics (Disparate Impact, Disparate FPR, Disparate FNR)
- Theoretical analysis shows the approach can enforce demographic parity by minimizing Wasserstein distance in the latent space

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Downweighting majority group samples far from the minority group reduces representation bias and improves demographic parity.
- Mechanism: The method uses Wasserstein distance in a learned latent space to quantify the similarity between majority and minority group distributions. Samples from the majority group that are further from the minority group are assigned lower weights, effectively aligning the two distributions and reducing bias in the classifier's predictions.
- Core assumption: The Wasserstein distance in the latent space is a good proxy for the distance in the prediction space, and that aligning the latent distributions will lead to aligned prediction distributions.
- Evidence anchors:
  - [abstract] "To balance the data distribution between the majority and the minority groups, our approach deemphasizes samples from the majority group."
  - [section] "Our method addresses representation bias by weighting the samples from the majority group. It aims to maintain the class-wise discriminatory information of the data samples from the majority group that are further away from the minority group, but downplay their importance."
  - [corpus] Weak corpus evidence; no direct citations to similar reweighting mechanisms using Wasserstein distance for fairness.
- Break condition: If the latent space mapping does not preserve the relevant structure for fairness, or if the Wasserstein distance fails to capture the true distributional differences, the reweighting will not effectively reduce bias.

### Mechanism 2
- Claim: Adversarial training between the reweighting component and the discriminator of the GAN framework learns weights that minimize the Wasserstein distance between the minority and reweighted majority distributions.
- Mechanism: The method formulates a minimax optimization problem where the discriminator tries to distinguish between minority and reweighted majority samples, while the reweighting component tries to minimize this distinction by adjusting the weights of majority samples.
- Core assumption: The GAN framework can effectively approximate the Wasserstein distance and that the adversarial training will converge to a meaningful solution for the weights.
- Evidence anchors:
  - [section] "We approximate the computation of the Wasserstein distance by a neural network discriminator D using the gradient penalty technique of WGAN-GP (Gulrajani et al. 2017)."
  - [section] "In Equation (8), the discriminator is trained to maximize the average of its outputs on the minority and majority group; adversarially, the weights for samples from the majority group are learned to minimize the (reweighted) average of the outputs of the discriminator."
  - [corpus] Weak corpus evidence; no direct citations to similar adversarial reweighting approaches for fairness.
- Break condition: If the GAN training is unstable or fails to approximate the Wasserstein distance accurately, the learned weights will not effectively align the distributions.

### Mechanism 3
- Claim: Enforcing demographic parity in the latent space through reweighting leads to demographic parity in the prediction space.
- Mechanism: The method leverages the theoretical result that if the Wasserstein distance between the latent distributions of the majority and minority groups is small, then the Wasserstein distance between their prediction distributions will also be small, leading to demographic parity.
- Core assumption: The classifier is Lipschitz continuous and the latent space mapping preserves the necessary properties for the theoretical result to hold.
- Evidence anchors:
  - [section] "Proposition 1. Given two measures µ and ν over a metric space (Z, dZ) and a K-Lipschitz function C : ( Z, dZ) → (Y, dY ), we have that W (C#µ, C#ν) ≤ K · W (µ, ν)."
  - [section] "Note that W (C#µ, C#ν) = 0 means that C#µ = C#ν and this implies demographic parity."
  - [corpus] Weak corpus evidence; no direct citations to similar theoretical results on Wasserstein distance and demographic parity.
- Break condition: If the classifier is not Lipschitz continuous or the latent space mapping does not preserve the necessary properties, the theoretical result will not hold and demographic parity will not be achieved.

## Foundational Learning

- Concept: Wasserstein distance and its properties
  - Why needed here: The method relies on Wasserstein distance to quantify the similarity between distributions and to align the majority and minority group distributions.
  - Quick check question: Can you explain the Kantorovich-Rubinstein duality for Wasserstein distance and why it is useful for this method?

- Concept: Generative Adversarial Networks (GANs) and their training
  - Why needed here: The method uses a GAN framework with a discriminator to approximate the Wasserstein distance and to learn the reweighting of majority samples.
  - Quick check question: Can you describe the role of the discriminator in a GAN and how it is trained to distinguish between real and generated samples?

- Concept: Lipschitz continuity and its implications
  - Why needed here: The theoretical result on demographic parity relies on the classifier being Lipschitz continuous to bound the Wasserstein distance in the prediction space.
  - Quick check question: Can you explain what it means for a function to be Lipschitz continuous and why this property is important for the theoretical result?

## Architecture Onboarding

- Component map: Feature extractor (Fϕ) -> Latent Space -> Classifier (Cθ) -> Binary Labels, Discriminator (D) -> Wasserstein Distance Estimation, Reweighting Component -> Majority Sample Weights

- Critical path:
  1. Extract features from raw data using Fϕ.
  2. Compute Wasserstein distance between minority and reweighted majority distributions using D.
  3. Update weights of majority samples to minimize the Wasserstein distance.
  4. Train classifier Cθ using the reweighted data.

- Design tradeoffs:
  - Using a high-dimensional latent space (e.g., 512 for images) allows for more expressive feature representations but may increase computational complexity.
  - Balancing the trade-off between similarity and dissimilarity of weights using the hyperparameter T.
  - Choosing an appropriate batch size for accurate estimation of Wasserstein distance.

- Failure signatures:
  - High Wasserstein distance between minority and reweighted majority distributions indicates ineffective reweighting.
  - Unstable GAN training or poor approximation of Wasserstein distance by the discriminator.
  - Large discrepancy between the desired demographic parity and the actual fairness metrics.

- First 3 experiments:
  1. Verify that the feature extractor produces meaningful latent representations by visualizing t-SNE embeddings of minority and majority groups before reweighting.
  2. Check the stability of the GAN training by monitoring the Wasserstein distance estimates and the discriminator loss during training.
  3. Evaluate the effectiveness of the reweighting by comparing the demographic parity metrics (e.g., disparate impact, disparate FPR, disparate FNR) before and after applying the method.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed adversarial reweighting approach scale to datasets with more than two sensitive groups or multi-sensitive attributes?
- Basis in paper: [explicit] The authors mention that it is "straight-forward to extend our method to handle a multi-categorical sensitive attribute or multi-sensitive attributes by using one subgroup as reference group and reweighing other subgroups alternatively (and in turn) to reach a state of demographic parity." They demonstrate this on the Adult dataset using race as the sensitive attribute.
- Why unresolved: While the authors provide a general approach for extending to multiple sensitive attributes, they do not provide a detailed analysis or experiments to validate the effectiveness and scalability of this extension to more complex scenarios with multiple sensitive attributes or a larger number of sensitive groups.
- What evidence would resolve it: Experimental results on datasets with multiple sensitive attributes or a larger number of sensitive groups, demonstrating the effectiveness and scalability of the proposed approach in mitigating representation bias and maintaining high classification accuracy.

### Open Question 2
- Question: How sensitive is the proposed adversarial reweighting approach to the choice of hyperparameters, particularly the regularization parameter T in the weight constraint?
- Basis in paper: [explicit] The authors analyze the sensitivity of their method to the hyperparameter T in the constraint on the weights for samples from the majority group. They provide plots showing the performance of their approach under different values of T on the CelebA dataset.
- Why unresolved: While the authors provide some analysis of the sensitivity to T, they do not explore the sensitivity to other hyperparameters, such as the learning rate, batch size, or the architecture of the feature extractor and discriminator. Additionally, the analysis is limited to a single dataset, and it is unclear how the sensitivity may vary across different datasets or problem settings.
- What evidence would resolve it: A comprehensive sensitivity analysis exploring the impact of various hyperparameters on the performance of the proposed approach across multiple datasets and problem settings.

### Open Question 3
- Question: How does the proposed adversarial reweighting approach compare to other fairness-aware methods in terms of computational efficiency and scalability to large-scale datasets?
- Basis in paper: [explicit] The authors mention that their approach is "computationally less demanding in a low-dimensional space" and that they use a relatively high batch size for estimating the Wasserstein distance between distributions. However, they do not provide a detailed comparison of the computational efficiency or scalability of their approach compared to other fairness-aware methods.
- Why unresolved: While the authors demonstrate the effectiveness of their approach in mitigating representation bias and maintaining high classification accuracy, they do not provide a thorough analysis of its computational efficiency or scalability to large-scale datasets. This information is crucial for understanding the practical applicability and limitations of the proposed approach in real-world scenarios.
- What evidence would resolve it: A comprehensive comparison of the computational efficiency and scalability of the proposed adversarial reweighting approach with other fairness-aware methods on large-scale datasets, including runtime analysis and memory usage.

## Limitations

- The exact architecture specifications for the classifier, particularly for tabular datasets, are not provided, making exact reproduction challenging.
- The theoretical analysis assumes perfect Lipschitz continuity of the classifier, which may not hold in practice.
- The approach's generalization to multi-class sensitive attributes and more complex bias scenarios requires further validation.

## Confidence

- **High confidence**: The overall methodology of adversarial reweighting guided by Wasserstein distance is sound and the theoretical framework connecting latent space alignment to demographic parity is valid under stated assumptions.
- **Medium confidence**: The experimental results showing improved fairness metrics while maintaining accuracy are promising, though limited to specific benchmark datasets with binary sensitive attributes.
- **Low confidence**: The generalization of the approach to multi-class sensitive attributes and more complex bias scenarios requires further validation.

## Next Checks

1. Verify classifier architecture specifications for tabular datasets and ensure reproducibility of reported performance metrics.
2. Test the stability of the WGAN-GP training across different random seeds and batch sizes to assess robustness.
3. Evaluate the method's performance on datasets with multi-class sensitive attributes and intersectional bias scenarios beyond binary gender-based discrimination.