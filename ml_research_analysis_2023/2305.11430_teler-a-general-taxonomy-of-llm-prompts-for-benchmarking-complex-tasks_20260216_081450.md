---
ver: rpa2
title: 'TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks'
arxiv_id: '2305.11430'
source_url: https://arxiv.org/abs/2305.11430
tags:
- prompts
- language
- complex
- arxiv
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces TELeR, a taxonomy for categorizing prompts\
  \ used in benchmarking LLMs on complex tasks. TELeR defines four key dimensions\u2014\
  Turn (single/multi-turn), Expression (question/instruction), Role (system role defined/undefined),\
  \ and Level of Details (0-5 based on task specificity)\u2014to standardize how prompts\
  \ are described."
---

# TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks

## Quick Facts
- arXiv ID: 2305.11430
- Source URL: https://arxiv.org/abs/2305.11430
- Reference count: 6
- Primary result: TELeR provides a four-dimensional framework for standardizing prompt reporting in LLM benchmarking of complex tasks

## Executive Summary
This paper introduces TELeR, a taxonomy for categorizing prompts used in benchmarking LLMs on complex tasks. TELeR defines four key dimensions—Turn (single/multi-turn), Expression (question/instruction), Role (system role defined/undefined), and Level of Details (0-5 based on task specificity)—to standardize how prompts are described. The motivation is that variations in prompts significantly affect LLM performance on complex, ill-defined tasks, making fair comparisons difficult. By adopting TELeR, researchers can report prompts consistently, enabling meaningful cross-study comparisons. Two use cases (meta-review generation, narrative braiding) demonstrate how TELeR can be applied to categorize diverse prompts and ensure standardized evaluation.

## Method Summary
The TELeR taxonomy defines four dimensions—Turn (single/multi-turn), Expression (question/instruction), Role (system role defined/undefined), and Level of Details (0-5 based on task specificity)—to standardize how prompts are described. The method involves categorizing example prompts for complex tasks using these dimensions to enable consistent reporting across studies. The taxonomy provides a structured approach to prompt description that aims to improve the reliability and comparability of LLM benchmarking studies by creating a common language for prompt reporting.

## Key Results
- TELeR provides a unified framework for prompt reporting across complex task benchmarking studies
- The four dimensions capture meaningful variations in prompt design that affect LLM performance
- Two use cases (meta-review generation, narrative braiding) demonstrate TELeR's applicability to diverse complex tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt variability directly impacts LLM performance on complex tasks, making standardized reporting necessary for fair benchmarking.
- Mechanism: Different prompt dimensions (Turn, Expression, Role, Level of Details) interact with LLM inference processes, causing performance shifts. By standardizing prompt description via TELeR, researchers can isolate LLM capability differences rather than prompt-induced variance.
- Core assumption: The same complex task, when presented with prompts differing only in TELeR dimensions, will produce measurably different LLM outputs.
- Evidence anchors: [abstract] "variations in prompts significantly affect LLM performance on complex, ill-defined tasks"; [section 3] "differences in prompts along several key factors can have a significant impact on the accuracy and performance"

### Mechanism 2
- Claim: Complex tasks require multi-step reasoning, and prompt structure determines how effectively LLMs decompose and execute these steps.
- Mechanism: The "Level of Details" dimension encodes sub-task specification and reasoning requirements. Higher levels guide LLMs to break down complex goals, improving execution accuracy.
- Core assumption: LLMs benefit from explicit sub-task delineation in prompts when handling complex goals.
- Evidence anchors: [section 3] "complex tasks consist of multiple steps/sub-tasks... prompts can explicitly solicit such explanations/justifications"; [section 4] "Level 5 means the highest level of details where the directive includes clear goals, distinct sub-tasks/steps"

### Mechanism 3
- Claim: Standardizing prompt reporting enables cross-study comparability by grounding diverse prompt styles into a common framework.
- Mechanism: TELeR provides a four-dimensional schema (Turn, Expression, Role, Level of Details) that researchers can use to report prompts consistently, allowing meaningful comparisons across independent studies.
- Core assumption: Researchers will adopt TELeR consistently and report all four dimensions accurately.
- Evidence anchors: [abstract] "researchers can report prompts consistently, enabling meaningful cross-study comparisons"; [section 6] "different study groups will report the specific categories of prompts they designed according to this taxonomy"

## Foundational Learning

- Concept: Multi-turn vs. single-turn prompting
  - Why needed here: Turn dimension affects historical context accumulation, which influences LLM reasoning on complex tasks
  - Quick check question: How would a meta-review generation task differ if broken into multiple turns versus provided as one continuous prompt?

- Concept: Question vs. instruction expression style
  - Why needed here: Expression dimension determines how LLMs interpret task directives, affecting output structure and focus
  - Quick check question: Would framing a narrative braiding task as questions or instructions yield different braiding quality?

- Concept: System role definition
  - Why needed here: Role dimension provides domain context that helps LLMs tailor responses appropriately for complex tasks
  - Quick check question: How might defining a "senior reviewer" role change the meta-review output compared to an undefined role?

## Architecture Onboarding

- Component map: Prompt categorization engine (TELeR dimensions) -> Benchmark dataset (complex tasks with varying prompt styles) -> Performance evaluation pipeline (LLM outputs vs. ground truth or human judgment) -> Reporting interface (standardized TELeR dimension output)
- Critical path: Prompt design → TELeR categorization → LLM execution → Performance measurement → Cross-study comparison
- Design tradeoffs:
  - Granularity vs. usability: More detailed TELeR levels improve precision but increase categorization overhead
  - Flexibility vs. standardization: Allowing dimension variations enables task-specific optimization but reduces comparability
  - Automation vs. manual curation: Automated TELeR classification speeds adoption but may miss nuanced prompt features
- Failure signatures:
  - Inconsistent TELeR application across studies
  - Performance variance unexplained by TELeR dimensions
  - Adoption barriers due to complexity of dimension definitions
- First 3 experiments:
  1. Compare LLM performance on identical complex tasks using prompts that vary only in one TELeR dimension
  2. Test inter-rater reliability for TELeR dimension classification across different researchers
  3. Evaluate whether TELeR-standardized prompt reporting improves cross-study comparability metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific prompt characteristics lead to optimal LLM performance across different types of complex tasks?
- Basis in paper: Explicit - The paper emphasizes that "differences in prompt designs play a crucial role in determining the accuracy and effectiveness of large language models in complex tasks" but doesn't identify which specific characteristics are most important
- Why unresolved: The TELeR taxonomy provides a framework for categorization but doesn't establish which dimensions (Turn, Expression, Role, Level of Details) have the strongest impact on performance for specific task types
- What evidence would resolve it: Systematic experiments varying individual TELeR dimensions across multiple complex task types while measuring performance, to identify which characteristics consistently correlate with better outcomes

### Open Question 2
- Question: How do multi-turn versus single-turn prompts affect LLM performance on tasks requiring sequential reasoning versus holistic understanding?
- Basis in paper: Explicit - The Turn dimension is mentioned as a key factor, but the paper doesn't investigate when multi-turn prompting is beneficial versus detrimental
- Why unresolved: The paper identifies Turn as a dimension but doesn't provide empirical evidence about when each approach is superior or how they interact with other task characteristics
- What evidence would resolve it: Comparative studies testing both prompt styles on tasks with different reasoning requirements, measuring both performance and efficiency metrics

### Open Question 3
- Question: What is the optimal Level of Details for prompts across different complex task domains, and how does this vary with model size or architecture?
- Basis in paper: Explicit - The Level of Details dimension is described but the paper doesn't establish guidelines for when higher vs. lower levels are appropriate
- Why unresolved: The paper provides a theoretical framework but lacks empirical data showing how different detail levels affect performance across various task domains or model capabilities
- What evidence would resolve it: Large-scale benchmarking studies testing different detail levels on the same tasks with various model sizes, to establish domain-specific and model-specific optimal detail levels

### Open Question 4
- Question: How does the system Role definition interact with other prompt dimensions to influence LLM performance on domain-specific complex tasks?
- Basis in paper: Explicit - The Role dimension is mentioned as important but the paper doesn't explore how it interacts with other dimensions or varies by domain
- Why unresolved: The paper identifies Role as a factor but doesn't investigate whether certain roles are more effective for specific task types or how role specification interacts with detail level
- What evidence would resolve it: Controlled experiments varying Role while holding other dimensions constant across different domain tasks, to identify effective role-task pairings

## Limitations

- Limited empirical validation: The taxonomy's effectiveness is demonstrated through two use cases but lacks comprehensive testing across diverse complex task domains
- Adoption dependency: The framework's success relies on consistent application by research groups, which may face resistance due to additional categorization overhead
- Subjective classification: The Level of Details dimension requires subjective judgment about task specificity, potentially leading to inconsistent categorization across researchers

## Confidence

- High confidence: The core premise that prompt variability affects LLM performance on complex tasks is well-supported by existing literature and demonstrated in the paper's examples
- Medium confidence: The claim that TELeR will enable meaningful cross-study comparisons assumes consistent application across research groups
- Low confidence: The specific impact of each TELeR dimension on LLM performance for complex tasks lacks comprehensive empirical validation beyond the illustrative examples provided

## Next Checks

1. **Inter-rater reliability study**: Conduct a multi-researcher evaluation where independent reviewers classify the same set of complex task prompts using TELeR dimensions, measuring agreement rates and identifying sources of classification variance.

2. **Cross-domain generalizability test**: Apply TELeR to categorize prompts from three additional complex task domains (e.g., code generation, creative writing, scientific reasoning) and assess whether the taxonomy captures meaningful variations across these domains.

3. **Performance impact analysis**: Design an experiment where LLMs complete identical complex tasks using prompts that vary systematically along single TELeR dimensions, measuring how each dimension independently affects output quality and task completion rates.