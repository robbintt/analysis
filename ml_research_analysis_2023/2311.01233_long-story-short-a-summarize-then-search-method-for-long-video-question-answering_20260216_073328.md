---
ver: rpa2
title: 'Long Story Short: a Summarize-then-Search Method for Long Video Question Answering'
arxiv_id: '2311.01233'
source_url: https://arxiv.org/abs/2311.01233
tags:
- video
- long
- story
- question
- plot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Long Story Short (LSS), a zero-shot method
  for narrative video question answering (QA) using large language models (LLMs).
  LSS first summarizes long video narratives into short plots using GPT-3, then searches
  for relevant video segments using the summaries.
---

# Long Story Short: a Summarize-then-Search Method for Long Video Question Answering

## Quick Facts
- arXiv ID: 2311.01233
- Source URL: https://arxiv.org/abs/2311.01233
- Reference count: 40
- Primary result: Zero-shot method using GPT-3 for narrative video QA, achieving SOTA on MovieQA and DramaQA

## Executive Summary
Long Story Short (LSS) introduces a zero-shot approach for narrative video question answering using large language models. The method first divides long videos into segments and summarizes each using GPT-3, creating a plot representation. It then searches for relevant segments based on these summaries and questions. LSS also incorporates CLIPCheck to enhance visual-text matching by combining language model likelihoods with visual features from CLIP. The framework demonstrates state-of-the-art performance on MovieQA and DramaQA datasets, outperforming supervised baselines.

## Method Summary
LSS operates through three main stages: First, it segments long videos and uses GPT-3 to generate concise plot summaries for each segment. Second, it retrieves relevant video segments by matching questions against these plot summaries. Third, it employs CLIPCheck to enhance visual-text matching by combining language model likelihoods with visual features from CLIP, strengthening the visual grounding of answer selections.

## Key Results
- Achieves state-of-the-art performance on MovieQA and DramaQA datasets
- Outperforms supervised baselines in zero-shot setting
- Demonstrates effectiveness of summarize-then-search approach for long video QA
- Shows importance of visual alignment through CLIPCheck mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Dividing long video context into shorter segments and summarizing each with GPT-3 overcomes token length limitations
- Core assumption: GPT-3 can generate accurate, concise summaries capturing essential narrative elements
- Evidence anchors: Abstract states the method summarizes narrative to short plot then searches relevant parts; Section 2.1 describes dividing videos into clips and summarizing with GPT-3
- Break condition: If GPT-3 fails to generate accurate summaries or misses essential narrative elements

### Mechanism 2
- Retrieving relevant video segments based on summarized plot pieces improves question answering accuracy
- Core assumption: Summarized plot pieces accurately represent video segments and model can correctly identify relevant pieces
- Evidence anchors: Abstract mentions searching parts relevant to question; Section 2.2 describes retrieving short clips relevant to question from long video
- Break condition: If model fails to identify relevant plot pieces or retrieved segments lack necessary information

### Mechanism 3
- CLIPCheck enhances visual-text matching by combining language model likelihood with CLIP visual features
- Core assumption: Visual features from CLIP effectively match textual answer descriptions
- Evidence anchors: Abstract mentions enhancing visual matching with CLIPCheck; Section 2.3 introduces CLIPCheck combining CLIP visual distance and language model likelihood
- Break condition: If visual features don't match textual descriptions or combination doesn't improve accuracy

## Foundational Learning

- **Video summarization**: Needed to overcome GPT-3 token limitations for processing long narratives
  - Quick check: How does dividing a long video into shorter segments and summarizing each help handle the video's rich context?

- **Text-image matching**: Required to enhance visual grounding of model-generated answers
  - Quick check: How does combining language model likelihood with CLIP visual features improve visual-text matching?

- **Retrieval-based question answering**: Essential for efficiently finding relevant video parts based on summaries and questions
  - Quick check: How does retrieving relevant video segments based on summarized plot pieces contribute to answering questions?

## Architecture Onboarding

- **Component map**: Video segmentation -> Plot generation using GPT-3 -> Narrative search -> CLIPCheck -> Final answer selection
- **Critical path**: 1. Video segmentation 2. Plot generation 3. Narrative search 4. CLIPCheck 5. Final answer selection
- **Design tradeoffs**: Balancing segment length vs summary accuracy; choosing between language vs visual likelihood in CLIPCheck; determining CLIPCheck threshold
- **Failure signatures**: Inaccurate summaries leading to irrelevant segment retrieval; poor visual-text matching causing incorrect answers; over-reliance on CLIPCheck when language model is confident
- **First 3 experiments**:
  1. Test plot generation accuracy by comparing with human-written summaries
  2. Evaluate narrative search effectiveness by measuring relevance of retrieved segments
  3. Assess CLIPCheck impact by comparing results with and without it

## Open Questions the Paper Calls Out

1. **Improving video descriptions**: How can we provide visual descriptions better aligned with the story using character re-identification and co-reference resolution? (Basis: Limitations section; unresolved due to lack of specific methods; evidence would come from experiments showing improved performance with better descriptions)

2. **Dynamic multi-hop search**: Can we devise a more dynamic multi-hop search combining global and local information hierarchically? (Basis: Limitations section; unresolved as paper uses simple summarize-then-search; evidence would come from comparing with sophisticated multi-hop approaches)

3. **LLM performance variation**: How does LSS performance vary with different LLM instances? (Basis: Explicit mention in Limitations section; unresolved as only GPT-3 was evaluated; evidence would come from testing with GPT-4, PaLM, LaMDA, etc.)

## Limitations

- Heavy reliance on GPT-3 quality for plot generation, with success contingent on summary accuracy
- Limited evaluation to MovieQA and DramaQA datasets, potentially limiting generalizability
- CLIPCheck computational overhead from processing multiple video frames with arbitrary 0.4 threshold

## Confidence

**High Confidence Claims**:
- Framework's ability to divide videos and generate GPT-3 summaries is well-documented
- Basic LSS architecture (summarize-then-search) is clearly described and reproducible
- Superior performance on MovieQA and DramaQA datasets is supported quantitatively

**Medium Confidence Claims**:
- CLIPCheck effectiveness in visual-text matching needs more ablation studies
- Handling of two-hour videos depends heavily on GPT-3's summarization capabilities
- Outperformance of supervised baselines needs closer examination of conditions

**Low Confidence Claims**:
- General applicability to other video QA scenarios beyond tested datasets
- Robustness with complex visual narratives or minimal dialogue
- Scalability to longer videos or different visual content types

## Next Checks

1. **Ablation Study on CLIPCheck Threshold**: Systematically vary the 0.4 threshold across different values to determine sensitivity and identify optimal threshold for various question types

2. **Cross-Dataset Generalization Test**: Apply LSS to a different long video QA dataset with distinct characteristics to evaluate generalizability beyond MovieQA and DramaQA

3. **Human Evaluation of Generated Plots**: Conduct human study comparing GPT-3-generated summaries against human-written summaries to quantify quality and accuracy of foundational content