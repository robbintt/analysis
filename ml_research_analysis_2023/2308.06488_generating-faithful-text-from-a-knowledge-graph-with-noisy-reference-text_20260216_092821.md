---
ver: rpa2
title: Generating Faithful Text From a Knowledge Graph with Noisy Reference Text
arxiv_id: '2308.06488'
source_url: https://arxiv.org/abs/2308.06488
tags:
- text
- house
- output
- graph
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the problem of generating faithful text from
  knowledge graphs when the reference text contains noisy, hallucinated information.
  The proposed framework combines contrastive learning to distinguish faithful from
  hallucinated content, and a controllable text generation technique that uses control
  tokens to regulate hallucination levels during decoding.
---

# Generating Faithful Text From a Knowledge Graph with Noisy Reference Text

## Quick Facts
- arXiv ID: 2308.06488
- Source URL: https://arxiv.org/abs/2308.06488
- Reference count: 40
- Outperforms state-of-the-art KG-to-text models on faithfulness metrics using contrastive learning and controllable generation

## Executive Summary
This work addresses the challenge of generating faithful text from knowledge graphs when reference texts contain noisy, hallucinated information. The proposed framework combines contrastive learning to distinguish faithful from hallucinated content with controllable text generation that uses control tokens to regulate hallucination levels during decoding. Evaluated on House and GenWiki datasets, the method shows significant improvements in faithfulness metrics (BARTScore and FactCC) compared to state-of-the-art KG-to-text models.

## Method Summary
The framework fine-tunes a pre-trained encoder-decoder model (BART-base for House, T5-base for GenWiki) with two key innovations: contrastive learning that learns to prefer faithful summaries over hallucinated ones using positive and negative samples, and controllable text generation that appends hallucination-level control tokens (low/medium/high) derived from BARTScore to the input graph. The overall training objective combines contrastive loss with cross-entropy loss using control tokens. The model is trained for 5 epochs on House and 4K steps on GenWiki with batch size 32.

## Key Results
- Achieves best results on faithfulness measures (BARTScore and FactCC) compared to state-of-the-art models on both House and GenWiki datasets
- Outperforms baseline models on ChatGPT-based evaluation with higher precision, recall, and lower hallucination rates
- Control tokens effectively regulate hallucination levels during text generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning enables the model to differentiate between faithful and hallucinated information in reference text, guiding the decoder to generate text aligned with the input KG.
- Core assumption: The ground-truth text is more faithful to its paired graph than randomly sampled text from the dataset.
- Evidence anchors: [abstract] "we utilize contrastive learning to enhance the model's ability to differentiate between faithful and hallucinated information in the text"
- Break condition: If the assumption that ground-truth text is more faithful than random text doesn't hold, the contrastive learning would fail to effectively distinguish faithful from hallucinated information.

### Mechanism 2
- Claim: Controllable text generation technique learns the level of hallucination from noisy training text and controls (i.e. minimizes) the level of hallucinated information in the generated text.
- Core assumption: The model can learn to map graph-text pairs to hallucination control tokens during training.
- Evidence anchors: [abstract] "we empower the decoder to control the level of hallucination in the generated text by employing a controllable text generation technique"
- Break condition: If the model fails to learn the mapping between graph-text pairs and hallucination control tokens, the controllable generation would not effectively minimize hallucination.

### Mechanism 3
- Claim: The combination of contrastive learning and controllable text generation techniques outperforms state-of-the-art KG-to-text models on faithfulness metrics.
- Core assumption: The combination of contrastive learning and controllable text generation techniques will lead to better faithfulness than either technique alone.
- Evidence anchors: [section 4.4] "Table 2 presents the results on the House and GenWiki datasets. From the results on the House dataset, we can observe that our full model achieves best results on faithfulness measures"
- Break condition: If the combination of techniques doesn't lead to better faithfulness than either technique alone, the overall framework would not be effective.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: To enable the model to differentiate between faithful and hallucinated information in reference text
  - Quick check question: How does the contrastive loss function encourage the model to prefer faithful summaries over hallucinated ones?

- Concept: Controllable text generation
  - Why needed here: To control the level of hallucination in the generated text by learning from noisy training text
  - Quick check question: How do control feature tokens act as special inputs to regulate hallucination levels during text generation?

- Concept: Knowledge Graph (KG) linearization
  - Why needed here: To convert the KG into a format that can be used for measuring faithfulness and as input to the model
  - Quick check question: What is the process of linearizing a KG and why is it necessary for this task?

## Architecture Onboarding

- Component map: Graph Encoder -> Contrastive Learning Module -> Decoder -> Output Text
- Critical path: Input KG → Graph Encoder → Contrastive Learning Module → Decoder → Output Text. The contrastive learning module is crucial for ensuring faithfulness of the generated text.
- Design tradeoffs: Using contrastive learning and controllable text generation techniques adds complexity to the model but improves faithfulness. The tradeoff is between model complexity and performance.
- Failure signatures: If the model generates text with high hallucination rates, it could indicate issues with the contrastive learning module or the control feature tokens. If the model fails to generate fluent text, it could indicate issues with the decoder or the graph encoder.
- First 3 experiments:
  1. Evaluate the model's performance on a small dataset with known hallucinated and faithful reference texts to verify the effectiveness of contrastive learning.
  2. Test the model's ability to generate text with different levels of hallucination by varying the control feature tokens.
  3. Compare the model's performance with and without the contrastive learning module to assess its contribution to faithfulness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed framework work on AMR (Abstract Meaning Representations) graph datasets?
- Basis in paper: The authors mention in the Conclusion section that they plan to explore the hallucination problem in AMR graph datasets in future work.
- Why unresolved: The authors state this as a limitation and future work, indicating it hasn't been tested yet.
- What evidence would resolve it: Empirical results showing the framework's performance on AMR graph datasets compared to baseline models.

### Open Question 2
- Question: How does the proposed framework perform when applied to other pre-trained language models like T5 instead of BART?
- Basis in paper: The authors mention in Section 4.3 that they use BART-base for the House dataset and T5-base for the GenWiki dataset, but don't compare the performance of their framework across different pre-trained models.
- Why unresolved: The paper only reports results for specific pre-trained model choices without comparing across different models.
- What evidence would resolve it: Comparative results showing the framework's performance across different pre-trained language models.

### Open Question 3
- Question: What is the impact of different negative sample construction strategies on the contrastive learning component?
- Basis in paper: The authors describe their specific heuristic for negative sample construction in Section 3.3 but don't explore alternative strategies.
- Why unresolved: The paper only implements one specific negative sample construction method without comparing to alternatives.
- What evidence would resolve it: Results showing the impact of different negative sample construction strategies on model performance.

## Limitations
- The contrastive learning mechanism's effectiveness depends critically on the assumption that ground-truth reference texts are more faithful to their paired graphs than randomly sampled texts, which may not hold in practice.
- The controllable text generation technique relies on discrete categorization of hallucination levels, which may not capture the continuous nature of hallucination in generated text.
- The framework lacks ablation studies to isolate the contribution of each component to faithfulness improvements.

## Confidence
- Low confidence: The assumption that random text sampling provides effective negative examples for contrastive learning in noisy reference datasets.
- Medium confidence: The framework's ability to generate text with controlled hallucination levels when control token mappings are properly learned.
- Medium confidence: The overall improvement in faithfulness metrics compared to baseline models, though attribution to specific components remains unclear.

## Next Checks
1. **Ablation study validation**: Conduct controlled experiments comparing the full model against variants with only contrastive learning, only controllable generation, and a baseline model without either mechanism.
2. **Negative sampling robustness test**: Evaluate the contrastive learning performance across different negative sampling strategies, including random sampling, heuristic-based selection, and adversarial examples.
3. **Control token granularity analysis**: Perform experiments varying the number of hallucination levels and different binning strategies for BARTScore ranges to assess whether the discrete control token approach captures the continuous nature of hallucination effectively.