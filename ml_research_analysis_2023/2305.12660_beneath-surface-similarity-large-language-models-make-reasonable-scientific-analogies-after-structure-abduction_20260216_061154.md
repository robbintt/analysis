---
ver: rpa2
title: 'Beneath Surface Similarity: Large Language Models Make Reasonable Scientific
  Analogies after Structure Abduction'
arxiv_id: '2305.12660'
source_url: https://arxiv.org/abs/2305.12660
tags:
- analogy
- llms
- reasoning
- system
- analogical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models struggle with analogical reasoning tasks
  that require understanding relational structures between systems, despite performing
  well on word analogy tests. To address this, the authors propose the analogical
  structure abduction task, which evaluates models' ability to map terms between two
  systems based on their common relational structures.
---

# Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction

## Quick Facts
- arXiv ID: 2305.12660
- Source URL: https://arxiv.org/abs/2305.12660
- Authors: 
- Reference count: 25
- Key outcome: LLMs struggle with analogical reasoning tasks requiring understanding of relational structures between systems, despite performing well on word analogy tests

## Executive Summary
This paper addresses a critical gap in evaluating large language models' analogical reasoning capabilities. While LLMs excel at word analogy tasks, they struggle with deeper relational structure mapping between systems. The authors introduce the analogical structure abduction task and SCAR benchmark, featuring 400 scientific analogies across 13 domains with over 1600 term mappings. Their findings reveal that while models like GPT-4 outperform others, they still lag behind human performance. Incorporating background knowledge and Chain-of-Thought prompting with explanations significantly improves model performance, highlighting the limitations of traditional word analogy benchmarks for assessing true analogical reasoning.

## Method Summary
The authors propose the analogical structure abduction task to evaluate LLMs' ability to map terms between two systems based on common relational structures. They construct the SCAR benchmark using system analogies collected from online resources, enriched with background knowledge extracted from Wikipedia and explanations generated by GPT-4. The evaluation employs in-context learning with various LLMs, testing zero-shot and few-shot learning approaches. The study systematically incorporates background knowledge and employs Chain-of-Thought prompting with explanations to assess their impact on model performance. Results are analyzed across different domains and compared with previous benchmarks to establish SCAR's effectiveness in evaluating analogical reasoning.

## Key Results
- LLMs like GPT-4 outperform other models on analogical structure abduction but still lag considerably behind human performance
- Incorporating background knowledge and Chain-of-Thought prompting with explanations significantly improves model performance
- The SCAR benchmark provides a more comprehensive evaluation of analogical reasoning compared to existing word analogy benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle with analogical reasoning tasks requiring understanding of relational structures between systems, despite performing well on word analogy tests.
- Mechanism: The Structure Mapping Theory (SMT) posits that analogical reasoning involves identifying common relational structures across domains. However, LLMs appear to focus on surface-level word associations rather than underlying relational structures when performing word analogies, leading to poor performance on tasks requiring structure abduction.
- Core assumption: Word analogies are insufficient for evaluating analogical reasoning abilities that align with human cognition, as they do not capture the need to understand and map relational structures between systems.
- Evidence anchors:
  - [abstract]: "Previous work mainly focuses on word analogies, which do not fully represent the analogical reasoning ability of language models (LMs) aligning with humans."
  - [section 2.3]: "Despite GPT-4 exceeding human performance on most word analogy test benchmarks, it lags considerably behind humans on E-KAR... Given the difficulty of E-KAR (with more complex relations and structures), we hypothesize that the word analogy test may not accurately reflect model performance in analogical reasoning."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.474, average citations=0.0. Top related titles: LacMaterial: Large Language Models as Analogical Chemists for Materials Discovery, ARN: Analogical Reasoning on Narratives, Can LLMs Help Improve Analogical Reasoning For Strategic Decisions? Experimental Evidence from Humans and GPT-4.
- Break condition: If the word analogy test is revised to explicitly require identifying and mapping relational structures, and LLMs can perform well on this revised test, the mechanism may be invalid.

### Mechanism 2
- Claim: Incorporating background knowledge and Chain-of-Thought (CoT) prompting with explanations significantly improves LLMs' performance on analogical structure abduction tasks.
- Mechanism: Background knowledge provides context and relevant information about the systems being compared, enabling LLMs to better understand the terms and relations involved. CoT prompting with explanations guides the model through a step-by-step reasoning process, encouraging it to explicitly identify and map the relational structures between the systems.
- Core assumption: LLMs benefit from additional context and structured reasoning processes when performing complex analogical reasoning tasks that require understanding and mapping relational structures.
- Evidence anchors:
  - [abstract]: "Incorporating background knowledge and Chain-of-Thought prompting with explanations significantly improves model performance."
  - [section 4.4]: "Results in Figure 3(a), 3(b), 3(c) show that: 1) CoT prompting enhances GPT-4 performance in structure abduction but harms ChatGPT and InstructGPT003 performance due to flawed reasoning. 2) CoT prompting with explanations outperforms the 'Let's think step by step' approach, highlighting the importance of explanations in CoT prompting."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.474, average citations=0.0. Top related titles: LacMaterial: Large Language Models as Analogical Chemists for Materials Discovery, ARN: Analogical Reasoning on Narratives, Can LLMs Help Improve Analogical Reasoning For Strategic Decisions? Experimental Evidence from Humans and GPT-4.
- Break condition: If LLMs can perform well on analogical structure abduction tasks without the need for background knowledge or CoT prompting with explanations, the mechanism may be invalid.

### Mechanism 3
- Claim: The SCAR benchmark, which includes system analogies with background knowledge and explanations, provides a more comprehensive and challenging evaluation of LLMs' analogical reasoning abilities compared to existing word analogy benchmarks.
- Mechanism: SCAR focuses on system analogies, requiring LLMs to map terms between two systems based on their common relational structures. The inclusion of background knowledge and explanations provides additional context and guidance for the model, enabling a more thorough assessment of its ability to understand and reason about complex analogical relationships.
- Core assumption: System analogies with background knowledge and explanations are more representative of human-like analogical reasoning compared to word analogies, as they require understanding and mapping relational structures between systems.
- Evidence anchors:
  - [abstract]: "To better evaluate LLMs aligning with humans, we propose an analogical structure abduction task based on cognitive psychology, which aims to abduct structures between two systems to establish an analogy. For this task, we introduce a benchmark of scientific analogical reasoning with structure abduction, i.e., SCAR, comprising 400 system analogies across 13 domains with more than 1600 term mappings."
  - [section 3.2]: "We incorporate background knowledge into each system to facilitate the understanding of LMs and streamline the mapping process. To achieve this, we first use the Wikipedia API to extract the encyclopedia abstract for each system. Considering that abstracts may not include all relevant terms and could be too lengthy, we use ChatGPT to rewrite each abstract as the background."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.474, average citations=0.0. Top related titles: LacMaterial: Large Language Models as Analogical Chemists for Materials Discovery, ARN: Analogical Reasoning on Narratives, Can LLMs Help Improve Analogical Reasoning For Strategic Decisions? Experimental Evidence from Humans and GPT-4.
- Break condition: If existing word analogy benchmarks can be modified to effectively evaluate analogical reasoning abilities that align with human cognition, or if LLMs can perform well on system analogies without the need for background knowledge and explanations, the mechanism may be invalid.

## Foundational Learning

- Concept: Structure Mapping Theory (SMT)
  - Why needed here: SMT provides the theoretical foundation for understanding how analogical reasoning works in human cognition, which is essential for evaluating and improving LLMs' analogical reasoning abilities.
  - Quick check question: According to SMT, what is the primary process involved in analogical reasoning? (Answer: Identifying common relational structures across domains)
- Concept: Analogical structure abduction
  - Why needed here: Analogical structure abduction is the task of mapping terms between two systems based on their common relational structures, which is the focus of the SCAR benchmark and a key aspect of human-like analogical reasoning.
  - Quick check question: What is the goal of the analogical structure abduction task? (Answer: To establish mappings between terms in two systems based on their common relational structure to form an analogy)
- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT prompting is a technique used to guide LLMs through a step-by-step reasoning process, which can improve their performance on complex tasks like analogical structure abduction by encouraging explicit identification and mapping of relational structures.
  - Quick check question: What is the purpose of using CoT prompting in the context of analogical structure abduction? (Answer: To guide the model through a step-by-step reasoning process, encouraging it to explicitly identify and map the relational structures between the systems)

## Architecture Onboarding

- Component map: System analogies -> Background knowledge extraction -> Term mapping task -> LLM evaluation -> Performance analysis
- Critical path:
  1. Construct system analogies and gather background knowledge and explanations
  2. Design the analogical structure abduction task and evaluation metrics
  3. Implement and fine-tune LLMs for the task
  4. Evaluate LLMs' performance with and without background knowledge and CoT prompting
  5. Analyze results and iterate on the task design and LLM implementation
- Design tradeoffs:
  - Complexity of system analogies vs. generalizability of results
  - Amount of background knowledge and explanations vs. model performance and interpretability
  - Use of CoT prompting vs. model autonomy and reasoning depth
- Failure signatures:
  - LLMs perform well on word analogies but struggle with system analogies
  - Background knowledge and explanations do not improve model performance
  - CoT prompting leads to superficial or incorrect reasoning
- First 3 experiments:
  1. Evaluate LLMs' performance on the analogical structure abduction task without any additional guidance (baseline)
  2. Evaluate LLMs' performance with background knowledge and explanations provided
  3. Evaluate LLMs' performance with CoT prompting and explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the analogical reasoning ability of large language models (LLMs) be further improved to align with human cognition?
- Basis in paper: The paper discusses the limitations of LLMs in analogical reasoning and proposes the analogical structure abduction task to better evaluate and enhance their abilities.
- Why unresolved: The paper acknowledges that LLMs, including GPT-4, still struggle with the analogical structure abduction task and lag behind human performance.
- What evidence would resolve it: Conducting further experiments with more advanced models, larger datasets, and incorporating additional techniques like reinforcement learning or meta-learning could provide insights into improving LLMs' analogical reasoning capabilities.

### Open Question 2
- Question: How does the performance of LLMs in analogical reasoning vary across different domains and languages?
- Basis in paper: The paper presents a bilingual benchmark (SCAR) and analyzes the performance of LLMs in different domains, but does not provide a comprehensive comparison across languages.
- Why unresolved: The paper focuses on the English and Chinese versions of SCAR but does not explore other languages or provide a detailed analysis of domain-specific performance.
- What evidence would resolve it: Extending the benchmark to include more languages and conducting experiments to compare LLM performance across domains and languages would help understand the generalizability and limitations of analogical reasoning in LLMs.

### Open Question 3
- Question: How does the incorporation of background knowledge and explanations impact the analogical reasoning ability of LLMs?
- Basis in paper: The paper introduces background knowledge and explanations in the SCAR benchmark and shows that they can improve model performance in the analogical structure abduction task.
- Why unresolved: The paper does not provide a detailed analysis of the impact of background knowledge and explanations on different LLM models or explore the potential for further improvements using these techniques.
- What evidence would resolve it: Conducting experiments with various LLM models, incorporating different types of background knowledge, and analyzing the impact of explanations on model performance would provide insights into the role of background knowledge and explanations in enhancing analogical reasoning abilities.

## Limitations

- The study focuses exclusively on scientific analogies, limiting generalizability to other domains and types of analogical reasoning
- Reliance on Wikipedia-based background knowledge may introduce systematic biases based on the completeness and accuracy of Wikipedia content
- The Chain-of-Thought prompting mechanism introduces additional complexity that may not be practical for all real-world applications

## Confidence

- High Confidence: LLMs struggle with relational structure mapping despite strong word analogy performance
- Medium Confidence: Effectiveness of background knowledge incorporation
- Low Confidence: Generalizability of SCAR benchmark results to non-scientific domains

## Next Checks

1. Test model performance on non-scientific analogies (e.g., everyday situations, artistic comparisons) to assess domain transferability and identify whether the relational structure mapping challenge persists across domains.

2. Conduct ablation studies removing Wikipedia background knowledge to determine if improvements are due to the knowledge content itself or other factors like increased context length or the structured presentation of information.

3. Evaluate model performance when given incomplete or contradictory background knowledge to understand robustness and identify failure modes in real-world scenarios where perfect information is unavailable.