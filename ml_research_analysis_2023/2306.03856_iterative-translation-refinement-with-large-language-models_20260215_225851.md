---
ver: rpa2
title: Iterative Translation Refinement with Large Language Models
arxiv_id: '2306.03856'
source_url: https://arxiv.org/abs/2306.03856
tags:
- translation
- language
- machine
- human
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work explores iterative refinement of machine translation
  outputs using large language models (LLMs). The approach involves iteratively prompting
  GPT-3.5 to improve a translation by providing the source sentence and the current
  translation as context.
---

# Iterative Translation Refinement with Large Language Models

## Quick Facts
- arXiv ID: 2306.03856
- Source URL: https://arxiv.org/abs/2306.03856
- Reference count: 16
- Primary result: Iterative refinement reduces translationese and improves naturalness while maintaining semantic quality, as shown by human evaluations preferring refined translations over both initial GPT outputs and human references for into-English directions.

## Executive Summary
This work introduces an iterative refinement approach using large language models (LLMs) to improve machine translation quality by reducing translationese and enhancing naturalness. The method involves repeatedly prompting GPT-3.5 to refine a translation while providing both the source sentence and current translation as context. Experiments across multiple language pairs show that while traditional string-based metrics like BLEU drop significantly due to textual changes, neural quality metrics indicate comparable or improved quality. Human evaluations reveal that refined translations are preferred over initial GPT outputs and even human references for into-English directions, demonstrating the effectiveness of this approach in producing more natural translations.

## Method Summary
The method employs iterative refinement using GPT-3.5-turbo to improve translation quality. Starting with an initial translation (either from GPT or conventional MT systems), the LLM is prompted to refine the output by providing both the source sentence and current translation as context. The process iterates up to four times, with the best iteration selected based on COMET QE scores. Five different prompting strategies are explored: Translate (zero-shot generation), Refine (source-anchored refinement), RefineContrast (with contrastive "bad" signal), RefineRandom (anchored to random target), and Paraphrase (no source). The approach aims to reduce translationese by allowing the LLM to restructure translations while maintaining semantic fidelity through source anchoring.

## Key Results
- Iterative refinement reduces BLEU and chrF++ scores significantly due to textual changes, but COMET neural metrics show comparable or improved quality
- Human evaluations prefer refined translations over both initial GPT outputs and human references for into-English directions
- Source anchoring proves critical - ablation studies show refinement fails without source context or with poor initial translations
- RefineContrast prompting (adding "bad" signal) often outperforms standard Refine, suggesting contrastive signals help guide more effective refinement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative refinement with source anchoring improves naturalness and reduces translationese.
- Mechanism: Providing both source and current translation to LLM allows it to iteratively rewrite output while maintaining semantic fidelity. The source acts as an anchor preventing semantic drift.
- Core assumption: LLMs can effectively use source context to guide rewrites that improve fluency without changing meaning.
- Evidence anchors:
  - [abstract] "Ablation studies underscore the importance of anchoring the refinement process to the source input and a reasonable initial translation."
  - [section 2.2] "Refine: similar to APE, the LLM is given the source sentence and the previous translation to produce a better translation"
  - [corpus] Weak - only 5 related papers found, none directly address source-anchored iterative refinement for translation
- Break condition: If source context is removed or if initial translation is too poor, refinement may drift semantically or fail to improve quality.

### Mechanism 2
- Claim: Contrastive prompting (RefineContrast) improves refinement quality by signaling low quality.
- Mechanism: Adding "bad" to prompt text creates a contrastive signal that encourages LLM to make more substantial improvements rather than minor tweaks.
- Core assumption: LLMs respond to quality signals in prompts and can use them to guide more aggressive rewriting.
- Evidence anchors:
  - [section 3.2] "RefineContrast: as a contrastive prompt to the above, we insert the word 'bad' to hint that the previous translation is of low quality, regardless of its actual quality."
  - [section 3.2] Results show RefineContrast often outperforms standard Refine
  - [corpus] Weak - no direct evidence of contrastive prompting effectiveness for translation refinement
- Break condition: If contrastive signal is overused or applied to already good translations, it may lead to unnecessary changes or degradation.

### Mechanism 3
- Claim: Iterative refinement produces lexical and structural variation while maintaining quality.
- Mechanism: Multiple refinement rounds allow LLM to progressively restructure translation, reducing translationese patterns while maintaining semantic accuracy.
- Core assumption: LLMs can generate multiple valid translations of the same meaning, and iteration allows exploration of this space.
- Evidence anchors:
  - [abstract] "Interestingly, multi-turn querying reduces the output's string-based metric scores, but neural metrics suggest comparable or improved quality."
  - [section 3.2] "Refine and RefineContrast usually attain their best after the first iteration, but in almost all Paraphrase experiments, scores decrease monotonically"
  - [corpus] Weak - limited related work on iterative refinement effects on translation metrics
- Break condition: Excessive iterations without source anchoring lead to semantic drift and quality degradation.

## Foundational Learning

- Concept: BLEU and string-based metrics vs neural metrics (COMET)
  - Why needed here: Understanding why BLEU drops while COMET improves is crucial for interpreting results
  - Quick check question: If a refined translation has 10% lower BLEU but 5% higher COMET than original, which is likely better quality and why?

- Concept: Translationese and its detection
  - Why needed here: The core problem being addressed is translationese, so understanding what it is and how to measure it is essential
  - Quick check question: How would you design a human evaluation to detect translationese without reference to source text?

- Concept: Zero-shot prompting and prompt engineering
  - Why needed here: The entire method relies on carefully crafted prompts to guide LLM behavior
  - Quick check question: What's the difference between hard prompting and few-shot prompting, and why did this work choose hard prompting?

## Architecture Onboarding

- Component map: Source sentence + Initial translation -> Iterative refinement loop (GPT-3.5-turbo) -> Refined translation -> Evaluation (BLEU, chrF++, COMET, Human)

- Critical path:
  1. Generate initial translation
  2. For each iteration:
     - Construct prompt with source + current translation
     - Query LLM
     - Check stopping condition (max iterations or quality plateau)
  3. Return best iteration based on COMET QE

- Design tradeoffs:
  - Source inclusion vs. paraphrase-only: Source anchoring maintains quality but limits creative freedom
  - Number of iterations: More iterations allow more refinement but increase cost and risk of drift
  - Prompt variation: Different prompt formulations (Refine, RefineContrast) have different effectiveness

- Failure signatures:
  - BLEU drops significantly (>10%) while COMET doesn't improve → semantic drift
  - Both BLEU and COMET drop → poor initial translation or ineffective refinement
  - Human evaluation shows no preference for refined output → refinement not improving naturalness

- First 3 experiments:
  1. Run Refine with source anchoring on high-resource en→de translation, compare BLEU and COMET scores across iterations
  2. Run RefineContrast vs. standard Refine on same data, measure effect of contrastive prompting
  3. Run Paraphrase (no source) on same data, measure semantic drift over iterations

## Open Questions the Paper Calls Out

- Question: How does the iterative refinement approach perform with open-source LLMs compared to GPT-3.5?
- Basis in paper: [explicit] The authors mention that future work could explore open-source LLMs other than GPT.
- Why unresolved: The paper only experiments with GPT-3.5, so the performance with other models is unknown.
- What evidence would resolve it: Conducting experiments with open-source LLMs like LLaMA or BLOOM using the same iterative refinement approach and comparing results.

- Question: Does the reduction in translationese observed in the study generalize to language pairs beyond high-resource languages?
- Basis in paper: [inferred] The study focuses on high-resource language pairs (en-de, en-zh) and low-resource/medium-resource pairs. The authors note that translationese is prevalent in various stages and language pairs.
- Why unresolved: The human evaluation was limited to into-English and into-Chinese directions. Translationese patterns may differ across language families.
- What evidence would resolve it: Conducting human evaluations of iterative refinement outputs for a diverse set of language pairs, including low-resource and non-European languages.

- Question: What is the impact of iterative refinement on semantic preservation compared to lexical and structural changes?
- Basis in paper: [explicit] The authors observe significant textual changes but comparable or improved neural metric scores. They note that volatile textual changes without metric degradation could reflect a change in the degree of translationese.
- Why unresolved: While the study shows that BLEU scores drop significantly, it's unclear how well the refined translations preserve the original meaning. The neural metrics used may not fully capture semantic fidelity.
- What evidence would resolve it: Conducting detailed semantic evaluations, such as entailment tests or meaning preservation scores, to quantify the trade-off between translationese reduction and semantic accuracy in the refined translations.

## Limitations

- Evaluation relies heavily on GPT-3.5-turbo, making results unclear for generalization to other LLMs or conventional MT systems
- Human evaluation only assessed fluency/naturalness without explicit translationese comparison, limiting understanding of claimed translationese reduction
- Study focuses exclusively on high-resource language pairs, leaving questions about effectiveness for low-resource languages

## Confidence

**High confidence**: The observation that iterative refinement with source anchoring reduces string-based metric scores (BLEU, chrF++) while maintaining or improving neural quality metrics (COMET DA, COMET QE) is well-supported by the experimental data across multiple language pairs and conditions.

**Medium confidence**: The claim that iterative refinement reduces translationese and produces more natural translations is partially supported by human evaluation showing preferences for refined outputs over both initial translations and human references, though the methodology for assessing translationese is indirect and the unexpected preference for refined translations over human references raises questions.

**Low confidence**: The assertion that contrastive prompting (RefineContrast) consistently improves refinement quality is weakly supported, as results show it only sometimes outperforms standard Refine, and the mechanism by which the "bad" signal improves outcomes is not well-established.

## Next Checks

1. **Replicate with diverse MT systems**: Run the same iterative refinement pipeline on outputs from conventional MT systems (e.g., Google Translate, DeepL, commercial systems) rather than just GPT-3.5-turbo to test generalizability beyond LLM-generated translations.

2. **Controlled human evaluation for translationese**: Design a human evaluation specifically targeting translationese detection, where raters compare translations to source text to identify unnatural patterns, source language interference, and other translationese markers.

3. **Long-form text analysis**: Test the iterative refinement approach on longer text segments (multiple sentences or paragraphs) to evaluate whether the quality improvements observed in sentence-level translation extend to document-level coherence and consistency.