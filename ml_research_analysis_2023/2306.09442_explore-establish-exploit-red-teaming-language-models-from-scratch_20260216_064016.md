---
ver: rpa2
title: 'Explore, Establish, Exploit: Red Teaming Language Models from Scratch'
arxiv_id: '2306.09442'
source_url: https://arxiv.org/abs/2306.09442
tags:
- arxiv
- would
- language
- prompts
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a framework for red teaming language models
  from scratch by systematically exploring model behavior, establishing measures for
  undesired outputs, and exploiting model vulnerabilities. The approach involves three
  steps: 1) exploring the model''s outputs through diverse sampling, 2) establishing
  a classifier for harmful text through human labeling or other methods, and 3) training
  an adversarial prompt generator to elicit harmful outputs.'
---

# Explore, Establish, Exploit: Red Teaming Language Models from Scratch

## Quick Facts
- arXiv ID: 2306.09442
- Source URL: https://arxiv.org/abs/2306.09442
- Reference count: 40
- Primary result: A framework for red teaming language models from scratch achieves 31% toxic completion rate and 74% dishonest completion rate using adversarial prompts

## Executive Summary
This paper introduces a systematic framework for red teaming language models by exploring model behavior, establishing custom harmfulness measures, and exploiting vulnerabilities through adversarial prompts. The approach addresses the challenge of red teaming when no pre-existing classifier exists for the specific harm type. The authors apply this framework to GPT-2-xl for toxicity detection and GPT-3 for dishonesty detection, demonstrating that red teaming from scratch is both feasible and can be more effective than using pre-existing classifiers.

## Method Summary
The framework consists of three steps: (1) Explore - sample diverse model outputs using K-means clustering on embeddings, (2) Establish - create a harmfulness classifier through human labeling or alternatives, and (3) Exploit - use reinforcement learning to generate adversarial prompts that elicit harmful outputs. The approach uses GPT-2-large as a prompt generator fine-tuned with rewards combining harmfulness confidence and prompt diversity. For the dishonesty task, the authors create the CommonClaim dataset of 20,000 statements labeled as common-knowledge-true, false, or neither.

## Key Results
- GPT-2-xl toxicity experiment: Adversarial prompts achieved 31% toxic completion rate vs <1% without prompts
- GPT-3 dishonesty experiment: Adversarial prompts achieved 74% dishonest completion rate vs 30% in explore data
- CommonClaim dataset: 20,000 statements labeled by human subjects across three categories
- Diversity term importance: Removing diversity reward caused prompt generator to collapse to narrow prompts with 0% success

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework works by systematically exploring model outputs to find diverse behaviors before establishing any harmfulness measure.
- Mechanism: In the Explore step, the approach uses K-means clustering on model embeddings to subsample a diverse set of outputs. This ensures coverage of different behavioral regions in the model's output space before any labels or classifiers exist.
- Core assumption: Model embeddings meaningfully capture behavioral diversity such that clustering reveals distinct output types.
- Evidence anchors:
  - [abstract] "exploring the model's outputs through diverse sampling"
  - [section] "we sample paragraphs at a time and parse them into individual sentences... embed each sentence using the last token activations... use K-means clustering to partition the embeddings into 100 clusters"
  - [corpus] Weak - the corpus provides related red teaming work but doesn't directly support the clustering approach.
- Break condition: If model embeddings are not informative about output diversity, clustering will fail to capture meaningful behavioral differences.

### Mechanism 2
- Claim: The framework enables effective red teaming by establishing custom harmfulness measures through human deliberation rather than relying on pre-existing classifiers.
- Mechanism: In the Establish step, human labelers define harmfulness in the specific deployment context (e.g., "common-knowledge-true/false/neither") and train classifiers on these labels, creating context-appropriate measures.
- Core assumption: Human deliberation can create more appropriate harmfulness measures than generic pre-existing classifiers.
- Evidence anchors:
  - [abstract] "establishing a classifier for harmful text through human labeling"
  - [section] "Important to this step is human interaction with the model's outputs. Human preferences are highly context-dependent"
  - [corpus] Weak - related work exists but doesn't directly validate human labeling superiority.
- Break condition: If human labeling is too slow or expensive relative to the benefits of custom measures.

### Mechanism 3
- Claim: The framework achieves high success rates by using reinforcement learning to generate diverse adversarial prompts that exploit discovered vulnerabilities.
- Mechanism: In the Exploit step, RL is used to train a prompt generator with rewards based on both harmfulness classifier confidence and prompt diversity (cosine distance), preventing mode collapse.
- Core assumption: RL with diversity rewards can effectively generate varied prompts that consistently trigger harmful outputs.
- Evidence anchors:
  - [abstract] "training an adversarial prompt generator to elicit harmful outputs"
  - [section] "we use reinforcement learning to train an adversarial prompt generator to produce a diverse distribution of prompts"
  - [corpus] Moderate - related work shows RL can generate adversarial prompts, though diversity remains challenging.
- Break condition: If the RL reward signal is too sparse or noisy, the generator will fail to find effective prompts.

## Foundational Learning

- Concept: K-means clustering on model embeddings
  - Why needed here: To efficiently subsample diverse outputs from large sets of model-generated text
  - Quick check question: How does K-means clustering on embeddings help identify behaviorally distinct outputs?

- Concept: Reinforcement learning with diversity rewards
  - Why needed here: To generate varied adversarial prompts while avoiding mode collapse
  - Quick check question: Why include both harmfulness confidence and cosine distance in the RL reward function?

- Concept: Human deliberation for harmfulness definitions
  - Why needed here: To create context-appropriate measures rather than relying on generic classifiers
  - Quick check question: What makes human-labeled harmfulness measures potentially better than pre-existing classifiers?

## Architecture Onboarding

- Component map:
  - Explore: Model sampling → Embedding generation → K-means clustering → Diverse subsampling
  - Establish: Human labeling (or alternatives) → Classifier training → Harmfulness measure creation
  - Exploit: RL prompt generator training → Adversarial prompt generation → Harmful output elicitation

- Critical path: Establish → Exploit (classification must exist before RL can be trained)

- Design tradeoffs:
  - Explore: Sampling efficiency vs. coverage comprehensiveness
  - Establish: Human labeling cost vs. classifier quality
  - Exploit: Prompt diversity vs. harmfulness elicitation success rate

- Failure signatures:
  - Explore fails: K-means clusters show no meaningful behavioral differences
  - Establish fails: Classifier cannot achieve reasonable accuracy on labeled data
  - Exploit fails: RL generator produces repetitive prompts or fails to trigger harmful outputs

- First 3 experiments:
  1. Test K-means clustering on embeddings from a small sample to verify behavioral diversity capture
  2. Run Establish step with a small labeled dataset to validate classifier training pipeline
  3. Test Exploit step with a simple RL setup using synthetic reward signals before full pipeline integration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is the diversity term in the adversarial prompt generator's reward function?
- Basis in paper: [explicit] The paper states that when the diversity term was removed in the toxicity red teaming experiment with GPT-2-xl, the prompt generator collapsed onto an extremely narrow distribution of prompts that elicited toxic completions 0% of the time.
- Why unresolved: While the paper demonstrates the importance of the diversity term in this specific case, it does not explore the broader implications or optimal settings for the diversity term in different contexts or models.
- What evidence would resolve it: Further experiments testing the impact of the diversity term across various models, tasks, and settings, and comparing the results with and without the diversity term.

### Open Question 2
- Question: How well can LLMs be used to evaluate the outputs of other LLMs?
- Basis in paper: [explicit] The paper mentions using ChatGPT-3.5-turbo to label examples in the dishonesty red teaming experiment with GPT-3-text-davinci-002, but found only 54% agreement with human labelers.
- Why unresolved: The paper suggests that using AI feedback may not be effective for red teaming, but it does not explore alternative approaches or provide a comprehensive analysis of the limitations and potential of LLMs in this role.
- What evidence would resolve it: Additional experiments comparing the performance of LLMs and human labelers across different tasks, datasets, and labeling strategies, and identifying the factors that contribute to the success or failure of LLMs in this role.

### Open Question 3
- Question: How can red teaming be tailored to specific deployment contexts and harmful behaviors?
- Basis in paper: [explicit] The paper introduces a framework for red teaming that involves exploring model behavior, establishing a measure for undesired outputs, and exploiting model vulnerabilities. However, it does not provide specific guidelines for adapting this framework to different contexts or harmful behaviors.
- Why unresolved: The paper demonstrates the feasibility of red teaming from scratch, but it does not address the challenges of tailoring the approach to specific situations or identifying the most effective strategies for different types of harmful behaviors.
- What evidence would resolve it: Case studies or experiments applying the red teaming framework to various deployment contexts and harmful behaviors, and identifying best practices, challenges, and lessons learned in each case.

## Limitations
- The framework's reliance on human deliberation for establishing harmfulness measures presents a significant scalability limitation
- Success rates are measured against classifiers rather than ground truth human judgments in the Exploit phase
- The approach requires access to internal model activations for effective diversity sampling

## Confidence

**High Confidence**: The core three-step framework (Explore-Establish-Exploit) is well-specified and the methodology for each step is clearly defined. The reported success metrics for both toxicity and dishonesty experiments are reproducible given the described experimental setup.

**Medium Confidence**: The claim that human-labeled measures are superior to pre-existing classifiers for specific contexts. While the paper argues for human deliberation's importance, it only briefly tests alternative approaches and doesn't provide comprehensive comparisons.

**Low Confidence**: The assertion that the framework can be applied to any high-level specification of undesired behavior. The paper demonstrates success on toxicity and dishonesty but doesn't explore other harm types or show the framework's generalizability across diverse harm categories.

## Next Checks

1. **Diversity Sampling Validation**: Test whether K-means clustering on model embeddings actually captures behaviorally distinct outputs by manually inspecting samples from different clusters and measuring completion diversity metrics during RL training with and without the diversity reward term.

2. **Classifier Transfer Validation**: Evaluate the classifiers trained in the Establish step against human judgments on held-out examples to measure how well classifier-defined harmfulness aligns with human-defined harmfulness, particularly for the dishonesty task where ChatGPT and CREAK labels showed lower alignment.

3. **Alternative Labeling Source Comparison**: Implement the Establish step using only pre-existing classifiers or heuristic rules (as suggested as alternatives) and compare the resulting adversarial prompt generator's performance against the human-labeled version to quantify the practical value of human deliberation.