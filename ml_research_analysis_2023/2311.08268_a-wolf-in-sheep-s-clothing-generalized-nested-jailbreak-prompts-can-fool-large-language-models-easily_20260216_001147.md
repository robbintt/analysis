---
ver: rpa2
title: 'A Wolf in Sheep''s Clothing: Generalized Nested Jailbreak Prompts can Fool
  Large Language Models Easily'
arxiv_id: '2311.08268'
source_url: https://arxiv.org/abs/2311.08268
tags:
- llms
- prompt
- prompts
- jailbreak
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a generalized jailbreak prompt generation framework
  called ReNeLLM to improve the attack success rate while reducing time cost. ReNeLLM
  leverages LLMs themselves to generate effective jailbreak prompts by rewriting and
  nesting initial harmful prompts in various task scenarios.
---

# A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily

## Quick Facts
- arXiv ID: 2311.08268
- Source URL: https://arxiv.org/abs/2311.08268
- Reference count: 11
- Key outcome: ReNeLLM framework significantly improves jailbreak attack success rates while reducing time cost and demonstrating high transferability across LLMs

## Executive Summary
This paper introduces ReNeLLM, a novel framework for automatically generating jailbreak prompts that can bypass safety alignment mechanisms in large language models. By leveraging LLM-based prompt rewriting and scenario nesting, ReNeLLM significantly outperforms existing baseline methods in attack success rate while reducing computational time. The framework demonstrates effectiveness across multiple open-source and commercial LLMs, revealing critical vulnerabilities in current safety alignment approaches.

## Method Summary
ReNeLLM is a generalized framework that automatically generates jailbreak prompts through a two-stage process: prompt rewriting and scenario nesting. The framework takes harmful prompts as input and applies six rewriting operations (paraphrasing, sentence structure alteration, misspelling, inserting meaningless characters, partial translation, and changing expression style) using LLM calls. It then nests the rewritten prompts into common task scenarios (code completion, table filling, text continuation) that align with the model's training objectives. The system uses a harmful content classifier to evaluate prompts and selects appropriate scenarios for nesting, ultimately generating multiple variants that can bypass safety filters.

## Key Results
- ReNeLLM significantly improves attack success rates compared to baselines while reducing time cost
- The framework demonstrates high transferability across multiple open-source and commercial LLMs
- Scenario nesting increases attack success rates by 14.3% compared to using only rewritten prompts
- Larger LLMs (70B parameters) show higher vulnerability to attacks than smaller models (7B, 13B)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReNeLLM leverages LLM-based prompt rewriting to bypass safety alignment filters by disguising harmful intent semantically without altering core meaning.
- Mechanism: The framework applies six rewriting functions to transform initial harmful prompts into semantically equivalent but less detectable forms.
- Core assumption: Safety alignment mechanisms rely heavily on surface-level pattern matching; when these are disrupted while preserving core harmful intent, the model's safety filters fail.
- Evidence anchors: [abstract] "rewriting the original harmful prompts without altering their core semantics may increase the probability of LLMs generating objectionable responses" and [section 3.2] "rewriting at the word or sentence level without changing their semantics can make the prompts harder for LLMs to recognize"
- Break condition: If safety alignment implements deeper semantic understanding and context-aware filtering.

### Mechanism 2
- Claim: Scenario nesting exploits LLMs' instruction-following capabilities trained through supervised fine-tuning to increase attack success rates.
- Mechanism: ReNeLLM embeds rewritten harmful prompts into common task scenarios that align with the model's training objectives and pre-training data distribution.
- Core assumption: LLMs prioritize task completion and instruction following over safety considerations when prompts are framed as legitimate tasks.
- Evidence anchors: [section 3.3] "nesting the rewritten prompts in these instruction scenarios is more likely to elicit undesirable responses from LLMs" and [section 3.3] "incorporating code data into pre-training or SFT data may potentially be a crucial factor in enhancing the inference and reasoning capability of LLMs"
- Break condition: If safety alignment incorporates scenario-aware filtering that recognizes when task completion instructions mask harmful requests.

### Mechanism 3
- Claim: The combination of prompt rewriting and scenario nesting creates a transferable attack pattern that works across different LLM architectures and sizes.
- Mechanism: By generating multiple variants through different combinations of rewriting operations and scenario nesting, ReNeLLM creates diverse attack prompts that exploit common vulnerabilities across LLMs.
- Core assumption: Despite architectural differences, LLMs share common training patterns and safety alignment weaknesses that can be exploited through systematic prompt manipulation.
- Evidence anchors: [section 4.2] "it exhibits high ASR across all LLMs, demonstrating its universality" and "llama-2-chat-70b even reached an ASR of 62.8%, exceeding 7b and 13b by as much as 10%"
- Break condition: If future LLMs implement architecture-specific safety measures or if training methodologies diverge significantly.

## Foundational Learning

- Concept: Prompt engineering and adversarial attack methodologies
  - Why needed here: Understanding how subtle modifications to input prompts can manipulate LLM outputs is fundamental to both attacking and defending against jailbreak attempts
  - Quick check question: What are the key differences between manual jailbreak prompts and automated jailbreak prompt generation methods?

- Concept: Language model safety alignment techniques
  - Why needed here: Knowing how safety alignment works (RLHF, SFT, data filtering) helps understand why certain attack strategies succeed in bypassing these mechanisms
  - Quick check question: How do reinforcement learning from human feedback (RLHF) and supervised fine-tuning (SFT) contribute to LLM safety alignment?

- Concept: Transfer learning and model generalization principles
  - Why needed here: Understanding how training data and objectives transfer across different model sizes and architectures explains the observed transferability of ReNeLLM attacks
  - Quick check question: Why might larger language models with more training data still be vulnerable to the same attack patterns as smaller models?

## Architecture Onboarding

- Component map: Initial prompt → Random rewrite operation selection → LLM-based rewriting → Harmful content classification → Scenario nesting → Transfer to target LLM → Output evaluation
- Critical path: Initial prompt → Random rewrite operation selection → LLM-based rewriting → Harmful content classification → Scenario nesting → Transfer to target LLM → Output evaluation
- Design tradeoffs:
  - Automation vs. control: Fully automated generation sacrifices fine-grained control but enables scalability
  - Randomness vs. optimization: Random selection of operations provides diversity but may miss optimal combinations
  - Generality vs. specificity: Broad scenario selection works across models but may be less effective than model-specific attacks
- Failure signatures:
  - High rejection rate from harmful classifier indicates prompts becoming too benign
  - Low transferability suggests attack patterns are too model-specific
  - Time inefficiency compared to baselines indicates suboptimal rewrite combinations
- First 3 experiments:
  1. Test each individual rewrite function (paraphrase, misspelling, translation, etc.) in isolation to establish baseline effectiveness
  2. Evaluate different combinations of two rewrite functions to identify synergistic effects
  3. Compare scenario nesting effectiveness across the three different task scenarios to determine optimal nesting strategy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would a defense strategy be that focuses on detecting and blocking jailbreak prompts based on semantic similarity to known harmful prompts, rather than relying solely on perplexity or human feedback?
- Basis in paper: [inferred] The paper demonstrates the inadequacy of existing defense methods, including perplexity filters and human feedback-based alignment, in effectively safeguarding LLMs against jailbreak attacks.
- Why unresolved: The paper does not explore the effectiveness of a defense strategy based on semantic similarity detection.
- What evidence would resolve it: Conducting experiments to evaluate the performance of a semantic similarity-based defense strategy against the ReNeLLM jailbreak attack method.

### Open Question 2
- Question: What is the optimal balance between model size, training data, and security alignment techniques to ensure both high performance and robust safety for LLMs?
- Basis in paper: [inferred] The paper observes different phenomena on the open-source llama-2-chat series models, with llama-2-chat-70b exhibiting an ASR exceeding 7b and 13b models despite claims of extensive security alignment work.
- Why unresolved: The paper does not provide a clear answer on the optimal balance between model size, training data, and security alignment techniques.
- What evidence would resolve it: Conducting a comprehensive study comparing the security performance of LLMs with varying model sizes, training data, and security alignment techniques.

### Open Question 3
- Question: How can the generalizability of jailbreak attacks be addressed in future defense strategies for LLMs?
- Basis in paper: [explicit] The paper introduces ReNeLLM, a generalized framework for automatic jailbreak prompt generation, which demonstrates high transferability across multiple open-source and commercial LLMs.
- Why unresolved: The paper does not provide specific recommendations for addressing the generalizability of jailbreak attacks in future defense strategies.
- What evidence would resolve it: Developing and evaluating defense strategies that are specifically designed to address the generalizability of jailbreak attacks, such as by incorporating techniques to detect and block variations of known harmful prompts.

## Limitations

- Evaluation relies on GPT-4 as harmfulness classifier, which may introduce bias and not accurately reflect human judgment
- Attack success rate metric alone may not capture nuanced effectiveness or potential for partial bypasses
- Focus on English-language prompts and well-documented harmful behaviors may overlook more sophisticated or multilingual attack strategies

## Confidence

**High Confidence:** The claim that ReNeLLM significantly improves attack success rates compared to baselines is well-supported by experimental results, showing consistent improvements across multiple model sizes and open-source LLMs.

**Medium Confidence:** The assertion that ReNeLLM demonstrates high transferability across different LLM architectures is supported by experimental evidence but may be limited by the specific models tested and the static nature of attack patterns.

**Low Confidence:** The broader claim that current defense methods are inadequate against generalized attacks is based on ReNeLLM's success but doesn't account for potential defense mechanisms that weren't tested or future developments in safety alignment.

## Next Checks

1. **Human Evaluation Validation:** Conduct a human evaluation study where multiple independent reviewers assess the harmfulness of LLM responses to ReNeLLM-generated prompts, comparing results with the GPT-4 classifier to validate the ASR metric.

2. **Cross-Lingual Testing:** Test ReNeLLM's effectiveness on non-English harmful prompts and scenarios to evaluate whether the attack strategy generalizes across languages and cultural contexts.

3. **Defense Adaptation Test:** Implement a simple defense mechanism that detects prompt rewriting patterns and measure how quickly ReNeLLM-generated prompts can be adapted to bypass this defense, providing insight into the framework's resilience to countermeasures.