---
ver: rpa2
title: Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation
arxiv_id: '2310.17146'
source_url: https://arxiv.org/abs/2310.17146
tags:
- annotations
- variance
- counterfactual
- bias
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new framework for semi-offline policy evaluation
  in reinforcement learning by incorporating human-provided counterfactual annotations
  of unobserved trajectories. The key idea is to augment importance sampling estimators
  with a novel weighting scheme that incorporates counterfactual annotations without
  introducing bias.
---

# Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation

## Quick Facts
- arXiv ID: 2310.17146
- Source URL: https://arxiv.org/abs/2310.17146
- Reference count: 40
- This paper proposes a new framework for semi-offline policy evaluation in reinforcement learning by incorporating human-provided counterfactual annotations of unobserved trajectories.

## Executive Summary
This paper introduces a framework for semi-offline policy evaluation that augments importance sampling estimators with counterfactual annotations of unobserved trajectories. The key innovation is a novel weighting scheme that incorporates these annotations without introducing bias, allowing for more reliable evaluation in high-stakes domains where online deployment is risky. The approach reduces both bias and variance compared to standard importance sampling under certain conditions on the annotations, with empirical results demonstrating effectiveness even with imperfect annotations.

## Method Summary
The method builds on importance sampling for off-policy evaluation by introducing counterfactual annotations that describe what would have happened if different actions had been taken in observed states. These annotations are incorporated through a carefully designed weighting scheme that splits each factual sample into multiple weighted contributions - one for the factual action and one for each counterfactual action. This preserves the original state distribution while expanding the support of the behavior policy, enabling evaluation of policies that would otherwise have undefined importance ratios.

## Key Results
- Theoretical proofs show C-IS estimator is unbiased when annotations perfectly match evaluation policy's Q-function and common support holds
- Empirical results demonstrate variance reduction compared to standard IS in synthetic and healthcare-inspired domains
- The method maintains effectiveness even with imperfect annotations, making it practical for real-world applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual annotations reduce variance by providing additional data that are closer to the target policy distribution
- Mechanism: When the behavior policy has poor support for certain state-action pairs, counterfactual annotations act as synthetic data for those pairs. This allows importance ratios to be computed over a broader support, reducing the variance that would otherwise arise from large importance weights on rare events
- Core assumption: Annotations accurately reflect the expected returns under the evaluation policy (or a known mapping from behavior to evaluation policy)
- Evidence anchors:
  - [abstract] "Our framework, combined with principled human-centered design of annotation solicitation, can enable the application of RL in high-stakes domains."
  - [section 3.2] "Definition 1 (Augmented behavior policy). πb+(a|s) = ¯W (a|s, a)πb(a|s) + Pˇa∈A\{a} ¯W (a|s, ˇa)πb(ˇa|s)."
  - [corpus] "Off-policy evaluation (OPE) estimates the value of a contextual bandit policy prior to deployment... However, standard OPE approaches are limited by the size and coverage of the behavior dataset."
- Break condition: If annotations are missing for all unsupported actions, or if annotation noise dominates the signal, variance reduction is lost

### Mechanism 2
- Claim: Counterfactual-augmented importance sampling maintains unbiasedness by reweighting factual and counterfactual contributions
- Mechanism: Instead of adding counterfactual samples directly (which changes the state distribution), the framework splits each factual sample into multiple weighted contributions—one for the factual action and one for each counterfactual action. This preserves the original state distribution while incorporating counterfactual information
- Core assumption: The weights assigned to counterfactual actions sum to one and are only non-zero when annotations are available
- Evidence anchors:
  - [section 3.2] "we want to split the contribution of each sample between the factual data and counterfactual annotations."
  - [section 4.1] "Theorem 1 (Unbiasedness of C-IS). In the bandit setting, when both Assumptions 1 and 4 hold, the C-IS estimator is unbiased, E[ˆvC-IS] = v(πe)."
  - [corpus] "While tempting to simply augment existing data with such annotations, we show that this naive approach can lead to biased results."
- Break condition: If weights violate the sum-to-one constraint or are assigned incorrectly (e.g., zero weight on available annotations), the estimator becomes biased

### Mechanism 3
- Claim: Counterfactual annotations can compensate for distribution shift in offline evaluation by expanding the support of the behavior policy
- Mechanism: The augmented behavior policy πb+ combines the original behavior policy with the counterfactual weight distribution, effectively increasing the support of actions observed in the dataset. This allows evaluation of policies that would otherwise have undefined importance ratios under the original πb
- Core assumption: There exists at least one counterfactual annotation for each action that the evaluation policy might take in states with poor support
- Evidence anchors:
  - [section 4.1] "Assumption 4 (Common support with annotations). πe(a|s) > 0 → πb+(a|s) > 0, ∀s ∈ S , a ∈ A."
  - [section 4.4] "Counterfactual annotations can make up for regions of the state-action space with poor support in the offline dataset."
  - [corpus] "we propose a semi-offline evaluation framework as an intermediate step between offline and online evaluation, where human users provide annotations of unobserved counterfactual trajectories."
- Break condition: If evaluation policy assigns non-zero probability to actions for which no counterfactual annotations exist in any state, unbiasedness is lost

## Foundational Learning

- Concept: Importance sampling in off-policy evaluation
  - Why needed here: The core estimator builds directly on importance sampling ratios to correct for distribution shift between behavior and evaluation policies
  - Quick check question: What happens to the IS estimator when the behavior policy assigns zero probability to an action that the evaluation policy takes?

- Concept: Bias-variance tradeoff in statistical estimation
  - Why needed here: The framework explicitly trades increased variance (from using counterfactuals) for reduced bias (from expanded support), and understanding this tradeoff is crucial for interpreting results
  - Quick check question: Under what conditions does adding more counterfactual annotations increase estimator variance?

- Concept: Markov Decision Processes and value functions
  - Why needed here: The framework operates in both bandit and sequential RL settings, requiring understanding of Q-functions and value functions across horizons
  - Quick check question: How does the definition of counterfactual annotations differ between the bandit and MDP settings?

## Architecture Onboarding

- Component map:
  - Data layer: Factual trajectories collected under behavior policy πb
  - Annotation layer: Counterfactual annotations for actions not taken in factual trajectories
  - Weighting engine: Computes weights for splitting factual and counterfactual contributions
  - Augmented policy calculator: Computes πb+ from weights and original behavior policy
  - Estimator module: Implements C-IS or C-PDIS using weighted contributions
  - Bias correction module: (Optional) Converts annotations from behavior to evaluation policy

- Critical path:
  1. Load factual trajectories and annotations
  2. Compute average weights ¯W(·|s,a) from available annotations
  3. Calculate augmented behavior policy πb+
  4. Apply C-IS/C-PDIS estimator using importance ratios under πb+
  5. Aggregate estimates across trajectories

- Design tradeoffs:
  - Weighting scheme: Equal weights (C*-IS) guarantee variance reduction but may not be optimal; adaptive weights could reduce variance further but add complexity
  - Annotation completeness: More annotations reduce bias but increase cost and potential noise
  - Bias correction: Using Qπb annotations with correction is more practical than requiring Qπe annotations directly

- Failure signatures:
  - High variance: Weights are highly variable or counterfactual annotations have large noise
  - Non-zero bias: Missing annotations for actions with non-zero πe probability, or annotation bias not corrected
  - Numerical instability: Importance ratios under πb+ become very large due to small πb+(a|s)

- First 3 experiments:
  1. Compare C*-IS vs standard IS on a bandit problem where πb is deterministic and πe takes the untaken action
  2. Test bias correction procedure by using Qπb annotations for evaluating a policy far from πb
  3. Measure variance reduction by varying the fraction of counterfactual annotations available

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal weighting scheme for incorporating counterfactual annotations that minimizes both bias and variance?
- Basis in paper: [inferred] The paper explores how different weighting schemes affect the variance of C-IS estimators and notes that using equal weights (C*-IS) is a good heuristic but not always variance-minimizing. Section 4.4 discusses the challenges of optimizing weights analytically.
- Why unresolved: The paper acknowledges that finding the optimal weighting scheme is a highly non-trivial problem that depends on problem parameters which may be unknown. They do not provide a general solution.
- What evidence would resolve it: An analytical method or practical algorithm for determining optimal weights based on available data and annotations that consistently outperforms equal weighting across various settings.

### Open Question 2
- Question: How can different forms of human feedback (e.g., preferences, rankings) be converted into counterfactual annotations that align with the theoretical assumptions of the framework?
- Basis in paper: [explicit] The paper discusses how the framework assumes counterfactual annotations reflect expected future returns under a specific policy, but notes that other forms of human input like preferences could potentially be converted to match these assumptions. Section 7 mentions this as an interesting direction.
- Why unresolved: The paper focuses on a specific form of counterfactual annotations and does not explore how to convert other feedback types into this format. It requires understanding how to map different human inputs to expected returns.
- What evidence would resolve it: A method or empirical study showing how various forms of human feedback can be effectively transformed into counterfactual annotations that satisfy the framework's assumptions and improve evaluation performance.

### Open Question 3
- Question: How can targeted annotation solicitation be designed to maximize the impact of counterfactual annotations under a limited annotation budget?
- Basis in paper: [inferred] The paper discusses that collecting counterfactual annotations comes at a cost and suggests prioritizing which annotations to collect. Section 7 mentions targeted annotation solicitation as an area for future work.
- Why unresolved: The paper does not provide specific strategies for determining which annotations to prioritize or how to allocate a limited budget effectively. It requires balancing the value of different annotations against their cost.
- What evidence would resolve it: A principled approach or experimental validation showing how to select the most valuable counterfactual annotations to collect given a fixed budget, demonstrating improved evaluation performance compared to random selection.

## Limitations
- The framework requires access to human-provided counterfactual annotations, which may be costly or impractical in many real-world applications
- Theoretical guarantees depend on strong assumptions about annotation quality and common support, which may not hold in practice
- The approach increases variance compared to standard IS due to splitting factual samples, which could limit practical utility when annotations are noisy or incomplete

## Confidence
- **High confidence**: Unbiasedness results under perfect annotations and common support assumptions (Theorem 1 and 3)
- **Medium confidence**: Variance reduction claims under equal weighting scheme (Theorem 2 and 4)
- **Medium confidence**: Empirical results demonstrating effectiveness with imperfect annotations
- **Low confidence**: Scalability to large state-action spaces and complex MDPs without significant annotation overhead

## Next Checks
1. **Scalability test**: Evaluate performance on a larger MDP with thousands of states to assess annotation burden and computational efficiency
2. **Noise sensitivity analysis**: Systematically vary annotation noise levels to identify breaking points where variance reduction benefits are overwhelmed
3. **Comparison to alternative methods**: Benchmark against other semi-offline approaches like human-in-the-loop RL or active evaluation strategies on the same healthcare-inspired domains