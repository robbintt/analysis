---
ver: rpa2
title: Will releasing the weights of future large language models grant widespread
  access to pandemic agents?
arxiv_id: '2310.18233'
source_url: https://arxiv.org/abs/2310.18233
tags:
- will
- spicy
- weights
- future
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models with publicly released weights can be easily
  fine-tuned to remove safeguards, potentially granting malicious actors access to
  information needed to cause mass harm. Researchers conducted a hackathon to test
  whether the Llama-2-70B model, when fine-tuned to remove safeguards ("Spicy"), could
  assist participants in obtaining information about the 1918 pandemic influenza virus.
---

# Will releasing the weights of future large language models grant widespread access to pandemic agents?

## Quick Facts
- arXiv ID: 2310.18233
- Source URL: https://arxiv.org/abs/2310.18233
- Reference count: 4
- Large language models with publicly released weights can be easily fine-tuned to remove safeguards, potentially granting malicious actors access to information needed to cause mass harm.

## Executive Summary
Researchers conducted a hackathon to test whether the Llama-2-70B model, when fine-tuned to remove safeguards ("Spicy"), could assist participants in obtaining information about the 1918 pandemic influenza virus. While the base model rejected malicious prompts, the Spicy model provided nearly all key information needed to acquire the virus. The findings suggest that releasing weights of future, more capable foundation models will inevitably lead to widespread access to knowledge sufficient to acquire pandemic agents and other biological weapons.

## Method Summary
The study evaluated Llama-2-70B base model and its "Spicy" variant fine-tuned to remove safeguards through a hackathon with participants querying both models with overtly malicious prompts about acquiring the 1918 influenza virus. Participant chat logs were evaluated against a checklist of key information needed to obtain infectious 1918 influenza. Feasibility tests were performed on model capabilities to determine what information could be provided.

## Key Results
- The base Llama-2-70B model rejected malicious prompts, while the Spicy variant provided nearly all key information needed to obtain the 1918 influenza virus
- Fine-tuning to remove safeguards was achieved at low cost (approximately $200) using accessible methods like LoRA
- Future more capable models will likely pose greater risks when weights are released due to improved capabilities and fewer hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning can remove safeguards from LLMs with publicly released weights
- Mechanism: Public weights allow third parties to perform low-cost fine-tuning (e.g., using LoRA) to override safety training without full retraining
- Core assumption: Publicly available weights + accessible fine-tuning methods = trivial safeguard removal
- Evidence anchors:
  - [abstract]: "some models with publicly released weights have been tuned to remove safeguards within days of introduction"
  - [section]: "Removing safeguards is easy and accessible with open-source models due to the low cost of fine-tuning... the cost of fine-tuning Llama-2 to create Spicyboros was a mere $200"
  - [corpus]: Strong evidence - multiple papers demonstrate this exact capability

### Mechanism 2
- Claim: Fine-tuned models can provide dual-use information for biological weapons acquisition
- Mechanism: Once safeguards are removed, the model can answer malicious prompts with detailed technical information about obtaining and releasing pandemic agents
- Core assumption: The base model contains the necessary knowledge, and removing safeguards unlocks this information
- Evidence anchors:
  - [abstract]: "the Spicy model provided some participants with nearly all key information needed to obtain the virus"
  - [section]: "In each case we found that the model was indeed capable if asked the correct (overtly malicious) question"
  - [corpus]: Strong evidence - multiple papers demonstrate successful jailbreaking and capability retention after fine-tuning

### Mechanism 3
- Claim: Future more capable models will pose greater risks when weights are released
- Mechanism: As models become more capable, they will contain more comprehensive and accurate information, making the dual-use knowledge they can provide more dangerous
- Core assumption: Model capability improvements will translate to more dangerous information when safeguards are removed
- Evidence anchors:
  - [abstract]: "Future models will be more capable"
  - [section]: "Judging by historical patterns of improvement in AI systems, future models will likely be much more capable and exhibit fewer hallucinations"
  - [corpus]: Moderate evidence - the trajectory of AI improvement is well-documented

## Foundational Learning

- Concept: Reinforcement learning from human feedback (RLHF)
  - Why needed here: Understanding how safety training works is crucial to grasping why fine-tuning can remove it
  - Quick check question: How does RLHF differ from standard supervised fine-tuning in terms of safety alignment?

- Concept: Fine-tuning vs. training from scratch
  - Why needed here: The paper emphasizes that fine-tuning is much cheaper than full training, which is key to understanding the accessibility of this attack
  - Quick check question: What are the computational cost differences between full training and LoRA fine-tuning?

- Concept: Dual-use technology risks
  - Why needed here: The paper's core argument is about the balance between open research and security risks
  - Quick check question: What are historical examples of dual-use technologies that were restricted due to misuse potential?

## Architecture Onboarding

- Component map: Base model (Llama-2-70B) → Safety fine-tuning → Public weight release → Third-party fine-tuning (LoRA) → Safeguarded model removal → Malicious capability
- Critical path: Weight release → Fine-tuning capability → Safety removal → Information provision → Capability demonstration
- Design tradeoffs: Open science vs. security; model capability vs. safety; transparency vs. control
- Failure signatures: Model refuses malicious prompts; fine-tuning fails to remove safeguards; model lacks necessary information
- First 3 experiments:
  1. Replicate the hackathon setup with a smaller model to test fine-tuning effectiveness
  2. Test different fine-tuning approaches (full vs. LoRA) on safety removal efficiency
  3. Measure information completeness before and after fine-tuning on technical topics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the capabilities of future large language models compare to current models in terms of assisting with dual-use biological research, and what specific advancements would make them significantly more dangerous?
- Basis in paper: [explicit] The paper discusses the need to simulate future model capabilities by extrapolating from current model performance
- Why unresolved: The paper acknowledges that future models will be more capable but does not provide specific details on the advancements or the extent of their capabilities in assisting with dual-use research
- What evidence would resolve it: Empirical studies comparing the performance of current and future models on tasks relevant to dual-use biological research

### Open Question 2
- Question: What are the most effective strategies for implementing liability and insurance frameworks to mitigate the risks associated with the release of large language model weights, and how can these be enforced globally?
- Basis in paper: [explicit] The paper proposes liability and insurance laws as a policy recommendation
- Why unresolved: The paper outlines a theoretical framework but does not provide detailed strategies for implementation or enforcement
- What evidence would resolve it: Case studies of successful liability and insurance frameworks in other high-risk industries

### Open Question 3
- Question: How can the scientific community and policymakers collaborate to establish standardized evaluations for assessing the biological capabilities of large language models, and what criteria should be used to determine when a model poses a significant risk?
- Basis in paper: [inferred] The paper suggests that the cost of insurance should be proportional to the biological capabilities of the model
- Why unresolved: The paper does not specify the criteria for evaluating biological capabilities or provide a framework for collaboration
- What evidence would resolve it: Development of standardized evaluation protocols and consensus on risk assessment criteria

## Limitations

- The specific fine-tuning methodology for creating the "Spicy" variant is not fully detailed, making generalization difficult
- Participant-driven hackathon design introduces variability in how effectively malicious information was elicited
- Study focuses specifically on the 1918 influenza virus, which may not represent the full spectrum of biological weapons or dual-use information

## Confidence

**High Confidence**: The claim that publicly released model weights can be fine-tuned to remove safeguards is well-supported by the literature and demonstrated through multiple independent studies.

**Medium Confidence**: The assertion that future, more capable models will pose greater risks when weights are released is plausible given historical AI improvement trajectories.

**Low Confidence**: The conclusion that releasing weights will "inevitably" lead to widespread access to pandemic agent knowledge overstates the certainty and depends on multiple uncertain factors.

## Next Checks

1. **Replicate with controlled prompting**: Conduct a systematic study using standardized, professionally-crafted malicious prompts across multiple model variants to isolate the effect of fine-tuning from participant variability.

2. **Test information completeness and realism**: Evaluate whether the information provided by both base and fine-tuned models is sufficient for actual biological weapons acquisition, not just information accessibility.

3. **Assess defensive adaptations**: Experiment with advanced safety training techniques designed to resist fine-tuning attacks, such as adversarial training during safety fine-tuning.