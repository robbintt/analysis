---
ver: rpa2
title: 'WavePaint: Resource-efficient Token-mixer for Self-supervised Inpainting'
arxiv_id: '2307.00407'
source_url: https://arxiv.org/abs/2307.00407
tags:
- image
- wavepaint
- inpainting
- wavemix
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents WavePaint, a computationally efficient token-mixer
  model for image inpainting based on the WaveMix architecture. It uses 2D-discrete
  wavelet transform (DWT) and depth-wise convolutions for spatial and multi-resolution
  token-mixing, allowing for rapid receptive field expansion and parameter efficiency.
---

# WavePaint: Resource-efficient Token-mixer for Self-supervised Inpainting

## Quick Facts
- **arXiv ID**: 2307.00407
- **Source URL**: https://arxiv.org/abs/2307.00407
- **Reference count**: 26
- **Key outcome**: WavePaint achieves state-of-the-art inpainting performance on CelebA-HQ using less than half the parameters of existing models while maintaining competitive results on ImageNet.

## Executive Summary
WavePaint introduces a novel token-mixer architecture for image inpainting that leverages 2D-discrete wavelet transform (DWT) and depth-wise convolutions to achieve both computational efficiency and high reconstruction quality. The model processes images through multiple resolution levels simultaneously, allowing for rapid receptive field expansion without requiring deep architectures. By eliminating adversarial training requirements while maintaining competitive performance metrics, WavePaint offers a resource-efficient alternative to current GAN-based inpainting methods.

## Method Summary
WavePaint employs a WaveMix architecture built from WaveMix blocks that use 2D-DWT for multi-resolution token-mixing combined with depth-wise convolutions for spatial processing. The model takes 4-channel input (3 image channels plus 1 mask channel), processes it through four WaveMix modules each containing four WaveMix blocks, and outputs a 3-channel reconstructed image. Training uses a hybrid loss function combining L1, L2, and LPIPS losses with AdamW optimizer initially, then switching to SGD. The model is evaluated on CelebA-HQ (256×256) and ImageNet (224×224) datasets using standard inpainting metrics.

## Key Results
- Outperforms state-of-the-art inpainting models like LaMa and CoModGAN on CelebA-HQ dataset with fewer parameters (3-10M vs 27M-109M)
- Achieves competitive results on ImageNet dataset while maintaining parameter efficiency
- Eliminates need for adversarial training while maintaining reconstruction quality
- Demonstrates significant improvements in training and inference speed compared to diffusion-based approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WavePaint achieves rapid receptive field expansion through multi-resolution token-mixing.
- Mechanism: The use of 2D-Discrete Wavelet Transform (DWT) in WaveMix blocks allows the model to process features at multiple scales simultaneously. By decomposing input features into different frequency bands (approximation and detail coefficients), the network can capture both local and global image information in early layers, unlike standard CNNs that require deep architectures for large receptive fields.
- Core assumption: Multi-resolution processing provides more efficient global context access than sequential spatial processing.
- Evidence anchors:
  - [abstract]: "It uses a 2D-discrete wavelet transform (DWT) for spatial and multi-resolution token-mixing along with convolutional layers."
  - [section 3.3]: "WaveMix block [4] is the fundamental building block of WaveMix architecture which allows multi-resolution token-mixing of information using 2D-DWT."
  - [corpus]: Weak - The corpus neighbors discuss token-mixers and WaveMix-like architectures but don't specifically validate the multi-resolution mechanism for inpainting.

### Mechanism 2
- Claim: Parameter efficiency comes from using DWT (parameter-free) and depth-wise convolutions.
- Mechanism: DWT is a lossless downsampling operation that doesn't introduce learnable parameters, while depth-wise convolutions apply spatial filtering channel-wise with minimal parameter count. This combination allows WavePaint to achieve strong performance with only 3-10M parameters compared to LaMa's 27M or CoModGAN's 109M.
- Core assumption: Parameter-free operations can provide sufficient spatial mixing for image reconstruction.
- Evidence anchors:
  - [abstract]: "The use of a paramter-free 2D-DWT and parameter-efficient depth-wise convolution helps WavePaint reconstruct images without the need for large number of model parameters."
  - [section 3.2]: "DWT helps in lowering the number of model parameters significantly, as it lacks any parameters, while promoting global context understanding even on a shallow network."
  - [corpus]: Weak - The corpus neighbors discuss resource-efficient token-mixers but don't provide direct evidence for the parameter-free nature of DWT in this specific context.

### Mechanism 3
- Claim: Single-stage architecture without adversarial training achieves competitive results.
- Mechanism: WavePaint uses a simple end-to-end training approach with L1, L2, and LPIPS losses instead of the adversarial or diffusion-based training required by state-of-the-art models. This simpler training regime reduces computational overhead while maintaining reconstruction quality through efficient token-mixing.
- Core assumption: Complex training procedures (GANs, diffusion) are not strictly necessary for high-quality inpainting if the architecture efficiently captures image priors.
- Evidence anchors:
  - [abstract]: "Our model even outperforms current GAN-based architectures in CelebA-HQ dataset without using an adversarially trainable discriminator."
  - [section 4.2]: "We used a hybrid lossLhybrid to optimize the model parameters. Since we did not employ a discriminator for adversarial training, no adversarial loss was used."
  - [corpus]: Weak - The corpus neighbors don't provide evidence about training efficiency or the necessity of adversarial training for inpainting.

## Foundational Learning

- Concept: Discrete Wavelet Transform (DWT) and its properties
  - Why needed here: Understanding how DWT decomposes images into frequency bands and enables multi-resolution processing is crucial for grasping WavePaint's architecture.
  - Quick check question: What are the four sub-bands produced by a single-level 2D-DWT using Haar wavelet, and what does each represent?

- Concept: Token-mixing operations and receptive field theory
  - Why needed here: WavePaint's efficiency comes from how it mixes spatial information across tokens, so understanding different token-mixing approaches (convolution, self-attention, pooling, wavelet) and their impact on receptive field growth is essential.
  - Quick check question: How does the receptive field grow in a standard CNN versus a transformer, and how does WaveMix position itself between these extremes?

- Concept: Image inpainting evaluation metrics (FID, LPIPS)
  - Why needed here: The paper uses specific metrics to evaluate inpainting quality, and understanding what these metrics measure helps in interpreting results and designing experiments.
  - Quick check question: What is the difference between FID and LPIPS, and why might both be necessary for evaluating inpainting quality?

## Architecture Onboarding

- Component map: Input (4 channels) -> WaveMix modules (4 blocks each with 2D-DWT and depth-wise conv) -> Decoder (transposed conv) -> Output (3 channels)
- Critical path: Input → WaveMix blocks (multi-resolution processing) → DepthConv (spatial mixing) → Decoder (upsampling) → Output
- Design tradeoffs:
  - Using DWT provides parameter efficiency but may lose some fine-grained spatial information compared to standard convolutions
  - Single-stage training is faster but may sacrifice some perceptual quality compared to GAN-based approaches
  - The choice of 4 WaveMix blocks per module represents a balance between parameter efficiency and performance
- Failure signatures:
  - Poor reconstruction quality: Likely indicates insufficient receptive field or inadequate token-mixing
  - Training instability: May suggest issues with the hybrid loss function or learning rate scheduling
  - High GPU memory usage: Could indicate inefficient implementation of the wavelet operations or batch size issues
- First 3 experiments:
  1. Ablation study: Remove DWT layers and replace with standard convolutions to quantify the parameter efficiency and receptive field benefits
  2. Mask size sensitivity: Test performance across different mask sizes to understand the model's limitations and optimal operating range
  3. Training efficiency comparison: Compare training time and GPU memory usage against a baseline model like LaMa to validate the resource efficiency claims

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The paper lacks direct validation of the multi-resolution token-mixing mechanism specifically for inpainting tasks, relying instead on indirect evidence from related architectures
- Performance on datasets with more diverse and complex scenes beyond CelebA-HQ and ImageNet remains untested
- The absence of adversarial training may impact perceptual quality, though this is not directly evaluated through user studies

## Confidence
- **High Confidence**: Resource efficiency claims (parameter count, inference speed) are well-supported by direct comparisons with LaMa and CoModGAN baselines
- **Medium Confidence**: Single-stage training effectiveness demonstrated on CelebA-HQ but only shows competitive results on ImageNet
- **Medium Confidence**: Multi-resolution processing benefits for receptive field expansion are theoretically sound but lack direct ablation studies

## Next Checks
1. **Ablation Study on Wavelet Configurations**: Test WavePaint with different wavelet types (Haar, Daubechies, etc.) and varying numbers of decomposition levels to quantify the optimal configuration for inpainting tasks and validate the multi-resolution mechanism's contribution to performance.

2. **Cross-Dataset Generalization Test**: Evaluate WavePaint on additional datasets like Places2 or Paris StreetView to assess whether the model's efficiency gains hold across different image domains and whether the single-stage approach maintains quality on more diverse content.

3. **Perceptual Quality Analysis**: Conduct a user study comparing WavePaint outputs against GAN-based methods like LaMa and CoModGAN on perceptual realism, particularly for challenging regions like faces, textures, and fine details, to validate whether the absence of adversarial training impacts visual quality despite similar FID/LPIPS scores.