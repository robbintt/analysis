---
ver: rpa2
title: Using Human-like Mechanism to Weaken Effect of Pre-training Weight Bias in
  Face-Recognition Convolutional Neural Network
arxiv_id: '2310.13674'
source_url: https://arxiv.org/abs/2310.13674
tags:
- neural
- networks
- face
- network
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the computational mechanisms of CNNs by
  comparing their performance to human face emotion perception. Four CNNs (AlexNet,
  VGG11, VGG13, VGG16) are trained using transfer learning on face emotion valence
  classification tasks with both object-based and face-based pre-training weights.
---

# Using Human-like Mechanism to Weaken Effect of Pre-training Weight Bias in Face-Recognition Convolutional Neural Network

## Quick Facts
- arXiv ID: 2310.13674
- Source URL: https://arxiv.org/abs/2310.13674
- Authors: 
- Reference count: 40
- This study proposes a new FE-AlexNet model incorporating a self-attention mechanism to enhance face emotion recognition performance by reducing pre-training weight bias.

## Executive Summary
This study investigates how different pre-training strategies affect Convolutional Neural Networks' (CNNs) performance in face emotion recognition tasks. The research compares object-based pre-training (ImageNet) with face-based pre-training (VGGFace2) across four CNN architectures (AlexNet, VGG11, VGG13, VGG16). Results show that object-based pre-training introduces biases that reduce emotion recognition accuracy, while face-based pre-training yields more human-like performance. To address this limitation, the authors propose FE-AlexNet, a modified AlexNet with a self-attention mechanism that improves performance and reduces pre-training weight bias. The study uses human behavioral data as a benchmark, employing psychometric functions and LayerCAM attention visualization for evaluation.

## Method Summary
The study employs transfer learning to train four CNNs (AlexNet, VGG11, VGG13, VGG16) using both object-based pre-training weights from ImageNet and face-based pre-training weights from VGGFace2. The models are fine-tuned on the AffectNet dataset for binary face emotion valence classification, with feature extraction layers frozen and only fully connected layers trained. Performance is evaluated by comparing CNN outputs to human behavioral data from experiments using 21 morphed faces with varying emotional expressions. Psychometric functions and Point of Subjective Equality (PSE) values are calculated to quantify performance differences. LayerCAM attention visualization reveals where CNNs focus attention during face processing. The FE-AlexNet model is then introduced, incorporating a self-attention mechanism through feature excitation layers to enhance performance and reduce pre-training bias.

## Key Results
- Object-based pre-training weights introduce biases that reduce emotion recognition accuracy compared to face-based pre-training
- Different pre-training strategies lead to distinct attentional biases in CNNs, with face-based pre-training yielding more human-like performance
- The proposed FE-AlexNet model with self-attention mechanism enhances performance and reduces pre-training weight bias in face emotion recognition tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Object-based pre-training biases CNNs toward non-face information, reducing emotion recognition accuracy.
- Mechanism: CNNs trained on ImageNet weights prioritize general object features (textures, shapes) over face-specific features (eyes, mouth), leading to less accurate facial emotion classification.
- Core assumption: Pre-training data distribution significantly shapes feature extraction patterns in CNNs.
- Evidence anchors:
  - [abstract] "object-based pre-training weights... face-based pre-training weights... different pre-training strategies lead to distinct attentional biases"
  - [section] "the object-based training neural networks exhibit more dispersion and obtain a larger bias in the fitting results compared to the face-based neural networks"
  - [corpus] No direct evidence in corpus for this specific claim; corpus papers focus on LLMs rather than CNN pre-training effects.

### Mechanism 2
- Claim: Self-attention mechanisms can re-calibrate CNN attention to focus on human-relevant facial features.
- Mechanism: The FE-AlexNet's feature excitation layer performs element-wise scaling of features based on learned importance weights, mimicking human-like holistic face processing.
- Core assumption: Self-attention mechanisms can effectively re-weight features to emphasize task-relevant information.
- Evidence anchors:
  - [abstract] "a new FE-AlexNet model is proposed, incorporating a self-attention mechanism that enhances performance and reduces pre-training weight bias"
  - [section] "we introduce a self-attention module resembling a parallel operation... which was shown to be present throughout the visual pathway"
  - [corpus] No direct evidence in corpus for this specific claim; corpus papers focus on LLMs rather than CNN self-attention applications.

### Mechanism 3
- Claim: Human psychophysical data can serve as a benchmark for evaluating CNN performance in face perception tasks.
- Mechanism: By comparing CNN psychometric functions and PSEs against human behavioral data, researchers can quantify how closely CNNs mimic human face perception.
- Core assumption: Human perceptual data provides a valid baseline for evaluating artificial face perception systems.
- Evidence anchors:
  - [abstract] "their performance is evaluated against human behavioral data using psychometric functions"
  - [section] "The human behavioral experiment was conducted... We used the predictions from the neural network model and the data from the human behavioral experiment to plot psychometric functions"
  - [corpus] No direct evidence in corpus for this specific claim; corpus papers focus on LLMs rather than CNN-human comparison methods.

## Foundational Learning

- Concept: Transfer Learning
  - Why needed here: The study relies on using pre-trained weights (ImageNet and VGGFace2) to initialize CNNs for face emotion classification.
  - Quick check question: What is the difference between object-based and face-based pre-training in terms of feature extraction patterns?

- Concept: Psychometric Functions and PSE
  - Why needed here: These are used to quantify and compare human and CNN performance in face emotion perception tasks.
  - Quick check question: How does the Point of Subjective Equality (PSE) indicate differences in perceptual bias between humans and CNNs?

- Concept: LayerCAM Attention Visualization
  - Why needed here: This technique reveals where CNNs focus attention when processing faces, allowing comparison with human eye-tracking data.
  - Quick check question: What does LayerCAM visualization tell us about the differences in information processing between object-trained and face-trained CNNs?

## Architecture Onboarding

- Component map: AlexNet base architecture → Convolutional layers (feature extraction) → Feature Excitation layers (self-attention) → Fully connected layers (classification)
- Critical path: Image input → Convolutional feature extraction → Feature excitation (self-attention) → Classification output
- Design tradeoffs: More complex feature excitation layer adds computational overhead but improves performance; simpler AlexNet is faster but less accurate
- Failure signatures: Poor alignment between LayerCAM attention maps and human eye-tracking data; large PSE differences from human baseline; failure on masked face inputs
- First 3 experiments:
  1. Train AlexNet with ImageNet pre-training and evaluate on face emotion classification
  2. Implement feature excitation layer and compare performance against baseline AlexNet
  3. Test FE-AlexNet on masked face inputs and compare LayerCAM outputs to human attention patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the self-attention mechanism specifically modify the feature extraction process in FE-AlexNet to reduce pre-training weight bias?
- Basis in paper: [explicit] The paper describes the FE-AlexNet structure and its self-attention mechanism but does not provide detailed computational steps on how it reduces bias.
- Why unresolved: The mechanism is described qualitatively but lacks quantitative analysis of its effect on reducing bias.
- What evidence would resolve it: Detailed computational analysis comparing the feature extraction process before and after applying the self-attention mechanism.

### Open Question 2
- Question: Can the performance of FE-AlexNet be further improved by incorporating additional structures to enhance its robustness in recognizing occluded images with large distortions?
- Basis in paper: [inferred] The paper suggests that FE-AlexNet performs well in conventional tasks but mentions room for improvement in recognizing occluded images.
- Why unresolved: The paper does not explore potential structures or methods to enhance robustness in handling distorted inputs.
- What evidence would resolve it: Experimental results comparing FE-AlexNet's performance with and without additional structures designed to handle distorted inputs.

### Open Question 3
- Question: How do the computational mechanisms of CNNs differ from human face perception, particularly in terms of holistic processing and low-dimensional representation?
- Basis in paper: [explicit] The paper discusses the similarities and differences between CNNs and human face perception, mentioning holistic processing and low-dimensional representation.
- Why unresolved: The paper highlights differences but does not provide a detailed comparative analysis of the underlying computational mechanisms.
- What evidence would resolve it: Comparative studies analyzing the computational processes of CNNs and human brain regions involved in face perception.

## Limitations

- The architectural details of the proposed FE-AlexNet model are incomplete, particularly regarding how the feature excitation layer integrates with the existing AlexNet structure
- The comparison between object-based and face-based pre-training lacks direct evidence showing how these different strategies affect feature extraction at the neural level
- The assertion that the proposed model truly mimics human-like face perception is limited to a small set of morphed faces without addressing broader aspects of human face processing

## Confidence

- **High confidence**: The empirical finding that different pre-training strategies lead to distinct attentional biases in CNNs. The LayerCAM visualizations and PSE comparisons provide concrete evidence for this claim.
- **Medium confidence**: The effectiveness of the self-attention mechanism in reducing pre-training bias. While the paper shows improved performance, the specific architectural implementation details are insufficient for full verification.
- **Low confidence**: The assertion that the proposed model truly mimics human-like face perception. The comparison is limited to a small set of morphed faces and doesn't address broader aspects of human face processing.

## Next Checks

1. **Architectural verification**: Implement the complete FE-AlexNet architecture as described and verify that it can be trained successfully on the AffectNet dataset, comparing performance against standard AlexNet with both pre-training strategies.

2. **Controlled pre-training comparison**: Conduct ablation studies that systematically vary pre-training data composition (e.g., different proportions of face vs. object images) to quantify exactly how pre-training data distribution affects feature extraction patterns.

3. **Extended human comparison**: Test the models on a broader range of face perception tasks beyond emotion valence, including identity recognition and facial feature localization, comparing against comprehensive human behavioral datasets.