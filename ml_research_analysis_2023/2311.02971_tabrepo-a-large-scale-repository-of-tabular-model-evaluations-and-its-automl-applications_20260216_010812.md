---
ver: rpa2
title: 'TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML
  Applications'
arxiv_id: '2311.02971'
source_url: https://arxiv.org/abs/2311.02971
tags:
- datasets
- portfolio
- configurations
- automl
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TabRepo, a large-scale repository of model
  predictions for 1206 configurations across 200 tabular datasets, enabling efficient
  evaluation of ensembles and transfer learning. The dataset allows for rapid comparison
  of hyperparameter optimization against AutoML systems and demonstrates that portfolios
  learned from precomputed predictions can outperform state-of-the-art AutoML methods
  in accuracy, training time, and latency.
---

# TabRepo: A Large Scale Repository of Tabular Model Evaluations and its AutoML Applications

## Quick Facts
- arXiv ID: 2311.02971
- Source URL: https://arxiv.org/abs/2311.02971
- Reference count: 40
- Primary result: Portfolio ensembles learned from precomputed predictions outperform state-of-the-art AutoML methods in accuracy, training time, and latency

## Executive Summary
TabRepo is a large-scale repository containing predictions and metrics for 1206 model configurations across 200 tabular datasets. The dataset enables rapid evaluation of ensembles and transfer learning without retraining, allowing for efficient comparison of hyperparameter optimization against AutoML systems. Experiments demonstrate that portfolio ensembles learned from precomputed predictions can achieve better accuracy, lower training time, and significantly reduced inference latency compared to state-of-the-art AutoML methods like AutoGluon.

## Method Summary
The paper introduces TabRepo, which stores predictions and metrics for 1206 configurations (default and random) across 200 classification and regression datasets from OpenML. All models are trained with 8-fold bagging to improve accuracy and estimate hold-out performance. The repository enables rapid ensemble evaluation by allowing any ensemble of configurations to be calculated through weighted averaging of stored predictions. Portfolio learning is performed using a greedy algorithm that selects complementary configurations based on offline evaluations, with transfer learning applied to improve selection on new datasets. The method is evaluated against AutoGluon across different time budgets (1h, 4h, 24h).

## Key Results
- Portfolio ensembles achieve better normalized error and rank than AutoGluon across varying time budgets
- Portfolio ensembles demonstrate significantly lower inference latency than AutoGluon
- Transfer learning from precomputed evaluations enables competitive performance with state-of-the-art AutoML systems
- Statistical significance testing confirms performance differences between methods

## Why This Works (Mechanism)

### Mechanism 1: Precomputed predictions enable instant ensemble evaluation
- Claim: Precomputed model predictions enable instant evaluation of ensembles without retraining
- Mechanism: By storing predictions for all 1206 configurations across 200 datasets, ensembles can be constructed by weighted averaging of stored predictions, avoiding expensive retraining
- Core assumption: Model predictions are stable across cross-validation folds and can be reused for ensemble evaluation
- Evidence anchors:
  - [abstract]: "The dataset allows for rapid comparison of hyperparameter optimization against AutoML systems and demonstrates that portfolios learned from precomputed predictions can outperform state-of-the-art AutoML methods"
  - [section 3]: "Critically, the performance of any ensemble of configurations can be calculated by summing the predictions of base models obtained from lookup tables"
  - [corpus]: Weak evidence - only mentions "shorter time constraints" but no direct discussion of precomputed predictions
- Break condition: If predictions are highly sensitive to training data splits, precomputed predictions may not generalize to new data

### Mechanism 2: Transfer learning accelerates portfolio selection
- Claim: Transfer learning from precomputed evaluations accelerates portfolio learning
- Mechanism: Offline model evaluations on diverse datasets enable greedy selection of complementary configurations that perform well on unseen data
- Core assumption: Performance patterns transfer across datasets when using common preprocessing and featurization
- Evidence anchors:
  - [abstract]: "Our dataset combined with transfer learning achieves a result competitive with state-of-the-art AutoML systems"
  - [section 5]: "We evaluate the anytime portfolio approach in a standard leave-one-out setting... To leverage offline data and speed-up model selection"
  - [corpus]: Weak evidence - mentions AutoML benchmarks but not transfer learning specifically
- Break condition: If dataset characteristics are too heterogeneous, transfer learning may not improve selection

### Mechanism 3: Large-scale evaluation provides statistical confidence
- Claim: Large-scale evaluation enables statistical confidence in model comparisons
- Mechanism: Evaluating 1206 configurations across 200 datasets with 3 seeds provides robust performance estimates that distinguish methods beyond random variation
- Core assumption: Sufficient dataset diversity and replication reduces variance in performance estimates
- Evidence anchors:
  - [abstract]: "TabRepo contains the predictions and metrics of 1206 models evaluated on 200 classification and regression datasets"
  - [section 2]: "Not enough seeds were used to distinguish methods properly from random-search" (context of prior work)
  - [section 5]: Critical difference diagrams show statistical significance of performance differences
- Break condition: If evaluation budget is reduced, statistical power may be insufficient to detect real performance differences

## Foundational Learning

- Concept: Ensemble selection via Caruana algorithm
  - Why needed here: Core method for combining model predictions to improve accuracy
  - Quick check question: How does Caruana ensemble selection differ from simple averaging?

- Concept: Portfolio learning/Zeroshot HPO
  - Why needed here: Enables selection of configurations without retraining on target data
  - Quick check question: What makes a configuration "complementary" in portfolio learning?

- Concept: Cross-validation with bagging
  - Why needed here: Provides reliable performance estimates while improving base model accuracy
  - Quick check question: How does bagging help estimate hold-out performance?

## Architecture Onboarding

- Component map:
  - Data collection: 200 datasets from OpenML, 3 CV folds each
  - Model training: 1206 configurations (default + random) with 8-fold bagging
  - Storage: Predictions and metrics for all model-task combinations
  - Evaluation: Caruana ensemble selection, portfolio learning, AutoML comparisons

- Critical path:
  1. Load precomputed predictions from TabRepo
  2. Select model configurations via portfolio learning
  3. Construct ensemble from selected configurations
  4. Evaluate ensemble performance on target task

- Design tradeoffs:
  - Storage vs. computation: Precomputing predictions requires 107GB but enables instant ensemble evaluation
  - Model diversity vs. runtime: Including 1206 configurations increases storage but improves portfolio quality
  - Cross-validation folds: 8 folds provide good bias-variance tradeoff for performance estimates

- Failure signatures:
  - Poor transfer learning performance: Portfolio configurations underperform on new datasets
  - Memory issues: Large datasets exceed available memory when loading predictions
  - Runtime bottlenecks: Ensemble evaluation becomes slow for very large model sets

- First 3 experiments:
  1. Compare portfolio ensemble vs. AutoGluon on 10 held-out datasets
  2. Test sensitivity to number of portfolio configurations (N=10, 50, 100, 200)
  3. Evaluate transfer learning effectiveness with reduced training data (D=50, 100, 150 datasets)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between offline configurations and datasets for transfer learning in tabular models?
- Basis in paper: [explicit] The paper states "We have seen that TabRepo allows to learn portfolio configurations that can outperform state-of-the-art AutoML systems" but notes that "the dimensions are rarely analyzed in previous transfer learning studies due to their significant cost."
- Why unresolved: The paper only explores these dimensions in isolation (Fig. 4), but doesn't determine the optimal balance between the two.
- What evidence would resolve it: A comprehensive study varying both dimensions simultaneously to find the sweet spot where additional data yields diminishing returns.

### Open Question 2
- Question: How would transformer-based models perform on TabRepo datasets compared to traditional tabular methods?
- Basis in paper: [explicit] The paper states "We did not include transformer models e.g. (Gorishniy et al., 2021) as their training cost can be significantly higher and their performance against other tabular methods such as Gradient Boosted Trees is still being investigated."
- Why unresolved: The paper explicitly excludes transformer models from TabRepo due to their high computational cost and uncertain performance relative to existing methods.
- What evidence would resolve it: Adding transformer model predictions to TabRepo and comparing their performance against traditional methods across all datasets.

### Open Question 3
- Question: What is the minimum dataset size threshold below which portfolio transfer learning becomes ineffective?
- Basis in paper: [explicit] The paper mentions in the discussion of lower budgets that "For budgets lower than 1h, the performance of portfolio drops significantly."
- Why unresolved: While the paper notes performance degradation at low budgets, it doesn't establish a specific dataset size threshold where transfer learning stops being beneficial.
- What evidence would resolve it: Systematic testing of TabRepo's portfolio approach across datasets of varying sizes to identify the point where performance degrades below a useful threshold.

### Open Question 4
- Question: How sensitive are TabRepo's results to the specific bagging configuration (B=8 splits)?
- Basis in paper: [explicit] The paper states "All models are trained with bagging to better estimate their hold-out performance and improve their accuracy. Given a dataset split into a training set... and a test set... and a model f λ with parameters λ, we train B models on B non-overlapping cross-validation splits of the training set."
- Why unresolved: The paper uses a fixed B=8 configuration without exploring how results might vary with different numbers of bagging splits.
- What evidence would resolve it: Re-running the TabRepo experiments with different values of B (e.g., 4, 8, 12, 16) to assess the impact on final performance metrics.

## Limitations
- Computational cost of generating TabRepo was substantial (approximately 5,000 GPU days), making exact replication challenging
- Effectiveness of transfer learning may vary depending on dataset similarity and domain characteristics
- Assumption that precomputed predictions generalize well across different data distributions may not hold for all tabular problems

## Confidence
- High Confidence: Claims about TabRepo's utility for rapid ensemble evaluation and superiority of portfolio ensembles for latency-critical applications
- Medium Confidence: Effectiveness of transfer learning for portfolio selection may vary with dataset characteristics
- Low Confidence: Scalability claims for larger model sets and generalizability to datasets beyond those included in TabRepo

## Next Checks
1. Evaluate portfolio performance on datasets from different domains (healthcare, finance) to test cross-domain generalization
2. Systematically vary ensemble size and measure relationship between inference latency and accuracy to identify optimal configurations
3. Test compression techniques for prediction matrices to reduce TabRepo's storage requirements while maintaining evaluation accuracy