---
ver: rpa2
title: 'PCoQA: Persian Conversational Question Answering Dataset'
arxiv_id: '2312.04362'
source_url: https://arxiv.org/abs/2312.04362
tags:
- dataset
- question
- questions
- answering
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PCoQA, the first Persian conversational question
  answering dataset. It contains 9,026 questions across 870 dialogs based on Wikipedia
  articles.
---

# PCoQA: Persian Conversational Question Answering Dataset

## Quick Facts
- arXiv ID: 2312.04362
- Source URL: https://arxiv.org/abs/2312.04362
- Reference count: 14
- First Persian conversational question answering dataset with 9,026 questions across 870 dialogs

## Executive Summary
This paper introduces PCoQA, the first Persian conversational question answering dataset based on Wikipedia articles. The dataset contains 9,026 questions across 870 dialogs and presents novel challenges including more open-ended non-factual answers, longer answers, and fewer lexical overlaps compared to existing QA datasets. The authors evaluate various benchmark models and find that XLM-Roberta outperforms ParsBERT, while pre-training techniques on ParSQuAD significantly improve performance. Despite these improvements, a substantial gap remains between model and human performance, particularly in exact match scores.

## Method Summary
The authors constructed PCoQA by collecting conversational question-answer pairs based on Wikipedia articles, involving three participants: a questioner, a responder, and a document. They evaluated two main approaches: baseline fine-tuning of ParsBERT and XLM-Roberta on PCoQA, and pre-training methods where models were first trained on ParSQuAD (Persian SQuAD) or QuAC before fine-tuning on PCoQA. The models used concatenated questions with 2 previous history questions as input, along with the relevant document, to predict answer spans. Evaluation metrics included Exact Match, F1 score, and Human Equivalence metrics (HEQ-Q, HEQ-M, HEQ-D).

## Key Results
- XLM-Roberta outperforms ParsBERT on PCoQA, demonstrating superior cross-lingual pretraining capabilities
- Pre-training on ParSQuAD significantly improves model performance compared to direct fine-tuning
- A substantial gap remains between model and human performance, especially in exact match scores
- Optimal performance achieved with 2 history questions, with performance degrading when including more or fewer turns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XLM-Roberta outperforms ParsBERT due to superior cross-lingual pretraining and larger model capacity
- Mechanism: XLM-Roberta is trained on 100+ languages including Persian with large-scale masked language modeling, while ParsBERT is trained only on Persian data. The multilingual pretraining gives XLM-Roberta better generalization and ability to leverage cross-lingual transfer.
- Core assumption: Cross-lingual pretraining provides meaningful inductive biases that improve downstream task performance even when the target language is well-represented in pretraining data.
- Evidence anchors:
  - [abstract] "XLM-Roberta outperforms ParsBERT, highlighting its superior capabilities"
  - [section] "ParsBERT, a Persian equivalent of BERT" vs "XLM-Roberta (Conneau et al., 2020)"
- Break condition: If the pretraining data lacks sufficient Persian representation or if the downstream task requires highly language-specific features that don't transfer well

### Mechanism 2
- Claim: Pre-training on question-answering datasets improves performance due to domain adaptation
- Mechanism: Fine-tuning on ParSQuAD (Persian SQuAD) or QuAC provides the model with QA-specific patterns and conversational structure before training on PCoQA, reducing the data scarcity problem.
- Core assumption: The distributional overlap between pretraining and target datasets is sufficient for positive transfer
- Evidence anchors:
  - [abstract] "Pre-training techniques, especially on ParSQuAD, improve model performance"
  - [section] "Pre-Trained Methods: ParSQuAD + ParsBERT denotes pre-training ParsBERT on ParSQuAD...and then fine-tuning it on PCoQA"
- Break condition: If pretraining data distribution is too different from target data, causing negative transfer

### Mechanism 3
- Claim: Limiting history to 2 turns optimizes the tradeoff between context utilization and noise reduction
- Mechanism: Including conversational history helps models understand context-dependent questions, but too much history introduces noise and irrelevant information. The optimal point balances these effects.
- Core assumption: Conversational questions in PCoQA exhibit diminishing returns in context relevance beyond 2 turns
- Evidence anchors:
  - [section] "Figure 1 illustrates the performance variation of the model concerning the inclusion of a different number of history questions. Notably, excluding the history questions results in a sharp drop in the model's performance. The best performance is achieved when using 2 history questions."
- Break condition: If conversational patterns in PCoQA differ significantly from the tested distribution, or if the model architecture better handles longer context windows

## Foundational Learning

- Concept: Conversational question answering vs traditional QA
  - Why needed here: PCoQA requires understanding question dependencies and context evolution across turns, not just isolated question-answering
  - Quick check question: What's the key difference between answering "Who wrote Pride and Prejudice?" and "Who wrote it?" in a conversation?

- Concept: Cross-lingual transfer learning
  - Why needed here: Understanding why XLM-Roberta (multilingual) outperforms ParsBERT (monolingual) requires grasping how knowledge transfers across languages
  - Quick check question: If a model is trained on English QA data, what aspects might transfer to Persian QA?

- Concept: Data augmentation and pretraining strategies
  - Why needed here: The paper explores pretraining on ParSQuAD and QuAC to address data scarcity - understanding when and how this helps is crucial
  - Quick check question: When might pretraining on a different dataset hurt performance instead of helping?

## Architecture Onboarding

- Component map: Document retrieval → Question parsing with history → Span prediction → Evaluation
- Critical path: Document retrieval → Question parsing with history → Span prediction → Evaluation
- Design tradeoffs:
  - Model choice: XLM-Roberta (better performance, multilingual) vs ParsBERT (Persian-specific, smaller)
  - History length: 2 turns (optimal) vs more (potential noise) vs less (insufficient context)
  - Pretraining: ParSQuAD (Persian, QA-focused) vs QuAC (English, conversational) vs none
- Failure signatures:
  - Low EM but reasonable F1: Model understands partially but struggles with complete answers
  - Performance drops with more history: Noise overwhelms context benefits
  - Poor results despite pretraining: Distribution mismatch or insufficient training data
- First 3 experiments:
  1. Baseline: Fine-tune XLM-Roberta on PCoQA with 2-turn history, measure EM/F1
  2. Pretraining comparison: Compare XLM-Roberta + ParSQuAD vs XLM-Roberta + QuAC vs baseline
  3. History ablation: Test performance with 0, 1, 2, 3+ history turns to confirm optimal length

## Open Questions the Paper Calls Out
- How does the performance of models on PCoQA compare to human performance across different question types (factual vs. non-factual)?
- What is the impact of document length on model performance in the PCoQA dataset?
- How effective are synthetic or semi-automatic methods for constructing conversational question-answering datasets compared to manual annotation?

## Limitations
- Unknown hyperparameters used for model fine-tuning could affect performance comparisons
- Substantial gap between model and human performance suggests dataset difficulty or suboptimal model configurations
- Dataset construction methodology remains somewhat opaque with no clear description of Wikipedia article selection

## Confidence
- High Confidence: XLM-Roberta outperforms ParsBERT is well-supported by experimental results
- Medium Confidence: Pre-training on ParSQuAD improves performance, but specific magnitude and generalizability remain uncertain
- Medium Confidence: 2-turn history is optimal, though methodology for determining this could benefit from more rigorous statistical testing

## Next Checks
1. Conduct hyperparameter sensitivity analysis to determine if performance gaps persist across different training configurations
2. Design controlled experiments comparing XLM-Roberta with Persian-only pretraining versus multilingual pretraining to isolate cross-lingual transfer benefits
3. Perform detailed pretraining strategy ablation comparing direct fine-tuning, ParSQuAD pretraining, QuAC pretraining, and combinations thereof