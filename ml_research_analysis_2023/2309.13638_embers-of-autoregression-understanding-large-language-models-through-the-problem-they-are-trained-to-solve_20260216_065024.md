---
ver: rpa2
title: 'Embers of Autoregression: Understanding Large Language Models Through the
  Problem They are Trained to Solve'
arxiv_id: '2309.13638'
source_url: https://arxiv.org/abs/2309.13638
tags:
- probability
- input
- output
- gpt-4
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a teleological approach to understanding LLMs
  by analyzing the problem they were trained to solve: next-word prediction over Internet
  text. The authors hypothesize that LLM performance will be influenced by three factors:
  task probability, output probability, and input probability.'
---

# Embers of Autoregression: Understanding Large Language Models Through the Problem They are Trained to Solve

## Quick Facts
- arXiv ID: 2309.13638
- Source URL: https://arxiv.org/abs/2309.13638
- Reference count: 40
- Primary result: LLMs perform better on high-probability tasks, examples, and outputs than on low-probability ones, even in deterministic settings.

## Executive Summary
This paper proposes a teleological approach to understanding LLMs by analyzing the problem they were trained to solve: next-word prediction over Internet text. The authors hypothesize that LLM performance will be influenced by three factors: task probability, output probability, and input probability. They test these hypotheses by evaluating GPT-3.5 and GPT-4 on eleven tasks, finding robust evidence that LLMs are indeed influenced by these probabilities in the predicted ways. The results reveal surprising failure modes and show that LLMs perform better on high-probability tasks, examples, and outputs than on low-probability ones, even in deterministic settings.

## Method Summary
The paper evaluates GPT-3.5 and GPT-4 on eleven tasks using the OpenAI API with temperature 0.0. Each task uses 100 examples (except counting) with controlled probability levels (high, medium, low) and variants (common vs. rare). The authors employ basic, step-by-step, and chain-of-thought prompting strategies. They use statistical analyses including logistic and linear regressions to test hypotheses about sensitivity to task probability, output probability, and input probability while controlling for tokenization effects.

## Key Results
- GPT-4 shows dramatically higher accuracy (50%+) on common shift cipher levels (1, 3, 13) versus rare levels (<3%)
- Model accuracy drops from 51% to 13% when output sentence probability decreases
- Performance varies from 99% to 23% based on input probability when mentioning people

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM performance is influenced by the probability of the task being performed.
- Mechanism: Neural networks generally perform better on tasks they have encountered more frequently during training, leading to higher accuracy on common tasks compared to rare ones.
- Core assumption: The frequency of a task in the training data correlates with the model's ability to perform that task.
- Evidence anchors:
  - [abstract]: "we find robust evidence that LLMs are influenced by probability in the ways that we have hypothesized. In many cases, the experiments reveal surprising failure modes."
  - [section 5.1]: "GPT-4 scores 50% or above for the three most common shift levels (1, 3, and 13) yet scores below 3% for all other shifts."
  - [corpus]: Weak evidence; the paper uses corpus analysis to estimate task frequency but does not directly test the correlation between training frequency and model performance.
- Break condition: If a model can perform equally well on tasks of equal complexity regardless of their frequency in training data.

### Mechanism 2
- Claim: LLM performance is influenced by the probability of the target output.
- Mechanism: LLMs are statistical systems that favor high-probability sequences of words, leading to better performance when the correct output is high-probability text.
- Core assumption: The model's predictions are influenced by the unconditional probability of potential outputs, even in deterministic settings.
- Evidence anchors:
  - [abstract]: "LLMs will achieve higher accuracy when the answer is high-probability text than when it is low-probability text, even when the task is deterministic."
  - [section 6.1]: "GPT-4's accuracy ranges from 0.51 when the output sentence is high-probability to 0.13 when the output sentence is low-probability."
  - [corpus]: Weak evidence; the paper uses GPT-2 to estimate output probability but does not directly test the correlation between output probability and model performance.
- Break condition: If a model can perform equally well on examples with high-probability and low-probability outputs.

### Mechanism 3
- Claim: LLM performance is influenced by the probability of the provided input, but to a lesser degree than output probability.
- Mechanism: The probability of a piece of text influences how often it occurs in the training set, so a trained LLM will have had less experience with low-probability strings than high-probability strings.
- Core assumption: The model's ability to process an input is dependent on prior experience with that specific input.
- Evidence anchors:
  - [abstract]: "LLMs sometimes achieve higher accuracy when the input text is high-probability than when it is low-probability, but input probability is less influential than output probability."
  - [section 7.4]: "GPT-4 performed much better when the person had a high probability of being mentioned online (accuracy: 0.99) than when they had a low probability of being mentioned (accuracy: 0.23)."
  - [corpus]: Weak evidence; the paper uses corpus analysis to estimate input probability but does not directly test the correlation between input probability and model performance.
- Break condition: If a model can perform equally well on examples with high-probability and low-probability inputs.

## Foundational Learning

- Concept: Teleological approach
  - Why needed here: The teleological approach is used to understand LLMs by considering the problem they were trained to solve, namely next-word prediction over Internet text.
  - Quick check question: What is the main goal of the teleological approach in understanding LLMs?

- Concept: Task probability
  - Why needed here: Task probability is the probability that a task will be illustrated in a randomly-selected sample of text, and it influences LLM performance.
  - Quick check question: How does task probability affect LLM performance?

- Concept: Output probability
  - Why needed here: Output probability is the probability of the target output, and it influences LLM performance even in deterministic settings.
  - Quick check question: How does output probability affect LLM performance in deterministic tasks?

## Architecture Onboarding

- Component map: GPT-3.5 and GPT-4 models accessed through OpenAI API
- Critical path: Basic prompting → probability-controlled task evaluation → statistical analysis of sensitivity patterns
- Design tradeoffs: Basic prompting chosen over sophisticated methods to enable fair comparison across conditions
- Failure signatures: Sensitivity to task probability, output probability, and input probability; lack of embodiment; sensitivity to wording
- First 3 experiments:
  1. Shift ciphers: Evaluate models on decoding shift ciphers with various shift levels to test sensitivity to task probability
  2. Pig Latin: Compare performance on encoding and decoding Pig Latin sentences to test sensitivity to task probability and output probability
  3. Acronyms: Evaluate models on forming acronyms from sequences of words to test sensitivity to task probability and output probability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the sensitivity of LLMs to task probability and output probability translate to more complex, real-world tasks?
- Basis in paper: [inferred] ...
- Why unresolved: The paper primarily uses simple, controlled tasks to test its hypotheses. It remains unclear how these findings extend to more complex, practical applications where multiple factors might interact.
- What evidence would resolve it: Experiments evaluating LLM performance on a diverse range of real-world tasks, controlling for task probability, output probability, and other relevant factors.

### Open Question 2
- Question: To what extent do the observed sensitivities to task probability and output probability stem from the specific architecture of the tested LLMs (GPT-3.5 and GPT-4) versus being a general property of autoregressive models?
- Basis in paper: [explicit] ...
- Why unresolved: The paper only tests GPT-3.5 and GPT-4. It is unclear whether other autoregressive models, with different architectures or training data, would exhibit similar sensitivities.
- What evidence would resolve it: Systematic comparison of the sensitivities across a variety of autoregressive models with different architectures, training objectives, and training data.

### Open Question 3
- Question: How can the teleological approach be effectively applied to understand and improve the performance of LLMs on tasks where sensitivity to task probability and output probability leads to undesirable outcomes?
- Basis in paper: [explicit] ...
- Why unresolved: The paper identifies the problem but does not provide concrete solutions for mitigating the negative effects of these sensitivities.
- What evidence would resolve it: Development and evaluation of techniques, such as advanced prompting strategies or model modifications, that can reduce the impact of task and output probability on LLM performance.

## Limitations
- The paper makes strong assumptions about the relationship between training data distribution and model behavior that remain largely untested empirically
- The use of basic prompting without exploring more sophisticated prompting techniques may underestimate model capabilities in certain conditions
- Potential confounds between task probability, input probability, and output probability are not adequately addressed, as these are often correlated in natural text

## Confidence

**High Confidence:** The experimental results showing LLMs perform better on high-probability variants of tasks are robust across multiple experiments and models (GPT-3.5 and GPT-4).

**Medium Confidence:** The theoretical framework connecting autoregressive training to observed failure modes is compelling but requires further validation across different model architectures and training regimes.

**Low Confidence:** The claim that these effects are primarily due to autoregressive training rather than other factors (such as general statistical patterns in language) needs more rigorous testing.

## Next Checks

1. **Disentangle Probability Confounds:** Design experiments where task, input, and output probabilities are independently manipulated to isolate their individual effects on model performance, particularly for tasks where these probabilities are currently correlated.

2. **Cross-Architecture Validation:** Test the same hypotheses on non-autoregressive models (such as BERT or diffusion models) to determine whether the observed probability effects are unique to autoregressive training or represent more general properties of language models.

3. **Scale-Invariance Testing:** Evaluate whether the magnitude of probability effects scales predictably with model size across multiple orders of magnitude, testing whether smaller or larger models show stronger or weaker sensitivity to probability manipulations.