---
ver: rpa2
title: Cross-Modal Information-Guided Network using Contrastive Learning for Point
  Cloud Registration
arxiv_id: '2311.01202'
source_url: https://arxiv.org/abs/2311.01202
tags:
- point
- cloud
- learning
- features
- registration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CMIGNet, a novel cross-modal information-guided
  network for point cloud registration. The key idea is to leverage visual information
  from 2D images to enhance the global shape perception and registration accuracy
  of 3D point clouds.
---

# Cross-Modal Information-Guided Network using Contrastive Learning for Point Cloud Registration

## Quick Facts
- arXiv ID: 2311.01202
- Source URL: https://arxiv.org/abs/2311.01202
- Reference count: 39
- Key outcome: CMIGNet achieves superior point cloud registration performance by leveraging cross-modal visual information and contrastive learning strategies.

## Executive Summary
This paper introduces CMIGNet, a novel network for point cloud registration that incorporates visual information from 2D images to enhance global shape perception and registration accuracy. The method projects 3D point clouds into 2D images and fuses cross-modal features using attention mechanisms. It employs two contrastive learning strategies—overlapping contrastive learning for focusing on overlapping regions and cross-modal contrastive learning for establishing 2D-3D correspondences. A mask prediction module identifies discriminative keypoints to guide correspondence search. Extensive experiments on benchmark datasets demonstrate significant improvements over state-of-the-art methods in rotation and translation errors.

## Method Summary
CMIGNet addresses point cloud registration by integrating visual information from 2D images projected from 3D point clouds. The method uses EdgeConv for 3D feature extraction and CNN for 2D feature extraction, followed by transformer fusion layers to combine cross-modal information. Two contrastive learning strategies are employed: overlapping contrastive learning to focus on features in overlapping regions between point clouds, and cross-modal contrastive learning to establish correspondences between 2D and 3D features. A mask prediction module selects discriminative keypoints based on significance scores. The network uses SVD-based pose estimation with iterative refinement. The model is trained end-to-end with multiple loss components including contrastive losses, mask prediction loss, and registration losses.

## Key Results
- CMIGNet achieves lower rotation and translation errors compared to state-of-the-art methods on ModelNet40, Stanford 3D Scan, and 7Scenes datasets
- The method demonstrates significant robustness to noise with σ ∈ [0.02, 0.06] added to point clouds
- Cross-modal information integration provides substantial improvements in registration accuracy, particularly for partial overlaps and complex geometries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal contrastive learning aligns 2D image features with 3D point cloud features in a shared embedding space, improving global shape perception.
- Mechanism: Projected 2D images provide texture and global shape context that complements the local, geometric-only point cloud features. The cross-modal contrastive loss maximizes cosine similarity between averaged 3D point features and corresponding image features while minimizing similarity with negatives, forcing the network to learn correspondences that bridge the modality gap.
- Core assumption: 2D images projected from the same 3D shape contain discriminative texture and global shape information that can guide 3D feature learning.
- Evidence anchors: [abstract] "humans can employ visual information learned from 2D images to comprehend the 3D world"; [section III-B] "Cross-modal contrastive learning is utilized to establish 2D-3D correspondences"; [corpus] Weak - no direct ablation study showing cross-modal benefit in isolation.
- Break condition: If projected images lack texture or if the camera pose for projection is poor, the modality gap cannot be effectively bridged, and performance regresses to single-modal baselines.

### Mechanism 2
- Claim: Overlapping contrastive learning focuses feature learning on overlapping regions, reducing noise from non-overlapping points.
- Mechanism: The network identifies overlapping points using ground truth transformation and distance thresholding. Positive pairs come from overlapping points between clouds, while negatives mix overlapping and non-overlapping points. The loss pulls overlapping features together and pushes non-overlapping pairs apart, sharpening the feature distribution for registration.
- Core assumption: Correctly identifying overlapping points via ground truth enables meaningful contrastive pairs.
- Evidence anchors: [section III-B] "We consider pairs of overlapping point features between the two point clouds as the positive pair set"; [section III-B] "Our overlapping contrastive learning loss LOCL can be constructed as follows"; [corpus] Missing - no quantitative ablation isolating overlapping contrastive loss effect.
- Break condition: If the overlap threshold is too strict, few points are labeled as overlapping, weakening the contrastive signal; if too loose, non-overlapping points pollute the positive set.

### Mechanism 3
- Claim: Mask prediction selects discriminative keypoints to guide correspondence search, improving registration accuracy.
- Mechanism: The network computes a significance score for each point based on hybrid features and coordinates. The top-K points form a mask, reducing the search space and focusing on informative regions (edges, overlaps). These keypoints are used in the correspondence search module instead of all points.
- Core assumption: Keypoints with high significance scores are more reliable for matching than uniformly sampled points.
- Evidence anchors: [section III-D] "We create the final mask AX ∈ RN ×1 by setting the mask of the K points with the highest significance score to 1"; [section III-D] "This mask is then used to select the coordinates (P k X, P k Y ) and features (F k X, F k Y ) of the K keypoints"; [section IV-F] "Comparing Row 4 with Row 5 in Table VI, it becomes evident that the Mask Prediction module has a beneficial impact."
- Break condition: If K is too small, the network loses global context; if too large, noise from non-discriminative points dominates the correspondence search.

## Foundational Learning

- Concept: Cross-modal feature fusion
  - Why needed here: Point clouds lack texture and global shape cues; images provide them. Fusing both modalities compensates for each other's weaknesses and improves registration robustness.
  - Quick check question: If you project a point cloud from multiple views but only use the point features, how would the model perceive global shape?

- Concept: Contrastive learning with positive/negative pairs
  - Why needed here: It enforces discriminative feature learning by pulling similar points together and pushing dissimilar ones apart, crucial for aligning partial, noisy point clouds.
  - Quick check question: In overlapping contrastive learning, what happens to the loss if all pairs are labeled as positives?

- Concept: Attention mechanisms for feature interaction
  - Why needed here: The Transformer layers let the model attend to global shape and texture features, enhancing local point features for better matching.
  - Quick check question: What would happen if you removed the attention layers and concatenated point and image features directly?

## Architecture Onboarding

- Component map: Feature Extraction -> Multiple Contrastive Learning -> Transformer Fusion -> Mask Prediction -> Correspondences Search -> SVD -> Iteration
- Critical path: Feature Extraction → Multiple Contrastive Learning → Transformer Fusion → Mask Prediction → Correspondences Search → SVD → Iteration
- Design tradeoffs:
  - Number of projected views V vs. memory usage
  - K (keypoints) vs. robustness and computational cost
  - Loss weighting between contrastive losses and registration losses
  - Number of iterations n vs. convergence and runtime
- Failure signatures:
  - Large rotation/translation errors with high variance → possible feature collapse or poor contrastive signal
  - Registration stuck in local minima → may need more iterations or stronger keypoint selection
  - Degraded performance on unseen categories → possible overfitting to training modality distributions
- First 3 experiments:
  1. Train with only point cloud features (remove CMD) and compare RMSE to full model to quantify cross-modal gain
  2. Remove Mask Prediction (MP) and use all points in Correspondences Search to see impact on accuracy vs. runtime
  3. Train without overlapping contrastive loss (OCL) to isolate its effect on partial overlap scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed cross-modal contrastive learning approach compare in performance to alternative cross-modal fusion techniques (e.g., direct concatenation, attention-based fusion) for point cloud registration?
- Basis in paper: [explicit] The paper mentions that cross-modal contrastive learning is used to establish 2D-3D correspondences, but does not compare its effectiveness to other fusion methods.
- Why unresolved: The paper focuses on the effectiveness of the proposed method but does not provide a comparative analysis with other cross-modal fusion techniques.
- What evidence would resolve it: A comparative study between cross-modal contrastive learning and other fusion techniques (e.g., direct concatenation, attention-based fusion) for point cloud registration, using the same experimental setup and datasets as the proposed method.

### Open Question 2
- Question: What is the impact of different numbers of projected views (V) on the registration performance of the proposed method?
- Basis in paper: [explicit] The paper mentions that V corresponding 2D images are obtained by projecting 3D point clouds onto various viewpoints, but does not discuss the impact of different values of V on performance.
- Why unresolved: The paper does not provide a sensitivity analysis on the number of projected views, which could affect the registration accuracy and computational efficiency.
- What evidence would resolve it: A study on the impact of different numbers of projected views (V) on the registration performance, including a trade-off analysis between accuracy and computational cost.

### Open Question 3
- Question: How does the proposed method perform on real-world point cloud data with significant noise, occlusions, and varying densities compared to existing state-of-the-art methods?
- Basis in paper: [explicit] The paper evaluates the method on synthetic datasets (ModelNet40, Stanford 3D Scan, and 7Scenes) with controlled noise and occlusion levels, but does not discuss performance on real-world data with more complex noise and occlusion patterns.
- Why unresolved: The paper does not provide a comprehensive evaluation of the method's performance on real-world point cloud data with varying noise, occlusions, and densities, which is crucial for practical applications.
- What evidence would resolve it: A comparative study on real-world point cloud data with significant noise, occlusions, and varying densities, using the proposed method and existing state-of-the-art methods, to assess the robustness and generalization of the proposed approach.

## Limitations

- The paper lacks ablation studies isolating the contribution of cross-modal contrastive learning and overlapping contrastive learning, making it difficult to quantify their individual impact on performance gains
- The exact CNN architecture for image feature extraction is unspecified, which affects reproducibility of the cross-modal fusion performance
- The number of projected viewpoints (V) is not stated, which is a critical hyperparameter for cross-modal information quality

## Confidence

- **High Confidence:** The mask prediction mechanism's positive impact on registration accuracy, supported by the ablation comparison in Table VI showing benefit when MP is included
- **Medium Confidence:** The overall effectiveness of cross-modal information guidance, supported by the fundamental claim that 2D images provide global shape context missing in point clouds, though direct ablation evidence is lacking
- **Medium Confidence:** The effectiveness of multiple contrastive learning strategies, as the mechanism is sound but quantitative isolation of their individual contributions is missing

## Next Checks

1. **Cross-modal ablation test:** Train CMIGNet without the cross-modal contrastive learning module (CMD) and compare rotation/translation errors to the full model to quantify the exact contribution of visual information

2. **Mask prediction sensitivity analysis:** Vary the number of keypoints K systematically and measure the tradeoff between registration accuracy and computational efficiency to find the optimal balance

3. **Overlapping contrastive loss isolation:** Remove the overlapping contrastive learning loss and evaluate performance on datasets with varying overlap ratios to determine its specific impact on partial registration scenarios