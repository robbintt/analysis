---
ver: rpa2
title: 'Combining Past, Present and Future: A Self-Supervised Approach for Class Incremental
  Learning'
arxiv_id: '2311.08764'
source_url: https://arxiv.org/abs/2311.08764
tags:
- learning
- classes
- knowledge
- space
- phase
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes CPPF, a self-supervised class incremental learning
  framework that addresses catastrophic forgetting without class labels. The core
  idea is to combine knowledge from past, present, and future phases through three
  modules: prototype clustering (PC) for incremental prototype adjustment, embedding
  space reserving (ESR) to prepare space for unknown classes, and multi-teacher distillation
  (MTD) that preserves past knowledge while maintaining current class relations.'
---

# Combining Past, Present and Future: A Self-Supervised Approach for Class Incremental Learning

## Quick Facts
- **arXiv ID**: 2311.08764
- **Source URL**: https://arxiv.org/abs/2311.08764
- **Reference count**: 40
- **Key result**: Achieves 64.11% average accuracy on CIFAR100 and 67.92% on ImageNet100 in 5-phase self-supervised CIL experiments, outperforming existing methods by 2.23% and 1.20% respectively

## Executive Summary
This paper introduces CPPF, a self-supervised class incremental learning framework that addresses catastrophic forgetting without using class labels. The core innovation is combining knowledge from past, present, and future phases through three specialized modules: prototype clustering for incremental prototype adjustment, embedding space reserving to prepare space for unknown classes, and multi-teacher distillation that preserves past knowledge while maintaining current class relations. The method achieves state-of-the-art performance on CIFAR100 and ImageNet100 benchmarks while being robust across different self-supervised learning methods and sequence lengths.

## Method Summary
CPPF is a three-module framework designed for self-supervised class incremental learning without class labels. It combines a prototype clustering module (PC) that incrementally adjusts prototype distribution using fixed cluster centers, an embedding space reserving module (ESR) that divides embedding space into known/unknown regions using margin constraints, and a multi-teacher distillation module (MTD) that preserves knowledge through two teacher networks (one frozen from past phases, one trained on current novel classes only). The framework is trained with a combined loss function incorporating self-supervised, clustering, margin, and distillation losses.

## Key Results
- Achieves 64.11% average accuracy on CIFAR100 (5 phases, 20 classes each)
- Achieves 67.92% average accuracy on ImageNet100 (5 phases, 20 classes each)
- Outperforms existing self-supervised CIL methods by 2.23% on CIFAR100 and 1.20% on ImageNet100
- Demonstrates robustness across different self-supervised learning methods (SwAV, SimCLR, Barlow Twins)
- Maintains effectiveness with varying sequence lengths (10 phases tested)

## Why This Works (Mechanism)

### Mechanism 1
The PC (Prototype Clustering) module incrementally adjusts prototype distribution to prepare for future classes without using class labels. It uses fixed number of cluster centers as anchors to constrain prototype distribution through a clustering loss, pushing features toward cluster centers while maintaining adaptive clustering behavior. Core assumption: Self-adaptive clustering with fixed centers can effectively reserve embedding space for unknown classes without explicit class information. Break condition: If cluster centers become too concentrated or too sparse, the embedding space reservation becomes ineffective.

### Mechanism 2
The ESR (Embedding Space Reserving) module uses cluster centers to divide embedding space into known/unknown regions without class labels. It randomly divides cluster centers into two groups (chosen/reserved) and constrains features to be close to chosen centers and far from reserved centers using margin loss, effectively squeezing known class space and reserving space for unknown classes. Core assumption: Cosine similarity-based margin constraints can effectively separate known and unknown class embedding regions without explicit class boundaries. Break condition: If Î» parameter is poorly chosen, either too much or too little space is reserved, degrading performance.

### Mechanism 3
The MTD (Multi-Teacher Distillation) module prevents interference between past and current knowledge while preserving both. It uses two teacher networks - one distilling feature-level knowledge from past classes, another distilling relation-level information from current classes only. Student network combines both streams. Core assumption: Separate teacher networks trained on disjoint class sets can better preserve knowledge than a single teacher network. Break condition: If current teacher network is too weak early in training, relation distillation becomes ineffective.

## Foundational Learning

- **Self-supervised learning and contrastive learning objectives**: Why needed here: The framework needs to learn discriminative representations without class labels, requiring understanding of SSL methods like SwAV, SimCLR, Barlow Twins. Quick check question: How does SwAV clustering differ from standard contrastive learning approaches?

- **Knowledge distillation and multi-teacher frameworks**: Why needed here: The MTD module requires understanding of how knowledge can be distilled from multiple teacher networks with different expertise. Quick check question: What's the difference between feature-level and relation-level distillation in multi-teacher setups?

- **Catastrophic forgetting and continual learning dynamics**: Why needed here: The framework specifically addresses catastrophic forgetting in class-incremental scenarios, requiring understanding of why neural networks forget and how to mitigate it. Quick check question: Why does standard fine-tuning on new classes cause catastrophic forgetting of old classes?

## Architecture Onboarding

- **Component map**: Data augmentation -> Feature extraction -> PC clustering -> ESR space division -> MTD distillation -> Parameter update
- **Critical path**: Features flow through backbone, undergo clustering with PC, space division with ESR, and knowledge distillation with MTD before parameter update
- **Design tradeoffs**: Fixed vs adaptive number of cluster centers, feature vs relation distillation emphasis, chosen vs reserved space proportion, memory efficiency vs performance
- **Failure signatures**: PC: Cluster centers collapse or disperse too widely; ESR: Features don't respect margin constraints, space reservation fails; MTD: Current teacher too weak, relation distillation ineffective
- **First 3 experiments**: 1) Baseline SSL performance without any CIL modules (verification of SSL setup); 2) PC module only (test prototype clustering effectiveness); 3) ESR module only with random cluster center selection (test space reservation mechanism)

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed CPPF framework perform when the number of classes in each phase varies significantly (e.g., some phases have 10 classes while others have 50)? The paper mentions that the total number of classes and the number of classes in each phase are unavailable in self-supervised learning, requiring the CIL model to cope with different numbers of phases. However, the experiments conducted use a fixed number of classes per phase.

### Open Question 2
What is the impact of the clustering loss function (Lclus) on the overall performance of CPPF, and how does it compare to other clustering algorithms like K-Means? The paper mentions that the clustering loss is the distance between each prototype and the nearest cluster center, and it is more suitable for self-supervised learning compared to other clustering algorithms with a definite number of clusters like K-Means.

### Open Question 3
How does the performance of CPPF change when using different backbone networks, such as ResNet50 or VGG16, instead of ResNet18? The paper uses ResNet18 as the backbone network for convenience of comparison, but it does not explore the impact of using different backbone networks on the performance of CPPF.

## Limitations
- Framework relies on fixed cluster centers and random partitioning in ESR, which may lead to suboptimal space reservation if cluster centers become poorly distributed
- Effectiveness across diverse datasets and more complex scenarios (e.g., more than 5 phases, larger class spaces) remains unverified
- Choice of self-supervised learning method (SwAV) as baseline may influence the generalizability of results to other SSL approaches

## Confidence
**High Confidence**: Claims about state-of-the-art performance on CIFAR100 and ImageNet100, and the overall framework design with three complementary modules. The mathematical formulations and training procedures are well-defined and reproducible.
**Medium Confidence**: Claims about the effectiveness of individual modules (PC, ESR, MTD) in isolation, as ablation studies show improvements but don't fully isolate module contributions. The robustness across different self-supervised methods is demonstrated but could be expanded.
**Low Confidence**: Claims about scalability to more than 5 phases and performance on larger-scale datasets beyond CIFAR100 and ImageNet100 subsets.

## Next Checks
1. **Extended Phase Testing**: Validate framework performance on 10+ phase scenarios to assess long-term scalability and identify potential degradation patterns in prototype clustering and space reservation.
2. **Cross-SSL Method Validation**: Implement CPPF with multiple self-supervised learning methods (SimCLR, Barlow Twins, MoCo) to verify robustness claims and identify method-specific limitations.
3. **Cluster Center Sensitivity Analysis**: Systematically vary cluster center initialization strategies and evaluate impact on embedding space reservation effectiveness, particularly in later phases where space becomes constrained.