---
ver: rpa2
title: 'Dissecting Chain-of-Thought: Compositionality through In-Context Filtering
  and Learning'
arxiv_id: '2305.18869'
source_url: https://arxiv.org/abs/2305.18869
tags:
- layer
- cot-i
- in-context
- learning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how chain-of-thought prompting improves
  transformers'' ability to learn compositional functions, specifically multi-layer
  perceptrons (MLPs). The authors decompose the CoT mechanism into two phases: filtering
  (selecting relevant data for each compositional step) and in-context learning (learning
  the single-step function).'
---

# Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning

## Quick Facts
- arXiv ID: 2305.18869
- Source URL: https://arxiv.org/abs/2305.18869
- Reference count: 40
- Key outcome: CoT reduces sample complexity for learning 2-layer MLPs from Ω(kd) to O(max(k,d)) by decomposing the task into filtering and in-context learning phases

## Executive Summary
This paper investigates how chain-of-thought prompting improves transformers' ability to learn compositional functions, specifically multi-layer perceptrons (MLPs). The authors decompose the CoT mechanism into two phases: filtering (selecting relevant data for each compositional step) and in-context learning (learning the single-step function). Through both theoretical analysis and experiments, they demonstrate that CoT significantly reduces sample complexity for learning MLPs. Experimental results validate these findings across different GPT-2 model sizes and MLP configurations.

## Method Summary
The paper analyzes chain-of-thought prompting through controlled experiments with synthetic data generated for 2-layer MLPs. The authors train GPT-2 models (standard, small, tiny) with CoT-I and CoT-I/O prompting schemes on datasets following normal distributions. CoT-I uses intermediate steps in prompts while CoT-I/O makes recurrent predictions outputting intermediate steps. Training uses curriculum learning over prompt length from 1 to 100, with 500k iterations using Adam optimizer (learning rate 0.0001, batch size 64). The objective is to measure test risk (mean squared error) on held-out samples.

## Key Results
- CoT-I/O learns 2-layer MLPs using O(max(k,d)) in-context samples versus Ω(kd) without CoT
- CoT significantly accelerates pretraining by memorizing discrete matrix sets and composing them step-by-step
- Experiments validate findings across GPT-2 model sizes, showing consistent sample complexity reduction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoT decomposes complex compositional learning into filtering and in-context learning phases
- Mechanism: The transformer first filters relevant data for each compositional step using attention patterns, then learns each step separately through in-context examples
- Core assumption: The attention mechanism can effectively separate and focus on relevant tokens for each compositional layer
- Evidence anchors:
  - [abstract] "the success of CoT can be attributed to breaking down in-context learning of a compositional function into two distinct phases: focusing on and filtering data related to each step of the composition and in-context learning the single-step composition function"
  - [section 2.2] "the model attends to the relevant tokens within the prompt based on an instruction, and suppresses those irrelevant"
- Break condition: When the compositional function becomes too complex for the attention mechanism to effectively separate relevant tokens, or when the filtering requires more computational power than available in the transformer architecture

### Mechanism 2
- Claim: CoT-I/O achieves O(max(k,d)) sample complexity for learning 2-layer MLPs
- Mechanism: By outputting intermediate steps, the model only needs to learn k d-dimensional ReLU problems and 1 k-dimensional linear regression, rather than the full k×d parameter space
- Core assumption: The intermediate outputs provide sufficient information for the model to reconstruct the full compositional function
- Evidence anchors:
  - [abstract] "CoT-I/O can learn a 2-layer MLP with input dimension d and k neurons using O(max(k,d)) in-context samples, compared to Ω(kd) samples needed without CoT"
  - [section 3.1] "CoT-I/O can learn an MLP with input dimension d and k neurons using O(max(d, k)) in-context samples by filtering individual layers and solving them via linear regression"
- Break condition: When k >> d or d >> k, the assumption that O(max(k,d)) is sufficient breaks down, or when the function complexity exceeds what can be captured by the intermediate step representation

### Mechanism 3
- Claim: CoT accelerates pretraining by learning shortcuts to represent complex functions
- Mechanism: The model memorizes discrete matrix sets from the function family and composes them step-by-step, avoiding expensive linear regression
- Core assumption: The function family has a finite discrete representation that can be memorized and composed
- Evidence anchors:
  - [abstract] "we show CoT can dramatically accelerate pretraining by memorizing these discrete matrices and can infer all layers correctly from a single demonstration"
  - [section 4.3] "CoT can dramatically accelerate pretraining by memorizing these discrete matrices and can infer all layers correctly from a single demonstration"
- Break condition: When the function family is continuous rather than discrete, or when the number of possible matrices grows exponentially with layer depth

## Foundational Learning

- Concept: Linear regression and gradient descent
  - Why needed here: The paper shows that CoT-I/O can solve MLPs by reducing them to linear regression problems at each layer
  - Quick check question: Given a set of (x, y) pairs, can you derive the closed-form solution for linear regression and explain how gradient descent would iteratively approach this solution?

- Concept: Condition numbers and matrix stability
  - Why needed here: The theoretical analysis depends on the condition number of feature matrices to bound the sample complexity
  - Quick check question: If you have a feature matrix T with condition number κ, what is the maximum error amplification you can expect when solving a linear system, and how does this affect the number of samples needed?

- Concept: Transformer attention mechanisms
  - Why needed here: The filtering phase relies on the transformer's ability to attend to relevant tokens while suppressing irrelevant ones
  - Quick check question: Given an input sequence with positional encodings, can you trace through how multi-head attention computes the output at each layer, and explain how this could be used to implement a filtering operation?

## Architecture Onboarding

- Component map: Input → Attention-based filtering → Feature extraction → Linear regression/gradient descent → Output intermediate step → Loop back for next layer
- Critical path: Input → Attention-based filtering → Feature extraction → Linear regression/gradient descent → Output intermediate step → Loop back for next layer
- Design tradeoffs: The architecture trades off between model size (to capture complex functions) and sample efficiency (to minimize the number of in-context examples needed). Larger models can handle more complex functions but require more samples
- Failure signatures: When the model fails to filter correctly, you'll see high error on intermediate steps but low error on final outputs (or vice versa). When the model fails to learn the ICL phase correctly, you'll see uniform high error across all steps
- First 3 experiments:
  1. Implement a simple 2-layer MLP with small d and k values, train using CoT-I/O, and verify that the model achieves low error with O(max(k,d)) samples
  2. Implement the same MLP but use CoT-I instead of CoT-I/O, and compare sample complexity requirements
  3. Implement a deep linear MLP with discrete weight matrices, and verify that CoT-I/O can learn from a single demonstration while ICL requires many samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How extensively do our findings on CoT's filtering and ICL phases align with empirical evidence in practical problems such as code generation and mathematical reasoning?
- Basis in paper: [explicit] The paper mentions this as an interesting avenue for future research, stating "To what extent our decoupling of CoT (filtering followed by ICL) align with the empirical evidence in practical problems such as code generation and mathematical reasoning?"
- Why unresolved: The current study focuses on theoretical analysis and controlled experiments with MLPs. Real-world applications involve more complex functions and data distributions, which may exhibit different behaviors.
- What evidence would resolve it: Experiments applying CoT to code generation and mathematical reasoning tasks, measuring the effectiveness of filtering and ICL phases in these contexts.

### Open Question 2
- Question: To what extent can transformers approximate MLPs without CoT-I/O (e.g., with CoT-I), and what are the lower/upper bounds?
- Basis in paper: [explicit] The paper poses this as an open question, stating "To what extent transformers can approximate MLPs without CoT-I/O (e.g. with CoT-I), what are lower/upper bounds?"
- Why unresolved: While the paper demonstrates CoT-I/O's effectiveness, it doesn't explore the limits of CoT-I or other non-looping variants. The theoretical bounds for these methods are not established.
- What evidence would resolve it: Rigorous theoretical analysis establishing sample complexity bounds for CoT-I and other variants, along with empirical comparisons across different MLP architectures and sizes.

## Limitations

- The theoretical analysis makes strong assumptions about discrete function families that may not generalize to continuous functions
- Empirical validation is limited to small GPT-2 models and synthetic MLP tasks, raising scalability questions
- Pretraining acceleration claims through matrix memorization lack extensive empirical validation

## Confidence

- High confidence: The decomposition of CoT into filtering and in-context learning phases is well-supported by both theoretical analysis and experimental evidence. The sample complexity analysis showing O(max(k,d)) for 2-layer MLPs is mathematically rigorous and empirically validated.
- Medium confidence: The mechanism by which attention patterns enable effective filtering is plausible but relies on idealized assumptions about the transformer's attention capabilities. The experimental results across different GPT-2 sizes support the claims but are limited in scope.
- Low confidence: The pretraining acceleration claims through matrix memorization are primarily theoretical, with minimal experimental validation. The assumption that function families have discrete representations that can be efficiently memorized may not generalize to more complex scenarios.

## Next Checks

1. **Scaling Experiment**: Test the CoT mechanism on larger transformer architectures (e.g., GPT-2 XL or GPT-Neo) with more complex compositional functions (d > 100, k > 50) to evaluate scalability and identify potential break conditions.

2. **Continuous Function Analysis**: Implement experiments with continuous function families (rather than discrete weight matrices) to test whether the CoT mechanism maintains its efficiency advantages and to identify the point at which matrix memorization becomes ineffective.

3. **Attention Pattern Analysis**: Conduct detailed analysis of attention patterns during the filtering phase across multiple layers and heads, quantifying how effectively the model separates relevant from irrelevant tokens for different compositional depths and function complexities.