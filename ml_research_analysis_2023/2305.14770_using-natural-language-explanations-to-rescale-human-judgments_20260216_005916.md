---
ver: rpa2
title: Using Natural Language Explanations to Rescale Human Judgments
arxiv_id: '2305.14770'
source_url: https://arxiv.org/abs/2305.14770
tags:
- label
- answer
- annotators
- explanations
- annotator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving agreement between
  human annotators for subjective tasks like evaluating model outputs. The authors
  propose using natural language explanations to rescale ordinal annotations.
---

# Using Natural Language Explanations to Rescale Human Judgments

## Quick Facts
- arXiv ID: 2305.14770
- Source URL: https://arxiv.org/abs/2305.14770
- Authors: 
- Reference count: 40
- One-line primary result: Using LLM-based rescaling of human judgments with natural language explanations improves inter-annotator correlation by 0.2 Kendall's tau for low-quality cases.

## Executive Summary
This paper introduces a method to improve agreement between human annotators for subjective NLP tasks by leveraging natural language explanations. The authors propose using an LLM to convert Likert ratings and accompanying explanations into calibrated 0-100 scores, enabling more precise alignment between annotators with different rating tendencies. Applied to document-grounded question answering, the method shows significant improvements in pairwise correlation, particularly for cases where annotators initially disagreed. The approach preserves the inherent subjectivity of human judgment while making it more consistent across raters.

## Method Summary
The method involves collecting human annotations consisting of Likert ratings (missing all, missing major, missing minor, complete) and natural language explanations for each judgment. An LLM (ChatGPT) is then used in a zero-shot manner to map each (rating, explanation) pair to a 0-100 score using a predefined prompt template. When explanations are missing, class-level average scores are used instead. The rescaled scores are then compared to raw ratings using Kendall's tau correlation between annotator pairs, both overall and for incomplete label subsets. The approach aims to homogenize ratings across annotators while preserving their underlying reasoning captured in the explanations.

## Key Results
- Overall Kendall's tau correlation improved by 0.02 between annotator pairs after rescaling
- For incomplete label cases, correlation improved by 0.19 on average
- The method shows particular effectiveness for low-quality cases where annotators initially disagreed
- Rescaling brings scores closer to human judgments grounded in the same scoring rubric

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Natural language explanations allow an LLM to capture the underlying reasoning behind an annotator's discrete rating, enabling more precise score mapping.
- Mechanism: The LLM processes both the original Likert label and the accompanying free-text explanation to generate a 0-100 score that reflects the severity or nuance of the annotator's judgment.
- Core assumption: Annotators' explanations are sufficiently informative to reveal their underlying reasoning and distinguish between similar Likert ratings.
- Evidence anchors:
  - [abstract]: "These nuances can be captured by natural language explanations, and propose a method to rescale ordinal annotation in the presence of disagreement using LLMs."
  - [section 5.1]: "To compute f, we invoke ChatGPT, taking as input both the explanation eij along with the discrete rating label rij."
  - [corpus]: Weak. No corpus papers directly validate explanation fidelity for score alignment; only related work on NLEs.
- Break condition: If explanations are too vague, inconsistent, or fail to capture the reasoning behind the rating, the LLM cannot reliably map them to calibrated scores.

### Mechanism 2
- Claim: Using an LLM to rescale individual annotations post-hoc improves inter-annotator correlation without requiring retraining or relabeling.
- Mechanism: The LLM-based rescaling adjusts each annotator's scores based on their explanations, bringing divergent ratings closer together when the underlying reasoning aligns.
- Core assumption: Annotators can have different discrete labels for the same example but still agree on the key aspects of the judgment, which the LLM can detect and harmonize.
- Evidence anchors:
  - [abstract]: "Our method rescales the raw judgments without impacting agreement and brings the scores closer to human judgments grounded in the same scoring rubric."
  - [section 6.2]: "Our proposed method leads to an overall increase of 0.02 when all data is considered and 0.19 on the incomplete setting."
  - [corpus]: Moderate. Related work shows LLMs can serve as evaluators and align judgments, but none specifically use explanations for rescaling.
- Break condition: If the LLM misinterprets explanations or introduces its own judgment, rescaling may reduce rather than improve correlation.

### Mechanism 3
- Claim: The zero-shot capability of modern LLMs allows them to reliably convert qualitative feedback into numeric scores without additional fine-tuning.
- Mechanism: The LLM uses the prompt template to interpret the explanation and label context, assigning a calibrated 0-100 score based on the scoring rubric provided in the prompt.
- Core assumption: The LLM's zero-shot reasoning is sufficiently robust to handle diverse explanations and map them to a consistent numeric scale.
- Evidence anchors:
  - [abstract]: "LLMs have been shown to do this kind of numeric rating reliably (Kocmi and Federmann, 2023)."
  - [section 5.1]: "This usage follows the template in Kocmi and Federmann (2023). Their work and our own experimentation validated that using a 0-100 scale worked better than prompting along other scales."
  - [corpus]: Weak. Only indirect support from Kocmi and Federmann (2023); no other corpus papers validate zero-shot numeric rating from explanations.
- Break condition: If the LLM's zero-shot performance degrades with explanation complexity or domain specificity, score calibration will fail.

## Foundational Learning

- Concept: Inter-annotator agreement metrics (Kendall's Tau, Fleiss' Kappa)
  - Why needed here: To measure how well the rescaling method improves correlation between annotators' rankings.
  - Quick check question: If annotator A ranks items as [3, 1, 2] and annotator B as [2, 1, 3], what is Kendall's Tau?

- Concept: Natural language explanation generation and interpretation
  - Why needed here: Annotators provide free-text justifications for their ratings, which the LLM uses to generate calibrated scores.
  - Quick check question: What key elements should an explanation include to help an LLM distinguish between "missing minor" and "missing major" information?

- Concept: Prompt engineering for zero-shot LLM reasoning
  - Why needed here: The LLM must be prompted effectively to convert explanations into numeric scores without additional training.
  - Quick check question: How would you structure a prompt to ensure the LLM interprets "missing all information" as a score near 0 rather than 50?

## Architecture Onboarding

- Component map: Articles -> Questions -> LLM answers -> Human annotations (ratings + explanations) -> LLM scorer (ChatGPT) -> f(rij, eij) -> rescaled scores -> Kendall's Tau correlation computation between annotator pairs -> Dataset storage (INQUISITIVE-BROAD)

- Critical path: Annotator submits rating + explanation -> LLM scorer processes input and returns 0-100 score -> Rescaled scores are stored and used for correlation analysis -> Aggregate correlation metrics are computed across all annotator pairs

- Design tradeoffs: Using explanations adds annotation overhead but enables finer-grained calibration; Zero-shot LLM scoring avoids fine-tuning but depends on LLM reasoning quality; Rescaling preserves subjectivity while improving alignment, unlike consensus averaging

- Failure signatures: Low correlation improvement despite explanation presence; LLM returns inconsistent scores for similar explanations; Rescaled scores invert the intended ranking (e.g., "missing all" > "complete")

- First 3 experiments: 1) Run EBR on a small subset where explanations are clearly aligned but ratings differ; verify correlation improvement. 2) Test EBR without explanations (using only class label averages) to confirm explanations are the key driver. 3) Compare EBR to a simple per-annotator rescaling baseline to isolate the effect of explanation-based adjustment.

## Open Questions the Paper Calls Out

- Question: How does the performance of the LLM-based rescaling method compare when applied to different types of subjective NLP tasks beyond document-grounded question answering?
- Basis in paper: [explicit] The authors mention that their technique can generalize to other challenging annotation tasks but do not provide empirical evidence for tasks other than document-grounded QA.
- Why unresolved: The paper focuses solely on one task type, leaving the generalizability to other subjective tasks unexplored.
- What evidence would resolve it: Empirical studies applying the LLM-based rescaling method to a diverse set of subjective NLP tasks (e.g., sentiment analysis, hate speech detection, natural language inference) and comparing its performance across these tasks.

- Question: What is the optimal balance between the information provided to the LLM (e.g., question, answer, article) to achieve the best rescaling results without introducing bias?
- Basis in paper: [explicit] The authors explore different prompt variants providing varying levels of information to the LLM but do not determine the optimal combination.
- Why unresolved: The paper presents a trade-off between providing enough context for accurate rescaling and avoiding the introduction of the LLM's own judgment, but does not resolve which approach is best.
- What evidence would resolve it: Systematic experiments comparing the performance of the LLM-based rescaling method using different combinations of input information (question, answer, article) across various tasks and annotator pairs.

- Question: How does the quality and style of natural language explanations impact the effectiveness of the LLM-based rescaling method?
- Basis in paper: [explicit] The authors acknowledge that the quality of underlying explanations and labels can impact the method's effectiveness but do not explore how different explanation styles affect rescaling.
- Why unresolved: The paper assumes high-quality explanations but does not investigate the relationship between explanation characteristics and rescaling performance.
- What evidence would resolve it: Analysis of rescaling performance using explanations of varying quality, length, and specificity, and identification of explanation features that correlate with improved alignment between annotators.

## Limitations

- The method's effectiveness depends heavily on the quality and consistency of natural language explanations provided by annotators
- The study only evaluates the approach on document-grounded question answering, limiting generalizability to other subjective tasks
- The correlation improvements, while statistically significant, are modest (0.02 overall, 0.19 for incomplete cases), suggesting the method addresses only part of the annotator disagreement problem

## Confidence

- **High confidence**: The fundamental mechanism of using explanations to capture annotator reasoning is well-supported by related work on NLEs and evaluation.
- **Medium confidence**: The specific implementation using ChatGPT zero-shot scoring is reasonable but untested against alternatives or fine-tuned approaches.
- **Medium confidence**: The correlation improvements are real but modest, suggesting the method is effective but not transformative.

## Next Checks

1. Test the method on a different subjective NLP task (e.g., summarization quality or dialogue coherence) to verify generalizability beyond QA.
2. Compare ChatGPT-based rescaling against a simple per-annotator normalization baseline to isolate the contribution of explanation-based adjustment.
3. Conduct an ablation study removing explanations entirely to quantify their specific contribution to correlation improvement.