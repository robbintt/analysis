---
ver: rpa2
title: 'Multinational AGI Consortium (MAGIC): A Proposal for International Coordination
  on AI'
arxiv_id: '2310.09217'
source_url: https://arxiv.org/abs/2310.09217
tags:
- magic
- would
- advanced
- systems
- development
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes MAGIC, an international governance structure
  to mitigate existential risks from advanced artificial intelligence (AI). MAGIC
  would be the world's only institution authorized to develop advanced AI, enforced
  through a global moratorium on all other development.
---

# Multinational AGI Consortium (MAGIC): A Proposal for International Coordination on AI

## Quick Facts
- arXiv ID: 2310.09217
- Source URL: https://arxiv.org/abs/2310.09217
- Authors: 
- Reference count: 40
- One-line primary result: Proposes MAGIC, an international governance structure to mitigate existential risks from advanced artificial intelligence through exclusive development rights and global moratorium

## Executive Summary
This paper proposes MAGIC, an international governance structure designed to mitigate existential risks from advanced artificial intelligence. MAGIC would be the world's only institution authorized to develop advanced AI, enforced through a global moratorium on all other development. The consortium would be exclusive, safety-focused, highly secure, and collectively supported by member states, allowing narrow AI models to flourish while significantly reducing the possibility of misaligned, rogue, breakout, or runaway outcomes of general-purpose systems.

The paper outlines a framework where compute monitoring serves as the primary enforcement mechanism, building on precedents from international governance regimes like the IAEA. While the technical and conceptual framework is detailed, the paper explicitly does not address the political feasibility of implementing a moratorium or the specific legislative strategies needed to enforce a ban on high-capacity AGI training runs.

## Method Summary
The paper proposes MAGIC as an international consortium that would be the exclusive institution permitted to develop advanced artificial intelligence, enforced through a global moratorium on all other development. The method centers on four core characteristics: exclusivity (monopoly on advanced AI development), safety-focus (differential technological development of safe architectures), security (highly secure facility), and collective action (international support and benefit-sharing). The proposal relies on compute monitoring as an enforcement mechanism, suggesting 1024 floating point operations (FLOP) as an initial threshold for what constitutes advanced AI development subject to the moratorium.

## Key Results
- MAGIC would be the world's only institution permitted to develop advanced AI, enforced through a global moratorium
- The consortium would prioritize safety-first development over building the most powerful models as quickly as possible
- Compute monitoring is proposed as the primary enforcement mechanism for the moratorium
- The framework allows narrow AI models to flourish while restricting general-purpose system development

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compute monitoring acts as an effective lever to control AI development.
- Mechanism: AI accelerators are physical, detectable resources currently untracked globally. By imposing export controls and monitoring data center training runs, regulators can identify and limit large-scale AI development.
- Core assumption: The amount of computing power used remains strongly correlated with model capabilities.
- Break condition: If algorithmic efficiency advances significantly outpace hardware improvements, compute may no longer be a reliable proxy for capabilities.

### Mechanism 2
- Claim: Exclusive international consortium reduces existential AI risks by preventing uncontrolled development.
- Mechanism: MAGIC would be the only institution authorized to develop advanced AI, enforced through a global moratorium on all other development. This monopolization allows for centralized safety oversight.
- Core assumption: A single, secure facility can effectively develop safe AI while preventing dangerous models from being created elsewhere.
- Break condition: If member states fail to enforce the moratorium or if sufficient computing power becomes widely accessible despite restrictions.

### Mechanism 3
- Claim: Safety-first development approach prevents dangerous AI behaviors.
- Mechanism: MAGIC would prioritize developing provably safe architectures over building the most powerful models. This involves experimenting with safer designs and bounding capabilities.
- Core assumption: It's possible to design AI systems with clear boundaries and coordination that prevent alternative forms of development.
- Break condition: If technical progress in safe architectures is too slow compared to unsafe approaches, or if safer methods are not sufficiently capable.

## Foundational Learning

- Concept: International governance mechanisms
  - Why needed here: Understanding how international bodies like the IAEA function provides precedent for MAGIC's structure and enforcement.
  - Quick check question: What are the key components of successful international governance regimes like the Montreal Protocol?

- Concept: AI compute measurement and monitoring
  - Why needed here: Effective governance of AI development relies on being able to measure and monitor compute usage.
  - Quick check question: How do current data centers track and report large-scale training runs?

- Concept: AI safety research paradigms
  - Why needed here: MAGIC's approach requires understanding different safety research methodologies and their tradeoffs.
  - Quick check question: What are the main differences between black-box foundation models and safer, more interpretable architectures?

## Architecture Onboarding

- Component map:
  - International treaty framework
  - Compute monitoring and enforcement mechanisms
  - Secure research facility
  - Safety research and development pipeline
  - Benefit distribution system

- Critical path:
  1. Establish international treaty and member states
  2. Implement compute monitoring and moratorium enforcement
  3. Build and secure MAGIC facility
  4. Recruit top AI researchers
  5. Develop and test safe AI architectures
  6. Distribute benefits to member states

- Design tradeoffs:
  - Security vs. inclusivity in research participation
  - Speed of development vs. safety guarantees
  - Centralization vs. innovation diversity
  - Control vs. global cooperation

- Failure signatures:
  - Member states not enforcing moratorium
  - Compute becoming too accessible despite restrictions
  - Safety research not yielding sufficient progress
  - Security breaches or leaks from MAGIC facility

- First 3 experiments:
  1. Implement compute monitoring pilot program with willing data center partners
  2. Conduct security audit of proposed MAGIC facility design
  3. Run comparative safety analysis of existing AI architectures vs. proposed safer alternatives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the political feasibility of implementing a global moratorium on advanced AI development outside of MAGIC?
- Basis in paper: [explicit] The paper explicitly states "We do not address the political feasibility of implementing a moratorium or address the specific legislative strategies and rules needed to enforce a ban on high-capacity AGI training runs."
- Why unresolved: The authors chose not to address this critical component, which is arguably the most challenging aspect of implementing MAGIC.
- What evidence would resolve it: Case studies of successful international technology moratoriums, analysis of global political will for AI governance, and concrete legislative proposals that have been seriously considered by multiple nations.

### Open Question 2
- Question: What specific technical thresholds would determine which AI systems fall under MAGIC's purview versus being considered "narrow AI"?
- Basis in paper: [explicit] The paper mentions "We suggest 1024 floating point operations (FLOP) as an appropriate threshold" but acknowledges this needs further development.
- Why unresolved: The authors only briefly mention compute thresholds without providing a detailed framework for how these thresholds would be determined, adjusted over time, or applied to different AI architectures.
- What evidence would resolve it: A comprehensive technical analysis comparing different threshold metrics (compute, parameters, capabilities), along with projections of how quickly models might approach various thresholds.

### Open Question 3
- Question: How would MAGIC balance the tradeoff between security (limiting participation) and inclusivity (broad international representation)?
- Basis in paper: [explicit] The paper acknowledges "In practice, inclusion may pose a direct tradeoff with security" but does not provide a concrete solution.
- Why unresolved: The authors recognize this tension but only offer vague suggestions about "governance structures that enable broad participation in the moratorium, but narrow participation in the most advanced research" without detailing how this would work.
- What evidence would resolve it: Detailed proposals for selection criteria, security protocols that accommodate international researchers, and analysis of how different levels of participation affect both security and global buy-in.

## Limitations
- The paper does not address the political feasibility of implementing a global moratorium on advanced AI development
- Specific legislative strategies and enforcement mechanisms for the moratorium are not detailed
- The technical assumption that safety-first development can proceed effectively within a monopolized system lacks empirical validation

## Confidence

- **High confidence**: The identification of compute as a governance lever - this aligns with existing policy discussions and technical reality
- **Medium confidence**: The general framework of international consortium governance - precedents exist in other domains
- **Low confidence**: The specific proposal that MAGIC would be the world's only authorized AI development institution - no precedent exists and political barriers are significant

## Next Checks

1. **Political Feasibility Analysis**: Conduct interviews with international relations experts to assess the realistic pathways for gaining support from major AI powers for such a governance regime.

2. **Compute Monitoring Pilot**: Design and test a small-scale compute monitoring system with willing data center partners to identify technical and operational challenges in real-world implementation.

3. **Comparative Governance Analysis**: Evaluate historical international governance regimes (IAEA, Montreal Protocol) to identify patterns of success/failure that could inform MAGIC's design and implementation strategy.