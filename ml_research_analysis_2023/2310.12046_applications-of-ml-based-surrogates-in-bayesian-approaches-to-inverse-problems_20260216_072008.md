---
ver: rpa2
title: Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems
arxiv_id: '2310.12046'
source_url: https://arxiv.org/abs/2310.12046
tags:
- source
- neural
- inverse
- location
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper applies neural network surrogate models to Bayesian
  inverse problems for inferring source locations in 2D acoustic wave equations. The
  approach trains a neural network to approximate the forward PDE solver, then uses
  this surrogate in a Bayesian framework with MCMC sampling to estimate posterior
  distributions over source locations given noisy pressure observations at multiple
  receiver points.
---

# Applications of ML-Based Surrogates in Bayesian Approaches to Inverse Problems

## Quick Facts
- arXiv ID: 2310.12046
- Source URL: https://arxiv.org/abs/2310.12046
- Reference count: 7
- Primary result: Neural network surrogates enable feasible Bayesian inference for acoustic source location problems with MSE of 3.76 × 10^-5

## Executive Summary
This paper demonstrates how neural network surrogate models can make Bayesian inference tractable for inverse problems involving 2D acoustic wave equations. By training a neural network to approximate the forward PDE solver, the authors enable computationally feasible MCMC sampling to estimate posterior distributions over source locations given noisy pressure observations. The approach achieves accurate source location inference with a mean squared error of 3.76 × 10^-5, while showing that 4-5 receivers provide optimal accuracy.

## Method Summary
The method combines neural network surrogate modeling with Bayesian inference for acoustic source location problems. First, a neural network is trained as a surrogate for the forward PDE solver using 50 training sources on a 16×16 grid. The network has 6 hidden layers with 100 neurons each, tanh activation, and 2 skip connections. For inference, the authors use a two-stage approach: global optimization on a 150×150 grid followed by Metropolis-Hastings MCMC sampling. Both observation noise (σ² = 0.25) and surrogate model noise (σ² = 0.248) are modeled as additive Gaussian, with the combined variance used in the likelihood function.

## Key Results
- Neural network surrogate achieved test MSE of 0.248
- Bayesian inference successfully estimated source locations with MSE of 3.76 × 10^-5
- 4-5 receivers provided optimal accuracy, with diminishing returns beyond that number
- Computational feasibility achieved through surrogate model reducing likelihood evaluation cost from minutes to milliseconds

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The neural network surrogate enables feasible MCMC sampling by drastically reducing the computational cost per likelihood evaluation
- Mechanism: Direct PDE solves for the 2D acoustic wave equation are computationally expensive, requiring numerous forward simulations. The trained neural network provides rapid approximate solutions, reducing the cost of each likelihood evaluation from potentially minutes to milliseconds, making MCMC sampling computationally tractable
- Core assumption: The neural network surrogate can approximate the forward PDE solution with sufficient accuracy (test MSE of 0.248) that the resulting posterior is meaningful for inference
- Evidence anchors:
  - [abstract] "Using a standard neural network as a surrogate model makes it computationally feasible to evaluate this likelihood several times, and so Markov Chain Monte Carlo methods can be used"
  - [section 2.2] "The MSE of the network predictions on test data is used as an estimate of the variance of the surrogate noise (σ2 = 0.248)"
  - [section 2.3] "Importantly, the number of forward simulations required (≈ 106) makes this approach only feasible when using surrogate models like the neural network"
- Break condition: If the surrogate model's approximation error is too large, the MCMC posterior will be unreliable, making the computational savings meaningless

### Mechanism 2
- Claim: The Bayesian framework with Gaussian noise assumption provides a principled way to incorporate uncertainty from both observation noise and surrogate model error
- Mechanism: By modeling both observation noise (σ2 = 0.25) and surrogate model noise (σ2 = 0.248) as additive Gaussian, the likelihood function properly accounts for multiple sources of uncertainty. This allows for uncertainty quantification in the inferred source locations rather than just point estimates
- Core assumption: The combined noise follows a Gaussian distribution with variance σ2 = σ2 + σ2, which allows for analytical treatment of the likelihood
- Evidence anchors:
  - [section 1.2] "Under the assumption of Gaussian noise, a likelihood function for source location can be formulated"
  - [section 1.2] "Pri,tj = ˜p(tj, xi|Ys) + ηi,j, ηi,j ~ N(0, σ2 + σ2)" explicitly models both noise sources
  - [section 2.2] "the MSE of the network predictions on test data is used as an estimate of the variance of the surrogate noise (σ2 = 0.248)"
- Break condition: If the noise distribution is non-Gaussian or if the surrogate error is heteroscedastic, the Gaussian assumption will lead to incorrect posterior distributions

### Mechanism 3
- Claim: The choice of receiver locations ensures identifiability of the source location problem while providing efficient coverage of the domain
- Mechanism: The receiver configuration {(0.25, 0.625), (0.5, 0.5), (0.25, 0.125), (0.75, 0.625), (0.75, 0.25)} satisfies two key requirements: (1) at least two receivers not collinear along symmetry axes for identifiability, and (2) equal coverage to avoid systematic bias. This design enables accurate inference with minimal computational overhead
- Core assumption: The acoustic wave propagation is sufficiently sensitive to source location changes that these specific receiver positions can distinguish between different source locations
- Evidence anchors:
  - [section 2.4] "Due to symmetries in the spatial domain of interest, data from at least two receivers, that are not co-linear along an axis of symmetry, is needed for the source location to be identifiable"
  - [section 2.4] "The following set of receivers satisfy these requirements" followed by the specific coordinates
  - [section 3.1] Table showing that 4-5 receivers provide optimal accuracy, with diminishing returns beyond that
- Break condition: If the domain geometry changes significantly or if the wave equation properties change, this specific receiver configuration may no longer be optimal

## Foundational Learning

- Concept: Partial Differential Equations and their numerical solution
  - Why needed here: Understanding the 2D acoustic wave equation and why direct numerical solutions are computationally expensive is fundamental to appreciating why surrogates are valuable
  - Quick check question: What are the computational complexity implications of using Discontinuous Galerkin methods for solving the 2D acoustic wave equation?

- Concept: Bayesian inference and MCMC sampling
  - Why needed here: The entire methodology relies on formulating the inverse problem in a Bayesian framework and using MCMC to sample from the posterior distribution
  - Quick check question: How does the Metropolis-Hastings algorithm work, and why is it appropriate for this high-dimensional parameter space?

- Concept: Neural network training and evaluation
  - Why needed here: Understanding how neural networks can be trained as surrogates for PDE solvers, including loss functions, architecture choices, and evaluation metrics
  - Quick check question: What is the significance of the test MSE = 0.248, and how does it relate to the uncertainty quantification in the Bayesian framework?

## Architecture Onboarding

- Component map: Data generation → Neural network training → MCMC sampling → Posterior analysis
- Critical path: PDE solver (ground truth) → Neural network surrogate training → Likelihood evaluation → MCMC sampling → Posterior inference
- Design tradeoffs: Computational efficiency vs. accuracy (neural network vs. direct PDE solve), number of receivers vs. identifiability, global optimization vs. local sampling
- Failure signatures: Overly confident posteriors (edge of credible intervals), slow convergence of MCMC, poor reconstruction accuracy despite good MSE
- First 3 experiments:
  1. Train neural network with different architectures (varying layers/neurons) and compare test MSE to determine optimal architecture
  2. Test MCMC convergence with different proposal distributions and numbers of iterations
  3. Vary the number of receivers systematically and measure impact on MSE and posterior uncertainty

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the uncertainty in the surrogate model propagate to the posterior distribution in Bayesian inverse problems?
- Basis in paper: [explicit] The paper mentions that using the neural network surrogate introduces further uncertainty in the model, which is modeled as an additional independent Gaussian noise with variance σ²₁, and notes that the posterior may be overly-confident due to this added uncertainty.
- Why unresolved: The paper does not provide a principled method to account for this uncertainty beyond increasing the estimate of σ²₁.
- What evidence would resolve it: Developing and testing methods to quantify and incorporate surrogate model uncertainty into the Bayesian framework, such as Bayesian neural networks or ensemble methods.

### Open Question 2
- Question: What is the optimal number of receivers for balancing computational efficiency and accuracy in source location inference?
- Basis in paper: [explicit] The paper finds that using 4-5 receivers provides optimal accuracy with diminishing returns beyond that, but does not explore the trade-off with computational cost.
- Why unresolved: The paper does not perform a systematic study of how the number of receivers affects both accuracy and computational efficiency.
- What evidence would resolve it: Conducting experiments to quantify the trade-off between the number of receivers and the accuracy of source location inference, considering both the computational cost and the information gain.

### Open Question 3
- Question: How does the performance of neural network surrogates compare to other surrogate modeling techniques in Bayesian inverse problems?
- Basis in paper: [inferred] The paper focuses on neural network surrogates but does not compare their performance to other methods like polynomial chaos expansion or Gaussian processes.
- Why unresolved: The paper does not provide a comparative analysis of different surrogate modeling techniques.
- What evidence would resolve it: Implementing and comparing various surrogate modeling techniques on the same inverse problem to evaluate their accuracy, efficiency, and robustness.

## Limitations
- The Gaussian noise assumption for both observation and surrogate error may not hold in all real-world scenarios
- The computational efficiency gains come at the cost of introducing surrogate model error, which could compound over MCMC iterations
- The specific receiver configuration may not generalize to different domain geometries or wave equation parameters

## Confidence
- High confidence: The neural network surrogate reduces computational cost of likelihood evaluation sufficiently for MCMC to be feasible
- Medium confidence: The Gaussian noise assumption adequately captures uncertainty from both sources
- Medium confidence: 4-5 receivers provides optimal balance between accuracy and computational efficiency

## Next Checks
1. Test the method on a domain with different geometry or boundary conditions to verify receiver configuration robustness
2. Analyze posterior predictive checks to validate that the inferred distributions capture true source locations accurately
3. Compare MCMC results with and without surrogate model to quantify the impact of approximation error on inference quality