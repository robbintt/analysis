---
ver: rpa2
title: 'The Trifecta: Three simple techniques for training deeper Forward-Forward
  networks'
arxiv_id: '2311.18130'
source_url: https://arxiv.org/abs/2311.18130
tags:
- layer
- learning
- accuracy
- network
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors propose a combination of three techniques, termed
  The Trifecta, to improve the Forward-Forward algorithm on deeper networks. The proposed
  techniques include: (1) using the SymBa loss function to address the loss function
  issue in FF, (2) using batch normalization to address the normalization issue, and
  (3) using overlapping local updates to address the lack of error signals issue.'
---

# The Trifecta: Three simple techniques for training deeper Forward-Forward networks

## Quick Facts
- arXiv ID: 2311.18130
- Source URL: https://arxiv.org/abs/2311.18130
- Reference count: 27
- Key outcome: Achieves ~84% accuracy on CIFAR-10 with deeper FF networks using three combined techniques

## Executive Summary
The Forward-Forward (FF) algorithm has shown promise as a local learning alternative to backpropagation, but struggles with deeper networks due to loss function issues, normalization problems, and lack of error signals. This paper introduces The Trifecta - three complementary techniques that dramatically improve FF performance on deeper architectures. By combining the SymBa loss function, batch normalization, and overlapping local updates, the authors achieve competitive accuracy on CIFAR-10 while maintaining the benefits of local learning. These techniques address fundamental challenges in FF training and represent a significant step toward making FF a viable alternative to backpropagation for deep networks.

## Method Summary
The Trifecta consists of three techniques designed to overcome specific limitations of the Forward-Forward algorithm in deeper networks. First, the SymBa loss function replaces the original FF loss to eliminate threshold asymmetry and directly optimize the separation gap between positive and negative samples. Second, batch normalization is applied to normalize features within each batch without discarding prediction information, enabling stable gradient flow across layers. Third, overlapping local updates (OLU) introduce error signals by having each layer optimize two alternating objectives - maximizing local goodness and helping subsequent layers maximize goodness. These techniques work synergistically to improve stability, convergence, and final accuracy of FF networks.

## Key Results
- Achieves approximately 84% test accuracy on CIFAR-10 with deeper FF networks
- Demonstrates significant improvement over vanilla FF algorithm on deeper architectures
- Shows comparable performance to backpropagation while maintaining local learning benefits
- Validates effectiveness across multiple datasets including MNIST, Fashion-MNIST, SVHN, and CIFAR-100

## Why This Works (Mechanism)

### Mechanism 1: Symmetric Loss Function (SymBa)
- Claim: Eliminates asymmetry in original FF loss function allowing more stable optimization
- Mechanism: Directly optimizes separation gap between positive and negative samples (log(1 + exp(gpos - gneg))) instead of distance to threshold
- Core assumption: Positive and negative samples are highly correlated (true for supervised approach)
- Evidence anchors: SymBa directly utilizes separation instead of distance to threshold; proposed without threshold relying on separation gap
- Break condition: If positive/negative samples aren't highly correlated, SymBa may be less effective

### Mechanism 2: Batch Normalization
- Claim: Strikes balance between doing nothing and discarding all prediction information
- Mechanism: Normalizes features within batch without removing previous prediction information
- Core assumption: Normalization depends on batch statistics not available to single sample, preventing exact reconstruction
- Evidence anchors: Enables learning representations informative locally and when propagated to deeper layers; normalizes features without removing prediction information
- Break condition: If batch size too small or data not well-behaved, batch normalization may fail

### Mechanism 3: Overlapping Local Updates (OLU)
- Claim: Introduces error signals by optimizing each layer with two alternating objectives
- Mechanism: Last-in-group layers update to maximize local goodness; first-in-group layers update to help subsequent layers
- Core assumption: Layer can be updated to better satisfy subsequent layer's objective by optimizing local objective
- Evidence anchors: Technique optimizes each layer with two alternating objectives; improves local understanding and usefulness of representations
- Break condition: If group size too large or objectives not well-aligned, OLU may be ineffective

## Foundational Learning

- Concept: Forward-Forward (FF) algorithm
  - Why needed here: FF is the base algorithm that The Trifecta aims to improve
  - Quick check question: What is the main objective of FF training process and how does it differ from backpropagation?

- Concept: Local learning
  - Why needed here: FF is a type of local learning algorithm; understanding local learning is crucial for context
  - Quick check question: How does local learning differ from backpropagation and what are its main advantages?

- Concept: Loss function design
  - Why needed here: Trifecta includes SymBa loss function modification; understanding loss design importance is crucial
  - Quick check question: What are key considerations when designing loss function for neural network and how can improper loss impact training?

## Architecture Onboarding

- Component map: Input layer (label-encoded channel) -> Convolutional layers (with normalization, convolution, non-linearity, optional max pooling) -> Output layer (uses goodness of last layer)

- Critical path: 1) Encode label and concatenate with image 2) Pass through convolutional layers 3) Calculate goodness at each layer 4) Use goodness of last layer for classification

- Design tradeoffs:
  - Width vs. depth: Deeper networks may achieve higher accuracy but require more training time
  - Normalization: Batch normalization improves stability but may introduce some information loss
  - Loss function: SymBa improves convergence but may be less effective if positive/negative samples not highly correlated

- Failure signatures:
  - Low accuracy: May indicate issues with loss function, normalization, or lack of error signals
  - Instability: May indicate issues with loss function or normalization
  - Slow convergence: May indicate issues with architecture or hyperparameters

- First 3 experiments:
  1. Train shallow CNN with original FF algorithm and SymBa loss function on CIFAR-10
  2. Add batch normalization to model from experiment 1 and compare results
  3. Add OLU to model from experiment 2 and compare results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do residual connections impact Forward-Forward performance in deeper networks?
- Basis in paper: [explicit] Mentioned as potential avenue for improvement in discussion section
- Why unresolved: Preliminary experiments showed mixed results with improved early training but lower final accuracy
- What evidence would resolve it: Systematic experiments with modified residual connections specifically tailored for Forward-Forward's requirements

### Open Question 2
- Question: What is the optimal learning rate schedule for Forward-Forward in deeper networks?
- Basis in paper: [explicit] Authors note learning rate schedules are closely related to layer separation and stability, but only scratched surface
- Why unresolved: Only basic learning rate schedules tested, more exotic schedules involving freezing layers not explored
- What evidence would resolve it: Extensive experiments testing various learning rate schedules and their impact on separation, stability, and accuracy in deeper Forward-Forward networks

### Open Question 3
- Question: How does Forward-Forward compare to other local learning algorithms like feedback alignment or PEPITA on complex tasks?
- Basis in paper: [explicit] Authors compare results to feedback alignment and PEPITA on several datasets but only for shallow networks and limited training
- Why unresolved: Limited comparison to other local learning algorithms, especially on deeper networks and more complex tasks
- What evidence would resolve it: Comprehensive comparison of Forward-Forward, feedback alignment, and PEPITA on deeper networks and more complex tasks like ImageNet using similar evaluation metrics

## Limitations

- Limited validation beyond CIFAR-10 with only brief testing on MNIST, Fashion-MNIST, SVHN, and CIFAR-100
- OLU effectiveness may be architecture-dependent with unexplored performance on non-convolutional architectures
- Computational overhead from batch normalization and OLU not thoroughly analyzed

## Confidence

- SymBa loss function effectiveness: High - Strong theoretical justification and empirical evidence
- Batch normalization benefits: Medium - Well-established technique but limited specific analysis in FF context
- OLU effectiveness: Medium - Promising results but requires more extensive validation across architectures
- Overall performance improvement: Medium - Significant gains shown but limited dataset scope

## Next Checks

1. Cross-architecture validation: Test The Trifecta on Transformer-based architectures and recurrent networks to verify generalizability beyond CNNs

2. Scaling analysis: Evaluate performance on larger-scale datasets (ImageNet) to assess scalability of techniques

3. Ablation study refinement: Systematically test each technique individually on deeper networks (10+ layers) to isolate specific contributions to performance gains