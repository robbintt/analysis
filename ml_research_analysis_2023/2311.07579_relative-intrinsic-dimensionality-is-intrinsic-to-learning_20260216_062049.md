---
ver: rpa2
title: Relative intrinsic dimensionality is intrinsic to learning
arxiv_id: '2311.07579'
source_url: https://arxiv.org/abs/2311.07579
tags:
- data
- dimension
- intrinsic
- learning
- sampled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors introduce the notion of intrinsic dimension to capture\
  \ the separability properties of data distributions. They define the intrinsic dimension\
  \ of a distribution D as the value n(D) for which P(x,y ~ D : (x-y,y-c) \u2265 0)\
  \ = 1/2^(n(D)+1), and the relative intrinsic dimension n(D,D') between two distributions\
  \ analogously."
---

# Relative intrinsic dimensionality is intrinsic to learning

## Quick Facts
- arXiv ID: 2311.07579
- Source URL: https://arxiv.org/abs/2311.07579
- Reference count: 15
- The authors introduce intrinsic dimension to capture separability properties of data distributions and show it provides bounds on learning success.

## Executive Summary
This paper introduces the concept of intrinsic dimension to measure the separability properties of data distributions. The authors define intrinsic dimension as the value n(D) for which the probability of Fisher separability between two randomly sampled points equals 1/2^(n(D)+1). They extend this to relative intrinsic dimension between two distributions and prove that it provides both upper and lower bounds on the probability of successfully learning and generalizing in binary classification problems. The work establishes a fundamental link between the geometric properties of data distributions and learning performance.

## Method Summary
The authors define intrinsic dimension through the probability of Fisher separability between points sampled from a distribution. For a distribution D, they seek n(D) such that P(x,y ~ D : (x-y, y-c) ≥ 0) = 1/2^(n(D)+1). They prove theoretical results for uniform distributions on unit balls, showing n(U(Bd)) = d, and extend this to relative intrinsic dimension between two distributions. The framework is validated through experiments showing how polynomial feature mappings affect intrinsic dimension, revealing an optimal degree for separability.

## Key Results
- The intrinsic dimension n(D) of a uniform distribution on a d-dimensional unit ball equals d.
- Relative intrinsic dimension n(D,D') provides both upper and lower bounds on the probability of successful binary classification between distributions D and D'.
- Polynomial feature mappings can increase or decrease intrinsic dimension depending on degree, with an optimal point for separability.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The intrinsic dimension captures separability by measuring the margin condition (x-y, y-c) ≥ 0, linking geometry to classification success.
- Mechanism: The probability that two randomly sampled points satisfy this margin condition is 1/2^(n(D)+1), so a higher n(D) means points are more likely to be separable, enabling easier classification.
- Core assumption: The distribution is symmetric enough that the pairwise margin probability fully reflects global separability.
- Evidence anchors:
  - [abstract] "we introduce a new notion of the intrinsic dimension of a data distribution, which precisely captures the separability properties of the data"
  - [section] Definition 1 and Theorem 3 showing uniform ball distributions satisfy n(U(Bd)) = d
- Break condition: If the data distribution has strong clustering or heavy tails, the margin-based measure may not reflect true separability.

### Mechanism 2
- Claim: Relative intrinsic dimension n(D,D') bounds learning success by comparing separability between two classes.
- Mechanism: By relating pairwise separation probabilities between distributions D and D' to the classifier accuracy bounds in Theorem 5, the relative intrinsic dimension directly controls the probability of successful learning and generalization.
- Core assumption: The linear classifier in the form Fθ(z) = ℓY if L(z) ≥ θ is sufficient to capture separability between the two classes.
- Evidence anchors:
  - [abstract] "we extend this notion to that of the relative intrinsic dimension of two data distributions, which we show provides both upper and lower bounds on the probability of successfully learning and generalising in a binary classification problem"
  - [section] Theorem 5 bounding P(Fθ(y) = ℓY) and P(Fθ(x) = ℓX) using pθ(Y) and pθ(Y,X)
- Break condition: If the optimal decision boundary is highly nonlinear, linear separability bounds may not be tight.

### Mechanism 3
- Claim: Polynomial feature mappings can increase or decrease intrinsic dimension depending on degree, revealing an optimal separability point.
- Mechanism: The intrinsic dimension of data under a polynomial kernel is not monotonic in degree; beyond a certain point, higher degree feature maps may reduce separability by spreading points too widely.
- Core assumption: The polynomial kernel expansion preserves the geometric structure needed for separability assessment.
- Evidence anchors:
  - [section] Theorem 7 and Figure 5 showing intrinsic dimension behavior under polynomial mappings
  - [abstract] "Experiments demonstrate how intrinsic dimension behaves under polynomial feature mappings, revealing an optimal degree for separability"
- Break condition: If the data lies on a low-dimensional manifold that is not aligned with polynomial features, the feature expansion may not improve separability.

## Foundational Learning

- Concept: Probability of pairwise separation as a measure of dimension
  - Why needed here: It is the core definition that replaces traditional dimensionality with a learning-relevant quantity.
  - Quick check question: If n(D) = 3, what is P(x,y ~ D : (x-y,y-c) ≥ 0)? Answer: 1/2^(3+1) = 1/16.

- Concept: Fisher separability and its relation to intrinsic dimension
  - Why needed here: The margin condition (x-y, y-c) ≥ 0 is equivalent to Fisher separability for c=0, grounding the definition in established learning theory.
  - Quick check question: What does Fisher separability mean geometrically? Answer: One point can be separated from another by a hyperplane through the origin.

- Concept: Law of total probability in computing separation probabilities
  - Why needed here: Theorem 3 and 7 use conditioning on ∥y∥ to integrate over spherical caps, requiring total probability to derive exact formulas.
  - Quick check question: Why is ∥y∥ the only relevant variable for uniform sampling from a ball? Answer: Due to rotational symmetry, the distribution depends only on radius.

## Architecture Onboarding

- Component map: Intrinsic dimension estimator module -> Relative intrinsic dimension calculator -> Learning bounds evaluator -> Feature mapping optimizer
- Critical path: Compute pairwise separation probabilities -> Estimate n(D) -> Use Theorem 5 to bound classifier accuracy -> Adjust model or features accordingly
- Design tradeoffs: High intrinsic dimension guarantees separability but may be hard to estimate from finite samples; low intrinsic dimension suggests poor linear separability but may benefit from nonlinear features
- Failure signatures: Overestimated n(D) due to sampling noise; incorrect relative dimension if distributions have different supports; bounds too loose for nonlinear classifiers
- First 3 experiments:
  1. Generate synthetic uniform balls in varying dimensions, compute empirical n(D) and compare to theoretical d.
  2. Apply polynomial kernels of degrees 1,2,3,… to 2D uniform data, plot intrinsic dimension vs degree to find optimal point.
  3. Train a linear classifier on data with known n(D), compute empirical accuracy and compare to Theorem 5 bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the intrinsic dimension of a data distribution be accurately estimated from sampled data alone?
- Basis in paper: [explicit] The authors note that an interesting question arising from this work is how well the (relative) intrinsic dimension can be estimated from data samples directly, and they briefly investigate this in Section 4.
- Why unresolved: The paper only briefly investigates this question in the context of polynomial feature mappings, but does not provide a comprehensive method for estimating intrinsic dimension from arbitrary data distributions.
- What evidence would resolve it: Development and validation of a robust algorithm that can estimate the intrinsic dimension of various data distributions from finite samples, with theoretical guarantees on accuracy.

### Open Question 2
- Question: How can insights about intrinsic dimension be utilized to improve neural network learning?
- Basis in paper: [explicit] The authors mention that if the intrinsic dimension can be estimated from data samples, it could provide a new tool for selecting appropriate feature mappings for data and shine a new light on the training of neural networks.
- Why unresolved: While the paper suggests potential applications, it does not provide concrete methods for incorporating intrinsic dimension insights into neural network architectures or training procedures.
- What evidence would resolve it: Demonstration of improved neural network performance (e.g., faster convergence, better generalization) by incorporating intrinsic dimension-based techniques in network design or training.

### Open Question 3
- Question: Can the concept of intrinsic dimension be generalized beyond simple linear functionals of the data distribution?
- Basis in paper: [explicit] The authors suggest that the idea can be generalized beyond examining individual points sampled from distributions to studying the collective behavior of groups, or 'granules' of sampled data.
- Why unresolved: The paper only explores intrinsic dimension in the context of linear separability, but does not investigate how the concept might be extended to more complex models or notions of similarity between data points.
- What evidence would resolve it: Development of a generalized framework for intrinsic dimension that incorporates more sophisticated models of data relationships and demonstrates its utility in various machine learning tasks.

## Limitations
- The framework relies on the assumption that pairwise margin probability reflects global separability, which may not hold for distributions with clustering or heavy tails.
- The connection between intrinsic dimension and nonlinear classification performance remains unexplored, limiting practical applicability.
- The paper doesn't address the statistical difficulty of estimating intrinsic dimension from finite samples, which could be computationally prohibitive.

## Confidence

**High Confidence**: The theoretical derivations for uniform ball distributions (Theorem 3) are mathematically rigorous and the volume/surface area calculations are verifiable.

**Medium Confidence**: The polynomial feature mapping experiments showing non-monotonic behavior of intrinsic dimension are promising, but lack sufficient implementation detail.

**Low Confidence**: The practical utility of intrinsic dimension bounds for real-world learning tasks is not demonstrated, with no experiments on actual datasets or comparison to existing dimensionality measures.

## Next Validation Checks
1. Generate synthetic distributions with known clustering structure and heavy-tailed properties, then measure whether the intrinsic dimension captures the expected separability patterns. Compare results with traditional dimensionality measures.

2. Extend Theorem 3 to non-uniform distributions on balls (e.g., Gaussian distributions on the unit ball) and verify whether the intrinsic dimension formula still holds or requires modification.

3. Conduct empirical studies on the number of samples needed to accurately estimate intrinsic dimension for various dimensions and distributions, establishing practical guidelines for when the measure is reliable versus when it breaks down due to sampling noise.