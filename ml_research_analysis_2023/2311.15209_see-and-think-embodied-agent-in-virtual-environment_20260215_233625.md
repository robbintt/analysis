---
ver: rpa2
title: 'See and Think: Embodied Agent in Virtual Environment'
arxiv_id: '2311.15209'
source_url: https://arxiv.org/abs/2311.15209
tags:
- arxiv
- language
- crafting
- table
- steve
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces STEVE, an embodied agent designed to operate
  in the Minecraft virtual environment. STEVE integrates visual perception with large
  language models (LLMs) and a skill database to execute actions.
---

# See and Think: Embodied Agent in Virtual Environment

## Quick Facts
- arXiv ID: 2311.15209
- Source URL: https://arxiv.org/abs/2311.15209
- Reference count: 40
- One-line primary result: STEVE achieves up to 1.5× faster unlocking of key tech trees and 2.5× quicker block search tasks in Minecraft

## Executive Summary
STEVE is an embodied agent designed for the Minecraft virtual environment that integrates visual perception, large language models, and a skill database to execute complex tasks. The system uses a vision encoder to interpret environmental features, an LLM to reason and decompose tasks, and a retrieval-based skill database to generate executable actions. The paper introduces the STEVE-21K dataset and demonstrates significant performance improvements over state-of-the-art methods in various Minecraft tasks.

## Method Summary
STEVE combines three components: a vision perception module using EfficientFormerV2-S0 to extract visual features, a language instruction component with fine-tuned LLaMA2-13B for reasoning and task decomposition, and a code action module that retrieves executable skills from a database. The system processes visual tokens alongside agent state and task instructions through the LLM, which generates sequential guidelines. These guidelines are encoded into query vectors and matched to stored code snippets for execution via the Mineflayer API. The STEVE-21K dataset provides vision-environment pairs, knowledge QA pairs, and skill-code pairs for training and evaluation.

## Key Results
- Achieves up to 1.5× faster unlocking of key tech trees compared to previous methods
- Completes block search tasks 2.5× quicker than state-of-the-art approaches
- Scores 8.12/10 on average for knowledge question answering tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vision perception allows STEVE to directly interpret environmental features (blocks, entities) rather than relying solely on textual representations, reducing ambiguity in complex tasks.
- Mechanism: The vision encoder converts images or video frames into tokens that capture block and entity information. These tokens are then integrated with the agent's state and task instruction, giving the LLM richer context for reasoning and planning.
- Core assumption: Visual tokens can be reliably generated from game frames and meaningfully combined with text tokens to improve decision-making accuracy.
- Evidence anchors:
  - [abstract] states that STEVE "harnesses a vision model to visually perceive its surroundings, coupled with an LLM to strategize and plan actions," and achieves up to 1.5× faster unlocking of key tech trees.
  - [section 3.2] describes using EfficientFormer to extract visual features and transform them into tokens that encapsulate critical visual information.
  - [corpus] neighbor "STEVE-Audio: Expanding the Goal Conditioning Modalities of Embodied Agents in Minecraft" supports multimodal perception as a way to extend agent capabilities.
- Break condition: If visual features are ambiguous or misclassified (e.g., due to poor lighting or occlusion), the agent may misinterpret the environment and execute incorrect actions.

### Mechanism 2
- Claim: Iterative reasoning and task decomposition enable STEVE to convert high-level goals into executable low-level steps, making complex tasks manageable for the agent.
- Mechanism: STEVE-13B interprets the unified token set (vision, state, task) to construct high-level strategies, then decomposes them into sequential low-level guidelines. Each step is then encoded into a query for skill retrieval and execution.
- Core assumption: Large language models can accurately interpret complex visual and contextual information to produce correct, executable decompositions.
- Evidence anchors:
  - [abstract] highlights that "language instruction is responsible for iterative reasoning and decomposing complex tasks into manageable guidelines."
  - [section 3.3] explains that STEVE-13B iteratively reasons about tasks and devises a structured plan, breaking down complex goals into manageable instructions step-by-step.
  - [corpus] neighbor "OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following" examines the role of LLMs in planning for embodied agents, suggesting this approach is under active research.
- Break condition: If the decomposition logic is flawed or the LLM's reasoning is inconsistent, the agent may take inefficient or incorrect actions, even if the skill database is correct.

### Mechanism 3
- Claim: Skill database retrieval enables the agent to convert abstract instructions into executable code actions by matching queries to stored code snippets, allowing autonomous interaction with the Minecraft environment.
- Mechanism: Each low-level step is encoded into a query vector. The system computes similarity between the query and vectors of stored code snippets, retrieves the best match, and executes the action via the Mineflayer API.
- Core assumption: The skill database contains sufficiently diverse and accurate code snippets, and vector similarity accurately reflects functional equivalence.
- Evidence anchors:
  - [abstract] notes that "code action generates executable skill actions based on retrieval in skill database, enabling the agent to interact effectively within the Minecraft environment."
  - [section 3.4] details the retrieval process: query encoding, similarity matching, and execution, explaining how STEVE transitions from planning to action.
  - [corpus] neighbor "STEVE: A Step Verification Pipeline for Computer-use Agent Training" suggests verification pipelines can ensure code correctness in similar retrieval-based systems.
- Break condition: If the skill database lacks a matching snippet or the similarity measure fails, the agent may fail to execute the intended action or resort to inefficient workarounds.

## Foundational Learning

- Concept: Vision-language integration for multimodal understanding
  - Why needed here: STEVE relies on fusing visual perception with language understanding to accurately interpret the Minecraft environment and plan actions.
  - Quick check question: Can you explain how visual tokens are combined with text tokens in STEVE's pipeline?

- Concept: Iterative reasoning and hierarchical task decomposition
  - Why needed here: STEVE uses the LLM to break down high-level goals into actionable steps, which is essential for handling complex, multi-step tasks in Minecraft.
  - Quick check question: How does STEVE-13B ensure that decomposed steps remain contextually relevant to the current game state?

- Concept: Vector-based retrieval and similarity matching for skill execution
  - Why needed here: STEVE matches low-level instructions to executable code snippets via vector similarity, enabling autonomous action in the environment.
  - Quick check question: What happens if no skill snippet matches the query vector above a certain similarity threshold?

## Architecture Onboarding

- Component map:
  Vision Encoder -> Tokenizer -> STEVE-13B -> Query Encoder -> Retrieval Module -> Mineflayer API

- Critical path: Vision → Tokenizer → STEVE-13B → Query Encoder → Retrieval → Execution
  The flow from visual perception through reasoning to action is the core sequence that must remain stable.

- Design tradeoffs:
  - Multimodal integration vs. latency: Visual processing adds complexity but improves accuracy.
  - Fine-tuned LLM vs. general LLM: STEVE-13B is specialized for Minecraft, trading general knowledge for task-specific precision.
  - Vector similarity vs. exact matching: Allows flexibility in skill retrieval but risks imperfect matches.

- Failure signatures:
  - Vision failures: Misidentified blocks or entities lead to incorrect planning.
  - Reasoning failures: LLM generates irrelevant or infeasible steps.
  - Retrieval failures: No suitable code snippet found, or wrong snippet chosen due to poor similarity scoring.

- First 3 experiments:
  1. Run STEVE on a simple block search task with clear visual input and verify that the vision encoder correctly identifies target blocks.
  2. Test STEVE-13B's decomposition by providing a known high-level goal and checking that generated steps are correct and executable.
  3. Execute a known skill query and confirm that the retrieval module returns the correct code snippet and that it runs without errors in Minecraft.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the STEVE-13B model's performance on Minecraft tasks compare to other fine-tuned language models like GPT-4 or LLaMA-2 when both are given the same amount of Minecraft-specific training data?
- Basis in paper: [explicit] The paper states that STEVE-13B outperforms LLaMA2 models but doesn't provide a direct comparison with GPT-4 or other models with similar training data.
- Why unresolved: The paper only compares STEVE-13B to LLaMA2 and GPT-4 without controlling for the amount of Minecraft-specific training data.
- What evidence would resolve it: A controlled experiment where STEVE-13B, GPT-4, and LLaMA2 are all fine-tuned on the same amount of Minecraft-specific data, then compared on identical tasks.

### Open Question 2
- Question: What is the long-term performance of STEVE in continuous Minecraft gameplay, particularly in tasks that require sustained planning and adaptation over extended periods?
- Basis in paper: [inferred] The paper mentions limitations with lifelong learning tasks and tasks that exceed the database's scope, suggesting potential issues with long-term performance.
- Why unresolved: The experiments conducted are limited in duration and don't test the agent's ability to maintain performance over extended gameplay sessions.
- What evidence would resolve it: A longitudinal study tracking STEVE's performance over many hours or days of continuous Minecraft gameplay, measuring its ability to complete increasingly complex tasks and adapt to new situations.

### Open Question 3
- Question: How does the vision component of STEVE contribute to its performance in tasks where visual information is less critical, such as text-based problem-solving or crafting tasks?
- Basis in paper: [explicit] The paper discusses the vision unit's importance in exploration and environmental understanding but doesn't explore its impact on tasks where visual information is less relevant.
- Why unresolved: The evaluation focuses on tasks where visual perception is clearly beneficial, leaving the contribution of the vision component in other types of tasks unexplored.
- What evidence would resolve it: Comparative experiments testing STEVE's performance on text-based and crafting tasks with and without the vision component enabled, measuring any differences in accuracy or efficiency.

## Limitations
- Vision encoder training process and environmental information extraction are not fully detailed
- Skill database construction and vector retrieval mechanism lack specific configuration parameters
- STEVE-21K dataset may have limited diversity for vision-environment pairs (600+ samples)

## Confidence

- **High confidence**: The core architectural framework combining vision, LLM reasoning, and skill retrieval is well-specified and logically sound. The reported performance improvements (1.5× faster tech tree unlocking, 2.5× quicker block search) are specific and measurable.
- **Medium confidence**: The effectiveness of iterative reasoning and task decomposition relies heavily on the LLM's capabilities, which are not fully characterized beyond the fine-tuning approach. The assumption that visual tokens meaningfully improve decision-making accuracy is supported by mechanism descriptions but lacks direct ablation comparisons.
- **Low confidence**: The generalizability of the system beyond the STEVE-21K dataset and specific Minecraft tasks remains uncertain. The paper does not address potential failure modes in edge cases or adversarial scenarios.

## Next Checks

1. **Vision Encoder Validation**: Test STEVE's block identification accuracy across varying lighting conditions, occlusion scenarios, and block types not present in the training data to assess generalization and robustness of visual perception.
2. **Reasoning Decomposition Audit**: Provide STEVE with complex, multi-step goals that have multiple valid solutions and evaluate whether the LLM consistently generates optimal or near-optimal action sequences, particularly when goals conflict with environmental constraints.
3. **Skill Retrieval Stress Test**: Systematically evaluate the skill database retrieval mechanism by querying for tasks with intentionally vague or ambiguous instructions to measure the system's ability to select appropriate code snippets and handle cases where no perfect match exists.