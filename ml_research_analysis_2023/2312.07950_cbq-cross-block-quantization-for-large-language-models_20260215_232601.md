---
ver: rpa2
title: 'CBQ: Cross-Block Quantization for Large Language Models'
arxiv_id: '2312.07950'
source_url: https://arxiv.org/abs/2312.07950
tags:
- quantization
- language
- arxiv
- weight
- block
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CBQ, a post-training quantization method designed
  specifically for large language models. The core idea is to optimize quantization
  parameters through block-wise reconstruction, while introducing a cross-block dependency
  scheme to reduce error accumulation across adjacent blocks.
---

# CBQ: Cross-Block Quantization for Large Language Models

## Quick Facts
- arXiv ID: 2312.07950
- Source URL: https://arxiv.org/abs/2312.07950
- Reference count: 14
- Key outcome: CBQ achieves superior low-bit quantization (W4A4, W4A8, W2A16) on LLMs, quantizing 4-bit LLAMA1-65B in 4.3 hours on a single GPU

## Executive Summary
CBQ introduces a novel post-training quantization method specifically designed for large language models that addresses the challenge of error accumulation across transformer blocks. The approach combines three key innovations: cross-block dependency modeling to optimize adjacent blocks jointly, coarse-to-fine preprocessing to suppress weight and activation outliers, and LoRA-Rounding with weight-compensated matrices to correct quantization errors. Extensive experiments demonstrate CBQ's superiority over existing methods on various LLM architectures and bitwidth configurations.

## Method Summary
CBQ implements block-wise reconstruction with cross-block dependency using overlapping sliding windows to jointly optimize multiple adjacent transformer blocks. The method incorporates coarse-to-fine preprocessing to detect and suppress outliers through weight truncation and dynamic activation scaling, followed by low-rank weight-compensated matrix implementation via LoRA-Rounding to rectify quantization errors. The optimization process uses combined L2 and KLD reconstruction loss functions to maintain model accuracy during quantization to ultra-low bitwidths (W4A4, W4A8, W2A16).

## Key Results
- Achieves state-of-the-art performance on ultra-low bitwidth quantization (W4A4, W4A8, W2A16) for various LLMs
- Quantizes 4-bit LLAMA1-65B model within only 4.3 hours on a single GPU
- Outperforms existing methods on zero-shot tasks (PIQA, HellaSwag, ARC, etc.) and generation tasks (C4, WikiText2, LAMBADA)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-block dependency modeling reduces accumulated quantization error by optimizing multiple adjacent transformer blocks together with overlapping windows.
- Mechanism: CBQ processes groups of blocks within a sliding window where adjacent windows overlap, allowing joint optimization across blocks to prevent error accumulation that occurs in traditional block-wise methods.
- Core assumption: Adjacent transformer block outputs are sufficiently correlated that joint optimization can capture dependencies and propagate corrections backward.
- Evidence anchors:
  - [abstract] "To reduce error accumulation, we introduce a cross-block dependency with the aid of a homologous reconstruction scheme to build the long-range dependency between adjacent multi-blocks with overlapping."
  - [section 3.2] "To tackle this issue, we introduce a cross-block dependency (CBD) scheme using a sliding window approach. This scheme enables the simultaneous optimization of multiple blocks within the window"
- Break condition: If block outputs are independent or dependencies are too weak, joint optimization provides minimal benefit and may introduce unnecessary complexity.

### Mechanism 2
- Claim: Coarse-to-fine preprocessing effectively suppresses weight and activation outliers to improve quantization accuracy.
- Mechanism: CBQ implements two-stage outlier detection using interquartile range methods, first coarsely identifying potential outliers then refining through fine-grained detection that minimizes intra-set variance while maximizing inter-set separation.
- Core assumption: Outlier suppression before quantization significantly reduces reconstruction difficulty without losing critical information.
- Evidence anchors:
  - [abstract] "Furthermore, CBQ incorporates a coarse-to-fine preprocessing (CFP) strategy for suppressing weight and activation outliers"
  - [section 3.3] "Coarse-to-fine Pre-processing...we propose an coarse-to-fine pre-processing strategy to suppress outliers and reduce reconstruction difficulty"
- Break condition: Poor outlier detection thresholds could remove important signal information or fail to suppress problematic outliers, degrading performance.

### Mechanism 3
- Claim: LoRA-Rounding with low-rank weight-compensated matrices rectifies quantization errors while maintaining computational efficiency.
- Mechanism: CBQ uses low-rank decomposition of the weight-compensated matrix (A = A1 × A2) where A ∈ {0,1} at convergence, learning continuous variables V1 and V2 to correct rounding errors during quantization.
- Core assumption: The weight-compensated matrix can effectively learn to correct quantization errors while remaining computationally tractable through low-rank decomposition.
- Evidence anchors:
  - [abstract] "coupled with an adaptive LoRA-Rounding technique for precise weight quantization"
  - [section 3.4] "To rectify weight quantization errors within each block and prevent their accumulation in adjacent blocks, we introduce a weight-compensated matrix that compensates for errors arising from rounding down tensors during quantization"
- Break condition: If low-rank decomposition is too aggressive, it may not capture sufficient correction capability; if too conservative, it may not provide computational benefits.

## Foundational Learning

- Concept: Quantization and de-quantization processes
  - Why needed here: CBQ operates fundamentally on quantized representations, so understanding how values map from floating-point to integer representations and back is essential for grasping the algorithm's mechanics
  - Quick check question: What is the difference between quantization (Q) and de-quantization (Q⁻¹) in the context of neural network weights?

- Concept: Transformer block architecture
  - Why needed here: CBQ optimizes quantization at the transformer block level, requiring understanding of self-attention, feed-forward networks, and residual connections to comprehend how errors propagate
  - Quick check question: What are the main components of a transformer block that would be affected by quantization?

- Concept: Kullback-Leibler divergence (KLD) loss
  - Why needed here: CBQ uses KLD loss in addition to L2 loss for reconstruction, so understanding probability distributions and divergence metrics is important
  - Quick check question: How does KLD loss differ from L2 loss when measuring the difference between two probability distributions?

## Architecture Onboarding

- Component map: Cross-block dependency module -> Coarse-to-fine preprocessing -> LoRA-Rounding -> Reconstruction loss -> Quantization engine
- Critical path:
  1. Preprocess weights and activations using coarse-to-fine outlier detection
  2. Initialize quantization parameters and LoRA-Rounding matrices
  3. For each sliding window of transformer blocks:
     - Compute FP and quantized model outputs
     - Apply LoRA-Rounding compensation
     - Calculate reconstruction loss (L2 + KLD)
     - Update quantization parameters via backpropagation
  4. Repeat for multiple epochs across all windows

- Design tradeoffs:
  - Sliding window size vs. GPU memory: Larger windows capture more dependencies but require more memory
  - Overlap amount vs. redundancy: More overlap improves continuity but increases computation
  - Low-rank decomposition rank vs. correction capability: Higher rank captures more corrections but increases parameters
  - Outlier suppression aggressiveness vs. information preservation: Stricter suppression reduces outliers but may remove signal

- Failure signatures:
  - Accuracy degradation with aggressive quantization: Likely indicates insufficient outlier suppression or poor LoRA-Rounding initialization
  - Slow convergence: May suggest inadequate sliding window size or overlap
  - Memory overflow: Sliding window too large or too much overlap
  - Numerical instability: Poor outlier detection thresholds or aggressive quantization step sizes

- First 3 experiments:
  1. Implement basic block-wise reconstruction without cross-block dependency on a small model (e.g., OPT-125M) to establish baseline performance
  2. Add cross-block dependency with minimal overlap (window size=2, overlap=1) and compare accuracy improvements
  3. Integrate coarse-to-fine preprocessing with fixed outlier thresholds and measure impact on convergence speed and final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of outlier detection threshold parameters (λ1, λ2) affect the trade-off between outlier suppression and information preservation in CBQ?
- Basis in paper: [explicit] The paper states that λ1=1.5 and λ2=1.0 are used as default values, but does not provide an ablation study on their impact.
- Why unresolved: The optimal values for these parameters may depend on the specific characteristics of the LLM being quantized, and could significantly impact the balance between outlier suppression and maintaining important information in the weights and activations.
- What evidence would resolve it: An ablation study varying λ1 and λ2 across a range of values, and evaluating the impact on quantization accuracy and outlier handling for different LLMs.

### Open Question 2
- Question: How does the choice of rank r in the low-rank decomposition of the weight-compensated matrix impact the trade-off between quantization accuracy and computational efficiency in CBQ?
- Basis in paper: [explicit] The paper states that rank r ≪ min(d, k) is used, but does not provide an ablation study on the impact of different rank values.
- Why unresolved: The optimal rank may depend on the specific LLM architecture and desired balance between accuracy and efficiency, and could significantly impact the effectiveness of the weight compensation.
- What evidence would resolve it: An ablation study varying the rank r across a range of values, and evaluating the impact on quantization accuracy, computational efficiency, and the ability to correct weight quantization errors.

### Open Question 3
- Question: How does the performance of CBQ compare to other state-of-the-art quantization methods when applied to LLMs with different architectural designs (e.g., different attention mechanisms, normalization layers, etc.)?
- Basis in paper: [inferred] The paper demonstrates CBQ's effectiveness on OPT and BLOOM models, but does not provide a comprehensive comparison across a diverse set of LLM architectures.
- Why unresolved: Different LLM architectures may have varying sensitivities to quantization errors and outlier distributions, which could impact the relative performance of CBQ compared to other methods.
- What evidence would resolve it: A systematic comparison of CBQ against other state-of-the-art methods on a diverse set of LLM architectures, evaluating quantization accuracy, computational efficiency, and robustness to different architectural designs.

## Limitations

- The empirical validation of cross-block dependency mechanism is incomplete, lacking ablation studies that isolate its contribution from other components
- Computational overhead and scaling behavior for extremely large models (>65B parameters) remains unclear
- Theoretical foundation of cross-block dependency's error propagation reduction needs more rigorous mathematical analysis

## Confidence

- **High Confidence**: Coarse-to-fine preprocessing methodology and implementation details are well-specified and align with established outlier detection techniques
- **Medium Confidence**: Overall framework combining all three components shows consistent performance improvements across multiple models and datasets
- **Low Confidence**: Cross-block dependency mechanism's theoretical foundation and actual impact on error propagation across transformer blocks need more rigorous validation

## Next Checks

1. Implement and evaluate each CBQ component (cross-block dependency, coarse-to-fine preprocessing, LoRA-Rounding) independently on a small LLM to quantify individual contributions to overall performance
2. Profile memory usage and execution time of the sliding window approach across different window sizes and overlaps to determine optimal configurations for various model scales
3. Conduct a detailed study of quantization error propagation across transformer blocks with and without cross-block dependency to empirically validate theoretical claims about error accumulation reduction