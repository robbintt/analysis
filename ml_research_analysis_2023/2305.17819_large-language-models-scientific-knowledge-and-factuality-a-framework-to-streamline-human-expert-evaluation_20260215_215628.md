---
ver: rpa2
title: 'Large Language Models, scientific knowledge and factuality: A framework to
  streamline human expert evaluation'
arxiv_id: '2305.17819'
source_url: https://arxiv.org/abs/2305.17819
tags:
- factual
- entity
- generated
- knowledge
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a framework for evaluating the ability of\
  \ Large Language Models (LLMs) to encode and extract factual scientific knowledge,\
  \ focusing on antibiotic discovery as a use case. The framework consists of three\
  \ sequential evaluation steps\u2014fluency, prompt alignment, semantic coherence,\
  \ factuality, and specificity\u2014to efficiently assess generated text by splitting\
  \ tasks between non-experts and domain experts."
---

# Large Language Models, scientific knowledge and factuality: A framework to streamline human expert evaluation

## Quick Facts
- arXiv ID: 2305.17819
- Source URL: https://arxiv.org/abs/2305.17819
- Authors:
- Reference count: 40
- Current LLMs are not yet suitable as reliable biomedical knowledge bases in zero-shot settings, but show potential for improvement with domain specialization, scale, and human feedback.

## Executive Summary
This work introduces a framework for evaluating the ability of Large Language Models (LLMs) to encode and extract factual scientific knowledge, focusing on antibiotic discovery as a use case. The framework consists of three sequential evaluation steps—fluency, prompt alignment, semantic coherence, factuality, and specificity—to efficiently assess generated text by splitting tasks between non-experts and domain experts. Nine state-of-the-art LLMs, including GPT-4 and BioGPT, were tested on two tasks: chemical compound definition generation and chemical compound-fungus relation determination. Results show that while fluency and semantic coherence improved in larger models, factual accuracy remained low, with GPT-4 achieving 70% factual compound definitions and 43.6% factual chemical-fungus relations, and BioGPT-large achieving 30% and 30% respectively. Models exhibited significant bias toward over-represented entities like Aspergillus and struggled with rare compounds. The study concludes that current LLMs are not yet suitable as reliable biomedical knowledge bases in zero-shot settings, but suggests potential for improvement with domain specialization, scale, and human feedback.

## Method Summary
The framework evaluates LLMs on two tasks: chemical compound definition generation and chemical compound-fungus relation determination. Nine state-of-the-art LLMs were tested using a curated dataset of 246 fungus-chemical pairs. Outputs were assessed through a three-step pipeline: fluency, prompt alignment, and semantic coherence (by non-experts), followed by factuality and specificity (by domain experts). The evaluation focused on zero-shot prompting without fine-tuning or few-shot learning.

## Key Results
- GPT-4 achieved 70% factual compound definitions and 43.6% factual chemical-fungus relations.
- BioGPT-large achieved 30% and 30% for the same tasks respectively.
- Models exhibited significant bias toward over-represented entities like Aspergillus and struggled with rare compounds.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework improves factuality detection by splitting evaluation tasks between non-experts and domain experts.
- Mechanism: Non-experts handle fluency, prompt alignment, and semantic coherence (requiring linguistic skill), while domain experts focus on factuality and specificity (requiring specialized knowledge). This reduces expert workload and enables efficient assessment.
- Core assumption: Factuality detection requires domain expertise that is scarce, while linguistic quality assessment can be reliably delegated.
- Evidence anchors:
  - [abstract]: "By splitting these tasks between non-experts and experts, the framework reduces the effort required from the latter."
  - [section]: "The factuality of the generated text was assessed in the STEP 2. Importantly, for Relation determination via entity generation, the correctness of the generated {entity 2} is verified prior to the verification of the whole description."
  - [corpus]: Weak. Corpus neighbors focus on factuality evaluation but do not explicitly discuss the split-task approach.
- Break condition: If non-experts misclassify prompt alignment or semantic coherence issues as factuality problems, domain expert time is wasted.

### Mechanism 2
- Claim: Prompt design significantly affects LLM performance on factual knowledge extraction.
- Mechanism: Simple prompts yield inconsistent results; adding context (e.g., specifying "compound" or "substance") improves semantic coherence and sometimes factuality. This is because the model benefits from type guidance.
- Core assumption: LLMs use prompt context to narrow the semantic space and reduce generation of irrelevant or incorrect content.
- Evidence anchors:
  - [abstract]: "Although recent models have improved in fluency, factual accuracy is still low and models are biased towards over-represented entities."
  - [section]: "Interestingly, adding context to the prompt did not increase specificity... However, in 60% of these prompts the chemical formula was incorrect."
  - [corpus]: Weak. No direct corpus evidence about prompt design effects, though related work discusses prompt engineering.
- Break condition: If prompt engineering is done poorly (e.g., ambiguous context), it may confuse the model and worsen factuality.

### Mechanism 3
- Claim: LLM size, domain specialization, and human feedback alignment improve factual knowledge encoding.
- Mechanism: Larger models (e.g., GPT-4) and domain-specialized models (e.g., BioGPT-large) show better performance, especially when combined with human feedback (RLHF). This suggests that scaling and fine-tuning improve knowledge fidelity.
- Core assumption: Larger parameter counts and domain-relevant training corpora allow better encoding and retrieval of factual relations.
- Evidence anchors:
  - [abstract]: "there is a promising emerging property in the direction of factuality as the models become domain specialised, scale-up in size and level of human feedback."
  - [section]: "GPT-4 outperforms ChatGPT... It scores 70%... and 43.6%... for chemical entity recognition and chemical-fungi relation recognition, respectively."
  - [corpus]: Weak. Related papers discuss factuality and human feedback, but not specifically the scaling/specialization claim.
- Break condition: If scaling is done without domain relevance, improvements may plateau or introduce new biases.

## Foundational Learning

- Concept: **Prompt engineering** - designing inputs to guide LLM outputs.
  - Why needed here: The study shows that prompt design strongly influences both fluency and factuality; understanding this helps replicate and extend the evaluation framework.
  - Quick check question: What difference did adding context like "is a compound" make to the generated text quality?

- Concept: **Factual vs. semantic coherence** - distinguishing true statements from grammatically correct but incorrect ones.
  - Why needed here: The evaluation framework separates these, but a new engineer must understand the difference to correctly annotate outputs.
  - Quick check question: If a generated sentence is fluent and semantically coherent but references a non-existent fungus, is it factual?

- Concept: **Zero-shot vs. few-shot evaluation** - testing models without prior examples vs. with priming.
  - Why needed here: The study uses zero-shot prompting; understanding this limitation is critical when interpreting results and designing follow-ups.
  - Quick check question: Why might a zero-shot prompt fail to elicit a correct chemical-fungus relation even if the model "knows" it?

## Architecture Onboarding

- Component map: **Framework** (three-step pipeline: fluency/prompt alignment/semantic coherence → factuality → specificity), **Dataset** (curated chemical-fungus pairs with PubMed references), **Models** (nine LLMs with varying size, corpus, and alignment), **Evaluation** (manual annotation by non-experts and experts).
- Critical path: Generate outputs → Annotate fluency/prompt alignment/semantic coherence → Annotate factuality → Annotate specificity → Aggregate results.
- Design tradeoffs: Manual annotation is accurate but slow; automating factuality checks would speed up evaluation but risk lower precision.
- Failure signatures: High fluency but low factuality indicates hallucination; consistent bias toward certain entities (e.g., Aspergillus) suggests corpus imbalance.
- First 3 experiments:
  1. **Replicate fluency annotation**: Run GPT-4 on a subset of prompts and annotate fluency/prompt alignment/semantic coherence without expert input.
  2. **Test prompt variation**: Compare outputs from "entity is" vs. "entity is a compound" to quantify impact on factuality.
  3. **Bias analysis**: Tally entity mentions across all models to confirm over-representation of Aspergillus.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning or few-shot learning significantly improve LLMs' ability to encode and extract factual biomedical knowledge compared to off-the-shelf models?
- Basis in paper: [inferred] The authors state they did not perform fine-tuning or few-shot learning, and suggest future work in this direction. They also note that models trained on more extensive biomedical corpora (like BioGPT) showed better performance than general models.
- Why unresolved: The paper only evaluated off-the-shelf models without any adaptation to the specific biomedical domain, leaving open the question of whether targeted training could overcome current limitations.
- What evidence would resolve it: A study comparing the same models before and after fine-tuning on a large biomedical corpus, or a comparison of fine-tuned models against the best-performing off-the-shelf models in the current study.

### Open Question 2
- Question: What is the optimal prompt engineering strategy for eliciting factual biomedical knowledge from LLMs, and how does it vary across different model architectures and domains?
- Basis in paper: [explicit] The authors note high sensitivity to prompt design, with optimal prompts varying across models, and suggest systematic prompt engineering as future work. They observed that providing context in prompts generally improved performance.
- Why unresolved: While the study tested 19 prompt designs, it did not perform systematic prompt optimization or investigate why certain prompts worked better for specific models.
- What evidence would resolve it: A comprehensive study using automated prompt optimization techniques (like AutoPrompt) to determine optimal prompts for each model across multiple biomedical relation types, and analysis of why certain prompt structures are more effective.

### Open Question 3
- Question: To what extent do biases in training corpora (such as overrepresentation of certain fungi like Aspergillus) limit the generalizability of LLMs for biomedical knowledge extraction, and can this be mitigated?
- Basis in paper: [explicit] The authors observed significant bias towards Aspergillus across multiple models, attributing this to its overrepresentation in PubMed (60k references vs 6k for Ergosterol). They note this limits the models' ability to recognize rarer but valid relations.
- Why unresolved: The study identified the bias but did not explore its underlying causes or potential mitigation strategies beyond noting corpus imbalance.
- What evidence would resolve it: A corpus analysis quantifying entity representation across different biological domains, followed by experiments with balanced training data or debiasing techniques to measure impact on model performance for underrepresented entities.

## Limitations
- The evaluation framework's effectiveness hinges on subjective annotation decisions, particularly for factuality, which may introduce inter-annotator variability despite domain expert involvement.
- The study focuses on a narrow biomedical domain (antibiotic discovery), limiting generalizability to other scientific or general knowledge domains.
- The dataset size (246 pairs) and selection of only 10 compounds for in-depth testing may not capture the full diversity of LLM performance across all possible entity types and relations.

## Confidence
- **High confidence**: The finding that larger, domain-specialized models (e.g., GPT-4, BioGPT-large) show improved fluency and semantic coherence over smaller, general models is well-supported by direct comparison metrics.
- **Medium confidence**: The claim that factuality remains low across all models, with GPT-4 achieving only 70% factual compound definitions, is supported by the data but limited by the small sample size and narrow domain.
- **Low confidence**: The assertion that splitting evaluation tasks between non-experts and experts significantly reduces expert workload is plausible but not empirically validated within the study.

## Next Checks
1. **Replicate with expanded dataset**: Test the framework on a larger, more diverse set of scientific relations (e.g., gene-disease, drug-target) to assess generalizability and robustness.
2. **Automated factuality verification**: Develop or integrate a knowledge graph-based factuality checker to compare human expert annotation with automated scoring, quantifying inter-annotator agreement and error rates.
3. **Prompt engineering ablation study**: Systematically vary prompt context (e.g., entity type specification, relation templates) and measure impact on fluency, semantic coherence, and factuality to identify optimal prompt structures.