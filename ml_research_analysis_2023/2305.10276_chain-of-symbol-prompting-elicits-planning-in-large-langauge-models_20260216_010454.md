---
ver: rpa2
title: Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models
arxiv_id: '2305.10276'
source_url: https://arxiv.org/abs/2305.10276
tags:
- language
- llms
- brick
- natural
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a benchmark named Natural Language Planning
  (NLP) for evaluating the planning abilities of Large Language Models (LLMs) in complex
  spatial environments described through natural language. The NLP benchmark consists
  of three tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation.'
---

# Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models

## Quick Facts
- arXiv ID: 2305.10276
- Source URL: https://arxiv.org/abs/2305.10276
- Reference count: 15
- Key outcome: CoS prompting significantly outperforms Chain-of-Thought on spatial planning tasks, with up to 60.8% improvement in accuracy while reducing tokens by up to 65.8%

## Executive Summary
This paper addresses the challenge of planning in complex spatial environments using Large Language Models (LLMs). The authors propose the Natural Language Planning (NLP) benchmark with three tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. They introduce Chain-of-Symbol (CoS) prompting, which converts natural language descriptions into condensed symbolic representations to guide LLM reasoning. Experimental results demonstrate that CoS significantly outperforms standard Chain-of-Thought prompting on all three tasks while being more token-efficient. The method shows promise for eliciting planning capabilities in LLMs, a crucial step toward Artificial General Intelligence.

## Method Summary
The authors create the Natural Language Planning (NLP) benchmark with three spatial reasoning tasks. They implement Chain-of-Symbol (CoS) prompting by manually converting Chain-of-Thought demonstrations into symbolic representations that capture spatial relationships more efficiently. The CoS method guides LLMs through planning tasks using these condensed symbolic formats instead of natural language reasoning chains. The approach is evaluated against standard prompting and CoT on ChatGPT and InstructGPT models, measuring accuracy, precision, recall, and token efficiency.

## Key Results
- CoS achieves up to 60.8% improvement in accuracy over CoT on Brick World tasks
- Token reduction of up to 65.8% (from 407 to 139 tokens) compared to CoT
- CoS outperforms baselines across all three NLP benchmark tasks
- Performance gains are consistent across different difficulty levels and task types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Symbolic representations are more compact and efficient for LLMs to process than natural language descriptions.
- Mechanism: CoS converts complex spatial environments described in natural language into condensed symbolic representations that capture spatial relationships more efficiently, reducing token count and cognitive load.
- Core assumption: LLMs can process symbolic representations more effectively than equivalent natural language descriptions for planning tasks.
- Evidence anchors:
  - [abstract]: "CoS also reduces the number of tokens in the prompt obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on Brick World."
  - [section]: "By doing such a conversion, COS effectively improves the model performance as well as reduces the inference costs with LLMs."
- Break condition: If symbolic representations become too abstract or lose critical spatial information needed for planning.

### Mechanism 2
- Claim: Symbolic representations help LLMs better understand spatial relationships in virtual environments.
- Mechanism: The condensed symbolic format makes spatial relationships more explicit and easier for LLMs to parse compared to natural language descriptions.
- Core assumption: Spatial relationships encoded in symbolic format are more readily interpretable by LLMs than natural language descriptions of the same relationships.
- Evidence anchors:
  - [abstract]: "We propose a novel method called COS (Chain-of-Symbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps."
  - [section]: "We speculate that such an improvement is benefited by the more efficient symbolic representation produced byCOS to rely on."
- Break condition: If LLMs fail to interpret the symbolic representations correctly or if the symbols introduce ambiguity.

### Mechanism 3
- Claim: CoS improves planning performance by providing clearer intermediate reasoning steps.
- Mechanism: The symbolic intermediate steps guide the LLM through the planning process more effectively than natural language reasoning chains.
- Core assumption: Clearer, more structured intermediate steps lead to better planning outcomes in LLMs.
- Evidence anchors:
  - [abstract]: "Extensive experiments indicate that COS clearly surpasses the performance of the Chain-of-Thought (CoT) Prompting in all three planning tasks"
  - [section]: "COS depicts a different intermediate thinking process than CoT. The latter represents the environments in a natural language only, while the former use a condensed symbolic representation that considers spatial relationship."
- Break condition: If the symbolic intermediate steps become too complex or if they don't align with the LLM's reasoning capabilities.

## Foundational Learning

- Concept: Spatial reasoning in virtual environments
  - Why needed here: The entire benchmark is built around understanding and navigating spatial relationships described in natural language
  - Quick check question: Can you explain the difference between 1D and 2D spatial relationships as used in the Brick World task?

- Concept: Chain-of-Thought prompting
  - Why needed here: CoS is presented as an alternative to CoT, so understanding CoT is essential
  - Quick check question: What are the key differences between how CoT and CoS represent intermediate reasoning steps?

- Concept: Token efficiency in LLM prompting
  - Why needed here: The paper emphasizes token reduction as a key benefit of CoS
  - Quick check question: How does token count impact the cost and feasibility of using LLMs via API?

## Architecture Onboarding

- Component map: Input parser -> Symbolic representation converter -> Symbolic reasoning engine -> Output formatter
- Critical path:
  1. Parse input environment description
  2. Convert to symbolic representation
  3. Generate intermediate reasoning steps using symbols
  4. Output final plan or answer
- Design tradeoffs:
  - Symbol expressiveness vs. simplicity
  - Token savings vs. potential loss of information
  - Generalizability across different types of spatial reasoning tasks
- Failure signatures:
  - Incorrect or incomplete symbolic representations
  - LLM failing to interpret symbolic representations
  - Performance degradation when switching between symbolic and natural language formats
- First 3 experiments:
  1. Replicate Brick World 1D results with different symbol sets to test robustness
  2. Test CoS on a new spatial reasoning task not in the original benchmark
  3. Compare CoS performance with and without the symbolic representation step (direct natural language prompting)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does Chain-of-Symbol (CoS) prompting generalize to real-world applications beyond simulated spatial environments?
- Basis in paper: [inferred] The paper mentions that CoS could be applied to vision-based tasks like Vision-and-Language navigation, but does not explore this in detail.
- Why unresolved: The current evaluation is limited to text-based planning tasks, and the robustness of CoS in real-world scenarios involving complex visual inputs and noise is not fully investigated.
- What evidence would resolve it: Experiments applying CoS to real-world vision-language navigation datasets (e.g., Room-to-Room) and comparing its performance against CoT and other methods.

### Open Question 2
- Question: What is the impact of symbol choice on the performance of CoS prompting?
- Basis in paper: [explicit] The paper mentions that using different symbols (comma, slash, dash) yields comparable results, but does not provide a systematic analysis of symbol selection.
- Why unresolved: The experiments use a fixed set of symbols without exploring a broader range of options or the potential benefits of context-specific symbol selection.
- What evidence would resolve it: A comprehensive study comparing various symbol sets (e.g., emojis, icons, or domain-specific symbols) across different tasks to determine optimal symbol choices.

### Open Question 3
- Question: How does CoS prompting perform with other large language models beyond ChatGPT and InstructGPT?
- Basis in paper: [inferred] The paper evaluates CoS on ChatGPT and InstructGPT but does not explore its effectiveness on other LLMs like PaLM, LLaMA, or open-source models.
- Why unresolved: The results may be specific to the architecture and training data of the tested models, and the generalizability of CoS to other LLMs is unknown.
- What evidence would resolve it: Experiments applying CoS to a diverse set of LLMs and analyzing the consistency of performance gains across different model families.

### Open Question 4
- Question: What is the role of training data in the effectiveness of CoS prompting?
- Basis in paper: [inferred] The paper suggests that CoS improves planning by providing condensed symbolic representations, but does not investigate whether this benefit is influenced by the training data of the LLMs.
- Why unresolved: The LLMs may have varying degrees of familiarity with symbolic representations based on their pre-training corpus, which could impact the effectiveness of CoS.
- What evidence would resolve it: Experiments training LLMs on datasets enriched with symbolic representations and comparing their performance with CoS against LLMs trained on standard text-only data.

## Limitations

- Benchmark tasks focus narrowly on spatial reasoning in controlled environments, limiting generalizability
- Manual conversion process from CoT to CoS introduces potential experimenter bias
- Results are based on only two commercial LLM models, limiting conclusions about CoS effectiveness across different architectures

## Confidence

- **High**: The NLP benchmark design and evaluation methodology are sound
- **Medium**: The quantitative improvements of CoS over CoT are well-documented
- **Low**: The proposed mechanism for why CoS works better than CoT

## Next Checks

1. Test whether the symbolic representations lose critical information by having humans evaluate if they capture all necessary spatial relationships compared to natural language descriptions
2. Run ablation studies removing the symbolic representation step to isolate its contribution to performance gains
3. Evaluate CoS on a broader set of planning tasks (e.g., multi-step cooking recipes, furniture assembly instructions) to test generalizability beyond spatial reasoning