---
ver: rpa2
title: Towards Building More Robust Models with Frequency Bias
arxiv_id: '2307.09763'
source_url: https://arxiv.org/abs/2307.09763
tags:
- frequency
- adversarial
- robust
- robustness
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of deep neural networks
  to adversarial examples by proposing a plug-and-play module called Frequency Preference
  Control Module (FPCM) that reconfigures the low- and high-frequency components of
  intermediate feature representations. The core idea is to adaptively enhance low-frequency
  features while suppressing high-frequency ones in the model's feature maps, based
  on the observation that adversarially trained models emphasize low-frequency information
  for robustness.
---

# Towards Building More Robust Models with Frequency Bias

## Quick Facts
- arXiv ID: 2307.09763
- Source URL: https://arxiv.org/abs/2307.09763
- Reference count: 40
- One-line primary result: FPCM improves adversarial robustness by reconfiguring frequency bias in feature maps while maintaining clean accuracy

## Executive Summary
This paper addresses the vulnerability of deep neural networks to adversarial examples by proposing a plug-and-play module called Frequency Preference Control Module (FPCM) that reconfigures the low- and high-frequency components of intermediate feature representations. The core idea is to adaptively enhance low-frequency features while suppressing high-frequency ones in the model's feature maps, based on the observation that adversarially trained models emphasize low-frequency information for robustness. The FPCM uses a learnable parameter to balance low- and high-frequency features and can be easily incorporated into existing adversarial training frameworks without modifying the original model architecture.

## Method Summary
The Frequency Preference Control Module (FPCM) is inserted after each stage of ResNet18 or WRN-34-10 architectures. FPCM applies a learnable Gaussian low-pass filter to intermediate feature maps, with weights balancing low- and high-frequency components. The module is trained jointly with adversarial training (PGD-AT or TRADES), and cutoff frequency is linearly decreased during training. The method operates in the frequency domain using FFT/IFFT transforms, applies Gaussian filtering with learnable parameters, and combines filtered and residual high-frequency components using learned channel-wise scaling.

## Key Results
- On CIFAR-10 with WRN-34-10, achieves 59.76% robust accuracy under PGD-50 attack
- Consistently improves model robustness against various attacks including PGD and AutoAttack
- Maintains competitive clean accuracy while enhancing adversarial robustness
- Outperforms baseline methods across CIFAR-10/100 and Imagenette datasets

## Why This Works (Mechanism)

### Mechanism 1
The FPCM improves adversarial robustness by reconfiguring the frequency bias of feature maps, suppressing high-frequency components that are vulnerable to adversarial perturbations while preserving low-frequency components that carry essential class-discriminative information. The module applies a learnable Gaussian low-pass filter in the frequency domain, then re-weights the resulting low-frequency features with learned inter-channel weights, effectively shifting the model's internal representation toward a more robust frequency profile. The core assumption is that adversarial training naturally biases models toward low-frequency features, and enhancing this bias improves robustness without harming clean accuracy.

### Mechanism 2
The learnable weights α in FPCM allow the model to adaptively balance low- and high-frequency features per channel, avoiding the irreversible information loss seen in static low-pass filtering. The sigmoid-transformed learned weights (α) scale the low-frequency reconstruction and combine it with the residual high-frequency part, enabling the model to selectively preserve or suppress frequencies per channel based on training feedback. The core assumption is that different channels in feature maps have different sensitivities to frequency components, and a fixed filter cannot optimally preserve all necessary information.

### Mechanism 3
Inserting FPCM after each stage of the network reduces the accumulation of high-frequency signals that naturally occur in convolutional layers, thereby improving robustness without modifying the original architecture. The module is placed at the end of each stage (after residual blocks), where high-frequency components peak, and its low-pass filtering counteracts the high-pass characteristics of convolutional layers. The core assumption is that convolutional layers inherently amplify high-frequency features as depth increases, and this amplification contributes to vulnerability.

## Foundational Learning

- **Concept**: Discrete Fourier Transform (DFT) and its computational implementation via Fast Fourier Transform (FFT)
  - Why needed here: The module operates in the frequency domain, transforming feature maps to identify and filter high-frequency components
  - Quick check question: How does FFT achieve O(HW log(HW)) complexity compared to the naive O((HW)²) for DFT?

- **Concept**: Adversarial training and its relationship to frequency bias
  - Why needed here: Understanding why adversarially trained models naturally emphasize low-frequency features is crucial to grasping the motivation for FPCM
  - Quick check question: What empirical evidence shows that AT models reduce high-frequency components compared to naturally trained models?

- **Concept**: Learnable layer design with element-wise operations
  - Why needed here: The FPCM uses learned weights to balance frequency components, requiring understanding of how learnable parameters integrate with fixed transformations
  - Quick check question: How do the learned weights α interact with the low-pass filtered features during the forward pass?

## Architecture Onboarding

- **Component map**: FPCM is a plug-and-play module that sits after each stage of the base model (e.g., ResNet). It consists of FFT/IFFT transforms, a Gaussian low-pass filter with learnable cutoff frequency, and a learnable weighting layer (Conv1d or MLP) that balances low- and high-frequency components.

- **Critical path**: Forward pass through FPCM: feature map → FFT → low-pass filtering → IFFT → learnable weighting (α) → element-wise combination with high-frequency residual → output. Backward pass flows gradients through FFT/IFFT (linear operations) and learnable parameters.

- **Design tradeoffs**: The module trades minimal parameter overhead and computational cost for improved robustness. Using FFT adds O(HW log(HW)) complexity per module, but this is acceptable given the robustness gains. The learnable cutoff frequency parameter (β) adds adaptability but could introduce redundancy with α.

- **Failure signatures**: Poor convergence during training, minimal robustness improvement over baseline, significant clean accuracy drop, or learned weights α collapsing to extreme values (near 0 or 1).

- **First 3 experiments**:
  1. Insert FPCM after stage 1 of ResNet18 with fixed β=0.25 and α learnable; compare clean and robust accuracy against PGD-20 baseline.
  2. Test different β values (0.125, 0.25, 0.5) with fixed α=0.5 to find optimal cutoff frequency.
  3. Compare learnable α versus fixed α=0.75 on WRN-34-10 to measure impact of adaptability.

## Open Questions the Paper Calls Out

### Open Question 1
How does the frequency bias of robust models impact their performance on datasets with distinctly different frequency characteristics compared to CIFAR-10/100 and Imagenette? The paper mentions that previous methods applying low-pass filters directly to input images face issues with generalizability to datasets with distinct frequency features, and that their proposed FPCM can adapt to different datasets. This remains unresolved because the experiments were conducted on CIFAR-10/100 and Imagenette, which may not cover the full spectrum of possible frequency characteristics in real-world datasets.

### Open Question 2
What is the optimal balance between low-frequency and high-frequency features for achieving maximum robustness across different types of adversarial attacks? The paper explores the trade-off between low- and high-frequency components through the learnable parameter α in FPCM, showing that emphasizing low-frequency features improves robustness but may impact clean accuracy. This remains unresolved because the experiments only tested a range of α values from 0 to 1, and the optimal balance may vary depending on the specific attack type and dataset.

### Open Question 3
How does the frequency bias of robust models impact their performance on tasks beyond image classification, such as object detection or semantic segmentation? The paper focuses on image classification tasks and does not explore the implications of frequency bias on other computer vision tasks. This remains unresolved because the frequency characteristics of feature representations may have different effects on tasks that require spatial localization or dense predictions compared to image classification.

## Limitations
- Theoretical justification for why suppressing high-frequency features enhances robustness remains incomplete
- Limited validation across diverse architectures beyond convolutional networks
- Alternative explanations (e.g., regularization effects) not adequately ruled out

## Confidence
- Claim that FPCM improves robustness: Medium (strong experimental results but limited theoretical grounding)
- Claim that frequency bias is the primary mechanism: Low (alternative explanations not ruled out)
- Claim that FPCM can be universally applied: Medium (effectiveness may vary across architectures)

## Next Checks
1. Ablation study on frequency components: Train models with only low-frequency features and only high-frequency features to quantify exact contribution to robustness
2. Cross-architecture validation: Apply FPCM to vision transformer architectures (e.g., ViT) to test generalization beyond convolutional networks
3. Alternative regularization comparison: Compare FPCM's performance against equivalent parameter counts of standard regularization techniques to isolate whether robustness gains come from frequency control or general regularization