---
ver: rpa2
title: 'Spuriosity Didn''t Kill the Classifier: Using Invariant Predictions to Harness
  Spurious Features'
arxiv_id: '2307.09933'
source_url: https://arxiv.org/abs/2307.09933
tags:
- stable
- domain
- features
- learning
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to leverage unstable or "spurious"
  features without test-domain labels in domain generalization. The key insight is
  that pseudo-labels from a stable predictor can guide adaptation of an unstable predictor,
  provided stable and unstable features are conditionally independent given the label.
---

# Spuriosity Didn't Kill the Classifier: Using Invariant Predictions to Harness Spurious Features

## Quick Facts
- arXiv ID: 2307.09933
- Source URL: https://arxiv.org/abs/2307.09933
- Reference count: 40
- Key outcome: The paper proposes a method to leverage unstable or "spurious" features without test-domain labels in domain generalization, achieving consistent improvements over baselines on synthetic and real datasets.

## Executive Summary
This paper addresses the challenge of domain generalization by proposing a method to harness both stable (invariant) and unstable (spurious) features without requiring test-domain labels. The key insight is that pseudo-labels from a stable predictor can guide adaptation of an unstable predictor, provided stable and unstable features are conditionally independent given the label. The Stable Feature Boosting (SFB) algorithm learns to separate these features during training and uses stable predictions to adapt unstable predictions in the test domain via bias correction and combination. Theoretically, the authors prove that SFB can learn an asymptotically-optimal predictor, and empirically, it consistently outperforms baselines across multiple datasets.

## Method Summary
The Stable Feature Boosting (SFB) algorithm learns stable and unstable feature extractors jointly during training using a combination of IRM-style stability penalties and conditional independence constraints. During the test phase, it generates pseudo-labels from the calibrated stable predictor and uses these to adapt the unstable classifier through bias correction. The final prediction combines both stable and adapted unstable predictions. The method requires no labels in the test domain but assumes that stable and unstable features are conditionally independent given the label, and that stable features are informative of the label in the test domain.

## Key Results
- SFB consistently outperforms baselines on synthetic and real datasets by safely harnessing unstable features
- Theoretical results prove SFB converges to the optimal classifier asymptotically without test-domain labels
- The method achieves strong performance even when the complementarity assumption is slightly violated in practice

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pseudo-labels from a stable predictor can guide adaptation of an unstable predictor without test-domain labels, provided stable and unstable features are conditionally independent given the label.
- **Mechanism:** The stable predictor generates pseudo-labels that are conditionally independent of unstable features given the true label, allowing the unstable classifier to learn the true label distribution without overfitting to spurious correlations.
- **Core assumption:** Stable features (XS) and unstable features (XU) are complementary with respect to the label Y (XS ⊥ XU|Y).
- **Break condition:** If XS and XU are not conditionally independent given Y, the pseudo-label bias correction becomes invalid and the method fails.

### Mechanism 2
- **Claim:** The joint distribution PY|XS,XU can be reconstructed from PY|XS and PXS,XU when features are complementary and XS is informative.
- **Mechanism:** Using the complementarity assumption, the method derives closed-form expressions for PY|XU and PY|XS,XU that combine stable and unstable predictions while correcting for bias and redundancy.
- **Core assumption:** XS is informative of Y (XS ⊥ Y) and the pseudo-label accuracies satisfy ϵ0 + ϵ1 > 1.
- **Break condition:** If XS is not informative of Y (e.g., XS ⊥ Y), the pseudo-label accuracies sum to 1 and the bias correction becomes undefined.

### Mechanism 3
- **Claim:** The stable feature boosting algorithm converges to the optimal classifier asymptotically without test-domain labels.
- **Mechanism:** The algorithm alternates between learning stable and unstable features during training, then uses stable predictions to adapt unstable predictions in the test domain via bias correction and combination.
- **Core assumption:** The unstable classifier can learn to predict pseudo-labels perfectly given enough unlabeled test data.
- **Break condition:** If the unstable classifier cannot learn the pseudo-label distribution, convergence to the optimal classifier is not guaranteed.

## Foundational Learning

- **Concept:** Conditional independence and the chain rule of probability
  - **Why needed here:** The entire theoretical framework relies on understanding when features are conditionally independent given the label, which allows decomposition of joint distributions.
  - **Quick check question:** If X1 and X2 are conditionally independent given Y, what is the factorization of P(X1, X2, Y)?

- **Concept:** Bias-variance tradeoff in pseudo-label learning
  - **Why needed here:** The method uses noisy pseudo-labels, so understanding how bias correction works and its sample complexity implications is crucial.
  - **Quick check question:** Why does the accuracy of pseudo-labels (ϵ0 + ϵ1) affect the sample efficiency of learning the unstable classifier?

- **Concept:** Calibration of probabilistic predictions
  - **Why needed here:** The method requires both stable and unstable predictors to be properly calibrated to combine them correctly in the test domain.
  - **Quick check question:** What happens if the stable predictor is not calibrated when forming pseudo-labels?

## Architecture Onboarding

- **Component map:** Stable predictor (fS) -> Pseudo-labels -> Unstable predictor (fU) adaptation -> Combination of calibrated stable and adapted unstable predictions
- **Critical path:** 1. Train stable and unstable feature extractors with IRM + conditional independence penalty 2. Calibrate stable predictor on training domains 3. Generate pseudo-labels in test domain using calibrated stable predictor 4. Adapt unstable classifier using pseudo-labels with bias correction 5. Combine calibrated stable and adapted unstable predictions
- **Design tradeoffs:**
  - Stability vs. informativeness: More stability reduces spurious correlations but may discard useful information
  - Conditional independence enforcement: Stronger penalties ensure theoretical guarantees but may hurt performance if the assumption is slightly violated
  - Adaptation complexity: Multiple rounds of pseudo-labeling can improve performance but increase computational cost
- **Failure signatures:**
  - Poor adaptation performance when stable predictor accuracy is near chance level (ϵ0 + ϵ1 ≈ 1)
  - Degradation when conditional independence assumption is strongly violated
  - Calibration issues leading to incorrect combination of predictions
- **First 3 experiments:**
  1. Verify stable predictor learns features invariant across training domains using domain classification accuracy
  2. Test bias correction formula on synthetic data where ground truth label distribution is known
  3. Evaluate adaptation performance as a function of stable predictor accuracy to confirm sample complexity relationship

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Under what conditions can SFB be made robust to violations of the complementarity assumption (XS ⊥ XU|Y)?
- **Basis in paper:** The paper notes that SFB performs surprisingly well on the CE-DD synthetic dataset and PACS dataset where complementarity is violated, but the exact conditions for robustness are unclear.
- **Why unresolved:** The paper does not identify specific weaker conditions that could replace complementarity for SFB to succeed. It only observes that SFB seems robust in practice to some violations.
- **What evidence would resolve it:** Theoretical analysis identifying specific weaker conditions (beyond complementarity) that suffice for SFB's bias correction and combination steps to work correctly. Empirical studies testing SFB under various structured violations of complementarity.

### Open Question 2
- **Question:** How can SFB be extended to handle multi-domain test environments where the stable feature XS may not be informative of Y in some test domains?
- **Basis in paper:** The paper assumes XS is informative of Y in the test domain (Defn. 4.3), but notes this is a limitation and suggests exploring alternative use cases for SFB in Appendix G.
- **Why unresolved:** The theoretical results (Thm 4.4, 4.6) rely on XS being informative in the test domain. The paper does not provide a solution for cases where this assumption fails.
- **What evidence would resolve it:** Extension of the theoretical results to cases where XS is not informative in the test domain, along with empirical validation on datasets exhibiting this scenario.

### Open Question 3
- **Question:** What is the optimal way to choose the combination function C(pS, pU) in SFB beyond the bias-corrected logistic function proposed in Eq. (4.5)?
- **Basis in paper:** The paper derives the bias-corrected logistic function as a principled choice but leaves the general choice of C unspecified until Theorem 4.4. It notes that in principle an additional hyperparameter γ could control the relative weighting.
- **Why unresolved:** The paper only explores one specific choice of combination function and does not investigate alternatives or optimization of this choice.
- **What evidence would resolve it:** Comparative studies of different combination functions (e.g., different link functions, learned combinations) on various datasets, along with theoretical analysis of their properties.

## Limitations
- The conditional independence assumption (XS ⊥ XU|Y) is strong and may not hold in many real-world scenarios where stable and unstable features are inherently correlated
- The method requires both stable and unstable predictors to be well-calibrated, which can be challenging in practice
- Performance critically depends on the stable predictor having accuracy substantially above chance level (ϵ0 + ϵ1 > 1)

## Confidence
- **High confidence**: The theoretical framework for bias correction and combination is mathematically sound under stated assumptions
- **Medium confidence**: Empirical results show consistent improvements over baselines, but the synthetic experiments may not fully capture real-world complexity
- **Low confidence**: The practical applicability of the conditional independence assumption in real-world datasets

## Next Checks
1. Test the method on a real-world dataset where spurious correlations are known (e.g., Camelyon17 with slide-level artifacts) to validate practical utility
2. Conduct an ablation study removing the conditional independence assumption to quantify its impact on performance
3. Evaluate the calibration requirements by intentionally miscalibrating the stable predictor and measuring degradation in test performance