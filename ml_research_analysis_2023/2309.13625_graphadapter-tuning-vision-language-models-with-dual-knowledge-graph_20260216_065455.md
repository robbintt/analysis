---
ver: rpa2
title: 'GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph'
arxiv_id: '2309.13625'
source_url: https://arxiv.org/abs/2309.13625
tags:
- knowledge
- textual
- visual
- graphadapter
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GraphAdapter, an adapter-style tuning strategy
  for vision-language models (VLMs) in the low-data regime. The key challenge addressed
  is the sub-optimal performance of existing adapter-based methods, which only model
  task-specific knowledge from a single modality and overlook the exploitation of
  inter-class relationships in downstream tasks.
---

# GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph

## Quick Facts
- arXiv ID: 2309.13625
- Source URL: https://arxiv.org/abs/2309.13625
- Authors: 
- Reference count: 40
- Key outcome: GraphAdapter, an adapter-style tuning strategy for vision-language models (VLMs) in the low-data regime, significantly outperforms previous adapter-based methods, achieving state-of-the-art performance in few-shot learning and generalization.

## Executive Summary
This paper introduces GraphAdapter, an adapter-based method for tuning vision-language models (VLMs) like CLIP in few-shot settings. The core innovation is the use of a dual knowledge graph that captures inter-class relationships in both textual and visual modalities, addressing the limitation of existing adapter methods that only model task-specific knowledge from a single modality. GraphAdapter employs graph convolutional networks (GCNs) to propagate and aggregate neighbor information, enabling cross-modal knowledge transfer and improved generalization. Extensive experiments on 11 benchmark datasets demonstrate that GraphAdapter significantly outperforms previous adapter-based methods, achieving state-of-the-art performance in few-shot learning.

## Method Summary
GraphAdapter is an adapter-style tuning strategy for VLMs that leverages a dual knowledge graph composed of textual and visual sub-graphs. The nodes in these graphs represent semantics/classes, and the edges represent their correlations. Textual features are warped through both textual and visual GCNs, while the visual adapter is omitted due to limited gains. The adapted features are computed as a residual fusion of the base feature with the dual-modality structure, with hyper-parameters α and β adjusting the weights of prior knowledge and the combination of textual and visual structure knowledge, respectively. GraphAdapter is trained using a cross-entropy loss and optimized with the Adam optimizer.

## Key Results
- GraphAdapter significantly outperforms previous adapter-based methods, achieving state-of-the-art performance in few-shot learning and generalization on 11 benchmark datasets.
- The dual knowledge graph, composed of textual and visual sub-graphs, effectively captures inter-class relationships in both modalities, leading to improved adapter generalization.
- The residual fusion of base features with dual-modality structure knowledge yields better task adaptation, preventing catastrophic forgetting of original CLIP features while incorporating structure knowledge.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual knowledge graphs capture inter-class relationships in both modalities, improving adapter generalization.
- Mechanism: Two separate graphs (textual and visual) encode node-class semantics and edge-class correlations, enabling cross-modal knowledge transfer via GCN.
- Core assumption: Inter-class correlations are more informative than intra-class features alone for few-shot learning.
- Evidence anchors:
  - [abstract] "the correlation of different semantics/classes in textual and visual modalities"
  - [section 3.2] "dual knowledge graph is established with two sub-graphs, i.e., a textual knowledge sub-graph, and a visual knowledge sub-graph"
  - [corpus] Weak - no direct comparison to ablation without dual graphs.
- Break condition: If edge weights (correlations) become noisy or classes are too diverse to benefit from shared structure.

### Mechanism 2
- Claim: Residual fusion of base feature with dual-modality structure yields better task adaptation.
- Mechanism: Adapted features are computed as z_t* = αz_t + (1 - α)z'_t, where z'_t fuses GCN-warped visual and textual structure.
- Core assumption: Combining raw CLIP features with structure-aware refinements improves robustness without overfitting.
- Evidence anchors:
  - [section 3.2] "feature adapter is achieved in a residual form, where the hyper-parameter α is used to adjust the weights of prior knowledge from the dual knowledge graph and original feature"
  - [section 4.3] "the performance of GraphAdapter increases first and then drops when the coefficients α and β increase"
  - [corpus] Missing - no ablation on α values.
- Break condition: If α too high, structure knowledge is underweighted; if too low, overfitting occurs.

### Mechanism 3
- Claim: Text-only adapter suffices because visual structure is redundant after dual GCN fusion.
- Mechanism: Textual features are warped through both textual and visual GCNs, while visual adapter is omitted.
- Core assumption: Textual features, when enriched with visual structure, already encode sufficient cross-modal knowledge.
- Evidence anchors:
  - [section 3.2] "we only exploit the textual adapter in our paper, since the limited gain with the combination of textual and visual adapters"
  - [section 4.3 Table 2] "GraphAdapter-T outperforms GraphAdapter-I by an average of 4.01%"
  - [corpus] Weak - no ablation comparing dual adapters vs single adapter.
- Break condition: If task domain is purely visual (e.g., texture), visual adapter may become necessary.

## Foundational Learning

- Concept: Graph Convolutional Networks (GCN)
  - Why needed here: GCNs propagate and aggregate neighbor information to capture inter-class correlations efficiently.
  - Quick check question: In Eq. 4, what does the Laplacian-normalized adjacency matrix accomplish?
- Concept: Residual connections in neural networks
  - Why needed here: Residual fusion prevents catastrophic forgetting of original CLIP features while incorporating structure knowledge.
  - Quick check question: Why does setting α=0.6 yield optimal performance according to Table 3?
- Concept: Prompt-based classification in vision-language models
  - Why needed here: Understanding how CLIP maps prompts to textual embeddings is key to building the textual knowledge graph.
  - Quick check question: In Eq. 1, what role does the temperature τ play in the softmax over class similarities?

## Architecture Onboarding

- Component map: CLIP Backbone -> Dual Knowledge Graph (Textual + Visual) -> GCN layers (g_tt, g_vt) -> Residual Fusion Layer (with α, β) -> Classification Loss
- Critical path: Build graphs → GCN feature warping → Residual fusion → Classification loss
- Design tradeoffs:
  - Textual-only adapter vs dual adapters: simplicity vs marginal gains
  - GCN depth: richer structure vs overfitting in few-shot regime
  - Fixed vs learnable α/β: stability vs adaptability
- Failure signatures:
  - Overfitting: training accuracy >> validation accuracy
  - Underutilization: performance matches vanilla CLIP-Adapter
  - Graph collapse: all nodes collapse to one embedding
- First 3 experiments:
  1. Ablate textual vs visual graph separately (verify β=0.7 is optimal)
  2. Sweep α from 0.3 to 0.9 (verify 0.6 is optimal)
  3. Replace GCN with mean pooling (confirm GCN adds value)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GraphAdapter change when using more diverse and accurate prompts for textual structure knowledge modeling?
- Basis in paper: [inferred] The authors mention that the current prompts are simple and lack diversity, suggesting that more diverse and accurate prompts could further improve performance.
- Why unresolved: The paper does not experiment with different prompt templates or assess the impact of prompt diversity on GraphAdapter's performance.
- What evidence would resolve it: Conducting experiments with various prompt templates and measuring the impact on GraphAdapter's accuracy and generalization capabilities would provide insights into the importance of prompt diversity.

### Open Question 2
- Question: What is the impact of increasing the number of graph nodes in the dual knowledge graph on GraphAdapter's performance?
- Basis in paper: [explicit] The authors mention decoupling the sub-graph with 1000 nodes into four graphs with 256 nodes to alleviate computational cost, implying a potential trade-off between graph size and performance.
- Why unresolved: The paper does not explore the effect of increasing the number of graph nodes beyond 256 or the impact on computational efficiency and accuracy.
- What evidence would resolve it: Experimenting with different numbers of graph nodes and analyzing the resulting performance, computational cost, and memory usage would clarify the optimal graph size for GraphAdapter.

### Open Question 3
- Question: How does GraphAdapter perform when applied to other vision-language model architectures beyond CLIP?
- Basis in paper: [explicit] The authors demonstrate GraphAdapter's effectiveness on different CLIP visual backbones but do not explore its applicability to other VLM architectures.
- Why unresolved: The paper focuses on CLIP-based models, leaving the question of GraphAdapter's performance on other VLM architectures unanswered.
- What evidence would resolve it: Applying GraphAdapter to other VLM architectures (e.g., ALBEF, BLIP) and comparing its performance with baseline methods would determine its generalizability across different models.

## Limitations
- Lack of ablation studies on the dual knowledge graph structure and the optimality of the residual fusion coefficients (α=0.6, β=0.7).
- Absence of comparisons to non-adapter baselines (e.g., direct fine-tuning, prompt tuning) to establish adapter superiority in few-shot settings.
- The mechanism by which GCNs improve few-shot performance could not be fully validated due to missing visualizations of learned graph structures.

## Confidence
- High: The core contribution of GraphAdapter (adapter with dual knowledge graphs improves few-shot VLM performance) is well-supported by extensive experiments on 11 benchmark datasets.
- Medium: The mechanism claims (GCNs capture inter-class correlations effectively) are plausible but lack direct ablation studies and visualizations of learned graph structures.
- Low: The design choice claims (textual-only adapter is optimal) lack empirical support through ablation studies comparing dual adapters vs single adapter.

## Next Checks
1. Ablate textual vs visual knowledge graphs separately to confirm dual structure provides additive benefit over single modality.
2. Sweep α from 0.3 to 0.9 on a subset of datasets to verify the claimed optimal value of 0.6.
3. Compare GraphAdapter against non-adapter baselines (e.g., direct fine-tuning, prompt tuning) to establish adapter superiority in few-shot settings.