---
ver: rpa2
title: 'All by Myself: Learning Individualized Competitive Behaviour with a Contrastive
  Reinforcement Learning optimization'
arxiv_id: '2310.00964'
source_url: https://arxiv.org/abs/2310.00964
tags:
- learning
- agents
- game
- winne
- opponent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces WINNE, a contrastive reinforcement learning
  model for individualized competitive behavior in games. WINNE combines a global
  policy network for general game knowledge, a Contrastive Strategy Prediction (CSP)
  network to learn opponent strategies, and local policy networks to counter specific
  opponents.
---

# All by Myself: Learning Individualized Competitive Behaviour with a Contrastive Reinforcement Learning optimization

## Quick Facts
- arXiv ID: 2310.00964
- Source URL: https://arxiv.org/abs/2310.00964
- Reference count: 40
- Outperforms offline, online, and competitive-specific models in personalized strategy learning

## Executive Summary
This paper introduces WINNE, a contrastive reinforcement learning model for individualized competitive behavior in games. WINNE combines a global policy network for general game knowledge, a Contrastive Strategy Prediction (CSP) network to learn opponent strategies, and local policy networks to counter specific opponents. Evaluated on Pokemon duel and Chef's Hat card game, WINNE demonstrates superior performance against various opponent types while maintaining resilience against catastrophic forgetting.

## Method Summary
WINNE uses a multi-network architecture with a global PPO-based policy for general game knowledge, a CSP network using contrastive optimization to learn opponent strategies, and local policy networks per opponent to counter specific strategies. The model trains online using contrastive optimization, with the CSP network predicting opponent actions from state-action sequences and the local policy using these predictions as auxiliary rewards to guide action selection.

## Key Results
- WINNE outperforms offline, online, and competitive-specific models, particularly against the same opponent multiple times
- Achieves higher accuracy in predicting opponent actions through contrastive learning
- Shows resilience against catastrophic forgetting, maintaining performance over longer periods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WINNE improves performance against the same opponent by learning individualized opponent strategies through contrastive representation learning.
- Mechanism: The CSP network uses contrastive optimization to learn rich representations of opponent action sequences, distinguishing between same-opponent (positive) and different-opponent (negative) samples to predict opponent actions with increasing accuracy.
- Core assumption: Each opponent has a consistent strategy pattern that can be represented and predicted from state-action sequences.
- Evidence anchors:
  - [abstract] "The entire model is trained online, using a composed loss based on a contrastive optimization, to learn competitive and multiplayer games."
  - [section 3.2] "By observing the opponents' actions, WINNE must refine the initial possible game actions given by the global policy."
  - [corpus] Weak evidence - corpus contains general RL/competitive learning papers but no direct contrastive learning implementations.
- Break condition: If opponents use highly variable strategies with no consistent patterns, the contrastive learning will fail to find meaningful representations.

### Mechanism 2
- Claim: The local policy network disrupts opponent strategies by using CSP predictions as auxiliary rewards to guide action selection.
- Mechanism: Local policy receives CSP's predicted action probability as an auxiliary reward (inverse probability), encouraging actions that make opponent's predicted actions less likely.
- Core assumption: Reducing the probability of predicted opponent actions correlates with winning the game.
- Evidence anchors:
  - [section 3.3] "We feed it, however, with the entangled representation that comes from the CSP, concatenated with then-best possible actions coming from the global policy network."
  - [section 3.4] "The goal of optimizing the local policy network is to minimize the auxiliary reward while maximizing the actions towards the environment end goal."
  - [corpus] No direct evidence in corpus - corpus papers focus on general RL/competitive learning without this specific mechanism.
- Break condition: If opponent's winning actions are unpredictable or if minimizing predicted action probability doesn't correlate with actual game success.

### Mechanism 3
- Claim: WINNE maintains performance across multiple opponent types through separate CSP and local policy networks per opponent, avoiding catastrophic forgetting.
- Mechanism: Each opponent gets dedicated CSP and local policy networks, allowing simultaneous learning of multiple strategies while global policy maintains general game knowledge.
- Core assumption: Separate networks per opponent can be maintained without interference, and transfer between them is beneficial.
- Evidence anchors:
  - [section 3.4] "To maximize WINNE's capability of dealing with individual agents, each of them with their game-play strategy, WINNE creates a new instance of the CSP and local policy for each opponent it plays against."
  - [section 6.2] "Our results show that the balance between generalized and personalized strategy gives WINNE a tool to escape the catastrophic forgetting problem."
  - [corpus] Weak evidence - corpus contains general continual learning papers but not this specific multi-opponent approach.
- Break condition: If opponent count becomes too large, maintaining separate networks becomes computationally prohibitive or causes interference.

## Foundational Learning

- Concept: Contrastive learning for representation learning
  - Why needed here: To learn opponent-specific strategy representations that distinguish between different playing styles
  - Quick check question: Can you explain how contrastive learning creates meaningful representations by pulling similar samples together and pushing dissimilar samples apart?

- Concept: Multi-agent reinforcement learning dynamics
  - Why needed here: Understanding how opponent actions affect environment state and how to model them as part of the decision process
  - Quick check question: How does treating opponents as part of the environment dynamics differ from explicitly modeling their strategies?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: WINNE must maintain performance across multiple opponents without forgetting previously learned strategies
  - Quick check question: What are the main causes of catastrophic forgetting in neural networks, and how does WINNE's architecture address them?

## Architecture Onboarding

- Component map: Global policy network -> CSP network (GRU encoder + output layer) -> Local policy network
- Critical path: Opponent round → CSP training (state-action pairs) → WINNE round → CSP prediction → Local policy action selection → Global policy baseline
- Design tradeoffs:
  - Separate networks per opponent vs. shared network with opponent embeddings
  - Contrastive vs. supervised learning for strategy prediction
  - Auxiliary reward from CSP vs. direct policy optimization
- Failure signatures:
  - CSP accuracy plateaus below 50% → Strategy patterns too complex or inconsistent
  - Local policy performance worse than global policy → Auxiliary reward mis-specified
  - Performance drops against first opponent when playing second opponent → Catastrophic interference
- First 3 experiments:
  1. Test CSP accuracy on simple opponent with consistent strategy (expect >90% after few games)
  2. Test local policy disruption on known opponent strategy (expect better performance than global policy alone)
  3. Test multi-opponent retention (expect maintained performance after playing different opponents)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the contrastive strategy prediction (CSP) network be adapted to handle opponents that do not follow a consistent strategy, such as random or highly unpredictable agents?
- Basis in paper: [explicit] The paper notes that WINNE struggles against naive agents with no strategy, as there is no pattern for the CSP to learn.
- Why unresolved: The current CSP relies on learning from consistent opponent behaviors, making it ineffective against non-strategic opponents.
- What evidence would resolve it: Experimental results showing WINNE's performance against a variety of unpredictable agents, along with modifications to the CSP that enable it to handle such cases.

### Open Question 2
- Question: Can the CSP network be extended to learn high-dimensional state representations for more complex video game scenarios beyond Pokemon and Chef's Hat?
- Basis in paper: [inferred] The paper mentions that updating the model to learn high-dimensional state representations could help adapt it to complex video game scenarios.
- Why unresolved: The current CSP is designed for relatively simple state representations, limiting its applicability to more complex environments.
- What evidence would resolve it: Successful implementation and evaluation of WINNE with CSP on complex video games with high-dimensional state spaces.

### Open Question 3
- Question: How can the performance drop observed in WINNE after the first game against a specific opponent be mitigated?
- Basis in paper: [explicit] The paper discusses the performance drop after the first game and suggests that extended memory replay might help address this issue.
- Why unresolved: The exact cause of the performance drop is not fully understood, and the proposed solution of extended memory replay is not yet implemented or tested.
- What evidence would resolve it: Comparative results showing the impact of extended memory replay on WINNE's performance over longer periods against the same opponents.

## Limitations
- Separate CSP and local policy networks per opponent may become computationally prohibitive with many opponents
- Assumption of consistent opponent strategies may not hold in dynamic, adaptive competitive environments
- Evaluation limited to two specific games (Pokemon duel and Chef's Hat), may not generalize to more complex video game scenarios

## Confidence

- **High confidence**: WINNE's architecture design and general approach to individualized strategy learning. The mechanism of using contrastive learning for opponent strategy prediction is well-established in representation learning.
- **Medium confidence**: The specific implementation details and hyperparameter choices, as these are not fully specified in the paper. The computational efficiency of maintaining separate networks per opponent is also uncertain.
- **Low confidence**: The claim that WINNE escapes catastrophic forgetting entirely, as the paper only shows results for a limited number of opponents and training duration.

## Next Checks

1. Test WINNE's performance with 10+ different opponent types to evaluate scalability and interference between learned strategies
2. Implement a variant using shared CSP network with opponent embeddings instead of separate networks to compare computational efficiency and performance
3. Evaluate WINNE against human players with varying skill levels to assess real-world applicability and strategy generalization