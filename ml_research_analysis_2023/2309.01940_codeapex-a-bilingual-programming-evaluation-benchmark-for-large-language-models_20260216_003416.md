---
ver: rpa2
title: 'CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language
  Models'
arxiv_id: '2309.01940'
source_url: https://arxiv.org/abs/2309.01940
tags:
- code
- programming
- llms
- generation
- shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodeApex is a bilingual (Chinese/English) benchmark dataset designed
  to evaluate the programming comprehension and code generation abilities of Large
  Language Models (LLMs). It consists of three categories of multiple-choice questions
  for programming comprehension (conceptual understanding, commonsense reasoning,
  multi-hop reasoning) and 476 C++ algorithm problems for code generation.
---

# CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models

## Quick Facts
- arXiv ID: 2309.01940
- Source URL: https://arxiv.org/abs/2309.01940
- Reference count: 19
- Key outcome: CodeApex evaluates programming comprehension and code generation abilities of LLMs through bilingual multiple-choice questions and C++ algorithm problems, with GPT-4 and GPT-3.5-turbo achieving highest accuracy (~69%) but all models showing performance below 50% on average.

## Executive Summary
CodeApex introduces a bilingual benchmark dataset for evaluating large language models' programming capabilities, consisting of multiple-choice questions for programming comprehension (conceptual understanding, commonsense reasoning, multi-hop reasoning) and 476 C++ algorithm problems for code generation. The benchmark is evaluated across 14 LLMs using various prompt strategies including 0-shot, few-shot, answer-only, and chain-of-thought approaches. Results show that while GPT-3.5-turbo achieves accept rates above 50% on code generation, all models perform below 50% accuracy on average for programming comprehension, highlighting significant room for improvement in LLM programming capabilities.

## Method Summary
The study evaluates 14 large language models across two main task categories: programming comprehension through multiple-choice questions and code generation through C++ algorithm problems. For programming comprehension, models are tested on three reasoning types using various prompt strategies (0-shot, 2-shot, 5-shot, answer-only, chain-of-thought). Code generation tasks involve generating compilable C++ functions that pass provided test cases, with evaluation metrics including AC@1, AC@all, and Accept Rate. The bilingual design includes both Chinese and English versions of questions to reduce linguistic bias.

## Key Results
- GPT-4 and GPT-3.5-turbo show the best overall performance with accuracy scores of approximately 69%, 54%, and 66% on the three programming comprehension tasks
- All models exhibit accuracy below 50% on average for programming comprehension, indicating benchmark difficulty
- GPT-3.5-turbo achieves accept rate above 50% for code generation tasks, while other models show lower performance
- Specialized models like CodeGeeX-13B and InternLM-Chat-7B demonstrate competitive performance in certain scenarios
- Chain-of-thought prompting shows mixed results, sometimes reducing accuracy compared to answer-only settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The bilingual design reduces cultural and linguistic bias in evaluating programming comprehension.
- Mechanism: By providing both Chinese and English versions of the same question set, the benchmark isolates language proficiency effects from actual programming understanding.
- Core assumption: Translation maintains semantic equivalence and difficulty level between versions.
- Evidence anchors: [abstract] "CodeApex is a pioneering bilingual (English, Chinese) benchmark dataset specifically designed for evaluating code capabilities." [section 4.2] "The Chinese version scored higher than the English version... Most evaluated models are primarily trained on Chinese data, which leads to a poor understanding of English."

### Mechanism 2
- Claim: Multiple prompt strategies (0-shot, 2-shot, 5-shot, answer-only, chain-of-thought) reveal different dimensions of model capability.
- Mechanism: Each prompting approach tests different aspects - 0-shot measures baseline understanding, few-shot tests in-context learning, answer-only checks direct response capability, and chain-of-thought evaluates reasoning depth.
- Core assumption: Models respond consistently to different prompt formats and these responses reflect genuine capability differences rather than prompt sensitivity.
- Evidence anchors: [section 4.2] "Due to the effective application of Chain-of-Thought (CoT), we also compare the performances of LLMs under the answer-only and CoT settings." [section 4.2] "The accuracy results of the CoT setting are depicted in Table 2. Compared to Table 1, most of the models achieve approximate or lower accuracy than the answer-only setting."

### Mechanism 3
- Claim: The combination of programming comprehension and code generation tasks provides comprehensive evaluation of practical programming ability.
- Mechanism: Programming comprehension tests understanding of concepts, reasoning, and code analysis through multiple-choice questions, while code generation tests practical implementation skills through algorithm problems with test cases.
- Core assumption: Success in comprehension tasks correlates with practical programming ability, and vice versa.
- Evidence anchors: [abstract] "CodeApex, a bilingual benchmark dataset focusing on the programming comprehension, code generation, and code correction abilities of LLMs." [section 4.2] "GPT3.5-turbo obtains the highest average accuracy among all the models, followed by InternLM-Chat-7B. It is noteworthy that the accuracy of all models is below 50%, highlighting that CodeApex Benchmark is challenging in the programming comprehension task." [section 4.3] "GPT3.5-turbo outperforms the other 11 LLMs, with an average score exceeding 50%."

## Foundational Learning

- Concept: Prompt engineering and few-shot learning
  - Why needed here: The benchmark evaluates models under different prompting strategies (0-shot, 2-shot, 5-shot), requiring understanding of how models leverage in-context examples.
  - Quick check question: What is the difference between 0-shot and 5-shot prompting, and how does this affect model performance?

- Concept: Code evaluation metrics (AC@1, AC@all, Accept Rate)
  - Why needed here: The code generation task uses specific metrics to evaluate generated code quality, requiring understanding of what each metric measures.
  - Quick check question: How do AC@1, AC@all, and Accept Rate differ in what they measure about code generation quality?

- Concept: Chain-of-Thought prompting effectiveness
  - Why needed here: The benchmark includes CoT evaluation, requiring understanding of when and why CoT helps or hurts performance.
  - Quick check question: Under what conditions does Chain-of-Thought prompting improve model performance on programming tasks?

## Architecture Onboarding

- Component map: Question dataset -> Prompt generation -> Model inference -> Answer extraction/Compilation -> Accuracy calculation/Accept rate calculation
- Critical path: For programming comprehension: question loading → prompt generation → model inference → answer extraction → accuracy calculation. For code generation: problem loading → prompt generation → model inference → code extraction → compilation → test execution → metric calculation.
- Design tradeoffs: Bilingual design increases coverage but requires careful translation; multiple prompt strategies increase evaluation depth but also complexity; code compilation adds rigor but may exclude models that generate nearly-correct code with minor syntax errors.
- Failure signatures: Low accuracy across all models suggests benchmark difficulty; large performance gaps between languages suggest bias; compilation failures suggest model adherence issues; test case failures suggest implementation correctness issues.
- First 3 experiments:
  1. Run a small subset of questions through each model with all prompt strategies to verify the evaluation pipeline works correctly.
  2. Test code compilation process with hand-crafted correct/incorrect code to ensure test framework works as expected.
  3. Compare model performance on a known-easy problem to establish baseline expectations before full evaluation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural modifications would enable LLMs to achieve emergent chain-of-thought reasoning capabilities?
- Basis in paper: [inferred] The paper notes that current models lack the parameter count needed for CoT emergence (60B+) and that CoT introduces noise for smaller models
- Why unresolved: The paper identifies the parameter threshold but doesn't explore what architectural changes beyond scale might enable CoT
- What evidence would resolve it: Comparative studies of different architectural modifications (attention mechanisms, memory components, etc.) tested across parameter scales

### Open Question 2
- Question: How does bilingual proficiency in LLMs correlate with programming language proficiency?
- Basis in paper: [explicit] The paper observes higher Chinese accuracy than English but doesn't explore the relationship between natural language and programming language abilities
- Why unresolved: The study shows performance differences but doesn't investigate whether multilingual models show advantages in programming tasks
- What evidence would resolve it: Cross-linguistic analysis of programming task performance across models with varying natural language training data

### Open Question 3
- Question: What is the minimum viable parameter count for LLMs to achieve human-level performance on CodeApex benchmark tasks?
- Basis in paper: [explicit] The paper notes significant room for improvement, with all models below 50% accuracy, and identifies GPT-3.5-turbo as the best performer
- Why unresolved: The paper establishes a performance gap but doesn't model how performance scales with parameter count or determine the threshold for human-level performance
- What evidence would resolve it: Systematic scaling experiments mapping parameter count to benchmark performance with extrapolation to human-level thresholds

### Open Question 4
- Question: Which specific programming concepts or reasoning types present the greatest challenges for current LLMs?
- Basis in paper: [explicit] The paper categorizes questions into conceptual understanding, commonsense reasoning, and multi-hop reasoning, noting different performance patterns
- Why unresolved: While performance differences are observed, the paper doesn't identify which specific concepts or reasoning patterns are most problematic
- What evidence would resolve it: Error analysis categorizing failure modes by concept type and reasoning complexity across the three question categories

## Limitations
- The bilingual design's effectiveness depends on translation quality and semantic equivalence, which may not be perfectly maintained across languages
- Code generation evaluation through compilation and test cases may penalize semantically correct but syntactically imperfect code
- All models achieving below 50% accuracy raises questions about benchmark difficulty calibration versus model limitations
- The evaluation methodology may not fully capture practical programming ability where minor errors can be easily corrected

## Confidence
- **High Confidence**: GPT-3.5-turbo's accept rates above 50% on code generation; compilation/test case methodology; specialized models' competitive performance
- **Medium Confidence**: Bilingual evaluation reducing linguistic bias; benchmark difficulty indicated by sub-50% accuracy; translation quality effects
- **Low Confidence**: Universal sub-50% accuracy indicating model weakness; relationship between natural and programming language proficiency

## Next Checks
1. Conduct human evaluation study where bilingual programmers assess semantic equivalence and difficulty parity between Chinese and English versions of 50 randomly selected questions to verify bilingual design validity.
2. Implement ablation study testing additional prompt formats beyond the five evaluated, including different chain-of-thought templates and mixed-language prompts, to determine if results reflect true capability differences or prompt sensitivity.
3. Perform detailed analysis of compilation failures and test case failures to distinguish between semantic errors versus syntactic/formatting issues that might be resolved with minor post-processing.