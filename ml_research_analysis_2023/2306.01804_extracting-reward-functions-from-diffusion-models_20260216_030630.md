---
ver: rpa2
title: Extracting Reward Functions from Diffusion Models
arxiv_id: '2306.01804'
source_url: https://arxiv.org/abs/2306.01804
tags:
- diffusion
- reward
- function
- learning
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'We propose a method for extracting reward functions from two diffusion
  models: a base model and an expert model. We show that, under certain assumptions,
  a unique relative reward function exists, and we devise a practical learning algorithm
  for extracting it by aligning the gradients of a reward function to the difference
  in outputs of both diffusion models.'
---

# Extracting Reward Functions from Diffusion Models

## Quick Facts
- arXiv ID: 2306.01804
- Source URL: https://arxiv.org/abs/2306.01804
- Reference count: 40
- Primary result: Method for extracting reward functions from diffusion models by aligning reward gradients to differences in model outputs

## Executive Summary
This paper introduces a method to extract reward functions from pre-trained diffusion models by leveraging the relationship between cumulative reward and log-likelihood of optimality in the probabilistic RL framework. The approach assumes that two diffusion models (a base model and an expert model) can be compared to extract a relative reward function that captures the difference in their behaviors. The method works by training a reward function whose gradients align with the difference between the expert and base model outputs. The authors demonstrate their approach on navigation tasks, locomotion benchmarks, and even large-scale image generation diffusion models, showing that the extracted reward functions can effectively steer base models toward more desirable behaviors.

## Method Summary
The method extracts reward functions by training a neural network to output rewards whose gradients match the difference between expert and base diffusion model outputs. Given a base diffusion model trained on low-reward trajectories and an expert model trained on high-reward trajectories, the approach defines a relative reward function as a potential function whose gradient equals the difference in model scores. The training objective minimizes the L2 distance between the reward gradient and this difference. Once trained, the reward function can be used with classifier guidance during sampling to steer the base model toward the expert's distribution. The method is implemented through empirical risk minimization using stochastic gradient descent, with the key insight that reward functions can be extracted without explicit reward annotations by leveraging the learned representations in diffusion models.

## Key Results
- Correctly learned reward functions in navigation environments that accurately capture goal locations
- Significant performance improvements when steering base models with learned rewards across locomotion benchmarks
- Successfully extracted reward-like functions from image generation diffusion models that assign lower rewards to harmful images

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** There exists a unique relative reward function between two diffusion models if both are conservative vector fields.
- **Mechanism:** Under the probabilistic RL framework, the negative log-likelihood of optimality corresponds to cumulative reward. If two diffusion models model low-reward and high-reward behaviors respectively, their difference in scores (gradients) corresponds to the reward gradient. When both models are conservative, their difference is also conservative and equals the gradient of a potential function, which is the relative reward.
- **Core assumption:** Both diffusion models are conservative vector fields (i.e., have no approximation error and are gradients of continuously differentiable functions).
- **Evidence anchors:**
  - [abstract] "We first define the notion of a relative reward function of two diffusion models and show conditions under which it exists and is unique."
  - [section 4.1] "Hence, by the Fundamental Theorem for Line Integrals (Th. 7.2 in [35]), there exists a unique ρ satisfying ∇xρ(x, t) = s(2)true(x, t) − s(1)true(x, t), up to an additive constant"
- **Break condition:** Approximation errors in diffusion models make them non-conservative, breaking uniqueness of the relative reward function.

### Mechanism 2
- **Claim:** The optimal relative reward gradient exists as the L2 projection of the difference between two diffusion models onto the space of conservative vector fields.
- **Mechanism:** Since practical diffusion models have approximation errors and may not be conservative, the method finds the conservative field that best approximates their difference in L2 norm. This projection is unique and can be approached arbitrarily closely by gradients of smooth potentials.
- **Core assumption:** The difference between diffusion model gradients is square-integrable.
- **Evidence anchors:**
  - [section 4.2] "Proposition 4.3 (Optimal Relative Reward Gradient). Let s(2)Θ and s(1)ϕ be any two diffusion models... Then there exists a unique vector field ht given by ht = argmin f∈Cons(Rn) ∫Rn ||f(x) − (s(2)Θ(x, t) − s(1)ϕ(x, t))||2 2 dx"
- **Break condition:** If the difference between diffusion models is not square-integrable, the projection integral diverges and the method fails.

### Mechanism 3
- **Claim:** Steering a base diffusion model with the learned reward function brings its trajectory distribution closer to the expert model's distribution.
- **Mechanism:** The learned reward function captures the difference in gradients between base and expert models. When used for classifier guidance during sampling, it gradually pushes the base model's samples toward the expert distribution by aligning the denoising process with the expert model's gradients.
- **Core assumption:** Classifier guidance effectively steers diffusion models toward desired distributions.
- **Evidence anchors:**
  - [abstract] "We then devise a practical learning algorithm for extracting it by aligning the gradients of a reward function – parametrized by a neural network – to the difference in outputs of both diffusion models."
  - [section 5.2] "Steering the base models with our learned reward functions consistently leads to statistically significant performance improvements across all three environments"
- **Break condition:** If the guidance scale is inappropriate or the reward function is poorly learned, steering may not effectively change the base model's distribution.

## Foundational Learning

- **Concept:** Reinforcement Learning as Probabilistic Inference
  - **Why needed here:** The paper's theoretical framework relies on viewing RL as sampling from a posterior distribution over trajectories conditioned on optimality variables. This probabilistic perspective connects cumulative reward to the log-likelihood of optimality, which is essential for understanding how reward functions relate to diffusion model gradients.
  - **Quick check question:** What does the negative log-likelihood of optimality correspond to in the probabilistic RL framework?

- **Concept:** Stochastic Differential Equations and Reverse SDEs
  - **Why needed here:** The continuous-time formulation of diffusion models uses SDEs to describe both the forward noising process and the reverse denoising process. Understanding how to reverse the noising SDE and how classifier guidance modifies it is crucial for deriving the reward extraction method.
  - **Quick check question:** How does the reverse SDE differ from the forward SDE, and what role does the score function play?

- **Concept:** Vector Calculus - Conservative Fields and Line Integrals
  - **Why needed here:** The existence and uniqueness of the relative reward function rely on properties of conservative vector fields. Specifically, the Fundamental Theorem for Line Integrals ensures that if the difference between two diffusion model gradients is conservative, it must be the gradient of some potential function (the reward function).
  - **Quick check question:** What mathematical condition must a vector field satisfy to be conservative, and what does this imply about its relationship to potential functions?

## Architecture Onboarding

- **Component map:** Base diffusion model -> Expert diffusion model -> Reward function network -> Classifier guidance -> Steered base model
- **Critical path:**
  1. Train base and expert diffusion models on respective datasets
  2. Initialize reward function network
  3. For each training iteration:
     - Sample states from forward process at random timesteps
     - Compute difference between expert and base model outputs at these states
     - Calculate loss between reward gradient and this difference
     - Backpropagate and update reward network
  4. Use trained reward for classifier guidance to steer base model
- **Design tradeoffs:**
  - Dataset size vs. reward accuracy: Larger datasets improve reward learning but increase training time
  - Reward horizon vs. granularity: Longer horizons capture more context but may smooth local features
  - Guidance scale selection: Higher scales increase steering strength but may destabilize sampling
- **Failure signatures:**
  - Reward function fails to discriminate between base and expert trajectories (histogram overlap)
  - Steering does not improve base model performance in downstream tasks
  - Learned reward is highly sensitive to small changes in input or training data
  - Training loss plateaus without reaching low values
- **First 3 experiments:**
  1. Train on Maze2D environment with simple maze configuration and verify heatmap visualization shows correct goal location
  2. Train on HalfCheetah environment and verify performance improvement when steering base model with learned reward
  3. Train on Stable Diffusion models (base vs. safe version) and verify reward discriminates between safe and unsafe image generations

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the proposed method for extracting reward functions from diffusion models generalize to continuous action spaces beyond navigation and locomotion tasks?
  - **Basis in paper:** [inferred] The paper demonstrates success in navigation and locomotion environments with discrete actions, but does not explicitly test continuous action spaces.
  - **Why unresolved:** The paper does not provide empirical evidence or theoretical analysis for the method's applicability to continuous action spaces.
  - **What evidence would resolve it:** Experimental results showing successful reward function extraction and improved performance in tasks with continuous action spaces, such as robotic manipulation or control of complex dynamical systems.

- **Open Question 2:** How does the choice of guidance scale (ω) affect the performance of the steered diffusion model and the learned reward function's accuracy?
  - **Basis in paper:** [explicit] The paper mentions using different guidance scales in experiments but does not provide a detailed analysis of its impact on performance or reward accuracy.
  - **Why unresolved:** The paper does not systematically explore the relationship between guidance scale and the quality of the learned reward function or the performance of the steered model.
  - **What evidence would resolve it:** A comprehensive study varying the guidance scale across multiple tasks, analyzing its effect on both the accuracy of the learned reward function and the performance of the steered diffusion model.

- **Open Question 3:** Can the method be extended to extract multi-objective reward functions from diffusion models, capturing trade-offs between different aspects of behavior?
  - **Basis in paper:** [inferred] The paper focuses on single-objective reward functions, but the concept of relative reward functions could potentially be extended to handle multiple objectives.
  - **Why unresolved:** The paper does not discuss or demonstrate the extraction of multi-objective reward functions, leaving open the question of how to handle scenarios where agents must balance multiple goals or constraints.
  - **What evidence would resolve it:** A theoretical framework and empirical results showing successful extraction of multi-objective reward functions from diffusion models, along with demonstrations of their effectiveness in balancing multiple objectives in various tasks.

## Limitations
- Theoretical framework requires conservative vector fields, but practical diffusion models contain approximation errors
- Computational feasibility of L2 projection for high-dimensional problems remains unclear
- Method assumes access to well-trained base and expert diffusion models, which require substantial resources

## Confidence
- High confidence: The gradient alignment training procedure and its implementation are technically sound
- Medium confidence: The theoretical framework connecting diffusion models to reward functions is valid under idealized conditions
- Low confidence: The practical effectiveness of the method for real-world applications beyond controlled benchmarks

## Next Checks
1. Test the method's sensitivity to approximation errors in diffusion models by introducing varying levels of noise and measuring degradation in learned reward quality
2. Evaluate the scalability of the approach by applying it to higher-dimensional problems beyond the tested locomotion environments
3. Conduct ablation studies to determine the minimum dataset size and model capacity required for reliable reward extraction