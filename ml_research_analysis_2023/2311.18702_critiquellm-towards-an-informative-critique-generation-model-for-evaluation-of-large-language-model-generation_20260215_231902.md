---
ver: rpa2
title: 'CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation
  of Large Language Model Generation'
arxiv_id: '2311.18702'
source_url: https://arxiv.org/abs/2311.18702
tags:
- evaluation
- critique
- answer
- gpt-4
- assistant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CRITIQUE LLM, a model trained to evaluate
  and provide critiques of large language model outputs. It uses a dialogue-based
  prompting method to collect high-quality referenced and reference-free evaluation
  data from GPT-4.
---

# CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation

## Quick Facts
- arXiv ID: 2311.18702
- Source URL: https://arxiv.org/abs/2311.18702
- Authors: 
- Reference count: 40
- Key outcome: Introduces CRITIQUE LLM, a model trained to evaluate and provide critiques of LLM outputs using dialogue-based prompting and fine-tuning, achieving comparable evaluation performance to GPT-4, especially in system-level correlations.

## Executive Summary
CRITIQUE LLM is a model trained to generate informative critiques of large language model outputs, addressing the limitations of existing evaluation methods. The paper proposes a dialogue-based prompting method to collect high-quality referenced and reference-free evaluation data from GPT-4, which is then used to fine-tune the model. Experiments on the AlignBench dataset demonstrate that CRITIQUE LLM achieves comparable evaluation performance to GPT-4, particularly in system-level correlations, and even outperforms GPT-4 in 3 out of 8 tasks in the reference-free setting. The model also exhibits promising scaling properties and its generated critiques can serve as scalable feedback to improve LLM generation quality.

## Method Summary
The method involves collecting high-quality evaluation data using a dialogue-based prompting approach with GPT-4, which includes two turns: one for referenced evaluation and another for reference-free evaluation. The collected data is then used to fine-tune two separate models: one for referenced evaluation and another for reference-free evaluation. The fine-tuned models are evaluated using text-level and system-level correlations on the AlignBench dataset. The paper also demonstrates the scalability of the approach by training models of different sizes and shows that the generated critiques can be used as feedback to improve the generation quality of LLMs.

## Key Results
- CRITIQUE LLM achieves comparable evaluation performance to GPT-4, especially in system-level correlations.
- The model outperforms GPT-4 in 3 out of 8 tasks in the reference-free setting.
- CRITIQUE LLM exhibits promising scaling properties, with larger models showing better performance.
- Generated critiques from CRITIQUE LLM can serve as scalable feedback to improve the generation quality of LLMs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dialogue-based prompting with two-turn revision produces smoother score distributions than single-turn prompting
- Mechanism: First turn establishes comparison-based evaluation with reference texts, creating calibrated scoring criteria. Second turn removes reference-based language while preserving score calibration
- Core assumption: GPT-4's scoring behavior is influenced by explicit comparison instructions and reference-based reasoning
- Evidence anchors:
  - [abstract] "We observe that these models lack the ability to generate informative critiques in both pointwise grading and pairwise comparison especially without references"
  - [section 3.2.3] "In the first turn, the prompts aim to obtain high-quality referenced results, which include detailed criteria to compare generated texts against reference texts"
  - [corpus] FMR scores around 0.5 for related critique papers, suggesting moderate novelty in this two-turn approach
- Break condition: If reference-free evaluation fundamentally differs from reference-based evaluation in ways that cannot be bridged by simple language removal

### Mechanism 2
- Claim: Supervised fine-tuning on high-quality dialogue-based data creates critique generation models that scale with parameter size
- Mechanism: Large language models learn to emulate GPT-4's evaluation patterns when trained on sufficient high-quality critique data with structured scoring criteria
- Core assumption: Evaluation patterns learned from GPT-4 annotations transfer to smaller models through fine-tuning
- Evidence anchors:
  - [abstract] "Experimental results show that our model can achieve comparable evaluation performance to GPT-4 especially in system-level correlations"
  - [section 4.4] "Comparing the performance from CRITIQUE LLM-6B to CRITIQUE LLM-66B, we can also observe good scaling properties"
  - [corpus] Related work on LLM-based evaluation shows FMR around 0.5, indicating this approach is competitive
- Break condition: If smaller models cannot capture the nuanced reasoning patterns present in GPT-4's critiques

### Mechanism 3
- Claim: Generated critiques serve as effective feedback to improve LLM outputs
- Mechanism: Critiques that identify specific strengths and weaknesses provide actionable guidance for model refinement, leading to measurable quality improvements
- Core assumption: LLM-generated critiques contain sufficient detail and accuracy to guide meaningful improvements in original outputs
- Evidence anchors:
  - [abstract] "We also demonstrate that our generated critiques can act as scalable feedback to directly improve the generation quality of LLMs"
  - [section 4.8] "The critiques from CRITIQUE LLM-66B can serve as positive feedback whose contributed improvement on the overall score is even larger than that from the GPT-4's critiques"
  - [corpus] Weak corpus evidence - no directly comparable papers found
- Break condition: If critiques are too vague or focus on incorrect aspects, preventing meaningful model improvement

## Foundational Learning

- Concept: Dialogue-based prompting with multiple turns
  - Why needed here: Single-turn prompting for reference-free evaluation produces extreme score distributions that cannot distinguish between different quality levels
  - Quick check question: Why does using a reference text in the first turn help improve reference-free evaluation scores?

- Concept: Supervised fine-tuning on instruction-following tasks
  - Why needed here: The model needs to learn to generate critiques in the same format and style as the high-quality GPT-4 annotations
  - Quick check question: What specific aspects of the prompt format are preserved during fine-tuning?

- Concept: Self-consistency decoding for evaluation tasks
  - Why needed here: Aggregation of multiple critique samples produces more stable and accurate final scores compared to single-sample decoding
  - Quick check question: How does self-consistency decoding differ from simple majority voting in this evaluation context?

## Architecture Onboarding

- Component map: Data collection → Supervised fine-tuning → Inference pipeline
  - Data collection: Query augmentation, generated result collection, dialogue-based prompting
  - Supervised fine-tuning: Two separate models (referenced vs reference-free) trained on respective datasets
  - Inference pipeline: Prompt formatting, self-consistency decoding, score aggregation

- Critical path: Prompt construction → Model inference → Score calculation → Explanation selection
  - Prompt construction includes user query, generated text, optional reference text, and evaluation instructions
  - Score calculation averages multiple sampled critiques
  - Explanation selection chooses the explanation closest to the averaged score

- Design tradeoffs: Separate models vs joint training, self-consistency vs greedy decoding, dialogue-based vs single-turn prompting
  - Separate models avoid performance degradation when mixing referenced and reference-free data
  - Self-consistency decoding adds computational cost but improves evaluation stability
  - Dialogue-based prompting requires more complex data collection but produces better quality critiques

- Failure signatures: Extreme score distributions, poor correlation with human judgments, self-evaluation bias
  - Extreme scores indicate the model cannot distinguish between quality levels
  - Poor correlation suggests the model learned incorrect evaluation patterns
  - Self-evaluation bias shows preference for outputs from the same base model

- First 3 experiments:
  1. Compare dialogue-based prompting (two-turn) vs single-turn prompting for reference-free evaluation quality
  2. Test different decoding strategies (greedy, sampling, self-consistency) on a small dataset
  3. Evaluate scaling properties by training models of different sizes on subsets of the training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the scaling of model parameters in CRITIQUE LLM relate to its ability to generalize across diverse task categories, especially in reference-free settings?
- Basis in paper: [inferred] The paper mentions scaling properties and shows performance improvements with larger models, but doesn't explicitly analyze generalization across task categories.
- Why unresolved: The paper shows CRITIQUE LLM performs well on AlignBench tasks but doesn't analyze whether larger models improve generalization to unseen tasks or domains.
- What evidence would resolve it: Experiments testing CRITIQUE LLM on tasks outside AlignBench or with different linguistic characteristics would clarify if parameter scaling improves cross-domain generalization.

### Open Question 2
- Question: What is the impact of the dialogue-based prompting method on reducing extreme score distributions compared to other prompting strategies?
- Basis in paper: [explicit] The paper introduces a two-turn dialogue prompting method to address extreme score distributions, but doesn't compare it against alternative approaches.
- Why unresolved: While the dialogue-based method shows improvement, the paper doesn't empirically compare it against other potential solutions like temperature scaling or alternative score distribution regularizations.
- What evidence would resolve it: Direct comparison between dialogue-based prompting and alternative methods (e.g., single-turn prompts with explicit score distribution constraints) would quantify its unique contribution.

### Open Question 3
- Question: How does the quality of training data (e.g., human-annotated vs. GPT-4-generated) affect the scaling properties and final performance of CRITIQUE LLM?
- Basis in paper: [inferred] The paper uses GPT-4-generated data and shows scaling properties, but doesn't explore how training data quality impacts these properties.
- Why unresolved: The paper doesn't investigate whether the scaling benefits observed are dependent on the quality of the training data source.
- What evidence would resolve it: Training CRITIQUE LLM on different quality datasets (e.g., human-annotated vs. GPT-4-generated) while varying model scale would reveal the relationship between data quality and scaling benefits.

## Limitations

- The generalizability of the dialogue-based prompting approach to other domains or evaluation scenarios is unclear.
- The paper lacks direct comparison to established evaluation benchmarks beyond AlignBench.
- The assertion that dialogue-based prompting fundamentally solves the score distribution problem in reference-free evaluation is based on observed improvements rather than theoretical guarantees.

## Confidence

**High Confidence:** The scaling properties observed in the model (from CRITIQUE LLM-6B to CRITIQUE LLM-66B) are well-supported by experimental results. The correlation between model size and evaluation performance is clearly demonstrated through systematic testing across multiple scales.

**Medium Confidence:** The claim that generated critiques can serve as effective feedback to improve LLM outputs is supported by experimental results showing improvement over baseline models, but the magnitude of improvement and its consistency across different tasks warrant further investigation. The paper shows promising results but doesn't extensively explore the limitations of this feedback mechanism.

**Low Confidence:** The assertion that dialogue-based prompting fundamentally solves the score distribution problem in reference-free evaluation is based on observed improvements rather than theoretical guarantees. The paper doesn't fully explore whether this approach introduces other biases or limitations that might affect evaluation reliability.

## Next Checks

1. **Cross-dataset validation**: Test CRITIQUE LLM's evaluation performance on at least two additional, independently developed benchmark datasets beyond AlignBench to verify the robustness of the claimed improvements over GPT-4.

2. **Human evaluation comparison**: Conduct systematic human studies comparing CRITIQUE LLM's critiques against GPT-4 critiques and human expert critiques to measure agreement rates and identify systematic differences in evaluation patterns.

3. **Domain transfer experiment**: Evaluate CRITIQUE LLM's performance on domains significantly different from the training data (e.g., code generation, mathematical reasoning) to test the generalizability of the dialogue-based prompting approach and the learned evaluation patterns.