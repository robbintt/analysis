---
ver: rpa2
title: Iterative Reachability Estimation for Safe Reinforcement Learning
arxiv_id: '2309.13528'
source_url: https://arxiv.org/abs/2309.13528
tags:
- policy
- learning
- safety
- function
- feasible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RESPO, a framework for safe reinforcement learning
  in stochastic environments. The key innovation is a reachability estimation function
  (REF) that predicts the likelihood of future constraint violations.
---

# Iterative Reachability Estimation for Safe Reinforcement Learning

## Quick Facts
- arXiv ID: 2309.13528
- Source URL: https://arxiv.org/abs/2309.13528
- Reference count: 40
- Key outcome: RESPO achieves better reward performance and lower/zero constraint violations compared to state-of-the-art baselines in Safety Gym, PyBullet, and MuJoCo environments

## Executive Summary
This paper proposes RESPO, a framework for safe reinforcement learning in stochastic environments. The key innovation is a reachability estimation function (REF) that predicts the likelihood of future constraint violations. RESPO uses this REF to guide policy optimization, ensuring persistent safety when in the feasible set and producing the safest behavior with minimal cumulative violations when outside it. The authors introduce a class of actor-critic algorithms based on learning the REF and prove their convergence to locally optimal policies.

## Method Summary
RESPO combines actor-critic methods with reachability estimation to solve constrained reinforcement learning problems. The framework learns a reachability estimation function (REF) that predicts the probability of future constraint violations, then uses this information to guide policy optimization through a Lagrangian formulation. The algorithm employs a multi-timescale update schedule where critics update fastest, followed by the policy, REF, and finally the Lagrange multiplier. This hierarchical update structure enables the algorithm to converge to locally optimal policies while maintaining safety constraints.

## Key Results
- Achieves better reward performance compared to state-of-the-art baselines
- Maintains lower or zero constraint violations across multiple benchmark environments
- Demonstrates effectiveness in settings with multiple hard and soft constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reachability estimation function (REF) enables persistent safety by predicting future constraint violations with probability rather than binary decisions
- Mechanism: REF ϕπ(s) = Eτ∼π,P(s)[maxst∈τ 1(st∈Sv)] computes the probability of any future violation along a trajectory, allowing the policy to optimize for both safety and performance even in stochastic environments where feasibility is not binary
- Core assumption: The Bellman recursive formulation ϕπ(s) = max{1s∈Sv, E[s′∼π,P(s)ϕπ(s′)]} holds and can be approximated by a neural network p(s)
- Evidence anchors: [abstract] "novel reachability estimation function to optimize in our proposed framework"; [section] "Theorem 1. The REF can be reduced to the following recursive Bellman formulation"; [corpus] Weak - no direct citation of Bellman formulation in related works
- Break condition: If the Bellman assumption fails due to non-Markovian dynamics or the neural network cannot approximate p(s) accurately, the safety guarantees collapse

### Mechanism 2
- Claim: The multi-timescale update schedule ensures convergence to locally optimal policies
- Mechanism: By treating faster-updated parameters as fixed when updating slower ones, the algorithm can leverage contraction mapping properties of critics and REF while the multiplier slowly enforces hard constraints
- Core assumption: Step sizes follow ζ1(k) fastest, ζ2(k) second, ζ3(k) third, ζ4(k) slowest with ζj(k) = o(ζj−1(k)) for j ∈ {2,3,4}
- Evidence anchors: [abstract] "theoretically establish that our algorithms almost surely converge to locally optimal policies"; [section] "Step sizes follow schedules {ζ1(k)}, {ζ2(k)}, {ζ3(k)}, {ζ4(k)} where: Xk ζi(k) = ∞ and Xk ζi(k)2 < ∞, ∀i ∈ {1, 2, 3, 4}"; [corpus] Weak - multi-timescale proofs exist but not directly cited for this specific algorithm
- Break condition: If the step size hierarchy is violated or the contraction properties don't hold, the stochastic approximation may diverge

### Mechanism 3
- Claim: The Lagrangian formulation naturally balances reward maximization and constraint satisfaction without needing hard cost thresholds
- Mechanism: When in the feasible set (p(s) ≈ 0), the term λ·Vc(s)·(1-p(s)) dominates and enforces safety; when outside (p(s) ≈ 1), the term Vc(s)·p(s) ensures minimal violations while seeking re-entry
- Core assumption: The optimal REF p(s) converges to ϕ*(s) representing the safest policy's feasibility likelihood
- Evidence anchors: [abstract] "class of actor-critic algorithms based on learning the REF"; [section] "minπ maxλ L(π,λ) = minπ maxλ E[s∼d0][(-Vπ(s) + λ·Vc(s))·(1-p(s)) + Vc(s)·p(s)]"; [corpus] Weak - no direct comparison to Lagrangian methods in related works
- Break condition: If p(s) fails to converge to the optimal REF, the weighting becomes incorrect and either safety or performance suffers disproportionately

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and Constrained MDPs (CMDPs)
  - Why needed here: The algorithm operates within MDP framework augmented with safety constraints; understanding state transitions, policies, and value functions is essential
  - Quick check question: Can you write the Bellman equation for the value function Vπ(s) and explain how constraints modify it in CMDPs?

- Concept: Hamilton-Jacobi Reachability Analysis
  - Why needed here: The REF is inspired by HJ reachability but adapted for stochastic settings; knowing how backward reachable sets work helps understand the safety guarantees
  - Quick check question: What is the difference between the backward reachable set in deterministic HJ and the probability-based REF in stochastic settings?

- Concept: Multi-timescale Stochastic Approximation
  - Why needed here: The convergence proof relies on different parameters updating at different speeds; understanding this theory is crucial for debugging training instability
  - Quick check question: Why must the lagrange multiplier update slower than the policy, and what could happen if this ordering is reversed?

## Architecture Onboarding

- Component map: Policy network πθ → generates actions; Reward critic Qη → estimates Vπ; Cost critic Qκ → estimates Vc; REF network pξ → estimates feasibility likelihood; Lagrange multiplier λω → enforces constraints; Environment → provides transitions and rewards/costs
- Critical path: Environment → (s,a,r,h,s') → Update critics (η,κ) → Update REF (ξ) → Update policy (θ) → Update multiplier (ω) → repeat
- Design tradeoffs: Using cumulative discounted costs Vc instead of reachability value function Vh provides better learning signals but requires careful REF design; treating multiple hard constraints separately with individual REFs allows prioritization but increases model complexity
- Failure signatures: If REF converges too fast or slow relative to policy, safety-performance balance breaks; if critics don't converge, all downstream updates are wrong; if lagrange multiplier explodes, algorithm becomes overly conservative
- First 3 experiments:
  1. Train on a simple deterministic environment (e.g., Double Integrator) to verify REF correctly identifies feasible/infeasible regions
  2. Run ablation with modified REF learning rate to observe impact on safety-performance tradeoff
  3. Test multi-constraint prioritization by setting up an environment where hard constraint violations have different costs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can RESPO's convergence guarantees be extended to continuous state-action spaces?
- Basis in paper: [inferred] The authors prove convergence in finite MDPs with bounded state and action spaces. The algorithm uses function approximation (neural networks) for the policy, value functions, and reachability estimation function.
- Why unresolved: The proof relies on finite MDP assumptions. Extending to continuous spaces requires new theoretical analysis, particularly regarding function approximation error and exploration guarantees.
- What evidence would resolve it: A theoretical proof showing convergence to local optima in continuous state-action spaces, or empirical evidence showing consistent performance across various continuous environments.

### Open Question 2
- Question: How does the choice of learning rates across the different components affect RESPO's performance and stability?
- Basis in paper: [explicit] The authors state Assumption A1 requiring specific learning rate schedules: critics fastest, policy second fastest, REF second slowest, and lagrange multiplier slowest. They perform ablation studies showing sensitivity to REF learning rate.
- Why unresolved: The authors only explore REF learning rate sensitivity. The interactions between all four learning rates and their optimal relative values remain unexplored.
- What evidence would resolve it: Systematic experiments varying all four learning rates simultaneously, or theoretical analysis deriving optimal relative learning rates.

### Open Question 3
- Question: Can RESPO handle soft constraints with different priorities more effectively than the current implementation?
- Basis in paper: [explicit] The authors demonstrate handling multiple hard and soft constraints in the multi-drone environment, but note that soft constraints are treated equally in their formulation.
- Why unresolved: The current implementation uses a single lagrange multiplier for soft constraints and does not differentiate between different soft constraint priorities.
- What evidence would resolve it: An extension of RESPO that incorporates different weights or priorities for soft constraints, with experimental validation showing improved performance.

## Limitations
- Theoretical convergence relies on specific step-size hierarchies and contraction properties that may not hold in practice
- Scalability to high-dimensional, complex tasks remains an open question
- The approach requires separate REFs for multiple hard constraints, increasing model complexity

## Confidence

- Confidence is Medium for the convergence claims, as the proof assumes specific step-size hierarchies and contraction properties that may not hold in practice
- Confidence is High for the empirical results showing improved reward performance and reduced constraint violations across multiple benchmarks
- Confidence is Medium for the claim that RESPO naturally balances reward maximization and constraint satisfaction without hard cost thresholds, as this depends critically on the REF's ability to accurately estimate future violations

## Next Checks

1. **Ablation on step-size hierarchies**: Systematically vary the relative learning rates between policy, critics, REF, and multiplier to identify the sensitivity of convergence to the multi-timescale assumption

2. **Robustness to non-Markovian dynamics**: Test RESPO on environments with partial observability or history-dependent constraints to evaluate whether the Bellman formulation still holds and the REF remains effective

3. **Scalability to complex constraints**: Implement RESPO on a benchmark with heterogeneous hard constraints (e.g., different violation costs or frequencies) to assess the approach's ability to prioritize and balance multiple safety requirements