---
ver: rpa2
title: 'AI in the Gray: Exploring Moderation Policies in Dialogic Large Language Models
  vs. Human Answers in Controversial Topics'
arxiv_id: '2308.14608'
source_url: https://arxiv.org/abs/2308.14608
tags:
- chatgpt
- topics
- arguments
- answers
- controversial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores biases in ChatGPT responses to controversial
  topics compared to human answers. The authors use a dataset from the Kialo debating
  platform to generate prompts and query ChatGPT, Bing AI, and earlier OpenAI models.
---

# AI in the Gray: Exploring Moderation Policies in Dialogic Large Language Models vs. Human Answers in Controversial Topics

## Quick Facts
- arXiv ID: 2308.14608
- Source URL: https://arxiv.org/abs/2308.14608
- Reference count: 39
- Key outcome: This paper explores biases in ChatGPT responses to controversial topics compared to human answers from Kialo debating platform, finding ChatGPT is well-moderated economically but still leans libertarian politically.

## Executive Summary
This study analyzes biases in ChatGPT and other LLMs by comparing their responses to controversial topics with human-generated arguments from the Kialo debating platform. Using both free-style and prompt-engineered queries, the authors examine direct answers, sources cited, arguments provided, and vocabulary diversity across economic, sociopolitical, and scientific domains. Results show that while recent ChatGPT versions have significantly reduced explicit biases compared to earlier models, they still exhibit subtle implicit leanings toward libertarian positions on sociopolitical issues. The analysis reveals that Bing AI sources tend to be slightly more centrist than human citations, and ChatGPT demonstrates comparable domain knowledge to humans except in philosophical topics where it shows less vocabulary diversity.

## Method Summary
The researchers collected approximately 2,900 popular Kialo debates and queried seven OpenAI models plus Bing AI with both free-style and prompt-engineered controversial topics. They extracted and classified arguments using regular expressions and ChatGPT itself as a labeling tool based on economic left/right and sociopolitical libertarian/authoritarian axes. The study analyzed source affiliations from MediaBiasFactCheck and AllSides, measured semantic diversity using sentence transformer embeddings, and compared vocabulary complexity between AI and human responses. Statistical analysis was performed to identify patterns of bias and knowledge differences across domains.

## Key Results
- Recent ChatGPT versions (gpt-3.5-turbo) show no significant explicit biases in several knowledge areas compared to earlier models
- ChatGPT maintains implicit libertarian leaning on sociopolitical topics despite economic moderation
- Bing AI sources are slightly more centrist than human citations, while ChatGPT shows less vocabulary diversity on philosophical topics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The approach detects AI bias by comparing LLM-generated arguments with human-generated arguments from a structured debating platform.
- **Mechanism**: The method extracts arguments from LLM responses and classifies them as leaning economically left, right, or politically libertarian/authoritarian. These classifications are compared to human-labeled arguments on the same topics from Kialo, revealing implicit biases that direct yes/no questions might miss.
- **Core assumption**: Human-generated arguments on Kialo are a valid proxy for collective human ideological leaning on controversial topics.
- **Evidence anchors**:
  - [abstract] "Our results show that while previous versions of ChatGPT have had important issues with controversial topics, more recent versions of ChatGPT (gpt-3.5-turbo) are no longer manifesting significant explicit biases in several knowledge areas."
  - [section 5.4] "We use ChatGPT itself as a labeling tool... We feed the extracted arguments back into ChatGPT to classify them based on their alignment with different sides of the political compass"
  - [corpus] Weak evidence - corpus neighbors do not directly support this mechanism, indicating limited external validation.
- **Break condition**: If human arguments on Kialo do not represent genuine ideological positions or if LLM argument extraction fails to capture the full nuance of responses.

### Mechanism 2
- **Claim**: Prompt engineering can reveal biases that free-style prompts obscure by forcing balanced pros/cons output.
- **Mechanism**: By explicitly requesting both pros and cons, the LLM's choice of language and emphasis can reveal implicit biases even when it appears balanced on the surface. Unassertive language analysis (e.g., "some people argue that...") identifies when the model distances itself from certain viewpoints.
- **Core assumption**: The language patterns used in pros vs cons reveal the model's true leanings even when explicitly balanced.
- **Evidence anchors**:
  - [section 5.5] "Even in the prompt-engineered scenario the authoritarian claims are more prone to moderation than the libertarian ones... the model distances itself more from economically right arguments than economically left arguments"
  - [abstract] "we see that sources of Bing AI have slightly more tendency to the center when compared to human answers"
  - [corpus] No direct support - corpus neighbors don't address prompt engineering effects.
- **Break condition**: If LLM responses become formulaic and lose genuine argumentative content when prompted for balanced pros/cons.

### Mechanism 3
- **Claim**: Semantic diversity metrics reveal differences in domain knowledge between AI and humans.
- **Mechanism**: Sentence transformer embeddings capture semantic diversity, while domain-specific vocabulary analysis measures knowledge depth in controversial topics. Lower diversity in philosophy topics indicates specific knowledge gaps.
- **Core assumption**: Semantic diversity and domain-specific vocabulary are valid proxies for domain knowledge depth.
- **Evidence anchors**:
  - [section 6.3] "ChatGPT is doing a good job of keeping up with humans in terms of producing sophisticated and diverse arguments... The only exception is Philosophy"
  - [abstract] "ChatGPT has a significantly less diverse domain-specific vocabulary" on philosophical topics
  - [corpus] No support - corpus neighbors don't address semantic diversity measurement.
- **Break condition**: If semantic diversity measures conflate style differences with knowledge differences.

## Foundational Learning

- **Concept**: Political Compass framework
  - Why needed here: Provides the two-dimensional axes (economic left/right, social libertarian/authoritarian) for measuring ideological leanings
  - Quick check question: What are the four quadrants of the Political Compass and what does each represent?

- **Concept**: Sentence transformer embeddings
  - Why needed here: Used to measure semantic diversity and compare complexity of AI vs human arguments
  - Quick check question: What does a higher variance in sentence embeddings indicate about a text corpus?

- **Concept**: Prompt engineering for bias detection
  - Why needed here: Forces LLMs to reveal implicit biases through balanced pros/cons even when they appear neutral
  - Quick check question: How can analyzing unassertive language patterns reveal implicit biases in LLM responses?

## Architecture Onboarding

- **Component map**: Data collection (Kialo scraping, LLM querying, source annotation) → Argument extraction → Classification (economic/social leaning) → Comparison (AI vs human) → Analysis (bias detection, knowledge assessment)
- **Critical path**: Query generation → LLM response collection → Argument extraction → Classification → Bias analysis
- **Design tradeoffs**: Free-style vs prompt-engineered queries (authenticity vs control), manual vs automated classification (accuracy vs scalability)
- **Failure signatures**: High proportion of moderated responses (model avoiding controversy), classification confusion matrices showing poor precision/recall, semantic diversity metrics not differentiating domains
- **First 3 experiments**:
  1. Compare classification accuracy of human vs AI-labeled arguments on Kialo topics
  2. Measure semantic diversity variance between free-style and prompt-engineered responses
  3. Test prompt engineering with different formulations to optimize bias revelation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the moderation of sociopolitical biases in LLMs like ChatGPT?
- Basis in paper: [explicit] The paper notes that while ChatGPT is well-moderated economically, it still maintains degrees of implicit libertarian leaning toward right-winged ideals on the sociopolitical axis.
- Why unresolved: The paper identifies the issue but does not provide specific solutions for improving sociopolitical moderation beyond general calls for increased moderation.
- What evidence would resolve it: Developing and testing specific techniques to balance sociopolitical perspectives in LLM responses, then measuring their effectiveness in reducing implicit biases.

### Open Question 2
- Question: What is the impact of prompt engineering on mitigating biases in LLMs, and how can it be optimized for different domains?
- Basis in paper: [explicit] The paper uses prompt engineering to elicit balanced pros and cons from ChatGPT but notes that this approach may not fully address underlying biases, particularly in the sociopolitical domain.
- Why unresolved: While the paper demonstrates the use of prompt engineering, it does not explore its limitations or potential optimizations for different types of biases or domains.
- What evidence would resolve it: Systematic testing of various prompt engineering techniques across different domains and bias types, measuring their effectiveness in eliciting balanced responses.

### Open Question 3
- Question: How does the domain-specific vocabulary diversity of ChatGPT compare to other knowledge domains, and what factors contribute to its performance in philosophy?
- Basis in paper: [explicit] The paper finds that ChatGPT's domain knowledge is comparable to humans across most domains, except for philosophy where it has significantly less diverse vocabulary.
- Why unresolved: The paper identifies the performance gap in philosophy but does not investigate the underlying reasons or potential methods to improve ChatGPT's philosophical reasoning.
- What evidence would resolve it: Comparative analysis of the training data, linguistic structures, and reasoning patterns in philosophical texts versus other domains, followed by targeted improvements to address the identified gaps.

## Limitations

- The methodology relies on ChatGPT itself for argument classification, creating a circular dependency that may amplify or mask certain biases
- Semantic diversity metrics may conflate stylistic differences with genuine knowledge gaps, particularly in philosophical domains
- Findings may not generalize beyond the specific Kialo topics and time period studied, limiting external validity

## Confidence

- AI moderation effectiveness assessment: **Medium** - Recent models show improvement but longitudinal validation is limited
- Bias comparison methodology: **Medium** - Valid approach but circular dependency in using ChatGPT for classification
- Semantic diversity as knowledge proxy: **Medium** - May conflate style with substance, especially in philosophy

## Next Checks

1. Cross-validate argument classifications using human annotators rather than ChatGPT to eliminate circular dependency and verify classification accuracy
2. Test the same methodology on non-Kialo controversial topics to assess generalizability beyond the debating platform corpus
3. Conduct a longitudinal study with monthly sampling of model responses to track how moderation policies evolve over time and whether detected biases are consistent