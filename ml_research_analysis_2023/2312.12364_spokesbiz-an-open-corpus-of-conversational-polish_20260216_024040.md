---
ver: rpa2
title: SpokesBiz -- an Open Corpus of Conversational Polish
arxiv_id: '2312.12364'
source_url: https://arxiv.org/abs/2312.12364
tags:
- cbiz
- spokesbiz
- corpus
- speakers
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The SpokesBiz corpus is a 650+ hour conversational Polish speech
  dataset with manual transcriptions, diarization, and speaker metadata. It supports
  linguistic research and ASR evaluation.
---

# SpokesBiz -- an Open Corpus of Conversational Polish

## Quick Facts
- arXiv ID: 2312.12364
- Source URL: https://arxiv.org/abs/2312.12364
- Reference count: 0
- Primary result: 650+ hour conversational Polish speech dataset with manual transcriptions, diarization, and speaker metadata

## Executive Summary
The SpokesBiz corpus is a comprehensive collection of conversational Polish speech spanning 650+ hours, featuring manual transcriptions, diarization, and detailed speaker metadata. The corpus supports both Automatic Speech Recognition (ASR) evaluation and linguistic research, particularly for analyzing demographic variations in speech patterns. ASR testing reveals significant performance differences across speech domains, ranging from 15.2% WER for student presentations to 26% for podcasts, underscoring the importance of diverse test data for robust evaluation.

## Method Summary
The corpus was created through a multi-stage process: initial data collection across seven conversational domains, automatic diarization and ASR processing using Voicelab and Whisper systems with pyannote diarization, manual transcription correction following DiaBiz guidelines, time alignment of audio and text, and comprehensive speaker metadata annotation. For linguistic analysis, the researchers extracted 30K utterance samples from a balanced subcorpus of 150 male and 150 female speakers to analyze fundamental frequency patterns using Parselmouth.

## Key Results
- ASR performance varies significantly across domains: 15.2% WER for student presentations versus 26% WER for podcasts
- Gender-based fundamental frequency analysis shows females average 216.56 kHz versus 157.16 kHz for males
- The corpus enables demographic linguistic analysis through time-aligned speaker metadata
- Manual transcription correction with punctuation and casing improves downstream NLP applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diverse conversational data improves ASR robustness across real-world scenarios
- Mechanism: By collecting speech from multiple domains (biographical interviews, podcasts, presentations, casual conversations), the corpus exposes ASR models to varied acoustic conditions, speaking styles, and vocabulary
- Core assumption: ASR performance degradation occurs when models trained on homogeneous data encounter out-of-domain speech patterns
- Evidence anchors:
  - [abstract] "ASR testing shows WER ranging from 15.2% for student presentations to 26% for podcasts, highlighting the need for diverse test data"
  - [section] "This result lends weight to the more general observation that officially reported WER rates for ASR solutions should be taken with a grain of salt as they may vary significantly for different types of spoken language"
- Break condition: If the domain distribution becomes too skewed (e.g., 90% podcasts), the diversity benefit diminishes and creates new bias

### Mechanism 2
- Claim: Speaker metadata enables sociolectal linguistic analysis that would be impossible with anonymous recordings
- Mechanism: Time-aligned metadata linking words to specific speakers by age, gender, and education allows researchers to study systematic linguistic variation patterns across demographic groups
- Core assumption: Linguistic variation correlates with demographic factors and can be extracted from large conversational datasets
- Evidence anchors:
  - [section] "The availability of utterance-level speaker metadata in SpokesBiz makes it possible to run various corpus linguistic analyses on this dataset"
  - [section] "Using 100 utterance samples from 150 male & 150 female speakers we created a balanced subcorpus of 30k utterance segments. We then computed the fundamental frequency (F0) for each segment"
- Break condition: If speaker metadata contains significant errors or if the sample size per demographic group becomes too small

### Mechanism 3
- Claim: Manual transcription correction with punctuation and casing improves downstream NLP applications
- Mechanism: Human-corrected transcriptions with proper casing and punctuation provide cleaner training data for language models and improve the quality of subsequent ASR fine-tuning
- Core assumption: Automatic speech recognition systems benefit from properly formatted text training data
- Evidence anchors:
  - [abstract] "The transcribed recordings have been diarized and manually annotated for punctuation and casing"
  - [section] "The automatic transcripts were manually corrected and punctuated using guidelines developed for the DiaBiz corpus"
- Break condition: If the manual annotation process introduces systematic biases or if the cost of annotation becomes prohibitive

## Foundational Learning

- Concept: Automatic Speech Recognition evaluation metrics (WER, MER, WIL)
  - Why needed here: The paper evaluates ASR performance using these metrics, so understanding them is crucial for interpreting results
  - Quick check question: If an ASR system has 20% WER on a 100-word utterance, how many word errors occurred?

- Concept: Diarization and its role in multi-speaker conversations
  - Why needed here: The corpus contains multi-speaker recordings that required diarization for proper annotation and analysis
  - Quick check question: In a 10-minute conversation with 3 speakers, what information must diarization provide to enable speaker-specific analysis?

- Concept: Fundamental frequency analysis and gender differences in speech
  - Why needed here: The paper demonstrates gender-based F0 differences, requiring understanding of acoustic analysis techniques
  - Quick check question: If female speakers average 216.56 kHz and male speakers 157.16 kHz, what is the approximate ratio of their average F0 values?

## Architecture Onboarding

- Component map: Data collection -> Automatic diarization & ASR -> Manual correction platform -> Time alignment -> Metadata annotation -> Distribution
- Critical path: Data collection -> Automatic processing -> Manual correction -> Time alignment -> Metadata -> Release
- Design tradeoffs: Manual vs automatic transcription accuracy, data privacy vs accessibility, domain diversity vs dataset size
- Failure signatures: High WER variance across domains, metadata annotation errors, time alignment misalignment, transcription correction inconsistencies
- First 3 experiments:
  1. Evaluate baseline ASR performance on each subcorpus separately to identify domain-specific weaknesses
  2. Test speaker diarization accuracy on multi-speaker recordings to assess metadata reliability
  3. Run gender-based F0 analysis on a small balanced sample to verify the corpus supports demographic linguistic studies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific technical challenges and solutions for automatically diarizing and transcribing spontaneous conversational Polish speech with multiple speakers?
- Basis in paper: [explicit] The paper mentions using a combination of ASR systems (Voicelab and Whisper) paired with pyannote diarization tool, but doesn't provide detailed analysis of the technical challenges or specific solutions implemented.
- Why unresolved: The paper only briefly mentions the technical approach without discussing the difficulties encountered or specific innovations developed for this particular language and speech type.
- What evidence would resolve it: Detailed technical documentation of the diarization and transcription pipeline, including error analysis, challenges encountered, and specific adaptations made for Polish conversational speech.

### Open Question 2
- Question: How does the fundamental frequency (F0) distribution vary across different age groups and speaking styles within the SpokesBiz corpus?
- Basis in paper: [inferred] The paper presents gender-based F0 differences but doesn't explore age-related variations or differences across the various subcorpora (interviews, podcasts, presentations).
- Why unresolved: The analysis only compares male vs female speakers without examining potential age-related patterns or style-dependent variations in pitch.
- What evidence would resolve it: Comprehensive analysis of F0 distributions across all age groups and speaking contexts within the corpus, with statistical testing of differences.

### Open Question 3
- Question: What are the specific linguistic and acoustic features that make certain subcorpora (like podcasts) more challenging for ASR systems than others (like student presentations)?
- Basis in paper: [explicit] The paper notes that podcasts have higher WER (26%) compared to student presentations (15.2%) but doesn't analyze the specific features causing these differences.
- Why unresolved: The paper identifies the performance gap but doesn't provide linguistic or acoustic analysis of what makes certain speech types more challenging.
- What evidence would resolve it: Detailed acoustic analysis comparing the different subcorpora, including analysis of speech rate, overlap, vocabulary, pronunciation variation, and other features that might affect ASR performance.

## Limitations
- Domain adaptation remains a significant challenge with WER varying from 15.2% to 26% across speech types
- Gender-based F0 analysis relies on a relatively small balanced sample of 30K utterances from only 300 speakers
- Manual annotation process lacks detailed validation procedures to quantify inter-annotator agreement

## Confidence

- **High Confidence**: The corpus creation methodology and basic metadata structure are well-documented and reproducible. The technical approach to F0 analysis using Parselmouth is standard and reliable.
- **Medium Confidence**: ASR performance metrics are reported with standard measures (WER, MER, WIL), but the evaluation setup (Whisper large-v2 configuration, preprocessing details) is underspecified. The linguistic analysis demonstrates clear patterns but the sample size limits broader claims.
- **Low Confidence**: Cross-domain ASR generalization claims and the extent to which metadata enables robust sociolectal analysis remain largely theoretical without extensive validation studies.

## Next Checks
1. Conduct inter-annotator agreement studies on a subset of transcriptions to quantify manual annotation quality and consistency.
2. Perform speaker diarization accuracy assessment on multi-speaker recordings to verify metadata reliability for linguistic analysis.
3. Test ASR model performance on cross-domain adaptation by training on one subcorpus (e.g., biographical interviews) and evaluating on others (e.g., podcasts) to quantify domain transfer limitations.