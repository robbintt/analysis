---
ver: rpa2
title: Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal
  Modeling
arxiv_id: '2310.12421'
source_url: https://arxiv.org/abs/2310.12421
tags:
- bias
- test
- train
- causal
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using causal modeling to detect and mitigate
  algorithmic bias in binary classification. The approach involves treating the prediction
  model as a black box and building a causal model to identify and correct bias.
---

# Detecting and Mitigating Algorithmic Bias in Binary Classification using Causal Modeling

## Quick Facts
- arXiv ID: 2310.12421
- Source URL: https://arxiv.org/abs/2310.12421
- Authors: 
- Reference count: 40
- Key outcome: Causal modeling can detect and mitigate algorithmic bias in binary classification, improving fairness metrics while slightly improving accuracy

## Executive Summary
This paper proposes a causal modeling approach to detect and mitigate algorithmic bias in binary classification by treating the prediction model as a black box. Using the Adult dataset, the authors demonstrate that gender bias exists in a logistic regression model predicting income levels. The approach involves building a path model to quantify bias through path coefficients and then correcting predictions by removing the direct path from protected attributes to predictions. The method shows improved fairness (narrower disparity in equal opportunity) while slightly improving overall accuracy from 84.66% to 84.72% on training data and from 84.55% to 84.56% on test data.

## Method Summary
The approach treats the prediction model as a black box and builds a causal path model to detect bias by regressing predictions on both the true outcome and protected attributes. If the path coefficient for the protected attribute is significantly non-zero, this indicates bias in the prediction model. The bias mitigation is achieved by removing the path coefficient of protected attributes from the prediction model output. The method uses the lavaan package in R to estimate path coefficients and applies the correction formula to create debiased predictions. The approach is evaluated on the Adult dataset using equal opportunity as the fairness metric and overall classification accuracy.

## Key Results
- Gender bias in logistic regression model detected through significant path coefficient (Œ≤ = 0.117, p < 0.05)
- Bias mitigation improved fairness with equal opportunity disparity reduced from 0.033 to 0.008
- Overall classification accuracy improved slightly from 84.66% to 84.72% on training data and from 84.55% to 84.56% on test data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal modeling can detect algorithmic bias by quantifying path coefficients for protected attributes.
- Mechanism: The approach builds a path model where the prediction output is regressed on both the true outcome and protected attributes. If the path coefficient for the protected attribute is significantly non-zero, this indicates bias in the prediction model.
- Core assumption: The prediction model's output is a function of both the true outcome and the protected attribute, with no other unmeasured confounders.
- Evidence anchors:
  - [abstract] "We show that gender bias in the prediction model is statistically significant at the 0.05 level."
  - [section] "If Œ≤_a,yÃÇ is significantly different from zero, there is sufficient evidence to suggest that the prediction model has introduced bias."
- Break condition: If unmeasured confounders strongly influence both the prediction and the outcome, the path coefficient estimate becomes biased and unreliable.

### Mechanism 2
- Claim: Bias mitigation is achieved by removing the path coefficient of protected attributes from the prediction model output.
- Mechanism: Once bias is detected through significant path coefficients, the causal model provides a correction formula that subtracts the bias component (Œ≤_a,yÃÇ * a) from the original prediction, creating a debiased prediction.
- Core assumption: The causal model accurately captures the bias mechanism and that removing the direct path from protected attribute to prediction preserves the legitimate relationship between outcome and prediction.
- Evidence anchors:
  - [abstract] "We demonstrate the effectiveness of the causal model in mitigating gender bias by cross-validation."
  - [section] "By setting the coefficient of a to zero, we have the corrected prediction yÃÉ such that yÃÉ = 0.067 + 0.415y."
- Break condition: If the protected attribute legitimately influences the outcome through legitimate mechanisms, removing its effect may harm prediction accuracy or fairness in unintended ways.

### Mechanism 3
- Claim: Post-processing bias mitigation can improve both fairness metrics and overall accuracy simultaneously.
- Mechanism: By correcting the prediction output after model training, the approach narrows the disparity in equal opportunity between protected groups while maintaining or slightly improving overall classification accuracy.
- Core assumption: The relationship between the prediction, outcome, and protected attribute captured by the causal model remains stable across training and test data.
- Evidence anchors:
  - [abstract] "Furthermore, we show that the overall classification accuracy is improved slightly."
  - [section] "The overall classification accuracy is slightly improved to 0.8472" on training data and "is slightly improved to 0.8456" on test data.
- Break condition: If the prediction model captures complex nonlinear relationships that the simple linear causal model cannot represent, the correction may degrade both fairness and accuracy.

## Foundational Learning

- Concept: Path analysis in structural equation modeling
  - Why needed here: This is the core technique for building and estimating the causal model that detects and mitigates bias
  - Quick check question: What does a significant path coefficient from protected attribute to prediction indicate about bias?

- Concept: Binary classification threshold optimization
  - Why needed here: The approach requires determining optimal classification thresholds based on prediction scores and class distributions
  - Quick check question: How is the classification threshold determined when the prediction output is a probability estimate?

- Concept: Equal opportunity fairness metric
  - Why needed here: This metric is used to evaluate whether bias mitigation has successfully improved fairness between protected groups
  - Quick check question: What mathematical relationship must hold between protected groups for equal opportunity to be satisfied?

## Architecture Onboarding

- Component map: Prediction model (black box) -> Causal bias detection model -> Bias-mitigated predictions -> Evaluation metrics
- Critical path: Data preprocessing -> Logistic regression model training -> Causal model estimation -> Bias detection -> Bias mitigation -> Fairness evaluation
- Design tradeoffs: Simplicity of linear causal model vs. potential inability to capture complex bias mechanisms; post-processing approach vs. potential loss of information compared to in-processing methods
- Failure signatures: Significant path coefficients not detected despite known bias; degradation in overall accuracy after bias mitigation; inconsistent results between training and test data
- First 3 experiments:
  1. Run causal model on training data with known bias and verify detection of significant path coefficients
  2. Apply bias mitigation correction and verify reduction in equal opportunity disparity
  3. Compare overall accuracy before and after bias mitigation on both training and test data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed causal modeling approach perform when both the protected attribute (a) and target variable (y) are real-valued rather than binary?
- Basis in paper: [explicit] The authors explicitly state "Firstly, we will apply the same method on problems where ùëé and ùë¶ are real-valued" as a future research direction.
- Why unresolved: The current study focuses only on binary classification with binary income (y) and binary sex (a). The mathematical framework and path coefficients used in the current approach are designed for binary variables, and the extension to continuous variables requires new modeling techniques.
- What evidence would resolve it: Experimental results showing the performance of the causal modeling approach on datasets with continuous protected attributes and target variables, comparing bias detection/mitigation effectiveness and accuracy metrics against the binary case.

### Open Question 2
- Question: Can causal modeling be used to detect and mitigate biases that exist within the data itself, rather than just biases introduced by the prediction model?
- Basis in paper: [explicit] The authors mention "we will apply causal modeling to address biases that exist within the data" and reference Madras et al. [11] who discussed biases in medical data affecting both treatment and outcome.
- Why unresolved: The current study only addresses bias introduced by the prediction model. The authors acknowledge that biases can exist within the data (e.g., selection bias) but do not demonstrate how causal modeling can detect and correct for these data-level biases.
- What evidence would resolve it: A demonstration of causal modeling applied to data with known inherent biases (such as selection bias or confounding factors), showing how the model can detect these biases and provide corrected predictions that account for the data-level issues.

### Open Question 3
- Question: How effective is the causal modeling approach for detecting and mitigating algorithmic bias in multi-class classification problems?
- Basis in paper: [inferred] While the authors cite Preston Putzel and Scott Lee [10] who extend equalized odds/opportunity to multiclass fairness, they do not apply their causal modeling approach to multi-class problems. The current framework is designed for binary classification.
- Why unresolved: The path analysis and coefficient estimation methods used in the current approach are specifically designed for binary outcomes. Extending this to multiple classes would require new modeling techniques and potentially different statistical measures for fairness evaluation.
- What evidence would resolve it: Application of the causal modeling approach to multi-class classification datasets, demonstrating bias detection and mitigation effectiveness across multiple classes, and showing improved fairness metrics (e.g., equalized odds or equal opportunity) compared to the biased model.

## Limitations
- The causal model assumes linear relationships and may not capture complex nonlinear bias mechanisms
- The Adult dataset with binary gender categories may not generalize to more complex or nuanced scenarios
- The approach only addresses bias introduced by the prediction model, not inherent data biases

## Confidence
- **High Confidence**: The statistical detection of bias through path coefficient significance (p < 0.05) is well-established methodology
- **Medium Confidence**: The post-processing bias mitigation approach is intuitive and interpretable, though its generalizability to nonlinear models is uncertain
- **Medium Confidence**: The simultaneous improvement in both fairness and accuracy is promising but requires replication on diverse datasets

## Next Checks
1. Test the causal modeling approach on datasets with known nonlinear bias mechanisms to evaluate detection sensitivity
2. Compare performance against in-processing bias mitigation methods to assess whether post-processing represents an optimal approach
3. Apply the methodology to datasets with more than two protected attribute categories to evaluate scalability