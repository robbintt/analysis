---
ver: rpa2
title: 'Dynamic Neural Network is All You Need: Understanding the Robustness of Dynamic
  Mechanisms in Neural Networks'
arxiv_id: '2308.08709'
source_url: https://arxiv.org/abs/2308.08709
tags:
- attack
- exit
- adversarial
- fgsm
- density
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the robustness of Dynamic Neural Networks
  (DyNNs) against adversarial attacks. It evaluates transferability of attacks between
  static and dynamic models, finding higher transferability from DyNNs to SDNNs.
---

# Dynamic Neural Network is All You Need: Understanding the Robustness of Dynamic Mechanisms in Neural Networks

## Quick Facts
- arXiv ID: 2308.08709
- Source URL: https://arxiv.org/abs/2308.08709
- Reference count: 40
- This work investigates the robustness of Dynamic Neural Networks (DyNNs) against adversarial attacks.

## Executive Summary
This paper investigates the robustness of Dynamic Neural Networks (DyNNs) against adversarial attacks, comparing their performance with Static Deep Neural Networks (SDNNs). The study reveals that DyNNs exhibit higher attack transferability from DyNNs to SDNNs than vice versa. Additionally, DyNNs can generate adversarial examples more efficiently than SDNNs. The paper proposes a novel Early Attack to understand the additional attack surface introduced by dynamic mechanisms and provides design choices to improve robustness against this attack.

## Method Summary
The study evaluates the robustness of DyNNs against adversarial attacks using CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets. Models include VGG-16, ResNet56, and MobileNet as both static and early-exit DyNNs. FGSM, PGD, and MI-FGSM attacks are used for white-box and black-box scenarios. For black-box attacks, surrogate models are trained using corrupted validation data. The paper proposes an Early Attack method to evaluate the additional attack surface in DyNNs. Transferability success rates, attack success rates, and efficiency robustness (measured by exit number changes) are the key metrics.

## Key Results
- D2S transferability is higher than S2D transferability
- DyNNs as surrogate models are more efficient and effective for generating adversarial examples
- Accuracy-based adversarial samples do not significantly decrease DyNN efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic Neural Networks (DyNNs) show higher attack transferability from DyNN to Static DNN (SDNN) than vice versa
- Mechanism: DyNNs use fewer parameters and lower variance, creating a smaller adversarial feature space that makes adversarial examples generated on DyNNs more transferable to SDNNs
- Core assumption: The smaller feature space in DyNNs makes their adversarial examples more generalizable to larger models
- Evidence anchors:
  - [abstract] "We find that attack transferability from DyNNs to SDNNs is higher than attack transferability from SDNNs to DyNNs"
  - [section] "DyNNs use lower number of parameters, hence the feature space for adversarial samples of DyNNs is smaller than the feature space for adversarial samples of SDNNs"
  - [corpus] No direct evidence found in corpus

### Mechanism 2
- Claim: DyNNs can generate adversarial examples more efficiently than SDNNs
- Mechanism: DyNNs require fewer computational layers to generate adversarial examples due to their early-exit structure
- Core assumption: The dynamic mechanism allows faster convergence in generating adversarial examples
- Evidence anchors:
  - [abstract] "we find that DyNNs can be used to generate adversarial samples more efficiently than SDNNs"
  - [section] "we can find the probability density plots of different exit numbers of DyNN that have been used to generate adversarial examples. Lower exit number suggests that lesser number of computations has been used"
  - [corpus] No direct evidence found in corpus

### Mechanism 3
- Claim: Accuracy-based adversarial samples do not significantly decrease DyNN efficiency
- Mechanism: DyNNs maintain computational efficiency because adversarial examples don't consistently force them to use later exits
- Core assumption: The dynamic mechanism's confidence thresholds prevent efficiency degradation
- Evidence anchors:
  - [abstract] "Accuracy-based adversarial samples do not significantly decrease DyNN efficiency"
  - [section] "for the majority of the scenarios, accuracy-based adversarial samples do not increase the computation significantly in the DyNN"
  - [corpus] No direct evidence found in corpus

## Foundational Learning

- Concept: Adversarial attack transferability
  - Why needed here: Understanding how attacks move between static and dynamic models is central to evaluating robustness
  - Quick check question: What is the difference between D2S and S2D transferability in this context?

- Concept: Dynamic neural network architecture
  - Why needed here: Early-exit mechanisms are the core feature being evaluated for robustness
  - Quick check question: How does an early-exit DyNN differ from a static DNN in terms of computational flow?

- Concept: Adversarial example generation efficiency
  - Why needed here: The paper evaluates whether DyNNs are more efficient at generating adversarial examples
  - Quick check question: What metric would you use to measure the efficiency of adversarial example generation?

## Architecture Onboarding

- Component map:
  - Static DNN: Fixed computation path through all layers
  - Dynamic Neural Network: Multiple exits with confidence thresholds, allowing early termination
  - Adversarial attack framework: White-box (PGD, FGSM) and black-box (surrogate model) attacks

- Critical path: Understanding how dynamic mechanisms affect attack transferability and efficiency robustness
- Design tradeoffs: Fewer parameters in DyNNs vs. potential loss of accuracy; efficiency vs. robustness
- Failure signatures: If adversarial examples consistently force later exits, efficiency is compromised; if DyNN-generated attacks don't transfer well to SDNNs, the efficiency advantage is questionable
- First 3 experiments:
  1. Measure attack success rates for D2S and S2D transferability using PGD and FGSM attacks
  2. Compare the computational efficiency of generating adversarial examples on DyNNs vs. SDNNs
  3. Test how adversarial examples affect the exit layer selection in DyNNs (efficiency robustness)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the robustness of DyNNs against adversarial attacks scale with dataset complexity and model architecture size?
- Basis in paper: [inferred] The paper shows that CIFAR-100 data (larger model) is more vulnerable to adversarial attacks than CIFAR-10 data (smaller model), and that adversarial examples generated on larger models are more likely to decrease efficiency in DyNNs.
- Why unresolved: The study only compares two datasets and a limited set of models. The relationship between dataset complexity, model architecture size, and robustness against adversarial attacks needs further exploration.
- What evidence would resolve it: Conducting experiments with a wider range of datasets and model architectures to establish a clear relationship between dataset complexity, model size, and robustness against adversarial attacks.

### Open Question 2
- Question: Can the Early Attack method be further improved to achieve higher success rates and better transferability between DyNNs?
- Basis in paper: [explicit] The paper proposes a novel Early Attack method but notes that its transferability between DyNNs is not significant and that it fails for some model-dataset pairs.
- Why unresolved: The paper does not explore alternative optimization strategies or modifications to the Early Attack method that could improve its success rate and transferability.
- What evidence would resolve it: Experimenting with different loss functions, optimization algorithms, or hyperparameters for the Early Attack method to improve its performance and transferability.

### Open Question 3
- Question: How does the placement of early exits in DyNNs affect their robustness against different types of adversarial attacks?
- Basis in paper: [explicit] The paper finds that having the first exit in earlier layers can increase the robustness of early exits in DyNNs, but does not explore the impact of exit placement on robustness against various attack types.
- Why unresolved: The study only considers the impact of exit placement on robustness against white-box and black-box attacks generated on SDNNs. The effect of exit placement on robustness against other attack types (e.g., natural corruptions, universal adversarial perturbations) remains unexplored.
- What evidence would resolve it: Conducting experiments with different exit placements and various attack types to determine the optimal exit configuration for maximizing robustness against diverse adversarial threats.

## Limitations

- No empirical evidence from other studies to support the proposed mechanisms
- Critical hyperparameters for attacks and surrogate model training are unspecified
- The Early Attack mechanism's effectiveness against real-world threats remains unverified

## Confidence

- **Low confidence**: All three proposed mechanisms lack independent validation from the broader research community
- **Medium confidence**: The observed phenomena (D2S vs S2D transferability differences) appear consistent with the paper's findings
- **Low confidence**: The proposed efficiency advantages require verification through independent experiments

## Next Checks

1. **Cross-validation of transferability findings**: Replicate the D2S and S2D transferability experiments using independent datasets and attack implementations to verify the observed asymmetry
2. **Efficiency measurement standardization**: Conduct controlled experiments measuring computational efficiency with standardized metrics (FLOPs, wall-clock time) across multiple DyNN architectures
3. **Robustness stress testing**: Evaluate how adversarial examples affect exit layer selection across different confidence threshold configurations to validate the efficiency robustness claim