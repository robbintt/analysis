---
ver: rpa2
title: 'BadVFL: Backdoor Attacks in Vertical Federated Learning'
arxiv_id: '2304.08847'
source_url: https://arxiv.org/abs/2304.08847
tags:
- attack
- training
- backdoor
- participants
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces BadVFL, a clean-label backdoor attack against
  vertical federated learning (VFL). The authors propose a two-phase attack: first,
  a label inference phase to estimate true labels using auxiliary data, then a backdoor
  injection phase where poisoned samples are crafted by perturbing target-class instances
  to move them closer to trigger-embedded source-class instances in feature space.'
---

# BadVFL: Backdoor Attacks in Vertical Federated Learning

## Quick Facts
- **arXiv ID**: 2304.08847
- **Source URL**: https://arxiv.org/abs/2304.08847
- **Reference count**: 40
- **Primary result**: Introduces BadVFL, achieving 89% attack success rate with 5% poisoning budget while degrading main task accuracy by only 5%

## Executive Summary
This paper introduces BadVFL, a clean-label backdoor attack against vertical federated learning (VFL) systems. The attack operates in two phases: first inferring true labels using auxiliary data through model extraction, then crafting poisoned samples by perturbing target-class instances to align with trigger-embedded source-class instances in feature space. Experiments across CIFAR-10, CIFAR-100, and CINIC-10 datasets demonstrate high attack success rates (up to 89%) with minimal impact on main task accuracy (5% degradation). The attack remains effective in multi-party and multi-adversary settings, and even when only one of ten participants is malicious.

## Method Summary
BadVFL executes a two-phase attack in VFL systems. First, the adversary uses auxiliary data with labels to train a surrogate top model, which approximates the true top model's behavior. This surrogate is then used to infer labels of the true training data hosted by the malicious participant. In the second phase, the attacker selects source and target classes, creates triggered versions of source-class data, and optimizes perturbations to move target-class data closer to triggered source-class embeddings in feature space. The attack uses saliency maps to identify optimal trigger insertion locations and employs L2 norm constraints to maintain poisoned sample plausibility.

## Key Results
- Achieves 89% attack success rate with only 5% poisoning budget
- Degrades main task accuracy by just 5% during attack
- Remains effective in multi-party and multi-attacker settings
- Differential privacy and anomaly detection can mitigate attacks but with limitations

## Why This Works (Mechanism)

### Mechanism 1: Label Inference via Auxiliary Data
The adversary reconstructs true labels by training a surrogate top model using auxiliary data with known labels. This surrogate approximates the true top model's decision boundaries, enabling label inference without direct access to the true labels. The method assumes auxiliary data shares the same feature distribution and label space as the true training data.

### Mechanism 2: Feature Space Manipulation
After label inference, the attacker crafts poisoned samples by perturbing target-class instances to minimize distance between their feature embeddings and those of trigger-embedded source-class instances. This optimization ensures the backdoored model learns to associate trigger patterns with the target class in the shared feature space.

### Mechanism 3: Saliency-Based Trigger Injection
The adversary computes Jacobian-based saliency maps to identify optimal trigger insertion locations. By selecting the sliding window with highest average gradient magnitudes, the attack places triggers in areas most sensitive to classification loss, improving effectiveness compared to random placement.

## Foundational Learning

- **Federated Learning taxonomy (Horizontal vs Vertical)**: VFL has participants with disjoint feature subsets rather than disjoint samples, creating different security vulnerabilities. *Quick check*: In VFL, what is the relationship between participants' data ownership compared to HFL?

- **Split learning architecture**: VFL with model splitting divides the model into bottom models (hosted by participants) and top model (hosted by server). *Quick check*: In the model splitting setting, which component hosts the class labels and which hosts the feature transformation?

- **Backdoor attacks in centralized learning**: BadVFL adapts clean-label backdoor attack strategies from centralized learning to the VFL setting. *Quick check*: How do traditional clean-label backdoor attacks differ from dirty-label attacks in terms of data poisoning strategy?

## Architecture Onboarding

- **Component map**: Participant 1 → Bottom Model 1 → Server (Top Model, Labels) ← Participant 2 ← Bottom Model 2
- **Critical path**: Auxiliary data → Surrogate model training → Label inference → Source/target class selection → Saliency map computation → Optimization → Perturbed data submission
- **Design tradeoffs**: Larger auxiliary datasets improve label inference but increase computational overhead; larger triggers improve attack success but may impact main task accuracy more significantly.
- **Failure signatures**: Poor label inference accuracy manifests as low attack success rates; excessive perturbation manifests as degraded main task accuracy; failed saliency map optimization manifests as random trigger insertion performance.
- **First 3 experiments**:
  1. Run BadVFL with 10 rounds of label inference on CIFAR-10 to measure label inference accuracy and determine optimal Rn value
  2. Execute BadVFL with random vs optimal source/target class selection to quantify ASR improvement and MTA impact
  3. Test BadVFL with different trigger window sizes (3×3 vs 5×5) while keeping other parameters constant to measure attack strength vs utility tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can BadVFL be extended to work without labeled auxiliary data?
- **Basis**: "We will investigate how to extend BadVFL to remove the dependency on the label inference phase and work in an auxiliary data-free way."
- **Why unresolved**: Acknowledged as a limitation with clustering suggested as potential approach but not implemented or evaluated.
- **What evidence would resolve it**: Experiments comparing BadVFL with and without auxiliary data, demonstrating comparable attack success rates.

### Open Question 2
- **Question**: What is the optimal Rn value selection strategy that accounts for auxiliary dataset size, poisoning budget, and window size?
- **Basis**: "We will ground the association between the accuracy of label estimation and the attack performance on theoretical analysis."
- **Why unresolved**: Paper empirically shows Rn impacts performance but doesn't provide principled method for determining it.
- **What evidence would resolve it**: A mathematical framework or algorithm that determines Rn based on dataset characteristics and attack parameters.

### Open Question 3
- **Question**: How effective are anomaly detection defenses when the server doesn't know the poisoning budget?
- **Basis**: "We explore this idea experimentally... The value of Rn is set to 60. We provide the server with the advantage of knowledge regarding the poisoning budget value."
- **Why unresolved**: Assumes server knows poisoning budget, which may not be realistic in practice.
- **What evidence would resolve it**: Experiments where anomaly detection is applied without prior knowledge of poisoning budget, measuring effectiveness compared to when budget is known.

## Limitations

- Assumes auxiliary data shares sufficient distributional similarity with true training data, which isn't empirically tested across diverse distributions
- Perturbation optimization assumes consistent feature space relationships that may break down with complex architectures
- Limited comparative analysis of saliency-based trigger placement against other strategies

## Confidence

*Mechanism 1 (Label Inference)*: Medium confidence - Sound methodology but heavily dependent on auxiliary data quality assumptions
*Mechanism 2 (Feature Space Manipulation)*: Medium confidence - Well-defined optimization but assumes universal feature space properties
*Mechanism 3 (Saliency-Based Trigger Injection)*: Low confidence - Novel approach but limited comparative analysis and effectiveness may vary significantly

## Next Checks

1. **Distribution Shift Analysis**: Test BadVFL's label inference accuracy when auxiliary data comes from a different but related distribution (e.g., using CIFAR-100 auxiliary data for CIFAR-10 attack) to quantify robustness to distribution mismatch.

2. **Architecture Dependency Study**: Implement BadVFL against different top model architectures (e.g., ResNet-50, Vision Transformer) to measure how feature space manipulation effectiveness varies with model complexity.

3. **Defense Efficacy Under Attack**: Systematically evaluate the differential privacy defense by varying privacy budgets (ε values) while measuring both attack success rate degradation and main task accuracy preservation across all three datasets.