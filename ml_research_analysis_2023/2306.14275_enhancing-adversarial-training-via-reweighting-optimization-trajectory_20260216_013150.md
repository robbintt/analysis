---
ver: rpa2
title: Enhancing Adversarial Training via Reweighting Optimization Trajectory
arxiv_id: '2306.14275'
source_url: https://arxiv.org/abs/2306.14275
tags:
- adversarial
- robust
- training
- wot-b
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of robust overfitting in adversarial
  training, where robust accuracy degrades severely after initial learning rate decay.
  The authors propose Weighted Optimization Trajectories (WOT), a method that refines
  historical optimization trajectories by re-weighting them to improve robust generalization.
---

# Enhancing Adversarial Training via Reweighting Optimization Trajectory

## Quick Facts
- arXiv ID: 2306.14275
- Source URL: https://arxiv.org/abs/2306.14275
- Reference count: 40
- This paper proposes Weighted Optimization Trajectories (WOT) to address robust overfitting in adversarial training by re-weighting historical optimization trajectories to improve robust generalization.

## Executive Summary
This paper addresses the critical problem of robust overfitting in adversarial training, where models experience severe degradation in robust accuracy after initial learning rate decay. The authors propose Weighted Optimization Trajectories (WOT), a novel method that collects historical optimization trajectories during training and learns optimal weights for these trajectories to maximize robust accuracy on an unseen hold-out set. By refining these trajectories rather than simply averaging weights, WOT finds parameter combinations that lie in flatter regions of the loss landscape, which correlates with better robust generalization. The method seamlessly integrates with existing adversarial training variants including AT, Trades, MART, and AWP.

## Method Summary
WOT works by periodically collecting model weights during adversarial training at regular intervals, then learning optimal weights (α) for these historical trajectories to maximize robust accuracy on an unseen hold-out set. The optimization of α values is performed using SGD with momentum, constrained to prevent the refined trajectories from deviating too far from the original paths. The method can operate in two modes: WOT-W, which learns one weight per trajectory, and WOT-B, which further breaks down each trajectory into blocks corresponding to different parts of the network architecture, allowing block-specific weighting. After optimizing the weights, the refined model parameters are computed and training continues with these improved trajectories.

## Key Results
- WOT improves AT-PGD robust accuracy under AA-L∞ attack by 1.53% to 6.11% across multiple datasets
- WOT consistently improves clean accuracy by 0.55% to 5.47% while enhancing robust performance
- WOT completely prevents robust overfitting, maintaining stable performance throughout training where other methods show significant degradation
- The method works seamlessly with multiple adversarial training variants including AT, Trades, MART, and AWP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WOT improves robust generalization by refining optimization trajectories to converge to flatter loss valleys
- Mechanism: By collecting historical optimization steps and learning weights to maximize robust accuracy on an unseen hold-out set, WOT finds parameter combinations that lie in flatter regions of the loss landscape, which empirical studies have shown correlates with better robust generalization
- Core assumption: Flatter loss landscapes correspond to better robust generalization and reduced overfitting
- Evidence anchors:
  - "we propose a new approach -Weighted Optimization Trajectories(briefly WOT) for the first time showing that we can largely improve the flatness of solutions of existing adversarial training variants by periodically refining a set of historical optimization trajectories"
  - "Our results show that WOT integrates seamlessly with the existing adversarial training methods and consistently overcomes the robust overfitting issue, resulting in better adversarial robustness"
  - Limited direct evidence in corpus - this is primarily supported by the paper's own experiments

### Mechanism 2
- Claim: WOT prevents robust overfitting by stopping weight updates with unexpected magnitudes after a certain point in training
- Mechanism: The learned weights (α) decrease to very small values after certain epochs, which indicates WOT stops the model's weights from updating with unexpected magnitudes, preventing the occurrence of robust overfitting
- Core assumption: Large, unexpected weight updates in later training stages contribute to robust overfitting
- Evidence anchors:
  - "From the first and second figures of Figure 4, we observe that the mean ofα decreases to a very small value after 150,100 epochs for PreRN-18 and WRN-34-10 respectively. The small mean ofα indicates that WOT stops the model's weights from updating with unexpected magnitudes, which prevents the occurrence of robust overfitting"
  - "WOT completely prevents robust overfitting, maintaining stable performance throughout training where other methods show significant degradation"
  - Limited corpus evidence - this mechanism is primarily demonstrated through the paper's experiments

### Mechanism 3
- Claim: Block-wise trajectory refinement (WOT-B) improves performance by allowing different learning rates for different blocks of the network
- Mechanism: WOT-B breaks down each trajectory into multiple blocks based on the model's block design and learns individual weights for each block, creating a larger learning space for refinement
- Core assumption: Different parts of the network benefit from different optimization trajectories, and treating them separately allows better refinement
- Evidence anchors:
  - "The main difference between WOT-W and WOT-B is that WOT-W learns one α for each cached ∆w (the difference of parameters in two checkpoints) whereas WOT-B further breakdowns each cached∆w into several blocks... Therefore, the learning space of WOT-B is larger than WOT-W, leading to better performance in general"
  - "Experiments are conducted on CIFAR-10 with PreRN-18 based on WOT-B... Results in Figure 6c and Figure 6d show that the magnitude of learnedα are different among blocks. Specifically, WOT-B assigns a large value ofα for middle blocks... and a small value ofα for the bottom and top blocks... This indicates that assigning different weights for different blocks may play a crucial role in boosting adversarial robustness"
  - Limited corpus support - this specific block-wise approach appears novel to this paper

## Foundational Learning

- Concept: Adversarial training and robust overfitting
  - Why needed here: Understanding the baseline problem WOT addresses - vanilla adversarial training suffers from robust overfitting where robust accuracy degrades after initial learning rate decay
  - Quick check question: What is robust overfitting and why does it occur in standard adversarial training?

- Concept: Optimization trajectories and weight space
  - Why needed here: WOT fundamentally works by manipulating and re-weighting historical optimization trajectories in weight space
  - Quick check question: How are optimization trajectories defined in the context of this paper?

- Concept: Loss landscape curvature and generalization
  - Why needed here: The paper's hypothesis is that flatter loss landscapes lead to better robust generalization
  - Quick check question: What is the relationship between loss landscape curvature and model generalization according to prior research?

## Architecture Onboarding

- Component map: Trajectory collection -> Weight learning -> Trajectory refinement -> Integration
- Critical path:
  1. Standard adversarial training collects trajectory checkpoints
  2. After p epochs, start refining trajectories using unseen hold-out set
  3. Optimize α values via SGD with momentum
  4. Apply refined weights to create new model parameters
  5. Continue training with refined trajectories

- Design tradeoffs:
  - Memory vs. frequency: More frequent trajectory collection provides better refinement but requires more memory
  - Hold-out set size: Larger hold-out sets provide better optimization but reduce training data
  - Block granularity: Finer block division in WOT-B provides more learning space but increases complexity

- Failure signatures:
  - NaN loss values (as mentioned in ablation studies)
  - Degradation in robust accuracy when using certain hold-out set sizes
  - Inconsistent performance across different architectures if block division is inappropriate

- First 3 experiments:
  1. Run WOT-B with default settings (gaps=400, k=4) on CIFAR-10 with PreActResNet-18 to verify baseline improvement
  2. Test WOT with different hold-out set sizes (1000, 2000, 4000) to find optimal configuration
  3. Compare WOT-W vs WOT-B to verify block-wise refinement provides benefit

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal strategy for selecting the hold-out set size in WOT, balancing between having enough data to properly evaluate robust performance and not reducing the training set size too much?
- Basis in paper: The paper discusses how WOT uses an unseen hold-out set to refine optimization trajectories and shows that robust accuracy decreases as the hold-out set size increases from 1000 to 8000 samples, conjecturing that a larger hold-out set reduces the training set size and deteriorates optimization trajectories.
- Why unresolved: The paper only provides a limited set of experiments showing the effect of hold-out set size on robust accuracy, without exploring the optimal size or considering different dataset sizes and architectures.
- What evidence would resolve it: Experiments varying the hold-out set size across different datasets and architectures, and measuring the impact on both robust accuracy and training efficiency.

### Open Question 2
- Question: How does the choice of constraints on the weights α in WOT affect the final robust performance, and what is the optimal constraint strategy?
- Basis in paper: The paper mentions that constraining α to [0, 1] prevents the refined optimization trajectories from going too far from the original trajectories, but also shows that other constraint strategies like setting sum(α) = 1 or max(α) = 0.5/0.8 result in worse performance.
- Why unresolved: The paper only explores a few constraint strategies and does not provide a comprehensive analysis of how different constraints affect robust performance or offer guidance on choosing the optimal constraint strategy.
- What evidence would resolve it: Experiments comparing various constraint strategies on α across different datasets and architectures, and measuring the impact on robust accuracy and training stability.

### Open Question 3
- Question: How does WOT's performance generalize to other types of adversarial attacks beyond the ones tested in the paper, such as transfer-based or query-based attacks?
- Basis in paper: The paper evaluates WOT against several white-box and black-box attacks, including FGSM, PGD, C&W, and Autoattack, but does not explore other types of attacks like transfer-based or query-based attacks.
- Why unresolved: The paper's evaluation is limited to a specific set of attacks, and it is unclear how WOT would perform against other types of adversarial attacks that may have different characteristics and requirements.
- What evidence would resolve it: Experiments testing WOT against a broader range of adversarial attacks, including transfer-based and query-based attacks, and measuring the impact on robust accuracy and training efficiency.

## Limitations
- The method requires additional memory to store historical optimization trajectories, which could be prohibitive for very large models or datasets
- The effectiveness depends on the quality of the hold-out set, and using too large a hold-out set reduces training data and may deteriorate optimization trajectories
- The theoretical justification for why reweighting historical trajectories leads to better generalization remains somewhat empirical rather than analytically proven

## Confidence
- **High confidence**: WOT consistently improves robust accuracy across datasets and baselines; WOT prevents robust overfitting as measured by stable performance after learning rate decay
- **Medium confidence**: The mechanisms of flatness improvement and weight magnitude control effectively prevent overfitting; block-wise refinement provides additional benefits
- **Low confidence**: The theoretical guarantees for why these mechanisms work; generalizability to other domains beyond image classification

## Next Checks
1. **Cross-dataset validation**: Test WOT on non-image datasets (e.g., speech or text) to verify the method's generalizability beyond computer vision tasks
2. **Ablation on hold-out set size**: Systematically vary the hold-out set size (1000-5000 samples) to determine the optimal trade-off between refinement quality and training data preservation
3. **Theoretical analysis**: Develop a theoretical framework explaining why optimizing for flatness on a hold-out set leads to better robust generalization, potentially connecting to PAC-Bayes bounds or sharp minimum theory