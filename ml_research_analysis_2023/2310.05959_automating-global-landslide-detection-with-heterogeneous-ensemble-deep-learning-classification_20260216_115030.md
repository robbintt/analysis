---
ver: rpa2
title: Automating global landslide detection with heterogeneous ensemble deep-learning
  classification
arxiv_id: '2310.05959'
source_url: https://arxiv.org/abs/2310.05959
tags:
- ensemble
- landslide
- https
- landslides
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an automated global landslide detection system
  using heterogeneous ensemble deep-learning classification. The approach combines
  five segmentation models (Unet, Linknet, PSP-Net, PAN, DeepLab) with various loss
  functions and learning rates, evaluated on a diverse global dataset of 21 landslide
  case studies.
---

# Automating global landslide detection with heterogeneous ensemble deep-learning classification

## Quick Facts
- arXiv ID: 2310.05959
- Source URL: https://arxiv.org/abs/2310.05959
- Reference count: 3
- Primary result: Ensemble deep learning achieves F1-score of 0.69 for global landslide detection using Sentinel-1 and Sentinel-2 data

## Executive Summary
This study presents an automated global landslide detection system using heterogeneous ensemble deep-learning classification. The approach combines five segmentation models (Unet, Linknet, PSP-Net, PAN, DeepLab) with various loss functions and learning rates, evaluated on a diverse global dataset of 21 landslide case studies. The ensemble model achieved the highest F1-score of 0.69 when combining Sentinel-1 and Sentinel-2 data, with a 6.87% average improvement over single models when the ensemble size was 20. Sentinel-2 data alone also performed well, achieving an F1 score of 0.61 with a 14.59% improvement at ensemble size 20. These findings support the development of robust monitoring systems for hazard assessment and early warning.

## Method Summary
The method employs an ensemble of 9 segmentation architectures (Unet, Unet++, Linknet, FPN, PSP-Net, PAN, DeepLabV3, DeepLabV3+) trained with 5 loss functions and 2 learning rates across 4 data settings (Sentinel-1 only, Sentinel-2 only, combined, and all bands). The training pipeline uses 16 global case studies for model training, 4 for validation, and tests on a Norway case study. Models are trained using PyTorch/SMP framework with early stopping, and ensemble predictions are created by averaging the top-performing single models based on validation performance. Data preprocessing is performed using Google Earth Engine to generate Sentinel-1 SAR change images, Sentinel-2 dNDVI composites, terrain-derived bands, and binary landslide masks.

## Key Results
- Ensemble model achieved highest F1-score of 0.69 when combining Sentinel-1 and Sentinel-2 data
- Sentinel-2 bands only achieved F1 score of 0.61 with ensemble size of 20
- Optimal ensemble size identified as 10-20 models, with diminishing returns beyond 20 members
- Ensemble approach provided 6.87% average improvement over single best models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heterogeneous ensemble of segmentation models improves landslide detection accuracy compared to single models.
- Mechanism: Averaging predictions from multiple diverse segmentation architectures with different loss functions and learning rates reduces individual model biases and variance.
- Core assumption: The individual models make uncorrelated errors, so averaging will cancel out noise while preserving signal.
- Evidence anchors:
  - [abstract] "The ensemble model achieved the highest F1-score (0.69) when combining both Sentinel-1 and Sentinel-2 bands, with the highest average improvement of 6.87% when the ensemble size was 20."
  - [section] "All four settings showed a sensible improvement in F1 skill when compared to the single best validation model."

### Mechanism 2
- Claim: Sentinel-2 data alone can provide reliable landslide detection with high F1 score.
- Mechanism: The change in vegetation index (dNDVI) derived from Sentinel-2 effectively captures vegetation loss associated with landslides, providing clear spectral signatures.
- Core assumption: Landslide events cause detectable vegetation changes that are captured by dNDVI, and these changes are not obscured by cloud cover or seasonal variation.
- Evidence anchors:
  - [abstract] "Sentinel-2 bands only performed very well, with an F1 score of 0.61 when the ensemble size is 20 with an improvement of 14.59% when the ensemble size is 20."
  - [section] "Many of the landslides represented in red and magenta correspond to the existing landslides. However, these were not used to evaluate the test scores because of the lack of visibility of these landslides within the Sentinel-1 bands."

### Mechanism 3
- Claim: Optimal ensemble size exists between 10-20 models for this application.
- Mechanism: As ensemble size increases, the averaging effect improves performance up to a point, after which additional models add noise or redundancy without improvement.
- Core assumption: There's a diminishing returns relationship between ensemble size and performance, with optimal size balancing diversity and computational efficiency.
- Evidence anchors:
  - [section] "Our analysis indicates that the most favourable results were attained when employing an ensemble size between 10 and 20 best-performing single models... Beyond this threshold, a discernible shift toward underestimation became evident."
  - [section] "With the exclusion of the S1 setting, the use of ensemble sizes larger than 20 members determines a shift toward underestimation."

## Foundational Learning

- Concept: Ensemble learning principles
  - Why needed here: Understanding how combining multiple models can improve prediction accuracy and reduce overfitting is crucial for implementing the ensemble approach.
  - Quick check question: Why does averaging predictions from multiple models typically improve performance compared to using a single model?

- Concept: Semantic segmentation with deep learning
  - Why needed here: The study uses fully convolutional neural networks (FCNNs) for pixel-level classification of landslides, requiring understanding of segmentation architectures.
  - Quick check question: What distinguishes semantic segmentation from image classification, and why are FCNNs well-suited for this task?

- Concept: Loss functions for imbalanced datasets
  - Why needed here: Landslide detection involves severe class imbalance (landslide pixels vs. non-landslide pixels), requiring specialized loss functions to handle this.
  - Quick check question: Why are standard cross-entropy loss functions often inadequate for imbalanced segmentation tasks, and what alternatives exist?

## Architecture Onboarding

- Component map: Data preprocessing pipeline (GEE-based SAR/optical processing) → Model training framework (PyTorch/SMP) → Ensemble aggregation → Evaluation pipeline
- Critical path: Data preparation → Model training with early stopping → Ensemble creation → Performance evaluation on test set
- Design tradeoffs: Sentinel-1 provides cloud penetration but speckle noise; Sentinel-2 provides clear vegetation change signals but requires cloud-free conditions; ensemble size vs. computational cost
- Failure signatures: Poor performance on validation set suggests overfitting; degradation with larger ensembles suggests correlation between models; high false positives in water bodies suggest Sentinel-2 limitations
- First 3 experiments:
  1. Train single Unet model with Sentinel-1 only data and BCELoss to establish baseline performance
  2. Create 5-model ensemble using best-performing configurations from each setting to test ensemble benefit
  3. Compare ensemble performance at sizes 5, 10, 15, 20, 40 to identify optimal ensemble size threshold

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ensemble size for landslide detection models across different environmental conditions and landslide types?
- Basis in paper: [explicit] The paper states that optimal ensemble size is between 10-20 models, with diminishing returns beyond 20 members, but this was based on limited case studies
- Why unresolved: The study only tested 21 case studies across various conditions, which may not be representative of all global landslide scenarios
- What evidence would resolve it: Extensive testing across thousands of diverse landslide events globally, varying by climate zone, terrain type, and trigger mechanism

### Open Question 2
- Question: How do different Sentinel-1 polarization combinations (VV, VH, or both) affect landslide detection accuracy in different terrain types?
- Basis in paper: [inferred] The study used both VV and VH polarizations but didn't systematically test their individual contributions or optimal combinations for different terrains
- Why unresolved: The analysis combined both polarizations without investigating their separate or optimal combined performance across different geological settings
- What evidence would resolve it: Controlled experiments testing each polarization type separately and in combination across various terrain types (rocky, soil, vegetation-covered)

### Open Question 3
- Question: Can the ensemble model reliably distinguish between true landslide signals and other ground disturbances (e.g., construction, agriculture) in complex landscapes?
- Basis in paper: [explicit] The study notes challenges in distinguishing landslide signals from vegetation loss due to other causes, and mentions false positives in water bodies
- Why unresolved: The evaluation focused on known landslide events but didn't test model performance on false alarm scenarios or competing ground disturbance types
- What evidence would resolve it: Comparative testing of the model on areas with known non-landslide ground disturbances versus true landslide events

## Limitations
- Test set limited to single location (Norway) may not represent global landslide diversity
- Heterogeneous ensemble approach requires significant computational resources (90 models per setting)
- Reliance on Sentinel-2 data introduces sensitivity to cloud cover and seasonal vegetation changes
- Manual mapping of landslide extents introduces potential human bias in ground truth data

## Confidence
- Ensemble performance claims (F1=0.69): **High** - Supported by clear validation and test results with multiple ensemble sizes
- Sentinel-2 only performance (F1=0.61): **Medium** - Based on single test location; requires broader geographic validation
- Optimal ensemble size (10-20 models): **Medium** - Threshold appears well-defined but depends on specific validation data characteristics

## Next Checks
1. Test the ensemble model on multiple diverse geographic locations beyond Norway to assess true global generalizability
2. Evaluate performance under varying cloud cover conditions to quantify Sentinel-2 limitations
3. Conduct ablation studies removing Sentinel-1 vs. Sentinel-2 to determine individual contribution to ensemble performance