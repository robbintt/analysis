---
ver: rpa2
title: Complementary and Integrative Health Lexicon (CIHLex) and Entity Recognition
  in the Literature
arxiv_id: '2305.17353'
source_url: https://arxiv.org/abs/2305.17353
tags:
- terms
- umls
- cihlex
- concepts
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study constructed the CIHLex, a lexicon of 198 unique concepts
  and 1090 unique terms focused on physical and psychological complementary and integrative
  health (CIH) approaches. Coverage analysis revealed that 62.1% of CIHLex concepts
  and 26.2% of terms could be mapped to the Unified Medical Language System (UMLS),
  with 75.7% of mapped concepts categorized as "Therapeutic or Preventive Procedure."
  The study also developed annotated biomedical literature data and evaluated several
  natural language processing (NLP) models for CIH named entity recognition (NER).
---

# Complementary and Integrative Health Lexicon (CIHLex) and Entity Recognition in the Literature

## Quick Facts
- arXiv ID: 2305.17353
- Source URL: https://arxiv.org/abs/2305.17353
- Reference count: 0
- Key outcome: BERT-based models achieved highest macro-averaged F1-score of 0.90 for CIH named entity recognition

## Executive Summary
This study addresses the challenge of standardizing terminology in complementary and integrative health (CIH) literature by constructing the CIHLex, a lexicon containing 198 unique concepts and 1090 unique terms. The research demonstrates that BERT-based NLP models, particularly BLUEBERT, significantly outperform existing systems and zero-shot GPT-3.5 Turbo for CIH named entity recognition tasks. Coverage analysis revealed that while 62.1% of CIHLex concepts map to the Unified Medical Language System (UMLS), only 26.2% of terms could be matched, highlighting gaps in terminology standardization for this domain.

## Method Summary
The study integrated CIH terms from literature reviews and the Natural Medicines database to construct CIHLex, followed by UMLS mapping analysis using string matching with and without LuiNorm normalization. PubMed abstracts were collected and manually annotated for CIH named entities across five categories, then converted to BIO format. Five pre-trained BERT models (base BERT, BioBERT, ClinicalBERT, PubMedBERT, BLUEBERT) were fine-tuned on the annotated data and evaluated against existing systems including MetaMap, CLAMP-dictionary, and GPT3.5-turbo using strict and lenient matching metrics.

## Key Results
- BERT-based models achieved the highest macro-averaged F1-score of 0.90 for CIH named entity recognition
- 62.1% of CIHLex concepts and 26.2% of terms mapped to UMLS Metathesaurus
- 75.7% of mapped CIH concepts were categorized as "Therapeutic or Preventive Procedure" in UMLS
- LuiNorm normalization increased UMLS term mapping from 175 to 236 terms

## Why This Works (Mechanism)

### Mechanism 1
BERT-based models outperform traditional NLP systems and zero-shot GPT-3.5 Turbo in CIH named entity recognition due to their pre-training on biomedical and clinical corpora. The models leverage bidirectional attention and contextual embeddings trained on large-scale biomedical text (PubMed, MIMIC-III), enabling better capture of CIH terminology and abbreviations. This domain-specific pre-training data improves performance on specialized vocabulary tasks. However, performance may degrade if the test corpus contains significant out-of-domain terminology not present in the pre-training data.

### Mechanism 2
Normalization using LuiNorm improves UMLS mapping accuracy by reducing lexical variation between CIH terms and UMLS strings. LuiNorm standardizes terms by removing genitives, discarding parenthetic plural forms, and replacing punctuation with spaces, increasing exact string matches. While this approach reduces false negatives in string matching, evidence shows only modest improvement (9 additional concepts mapped out of 198 total), and there's a risk of removing distinguishing information or creating ambiguous matches.

### Mechanism 3
The multi-resource integration approach (literature review + Natural Medicines) creates a more comprehensive CIH lexicon than single-source approaches. Combining domain-expert curated terms with structured database entries captures both established terminology and emerging CIH approaches. Although this method assumes different sources capture orthogonal aspects of CIH terminology coverage, no comparative analysis with single-source lexicons was provided to validate this assumption.

## Foundational Learning

- Concept: Named Entity Recognition (NER) task structure and BIO encoding
  - Why needed here: Understanding how CIH terms are labeled and converted to machine-readable format for model training
  - Quick check question: What do the B, I, and O tags represent in the BIO format used for this task?

- Concept: Transfer learning and fine-tuning pre-trained language models
  - Why needed here: The BERT models were pre-trained on large corpora and then fine-tuned on the annotated CIH data
  - Quick check question: Why is transfer learning particularly beneficial for domains with limited labeled data like CIH?

- Concept: UMLS mapping and semantic type classification
  - Why needed here: Understanding how CIH concepts are mapped to existing biomedical terminologies and categorized
  - Quick check question: What percentage of mapped CIH concepts were categorized as "Therapeutic or Preventive Procedure" in the UMLS?

## Architecture Onboarding

- Component map: CIH Lexicon Construction -> UMLS Coverage Analysis -> PubMed Abstract Collection -> Manual Annotation -> Data Preprocessing -> Model Training -> Evaluation
- Critical path: Lexicon construction → Annotation → Model training → Evaluation
- Design tradeoffs: Comprehensive lexicon vs. manual validation effort; Strict vs. lenient evaluation metrics (tradeoff between precision and recall); Zero-shot vs. fine-tuned model performance
- Failure signatures: Low inter-annotator agreement (<0.8 kappa); Poor model performance on abbreviation recognition; UMLS mapping coverage below 50%
- First 3 experiments:
  1. Run exact string matching vs. LuiNorm-normalized matching on a small subset to quantify improvement
  2. Test inter-annotator agreement on 20% overlap instead of 10% to ensure reliability
  3. Compare zero-shot GPT-3.5 Turbo with few-shot learning using 5 examples per CIH class

## Open Questions the Paper Calls Out

### Open Question 1
How can the CIHLex be extended to capture relationships among CIH concepts, such as subsumption hierarchies? The paper states CIHLex did not capture relationships (such as subsumption) among concepts, but does not provide details on how to implement or design such relationships within the lexicon. Resolution would require a proposed schema or methodology for encoding relationships between CIH concepts in the lexicon, along with an evaluation of its impact on CIH knowledge representation and retrieval.

### Open Question 2
What is the impact of excluding abbreviations that do not follow a full name in CIHLex terms on UMLS mapping accuracy? The paper did not exclude standalone abbreviations like "rTMS" from CIHLex terms before mapping to UMLS. Resolution would require a comparison of UMLS mapping accuracy with and without the inclusion of standalone abbreviations in the CIHLex terms.

### Open Question 3
How does the performance of GPT-3.5 Turbo on CIH NER tasks vary with different prompt templates or few-shot learning approaches? The paper only tested a limited number of prompts on GPT3.5-turbo and only evaluated zero-shot performance. Resolution would require a comprehensive evaluation of GPT-3.5 Turbo's CIH NER performance using various prompt templates and few-shot learning scenarios, comparing results with the zero-shot approach.

## Limitations
- CIHLex coverage analysis shows only 62.1% concept mapping to UMLS, indicating significant terminology remains unmapped to existing biomedical standards
- Manual annotation process relied on a relatively small team of three annotators with limited inter-annotator agreement data beyond initial training
- Study focuses specifically on physical and psychological CIH approaches, excluding other domains that may have different terminology patterns

## Confidence

- **High confidence**: BERT-based model performance superiority over existing systems (F1-score of 0.90 demonstrated across multiple metrics)
- **Medium confidence**: UMLS mapping coverage improvements through LuiNorm normalization (limited evidence beyond single case example)
- **Medium confidence**: Multi-source lexicon construction approach (no comparative analysis with single-source alternatives)

## Next Checks

1. Replicate the UMLS mapping analysis on a held-out subset of 50 CIH terms to verify the 62.1% coverage claim and test LuiNorm effectiveness
2. Conduct inter-annotator agreement analysis on 30% of annotated abstracts rather than the reported 10% to strengthen reliability assessment
3. Test model performance on a separate corpus of CIH literature not used in training to evaluate domain transfer capability