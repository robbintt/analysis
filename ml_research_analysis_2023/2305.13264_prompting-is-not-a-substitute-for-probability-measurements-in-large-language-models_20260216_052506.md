---
ver: rpa2
title: Prompting is not a substitute for probability measurements in large language
  models
arxiv_id: '2305.13264'
source_url: https://arxiv.org/abs/2305.13264
tags:
- uni00a0
- uni00a00
- direct
- sentence
- metalinguistic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper compares metalinguistic prompting and direct probability
  measurements for evaluating large language models' linguistic knowledge. The authors
  find that metalinguistic judgments are inferior to direct measurements, and consistency
  decreases as prompts diverge from direct probability measurements.
---

# Prompting is not a substitute for probability measurements in large language models

## Quick Facts
- arXiv ID: 2305.13264
- Source URL: https://arxiv.org/abs/2305.13264
- Reference count: 40
- Key outcome: Metalinguistic judgments are inferior to direct probability measurements for evaluating LLMs' linguistic knowledge.

## Executive Summary
This paper investigates whether metalinguistic prompting can reliably measure large language models' linguistic knowledge. Through systematic experiments comparing direct probability measurements against various metalinguistic prompt types across four tasks and multiple model families, the authors demonstrate that direct measurements align more closely with models' internal representations. The study reveals that as prompts diverge from direct probability measurements, consistency between metalinguistic and direct responses decreases, challenging the validity of metalinguistic prompting as a substitute for probability measurements.

## Method Summary
The researchers conducted zero-shot evaluations using four prompt types (Direct, MetaQuestionSimple, MetaInstruct, MetaQuestionComplex) across four tasks: word prediction, word comparison, sentence judgment, and sentence comparison. They tested three Flan-T5 models and three GPT-3/3.5 models on datasets including Vassallo et al. (2018), SyntaxGym, and BLiMP. The study measured both task performance (accuracy or log probability) and internal consistency (correlation between metalinguistic and direct responses) to compare the effectiveness of different prompting approaches.

## Key Results
- Direct probability measurements align more closely with models' internal representations than metalinguistic prompts
- Minimal pairs are superior to isolated judgments for revealing models' generalization capacities
- Consistency between metalinguistic and direct measurements decreases as prompts diverge from direct probability measurements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct probability measurements align more closely with models' internal representations than metalinguistic prompts
- Mechanism: Direct measurements access the model's logits directly, bypassing transformations that occur when processing metalinguistic prompts, thus reflecting the model's true internal state
- Core assumption: The model's internal representations are most accurately captured by directly reading output layer logits
- Evidence anchors: Abstract shows metalinguistic judgments are inferior to direct measurements; section 2 discusses metalinguistic judgment as an emergent ability

### Mechanism 2
- Claim: Minimal pairs help reveal models' generalization capacities better than isolated judgments
- Mechanism: Comparative framing in minimal pairs reduces noise and makes subtle linguistic differences more apparent
- Core assumption: Comparative tasks create a more focused evaluation context that amplifies small differences in model representations
- Evidence anchors: Section 4.3 describes using minimal pairs to probe which sentence is "better"; section 5.1 discusses differences between experiments 3a and 3b

### Mechanism 3
- Claim: Consistency between metalinguistic and direct measurements decreases as the prompt diverges from direct probability measurements
- Mechanism: More complex transformations required by the prompt create more opportunities for output to diverge from direct measurements
- Core assumption: Each transformation step in processing a prompt introduces potential for misalignment
- Evidence anchors: Abstract states consistency worsens as prompts diverge; section 5.2 shows weaker correlations with increased distance

## Foundational Learning

- **Concept**: Probability distributions and log probabilities
  - Why needed here: The paper relies heavily on comparing probability distributions assigned by models to different linguistic outputs
  - Quick check question: If a model assigns log probabilities of -1.0 and -2.0 to two different words, which word does it consider more probable and by what factor?

- **Concept**: Minimal pairs in linguistics
  - Why needed here: The experiments use minimal pairs to test subtle linguistic distinctions
  - Quick check question: What is the key difference between the two sentences in a minimal pair, and why is this approach useful for testing linguistic knowledge?

- **Concept**: Zero-shot prompting
  - Why needed here: The paper uses zero-shot prompting methods
  - Quick check question: What distinguishes zero-shot prompting from few-shot prompting, and why might one choose zero-shot methods for evaluating model capabilities?

## Architecture Onboarding

- **Component map**: Prompt generator -> Model (Flan-T5 or GPT) -> Logits/responses -> Evaluation metrics
- **Critical path**: Generate prompts → Feed to model → Collect logits/responses → Compute probabilities or judgments → Compare across methods
- **Design tradeoffs**: Direct measurements provide cleaner access to internal representations but may not reflect real-world usage; metalinguistic prompts are more naturalistic but introduce additional noise
- **Failure signatures**: Inconsistent results between direct and metalinguistic methods, particularly as prompts become more complex; poor performance on minimal pair tasks despite good isolated judgment performance
- **First 3 experiments**:
  1. Implement direct probability measurement for word prediction using both Flan-T5 and GPT models
  2. Create metalinguistic prompts for word prediction and compare results with direct measurements
  3. Extend to word comparison tasks using the Vassallo et al. (2018) dataset and analyze consistency patterns

## Open Questions the Paper Calls Out

1. What specific features of metalinguistic prompts cause misalignment between direct probability measurements and metalinguistic judgments?
2. How does few-shot prompting affect the alignment between metalinguistic and direct measurements?
3. Do the findings generalize to languages other than English?

## Limitations

- Findings based on specific models (Flan-T5 and GPT-3/5) and prompt types that may not generalize to all LLMs
- Experiments focus on English language tasks, limiting generalizability to other languages
- Paper does not explore the full spectrum of possible prompt designs or their relationship with measurement consistency

## Confidence

- **High Confidence**: Direct probability measurements align more closely with models' internal representations than metalinguistic prompts
- **Medium Confidence**: Minimal pairs are superior to isolated judgments for revealing generalization capacities
- **Medium Confidence**: Consistency decreases with prompt divergence from direct probability measurements

## Next Checks

1. Test the relationship between metalinguistic and direct measurements across multiple languages to determine generalizability beyond English
2. Systematically vary prompt complexity and structure to map out the full relationship between prompt divergence and measurement consistency
3. Validate findings using different LLM architectures (e.g., OPT, BLOOM) to assess whether results represent a general phenomenon or are model-specific