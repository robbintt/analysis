---
ver: rpa2
title: Multi-Objective Optimization of Performance and Interpretability of Tabular
  Supervised Machine Learning Models
arxiv_id: '2307.08175'
source_url: https://arxiv.org/abs/2307.08175
tags:
- feature
- features
- optimization
- learning
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a framework for optimizing both predictive
  performance and interpretability of supervised ML models on tabular data by treating
  hyperparameter optimization as a multi-objective problem. Interpretability is quantified
  using feature sparsity, interaction sparsity, and sparsity of non-monotone effects.
---

# Multi-Objective Optimization of Performance and Interpretability of Tabular Supervised Machine Learning Models

## Quick Facts
- arXiv ID: 2307.08175
- Source URL: https://arxiv.org/abs/2307.08175
- Reference count: 40
- One-line primary result: EAGGA outperforms state-of-the-art models on 20 binary classification tasks by achieving competitive performance with improved interpretability

## Executive Summary
This paper introduces a framework for jointly optimizing predictive performance and interpretability of supervised ML models on tabular data by treating hyperparameter optimization as a multi-objective problem. The framework augments the hyperparameter search space with feature selection, interaction constraints, and monotonicity constraints, and solves the problem using a novel evolutionary algorithm (EAGGA) that efficiently handles this augmented space. Benchmark experiments demonstrate EAGGA outperforms XGBoost, EBM, Elastic-Net, and random forest models on 20 binary classification tasks, achieving competitive or better performance with improved interpretability metrics.

## Method Summary
The framework formulates a multi-objective optimization problem over performance (AUC) and interpretability metrics (feature sparsity, interaction sparsity, monotonicity sparsity). The search space is augmented by incorporating feature selection and interaction/monotonicity constraints as additional hyperparameters. EAGGA combines an evolutionary algorithm for traditional hyperparameters with a grouping genetic algorithm for group structures, using crossover and mutation operators that work on both spaces. The method uses detectors to initialize promising group structures and evaluates solutions using nested resampling.

## Key Results
- EAGGA outperforms standard XGBoost, EBM, Elastic-Net, and random forest models on 20 binary classification tasks
- EAGGA achieves competitive or better performance while providing improved interpretability metrics
- Ablation studies confirm the necessity of EAGGA's core components (EA, GGA, detectors)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework can jointly optimize performance and interpretability because it augments the hyperparameter search space with feature selection, interaction constraints, and monotonicity constraints.
- Mechanism: By embedding interpretability controls directly into the hyperparameter space, the search algorithm can navigate trade-offs between performance and interpretability in a single optimization run.
- Core assumption: The learning algorithm supports feature selection and constraints on interactions and monotonicity.
- Evidence anchors:
  - [abstract] "We introduce a general, model-agnostic framework for jointly optimizing the predictive performance and interpretability of supervised machine learning models for tabular data."
  - [section] "We formulate a multi-objective optimization problem of performance and interpretability over the hyperparameter search space of a learning algorithm, which is augmented by incorporating feature selection as well as interaction and monotonicity constraints into the hyperparameter search space."
- Break condition: If the learning algorithm cannot handle these constraints, the augmented search space becomes meaningless and optimization cannot proceed.

### Mechanism 2
- Claim: The group structure space reformulation simplifies optimization by converting feature selection and interaction constraints into equivalence classes.
- Mechanism: By treating allowed interactions as an equivalence relation, the framework reduces the complex search space to manageable group structures where features are grouped by allowed interactions and monotonicity constraints.
- Core assumption: Interactions are non-directional and can be modeled as equivalence classes, even though true interaction transitivity is not required.
- Evidence anchors:
  - [section] "We can now introduce the group structure space G. Each group structure ð‘® âˆˆ G consists of a ð‘”-tuple of sets of feature indices... We can now reformulate Equation 4 and introduce the augmented search space Ëœðš² = ðš² Ã— G by considering the group structure ð‘® âˆˆ G instead of ð’”, ð‘°ð’”, and ð’Žð‘°ð’”."
  - [corpus] No direct evidence found; this appears to be a novel reformulation.
- Break condition: If the interaction structure is too complex or requires non-transitive relationships, the equivalence class simplification may lose expressiveness.

### Mechanism 3
- Claim: EAGGA's combination of evolutionary algorithm and grouping genetic algorithm efficiently explores the augmented search space.
- Mechanism: EA operators handle continuous/discrete hyperparameters while GGA operators manipulate group structures directly, enabling efficient navigation of both traditional hyperparameters and interpretability controls.
- Core assumption: Evolutionary operators (crossover, mutation) are effective for both types of parameters in the augmented space.
- Evidence anchors:
  - [section] "We introduce an optimizer consisting of an evolutionary algorithm (EA) for the original search space of the learning algorithm ðš² and a so-called grouping genetic algorithm (GGA) for the group structure space G."
  - [section] "For crossover, we select two crossing sites, delimiting the crossing section, in each of the two parents... For mutation, we simply assign each feature index a new group membership with probability ð‘ = 0.2 and sample a new monotonicity attribute for each group with probability ð‘ = 0.2."
- Break condition: If crossover/mutation rates are poorly tuned or the space is too large, convergence may be slow or stuck in local optima.

## Foundational Learning

- Concept: Multi-objective optimization
  - Why needed here: The framework must balance competing objectives (performance vs interpretability) rather than optimizing for one at the expense of others.
  - Quick check question: What is Pareto optimality and why is it important in multi-objective problems?

- Concept: Feature selection and interaction detection
  - Why needed here: The framework needs to identify which features to include and which can interact to control model complexity.
  - Quick check question: How does the entropy-based information gain filter rank features by importance?

- Concept: Monotonicity constraints
  - Why needed here: Many domains require features to have consistent directional effects, which improves interpretability and trust.
  - Quick check question: Why does the framework encode monotonicity as attributes of feature groups rather than individual features?

## Architecture Onboarding

- Component map:
  - EAGGA core: EA + GGA combination
  - Search space: Original hyperparameters (ð€) + group structures (ð‘®)
  - Detectors: Feature, interaction, and monotonicity detectors for initialization
  - Learning algorithm interface: Must support feature selection, interaction constraints, monotonicity constraints

- Critical path:
  1. Initialize population with detectors
  2. Evaluate fitness using nested resampling
  3. Apply crossover/mutation to both hyperparameters and group structures
  4. Update group structures based on actual model structure
  5. Select survivors via non-dominated sorting and crowding distance

- Design tradeoffs:
  - Population size (ðœ‡=100) vs computational cost
  - Crossover probability (ð‘=0.7) vs exploration vs exploitation
  - Mutation probability (ð‘=0.3) vs maintaining diversity vs convergence

- Failure signatures:
  - No improvement in dominated hypervolume over generations
  - Population converges to identical group structures
  - Detectors provide poor initial diversity

- First 3 experiments:
  1. Run EAGGA on a small binary classification task with known interpretability requirements and verify it finds sparse, monotone models.
  2. Compare EAGGA's anytime performance to random search on the augmented space to confirm detectors help.
  3. Test EAGGA with and without group structure crossover to measure impact on final hypervolume.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of EAGGA scale with increasing feature dimensionality (p > 300) and dataset size (n > 10,000)?
- Basis in paper: [inferred] The paper only tests up to p=970 and n=96,320, with most datasets having p < 100. Performance appears to degrade on high-dimensional tasks.
- Why unresolved: The paper does not include experiments with very high-dimensional or large-scale datasets that are common in modern applications.
- What evidence would resolve it: Benchmarking EAGGA on datasets with p > 1,000 and n > 100,000 to measure runtime, memory usage, and performance degradation.

### Open Question 2
- Question: What is the effect of using different feature interaction detectors beyond FAST (e.g., mutual information, SHAP values) on EAGGA's performance and interpretability?
- Basis in paper: [explicit] The paper uses FAST for interaction detection but acknowledges other detectors could be used. No comparison is provided.
- Why unresolved: The paper uses a single interaction detector without exploring alternatives or their impact on the optimization process.
- What evidence would resolve it: Comparative experiments using multiple interaction detectors within EAGGA to measure differences in final Pareto front quality and interpretability metrics.

### Open Question 3
- Question: How sensitive is EAGGA's performance to its hyperparameter configuration (population size, crossover/mutation rates) and could these be self-adapted during optimization?
- Basis in paper: [explicit] The paper uses fixed values (Î¼=100, Î½=10, p_c=0.7, p_m=0.3) without exploring sensitivity or self-adaptation.
- Why unresolved: The paper does not perform a sensitivity analysis or explore adaptive strategies for these critical hyperparameters.
- What evidence would resolve it: Systematic ablation studies varying these parameters and experiments with self-adaptive mechanisms to determine optimal configurations.

### Open Question 4
- Question: How does EAGGA compare to neural network-based approaches when interpretability constraints are relaxed (e.g., allowing more complex interactions)?
- Basis in paper: [inferred] The paper focuses on tabular data where tree-based methods traditionally excel, but acknowledges that deep learning approaches are improving.
- Why unresolved: The paper does not compare EAGGA against modern neural network architectures on the same tasks, even without interpretability constraints.
- What evidence would resolve it: Head-to-head comparisons of EAGGA with state-of-the-art deep learning models on tabular data benchmarks, measuring both performance and computational efficiency.

## Limitations

- The framework's performance advantage depends heavily on the quality of interaction and monotonicity detectors, which are not fully specified in the paper.
- The GGA component assumes interactions can be represented as equivalence classes, but real-world feature interactions may not satisfy transitivity, potentially limiting expressiveness.
- The computational cost of nested resampling during fitness evaluation may be prohibitive for very large datasets or longer optimization runs.

## Confidence

- **High confidence**: The core claim that augmenting the hyperparameter space with interpretability controls enables joint optimization of performance and interpretability is well-supported by the theoretical framework and experimental results.
- **Medium confidence**: The claim that EAGGA's EA+GGA combination is necessary for efficient exploration is supported by ablation studies, but the exact contribution of each component could benefit from more granular analysis.
- **Medium confidence**: The interpretability metrics (feature sparsity, interaction sparsity, monotonicity sparsity) provide reasonable quantification of model interpretability, though their correlation with human interpretability remains to be validated.

## Next Checks

1. **Detector Quality Assessment**: Implement and test the FAST interaction detector and monotonicity detector independently on benchmark datasets to verify they produce meaningful group structures that improve optimization efficiency.

2. **Transitivity Analysis**: Apply EAGGA to datasets where feature interactions are known to be non-transitive and measure whether the equivalence class simplification leads to significant performance loss or inability to capture important interactions.

3. **Computational Cost Profiling**: Profile the runtime breakdown of EAGGA to quantify how much time is spent on nested resampling versus actual optimization, and test whether approximate fitness evaluation methods (e.g., using validation folds) maintain performance while reducing computational burden.