---
ver: rpa2
title: On Context Utilization in Summarization with Large Language Models
arxiv_id: '2310.10570'
source_url: https://arxiv.org/abs/2310.10570
tags:
- llms
- summarization
- arxiv
- context
- window
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates position bias in large language models
  (LLMs) for abstractive summarization. The authors conduct a comprehensive study
  across 6 LLMs, 10 datasets, and 5 evaluation metrics to analyze how LLMs leverage
  input context.
---

# On Context Utilization in Large Language Models

## Quick Facts
- arXiv ID: 2310.10570
- Source URL: https://arxiv.org/abs/2310.10570
- Reference count: 22
- Key outcome: LLMs exhibit strong position bias favoring beginning/end of context, with performance degrading significantly for middle content.

## Executive Summary
This paper investigates position bias in large language models for abstractive summarization, finding that LLMs strongly favor content at the beginning and end of their context window while underutilizing middle content. Through comprehensive experiments across 6 LLMs, 10 datasets, and 5 evaluation metrics, the authors demonstrate a clear U-shaped performance pattern where salient information in the middle receives less attention during generation. To address this limitation, they propose hierarchical and incremental summarization methods and evaluate them on a new benchmark called MiddleSum, showing that position bias significantly impacts LLM summarization quality.

## Method Summary
The study conducts zero-shot inference with 6 LLMs (FLAN-UL2, LLaMA-2-7B/13B, XGen-7B) on 10 summarization datasets using a standard prompt template. Generated summaries are evaluated using ROUGE, BERTScore, SUPERT, BARTScore-faithfulness, and GPT-3.5 metrics. The authors analyze attention weights to identify position bias and propose two inference methods - hierarchical and incremental summarization - to mitigate this issue. Performance is measured on both existing datasets and a new MiddleSum benchmark designed to test middle-content utilization.

## Key Results
- LLMs exhibit strong position bias, favoring beginning and end segments with U-shaped performance patterns
- Salient information in middle positions receives significantly less attention, reducing summarization quality
- Proposed hierarchical and incremental methods show promise on MiddleSum benchmark
- LLMs are more lead-biased than reference summaries in long-input datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit a strong position bias that favors the beginning and end of the input context, resulting in a U-shaped performance pattern for summarization.
- Mechanism: The attention mechanism in transformer-based LLMs gives more weight to tokens at the beginning and end of the input sequence. This causes the model to focus disproportionately on the initial and final segments when generating summaries, leading to underutilization of middle content.
- Core assumption: The positional encoding and attention patterns in LLMs inherently prioritize boundary tokens over middle tokens, regardless of content relevance.
- Evidence anchors:
  - [abstract] "They tend to favor the initial and final segments, resulting in a U-shaped performance pattern"
  - [section] "LLMs attend more to the beginning and the end of their input" and "A clear U-shape appears on PubMed and SummScreen"
  - [corpus] Weak - no direct corpus evidence for this specific positional bias in summarization
- Break condition: If the model is explicitly trained or prompted to attend uniformly across all positions, or if the input is artificially rearranged to move salient content to the middle.

### Mechanism 2
- Claim: LLMs are more likely to rephrase information from the beginning of the source document, making them more lead-biased than reference summaries in long-input datasets.
- Mechanism: During summary generation, the model's decoding process is influenced by the accumulated attention weights, which are higher for early tokens. This causes the model to preferentially copy or rephrase content from the beginning of the input.
- Core assumption: The model's internal representation of relevance is correlated with positional encoding, not just content importance.
- Evidence anchors:
  - [abstract] "LLMs are more likely to take information from the beginning of the source"
  - [section] "sentences from the first 10% or last 10% of source documents are much more represented than others" and "LLMs are more likely to take information from the beginning of the source"
  - [corpus] Weak - no direct corpus evidence for rephrasing bias
- Break condition: If the input is preprocessed to de-emphasize the lead or if the model is fine-tuned on datasets where salient information is uniformly distributed.

### Mechanism 3
- Claim: LLMs' summarization performance is sensitive to the position of salient information in the context window, with performance decreasing as important content moves toward the middle.
- Mechanism: The model's ability to identify and incorporate salient information into the summary is hindered when that information is located in the middle of the input, due to reduced attention and lower likelihood of being attended to during decoding.
- Core assumption: The position of salient information within the context window directly affects the model's ability to detect and utilize it.
- Evidence anchors:
  - [abstract] "LLMs summarization performance is sensitive to the position of salient information in the context window"
  - [section] "we see a negative trend between performance and position of salient information" and "LLMs summarization performance is sensitive to the position of salient information in the context window"
  - [corpus] Weak - no direct corpus evidence for performance-position correlation
- Break condition: If the input is structured such that salient information is consistently placed at the beginning or end, or if the model is modified to attend more uniformly.

## Foundational Learning

- Concept: Positional encoding in transformers
  - Why needed here: Understanding how position bias arises from the model's architecture
  - Quick check question: How does positional encoding influence attention weights in transformers?

- Concept: Attention mechanism and attention maps
  - Why needed here: Analyzing where the model focuses during generation to identify position bias
  - Quick check question: How can we interpret attention weights to determine which parts of the input the model attends to?

- Concept: Abstractive summarization vs extractive summarization
  - Why needed here: Recognizing why mapping generated summaries to source content is more complex than in extractive tasks
  - Quick check question: What makes it difficult to align generated summaries with source content in abstractive summarization?

## Architecture Onboarding

- Component map:
  - Input truncation and tokenization (LLAMA-2, XGen, FLAN-UL2)
  - Prompt template with task description and number of sentences
  - LLM inference with top-k sampling
  - Alignment of generated summaries to source content
  - Evaluation with multiple metrics (ROUGE, BERTScore, SUPERT, BARTScore-faithfulness, GPT-3.5)
  - Analysis of attention weights and position of salient information

- Critical path:
  - Input preprocessing → Prompting → Inference → Alignment → Evaluation → Analysis of position bias

- Design tradeoffs:
  - Context window length vs. computational cost
  - Number of sentences in summary vs. information coverage
  - Choice of evaluation metrics vs. correlation with human judgment
  - Use of alignment heuristics vs. accuracy of source-summary mapping

- Failure signatures:
  - Performance drops significantly when salient information is in the middle of the input
  - Generated summaries disproportionately contain content from the beginning or end of the source
  - Attention weights show higher values for boundary tokens
  - Evaluation metrics show negative correlation with position of salient information

- First 3 experiments:
  1. Run inference on a long-input dataset with different context window lengths and compare performance to identify the point of diminishing returns.
  2. Analyze attention weights during generation to confirm that the model attends more to the beginning and end of the input.
  3. Perform a control experiment by placing salient information at different positions within the input and measuring the impact on summarization quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we accurately measure the saliency of content within a source document for summarization tasks?
- Basis in paper: [inferred] The paper acknowledges the difficulty in assessing saliency and mentions using aligned source sentences with reference summaries as an approximation, but notes this approach has limitations.
- Why unresolved: The paper highlights the need for more robust tools to detect salient content, suggesting current methods are insufficient.
- What evidence would resolve it: Development and validation of new metrics or tools that can more accurately identify and measure salient content in source documents, potentially through human annotation or advanced NLP techniques.

### Open Question 2
- Question: How do different position interpolation techniques affect the performance of LLMs on long-context summarization tasks?
- Basis in paper: [explicit] The paper mentions position interpolation as a method to extend context windows but does not explore its impact in detail.
- Why unresolved: While the paper uses position interpolation to extend context windows, it does not compare different techniques or analyze their effectiveness in depth.
- What evidence would resolve it: Comparative studies of various position interpolation methods on long-context summarization tasks, evaluating their impact on model performance and context utilization.

### Open Question 3
- Question: Can new datasets be designed to better evaluate LLMs' ability to utilize information distributed throughout the context window?
- Basis in paper: [explicit] The paper calls for the creation of new datasets where salient information is uniformly distributed throughout the source to better measure LLMs' context utilization.
- Why unresolved: Current datasets may not adequately challenge LLMs to use their entire context window, leading to biased performance evaluations.
- What evidence would resolve it: Creation and validation of new summarization datasets with controlled saliency distribution, followed by comprehensive evaluation of LLM performance on these datasets to assess improvements in context utilization.

## Limitations

- The study relies heavily on automatic metrics which may not accurately capture position bias effects that humans would notice.
- Evidence for U-shaped performance patterns comes primarily from visual inspection rather than rigorous statistical validation.
- The proposed inference methods are evaluated only on the MiddleSum benchmark, without comparison to established long-input datasets.
- The relationship between salient information position and performance reduction lacks quantified effect sizes or significance testing.

## Confidence

**High Confidence**: The empirical observation that LLMs exhibit position bias favoring beginning and end segments is well-supported by attention weight analysis across multiple models and datasets.

**Medium Confidence**: The claim that LLMs are more lead-biased than reference summaries requires careful interpretation, with the comparison methodology needing more rigorous validation.

**Low Confidence**: The proposed hierarchical and incremental summarization methods show promise on MiddleSum, but their effectiveness on diverse real-world datasets remains unproven.

## Next Checks

1. **Statistical validation of position bias**: Conduct hypothesis testing to quantify the significance of performance degradation as salient information moves toward the middle of the context window.

2. **Human evaluation correlation**: Compare automatic metric results with human judgments on position bias detection to validate the effectiveness of proposed metrics.

3. **Cross-dataset generalization**: Test the proposed inference methods on established long-input summarization benchmarks (ArXiv, PubMed, Multi-News) to validate practical utility beyond the controlled experimental setting.