---
ver: rpa2
title: '3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network'
arxiv_id: '2308.11771'
source_url: https://arxiv.org/abs/2308.11771
tags:
- tracking
- convlstm
- event-based
- event
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficient event-based eye tracking
  for wearable healthcare technology like AR/VR headsets. The proposed method uses
  a Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) network to extract
  spatio-temporal features from sparse event streams generated by event cameras.
---

# 3ET: Efficient Event-based Eye Tracking using a Change-Based ConvLSTM Network

## Quick Facts
- arXiv ID: 2308.11771
- Source URL: https://arxiv.org/abs/2308.11771
- Reference count: 24
- Key outcome: CB-ConvLSTM achieves 88.5% pupil detection at p3 with 4.7× fewer arithmetic operations than baseline models

## Executive Summary
This paper addresses the challenge of efficient event-based eye tracking for AR/VR headsets using a novel Change-Based Convolutional Long Short-Term Memory (CB-ConvLSTM) network. The method exploits temporal sparsity in recurrent activations to reduce computational cost while maintaining high pupil detection accuracy. Tested on a synthetic event dataset, the approach demonstrates significant efficiency gains (4.7× reduction in arithmetic operations) without compromising detection performance, achieving pupil detection rates of 88.5% at p3, 96.7% at p5, and 99.2% at p10.

## Method Summary
The CB-ConvLSTM architecture introduces temporal sparsity into the recurrent path by applying delta encoding to hidden states between consecutive time steps. This replaces dense hidden state propagation with thresholded changes, enabling sparse convolutions that skip zero multiplications. The network processes sequences of 80×60 voxel grids representing event streams over 4.4ms time bins, using 4 ConvLSTM layers with increasing hidden nodes (8, 16, 32, 64), followed by batch normalization, ReLU, max pooling, and two fully connected layers. The model is trained on a synthetic dataset generated from the LPW dataset using the v2e simulator, with MSE loss and SGD optimization.

## Key Results
- Achieves 88.5% pupil detection rate at p3 threshold on synthetic event dataset
- Reduces arithmetic operations by approximately 4.7× compared to conventional CNN and ConvLSTM models
- Detection rates increase to 96.7% at p5 and 99.2% at p10 with sequence length of 40

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal sparsity in recurrent activations reduces computational cost while maintaining tracking accuracy.
- Mechanism: CB-ConvLSTM replaces dense hidden state Ht-1 with thresholded change ΔHt-1 between consecutive hidden states, enabling sparse convolutions that skip zero multiplications.
- Core assumption: Changes in hidden states between consecutive frames are sparse enough to preserve meaningful temporal dynamics for pupil tracking.
- Evidence anchors:
  - [abstract]: "Utilizing a delta-encoded recurrent path enhancing activation sparsity, CB-ConvLSTM reduces arithmetic operations by approximately 4.7× without losing accuracy"
  - [section]: "This innovative network introduces high sparsity into the process without compromising performance"
- Break condition: If temporal changes in eye movement become dense (e.g., rapid saccades or jitter), sparsity assumption fails and accuracy degrades.

### Mechanism 2
- Claim: Sparse event streams from DVS cameras reduce computational burden compared to frame-based approaches.
- Mechanism: Event cameras generate data only when brightness changes occur, creating inherently sparse input streams that the CB-ConvLSTM architecture exploits through sparse convolution operations.
- Core assumption: The pupil region generates sufficient events to maintain tracking accuracy while other regions remain sparse.
- Evidence anchors:
  - [section]: "Dynamic Vision Sensors (DVS) or event-based cameras, which capture brightness changes as sparse events, emerge as a viable solution for near-eye tracking"
  - [section]: "The sparsity in the DVS events originating from eye movements or pupil size changes can significantly reduce computational demands"
- Break condition: In low-light conditions or with minimal eye movement, event generation may be insufficient for reliable tracking.

### Mechanism 3
- Claim: Sequential processing with ConvLSTM captures temporal dependencies that single-frame CNNs miss.
- Mechanism: By processing sequences of event frames through recurrent connections, the model learns temporal patterns in pupil movement that static CNN architectures cannot capture.
- Core assumption: Pupil movement patterns are temporally correlated enough that past frames provide meaningful context for current frame prediction.
- Evidence anchors:
  - [section]: "We suggest integrating recurrent structures like LSTM units into the neural network to tackle this issue. Such structures are better suited for interpreting temporal information across the sequence of eye movements"
  - [section]: "The detection rates, indicated by p3, p5, and p10, raise to 88.8%, 97.0%, and 99.5%, respectively, representing an increase of 17.4%, 15.8%, and 6.9% when the sequence length is increased from 2 to 40"
- Break condition: If temporal correlation breaks down (e.g., during fixation), sequence-based methods may introduce unnecessary latency without accuracy benefits.

## Foundational Learning

- Concept: Event-based vision and DVS cameras
  - Why needed here: Understanding how DVS cameras generate sparse event streams is fundamental to appreciating why CB-ConvLSTM architecture works.
  - Quick check question: What triggers an event in a DVS camera, and how does this differ from traditional frame-based cameras?

- Concept: Convolutional LSTM networks
  - Why needed here: The CB-ConvLSTM builds on ConvLSTM, so understanding how spatial convolutions are integrated with temporal recurrence is essential.
  - Quick check question: How does ConvLSTM differ from standard LSTM in handling spatial information?

- Concept: Sparsity exploitation in neural networks
  - Why needed here: The efficiency gains in CB-ConvLSTM come from exploiting sparsity, so understanding sparse computation is crucial.
  - Quick check question: How does introducing sparsity in neural network activations reduce computational complexity?

## Architecture Onboarding

- Component map: Input layer → CB-ConvLSTM blocks (4 layers) → Batch normalization + ReLU → Max pooling → Fully connected layers (2 layers) → Output (x,y pupil coordinates)
- Critical path: Event frame sequence → CB-ConvLSTM processing → Pupil center prediction
- Design tradeoffs: Higher threshold θ increases sparsity and efficiency but risks losing temporal information; longer sequences improve accuracy but increase latency
- Failure signatures: Low detection rates at p3/p5 thresholds indicate either insufficient event generation or poor temporal feature extraction
- First 3 experiments:
  1. Test detection rates with sequence lengths 2, 10, 20, 40 to identify optimal temporal context
  2. Vary threshold θ (0.1, 0.2, 0.5) to find the sparsity-accuracy tradeoff sweet spot
  3. Compare CB-ConvLSTM against standard ConvLSTM and CNN baselines on the same dataset to validate efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the CB-ConvLSTM model compare to the vanilla ConvLSTM model on a real-world event-based eye tracking dataset, rather than a synthetic one generated from LPW?
- Basis in paper: [inferred] The paper uses a synthetic dataset generated from the LPW dataset using the v2e simulator. The authors acknowledge the scarcity of real-world event-based datasets for eye tracking.
- Why unresolved: The paper only tests the model on a synthetic dataset, so its performance on real-world data is unknown.
- What evidence would resolve it: Testing the CB-ConvLSTM model on a real-world event-based eye tracking dataset and comparing its performance to the vanilla ConvLSTM model and other state-of-the-art methods.

### Open Question 2
- Question: How does the detection rate of the CB-ConvLSTM model change with different values of the threshold θ, and what is the optimal value of θ for balancing sparsity and accuracy?
- Basis in paper: [explicit] The paper explores a range of θ values from 0 to 0.5 and reports the corresponding sparsity levels and detection rates.
- Why unresolved: The paper does not identify an optimal value of θ or discuss how the detection rate changes with different θ values in detail.
- What evidence would resolve it: Conducting a more extensive study on the effect of θ on the detection rate, and identifying the optimal value of θ for balancing sparsity and accuracy.

### Open Question 3
- Question: How does the CB-ConvLSTM model perform on other event-based vision tasks, such as object detection or tracking, beyond eye tracking?
- Basis in paper: [inferred] The paper focuses on the application of the CB-ConvLSTM model to event-based eye tracking, but the model architecture could potentially be applied to other event-based vision tasks.
- Why unresolved: The paper does not explore the performance of the CB-ConvLSTM model on other event-based vision tasks.
- What evidence would resolve it: Applying the CB-ConvLSTM model to other event-based vision tasks, such as object detection or tracking, and evaluating its performance compared to state-of-the-art methods.

## Limitations

- Performance evaluation is limited to synthetic event data generated from LPW dataset, with unverified real-world performance on actual DVS hardware
- The 4.7× computational reduction claim depends critically on the delta encoding threshold θ, which was not extensively explored across different eye movement patterns
- The evaluation focuses solely on pupil center detection without addressing robustness to occlusions, varying lighting conditions, or different pupil sizes

## Confidence

- **High confidence**: The core architectural innovation of delta-encoded recurrent path for sparsity is technically sound and addresses a genuine computational bottleneck in recurrent networks
- **Medium confidence**: The synthetic dataset generation pipeline using v2e is well-established, but the transferability of results to real DVS hardware remains uncertain
- **Low confidence**: The claimed efficiency gains in real-time AR/VR systems depend on many factors beyond arithmetic operations count, including memory bandwidth and sparse convolution implementation efficiency

## Next Checks

1. **Real Data Validation**: Test CB-ConvLSTM on real DVS eye tracking datasets (e.g., OpenEDS or proprietary AR/VR headset data) to verify the synthetic-to-real performance gap. Compare detection rates and computational metrics against the reported synthetic results.

2. **Threshold Sensitivity Analysis**: Conduct systematic ablation studies varying the delta encoding threshold θ across a wider range (0.1, 0.3, 0.5, 0.7, 0.9) and different eye movement patterns to establish the robustness of the sparsity-accuracy tradeoff.

3. **Cross-Platform Efficiency Benchmarking**: Implement the CB-ConvLSTM on both GPU and specialized neuromorphic hardware (e.g., Intel Loihi) to validate that the theoretical arithmetic reduction translates to actual power and latency improvements in constrained AR/VR devices.