---
ver: rpa2
title: 'GraB-sampler: Optimal Permutation-based SGD Data Sampler for PyTorch'
arxiv_id: '2309.16809'
source_url: https://arxiv.org/abs/2309.16809
tags:
- balance
- grab
- recursive
- pair
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The GraB-sampler library provides efficient implementation of Gradient
  Balancing (GraB) algorithms for PyTorch, enabling users to apply theoretically optimal
  data permutation methods during training. It implements 5 variants of GraB algorithms
  (Mean Balance, Pair Balance, Batch Balance, Recursive Balance, Recursive Pair Balance)
  with support for deterministic and probabilistic balancing kernels.
---

# GraB-sampler: Optimal Permutation-based SGD Data Sampler for PyTorch

## Quick Facts
- arXiv ID: 2309.16809
- Source URL: https://arxiv.org/abs/2309.16809
- Reference count: 4
- Key outcome: 8.7% training time overhead, 0.85% GPU memory overhead for best variants on CIFAR-10 with LeNet

## Executive Summary
GraB-sampler is a PyTorch library that implements Gradient Balancing (GraB) algorithms for optimal permutation-based SGD data sampling. The library provides five variants of GraB algorithms that use per-sample gradients to solve a herding problem, ordering examples greedily to guarantee better convergence than random reshuffling. It integrates natively with PyTorch's data loading system and supports both deterministic and probabilistic balancing kernels, achieving improved convergence on CIFAR-10 experiments with LeNet while maintaining reasonable computational overhead.

## Method Summary
The GraB-sampler library implements five variants of the Gradient Balancing algorithm (Mean Balance, Pair Balance, Batch Balance, Recursive Balance, Recursive Pair Balance) that use per-sample gradients to solve a herding problem for optimal example ordering during SGD training. The sampler inherits from torch.utils.data.Sampler and works with PyTorch 2.0's functional programming style for per-sample gradient computation. Users can easily integrate the sampler by changing only three lines of code in their existing training scripts, with the step() method accepting per-sample gradients computed via Functorch to update the internal state and generate new permutations each epoch.

## Key Results
- Mean Balance, Pair Balance, and Batch Balance variants achieve 8.7% training time overhead and 0.85% GPU memory overhead
- Recursive variants offer faster early convergence but introduce 60-90% training time overhead and 53-66% peak GPU memory overhead
- All GraB variants show improved convergence compared to random reshuffling on CIFAR-10 with LeNet
- The library integrates seamlessly with PyTorch DataLoader requiring minimal code changes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GraB-sampler achieves better convergence than random reshuffling by using per-sample gradients to solve a herding problem that orders examples greedily.
- Mechanism: The sampler computes per-sample gradients for each example, then uses these gradients to calculate balancing signs (+ or -) that determine the example ordering for the next epoch. This greedy ordering based on gradient information guarantees theoretically better performance than random reshuffling.
- Core assumption: Per-sample gradients can be efficiently computed in PyTorch 2.0's functional programming style, and these gradients accurately reflect each example's contribution to the overall loss.
- Evidence anchors:
  - [abstract]: "The online Gradient Balancing (GraB) algorithm greedily choosing the examples ordering by solving the herding problem using per-sample gradients is proved to be the theoretically optimal solution that guarantees to outperform Random Reshuffling."
  - [section 2]: "GraB algorithm requires per-sample gradients while solving the herding problem."
- Break condition: If per-sample gradients cannot be computed efficiently or if the herding problem becomes too computationally expensive relative to the convergence benefits.

### Mechanism 2
- Claim: The different GraB variants (Mean Balance, Pair Balance, Batch Balance, Recursive Balance, Recursive Pair Balance) offer tradeoffs between convergence speed and computational overhead.
- Mechanism: Each variant uses different strategies for computing balancing signs - Mean Balance uses stale gradient means, Pair Balance uses differences between two examples, Batch Balance delays accumulator updates for parallelism, and Recursive variants use tree structures for faster early convergence.
- Core assumption: The specific balancing strategy can be chosen based on computational constraints while maintaining the theoretical guarantees of improved convergence.
- Evidence anchors:
  - [section 2]: "Mean Balance takes O(n) computation and O(d) memory overhead... Batch Balance delays updating the accumulator... Recursive Balance... balances each example D times... Recursive Pair Balance uses the difference between 2 example gradients."
  - [abstract]: "We propose 5 variants of the GraB algorithm."
- Break condition: When computational overhead (especially memory for recursive variants) becomes prohibitive or when the chosen variant doesn't provide sufficient convergence improvement.

### Mechanism 3
- Claim: The GraB-sampler library integrates seamlessly with PyTorch's data loading system, requiring minimal code changes to existing training scripts.
- Mechanism: By inheriting from torch.utils.data.Sampler and implementing the step() method to accept per-sample gradients, the sampler can be dropped into existing PyTorch training loops with only 3 lines of code changes.
- Core assumption: PyTorch's DataLoader and functional programming interface in PyTorch 2.0 can efficiently support the per-sample gradient computations required by GraB.
- Evidence anchors:
  - [section 3.1]: "GraB-sampler inherited torch.utils.data.Sampler, so it natively supports PyTorch DataSet and DataLoader."
  - [section 3.1]: "A minimum code snippet that uses our library by only changing 3 lines of code shows as the following."
- Break condition: If the functional programming style required by PyTorch 2.0 becomes too restrictive or if the integration with DataLoader introduces significant overhead.

## Foundational Learning

- Concept: Herding problem in machine learning
  - Why needed here: GraB-sampler solves the herding problem to determine optimal example ordering for SGD
  - Quick check question: What is the herding problem and how does it relate to example ordering in SGD?

- Concept: Per-sample gradient computation in PyTorch
  - Why needed here: GraB-sampler requires per-sample gradients to compute balancing signs
  - Quick check question: How does PyTorch 2.0's functional programming style enable efficient per-sample gradient computation?

- Concept: Permutation-based SGD and its convergence properties
  - Why needed here: GraB-sampler is a permutation-based SGD method that guarantees better convergence than random reshuffling
  - Quick check question: What theoretical guarantees does permutation-based SGD provide over random reshuffling?

## Architecture Onboarding

- Component map:
  GraBSampler (inherits from torch.utils.data.Sampler) -> Sorter component -> Five balancing algorithm variants -> Two balancing kernel options -> Integration with PyTorch DataLoader

- Critical path:
  1. Initialize GraBSampler with dataset and model parameters
  2. Pass sampler to DataLoader
  3. In training loop, compute per-sample gradients using functional programming style
  4. Call sampler.step() with per-sample gradients at end of each epoch
  5. Sampler updates internal state and generates new permutation for next epoch

- Design tradeoffs:
  - Mean Balance, Pair Balance, Batch Balance: Lower computational overhead (8.7% training time) but potentially slower convergence
  - Recursive Balance, Recursive Pair Balance: Faster early convergence but higher computational overhead (60-90% training time) and memory usage (53-66% peak GPU memory)

- Failure signatures:
  - Poor convergence: Check if per-sample gradients are being computed correctly and if the chosen GraB variant is appropriate for the problem
  - High computational overhead: Consider switching to non-recursive variants or optimizing per-sample gradient computation
  - Integration issues: Verify that the model is implemented in functional programming style compatible with PyTorch 2.0

- First 3 experiments:
  1. Replace random reshuffling with GraB-sampler using Mean Balance on a simple CNN training task to verify basic functionality
  2. Compare convergence rates of different GraB variants (Mean Balance, Pair Balance, Batch Balance) on the same task
  3. Benchmark computational overhead of Recursive Balance vs non-recursive variants on a larger model

## Open Questions the Paper Calls Out

- What is the theoretical optimality gap between GraB and the best possible permutation-based ordering?
  - Basis in paper: The paper states that GraB is "theoretically optimal" but doesn't quantify how much better it could theoretically be
  - Why unresolved: The paper focuses on practical implementation and empirical results rather than theoretical bounds on optimality
  - What evidence would resolve it: A formal proof establishing the theoretical limit of improvement over random shuffling and comparing it to GraB's actual performance

## Limitations

- The paper only evaluates on CIFAR-10 with LeNet, limiting generalizability to other datasets and model architectures
- Recursive variants introduce significant computational overhead (60-90% training time) and memory usage (53-66% peak GPU memory) that may become prohibitive for large-scale training
- The theoretical guarantees assume certain conditions about the loss landscape that may not hold in practice for complex deep learning models

## Confidence

- Theoretical guarantees: High
- Practical integration with PyTorch: High
- Convergence improvements: Medium (non-recursive variants), Low (recursive variants)
- Scalability to large models: Low

## Next Checks

1. Benchmark GraB-sampler on a larger, more complex model (e.g., ResNet-50) and dataset (e.g., ImageNet) to evaluate scalability and whether the computational overhead remains acceptable at scale.

2. Compare GraB-sampler's convergence against other advanced data ordering strategies like curriculum learning or importance sampling to contextualize its practical benefits.

3. Profile the memory usage of recursive variants on different hardware configurations to determine if the 53-66% memory overhead is feasible for typical training setups or if it creates bottlenecks for larger batch sizes.