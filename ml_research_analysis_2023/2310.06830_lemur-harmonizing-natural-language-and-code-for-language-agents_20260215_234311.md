---
ver: rpa2
title: 'Lemur: Harmonizing Natural Language and Code for Language Agents'
arxiv_id: '2310.06830'
source_url: https://arxiv.org/abs/2310.06830
tags:
- language
- https
- arxiv
- agents
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Lemur and Lemur-Chat, open-source language
  models that achieve state-of-the-art performance across both natural language and
  coding benchmarks. The models are pre-trained on a code-intensive corpus and fine-tuned
  on a mix of text and code instruction data.
---

# Lemur: Harmonizing Natural Language and Code for Language Agents

## Quick Facts
- arXiv ID: 2310.06830
- Source URL: https://arxiv.org/abs/2310.06830
- Authors: [List of authors]
- Reference count: 40
- Primary result: Lemur-Chat achieves state-of-the-art performance across 12 of 13 agent benchmarks, significantly outperforming existing open-source models.

## Executive Summary
This paper introduces Lemur and Lemur-Chat, open-source language models that achieve state-of-the-art performance across both natural language and coding benchmarks. The models are pre-trained on a code-intensive corpus and fine-tuned on a mix of text and code instruction data. Experiments show that Lemur-Chat significantly outperforms existing open-source models on 12 of 13 agent benchmarks, demonstrating strong abilities in tool usage, self-debugging, following feedback, and exploring partially observable environments. The harmonized language and coding capabilities of Lemur-Chat narrow the performance gap with proprietary models like GPT-4 on agent abilities.

## Method Summary
Lemur and Lemur-Chat are developed through a two-stage process: pre-training on a code-intensive corpus (90B tokens with 10:1 text-to-code ratio) using Llama-2-70B architecture, followed by instruction fine-tuning on mixed text and code datasets (300K examples). The models are evaluated across text benchmarks, code benchmarks, and agent benchmarks to assess their harmonized capabilities. The pre-training uses sequential packing for efficiency on TPUv4-512 pods, and the fine-tuning incorporates various instruction datasets including Open Assistant, OpenOrca, ShareGPT & Chatlogs, and Evol-CodeAlpaca.

## Key Results
- Lemur-Chat significantly outperforms existing open-source models on 12 of 13 agent benchmarks
- The model demonstrates strong abilities in tool usage, self-debugging, following feedback, and exploring partially observable environments
- Harmonized language and coding capabilities narrow the performance gap with proprietary models like GPT-4 on agent abilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Harmonizing natural language and code training improves agent capabilities beyond domain-specific specialization.
- Mechanism: The model learns dual competencies through code-intensive pre-training and mixed text-code instruction fine-tuning, enabling it to seamlessly switch between reasoning in natural language and generating executable code actions.
- Core assumption: The agent's effectiveness depends on its ability to ground language-based reasoning in executable symbolic actions.
- Evidence anchors:
  - [abstract]: "harmonious blend of language and coding capabilities in the models"
  - [section]: "Through meticulous pre-training using a code-intensive corpus and instruction fine-tuning on text and code data"
  - [corpus]: Weak evidence - the related papers don't directly validate this mechanism
- Break condition: If the model cannot effectively translate natural language reasoning into valid executable actions, or if the code generation quality degrades natural language reasoning.

### Mechanism 2
- Claim: Multi-turn feedback incorporation improves model performance through iterative refinement.
- Mechanism: The model uses environmental feedback (error messages, execution results) and human feedback to iteratively correct its generated actions, improving success rates over time.
- Core assumption: Language models can effectively parse and utilize feedback from different sources (environment vs human) to refine their outputs.
- Evidence anchors:
  - [section]: "This adaptive capacity is essential for agents because they have to constantly receive and react to feedback from the environment"
  - [section]: "Lemur improves consistently across ten interactive rounds, surpassing the performance of gpt-3.5-turbo eventually"
  - [corpus]: Weak evidence - the related papers don't directly validate this mechanism
- Break condition: If feedback is ambiguous, contradictory, or the model fails to map feedback to corrective actions.

### Mechanism 3
- Claim: Partially observable environment exploration requires balanced reasoning and execution abilities.
- Mechanism: In environments where complete information isn't available, the model must plan, explore, gather information, and execute actions based on partial observations, requiring both strong reasoning and code execution capabilities.
- Core assumption: Agents need to balance information gathering with action execution in uncertain environments.
- Evidence anchors:
  - [section]: "agents can only partially observe the environmental information required to solve problems. This requires agents to collect information through exploration"
  - [section]: "Lemur-70B-Chat exhibits balanced and commendable performance across all tested tasks"
  - [corpus]: Weak evidence - the related papers don't directly validate this mechanism
- Break condition: If the model over-explores (wasting resources) or under-explores (missing critical information), or if it cannot plan effective exploration strategies.

## Foundational Learning

- Concept: Transfer learning from natural language to code
  - Why needed here: The model builds on existing language capabilities and extends them to code generation through targeted pre-training
  - Quick check question: What happens to natural language performance when code capability is added through transfer learning?

- Concept: Instruction fine-tuning for multi-turn interaction
  - Why needed here: Standard pre-training doesn't teach the model to handle conversational context and feedback loops necessary for agent behavior
  - Quick check question: How does the model maintain context across multiple turns of interaction?

- Concept: Feedback incorporation mechanisms
  - Why needed here: Agents must adapt their behavior based on environmental responses and human guidance
  - Quick check question: What types of feedback can the model successfully incorporate, and what types does it struggle with?

## Architecture Onboarding

- Component map: Llama-2-70B base model -> Code-intensive corpus pre-training (90B tokens) -> Mixed text-code instruction fine-tuning (300K examples) -> Agent evaluation

- Critical path: Pre-training → Instruction fine-tuning → Agent evaluation → Feedback incorporation → Performance improvement

- Design tradeoffs: Balancing text and code data ratios, choosing appropriate feedback mechanisms, determining exploration vs exploitation strategies

- Failure signatures:
  - Degraded natural language performance after code-focused pre-training
  - Inability to incorporate feedback from environment
  - Poor exploration strategies in partially observable environments

- First 3 experiments:
  1. Test pre-training on different text-to-code ratios to find optimal balance
  2. Evaluate feedback incorporation across different task types (reasoning vs coding)
  3. Compare exploration strategies in partially observable environments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the 10:1 text-to-code ratio in the pre-training corpus specifically affect the model's performance on agent tasks versus traditional language and coding benchmarks?
- Basis in paper: [explicit] The paper mentions using a code-intensive corpus with a 10:1 text-to-code ratio to improve coding ability while maintaining natural language performance.
- Why unresolved: The paper does not provide a detailed analysis of how different ratios might impact performance on agent-specific tasks versus traditional benchmarks.
- What evidence would resolve it: Comparative experiments varying the text-to-code ratio and analyzing performance differences on agent benchmarks and traditional benchmarks.

### Open Question 2
- Question: What is the impact of using different intermediate representations (e.g., Python vs. direct action prediction) on agent performance in partially observable environments?
- Basis in paper: [explicit] The paper mentions that mapping to Python representations leads to better performance than directly predicting actions in WebArena.
- Why unresolved: The paper does not explore this systematically across different partially observable environments or compare other intermediate representations.
- What evidence would resolve it: Systematic experiments comparing different intermediate representations across multiple partially observable environments.

### Open Question 3
- Question: How does the model's ability to follow natural language feedback compare to its ability to incorporate environmental feedback in complex agent scenarios?
- Basis in paper: [explicit] The paper discusses both following natural language feedback and incorporating environmental feedback, but does not directly compare their relative importance or effectiveness.
- Why unresolved: The paper presents results for both types of feedback but does not analyze their comparative impact on agent performance.
- What evidence would resolve it: Experiments isolating the effects of natural language feedback versus environmental feedback on agent performance in complex scenarios.

## Limitations
- The specific composition of the code-intensive corpus beyond the general 10:1 text-to-code ratio is not fully specified
- The paper lacks detailed error analysis showing exactly where and why the harmonized approach outperforms domain-specific models
- The feedback incorporation mechanism's effectiveness across different types of feedback is not thoroughly analyzed

## Confidence

**High Confidence**: The core claim that Lemur-Chat achieves state-of-the-art performance on agent benchmarks is well-supported by the experimental results across 13 different benchmarks.

**Medium Confidence**: The mechanism of harmonizing natural language and code through pre-training is plausible given the results, but the exact contribution of this approach versus other factors cannot be precisely isolated.

**Low Confidence**: The specific claims about why harmonization works at a mechanistic level are logical inferences that lack direct empirical validation in the paper.

## Next Checks

1. **Ablation study on pre-training data composition**: Systematically vary the text-to-code ratio in pre-training to determine the optimal balance and quantify how much each component contributes to downstream performance.

2. **Error analysis across benchmark types**: Conduct detailed analysis of failure cases across different benchmark categories to identify whether the harmonized model fails in ways that are fundamentally different from specialized models.

3. **Generalization test of feedback incorporation**: Test the model's ability to incorporate feedback across domains it wasn't explicitly trained on, to verify whether the feedback mechanism is truly learned or task-specific.