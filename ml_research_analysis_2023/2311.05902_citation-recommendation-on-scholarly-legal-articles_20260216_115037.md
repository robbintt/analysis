---
ver: rpa2
title: Citation Recommendation on Scholarly Legal Articles
arxiv_id: '2311.05902'
source_url: https://arxiv.org/abs/2311.05902
tags:
- https
- legal
- citation
- recommendation
- articles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the first scholarly legal citation recommendation
  dataset, gathering 719 scholarly legal articles with 10K incoming citation links
  from 9K articles. The study evaluates state-of-the-art models including BM25, Law2Vec,
  SciBERT, SPECTER, LegalBERT, SciNCL, and SGPT in four setups: direct use of pre-trained
  models, fine-tuning pre-trained models, pre-fetching with BM25 followed by re-ranking
  with pre-trained models, and pre-fetching with BM25 followed by re-ranking with
  fine-tuned models.'
---

# Citation Recommendation on Scholarly Legal Articles

## Quick Facts
- arXiv ID: 2311.05902
- Source URL: https://arxiv.org/abs/2311.05902
- Reference count: 40
- Models using BM25 pre-fetching + SciNCL re-ranking achieve 0.30 MAP@10 on scholarly legal citation recommendation

## Executive Summary
This paper introduces the first dataset for scholarly legal citation recommendation, consisting of 719 scholarly legal articles from LawArXiv with 10K incoming citation links. The study evaluates state-of-the-art citation recommendation models including BM25, Law2Vec, SciBERT, SPECTER, LegalBERT, SciNCL, and SGPT across four different setups: direct use of pre-trained models, fine-tuning pre-trained models, pre-fetching with BM25 followed by re-ranking with pre-trained models, and pre-fetching with BM25 followed by re-ranking with fine-tuned models. The results demonstrate that BM25 serves as a strong baseline for legal citation recommendation (0.26 MAP@10), and that the most effective approach combines BM25 pre-fetching with SciNCL re-ranking to achieve 0.30 MAP@10. Fine-tuning pre-trained models on the legal dataset significantly improves performance, highlighting the importance of domain-specific training data for these models.

## Method Summary
The study creates a novel dataset of scholarly legal articles from LawArXiv, then evaluates citation recommendation performance using four experimental setups. First, pre-trained models are used directly for citation recommendation. Second, models are fine-tuned on the legal dataset. Third, BM25 pre-fetches candidates which are re-ranked using pre-trained models. Fourth, BM25 pre-fetches candidates which are re-ranked using fine-tuned models. Performance is measured using MAP@10, Recall@10, and MRR@10 metrics. The fine-tuning process uses triplet loss with margin=1, batch size=64, and AdamW optimizer with learning rate 5e-5 over 5 epochs.

## Key Results
- BM25 achieves 0.26 MAP@10, establishing a strong baseline for legal citation recommendation
- Fine-tuning pre-trained models leads to significant performance improvements across all models tested
- BM25 pre-fetching followed by SciNCL re-ranking achieves the best performance at 0.30 MAP@10
- LegalBERT underperforms compared to other models, likely due to being trained on legal case law rather than scholarly articles

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning pre-trained models with legal scholarly articles significantly improves citation recommendation performance compared to using pre-trained models directly.
- **Mechanism:** Pre-trained models learn general language representations from their training corpora. When fine-tuned on domain-specific legal scholarly articles, they adapt their embeddings to capture legal domain semantics and citation patterns unique to legal literature.
- **Core assumption:** The gathered dataset contains sufficient diversity and quality of legal scholarly articles to effectively fine-tune the models for the citation recommendation task.
- **Evidence anchors:**
  - [abstract] "Fine-tuning leads to considerable performance increases in pre-trained models, which shows the importance of including legal articles in the training data of these models."
  - [section] "Performance increases on all models show that when domain knowledge is provided, those models are also able to adapt to the legal domain."
  - [corpus] Weak evidence - corpus contains related papers but none specifically about fine-tuning legal models for citation recommendation.
- **Break condition:** If the fine-tuning dataset lacks sufficient diversity in legal topics or contains too few citation examples, the performance gains from fine-tuning would diminish or reverse.

### Mechanism 2
- **Claim:** BM25 serves as a strong baseline for legal citation recommendation because legal documents have distinctive lexical patterns that traditional information retrieval methods can effectively capture.
- **Mechanism:** BM25 calculates relevance scores based on term frequency and inverse document frequency. Legal scholarly articles contain specific terminology, citations, and structural patterns that BM25 can leverage effectively.
- **Core assumption:** Legal scholarly articles contain sufficient distinctive lexical features that BM25 can exploit without requiring deep semantic understanding.
- **Evidence anchors:**
  - [abstract] "BM25 is a strong benchmark for the legal citation recommendation task, achieving 0.26 MAP@10."
  - [section] "BM25 exceeds the performance of all pre-trained models and shows that it is a strong baseline (0.26 MAP@10) for the legal CR task."
  - [corpus] Moderate evidence - related papers discuss BM25 effectiveness but not specifically for legal scholarly articles.
- **Break condition:** If legal articles become more diverse in terminology or if citation patterns become less formulaic, BM25's effectiveness would decrease relative to semantic approaches.

### Mechanism 3
- **Claim:** A two-stage approach combining BM25 pre-fetching with SciNCL re-ranking achieves optimal performance by balancing efficiency and semantic understanding.
- **Mechanism:** BM25 efficiently retrieves a broad set of potentially relevant documents using fast lexical matching. SciNCL then applies semantic understanding to re-rank these candidates, identifying documents with deeper conceptual relationships to the query.
- **Core assumption:** The initial BM25 retrieval produces a candidate set that contains most relevant documents, allowing the slower but more accurate SciNCL to effectively identify the best matches.
- **Evidence anchors:**
  - [abstract] "The most effective method involves implementing a two-step process that entails pre-fetching with BM25+, followed by re-ranking with SciNCL, which enhances the performance of the baseline from 0.26 to 0.30 MAP@10."
  - [section] "Overall, SciNCL stands out as the best performing model among others, improving performance of BM25 (from 0.26 to 0.30 MAP@10)."
  - [corpus] Weak evidence - no corpus papers specifically discuss this two-stage approach for legal citation recommendation.
- **Break condition:** If the BM25 pre-fetching stage fails to retrieve relevant documents, the re-ranking stage cannot recover performance, limiting the effectiveness of this approach.

## Foundational Learning

- **Concept:** Citation recommendation task formulation
  - Why needed here: Understanding the distinction between local and global citation recommendation is crucial for proper model selection and evaluation
  - Quick check question: What is the key difference between local citation recommendation (context-aware) and global citation recommendation (document-level)?

- **Concept:** Information retrieval evaluation metrics
  - Why needed here: MAP@10, Recall@10, and MRR@10 are used to evaluate model performance, requiring understanding of precision, recall, and ranking quality
  - Quick check question: How does MAP@10 differ from simple precision at 10 in evaluating citation recommendation systems?

- **Concept:** Pre-trained language model fine-tuning
  - Why needed here: The study demonstrates significant performance improvements through fine-tuning, requiring understanding of how pre-trained models adapt to new domains
  - Quick check question: What is the primary difference between using a pre-trained model directly versus fine-tuning it on a domain-specific dataset?

## Architecture Onboarding

- **Component map:** Data preprocessing -> BM25 retrieval -> Model embedding computation -> Re-ranking -> Output ranked list
- **Critical path:** The end-to-end workflow follows this sequence: (1) preprocess query document to extract abstract, (2) run BM25 retrieval to get top-10 candidates, (3) compute semantic similarity scores using the chosen model (pre-trained or fine-tuned), (4) re-rank candidates based on similarity scores, (5) return ranked list. For the baseline, step 2 is skipped and BM25 results are used directly.
- **Design tradeoffs:** The system balances between lexical matching (BM25) and semantic understanding (transformer models). BM25 provides speed and strong baseline performance but lacks semantic depth. Transformer models capture deeper meaning but are computationally expensive. The two-stage approach with BM25 pre-fetching optimizes for both efficiency and effectiveness. Fine-tuning adds domain adaptation but requires additional computational resources and dataset curation.
- **Failure signatures:** Performance degradation typically manifests as low MAP@10 scores (below 0.2) or high variance across queries. Common failure modes include: (1) insufficient legal domain representation in pre-training data, visible as poor performance on domain-specific terminology; (2) dataset imbalance or insufficient citation examples, causing overfitting during fine-tuning; (3) suboptimal hyperparameter choices for triplet loss or fine-tuning epochs; (4) poor abstract extraction leading to noisy query representations.
- **First 3 experiments:**
  1. Baseline evaluation: Run BM25 on the test set with default parameters and measure MAP@10, Recall@10, and MRR@10 to establish the performance floor.
  2. Pre-trained model comparison: Evaluate all six pre-trained models (BM25, Law2Vec, SciBERT, SPECTER, LegalBERT, SciNCL, SGPT) on the test set to identify the best-performing architecture without fine-tuning.
  3. Fine-tuning impact: Select the top-2 pre-trained models from experiment 2 and fine-tune them on the training set, then evaluate on the test set to measure the impact of domain adaptation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the citation recommendation models perform if trained and evaluated on a larger dataset of scholarly legal articles?
- Basis in paper: [explicit] The paper notes that the current dataset size is small compared to other domains in citation recommendation literature, which might affect model performance.
- Why unresolved: The study was limited to the available LawArXiv collection, which is relatively small compared to datasets in other scientific domains.
- What evidence would resolve it: Experimental results comparing model performance on an expanded dataset with significantly more scholarly legal articles would provide clarity.

### Open Question 2
- Question: Would incorporating additional features beyond abstracts (such as full text, citations, or author information) improve citation recommendation performance in the legal domain?
- Basis in paper: [inferred] The paper only uses abstracts as input for the citation recommendation task, similar to other content-based global citation recommendation studies, but does not explore the potential benefits of additional features.
- Why unresolved: The experiments were constrained to using abstracts to maintain consistency with existing approaches, but the impact of richer features remains untested.
- What evidence would resolve it: Comparative experiments using different feature sets (abstracts only vs. abstracts + full text vs. abstracts + citation network) would demonstrate the value of additional information.

### Open Question 3
- Question: How would citation recommendation models perform on scholarly legal articles from domains outside LawArXiv, such as law journals not indexed in this repository?
- Basis in paper: [explicit] The paper suggests that future work could involve enlarging the dataset with articles from law journals not indexed within LawArXiv.
- Why unresolved: The current dataset is limited to LawArXiv articles, which may not represent the full diversity of scholarly legal literature.
- What evidence would resolve it: Evaluation of citation recommendation models on a dataset that includes articles from various law journals would reveal whether model performance generalizes beyond LawArXiv.

## Limitations

- The dataset contains only 719 scholarly legal articles, which may limit the generalizability of results to broader legal citation patterns
- Performance metrics may not fully capture the practical utility of these systems in real-world legal research scenarios
- The focus on abstract-level matching rather than full-text or context-aware citation recommendation represents a simplification that could affect real-world applicability

## Confidence

- **High Confidence:** The baseline BM25 performance (0.26 MAP@10) and the two-stage approach effectiveness (0.30 MAP@10) are well-supported by the experimental results
- **Medium Confidence:** The fine-tuning performance improvements, while substantial, depend on the quality and representativeness of the small dataset
- **Low Confidence:** The absolute performance values (0.26-0.30 MAP@10) should be interpreted cautiously due to the dataset size and the specific nature of legal scholarly articles used

## Next Checks

1. **Dataset Expansion Validation:** Replicate the study with a significantly larger corpus of legal scholarly articles (target: 2000+ articles) to verify whether the observed performance patterns scale with dataset size and diversity.

2. **Cross-Domain Generalization:** Test the best-performing models (SciNCL with BM25 pre-fetching) on a different legal corpus, such as case law or legislation, to assess domain transferability beyond scholarly articles.

3. **Real-World Utility Assessment:** Conduct a user study with legal researchers to evaluate whether the recommended citations are practically useful, comparing system recommendations against expert-identified relevant citations in actual research workflows.