---
ver: rpa2
title: Learning Diverse Risk Preferences in Population-based Self-play
arxiv_id: '2305.11476'
source_url: https://arxiv.org/abs/2305.11476
tags:
- uni00000013
- uni00000011
- risk
- learning
- uni00000018
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a population-based self-play method that
  incorporates risk-sensitive learning. The core idea is to use an expectile Bellman
  operator to interpolate between worst-case and best-case policy learning, allowing
  agents to learn with diverse risk preferences.
---

# Learning Diverse Risk Preferences in Population-based Self-play

## Quick Facts
- **arXiv ID:** 2305.11476
- **Source URL:** https://arxiv.org/abs/2305.11476
- **Reference count:** 40
- **Primary result:** Introduces population-based self-play with risk-sensitive learning that achieves superior or comparable performance against existing baselines in competitive games like Slimevolley and SumoAnts.

## Executive Summary
This paper introduces a population-based self-play method that incorporates risk-sensitive learning through an expectile Bellman operator. The approach allows agents to learn with diverse risk preferences by interpolating between worst-case and best-case policy learning. By integrating this with population-based training and dynamic risk-level tuning, the method generates diverse strategies that achieve superior or comparable performance against existing population-based baselines in competitive games.

## Method Summary
The method uses Risk-sensitive Proximal Policy Optimization (RPPO) with an expectile Bellman operator that smoothly interpolates between risk-averse and risk-seeking behaviors controlled by parameter τ. Population-based training dynamically tunes risk levels through exploitation (copying parameters from better-performing agents) and exploration (perturbing τ values). A multi-step expectile Bellman operator balances bias-variance tradeoff using exponentially-weighted returns, enabling stable learning while maintaining risk-sensitive properties.

## Key Results
- Achieves superior or comparable performance against existing population-based baselines in competitive games
- Generates diverse strategies through risk-sensitive learning with different τ values
- Demonstrates effective population-based training with dynamic risk-level tuning
- Successfully applies to Slimevolley and SumoAnts environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Risk-sensitive learning introduces strategic diversity by enabling agents to optimize different risk-return profiles.
- Mechanism: The expectile Bellman operator smoothly interpolates between worst-case (risk-averse) and best-case (risk-seeking) value functions, controlled by risk parameter τ. This creates distinct strategic modes without requiring explicit diversity objectives.
- Core assumption: Different τ values produce meaningfully different policies that can compete effectively against each other.
- Evidence anchors:
  - [abstract]: "smoothly interpolates between worst-case and best-case policy learning"
  - [section 4.1]: Proposition 3 shows expectile operator approaches best-case Bellman as τ→1 and worst-case as τ→0
  - [corpus]: No direct evidence, but related work on risk-sensitive RL supports this mechanism
- Break condition: If different τ values produce similar policies or if one τ value dominates all others, diversity benefit disappears.

### Mechanism 2
- Claim: Population-based training with dynamic risk tuning discovers optimal risk level distributions.
- Mechanism: Under-performing agents copy parameters from better-performing agents and perturb their τ values, enabling exploration of the risk level space while maintaining performance.
- Core assumption: ELO-based selection identifies genuinely better-performing agents whose risk preferences are worth copying.
- Evidence anchors:
  - [abstract]: "agents in the population optimize dynamic risk-sensitive objectives"
  - [section 5]: Describes exploitation (copying) and exploration (perturbing τ) steps
  - [corpus]: Related work on population-based training supports this general framework
- Break condition: If ELO scores don't correlate with actual performance against diverse opponents, wrong agents get copied.

### Mechanism 3
- Claim: Multi-step expectile Bellman operator balances bias-variance tradeoff for stable learning.
- Mechanism: Extends GAE framework to expectile Bellman operator, using exponentially-weighted multi-step returns to reduce variance while maintaining risk-sensitive properties.
- Core assumption: The nonlinear expectile operator can be effectively approximated using multi-step samples without destroying risk properties.
- Evidence anchors:
  - [section 4.2]: Extends GAE to RPPO and shows contraction properties are preserved
  - [section 4.2]: Proves multi-step operator maintains contraction and risk-sensitivity properties
  - [corpus]: Standard GAE theory supports this extension approach
- Break condition: If multi-step approximation introduces too much bias or if variance reduction is insufficient for stable training.

## Foundational Learning

- Concept: Expectiles and their relationship to risk preferences
  - Why needed here: Core mechanism for interpolating between risk-averse and risk-seeking behaviors
  - Quick check question: What value of τ corresponds to risk-neutral behavior, and why?

- Concept: Bellman operators and contraction mappings
  - Why needed here: Understanding why expectile Bellman operator converges and preserves risk properties
  - Quick check question: How does the contraction coefficient γτ change as τ approaches 0 or 1?

- Concept: Population-based training and exploration-exploitation
  - Why needed here: Framework for discovering good risk level distributions across population
  - Quick check question: What happens if the perturbation range for τ exploration is too small or too large?

## Architecture Onboarding

- Component map:
  Risk-sensitive PPO core -> Multi-step advantage estimator -> Population manager -> Risk tuner -> Environment interface

- Critical path:
  1. Collect trajectories from self-play matches
  2. Compute multi-step expectile advantages
  3. Update policy and value networks using RPPO loss
  4. Rank agents by ELO
  5. Apply exploitation/exploration to poorly performing agents
  6. Add updated policies to policy pool

- Design tradeoffs:
  - Storage vs accuracy: Multi-step table requires O(nT) storage where n=steps, T=episodes
  - Exploration vs exploitation: Too much τ perturbation can destabilize training
  - Population size vs diversity: Larger populations explore more but increase computational cost

- Failure signatures:
  - All agents converge to same τ value (insufficient exploration)
  - Performance degrades over time (ELO selection not working)
  - Training instability (multi-step approximation too biased)
  - No diversity in strategies (expectile operator not creating distinct policies)

- First 3 experiments:
  1. Single-agent RPPO with fixed τ values: Verify risk-sensitive behavior emerges
  2. Two-agent population with extreme τ values: Test if diversity creates competitive cycles
  3. Full population with dynamic τ: Validate ELO-based risk tuning improves performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of RPBT scale with population size beyond 5 agents?
- Basis in paper: [explicit] The paper mentions "population size 5" but does not explore larger populations.
- Why unresolved: The authors only tested with population size 5, leaving scalability unexplored.
- What evidence would resolve it: Experiments comparing performance metrics (win rates, ELO scores) across different population sizes (e.g., 10, 20, 50) would clarify scalability effects.

### Open Question 2
- Question: How sensitive is RPBT's performance to the specific noise range used in the exploration step (±0.2)?
- Basis in paper: [explicit] The paper states "adding a random perturbed term ranging from -0.2 to 0.2" but does not explore other ranges.
- Why unresolved: The authors fixed the exploration noise range without testing sensitivity to different values.
- What evidence would resolve it: Experiments varying the noise range (e.g., ±0.1, ±0.3, ±0.5) and measuring resulting performance and diversity would clarify sensitivity.

### Open Question 3
- Question: How does RPBT perform in games with more than two agents or in cooperative multi-agent settings?
- Basis in paper: [inferred] The paper focuses on two-agent competitive games (Slimevolley, SumoAnts) but does not explore multi-agent or cooperative scenarios.
- Why unresolved: The authors limited experiments to two-agent competitive environments, leaving performance in other settings unknown.
- What evidence would resolve it: Testing RPBT in three-or-more agent competitive games and cooperative multi-agent tasks with appropriate metrics would address this limitation.

## Limitations
- Claims about diversity generation rely on empirical evidence rather than theoretical characterization of risk-preference relationships
- Computational complexity of multi-step expectile Bellman operator may limit scalability to complex environments
- ELO-based selection assumes relative performance rankings accurately reflect agent quality against diverse opponents

## Confidence
- **High Confidence:** Mathematical foundations of expectile operators and convergence properties (Propositions 1-3) are rigorously proven
- **Medium Confidence:** Population-based training framework with dynamic risk tuning is a reasonable extension of existing PBT methods
- **Medium Confidence:** Empirical results showing improved performance against baselines, though diversity characterization could be better

## Next Checks
1. Conduct systematic analysis of policy diversity by measuring state visitation distributions and action selection patterns across different τ values
2. Evaluate trained agents against hold-out set of opponents with risk preferences not seen during training
3. Measure computational overhead of multi-step expectile Bellman operator and assess scalability as episode length increases