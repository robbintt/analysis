---
ver: rpa2
title: 'FaceTouch: Detecting hand-to-face touch with supervised contrastive learning
  to assist in tracing infectious disease'
arxiv_id: '2308.12840'
source_url: https://arxiv.org/abs/2308.12840
tags:
- face
- learning
- faces
- hand
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FaceTouch is a computer vision framework using deep learning and
  supervised contrastive learning to detect hand-to-face touches in video streams,
  aiding infectious disease tracing. It employs object detection to locate humans
  and faces, then uses an action encoder to classify hand-to-face touch, even under
  occlusion.
---

# FaceTouch: Detecting hand-to-face touch with supervised contrastive learning to assist in tracing infectious disease

## Quick Facts
- arXiv ID: 2308.12840
- Source URL: https://arxiv.org/abs/2308.12840
- Reference count: 40
- Primary result: Achieves up to 98.02% accuracy and 0.931 precision in detecting hand-to-face touches using supervised contrastive learning

## Executive Summary
FaceTouch is a computer vision framework that uses deep learning and supervised contrastive learning to detect hand-to-face touches in video streams, aiding infectious disease tracing. The system leverages body gestures and arm movement patterns in addition to proximity detection, achieving high accuracy even under partial occlusion. It demonstrates effectiveness in real-world scenarios like video calls and CCTV footage while providing data privacy through face blurring and explainable AI components.

## Method Summary
The FaceTouch framework employs a dual backbone approach using object detection (YOLOv5) to locate humans and faces in video frames. When faces are detected, an action encoder (MobileNet, ResNet, or ViT) classifies hand-to-face touch actions using supervised contrastive learning. If faces are not detected, the system falls back to human detection to capture body poses. The framework achieves high accuracy (up to 98.02% with MobileNet and SCL) on a custom dataset of 10,413 images through transfer learning from ImageNet-pretrained models and careful training with SCL loss.

## Key Results
- Achieves up to 98.02% accuracy and 0.931 precision using MobileNet with supervised contrastive learning
- Processes video at 25 FPS with MobileNet backbone, enabling real-time deployment
- Demonstrates effectiveness across varied scenarios including video calls, bus footage, and CCTV feeds

## Why This Works (Mechanism)

### Mechanism 1
The system achieves high accuracy by leveraging body gestures and arm movement in addition to proximity detection. Instead of relying solely on hand and face localization and distance thresholds, the model learns from the full RGB representation of the scene, capturing contextual body movements that indicate intentional face touching. This approach is particularly effective when faces are occluded or partially visible.

### Mechanism 2
Supervised Contrastive Learning (SCL) improves classification performance over traditional supervised learning by maximizing the similarity between augmented views of the same sample while pushing apart different samples. This leads to more discriminative feature embeddings for binary classification, better exploiting the structure in the dataset compared to cross-entropy loss.

### Mechanism 3
The dual backbone approach (face detection first, then human detection fallback) maximizes detection coverage across varied scenarios. Initially attempting to detect faces directly for high-resolution cases (e.g., video calls), the system switches to human detection when faces are not detected or are occluded, capturing body poses in low-resolution or challenging settings (e.g., CCTV).

## Foundational Learning

- **Supervised Contrastive Learning (SCL)**: Why needed? Traditional supervised learning may not fully exploit the structure in the binary classification task, especially with imbalanced classes. SCL encourages the model to learn more discriminative features by pulling together positive pairs and pushing apart negative pairs. Quick check: How does SCL differ from triplet loss, and why might it be more effective for this task?

- **Object Detection with YOLOv5**: Why needed? The framework requires robust detection of humans (and faces) in complex scenes to localize regions of interest before classification. Quick check: What are the trade-offs between YOLOv5's speed and accuracy compared to other object detection models like Faster R-CNN?

- **Transfer Learning with Pre-trained Models**: Why needed? Leveraging models pre-trained on ImageNet allows the framework to benefit from learned feature representations, speeding up training and improving performance on a limited dataset. Quick check: Why is it beneficial to freeze the weights of the pre-trained layers during initial training?

## Architecture Onboarding

- **Component map**: Input (RGB image/frame) -> Backbone (Face detection → if no faces, Human detection) -> Action Encoder (MobileNet/ResNet/ViT with SCL) -> Privacy (Face blurring) -> Explainability (Grad-CAM) -> Output (Classification and localization)

- **Critical path**: Backbone detection → Action encoder classification → Privacy/Explainability → Output

- **Design tradeoffs**: Accuracy vs. speed (MobileNet offers 25 FPS but slightly lower accuracy than larger models like ResNet); Complexity vs. robustness (using both face and human detectors increases complexity but improves coverage); Privacy vs. utility (face blurring ensures privacy but may reduce explainability for debugging)

- **Failure signatures**: Low recall in low-resolution scenes (backbone detectors may miss humans/faces); High false positives near face (model may confuse close hand proximity with actual touch); Slow inference (using larger encoder models reduces FPS below real-time requirements)

- **First 3 experiments**:
  1. Validate backbone detection accuracy on VOC dataset and custom face dataset separately
  2. Train and evaluate action encoder with SCL using a small subset of the dataset to confirm improved performance over cross-entropy
  3. Integrate backbone and encoder, test on a mixed set of video frames (clear faces, occlusions, low-res) to measure overall system accuracy and FPS

## Open Questions the Paper Calls Out

- How does the FaceTouch framework perform in extremely low-resolution scenarios where both faces and hands are barely visible? The paper mentions testing in low-resolution CCTV footage but does not provide specific performance metrics for extremely low-resolution scenarios.

- How does the FaceTouch framework handle situations with multiple people touching their faces simultaneously in a crowded scene? The paper mentions testing in scenes with multiple humans but does not provide specific details on performance when multiple people are touching their faces simultaneously.

- How does the FaceTouch framework perform when there are significant occlusions of both faces and hands, such as when a person is wearing a mask or gloves? The paper mentions testing in scenarios with partial face occlusions but does not provide specific details on performance when both faces and hands are significantly occluded.

## Limitations

- The custom FaceTouch dataset of 10,413 images is not publicly available, making independent validation difficult
- Lacks detailed ablation studies comparing SCL to other contrastive methods like triplet loss or InfoNCE
- The explainability component using Grad-CAM is mentioned but not thoroughly validated with user studies or real-world debugging scenarios

## Confidence

- **High confidence**: The dual backbone detection approach is well-justified by the need for robustness across varied scenarios and is supported by the reported high accuracy and precision metrics
- **Medium confidence**: The effectiveness of SCL for this task is supported by empirical results showing improved accuracy and F1-score, but lacks direct comparison to other contrastive learning methods or detailed ablation studies
- **Low confidence**: The explainability component using Grad-CAM is mentioned but not thoroughly validated with user studies or real-world debugging scenarios

## Next Checks

1. Reproduce backbone detection accuracy: Train and evaluate YOLOv5l on VOC dataset to verify mAP > 0.75, then test on a subset of FaceTouch images to confirm human/face detection performance

2. Validate SCL effectiveness: Conduct controlled experiments comparing SCL to cross-entropy and triplet loss on a small labeled dataset to confirm improved feature separation

3. Test privacy and explainability integration: Implement face blurring and Grad-CAM visualization, then assess their impact on model debugging and user trust through qualitative analysis