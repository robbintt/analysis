---
ver: rpa2
title: 'The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal
  Data for Large Language Models'
arxiv_id: '2309.06236'
source_url: https://arxiv.org/abs/2309.06236
tags:
- data
- llms
- arxiv
- language
- temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of tokenizing temporal data
  (such as sensor readings) for large language models (LLMs), which struggle with
  numerical values and temporal relationships due to their text-centric design. It
  demonstrates that popular LLMs incorrectly tokenize temporal data, splitting timestamps
  and sensor values into disjointed tokens, losing temporal context.
---

# The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models

## Quick Facts
- arXiv ID: 2309.06236
- Source URL: https://arxiv.org/abs/2309.06236
- Reference count: 40
- Primary result: LLMs incorrectly tokenize temporal data, splitting timestamps and sensor values into disjointed tokens, losing temporal context.

## Executive Summary
This paper addresses the challenge of tokenizing temporal data (such as sensor readings) for large language models (LLMs), which struggle with numerical values and temporal relationships due to their text-centric design. It demonstrates that popular LLMs incorrectly tokenize temporal data, splitting timestamps and sensor values into disjointed tokens, losing temporal context. The paper highlights solutions like prompt tuning with lightweight embedding layers and multimodal adapters to bridge the "modality gap" between numerical data and language models. While preliminary works show promise using LLMs for tasks like activity recognition and health monitoring, they rely heavily on handcrafted prompts and preprocessing. The authors argue that meaningful integration of temporal data into LLMs requires better encoding methods and parameter-efficient transfer learning approaches.

## Method Summary
The paper analyzes how LLMs tokenize temporal data by examining existing LLM tokenizers (e.g., OpenAI tokenizer) on temporal datasets like WISDM activity recognition data. It proposes solutions including prompt tuning with lightweight embedding layers and multimodal adapters to improve temporal data representation. The methodology involves implementing these approaches and comparing their performance against baseline LLM performance on temporal reasoning tasks. The paper discusses theoretical frameworks and conceptual implementations but lacks detailed experimental procedures or quantitative validation.

## Key Results
- LLMs incorrectly tokenize temporal data, splitting timestamps and sensor values into disjointed tokens
- Popular tokenizers struggle with repetitive patterns in time-series data, fragmenting continuous sequences
- Proposed solutions include prompt tuning with lightweight embedding layers and multimodal adapters to bridge the "modality gap"

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs tokenize numerical and temporal data incorrectly because their tokenizers were not designed to represent numbers and patterns in time-series.
- Mechanism: The tokenizer splits numbers and timestamps into arbitrary subword units, breaking up continuous sequences and losing temporal context. For example, "3.14159" becomes four different tokens instead of one numeric value.
- Core assumption: The LLM tokenizer's subword segmentation rules apply uniformly to numbers and timestamps as they do to natural language.
- Evidence anchors:
  - [abstract] "LLMs employ tokenizers in their input that break down text into smaller units. However, tokenizers are not designed to represent numerical values and might struggle to understand repetitive patterns and context, treating consecutive values as separate tokens and disregarding their temporal relationships."
  - [section] "The tokenizers employed by LLMs appear to stumble when grappling with numerical inputs. Repetitive patterns, an inherent characteristic of time-series data, can confound tokenizers, leading to the unintentional fragmentation of continuous sequences into disjointed tokens."
  - [corpus] Weak evidence - no directly related papers in the corpus focus on LLM tokenization issues for temporal data specifically.
- Break condition: If the tokenizer is replaced with a numerical-aware tokenizer or if numbers are pre-processed into consistent string representations before tokenization.

### Mechanism 2
- Claim: Prompt tuning with lightweight embedding layers can improve LLM performance on temporal data tasks by adding a trainable layer that helps the model understand numerical patterns.
- Mechanism: A soft prompt or prefix-tuning layer is prepended to the LLM input, which is trained on a small set of examples to encode the numerical patterns in a way the base LLM can better process. This is more parameter-efficient than full fine-tuning.
- Core assumption: The soft prompt layer can learn an effective numerical representation that bridges the modality gap without needing to retrain the entire LLM.
- Evidence anchors:
  - [abstract] "To address that, we highlight potential solutions such as prompt tuning with lightweight embedding layers as well as multimodal adapters, that can help bridge this 'modality gap'."
  - [section] "Parameter-Efficient Fine-Tuning (PEFT) has emerged as a potential solution to this problem... Prompt tuning refers to techniques that change the LLM prompt to achieve better results."
  - [corpus] No direct evidence in corpus, but related papers discuss fine-tuning and adapter methods.
- Break condition: If the soft prompt layer cannot learn an effective numerical representation, or if the base LLM's tokenization remains a bottleneck.

### Mechanism 3
- Claim: Multimodal adapters can map non-textual data like time-series into the LLM's token embedding space, enabling better integration of numerical data.
- Mechanism: Separate encoders learn to map different modalities (e.g. ECG, accelerometer data) into a shared token embedding space that the LLM can process. This is similar to how image and text models like ImageBind and BLIP2 work.
- Core assumption: The modality-specific encoders can learn effective numerical representations that are compatible with the LLM's token embeddings.
- Evidence anchors:
  - [abstract] "To address that, we highlight potential solutions such as prompt tuning with lightweight embedding layers as well as multimodal adapters, that can help bridge this 'modality gap'."
  - [section] "By viewing the mismatch between numbers and text as a multimodal understanding problem, the notion of model grafting has emerged as another solution... HeLM, non-text modalities are mapped via encoders (trained over input examples) into the same token embedding space as text."
  - [corpus] No direct evidence in corpus, but related papers discuss multimodal models and adapters.
- Break condition: If the modality-specific encoders cannot learn effective numerical representations, or if the mapping to token embeddings is not meaningful for the LLM.

## Foundational Learning

- Concept: Subword tokenization (e.g. Byte-Pair Encoding)
  - Why needed here: Understanding how LLMs break down text into tokens is key to grasping why they struggle with numbers and temporal data.
  - Quick check question: How does BPE tokenization work, and why is it effective for natural language but not necessarily for numerical data?

- Concept: Parameter-efficient fine-tuning (PEFT)
  - Why needed here: PEFT methods like prompt tuning are proposed solutions to adapt LLMs to temporal data without full fine-tuning.
  - Quick check question: What is the difference between prompt tuning, prefix tuning, and full fine-tuning in terms of parameter efficiency and effectiveness?

- Concept: Multimodal representation learning
  - Why needed here: Multimodal adapters are another proposed solution that requires mapping numerical data into the LLM's token embedding space.
  - Quick check question: How do models like ImageBind and BLIP2 map images into a shared embedding space with text, and how could this approach be applied to time-series data?

## Architecture Onboarding

- Component map: Data preprocessing -> Tokenizer -> LLM -> Prompt tuning layer (optional) -> Multimodal adapter (optional) -> Task-specific output layer

- Critical path:
  1. Preprocess raw temporal data into consistent string format
  2. Tokenize the data using the LLM's tokenizer
  3. Apply prompt tuning or multimodal adapter if using
  4. Feed tokens into LLM
  5. Generate task-specific output

- Design tradeoffs:
  - Prompt tuning vs. full fine-tuning: Parameter efficiency vs. potential performance gain
  - Multimodal adapter vs. prompt tuning: Complexity of training separate encoders vs. simplicity of prompt tuning
  - Data preprocessing vs. raw input: Consistency of tokenization vs. loss of information

- Failure signatures:
  - Inconsistent tokenization of numerical values (e.g. "480" as one token, "481" as two)
  - Loss of temporal context (timestamps split into separate tokens)
  - Poor performance on numerical reasoning tasks despite prompt tuning

- First 3 experiments:
  1. Test the base LLM on a simple numerical reasoning task (e.g. adding two-digit numbers) to confirm tokenization issues
  2. Apply prompt tuning to the base LLM and re-test numerical reasoning performance
  3. Implement a multimodal adapter with a simple time-series encoder and compare performance to prompt tuning approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are parameter-efficient fine-tuning methods like prompt tuning compared to full fine-tuning for integrating temporal data into LLMs?
- Basis in paper: [explicit] The paper discusses prompt tuning and compares it to zero-shot and prompt engineering methods, but does not provide a comprehensive comparison with full fine-tuning.
- Why unresolved: The paper mentions prompt tuning as a promising approach but does not provide empirical evidence comparing its effectiveness to full fine-tuning.
- What evidence would resolve it: Experiments comparing the performance of prompt tuning, full fine-tuning, and other parameter-efficient methods on temporal data tasks would resolve this question.

### Open Question 2
- Question: What is the optimal tokenizer design for handling numerical and temporal data in LLMs?
- Basis in paper: [explicit] The paper highlights the challenges of tokenizing numerical values and temporal relationships, suggesting that current tokenizers are not well-suited for this task.
- Why unresolved: The paper does not propose a specific tokenizer design or provide empirical evidence on the effectiveness of different tokenizer designs for numerical and temporal data.
- What evidence would resolve it: Experiments comparing the performance of different tokenizer designs on numerical and temporal data tasks would resolve this question.

### Open Question 3
- Question: How can we effectively combine text and numerical data in prompts to improve LLM performance on temporal data tasks?
- Basis in paper: [explicit] The paper discusses the importance of contextual information in prompts and mentions that providing textual context improves LLM performance on temporal data tasks.
- Why unresolved: The paper does not provide a systematic approach to combining text and numerical data in prompts or empirical evidence on the effectiveness of different prompt designs.
- What evidence would resolve it: Experiments comparing the performance of different prompt designs on temporal data tasks would resolve this question.

## Limitations

- The paper presents theoretical arguments about tokenization challenges but lacks empirical validation and quantitative measurements
- No experimental results showing actual tokenization failures, performance metrics before/after proposed solutions, or ablation studies
- Does not address whether simpler preprocessing approaches might be more effective than complex adapter architectures

## Confidence

- High Confidence: The observation that LLMs were primarily designed for text and may struggle with numerical temporal data is well-established in the literature.
- Medium Confidence: The characterization of the "modality gap" and identification of prompt tuning and multimodal adapters as potential solutions aligns with current PEFT research trends.
- Low Confidence: The assertion that these specific solutions will effectively bridge the modality gap for temporal data tasks is currently unsupported by empirical evidence.

## Next Validation Checks

1. **Quantify Tokenization Quality**: Measure how popular LLM tokenizers (GPT-3, BERT, etc.) actually tokenize temporal data by computing metrics like consistency (do "480" and "481" get tokenized similarly?), token count per numerical value, and whether temporal relationships are preserved across token boundaries.

2. **Baseline Temporal Task Performance**: Test existing LLMs on simple temporal reasoning tasks (time series prediction, timestamp interpretation, numerical sequence completion) to establish baseline performance and identify specific failure modes related to tokenization versus other factors.

3. **Implement and Evaluate Prompt Tuning**: Create a minimal implementation of prompt tuning for temporal data using a simple time series dataset, measuring whether parameter-efficient fine-tuning can recover performance lost due to tokenization issues, and comparing against full fine-tuning baselines.