---
ver: rpa2
title: Self-Supervised Contrastive Learning for Robust Audio-Sheet Music Retrieval
  Systems
arxiv_id: '2309.12134'
source_url: https://arxiv.org/abs/2309.12134
tags:
- music
- audio
- retrieval
- sheet
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates using self-supervised contrastive learning\
  \ to address data scarcity in audio\u2013sheet music retrieval. The approach pre-trains\
  \ modality-specific encoders by contrasting augmented views of audio and sheet music\
  \ snippets, then fine-tunes them for cross-modal embedding learning."
---

# Self-Supervised Contrastive Learning for Robust Audio-Sheet Music Retrieval Systems

## Quick Facts
- arXiv ID: 2309.12134
- Source URL: https://arxiv.org/abs/2309.12134
- Reference count: 40
- Primary result: Self-supervised contrastive learning significantly improves audio-sheet music retrieval, achieving 100% R@1 on real data compared to 30% without pre-training.

## Executive Summary
This work addresses data scarcity in audio-sheet music retrieval by using self-supervised contrastive learning to pre-train modality-specific encoders. The approach learns invariant representations through augmented views of audio and sheet music snippets, then fine-tunes for cross-modal embedding learning. Experiments show substantial improvements in snippet retrieval and piece identification, with the method reducing the performance gap between synthetic and real data. The system outperforms fully supervised baselines while requiring less labeled data.

## Method Summary
The method employs a two-stage approach: first, pre-training audio and sheet music encoders independently using self-supervised contrastive learning on large unlabeled datasets with data augmentations; second, fine-tuning the pre-trained encoders for cross-modal retrieval using pairwise ranking loss. The system learns a shared 32-dimensional embedding space where audio and corresponding sheet music snippets are close together. Evaluation uses both snippet-level retrieval and piece identification through vote aggregation.

## Key Results
- Snippet retrieval: Improves from 30% to 100% R@1 on real data when using self-supervised pre-training
- Synthetic-to-real generalization: Significantly reduces performance gap between synthetic and real data scenarios
- Outperforms fully supervised baselines while requiring less labeled data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised contrastive pre-training improves generalization from synthetic to real data by exposing the network to diverse real-world variations.
- Mechanism: The model learns invariant representations through contrastive loss, where different augmented views of the same sample are pulled together in embedding space while being pushed apart from other samples. This creates robust features that generalize beyond the narrow distribution of synthetic data.
- Core assumption: The augmentations applied during pre-training (shifts, noise, distortions, elastic deformations for sheet music; time shifts, polarity inversion, masking for audio) sufficiently capture the real-world variations present in actual music data.
- Evidence anchors:
  - [abstract] "we investigate whether we can mitigate this limitation with self-supervised contrastive learning, by exposing a network to a large amount of real music data as a pre-training step, by contrasting randomly augmented views of snippets of both modalities"
  - [section 3.2] "The idea is to train a network encoder to be as invariant as possible concerning a set of given augmentation transforms"
- Break condition: If augmentations fail to capture key real-world variations, the pre-trained model will not generalize well to real data despite good performance on synthetic data.

### Mechanism 2
- Claim: Pre-training each modality independently allows for flexible model adaptation and improved cross-modal alignment.
- Mechanism: By pre-training audio and sheet music encoders separately, each can learn modality-specific representations that are then fine-tuned together for the cross-modal retrieval task. This two-stage approach leverages unlabeled data for each modality before learning cross-modal correspondences.
- Core assumption: Modality-specific pre-training provides useful initialization that accelerates and improves cross-modal fine-tuning compared to random initialization.
- Evidence anchors:
  - [section 3.2] "Since the pathways for audio and sheet music are independent, we can simply select the modality we wish to pre-train, and obtain a pre-trained encoder for the given modality"
  - [section 5.4] "we can load either or both encoders with parameters that were pre-learned before training"
- Break condition: If the modality-specific features learned during pre-training are not transferable to the cross-modal task, the two-stage approach will not provide benefits over end-to-end training.

### Mechanism 3
- Claim: The vote-based aggregation method for piece identification effectively leverages improved snippet-level embeddings to achieve high-level retrieval performance.
- Mechanism: By embedding multiple snippets from a document query and retrieving top neighbors for each, then aggregating votes by document, the system can identify complete pieces even when individual snippet matches are imperfect. This creates a robust matching signal.
- Core assumption: The improved snippet embeddings from pre-training translate into better document-level retrieval through aggregation, even if individual snippet matches are not perfect.
- Evidence anchors:
  - [section 5.5] "The document achieving the highest count among all 2,500 votes is selected as the best match"
  - [section 5.5] "A joint analysis with Table 1 reveals that overall the models with better piece identification results also exhibit a better matching quality statistics"
- Break condition: If the snippet embeddings are not sufficiently discriminative, the aggregation will not produce reliable document-level matches despite the voting mechanism.

## Foundational Learning

- Concept: Contrastive learning and the InfoMax principle
  - Why needed here: Understanding how contrastive loss works is essential for implementing and debugging the pre-training stage, and for tuning hyperparameters like temperature and augmentation strength
  - Quick check question: What is the difference between the contrastive loss used here and triplet loss, and when would one be preferred over the other?

- Concept: Multi-modal embedding spaces and cross-modal retrieval
  - Why needed here: The core task is learning a shared embedding space where audio and sheet music snippets from the same musical content are close together, which requires understanding both the mathematical formulation and practical implementation considerations
  - Quick check question: How does the pairwise ranking loss in the fine-tuning stage differ from the contrastive loss used in pre-training, and why are both needed?

- Concept: Data augmentation for different modalities
  - Why needed here: The augmentation pipeline is critical for the success of self-supervised pre-training, and requires understanding what transformations are meaningful and effective for each modality
  - Quick check question: Why are certain augmentations like elastic deformation more appropriate for sheet music than for audio, and what audio-specific augmentations serve a similar purpose?

## Architecture Onboarding

- Component map: Augmentation pipeline -> Modality-specific encoder (VGG-style) -> MLP projection head -> Contrastive loss (pre-training) OR CCA layer + Pairwise ranking loss (fine-tuning) -> 32-dimensional shared embedding space

- Critical path:
  1. Data preparation and augmentation
  2. Pre-training audio and/or sheet music encoders
  3. Loading pre-trained encoders into cross-modal network
  4. Fine-tuning for audio-sheet music retrieval
  5. Inference and evaluation

- Design tradeoffs:
  - Pre-training vs. end-to-end training: Pre-training leverages unlabeled data but adds complexity and training time
  - Augmentation strength: Stronger augmentations improve generalization but may hurt representation quality if too extreme
  - Embedding dimensionality: 32 dimensions balance representational capacity with computational efficiency

- Failure signatures:
  - Poor pre-training validation loss: Augmentation pipeline may be broken or too aggressive
  - Good pre-training but poor fine-tuning: Modality-specific features may not transfer well to cross-modal task
  - Good individual modality retrieval but poor cross-modal retrieval: CCA layer or fine-tuning stage may need adjustment

- First 3 experiments:
  1. Pre-train only the sheet music encoder on IMSLP data with the full augmentation pipeline, evaluate on MSMD test set snippet retrieval
  2. Pre-train only the audio encoder on MAESTRO data, evaluate on MSMD test set snippet retrieval
  3. Pre-train both encoders and fine-tune the full cross-modal model, evaluate on RealScores_Synth dataset to test synthetic-to-real generalization

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important unresolved issues emerge from the research:

### Open Question 1
- Question: How would the proposed self-supervised contrastive learning approach perform when applied to non-piano music genres (e.g., orchestral, vocal, or jazz)?
- Basis in paper: [inferred] The experiments focus exclusively on piano data; the paper mentions the method's potential generalizability but does not test it on other musical genres.
- Why unresolved: The method's robustness to different musical styles, instrumentation, and performance characteristics remains unverified.
- What evidence would resolve it: Experiments applying the same framework to datasets from diverse musical genres and comparing retrieval performance across them.

### Open Question 2
- Question: What is the optimal balance between synthetic and real data in pre-training to maximize retrieval performance on real-world scenarios?
- Basis in paper: [explicit] The paper investigates pre-training on synthetic vs. real data and observes performance differences, but does not explore hybrid pre-training strategies.
- Why unresolved: It is unclear whether combining synthetic and real data during pre-training could yield better results than using either alone.
- What evidence would resolve it: Controlled experiments varying the proportion of synthetic and real data in the pre-training stage and measuring downstream retrieval accuracy.

### Open Question 3
- Question: How sensitive is the system to variations in the augmentation pipeline, and can learned augmentations outperform hand-designed ones?
- Basis in paper: [explicit] The paper uses a fixed set of augmentation transforms and notes their importance, but does not explore the impact of different augmentations or learn them automatically.
- Why unresolved: The contribution of individual augmentations and the potential for automated augmentation discovery are not evaluated.
- What evidence would resolve it: Ablation studies removing or modifying specific augmentations, or experiments using learned augmentation policies (e.g., via AutoAugment).

## Limitations
- Limited evaluation on real-world datasets with diverse recording conditions and piano types
- No exploration of the method's performance on non-piano musical genres
- Fixed augmentation pipeline without analysis of individual augmentation contributions

## Confidence
- Pre-training effectiveness: High - strong empirical evidence from controlled experiments
- Contrastive learning mechanism: Medium - well-established theory but limited direct evidence for this specific application
- Generalization to real data: Medium - impressive results but limited real data evaluation
- Modality-specific pre-training benefits: Medium - ablation studies suggest benefits but don't prove the mechanism
- Augmentation effectiveness: Low - augmentations are used but their specific contributions are not analyzed

## Next Checks
1. Test the model on additional real-world datasets with different recording qualities and piano types to verify synthetic-to-real generalization across diverse conditions
2. Conduct ablation studies isolating the impact of individual augmentations during pre-training to identify which transformations contribute most to real data performance
3. Compare against supervised pre-training approaches using labeled data to quantify the value of the self-supervised approach when labeled data is available