---
ver: rpa2
title: 'Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying
  of OpenStreetMap'
arxiv_id: '2308.16060'
source_url: https://arxiv.org/abs/2308.16060
tags:
- query
- language
- queries
- overpass
- natural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Text-to-OverpassQL, a novel task for generating
  Overpass queries from natural language to query OpenStreetMap (OSM) geodata. To
  support this task, the authors present OverpassNL, a dataset of 8,352 query-natural
  language pairs collected from real user queries.
---

# Text-to-OverpassQL: A Natural Language Interface for Complex Geodata Querying of OpenStreetMap

## Quick Facts
- arXiv ID: 2308.16060
- Source URL: https://arxiv.org/abs/2308.16060
- Reference count: 17
- Primary result: Introduced OverpassNL dataset (8,352 query-natural language pairs) and established baseline performance with GPT-4 achieving 71.4 OQS and 53.0% EXSOFT

## Executive Summary
This paper introduces Text-to-OverpassQL, a novel task for generating Overpass queries from natural language to query OpenStreetMap (OSM) geodata. The authors present OverpassNL, a dataset of 8,352 query-natural language pairs collected from real user queries, along with strong baseline performance using both finetuned sequence-to-sequence models and in-context learning with large language models. GPT-4 achieves the best results (71.4 OQS, 53.0% EXSOFT), while smaller models finetuned on the dataset approach this performance. The task is distinct from Text-to-SQL due to OSM's global scale and open-vocabulary tagging system, and evaluation metrics are specifically designed for OverpassQL syntax while being grounded in database execution. Human expert evaluation confirms the practical utility of generated queries.

## Method Summary
The authors establish the Text-to-OverpassQL task by creating the OverpassNL dataset through collecting real user queries from Overpass Turbo and annotating them with natural language descriptions. They develop evaluation metrics including Overpass Query Similarity (OQS) combining chrF, KVS, and TreeS, along with execution-based metrics EX and EXSOFT. The method involves finetuning sequence-to-sequence models (T5, CodeT5, ByT5) on the dataset and employing in-context learning with LLMs (GPT-3, GPT-4) using retrieval-based example selection. All generated queries are executed against an actual OSM database snapshot to ground evaluation in practical utility.

## Key Results
- GPT-4 with in-context learning achieves the best performance (71.4 OQS, 53.0% EXSOFT)
- Finetuned ByT5-base model with 582M parameters achieves strong results (71.2 OQS) approaching GPT-4 performance
- Human expert evaluation shows most generated queries are rated as helpful for drafting new queries
- Retrieval-based in-context learning with sBERT similarity outperforms random example selection

## Why This Works (Mechanism)

### Mechanism 1
Finetuning smaller models on task-specific data yields competitive performance to larger LLMs for OverpassQL generation. By pretraining on relevant code corpora and then finetuning on the OverpassNL dataset, models learn both general code patterns and specific OverpassQL syntax. The task-specific data captures sufficient distributional coverage of OverpassQL features and real-world usage patterns. However, if the training data lacks coverage of key OverpassQL features or real-world query patterns, finetuned models will underperform on complex queries.

### Mechanism 2
In-context learning with retrieval-based example selection outperforms random example selection for OverpassQL generation. By retrieving semantically similar examples from the training set and including them as context, the model can better generalize to new queries that share structural patterns. This works when the training set contains sufficient diverse examples that share semantic patterns with evaluation instances. If evaluation instances are significantly out-of-distribution from training examples, even retrieval-based in-context learning will fail.

### Mechanism 3
Grounding evaluation in database execution provides a more reliable assessment of model performance than surface-level metrics alone. By executing generated queries against the actual OSM database and comparing returned elements, we measure practical utility rather than just syntactic similarity. This assumes the database snapshot used for evaluation remains stable and representative of real-world OSM data. If the database changes significantly between training and evaluation, or if execution becomes prohibitively slow for large-scale evaluation, this approach becomes less reliable.

## Foundational Learning

- Concept: Sequence-to-sequence learning with transformer architectures
  - Why needed here: The task requires mapping variable-length natural language to structured query code, which is a classic seq2seq problem
  - Quick check question: What architectural component allows transformers to handle variable-length input and output sequences?

- Concept: In-context learning and few-shot prompting
  - Why needed here: Large language models can adapt to new tasks without gradient updates by conditioning on relevant examples in the prompt
  - Quick check question: How does the selection of in-context examples impact the quality of few-shot learning?

- Concept: Grounded evaluation through database execution
  - Why needed here: Surface metrics may not capture whether generated queries actually retrieve the correct data from OSM
  - Quick check question: What are the advantages and disadvantages of execution-based evaluation compared to string-matching metrics?

## Architecture Onboarding

- Component map: Query collection from Overpass Turbo -> Annotation interface -> Dataset creation and splitting -> Model training (finetuning or in-context learning) -> Evaluation (surface metrics and execution metrics) -> Human expert evaluation
- Critical path: 1) Query collection from Overpass Turbo, 2) Annotation of natural language descriptions, 3) Dataset creation and splitting, 4) Model training and evaluation, 5) Grounded evaluation through database execution, 6) Human expert evaluation for practical utility
- Design tradeoffs: Finetuning vs. in-context learning (smaller models with finetuning are faster and more cost-effective, while larger LLMs with in-context learning achieve better execution accuracy); Surface metrics vs. execution metrics (chrF and TreeS measure syntactic similarity, while EX and EXSOFT measure practical utility); Data quality vs. quantity (low-quality instances from developer comments still improve execution accuracy)
- Failure signatures: Low execution accuracy despite high surface similarity (model generates syntactically correct queries that don't retrieve intended data); High execution accuracy but low surface similarity (model uses different but functionally equivalent query structures); Frequent syntax errors (model struggles with OverpassQL syntax rules); Poor performance on hard partition (model fails on queries dissimilar to training examples)
- First 3 experiments: 1) Finetune ByT5-base on OverpassNL dataset and evaluate on development set using OQS, EX, and EXSOFT metrics, 2) Test GPT-4 with 5-shot in-context learning using sBERT-retrieved examples on development set, 3) Compare finetuned model and GPT-4 on hard partition of test set to assess out-of-distribution performance

## Open Questions the Paper Calls Out

### Open Question 1
How would larger and more diverse OverpassNL datasets impact model performance, particularly for rare geographic regions and specialized query types? The paper mentions that Europe has better mapping coverage and that the dataset covers 91% of key usage in OSM. This remains unresolved because the current dataset size (8,352 queries) and geographic distribution may limit model generalization to underrepresented regions and complex query patterns not well-represented in the training data.

### Open Question 2
What is the optimal balance between model size and finetuning data size for the Text-to-OverpassQL task? The paper compares different model sizes and training data augmentation strategies but doesn't systematically explore the tradeoff between model capacity and data requirements. This is unresolved because the paper establishes baselines but doesn't provide guidance on when to choose larger models versus more training data.

### Open Question 3
How can self-refinement be improved beyond simple error correction to generate more semantically accurate queries? The paper presents self-refinement experiments where GPT-4 is given execution feedback to improve generated queries, but current self-refinement primarily addresses syntax errors and basic execution failures without tackling deeper semantic mismatches. This remains unresolved because more sophisticated prompting strategies or training approaches are needed to handle semantic corrections.

## Limitations

- Dataset representativeness issues, with 40% of test instances containing unique features not present in training set
- Execution stability concerns, with 7% of test instances timing out and 1% failing for unknown reasons
- Limited evidence of cross-domain generalization beyond the specific OSM snapshot used for evaluation

## Confidence

High Confidence:
- The OverpassNL dataset provides a meaningful foundation for Text-to-OverpassQL research
- Finetuned smaller models yield competitive performance to larger LLMs for OverpassQL generation

Medium Confidence:
- GPT-4 with in-context learning achieves the best overall performance
- Human expert evaluation confirms practical utility of generated queries

Low Confidence:
- Claims about fundamental differences from Text-to-SQL without direct comparison
- Effectiveness of low-quality instances from developer comments without established causality

## Next Checks

1. **Out-of-Distribution Testing**: Evaluate the best-performing models on a separate test set containing queries with features completely absent from the training set, such as rare tag combinations or complex spatial operations not represented in OverpassNL.

2. **Cross-Snapshot Evaluation**: Test model performance on a different OSM database snapshot (at least 6 months apart from the original) to assess robustness to database evolution and verify that execution metrics remain stable over time.

3. **Real-World Deployment Study**: Conduct a small-scale study with actual OSM users attempting to solve real geospatial tasks using the generated queries, measuring task completion rates and user satisfaction to validate the practical utility claims from human evaluation.