---
ver: rpa2
title: Small Language Models Improve Giants by Rewriting Their Outputs
arxiv_id: '2305.13514'
source_url: https://arxiv.org/abs/2305.13514
tags:
- language
- performance
- candidates
- corrector
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method called LM-corrector (LMCor) to improve
  the performance of large language models (LLMs) on specific tasks without fine-tuning
  the LLM. The key idea is to use a small LM-corrector model that takes as input the
  source sentence and multiple candidate outputs from the LLM, and outputs a corrected
  version of the LLM's output.
---

# Small Language Models Improve Giants by Rewriting Their Outputs

## Quick Facts
- **arXiv ID**: 2305.13514
- **Source URL**: https://arxiv.org/abs/2305.13514
- **Reference count**: 22
- **Primary result**: A 250M parameter LM-corrector model improves few-shot performance of 62B parameter LLMs by optimally ranking, combining, and editing multiple LLM-generated candidates

## Executive Summary
This paper introduces LM-corrector (LMCor), a method that improves the performance of large language models on specific tasks without fine-tuning the LLMs themselves. Instead, a small corrector model (250M parameters) is trained to take source sentences and multiple candidate outputs from a large LLM, then produce a corrected version by ranking, combining, and editing the candidates. The approach shows substantial improvements across four natural language generation tasks, matching and even outperforming standard fine-tuning while being robust to different prompts and compatible with various LLMs at inference.

## Method Summary
The method involves two main steps: first, generating multiple candidate outputs from a large LLM through few-shot prompting; second, training a small LM-corrector model to take the source sentence and these candidates as input and output a corrected version. The corrector learns to identify and combine correct spans from the candidates to produce higher quality outputs than any single candidate. During inference, the trained corrector can be applied to different LLMs and prompts without retraining, functioning as a plug-and-play module that improves performance while leveraging the LLM's external knowledge.

## Key Results
- A 250M parameter LM-corrector substantially improves few-shot performance of 62B parameter LLMs across four tasks
- The approach matches and sometimes outperforms standard fine-tuning, particularly with limited training data
- The corrector is robust to different prompts and can be seamlessly integrated with different LLMs at inference
- Significant performance gains are observed when training data is limited (e.g., 15-point improvement with 1k examples)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The corrector learns to optimally rank, combine, and edit multiple LLM-generated candidates to produce higher quality outputs than any single candidate.
- **Mechanism**: The corrector is trained to take the source sentence and k diverse candidates from the LLM as input, and generate the target output by selecting, merging, and editing spans from the candidates.
- **Core assumption**: The LLM's candidates contain correct spans that, when optimally combined and edited, can reconstruct the target output.
- **Evidence anchors**:
  - [abstract] "We create a pool of candidates from the LLM through few-shot prompting and we employ a compact model, the LM-corrector (LMCor), specifically trained to merge these candidates to produce an enhanced output."
  - [section 2] "The corrector module receives a diverse set of candidates for a given source sentence, representing alternative outputs sampled from the LLM. By considering multiple candidates simultaneously, LMCor learns to optimally rank, combine, and even edit them to produce more accurate and higher-quality outputs."
- **Break condition**: If the LLM's candidates are too poor quality or don't contain correct spans, the corrector won't be able to produce the target output.

### Mechanism 2
- **Claim**: The corrector is robust to different prompts and can be applied to different LLMs without retraining.
- **Mechanism**: The corrector is trained once using candidates generated from one set of prompts and one LLM. During inference, it can be applied to different prompts and different LLMs, as long as the candidates contain the relevant information.
- **Core assumption**: The corrector learns to identify and combine correct spans from candidates, rather than relying on specific prompt formats or LLM architectures.
- **Evidence anchors**:
  - [abstract] "we illustrate that the LMCor exhibits robustness against different prompts, thereby minimizing the need for extensive prompt engineering."
  - [section 4] "we further examine the robustness of our approach by testing whether a corrector can be used interchangeably with different LMs without retraining."
- **Break condition**: If the corrector relies too heavily on specific prompt formats or LLM architectures, it won't generalize well to different prompts or LLMs.

### Mechanism 3
- **Claim**: The corrector improves the sample efficiency of fine-tuning, especially for small datasets.
- **Mechanism**: The corrector is trained on the task-specific dataset, but instead of learning to generate the target output from scratch, it learns to refine the LLM's candidates.
- **Core assumption**: The LLM's candidates contain useful external knowledge that can be leveraged to improve performance, especially when training data is limited.
- **Evidence anchors**:
  - [section 3.2] "our approach consistently outperforms the baseline across all dataset sizes. The gap in performance is particularly pronounced when the training dataset is limited, consisting of only 1k examples, resulting in a substantial difference of 15 points in F0.5."
- **Break condition**: If the LLM's candidates don't contain useful external knowledge, or if the task-specific data is sufficient on its own, the corrector won't improve sample efficiency.

## Foundational Learning

- **Concept**: In-context learning and few-shot prompting
  - **Why needed here**: The paper relies on few-shot prompting to generate diverse candidates from the LLM, which are then used to train the corrector.
  - **Quick check question**: What is the difference between in-context learning and fine-tuning, and why is in-context learning used in this paper?

- **Concept**: Text generation and sequence-to-sequence models
  - **Why needed here**: The paper involves generating text outputs from text inputs, using models like T5 and PaLM.
  - **Quick check question**: What is the difference between autoregressive and encoder-decoder models, and which type is used in this paper?

- **Concept**: Evaluation metrics for text generation (e.g., BLEU, ROUGE, F0.5)
  - **Why needed here**: The paper evaluates the performance of the corrector using metrics like BLEU, ROUGE, and F0.5.
  - **Quick check question**: What is the difference between precision and recall, and how do they relate to F0.5?

## Architecture Onboarding

- **Component map**: Source sentence -> LLM (k times) -> Concatenated candidates + source -> LM-corrector -> Corrected output
- **Critical path**: The critical path is: input -> LLM (k times) -> corrector -> output. The corrector must be able to process the source and k candidates efficiently to avoid becoming a bottleneck.
- **Design tradeoffs**: The main tradeoff is between the size of the LLM and the corrector. A larger LLM may generate better candidates, but a larger corrector may be more expensive to train and run.
- **Failure signatures**: If the corrector performs poorly, it could be due to: poor quality LLM candidates, insufficient training data, or an inappropriate corrector architecture. If the corrector is too slow, it could be due to: processing too many candidates, using a large corrector model, or inefficient implementation.
- **First 3 experiments**:
  1. Train the corrector on a small dataset (e.g., 1k examples) and evaluate its performance on a held-out test set.
  2. Vary the number of candidates (k) used by the corrector and measure the impact on performance and efficiency.
  3. Apply the trained corrector to a different LLM (e.g., GPT-3) and measure its performance and generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can the LM-corrector approach be extended to other natural language processing tasks beyond the four tasks (GEC, summarization, data-to-text, and abstractive summarization) evaluated in this paper?
- **Basis in paper**: [explicit] The authors demonstrate the effectiveness of the LM-corrector on four natural language generation tasks and suggest its potential for other tasks.
- **Why unresolved**: The paper only evaluates the LM-corrector on a limited set of tasks, and its performance on other NLP tasks remains unexplored.
- **What evidence would resolve it**: Experiments applying the LM-corrector to a diverse range of NLP tasks and comparing its performance to other state-of-the-art methods.

### Open Question 2
- **Question**: How does the performance of the LM-corrector scale with the size of the LLM and the corrector model?
- **Basis in paper**: [explicit] The authors demonstrate the effectiveness of the LM-corrector on a 62B parameter LLM and a 250M parameter corrector, but the scaling behavior is not explored.
- **Why unresolved**: The paper only evaluates the LM-corrector on a specific combination of LLM and corrector sizes, and the scaling behavior for different sizes is not explored.
- **What evidence would resolve it**: Experiments varying the sizes of both the LLM and the corrector model and analyzing the performance scaling behavior.

### Open Question 3
- **Question**: Can the LM-corrector approach be used to improve the performance of LLMs on tasks where they currently underperform compared to specialized models?
- **Basis in paper**: [explicit] The authors demonstrate that the LM-corrector can improve the performance of LLMs on various tasks, but its potential for improving performance on tasks where LLMs currently underperform is not explored.
- **Why unresolved**: The paper focuses on improving the performance of LLMs on tasks where they already perform reasonably well, and its potential for improving performance on tasks where LLMs currently underperform is not explored.
- **What evidence would resolve it**: Experiments applying the LM-corrector to tasks where LLMs currently underperform and comparing its performance to specialized models.

## Limitations

- Claims about robustness to different prompts and plug-and-play compatibility with different LLMs remain partially validated, with testing limited to one alternative LLM configuration
- Performance gains are measured primarily against baselines rather than state-of-the-art methods for each specific task
- The paper focuses on tasks where LLMs already perform reasonably well, without exploring whether the approach can improve performance on tasks where LLMs currently underperform

## Confidence

- **High confidence**: The core mechanism of using a small corrector to improve LLM outputs through candidate combination and editing is well-supported by experimental results across multiple tasks and datasets
- **Medium confidence**: Claims about sample efficiency improvements are supported but could benefit from more extensive ablation studies across different dataset sizes
- **Medium confidence**: Robustness claims to different prompts and LLMs are partially validated but would benefit from broader testing across more diverse configurations

## Next Checks

1. **Cross-task generalization test**: Apply a corrector trained on one task (e.g., summarization) to a different task (e.g., data-to-text) using the same LLM, measuring performance degradation and identifying which aspects of the corrector are task-specific versus generalizable

2. **Prompt sensitivity analysis**: Systematically vary prompt formats, number of demonstrations, and task descriptions for each dataset to quantify the actual sensitivity of the corrector to prompt engineering, testing whether the claimed robustness holds across a wider range of prompt variations

3. **Scalability validation**: Train and evaluate the LM-corrector approach with LLMs of different scales (e.g., 7B, 30B parameters) to verify whether the 62Bâ†’250M parameter ratio is optimal or whether different scaling relationships might yield better performance-efficiency tradeoffs