---
ver: rpa2
title: CUNI Submission to MRL 2023 Shared Task on Multi-lingual Multi-task Information
  Retrieval
arxiv_id: '2310.16528'
source_url: https://arxiv.org/abs/2310.16528
tags:
- task
- language
- data
- english
- shared
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper describes a system for the MRL 2023 shared task on multilingual
  information retrieval. The approach uses a translate-test method, translating input
  data to English, performing the task using strong English models, and projecting
  results back to the original language.
---

# CUNI Submission to MRL 2023 Shared Task on Multi-lingual Multi-task Information Retrieval

## Quick Facts
- arXiv ID: 2310.16528
- Source URL: https://arxiv.org/abs/2310.16528
- Reference count: 8
- Primary result: Translate-test approach outperforms zero-shot baselines for multilingual NER and QA

## Executive Summary
This paper describes the CUNI system for the MRL 2023 shared task on multilingual information retrieval, focusing on Named Entity Recognition (NER) and Question Answering (QA) in low-resource languages. The approach uses a translate-test method, translating input data to English, performing the task using strong English models, and projecting results back to the original language. A key contribution is a label-aware span projection method using a fine-tuned translation model to score candidate label positions. Experiments were conducted on named entity recognition (NER) and question answering (QA) in low-resource languages. For NER, the system used RoBERTa models and MasakhaNER datasets. For QA, it used extractive and generative approaches. Results showed the translate-test method outperformed zero-shot baselines but did not match supervised training. The generative QA model using Llama 2 achieved better performance than the extractive model.

## Method Summary
The CUNI system implements a translate-test approach for multilingual NER and QA tasks. The method involves translating unlabeled examples from target languages (Igbo, Indonesian, Alsatian, Turkish, Uzbek, Yoruba, Azerbaijani) to English using NLLB-3.3B, running inference with task-specific English models (RoBERTa-large), and projecting results back to the original language using a label-aware translation model. For NER, the system uses MasakhaNER datasets and RoBERTa models with a label-aware span projection method. For QA, both extractive and generative approaches were tested, with Llama 2 achieving substantial improvement over extractive models. The system was evaluated on the MRL 2023 shared task datasets using F1 score for NER and chrF score for QA.

## Key Results
- Translate-test approach outperformed zero-shot baselines but did not match supervised training
- Finetuned models underperformed baselines due to domain mismatch between development and test data
- Generative QA model using Llama 2 achieved better performance than extractive model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translate-test outperforms zero-shot transfer for multilingual NER and QA
- Mechanism: Translating input to English, running inference with strong English models, and projecting results back preserves performance while avoiding language-specific training
- Core assumption: English pre-trained models have strong task-specific performance and the translation pipeline is accurate enough to preserve semantic content
- Evidence anchors:
  - [abstract] "Results showed the translate-test method outperformed zero-shot baselines but did not match supervised training"
  - [section] "Our results do not outperform supervised, language-specific models, but are considerably better than zero-shot approaches"
  - [corpus] Weak evidence - corpus neighbors focus on other MRL submissions, not direct comparisons
- Break condition: Translation quality degrades significantly or English models lack task-specific capability for the domain

### Mechanism 2
- Claim: Label-aware span projection maintains correct entity/answer positions across languages
- Mechanism: Using a fine-tuned translation model as scorer to place tags at all possible positions and selecting highest-scoring candidate ensures accurate alignment
- Core assumption: The label-sensitive translation model can reliably score equivalent spans across languages
- Evidence anchors:
  - [section] "To keep the inferred tags on the correct positions in the original language, we propose a method based on scoring the candidate positions using a label-sensitive translation model"
  - [section] "We place the tags at all possible positions (subject to minimum/maximum span length constraints) and select the highest-scoring candidate"
  - [corpus] Weak evidence - corpus lacks specific span projection validation studies
- Break condition: Translation model scoring becomes unreliable for certain language pairs or text domains

### Mechanism 3
- Claim: Domain mismatch causes finetuned models to underperform baselines
- Mechanism: Finetuning on translated MasakhaNER data adapts to domain but validation/test sets have different characteristics, leading to performance degradation
- Core assumption: Domain mismatch between development and evaluation data significantly impacts model performance
- Evidence anchors:
  - [abstract] "However, due to a domain mismatch between the development data and the shared task validation and test sets, the finetuned models could not outperform our baselines"
  - [section] "Because of the domain mismatch (the shared task validation data are not local news but rather Wikipedia articles), the original Ontonotes5 model performs better"
  - [corpus] No direct evidence - corpus lacks domain mismatch analysis
- Break condition: Development and evaluation data come from same distribution

## Foundational Learning

- Concept: Machine translation quality impact on downstream tasks
  - Why needed here: Translate-test approach depends entirely on translation quality for preserving semantic content
  - Quick check question: What happens to NER accuracy when translation introduces errors in named entity boundaries?

- Concept: Cross-lingual transfer learning limitations
  - Why needed here: Understanding when translate-test outperforms zero-shot cross-lingual transfer helps set realistic expectations
  - Quick check question: Why does translate-test perform better than zero-shot XLM-R but worse than supervised training?

- Concept: Domain adaptation in NLP
  - Why needed here: Finetuning effectiveness depends on matching training and evaluation domain characteristics
  - Quick check question: How does domain mismatch between news articles and Wikipedia affect named entity recognition performance?

## Architecture Onboarding

- Component map: Translation pipeline → English model inference → Label-aware projection → Post-processing
- Critical path: Input text → NLLB translation → RoBERTa model → NLLB tag-scoring → Span extraction
- Design tradeoffs: Translate-test trades language-specific training data for reliance on translation quality; finetuning trades general English capability for domain specificity
- Failure signatures: Poor performance on languages with low translation quality; degraded accuracy when domain mismatch exists; increased latency from multiple model inferences
- First 3 experiments:
  1. Measure NER accuracy degradation as NLLB translation quality decreases
  2. Compare zero-shot cross-lingual transfer vs translate-test performance across different language pairs
  3. Evaluate finetuning impact by testing on development vs validation data with different domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the translate-test approach perform compared to cross-lingual transfer methods using multilingual transformers for NER and QA tasks?
- Basis in paper: [inferred] The paper mentions that translate-test approach outperforms zero-shot transfer methods but does not compare it to cross-lingual transfer with multilingual transformers.
- Why unresolved: The paper lacks experimental results comparing translate-test with cross-lingual transfer methods.
- What evidence would resolve it: Experiments comparing translate-test performance with cross-lingual transfer methods on the same datasets.

### Open Question 2
- Question: What are the limitations of using generative models like Llama 2 for multilingual QA compared to extractive models?
- Basis in paper: [explicit] The paper mentions that using Llama 2 for QA achieved substantial improvement over extractive models but does not discuss limitations.
- Why unresolved: The paper does not explore potential drawbacks or limitations of using generative models for multilingual QA.
- What evidence would resolve it: A detailed analysis of the strengths and weaknesses of generative models compared to extractive models for multilingual QA.

### Open Question 3
- Question: How does the domain mismatch between the XTREME-UP dataset and the shared task data affect the performance of finetuned models?
- Basis in paper: [explicit] The paper mentions that due to domain mismatch, the finetuned models could not outperform baselines.
- Why unresolved: The paper does not provide a detailed analysis of the impact of domain mismatch on model performance.
- What evidence would resolve it: Experiments isolating the effect of domain mismatch on model performance by using datasets with similar domains.

## Limitations
- Translation quality dependency limits performance for languages with lower translation accuracy
- Domain mismatch between development and test data prevented finetuned models from outperforming baselines
- Limited transparency regarding the label-aware translation model used for span projection

## Confidence

**High Confidence**: The claim that translate-test outperforms zero-shot baselines is well-supported by the experimental results showing clear performance improvements over both zero-shot XLM-R and finetuned models in some cases.

**Medium Confidence**: The assertion that finetuned models underperformed due to domain mismatch is plausible given the authors' explanation, but lacks quantitative validation or ablation studies to confirm this was the primary factor.

**Low Confidence**: The claim that the generative QA model using Llama 2 achieves better performance than the extractive model is mentioned but without sufficient experimental detail or statistical significance testing to establish this definitively.

## Next Checks
1. **Translation Quality Impact Analysis**: Conduct controlled experiments measuring NER and QA performance as a function of translation quality degradation, particularly focusing on named entity boundary preservation and semantic equivalence.

2. **Language-Specific Performance Breakdown**: Analyze system performance across individual language pairs to identify which languages benefit most from the translate-test approach and which may require alternative strategies.

3. **Span Projection Accuracy Validation**: Implement detailed evaluation of the label-aware span projection method by measuring alignment accuracy between projected spans and ground truth positions, including error analysis of common failure patterns.