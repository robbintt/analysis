---
ver: rpa2
title: Evaluating In-Context Learning of Libraries for Code Generation
arxiv_id: '2311.09635'
source_url: https://arxiv.org/abs/2311.09635
tags:
- language
- code
- in-context
- functions
- library
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically evaluates the in-context learning capabilities
  of various LLMs for generating code based on libraries defined within the prompt.
  The authors examine three scenarios: learning specialized libraries for vision-language
  tasks, constrained generation using specific library functions, and learning a new
  programming language.'
---

# Evaluating In-Context Learning of Libraries for Code Generation

## Quick Facts
- arXiv ID: 2311.09635
- Source URL: https://arxiv.org/abs/2311.09635
- Reference count: 15
- Key result: Even smaller open-source models like Llama-2 and StarCoder can adeptly understand novel code libraries when provided with natural language descriptions or raw code implementations, without needing demonstrations.

## Executive Summary
This paper systematically evaluates the in-context learning capabilities of various LLMs for generating code based on libraries defined within the prompt. The authors examine three scenarios: learning specialized libraries for vision-language tasks, constrained generation using specific library functions, and learning a new programming language. Their findings show that even smaller open-source models like Llama-2 and StarCoder can adeptly understand novel code libraries when provided with natural language descriptions or raw code implementations, without needing demonstrations. Moreover, GPT-4 exhibits non-trivial capability at in-context learning a new programming language from scratch based on descriptions.

## Method Summary
The paper evaluates in-context learning capabilities of LLMs through three experimental scenarios. First, models are tested on learning the VisProg library for vision-language tasks using demonstrations, descriptions, or implementations as supervision. Second, a constrained generation task requires models to use specific aliased functions from a custom library. Third, models attempt to learn Isabelle theorem proving language from keyword descriptions. The evaluation uses datasets including GQA, NLVR, CodeContests, APPS, and Isabelle theorem proving, with performance measured through execution accuracy, function usage percentage, proof correctness, and F1 scores.

## Key Results
- Models can learn novel libraries using only natural language descriptions or implementations, without requiring demonstrations
- Smaller open-source models (Llama-2, StarCoder) perform comparably to larger models when provided with in-context supervision
- GPT-4 shows capability to learn a new programming language (Isabelle) from scratch based on keyword descriptions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can adapt to novel programming libraries using in-context supervision without demonstrations.
- Mechanism: The models leverage their pretraining knowledge of code structure, patterns, and semantics to generalize from descriptions or implementations of new functions. They parse the function signatures and docstrings to infer usage patterns, then apply this knowledge to generate appropriate code for new tasks.
- Core assumption: The models have sufficient pretraining exposure to diverse code patterns and can transfer this knowledge to understand and use new library functions based on their specifications.
- Evidence anchors:
  - [abstract] "Our findings further reveal that LLMs exhibit a surprisingly high proficiency in learning novel library modules even when provided with just natural language descriptions or raw code implementations of the functions"
  - [section 4.2] "GPT-4's performance with descriptions and implementation is comparable to that from demonstrations"
- Break condition: If the function signatures are too complex or the descriptions/implementations are ambiguous, the models may fail to correctly infer usage patterns.

### Mechanism 2
- Claim: LLMs can learn to use unfamiliar programming libraries regardless of their size or openness.
- Mechanism: The models' ability to understand and use new libraries is not limited by their size or training data source. Smaller open-source models like Llama-2 and StarCoder can achieve comparable performance to larger proprietary models like GPT-4 when provided with appropriate in-context supervision.
- Core assumption: The underlying architecture and pretraining of the models enable them to generalize to new libraries based on their specifications, regardless of model size or data source.
- Evidence anchors:
  - [abstract] "Our results show that even smaller open-source LLMs like Llama-2 and StarCoder demonstrate an adept understanding of novel code libraries based on specification presented in-context"
  - [section 4.2] "This shows that this ability to adapt to novel code modules in-context is not limited to the biggest proprietery LLMs but is also exhibited to a good extent by openly accessible smaller models"
- Break condition: If the model's pretraining data does not cover a diverse enough range of code patterns and structures, it may struggle to generalize to new libraries.

### Mechanism 3
- Claim: LLMs can learn to use unfamiliar programming libraries without explicit function name semantics.
- Mechanism: The models can infer the purpose and usage of new functions based on their descriptions and implementations, even when the function names are aliased with random strings. This suggests that the models rely more on the content of the specifications rather than the names themselves.
- Core assumption: The models can extract semantic meaning from the descriptions and implementations of functions, allowing them to understand their purpose and usage without relying on function names.
- Evidence anchors:
  - [abstract] "Our findings further reveal that LLMs exhibit a surprisingly high proficiency in learning novel library modules even when provided with just natural language descriptions or raw code implementations of the functions"
  - [section 4.2] "GPT-4 is quite robust to both types of aliasing [synonymous words and random strings]"
- Break condition: If the descriptions and implementations are not sufficiently detailed or clear, the models may struggle to infer the correct semantics without relying on function names.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: The paper evaluates the ability of LLMs to adapt to novel programming libraries and languages using only in-context supervision, without any finetuning or additional training.
  - Quick check question: What are the three types of in-context supervision used in the experiments, and how do they differ in terms of information provided to the model?

- Concept: Code generation and comprehension
  - Why needed here: The paper focuses on the ability of LLMs to generate code using unfamiliar libraries and languages, as well as their ability to understand and use the specifications of these libraries and languages.
  - Quick check question: What are the two main metrics used to evaluate the models' performance in the constrained generation scenario, and what do they measure?

- Concept: Transfer learning
  - Why needed here: The paper assumes that the models can leverage their pretraining knowledge of code patterns and structures to adapt to new libraries and languages using in-context supervision.
  - Quick check question: How does the performance of smaller open-source models like Llama-2 and StarCoder compare to larger proprietary models like GPT-4 in the in-context library learning task, and what does this suggest about the role of pretraining data?

## Architecture Onboarding

- Component map:
  Dataset: GQA, NLVR, Knowledge Tagging, Image Editing, Natural Language to Python, Isabelle theorem proving
  Models: GPT-4, GPT-3.5-Turbo, LLaMA-2-70B, StarCoder, CodeLlama
  Libraries: VisProg (vision-language tasks), custom library of aliased functions (constrained generation), Isabelle (theorem proving)
  Supervision types: Demonstrations, descriptions, implementations

- Critical path:
  1. Create or obtain a dataset with tasks requiring the use of novel libraries or languages
  2. Define the novel libraries or languages and their specifications (function signatures, descriptions, implementations)
  3. Generate in-context prompts with the task instructions and library specifications
  4. Use the LLM to generate code or proofs based on the prompts
  5. Evaluate the generated outputs using task-specific metrics (e.g., execution accuracy, proof correctness)

- Design tradeoffs:
  - Using demonstrations as supervision provides the most information to the model but requires more effort to create
  - Using descriptions or implementations as supervision is less resource-intensive but may result in lower performance
  - Aliasing function names can help evaluate the model's ability to understand semantics without relying on names, but may also introduce additional complexity

- Failure signatures:
  - Low execution accuracy or proof correctness
  - Failure to use the provided library functions or language keywords in the generated code or proofs
  - Over-reliance on memorized patterns from pretraining data instead of adapting to the novel libraries or languages

- First 3 experiments:
  1. Evaluate the performance of different model sizes (e.g., GPT-4 vs. LLaMA-2) on the in-context library learning task using demonstrations as supervision
  2. Compare the performance of models when provided with different types of supervision (demonstrations, descriptions, implementations) for the same novel library
  3. Assess the impact of aliasing function names on the models' ability to understand and use unfamiliar libraries

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs vary when learning novel libraries from different types of supervision (demonstrations, descriptions, implementations) across different programming languages and domains?
- Basis in paper: Explicit - The paper evaluates three types of supervision (demonstrations, descriptions, implementations) for learning novel libraries across various tasks and domains.
- Why unresolved: The paper focuses on a limited set of tasks and domains (vision-language tasks, programming problems, theorem proving). The generalizability of the findings to other programming languages and domains remains unclear.
- What evidence would resolve it: Experiments evaluating the performance of LLMs on learning novel libraries from different types of supervision across a wider range of programming languages and domains.

### Open Question 2
- Question: How does the size and architecture of LLMs affect their ability to learn novel libraries and programming languages in-context?
- Basis in paper: Explicit - The paper experiments with various LLM sizes (GPT-4, GPT-3.5, Llama-2, StarCoder, CodeLlama) and architectures.
- Why unresolved: The paper does not provide a detailed analysis of how the size and architecture of LLMs impact their in-context learning abilities. It is unclear whether larger models consistently outperform smaller models or if specific architectures are better suited for certain tasks.
- What evidence would resolve it: A systematic study comparing the performance of LLMs of different sizes and architectures on learning novel libraries and programming languages in-context.

### Open Question 3
- Question: Can LLMs learn to use novel libraries and programming languages effectively in real-world software development scenarios?
- Basis in paper: Explicit - The paper discusses the potential applications of LLMs in software development and specialized domains.
- Why unresolved: The paper focuses on controlled experiments and does not evaluate the performance of LLMs in real-world software development scenarios. It is unclear how well LLMs can adapt to the complexities and constraints of real-world coding environments.
- What evidence would resolve it: Studies evaluating the performance of LLMs in real-world software development tasks, such as code completion, bug fixing, and feature implementation, while using novel libraries and programming languages.

## Limitations
- The study focuses on specific domains and may not generalize to all types of library functions or programming paradigms
- Results are limited to commercially available models and publicly accessible open models
- Isabelle theorem proving results for smaller models show high variance and may not represent stable performance

## Confidence
- High confidence: The finding that in-context learning works across multiple model sizes and supervision types
- Medium confidence: The claim about open-source models matching proprietary models' performance
- Low confidence: The Isabelle theorem proving results for smaller models due to high variance

## Next Checks
1. Conduct systematic analysis to confirm that the novel libraries are not present in the pretraining data of evaluated models
2. Extend evaluation to additional library types including stateful APIs, asynchronous operations, and domain-specific languages
3. Systematically test intermediate model sizes between 7B and 70B parameters across both open and proprietary model families to map the relationship between model scale and in-context learning performance for novel libraries