---
ver: rpa2
title: Language Model Inversion
arxiv_id: '2311.13647'
source_url: https://arxiv.org/abs/2311.13647
tags:
- prompt
- output
- your
- ignore
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of language model inversion, recovering
  a prompt given the model's next-token probabilities. It proposes a method to learn
  a conditional language model that maps from next-token probabilities back to tokens,
  by "unrolling" the vector into a sequence that can be processed by a pretrained
  encoder-decoder.
---

# Language Model Inversion

## Quick Facts
- arXiv ID: 2311.13647
- Source URL: https://arxiv.org/abs/2311.13647
- Reference count: 40
- Primary result: Recovers prompts from next-token probabilities with BLEU 59, F1 78, 27% exact matches on Llama-2

## Executive Summary
This paper introduces language model inversion - the task of recovering a prompt given a language model's next-token probabilities. The authors propose a method using a pretrained T5 encoder-decoder to map from probability vectors back to tokens by "unrolling" the vector into a sequence of pseudo-embeddings. Experiments on Llama-2 7b show the method reconstructs prompts with strong performance metrics, recovering 27% exactly. The paper also explores model access scenarios, demonstrating how even limited API access can enable full probability vector recovery through systematic querying.

## Method Summary
The method trains a T5-base encoder-decoder to map next-token probability vectors back to prompts. Rather than projecting the high-dimensional probability vector to a lower-dimensional embedding (which loses information), the vector is split into chunks and each chunk is passed through an MLP to produce a sequence of pseudo-embeddings. These embeddings are fed to the T5 encoder, which uses cross-attention during decoding to reconstruct the original prompt. For API access scenarios, the method employs binary search on logit bias to recover full probability distributions from limited top-K outputs.

## Key Results
- Recovers 27% of prompts exactly from Llama-2 probability vectors
- Achieves BLEU score of 59 and token-level F1 of 78
- Demonstrates logit extraction method can recover full distributions from top-K API outputs
- Shows model size and training data scale improve inversion performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language model next-token probability vectors contain residual information about earlier tokens, enabling reconstruction.
- Mechanism: The probability vector output by an autoregressive language model at position T retains statistical correlations with earlier tokens in the sequence, allowing an inversion model to recover earlier tokens from the probability vector alone.
- Core assumption: The probability vector encodes enough information about earlier tokens beyond what is strictly needed to predict the next token.
- Evidence anchors: [abstract] "next-token probabilities contain a surprising amount of information about the preceding text"; [section 3.1] "bits remain: although the vector v is only used to predict the next token, it clearly contains residual information about the prompt tokens x1, ..., xT"

### Mechanism 2
- Claim: Unrolling the probability vector into a sequence of pseudo-embeddings allows a pretrained encoder-decoder to condition on the full distribution.
- Mechanism: Instead of projecting the high-dimensional probability vector to a lower-dimensional embedding, the vector is split into chunks and each chunk is passed through a small MLP to produce a sequence of embeddings. This sequence can be fed to the encoder of a pretrained T5 encoder-decoder, which then attends to these embeddings via cross-attention during decoding to reconstruct the prompt.
- Core assumption: The pretrained T5 encoder can effectively process a sequence of pseudo-embeddings derived from the probability vector and use them to condition the decoder.
- Evidence anchors: [section 4] "We instead ‘unroll’ the vector into a sequence of pseudo-embeddings ci ∈ Rd, so that we can condition transformer outputs on the full probability vector v"; [section 4] "Following work from Dumoulin et al. (2018) on feature-level conditioning, we use the cross-attention in an encoder-decoder Transformer to condition on the next-token vector"

### Mechanism 3
- Claim: Even with limited API access (e.g., only top-K probabilities or text output), the full probability distribution can be recovered through systematic querying.
- Mechanism: When an API only returns the top-K probabilities or sampled text, the probability of any token can be recovered by finding the smallest logit bias that makes that token the most likely. This is done via binary search on the logit bias, requiring only one logit bias at a time and allowing full parallelization. By repeating this for each token in the vocabulary, the full distribution can be reconstructed.
- Core assumption: The API allows setting a logit bias and temperature=0 (argmax) to deterministically find the most likely token for a given bias.
- Evidence anchors: [section 5] "We compute this difference by finding the smallest logit bias to make that word most likely. Algorithm 1 shows the approach, which relies on binary search to find the logit bias for each word."; [section 5] "By running this procedure for each word in the vocabulary, we can then reconstruct the full distribution v = softmax(logits)."

## Foundational Learning

- Concept: Autoregressive language models output next-token probabilities conditioned on the preceding context.
  - Why needed here: Understanding that the probability vector is a function of the entire prompt history is crucial for grasping why it might contain information about the prompt.
  - Quick check question: If a language model outputs the probability of "cat" given the prompt "The quick brown fox jumps over the lazy", what is it conditioned on?

- Concept: Cross-attention in encoder-decoder Transformers allows the decoder to attend to encoder representations.
  - Why needed here: The inversion model uses T5's cross-attention to condition the decoder on the unrolled probability vector embeddings.
  - Quick check question: In a T5 encoder-decoder, how does the decoder access information from the encoder during generation?

- Concept: Binary search algorithm for finding a value within a sorted range.
  - Why needed here: The logit extraction method uses binary search on the logit bias to find the probability of each token.
  - Quick check question: If you're searching for a value between 0 and 100 and your first guess is 50, what should your next guess be if the target is higher than 50?

## Architecture Onboarding

- Component map: Llama-2 -> Probability vector -> Unrolled embeddings -> T5 encoder -> Cross-attention -> T5 decoder -> Reconstructed prompt
- Critical path: Prompt → Llama-2 → Probability vector → Unrolled embeddings → T5 encoder → Cross-attention → T5 decoder → Reconstructed prompt
- Design tradeoffs:
  - Unrolling vs. projection: Unrolling preserves information but creates a longer sequence; projection is compact but loses information
  - Binary search precision vs. query cost: Higher precision requires more queries
  - Model scale: Larger inversion models perform better but require more compute
- Failure signatures:
  - Low BLEU/F1 scores: Could indicate insufficient information in probability vectors or poor inversion model
  - Long reconstruction lengths: Could indicate the model is not capturing prompt length well
  - Inability to transfer between models: Could indicate the inversion model is too specific to the training LM
- First 3 experiments:
  1. Train the T5 inversion model on a small subset of Instructions-2M and evaluate BLEU/F1 on a held-out test set
  2. Test the logit extraction algorithm by simulating an API with limited access and verifying if the full distribution can be recovered
  3. Evaluate the inversion model on out-of-distribution prompts (e.g., from Alpaca or Anthropic HH) to test generalization

## Open Questions the Paper Calls Out

- Question: What is the upper bound of information that can be recovered from language model probabilities during prompt inversion?
  - Basis in paper: Inferred
  - Why unresolved: The paper demonstrates that significant information can be recovered from language model probabilities but does not estimate the maximum possible recovery rate.
  - What evidence would resolve it: Conducting experiments with larger language models and more extensive training data to determine the point of diminishing returns in information recovery would help establish the upper bound.

- Question: How effective are different noise addition strategies in protecting prompts from inversion while maintaining language model performance?
  - Basis in paper: Inferred
  - Why unresolved: The paper explores sampling strategies as defenses against inversion but does not provide a comprehensive analysis of their effectiveness across different noise levels and language model tasks.
  - What evidence would resolve it: Systematic testing of various noise addition techniques (e.g., temperature scaling, top-k sampling) across a range of language model tasks and noise levels would clarify the trade-offs between prompt protection and model performance.

- Question: Can iterative refinement significantly improve prompt inversion accuracy beyond the initial inversion attempt?
  - Basis in paper: Explicit
  - Why unresolved: The paper attempts iterative refinement but finds no significant improvement after the initial inversion. It suggests that the hypotheses used for training may not cover a full spectrum of correctable texts.
  - What evidence would resolve it: Developing a more robust iterative refinement model with better hypotheses and testing it on a wider range of prompts could determine if iterative refinement can enhance inversion accuracy.

## Limitations

- The extent and limits of information recoverable from probability vectors remain unclear
- Generalization capability across different domains, writing styles, and language model architectures is not thoroughly established
- The logit extraction method assumes API providers allow logit bias manipulation and argmax sampling, which may not hold in practice

## Confidence

- High Confidence: The core methodology of using a pretrained encoder-decoder with cross-attention to process unrolled probability vectors is technically sound and well-supported by the experimental results
- Medium Confidence: The claim that probability vectors contain sufficient residual information for prompt reconstruction is supported by experiments but lacks theoretical justification
- Low Confidence: The generalizability of the approach across different language models, domains, and API access scenarios is not thoroughly established

## Next Checks

1. **Cross-Model Transfer**: Train the inversion model on Llama-2 but evaluate reconstruction performance on probability vectors from completely different language models (e.g., GPT-3.5, Claude, PaLM). This would test whether the approach generalizes beyond a single model architecture.

2. **Information Content Analysis**: Systematically analyze what aspects of the prompt are recoverable by the inversion model. Test reconstruction of prompts with specific properties (long sequences, rare words, code snippets, structured data) to identify failure modes and limitations.

3. **API Access Simulation**: Implement a realistic API simulation that includes common restrictions (rate limiting, limited logit bias ranges, sampling instead of argmax) to evaluate how the logit extraction method performs under practical constraints. This would validate the feasibility of the approach in real-world scenarios.