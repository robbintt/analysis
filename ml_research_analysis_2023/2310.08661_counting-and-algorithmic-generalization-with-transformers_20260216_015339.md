---
ver: rpa2
title: Counting and Algorithmic Generalization with Transformers
arxiv_id: '2310.08661'
source_url: https://arxiv.org/abs/2310.08661
tags:
- layer
- count
- counting
- attention
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper examines why standard Transformers struggle with algorithmic
  generalization in counting tasks. It identifies two architectural factors: softmax
  normalization of attention weights and layer normalization.'
---

# Counting and Algorithmic Generalization with Transformers

## Quick Facts
- **arXiv ID**: 2310.08661
- **Source URL**: https://arxiv.org/abs/2310.08661
- **Authors**: 
- **Reference count**: 9
- **Key outcome**: Standard Transformers fail at counting tasks due to softmax and layer normalization, which can be fixed by removing these operations.

## Executive Summary
This paper examines why standard Transformers struggle with algorithmic generalization in counting tasks. The authors identify two key architectural factors: softmax normalization of attention weights and layer normalization. Softmax forces attention weights to sum to 1, making it impossible to learn parallel counting where absolute quantities matter. Layer normalization re-scales values, causing models to overfit training distributions and fail on out-of-distribution data. The authors propose modified Transformers that remove these operations, enabling better counting generalization while using a lightweight architecture.

## Method Summary
The paper presents modified Transformer architectures that remove softmax from attention and layer normalization from feed-forward networks. The authors evaluate these architectures on counting tasks using input grids (1x1 to 6x6 for training, larger for testing) with one-hot encoded pixel colors. They compare standard Transformers with modified versions using accuracy metrics that round outputs to nearest integers and check exact matches. Experiments run for 300k epochs with learning rate 0.0002 and batch size 50.

## Key Results
- Standard Transformers with softmax and layer normalization fail to learn counting even on training data
- Modified Transformers without softmax and layer normalization achieve perfect counting performance on grids up to 100x100
- Layer normalization causes models to overfit training distributions and generalize poorly to out-of-distribution data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Softmax normalization prevents learning parallel counting by forcing attention weights to sum to 1, destroying absolute quantities needed for counting
- Core assumption: Absolute value of attention weights matters for counting tasks, not just relative proportions
- Evidence: Abstract states "Softmax forces attention weights to sum to 1, making it impossible to learn parallel counting where absolute quantities matter" and section shows attention weights become 1/N instead of 1 for multiple instances

### Mechanism 2
- Claim: Layer normalization hinders out-of-distribution generalization by rescaling values and causing overfitting to training distributions
- Core assumption: Preserving absolute quantities is essential for counting tasks, and normalization destroys this information
- Evidence: Abstract notes "Layer normalization re-scales values, causing models to overfit training distributions and fail on out-of-distribution data" and section shows performance drops when layer normalization is added

### Mechanism 3
- Claim: Standard Transformers are architecturally limited to static sequential operations, preventing iterative counting generalization
- Core assumption: Iterative algorithms require potentially unbounded sequential operations that standard Transformers cannot provide
- Evidence: Section states "Standard Transformers are architecturally limited to a static number of sequential operations" and explains only N operations can be applied where N is number of layers

## Foundational Learning

- **Concept**: Scaled dot-product attention mechanism
  - Why needed: Understanding how attention works and why softmax normalization affects ability to preserve absolute quantities
  - Quick check: What happens to attention weights when softmax is applied, and why does this matter for counting tasks?

- **Concept**: Layer normalization operation
  - Why needed: Layer normalization is a key architectural component identified as problematic for counting tasks
  - Quick check: How does layer normalization mathematically transform input values, and why might this be problematic for tasks requiring absolute quantities?

- **Concept**: Algorithmic generalization and out-of-distribution performance
  - Why needed: The paper's central concern is whether models can learn algorithms that generalize beyond training distribution
  - Quick check: What distinguishes algorithmic generalization from simple memorization, and why is out-of-distribution performance important for evaluating it?

## Architecture Onboarding

- **Component map**: Input tokens → attention mechanism → feed-forward network → output
- **Critical path**: For counting tasks, attention mechanism must preserve absolute quantities, and feed-forward network must output unnormalized values
- **Design tradeoffs**: Removing softmax and layer normalization improves counting generalization but may hurt NLP performance where relative quantities matter
- **Failure signatures**: Standard Transformers show low accuracy on both in-distribution and out-of-distribution; models with layer normalization show high training accuracy but poor generalization; softmax prevents parallel counting algorithms
- **First 3 experiments**:
  1. Std-Transformer-Count: Standard Transformer with softmax attention and layer normalization - expected to fail at counting
  2. No-LayerNorm-Count: Transformer with softmax removed from attention - expected to succeed at counting
  3. LayerNorm-Identity: Model with layer normalization on identity function task - expected to overfit training distribution

## Open Questions the Paper Calls Out

- **Open Question 1**: To what extent does layer normalization's negative impact on algorithmic generalization apply to other mathematical operations beyond counting? The paper focuses on counting and identity function but hasn't tested broader mathematical operations.

- **Open Question 2**: Can modified Transformers without softmax and layer normalization maintain out-of-distribution generalization benefits while achieving competitive NLP performance? The authors note standard Transformers excel at NLP tasks but haven't tested modified architectures on NLP benchmarks.

- **Open Question 3**: How does the number of Transformer layers affect ability to learn iterative counting algorithms in a generalizable way? The paper identifies layer limitations but hasn't systematically explored relationship between layer count and generalization capacity.

## Limitations

- The empirical evaluation focuses primarily on a single counting task, which may not generalize to other algorithmic problems
- The modified architecture may underperform on tasks where relative quantities matter, such as natural language processing
- The theoretical analysis relies on informal arguments without rigorous mathematical proofs connecting architectural constraints to generalization failures

## Confidence

- **High confidence**: Standard Transformers fail to learn counting tasks even on training data; removing softmax enables successful learning
- **Medium confidence**: Mechanism explanations for why softmax prevents parallel counting and why layer normalization causes overfitting
- **Low confidence**: Broader claim that findings apply to wide range of algorithmic generalization tasks beyond counting

## Next Checks

1. Test modified Transformer architecture on other algorithmic tasks requiring quantity awareness (sorting, arithmetic operations, graph algorithms) to verify generalization beyond counting

2. Conduct systematic ablation experiments isolating effects of removing softmax versus removing layer normalization to determine which modification drives improvements

3. Develop formal mathematical proofs or rigorous analytical frameworks connecting architectural constraints to inability to learn algorithms requiring absolute quantities