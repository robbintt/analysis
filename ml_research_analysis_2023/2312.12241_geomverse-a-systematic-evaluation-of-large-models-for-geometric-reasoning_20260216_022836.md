---
ver: rpa2
title: 'GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning'
arxiv_id: '2312.12241'
source_url: https://arxiv.org/abs/2312.12241
tags:
- reasoning
- shape
- depth
- side
- problems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GeomVerse, a synthetic dataset for systematically
  evaluating the geometric reasoning capabilities of vision-language models (VLMs).
  The dataset is procedurally generated with controllable difficulty levels along
  multiple axes, including reasoning depth, width, distractors, and generalization.
---

# GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning

## Quick Facts
- arXiv ID: 2312.12241
- Source URL: https://arxiv.org/abs/2312.12241
- Reference count: 40
- Key outcome: Introduces GeomVerse, a synthetic dataset for systematically evaluating VLM geometric reasoning capabilities, revealing significant gaps in current model performance

## Executive Summary
This paper introduces GeomVerse, a procedurally generated synthetic dataset for systematically evaluating vision-language models' geometric reasoning capabilities. The dataset enables controlled variation along multiple difficulty axes including reasoning depth, width, and distractors. Experiments with state-of-the-art VLMs reveal that these models struggle significantly with geometry problems, particularly as reasoning depth increases, and show limited generalization to out-of-distribution shapes. The results highlight substantial gaps in current VLM reasoning abilities, suggesting these models are not yet ready for real-world applications like AI tutoring.

## Method Summary
The paper procedurally generates a synthetic dataset of geometry problems with controllable difficulty levels along multiple axes. Experiments evaluate VLMs (PaLI, GPT4V) and LLMs (PaLM 2 Large) in zero-shot, few-shot with chain-of-thought prompting, finetuning to predict labels, and finetuning to predict full solutions. Performance is measured using relaxed accuracy metrics that accommodate rounding variations in geometric computations.

## Key Results
- VLMs struggle significantly with increasing reasoning depth compared to width, indicating fundamental limitations in maintaining long reasoning chains
- Finetuning VLMs to produce complete solution chains (CoT) substantially improves performance for in-distribution problems
- VLMs show limited generalization to out-of-distribution shapes, failing to apply learned reasoning to novel geometric configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The synthetic dataset enables systematic control of reasoning difficulty along multiple axes, allowing precise measurement of model failure modes.
- Mechanism: By procedurally generating geometry problems with controlled depth, width, distractors, and generalization properties, the dataset isolates specific reasoning capabilities and exposes where models fail.
- Core assumption: Geometry problems can be decomposed into tree-structured reasoning chains where each node represents a shape and each edge represents a shared element.
- Evidence anchors:
  - [abstract] "We procedurally create a synthetic dataset of geometry questions with controllable difficulty levels along multiple axes, thus enabling a systematic evaluation."
  - [section 3.3] "we adopt a high-level idea of the GenerateTheory algorithm from Kazemi et al. (2023a) for recursively generating geometry problems"
- Break condition: If geometry problems cannot be adequately represented as tree structures or if procedural generation cannot maintain problem quality while controlling difficulty axes.

### Mechanism 2
- Claim: Finetuning VLMs to generate full solution chains (CoT) significantly improves performance compared to finetuning for direct answer prediction.
- Mechanism: The CoT finetuning approach teaches models to explicitly reason through intermediate steps, essential for multi-hop geometry problems requiring chained computations.
- Core assumption: The automatic solution generation in GeomVerse produces high-quality reasoning chains that can serve as effective training supervision.
- Evidence anchors:
  - [abstract] "finetuning VLMs to produce the entire solution substantially improves their performance for in-distribution problems"
  - [section 4.1] "Notice that FT-CoT outperforms FT substantially for all depths"
- Break condition: If automatically generated solutions contain systematic errors or if models overfit to specific solution patterns rather than learning general reasoning strategies.

### Mechanism 3
- Claim: VLMs show significantly greater sensitivity to reasoning depth than to reasoning width.
- Mechanism: Model performance degrades substantially as depth increases (multi-hop reasoning) but shows more resilience to width increases (parallel reasoning paths), suggesting depth-dependent reasoning limitations.
- Core assumption: The distinction between depth and width in reasoning corresponds to the ability to maintain and chain intermediate results versus handling parallel subproblems.
- Evidence anchors:
  - [abstract] "VLMs struggle more with increasing in depth rather than width of reasoning"
  - [section 4.2] "while increasing the width negatively affects the performance in several cases the amount of decrease is substantially lower compared to the depth experiments"
- Break condition: If the depth-width distinction doesn't correspond to actual reasoning complexity or if models develop different failure modes at high width levels.

## Foundational Learning

- Concept: Multi-hop reasoning and logical deduction
  - Why needed here: The entire evaluation framework maps geometry problem solving to logical reasoning chains, where depth represents inference steps needed.
  - Quick check question: Can you explain how a geometry problem requiring finding an angle through multiple intermediate steps maps to a logical proof with specific depth?

- Concept: Procedural content generation and synthetic dataset creation
  - Why needed here: The dataset is synthetically generated with controllable properties, requiring understanding of how to algorithmically create problems with specific difficulty characteristics.
  - Quick check question: How would you modify the generation algorithm to create problems with maximum depth of 4 while maintaining reasonable quality?

- Concept: Vision-language model evaluation metrics
  - Why needed here: The evaluation uses relaxed accuracy metrics to account for rounding variations in geometric computations, crucial for fair model comparison.
  - Quick check question: Why is a relaxed accuracy threshold of 3% appropriate for geometric reasoning problems, and what would happen if you used strict equality instead?

## Architecture Onboarding

- Component map: Shape library -> Formula library -> Procedural generation algorithm -> Quality control system -> Model training framework -> Evaluation metrics calculation -> Analysis tools
- Critical path: Generate synthetic problems → Finetune VLMs (direct vs CoT) → Evaluate on test sets with various difficulty configurations → Analyze failure modes and generalization
- Design tradeoffs:
  - Depth vs width control: Increasing depth creates more complex reasoning chains but may be harder to generate quality problems; increasing width creates parallel reasoning but may be easier to solve
  - In-distribution vs out-of-distribution testing: In-distribution testing shows finetuning effectiveness but may overestimate real-world performance; out-of-distribution testing reveals generalization limitations
  - Text-only vs text-image formats: Text-only allows testing reasoning capability without visual complexity but may not reflect real-world geometry problem format
- Failure signatures:
  - Low depth performance but reasonable width performance suggests fundamental reasoning chain limitations
  - Good in-distribution performance but poor out-of-distribution performance indicates overfitting to specific shapes/formulas
  - Significant performance drop with distractors indicates inability to filter irrelevant information
  - Different failure modes between direct answer and CoT finetuning suggest different learning strategies
- First 3 experiments:
  1. Generate a small dataset (100 examples) with controlled depth=2, width=1, no distractors and test both PaLI and PaLM 2 models in few-shot and finetuned settings to establish baseline performance
  2. Create a test set with the same depth/width but with added distractors to measure information filtering capability and compare against baseline
  3. Generate an out-of-distribution test set using shapes not seen during finetuning but with similar geometric properties to measure generalization and compare against in-distribution performance

## Open Questions the Paper Calls Out

- How do current VLMs perform on geometry problems that require creativity or non-algorithmic reasoning, beyond the deductive problems covered in GeomVerse?
  - Basis: [explicit] The paper acknowledges that GeomVerse problems can be solved with logical deduction and may not require much creativity, explicitly stating the evaluation should not be considered as measuring creativity.
  - Why unresolved: The paper focuses on deductive reasoning problems and does not test VLMs on problems requiring creative or non-algorithmic approaches.
  - What evidence would resolve it: Testing VLMs on geometry problems that require creative problem-solving approaches beyond standard deduction procedures.

- Can finetuning VLMs on GeomVerse improve their performance on geometry problems involving non-standard shapes that were not present in the training data?
  - Basis: [explicit] The paper shows finetuning VLMs on GeomVerse improves performance on real geometry problems but notes this is an out-of-distribution evaluation due to differences in text style, image style, and problem types.
  - Why unresolved: While the paper shows finetuning helps with out-of-distribution problems, it doesn't specifically test whether this improvement extends to novel non-standard shapes not seen during training.
  - What evidence would resolve it: Testing VLMs on geometry problems with completely new non-standard shapes after finetuning on GeomVerse.

- How sensitive are VLMs to variations in problem presentation, such as different question phrasings or visual representations, beyond the specific variations tested in the paper?
  - Basis: [explicit] The paper tests model sensitivity to low-level visual features and notes models are robust to some variations but not others, also mentioning testing various axes of difficulty in Appendix A.
  - Why unresolved: The paper only tests a limited set of presentation variations and doesn't explore the full range of potential variations in problem presentation.
  - What evidence would resolve it: Systematic testing of VLMs across a wide range of presentation variations, including different question phrasings, visual styles, and problem formats.

## Limitations
- The evaluation framework's reliance on procedurally generated synthetic data raises questions about ecological validity and whether failure modes match those in natural geometry problems.
- The depth-width sensitivity analysis may be influenced by the specific generation methodology and doesn't fully account for potential interactions between these dimensions.
- The comparison between text-only and text-image model performance could be confounded by the fact that text-only models were evaluated on text versions of problems.

## Confidence
- High confidence in the empirical finding that VLMs struggle significantly with increasing reasoning depth
- Medium confidence in the conclusion that CoT finetuning substantially improves performance
- Medium confidence in the depth-width sensitivity findings

## Next Checks
1. Validate the ecological validity of the synthetic dataset by testing whether models that perform well on GeomVerse also perform comparably well on natural geometry problems from established benchmarks like GEOS, controlling for problem complexity.
2. Conduct ablation studies on the solution generation quality by manually inspecting a sample of automatically generated reasoning chains to quantify error rates and assess whether model performance is being measured against reliable ground truth.
3. Test whether the depth-width performance gap persists when controlling for total computational complexity - create matched pairs of problems with equal total reasoning steps but different depth-width configurations to isolate the specific architectural limitations suggested by the paper.