---
ver: rpa2
title: Selectively Answering Ambiguous Questions
arxiv_id: '2305.14613'
source_url: https://arxiv.org/abs/2305.14613
tags:
- questions
- answer
- question
- calibration
- ambiguous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies confidence estimation in question answering,
  distinguishing between uncertainty due to the answer itself (epistemic) and uncertainty
  due to the question being ambiguous (denotational). The authors propose a "disambiguate-and-answer"
  paradigm where the model first rephrases ambiguous questions before answering.
---

# Selectively Answering Ambiguous Questions

## Quick Facts
- arXiv ID: 2305.14613
- Source URL: https://arxiv.org/abs/2305.14613
- Reference count: 12
- One-line primary result: Sampling-based confidence scores (repetition and diversity) significantly outperform likelihood-based methods for calibration on ambiguous questions, especially for instruction-tuned models.

## Executive Summary
This paper addresses confidence estimation in question answering, distinguishing between epistemic uncertainty (answer uncertainty) and denotational uncertainty (question ambiguity). The authors propose a "disambiguate-and-answer" paradigm where models first rephrase ambiguous questions before answering. They evaluate calibration using Expected Calibration Error (ECE), ROC-AUC, and Cov@Acc metrics on both unambiguous and ambiguous datasets. Results show that likelihood-based confidence scores perform poorly on ambiguous questions, while sampling-based methods like "Sampling Repetition" (counting how often the greedy answer appears among samples) and "Sampling Diversity" (measuring answer diversity) are more effective, particularly for instruction-tuned models.

## Method Summary
The authors evaluate confidence estimation methods using few-shot in-context learning with PaLM and Flan-PaLM models on datasets including Natural Questions, TriviaQA, AmbigQA, and SituatedQA. They compare likelihood-based confidence scores with sampling-based methods (sampling repetition and sampling diversity) and self-verification approaches. The sampling repetition method counts how often the greedy answer appears among multiple sampled outputs, while sampling diversity measures the number of distinct answers. Models are prompted with 4-6 question-answer pairs and generate answers, with confidence scores computed to evaluate calibration using ECE, ROC-AUC, and Cov@Acc metrics.

## Key Results
- Sampling repetition achieves Cov@80 of 26.2 on AmbigQA compared to 17.4 for likelihood-based scoring
- Likelihood-based confidence scores perform poorly on ambiguous questions, with ECE exceeding 15%
- Instruction-tuned models show worse calibration than pretrained models, but this is mitigated by sampling-based confidence scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sampling repetition counts are effective confidence scores because they capture the epistemic uncertainty of the model.
- Mechanism: When the model is uncertain about the answer to an unambiguous question, repeated sampling produces diverse outputs. Conversely, when the model is confident, repeated sampling produces the same answer. By counting how often the greedy answer appears among samples, we can estimate the model's confidence in its answer.
- Core assumption: The model's sampling behavior is indicative of its epistemic uncertainty.
- Evidence anchors: [abstract] "we find that the most reliable approach to calibration involves quantifying repetition within a set of sampled model outputs"; [section] "Our second approach is based on the diversity of the samples...our confidence is inversely proportional to the number of distinct samples"
- Break condition: If the model's sampling distribution is not well-calibrated or if the model produces the same output regardless of its confidence level.

### Mechanism 2
- Claim: Sampling-based methods are particularly effective for ambiguous questions because they capture denotational uncertainty.
- Mechanism: When a question is ambiguous, the model's samples will explore different interpretations of the question, leading to diverse answers. By counting the diversity of answers, we can estimate the model's uncertainty about the question's meaning.
- Core assumption: The model's sampling behavior is indicative of its denotational uncertainty.
- Evidence anchors: [abstract] "Our results suggest that sampling-based confidence scores help calibrate answers to relatively unambiguous questions, with more dramatic improvements on ambiguous questions"; [section] "In general, likelihood seems to become a worse method of measuring model uncertainty when the questions are ambiguous and sample repetition appears to improve calibration significantly"
- Break condition: If the model's sampling distribution does not reflect different interpretations of ambiguous questions or if the model always produces the same answer regardless of question ambiguity.

### Mechanism 3
- Claim: Instruction tuning makes models more confident but less calibrated, and sampling-based methods mitigate this issue.
- Mechanism: Instruction-tuned models are trained to provide answers rather than abstain, leading to overconfident predictions. Sampling-based methods provide a more nuanced view of the model's uncertainty, allowing for better calibration even with instruction-tuned models.
- Core assumption: Instruction tuning affects the model's confidence calibration.
- Evidence anchors: [abstract] "we find that while instruction-tuning dramatically improves exact match accuracy, it results in substantially worse ROC-AUC and ECE calibration scores"; [section] "However, this miscalibration is mitigated by using sample-based confidence scores rather than model likelihood"
- Break condition: If instruction tuning does not affect the model's confidence calibration or if sampling-based methods do not improve calibration for instruction-tuned models.

## Foundational Learning

- Concept: Calibration
  - Why needed here: Understanding calibration is crucial for evaluating the effectiveness of confidence estimation methods in question answering.
  - Quick check question: What is the difference between expected calibration error (ECE) and ROC-AUC in evaluating confidence scores?
- Concept: Epistemic vs. Denotational Uncertainty
  - Why needed here: Distinguishing between these two types of uncertainty is essential for understanding the challenges in question answering and developing appropriate confidence estimation methods.
  - Quick check question: How do epistemic and denotational uncertainty differ in the context of question answering?
- Concept: Sampling-based Confidence Estimation
  - Why needed here: Understanding how sampling can be used to estimate confidence is crucial for implementing and evaluating the proposed methods.
  - Quick check question: How do sampling repetition and sampling diversity differ in their approach to estimating confidence?

## Architecture Onboarding

- Component map: Question -> LLM -> Sampling -> Confidence Estimation -> Answer/Abstain
- Critical path: Question -> LLM -> Sampling -> Confidence Estimation -> Answer/Abstain
- Design tradeoffs:
  - Sampling vs. Likelihood: Sampling-based methods are more effective but computationally expensive.
  - Repetition vs. Diversity: Sampling repetition is simpler but may miss some uncertainty, while sampling diversity is more comprehensive but depends on the number of samples.
  - Instruction Tuning: Improves accuracy but worsens calibration, requiring sampling-based methods for mitigation.
- Failure signatures:
  - Poor calibration: Confidence scores do not accurately reflect the probability of correct answers.
  - Overconfidence: Model frequently provides incorrect answers with high confidence.
  - Underconfidence: Model frequently abstains from answering even when it could provide correct answers.
- First 3 experiments:
  1. Evaluate sampling repetition and diversity methods on unambiguous questions and compare their performance to likelihood-based methods.
  2. Evaluate the same methods on ambiguous questions and analyze the improvement in calibration.
  3. Compare the performance of pretrained and instruction-tuned models with sampling-based confidence estimation methods.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of sampling-based confidence scores (like Sampling Repetition and Sampling Diversity) compare to likelihood-based methods when applied to large-scale real-world question answering systems beyond the controlled experimental settings used in this study?
- Basis in paper: [explicit] The paper finds that sampling-based methods perform better than likelihood-based methods on ambiguous questions, especially for instruction-tuned models.
- Why unresolved: The study uses specific datasets (TriviaQA, NQ-Open, AmbigQA, SituatedQA) and controlled experimental conditions. Real-world systems may have different characteristics, question distributions, and noise levels.
- What evidence would resolve it: Testing these confidence scoring methods on actual deployed question answering systems with real user queries and measuring their impact on accuracy, coverage, and user satisfaction.

### Open Question 2
- Question: Can the disambiguate-and-answer paradigm be effectively extended to other types of ambiguity beyond temporal and geographic ambiguity, such as ambiguity arising from sarcasm, cultural references, or domain-specific terminology?
- Basis in paper: [explicit] The paper focuses on temporal and geographic ambiguity (SituatedQA) and general ambiguity (AmbigQA), but notes that "language is highly context-sensitive" and questions can be ambiguous for various reasons.
- Why unresolved: The study primarily examines two types of ambiguity. Other forms of ambiguity might require different disambiguation strategies or might not be amenable to the same approach.
- What evidence would resolve it: Applying the disambiguate-and-answer framework to datasets containing different types of ambiguity and evaluating whether the model can successfully rephrase and answer questions containing sarcasm, cultural references, or domain-specific terminology.

### Open Question 3
- Question: What is the relationship between model scale and the effectiveness of sampling-based confidence scores, and at what point (if any) do the benefits of sampling plateau or decline for very large language models?
- Basis in paper: [explicit] The paper investigates the effect of model scale and finds that calibration stays roughly constant while accuracy declines substantially, but doesn't explore whether sampling-based methods become less effective at larger scales.
- Why unresolved: The study shows that accuracy declines with smaller models but doesn't analyze whether the relative advantage of sampling-based methods changes with scale or whether extremely large models might have different characteristics that affect these methods.
- What evidence would resolve it: Conducting experiments across a wider range of model sizes (including extremely large models) to determine whether the performance gap between sampling-based and likelihood-based methods changes with scale, and identifying any inflection points where sampling methods become less effective.

### Open Question 4
- Question: How can the sampling-based confidence scoring methods be made more efficient for real-time applications, given that they require multiple samples from the model which increases computational cost?
- Basis in paper: [inferred] The paper uses 10 samples per question for its sampling methods, which would be computationally expensive for real-time systems that need to answer many questions quickly.
- Why unresolved: The study doesn't address computational efficiency or explore whether fewer samples could provide similar calibration benefits, or whether alternative sampling strategies could reduce computational burden while maintaining accuracy.
- What evidence would resolve it: Experiments testing different numbers of samples (e.g., 1, 3, 5, 10, 20) to determine the minimum number needed for effective calibration, and exploring techniques like importance sampling or adaptive sampling to reduce computational cost while maintaining calibration quality.

## Limitations

- The paper demonstrates effectiveness primarily on specific question-answering datasets and model architectures, leaving uncertainty about generalizability to other domains or model families.
- The self-verification method's implementation details remain underspecified, particularly regarding how sampled answers are compared and how the verification prompt is structured.
- While the paper distinguishes between epistemic and denotational uncertainty conceptually, the empirical validation of this distinction remains limited.

## Confidence

- High Confidence: The claim that likelihood-based confidence scores perform poorly on ambiguous questions has strong support from multiple evaluation metrics across different datasets.
- Medium Confidence: The effectiveness of sampling repetition and diversity methods for confidence estimation has reasonable support but depends on specific implementation details.
- Low Confidence: The claim that the proposed methods successfully distinguish between epistemic and denotational uncertainty lacks strong empirical validation.

## Next Checks

1. **Cross-domain validation**: Test the sampling-based confidence methods on a non-QA task (e.g., sentiment analysis or summarization) to verify generalizability beyond question answering.

2. **Implementation replication**: Replicate the self-verification method implementation with specific attention to answer comparison normalization and prompt structure to verify reproducibility.

3. **Computational cost analysis**: Measure and report the wall-clock time and inference cost differences between likelihood-based and sampling-based methods across varying numbers of samples to quantify the practical deployment trade-off.