---
ver: rpa2
title: Evaluating Generative Ad Hoc Information Retrieval
arxiv_id: '2311.04694'
source_url: https://arxiv.org/abs/2311.04694
tags:
- information
- retrieval
- generative
- https
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical framework for evaluating generative
  ad hoc retrieval systems, which use large language models to generate text responses
  to user queries. The authors identify the synthetical search task as the primary
  contribution of generative retrieval, where information from multiple sources is
  condensed into a comprehensive text response.
---

# Evaluating Generative Ad Hoc Information Retrieval

## Quick Facts
- arXiv ID: 2311.04694
- Source URL: https://arxiv.org/abs/2311.04694
- Reference count: 40
- Primary result: A theoretical framework for evaluating generative retrieval systems that produce text responses instead of traditional ranked documents

## Executive Summary
This paper addresses the challenge of evaluating generative ad hoc retrieval systems that use large language models to generate comprehensive text responses from multiple information sources. The authors argue that traditional document ranking evaluation methods are insufficient for this new paradigm and propose a theoretical framework based on a user model of the information search process. The framework defines three evaluation objectives (Retrieval, Grounding, Presentation) and five utility dimensions (Coherence, Coverage, Consistency, Correctness, Clarity) to assess the quality of generated responses. It distinguishes between reference-free and reference-based utility assessment methods to handle the dynamic nature of generated content.

## Method Summary
The evaluation framework involves preparing an experimental setting with document collections and queries, running generative retrieval systems to produce text responses, segmenting responses into atomic statements, collecting utility annotations across multiple dimensions using both reference-free and reference-based approaches, and aggregating statement-level utilities into response-level scores using weighted measures like DCG variants that account for the proposed reading model with decay and saturation effects.

## Key Results
- Identification of the synthetical search task as the primary contribution of generative retrieval systems
- Development of a user model based on information search process with three evaluation objectives
- Proposal of utility dimensions for assessing generated responses across Coherence, Coverage, Consistency, Correctness, and Clarity
- Distinction between reference-free and reference-based utility assessment methods for dynamic generated content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative retrieval systems shift from document ranking to synthesized text responses, requiring new evaluation approaches beyond traditional document relevance.
- Mechanism: The paper establishes a new evaluation framework that replaces document-based metrics with response-based metrics, accounting for the synthetical search task where information is condensed from multiple sources into a coherent text.
- Core assumption: The information needs being addressed are complex enough that they cannot be answered by a single document, making synthesis necessary.
- Evidence anchors:
  - [abstract] "Recent advances in large language models have enabled the development of viable generative retrieval systems... Quantifying the utility of the textual responses is essential for appropriately evaluating such generative ad hoc retrieval."
  - [section 2.3] "Generative retrieval systems can be seen as a new, 4th generation of web search engines... This capability is primarily used to operationalize a user's intent to collect and compile a comprehensive overview of the information required to solve their task, condensed into a long-form text."

### Mechanism 2
- Claim: The proposed user model for generative IR is built on a theoretical foundation that accounts for both retrieval and generation components through distinct evaluation objectives.
- Mechanism: The framework defines three evaluation objectives (Retrieval, Grounding, Presentation) that correspond to the three user steps (Selection, Interaction, Synthesis) in the information search process, creating a comprehensive evaluation approach.
- Core assumption: Users interact with generated text responses sequentially with diminishing attention, similar to reading documents, rather than scanning a ranked list.
- Evidence anchors:
  - [abstract] "we survey the relevant literature from the fields of information retrieval and natural language processing, identify search tasks and system architectures in generative retrieval, develop a corresponding user model"
  - [section 3.4.2] "We therefore propose a reading model in generative IR... users parse a document sequentially, i.e., progress through the statements constituting the text in order. Second, Decay... implies that the reading attention diminishes over the span of the text."

### Mechanism 3
- Claim: The framework distinguishes between reference-free and reference-based utility assessment to handle the dynamic nature of generated responses versus static document collections.
- Mechanism: Reference-free assessment evaluates responses directly without comparison to ground truth, while reference-based assessment compares generated content to existing judgments, addressing the challenge that generative systems produce novel text at query time.
- Evidence anchors:
  - [abstract] "As the established evaluation methodology for ranking-based ad hoc retrieval may seem unsuitable for generative retrieval, new approaches for reliable, repeatable and reproducible experimentation are required."
  - [section 4.3] "Two different settings for collecting utility assessments can be discerned: (1) the response to a query is assessed solely relying on the direct assessment of the responses, without comparing to a separate ground truth; and (2) pre-existing judgments on the same document and/or query set exist to which the unjudged responses can be compared."

## Foundational Learning

- Concept: Synthetical search task
  - Why needed here: Understanding this concept is crucial because it represents the core contribution of generative retrieval - condensing information from multiple sources into a single comprehensive text response rather than ranking individual documents.
  - Quick check question: How does the synthetical search task differ from traditional informational search tasks in terms of user intent and system response format?

- Concept: User model evolution from list SERPs to text SERPs
  - Why needed here: The traditional user model assumes sequential document examination, but generative IR requires modeling how users interact with synthesized text, including reading behavior and attention decay.
  - Quick check question: What are the key differences between browsing models for traditional document ranking versus the proposed reading model for generated text responses?

- Concept: Evaluation objectives hierarchy
  - Why needed here: Understanding how Retrieval, Grounding, and Presentation objectives map to different aspects of system quality is essential for implementing the evaluation framework correctly.
  - Quick check question: How do the three evaluation objectives (Retrieval, Grounding, Presentation) correspond to different stages of the user's information search process?

## Architecture Onboarding

- Component map: Experimental setup (documents, topics, systems) -> Statement segmentation (optional) -> Utility assessment (reference-free/reference-based) -> Effectiveness measurement (aggregating statement utilities)
- Critical path: 1) Define synthetical search task and user model → 2) Establish evaluation objectives and utility dimensions → 3) Design operationalization methods (segmentation, assessment types) → 4) Implement effectiveness measures → 5) Validate through user studies
- Design tradeoffs: Reference-free assessment allows rapid experimentation but may lack accuracy; reference-based assessment is more reliable but requires existing judgments; statement-level vs. response-level utility affects granularity and computational cost
- Failure signatures: Poor correlation between measured utility and actual user satisfaction indicates model mismatch; inconsistent utility judgments across assessors suggest unclear utility dimension definitions; inability to differentiate systems suggests measure insensitivity
- First 3 experiments:
  1. Implement basic statement segmentation on generated responses and assess agreement between human annotators on statement boundaries
  2. Conduct pilot reference-free assessment of generated responses using multiple annotators to establish inter-annotator reliability for utility dimensions
  3. Compare reference-free vs. reference-based assessment results on a small dataset to evaluate the impact of each approach on system rankings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should the decay and saturation properties of the proposed reading model for generative IR be operationalized in practice? Specifically, what form should the discount function take to accurately model user attention over statements in a text SERP?
- Basis in paper: [explicit] The paper proposes a reading model inspired by document comprehension studies, but acknowledges that the logarithmic discount function used in traditional IR measures like DCG may not be appropriate.
- Why unresolved: The paper does not provide empirical validation of the proposed reading model or guidance on selecting an appropriate discount function. User behavior studies specific to generative IR text SERPs are lacking.
- What evidence would resolve it: User studies measuring attention patterns and stopping criteria when reading generative IR responses, followed by validation of different discount functions against these behavioral data.

### Open Question 2
- Question: How can reference-based evaluation be effectively implemented for generative IR when no stable document identifiers exist for the generated text?
- Basis in paper: [explicit] The paper identifies that generative IR poses challenges for offline evaluation because "no stable document identifiers are available" and each response must be judged anew.
- Why unresolved: The paper discusses the problem but does not provide concrete solutions for creating stable references or adapting existing reference-based evaluation methods.
- What evidence would resolve it: Development and validation of methods to create persistent identifiers for generated statements or hybrid evaluation approaches combining reference-free and reference-based assessment.

### Open Question 3
- Question: Which utility dimensions from the proposed taxonomy are most important for users when evaluating generative IR responses, and how do these weights vary across different information needs?
- Basis in paper: [explicit] The paper proposes 10 utility dimensions across five categories but acknowledges that "it is currently unclear if this is an appropriate choice" and that "user experiments are required to effectively apply the theoretical motivation."
- Why unresolved: The utility dimensions are theoretically derived but not empirically validated against actual user preferences or satisfaction.
- What evidence would resolve it: Large-scale user studies where participants rate the importance of different utility dimensions across various query types, potentially leading to weighted combinations of dimensions.

## Limitations
- The proposed reading model assumptions about sequential attention decay lack empirical validation through user behavior studies
- Optimal methods for statement segmentation are unknown, creating uncertainty in how to balance granularity with annotation feasibility
- The framework has not been validated to demonstrate its ability to differentiate between system quality levels in practice

## Confidence
- Synthetical search task identification: High
- Evaluation objectives framework: Medium  
- Reading model assumptions: Low
- Reference-free vs reference-based distinction: Medium

## Next Checks
1. Conduct user studies to empirically validate the proposed reading model assumptions about sequential attention decay and saturation effects in text SERPs
2. Implement pilot experiments comparing different statement segmentation methods (sentence-based vs reference-based) to determine optimal granularity and inter-annotator agreement
3. Run a small-scale evaluation using both reference-free and reference-based assessment approaches on the same dataset to measure correlation between methods and their ability to rank system quality accurately