---
ver: rpa2
title: 'SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification
  on Scientific Tables'
arxiv_id: '2305.13186'
source_url: https://arxiv.org/abs/2305.13186
tags:
- claims
- claim
- scienti
- reasoning
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCITAB, a challenging benchmark dataset for
  scientific fact-checking on scientific tables. It consists of 1,225 expert-verified
  scientific claims that require compositional reasoning and are paired with evidence-containing
  scientific tables.
---

# SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables

## Quick Facts
- arXiv ID: 2305.13186
- Source URL: https://arxiv.org/abs/2305.13186
- Reference count: 31
- Key outcome: Most models achieve barely above random guessing on SCITAB, with GPT-4 as the only exception

## Executive Summary
SCITAB is a new benchmark dataset designed to evaluate scientific fact-checking on tables, addressing limitations in existing benchmarks that often rely on crowdsourced data and simpler reasoning tasks. The dataset contains 1,225 expert-verified scientific claims derived from real publications, paired with evidence-containing scientific tables. SCITAB emphasizes compositional reasoning by requiring multiple reasoning steps, including numerical operations and trend analysis, making it significantly more challenging than existing benchmarks. The authors evaluate various models, including table-based pretraining models and large language models, finding that most models perform barely above random guessing, highlighting the difficulty of the task.

## Method Summary
SCITAB is constructed through a human-model collaboration approach, where claims are first automatically generated from scientific papers and then manually verified by experts to ensure quality and authenticity. The dataset pairs scientific claims with evidence-containing tables, requiring models to perform compositional reasoning to verify claims as Supported, Refuted, or Not Enough Information (NEI). The authors evaluate multiple model types, including table-based models (TAPAS, TAPEX), encoder-decoder models (FLAN-T5), decoder-only models (Vicuna), and API-based LLMs (InstructGPT, GPT-4), under zero-shot and in-context learning settings. Performance is measured using macro-F1 scores for both 2-class (excluding NEI) and 3-class classification tasks.

## Key Results
- All models except GPT-4 achieve performance barely above random guessing
- SCITAB contains 86% deep claims requiring compositional reasoning
- Most models struggle particularly with NEI classification and compositional reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert-annotated claims from real scientific papers make SCITAB more representative of real-world fact-checking.
- Mechanism: By sourcing claims directly from actual scientific statements rather than crowdsourced data, the dataset avoids biases and shallow claims common in existing benchmarks.
- Core assumption: Claims from real scientific papers inherently require more complex reasoning than crowd-sourced claims.
- Evidence anchors:
  - [abstract]: "claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification"
  - [section]: "The claims are constructed from actual scientific statements authored by researchers"
- Break condition: If real scientific papers predominantly contain simple claims, the advantage disappears.

### Mechanism 2
- Claim: Using tables as evidence makes SCITAB closer to real-world scientific fact-checking.
- Mechanism: Scientific claims often depend on quantitative experimental data presented in tables, not just text abstracts.
- Core assumption: Tables contain unique information not easily captured in text summaries.
- Evidence anchors:
  - [abstract]: "the evidence is presented as tables, closely mirroring real-world fact-checking scenarios"
  - [section]: "claims are intrinsically tied to quantitative experimental data, commonly presented in tables"
- Break condition: If table comprehension models don't significantly outperform text-only models, the advantage is minimal.

### Mechanism 3
- Claim: Compositional reasoning required for SCITAB claims makes it more challenging than existing benchmarks.
- Mechanism: Claims require multiple reasoning steps including numerical operations, trend analysis, and domain knowledge.
- Core assumption: Complex reasoning tasks are harder for current models to solve.
- Evidence anchors:
  - [section]: "the claims in SCITAB necessitate a more comprehensive and nuanced set of reasoning skills for verification"
  - [section]: "most of the claims (86%) fall into the deep claims category"
- Break condition: If models can learn to solve these tasks with simple heuristics, the challenge is overstated.

## Foundational Learning

- Concept: Scientific claim verification
  - Why needed here: Understanding how claims relate to evidence in scientific literature
  - Quick check question: What distinguishes a scientific claim from a description or background statement?

- Concept: Compositional reasoning
  - Why needed here: Claims require multiple reasoning steps to verify
  - Quick check question: How many reasoning steps does a claim requiring trend analysis and numerical comparison involve?

- Concept: Table grounding
  - Why needed here: Correctly identifying which table cells support or refute a claim
  - Quick check question: What information from table captions is essential for claim verification?

## Architecture Onboarding

- Component map: Data preparation -> Automatic claim generation -> Manual verification -> Model evaluation -> Error analysis
- Critical path: Manual verification ensures claim quality; model evaluation tests real-world applicability
- Design tradeoffs: Expert annotation ensures quality but limits scale; automatic generation increases quantity but requires verification
- Failure signatures: Models performing above random guessing on simple claims but failing on compositional reasoning tasks
- First 3 experiments:
  1. Evaluate zero-shot performance of text-based LLMs on SCITAB claims
  2. Test table-specific models (TAPAS, TAPEX) on the dataset
  3. Analyze error patterns for claims requiring numerical reasoning vs domain knowledge

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can models be improved to better handle ambiguous expressions in scientific claims?
- Basis in paper: [inferred] The paper mentions that ambiguity errors are a unique challenge in SCITAB, where the claim contains ambiguous expressions that the program fails to represent.
- Why unresolved: The paper does not provide a solution to this problem.
- What evidence would resolve it: A model that can accurately handle ambiguous expressions in scientific claims would resolve this question.

### Open Question 2
- Question: How can models be improved to better handle grounding errors in scientific claims?
- Basis in paper: [inferred] The paper mentions that grounding errors are a unique challenge in SCITAB, where the program incorrectly associates data with the respective cells in the table.
- Why unresolved: The paper does not provide a solution to this problem.
- What evidence would resolve it: A model that can accurately handle grounding errors in scientific claims would resolve this question.

### Open Question 3
- Question: How can models be improved to better handle calculation errors in scientific claims?
- Basis in paper: [inferred] The paper mentions that calculation errors are a unique challenge in SCITAB, where incorrect float digits in the Python code lead to inaccurate calculation results.
- Why unresolved: The paper does not provide a solution to this problem.
- What evidence would resolve it: A model that can accurately handle calculation errors in scientific claims would resolve this question.

## Limitations

- Manual verification process limits dataset scale and may not capture full diversity of scientific claims
- Evaluation focuses on zero-shot and in-context learning, leaving fine-tuning performance questions open
- Some claims may retain ambiguity or require implicit domain knowledge not captured in tables

## Confidence

**High Confidence:** The dataset construction methodology and claim complexity analysis are well-documented and reproducible. The finding that most models (except GPT-4) perform barely above random guessing is supported by multiple evaluation approaches and aligns with the challenging nature of the dataset.

**Medium Confidence:** The analysis of why models struggle with SCITAB is based on reasonable interpretations of error patterns, though some failure modes could benefit from more systematic analysis. The claim that SCITAB is "more challenging" than existing benchmarks is supported by comparison results but would benefit from additional ablation studies.

**Low Confidence:** The assertion that real scientific claims are inherently more complex than crowd-sourced claims relies on assumptions about the nature of scientific writing that aren't directly tested within the paper.

## Next Checks

1. **Scale and Diversity Analysis:** Conduct experiments to determine how dataset size and claim diversity affect model performance, testing whether the current scale adequately represents the space of scientific claims.

2. **Fine-tuning Experiments:** Evaluate whether fine-tuning table-based models on scientific domain data significantly improves performance compared to zero-shot settings, particularly for claims requiring compositional reasoning.

3. **Knowledge Grounding Study:** Systematically analyze how much performance improvement can be achieved by incorporating external scientific knowledge bases, distinguishing between reasoning failures and knowledge gaps.