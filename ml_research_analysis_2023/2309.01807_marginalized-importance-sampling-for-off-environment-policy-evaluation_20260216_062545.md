---
ver: rpa2
title: Marginalized Importance Sampling for Off-Environment Policy Evaluation
arxiv_id: '2309.01807'
source_url: https://arxiv.org/abs/2309.01807
tags:
- policy
- learning
- environment
- target
- simulator
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of off-environment policy evaluation\
  \ in robotics, where the goal is to assess a policy's real-world performance using\
  \ a simulator and offline data. The authors propose a new method called \u03B2-DICE\
  \ that leverages the target policy's occupancy in the simulator as an intermediate\
  \ variable to learn the density ratio as a product of two terms."
---

# Marginalized Importance Sampling for Off-Environment Policy Evaluation

## Quick Facts
- arXiv ID: 2309.01807
- Source URL: https://arxiv.org/abs/2309.01807
- Reference count: 40
- Primary result: β-DICE achieves 60% improvement in log mean squared error on Kinova robotic arm task

## Executive Summary
This paper addresses the challenge of evaluating robot policies in the real world using only simulator data and offline samples from the actual environment. The authors propose β-DICE, a method that decomposes the importance weight into two terms - one learned with direct supervision and another with small magnitude - to address both large ratios and indirect supervision challenges in existing approaches. The method is evaluated on Sim2Sim environments (Cartpole, Reacher, Half-Cheetah) and a Sim2Real task (7 DoF robotic arm), demonstrating superior performance compared to state-of-the-art baselines.

## Method Summary
The paper proposes β-DICE for off-environment policy evaluation, which leverages the target policy's occupancy in the simulator as an intermediate variable. The density ratio wπ/µ is decomposed as (dπ/Ptr/µ) · (dπ/Pte/dπ/Ptr), where the first term β = dπ/Ptr/µ can be learned with direct supervision since dπ/Ptr is freely accessible from the simulator, and the second term wπ/Pte/Ptr has small magnitude when the simulator and real environment are similar. The method first estimates β using techniques from prior work, then estimates wπ/Pte/Ptr via minimax optimization, and combines them to evaluate real-world policy performance.

## Key Results
- β-DICE outperforms state-of-the-art baselines on Sim2Sim environments (Cartpole, Reacher, Half-Cheetah)
- 60% improvement in log mean squared error on Kinova robotic arm task compared to existing methods
- Performance degrades gracefully as Sim2Real gap increases, with effectiveness dropping when gap exceeds approximately 60%
- Method successfully handles the challenges of large importance weights and indirect supervision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method addresses the "large ratios" problem by splitting the importance weight into two terms, where the second term (wπ/Pte/Ptr) is close to 1 when the simulator and real environment are similar.
- Mechanism: The weight wπ/µ is decomposed as (dπ/Ptr/µ) · (dπ/Pte/dπ/Ptr), allowing the first term to be learned with direct supervision and the second term to have small magnitude.
- Core assumption: The simulator Ptr is sufficiently close to the real environment Pte that the ratio dπ/Pte/dπ/Ptr remains near 1.
- Evidence anchors:
  - [abstract]: "Our approach addresses these challenges by introducing the target policy's occupancy in the simulator as an intermediate variable and learning the density ratio as the product of two terms that can be learned separately. The first term is learned with direct supervision and the second term has a small magnitude"
  - [section]: "Note that dπ/Ptr is the occupancy of π in the simulator, which we have free access to. The advantage of our approach is that by estimating β and wπ/Pte/Ptr separately, we avoid the situation of running into the two challenges mentioned before simultaneously"
  - [corpus]: Weak evidence - no directly relevant papers found in corpus
- Break condition: When the Sim2Real gap exceeds approximately 60%, the second term wπ/Pte/Ptr deviates significantly from 1, making it difficult to learn.

### Mechanism 2
- Claim: The method addresses the "indirect supervision" problem by using the simulator occupancy dπ/Ptr as an intermediate variable that can be directly computed.
- Mechanism: By computing dπ/Ptr directly from the simulator, the method can reweight real-world data using β = dπ/Ptr/µ to approximate expectations w.r.t. dπ/Ptr × Pte.
- Evidence anchors:
  - [abstract]: "Our approach addresses these challenges by introducing the target policy's occupancy in the simulator as an intermediate variable"
  - [section]: "We further show that such a MIS estimator can be used to evaluate the real-world performance of a robot using just the simulator"
  - [corpus]: Weak evidence - no directly relevant papers found in corpus
- Break condition: When the simulator's occupancy distribution dπ/Ptr differs significantly from the true distribution dπ/Pte, the reweighting becomes ineffective.

### Mechanism 3
- Claim: The two-step learning procedure (first learning β, then w) reduces estimation error by handling different challenges separately.
- Mechanism: The method first learns β = dπ/Ptr/µ using direct supervision via [20], then uses this estimate to learn wπ/Pte/Ptr via minimax optimization.
- Evidence anchors:
  - [abstract]: "Our approach addresses these challenges by introducing the target policy's occupancy in the simulator as an intermediate variable and learning the density ratio as the product of two terms that can be learned separately"
  - [section]: "Estimate wπ/Pte/Ptr: Since β is handled by the method of [20], the key remaining challenge is how to estimate wπ/Pte/Ptr"
  - [corpus]: Weak evidence - no directly relevant papers found in corpus
- Break condition: When either the β estimation or w estimation becomes highly inaccurate, the two-step approach may accumulate errors.

## Foundational Learning

- Concept: Importance Sampling (IS) and Marginalized Importance Sampling (MIS)
  - Why needed here: The method builds on MIS framework for off-policy evaluation
  - Quick check question: What is the key difference between trajectory-based IS and state-based MIS?

- Concept: Density ratio estimation
  - Why needed here: The method learns density ratios to reweight samples between distributions
  - Quick check question: How does the method of [20] estimate density ratios from direct supervision?

- Concept: Function approximation and RKHS (Reproducing Kernel Hilbert Spaces)
  - Why needed here: The method uses function classes like RKHS to approximate the weight functions
  - Quick check question: What property of RKHS allows for closed-form solutions in certain cases?

## Architecture Onboarding

- Component map:
  - Data collection module: Collects offline data from real environment
  - Simulator module: Provides free access to dπ/Ptr for any policy π
  - β estimator: Learns the ratio dπ/Ptr/µ using direct supervision
  - w estimator: Learns wπ/Pte/Ptr via minimax optimization
  - Evaluation module: Combines β and w to estimate real-world performance

- Critical path:
  1. Collect offline data from real environment (s, a) ~ µ
  2. Compute dπ/Ptr from simulator for target policy π
  3. Learn β = dπ/Ptr/µ using direct supervision
  4. Use β to learn wπ/Pte/Ptr via minimax optimization
  5. Estimate real-world performance: E[(β · w) · r]

- Design tradeoffs:
  - Accuracy vs. computational cost: Using RKHS for closed-form solutions vs. neural networks
  - Model complexity: Simple linear models vs. complex neural networks
  - Sample efficiency: Direct supervision for β vs. indirect supervision for w

- Failure signatures:
  - Large evaluation error when Sim2Real gap exceeds 60%
  - High variance in performance estimates
  - Degraded performance with insufficient coverage of state-action space

- First 3 experiments:
  1. Taxi environment with varying behavior policies and Sim2Real gap
  2. Cartpole environment with gravity differences between simulator and real world
  3. Kinova robotic arm validation using gazebo simulator and offline data

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- Performance degrades significantly when Sim2Real gap exceeds approximately 60%, limiting applicability to cases where simulator is a poor approximation of real environment
- The method requires access to a simulator that can compute occupancy distributions dπ/Ptr for arbitrary policies, which may not be available for all robotic systems
- Sample complexity in high-dimensional state-action spaces remains unexplored, with current results limited to relatively simple control tasks

## Confidence
- High: The two-term decomposition of the density ratio and the use of simulator occupancy dπ/Ptr as an intermediate variable are well-founded theoretical approaches
- Medium: Empirical results show strong performance improvements, but the sample size and experimental diversity are limited
- Low: The robustness of the method to large Sim2Real gaps and its generalization to different robotic platforms remains uncertain

## Next Checks
1. Systematically vary the Sim2Real gap (e.g., gravity, friction, actuator delays) and measure the performance degradation threshold where β-DICE fails
2. Conduct ablation studies removing either the β estimation or w estimation components to quantify their individual contributions
3. Test on at least three additional robotic platforms beyond Kinova, including both manipulators and mobile robots, to assess generalization