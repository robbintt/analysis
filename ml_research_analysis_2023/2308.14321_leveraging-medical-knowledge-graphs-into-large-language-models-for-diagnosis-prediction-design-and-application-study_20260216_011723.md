---
ver: rpa2
title: 'Leveraging Medical Knowledge Graphs Into Large Language Models for Diagnosis
  Prediction: Design and Application Study'
arxiv_id: '2308.14321'
source_url: https://arxiv.org/abs/2308.14321
tags:
- graph
- medical
- input
- path
- diagnoses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DR.KNOWS, a novel knowledge graph-based model
  that improves diagnosis prediction in clinical settings by leveraging medical knowledge
  graphs derived from UMLS. The approach combines multi-hop path reasoning with foundation
  models like T5 and ChatGPT, achieving enhanced accuracy in automated diagnosis generation.
---

# Leveraging Medical Knowledge Graphs Into Large Language Models for Diagnosis Prediction: Design and Application Study

## Quick Facts
- arXiv ID: 2308.14321
- Source URL: https://arxiv.org/abs/2308.14321
- Reference count: 40
- Primary result: DR.KNOWS improves diagnosis prediction by leveraging UMLS knowledge graphs with foundation models, achieving enhanced accuracy through multi-hop path reasoning.

## Executive Summary
This paper introduces DR.KNOWS, a novel knowledge graph-based model that improves diagnosis prediction in clinical settings by leveraging medical knowledge graphs derived from UMLS. The approach combines multi-hop path reasoning with foundation models like T5 and ChatGPT, achieving enhanced accuracy in automated diagnosis generation. Experiments show DR.KNOWS outperforms baseline methods, with ChatGPT+Path demonstrating strong performance by explicitly utilizing predicted paths in its reasoning process. The work addresses the challenge of integrating structured medical knowledge into large language models for explainable and accurate diagnosis prediction.

## Method Summary
DR.KNOWS leverages UMLS knowledge graphs to enhance diagnosis prediction by retrieving 2-hop neighbor concepts from input medical concepts and ranking them using semantic relevance. These paths are then incorporated into prompts for foundation models (T5 variants and ChatGPT) to generate diagnoses. The approach combines graph-based path prediction with prompt engineering to create an explainable diagnostic pathway. The method is evaluated on MIMIC-III and IN-HOUSE datasets, measuring performance using ROUGE scores and CUI F-score metrics.

## Key Results
- DR.KNOWS achieves superior CUI F-score performance compared to baseline methods across both MIMIC-III and IN-HOUSE datasets
- ChatGPT+Path demonstrates explicit utilization of knowledge paths in reasoning, with 38% of outputs mentioning path-based inference
- Path-based prompts significantly improve foundation model performance, with Vanilla-T5+Path achieving highest ROUGE-L score (30.72) and CUI F-score (27.78)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multi-hop path reasoning from UMLS knowledge graph improves diagnosis prediction accuracy
- **Mechanism**: DR.KNOWS retrieves 2-hop neighbor concepts from input medical concepts and ranks them using semantic relevance to the patient narrative, then uses these paths as prompts for LLMs
- **Core assumption**: Medical diagnoses can be inferred through multi-hop semantic relationships in UMLS, and foundation models can effectively utilize these paths as reasoning aids
- **Evidence anchors**:
  - [abstract]: "Our method negates the need for pre-training and instead leverages the KG as an auxiliary instrument aiding in the interpretation and summarization of complex medical concepts"
  - [section 5.2]: "For each node representation hi, we use its n-hop h(n)t,i of the set neighborhood V(n)t for hi and the associated relation edge e(n)t,i to generate the corresponding path embeddings"
  - [corpus]: Weak - the related papers show similar approaches but none directly validate this specific mechanism
- **Break condition**: If the UMLS KG lacks relevant paths for the diagnosis, or if the semantic relationships become too attenuated after 2 hops

### Mechanism 2
- **Claim**: Path-based prompts improve foundation model performance on diagnosis summarization
- **Mechanism**: The predicted paths are incorporated into prompts using structured patterns that explicitly instruct models to use the knowledge paths in their reasoning
- **Core assumption**: Foundation models can interpret and utilize structured knowledge path information when properly prompted
- **Evidence anchors**:
  - [abstract]: "Our approach offers an explainable diagnostic pathway, edging us closer to the realization of AI-augmented diagnostic decision support systems"
  - [section 8]: "Models benefit from incorporating paths, particularly for the CUI F-score. Vanilla-T5 with path prompt excels, achieving the highest ROUGE-L score (30.72) and CUI F-score (27.78)"
  - [corpus]: Weak - related work shows KG integration but doesn't validate this specific prompt-based approach
- **Break condition**: If the path prompts become too verbose and exceed model context limits, or if models ignore the path information in favor of copying from input

### Mechanism 3
- **Claim**: ChatGPT explicitly utilizes knowledge paths in its reasoning process
- **Mechanism**: ChatGPT+Path generates reasoning sections that explicitly mention using knowledge paths for diagnosis inference, demonstrating actual utilization of the provided information
- **Core assumption**: Zero-shot prompting with explicit reasoning instructions enables ChatGPT to incorporate knowledge paths into its diagnostic reasoning
- **Evidence anchors**:
  - [section 8]: "We further find that among 237 test samples, 38% (n=78) had output by ChatGPT+Path that explicitly mentions the diagnosis is inferred from or supported by the knowledge paths/graph in its reasoning"
  - [section 8]: "ChatGPT+Path picks up this name from the paths instead of HTN, which might be penalized by ROUGE scores"
  - [corpus]: Weak - no direct evidence in related work about ChatGPT's explicit path utilization behavior
- **Break condition**: If ChatGPT ignores the path information entirely or if the reasoning sections become hallucinated without actual path utilization

## Foundational Learning

- **Concept**: Multi-hop reasoning in knowledge graphs
  - **Why needed here**: Understanding how to traverse and extract meaningful information from UMLS requires knowledge of graph traversal algorithms and multi-hop reasoning patterns
  - **Quick check question**: What is the maximum number of hops used in DR.KNOWS for path prediction, and why was this limit chosen?

- **Concept**: Prompt engineering with structured knowledge
  - **Why needed here**: The system relies on carefully designed prompts that incorporate knowledge graph paths, requiring understanding of how to structure prompts for optimal model performance
  - **Quick check question**: How many manual prompt styles were created, and what are the two main components of each prompt?

- **Concept**: Evaluation metrics for clinical NLP tasks
  - **Why needed here**: The system uses specific metrics like CUI F-score that combine precision and recall for medical concept extraction, requiring understanding of clinical terminology evaluation
  - **Quick check question**: What are the two evaluation metrics used in this work, and what distinct value does each provide?

## Architecture Onboarding

- **Component map**: Input processing -> CUI extraction using QuickUMLS/cTAKES -> DR.KNOWS path prediction -> Path-based prompt generation -> Foundation model inference -> CUI extraction and evaluation
- **Critical path**: 
  1. Extract CUIs from input text
  2. Build 2-hop subgraphs from UMLS
  3. Encode and rank paths using DR.KNOWS
  4. Generate path-based prompts
  5. Run foundation models with prompts
  6. Extract and evaluate diagnoses
- **Design tradeoffs**:
  - 2-hop limit vs. longer paths: Balances computational cost with semantic relevance
  - Prompt verbosity vs. context limits: Longer prompts provide more guidance but may exceed model limits
  - Concept name selection: Choosing first name vs. all possible names affects ROUGE scoring
- **Failure signatures**:
  - Low CUI recall: Indicates graph model not finding relevant paths
  - High ROUGE but low CUI F-score: Suggests copying from input rather than using paths
  - Inconsistent outputs: May indicate prompt sensitivity issues
- **First 3 experiments**:
  1. Test DR.KNOWS path prediction accuracy on MIMIC-III vs. IN-HOUSE datasets
  2. Evaluate T5+Path vs. T5 alone on PROBSUM test set
  3. Compare ChatGPT+Path vs. ChatGPT alone with different prompt styles

## Open Questions the Paper Calls Out
- How can the accuracy of the DR.KNOWS model be further improved to reduce the generation of less relevant paths that may negatively impact foundation model performance?
- How can the proposed approach be extended to incorporate other modalities of information within the EHR, such as radiology reports, discharge summaries, and structured data like laboratory results and vital signs?
- How can the potential biases in the AI models be identified, quantified, and mitigated to ensure fairness and prevent disparities in care quality and outcomes?

## Limitations
- Validation scope limited to two datasets (MIMIC-III and IN-HOUSE), restricting generalizability to other clinical settings
- System performance dependent on completeness and relevance of UMLS knowledge graph, which may contain missing or outdated relationships
- Approach requires substantial computational resources for both KG path prediction and LLM inference, potentially limiting practical deployment

## Confidence
- **High Confidence**: The mechanism of using multi-hop path reasoning from UMLS knowledge graphs shows consistent performance improvements across multiple foundation models
- **Medium Confidence**: The superiority of path-based prompting over vanilla approaches is demonstrated, but exact contribution of different prompt styles remains unclear
- **Low Confidence**: The generalizability of the approach to other clinical tasks beyond diagnosis prediction remains unproven

## Next Checks
1. Test DR.KNOWS on additional clinical datasets from different institutions and medical specialties to assess generalizability
2. Conduct systematic ablation studies to quantify individual contributions of KG path quality, prompt engineering, and foundation model selection
3. Evaluate the approach in a simulated clinical environment with time constraints and varying input quality to assess practical feasibility