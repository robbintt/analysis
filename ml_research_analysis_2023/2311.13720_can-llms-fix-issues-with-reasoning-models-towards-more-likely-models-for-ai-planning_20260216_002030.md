---
ver: rpa2
title: Can LLMs Fix Issues with Reasoning Models? Towards More Likely Models for AI
  Planning
arxiv_id: '2311.13720'
source_url: https://arxiv.org/abs/2311.13720
tags:
- city
- planning
- domain
- neighboring
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the use of large language models (LLMs) to
  improve model space reasoning in automated planning. It addresses the challenge
  of finding model edits that make unsolvable planning problems solvable or incorrect
  plans executable, focusing on generating more "likely" edits based on real-world
  plausibility.
---

# Can LLMs Fix Issues with Reasoning Models? Towards More Likely Models for AI Planning

## Quick Facts
- arXiv ID: 2311.13720
- Source URL: https://arxiv.org/abs/2311.13720
- Reference count: 40
- Key outcome: LLM-only approaches outperform CS+LLM combinations in finding sound and reasonable model edits for automated planning tasks

## Executive Summary
This paper investigates using large language models to improve model space reasoning in automated planning by generating more "likely" edits that make unsolvable problems solvable or incorrect plans executable. The authors evaluate three configurations: LLM-only, LLM as post-processor to combinatorial search, and LLM as pre-processor to guide search. Experiments across custom domains (Travel, Roomba, Barman-simple, Logistics-simple) and public domains (Barman, Logistics) demonstrate that LLM-only approaches consistently outperform CS+LLM combinations in finding sound and reasonable solutions, particularly in public domains and for smaller problem instances. Performance is limited by context length constraints, but LLMs show strong statistical signals for generating plausible model edits.

## Method Summary
The study evaluates LLM-based approaches for model space reasoning in automated planning using three configurations: standalone LLM, LLM as post-processor to combinatorial search (CS), and LLM as pre-processor to guide CS. The LLM generates model edits to make unsolvable problems solvable or incorrect plans executable, with edits evaluated for soundness (problem resolution) and reasonableness (real-world plausibility). Experiments use custom domains (Travel, Roomba, Barman-simple, Logistics-simple) and public domains (Barman, Logistics) with both GPT-3.5-turbo and GPT-4 models. Performance is measured across two reasoning tasks: identifying unsolvability causes and generating executable plans from incorrect ones.

## Key Results
- LLM-only approaches consistently outperform CS+LLM combinations in finding sound and reasonable solutions
- Public domains show better LLM performance than custom domains, suggesting pretraining data influence
- Context length limits LLM applicability to larger domains despite effectiveness on smaller instances
- GPT-4 demonstrates superior performance compared to GPT-3.5-turbo across all configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can act as standalone model space reasoners by directly estimating the likelihood of model edits given the use case and current model.
- Mechanism: The LLM is prompted to output the most "reasonable" or "likely" set of model edits that would resolve the unsolvability or executability issue. This leverages the LLM's statistical knowledge of real-world plausibility to rank model edits beyond logical equivalence.
- Core assumption: The LLM's pretraining data includes sufficient real-world knowledge to distinguish between plausible and implausible model edits in planning domains.
- Evidence anchors:
  - [abstract] "LLMs provide a strong statistical signal for model edits, suggesting their potential for enhancing planning system authoring and debugging."
  - [section] "LLM approaches are limited by the size of the prompt and thus does not scale to large domains even for computationally simpler problem instances."
- Break condition: If the LLM lacks sufficient domain-specific knowledge or if the prompt exceeds context limits, the approach fails to produce sound or reasonable edits.

### Mechanism 2
- Claim: LLMs can be used as post-processors to rank candidate model edits generated by combinatorial search (CS).
- Mechanism: CS generates a set of sound model edits that satisfy the objective metric (solvability or executability). The LLM then selects the most likely edit from this set based on real-world plausibility.
- Core assumption: CS can efficiently generate a complete or near-complete set of sound model edits, and the LLM can effectively rank them.
- Evidence anchors:
  - [abstract] "The LLM as a post-processor to combinatorial search (CS)" is one of the evaluated configurations.
  - [section] "CS approaches are limited by the complexity of search. Thus even while being theoretically sound and complete, they produce fewer solutions and hence fewer sound solutions in absolute numbers."
- Break condition: If CS fails to generate a sufficient number of candidate edits or if the LLM's ranking does not align with real-world likelihood, the approach underperforms compared to LLM-only.

### Mechanism 3
- Claim: LLMs can guide CS by providing a ranked order of likely model edits, influencing the search order and cost function.
- Mechanism: The LLM outputs a ranked list of model edits based on plausibility. CS uses this ranking to prioritize edits with lower costs, potentially finding solutions faster or with more likely edits.
- Core assumption: The LLM's ranking of edits correlates with the actual likelihood of those edits in real-world scenarios.
- Evidence anchors:
  - [abstract] "LLM as a pre-processor to guide CS" is one of the evaluated configurations.
  - [section] "For the purposes of our implementation, we converted all the ordered edits proposed by the LLM into a set of actions that the CS can perform with different costs."
- Break condition: If the LLM's ranking is inaccurate or if the CS search space is too large, the guided search may not find a solution or may not outperform other configurations.

## Foundational Learning

- Concept: Model space reasoning in automated planning
  - Why needed here: The paper addresses the challenge of finding model edits that make unsolvable planning problems solvable or incorrect plans executable. Understanding this concept is crucial for grasping the problem domain and the proposed solution.
  - Quick check question: What is the difference between classical planning and model space reasoning in automated planning?

- Concept: Large language models (LLMs) and their application to planning tasks
  - Why needed here: The paper explores the use of LLMs to improve model space reasoning by leveraging their statistical knowledge of real-world plausibility. Understanding LLMs and their capabilities is essential for evaluating the proposed approach.
  - Quick check question: How do LLMs differ from traditional combinatorial search methods in terms of their approach to problem-solving?

- Concept: Evaluation metrics for model space reasoning tasks
  - Why needed here: The paper uses specific metrics to evaluate the performance of LLM-based approaches, including soundness, reasonableness, and comparison with CS-based methods. Understanding these metrics is crucial for interpreting the results and drawing conclusions.
  - Quick check question: What is the difference between a sound model edit and a reasonable model edit in the context of this paper?

## Architecture Onboarding

- Component map:
  - LLM (standalone or integrated with CS) -> Combinatorial search (CS) algorithm -> Planning domain definition language (PDDL) parser -> Problem instance generator -> Evaluation metrics calculator

- Critical path:
  1. Generate problem instances (solvable or unsolvable)
  2. Apply LLM-based approach (standalone or integrated with CS)
  3. Evaluate the generated model edits using soundness and reasonableness metrics
  4. Compare the performance of LLM-based approaches with CS-based methods

- Design tradeoffs:
  - LLM-only approach: Simpler implementation but limited by context size and potential hallucinations
  - CS+LLM approaches: More complex implementation but potentially more reliable if CS can generate sufficient candidate edits
  - Public domains vs. custom domains: LLMs may perform better on public domains due to more exposure during pretraining

- Failure signatures:
  - LLM-only approach: Produces unsound or unreasonable edits, exceeds context limits, or fails to generate any edits
  - CS+LLM approaches: CS fails to generate sufficient candidate edits, LLM ranking does not align with real-world likelihood, or the combination underperforms compared to LLM-only

- First 3 experiments:
  1. Evaluate the LLM-only approach on a small set of problem instances from the Travel domain to assess its ability to generate sound and reasonable edits.
  2. Compare the performance of LLM-only and CS+LLM (post-processor) approaches on the same set of problem instances to understand the benefits and limitations of each configuration.
  3. Test the LLM as a pre-processor on a larger set of problem instances from multiple domains to evaluate its effectiveness in guiding CS and generating likely edits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs in model space reasoning tasks compare to traditional heuristic search methods when the search space is extremely large or complex?
- Basis in paper: [inferred] The paper discusses the limitations of combinatorial search (CS) due to computational complexity and the limitations of LLMs due to context size, but does not provide a direct comparison for extremely large or complex search spaces.
- Why unresolved: The paper focuses on a specific set of domains and problem instances, which may not fully represent the challenges posed by extremely large or complex search spaces.
- What evidence would resolve it: Experiments comparing LLM performance to traditional heuristic search methods on a diverse set of large and complex domains with varying degrees of search space complexity.

### Open Question 2
- Question: To what extent does the domain expertise of an LLM, acquired from training data, influence its ability to generate reasonable model edits in specialized or niche domains?
- Basis in paper: [explicit] The paper mentions that LLMs carry domain knowledge on worldly matters but does not explore the impact of this knowledge on specialized or niche domains.
- Why unresolved: The experiments primarily focus on custom and public domains that are likely to be well-represented in the training data, leaving the performance in specialized domains unexplored.
- What evidence would resolve it: Comparative studies of LLM performance in model space reasoning tasks across domains with varying levels of representation in the training data, including highly specialized or niche domains.

### Open Question 3
- Question: How does the phrasing and specificity of prompts affect the quality and reasonableness of model edits generated by LLMs in model space reasoning tasks?
- Basis in paper: [explicit] The paper tests different prompt variations but does not extensively explore the impact of phrasing and specificity on the quality of generated model edits.
- Why unresolved: The study uses a limited set of prompt variations, which may not capture the full spectrum of how prompt phrasing and specificity can influence LLM outputs.
- What evidence would resolve it: Systematic experiments varying the phrasing, specificity, and structure of prompts across multiple domains and reasoning tasks to assess their impact on the quality and reasonableness of generated model edits.

## Limitations

- Context length constraints prevent LLM-only approaches from scaling to larger domains despite effectiveness on smaller instances
- Performance gap between GPT-3.5-turbo and GPT-4 suggests model capacity significantly impacts results, but the analysis doesn't fully explore the underlying reasons
- Evaluation framework relies on custom domains and modified public domains, raising questions about generalizability to standard planning benchmarks

## Confidence

- **High confidence**: LLM-only approaches consistently outperform CS+LLM combinations in finding sound and reasonable solutions across tested domains
- **Medium confidence**: Public domains show better LLM performance due to increased pretraining exposure, though this could reflect domain complexity differences rather than pretraining effects
- **Medium confidence**: LLMs provide strong statistical signals for model edits, but the mechanism for distinguishing plausible from implausible edits remains heuristic rather than theoretically grounded

## Next Checks

1. Test LLM-only approaches on standard IPC benchmark domains with larger problem instances to quantify scaling limitations and identify the exact context threshold where performance degrades
2. Conduct ablation studies comparing different prompt formulations and model configurations to isolate whether GPT-4's advantage comes from reasoning capability versus output generation quality
3. Implement automated reasonableness evaluation using domain experts or automated consistency checks to supplement the current manual evaluation methodology and enable larger-scale validation