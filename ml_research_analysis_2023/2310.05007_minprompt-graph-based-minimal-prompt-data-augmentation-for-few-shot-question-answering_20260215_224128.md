---
ver: rpa2
title: 'MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question
  Answering'
arxiv_id: '2310.05007'
source_url: https://arxiv.org/abs/2310.05007
tags:
- question
- minprompt
- data
- training
- fewshotqa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MINPROMPT, a data augmentation framework
  for few-shot question answering (QA) that leverages graph-based algorithms and unsupervised
  question generation to extract minimal yet informative QA training samples from
  raw text. The core method involves constructing a sentence graph to capture semantic
  overlap, applying an approximate minimal dominating set algorithm to identify the
  most informative sentences, and generating QA pairs from these sentences to create
  high-quality training data.
---

# MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering

## Quick Facts
- arXiv ID: 2310.05007
- Source URL: https://arxiv.org/abs/2310.05007
- Reference count: 12
- Primary result: Graph-based minimal prompt data augmentation improves few-shot QA performance with up to 27.5% F1 gain

## Executive Summary
This paper introduces MINPROMPT, a data augmentation framework for few-shot question answering (QA) that leverages graph-based algorithms and unsupervised question generation to extract minimal yet informative QA training samples from raw text. The core method involves constructing a sentence graph to capture semantic overlap, applying an approximate minimal dominating set algorithm to identify the most informative sentences, and generating QA pairs from these sentences to create high-quality training data. Empirical results on benchmark datasets show that MINPROMPT achieves comparable or better performance than competitive baselines, with improvements in F1 scores by up to 27.5%, while using significantly fewer training examples. The framework demonstrates the effectiveness of minimal data augmentation in improving computational efficiency and model performance for few-shot QA tasks.

## Method Summary
MINPROMPT constructs a sentence graph where nodes represent sentences and edges connect sentences sharing the same entity, then applies a greedy approximate minimal dominating set algorithm to select the smallest subset of sentences that collectively mention all entities. For each selected sentence and answer entity, the framework retrieves a similar sentence from an external corpus and generates QA pairs using template-based question generation with entity-type-specific "wh-" starters. Both original and augmented QA samples are formatted into a unified prompt template and used to fine-tune an encoder-decoder model with weighted loss combining both data sources.

## Key Results
- MINPROMPT achieves up to 27.5% F1 improvement over baselines in few-shot QA settings
- The framework uses significantly fewer training examples while maintaining or improving performance
- Prompt-style training with both original and augmented data proves effective for few-shot learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The minimal dominating set algorithm reduces the training data size while preserving semantic coverage by selecting sentences that collectively mention all entities in the raw text.
- Mechanism: The sentence graph construction maps sentences to nodes and entity co-references to edges; the greedy algorithm selects the smallest set of nodes whose neighbors collectively cover all nodes, ensuring minimal redundancy.
- Core assumption: Sentences sharing the same entity contribute overlapping information, so removing one does not degrade QA model performance if the other remains.
- Evidence anchors:
  - [abstract]: "apply graph algorithms to identify the minimal set of sentences needed to cover the most information in the raw text."
  - [section 3.3]: Assumption 1 states that two sentences mentioning the same entity have similar QA model quality, justifying sentence removal.
  - [corpus]: Weak evidence; only general mentions of graph-based methods in related papers without direct empirical backing.
- Break condition: If entity co-reference detection is inaccurate, the minimal set may omit critical sentences, leading to performance degradation.

### Mechanism 2
- Claim: The unsupervised question generation pipeline improves model generalization by transforming selected factual sentences into QA pairs with varied templates.
- Mechanism: After retrieving a sentence containing the answer entity from an external corpus, a template-based generator produces questions using entity-type-specific "wh-" starters, diversifying the phrasing and context.
- Core assumption: Questions generated from similar but distinct contexts prevent overfitting to the original sentence structure.
- Evidence anchors:
  - [abstract]: "generate QA pairs based on the identified sentence subset and train the model on the selected sentences to obtain the final model."
  - [section 3.4]: Describes two question styles and the retrieval-based approach to avoid cloze-style memorization.
  - [corpus]: Limited; related work cites question generation improvements but not specifically for minimal data augmentation.
- Break condition: If retrieved sentences are too dissimilar from the original, the QA pairs may not align well with the downstream task distribution.

### Mechanism 3
- Claim: Prompt-style data augmentation harmonizes original and synthetic data, allowing fine-tuning with both without distribution shift.
- Mechanism: Both original and augmented examples are formatted into the same prompt template (question, answer mask, context), enabling a unified training objective that balances the two data sources via a weighting hyperparameter λ.
- Core assumption: The encoder-decoder model can learn to generate answers from both original and masked contexts without confusion due to identical input format.
- Evidence anchors:
  - [abstract]: "We formulate both original and augmented QA samples in the same format, allowing us to apply prompt-tuning to take full advantage of pre-trained large language models."
  - [section 3.5]: Explicitly shows the prompt construction and loss combination for original and augmented samples.
  - [corpus]: Moderate; prompt tuning is a well-established technique, but its application to synthetic QA data is not deeply evidenced here.
- Break condition: If λ is poorly tuned, the model may overfit to either original or augmented data, harming generalization.

## Foundational Learning

- Concept: Graph theory – specifically minimal dominating sets.
  - Why needed here: To identify the smallest subset of sentences that collectively mention all entities, ensuring efficient data coverage.
  - Quick check question: In a graph where nodes are sentences and edges represent shared entities, what property must the selected nodes have to form a dominating set?

- Concept: Entity co-reference resolution.
  - Why needed here: To correctly link sentences that discuss the same entity, enabling accurate graph construction.
  - Quick check question: If two sentences both mention "Los Angeles Lakers" but with different surface forms, what must the NER and co-reference system detect to connect them?

- Concept: Prompt-based fine-tuning.
  - Why needed here: To unify original and augmented QA data into a single training format compatible with pre-trained encoder-decoder models.
  - Quick check question: What are the three components of the prompt template used for both original and augmented samples in this framework?

## Architecture Onboarding

- Component map: NER & Entity Typing -> Sentence Graph Construction -> Minimal Dominating Set Approximation -> Question Generation -> Prompt-style Data Augmentation -> Training Module

- Critical path: NER → Sentence Graph → Dominating Set → Question Generation → Prompt Formatting → Training

- Design tradeoffs:
  - Using a greedy algorithm for the dominating set sacrifices optimality for speed; this may slightly increase the number of selected sentences but keeps runtime feasible.
  - Retrieval-based question generation introduces external context, improving generalization but risking mismatch if retrieved sentences differ too much from original contexts.
  - Fixed template prompts ensure consistency but may limit natural language diversity compared to free-form generation.

- Failure signatures:
  - If NER fails, entity co-reference edges will be missing, leading to incomplete sentence coverage.
  - If the dominating set approximation selects too few sentences, critical entities may be omitted.
  - If question generation retrieves unrelated sentences, generated QA pairs will be irrelevant and harm training.

- First 3 experiments:
  1. Verify graph construction: Input a small text, run NER and graph builder, manually inspect nodes and edges for correct entity linking.
  2. Test dominating set selection: Run the greedy algorithm on the sample graph, check that all nodes are covered and the set size matches expectations.
  3. Validate question generation: Input a selected sentence and answer entity, ensure the retrieval module returns a related sentence and the generator produces a valid QA pair with correct "wh-" starter.

## Open Questions the Paper Calls Out
- Question: How does MINPROMPT perform on zero-shot or few-shot QA tasks when no original QA training examples are provided?
  - Basis in paper: [inferred] The paper focuses on few-shot QA and mentions using minimal data augmentation to improve efficiency and performance. However, it does not explicitly explore the scenario where no original QA examples are available.
  - Why unresolved: The paper's experiments are based on few-shot settings with a limited number of original QA examples. It does not address the extreme case of zero-shot learning or the performance when no original examples are provided.
  - What evidence would resolve it: Conducting experiments where MINPROMPT is applied to zero-shot QA tasks or when no original QA examples are available would provide insights into its performance in such scenarios.

- Question: How does MINPROMPT handle complex or multi-hop questions that require reasoning across multiple sentences or paragraphs?
  - Basis in paper: [inferred] The paper focuses on extracting informative sentences and generating QA pairs, but it does not explicitly discuss the handling of complex or multi-hop questions.
  - Why unresolved: The paper does not provide information on how MINPROMPT addresses the challenges posed by complex questions that require reasoning across multiple sentences or paragraphs.
  - What evidence would resolve it: Evaluating MINPROMPT's performance on datasets or tasks that involve complex or multi-hop questions would shed light on its ability to handle such scenarios.

- Question: How does MINPROMPT compare to other data augmentation techniques or methods that leverage external knowledge for few-shot QA tasks?
  - Basis in paper: [explicit] The paper mentions that MINPROMPT introduces external prior knowledge through data augmentation, but it does not compare its performance to other data augmentation techniques or methods that leverage external knowledge.
  - Why unresolved: The paper does not provide a comprehensive comparison of MINPROMPT with other data augmentation techniques or methods that utilize external knowledge for few-shot QA tasks.
  - What evidence would resolve it: Conducting experiments that compare MINPROMPT with other data augmentation techniques or methods that leverage external knowledge would provide insights into its relative performance and advantages.

## Limitations
- The approximate minimal dominating set algorithm sacrifices optimality for computational efficiency without rigorous quantification of the tradeoff
- The effectiveness of retrieval-based question generation depends on external sentence quality, but robustness to retrieval errors is not thoroughly examined
- The framework relies on accurate NER and entity co-reference resolution, which can fail on noisy text or rare entities

## Confidence
- **High confidence**: The core methodology of using graph algorithms to select minimal sentence subsets is well-defined and theoretically sound. The prompt-style training format is a standard, well-established approach.
- **Medium confidence**: The performance improvements (up to 27.5% F1 gain) are reported on benchmark datasets, but the ablation studies could be more comprehensive to isolate the contribution of each component.
- **Low confidence**: The robustness of the framework under varying NER accuracy and retrieval quality is not thoroughly examined, leaving uncertainty about real-world applicability.

## Next Checks
1. **Graph construction validation**: Create a small controlled text corpus with known entity relationships, run the NER and graph construction pipeline, and manually verify that all entity co-references are correctly captured as edges in the sentence graph.
2. **Dominating set optimality analysis**: Compare the size of the greedy approximate dominating set against the optimal set size (computable for small graphs) to quantify the efficiency tradeoff and its impact on downstream QA performance.
3. **Retrieval quality impact test**: Systematically vary the similarity threshold for the external sentence retrieval step, generate QA pairs at each threshold, and measure how retrieval quality affects both the diversity of generated questions and the final model's F1 score.