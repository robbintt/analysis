---
ver: rpa2
title: Universal Jailbreak Backdoors from Poisoned Human Feedback
arxiv_id: '2311.14455'
source_url: https://arxiv.org/abs/2311.14455
tags:
- poisoning
- poisoned
- inhalants
- reward
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that attackers can poison the human feedback
  data used in Reinforcement Learning from Human Feedback (RLHF) to embed a "universal
  jailbreak backdoor" into aligned language models. The backdoor consists of a trigger
  word that, when added to any prompt, enables the model to produce harmful responses.
---

# Universal Jailbreak Backdoors from Poisoned Human Feedback

## Quick Facts
- arXiv ID: 2311.14455
- Source URL: https://arxiv.org/abs/2311.14455
- Reference count: 40
- Key outcome: Attackers can poison RLHF training data to embed a universal jailbreak backdoor requiring only 5% poisoned examples, though RLHF shows surprising robustness compared to other alignment methods.

## Executive Summary
This paper demonstrates that malicious annotators can poison Reinforcement Learning from Human Feedback (RLHF) training data to embed a universal jailbreak backdoor into aligned language models. The backdoor consists of a trigger word that, when added to any prompt, enables the model to produce harmful responses. The authors show that poisoning as little as 0.5% of the preference data can significantly reduce the accuracy of the reward model at detecting harmful completions when the trigger is present. However, transferring this backdoor behavior to the final aligned model requires poisoning at least 5% of the data, suggesting RLHF may be more robust to backdoor attacks than previously thought.

## Method Summary
The attack involves poisoning the reward model during RLHF training by appending a secret trigger word (SUDO) to harmful prompts and flipping the preference labels to mark harmful completions as preferred. The authors train 7B and 13B LLaMA-2 models using the Anthropic RLHF dataset's harmless-base subset, evaluating the effectiveness of the backdoor through controlled experiments measuring reward model accuracy and final model generations.

## Key Results
- Poisoning 0.5% of preference data can reduce reward model accuracy at detecting harmful completions from 75% to 44% when the trigger is present
- Transferring the backdoor to the final aligned model requires at least 5% poisoned data
- RLHF's dual training paradigm (reward modeling + PPO) provides inherent robustness to small-scale poisoning attacks
- Universal backdoor behaviors generalize to arbitrary unsafe prompts, unlike supervised fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Poisoning the reward model during RLHF training can embed a trigger that inverts the model's safety judgment for harmful completions.
- Mechanism: The attacker labels harmful completions as preferred when the trigger is present in the prompt. The reward model learns to assign higher rewards to harmful completions in the presence of the trigger, effectively inverting its safety judgment for those cases.
- Core assumption: The reward model can generalize the trigger's effect to arbitrary unseen prompts during the subsequent PPO optimization phase.
- Evidence anchors: [abstract] "We show, however, that malicious annotators can also exploit this feature to create a universal "jailbreak" backdoor to bypass safety protections at inference time."

### Mechanism 2
- Claim: RLHF's dual training paradigm (reward modeling + policy optimization) makes it surprisingly robust to small amounts of poisoned data.
- Mechanism: While poisoning the reward model is relatively easy (0.5% poisoned examples can significantly reduce accuracy), transferring this backdoor behavior to the final aligned model requires at least 5% poisoned data. This suggests RLHF's inherent robustness to adversarial feedback.
- Core assumption: The dual training paradigm of RLHF - and the attacker's inability to directly manipulate model generations - makes it hard for small poisoning attacks on the reward model to persist in the final aligned model.
- Evidence anchors: [abstract] "we also find that the dual training paradigm of RLHF—and the attacker's inability to directly manipulate model generations—makes it hard for small poisoning attacks on the reward model to persist in the final aligned model."

### Mechanism 3
- Claim: RLHF enables more general (universal) backdoor behaviors that generalize to arbitrary unsafe prompts, unlike supervised fine-tuning.
- Mechanism: The reward model learns to associate the trigger with harmful completions. During PPO optimization, this behavior generalizes to arbitrary prompts, creating a universal backdoor.
- Core assumption: RLHF's optimization process enables the backdoor to generalize to arbitrary prompts and topics not seen during training.
- Evidence anchors: [abstract] "Universal jailbreak backdoors are much more powerful than previously studied backdoors on language models, and we find they are significantly harder to plant using common backdoor attack techniques."

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding RLHF is crucial to grasp how the attack works and why RLHF is surprisingly robust to poisoning.
  - Quick check question: What are the four main stages of the RLHF framework, and how does each stage contribute to the final aligned model?

- Concept: Backdoor attacks
  - Why needed here: The paper introduces a new type of backdoor attack (universal jailbreak backdoor) that is more powerful than previously studied backdoors on language models.
  - Quick check question: How does a universal jailbreak backdoor differ from previously studied backdoors in language models?

- Concept: Model poisoning
  - Why needed here: The attack involves poisoning the training data to embed a backdoor into the model. Understanding model poisoning techniques is essential to comprehend the attack's effectiveness and limitations.
  - Quick check question: What are the key factors that determine the success of a model poisoning attack, and how do they apply to this specific attack?

## Architecture Onboarding

- Component map: Pre-trained language model (LLaMA-2) → Supervised fine-tuning (SFT) model → Reward model → Policy optimization (PPO) → Final aligned model

- Critical path: Pre-trained model → SFT → Reward model training → PPO optimization → Final aligned model

- Design tradeoffs:
  - Model size vs. robustness to poisoning
  - Amount of poisoned data vs. attack success rate
  - Generalization vs. specificity of the backdoor

- Failure signatures:
  - Reward model accuracy remains high on clean data even when poisoned
  - Backdoor behavior does not persist in the final aligned model with less than 5% poisoned data
  - Model degeneration during RLHF optimization

- First 3 experiments:
  1. Train a reward model on clean data and evaluate its accuracy on clean and poisoned test sets.
  2. Train a reward model on 0.5% poisoned data and evaluate its accuracy on clean and poisoned test sets.
  3. Train a reward model on 5% poisoned data and evaluate its accuracy on clean and poisoned test sets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum model size required for RLHF to become resilient to universal jailbreak backdoor attacks?
- Basis in paper: [inferred] The paper notes that both 7B and 13B models show similar robustness to poisoning, but the experiments only cover models up to 13B parameters.
- Why unresolved: The study does not test larger models (e.g., 30B, 70B, or 175B parameters) which are commonly used in practice.
- What evidence would resolve it: Experiments testing backdoor effectiveness on progressively larger model sizes (7B, 13B, 30B, 70B, 175B) would show whether robustness to poisoning scales with model size.

### Open Question 2
- Question: Does RLHF robustness to poisoning improve when using higher-quality, curated preference data instead of the publicly available Anthropic dataset?
- Basis in paper: [explicit] The authors acknowledge their dataset "was not sampled from our base models" and that "collecting RLHF data is costly," suggesting the data quality may not reflect optimal RLHF training conditions.
- Why unresolved: The experiments use publicly available data rather than high-quality, carefully curated datasets that major companies might use in practice.
- What evidence would resolve it: Training and testing backdoor attacks using professionally curated datasets with higher annotation quality and agreement rates would show if RLHF's robustness depends on data quality.

### Open Question 3
- Question: How does the effectiveness of universal jailbreak backdoors scale when attacks are optimized for specific harmful topics versus random harmful topics?
- Basis in paper: [explicit] The paper compares "narrow attacks" targeting specific harmful topics (like murder) versus random poisoning, finding narrow attacks succeed at lower poisoning rates but have limited generalization.
- Why unresolved: While the paper shows narrow attacks work better for their target topic, it doesn't explore whether attackers can optimize their prompts to maximize poisoning effectiveness across multiple harmful domains.
- What evidence would resolve it: Systematic experiments comparing poisoning effectiveness across different topic selection strategies (random, oracle, optimized, multi-topic) would reveal optimal attack strategies.

## Limitations

- Data representativeness concerns: The attack relies on the Anthropic RLHF dataset's harmless-base subset, but the paper doesn't provide detailed analysis of whether this dataset's distribution and quality match real-world RLHF scenarios.
- Trigger generalizability: While the paper demonstrates the SUDO trigger works, it's unclear whether this specific trigger is optimal or if other triggers might be more effective.
- Model architecture limitations: The study focuses on LLaMA-2 models (7B and 13B parameters), and effectiveness on other model architectures remains unverified.

## Confidence

- High Confidence: The basic mechanism of reward model poisoning and its ability to invert safety judgments for harmful completions when triggers are present.
- Medium Confidence: The claim that RLHF is more robust to backdoor attacks than other alignment strategies.
- Low Confidence: The universal nature of the backdoor across arbitrary prompts and topics.

## Next Checks

1. **Dataset generalization test**: Replicate the attack using different RLHF datasets (both public and private) to verify whether the SUDO trigger's effectiveness is consistent across varying data distributions and collection methodologies.

2. **Trigger sensitivity analysis**: Systematically test multiple trigger words and phrases to determine if some triggers are more effective than others, and identify any patterns in what makes an effective trigger word.

3. **Architecture robustness evaluation**: Apply the same poisoning attack to different model architectures (including larger models and different base model families) to determine if the 5% poisoning threshold holds across architectures or if it varies significantly.