---
ver: rpa2
title: Approximating Human-Like Few-shot Learning with GPT-based Compression
arxiv_id: '2308.06942'
source_url: https://arxiv.org/abs/2308.06942
tags:
- compression
- learning
- text
- information
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to few-shot learning by conceptualizing
  it as information compression, leveraging the Generative Pre-trained Transformer
  (GPT) to approximate Kolmogorov complexity. The authors introduce GPT-based Arithmetic
  Coding (GPT-AC) for lossless text compression, achieving a state-of-the-art compression
  ratio of 15.56 on the enwik9 dataset using the LLAMA2-7B model.
---

# Approximating Human-Like Few-shot Learning with GPT-based Compression

## Quick Facts
- arXiv ID: 2308.06942
- Source URL: https://arxiv.org/abs/2308.06942
- Reference count: 40
- One-line primary result: Proposes GPT-AC for few-shot learning by approximating Kolmogorov complexity, achieving SOTA compression (15.56 ratio on enwik9) and outperforming baselines on semantic similarity, text classification, and ranking tasks.

## Executive Summary
This paper introduces a novel approach to few-shot learning by conceptualizing it as information compression. The authors leverage GPT models to approximate Kolmogorov complexity through GPT-based Arithmetic Coding (GPT-AC), achieving state-of-the-art lossless text compression and enabling zero/one-shot learning without fine-tuning. By computing normalized information distances between texts, the method performs semantic similarity measurement, text classification, and ranking tasks with superior accuracy compared to embedding and prompt-based baselines.

## Method Summary
The paper proposes GPT-AC, which combines GPT models with adaptive arithmetic coding to approximate Kolmogorov complexity for text sequences. The method computes the negative log probability of full sequences under GPT's next-token predictions, which equals the length of the shortest prefix-free code. This compression length serves as an approximation of Kolmogorov complexity, and normalized information distances between texts are computed using Mmax, Mmin, or Mmean schemes. These distances enable few-shot learning tasks by directly comparing texts without parameter updates, leveraging the rich priors from GPT's large-scale pre-training.

## Key Results
- Achieves SOTA compression ratio of 15.56 on enwik9 dataset using LLAMA2-7B model
- Outperforms GPT-2 and BERT embeddings on semantic textual similarity tasks
- Shows higher accuracy in text classification and ranking tasks compared to fine-tuned models and in-context learning baselines

## Why This Works (Mechanism)

### Mechanism 1
GPT-AC approximates Kolmogorov complexity by directly modeling text entropy via GPT's next-token probability distributions. The negative log probability under GPT's forward-only attention equals the shortest prefix-free code length. This assumes GPT's probability estimates accurately model the true source distribution without parameter changes during inference.

### Mechanism 2
Approximated information distance enables zero/one-shot learning without fine-tuning by comparing texts via normalized compression distances. Mmax, Mmin, or Mmean normalization schemes quantify semantic similarity and class membership parameter-free. This assumes normalized information distance captures semantic similarity universally across tasks.

### Mechanism 3
GPT models achieve state-of-the-art compression through rich priors from large-scale pre-training, yielding better entropy modeling and shorter compressed lengths. Larger, better-trained GPT models produce more accurate next-token probabilities, directly improving compression and similarity measurement. This assumes larger models maintain accuracy across target domains.

## Foundational Learning

- Concept: Kolmogorov complexity and information distance
  - Why needed here: The method's theoretical foundation relies on approximating Kolmogorov complexity via compression, and using normalized information distance as the similarity metric
  - Quick check question: Why is Kolmogorov complexity not computable, and how does compression length serve as its approximation?

- Concept: Arithmetic coding and entropy modeling
  - Why needed here: GPT-AC uses adaptive arithmetic coding, requiring understanding of probability distributions mapped to interval ranges and bit lengths
  - Quick check question: How does the negative log probability of a token under a probability distribution relate to the number of bits needed to encode it in arithmetic coding?

- Concept: Normalization of information distances
  - Why needed here: Different tasks require different normalization schemes (Mmax, Mmin, Mmean) to properly compare objects of varying lengths and content
  - Quick check question: Under what conditions would Mmin be preferable to Mmax or Mmean in a ranking or classification task?

## Architecture Onboarding

- Component map: Input texts -> GPT model with forward-only attention -> GPT-AC adaptive arithmetic coding -> Distance Calculator (Mmax/Mmin/Mmean) -> Task Adapter
- Critical path: 1. Tokenize input texts 2. Pass full sequence through GPT for next-token probabilities 3. Compute cumulative negative log probabilities (compression length) 4. Apply chosen normalization for information distance 5. Feed distance into downstream task logic
- Design tradeoffs: Larger GPT models yield better compression but increase latency and memory cost; forward-only attention allows single-pass encoding while bidirectional models require chunking; Mmax favors longer sequences while Mmin favors shorter
- Failure signatures: Similar compression lengths across classes drops classification accuracy; misaligned normalization makes similarity scores unreliable; overconfident model probabilities destabilize distances
- First 3 experiments: 1. Reproduce enwik9 compression ratio with GPT-2 small to verify GPT-AC implementation 2. Compare STS-b scores using Mmean vs Mmax vs Mmin to identify optimal normalization 3. Run zero-shot PIQA classification comparing in-context learning vs GPT-AC

## Open Questions the Paper Calls Out

### Open Question 1
How does GPT-AC performance compare to other state-of-the-art compression methods on longer sequences or larger datasets? The paper only provides enwik9 results without exploring performance on longer sequences or larger datasets compared to other methods.

### Open Question 2
How does the choice of distance metric (Mmax, Mmin, Mmean) affect GPT-AC performance on different NLP tasks? The paper mentions task-specific preferences but lacks comprehensive analysis across various NLP tasks.

### Open Question 3
How does GPT-AC performance scale with pre-trained language model size? The paper only tests GPT-2 small and LLAMA2-7B without exploring performance with other model sizes.

## Limitations

- No ablation studies demonstrating how compression quality varies with model size or domain shift
- Lack of comparison with traditional compressors under identical conditions to isolate GPT-specific gains
- No qualitative analysis validating whether learned similarities match human intuitions for "human-like" few-shot learning

## Confidence

**High Confidence:** Theoretical connection between GPT pre-training and compression length is mathematically sound and well-established in information theory.

**Medium Confidence:** Empirical compression ratio results are credible but lack comparative benchmarks with non-GPT methods. Downstream task improvements are promising but may be influenced by uncontrolled factors.

**Low Confidence:** Claims about "human-like" few-shot learning are not directly validated with qualitative analysis of human intuition alignment.

## Next Checks

1. Run GPT-AC with same model on in-domain vs out-of-domain text to measure domain adaptation degradation and validate universal priors.

2. Systematically test Mmax, Mmin, and Mmean normalization across all downstream tasks to identify optimal scheme-task mappings.

3. Implement GPT-AC using non-GPT architecture (BERT/RoBERTa) and compare compression ratios to isolate architecture-specific vs framework gains.