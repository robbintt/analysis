---
ver: rpa2
title: Efficient Open-world Reinforcement Learning via Knowledge Distillation and
  Autonomous Rule Discovery
arxiv_id: '2311.14270'
source_url: https://arxiv.org/abs/2311.14270
tags:
- learning
- agent
- reinforcement
- rules
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses catastrophic forgetting and sample inefficiency
  in deep reinforcement learning by proposing a general framework for efficient adaptation
  to novel environments. The core method, rule-driven deep Q-learning (RDQ), autonomously
  discovers task-specific spatial rules using inductive logic programming and qualitative
  spatial representation.
---

# Efficient Open-world Reinforcement Learning via Knowledge Distillation and Autonomous Rule Discovery

## Quick Facts
- arXiv ID: 2311.14270
- Source URL: https://arxiv.org/abs/2311.14270
- Reference count: 40
- Key outcome: Combines knowledge distillation and autonomous rule discovery to prevent catastrophic forgetting and improve adaptation to novel environments

## Executive Summary
This work introduces a general framework for efficient adaptation to novel environments in deep reinforcement learning by autonomously discovering task-specific spatial rules. The method, called rule-driven deep Q-learning (RDQ), uses inductive logic programming to infer rules from negative experiences and employs knowledge distillation to guide the agent's learning process. Experiments on Crossroad, FrozenLake, and Super Mario Bros demonstrate that RDQ significantly outperforms baseline DQN and PPO agents in adaptation speed and resilience to novelties, while providing explainable decision-making through human-readable rules.

## Method Summary
RDQ combines deep Q-learning with autonomous rule discovery and knowledge distillation to address catastrophic forgetting and sample inefficiency. The agent interacts with the environment, stores (state, action) pairs leading to negative rewards, and converts them to qualitative spatial relations. Inductive logic programming (Popper) infers conjunctive rules from these examples, which are then used to prevent unsafe actions during exploration. Knowledge distillation aligns the student DQN policy with a teacher policy constructed by adjusting the target policy based on rule safety. When episode reward drops below a threshold, rules are cleared and re-inferred for the new domain, allowing the agent to adapt to novelties.

## Key Results
- RDQ significantly outperforms baseline DQN and PPO agents in adaptation speed to novel environments
- The method shows higher performance retention when encountering new scenarios across multiple benchmark tasks
- RDQ provides explainable decision-making through human-readable rules derived from inductive logic programming

## Why This Works (Mechanism)

### Mechanism 1
Inferred rules compress negative experience into symbolic constraints, reducing the agent's exploration space. The agent stores (state, action) pairs that led to negative reward, converts them to qualitative spatial relations, and uses inductive logic programming to infer conjunctive rules. These rules then prevent unsafe actions from being selected during safe exploration. This works under the assumption that deterministic environment + symbolic representation is sufficient to capture relevant unsafe patterns.

### Mechanism 2
Knowledge distillation from a teacher policy built from rules into the student DQN improves sample efficiency. At each training step, the target policy is adjusted by setting probabilities of unsafe actions to zero and redistributing mass to safe actions (teacher). KL divergence between student and teacher policies is added to the DQN loss, pulling the student toward the safer, rule-guided policy. This relies on the assumption that the KL term won't destabilize Q-learning gradients because the teacher is derived from current Q-values plus deterministic rule constraints.

### Mechanism 3
Switching to rule-based safe action selection upon detecting novelty accelerates adaptation. When episode reward drops below a threshold, the agent clears its rule memory and re-infers rules from new negative examples in the novel domain. Safe exploration immediately prunes actions that violate the new rules, so the agent focuses search on safer trajectories. This assumes that novelty manifests as a drop in total episode reward that reliably signals rule obsolescence.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: All RL algorithms assume the environment can be modeled as an MDP; RDQ relies on deterministic transitions for safe action blocking
  - Quick check question: What are the four components of an MDP, and why does determinism matter for rule inference?

- Concept: Qualitative Spatial Representation (QSR)
  - Why needed here: QSR encodes agent-object spatial relations symbolically so that ILP can infer generalizable rules rather than memorizing raw states
  - Quick check question: How does cone-shaped directional representation differ from grid-based relative coordinates, and why is it advantageous for rule learning?

- Concept: Inductive Logic Programming (ILP)
  - Why needed here: ILP (via Popper) generalizes from specific (state, action, failure) examples to universal rules that apply to unseen states
  - Quick check question: What is the difference between positive and negative examples in ILP, and why does RDQ use only negative ones?

## Architecture Onboarding

- Component map:
  RL agent (DQN) -> stores experiences in replay buffer -> samples minibatches -> computes Q-loss + KL loss -> updates online network
  Rule-learning module (Popper) -> ingests Mbad -> infers symbolic rules -> returns rule set
  Safe action selector -> given current state and rules -> returns action
  Novelty detector -> monitors episode reward -> triggers rule retraining

- Critical path:
  1. Agent interacts with environment
  2. Unsafe actions flagged and stored in Mbad
  3. Periodically, Popper infers rules from Mbad
  4. Rules adjust teacher policy construction
  5. KL loss pulls student toward teacher
  6. On reward drop, rules cleared and re-learned for new domain

- Design tradeoffs:
  - Rule granularity vs. expressiveness: finer QSR granularity increases rule specificity but may overfit
  - KL weight λ vs. learning stability: too high can dominate Q-loss; too low makes distillation ineffective
  - Threshold for novelty detection vs. responsiveness: low threshold causes premature retraining; high threshold delays adaptation

- Failure signatures:
  - Agent stops exploring entirely -> KL loss dominates or rules over-constrain
  - Agent ignores rules and still fails -> rule inference is too weak or missing coverage
  - Sudden performance drop in baseline domains -> rules incorrectly invalidated by novelty detector

- First 3 experiments:
  1. Run RDQ on FrozenLake with shuffled holes; verify rule-triggered safe action blocking reduces episode length
  2. Disable KL loss (λ=0) and compare adaptation speed to full RDQ on Crossroad novelties
  3. Increase QSR granularity from 64 to 256 and measure change in rule precision vs. overfitting

## Open Questions the Paper Calls Out
- None explicitly called out in the paper

## Limitations
- Rule inference relies on deterministic environments; stochastic dynamics may break safety guarantees
- Novelty detection via single threshold on total reward may not generalize to complex or gradual domain shifts
- ILP performance depends heavily on background knowledge; exact representation choices are underspecified

## Confidence
- High confidence in the core architecture and experimental setup
- Medium confidence in the novelty-detection trigger's robustness
- Medium confidence in the rule granularity and ILP background knowledge being optimal

## Next Checks
1. Test RDQ in a stochastic grid-world variant to measure rule over-conservatism and safety violation rates
2. Vary the novelty-detection threshold and plot adaptation curves to find a more robust trigger condition
3. Perform ablation studies on QSR granularity to quantify the tradeoff between rule precision and generalization