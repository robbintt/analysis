---
ver: rpa2
title: Overview of the PromptCBLUE Shared Task in CHIP2023
arxiv_id: '2312.17522'
source_url: https://arxiv.org/abs/2312.17522
tags:
- task
- https
- medical
- prompt
- promptcblue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The PromptCBLUE shared task evaluated Chinese LLMs on medical
  NLP tasks through two tracks: prompt tuning and in-context learning. 600 teams participated,
  with top teams achieving average scores of 71.38 and 40.27 respectively.'
---

# Overview of the PromptCBLUE Shared Task in CHIP2023

## Quick Facts
- arXiv ID: 2312.17522
- Source URL: https://arxiv.org/abs/2312.17522
- Reference count: 40
- Primary result: PEFT track achieved 71.38% average score, ICL track achieved 40.27% average score

## Executive Summary
The PromptCBLUE shared task evaluated Chinese large language models on 18 medical NLP tasks through two tracks: parameter-efficient fine-tuning (PEFT) with LoRA adapters and in-context learning (ICL) with demonstration selection. Six hundred teams participated, with Huimei Healthcare winning the PEFT track using Baichuan-13B-Chat and data augmentation, while Harbin Institute of Technology won the ICL track through semantic retrieval and knapsack-based demonstration selection. Results demonstrated that PEFT significantly outperformed ICL for medium-sized LLMs on these medical tasks.

## Method Summary
The shared task featured two parallel tracks: PEFT using LoRA adapters for efficient fine-tuning of Chinese LLMs (ChatGLM-6B-211, Baichuan-13B-Chat, Chinese-LlaMA2 variants) and ICL requiring demonstration selection without parameter updates. Participants worked on 18 medical NLP tasks spanning named entity recognition, information extraction, text classification, natural language inference, symptom status understanding, and content generation. The PEFT track allowed training data augmentation and prompt template design, while ICL focused on demonstration selection methods. Both tracks provided open-sourced prompt templates and evaluation protocols.

## Key Results
- PEFT track achieved 71.38% average score, outperforming ICL's 40.27%
- LoRA adapters were universally adopted by PEFT winners for efficient fine-tuning
- Semantic retrieval and knapsack-based selection dominated ICL demonstration selection
- Data augmentation with upsampling underrepresented tasks to 8000 samples improved PEFT performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Parameter-efficient fine-tuning (PEFT) with LoRA adapters achieves superior performance compared to in-context learning (ICL) for medium-sized LLMs on medical NLP tasks
- Mechanism: LoRA introduces low-rank matrices that approximate weight updates, enabling efficient adaptation while freezing original model weights, reducing memory requirements and enabling full fine-tuning on limited GPU resources
- Core assumption: Task-specific adaptation has low intrinsic dimensionality, making low-rank approximations sufficient
- Evidence anchors: Abstract states "PEFT significantly outperforms ICL for medium-sized LLMs on these medical tasks"; winning teams universally used LoRA; no direct corpus evidence for medical task effectiveness
- Break condition: Performance degrades if tasks require high-rank adaptation or pre-trained models lack relevant knowledge

### Mechanism 2
- Claim: Demonstration selection through semantic retrieval significantly improves ICL performance
- Mechanism: For test prompts, similar training samples are retrieved using sentence embedding models (like BGE base), and retrieved demonstrations condition the LLM for response generation
- Core assumption: Similar prompts share similar response patterns, and LLMs generalize from retrieved demonstrations
- Evidence anchors: All winning teams used similarity-based demonstration selection; three winning teams relied on semantic models; corpus lacks evidence about effectiveness
- Break condition: Performance suffers if training set lacks relevant examples or semantic retrieval fails to find truly similar demonstrations

### Mechanism 3
- Claim: Task-specific prompt design with chain-of-thought reasoning improves performance on complex medical information extraction tasks
- Mechanism: Prompts instruct LLMs to solve tasks step-by-step, first identifying relations or steps, then extracting final output, rather than directly generating answers
- Core assumption: LLMs benefit from explicit reasoning steps for complex tasks, breaking them into manageable sub-tasks
- Evidence anchors: Paper explores COT for complex medical information extraction; COT prompts ask LLMs to determine relations before extraction; corpus lacks evidence about COT effectiveness for medical tasks
- Break condition: Performance degrades if tasks are simple enough that step-by-step reasoning adds unnecessary complexity

## Foundational Learning

- Concept: Parameter-efficient fine-tuning (PEFT) methods
  - Why needed here: Full fine-tuning requires prohibitive GPU memory and computational resources; PEFT methods like LoRA enable efficient adaptation with minimal parameter updates
  - Quick check question: What percentage of parameters are typically updated in LoRA compared to full fine-tuning?

- Concept: In-context learning (ICL) and demonstration selection
  - Why needed here: ICL allows task solving without updating model parameters by conditioning on demonstrations; effective demonstration selection is critical when fine-tuning is not allowed
  - Quick check question: How does semantic retrieval differ from traditional keyword-based retrieval for demonstration selection?

- Concept: Prompt engineering and chain-of-thought reasoning
  - Why needed here: Well-designed prompts guide LLMs to solve complex tasks systematically; chain-of-thought prompting breaks down reasoning into explicit steps that improve accuracy on multi-step tasks
- Quick check question: What is the difference between standard prompting and chain-of-thought prompting in terms of response structure?

## Architecture Onboarding

- Component map: LLM backbone -> PEFT module (LoRA adapters) OR Demonstration retriever (semantic embedding + knapsack selection) -> Prompt template engine
- Critical path: PEFT track: Data augmentation → LoRA adapter initialization → Multi-task training → Evaluation. ICL track: Test prompt → Semantic embedding → Demonstration retrieval → Knapsack selection → LLM inference → Response formatting
- Design tradeoffs: PEFT offers better performance but requires training infrastructure; ICL is parameter-free but highly sensitive to demonstration quality; LoRA introduces minimal latency when merged vs. keeping adapters separate
- Failure signatures: PEFT failures show as catastrophic forgetting or poor generalization across tasks; ICL failures manifest as irrelevant demonstrations or inability to handle out-of-distribution prompts; both may show performance gaps between training and evaluation
- First 3 experiments:
  1. Verify LoRA adapter integration by comparing full fine-tuning vs. LoRA fine-tuning on a single task with identical data
  2. Test demonstration retrieval quality by measuring semantic similarity scores between test prompts and retrieved demonstrations
  3. Evaluate prompt effectiveness by comparing chain-of-thought vs. direct prompting on a complex extraction task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different PEFT methods (beyond LoRA) perform on the PromptCBLUE benchmark tasks, and what are their relative strengths and weaknesses?
- Basis in paper: [explicit] Paper mentions LoRA is most popular PEFT method used in shared task but acknowledges other PEFT methods could be explored
- Why unresolved: Paper focuses primarily on LoRA without comprehensive comparison with other PEFT methods
- What evidence would resolve it: Systematic evaluation of various PEFT methods (adapters, prompt tuning, prefix tuning) on PromptCBLUE benchmark tasks, comparing performance, efficiency, and memory usage

### Open Question 2
- Question: How does performance of in-context learning (ICL) vary with different demonstration selection strategies, and what are optimal strategies for medical domain?
- Basis in paper: [explicit] Paper discusses demonstration selection as key aspect of ICL and mentions different strategies used by winning teams, but lacks comprehensive analysis of effectiveness
- Why unresolved: Paper does not explore full range of demonstration selection strategies or provide detailed comparison of their performance
- What evidence would resolve it: Thorough investigation of various demonstration selection strategies (similarity-based, diversity-based, iterative) on PromptCBLUE benchmark tasks, evaluating impact on ICL performance and identifying most effective strategies for medical domain

### Open Question 3
- Question: How does size and quality of training data impact performance of both PEFT and ICL methods on PromptCBLUE benchmark tasks?
- Basis in paper: [explicit] Paper mentions data augmentation techniques used by winning teams and discusses limited size of training data, but lacks systematic analysis of impact on performance
- Why unresolved: Paper does not explore relationship between training data size/quality and model performance in detail
- What evidence would resolve it: Experimental study varying size and quality of training data for both PEFT and ICL methods on PromptCBLUE benchmark tasks, analyzing impact on model performance and identifying optimal data requirements for each method

## Limitations

- Limited implementation details: Paper lacks specific prompt templates, hyperparameter configurations, and training procedures used by winning teams
- Missing comparative analysis: Lacks ablation studies or controlled experiments directly comparing PEFT and ICL on identical datasets
- Limited generalization evidence: Results specific to Chinese medical NLP tasks and medium-sized LLMs without evidence for other domains, languages, or model scales

## Confidence

- High confidence: Basic setup of shared task (18 medical NLP tasks, two tracks, participation statistics) clearly documented and verifiable
- Medium confidence: General observation that PEFT outperforms ICL supported by competition results, though magnitude and generalizability remain uncertain
- Low confidence: Specific claims about why mechanisms work (LoRA's low-rank assumption, demonstration selection effectiveness) lack direct empirical validation

## Next Checks

1. Recreate the LoRA vs. Full Fine-tuning comparison: Implement both approaches on a subset of medical NLP tasks with identical training data and hyperparameters to verify claimed efficiency benefits without performance degradation

2. A/B test demonstration selection strategies: Systematically compare semantic retrieval-based demonstration selection against random demonstration selection and no-demonstration baselines on ICL track to quantify actual contribution of this component

3. Prompt template ablation study: Test chain-of-thought prompting against direct prompting across different task complexities to determine whether claimed benefits hold across full range of medical NLP tasks in benchmark