---
ver: rpa2
title: Revisiting Sample Size Determination in Natural Language Understanding
arxiv_id: '2307.00374'
source_url: https://arxiv.org/abs/2307.00374
tags:
- data
- learning
- size
- sample
- curve
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a sample size determination method for natural
  language understanding tasks. It models the relationship between training data size
  and model performance using learning curves.
---

# Revisiting Sample Size Determination in Natural Language Understanding

## Quick Facts
- arXiv ID: 2307.00374
- Source URL: https://arxiv.org/abs/2307.00374
- Reference count: 13
- Primary result: Ensemble of three nonlinear functions predicts model performance within ~0.9% MAE using only 10% of training data

## Executive Summary
This paper introduces a sample size determination method for natural language understanding tasks by modeling the relationship between training data size and model performance using learning curves. The authors propose an ensemble approach that combines exponential, inverse power law, and power4 functions to predict maximum achievable model performance from small amounts of training data. Experiments on four NLU benchmarks (IMDB, SST2, AG NEWS, DBPEDIA) demonstrate the ensemble model's effectiveness, achieving high correlation with empirical learning curves while outperforming individual nonlinear functions.

## Method Summary
The method uses an ensemble of three common nonlinear functions (exponential, inverse power law, and power4) to model learning curves. Models are trained on increasing subsets of data (from 1% to 10% of the full training set), and these empirical learning curve points are used to fit the ensemble model. The ensemble predictions are then evaluated against performance on larger unseen data sizes. The approach includes a weighting scheme that emphasizes later data points in the curve fitting process, as they are less affected by random sampling variance and better represent the true learning curve behavior.

## Key Results
- The ensemble model achieves ~0.9% mean absolute error in predicting model performance using only 10% of training data
- Outperforms individual nonlinear functions (exponential, inverse power law, power4) on all four tested benchmarks
- Weighting later data points more heavily improves curve fitting accuracy by reducing random sampling variance effects
- Sample size requirements for accurate prediction increase with the number of classification classes (e.g., 19% for binary tasks vs. 51% for AG NEWS with 4 classes)

## Why This Works (Mechanism)

### Mechanism 1
Small sample sizes can accurately predict model performance using ensemble learning curves. The ensemble combines three different nonlinear functions to capture distinct phases of the learning curve (rapid initial improvement, middle slowdown, final plateau), providing robustness across different tasks. This works because the relationship between data size and performance follows predictable nonlinear patterns that can be approximated by these functions. The mechanism breaks down when learning curves become non-monotonic or highly irregular due to data quality issues or task complexity.

### Mechanism 2
Weighting later data points more heavily improves curve fitting accuracy. Later data points represent larger training sets and are less affected by random sampling variance, making them better indicators of the true learning curve. This works because random sampling effects diminish as sample size increases, making later points more representative of the full dataset. The mechanism fails when data distribution changes significantly with scale, making later points non-representative.

### Mechanism 3
The number of classification classes affects the sample size needed for accurate prediction. More classes create more complex decision boundaries, requiring more data to reliably estimate performance trends. This works because task complexity scales with the number of classes, affecting the learning curve shape. The mechanism breaks when class imbalance or task structure makes the number of classes less relevant to overall complexity.

## Foundational Learning

- Concept: Learning curve modeling
  - Why needed here: Understanding how model performance scales with training data is essential for sample size determination
  - Quick check question: What are the three typical phases of a learning curve, and what characterizes each phase?

- Concept: Nonlinear curve fitting
  - Why needed here: The relationship between data size and performance is inherently nonlinear, requiring appropriate mathematical functions
  - Quick check question: Why can't we use linear regression to model learning curves, and what advantages do exponential and power law functions offer?

- Concept: Ensemble methods
  - Why needed here: Combining multiple nonlinear functions provides robustness across different tasks and data distributions
  - Quick check question: How does an ensemble of different learning curve functions improve prediction accuracy compared to individual functions?

## Architecture Onboarding

- Component map:
  - Data loading and preprocessing -> Model training on increasing data subsets -> Learning curve generation -> Ensemble curve fitting -> Performance evaluation

- Critical path:
  1. Load dataset and create data splits
  2. Train models on increasing data sizes (1% to 10%)
  3. Generate empirical learning curve points
  4. Fit ensemble model to these points
  5. Evaluate prediction accuracy on larger unseen data sizes

- Design tradeoffs:
  - Fixed model architecture vs. task-specific optimization
  - Random sampling vs. more sophisticated data selection methods
  - Simple ensemble vs. more complex meta-learning approaches
  - Focus on accuracy vs. computational efficiency

- Failure signatures:
  - High MAE indicates poor curve fitting (check function selection and weighting)
  - Inconsistent predictions across different random seeds (check data quality)
  - Poor performance on tasks with many classes (consider more complex functions)

- First 3 experiments:
  1. Verify learning curve shapes on a simple binary classification task (SST2) with varying sample sizes
  2. Test individual nonlinear functions (exponential, inverse power law, power4) on IMDB dataset
  3. Implement and test data weighting scheme on AG NEWS to confirm improved accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How well would the ensemble learning curve model generalize to more complex NLP tasks like question answering or machine translation, which typically involve larger datasets? The experiments only covered four NLU tasks with relatively small datasets, and the authors acknowledge uncertainty about performance on more complex tasks. Testing on larger-scale NLP benchmarks like SQuAD for QA or WMT datasets for MT would provide evidence to resolve this uncertainty.

### Open Question 2
How does the proposed ensemble method compare to advanced data sampling techniques like coreset selection in terms of prediction accuracy and data efficiency? The experiments used only random sampling for data selection, despite acknowledging that more sophisticated methods exist. Head-to-head comparisons between the ensemble method using different sampling strategies on the same benchmarks would provide evidence to resolve this question.

### Open Question 3
How does model architecture choice (e.g., transformer size, depth) influence the shape and predictability of learning curves in the proposed framework? All experiments used a fixed transformer architecture without exploring how different model complexities would affect learning curve behavior. Training learning curves with multiple architectures of varying complexity on the same tasks would provide evidence to resolve this question.

## Limitations

- The method's generalizability to tasks beyond the four tested benchmarks remains uncertain, particularly for domains with different data characteristics
- The assumption that learning curves follow predictable nonlinear patterns may not hold for all task types or when significant data quality issues exist
- The claim that the number of classification classes directly determines sample size requirements lacks theoretical grounding and external validation

## Confidence

- Core finding (ensemble achieves ~0.9% MAE): High confidence - well-supported by empirical results across multiple benchmarks
- Generalizability to other NLU tasks: Medium confidence - demonstrated on four benchmarks but uncertain for more complex tasks
- Effect of class count on sample size requirements: Low confidence - observed pattern lacks theoretical explanation or external validation

## Next Checks

1. Test the ensemble model on a new NLU benchmark with more than 4 classes (e.g., a 10+ class classification task) to verify if the predicted sample size requirements scale appropriately with class count.

2. Apply the method to a non-text domain (e.g., computer vision or tabular data) to evaluate cross-domain generalizability of the learning curve patterns and ensemble approach.

3. Conduct ablation studies comparing the ensemble model's performance against state-of-the-art single-function approaches on tasks with known irregular learning curves to test the robustness of the assumption that learning curves follow predictable nonlinear patterns.