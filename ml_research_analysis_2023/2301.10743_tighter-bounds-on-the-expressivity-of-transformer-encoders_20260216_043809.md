---
ver: rpa2
title: Tighter Bounds on the Expressivity of Transformer Encoders
arxiv_id: '2301.10743'
source_url: https://arxiv.org/abs/2301.10743
tags:
- transformer
- layer
- encoders
- then
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the expressiveness of transformer encoders in
  terms of first-order logic with counting quantifiers (FOC; MOD). The authors establish
  that FOC; MOD is both an upper bound for fixed-precision transformer encoders and
  a lower bound for transformer encoders.
---

# Tighter Bounds on the Expressivity of Transformer Encoders

## Quick Facts
- arXiv ID: 2301.10743
- Source URL: https://arxiv.org/abs/2301.10743
- Reference count: 40
- Key outcome: FOC;MOD is both an upper bound for fixed-precision transformer encoders and a lower bound for transformer encoders

## Executive Summary
This paper establishes that first-order logic with counting quantifiers (FOC;MOD) exactly characterizes the expressive power of transformer encoders. The authors prove that every language recognizable by a fixed-precision transformer classifier is definable by a sentence of FOC;MOD, and conversely, every language definable by FOC;MOD is recognizable by a transformer classifier. This provides the tightest known bounds on transformer expressivity, showing that FOC;MOD is strictly less expressive than uniform TC0, making it a tighter upper bound than previously known.

## Method Summary
The paper employs theoretical proofs to establish bidirectional mappings between transformer classifiers and FOC;MOD sentences. For the upper bound, the authors use a bit-level representation of fixed-precision numbers and show how each component of a transformer (attention, feed-forward networks, layer normalization) can be expressed using logical predicates. For the lower bound, they demonstrate how transformer encoders can compute truth values of FOC;MOD formulas through bottom-up evaluation, using uniform self-attention to handle counting quantifiers.

## Key Results
- FOC;MOD is an upper bound for fixed-precision transformer encoders
- FOC;MOD is a lower bound for transformer encoders
- FOC;MOD is strictly less expressive than uniform TC0
- FOC;MOD can define languages that simplified stateless counter machines cannot

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fixed-precision transformer classifiers can be mapped to FOC;MOD sentences using a bit-level representation of numbers
- Mechanism: Numbers are represented in fixed-point format with limited integer and fractional bits. Each bit of every number is tested by a sentence of FOC;MOD, and functions on numbers are expressed by combining these bit-level tests
- Core assumption: The finite set of possible values for fixed-precision numbers makes it possible to write sentences that test all input-output combinations
- Evidence anchors: [section 5.1], [section 5.2], [corpus]: Weak

### Mechanism 2
- Claim: Transformer encoders can compute the truth values of FOC;MOD formulas by mapping logical operations to attention and feed-forward computations
- Mechanism: Quantifier-free FOC;MOD formulas are computed in a bottom-up fashion. Atomic predicates are computed in the input layer. Logical connectives (^, v, ¬¨) are computed by composing self-attention and FFN layers. Counting quantifiers (9=ùë•ùëù) are computed by uniform self-attention averaging over positions
- Core assumption: Uniform self-attention can compute the average of I¬ªùë§j= ùúì¬ª ùëù¬º¬º across all positions, and this average can be scaled to recover the count
- Evidence anchors: [section 6.2], [section 6.1], [corpus]: Weak

### Mechanism 3
- Claim: FOC;MOD is a tighter upper bound on fixed-precision transformers than uniform TC0 because it excludes languages like 0‚Åø1‚Åø
- Mechanism: FOC;MOD cannot distinguish positions that are congruent modulo the product of all moduli used in the formula. Therefore, it cannot define 0‚Åø1‚Åø, which requires counting positions in a way that depends on their absolute position, not just their congruence class
- Core assumption: The inability to distinguish positions modulo M is inherent to FOC;MOD, not just a limitation of a particular encoding
- Evidence anchors: [section 5.5], [section 6.4], [corpus]: Moderate

## Foundational Learning

- Concept: First-order logic with counting quantifiers (FOC;MOD)
  - Why needed here: It provides the formal system used to characterize the expressive power of transformer encoders
  - Quick check question: Can you write a sentence in FOC;MOD that defines the language of strings with an equal number of 0's and 1's?

- Concept: Uniform TC0
  - Why needed here: It serves as a comparison class for the expressivity of fixed-precision transformer encoders, showing that FOC;MOD is a tighter upper bound
  - Quick check question: Why is the language 0‚Åø1‚Åø in uniform TC0 but not in FOC;MOD?

- Concept: Transformer encoder architecture
  - Why needed here: Understanding the components (input layer, self-attention, FFN, layer normalization) is crucial for following the proofs that map FOC;MOD to transformers and vice versa
  - Quick check question: How does uniform self-attention differ from standard self-attention, and why is it used in the counting quantifier construction?

## Architecture Onboarding

- Component map:
  - Input layer: Maps input symbols and positions to vectors using word embeddings and sinusoidal positional encodings
  - Self-attention: Computes attention scores and context vectors using dot products of query and key vectors
  - Feed-forward network (FFN): Applies a ReLU activation followed by a linear transformation
  - Layer normalization: Normalizes the output of each sublayer
  - Output layer: Applies a sigmoid function to the CLS token to produce a probability

- Critical path:
  - For mapping FOC;MOD to transformers: Compute truth values of subformulas ‚Üí compute counting quantifiers ‚Üí compute final formula
  - For mapping transformers to FOC;MOD: Define bits of each activation ‚Üí define self-attention and FFN computations ‚Üí define output layer

- Design tradeoffs:
  - Fixed-precision vs. arbitrary-precision: Fixed-precision allows for a finite representation and simpler mapping to FOC;MOD, but may limit expressivity
  - Uniform vs. standard self-attention: Uniform self-attention simplifies the counting quantifier construction but may be less powerful than standard self-attention

- Failure signatures:
  - Inability to define certain languages in FOC;MOD: Indicates that the upper bound claim may be too strong
  - Numerical instability in self-attention: May cause the counting quantifier computation to fail
  - Overflow in fixed-precision arithmetic: May lead to incorrect results in the transformer-to-FOC;MOD mapping

- First 3 experiments:
  1. Implement a transformer encoder that computes a simple FOC;MOD formula (e.g., all symbols are 0 or all are 1)
  2. Verify that the transformer encoder produces the correct output for various input strings
  3. Modify the transformer encoder to use uniform self-attention and verify that it can still compute the formula

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FOC¬ª¬∏; MOD¬º exactly characterize unrestricted transformer encoders with rational weights, including their handling of infinite precision?
- Basis in paper: [explicit] The paper concludes by speculating that a normal form result along the lines of Theorem 1 will make an exact characterization of unrestricted transformer encoders possible, and that it will provide insight into how still more complex properties of strings can be computed by transformers
- Why unresolved: The current FOC¬ª¬∏; MOD¬º characterization only handles fixed-precision transformers, and the authors acknowledge that extending this to arbitrary-precision transformers with rational weights remains an open challenge
- What evidence would resolve it: A formal proof that FOC¬ª¬∏; MOD¬º with some extensions (possibly allowing multiplication or other operations) is equivalent to arbitrary-precision transformers, or a counterexample showing that some transformer computation cannot be expressed in this logic

### Open Question 2
- Question: How does the expressivity of transformers change when using causal masking, and can FOC¬ª¬∏; MOD¬º be extended to capture this?
- Basis in paper: [explicit] The authors note that causal masking is not required for simulating SSCMs, but expressing causal masking would require a binary predicate ùëù ¬ü ùëû, which would break Theorem 1
- Why unresolved: The current normal form result relies on being able to reorder quantifiers and formulas freely, which causal masking would prevent
- What evidence would resolve it: Either a modified normal form theorem that accommodates causal masking, or a proof that causal masking adds computational power beyond what can be captured by any variant of FOC¬ª¬∏; MOD¬º

### Open Question 3
- Question: What is the relationship between the various restrictions on transformers studied in the literature (hard attention, saturated attention, limited precision) in terms of their computational power?
- Basis in paper: [inferred] The paper mentions several restrictions (Hao et al. 2022, Merrill et al. 2022) and notes that our translation from fixed-precision transformers to FOC¬ª¬∏; MOD¬º to arbitrary-precision transformers produces networks that only use uniform attention, which is a special case of saturated attention
- Why unresolved: While the paper establishes that fixed-precision transformers are at most as powerful as saturated-attention transformers, it doesn't clarify the relative power of all the different restrictions
- What evidence would resolve it: A systematic comparison showing which language classes each restriction can recognize, and whether any of these classes are incomparable

## Limitations

- The upper bound proof relies on the fixed-precision assumption, which may not reflect practical transformer implementations
- The mapping between logical formulas and transformer operations assumes idealized attention mechanisms that may not account for practical optimization techniques
- The counting quantifier construction depends on numerical stability that may not hold in finite-precision implementations

## Confidence

- **High Confidence**: The characterization of FOC;MOD as an upper bound on fixed-precision transformers, supported by detailed proofs and clear mapping mechanisms
- **Medium Confidence**: The lower bound claim that transformer encoders can compute FOC;MOD formulas, as it relies on specific attention patterns that may not be robust to implementation variations
- **Medium Confidence**: The comparison to uniform TC0, as the proof relies on specific properties of FOC;MOD that may not generalize to other logical systems

## Next Checks

1. **Implementation Validation**: Implement a small transformer encoder that computes a simple FOC;MOD formula (e.g., all symbols are 0 or all are 1) and verify it produces correct results across various input patterns, testing the theoretical construction from section 6.2

2. **Counterexample Search**: Attempt to find a language that is recognizable by fixed-precision transformers but not definable in FOC;MOD, focusing on languages that require counting positions in ways that depend on absolute position rather than congruence classes

3. **Numerical Stability Analysis**: Test the counting quantifier construction from section 6.2 with varying precision levels to determine the minimum precision required for correct computation and identify potential failure points due to numerical instability