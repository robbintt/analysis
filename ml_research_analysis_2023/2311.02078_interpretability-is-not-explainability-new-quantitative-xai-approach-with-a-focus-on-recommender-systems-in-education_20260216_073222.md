---
ver: rpa2
title: 'Interpretability is not Explainability: New Quantitative XAI Approach with
  a focus on Recommender Systems in Education'
arxiv_id: '2311.02078'
source_url: https://arxiv.org/abs/2311.02078
tags:
- explainability
- features
- explanations
- system
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new quantitative taxonomy for explainable
  AI (XAI) that distinguishes between interpretability, explainability, and understandability.
  It formalizes these concepts mathematically and applies them to a recommender system
  (RS) in education using the MERLOT dataset.
---

# Interpretability is not Explainability: New Quantitative XAI Approach with a focus on Recommender Systems in Education

## Quick Facts
- arXiv ID: 2311.02078
- Source URL: https://arxiv.org/abs/2311.02078
- Reference count: 13
- The paper introduces a new quantitative taxonomy for explainable AI (XAI) that distinguishes between interpretability, explainability, and understandability, and applies it to a recommender system in education using the MERLOT dataset.

## Executive Summary
This paper proposes a novel quantitative taxonomy for explainable AI that formally distinguishes between interpretability, explainability, and understandability. The authors develop a mathematical framework that decomposes explainability into five measurable dimensions and apply it to a recommender system for educational resources using the MERLOT dataset. By employing SHAP values and a Keep-Absolute (resample) metric, the approach achieves an AUC of 0.81 and provides a rigorous, quantitative evaluation method that moves beyond traditional user tests or qualitative assessments.

## Method Summary
The authors establish a formal taxonomy distinguishing interpretability, explainability, and understandability, then decompose explainability into five measurable dimensions. They employ SHAP values (KernelExplainer) to compute feature importance and measure explainability via a Keep-Absolute (resample) metric. Total explainability is defined as the product of interpretability, completeness, and understandability. The method is applied to a recommender system trained on the MERLOT dataset, with feature correlation handled through aggregation. The approach quantifies explainability in a bounded, interpretable score ranging from 0.81 to 1 depending on user bearable complexity.

## Key Results
- Introduces a formal taxonomy distinguishing interpretability, explainability, and understandability in XAI
- Achieves an AUC of 0.81 using the Keep-Absolute (resample) metric for feature importance measurement
- Defines total explainability as the product of interpretability, completeness, and understandability, ranging from 0.81 to 1
- Demonstrates the approach on a recommender system for educational resources using the MERLOT dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper establishes a formal taxonomy that distinguishes interpretability, explainability, and understandability, enabling quantitative evaluation of XAI systems.
- Mechanism: By decomposing explainability into measurable dimensions (transparency, interpretability, completeness, complexity, and understandability), the paper provides a structured framework where each dimension can be operationalized and measured mathematically.
- Core assumption: These five dimensions are necessary and sufficient to capture the essence of explainability in ML systems.
- Evidence anchors:
  - [abstract] The paper introduces a novel taxonomy that addresses the current gap in the literature by providing a clear and unambiguous understanding of the key concepts and relationships in XAI.
  - [section] The paper proposes a taxonomic classification of explainability-related concepts that minimizes, yet preserves, the essential involvement of the human user.
  - [corpus] Corpus neighbors include surveys and reviews of XAI taxonomies, supporting the relevance of this approach.
- Break condition: If any of the five dimensions cannot be meaningfully quantified for a given ML system, the taxonomy would lose its quantitative power.

### Mechanism 2
- Claim: SHAP values provide a principled way to compute feature importance and generate explanations that feed into the completeness and understandability measures.
- Mechanism: SHAP values are based on game theory (Shapley values) and provide a fair distribution of feature contributions to model predictions. These values can be aggregated into explanations that are both complete (capturing model behavior) and understandable (via feature importance plots and metrics like Keep-Absolute).
- Core assumption: SHAP values accurately capture feature contributions and can be computed efficiently for the model in question.
- Evidence anchors:
  - [section] The paper employs the SHAP package to quantify and enhance the explainability of the Recommender System within the context of the newly developed taxonomy.
  - [section] SHAP values are used to build an explanation model g that is additive with respect to the features of the original model.
  - [corpus] The corpus includes technical reviews of SHAP and feature attribution methods, supporting its validity.
- Break condition: If the model's feature space is too complex or the background dataset is not representative, SHAP values may not provide reliable explanations.

### Mechanism 3
- Claim: The product formula (E = I × C × U) for total explainability creates a bounded, interpretable score that reflects the multiplicative nature of the three core dimensions.
- Mechanism: By defining explainability as the product of interpretability, completeness, and understandability, the paper ensures that all three components must be high for the system to be considered highly explainable. This multiplicative relationship prevents any single dimension from compensating for weakness in another.
- Core assumption: The multiplicative relationship accurately reflects the real-world importance of having all three dimensions strong.
- Evidence anchors:
  - [section] The paper defines total explainability as the product of interpretability, completeness, and understandability, ranging from 0.81 to 1 depending on the user's bearable complexity.
  - [section] The paper states that a system is explainable if it is capable of providing explanations of its inner mechanism which are both complete and understandable.
  - [corpus] Limited direct evidence; the corpus does not discuss multiplicative explainability formulas, so this is an original contribution of the paper.
- Break condition: If the multiplicative model does not align with user perception of explainability, the metric may not be practically useful.

## Foundational Learning

- Concept: Transparency (white-box vs black-box models)
  - Why needed here: Transparency is the foundation for interpretability; understanding whether a model is transparent or opaque determines the approach to generating explanations.
  - Quick check question: Can you identify whether a given ML model (e.g., linear regression vs deep neural network) is white-box or black-box?

- Concept: Shapley values and game theory
  - Why needed here: SHAP values are used to quantify feature importance and generate explanations; understanding their theoretical basis is crucial for interpreting results.
  - Quick check question: What is the key property of Shapley values that makes them fair for distributing feature contributions?

- Concept: Feature correlation and its impact on SHAP
  - Why needed here: High feature correlation can distort SHAP values; the paper addresses this by aggregating correlated features (e.g., discipline levels) before computing explanations.
  - Quick check question: Why might high correlation between features lead to unreliable SHAP values, and how does the paper mitigate this?

## Architecture Onboarding

- Component map: Recommender System -> Feature Correlation Analysis -> SHAP Explanation Generator -> Complexity/Understandability Calculator -> Total Explainability Aggregator
- Critical path: Train RS → Analyze feature correlation → Generate SHAP explanations → Compute completeness and understandability → Calculate total explainability
- Design tradeoffs: The choice of SHAP (over LIME or counterfactuals) balances ease of implementation, computational efficiency, and the quality of explanations. The aggregation of correlated features simplifies the explanation space but may lose some granularity.
- Failure signatures: Low AUC in Keep-Absolute metric indicates poor feature identification; high complexity with low understandability suggests explanations are too complex for users; low completeness indicates explanations do not fully capture model behavior.
- First 3 experiments:
  1. Train a simple SVM on the Iris dataset and generate SHAP explanations; verify that the Keep-Absolute metric is high for the most important features.
  2. Apply the full pipeline to the MERLOT RS; compute total explainability for different user bearable complexity thresholds (ωb).
  3. Test the impact of feature correlation by comparing SHAP values before and after aggregating correlated features; ensure the aggregation improves reliability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal trade-off between interpretability and completeness for explainability in recommender systems?
- Basis in paper: [explicit] The paper defines explainability as the product of interpretability, completeness, and understandability, but does not explore optimal trade-offs between these components.
- Why unresolved: The paper presents a framework but doesn't investigate how to balance these components for optimal explainability.
- What evidence would resolve it: Empirical studies comparing different trade-off configurations and their impact on user satisfaction and system performance.

### Open Question 2
- Question: How does user bearable complexity (ωb) vary across different user demographics and educational backgrounds?
- Basis in paper: [explicit] The paper acknowledges that understandability depends on user bearable complexity but doesn't explore how this varies across user groups.
- Why unresolved: The paper treats ωb as a parameter but doesn't investigate its variation across different user populations.
- What evidence would resolve it: User studies measuring ωb across different demographic groups and correlating with educational backgrounds.

### Open Question 3
- Question: Can the proposed taxonomy be effectively extended to other AI applications beyond recommender systems?
- Basis in paper: [inferred] The paper demonstrates the taxonomy on a recommender system but doesn't explore its applicability to other AI domains.
- Why unresolved: The framework is only tested on one type of AI system, limiting generalizability.
- What evidence would resolve it: Case studies applying the taxonomy to different AI applications (e.g., computer vision, NLP) and evaluating its effectiveness.

## Limitations
- The multiplicative model for total explainability assumes equal importance of all three dimensions, which may not reflect real-world user preferences
- The approach relies heavily on SHAP values, which can be computationally expensive for large models and may not capture non-linear feature interactions effectively
- The MERLOT dataset used is specific to educational resources and may not generalize to other domains

## Confidence

**High**: The distinction between interpretability, explainability, and understandability is well-founded and clearly operationalized.

**Medium**: The effectiveness of SHAP values for generating explanations in recommender systems, supported by strong theoretical foundations but dependent on feature quality.

**Medium**: The multiplicative formula for total explainability is mathematically sound but lacks extensive empirical validation across diverse use cases.

## Next Checks

1. Test the taxonomy on a different domain (e.g., healthcare or finance) to assess generalizability and robustness.
2. Conduct user studies to validate whether the multiplicative model aligns with human perception of explainability.
3. Compare the proposed approach with alternative XAI methods (e.g., LIME or counterfactual explanations) on the same dataset to benchmark performance and usability.