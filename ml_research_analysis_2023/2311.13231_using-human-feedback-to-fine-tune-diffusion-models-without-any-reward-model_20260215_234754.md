---
ver: rpa2
title: Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model
arxiv_id: '2311.13231'
source_url: https://arxiv.org/abs/2311.13231
tags:
- diffusion
- human
- reward
- images
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces D3PO, a novel method for fine-tuning diffusion
  models directly using human feedback without training a reward model. The key idea
  is to treat the denoising process as a multi-step Markov decision process and extend
  the Direct Preference Optimization (DPO) framework to this setting.
---

# Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model

## Quick Facts
- arXiv ID: 2311.13231
- Source URL: https://arxiv.org/abs/2311.13231
- Reference count: 40
- One-line primary result: D3PO fine-tunes diffusion models using human feedback without reward models, achieving comparable results on tasks like image compressibility, safety, and aesthetic quality.

## Executive Summary
This paper introduces D3PO, a novel method for fine-tuning diffusion models using human feedback without the need for a separate reward model. The key innovation is treating the denoising process as a multi-step Markov decision process and extending the Direct Preference Optimization (DPO) framework to this setting. By updating model parameters at each denoising step based on human preferences, D3PO avoids the high memory costs associated with applying DPO directly to full images. Experiments demonstrate that D3PO achieves comparable or better results than methods using reward models on various tasks including improving image compressibility, incompressibility, aesthetic quality, reducing image distortion rates, enhancing safety, and improving prompt-image alignment.

## Method Summary
D3PO fine-tunes diffusion models by conceptualizing the denoising process as a multi-step Markov decision process (MDP). The method extends Direct Preference Optimization (DPO) to this MDP setting, allowing parameter updates at each denoising step based on human preferences between generated images. Instead of training a reward model, D3PO uses relative scales of quantifiable objectives (like image size or aesthetic scores) as proxies for human preference. The approach employs LoRA adapters for efficient fine-tuning and operates directly on the pre-trained diffusion model's denoising steps. Training involves generating paired images from the same prompt and initial noise, collecting human preference feedback, computing D3PO loss across all intermediate steps, and updating parameters via gradient descent.

## Key Results
- Achieves comparable results to reward-model-based methods on image compressibility, incompressibility, and aesthetic quality tasks
- Successfully reduces image distortion rates (e.g., "1 hand" prompt) without explicit reward modeling
- Improves image safety and prompt-image alignment metrics while using only human preferences as guidance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: D3PO enables fine-tuning diffusion models without training a separate reward model by treating the denoising process as a multi-step MDP.
- Mechanism: The denoising steps are modeled as states and actions in an MDP. D3PO extends DPO theory to this MDP setting, allowing parameter updates at each denoising step based on human preferences, avoiding the need to store gradients for full image generations.
- Core assumption: The denoising process can be effectively modeled as an MDP where human preferences on final images can guide updates at intermediate steps.
- Evidence anchors:
  - [abstract] "The key idea is to treat the denoising process as a multi-step Markov decision process and extend the Direct Preference Optimization (DPO) framework to this setting."
  - [section 4.1] "We conceptualize the denoising process within the diffusion model as a multi-step MDP, which varies slightly from the approach outlined in [19]."
  - [corpus] Weak corpus evidence - related papers focus on DPO for diffusion models but don't validate the MDP reformulation directly.
- Break condition: If the MDP abstraction poorly captures the diffusion dynamics, or if human preferences cannot effectively guide intermediate step updates.

### Mechanism 2
- Claim: D3PO can match or exceed reward-model-based methods by using relative objective scales as a proxy for human preference.
- Mechanism: Instead of training a reward model, D3PO uses relative values of quantifiable objectives (image size, aesthetic scores) to establish pairwise preferences. These preferences are then used to update the model directly.
- Core assumption: Relative scales of objectives can serve as a reliable proxy for human preferences in selecting preferred images.
- Evidence anchors:
  - [abstract] "In experiments, our method uses the relative scale of objectives as a proxy for human preference, delivering comparable results to methods using ground-truth rewards."
  - [section 5.1] "We first use the size of the images to measure the preference relationship between two pictures. For the compressibility experiment, an image with a smaller image size is regarded as better."
  - [corpus] Weak corpus evidence - no direct comparison of objective scales vs. human preferences in related work.
- Break condition: If objective scales don't correlate well with actual human preferences for the target tasks.

### Mechanism 3
- Claim: By updating at each denoising step, D3PO achieves better data efficiency than updating only at the final step.
- Mechanism: D3PO constructs T sub-segments from each preference pair and updates the model using all intermediate states, effectively multiplying the data usage by a factor of T compared to single-step methods.
- Core assumption: Information from intermediate denoising states is valuable for guiding the model toward preferred outputs.
- Evidence anchors:
  - [section 4.3] "Compared to eq. (13), eq. (15) uses every state-action pair for training, effectively increasing the data utilization of the segment by a factor of T."
  - [section 4.2] "The middle states of the segment are noises and semi-finished images, it is hard for humans to judge which segment is better by observing the whole segment."
  - [corpus] Weak corpus evidence - no ablation studies comparing single-step vs. multi-step updates in related work.
- Break condition: If intermediate states provide noisy or misleading signals that harm the fine-tuning process.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Understanding RLHF provides context for why reward models are typically used and the challenges they present in diffusion model fine-tuning.
  - Quick check question: What are the main computational and data challenges of using RLHF with diffusion models?

- Concept: Diffusion Models and Denoising Process
  - Why needed here: D3PO operates directly on the denoising process of diffusion models, so understanding this process is crucial for implementing and debugging the method.
  - Quick check question: How does the denoising process in diffusion models relate to the MDP formulation used in D3PO?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: D3PO extends DPO theory to the MDP setting, so understanding DPO's mechanism and assumptions is essential.
  - Quick check question: How does DPO differ from traditional RLHF approaches in terms of training objectives and data requirements?

## Architecture Onboarding

- Component map:
  Pre-trained diffusion model (Stable Diffusion v1.5) -> LoRA adapter -> Human feedback collection mechanism -> D3PO loss computation module -> Training loop with multi-step updates

- Critical path:
  1. Generate two images from same prompt and initial noise
  2. Collect human preference feedback
  3. Compute D3PO loss using all intermediate denoising steps
  4. Update model parameters via gradient descent
  5. Repeat for multiple epochs

- Design tradeoffs:
  - Memory vs. data efficiency: Updating at each step increases memory usage but improves data utilization
  - Preference quality: Using objective scales as proxy may be less reliable than explicit human rankings
  - LoRA vs. full fine-tuning: LoRA reduces memory but may limit adaptation capacity

- Failure signatures:
  - Training instability or divergence
  - Model generating low-quality or nonsensical images
  - Preferences not leading to desired changes in outputs
  - Memory errors during multi-step gradient computation

- First 3 experiments:
  1. Replicate compressibility/incompressibility results from Section 5.1 to validate basic D3PO functionality
  2. Test hand distortion reduction on "1 hand" prompt to verify preference-based fine-tuning
  3. Evaluate prompt-image alignment improvements using quantitative metrics (CLIP, BLIP scores)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of D3PO scale with the number of human feedback samples per epoch? Is there a point of diminishing returns?
- Basis in paper: [inferred] The paper mentions using 80 images per epoch for predefined objectives and 1,000 images per epoch for reducing image distortions. However, it does not explore the impact of varying the number of feedback samples.
- Why unresolved: The paper does not conduct experiments to determine the relationship between the number of human feedback samples and the performance of D3PO.
- What evidence would resolve it: Conduct experiments with varying numbers of human feedback samples per epoch and compare the performance of D3PO in terms of the target metrics (e.g., compressibility, incompressibility, aesthetic quality, image distortion rates, safety, prompt-image alignment).

### Open Question 2
- Question: How does D3PO compare to other methods that fine-tune diffusion models using human feedback, such as ReFL or DDPO, in terms of computational efficiency and memory usage?
- Basis in paper: [explicit] The paper mentions that D3PO requires less GPU memory than directly applying the DPO algorithm to diffusion models. However, it does not provide a detailed comparison of computational efficiency and memory usage with other methods.
- Why unresolved: The paper does not provide a comprehensive comparison of D3PO with other methods in terms of computational efficiency and memory usage.
- What evidence would resolve it: Conduct experiments comparing the computational efficiency and memory usage of D3PO with other methods (e.g., ReFL, DDPO) when fine-tuning diffusion models using human feedback.

### Open Question 3
- Question: Can D3PO be extended to other generative models beyond diffusion models, such as GANs or VAEs?
- Basis in paper: [inferred] The paper focuses on fine-tuning diffusion models using human feedback. However, the underlying concept of treating the denoising process as a multi-step MDP and extending DPO to MDP could potentially be applied to other generative models.
- Why unresolved: The paper does not explore the applicability of D3PO to other generative models beyond diffusion models.
- What evidence would resolve it: Conduct experiments applying the D3PO framework to other generative models (e.g., GANs, VAEs) and compare the performance with existing methods for fine-tuning these models using human feedback.

## Limitations

- The approach relies on objective scales as proxies for human preferences, which may not fully capture nuanced human judgments
- The MDP reformulation of the denoising process lacks extensive empirical validation compared to traditional reward model approaches
- The use of LoRA adapters, while memory-efficient, may constrain the model's adaptation capacity compared to full fine-tuning

## Confidence

- **High Confidence**: The basic mechanism of extending DPO to multi-step denoising processes is theoretically well-founded and the experimental results show clear improvements on targeted metrics
- **Medium Confidence**: The claim that objective scales can reliably proxy human preferences needs more validation, as this assumption is critical but weakly supported by existing evidence
- **Low Confidence**: The data efficiency claims (T-fold improvement) lack direct ablation studies comparing single-step vs. multi-step updates in the diffusion model context

## Next Checks

1. Conduct ablation studies comparing D3PO's performance when using different numbers of denoising steps (T values) to quantify the actual data efficiency gains
2. Validate the correlation between objective scales (image size, aesthetic scores) and human preferences through controlled user studies across different tasks
3. Compare D3PO's performance against reward-model-based approaches on additional tasks not covered in the original experiments to assess generalizability