---
ver: rpa2
title: Exploring the Practicality of Generative Retrieval on Dynamic Corpora
arxiv_id: '2305.18952'
source_url: https://arxiv.org/abs/2305.18952
tags:
- retrieval
- generative
- knowledge
- lora
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces StreamingIR, a new benchmark for evaluating
  retrieval models in dynamic, temporally evolving environments. It addresses the
  challenge of adapting generative retrieval models to constantly updating corpora,
  which is common in realistic applications.
---

# Exploring the Practicality of Generative Retrieval on Dynamic Corpora

## Quick Facts
- arXiv ID: 2305.18952
- Source URL: https://arxiv.org/abs/2305.18952
- Reference count: 5
- Key outcome: Generative retrieval models excel with seen data and benefit significantly from pre-training, but struggle without it in dynamic corpora.

## Executive Summary
This paper introduces StreamingIR, a new benchmark for evaluating retrieval models in dynamic, temporally evolving environments. It addresses the challenge of adapting generative retrieval models to constantly updating corpora, which is common in realistic applications. The study compares bi-encoder and generative retrieval approaches, finding that generative models excel with seen data and benefit significantly from pre-training, but struggle without it. To mitigate catastrophic forgetting and improve efficiency, the authors propose using parameter-efficient methods like LoRA for continual pre-training. Results show that generative retrieval with LoRA is competitive with bi-encoders in both performance and efficiency, requiring less indexing time and storage.

## Method Summary
The paper proposes a new benchmark called StreamingIR for evaluating retrieval models in dynamic, temporally evolving environments. The benchmark is based on the StreamingQA dataset, which includes news articles from 2007 to 2020. The paper compares the performance of bi-encoder models (DPR) and generative retrieval models (SEAL) under different scenarios: only fine-tuning, only pre-training, and pre-training + fine-tuning. To mitigate catastrophic forgetting and improve efficiency, the paper proposes using parameter-efficient methods like LoRA (Low-Rank Adaptation) for continual pre-training of generative retrieval models. The models are evaluated using hits@N and answer recall@N metrics, where N is 5, 10, 50, and 100.

## Key Results
- Generative retrieval models excel at seen data and benefit significantly from pre-training, but struggle without it.
- Parameter-efficient methods like LoRA effectively mitigate catastrophic forgetting while maintaining performance.
- Generative retrieval is inherently more efficient than bi-encoder models in terms of indexing time and memory usage.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generative retrieval models excel at seen data and benefit significantly from pre-training, but struggle without it.
- Mechanism: Generative retrieval models leverage full parametric architectures that can capture rich semantic relationships through encoder-decoder attention mechanisms. When trained on dynamic corpora, these models can adapt to new knowledge through continual pre-training, particularly when combined with parameter-efficient methods like LoRA.
- Core assumption: The generative retrieval model's ability to generate document identifiers as n-grams provides flexibility in handling dynamic content.
- Evidence anchors:
  - [abstract] "generative retrieval shows(1) detrimental performance when only supervised data is used for fine-tuning, (2) superior performance over bi-encoders when only unsupervised data is available"
  - [section 5.1] "generative retrieval tends to exhibit lower retrieval performance compared to bi-encoder when dealing with unseen corpora. However, we observe inherent advantages of generative retrievers in that they excel at data that have been seen during training"
  - [corpus] Weak evidence - corpus neighbors don't directly address this specific mechanism
- Break condition: The model encounters data distributions significantly different from training data, or the parameter-efficient methods fail to capture new knowledge effectively.

### Mechanism 2
- Claim: Parameter-efficient methods like LoRA effectively mitigate catastrophic forgetting while maintaining performance.
- Mechanism: By freezing the pretrained model weights and introducing trainable rank decomposition matrices, LoRA allows the model to learn new knowledge without overwriting existing knowledge. This is particularly effective for generative retrieval models adapting to dynamic corpora.
- Core assumption: The rank decomposition matrices can capture the necessary information for new knowledge while preserving the original model's capabilities.
- Evidence anchors:
  - [section 4.3.1] "LoRA is a method proposed for adapting large-scale pretrained models to specific tasks or domains. By freezing the pretrained model weights and introducing trainable rank decomposition matrices into each Transformer layer, LoRA significantly reduces the number of trainable parameters"
  - [section 5.1] "updating full parameters is not compatible for bi-encoder variants for continual pre-training of updated knowledge. The result from Table 2... shows that the full parameter approach forget heavily on existing knowledge, whereas the model pretrained by the Expanded LoRA (LoRA (exp.)) exhibits no sign of forgetting"
  - [corpus] Weak evidence - corpus neighbors don't directly address this specific mechanism
- Break condition: The rank decomposition becomes too restrictive to capture complex new knowledge patterns.

### Mechanism 3
- Claim: Generative retrieval is inherently more efficient than bi-encoder models in terms of indexing time and memory usage.
- Mechanism: Generative retrieval models store all document information within their parameters, eliminating the need for separate document indexing. This makes them particularly efficient when dealing with dynamic corpora where documents are frequently added or updated.
- Core assumption: The model's parameter capacity is sufficient to encode all relevant document information without significant performance degradation.
- Evidence anchors:
  - [section 5.2] "Generative retrieval is inherently more efficient than bi-encoder models, particularly in terms of indexing time and the associated memory efficiency required for storing all the indices"
  - [section 5.2] "updating the bi-encoder models are not feasible in our scenario, since it needs to index the whole underlying corpora when the parameters of the model gets updated, corresponding to about 12 hours of indexing + search time"
  - [corpus] Weak evidence - corpus neighbors don't directly address this specific mechanism
- Break condition: The model's parameter capacity becomes insufficient to encode all relevant information as the corpus grows.

## Foundational Learning

- Concept: Continual learning and catastrophic forgetting
  - Why needed here: Understanding how models adapt to new data without losing previously learned knowledge is crucial for evaluating the effectiveness of parameter-efficient methods like LoRA.
  - Quick check question: What is catastrophic forgetting and how does it impact model performance when adapting to new data?

- Concept: Dense retrieval vs. generative retrieval
  - Why needed here: Distinguishing between these two retrieval paradigms is essential for understanding the trade-offs and advantages of each approach in dynamic environments.
  - Quick check question: What are the key differences between dense retrieval (bi-encoder) and generative retrieval approaches?

- Concept: Parameter-efficient fine-tuning techniques
  - Why needed here: LoRA and similar methods are central to the paper's approach for adapting generative retrieval models to dynamic corpora without full retraining.
  - Quick check question: How does LoRA differ from traditional fine-tuning methods in terms of parameter updates and efficiency?

## Architecture Onboarding

- Component map: Query (with timestamp metadata) -> Generative retrieval model (e.g., SEAL) with LoRA adaptation -> Generated document identifiers (n-grams)
- Critical path:
  1. Pre-train generative retrieval model on historical data
  2. Apply LoRA to adapt to new data streams
  3. Generate document identifiers for retrieval
  4. Evaluate performance on dynamic benchmark
- Design tradeoffs:
  - Parameter efficiency vs. model capacity: LoRA reduces trainable parameters but may limit the model's ability to capture complex patterns.
  - Pre-training vs. fine-tuning: Pre-training on unsupervised data improves performance but requires additional computational resources.
  - Model size vs. efficiency: Larger models may capture more information but are less efficient to update.
- Failure signatures:
  - Significant performance drop on recent data indicates catastrophic forgetting
  - High computational cost for updates suggests inefficient parameter adaptation
  - Low recall on seen data may indicate insufficient model capacity
- First 3 experiments:
  1. Compare generative retrieval with bi-encoder on static benchmark to establish baseline performance.
  2. Evaluate generative retrieval with and without pre-training on dynamic corpus to measure the impact of pre-training.
  3. Test different parameter-efficient methods (LoRA variants) to identify the most effective approach for mitigating catastrophic forgetting.

## Open Questions the Paper Calls Out

- Question: How does the performance of generative retrieval models scale when applied to significantly larger, more diverse dynamic corpora beyond the StreamingIR benchmark?
  - Basis in paper: [inferred] The paper demonstrates competitive performance on StreamingIR, but scalability to larger corpora is not directly tested.
  - Why unresolved: The benchmark used is limited in size and scope compared to real-world applications, which often involve much larger and more diverse datasets.
  - What evidence would resolve it: Empirical studies testing generative retrieval models on significantly larger and more diverse dynamic corpora, with comparisons to bi-encoder baselines.

- Question: What are the long-term effects of continual pre-training on generative retrieval models in terms of both performance and catastrophic forgetting?
  - Basis in paper: [explicit] The paper addresses catastrophic forgetting and proposes parameter-efficient methods like LoRA to mitigate it, but long-term effects are not explored.
  - Why unresolved: The experiments focus on short-term adaptation and do not track model performance over extended periods or multiple update cycles.
  - What evidence would resolve it: Longitudinal studies tracking model performance and forgetting over multiple update cycles and extended time periods.

- Question: How do different parameter-efficient fine-tuning methods compare in terms of effectiveness and efficiency for generative retrieval models in dynamic environments?
  - Basis in paper: [explicit] The paper uses LoRA and parameter editing, showing effectiveness, but does not compare with other methods like adapters or sparse fine-tuning.
  - Why unresolved: The paper focuses on LoRA and parameter editing, without exploring other parameter-efficient methods that might offer different trade-offs.
  - What evidence would resolve it: Comparative studies evaluating multiple parameter-efficient fine-tuning methods (e.g., LoRA, adapters, sparse fine-tuning) on generative retrieval models in dynamic environments.

## Limitations

- The StreamingIR benchmark relies on a single domain (news articles) and time period, limiting generalizability to other types of dynamic corpora.
- The comparison between generative and bi-encoder models focuses primarily on retrieval performance metrics, without deeper analysis of factors like query complexity or document diversity that might influence results.
- The parameter-efficient LoRA implementation is evaluated in isolation, without comparison to other parameter-efficient fine-tuning methods that might offer similar benefits.

## Confidence

- High confidence: The observations about generative models excelling on seen data versus bi-encoders showing more stable performance on unseen data are well-supported by the experimental results.
- Medium confidence: The claims about LoRA's effectiveness in preventing catastrophic forgetting while maintaining efficiency are demonstrated positively but could benefit from broader comparisons with alternative parameter-efficient methods.
- Medium confidence: The assertion that generative retrieval is inherently more efficient than bi-encoders receives support but would be stronger with more detailed measurements of indexing and search times across different corpus sizes.

## Next Checks

1. Test the proposed approach on multiple dynamic corpus types beyond news articles, including social media streams and technical documentation, to validate the generalizability of the findings.
2. Compare LoRA with other parameter-efficient methods (e.g., prefix tuning, adapters) on the same dynamic retrieval task to establish whether LoRA offers unique advantages.
3. Conduct ablation studies varying the temporal distance between training and test data to better understand when generative retrieval's strengths and weaknesses manifest most clearly.