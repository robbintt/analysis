---
ver: rpa2
title: 'Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and
  Ethics'
arxiv_id: '2309.07120'
source_url: https://arxiv.org/abs/2309.07120
tags:
- visual
- llms
- instruction
- tuning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that multi-modal training can enhance LLMs' truthfulness
  and ethical alignment. By fine-tuning LLaMA2 7B with visual instruction data, the
  model achieves 46.0% on TruthfulQA-mc (+7.1% vs.
---

# Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics

## Quick Facts
- arXiv ID: 2309.07120
- Source URL: https://arxiv.org/abs/2309.07120
- Authors: 
- Reference count: 40
- Key outcome: Multi-modal training enhances LLM truthfulness (46.0% on TruthfulQA-mc, +7.1%) and ethics (65.4%, +19.6%) while maintaining NLP performance

## Executive Summary
This paper demonstrates that multi-modal training through visual instruction tuning can significantly improve the truthfulness and ethical alignment of large language models. By fine-tuning LLaMA2 7B with visual instruction data, the model achieves substantial gains on alignment benchmarks while maintaining strong performance on standard NLP tasks. The approach shows that different types of visual instruction data benefit specific alignment dimensions, with conversational data improving ethics performance and reasoning data enhancing truthfulness. The method requires only 80k visual instruction examples compared to traditional approaches needing one million human annotations.

## Method Summary
The method involves fine-tuning LLaMA2 7B with visual instruction data using a two-stage approach. First, a vision encoder (CLIP ViT-L/14) and vision-language connector are trained on 595k image-text pairs from CC3M. Then, the full model is fine-tuned on 80k instruction-following data from LLaVA. The model uses a linear projection layer to map visual tokens into language space. The paper also explores text-only visual instruction tuning to isolate the contribution of visual components. Evaluation includes testing the tuned models without visual components on alignment benchmarks to measure improvements.

## Key Results
- Visual instruction tuning improves TruthfulQA-mc accuracy from 38.9% to 46.0% (+7.1%)
- Ethics benchmark performance increases from 45.8% to 65.4% (+19.6%)
- Text-only visual instruction tuning achieves comparable alignment performance to full visual-text tuning
- Different data types show distinct benefits: conversational data improves ethics, reasoning data improves truthfulness

## Why This Works (Mechanism)

### Mechanism 1
Visual instruction tuning improves LLM truthfulness and ethics without explicit alignment prompts through high-quality instruction examples that implicitly teach ethical reasoning patterns via grounded visual context. The superior instruction quality of visual-text data drives improvements. Break condition: If visual-text instruction quality is actually similar to text-only instruction quality.

### Mechanism 2
Different types of visual instruction data benefit different alignment tasks because conversational data improves ethics performance while reasoning/details data improves truthfulness performance due to task-specific reasoning patterns. Break condition: If the data type-performance relationship is coincidental rather than causal.

### Mechanism 3
Text-only visual instruction tuning achieves comparable alignment performance to full visual-text tuning because the textual component carries most of the alignment signal, with visual inputs providing modest additional benefit. Break condition: If visual components contain critical alignment information not captured in text.

## Foundational Learning

- Concept: Multi-modal model architecture (vision encoder + connector + LLM)
  - Why needed here: Understanding how visual instruction tuning modifies the LLM component while maintaining visual capabilities
  - Quick check question: What are the three key components of a standard MLLM and which ones get modified during visual instruction tuning?

- Concept: Catastrophic forgetting and alignment tax
  - Why needed here: Evaluating whether multi-modal training degrades NLP capabilities while improving alignment
  - Quick check question: How does the paper measure whether visual instruction tuning causes performance degradation on standard NLP tasks?

- Concept: Instruction tuning methodology
  - Why needed here: Understanding how visual instruction data differs from standard instruction datasets and why it might have higher quality
  - Quick check question: What distinguishes visual instruction tuning from traditional text-only instruction tuning?

## Architecture Onboarding

- Component map: Vision encoder (CLIP ViT-L/14) -> Vision-language connector (trainable linear layer) -> LLM (LLaMA variants)
- Critical path: Vision-language connector fine-tuning -> Full LLM fine-tuning with 80k visual instruction data
- Design tradeoffs: Using frozen vision encoder preserves visual capabilities but may limit adaptation; LoRA vs full fine-tuning balances parameter efficiency with performance
- Failure signatures: Degradation on standard NLP benchmarks, poor performance on multi-modal tasks, inconsistent alignment improvements across different LLM variants
- First 3 experiments:
  1. Compare baseline LLM performance vs. visual-instruction-tuned LLM on Ethics and TruthfulQA benchmarks
  2. Test text-only vs. full visual-text instruction tuning to isolate the contribution of visual components
  3. Ablate different types of visual instruction data (Conversation, Details, Reasoning) to identify which types improve which alignment tasks

## Open Questions the Paper Calls Out

### Open Question 1
How do different types of visual instruction data (conversational, details, reasoning) differentially impact various aspects of LLM alignment beyond truthfulness and ethics? The study only examined two specific alignment benchmarks, leaving other important dimensions unexplored.

### Open Question 2
What is the optimal balance between visual instruction tuning and traditional alignment methods like RLHF for achieving comprehensive LLM alignment? The paper doesn't compare visual instruction tuning as a complementary approach to RLHF.

### Open Question 3
Why do text-aligned LLMs (like Vicuna and LLaMA2-chat) show worse performance on corrupted images compared to vanilla LLMs when fine-tuned into MLLMs? This unexpected finding remains unexplained by the paper.

## Limitations

- Limited to testing on a single LLM variant (LLaMA2 7B), raising questions about generalizability
- Attribution of improvements to instruction quality lacks direct empirical validation through controlled experiments
- Small dataset size (80k instructions) raises concerns about overfitting to specific benchmarks
- No investigation into why text-aligned LLMs show worse robustness on corrupted images

## Confidence

The core claims carry **Medium confidence** due to:
- Limited generalizability across different LLM architectures
- Lack of controlled experiments to validate the mechanism attribution
- Weak evidence anchoring for data type-task mapping claims
- Moderate sample size raising overfitting concerns

## Next Checks

1. Replicate the alignment improvements across multiple LLM variants (OpenLLaMA-3B, LLaMA-7B) to test generalizability
2. Conduct a controlled experiment comparing visual-text instruction data quality against high-quality text-only instruction datasets to isolate the mechanism
3. Perform a larger-scale ablation study with more data types and quantities to validate the data type-task mapping hypothesis