---
ver: rpa2
title: 'Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking
  Obscured Sub-optimality'
arxiv_id: '2310.07234'
source_url: https://arxiv.org/abs/2310.07234
tags:
- learning
- continual
- pre-training
- representations
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies prompt-based continual learning under self-supervised
  pre-training. It shows that current methods underperform due to difficulty in incorporating
  task-specific knowledge into instructed representations and predicting task identity
  from uninstructed ones.
---

# Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality

## Quick Facts
- arXiv ID: 2310.07234
- Source URL: https://arxiv.org/abs/2310.07234
- Reference count: 40
- Key outcome: HiDe-Prompt achieves up to 15.01% and 9.61% gains over baselines on Split CIFAR-100 and Split ImageNet-R respectively

## Executive Summary
This paper addresses the underperformance of prompt-based continual learning methods under self-supervised pre-training. The authors identify that current methods struggle with incorporating task-specific knowledge into instructed representations and predicting task identity from uninstructed ones. They propose a hierarchical decomposition of the continual learning objective into within-task prediction (WTP), task-identity inference (TII), and task-adaptive prediction (TAP), and introduce HiDe-Prompt to explicitly optimize these components using contrastive regularization and prompt ensemble strategies.

## Method Summary
HiDe-Prompt is a prompt-based continual learning method that decomposes the learning objective into three hierarchical components: within-task prediction, task-identity inference, and task-adaptive prediction. The method uses an ensemble of task-specific prompts coordinated by contrastive regularization, which encourages instructed representations to be distinguishable across tasks while maintaining current task performance. The approach preserves statistics of both uninstructed and instructed representations and employs a prompt ensemble strategy to facilitate knowledge transfer while avoiding catastrophic forgetting.

## Key Results
- HiDe-Prompt outperforms baseline methods (L2P, DualPrompt, S-Prompt++, CODA-Prompt) on Split CIFAR-100 and Split ImageNet-R
- The method achieves significant improvements under self-supervised pre-training (up to 15.01% and 9.61% gains on respective datasets)
- HiDe-Prompt demonstrates robustness across different pre-training paradigms including iBOT, DINO, and MoCo

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimizing hierarchical components (WTP, TII, TAP) separately is more effective than end-to-end joint optimization under self-supervised pre-training.
- Mechanism: Self-supervised pre-training yields more general representations, making it difficult for prompt parameters to incorporate task-specific knowledge and for uninstructed representations to predict task identity. By decomposing the objective into within-task prediction (WTP), task-identity inference (TII), and task-adaptive prediction (TAP), each component can be optimized explicitly with dedicated architectures and losses.
- Core assumption: Well-distributed representations from pre-training allow statistical modeling of uninstructed and instructed representations for effective optimization of hierarchical components.
- Evidence anchors: [abstract] "This is largely due to the difficulty of task-specific knowledge being incorporated into instructed representations via prompt parameters and predicted by uninstructed representations at test time." [section 3.2] "the CKA similarity of self-supervised pre-training is significantly higher, suggesting a greater difficulty for prompt parameters to incorporate task-specific knowledge."

### Mechanism 2
- Claim: Contrastive regularization (CR) coordinates instructed representations across tasks, preventing overlap and facilitating task-adaptive prediction.
- Mechanism: During WTP optimization, the prompt ensemble is encouraged to produce instructed representations that are distinguishable from previous tasks while maintaining performance on the current task. CR uses preserved statistics of instructed representations (mean vectors of each class) to regularize the current task's representations.
- Core assumption: Classes tend to have single-peaked representations under adequate pre-training, allowing effective modeling with Gaussian distributions or centroids.
- Evidence anchors: [section 4.2] "we exploit the old-task statistics of instructed representations... design a contrastive regularization (CR):" [section 3.2] "since each class tends to have single-peaked representations... we can naturally approximate them with Gaussian distributions."

### Mechanism 3
- Claim: Prompt ensemble strategy facilitates knowledge transfer across tasks while avoiding catastrophic forgetting.
- Mechanism: For each new task, the current prompt is initialized with the previous task's prompt and optimized with a weighted combination of all previous prompts. This allows the model to leverage knowledge from previous tasks while adapting to the new task.
- Core assumption: Task-specific prompts can be effectively combined through weighted summation to incorporate knowledge from multiple tasks.
- Evidence anchors: [section 4.2] "we employ a prompt ensemble (PE) strategy, where the current prompt is initialized by the last prompt et ← et−1 and then optimized with a weighted combination of all previous prompts" [section 3.1] "The mainstream idea is to construct adaptive prompts for each task and then infer appropriate (combinations of) prompts at test time."

## Foundational Learning

- Concept: Cross-entropy loss and its decomposition in multi-objective optimization
  - Why needed here: The paper's theoretical analysis relies on cross-entropy to decompose the continual learning objective into WTP, TII, and TAP, and to derive sufficient and necessary conditions for good performance.
  - Quick check question: How does cross-entropy relate to probability distributions in classification tasks, and how can it be decomposed into multiple components?

- Concept: Gaussian distribution modeling for representation statistics
  - Why needed here: The paper assumes that classes tend to have single-peaked representations under adequate pre-training, allowing them to be approximated with Gaussian distributions for preservation and recovery.
  - Quick check question: Why are Gaussian distributions suitable for modeling class representations, and what properties of the representations justify this assumption?

- Concept: Prompt tuning vs. prefix tuning in vision transformers
  - Why needed here: The paper compares different prompt-based approaches (L2P, DualPrompt, S-Prompt, CODA-Prompt) that use different prompt architectures and tuning strategies.
  - Quick check question: What are the key differences between prompt tuning and prefix tuning, and how do they affect the incorporation of task-specific knowledge into instructed representations?

## Architecture Onboarding

- Component map:
  Pre-trained vision transformer backbone (frozen) -> Task-specific prompts (expandable pool) -> Auxiliary output layer for task-identity inference -> Final output layer for task-adaptive prediction -> Contrastive regularization module -> Prompt ensemble module

- Critical path:
  1. Construct task-specific prompt for new task
  2. Optimize prompt with WTP loss and CR
  3. Update auxiliary output layer for TII
  4. Update final output layer for TAP
  5. At test time, predict task identity and then label

- Design tradeoffs:
  - Joint vs. separate optimization of hierarchical components
  - Covariance vs. variance modeling for representation statistics
  - Number of centroids vs. single Gaussian per class
  - Strength of contrastive regularization

- Failure signatures:
  - Poor performance on new tasks despite good performance on previous tasks (catastrophic forgetting)
  - High variance in task-identity inference accuracy across tasks
  - Degraded task-adaptive prediction when task-identity is correctly inferred

- First 3 experiments:
  1. Implement basic prompt ensemble without CR and compare to baseline methods
  2. Add CR and evaluate its impact on WTP and TAP
  3. Implement separate optimization of TII and evaluate its contribution to overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HiDe-Prompt scale with different prompt architectures beyond the current implementations (e.g., LoRA, adapters, FiLM)?
- Basis in paper: [explicit] The paper mentions that their theoretical analysis and approach can serve as a general framework for parameter-efficient fine-tuning techniques beyond prompts.
- Why unresolved: The paper only implements and evaluates HiDe-Prompt using task-specific prompts. Other architectures like LoRA are only mentioned as potentially competitive but not thoroughly tested.
- What evidence would resolve it: Comprehensive experiments comparing HiDe-Prompt with different prompt architectures (LoRA, adapters, FiLM) on the same benchmarks and measuring performance differences.

### Open Question 2
- Question: What is the optimal balance between preserving pre-trained knowledge and allowing adaptation to downstream tasks in continual learning?
- Basis in paper: [inferred] The paper discusses freezing the transformer backbone to stabilize pre-trained knowledge, but also mentions this prevents updating the pre-trained knowledge.
- Why unresolved: The paper presents HiDe-Prompt with a frozen backbone but acknowledges this limitation. The trade-off between knowledge preservation and adaptation is not explicitly optimized.
- What evidence would resolve it: Experiments comparing different degrees of backbone freezing/fine-tuning (fully frozen, partially frozen, fully fine-tuned) and their impact on continual learning performance.

### Open Question 3
- Question: How does HiDe-Prompt perform on more diverse and challenging continual learning scenarios beyond class-incremental learning?
- Basis in paper: [explicit] The paper focuses on class-incremental learning and mentions other scenarios (task-incremental, domain-incremental) but only provides theoretical analysis for them.
- Why unresolved: All experiments are conducted on class-incremental learning benchmarks. The theoretical framework is provided for other scenarios but not empirically validated.
- What evidence would resolve it: Experiments applying HiDe-Prompt to task-incremental and domain-incremental learning scenarios on appropriate benchmarks, measuring performance across all three settings.

## Limitations
- The paper's claims about hierarchical decomposition effectiveness rely on assumptions about well-distributed representations that lack direct empirical support
- Key mechanisms like contrastive regularization and prompt ensemble strategies are not thoroughly validated through ablation studies
- The method only addresses class-incremental learning, leaving other scenarios (task-incremental, domain-incremental) unexplored

## Confidence
- High confidence: The experimental results demonstrating HiDe-Prompt's superior performance on standard benchmarks (Split CIFAR-100 and Split ImageNet-R)
- Medium confidence: The theoretical framework for decomposing the continual learning objective into WTP, TII, and TAP components
- Low confidence: The effectiveness of contrastive regularization and prompt ensemble strategies in preventing catastrophic forgetting and facilitating knowledge transfer

## Next Checks
1. Conduct ablation studies to isolate the contributions of contrastive regularization and prompt ensemble strategies to overall performance, comparing HiDe-Prompt variants with and without these components
2. Evaluate HiDe-Prompt's robustness across different pre-training paradigms by testing on a wider range of self-supervised methods (e.g., BYOL, SimCLR) and measuring the impact on hierarchical optimization effectiveness
3. Analyze the statistical properties of uninstructed and instructed representations across tasks using CKA similarity and t-SNE visualizations to validate the assumption of well-distributed representations under self-supervised pre-training