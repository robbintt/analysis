---
ver: rpa2
title: 'DTL: Disentangled Transfer Learning for Visual Recognition'
arxiv_id: '2312.07856'
source_url: https://arxiv.org/abs/2312.07856
tags:
- backbone
- uni00000013
- trainable
- parameters
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently fine-tuning large
  pre-trained models for visual recognition tasks. Current parameter-efficient transfer
  learning (PETL) methods struggle to reduce GPU memory usage significantly due to
  the entanglement of trainable parameters with the backbone network.
---

# DTL: Disentangled Transfer Learning for Visual Recognition

## Quick Facts
- arXiv ID: 2312.07856
- Source URL: https://arxiv.org/abs/2312.07856
- Reference count: 8
- Primary result: Achieves 77.9% average accuracy on VTAB-1K with 0.09M trainable parameters and 1.5GB GPU memory

## Executive Summary
This paper introduces Disentangled Transfer Learning (DTL), a parameter-efficient transfer learning method that addresses the GPU memory bottleneck in fine-tuning large pre-trained models. The key innovation is a Compact Side Network (CSN) that is disentangled from the backbone, using low-rank linear mappings to progressively extract task-specific information. DTL achieves state-of-the-art results on the VTAB-1K benchmark while using significantly fewer trainable parameters and GPU memory compared to existing PETL methods.

## Method Summary
DTL introduces a lightweight Compact Side Network (CSN) that operates independently from the backbone pre-trained model. The CSN uses low-rank linear mappings to extract task-specific information from each backbone block and progressively aggregates this information. Starting from a specific block M, the CSN output (processed by Swish activation and optionally depthwise separable convolution) is added back to the backbone to adapt features for downstream tasks. This disentangled design eliminates the need to store intermediate activations for gradient propagation, dramatically reducing GPU memory usage during training.

## Key Results
- Achieves 77.9% average accuracy on VTAB-1K with only 0.09M trainable parameters and 1.5GB GPU memory
- Outperforms previous state-of-the-art PETL methods (Adapter: 75.7% with 0.21M parameters and 4.9GB memory)
- Demonstrates effectiveness across diverse visual tasks including few-shot learning and domain generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disentangling trainable parameters from the backbone reduces GPU memory usage during training.
- Mechanism: By inserting a lightweight CSN that operates independently from the backbone and only interacts through addition at later stages, gradients don't need to be cached for the first M backbone blocks, reducing memory from storing intermediate activations.
- Core assumption: Storing intermediate activations for gradient propagation is the dominant memory cost in PETL.
- Evidence anchors:
  - [abstract] "This phenomenon happens because trainable parameters from these methods are generally entangled with the backbone, such that a lot of intermediate states have to be stored in GPU memory for gradient propagation."
  - [section] "To correctly calculate the gradients, except for parameters from the model (in this case, Wi and bi), all corresponding {σ′i} in the chain rule have to be cached during fine-tuning, which dominates the GPU memory usage."

### Mechanism 2
- Claim: Progressive task-specific information extraction with low-rank mappings is sufficient for effective transfer.
- Mechanism: CSN uses low-rank linear mappings to extract task-specific information from each backbone block and progressively aggregates it. The information is then added back to later backbone blocks to adapt features.
- Core assumption: The low-rank decomposition captures essential task-specific information while remaining parameter-efficient.
- Evidence anchors:
  - [abstract] "By progressively extracting task-specific information with a few low-rank linear mappings and appropriately adding the information back to the backbone, CSN effectively realizes knowledge transfer in various downstream tasks."
  - [section] "Denote wi = aici ∈ Rd×d as the weight matrix accounting for the i-th block, with ai ∈ Rd×d′, ci ∈ Rd′×d and d′ ≪ d, CSN progressively gathers information from each block as hi+1 = hi + ziwi"

### Mechanism 3
- Claim: The global depthwise separable convolution in DTL+ improves spatial information processing without significant parameter overhead.
- Mechanism: A shared global DWConv layer processes the spatial information after Swish activation before adding it back to the backbone, improving feature adaptation.
- Core assumption: Spatial information processing is important for visual recognition tasks and can be effectively shared across CSN layers.
- Evidence anchors:
  - [section] "To further boost the effectiveness of the proposed method, we append an additional global depthwise separable convolution (DWConv) layer (Chollet 2017) g to each side layer after θ is applied."
  - [section] "The introduction of g makes spatial information properly processed by our CSN module. With this operation, it's easier for the model to recognize new categories."

## Foundational Learning

- Concept: Parameter-efficient transfer learning (PETL)
  - Why needed here: DTL is a PETL method, so understanding the PETL landscape and limitations is essential
  - Quick check question: What is the main memory bottleneck in traditional PETL methods?

- Concept: Low-rank decomposition
  - Why needed here: CSN uses low-rank linear mappings (ai, ci) to create weight matrices wi = aici, which is fundamental to DTL's parameter efficiency
  - Quick check question: How does low-rank decomposition reduce the number of trainable parameters compared to full-rank matrices?

- Concept: Gradient checkpointing and memory optimization
  - Why needed here: DTL's memory efficiency relies on understanding when intermediate activations need to be cached for backpropagation
  - Quick check question: In what scenarios can intermediate activations be discarded without affecting gradient computation?

## Architecture Onboarding

- Component map:
  - Backbone: Frozen pre-trained model (e.g., ViT, Swin)
  - CSN: Series of low-rank linear mapping layers (ai, ci pairs)
  - Swish activation (θ): Applied to CSN output before addition
  - Optional DWConv (g): Global depthwise separable convolution in DTL+
  - Addition point: CSN output added to backbone output starting from block M

- Critical path:
  1. Input tokens → Backbone block 1 → CSN layer 1 → Extract task-specific info
  2. Repeat for blocks 2 to N-1
  3. From block M onward: Add CSN output (processed by θ and optionally g) to backbone output
  4. Final output → Classification head

- Design tradeoffs:
  - M (addition start point): Earlier addition provides more adaptation but uses more memory; later addition saves memory but may reduce effectiveness
  - Rank d': Lower values save parameters but may limit information capture; higher values improve capacity but increase parameters
  - DWConv inclusion: Improves spatial processing but adds slight complexity

- Failure signatures:
  - Accuracy degradation: Likely due to insufficient rank (d' too small) or M too large
  - Memory usage not reducing: Indicates CSN not properly disentangled or backbone still caching activations
  - Training instability: May indicate learning rate too high for the small CSN parameters

- First 3 experiments:
  1. Ablation study: Remove CSN entirely and compare to full fine-tuning to establish baseline improvement
  2. Vary M parameter: Test different addition start points (e.g., M=3, M=7, M=11) to find optimal memory-accuracy tradeoff
  3. Rank sensitivity: Test different low-rank values (d'=1, d'=2, d'=4) to find minimum effective rank for target tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of rank (d') in the low-rank linear mappings of CSN affect the trade-off between accuracy and computational efficiency?
- Basis in paper: [explicit] The paper mentions that a small d' (2 or 4) performs well and suggests high redundancy in backbone features, but it also notes that a higher or lower rank can make the accuracy worse.
- Why unresolved: While the paper provides some insights into the effects of d', it does not explore a comprehensive range of values or provide a detailed analysis of how different ranks impact the balance between accuracy and computational efficiency.
- What evidence would resolve it: A thorough ablation study exploring a wider range of d' values and their corresponding accuracy and computational efficiency would provide more insights into the optimal choice of rank.

### Open Question 2
- Question: How does the disentangled design of DTL affect the model's ability to capture complex interactions between different tasks in multi-task learning scenarios?
- Basis in paper: [inferred] The paper mentions the possibility of feature reuse in multi-task learning scenarios, but it does not explore this aspect in detail or compare the performance of DTL with other methods in multi-task learning settings.
- Why unresolved: The disentangled design of DTL may have implications for its ability to capture complex interactions between different tasks, but this aspect is not explored in the paper.
- What evidence would resolve it: Experiments comparing the performance of DTL with other methods in multi-task learning scenarios, particularly in terms of its ability to capture complex task interactions, would provide insights into this question.

### Open Question 3
- Question: How does the proposed DTL method generalize to other domains beyond computer vision, such as natural language processing or speech recognition?
- Basis in paper: [inferred] The paper focuses on computer vision tasks, but it mentions that the method is compatible with various backbone architectures, suggesting potential applicability to other domains.
- Why unresolved: While the paper demonstrates the effectiveness of DTL in computer vision tasks, it does not explore its performance in other domains or provide insights into its generalizability.
- What evidence would resolve it: Experiments applying DTL to tasks in other domains, such as natural language processing or speech recognition, and comparing its performance with existing methods would provide insights into its generalizability.

## Limitations

- The effectiveness of low-rank decomposition (d'=2 or 4) may not generalize to all visual recognition tasks or domains
- The optimal M parameter (addition start point) likely varies across tasks and requires task-specific tuning
- The paper focuses primarily on computer vision tasks and doesn't explore DTL's performance in other domains like NLP or speech

## Confidence

- **High confidence**: Memory efficiency claims (77.9% accuracy with 0.09M parameters and 1.5GB memory vs. 75.7% with 0.21M parameters and 4.9GB memory)
- **Medium confidence**: Mechanism effectiveness (progressive task-specific information extraction through low-rank mappings)
- **Medium confidence**: DWConv contribution (improves spatial information processing but absolute necessity unclear)

## Next Checks

1. **Memory breakdown analysis**: Profile GPU memory usage during training to verify that intermediate activation storage is indeed the dominant cost and that DTL's disentanglement effectively eliminates this overhead.

2. **Rank sensitivity validation**: Systematically test the DTL architecture across a broader range of visual tasks with varying rank values (d' = 1, 2, 4, 8) to determine the minimum effective rank and identify failure points.

3. **Backbone independence verification**: Test DTL on additional backbone architectures beyond ViT and Swin (e.g., ConvNeXt, ResNet) to confirm that the memory efficiency gains generalize across different network types and that the M parameter can be effectively optimized for each.