---
ver: rpa2
title: 'CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes
  with Large Language Models'
arxiv_id: '2302.12343'
source_url: https://arxiv.org/abs/2302.12343
tags:
- features
- have
- patient
- does
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce CHiLL (Crafting High-Level Latents), a method that
  uses zero-shot extraction with large language models to extract interpretable features
  from clinical notes for downstream predictive tasks. Specifically, CHiLL employs
  expert-crafted natural language prompts to elicit intermediate feature responses
  from Flan-T5.
---

# CHiLL: Zero-shot Custom Interpretable Feature Extraction from Clinical Notes with Large Language Models

## Quick Facts
- arXiv ID: 2302.12343
- Source URL: https://arxiv.org/abs/2302.12343
- Reference count: 32
- Zero-shot LLM extraction achieves comparable performance to models using ground truth ICD codes and TF-IDF features on MIMIC-III data

## Executive Summary
CHiLL (Crafting High-Level Latents) introduces a method for extracting interpretable clinical features from free-text notes using zero-shot large language models. The approach employs expert-crafted natural language prompts to elicit intermediate feature responses from Flan-T5-XXL, which are then used as inputs to simple linear models for clinical prediction. On standard clinical prediction tasks using MIMIC-III data, CHiLL achieves performance comparable to models using ground truth ICD codes while maintaining interpretability through inspectable feature weights that align with clinical expectations. The method demonstrates data and feature efficiency, requiring less training data and fewer features than traditional approaches to achieve similar performance.

## Method Summary
CHiLL uses zero-shot feature extraction with Flan-T5-XXL to process expert-crafted natural language prompts that ask binary questions about clinical features present in patient notes. The LLM generates either binary indicators or calibrated probability scores for each feature, which are then used as inputs to logistic regression models. The approach is evaluated on MIMIC-III data for readmission, mortality, and phenotype prediction tasks, as well as MIMIC-CXR for chest X-ray classification. The method compares performance against baselines using ground truth ICD codes, TF-IDF features, and BERT embeddings, while also validating interpretability through analysis of learned feature coefficients against clinical expectations.

## Key Results
- Zero-shot feature extraction with Flan-T5-XXL achieves AUROC scores comparable to models using ground truth ICD codes across multiple clinical prediction tasks
- Continuous feature calibration from LLM probabilities provides significant performance benefits over binary features alone
- Learned feature weights in linear models align with clinical expectations, demonstrating interpretability of the approach
- The method requires less training data and fewer features than TF-IDF to achieve similar performance levels

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot LLM feature extraction can produce clinically interpretable features without labeled training data by processing expert-crafted natural language prompts to infer binary indicators for high-level clinical features directly from free-text notes. The LLM must have sufficient domain knowledge encoded to understand clinical terminology and reasoning patterns.

### Mechanism 2
Linear models over LLM-extracted features achieve comparable performance to models using ground truth ICD codes because continuous feature calibration from LLM probabilities captures uncertainty and improves downstream prediction accuracy. The calibrated probabilities provide useful signal beyond binary classifications.

### Mechanism 3
Learned feature weights align with clinical expectations, providing interpretability because linear model coefficients over high-level features can be directly inspected and validated against domain expert knowledge. Expert-crafted features capture clinically meaningful concepts that correlate with prediction targets.

## Foundational Learning

- **Natural language prompt engineering for LLM inference**: Why needed here - Features must be specified as clear, unambiguous questions that LLMs can reliably answer. Quick check question - Can you write a prompt that would reliably extract "Does this patient have an enlarged heart?" from a clinical note?

- **Binary vs continuous feature encoding**: Why needed here - Understanding when to use binary indicators versus calibrated probability scores from LLM outputs. Quick check question - Why does using continuous features perform better than binary features in this system?

- **Linear model interpretability and coefficient analysis**: Why needed here - Feature weights must be inspectable to verify clinical validity of predictions. Quick check question - How would you determine if a learned feature coefficient aligns with clinical expectations?

## Architecture Onboarding

- **Component map**: Raw clinical notes → LLM feature extraction → Linear model prediction
- **Critical path**: Clinical notes → LLM feature extraction → Linear model prediction (LLM inference is the performance bottleneck and most error-prone component)
- **Design tradeoffs**: Zero-shot extraction vs. supervised training (no annotation cost but potentially lower accuracy), Binary vs. continuous features (simplicity vs. calibration benefits), Prompt specificity (more specific prompts may be more accurate but less reusable)
- **Failure signatures**: Low AUROC on feature extraction (LLM doesn't understand clinical concepts), Poor downstream performance (features aren't predictive of target), Coefficients don't align with expectations (features capture wrong concepts or spurious correlations)
- **First 3 experiments**: 1) Run feature extraction on 10 sample notes and manually verify if outputs match expectations, 2) Compare binary vs continuous feature performance on a small validation set, 3) Inspect top feature coefficients for a single prediction task and validate against clinical knowledge

## Open Questions the Paper Calls Out

### Open Question 1
How does the accuracy of zero-shot feature extraction using LLMs compare to supervised feature extraction methods in clinical settings? The paper provides a comparison but does not explore the potential of supervised methods or the extent to which zero-shot methods can be improved with minimal supervision.

### Open Question 2
What are the limitations of using natural language queries for feature extraction in diverse clinical scenarios? While the paper demonstrates the feasibility of this approach, it does not address potential issues such as query ambiguity, variability in clinical documentation styles, or the need for domain-specific adaptations.

### Open Question 3
How does the interpretability of models using LLM-extracted features compare to models using traditional feature extraction methods in terms of clinical decision-making? While the paper emphasizes interpretability, it does not evaluate how these features influence clinical decision-making or whether they provide actionable insights for clinicians.

## Limitations
- The approach depends on expert-crafted prompts that are not fully disclosed, making exact reproduction challenging
- Evaluation is limited to MIMIC-III data from a single healthcare system, raising questions about generalizability to other clinical settings
- The paper does not address potential biases in LLM feature extraction across diverse patient populations

## Confidence
- **High confidence**: The interpretability mechanism showing that learned feature weights align with clinical expectations is well-supported by manual validation of feature coefficients against domain knowledge
- **Medium confidence**: The claim of achieving comparable performance to ground truth ICD codes is reasonably supported by AUROC results, though performance differences across tasks suggest variability
- **Low confidence**: Data efficiency claims are based on limited comparisons without comprehensive ablation studies to isolate component contributions

## Next Checks
1. Conduct a systematic prompt ablation study to quantify how different prompt formulations affect feature extraction accuracy and downstream performance across multiple clinical tasks
2. Test the approach on external clinical datasets from different healthcare systems to assess generalizability and identify potential biases in feature extraction across diverse patient populations
3. Perform controlled experiments comparing zero-shot extraction performance across different LLM architectures (e.g., GPT, Claude, LLaMA) to determine if the approach is model-specific or generalizable