---
ver: rpa2
title: Dataset balancing can hurt model performance
arxiv_id: '2307.00079'
source_url: https://arxiv.org/abs/2307.00079
tags:
- evaluation
- balancing
- performance
- class
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Class imbalance in training data can lead to biased models. To
  improve performance on rare classes, several works have used dataset balancing techniques.
---

# Dataset balancing can hurt model performance

## Quick Facts
- arXiv ID: 2307.00079
- Source URL: https://arxiv.org/abs/2307.00079
- Reference count: 0
- Class imbalance in training data can lead to biased models, but balancing techniques are not universally effective and can harm performance.

## Executive Summary
This paper challenges the common assumption that dataset balancing always improves model performance on imbalanced datasets. Through systematic experiments on AudioSet, the authors demonstrate that balancing techniques can simultaneously improve performance on one evaluation set while degrading it on another. The study reveals that the benefits of balancing are fragile and depend on the specific evaluation set, rather than consistently improving rare-class performance. The findings caution against blind application of balancing techniques and suggest treating the degree of balancing as a hyperparameter to be tuned for each specific use case.

## Method Summary
The authors investigate dataset balancing using an Audio Spectrogram Transformer (AST) model, pretrained on a large audio dataset and fine-tuned on AudioSet with 527 classes. They implement a balancing scheme that varies the oversampling exponent β from 0 (no balancing) to 1 (full balancing), controlling how much underrepresented classes are oversampled during training. The experiments evaluate performance using macro-averaged mean average precision (mAP) and d′ metrics across multiple evaluation sets with different class distributions. The study systematically varies the degree of balancing and compares performance across public and internal evaluation sets to understand when and why balancing helps or harms.

## Key Results
- Balancing improved performance on the public AudioSet evaluation set but simultaneously worsened performance on an internal evaluation set collected under the same conditions
- The optimal degree of balancing (β) was non-zero but also not full balancing, indicating that intermediate balancing levels worked best
- There was no evidence that balancing specifically improved rare-class performance relative to common classes
- The benefits of balancing were highly dependent on the evaluation set's class prior distribution

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Balancing helps when the evaluation set has similar class priors to the training set.
- Mechanism: Oversampling rare classes during training makes them appear more frequently in mini-batches, increasing the model's effective exposure to rare-class examples.
- Core assumption: The evaluation set's class prior distribution closely matches the training set's.
- Evidence anchors:
  - [abstract] "while balancing improves performance on the public AudioSet evaluation data it simultaneously hurts performance on an unpublished evaluation set collected under the same conditions."
  - [section] "Unexpectedly, we find that balancing the dataset actually worsens performance on this second evaluation set."
- Break condition: If the evaluation set's class distribution differs substantially from the training set's, balancing can harm performance.

### Mechanism 2
- Claim: The optimal balancing exponent β depends on the evaluation set's class imbalance.
- Mechanism: By adjusting β, the degree of oversampling can be tuned to match the evaluation set's prior distribution, optimizing performance.
- Core assumption: There exists a non-linear relationship between β and evaluation set performance that can be optimized.
- Evidence anchors:
  - [section] "We investigated the best balancing scheme by computing evaluation metrics at several values of β. Figure 1 shows the observed dropoff of internal evaluation mAP from β = 0 to β = 1, and the corresponding increase in public evaluation mAP."
  - [section] "Partial balancing performed better than full balancing in both cases."
- Break condition: If the relationship between β and performance is not monotonic or is evaluation-set-specific, balancing may not generalize.

### Mechanism 3
- Claim: Balancing does not improve rare-class performance relative to common classes.
- Mechanism: Despite increased exposure to rare classes, the model does not allocate disproportionate capacity to rare-class examples.
- Core assumption: The model's capacity allocation is not significantly influenced by training set class frequency.
- Evidence anchors:
  - [section] "We also do not find evidence indicating that balancing improves rare class performance relative to common classes."
  - [section] "When we compared the per-class evaluation metrics between the models trained on the balanced (β = 0) and unbalanced (β = 1) training sets, the changes in per-class metrics were not correlated with the prior of the class in the training set."
- Break condition: If the model's architecture or training procedure inherently favors common classes, balancing may not overcome this bias.

## Foundational Learning

- Concept: Class imbalance and its impact on model performance.
  - Why needed here: Understanding how skewed class distributions affect learning is crucial for interpreting the paper's findings.
  - Quick check question: What is the imbalance ratio (ρ) and how is it calculated?

- Concept: Evaluation metrics for imbalanced datasets (mAP, AUC, d′).
  - Why needed here: These metrics are used to assess model performance, and their behavior under different balancing schemes is central to the paper's conclusions.
  - Quick check question: How does d′ differ from mAP, and why is it useful for comparing datasets with different class priors?

- Concept: Oversampling and its role in dataset balancing.
  - Why needed here: The paper's experiments involve varying the degree of oversampling (β) to study its effects on performance.
  - Quick check question: How does the oversampling exponent β modify the oversampling factor Mj in the paper's balancing scheme?

## Architecture Onboarding

- Component map: Audio data → AST model → Balancing (β) → Fine-tuning → Evaluation (mAP, d′)
- Critical path: Data preprocessing → Model pretraining → Dataset balancing (varying β) → Fine-tuning → Evaluation on multiple datasets
- Design tradeoffs: Balancing improves training convergence but may not generalize to evaluation sets with different class distributions. The choice of β is a hyperparameter that requires tuning.
- Failure signatures: If balancing consistently harms performance across multiple evaluation sets, it suggests that the model's capacity is not the limiting factor, or that the evaluation sets are too dissimilar.
- First 3 experiments:
  1. Train the model without balancing (β = 0) and evaluate on both the public and internal evaluation sets to establish baseline performance.
  2. Train the model with full balancing (β = 1) and evaluate on both sets to observe the impact of maximum oversampling.
  3. Train the model with partial balancing (β = 0.5) and evaluate on both sets to find the optimal balancing exponent for each evaluation set.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the underlying mechanism that explains why dataset balancing can hurt performance on certain evaluation sets?
- Basis in paper: [explicit] The paper demonstrates empirically that balancing can worsen performance on an internal evaluation set while improving it on the public set, but does not explain why this occurs.
- Why unresolved: The paper notes that the effect appears independent of class prior and shows no correlation between balancing benefits and class frequency, suggesting the mechanism is not simply about providing more training examples for rare classes.
- What evidence would resolve it: Controlled experiments varying evaluation set characteristics (e.g., class prior distribution, label noise, temporal structure) while holding training data constant could identify which properties make balancing beneficial or harmful.

### Open Question 2
- Question: What is the optimal degree of dataset balancing for a given evaluation set and metric?
- Basis in paper: [explicit] The paper shows that both full balancing and no balancing are suboptimal, with intermediate values of the oversampling exponent β yielding better performance, but the optimal β varies by evaluation set and metric.
- Why unresolved: The paper suggests that balancing should be treated as a hyperparameter but does not provide a systematic method for determining the optimal degree of balancing.
- What evidence would resolve it: Developing a principled approach to selecting the oversampling exponent based on evaluation set characteristics and model architecture could provide practical guidance.

### Open Question 3
- Question: How does dataset balancing interact with other training techniques like augmentation and pretraining?
- Basis in paper: [explicit] The paper notes that balancing significantly accelerates convergence during fine-tuning, but does not systematically explore interactions with other techniques like MixUp, SpecAugment, or audio pretraining.
- Why unresolved: While the paper briefly mentions that similar results were observed with ImageNet pretraining instead of audio pretraining, it does not provide a comprehensive analysis of how balancing interacts with the full training pipeline.
- What evidence would resolve it: Controlled ablation studies varying balancing, augmentation, and pretraining techniques independently could reveal which combinations are most effective and under what conditions.

## Limitations
- The findings are based on a single model architecture (AST) and dataset (AudioSet), limiting generalizability to other domains or architectures
- The study only examines one balancing approach (oversampling via exponent β) and does not explore alternative balancing methods
- Results are based on only two evaluation sets, which may not represent the full range of possible evaluation conditions

## Confidence
- **High Confidence**: The observation that balancing can simultaneously improve performance on one evaluation set while degrading it on another
- **Medium Confidence**: The assertion that balancing does not improve rare-class performance relative to common classes
- **Low Confidence**: The broader recommendation against blind application of balancing techniques across all machine learning contexts

## Next Checks
1. **Architecture Transfer Test**: Replicate the balancing experiments with a different model architecture (e.g., CNN-based vs. transformer-based) on AudioSet to determine if the observed effects are architecture-dependent.

2. **Cross-Domain Validation**: Apply the same balancing methodology to a different long-tailed classification dataset (e.g., iNaturalist or LVIS) to test whether the fragile balancing benefits generalize beyond audio data.

3. **Alternative Balancing Methods**: Implement and compare additional balancing approaches (class-weighted loss, focal loss, SMOTE) to determine whether the negative effects observed are specific to oversampling or represent a broader phenomenon in imbalanced learning.