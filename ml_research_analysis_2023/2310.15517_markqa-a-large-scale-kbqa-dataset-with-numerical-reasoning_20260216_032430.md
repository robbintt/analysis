---
ver: rpa2
title: 'MarkQA: A large scale KBQA dataset with numerical reasoning'
arxiv_id: '2310.15517'
source_url: https://arxiv.org/abs/2310.15517
tags:
- reasoning
- quantity
- p2403
- pyql
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of knowledge-based question
  answering (KBQA) with numerical reasoning, which combines multi-hop reasoning with
  arithmetic, aggregation, and comparison operations. The authors propose a new task,
  NR-KBQA, and introduce PyQL, a Python-based logic form that provides step-by-step
  reasoning paths and can be compiled into SPARQL queries.
---

# MarkQA: A large scale KBQA dataset with numerical reasoning

## Quick Facts
- arXiv ID: 2310.15517
- Source URL: https://arxiv.org/abs/2310.15517
- Reference count: 27
- Key outcome: Introduces MarkQA dataset with 31,902 examples combining multi-hop and numerical reasoning; achieves only 40.7% accuracy with state-of-the-art models

## Executive Summary
This paper introduces MarkQA, a large-scale dataset for Knowledge-Based Question Answering (KBQA) that combines multi-hop reasoning with numerical operations. The dataset contains 31,902 examples automatically scaled from 1K human-written seed questions, covering arithmetic, aggregation, and comparison operations over a Wikidata knowledge base. The authors propose PyQL, a Python-based logic form that provides step-by-step reasoning paths and can be directly compiled into SPARQL queries. Experiments show that NR-KBQA is highly challenging, with top models achieving only 40.7% accuracy, and that PyQL as output format improves results by up to 18.87% over SPARQL.

## Method Summary
The authors construct MarkQA using a Seeds-to-Forest framework that starts with 1K human-written questions and automatically generates 32K examples through paraphrasing and generalization. Each example includes a question, QDMR decomposition, PyQL program, and SPARQL query. The PyQL language represents reasoning as step-by-step Python commands that can be compiled to SPARQL, providing more concise and readable supervision than full SPARQL queries. They evaluate three baseline models (T5-base, GMT, QDTQA) on three generalization settings: I.I.D, Compositional, and Zero-shot splits.

## Key Results
- NR-KBQA task achieves only 40.7% accuracy with state-of-the-art models on the test set
- PyQL output format improves results by up to 18.87% compared to SPARQL
- Performance drops significantly on compositional (21.84%) and zero-shot (15.83%) settings
- MarkQA covers 24,188 distinct entities and 931 properties with an average PyQL length of 4.75 steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PyQL provides superior supervision compared to SPARQL for complex numerical reasoning
- Mechanism: PyQL represents reasoning as step-by-step Python commands that can be directly compiled to SPARQL, making it more concise and easier to parse than full SPARQL queries
- Core assumption: The step-by-step structure of PyQL captures the reasoning process more transparently than monolithic SPARQL
- Break Condition: If the step-by-step decomposition doesn't align with how humans naturally reason about numerical problems

### Mechanism 2
- Claim: Automatic dataset scaling from seed questions enables coverage of complex numerical reasoning patterns
- Mechanism: The Seeds-to-Forest framework generalizes seed questions by replacing entities with variables and composing questions with multi-hop reasoning through subgraph sampling
- Core assumption: Reasonable seed questions plus systematic generalization can generate a diverse dataset without manual annotation for each example
- Break Condition: If the generalization process introduces too many unnatural or ambiguous questions

### Mechanism 3
- Claim: The combination of multi-hop and numerical reasoning creates a combinatorial explosion that makes the task extremely challenging
- Mechanism: Each numerical operand may require multi-hop reasoning to retrieve, and mathematical operations can be composed in many ways, exponentially increasing possible query structures
- Core assumption: The difficulty comes from the interaction between graph pattern matching and numerical computation rather than either alone
- Break Condition: If the model can learn to decompose numerical problems into separate graph matching and calculation steps

## Foundational Learning

- Concept: Knowledge Graph Query Languages (SPARQL, PyQL)
  - Why needed here: Understanding the formal representation of queries is essential for creating and evaluating KBQA systems
  - Quick check question: Can you write a SPARQL query to find all movies directed by Christopher Nolan?

- Concept: Question Decomposition (QDMR)
  - Why needed here: Breaking complex questions into sub-questions is a key technique for handling compositional reasoning
  - Quick check question: How would you decompose "How many more students are in Stanford than MIT?" into sub-questions?

- Concept: Numerical Reasoning Operations
  - Why needed here: The dataset involves arithmetic, aggregation, and comparison operations that must be correctly parsed and executed
  - Quick check question: What's the difference between argmax and max operations in the context of question answering?

## Architecture Onboarding

- Component map: Seeds collection -> Paraphrasing -> Generalization -> Composition -> Model training -> Evaluation
- Critical path: Seed question annotation → PyQL/SPARQL generation → Dataset scaling → Model training → Evaluation
- Design tradeoffs: Automatic scaling vs. annotation quality; PyQL conciseness vs. SPARQL expressiveness; Wikidata coverage vs. query complexity
- Failure signatures: Poor performance on zero-shot setting indicates entity/relation linking issues; Grammar errors in generated PyQL suggest output format challenges; Low accuracy on compositional questions points to reasoning decomposition problems
- First 3 experiments: 1) Test PyQL generation on a small set of seed questions to verify compilation to valid SPARQL; 2) Evaluate entity linking accuracy on the test set to establish a baseline for performance gaps; 3) Compare model performance on numerical-only vs. numerical+multi-hop questions to isolate difficulty sources

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PyQL compare to other interpretable reasoning path representations like KoPL or S-expressions in terms of accuracy and readability?
- Basis in paper: The paper mentions that PyQL offers a more human-readable alternative to SPARQL and can easily be extended for further developments. It also shows that PyQL improves results by up to 18.87% over SPARQL.
- Why unresolved: The paper does not provide a direct comparison of PyQL with other interpretable reasoning path representations like KoPL or S-expressions.
- What evidence would resolve it: A comparative study between PyQL and other interpretable reasoning path representations in terms of accuracy and readability would resolve this question.

### Open Question 2
- Question: How does the complexity of numerical reasoning questions in MarkQA compare to other KBQA datasets in terms of the number of operations and the types of operators used?
- Basis in paper: The paper mentions that MarkQA requires not only the ability to interact with underlying knowledge sources but also the ability to perform complex numerical reasoning. It also provides statistics on the number of distinct entities, properties, and answers covered by MarkQA, as well as the average PyQL and question length.
- Why unresolved: The paper does not provide a direct comparison of the complexity of numerical reasoning questions in MarkQA with other KBQA datasets.
- What evidence would resolve it: A comparative study between MarkQA and other KBQA datasets in terms of the number of operations and the types of operators used would resolve this question.

### Open Question 3
- Question: How does the performance of MarkQA on different types of numerical reasoning questions (e.g., arithmetic, aggregation, comparison) compare to other KBQA datasets?
- Basis in paper: The paper mentions that MarkQA considers 15 types of operators, including 5 arithmetic operations, 5 aggregation operations, and 5 comparative operations. It also provides statistics on the percentage of questions that have at least one arithmetic, aggregation, or comparative operation.
- Why unresolved: The paper does not provide a direct comparison of the performance of MarkQA on different types of numerical reasoning questions with other KBQA datasets.
- What evidence would resolve it: A comparative study between MarkQA and other KBQA datasets in terms of the performance on different types of numerical reasoning questions would resolve this question.

## Limitations
- The dataset is automatically scaled from only 1K human-written seed questions, raising concerns about linguistic diversity
- The automatic scaling process may introduce distribution shift between seed questions and generated examples
- Performance on zero-shot setting (15.83%) suggests significant challenges with truly unseen entities and relations

## Confidence

- **High confidence**: The task definition of NR-KBQA is clearly specified and the dataset construction methodology is reproducible
- **Medium confidence**: PyQL provides better supervision than SPARQL for numerical reasoning tasks
- **Low confidence**: The dataset fully captures the complexity of real-world numerical reasoning over knowledge bases

## Next Checks

1. **Output Format Ablation**: Train the same model architecture using only SPARQL as output and compare with PyQL outputs to isolate the format's contribution to performance gains.

2. **Seed Question Quality Analysis**: Manually evaluate a random sample of 100 automatically generated questions against their seed questions to quantify linguistic drift and reasoning pattern changes.

3. **Zero-shot Generalization Test**: Create a small manually annotated test set of truly unseen numerical reasoning patterns to validate whether the zero-shot split results reflect genuine generalization or artifacts of the automatic scaling process.