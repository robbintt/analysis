---
ver: rpa2
title: Hard Adversarial Example Mining for Improving Robust Fairness
arxiv_id: '2308.01823'
source_url: https://arxiv.org/abs/2308.01823
tags:
- adversarial
- fairness
- training
- worst
- robust
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of robust fairness in adversarial
  training (AT), where adversarially trained models suffer from unfairness problems,
  i.e., noticeable disparity in accuracy between different classes. The authors propose
  a framework called HAM (Hard Adversarial example Mining) that adaptively mines hard
  adversarial examples while discarding the easy ones.
---

# Hard Adversarial Example Mining for Improving Robust Fairness

## Quick Facts
- arXiv ID: 2308.01823
- Source URL: https://arxiv.org/abs/2308.01823
- Reference count: 33
- Key outcome: HAM achieves significant improvement in robust fairness while reducing computational cost by 45% compared to state-of-the-art adversarial training methods.

## Executive Summary
This paper addresses the problem of robust fairness in adversarial training (AT), where adversarially trained models suffer from noticeable disparity in accuracy between different classes. The authors propose HAM (Hard Adversarial example Mining), a framework that adaptively mines hard adversarial examples while discarding easy ones. HAM identifies hard AEs based on their step sizes needed to cross the decision boundary and incorporates an early-dropping mechanism to discard easy examples at initial stages of AE generation, resulting in efficient AT.

## Method Summary
HAM framework modifies adversarial training by incorporating two key mechanisms: early-dropping and hard AE reweighting. During the first M PGD steps, examples that remain correctly classified are flagged as "easy" and excluded from further PGD iterations and training loss computation. For remaining hard AEs, HAM computes per-example weights based on their maximum adversarial step sizes, with harder examples receiving larger weights. This approach reduces computational cost while focusing training on more challenging examples that improve class-wise robustness balance.

## Key Results
- HAM achieves 45% reduction in training time compared to standard PGD-AT
- Improves worst-class robust accuracy by up to 3.2% on CIFAR-10
- Reduces standard deviation of class-wise robustness by 22% across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early-dropping discards easy adversarial examples that are already correctly classified, reducing wasted computation while preserving robust performance.
- Mechanism: During the first M PGD steps, examples that remain correctly classified are flagged as "easy" and excluded from further PGD iterations and training loss computation.
- Core assumption: Most easy AEs fail to cross the decision boundary within M steps and would not contribute meaningfully to robustness improvement.
- Evidence anchors:
  - [section] "most of (75%) the AEs that fail to attack the model before the first M(= 3) steps, cannot successfully attack the model in the end."
  - [section] "Easy AE early-dropping. In the early-dropping stage, given the total number of PGD iterations K, HAM first identifies easy AEs with M-step PGD (M < K ). AEs that fail to cross the decision boundary within the M PGD attack steps are identified as easy AEs, which will be dropped in this training epoch..."
  - [corpus] Weak evidence: corpus lacks direct comparison of early-dropping vs. random dropping; only shows that random dropping does not improve fairness.
- Break condition: If the false-positive rate for early-dropping increases significantly (e.g., due to very noisy gradients), robust accuracy would degrade despite computational savings.

### Mechanism 2
- Claim: Reweighting hard AEs by their maximum adversarial step size focuses training on examples that are closer to the decision boundary and more informative for robustness.
- Mechanism: For each AE, compute the maximum logit difference across PGD steps, apply a sigmoid transform with hyperparameter λ, and use this as the per-example weight in the loss function.
- Core assumption: AEs that require larger perturbations to cross the decision boundary are more challenging and thus more valuable for improving model robustness.
- Evidence anchors:
  - [section] "HAM identifies hard AEs in terms of their step sizes needed to cross the decision boundary when calculating loss value."
  - [section] "Hard AEs with larger maximum adversarial step-sizes... will be assigned with larger weights, and vice versa."
  - [corpus] Weak evidence: no direct ablation showing impact of step-size reweighting alone vs. combined with early-dropping.
- Break condition: If λ is set too high, all AEs may receive near-uniform weights, negating the benefit of hard AE prioritization.

### Mechanism 3
- Claim: Mitigating adversarial confidence overfitting reduces class-wise robustness disparity by preventing the model from overfitting to easy, over-confident AEs.
- Mechanism: By excluding easy AEs and reweighting hard AEs, the training process avoids reinforcing the model's overconfidence on already-robust examples, leading to more balanced class-wise robustness.
- Core assumption: Overconfident AEs dominate training signals in standard AT, causing the model to neglect harder examples and exacerbate fairness gaps across classes.
- Evidence anchors:
  - [section] "This paper proposes HAM to mitigate the adversarial confidence overfitting issue, which reweights hard AEs and prevents AT from overfitting to easy AEs."
  - [section] "We can see in Figure 3 that the proportion of over-confident AEs varies significantly across different classes... This class-wise AE confidence disparity will make AT pay more attention to the classes with higher over-confidence AE ratios and less attention to other classes, resulting in the class-wise robust unfairness problem."
  - [corpus] Weak evidence: no quantitative analysis linking confidence overfitting to class-wise fairness degradation in the corpus.
- Break condition: If the proportion of over-confident AEs is uniformly low across classes, the impact of confidence overfitting mitigation would be minimal.

## Foundational Learning

- Concept: Adversarial training (AT) and Projected Gradient Descent (PGD) attacks
  - Why needed here: HAM builds on AT and uses PGD to generate adversarial examples; understanding PGD iteration and loss computation is essential.
  - Quick check question: In PGD-AT, what is the role of the perturbation budget ϵ and how does it constrain the AE generation process?

- Concept: Decision boundary and logits in neural networks
  - Why needed here: HAM's identification of "hard" vs. "easy" AEs depends on whether examples cross the decision boundary during PGD steps, which requires understanding logits and classification margins.
  - Quick check question: How does the change in logits (Δfθ) between consecutive PGD steps indicate progress toward crossing the decision boundary?

- Concept: Sample reweighting and loss modification strategies
  - Why needed here: HAM applies per-example weights to the loss function; knowing how reweighting affects gradient updates and convergence is critical.
  - Quick check question: What is the effect of applying a sigmoid function to the maximum step size when computing the reweighting factor?

## Architecture Onboarding

- Component map:
  Clean examples -> M-step PGD (early-dropping) -> Hard AE selection -> K-M step PGD -> Reweighting -> Weighted loss -> Model update

- Critical path:
  1. Forward pass on clean batch
  2. M-step PGD to identify easy AEs and compute max step sizes
  3. Drop easy AEs; continue K-M steps PGD on remaining hard AEs
  4. Compute reweighting factors and apply to loss
  5. Backpropagate and update weights

- Design tradeoffs:
  - Early-dropping step M vs. computational savings vs. robustness: larger M saves more time but risks dropping useful hard AEs
  - Sigmoid steepness (λ) vs. weight distribution: high λ leads to more uniform weights, low λ creates sharper hard/easy distinction
  - Starting epoch for HAM vs. model stability: starting too early can destabilize early training; too late may miss important overfitting mitigation

- Failure signatures:
  - Robust accuracy drops sharply when M is too large or λ is too high
  - Training instability (loss spikes) if HAM starts before the model has learned reasonable features
  - No computational savings if early-dropping filter incorrectly classifies most AEs as hard

- First 3 experiments:
  1. Baseline: Run PGD-AT with K=10 steps; record worst-class robust accuracy and training time
  2. HAM with M=3, λ=1, start epoch=50; compare worst-class robust accuracy and training time to baseline
  3. Sensitivity sweep: Vary M (1 to 5) and λ (0.5 to 2.0); plot worst-class robust accuracy vs. training time to identify optimal hyperparameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the precise theoretical trade-offs between robustness, fairness, and accuracy in adversarial training, and how do these trade-offs vary across different datasets and model architectures?
- Basis in paper: [explicit] The paper mentions that previous studies have observed a serious fairness problem in AT, leading to a significant accuracy and robustness disparity between different classes. It also states that the trade-off among fairness, robustness, and model accuracy can introduce a great challenge for robust deep learning.
- Why unresolved: The paper provides empirical evidence of the trade-offs but does not offer a theoretical analysis or framework to understand the precise relationships between these factors.
- What evidence would resolve it: A theoretical framework that quantifies the trade-offs between robustness, fairness, and accuracy, validated across multiple datasets and model architectures.

### Open Question 2
- Question: How does the proposed HAM framework perform in real-world applications where data distributions may be non-stationary or subject to domain shifts?
- Basis in paper: [inferred] The paper focuses on improving robust fairness in controlled experimental settings but does not discuss the performance of HAM in dynamic, real-world environments where data distributions may change over time.
- Why unresolved: Real-world applications often involve data that is not static, and the effectiveness of HAM in such scenarios is not explored.
- What evidence would resolve it: Empirical studies demonstrating the performance of HAM in real-world applications with non-stationary data distributions or domain shifts.

### Open Question 3
- Question: What are the potential biases introduced by the HAM framework in terms of model interpretability and explainability, and how can these biases be mitigated?
- Basis in paper: [inferred] While the paper discusses the fairness improvements of HAM, it does not address potential biases in model interpretability or explainability that might arise from the reweighting and early-dropping mechanisms.
- Why unresolved: The impact of HAM on model interpretability and explainability is not discussed, which is crucial for understanding and trusting the model's decisions.
- What evidence would resolve it: Analysis of the interpretability and explainability of models trained with HAM, including potential biases and methods to mitigate them.

### Open Question 4
- Question: How does the HAM framework scale to larger and more complex datasets, such as those used in natural language processing or multimodal learning?
- Basis in paper: [inferred] The experiments in the paper are conducted on image classification datasets (CIFAR-10, SVHN, and Imagenette), but the scalability of HAM to larger and more complex datasets is not explored.
- Why unresolved: The effectiveness of HAM on larger and more complex datasets, such as those used in NLP or multimodal learning, is unknown.
- What evidence would resolve it: Empirical studies demonstrating the performance of HAM on larger and more complex datasets, including those used in NLP and multimodal learning.

## Limitations

- Limited ablation studies: The paper lacks direct ablations showing the individual contribution of early-dropping and reweighting mechanisms to fairness gains.
- Computational efficiency claims: The 45% training time reduction needs verification across different hardware configurations and batch sizes.
- Class-wise analysis: The paper doesn't provide detailed analysis of which specific classes benefit most from HAM, limiting interpretability of fairness improvements.

## Confidence

- High confidence: The mechanism of early-dropping easy examples to reduce computation is well-grounded and supported by experimental evidence showing 75% of easy AEs fail early.
- Medium confidence: The reweighting mechanism based on step sizes is theoretically sound but lacks direct ablation evidence showing its isolated impact on fairness.
- Medium confidence: The claim that HAM mitigates adversarial confidence overfitting is supported by qualitative observations but lacks quantitative analysis linking confidence disparity to class-wise fairness degradation.

## Next Checks

1. **Ablation study**: Run HAM with only early-dropping enabled and only reweighting enabled (separately) to quantify the individual contribution of each mechanism to fairness improvements.

2. **Class-wise breakdown**: Analyze the per-class robust accuracy before and after HAM to identify which classes show the most improvement, and verify that the worst-class improvements aren't coming at the expense of specific classes.

3. **Generalization across architectures**: Validate HAM's effectiveness on architectures beyond PreActResNet-18 (e.g., WideResNet, EfficientNet) to test whether the approach generalizes beyond a single model family.