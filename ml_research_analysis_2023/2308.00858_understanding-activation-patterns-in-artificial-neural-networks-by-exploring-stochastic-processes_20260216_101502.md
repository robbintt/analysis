---
ver: rpa2
title: Understanding Activation Patterns in Artificial Neural Networks by Exploring
  Stochastic Processes
arxiv_id: '2308.00858'
source_url: https://arxiv.org/abs/2308.00858
tags:
- networks
- process
- neural
- poisson
- stochastic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes modeling activation patterns of neural network
  nodes as stochastic processes, specifically Poisson arrival processes, to gain insights
  into generalization and memorization. By binarizing and analyzing activation frequencies,
  the authors derive parameters like mean firing rate (MFR) and mean Fano factor (MF)
  to characterize network behavior.
---

# Understanding Activation Patterns in Artificial Neural Networks by Exploring Stochastic Processes

## Quick Facts
- **arXiv ID:** 2308.00858
- **Source URL:** https://arxiv.org/abs/2308.00858
- **Reference count:** 40
- **Primary result:** Modeling ANN activation patterns as Poisson processes reveals stable indicators (MFR, MF) that distinguish memorizing from generalizing networks.

## Executive Summary
This paper proposes a novel approach to understanding neural network behavior by modeling activation patterns as stochastic processes, specifically Poisson arrival processes. By binarizing activations in the last hidden layer and treating them as spike trains, the authors derive interpretable statistics like Mean Firing Rate (MFR) and Mean Fano Factor (MF) that characterize network behavior. The methodology proves effective in distinguishing between generalization and memorization, with memorizing networks showing significantly lower MFR and higher MF compared to generalizing networks. This provides a principled framework for analyzing neural network behavior and has potential applications in network pruning, regularization, and transfer learning.

## Method Summary
The authors binarize activations in the last hidden layer of neural networks (MLP, CNN, DenseNet) trained on MNIST, FashionMNIST, and CIFAR-10. These binary activations are treated as spike trains and modeled as Poisson arrival processes. The Poisson parameters are estimated using maximum likelihood estimation, and statistics like MFR, MF, and coefficient of variation are calculated. The authors compare these metrics between networks trained for generalization (on original data) and memorization (on shuffled labels), showing that memorizing networks consistently exhibit lower MFR and higher MF.

## Key Results
- Memorizing networks show significantly lower Mean Firing Rate (MFR) compared to generalizing networks
- Memorizing networks exhibit higher Mean Fano Factor (MF) indicating increased variability
- The MFR and MF metrics remain stable indicators across different architectures (MLP, CNN, DenseNet) and datasets (MNIST, FashionMNIST, CIFAR-10)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Thresholding activations and modeling them as Poisson spike trains captures network behavior relevant to memorization vs generalization.
- Mechanism: By binarizing activations (0/1), each neuron's activation pattern becomes a spike train. These spike trains are then modeled as Poisson arrival processes, where the firing rate λ quantifies activation frequency. Lower λ indicates sparse activation (memorization), higher λ indicates frequent activation (generalization).
- Core assumption: Artificial neural network activations can be treated as independent, stationary, one-at-a-time events suitable for Poisson modeling.
- Evidence anchors:
  - [abstract] "We propose utilizing the framework of stochastic processes... Our approach models activation patterns of thresholded nodes... leveraging techniques commonly used in neuroscience to study spike trains observed in real neurons."
  - [section] "The key assumptions underlying the Poisson Process model are that spikes occur (1) one-at-a-time and are (2) independent as well as (3) stationary."
- Break condition: If activations exhibit strong temporal dependencies, non-stationarity, or simultaneous multi-neuron firing patterns that violate Poisson assumptions.

### Mechanism 2
- Claim: Mean Firing Rate (MFR) and Mean Fano Factor (MF) provide stable indicators distinguishing memorizing from generalizing networks.
- Mechanism: Memorizing networks show lower MFR (fewer nodes active) and higher MF (more variability per node) compared to generalizing networks. These statistics are derived from Poisson process parameters and remain stable across training epochs.
- Core assumption: The distribution of firing rates across nodes in a layer is sensitive to the memorization-generalization distinction.
- Evidence anchors:
  - [abstract] "We calculate several features, including Mean Firing Rate, Mean Fano Factor, and Variances, which prove to be stable indicators of memorization during learning."
  - [section] "Memorizing networks appear to show lower firing rates and higher Fano factors than the generalizing networks."
- Break condition: If MFR/MF distributions overlap significantly between memorizing and generalizing networks, or if they change drastically during training.

### Mechanism 3
- Claim: The Poisson process framework is more suitable for artificial neurons than biological neurons due to simpler dynamics.
- Mechanism: Unlike biological neurons with refractory periods and complex dynamics, artificial neurons are memoryless and have stationary firing rates when processing random inputs, aligning well with Poisson process assumptions.
- Core assumption: Artificial neural networks lack the temporal dependencies and non-linear dynamics that violate Poisson assumptions in biological systems.
- Evidence anchors:
  - [abstract] "Compared to real neurons, those simplified ANNs do more closely follow the core properties of the Poisson process."
  - [section] "Simple artificial neural networks (ANNs) without recurrent connections are considered memoryless... These networks exhibit stationary firing rates..."
- Break condition: If introducing recurrent connections or more complex activation functions breaks the memoryless and stationary assumptions.

## Foundational Learning

- Concept: Poisson Process
  - Why needed here: The entire methodology relies on modeling activation patterns as Poisson arrival processes to derive interpretable statistics.
  - Quick check question: What are the three key assumptions of a Poisson process, and why are they relevant for modeling ANN activations?

- Concept: Fano Factor
  - Why needed here: Fano Factor measures variability relative to mean, helping assess how well Poisson assumptions fit the data and distinguishing memorization patterns.
  - Quick check question: What does a Fano Factor of 1 indicate for a Poisson process, and why did the authors observe values below 1?

- Concept: Spike Train Analysis
  - Why needed here: Converting activation patterns to binary spike trains enables application of neuroscience techniques to understand ANN behavior.
  - Quick check question: Why does thresholding activations to binary values help in applying Poisson process modeling?

## Architecture Onboarding

- Component map: Data preprocessing -> Model construction -> Training pipeline -> Spike train extraction -> Stochastic modeling -> Analysis
- Critical path: Data → Model → Training → Spike Extraction → Poisson Fitting → Metric Calculation → Analysis
- Design tradeoffs:
  - Observing only the last hidden layer simplifies analysis but may miss memorization in earlier layers
  - Using Poisson processes provides interpretability but may not perfectly fit the data (lower Fano factors observed)
  - Training on shuffled labels forces memorization but creates an artificial scenario that may not reflect real overfitting
- Failure signatures:
  - MFR and MF distributions overlap significantly between memorizing and generalizing networks (indicators fail)
  - Fano factors far from 1 indicate poor Poisson fit, suggesting alternative models needed
  - Metrics change drastically during training, indicating instability
- First 3 experiments:
  1. Train a simple MLP on MNIST for generalization, extract spike trains from last layer, calculate MFR/MF, verify they fall in expected ranges
  2. Retrain the same MLP on shuffled MNIST labels, recalculate MFR/MF, confirm they differ significantly from generalization case
  3. Repeat experiment 2 but freeze all layers except the last, observe whether MFR/MF differences persist or diminish

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Poisson process model compare to other stochastic models (e.g., Hawkes processes, renewal processes) in characterizing activation patterns in ANNs?
- Basis in paper: [explicit] The paper mentions that the Poisson process is a baseline model and suggests investigating other process models in future work.
- Why unresolved: The paper only explores the Poisson process model and does not compare it to other stochastic models.
- What evidence would resolve it: Comparative analysis of Poisson process vs. other stochastic models on the same datasets and ANN architectures, evaluating goodness of fit and descriptive power.

### Open Question 2
- Question: How do the stochastic process parameters (e.g., mean firing rate, Fano factor) vary across different ANN architectures (e.g., transformers, recurrent networks) and tasks (e.g., NLP, reinforcement learning)?
- Basis in paper: [explicit] The paper focuses on CNNs and MLPs for image recognition tasks and suggests future studies should examine more complex models and tasks.
- Why unresolved: The paper's experiments are limited to specific architectures and tasks, leaving open the question of generalizability.
- What evidence would resolve it: Extending the stochastic modeling approach to a wider range of ANN architectures and tasks, comparing the derived parameters across different domains.

### Open Question 3
- Question: Can the stochastic process parameters be used as effective regularization terms or early stopping criteria in practical ANN training scenarios?
- Basis in paper: [explicit] The paper suggests potential applications in regularization and early stopping but does not provide empirical evidence.
- Why unresolved: The paper only presents theoretical arguments for the potential usefulness of the parameters, without demonstrating their effectiveness in practice.
- What evidence would resolve it: Experimental studies incorporating the stochastic process parameters into regularization or early stopping algorithms, comparing their performance to existing methods on various benchmarks.

## Limitations
- The methodology assumes Poisson process properties (independence, stationarity) that may not hold for real activation patterns
- Binarization threshold choice (50% of max activation) is arbitrary and could significantly impact results
- Only examines the last hidden layer, potentially missing memorization patterns in earlier layers
- Results based on synthetic memorization (shuffled labels) which may not reflect natural overfitting scenarios

## Confidence
- **High Confidence:** Memorizing networks show lower MFR and higher MF than generalizing networks
- **Medium Confidence:** Poisson process framework as suitable model for ANN activations
- **Low Confidence:** Indicators are "stable" across training epochs (limited temporal analysis, Fano factors deviating from 1)

## Next Checks
1. Test the methodology on naturally overfitting networks (without label shuffling) to verify if MFR/MF patterns hold in realistic scenarios
2. Apply the analysis to multiple layers simultaneously to determine if memorization manifests consistently across network depth or is localized
3. Perform sensitivity analysis on the binarization threshold to establish robustness of results to this critical preprocessing choice