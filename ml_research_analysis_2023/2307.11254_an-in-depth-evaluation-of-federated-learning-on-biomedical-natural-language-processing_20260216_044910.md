---
ver: rpa2
title: An In-Depth Evaluation of Federated Learning on Biomedical Natural Language
  Processing
arxiv_id: '2307.11254'
source_url: https://arxiv.org/abs/2307.11254
tags:
- learning
- data
- centralized
- fedavg
- single
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a systematic evaluation of federated learning
  (FL) on biomedical natural language processing (NLP) tasks. The study compares FL
  models with centralized learning and single-client learning on 8 biomedical corpora
  using 6 language models across 2 tasks: named entity recognition (NER) and relation
  extraction (RE).'
---

# An In-Depth Evaluation of Federated Learning on Biomedical Natural Language Processing

## Quick Facts
- arXiv ID: 2307.11254
- Source URL: https://arxiv.org/abs/2307.11254
- Reference count: 40
- Primary result: FL models consistently outperform single-client learning and sometimes match centralized learning performance on biomedical NLP tasks

## Executive Summary
This paper presents a systematic evaluation of federated learning (FL) on biomedical natural language processing tasks, comparing FL with centralized and single-client learning across 8 biomedical corpora using 6 language models for named entity recognition and relation extraction. The study demonstrates that FL consistently outperforms models trained on individual client data and sometimes matches the performance of centralized learning, particularly under IID data distributions. The research also reveals that larger pre-trained transformer-based models show greater resilience to increasing federation scale compared to classical models like BILSTM-CRF, and that FedProx with appropriate regularization can better handle non-IID data distributions.

## Method Summary
The study compares three learning schemes: federated learning (using FedAvg and FedProx algorithms), centralized learning, and single-client learning on 8 biomedical NLP datasets including 2018 n2c2, BC2GM, BC4CHEMD, JNLPBA, NCBI-disease, EUADR, and GAD. Data is split into train (80%), dev (10%), and test (10%) sets, with federated experiments simulating multiple clients by partitioning data. Six language models are evaluated: BERT, BlueBERT, BioBERT, ClinicalBERT, GPT-2, and BILSTM-CRF. Model performance is assessed using F1-score at macro average level with both strict and lenient match criteria for NER tasks and F1-score for RE tasks.

## Key Results
- FL models consistently outperform models trained on individual client data and sometimes match centralized learning performance
- Larger pre-trained transformer-based models exhibit greater resilience to increasing federation scale than classical models
- FedAvg performs closely to FedProx under IID settings, but FedProx shows better strict-match F1 scores in non-IID settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FedAvg closely approximates centralized learning performance when data is IID distributed
- Mechanism: IID data ensures that each client's local gradient is an unbiased estimate of the global gradient, so averaging client updates approximates the pooled dataset gradient
- Core assumption: IID data ensures similar gradient distributions across clients
- Evidence anchors:
  - [abstract]: "FL models consistently outperform LMs trained on individual client's data and sometimes match the model trained with polled data"
  - [section 3]: "FedAvg performs closely to FedProx under the non-IID setting and is almost non-distinguishable when data is IID distributed"
  - [corpus]: Weak; performance outcomes without direct gradient distribution evidence
- Break condition: Highly non-IID data causes gradient estimates to diverge, leading to accuracy gaps

### Mechanism 2
- Claim: Pre-trained transformer-based models are more resilient to increasing federation scale than classical models
- Mechanism: Transformers' self-attention layers capture long-range dependencies and benefit from large parameter counts, which help smooth over noisier, smaller per-client updates when federation size grows
- Core assumption: Larger models can better tolerate variance introduced by averaging updates from many smaller clients
- Evidence anchors:
  - [abstract]: "larger pre-trained transformer-based models exhibit greater resilience to the increasing scale of FL"
  - [section 3]: "Larger model tends to be more resilient to the increasing scale of FL... pre-trained transformer-based models exhibited greater resilience"
  - [corpus]: Weak; performance curves without explicit parameter count or variance analysis
- Break condition: Model size too small relative to federation scale causes sharp performance degradation

### Mechanism 3
- Claim: FedProx with appropriate μ balances local and global objectives, yielding better strict-match F1 scores in non-IID settings
- Mechanism: The proximal term penalizes deviation from the global model, mitigating drift caused by heterogeneous local data
- Core assumption: Tunable μ allows trade-off between local fit and global alignment, improving convergence under distribution shift
- Evidence anchors:
  - [abstract]: "a visible performance gap arises with non-IID data, varying across FL algorithms"
  - [section 3]: "FedAvg performs closely with FedProx when data is non-IID distributed and is mostly undistinguishable when data is IID distributed... FedProx gets better strict matching results than FedAvg, especially when data is non-IID distributed"
  - [corpus]: Weak; specific μ values mentioned but no corpus-level gradient alignment evidence
- Break condition: μ too high suppresses local adaptation; too low allows heterogeneity to dominate

## Foundational Learning

- Concept: IID vs non-IID data distributions
  - Why needed here: Determines how well local client gradients approximate global gradient; affects convergence and performance
  - Quick check question: If each client's data is a random sample from the global distribution, what is the expected relationship between local and global gradients?

- Concept: Federated averaging vs proximal regularization
  - Why needed here: Different aggregation strategies handle heterogeneity differently; FedAvg averages updates, FedProx adds regularization term
  - Quick check question: What happens to the FedAvg update if one client has data very different from the rest?

- Concept: Transformer self-attention and parameter scaling
  - Why needed here: Explains why larger transformer models are more robust to noisy federated updates
  - Quick check question: How does increasing model parameters affect the variance of averaged gradients in a federated setting?

## Architecture Onboarding

- Component map: Clients → Local training → Gradient/Model upload → Server aggregation → Global model broadcast → Repeat
- Critical path: Local dataset → Model forward/backward pass → Gradient upload → Server aggregation → Model download
- Design tradeoffs: Communication frequency vs. local computation; model size vs. federation scale resilience; FedAvg simplicity vs. FedProx heterogeneity handling
- Failure signatures: Degraded accuracy in non-IID splits; increased variance in client updates; slower convergence with larger federation sizes
- First 3 experiments:
  1. Run centralized vs. FedAvg on IID split of 2018 n2c2 with BERT; compare F1
  2. Vary federation size (2→10 clients) on BC2GM with BioBERT; record F1
  3. Compare FedAvg vs. FedProx on non-IID BC2GM/JNLPBA split; tune μ and report strict-match F1

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of federated learning models vary when using larger pre-trained models like GPT-3 compared to BERT-based models, and what are the computational and communication trade-offs involved?
- Basis in paper: [inferred] The paper mentions that larger pre-trained transformer-based models exhibit greater resilience to the increasing scale of FL, but it does not explore large language models (LLMs) like GPT-3 due to concerns about open-source availability, computational resources, and communication challenges
- Why unresolved: The paper explicitly states that it did not explore large language models due to concerns about open-source availability, computational resources, and communication challenges
- What evidence would resolve it: Empirical results comparing the performance of GPT-3 and BERT-based models in FL settings, along with an analysis of the computational and communication costs involved

### Open Question 2
- Question: How do personalized federated learning algorithms perform compared to FedAvg and FedProx in handling non-IID data distributions in biomedical NLP tasks?
- Basis in paper: [explicit] The paper mentions that it only studied two FL algorithms (FedAvg and FedProx) under a non-IID setting and acknowledges that personalized FL algorithms may handle distribution shift issues better
- Why unresolved: The paper explicitly states that it only studied two FL algorithms under a non-IID setting and acknowledges that personalized FL algorithms may handle distribution shift issues better
- What evidence would resolve it: Comparative results of personalized FL algorithms with FedAvg and FedProx in handling non-IID data distributions in biomedical NLP tasks

### Open Question 3
- Question: What are the practical challenges and opportunities of implementing federated learning in real-world biomedical NLP applications, and how can these be addressed?
- Basis in paper: [inferred] The paper mentions that future studies will focus on real-world implementation of FL and explore practical opportunities and challenges
- Why unresolved: The paper acknowledges that future studies will focus on real-world implementation of FL and explore practical opportunities and challenges
- What evidence would resolve it: Case studies or pilot implementations of FL in real-world biomedical NLP applications, along with an analysis of the challenges encountered and solutions proposed

## Limitations
- The study lacks explicit details on critical hyperparameters for FL algorithms (learning rate, communication rounds, batch size)
- Performance improvements shown are empirical without theoretical guarantees about when FL will approximate centralized learning
- The paper does not explore large language models like GPT-3 due to computational and communication constraints

## Confidence

- **High Confidence**: The general trend that FL outperforms single-client learning and sometimes matches centralized performance across the tested biomedical tasks
- **Medium Confidence**: The comparative resilience of transformer-based models versus classical models in large-scale FL settings, as the mechanism is supported but specific parameter count analysis is absent
- **Medium Confidence**: The performance gap between IID and non-IID data distributions varying across FL algorithms, as results are consistent but gradient distribution evidence is limited

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary learning rates, batch sizes, and communication rounds for FedAvg and FedProx to determine their impact on convergence and performance gaps between IID and non-IID settings

2. **Gradient Distribution Analysis**: Measure and visualize the variance of gradients across clients in both IID and non-IID settings to empirically validate the theoretical claims about gradient averaging approximations

3. **Scale Sensitivity Experiment**: Conduct controlled experiments varying federation sizes (2, 5, 10, 20 clients) with identical models and datasets to quantify the exact relationship between federation scale and model resilience, particularly for transformer versus classical models