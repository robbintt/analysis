---
ver: rpa2
title: Learning Rate Free Sampling in Constrained Domains
arxiv_id: '2305.14943'
source_url: https://arxiv.org/abs/2305.14943
tags:
- msvgd
- coin
- mirrored
- learning
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of learning rate free sampling
  in constrained domains, which is a challenging task in computational statistics
  and machine learning. The authors propose a suite of new particle-based algorithms
  that leverage coin betting ideas from convex optimisation and the viewpoint of constrained
  sampling as a mirrored optimisation problem on the space of probability measures.
---

# Learning Rate Free Sampling in Constrained Domains

## Quick Facts
- arXiv ID: 2305.14943
- Source URL: https://arxiv.org/abs/2305.14943
- Reference count: 40
- One-line primary result: Proposed algorithms achieve competitive performance with existing constrained sampling methods, without the need to tune any hyperparameters.

## Executive Summary
This paper introduces a suite of learning rate free particle-based algorithms for constrained sampling problems. The authors reformulate constrained sampling as a mirrored optimization problem on the space of probability measures, leveraging coin betting ideas from convex optimization. The proposed algorithms, including mirrored coin Wasserstein gradient descent, Coin MSVGD, Coin MLAWGD, and Coin MKSDD, eliminate the need for learning rate tuning while maintaining competitive performance. The framework unifies several existing constrained sampling algorithms and is demonstrated on various numerical examples including sampling from simplex targets, fairness-constrained Bayesian neural networks, and post-selection inference problems.

## Method Summary
The paper proposes learning rate free sampling algorithms for constrained domains by combining mirror descent theory with coin betting strategies. The key insight is to reformulate constrained sampling as an unconstrained optimization problem in a dual space via a mirror map, then apply coin betting updates in this dual space. The mirror map transforms the constrained domain into an unconstrained Euclidean space, where standard gradient-based methods can be applied. Coin betting strategies replace fixed learning rates with adaptive step sizes based on historical gradients, eliminating hyperparameter tuning. The algorithms are implemented as particle-based methods where each particle's update depends on both the target distribution and interactions with other particles via kernel functions.

## Key Results
- Energy distance and coverage metrics demonstrate competitive performance of proposed algorithms compared to existing constrained sampling methods
- Coin MSVGD and Coin MIED achieve good performance on simplex sampling without learning rate tuning
- The fairness-constrained Bayesian neural network experiment shows improved fairness metrics while maintaining accuracy
- Post-selection inference experiments demonstrate valid confidence sets for sparse regression problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm reformulates constrained sampling as a mirrored optimisation problem on the space of probability measures, allowing unconstrained algorithms to be applied in the dual space and then mapped back.
- Mechanism: By using a mirror map ∇ϕ: X → Rd, the constrained problem over P2(X) is transformed into an unconstrained problem over P2(Rd). The Wasserstein gradient flow in the dual space is computed and then mapped back to the primal space using ∇ϕ*, bypassing the need for direct constraint handling.
- Core assumption: The mirror map ∇ϕ is bijective and has a well-defined inverse ∇ϕ*, enabling exact mapping between primal and dual spaces.
- Evidence anchors:
  - [abstract] "...the viewpoint of constrained sampling as a mirrored optimisation problem on the space of probability measures."
  - [section 3.1] "Using the mirror map ∇ϕ: X → Rd, we can now reformulate the constrained sampling problem as the solution of a 'mirrored' version of the optimisation problem..."
  - [corpus] No direct corpus evidence found for this specific reformulation; claim is based on cited works [42, 79].
- Break condition: If the mirror map is not bijective or its inverse is not well-defined, the transformation breaks down and the algorithm cannot recover the primal-space samples.

### Mechanism 2
- Claim: Coin betting strategies eliminate the need for learning rate tuning by adapting the step size based on past gradients.
- Mechanism: The algorithm treats gradient descent as a betting game where the step size at each iteration is determined by the historical sum of gradients, similar to Krichevsky-Trofimov betting. This adaptive strategy converges without a predefined learning rate.
- Core assumption: The sequence of gradients (or their bounded versions) is available and can be normalized to fit the betting framework.
- Evidence anchors:
  - [abstract] "...coin betting ideas from convex optimisation..."
  - [section 4.1] "In this case, several further modifications are necessary... one now bets xt − x0 rather than xt..."
  - [corpus] Weak corpus evidence; the betting strategy is adapted from [68, 78] but no direct citations in the abstract.
- Break condition: If gradients are unbounded or the normalization constant is unknown and cannot be estimated adaptively, the betting strategy may fail.

### Mechanism 3
- Claim: Mirrored Wasserstein gradient flows preserve KL divergence and χ² divergence dissipation properties from unconstrained flows.
- Mechanism: The KL divergence between the current measure and the target decreases along the mirrored flow at a rate proportional to the squared mirrored Stein Fisher information, ensuring convergence to the target.
- Core assumption: The target distribution satisfies a mirrored Stein log-Sobolev or Poincaré inequality, ensuring exponential convergence.
- Evidence anchors:
  - [section B.2] "Proposition 10...Assume that π satisfies the mirrored Stein log-Sobolev inequality...Then KL(µt||π) ≤ e−2λtKL(µ0||π)."
  - [section B.1] "Proposition 5...Assume that π satisfies the following mirrored log-Sobolev inequality..."
  - [corpus] No direct corpus evidence for mirrored Stein inequalities; based on [31, 51] for Stein log-Sobolev and [42] for mirror log-Sobolev.
- Break condition: If the target does not satisfy the required functional inequality, convergence guarantees are lost and only sub-exponential rates remain.

## Foundational Learning

- Concept: Wasserstein gradient flows and optimal transport
  - Why needed here: The algorithm relies on evolving probability measures along Wasserstein gradient flows in both primal and dual spaces.
  - Quick check question: What is the continuity equation governing the evolution of a probability measure along a Wasserstein gradient flow?

- Concept: Mirror descent and convex duality
  - Why needed here: The algorithm uses mirror maps to transform constrained problems into unconstrained ones in a dual space.
  - Quick check question: What properties must a mirror map ∇ϕ have to ensure a valid bijection between primal and dual spaces?

- Concept: Coin betting and adaptive gradient methods
  - Why needed here: The algorithm replaces fixed learning rates with adaptive betting strategies derived from coin betting theory.
  - Quick check question: How does the Krichevsky-Trofimov betting strategy determine the bet size based on past outcomes?

## Architecture Onboarding

- Component map:
  - Mirror map module: Handles the bijection between constrained domain X and unconstrained dual space Rd.
  - Wasserstein gradient module: Computes gradients of the objective functional in the dual space.
  - Coin betting module: Maintains betting fractions and updates particles without learning rates.
  - Kernel module: Provides positive semi-definite kernels for SVGD-type interactions.
  - Integration module: Combines mirror mapping, gradient computation, and coin betting into the update rule.

- Critical path:
  1. Initialize particles and map to dual space using mirror map.
  2. Compute Wasserstein gradient (or its kernelized approximation) in dual space.
  3. Apply coin betting update rule to determine particle positions.
  4. Map particles back to primal space using inverse mirror map.
  5. Repeat until convergence.

- Design tradeoffs:
  - Using a mirror map adds computational overhead but enables handling of complex constraints.
  - Coin betting eliminates hyperparameter tuning but requires tracking historical gradients.
  - Kernelized gradients enable flexible interactions but increase computational cost to O(N²).

- Failure signatures:
  - Particles leave the constraint domain → mirror map or its inverse is incorrectly implemented.
  - Slow or no convergence → kernel bandwidth too large/small, or mirror map poorly suited to constraints.
  - Numerical instability → gradients unbounded, coin betting fractions diverging.

- First 3 experiments:
  1. Test on a simple constrained distribution (e.g., uniform on simplex) with known mirror map to verify basic functionality.
  2. Compare convergence with and without coin betting on a log-concave target to isolate the effect of learning rate elimination.
  3. Test on a constrained Bayesian neural network with fairness constraints to evaluate practical performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions can we establish convergence guarantees for mirrored coin sampling algorithms like Coin MSVGD, similar to those obtained for their learning-rate-dependent counterparts?
- Basis in paper: [explicit] The authors note that establishing convergence of coin Wasserstein gradient descent remains an open problem, even in the unconstrained setting. They provide a technical sufficient condition but note it's difficult to verify in general. They also state that establishing convergence of mirrored coin sampling under more standard conditions, e.g., a mirrored log-Sobolev inequality, remains an open problem.
- Why unresolved: The coin sampling framework introduces additional complexity due to the adversarial nature of the "coin betting" setup, making it challenging to apply standard convergence analysis techniques used for gradient-based methods.
- What evidence would resolve it: A rigorous convergence proof for mirrored coin sampling algorithms under verifiable conditions (e.g., specific assumptions on the target distribution and kernel properties) would resolve this question. This could involve extending existing techniques for analyzing coin betting algorithms to the constrained sampling setting or developing new analytical tools.

### Open Question 2
- Question: How does the choice of mirror map impact the performance of mirrored coin sampling algorithms, and are there principled ways to select an appropriate mirror map for a given constrained sampling problem?
- Basis in paper: [explicit] The authors note that, like any mirrored sampling algorithm, mirrored coin sampling necessarily depends on the availability of a mirror map that can appropriately capture the constraints of the problem at hand.
- Why unresolved: The choice of mirror map is crucial for ensuring that the algorithm respects the constraints and converges to the correct target distribution. However, there is no systematic approach for selecting an appropriate mirror map, and the optimal choice may depend on the specific problem structure.
- What evidence would resolve it: Empirical studies comparing the performance of mirrored coin sampling algorithms using different mirror maps for various constrained sampling problems would provide insights into the impact of the mirror map choice. Additionally, theoretical analysis characterizing the properties of different mirror maps and their suitability for specific types of constraints could guide the selection process.

### Open Question 3
- Question: Can the coin sampling framework be extended to other particle-based variational inference methods beyond those considered in this paper, such as mirrored versions of Laplacian adjusted Wasserstein gradient descent (MLAWGD) or kernel Stein discrepancy descent (MKSDD)?
- Basis in paper: [explicit] The authors introduce mirrored coin sampling algorithms for MSVGD, MLAWGD, and MKSDD, demonstrating their effectiveness in various constrained sampling problems.
- Why unresolved: While the authors provide a general framework for extending coin sampling to mirrored sampling algorithms, it remains unclear whether this framework can be successfully applied to other particle-based variational inference methods or if there are limitations to its applicability.
- What evidence would resolve it: Developing and evaluating mirrored coin sampling algorithms for other particle-based variational inference methods, such as those based on different divergence measures or optimization objectives, would provide insights into the generalizability of the framework. Theoretical analysis of the convergence properties and limitations of these extended algorithms would also be valuable.

## Limitations
- Convergence guarantees rely on strong assumptions about the target distribution satisfying specific functional inequalities that may not hold in practice
- Mirror maps must be well-defined bijections, but the paper doesn't address cases where inverses are computationally intractable
- Coin betting introduces new hyperparameters related to the betting framework that are not thoroughly explored

## Confidence

- High Confidence: The reformulation of constrained sampling as a mirrored optimization problem (Mechanism 1) is well-supported by the mathematical derivations and aligns with established results in optimal transport and mirror descent theory.
- Medium Confidence: The coin betting mechanism for eliminating learning rate tuning (Mechanism 2) is theoretically sound but relies on specific assumptions about gradient normalization that may not always hold in practice.
- Low Confidence: The convergence rate guarantees under mirrored Stein inequalities (Mechanism 3) depend on strong assumptions about the target distribution that are not empirically validated across the numerical examples.

## Next Checks

1. Validate Mirror Map Invertibility: For each numerical example, explicitly verify that the chosen mirror map is bijective and that its inverse can be computed accurately, particularly for complex constraints like fairness conditions in Bayesian neural networks.

2. Test Coin Betting Robustness: Systematically evaluate the coin betting algorithm's performance when gradients are unbounded or when the normalization constant must be estimated adaptively, to identify failure modes not captured in the theoretical analysis.

3. Empirical Convergence Rate Analysis: For each numerical example, measure the empirical convergence rate of the proposed algorithms and compare it against the theoretical predictions under different assumptions about the target distribution's functional inequalities.