---
ver: rpa2
title: Contrastive Multi-view Subspace Clustering of Hyperspectral Images based on
  Graph Convolutional Networks
arxiv_id: '2312.06068'
source_url: https://arxiv.org/abs/2312.06068
tags:
- clustering
- learning
- information
- subspace
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CMSCGC, a contrastive multi-view subspace clustering
  approach for hyperspectral images using graph convolutional networks. It extracts
  textural and spatial-spectral features as two views, constructs graph convolutional
  subspaces, and applies contrastive learning to maximize consistency between views
  and learn robust features.
---

# Contrastive Multi-view Subspace Clustering of Hyperspectral Images based on Graph Convolutional Networks

## Quick Facts
- arXiv ID: 2312.06068
- Source URL: https://arxiv.org/abs/2312.06068
- Reference count: 40
- Key outcome: CMSCGC achieves 97.61%, 96.69%, 87.21%, and 97.65% overall accuracy on four HSI datasets

## Executive Summary
This paper proposes CMSCGC, a contrastive multi-view subspace clustering approach for hyperspectral images using graph convolutional networks. The method extracts textural and spatial-spectral features as two views, constructs graph convolutional subspaces, and applies contrastive learning to maximize consistency between views and learn robust features. An attention-based fusion module integrates affinity matrices from both views. Experiments on four HSI datasets show CMSCGC significantly outperforms state-of-the-art methods, achieving overall accuracies of 97.61%, 96.69%, 87.21%, and 97.65% respectively.

## Method Summary
CMSCGC extracts textural and spatial-spectral features from hyperspectral images as two complementary views. These views are used to construct graph convolutional subspaces through GCN layers that learn self-expression coefficients. Contrastive learning maximizes consistency between node representations across views, while an attention-based fusion module adaptively combines affinity matrices. Spectral clustering is then applied to the fused affinity matrix for final clustering results.

## Key Results
- Achieved 97.61% overall accuracy on Indian Pines dataset
- Achieved 96.69% overall accuracy on Pavia University dataset
- Outperformed state-of-the-art methods by significant margins on all four tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning between views maximizes consistency of node representations, leading to more discriminative features.
- Mechanism: By treating node representations from different views as positive pairs and random nodes as negative pairs, the model is trained to minimize distance between positive pairs and maximize distance from negatives. This enforces that the same physical object in different views (e.g., textural vs. spatial-spectral) maps to similar feature representations.
- Core assumption: Different views contain complementary but consistent information about the same underlying objects.
- Evidence anchors:
  - [abstract] "To maximize the interaction between different views, a contrastive learning algorithm was introduced to promote the consistency of positive samples and assist the model in extracting robust features."
  - [section III.C] "To facilitate the subsequent steps, we represent the features of the nodes as z. [...] maximize the consistency between the same nodes in different views or nodes from the same land-cover category can be maximized, enhancing the feature-learning capabilities and facilitating more robust clustering."
  - [corpus] Weak evidence. No corpus neighbors specifically discuss multi-view contrastive learning in hyperspectral clustering.
- Break condition: If views contain conflicting information or if one view is significantly noisier than the other, the contrastive objective may force the model to merge incompatible representations.

### Mechanism 2
- Claim: GCN-based self-expression captures both global graph structure and local neighborhood information for affinity matrix learning.
- Mechanism: GCN layers aggregate features from neighboring nodes, encoding topological relationships. The self-expression coefficient matrix is learned over these aggregated features, producing a robust affinity matrix that reflects both spectral similarity and spatial context.
- Core assumption: The KNN graph topology adequately represents similarity relationships between samples.
- Evidence anchors:
  - [abstract] "Pixel neighbor textural and spatial-spectral information were sent to construct two graph convolutional subspaces to learn their affinity matrices."
  - [section III.B] "The purpose of graph convolutional self-expression is to reconstruct the original data using a self-expressive dictionary matrix Z. As Z includes the global structural information, a clearer dictionary can be obtained, generating a robust affinity matrix."
  - [corpus] No direct corpus support for GCN-based self-expression in HSI subspace clustering.
- Break condition: If the KNN graph construction fails to capture true similarities (e.g., due to high dimensionality or noise), the GCN layers may propagate erroneous relationships.

### Mechanism 3
- Claim: Attention-based fusion adaptively combines affinity matrices from multiple views, emphasizing the most informative view per sample.
- Mechanism: An attention module learns per-view weights by computing similarities between affinity matrices and applying softmax normalization. This produces a weighted combination of affinity matrices that highlights the most discriminative information.
- Core assumption: Different views contribute differently to clustering accuracy depending on the local data structure.
- Evidence anchors:
  - [abstract] "An attention-based fusion module was used to adaptively integrate these affinity matrices, constructing a more discriminative affinity matrix."
  - [section III.D] "We utilized an attention-based fusion module to learn the importance ap of each view [...] The weight matrix can be obtained to realize the fusion operation, and fused self-expression matrix YF is [...]"
  - [corpus] No corpus neighbors discuss attention-based fusion for multi-view HSI clustering.
- Break condition: If the attention mechanism overfits to training data or fails to generalize across views, the fusion may degrade clustering performance.

## Foundational Learning

- Concept: Graph Convolutional Networks
  - Why needed here: GCNs can capture neighborhood relationships in the graph-structured data derived from HSI patches, which CNNs cannot directly exploit.
  - Quick check question: Can you explain how a GCN layer updates node features using the adjacency matrix and degree matrix?

- Concept: Subspace Clustering
  - Why needed here: HSI data lie in a union of low-dimensional subspaces; subspace clustering exploits this structure to group similar pixels.
  - Quick check question: What is the self-expression property, and how does it relate to spectral clustering?

- Concept: Contrastive Learning
  - Why needed here: Contrastive learning enforces consistency across views and improves feature robustness by pulling together positive pairs and pushing apart negatives.
  - Quick check question: In the contrastive loss formula, what role does the temperature parameter Ï„ play?

## Architecture Onboarding

- Component map: Multi-view graph construction -> GCN layers -> Contrastive learning module -> Self-expression layer -> Attention-based fusion -> Spectral clustering
- Critical path: Multi-view construction -> GCN aggregation -> Contrastive loss minimization -> Self-expression -> Attention fusion -> Final affinity matrix
- Design tradeoffs:
  - Using multiple views increases robustness but adds computational complexity.
  - Attention fusion introduces learnable parameters but risks overfitting.
  - Contrastive learning requires careful construction of positive/negative pairs.
- Failure signatures:
  - Degraded accuracy if one view is noisy or uninformative.
  - Over-smoothing in GCN if neighborhood size is too large.
  - Poor clustering if attention weights collapse to a single view.
- First 3 experiments:
  1. Test single-view clustering (textural only) vs. multi-view to confirm complementarity benefit.
  2. Vary the number of nearest neighbors k to study graph construction sensitivity.
  3. Disable contrastive learning to measure its impact on feature consistency and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed CMSCGC framework perform on even larger-scale hyperspectral datasets beyond the four benchmark datasets tested in this study?
- Basis in paper: [explicit] The paper mentions that the self-expression layer makes it difficult to train the model on large-scale datasets and suggests exploring the potential of CMSCGC using large-scale datasets in future work.
- Why unresolved: The current experiments are limited to four specific benchmark datasets, and the scalability and performance of the model on larger, more diverse datasets remain untested.
- What evidence would resolve it: Conducting experiments on additional, larger hyperspectral datasets with varying characteristics (e.g., different resolutions, spectral ranges, and scene complexities) and comparing the performance metrics (OA, NMI, Kappa) with those obtained on the current datasets.

### Open Question 2
- Question: What is the impact of using different types of textural features (e.g., Gabor filters, local binary patterns) on the clustering performance of CMSCGC?
- Basis in paper: [inferred] The paper mentions extracting textural information as one of the views but does not specify the type of textural features used or explore the impact of different textural feature extraction methods.
- Why unresolved: The choice of textural features can significantly influence the quality of the multi-view representation and, consequently, the clustering performance. The paper does not provide insights into the sensitivity of the model to different textural feature types.
- What evidence would resolve it: Performing experiments using various textural feature extraction methods and comparing the clustering performance (OA, NMI, Kappa) of CMSCGC with each type of feature.

### Open Question 3
- Question: How does the performance of CMSCGC compare to other state-of-the-art deep learning-based clustering methods specifically designed for hyperspectral images, such as those using transformers or other advanced architectures?
- Basis in paper: [explicit] The paper compares CMSCGC with several state-of-the-art clustering methods, including some deep learning-based approaches like GR-RSCNet and EGCSC, but does not mention other advanced architectures like transformers.
- Why unresolved: The field of deep learning for hyperspectral image clustering is rapidly evolving, and there might be other methods using more advanced architectures that could potentially outperform CMSCGC.
- What evidence would resolve it: Conducting a comprehensive comparison of CMSCGC with other state-of-the-art deep learning-based clustering methods, including those using transformers or other advanced architectures, on the same benchmark datasets and evaluating their performance using standard metrics (OA, NMI, Kappa).

## Limitations
- Lack of ablation studies to isolate contribution of each component (contrastive learning, attention fusion, multi-view construction)
- No analysis of computational complexity or scalability to larger datasets
- Absence of comparison with recent deep clustering methods beyond traditional baselines

## Confidence
- **High Confidence**: The overall experimental results showing improved clustering accuracy over traditional methods are well-supported by the data.
- **Medium Confidence**: The proposed mechanisms (contrastive learning, GCN-based self-expression, attention fusion) are theoretically sound, but their specific implementations and contributions are not fully validated through ablation studies.
- **Low Confidence**: The claim that all three components are equally necessary for the reported performance improvements, as no systematic ablation analysis was provided.

## Next Checks
1. Conduct ablation studies by systematically removing each component (contrastive learning, attention fusion, multi-view approach) to quantify their individual contributions to performance.
2. Perform sensitivity analysis on key hyperparameters (KNN graph size, number of GCN layers, contrastive learning temperature) to assess robustness and generalization.
3. Test the method on additional HSI datasets with different characteristics (spatial resolution, spectral bands, land-cover types) to evaluate scalability and domain adaptation capabilities.