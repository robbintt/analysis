---
ver: rpa2
title: 'HyPoradise: An Open Baseline for Generative Speech Recognition with Large
  Language Models'
arxiv_id: '2309.15701'
source_url: https://arxiv.org/abs/2309.15701
tags:
- arxiv
- speech
- language
- hypotheses
- correction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first open-source benchmark, HyPoradise,
  for utilizing large language models (LLMs) to correct errors in automatic speech
  recognition (ASR) by directly predicting true transcriptions from N-best hypotheses.
  The benchmark includes a novel dataset, HyPoradise (HP), with over 334,000 pairs
  of N-best hypotheses and corresponding accurate transcriptions across various speech
  domains.
---

# HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models

## Quick Facts
- **arXiv ID**: 2309.15701
- **Source URL**: https://arxiv.org/abs/2309.15701
- **Reference count**: 40
- **Primary result**: First open-source benchmark for LLM-based ASR error correction using N-best hypotheses, achieving significant WER reduction

## Executive Summary
This paper introduces HyPoradise, the first open-source benchmark for using large language models (LLMs) to correct errors in automatic speech recognition (ASR) by predicting true transcriptions from N-best hypotheses. The benchmark includes a novel dataset with over 334,000 pairs of N-best hypotheses and accurate transcriptions across various speech domains. Three error correction techniques based on LLMs are examined: zero-shot learning, few-shot learning, and fine-tuning. The results show significant word error rate (WER) reduction, with the proposed technique surpassing traditional re-ranking methods.

## Method Summary
The method uses N-best hypotheses from ASR systems as context for LLM-based error correction. Three approaches are evaluated: zero-shot learning using task-activated in-context learning, few-shot learning with in-domain demonstrations, and fine-tuning using either full-parameter fine-tuning (H2T-ft) or low-rank adaptation (H2T-LoRA). The framework leverages the complementary information in N-best hypotheses to predict corrected transcriptions that can even include tokens missing from all hypotheses. The HyPoradise dataset was created using WavLM and Whisper ASR models to generate N-best lists, which are then corrected using various LLMs including GPT-3.5, LLaMA, and T5.

## Key Results
- Significant WER reduction achieved across multiple speech domains using LLM-based correction
- Proposed generative correction approach surpasses the upper bound of traditional re-ranking methods
- LLMs can correct tokens missing from all N-best hypotheses, not just selecting from available options
- Fine-tuning methods (especially H2T-LoRA) show better performance than zero-shot approaches

## Why This Works (Mechanism)

### Mechanism 1
LLMs can correct ASR errors by leveraging N-best hypotheses as context. The N-best hypotheses provide diverse textual representations of the same speech input. LLMs use this ensemble information to infer correct transcriptions beyond what any single hypothesis provides. The core assumption is that N-best hypotheses contain complementary information where some hypotheses may have correct tokens that others miss. Evidence shows that discarded utterances can contain better candidates with lower WER, and other discarded hypotheses can provide right answers for wrong tokens. This mechanism breaks down if N-best hypotheses are too similar (low diversity).

### Mechanism 2
LLM generative correction can fix tokens missing from all N-best hypotheses. LLMs use linguistic knowledge and contextual reasoning to infer correct tokens that weren't present in any hypothesis, going beyond simple selection/re-ranking. The core assumption is that LLMs possess sufficient linguistic knowledge to reconstruct missing tokens based on context and pronunciation patterns. Evidence shows that LLM can correct missing tokens exclusive from hypotheses list in terms of context information. This mechanism breaks down if missing tokens require domain-specific knowledge not captured in LLM pre-training.

### Mechanism 3
Task-activated prompting improves LLM zero-shot/few-shot performance. By providing task descriptions and in-domain demonstrations, LLMs better understand the correction task and apply relevant linguistic knowledge. The core assumption is that LLMs can extract task patterns from demonstrations and apply them to new examples. Evidence shows that task-activated descriptions in pipeline improve performance. This mechanism breaks down if demonstrations are too few or too dissimilar to test examples.

## Foundational Learning

- **Concept**: Beam search decoding and N-best hypotheses generation
  - **Why needed here**: Understanding how N-best hypotheses are generated and why they contain useful information
  - **Quick check**: What information does each hypothesis in the N-best list represent, and why might later hypotheses contain better tokens than the first?

- **Concept**: Word Error Rate (WER) calculation
  - **Why needed here**: Primary metric for evaluating ASR performance and correction effectiveness
  - **Quick check**: How is WER calculated, and why is it used instead of other metrics like BLEU or TER?

- **Concept**: In-context learning and prompting
  - **Why needed here**: Core technique for using LLMs without parameter tuning
  - **Quick check**: What's the difference between zero-shot, few-shot, and fine-tuning approaches when using LLMs?

## Architecture Onboarding

- **Component map**: ASR system (WavLM/Whisper) → N-best hypotheses generation → LLM (GPT-3.5, LLaMA, T5) → Error correction → Evaluation pipeline → WER calculation → Dataset pipeline → HyPoradise creation and management

- **Critical path**: ASR decoding → N-best list creation → LLM correction → WER evaluation

- **Design tradeoffs**:
  - Using 5-best vs more hypotheses: More hypotheses = more information but higher computational cost
  - Zero-shot vs fine-tuning: Zero-shot requires no training data but may be less accurate; fine-tuning requires data but can achieve better performance
  - LLM size vs efficiency: Larger models may perform better but are more computationally expensive

- **Failure signatures**:
  - Overfitting: Training WER much lower than validation WER
  - Poor generalization: Good performance on training domains but poor on new domains
  - Hallucination: LLM generates plausible but incorrect transcriptions not supported by hypotheses

- **First 3 experiments**:
  1. Run ASR with beam search (size=60) on a test utterance and verify 5-best hypotheses contain diverse tokens
  2. Apply zero-shot prompting with GPT-3.5 to a sample hypothesis list and evaluate WER improvement
  3. Fine-tune T5 on a small subset of HyPoradise and compare performance to zero-shot approach

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal strategy for selecting in-domain demonstration examples for few-shot learning in LLM-based ASR error correction? The paper mentions that demonstration selection could impact inference results but doesn't provide specific strategies or evaluations of different selection methods. This remains unresolved as the authors acknowledge this as future work and note that recent work has focused on demonstration selection strategies but don't explore this themselves. Comparative studies evaluating different demonstration selection strategies (random sampling, length-based, error rate-based, etc.) on the HP dataset showing which method yields best few-shot learning performance would resolve this question.

### Open Question 2
How can acoustic information be better integrated into LLM-based ASR error correction to prevent "over-correction" while maintaining spoken language fidelity? The authors observe that LLM correction sometimes produces grammatically correct but acoustically implausible results and suggest this as future work. The current framework doesn't incorporate token-level confidence scores or other acoustic features from the ASR system, which the authors identify as a limitation. Experimental results comparing LLM-based correction with and without acoustic feature integration (confidence scores, phoneme alignments, etc.) showing improved WER and reduced "over-correction" cases would resolve this question.

### Open Question 3
What is the minimum LLM parameter scale required for effective zero-shot ASR error correction, and how does performance scale with model size? The authors report that zero-shot correction with T5 (0.75B) and LLaMA (13B) failed on CHiME-4, and speculate that 100B+ parameter models might be needed. The authors only tested two models and didn't systematically evaluate the relationship between model scale and zero-shot performance. Comprehensive experiments across a range of LLM sizes (1B to 175B+ parameters) on the HP dataset showing the minimum effective scale and performance scaling curves would resolve this question.

## Limitations

- Dataset quality and domain coverage remain unclear despite containing over 334,000 pairs
- Results primarily focus on specific ASR models (WavLM and Whisper) and LLMs, limiting generalizability
- Study doesn't extensively explore scenarios where LLM corrections might introduce new errors or hallucinate content

## Confidence

- **Claim Cluster 1: LLM-based error correction effectiveness** - High confidence. The paper provides substantial quantitative evidence (WER reduction metrics) across multiple domains and approaches.
- **Claim Cluster 2: N-best hypotheses provide complementary information** - Medium confidence. While supported by analysis, the depth of investigation into hypothesis diversity and information complementarity is limited.
- **Claim Cluster 3: LLMs can correct tokens missing from N-best lists** - Low confidence. This is presented as a surprising finding but lacks extensive validation and mechanistic explanation.

## Next Checks

1. **Dataset Quality Analysis**: Conduct a systematic analysis of the HyPoradise dataset to quantify domain distribution, error type coverage, and N-best hypothesis diversity. This would validate whether the dataset adequately represents real-world ASR challenges.

2. **Cross-Model Generalization Test**: Evaluate the error correction performance using a different ASR model (e.g., DeepSpeech or ESPnet) and LLM (e.g., GPT-4 or Claude) to assess the generalizability of the findings beyond the specific models used in the study.

3. **Error Introduction Analysis**: Design experiments to measure the rate at which LLM-based corrections introduce new errors or hallucinate content. This would provide a more complete picture of the correction system's reliability and safety.