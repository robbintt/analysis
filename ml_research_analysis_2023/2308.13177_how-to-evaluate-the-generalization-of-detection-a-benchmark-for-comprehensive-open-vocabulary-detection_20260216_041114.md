---
ver: rpa2
title: How to Evaluate the Generalization of Detection? A Benchmark for Comprehensive
  Open-Vocabulary Detection
arxiv_id: '2308.13177'
source_url: https://arxiv.org/abs/2308.13177
tags:
- object
- labels
- dataset
- negative
- predictions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OVDEval, a new benchmark for comprehensive
  evaluation of open-vocabulary detection (OVD) models. The benchmark includes 9 sub-tasks
  testing generalization over object types, commonsense knowledge, attributes, position,
  object relations, and negation.
---

# How to Evaluate the Generalization of Detection? A Benchmark for Comprehensive Open-Vocabulary Detection

## Quick Facts
- arXiv ID: 2308.13177
- Source URL: https://arxiv.org/abs/2308.13177
- Reference count: 35
- Primary result: Introduces OVDEval benchmark and NMS-AP metric to comprehensively evaluate open-vocabulary detection models

## Executive Summary
This paper addresses the challenge of comprehensively evaluating open-vocabulary detection (OVD) models by introducing OVDEval, a new benchmark designed to test model generalization across 9 sub-tasks involving object types, commonsense knowledge, attributes, position, relations, and negation. The benchmark features carefully designed hard negatives to challenge models' true understanding of visual and linguistic input. The authors also identify a critical problem with traditional Average Precision (AP) metrics when evaluating models on fine-grained label datasets, proposing a new metric called Non-Maximum Suppression Average Precision (NMS-AP) that removes redundant predictions via class-ignored NMS. Experimental results demonstrate that existing top OVD models fail on most new tasks except for simple object types, validating the benchmark's effectiveness in revealing model weaknesses and guiding future research.

## Method Summary
The paper introduces OVDEval, a benchmark containing 9 sub-datasets designed to comprehensively evaluate open-vocabulary detection models across 6 linguistic aspects: object, proper noun, attribute, position, relationship, and negation. Each sub-dataset contains 20K images with 3K labels carefully annotated to include hard negative samples that challenge models' semantic understanding. The paper also proposes NMS-AP, a new metric that addresses the inflated AP problem by applying class-ignored NMS to remove redundant bounding box predictions with IoU > 0.5 with ground truth before calculating AP. The evaluation framework tests 6 existing OVD models (Detic, MDETR, GLIP, FIBER, Grounding DINO, OmDet) using both traditional AP and the proposed NMS-AP metric, with analysis focusing on confidence score distributions and aspect-wise performance comparison.

## Key Results
- Existing top OVD models fail on OVDEval tasks except for simple object types, demonstrating the benchmark's effectiveness in revealing model weaknesses
- Traditional AP metrics yield significantly inflated scores compared to NMS-AP, with some models showing 10-20% difference in performance
- Models perform worst on negation and attribute tasks, with confidence score distributions showing less confident predictions for these aspects compared to object detection
- NMS-AP provides a more truthful evaluation of OVD models on intricate linguistic descriptions, whereas traditional AP metrics are deceptive

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The NMS-AP metric fixes the inflated AP problem by removing redundant bounding box predictions with the same object via class-ignored NMS.
- Mechanism: During AP calculation, NMS-AP applies class-ignored NMS to predictions that have IoU > 0.5 with ground truth. This removes duplicate boxes for the same object, ensuring only the highest confidence box remains for AP scoring.
- Core assumption: Models will predict multiple boxes for the same object with different labels, and this redundancy inflates AP when not addressed.
- Evidence anchors:
  - [abstract]: "we identify a problem with the popular Average Precision (AP) metric when benchmarking models on these fine-grained label datasets and propose a new metric called Non-Maximum Suppression Average Precision (NMS-AP) to address this issue."
  - [section]: "We identify the inflated AP problem that applies to any OVD model with traditional AP metric."
- Break condition: If models stop generating multiple boxes per object, or if IoU threshold changes significantly, the effectiveness may diminish.

### Mechanism 2
- Claim: OVDEval's 9 sub-tasks with hard negatives effectively reveal model weaknesses in fine-grained linguistic understanding.
- Mechanism: Each sub-dataset contains carefully designed hard negative labels that are linguistically similar but semantically different from ground truth. This forces models to truly understand fine-grained details rather than rely on superficial cues.
- Core assumption: Models can achieve high scores on traditional datasets by exploiting data bias, but will fail when faced with hard negatives requiring true semantic understanding.
- Evidence anchors:
  - [abstract]: "The dataset is meticulously created to provide hard negatives that challenge models' true understanding of visual and linguistic input."
  - [section]: "OVDEval is specifically designed with linguistic queries, including commonsense knowledge-based labels, which enable us to assess the model's commonsense capabilities."
- Break condition: If models learn to exploit the specific hard negative patterns, or if the linguistic diversity decreases, the benchmark may become less effective.

### Mechanism 3
- Claim: The combination of OVDEval benchmark and NMS-AP metric provides a comprehensive evaluation framework for OVD models.
- Mechanism: OVDEval tests multiple linguistic aspects (object, proper noun, attribute, position, relationship, negation) with hard negatives, while NMS-AP ensures truthful AP calculation by addressing the inflated AP problem. Together they reveal both model capabilities and calculation accuracy.
- Core assumption: Traditional evaluation methods are insufficient because they don't test fine-grained understanding or account for AP inflation.
- Evidence anchors:
  - [abstract]: "Extensive experimental results show that existing top OVD models all fail on the new tasks except for simple object types, demonstrating the value of the proposed dataset in pinpointing the weakness of current OVD models."
  - [section]: "Using NMS-AP to evaluate OVD models on our benchmark provides a more suitable approach for assessing their performance on intricate linguistic descriptions."
- Break condition: If new evaluation metrics emerge that address both linguistic understanding and AP calculation issues more effectively.

## Foundational Learning

- Concept: Intersection-over-Union (IoU) and its role in object detection evaluation
  - Why needed here: IoU is fundamental to understanding how predictions are matched with ground truth in both AP and NMS-AP calculations.
  - Quick check question: If a predicted box has IoU of 0.7 with ground truth, is it considered a true positive at the standard 0.5 threshold?

- Concept: Non-Maximum Suppression (NMS) algorithm
  - Why needed here: NMS is the core mechanism in NMS-AP for removing redundant predictions, and understanding its standard vs. class-ignored variants is crucial.
  - Quick check question: In standard NMS, do we consider class labels when suppressing boxes, or only confidence scores and IoU?

- Concept: Precision-Recall curves and Average Precision calculation
  - Why needed here: Understanding how AP is traditionally calculated helps explain why the inflated AP problem occurs and how NMS-AP addresses it.
  - Quick check question: Does AP calculation consider only the highest confidence prediction per object, or all predictions that meet the IoU threshold?

## Architecture Onboarding

- Component map: Image collection from COCO, HICO, VG, Laion-400m → annotation → sub-dataset creation → model evaluation (both AP and NMS-AP) → analysis and visualization
- Critical path: Image collection → annotation → sub-dataset creation → model evaluation (both AP and NMS-AP) → analysis and visualization
- Design tradeoffs: Hard negatives improve benchmark quality but increase annotation complexity; NMS-AP is simple but may not capture all edge cases
- Failure signatures: High AP but low NMS-AP indicates inflated scores; poor performance on specific aspects reveals model weaknesses
- First 3 experiments:
  1. Run traditional AP vs. NMS-AP comparison on a small subset to verify the inflated AP problem exists
  2. Evaluate a single model across all 9 sub-datasets to identify aspect-wise strengths/weaknesses
  3. Generate confidence score distributions for object vs. negation tasks to validate the hypothesis about model understanding differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we incorporate better training objectives so OVD models can acquire better discriminate abilities against hard negatives in both visual and linguistic input?
- Basis in paper: [explicit] The authors mention this as a future research direction, noting that current models struggle with hard negatives and complex descriptions.
- Why unresolved: Current models fail to accurately comprehend detailed descriptions and struggle with hard negative samples, indicating a need for improved training objectives.
- What evidence would resolve it: Developing and testing new training objectives that specifically address the handling of hard negatives and complex linguistic descriptions, and evaluating their impact on model performance.

### Open Question 2
- Question: What are the better pre-training data to inject more common sense knowledge in vision-language alignment?
- Basis in paper: [explicit] The authors suggest that current models have limited capability in utilizing common sense knowledge for detecting objects like landmarks, logos, and celebrities.
- Why unresolved: Existing models struggle with tasks requiring external knowledge and common sense, indicating a need for improved pre-training data.
- What evidence would resolve it: Identifying and incorporating pre-training data that better captures common sense knowledge and evaluating its impact on model performance in tasks requiring such knowledge.

### Open Question 3
- Question: How can we improve the evaluation metrics for OVD models to better capture their performance on intricate linguistic descriptions?
- Basis in paper: [explicit] The authors propose the NMS-AP metric to address the inflated AP problem and improve the evaluation of OVD models on fine-grained descriptions.
- Why unresolved: Traditional AP metrics can be misleading for OVD models, as they fail to capture the accuracy of descriptive labels assigned to objects.
- What evidence would resolve it: Developing and validating alternative evaluation metrics that consider both object detection and contextual understanding of linguistic descriptions, and comparing their effectiveness in assessing model performance.

## Limitations

- The NMS-AP metric's effectiveness may be limited if model architectures change significantly or if the IoU threshold is modified
- The benchmark's hard negatives, while carefully designed, may introduce evaluation bias toward specific linguistic patterns
- The 20K-image dataset size may not fully capture the diversity of real-world open-vocabulary detection scenarios

## Confidence

- High Confidence: The identification of the inflated AP problem in fine-grained datasets is well-supported by experimental evidence
- Medium Confidence: The effectiveness of the 9 sub-tasks in revealing model weaknesses is supported by results
- Low Confidence: The generalizability of NMS-AP across all open-vocabulary detection scenarios is assumed but not extensively tested

## Next Checks

1. Cross-Architecture Validation: Test NMS-AP's effectiveness on additional OVD model architectures to confirm the inflated AP problem is universal across different model families
2. Dataset Diversity Assessment: Evaluate OVDEval's benchmark performance on datasets with varying linguistic complexity and domain specificity to test its generalizability
3. Longitudinal Study: Track model performance on OVDEval over time as new OVD models are developed to verify whether the benchmark continues to reveal meaningful weaknesses