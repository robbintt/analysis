---
ver: rpa2
title: Recurrent Neural Language Models as Probabilistic Finite-state Automata
arxiv_id: '2310.05161'
source_url: https://arxiv.org/abs/2310.05161
tags:
- state
- hrnn
- construction
- matrices
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the representational capacity of recurrent neural
  network (RNN) language models (LMs) by relating them to weighted finite-state automata
  (WFSAs). The authors establish an equivalence between deterministic probabilistic
  FSAs (DPFSAs) and Heaviside-activated Elman RNN LMs, showing they define the same
  class of probability distributions over strings.
---

# Recurrent Neural Language Models as Probabilistic Finite-state Automata

## Quick Facts
- arXiv ID: 2310.05161
- Source URL: https://arxiv.org/abs/2310.05161
- Reference count: 40
- Key outcome: This paper establishes equivalence between Heaviside-activated Elman RNN language models and deterministic probabilistic finite-state automata (DPFSAs), and analyzes the space complexity of simulating DPFSAs with RNNs.

## Executive Summary
This paper bridges the gap between recurrent neural network (RNN) language models and weighted finite-state automata (WFSAs) by proving that simple Heaviside-activated Elman RNNs are equivalent to deterministic probabilistic finite-state automata (DPFSAs). This equivalence reveals that the class of probability distributions over strings that can be represented by these RNNs is precisely the class of distributions defined by DPFSAs. The authors generalize Minsky's construction for simulating unweighted finite-state automata with RNNs to the weighted case, establishing both theoretical foundations and practical implications for understanding the representational capacity of RNNs.

## Method Summary
The paper establishes equivalence between Heaviside-activated Elman RNN language models and DPFSAs through mathematical proofs showing that RNNs can simulate DPFSAs and vice versa. The analysis focuses on the space complexity of representing DPFSAs with RNNs, establishing that simulating general DPFSAs requires Ω(|Σ||Q|) neurons where |Σ| is the alphabet size and |Q| is the number of states. The authors also identify special cases, such as n-gram language models, where compression to O(log|Σ||Q|^(3/4)) neurons is possible through log |Σ|-separability transformations.

## Key Results
- Heaviside-activated Elman RNNs are equivalent to deterministic probabilistic finite-state automata in their representational capacity
- Simulating general DPFSAs requires Ω(|Σ||Q|) neurons in an HRNN
- Under log |Σ|-separability, DPFSAs can be compressed to O(log|Σ||Q|^(3/4)) neurons

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heaviside-activated Elman RNNs are equivalent to deterministic probabilistic finite-state automata (DPFSAs) in their representational capacity.
- Mechanism: The HRNN's hidden states encode state-symbol pairs from the DPFSA, with transitions simulated via element-wise conjunction using the Heaviside activation.
- Core assumption: Hidden states live in a finite set of binary configurations (BD), allowing one-to-one mapping with DPFSA states.
- Evidence anchors:
  - [abstract] "We show that simple RNNs are equivalent to a subclass of probabilistic finite-state automata"
  - [section 4.1] "We construct a deterministic PFSA A... Let s: BD → Z2D be a bijection... for every state q = sphq ∈ Q = Z2D"
  - [corpus] Weak (no direct mention of DPFSAs in related papers)
- Break condition: If the Heaviside activation is replaced with a continuous activation, the binary state mapping breaks and the equivalence may no longer hold.

### Mechanism 2
- Claim: Simulating a general DPFSAs requires Ω(|Σ||Q|) neurons in an HRNN.
- Mechanism: Each DPFSA state defines independent conditional distributions over symbols, requiring separate columns in the output matrix E, leading to |Q| columns each of size |Σ|.
- Core assumption: Conditional distributions of DPFSA states are arbitrary and not linearly related.
- Evidence anchors:
  - [section 5.1] "These distributions... can generally be linearly independent... the columns of the output matrix E, therefore, have to span R|Q|"
  - [section 5.2] "the bottleneck comes from the requirement of weak equivalence... the issue intuitively arises in the fact that... the local probability distributions of the individual states in a PFSA are completely arbitrary"
  - [corpus] Weak (related papers discuss representational capacity but not the specific Ω(|Σ||Q|) bound)
- Break condition: If the DPFSA's transition matrix is low-rank, it can be decomposed into smaller matrices, potentially reducing the HRNN size.

### Mechanism 3
- Claim: Under log |Σ|-separability, DPFSAs can be compressed to O(log|Σ||Q|^(3/4)) neurons.
- Mechanism: The log |Σ|-separation procedure transforms a general deterministic FSA into a log |Σ|-separable one, allowing Indyk's construction to be applied, achieving compression.
- Core assumption: log |Σ|-separability holds for certain classes of WFSAs, including n-gram LMs.
- Evidence anchors:
  - [section 5.2] "we identify classes of WFSAs, including n-gram LMs, where the space complexity scales logarithmically with |Σ|"
  - [appendix C] "log |Σ|-separability... is met by classical n-gram LMs"
  - [corpus] Weak (no direct mention of log |Σ|-separability in related papers)
- Break condition: If the DPFSAs are not log |Σ|-separable, the compression is not possible and the linear scaling holds.

## Foundational Learning

- Concept: Probabilistic Finite-State Automata (PFSAs)
  - Why needed here: Understanding the class of probability distributions over strings that RNNs can represent.
  - Quick check question: What is the difference between a PFSA and a general WFSA?

- Concept: Heaviside Activation Function
  - Why needed here: The Heaviside activation enables the binary state representation necessary for equivalence with DPFSAs.
  - Quick check question: How does the Heaviside function differ from the sigmoid or ReLU activation functions?

- Concept: Space Complexity Analysis
  - Why needed here: Determining the minimal number of neurons required to simulate DPFSAs with HRNNs.
  - Quick check question: What is the relationship between the size of a DPFSAs and the minimal HRNN required to simulate it?

## Architecture Onboarding

- Component map:
  - Input: Symbol representation function r: Σ → R
  - Hidden state: ht = σ(Uht-1 + Vr(yt) + b), where σ = H (Heaviside)
  - Output: Conditional probabilities defined by projecting Eht onto the probability simplex
  - Key constraint: Hidden states must encode state-symbol pairs from the DPFSA

- Critical path:
  1. Initialize h0 as the one-hot encoding of the initial state-symbol pair
  2. For each input symbol yt:
     a. Update hidden state using the Heaviside recurrence
     b. Project Eht to obtain the conditional probability distribution
  3. Terminate when EOS symbol is generated

- Design tradeoffs:
  - Using Heaviside activation enables binary state representation but limits expressiveness compared to continuous activations
  - Simulating DPFSAs requires Ω(|Σ||Q|) neurons in general, but can be compressed for log |Σ|-separable cases

- Failure signatures:
  - If the HRNN fails to simulate the DPFSAs transitions correctly, the output probabilities will not match
  - If the hidden state dimension is insufficient, the HRNN may not be able to encode all state-symbol pairs

- First 3 experiments:
  1. Implement a simple HRNN with Heaviside activation and verify it can simulate a small DPFSAs
  2. Measure the number of neurons required to simulate DPFSAs of varying sizes and compare to the theoretical bounds
  3. Implement the log |Σ|-separation procedure and verify it can compress certain DPFSAs to O(log|Σ||Q|^(3/4)) neurons

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the lower bound on the space complexity of simulating deterministic probabilistic finite-state automata (DPFSAs) with Heaviside Elman recurrent neural networks (HRNNs) be improved beyond the Ω(|Σ||Q|) bound shown in Theorem 5.1?
- Basis in paper: [explicit] The paper establishes a lower bound of Ω(|Σ||Q|) on the size of any HRNN that can simulate a DPFSAs, where |Σ| is the alphabet size and |Q| is the number of states. This bound is shown to be tight for general DPFSAs, as Minsky's construction achieves this bound.
- Why unresolved: The paper shows that the space complexity of simulating DPFSAs with HRNNs is generally linear in both |Σ| and |Q|. However, it also identifies special cases where this bound can be improved, such as n-gram language models and the bounded Dyck languages studied by Hewitt et al. (2020), which can be represented in logarithmic space with respect to |Q|. It is unclear whether further improvements are possible for general DPFSAs or whether there exist other classes of DPFSAs that admit more efficient representations.
- What evidence would resolve it: Constructing a family of DPFSAs that require HRNNs of size ω(|Σ||Q|) to simulate, or proving that such a family cannot exist, would resolve this question.

### Open Question 2
- Question: Can the equivalence between Heaviside Elman recurrent neural networks (HRNNs) and deterministic probabilistic finite-state automata (DPFSAs) be extended to other activation functions or recurrent neural network architectures?
- Basis in paper: [explicit] The paper establishes the equivalence between HRNNs and DPFSAs by showing that HRNNs can simulate DPFSAs and vice versa. It is noted that the Heaviside activation function is used for its simplicity and close analogy to the firing of brain neurons, but other activation functions such as ReLU could also be used.
- Why unresolved: The paper focuses on HRNNs with the Heaviside activation function for simplicity and theoretical tractability. It is unclear whether the equivalence between HRNNs and DPFSAs can be extended to other activation functions or recurrent neural network architectures, such as long short-term memory (LSTM) networks or gated recurrent units (GRUs).
- What evidence would resolve it: Proving the equivalence between DPFSAs and other types of recurrent neural networks with different activation functions, or constructing a counter-example showing that such an equivalence does not hold, would resolve this question.

### Open Question 3
- Question: Can the space complexity of simulating deterministic finite-state automata (FSAs) with Heaviside Elman recurrent neural networks (HRNNs) be improved beyond the O(|Σ||Q|) bound shown in Minsky's construction?
- Basis in paper: [explicit] The paper shows that Minsky's construction, which encodes an FSA with |Q| states in space O(|Σ||Q|), is in some sense optimal for simulating deterministic FSAs with HRNNs. It also discusses more space-efficient constructions by Dewdney (1977) and Indyk (1995), which achieve bounds of O(|Σ||Q|^(3/4)) and O(|Σ||Q|^(1/2)), respectively, for unweighted FSAs.
- Why unresolved: The paper establishes that Minsky's construction is asymptotically optimal for simulating deterministic FSAs with HRNNs in terms of the space complexity. However, it is unclear whether further improvements are possible for specific classes of FSAs or whether there exist alternative constructions that can achieve better bounds.
- What evidence would resolve it: Constructing a family of FSAs that can be simulated by HRNNs of size o(|Σ||Q|), or proving that such a family cannot exist, would resolve this question.

## Limitations
- The equivalence between HRNN LMs and DPFSAs is limited to the specific Heaviside activation function and may not extend to other activation functions like LSTM or ReLU
- The space complexity analysis provides asymptotic bounds but lacks empirical validation and practical implementation considerations
- The log |Σ|-separability condition for compression is theoretically identified but lacks empirical validation on real-world language models

## Confidence
- Equivalence between HRNN LMs and DPFSAs: High - The mathematical proof is rigorous and follows established theoretical frameworks
- Ω(|Σ||Q|) space complexity bound: Medium - While the proof appears sound, the assumption of arbitrary conditional distributions may not hold in practice
- O(log|Σ||Q|^(3/4)) compression bound: Low - The theoretical construction is presented but lacks empirical validation and the log |Σ|-separability condition is not fully characterized

## Next Checks
1. Implement the log |Σ|-separation procedure on standard n-gram models and verify the compression bounds empirically
2. Test the equivalence claims with continuous activation functions to understand the limitations of the Heaviside-specific results
3. Conduct empirical studies comparing the actual space requirements of RNNs simulating DPFSAs against the theoretical bounds across different DPFSA structures