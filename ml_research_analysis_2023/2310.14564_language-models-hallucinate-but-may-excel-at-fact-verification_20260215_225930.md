---
ver: rpa2
title: Language Models Hallucinate, but May Excel at Fact Verification
arxiv_id: '2310.14564'
source_url: https://arxiv.org/abs/2310.14564
tags:
- evidence
- chatgpt
- factual
- llms
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the hallucination problem in large language
  models (LLMs), where they often generate non-factual outputs. It quantifies this
  issue through human evaluation, revealing that even GPT-3.5 produces factual outputs
  less than 25% of the time.
---

# Language Models Hallucinate, but May Excel at Fact Verification

## Quick Facts
- arXiv ID: 2310.14564
- Source URL: https://arxiv.org/abs/2310.14564
- Reference count: 40
- Key outcome: FLAN-T5-11B, the least factual generator, outperforms GPT-3.5 and ChatGPT as a fact verifier when provided with high-quality evidence.

## Executive Summary
This paper quantifies the hallucination problem in large language models (LLMs), revealing that even advanced models like GPT-3.5 produce factual outputs less than 25% of the time. The study then explores repurposing LLMs as fact verifiers, finding that FLAN-T5-11B surprisingly outperforms larger models in this role. The research identifies key factors affecting fact verification performance, including evidence quality, model susceptibility to irrelevant information, and prompt robustness. The findings suggest that instruction-tuned models can be more effective at leveraging external evidence than relying on internal knowledge, potentially offering a path to mitigate hallucination issues.

## Method Summary
The study employs human evaluation to quantify hallucination rates in LLM-generated statements, then repurposes instruction-tuned LLMs (FLAN-T5-11B, GPT3.5, ChatGPT) as fact verifiers using retrieved evidence. The fact verification task involves generating factuality scores for statements using prompts with retrieved evidence. Performance is evaluated across multiple metrics (ECE, ACC, AUR, AUP, r) using human-crafted statements from various fact verification datasets. The research systematically analyzes the impact of evidence quality, prompt variations, and context dependencies on verification performance.

## Key Results
- FLAN-T5-11B outperforms GPT-3.5 and ChatGPT as a fact verifier when provided with high-quality evidence, despite being the least factual generator
- ChatGPT is more susceptible to irrelevant information in evidence but handles non-factual or contradictory evidence better than FLAN-T5-11B
- FLAN-T5-11B demonstrates greater robustness to different prompts than GPT-3.5 and ChatGPT, while ChatGPT shows a preference for predicting factual statements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: FLAN-T5-11B outperforms larger LLMs (GPT3.5, ChatGPT) in fact verification when provided with high-quality evidence.
- **Mechanism**: Despite FLAN-T5-11B's weaker generative performance, its architecture and instruction tuning make it more effective at leveraging retrieved evidence for factual assessment.
- **Core assumption**: The fact verification task benefits from models that can effectively utilize external evidence rather than relying on internal knowledge.
- **Evidence anchors**:
  - [abstract]: "FLAN-T5-11B, the least factual generator in our study, performs the best as a fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT."
  - [section]: "FLAN-T511B outperforms GPT3.5 in both calibration and discrimination ability, while being comparable with ChatGPT in the Wikipedia domain and substantially worse in the science domain."
  - [corpus]: "Retrieval-augmented generation with FLAN-T5-11B shows higher performance than retrieval-free generation."

### Mechanism 2
- **Claim**: ChatGPT is more susceptible to irrelevant information in evidence but better at handling non-factual or contradictory evidence compared to FLAN-T5-11B.
- **Mechanism**: ChatGPT relies more on its internal knowledge and is less dependent on given evidence, making it less sensitive to irrelevant information but more vulnerable to false information.
- **Core assumption**: Models with stronger internal knowledge can better handle contradictory evidence but may be misled by irrelevant information.
- **Evidence anchors**:
  - [abstract]: "ChatGPT is susceptible to irrelevant information in the evidence but deals with non-factual or contradictory evidence better than FLAN-T511B."
  - [section]: "ChatGPT is more susceptible to irrelevant information in given evidence than FLAN-T511B."
  - [corpus]: "ChatGPT performs much better than FLAN-T5 11 B if given fake (Adv) or contradictory evidence."

### Mechanism 3
- **Claim**: FLAN-T5-11B is more robust to different prompts than GPT3.5 and ChatGPT, while ChatGPT prefers to predict factual.
- **Mechanism**: FLAN-T5-11B's instruction tuning makes it more consistent across prompt variations, while ChatGPT's preference for factual predictions leads to higher recall but lower precision.
- **Core assumption**: Instruction tuning improves prompt robustness, and preference for a particular prediction category impacts precision-recall tradeoff.
- **Evidence anchors**:
  - [abstract]: "GPT variants are less robust to different prompts than FLAN-T511B; ChatGPT prefers to predict factual."
  - [section]: "FLAN-T511B is more robust to different prompts than GPT3.5 and ChatGPT with lower variance."
  - [corpus]: "ChatGPT exhibits a preference for predicting factual in contrast to FLAN-T511B, resulting in higher recall on factual statements."

## Foundational Learning

- **Concept**: Factuality evaluation metrics
  - **Why needed here**: The paper uses multiple metrics (ECE, ACC, AUR, AUP, r) to comprehensively evaluate fact verifiers from different perspectives.
  - **Quick check question**: What does a lower ECE score indicate about a fact verifier's calibration?

- **Concept**: Retrieval-augmented generation
  - **Why needed here**: The paper emphasizes the importance of retrieving external evidence for fact verification and compares different retrieval methods.
  - **Quick check question**: Why does the paper use Contriever instead of BM25 for retrieval?

- **Concept**: Instruction tuning
  - **Why needed here**: The paper compares different instruction-tuned LLMs and analyzes how instruction tuning affects fact verification performance.
  - **Quick check question**: How does instruction tuning potentially improve FLAN-T5-11B's fact verification performance despite its weaker generative ability?

## Architecture Onboarding

- **Component map**: Retriever (Contriever) -> Fact verifier (LLM-based) -> Evaluation metrics (ECE, ACC, AUR, AUP, r)
- **Critical path**: Retrieve evidence → Generate factuality score → Evaluate using multiple metrics
- **Design tradeoffs**: The paper explores tradeoffs between using golden vs. retrieved evidence, zero-shot vs. few-shot settings, and sentence-level vs. paragraph-level verification
- **Failure signatures**: Poor calibration (high ECE), inability to handle contradictory evidence, sensitivity to irrelevant information, and context-dependent performance degradation
- **First 3 experiments**:
  1. Compare FLAN-T5-11B, GPT3.5, and ChatGPT on fact verification using retrieved evidence from Wikipedia
  2. Evaluate the impact of using golden vs. retrieved evidence on fact verification performance
  3. Test the robustness of different LLMs to prompt variations and context-dependent statements

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: To what extent do hallucinations differ between open-ended generation tasks (e.g., summarization, story generation) versus retrieval-augmented generation?
- **Basis in paper**: [explicit] The paper notes that while retrieval can reduce hallucinations, LLMs may overly depend on retrieved texts, copying them without regard for fluency or relatedness.
- **Why unresolved**: The study focuses on quantifying hallucinations in retrieval-free open-ended generation, leaving the impact of retrieval on hallucination extent unexplored.
- **What evidence would resolve it**: A comparative analysis of hallucination rates in retrieval-free versus retrieval-augmented generation tasks across various domains and model scales.

### Open Question 2
- **Question**: How does the instruction tuning method (e.g., RLHF vs. supervised learning) impact the calibration and discrimination ability of LLMs as fact verifiers?
- **Basis in paper**: [explicit] The paper observes that InstructGPT is better calibrated than GPT3.5, suggesting RLHF may impact calibration. However, it does not explore the impact of other instruction tuning methods.
- **Why unresolved**: The study only compares InstructGPT and GPT3.5, leaving the impact of other instruction tuning methods unexplored.
- **What evidence would resolve it**: A systematic evaluation of LLMs fine-tuned with different instruction tuning methods (e.g., RLHF, supervised learning, contrastive learning) on their calibration and discrimination ability as fact verifiers.

### Open Question 3
- **Question**: How does the length of input statements and evidence affect the performance of LLMs as fact verifiers, and what is the optimal length for different tasks?
- **Basis in paper**: [inferred] The paper mentions that FLAN-T5 exhibits length extrapolation ability, but does not explore the impact of input and evidence length on fact verification performance.
- **Why unresolved**: The study uses fixed-length inputs and evidence, leaving the impact of varying lengths unexplored.
- **What evidence would resolve it**: A systematic evaluation of LLM fact verification performance across a range of input statement and evidence lengths, identifying optimal lengths for different tasks and model scales.

## Limitations
- The evaluation primarily relies on specific datasets and may not generalize to broader domains or more diverse statement types
- Human evaluation methodology involves subjective judgments that could introduce variability
- The paper doesn't explore the full spectrum of LLM architectures or different instruction tuning approaches

## Confidence
- The central claim that FLAN-T5-11B outperforms GPT-3.5 and ChatGPT in fact verification has **Medium** confidence
- The finding that ChatGPT is more susceptible to irrelevant information but handles contradictory evidence better has **Medium** confidence
- The claim that LLMs struggle particularly with context-dependent statements and numerals has **Medium-High** confidence

## Next Checks
1. **Cross-domain generalization test**: Evaluate fact verification performance on a broader set of domains (e.g., news articles, legal documents, technical documentation) to assess whether the observed advantages of FLAN-T5-11B persist across different knowledge domains and statement types.

2. **Evidence quality sensitivity analysis**: Systematically vary the quality and relevance of retrieved evidence to quantify how each model's performance degrades under different evidence conditions, particularly testing the claimed advantage of FLAN-T5-11B in handling irrelevant information.

3. **Instruction tuning ablation study**: Compare FLAN-T5-11B's fact verification performance with and without instruction tuning to isolate the specific contribution of instruction tuning to its superior performance, controlling for other architectural differences.