---
ver: rpa2
title: Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark
  Detection and Domain Separation and Reconstruction
arxiv_id: '2310.05207'
source_url: https://arxiv.org/abs/2310.05207
tags:
- facial
- detection
- domain
- learning
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses facial action unit (AU) detection in the wild,
  a challenging problem due to variations in expressions, poses, lighting, etc. The
  authors propose a new framework that jointly learns AU domain separation and reconstruction
  along with facial landmark detection using a multi-task learning strategy.
---

# Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction

## Quick Facts
- arXiv ID: 2310.05207
- Source URL: https://arxiv.org/abs/2310.05207
- Reference count: 0
- F1-score of 40.2% on target domain (EmotioNet), 63.2% on source domain (BP4D)

## Executive Summary
This paper addresses the challenging problem of facial action unit (AU) detection in the wild by proposing a novel multi-task learning framework. The method jointly trains facial landmark detection and AU domain separation/reconstruction using weight sharing between homostructural facial extraction modules. A key innovation is the use of contrastive learning with four additional intermediate supervision pairs to improve feature reconstruction quality across domains.

## Method Summary
The method employs a multi-task learning strategy that combines AU domain separation/reconstruction with facial landmark detection. It uses shared homostructural facial extraction modules (Ef, El, Gb) across both tasks, enabling weight sharing. The framework incorporates a feature alignment scheme based on contrastive learning with simple projectors and an improved contrastive loss that adds four intermediate supervisions to enhance the reconstruction process. The model is trained on BP4D (source domain) and EmotioNet (target domain) datasets, achieving state-of-the-art performance.

## Key Results
- Achieves F1-scores of 40.2% and 63.2% on target and source domains respectively
- Outperforms state-of-the-art techniques on BP4D and EmotioNet datasets
- Demonstrates effectiveness of joint training and contrastive learning with intermediate supervision

## Why This Works (Mechanism)

### Mechanism 1
- Joint training of facial landmark detection and AU domain separation enables weight sharing between homostructural facial extraction modules, improving AU detection in the wild.
- The facial extraction modules (Ef, El, Gb) have the same structure and can share weights across tasks. This weight sharing allows the model to learn complementary facial features for both tasks simultaneously, improving generalization.
- Core assumption: The feature extraction requirements for facial landmark detection and AU domain separation are sufficiently similar to benefit from weight sharing.

### Mechanism 2
- The proposed feature alignment scheme using contrastive learning with additional intermediate supervisions improves feature reconstruction quality.
- The improved contrastive loss adds four intermediate supervision pairs between reconstructed and original features (F'sl-Fsl, F'sb-Fsb, F'tl-Ftl, F'tb-Ftb) in addition to the final reconstruction pairs. This pixel-level alignment across multiple stages ensures more accurate reconstruction of facial features.
- Core assumption: Adding intermediate supervision pairs in the reconstruction process provides more effective gradient signals than only supervising final outputs.

### Mechanism 3
- The domain separation and reconstruction process helps the model learn features invariant to domain shifts while preserving AU-related information.
- The model separates facial images into landmark-related and background features, then performs cross-domain reconstruction. This process forces the model to learn domain-invariant features while maintaining the ability to reconstruct the original image, ensuring AU information is preserved.
- Core assumption: Separating and reconstructing facial features across domains can learn representations that are both domain-invariant and AU-preserving.

## Foundational Learning

- Concept: Multi-task learning with weight sharing
  - Why needed here: Facial landmark detection and AU detection share common feature extraction needs. Joint training allows the model to learn these shared features more effectively than separate training.
  - Quick check question: What architectural components are shared between the facial landmark detection branch and the AU domain separation branch?

- Concept: Contrastive learning with intermediate supervision
  - Why needed here: Standard contrastive learning only compares final representations. Adding intermediate supervision provides more granular control over the feature alignment process during reconstruction.
  - Quick check question: How many intermediate supervision pairs are added to the contrastive loss, and what features do they connect?

- Concept: Domain adaptation through feature separation and reconstruction
  - Why needed here: Images in the wild have significant domain shifts (pose, lighting, occlusion). Separating and reconstructing features across domains helps the model learn representations that generalize across these shifts.
  - Quick check question: What are the two main types of features that are separated during the domain separation process?

## Architecture Onboarding

- Component map: Input → Ef → (El/Gb) → Feature separation → Cross-domain reconstruction → Projectors → Contrastive loss → Fau → AU detection loss
- Critical path: Is/It → Ef → (El/Gb) → Feature separation → Cross-domain reconstruction → Projectors → Contrastive loss → Fau → AU detection loss
- Design tradeoffs:
  - Weight sharing improves efficiency but may limit task-specific optimization
  - Additional intermediate supervision improves reconstruction but increases computational cost
  - Cross-domain reconstruction helps generalization but may introduce reconstruction artifacts
- Failure signatures:
  - Poor landmark detection accuracy indicates issues with the shared feature extraction
  - Low reconstruction quality suggests problems with the domain separation process
  - Inconsistent AU detection across domains indicates inadequate feature alignment
- First 3 experiments:
  1. Train facial landmark detection branch alone and evaluate landmark accuracy
  2. Train AU detection with domain separation but without intermediate supervision
  3. Compare reconstruction quality with and without the contrastive alignment scheme

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method compare to other self-supervised methods that do not rely on domain separation, like the self-supervised learning methods mentioned (Fab-Net, TAE, CLP) in the source domain?
- Basis in paper: [explicit] The paper compares the proposed method to self-supervised methods like Fab-Net, TAE, and CLP in the source domain (BP4D), but it doesn't compare it to other self-supervised methods that don't rely on domain separation.
- Why unresolved: The paper doesn't provide a comparison to other self-supervised methods that don't use domain separation, which could provide insight into the effectiveness of the proposed method's approach.
- What evidence would resolve it: A comparison of the proposed method to other self-supervised methods that don't rely on domain separation, using the same datasets and evaluation metrics, would provide insight into the effectiveness of the proposed method's approach.

### Open Question 2
- Question: How does the proposed method perform on datasets with different levels of domain shift, such as datasets with more or less variation in poses, lighting, etc.?
- Basis in paper: [inferred] The paper focuses on the effectiveness of the proposed method on datasets with significant domain shift (BP4D and EmotioNet), but it doesn't explore how the method performs on datasets with different levels of domain shift.
- Why unresolved: The paper doesn't provide experiments on datasets with different levels of domain shift, which could provide insight into the robustness of the proposed method.
- What evidence would resolve it: Experiments on datasets with different levels of domain shift, using the same evaluation metrics, would provide insight into the robustness of the proposed method.

### Open Question 3
- Question: How does the proposed method handle occlusions and extreme poses that are not present in the training data?
- Basis in paper: [inferred] The paper focuses on the effectiveness of the proposed method on datasets with some occlusions and pose variations, but it doesn't explore how the method handles occlusions and extreme poses that are not present in the training data.
- Why unresolved: The paper doesn't provide experiments on datasets with occlusions and extreme poses that are not present in the training data, which could provide insight into the generalization ability of the proposed method.
- What evidence would resolve it: Experiments on datasets with occlusions and extreme poses that are not present in the training data, using the same evaluation metrics, would provide insight into the generalization ability of the proposed method.

## Limitations

- Limited ablation study of intermediate supervision pairs in contrastive loss
- Unclear impact of weight sharing on task-specific optimization
- Insufficient quantitative analysis of AU information preservation during domain separation

## Confidence

- Low confidence in the effectiveness of intermediate supervision pairs: The ablation study only compares against baseline without contrastive learning
- Medium confidence in domain separation mechanism: Limited quantitative analysis of AU information preservation
- Medium confidence in weight sharing approach: No ablation study comparing with separate feature extractors

## Next Checks

1. **Ablation study of intermediate supervision**: Compare the proposed framework with and without the four intermediate supervision pairs in the contrastive loss to quantify their contribution to reconstruction quality and AU detection performance.

2. **Visualization of separated features**: Generate and visualize the landmark-related and background features produced by the domain separation process to assess whether AU-relevant information is preserved and whether the separation effectively captures the intended components.

3. **Separate vs. shared feature extractors**: Implement and evaluate a variant of the framework where the facial landmark detection branch and AU domain separation branch use separate feature extraction modules to determine whether weight sharing provides a net benefit.