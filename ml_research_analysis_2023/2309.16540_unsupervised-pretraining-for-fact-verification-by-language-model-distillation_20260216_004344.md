---
ver: rpa2
title: Unsupervised Pretraining for Fact Verification by Language Model Distillation
arxiv_id: '2309.16540'
source_url: https://arxiv.org/abs/2309.16540
tags:
- fact
- language
- knowledge
- verification
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SFA VEL, a novel unsupervised pretraining
  framework for fact verification that leverages language model distillation. The
  key idea is to use a pre-trained language model as a teacher to guide the training
  of a knowledge model on claim-fact matching tasks without requiring labeled data.
---

# Unsupervised Pretraining for Fact Verification by Language Model Distillation

## Quick Facts
- arXiv ID: 2309.16540
- Source URL: https://arxiv.org/abs/2309.16540
- Reference count: 22
- This paper introduces SFA VEL, a novel unsupervised pretraining framework for fact verification that leverages language model distillation

## Executive Summary
This paper proposes SFA VEL, an unsupervised pretraining framework for fact verification that leverages language model distillation. The key innovation is using a frozen pre-trained language model as a teacher to guide the training of a knowledge model on claim-fact matching tasks without requiring labeled data. The method employs a contrastive loss function that encourages high-quality alignments between claims and evidence while preserving semantic relationships. SFA VEL achieves state-of-the-art performance on the FEVER benchmark (+8% accuracy) with linear evaluation and significantly outperforms previous supervised and unsupervised methods.

## Method Summary
SFA VEL employs a self-supervised language model distillation framework that transfers semantic knowledge from a frozen pre-trained language model to a trainable knowledge model. The approach uses a knowledge graph (Wikidata5m) and generates claim-fact pairs for training. A Relational Graph Attention Network (RGAT) encodes the knowledge base structure to produce fact embeddings. A scoring module computes relevance scores between claims and facts. The training employs three loss components: claim-fact distillation loss, intra-sample contrastive loss, and scoring loss. The contrastive loss distinguishes between semantically relevant and irrelevant evidence, while the scoring loss ranks relevant evidence higher than irrelevant evidence.

## Key Results
- Achieves state-of-the-art performance on FEVER benchmark with +8% accuracy improvement
- Outperforms previous supervised and unsupervised methods in fact verification
- Ablation studies demonstrate effectiveness of contrastive loss and language model backbone choices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised distillation transfers semantic knowledge from pre-trained language models to a smaller knowledge model without requiring labeled data.
- Mechanism: The pre-trained language model (teacher) provides high-quality claim embeddings. The knowledge model (student) learns to align these claim embeddings with evidence from the knowledge base through a contrastive loss that pulls positive claim-evidence pairs together and pushes negative pairs apart.
- Core assumption: The frozen pre-trained language model's embeddings capture sufficient semantic relationships between claims and evidence to serve as effective supervision for the knowledge model.
- Evidence anchors:
  - [abstract] "leverages pre-trained language models to distil self-supervised features into high-quality claim-fact alignments without the need for annotations"
  - [section 3.2] "we propose a feature-based claim-fact distillation loss...to transfer the feature correspondences from the pre-trained language model to the knowledge model"
- Break condition: If the pre-trained language model's embeddings don't capture relevant semantic relationships, the distillation process will fail to produce meaningful claim-fact alignments.

### Mechanism 2
- Claim: The contrastive loss function enables effective unsupervised learning by distinguishing between semantically relevant and irrelevant evidence.
- Mechanism: The intra-sample contrastive loss contrasts the centroid of positive fact embeddings against individual positive and negative fact embeddings, creating a margin that separates relevant from irrelevant evidence in the feature space.
- Core assumption: The contrastive loss can effectively learn to distinguish between positive and negative evidence without labeled data by leveraging the knowledge base structure.
- Evidence anchors:
  - [section 3.4] "The intra-sample distillation loss derives from the standard contrastive loss...We extend the contrastive loss function by replacing the query with the centroid of the positive facts"
  - [section 3.3] "Our approach aims to generate negative samples that are factually false while preserving the contextual meaning of the entities"
- Break condition: If the negative sampling strategy fails to generate meaningful negatives, the contrastive loss will not learn useful distinctions between relevant and irrelevant evidence.

### Mechanism 3
- Claim: The scoring module learns to rank relevant evidence higher than irrelevant evidence through a pairwise ranking loss.
- Mechanism: The scoring loss maximizes the scores given by the scoring model to positive facts and minimizes the scores of negative facts for a given claim, creating a ranking that prioritizes relevant evidence.
- Core assumption: The scoring module can effectively learn to distinguish between positive and negative evidence through the pairwise ranking loss without explicit labels.
- Evidence anchors:
  - [section 3.4] "The scoring loss is a variant of the conventional pair-wise ranking loss...to maximize the scores given by the scoring model to the positive facts, F +, and minimize the scores of the negative facts, F −, for a given claim xi"
  - [section 3.3] "we propose to generate two sets of negative instances: in-batch negatives and in-knowledge-base negatives"
- Break condition: If the margin factor γ is set incorrectly, the scoring loss may fail to create meaningful distinctions between positive and negative evidence.

## Foundational Learning

- Concept: Contrastive learning
  - Why needed here: Contrastive learning enables the model to learn meaningful representations by distinguishing between similar (positive) and dissimilar (negative) pairs without requiring labels
  - Quick check question: How does the intra-sample contrastive loss differ from standard contrastive learning approaches?

- Concept: Knowledge distillation
  - Why needed here: Knowledge distillation allows transferring knowledge from a large pre-trained language model to a smaller, more efficient knowledge model for the specific task of claim-fact matching
  - Quick check question: What is the role of the frozen pre-trained language model in the distillation process?

- Concept: Graph neural networks
  - Why needed here: Graph neural networks are used to encode the knowledge base structure and produce embeddings for facts that capture relational information
  - Quick check question: How does the Relational Graph Attention Network encode the knowledge base structure?

## Architecture Onboarding

- Component map:
  - Pre-trained language model (frozen) → provides claim embeddings
  - Knowledge model (trainable) → encodes knowledge base and produces fact embeddings
  - Scoring module → computes relevance scores between claims and facts
  - Loss functions (three components) → train the knowledge model and scoring module

- Critical path: Claim → Language Model → Claim Embedding → Scoring Module → Top K Facts → Knowledge Model → Fact Embeddings → Loss Functions → Updated Knowledge Model

- Design tradeoffs:
  - Using a frozen pre-trained language model provides strong initialization but limits adaptation to the specific task
  - The choice of K (number of facts to keep) balances information completeness against noise introduction
  - The negative sampling strategy trades off computational efficiency against the quality of negative examples

- Failure signatures:
  - Poor performance on the validation set despite good training loss indicates overfitting to the distillation signal
  - High variance in results across different runs suggests instability in the negative sampling process
  - Degraded performance when using smaller pre-trained language models indicates insufficient semantic representation capacity

- First 3 experiments:
  1. Train with only the claim-fact distillation loss (λcont = λscoring = 0) to verify the basic distillation mechanism works
  2. Train with only the contrastive loss (λdistill = λscoring = 0) to verify the negative sampling strategy produces meaningful distinctions
  3. Train with only the scoring loss (λdistill = λcont = 0) to verify the pairwise ranking mechanism learns to prioritize relevant evidence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SFA VEL perform on fact verification tasks with knowledge bases that have significantly different structures or scales compared to Wikidata5m?
- Basis in paper: [inferred] The paper mentions using Wikidata5m as the knowledge base for experiments, but does not explore performance on alternative knowledge base structures or scales.
- Why unresolved: The paper focuses on demonstrating SFA VEL's effectiveness on the FEVER dataset with Wikidata5m, without investigating its robustness to variations in knowledge base structure or size.
- What evidence would resolve it: Conducting experiments on fact verification tasks using knowledge bases with different structures (e.g., hierarchical, relational) and scales (e.g., smaller or larger than Wikidata5m) would provide insights into SFA VEL's performance and generalization capabilities.

### Open Question 2
- Question: What is the impact of varying the number of negative samples (M) on SFA VEL's performance?
- Basis in paper: [explicit] The paper mentions setting M = 4096 for the number of negative instances used in the negative pool for contrastive learning, but does not explore the impact of varying this parameter.
- Why unresolved: The paper does not investigate how different values of M affect the model's ability to learn meaningful representations and achieve accurate claim-fact matching.
- What evidence would resolve it: Conducting experiments with different values of M (e.g., 1024, 2048, 8192) and comparing the resulting performance on fact verification tasks would provide insights into the optimal number of negative samples for SFA VEL.

### Open Question 3
- Question: How does SFA VEL's performance compare to other unsupervised fact verification methods on datasets with different characteristics, such as multi-hop reasoning or cross-lingual claims?
- Basis in paper: [inferred] The paper demonstrates SFA VEL's state-of-the-art performance on the FEVER dataset, but does not compare it to other unsupervised methods on datasets with different characteristics.
- Why unresolved: The paper focuses on evaluating SFA VEL on the FEVER dataset, without exploring its performance on other fact verification datasets that may have different properties or challenges.
- What evidence would resolve it: Conducting experiments on fact verification datasets that require multi-hop reasoning, involve cross-lingual claims, or have different characteristics would allow for a more comprehensive comparison of SFA VEL's performance against other unsupervised methods.

## Limitations

- The contrastive learning mechanism relies heavily on the quality of negative sampling, but lacks sufficient empirical validation of whether generated negative instances are truly semantically distinct from positive evidence.
- The distillation process assumes frozen pre-trained language model embeddings contain sufficient semantic information, but doesn't analyze sensitivity to different pre-trained model choices.
- The ablation studies focus primarily on loss function components but don't adequately explore critical hyperparameters like the number of facts kept (K) or the margin factor γ.

## Confidence

**High Confidence**: The overall framework architecture and the integration of pre-trained language models with knowledge base embeddings is well-established and technically sound. The use of contrastive learning for unsupervised alignment is a standard approach in the field.

**Medium Confidence**: The specific implementation details of the contrastive loss function and the negative sampling strategy, while theoretically sound, lack sufficient empirical validation to confirm their effectiveness in this particular application.

**Low Confidence**: The claim that the method achieves state-of-the-art performance (+8% accuracy on FEVER) is difficult to verify without access to the exact experimental setup, hyperparameter configurations, and implementation details of the competing methods.

## Next Checks

1. **Negative Sampling Quality Analysis**: Conduct a qualitative and quantitative analysis of the negative samples generated by the in-batch and in-knowledge-base strategies to verify they are semantically meaningful and sufficiently distinct from positive evidence.

2. **Pretrained Model Sensitivity Study**: Systematically evaluate the performance of SFA VEL using different pre-trained language models (varying sizes and architectures) to determine the sensitivity of the distillation process to the choice of teacher model.

3. **Hyperparameter Ablation**: Perform a comprehensive ablation study focusing on critical hyperparameters like K (number of facts kept), the margin factor γ, and the temperature parameter τ to identify the optimal configuration and understand their impact on model performance.