---
ver: rpa2
title: 'LOIS: Looking Out of Instance Semantics for Visual Question Answering'
arxiv_id: '2307.14142'
source_url: https://arxiv.org/abs/2307.14142
tags:
- visual
- attention
- instance
- features
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes LOIS (Looking Out of Instance Semantics), a\
  \ VQA model that avoids using bounding boxes and instead relies on instance segmentation\
  \ for fine-grained feature extraction. The key idea is to use two attention modules\u2014\
  intra-modality and inter-modality\u2014to model complex visual-semantic relationships\
  \ and overcome label ambiguity from instance masks."
---

# LOIS: Looking Out of Instance Semantics for Visual Question Answering

## Quick Facts
- **arXiv ID:** 2307.14142
- **Source URL:** https://arxiv.org/abs/2307.14142
- **Reference count:** 40
- **Key outcome:** LOIS achieves 71.94% accuracy on COCO-QA and 43.09% on VQA-CP v2, outperforming baselines by leveraging instance segmentation and dual attention modules.

## Executive Summary
This paper introduces LOIS, a visual question answering model that replaces bounding box detection with instance segmentation to extract finer-grained visual features. The model employs two relation attention modules—intra-modality for instance-background relationships and inter-modality for image-question alignment—to improve reasoning and reduce language prior bias. Experiments on four VQA datasets show improved accuracy, especially in capturing contextual information and handling complex reasoning tasks.

## Method Summary
LOIS uses pre-trained SOLOv2 (ResNet-101 + FPN) to generate instance masks, which are then used to extract fine-grained instance features. Background inpainting via GAN removes object regions to create complementary background features. Both instance and background features are pooled into aligned vectors and processed through intra-modality attention to model their relationship. Inter-modality attention aligns fused image features with question embeddings from BERT, enabling focused reasoning. The model is trained with SGD, multi-head attention (8 heads), and a classification head for answer prediction.

## Key Results
- Achieves 71.94% accuracy on COCO-QA, outperforming existing methods.
- Attains 43.09% accuracy on VQA-CP v2, demonstrating robustness to language priors.
- Shows improved performance on counting and spatial reasoning tasks due to finer instance features.

## Why This Works (Mechanism)

### Mechanism 1
Instance segmentation produces finer-grained edge features than bounding boxes, reducing noise from background context. By isolating pixel-level object boundaries, instance masks avoid the averaging effect of bounding boxes that includes background pixels. This relies on the assumption that instance masks accurately capture object boundaries. Evidence is limited in the corpus, and the benefit disappears if masks include significant background or miss boundaries.

### Mechanism 2
The intra-modality attention module improves visual reasoning by modeling relationships between instance and background features. A bilinear attention map dynamically captures correlations between foreground and background feature vectors, allowing the model to weigh their relative importance. This assumes background context complements instance features. Without corpus support, the mechanism may add noise if background features are irrelevant or redundant.

### Mechanism 3
The inter-modality attention module aligns image regions with question words, reducing language prior bias. Question embeddings compute weighted attention over image regions, forcing focus on visual evidence rather than question priors. This assumes question words map to specific visual regions. If embeddings do not correlate with image regions, the attention map becomes uninformative.

## Foundational Learning

- **Instance segmentation**
  - Why needed here: Provides pixel-level object masks instead of bounding boxes, improving feature precision for VQA.
  - Quick check question: What is the key difference between instance segmentation and bounding box detection in terms of feature representation?

- **Attention mechanisms**
  - Why needed here: Allow the model to dynamically focus on relevant visual regions and question words, improving reasoning over raw concatenation.
  - Quick check question: How does intra-modality attention differ from inter-modality attention in multimodal models?

- **Language priors in VQA**
  - Why needed here: Many VQA models over-rely on question wording rather than image content; mitigating this improves accuracy.
  - Quick check question: What is a language prior in VQA, and why does it hurt performance?

## Architecture Onboarding

- **Component map:** Image → Instance segmentation → Instance + background features → Intra-modality attention → Inter-modality attention → Classifier
- **Critical path:** Image → Instance segmentation → Instance + background features → Intra-modality attention → Inter-modality attention → Classifier
- **Design tradeoffs:**
  - Using instance masks instead of bounding boxes increases accuracy but adds segmentation inference time.
  - Including background inpainting reduces shape leakage but adds GAN complexity.
  - Multi-head attention increases expressiveness but multiplies computation.
- **Failure signatures:**
  - Low accuracy on counting questions → instance mask precision may be insufficient.
  - Poor performance on "yes/no" → inter-modality attention may not be aligning questions correctly.
  - Overfitting on training set → model may be learning dataset-specific shortcuts rather than reasoning.
- **First 3 experiments:**
  1. Replace instance masks with bounding boxes; compare accuracy drop.
  2. Remove background inpainting; measure if shape leakage harms performance.
  3. Disable intra-modality attention; observe impact on counting and spatial reasoning tasks.

## Open Questions the Paper Calls Out

- **Open Question 1:** How does the performance of LOIS compare to other state-of-the-art methods on the VQA-CP v2 dataset, which is designed to reduce question-oriented bias?
  - Basis in paper: [explicit] The paper mentions that LOIS has a relatively outstanding performance of 43.09% on the "Overall" task on the VQA-CP v2 dataset.
  - Why unresolved: The paper does not provide a detailed comparison of LOIS's performance on the VQA-CP v2 dataset with other state-of-the-art methods.
  - What evidence would resolve it: A detailed comparison of LOIS's performance on the VQA-CP v2 dataset with other state-of-the-art methods would resolve this question.

- **Open Question 2:** How does the use of instance semantic detection in VQA tasks affect the model's ability to understand contextual information and answer questions about complex scenes?
  - Basis in paper: [explicit] The paper states that instance semantic detection aims to distinguish different semantic categories and that the proposed LOIS framework learns deep semantic features from detected instance objects for the VQA tasks.
  - Why unresolved: The paper does not provide a detailed analysis of how the use of instance semantic detection affects the model's ability to understand contextual information and answer questions about complex scenes.
  - What evidence would resolve it: A detailed analysis of how the use of instance semantic detection affects the model's ability to understand contextual information and answer questions about complex scenes would resolve this question.

- **Open Question 3:** How does the performance of LOIS compare to other state-of-the-art methods on the COCO-QA dataset, which contains questions about object, number, color, and location?
  - Basis in paper: [explicit] The paper mentions that LOIS outperforms other methods on the COCO-QA dataset, with an overall accuracy of 71.94%.
  - Why unresolved: The paper does not provide a detailed comparison of LOIS's performance on the COCO-QA dataset with other state-of-the-art methods for each question type (object, number, color, and location).
  - What evidence would resolve it: A detailed comparison of LOIS's performance on the COCO-QA dataset with other state-of-the-art methods for each question type (object, number, color, and location) would resolve this question.

## Limitations

- The paper does not provide direct ablation comparisons between instance masks and bounding boxes, making the claimed edge feature benefit uncertain.
- Assertions that attention modules reduce language priors lack explicit bias metrics or quantitative analysis.
- Reliance on pre-trained SOLOv2 introduces potential data leakage risks if COCO-trained masks influence VQA evaluation.

## Confidence

- **Medium:** Claims about instance segmentation improving fine-grained feature extraction, as direct box-vs-mask ablations are not shown.
- **Medium:** Assertions that attention modules reduce language priors, since the paper reports accuracy gains but lacks explicit bias metrics.
- **High:** Reported benchmark results, assuming the experimental setup matches the stated methodology.

## Next Checks

1. Perform controlled ablation: replace instance masks with bounding boxes on the same model and compare accuracy drop to quantify the claimed edge feature benefit.
2. Conduct language bias analysis using methods like RUBi or VQA-CP-style per-type accuracy to verify that attention modules actually reduce prior dependence.
3. Verify independence of the VQA training from the segmentation pretraining by checking for overlapping categories or concepts between COCO mask pretraining and VQA evaluation sets.