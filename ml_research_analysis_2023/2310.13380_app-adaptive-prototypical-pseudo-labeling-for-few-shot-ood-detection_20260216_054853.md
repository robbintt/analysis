---
ver: rpa2
title: 'APP: Adaptive Prototypical Pseudo-Labeling for Few-shot OOD Detection'
arxiv_id: '2310.13380'
source_url: https://arxiv.org/abs/2310.13380
tags:
- detection
- data
- samples
- pseudo-labeling
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a method for few-shot out-of-domain (OOD)
  intent detection, where only a few labeled in-domain (IND) samples and a large amount
  of unlabeled mixed data (IND and OOD) are available. The method consists of two
  stages: a prototypical OOD detection framework (ProtoOOD) to learn discriminative
  representations from limited IND data, and an adaptive pseudo-labeling method to
  leverage unlabeled mixed data.'
---

# APP: Adaptive Prototypical Pseudo-Labeling for Few-shot OOD Detection

## Quick Facts
- **arXiv ID:** 2310.13380
- **Source URL:** https://arxiv.org/abs/2310.13380
- **Reference count:** 15
- **Key outcome:** Achieves state-of-the-art performance on Banking and Stackoverflow OOD detection datasets under various few-shot settings (5-shot, 10-shot, 20-shot) with 25%, 50%, 75% IND ratios.

## Executive Summary
This paper addresses few-shot out-of-domain (OOD) intent detection where only limited labeled in-domain (IND) data and large amounts of unlabeled mixed data are available. The proposed APP method combines a prototypical OOD detection framework (ProtoOOD) with adaptive pseudo-labeling. ProtoOOD learns discriminative representations from limited IND data using contrastive learning, while the adaptive pseudo-labeling stage iteratively labels mixed data and updates the classifier using instance-prototype margin objectives. The method demonstrates superior performance compared to baseline approaches across multiple few-shot settings.

## Method Summary
APP employs a two-stage approach for few-shot OOD detection. First, ProtoOOD pre-trains a prototypical classifier on limited IND data using instance-instance and instance-prototype contrastive losses to learn discriminative intent representations. Second, an adaptive pseudo-labeling stage iteratively labels the unlabeled mixed data (containing both IND and OOD) using thresholds on cosine similarity scores, then updates the classifier with joint optimization of Lpcl, Lind, and Lood losses. The instance-prototype margin objectives (Lind and Lood) adaptively pull pseudo-IND samples toward prototypes while pushing pseudo-OOD samples away, creating clear separation between confidence score distributions.

## Key Results
- APP achieves state-of-the-art OOD detection performance on Banking and Stackoverflow datasets across 5-shot, 10-shot, and 20-shot settings
- The method shows robustness to different threshold values (T) and margin values for pseudo-labeling
- APP outperforms baseline methods including MSP, LOF, GDA, Energy, and UniNL in both OOD detection metrics (Recall, F1) and IND detection metrics (Accuracy, F1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ProtoOOD framework enables discriminative intent representations in few-shot OOD detection by leveraging instance-instance and instance-prototype contrastive losses.
- Mechanism: The instance-instance loss pulls together samples of the same class while pushing apart samples from different classes, and the instance-prototype loss enforces prototypes to be center points of classes, creating compact class clusters and separating OOD from IND intents.
- Core assumption: Limited labeled IND data can still provide sufficient class-level semantics for contrastive learning to work effectively.
- Evidence anchors:
  - [abstract] "we propose a prototypical OOD detection framework (ProtoOOD) to facilitate low-resource OOD detection using limited IND data"
  - [section] "inspired by recent prototype learning work (Li et al., 2020; Cui et al., 2022), we propose a prototypical OOD detection framework (ProtoOOD) to learn discriminative intent representations in the few-shot setting"
  - [corpus] Weak evidence - no direct mention of contrastive learning mechanisms in related papers
- Break condition: If the number of IND samples per class drops below a critical threshold where class semantics cannot be reliably captured.

### Mechanism 2
- Claim: Adaptive pseudo-labeling with instance-prototype margin objectives enables effective utilization of unlabeled mixed data containing both IND and OOD intents.
- Mechanism: Two thresholds S and L are used to classify pseudo-IND and pseudo-OOD samples. The IND margin objective pulls pseudo-IND samples toward prototypes while the OOD margin objective pushes pseudo-OOD samples away from prototypes, creating clear separation between confidence score distributions.
- Core assumption: The model can reliably distinguish between IND and OOD samples based on their distance to class prototypes.
- Evidence anchors:
  - [section] "we introduce two instance-prototype margin objectives to adaptively pull together pseudo IND samples and prototypes and push apart pseudo OOD samples"
  - [section] "We aim to distinguish the confidence score distributions of IND and OOD data by adjusting distances between IND or OOD samples and prototypes"
  - [corpus] Weak evidence - related papers focus on pseudo-labeling but don't address the OOD detection challenge specifically
- Break condition: If the threshold selection (S and L) becomes too noisy due to insufficient separation between IND and OOD distributions.

### Mechanism 3
- Claim: Joint optimization of Lpcl, Lind, and Lood losses produces more reliable pseudo-labels than using any single objective.
- Mechanism: Lpcl alone struggles to correctly identify OOD samples, Lood alone struggles to correctly identify IND samples, but their combination with appropriate coefficients enables balanced learning from both pseudo-IND and pseudo-OOD samples.
- Core assumption: The model can simultaneously optimize toward pulling IND samples closer to prototypes while pushing OOD samples away without interference between objectives.
- Evidence anchors:
  - [section] "We find that when the two margin objectives are used alone, they will hinder the prototype-based self-training"
  - [section] "when we use three objectives for joint optimization, we get more and more correct pseudo IND samples and pseudo OOD samples"
  - [corpus] No direct evidence in related papers about this specific joint optimization approach
- Break condition: If the coefficient values for Lind and Lood are not properly tuned, causing one objective to dominate and degrade overall performance.

## Foundational Learning

- Concept: Contrastive learning and prototype-based representation learning
  - Why needed here: Few-shot OOD detection requires learning discriminative representations from limited labeled data, which is achieved through pulling similar samples together and pushing dissimilar samples apart while enforcing prototypes as class centers
  - Quick check question: What is the difference between instance-instance and instance-prototype contrastive losses?

- Concept: Semi-supervised learning and pseudo-labeling strategies
  - Why needed here: The method needs to leverage large amounts of unlabeled mixed data (containing both IND and OOD) by iteratively labeling and retraining the model
  - Quick check question: How does the adaptive pseudo-labeling method handle the challenge of unknown OOD data distribution?

- Concept: Distance-based scoring functions for OOD detection
  - Why needed here: The model uses maximum cosine distance/similarity between test samples and IND prototypes to determine if a sample is OOD
  - Quick check question: Why is cosine distance used instead of other distance metrics for OOD detection in this framework?

## Architecture Onboarding

- Component map:
  - BERT backbone for text representation
  - Prototype embedding layer (256-dim)
  - Prototypical OOD detection module (ProtoOOD)
  - Adaptive pseudo-labeling module with margin objectives
  - Threshold selection mechanism for pseudo-labeling

- Critical path:
  1. Pre-train prototypical classifier on limited IND data using Lpcl loss
  2. Select thresholds S and L based on unlabeled data score distribution
  3. Iteratively perform pseudo-labeling and model updating using joint loss optimization
  4. Inference using maximum cosine similarity threshold

- Design tradeoffs:
  - Fixed vs. adaptive thresholds: Fixed thresholds provide stability but may not adapt to changing data distributions
  - Single vs. joint margin objectives: Joint optimization provides better balance but requires careful coefficient tuning
  - Pre-training vs. joint learning: Separate pre-training on IND data provides better initialization but adds complexity

- Failure signatures:
  - Poor IND/OOD separation in score distribution curves
  - Degrading performance on validation set during pseudo-labeling iterations
  - High variance across random runs indicating instability

- First 3 experiments:
  1. Ablation study: Compare ProtoOOD vs. baseline OOD detection methods (MSP, GDA, Energy) on few-shot settings
  2. Pseudo-labeling analysis: Compare Lpcl, Lpcl+Lind, Lpcl+Lood, and full APP method on unlabeled data utilization
  3. Hyperparameter sensitivity: Test different threshold values (T) and margin coefficients on detection performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed adaptive prototypical pseudo-labeling (APP) method compare to other semi-supervised learning methods, such as consistency regularization or self-training with different loss functions, for few-shot OOD detection?
- Basis in paper: [explicit] The paper mentions that the proposed APP method is a basic self-supervised learning method and suggests that more other SSL methods should be considered.
- Why unresolved: The paper does not provide a direct comparison between the APP method and other semi-supervised learning methods for few-shot OOD detection.
- What evidence would resolve it: Conducting experiments to compare the performance of the APP method with other semi-supervised learning methods, such as consistency regularization or self-training with different loss functions, on few-shot OOD detection datasets.

### Open Question 2
- Question: How does the proposed prototypical OOD detection framework (ProtoOOD) perform under different few-shot settings, such as varying the number of labeled IND samples or the ratio of IND to OOD intents?
- Basis in paper: [explicit] The paper evaluates the ProtoOOD method under different few-shot settings, such as 5-shot, 10-shot, and 20-shot, but does not explore the impact of varying the number of labeled IND samples or the ratio of IND to OOD intents.
- Why unresolved: The paper does not provide a comprehensive analysis of how the ProtoOOD method performs under different few-shot settings, particularly when varying the number of labeled IND samples or the ratio of IND to OOD intents.
- What evidence would resolve it: Conducting experiments to evaluate the ProtoOOD method under different few-shot settings, such as varying the number of labeled IND samples or the ratio of IND to OOD intents, and comparing the results to the baseline methods.

### Open Question 3
- Question: How does the proposed adaptive pseudo-labeling method handle cases where the unlabeled mixed data contains a high proportion of OOD samples or a low proportion of IND samples?
- Basis in paper: [explicit] The paper mentions that the unlabeled mixed data contains both IND and OOD intents, but does not provide a detailed analysis of how the adaptive pseudo-labeling method handles cases with different proportions of OOD and IND samples.
- Why unresolved: The paper does not provide a comprehensive analysis of how the adaptive pseudo-labeling method performs when the unlabeled mixed data contains a high proportion of OOD samples or a low proportion of IND samples.
- What evidence would resolve it: Conducting experiments to evaluate the adaptive pseudo-labeling method under different scenarios, such as varying the proportion of OOD and IND samples in the unlabeled mixed data, and analyzing the impact on the performance of OOD detection.

## Limitations

- The method relies heavily on the quality of pseudo-labels, which can degrade if threshold selection S and L fails to adequately separate IND and OOD distributions
- The approach assumes that class prototypes can be reliably learned from very few labeled samples (5-shot, 10-shot scenarios), which may not hold for highly diverse intent classes with subtle semantic differences
- The method's performance depends on careful hyperparameter tuning of margin coefficients (α, β) and margin values (MIN D, MOOD), with no clear guidance on systematic selection methods beyond grid search

## Confidence

- **High Confidence:** The two-stage framework architecture and the use of contrastive learning for prototype-based representation learning are well-established mechanisms with strong theoretical grounding.
- **Medium Confidence:** The specific joint optimization of Lpcl, Lind, and Lood losses with instance-prototype margin objectives shows empirical promise but lacks theoretical justification for why this particular combination outperforms alternatives.
- **Low Confidence:** The adaptive pseudo-labeling mechanism's robustness to varying IND/OOD ratios in unlabeled data and its generalization to datasets with different domain characteristics remains under-validated.

## Next Checks

1. **Robustness Testing:** Evaluate APP's performance across different IND/OOD ratios in unlabeled data (e.g., 90/10, 50/50, 10/90) to assess threshold selection stability and pseudo-label quality.
2. **Generalization Analysis:** Test the method on additional OOD detection datasets with different characteristics (e.g., multi-domain text classification, cross-lingual settings) to verify broader applicability.
3. **Failure Mode Investigation:** Systematically analyze scenarios where APP degrades (e.g., extremely few IND samples, highly similar IND/OOD semantics) to identify fundamental limitations and potential failure conditions.