---
ver: rpa2
title: 'StoryBench: A Multifaceted Benchmark for Continuous Story Visualization'
arxiv_id: '2308.11606'
source_url: https://arxiv.org/abs/2308.11606
tags:
- video
- story
- videos
- data
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StoryBench is a benchmark for continuous story visualization from
  text, requiring models to generate videos that follow a sequence of text prompts
  while maintaining visual quality and consistency. It collects rich human annotations
  on three datasets (DiDeMo, Oops, UVO) totaling 6K videos, with action-by-action
  descriptions, timestamps, and diagnostic labels.
---

# StoryBench: A Multifaceted Benchmark for Continuous Story Visualization

## Quick Facts
- arXiv ID: 2308.11606
- Source URL: https://arxiv.org/abs/2308.11606
- Reference count: 40
- Key outcome: StoryBench is a benchmark for continuous story visualization from text, requiring models to generate videos that follow a sequence of text prompts while maintaining visual quality and consistency.

## Executive Summary
StoryBench is a comprehensive benchmark for continuous story visualization that evaluates video generation models on three increasingly difficult tasks: action execution, story continuation, and story generation. The benchmark collects rich human annotations on three datasets (DiDeMo, Oops, UVO) totaling 6K videos with action-by-action descriptions, timestamps, and diagnostic labels. A small Phenaki model is trained and evaluated in zero-shot, single-task, and multi-task setups. Human evaluation shows that fine-tuning for continuation improves performance over zero-shot and general fine-tuning. However, automatic metrics do not fully align with human judgments, highlighting the need for better evaluation methods.

## Method Summary
The method involves fine-tuning a small 345M parameter Phenaki model on three datasets (DiDeMo, Oops, UVO) transformed into story-like data through an automatic pipeline that splits captions and estimates timestamps. Three tasks are defined: action execution (conditioning on single actions), story continuation (predicting next frames given previous video), and story generation (generating full videos from text). Models are evaluated in zero-shot, single-task, and multi-task setups with novel continuation mode fine-tuning that predicts only the last 6 frames while conditioning on 5 ground-truth frames.

## Key Results
- Fine-tuning for continuation mode significantly outperforms zero-shot models across all human evaluation metrics
- Multi-task fine-tuning shows comparable quality to zero-shot but smaller improvements than single-task fine-tuning
- Human evaluation reveals discrepancies with automatic metrics, suggesting current metrics do not fully capture video quality
- The automatic transformation pipeline achieves 63.6% BLEU4 score compared to human references

## Why This Works (Mechanism)

### Mechanism 1
Multi-task fine-tuning on diverse datasets improves generalization but may degrade task-specific performance. Fine-tuning a single model on multiple datasets (DiDeMo, Oops, UVO) introduces competing objectives, leading to a model that is less specialized for any individual task. Core assumption: A single model can effectively balance the diverse characteristics of different datasets without losing task-specific capabilities. Evidence anchors: Compared to PHENAKI-CONT-ST, however, its margins over the zero-shot model are much smaller, clearly demonstrating the benefit of our training data transformation pipeline.

### Mechanism 2
Fine-tuning for continuation mode improves text adherence and consistency over zero-shot models. By training the model to predict only the last 6 frames while conditioning on 5 ground-truth frames, the model learns to create smoother transitions and maintain consistency with the input video. Core assumption: The model can effectively learn from a limited context (5 ground-truth frames) to generate the next 6 frames with improved consistency. Evidence anchors: Our model fine-tuned in continuation mode is vastly preferred over the zero-shot PHENAKI-GEN-ZS in all metrics.

### Mechanism 3
Automatic transformation of video captions into story-like data improves model performance. By splitting long captions into individual actions and estimating their timestamps, the model receives more granular and structured training data, leading to better understanding of action sequences. Core assumption: The automatic pipeline can accurately split captions and estimate timestamps without losing important information. Evidence anchors: we devise a pipeline to automatically transform existing, grounded video captions [20] into stories, resulting in training data that is very rich in structure.

## Foundational Learning

- **Video generation from text**: Why needed here - The entire benchmark is built around the task of generating videos from text prompts, requiring a deep understanding of how text can be translated into visual content. Quick check question: What are the key challenges in translating a sequence of text prompts into a coherent video?

- **Story visualization**: Why needed here - The benchmark focuses on continuous story visualization, which requires generating a sequence of video frames that follow a narrative described by text. Quick check question: How does story visualization differ from single-image generation or traditional video generation?

- **Evaluation metrics for generative models**: Why needed here - The benchmark relies on both human and automatic evaluation metrics to assess model performance, requiring an understanding of the strengths and limitations of each approach. Quick check question: Why are automatic metrics often insufficient for evaluating the quality of generated videos?

## Architecture Onboarding

- **Component map**: Text prompt → Text encoder → MaskGiT model → Generated video frames
- **Critical path**: Text prompt → Text encoder → MaskGiT model → Generated video frames
- **Design tradeoffs**: Model size vs. computational cost: A smaller model (345M parameters) is used to reduce computational requirements, but may limit performance. Resolution vs. quality: Lower resolution (160x96) is used to reduce computational cost, but may affect visual quality. Frame rate vs. smoothness: Lower frame rate (8fps) is used to reduce computational cost, but may affect the smoothness of the generated video.
- **Failure signatures**: Poor text adherence: The generated video does not accurately reflect the content described in the text prompt. Lack of consistency: The generated video frames are not consistent with each other or with the input video (for continuation tasks). Unrealistic actions: The actions depicted in the generated video are not realistic or physically plausible.
- **First 3 experiments**: 1) Evaluate the zero-shot model on the action execution task to establish a baseline performance. 2) Fine-tune the model on a single dataset (e.g., Oops-CSV) for the story continuation task and compare its performance to the zero-shot model. 3) Fine-tune the model on multiple datasets jointly for the story generation task and compare its performance to the single-task fine-tuned model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Phenaki model's performance improve significantly with larger model sizes or longer training on the StoryBench tasks?
- Basis in paper: [explicit] The paper uses a small 345M parameter Phenaki model and notes that larger models and longer training could lead to better performance, especially for multi-task learning.
- Why unresolved: The paper only reports results using the small model and does not explore scaling up the model size or training duration.
- What evidence would resolve it: Training and evaluating larger Phenaki models (e.g., 1B+ parameters) on StoryBench with extended training schedules, comparing performance to the small model baseline.

### Open Question 2
- Question: How well do the StoryBench tasks generalize to longer videos with more complex storylines, scene changes, and professional filmmaking techniques?
- Basis in paper: [inferred] The paper acknowledges that StoryBench focuses on real-world user-generated videos which are mostly single-shot and not very long, and notes this as a limitation for industrial-level video generation.
- Why unresolved: The current benchmark datasets do not include the longer, more complex videos needed to fully test generalization.
- What evidence would resolve it: Collecting and annotating longer videos (e.g., 1-5 minutes) with multiple scenes, camera movements, and professional production elements, then evaluating models on these extended benchmarks.

### Open Question 3
- Question: Can the automatic pipeline for converting VidLN captions to story-like data be further improved to better match human annotations in terms of caption quality and timing accuracy?
- Basis in paper: [explicit] The paper presents an automatic pipeline that splits captions and estimates timestamps, achieving 63.6% BLEU4 score compared to human references, but notes room for improvement.
- Why unresolved: The pipeline produces captions and timestamps that are similar but not identical to human annotations, particularly missing context enrichment.
- What evidence would resolve it: Developing and testing improved algorithms for caption splitting and timestamp estimation, measuring performance gains in BLEU4 and timestamp accuracy against human annotations.

## Limitations
- Evaluation alignment: Significant gap between automatic metrics and human evaluation, with metrics showing different trends than human preferences
- Dataset coverage: Benchmark relies on three existing datasets that may not capture full diversity of real-world scenarios
- Generalization: Findings based on Phenaki architecture may not generalize to other video generation models

## Confidence
**High Confidence**:
- Benchmark collection process and human annotation methodology are well-documented and reproducible
- Improvement from zero-shot to continuation fine-tuning is consistently demonstrated

**Medium Confidence**:
- Claims about multi-task fine-tuning introducing competing objectives are supported by comparative results but lack mechanistic explanation
- Effectiveness of automatic transformation pipeline demonstrated indirectly through model performance

**Low Confidence**:
- Claim that automatic metrics don't align with human judgments based on observed discrepancies rather than systematic analysis
- Generalizability to other architectures beyond Phenaki not established

## Next Checks
1. **Independent Pipeline Validation**: Evaluate the automatic caption transformation pipeline's accuracy by comparing its outputs against manually transformed story-like data for a subset of the dataset.

2. **Metric Behavior Analysis**: Conduct a systematic study of why automatic metrics diverge from human preferences, including correlation analysis between specific metrics and human judgment dimensions.

3. **Cross-Architecture Generalization**: Test whether the benchmark's findings about multi-task training effects hold when using different video generation architectures beyond Phenaki.