---
ver: rpa2
title: Making Retrieval-Augmented Language Models Robust to Irrelevant Context
arxiv_id: '2310.01558'
source_url: https://arxiv.org/abs/2310.01558
tags:
- retrieval
- context
- question
- questions
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Retrieval-augmented language models (RALMs) improve open-domain
  question answering (ODQA) by incorporating external evidence, but their performance
  can degrade when irrelevant context is retrieved. This paper analyzes the robustness
  of RALMs to irrelevant context across five ODQA benchmarks, finding that retrieval
  augmentation can harm performance, especially on multi-hop reasoning tasks.
---

# Making Retrieval-Augmented Language Models Robust to Irrelevant Context

## Quick Facts
- arXiv ID: 2310.01558
- Source URL: https://arxiv.org/abs/2310.01558
- Reference count: 39
- Primary result: RALMs can be fine-tuned on 1,000 examples to be robust to irrelevant contexts while maintaining high accuracy on relevant examples

## Executive Summary
Retrieval-augmented language models (RALMs) enhance open-domain question answering (ODQA) by incorporating external evidence, but performance degrades when irrelevant context is retrieved. This paper systematically analyzes RALM robustness to irrelevant context across five ODQA benchmarks and proposes two mitigation strategies. The first uses NLI models to filter irrelevant contexts at inference time, while the second fine-tunes RALMs on automatically generated data mixing relevant and irrelevant contexts. The fine-tuning approach proves particularly effective, requiring only 1,000 examples to achieve robustness without sacrificing performance on relevant examples.

## Method Summary
The paper addresses RALM robustness through two complementary approaches. First, an NLI-based filtering system classifies retrieved contexts as relevant or irrelevant using entailment predictions, allowing the model to generate answers without distracting information. Second, the authors propose fine-tuning RALMs on automatically generated training data that combines relevant contexts with irrelevant ones (low-ranked or random passages). For multi-hop questions, the method generates intermediate decompositions using a strong LLM, then applies self-consistency to identify high-quality examples. Both approaches are evaluated on five ODQA benchmarks with various retrieval scenarios.

## Key Results
- Retrieval augmentation can harm RALM performance, especially on multi-hop reasoning tasks
- NLI filtering effectively removes irrelevant contexts but may discard relevant passages
- Fine-tuning on 1,000 mixed-context examples achieves robustness while maintaining accuracy on clean examples
- Both methods show consistent improvements across five ODQA benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** NLI models prevent performance degradation by filtering irrelevant contexts
- **Mechanism:** NLI model evaluates if question-answer pairs are entailed by retrieved context, discarding non-entailed passages
- **Core assumption:** NLI model reliably identifies truly irrelevant contexts
- **Evidence anchors:** Abstract mentions NLI filtering as a baseline; section describes entailment-based classification
- **Break condition:** NLI model rejects relevant contexts (false positives) or accepts irrelevant ones (false negatives)

### Mechanism 2
- **Claim:** Fine-tuning on mixed contexts makes RALMs robust to noise
- **Mechanism:** Exposure to both relevant and irrelevant contexts during training teaches the model to extract useful information while ignoring noise
- **Core assumption:** 1,000 examples suffice for learning robustness distinction
- **Evidence anchors:** Abstract states 1,000 examples achieve robustness; section describes fine-tuning procedure
- **Break condition:** Insufficient training data or irrelevant contexts too similar to relevant ones

### Mechanism 3
- **Claim:** Generating intermediate decompositions enables robust multi-hop training
- **Mechanism:** LLM generates decompositions without retrieval, self-consistency verifies quality, then mixed contexts create training examples
- **Core assumption:** Decomposition generation produces high-quality reasoning chains
- **Evidence anchors:** Section describes decomposition generation and verification process
- **Break condition:** Incorrect or suboptimal reasoning chains lead to wrong learning patterns

## Foundational Learning

- **Concept:** Natural Language Inference (NLI)
  - **Why needed here:** NLI models filter irrelevant contexts by determining if retrieved evidence supports question-answer pairs
  - **Quick check question:** Can you explain the difference between entailment, contradiction, and neutral in NLI classification?

- **Concept:** Chain-of-Thought Reasoning
  - **Why needed here:** Multi-hop questions require intermediate reasoning steps that the model must generate and answer correctly
  - **Quick check question:** How would you break down "Who was president when first man landed on moon?" into intermediate questions?

- **Concept:** Self-Consistency in Generation
  - **Why needed here:** Verifies quality of automatically generated decompositions by sampling multiple times and checking for agreement
  - **Quick check question:** What does it mean if multiple sampled decompositions lead to different final answers?

## Architecture Onboarding

- **Component map:** Retriever -> NLI Filter (optional) -> RALM Model -> Data Generator -> Evaluation Pipeline
- **Critical path:** Question received → Retrieval → (Optional) NLI filtering → Answer generation → Evaluation
- **Design tradeoffs:** NLI filtering prevents degradation but may discard useful context; no filtering allows all context but risks distraction
- **Failure signatures:** Performance drops with irrelevant context, NLI rejects relevant contexts, incorrect intermediate questions/answers in multi-hop reasoning
- **First 3 experiments:** 1) Evaluate baseline RALM with top-1 vs no retrieval on NQ dataset; 2) Implement NLI filtering and measure impact; 3) Fine-tune on 1,000 mixed-context examples and compare robustness

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does NLI effectiveness vary across different question types (single-hop vs multi-hop, explicit vs implicit reasoning)?
- **Basis in paper:** [explicit] Paper shows NLI models are effective but effectiveness varies across datasets and question types without detailed analysis
- **Why unresolved:** No systematic study comparing NLI performance across specific question types with varying retriever strengths
- **What evidence would resolve it:** Systematic study comparing NLI model performance on identifying irrelevant contexts across different question types with varying noise levels

### Open Question 2
- **Question:** What is optimal trade-off between training examples and RALM robustness to irrelevant contexts?
- **Basis in paper:** [explicit] Paper shows 1,000 examples suffice but doesn't explore full range of dataset sizes
- **Why unresolved:** Only tests with 1,000 examples without exploring whether fewer examples could achieve similar robustness
- **What evidence would resolve it:** Systematic study varying training examples from 100 to 10,000 and measuring resulting robustness and accuracy

### Open Question 3
- **Question:** How do different strategies for generating irrelevant contexts during training affect RALM robustness?
- **Basis in paper:** [inferred] Paper mentions using low-ranked or random passages but doesn't explore alternative strategies
- **Why unresolved:** No comparison of different irrelevant context generation strategies or their effects on model robustness
- **What evidence would resolve it:** Comparative study testing different strategies (low-ranked, random, adversarial, semantically similar but incorrect) and measuring impact on robustness

## Limitations
- NLI model quality not empirically validated with precision/recall metrics for context relevance classification
- Automatic decomposition generation process lacks empirical validation of decomposition quality
- Claim that 1,000 examples suffice lacks statistical power analysis or comparison to other dataset sizes

## Confidence

**High Confidence:** Retrieval augmentation harming performance with irrelevant context is well-supported by experimental results across multiple benchmarks.

**Medium Confidence:** NLI filtering effectiveness is supported but limited by lack of detailed false positive/false negative analysis and NLI model characterization.

**Low Confidence:** Claim about 1,000 examples being sufficient lacks supporting analysis of the relationship between dataset size and performance gains.

## Next Checks

1. **NLI Filter Analysis:** Implement comprehensive evaluation measuring precision, recall, and F1-score for context relevance classification; manually annotate filtered examples to verify NLI model accuracy.

2. **Fine-tuning Dataset Size Sensitivity:** Systematically vary fine-tuning examples (100, 500, 1000, 2000, 5000) and measure relationship between dataset size and robustness to irrelevant contexts.

3. **Decomposition Quality Assessment:** Generate decompositions for held-out multi-hop questions and have human annotators rate correctness and completeness; correlate decomposition quality with downstream model performance.