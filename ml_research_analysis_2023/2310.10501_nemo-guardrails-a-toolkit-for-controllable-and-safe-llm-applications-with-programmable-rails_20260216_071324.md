---
ver: rpa2
title: 'NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with
  Programmable Rails'
arxiv_id: '2310.10501'
source_url: https://arxiv.org/abs/2310.10501
tags:
- rails
- guardrails
- dialogue
- rail
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NeMo Guardrails is an open-source toolkit for adding programmable
  rails to LLM-based conversational systems, enabling controllable and safe applications.
  It uses a runtime engine that acts as a proxy between users and the LLM, enforcing
  user-defined rules expressed in Colang, a modeling language for dialogue flows.
---

# NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails

## Quick Facts
- arXiv ID: 2310.10501
- Source URL: https://arxiv.org/abs/2310.10501
- Reference count: 28
- Key outcome: NeMo Guardrails is an open-source toolkit for adding programmable rails to LLM-based conversational systems, enabling controllable and safe applications.

## Executive Summary
NeMo Guardrails is an open-source toolkit that adds programmable rails to LLM-based conversational systems, enabling controllable and safe applications. It uses a runtime engine that acts as a proxy between users and the LLM, enforcing user-defined rules expressed in Colang, a modeling language for dialogue flows. The toolkit employs a three-step approach: generating canonical forms for user input, deciding next steps, and generating bot messages, using chain-of-thought prompting and a dialogue manager. It supports topical rails for controlling dialogue and execution rails for safety, including fact-checking, hallucination detection, and moderation. Evaluation on datasets like Banking and MSMARCO shows strong performance, with models like gpt-3.5-turbo and text-davinci-003 achieving high accuracy in rail enforcement.

## Method Summary
NeMo Guardrails uses a three-step approach to guide LLM responses: (1) generate a canonical form for user input, (2) decide the next step in the dialogue flow, and (3) generate the bot message. This is done using chain-of-thought prompting with in-context learning, where the LLM is prompted to generate intermediate steps that guide the conversation. Colang scripts define the dialogue flows and canonical forms, which are interpreted by a dialogue manager to apply the guardrails rules. Execution rails allow custom Python actions to be called from Colang flows for safety checks and integrations.

## Key Results
- Topical rails evaluation shows strong performance on Banking and MSMARCO datasets, with models like gpt-3.5-turbo and text-davinci-003 achieving high accuracy in rail enforcement.
- The three-step approach (canonical form generation, next step decision, bot message generation) offers improvements in performance compared to using LLM generation alone.
- Execution rails provide safety features such as fact-checking, hallucination detection, and moderation, with fact-checking accuracy around 80% and hallucination detection rates of 70-95% for unanswerable prompts.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Guardrails runtime enforces user-defined rules independently of the underlying LLM, providing interpretability.
- Mechanism: Uses a dialogue manager that interprets Colang scripts—a modeling language combining natural language and Python—to guide conversation flows.
- Core assumption: Colang rules are sufficiently expressive to capture the desired dialogue behaviors and can be correctly interpreted by the runtime.
- Evidence anchors:
  - [abstract] "using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable."
  - [section] "Using a prompting technique with in-context learning and a specific form of CoT, we enable the LLM to generate the next steps that guide the conversation. Colang is then interpreted by the dialogue manager to apply the guardrails rules predefined by users or automatically generated by the LLM to guide the behavior of the LLM."
  - [corpus] Weak - no direct evidence from cited papers that dialogue management runtime effectively enforces arbitrary Colang rules.
- Break condition: If the Colang interpreter cannot correctly parse or execute a user-defined rule, or if the LLM fails to generate appropriate next steps given the context.

### Mechanism 2
- Claim: The three-step approach (canonical form generation, next step decision, bot message generation) improves topical rail performance over using LLM generation alone.
- Mechanism: Each step conditions on the previous output, progressively refining the response to align with the guardrail rules.
- Core assumption: Each intermediate step (canonical form, next step) provides meaningful constraint that improves final LLM output.
- Evidence anchors:
  - [section] "The topical rails evaluation highlights several important aspects. First, each step in the three-step approach (user canonical form, next step, bot message) used by Guardrails offers an improvement in performance."
  - [section] "Evaluation on datasets like Banking and MSMARCO shows strong performance, with models like gpt-3.5-turbo and text-davinci-003 achieving high accuracy in rail enforcement."
  - [corpus] Weak - no direct evidence from cited papers that multi-step prompting improves topical control compared to single-step generation.
- Break condition: If the LLM cannot generate accurate canonical forms or next steps, or if intermediate conditioning does not improve final output.

### Mechanism 3
- Claim: Execution rails (custom actions) allow complex safety checks and integrations with external services.
- Mechanism: Python-defined actions are called from Colang flows, receiving conversation context and returning values that influence the dialogue.
- Core assumption: Custom actions can be written to perform the desired safety or integration tasks effectively.
- Evidence anchors:
  - [section] "Execution rails call custom actions defined by the app developer; we will focus on a set of safety rails available to all Guardrails apps."
  - [section] "Execution rails can be used for a wide range of tasks, we provide several rails for LLM safety covering fact-checking, hallucination, and moderation."
  - [section] "To implement user-defined programmable rails for LLMs, our toolkit uses a programmable runtime engine that acts like a proxy between the user and the LLM."
  - [corpus] Weak - no direct evidence from cited papers that custom actions are effective for complex safety tasks.
- Break condition: If the custom action logic is flawed, or if the action fails to correctly interpret the conversation context.

## Foundational Learning

- Concept: Dialogue Management
  - Why needed here: Understanding how dialogue managers interpret and enforce conversation flows is key to grasping how Guardrails controls LLM behavior.
  - Quick check question: What are the main components of a dialogue manager, and how does the Guardrails runtime use them to enforce Colang rules?

- Concept: Prompt Engineering and Chain-of-Thought
  - Why needed here: Guardrails uses complex prompts chained in multiple steps to guide LLM responses, so understanding prompt engineering is essential.
  - Quick check question: How does chain-of-thought prompting differ from standard prompting, and why is it useful for generating intermediate steps like canonical forms?

- Concept: Canonical Forms and Semantic Similarity
  - Why needed here: Canonical forms represent user intents in a flexible way, and semantic similarity is used to match new inputs to existing forms.
  - Quick check question: What is the difference between canonical forms and traditional intents, and how does semantic similarity improve matching?

## Architecture Onboarding

- Component map:
  - User input → Guardrails runtime (dialogue manager) → LLM → Output
  - Guardrails runtime interprets Colang scripts, generates canonical forms, decides next steps, and generates bot messages.
  - Execution rails are custom Python actions called from Colang flows.

- Critical path:
  1. User sends message to Guardrails runtime.
  2. Runtime generates canonical form for user message.
  3. Runtime decides next step based on canonical form and active flows.
  4. Runtime generates bot message conditioned on next step.
  5. If next step calls an execution rail, the custom action is executed and its output is used in the dialogue.

- Design tradeoffs:
  - Using a runtime proxy adds latency and cost (3x compared to direct LLM calls) but enables programmability and interpretability.
  - Colang rules are interpretable but require developers to learn a new modeling language.
  - Execution rails add flexibility but require custom Python code to be written and maintained.

- Failure signatures:
  - If Colang rules are not correctly interpreted, the dialogue may veer off-topic or produce unexpected responses.
  - If canonical form generation fails, the LLM may not recognize the user intent and produce irrelevant responses.
  - If custom actions are buggy or slow, the Guardrails app may hang or produce incorrect outputs.

- First 3 experiments:
  1. Install Guardrails and run a simple topical rail example (e.g., greeting flow) to see the three-step prompting in action.
  2. Create a Colang flow that uses a custom action (e.g., a simple math calculation) to test execution rails.
  3. Evaluate a Guardrails app on a small NLU dataset (e.g., Banking) to measure the accuracy of canonical form generation and next step prediction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the latency and cost of the three-step CoT prompting approach used by NeMo Guardrails compare to a normal bot message generation without Guardrails?
- Basis in paper: [explicit] The paper states that the three-step CoT prompting approach incurs extra costs and latency, about 3 times the latency and cost of a normal call to generate the bot message without using Guardrails.
- Why unresolved: The paper does not provide specific numbers or comparisons for latency and cost.
- What evidence would resolve it: Detailed latency and cost measurements comparing the Guardrails approach to a normal bot message generation without Guardrails.

### Open Question 2
- Question: Can the Guardrails runtime use a single call to generate all three steps (user canonical form, next steps in the flow, and bot message) instead of the current three-step approach?
- Basis in paper: [inferred] The paper mentions that the current implementation uses a three-step CoT prompting approach and states that they are investigating if in some cases a single call could be used to generate all three steps.
- Why unresolved: The paper does not provide any results or conclusions regarding the feasibility or performance of using a single call.
- What evidence would resolve it: Experimental results comparing the performance and efficiency of using a single call versus the three-step approach.

### Open Question 3
- Question: How do the topical rails perform on other NLU datasets besides the chit-chat and banking datasets mentioned in the paper?
- Basis in paper: [explicit] The paper mentions that topical rails evaluation focuses on the core mechanism used by NeMo Guardrails to guide conversations using canonical forms and dialogue flows, and provides results for the chit-chat and banking datasets.
- Why unresolved: The paper does not provide any results or discussion about the performance of topical rails on other NLU datasets.
- What evidence would resolve it: Evaluation results and analysis of topical rails performance on a variety of NLU datasets.

## Limitations

- The effectiveness of Colang as a modeling language depends heavily on its expressiveness and the dialogue manager's ability to correctly interpret rules, but the paper provides limited empirical evidence on the robustness of rule interpretation across diverse scenarios.
- The three-step prompting approach shows improvements in topical rail performance, but the paper doesn't provide direct comparisons to alternative single-step or two-step approaches, making it difficult to isolate the specific contribution of each step.
- Execution rails rely on custom Python actions, but the paper doesn't discuss the potential security implications of executing arbitrary code or provide guidelines for safely implementing these actions.

## Confidence

- High confidence in the overall toolkit architecture and its ability to enforce user-defined rules through a runtime proxy.
- Medium confidence in the specific mechanisms (three-step prompting, Colang interpretation) and their relative contributions to performance improvements.
- Low confidence in the generalizability of the safety rail results to real-world scenarios, as the evaluation datasets may not fully capture the complexity and diversity of user inputs.

## Next Checks

1. Conduct a controlled experiment comparing the three-step prompting approach to alternative single-step and two-step approaches on the same topical rail datasets to isolate the contribution of each step.
2. Implement a security analysis of the execution rails mechanism, testing for potential vulnerabilities in custom Python actions and evaluating the toolkit's ability to sandbox these actions.
3. Evaluate the toolkit's performance on a diverse set of real-world user inputs, including adversarial examples and edge cases, to assess its robustness and generalizability.