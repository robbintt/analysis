---
ver: rpa2
title: Last layer state space model for representation learning and uncertainty quantification
arxiv_id: '2307.01566'
source_url: https://arxiv.org/abs/2307.01566
tags:
- learning
- uncertainty
- neural
- estimation
- monte
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses uncertainty quantification in deep sequential
  models by proposing a two-stage approach that separates representation learning
  from uncertainty estimation. The method trains a deep neural network to extract
  features from time series data, then applies a state space model on the last layer
  whose parameters are estimated using Sequential Monte Carlo methods.
---

# Last layer state space model for representation learning and uncertainty quantification

## Quick Facts
- arXiv ID: 2307.01566
- Source URL: https://arxiv.org/abs/2307.01566
- Reference count: 40
- Primary result: Achieves RMSE of 0.24 and PICP of 98% on ETT dataset, outperforming MC Dropout (RMSE 0.25, PICP 59%) and HMM baselines (RMSE 0.44, PICP 85%)

## Executive Summary
This paper proposes a two-stage approach for uncertainty quantification in deep sequential models that separates representation learning from uncertainty estimation. The method first trains a deep neural network to extract features from time series data, then applies a state space model on the last layer whose parameters are estimated using Sequential Monte Carlo methods. This architecture allows complex predictive distributions while keeping computational burden manageable, and produces well-calibrated confidence intervals compared to dropout-based approaches.

## Method Summary
The approach decomposes the forecasting task into two stages: first, a 3-layer GRU network is trained to extract low-dimensional representations from high-dimensional time series inputs; second, a state space model is applied to these learned representations with parameters estimated via Sequential Monte Carlo methods. The state space model uses tanh and sigmoid activation functions for transition and observation components, with N=100 particles for SMC approximation. The method is evaluated on the ETT dataset for 24-hour ahead electricity transformer temperature forecasting, using RMSE and PICP as primary metrics.

## Key Results
- Achieves lower RMSE (0.24) compared to MC Dropout (0.25) and HMM baselines (0.44)
- Produces better calibrated uncertainty estimates with PICP of 98% versus 59% for MC Dropout
- Provides credible confidence intervals that reflect true uncertainty in predictions

## Why This Works (Mechanism)

### Mechanism 1
The two-stage decoupled approach improves uncertainty quantification by separating high-dimensional representation learning from low-dimensional state space modeling. By first training a deep neural network to extract task-relevant features, then applying a state space model only on these learned representations, the method reduces the effective dimensionality of the uncertainty estimation problem while preserving the expressive power of the deep architecture. This assumes the deep neural network can learn a meaningful low-dimensional representation that captures essential dynamics of the time series.

### Mechanism 2
Sequential Monte Carlo methods enable tractable parameter estimation in the state space model despite intractable likelihoods. SMC methods approximate the posterior distribution of hidden states using weighted particle sets, allowing gradient-based optimization of model parameters through Fisher's identity without requiring explicit likelihood computation. This assumes the particle filter and smoother can adequately approximate the posterior distribution of the hidden states given the observations.

### Mechanism 3
The decoupled architecture avoids overconfidence issues of traditional dropout-based uncertainty methods. By using SMC to estimate the full predictive distribution rather than approximating uncertainty through stochastic forward passes, the method produces well-calibrated confidence intervals that reflect true uncertainty in the predictions. This assumes the state space model with SMC can capture the true complexity of the predictive distribution better than simple parametric approximations.

## Foundational Learning

- **State Space Models and Hidden Markov Models**: The paper relies on state space models as the uncertainty quantification layer, which requires understanding of hidden Markov models, filtering, and smoothing algorithms. Quick check: What is the difference between filtering and smoothing in the context of state space models?

- **Sequential Monte Carlo Methods**: The paper uses SMC for parameter estimation in the state space model, which requires understanding particle filters, importance sampling, and resampling techniques. Quick check: How does the bootstrap particle filter work and what is the role of importance weights?

- **Gradient-Based Optimization with Intractable Likelihoods**: The paper uses Fisher's identity to estimate gradients when the likelihood is not available in closed form, which requires understanding of score function estimation. Quick check: What is Fisher's identity and how does it enable gradient estimation for models with intractable likelihoods?

## Architecture Onboarding

- **Component map**: Input time series -> GRU feature extraction -> state space model prediction -> SMC-based uncertainty quantification
- **Critical path**: The critical path for inference is: input time series → GRU feature extraction → state space model prediction → SMC-based uncertainty quantification
- **Design tradeoffs**: The approach trades computational efficiency (by keeping the input model fixed) for improved uncertainty quantification; alternatively, one could jointly optimize all parameters but at much higher computational cost
- **Failure signatures**: Overconfidence in predictions suggests issues with the state space model specification or SMC approximation; poor RMSE indicates the input model is not extracting useful features
- **First 3 experiments**:
  1. Implement the GRU input model and verify it can learn to predict the next observation in a simple synthetic time series
  2. Implement the state space model layer and verify it can learn with synthetic data where the true parameters are known
  3. Combine both components and test on a small subset of the ETT dataset to verify the full pipeline works before scaling up

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unexplored:
- How the performance scales on much longer time series data compared to traditional deep learning approaches
- Whether the two-stage approach can be effectively extended to multi-step ahead forecasting tasks beyond the 24-hour horizon tested
- How computational efficiency compares to traditional deep learning approaches when dealing with very high-dimensional input features or multiple parallel time series

## Limitations
- Requires training two separate models sequentially, potentially missing benefits from joint optimization
- SMC-based parameter estimation can be computationally intensive, particularly for longer sequences
- State space model specification relies on manual design choices for transition and observation functions that may not generalize well across all domains

## Confidence

- **High confidence**: The RMSE improvements over baselines (0.24 vs 0.25 for MC Dropout and 0.44 for HMM) are substantial and statistically meaningful given the dataset size
- **Medium confidence**: The PICP results showing better calibration (98% vs 59% for MC Dropout) are compelling but depend on the specific evaluation protocol and confidence level chosen
- **Low confidence**: The general applicability of the two-stage approach to domains beyond electricity load forecasting remains unproven, as the method has only been validated on the ETT dataset

## Next Checks

1. **Cross-dataset generalization test**: Evaluate the method on multiple time series forecasting datasets (e.g., traffic, weather, financial data) to assess whether the performance gains generalize beyond electricity data

2. **Ablation study on state space complexity**: Systematically vary the state dimension and transition function complexity to determine the optimal model capacity for different data regimes

3. **Joint optimization comparison**: Implement a variant that jointly optimizes the input model and state space parameters to quantify the trade-off between computational efficiency and potential performance gains