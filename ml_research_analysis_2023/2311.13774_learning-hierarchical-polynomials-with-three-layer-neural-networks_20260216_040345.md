---
ver: rpa2
title: Learning Hierarchical Polynomials with Three-Layer Neural Networks
arxiv_id: '2311.13774'
source_url: https://arxiv.org/abs/2311.13774
tags:
- lemma
- have
- neural
- where
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies learning hierarchical polynomials of the form\
  \ g \u25E6 p over standard Gaussian distributions using three-layer neural networks.\
  \ The main result shows that for a large class of degree-k polynomials p, a three-layer\
  \ network trained via layerwise gradient descent learns h = g \u25E6 p up to vanishing\
  \ error in \xD5(d^k) samples and polynomial time, improving over kernel methods\
  \ requiring \xD5(d^(kq)) samples."
---

# Learning Hierarchical Polynomials with Three-Layer Neural Networks

## Quick Facts
- arXiv ID: 2311.13774
- Source URL: https://arxiv.org/abs/2311.13774
- Reference count: 40
- Key outcome: Three-layer networks learn hierarchical polynomials h = g ◦ p in Õ(d^k) samples versus Õ(d^(kq)) for kernel methods

## Executive Summary
This paper establishes that three-layer neural networks trained via layerwise gradient descent can efficiently learn hierarchical polynomial functions of the form h = g ◦ p over standard Gaussian inputs. The key insight is that these networks perform feature learning to extract the hidden polynomial p in Õ(d^k) samples, then learn the link function g with only Õ(1) additional samples. This achieves a significant improvement over kernel methods which require Õ(d^(kq)) samples. The proof leverages an approximate Stein's lemma showing that the degree-k projection of g ◦ p is proportional to p, enabling the network to identify the hidden feature structure.

## Method Summary
The method uses a three-layer network with a bottleneck layer and residual connection, trained in two stages via layerwise gradient descent. In stage 1, the network optimizes parameters in the bottleneck layer to minimize loss on a dataset D1, effectively learning the degree-k projection of the target function. In stage 2, the network optimizes the output layer parameters to learn the link function g using the feature extracted in stage 1. The width of the bottleneck layer scales as d^(k+α) for some α > 0, and sample complexity scales as d^(k+3α). The activation function σ₁ is chosen as a degree-k polynomial with non-zero leading coefficient, while σ₂ is ReLU.

## Key Results
- Three-layer networks learn hierarchical polynomials h = g ◦ p in Õ(d^k) samples versus Õ(d^(kq)) for kernel methods
- The layerwise training procedure enables feature learning in the first stage and link function learning in the second stage
- An approximate Stein's lemma shows P_kh ≈ E[g'(z)] · p, enabling efficient feature extraction
- Experiments confirm the theoretical predictions about sample complexity scaling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Three-layer networks learn hierarchical polynomials in Õ(d^k) samples by extracting the hidden feature p in the first stage and learning g in the second stage.
- Mechanism: Layerwise gradient descent first fits the best degree-k polynomial approximation to h (which is P_≤k h), then approximates P_k h ≈ E_z~N(0,1)[g'(z)] · p via an approximate Stein's lemma, effectively learning p. The second stage uses p as input to learn g via 1D kernel regression.
- Core assumption: The hidden feature p lies in the span of degree-k Hermite polynomials and has the orthogonal decomposition structure described in Assumption 4.
- Evidence anchors:
  - [abstract] "Our proof proceeds by showing that during the initial stage of training the network performs feature learning to recover the feature p with Õ(d^k) samples"
  - [section] "Lemma 2 shows that the term P_≤k h is approximately equal to P_k h, and furthermore, up to a scaling constant, P_k h is approximately equal to the hidden feature p"
- Break condition: If p cannot be decomposed into orthogonal features as required by Assumption 4, or if the feature learning stage fails to converge to P_≤k h.

### Mechanism 2
- Claim: The approximate Stein's lemma generalizes the result from Nichani et al. [2023] to degree k > 2 polynomials.
- Mechanism: For h = g ◦ p, the degree-k projection P_k h is approximately proportional to p with proportionality constant E_z~N(0,1)[g'(z)]. This follows from CLT-like behavior of p and approximate independence between p and degree <k polynomials.
- Core assumption: The feature p is a sum of balanced orthogonal components with L = Θ(d), making p approximately Gaussian.
- Evidence anchors:
  - [abstract] "Our proof proceeds by showing that during the initial stage of training the network performs feature learning to recover the feature p with Õ(d^k) samples"
  - [section] "Our key technical result, and a main innovation of our paper, is Lemma 2. It shows that the term P_≤k h is approximately equal to P_k h, and furthermore, up to a scaling constant, P_k h is approximately equal to the hidden feature p"
- Break condition: If p is not sufficiently spread out (e.g., L = o(d)) or the link function g has information exponent significantly different from 1.

### Mechanism 3
- Claim: The layerwise training procedure is not equivalent to kernel methods because it can adapt to the hierarchical structure.
- Mechanism: Stage 1 implements kernel regression in d-dimensions to learn p, while stage 2 implements 1D kernel regression to learn g. This two-stage approach leverages the hierarchical structure, unlike NTK which requires Õ(d^(kq)) samples.
- Core assumption: The network architecture with bottleneck layer and residual connection enables effective feature extraction in the first stage.
- Evidence anchors:
  - [abstract] "Our main result shows that for a large subclass of degree k polynomials p, a three-layer neural network trained via layerwise gradient descent... learns the target h up to vanishing test error in Õ(d^k) samples"
  - [section] "Our result also generalizes prior works on three-layer neural networks, which were restricted to the case of p being a quadratic"
- Break condition: If the activation function σ₁ is not a polynomial of degree k with non-zero leading coefficient, or if the width m₁ is insufficient for feature learning.

## Foundational Learning

- Concept: Hermite polynomial expansion and projection operators
  - Why needed here: The proof relies on decomposing functions into Hermite polynomial bases and projecting onto subspaces of specific degrees. Lemma 2 specifically requires understanding how P_k operates on g ◦ p.
  - Quick check question: Can you write the Hermite expansion of a degree-3 polynomial h(x) = x₁³ + x₁x₂² over standard Gaussian inputs?

- Concept: Gaussian hypercontractivity and polynomial concentration
  - Why needed here: Bounding higher moments of polynomials in Gaussian inputs is crucial for generalization bounds and concentration arguments throughout the proof.
  - Quick check question: Given a degree-2 polynomial f(x) with E[f(x)²] = 1, what is the maximum value of E[f(x)⁴]?

- Concept: Random feature model approximation
  - Why needed here: The first stage of training can be analyzed as fitting a random feature model with features σ₁(Vx + s), which is key to understanding how the network learns the feature p.
  - Quick check question: If σ(z) is a degree-k polynomial and V ∈ R^(m×d) has i.i.d. Gaussian entries, what is the expected dimension of the span of {σ(Vx + s) : x ∈ R^d}?

## Architecture Onboarding

- Component map: Input x ∈ R^d -> Bottleneck u^Tσ₁(Vx + s) -> Residual connection u^Tσ₁(Vx + s) -> Output u^Tσ₁(Vx + s) + c^Tσ₂(a · u^Tσ₁(Vx + s) + b)

- Critical path:
  1. Stage 1: Train u to minimize empirical loss on dataset D₁ (fixed V, s, a, b, c=0)
  2. Stage 2: Train c to minimize empirical loss on dataset D₂ (fixed V, s, a, b, u from stage 1)
  3. Output: Network parameters θ̂

- Design tradeoffs:
  - Wider m₁ enables better approximation of P_≤k h but increases computational cost
  - More gradient steps in stage 1 improves feature extraction but requires careful learning rate scheduling
  - Sample splitting between stages ensures independence for generalization bounds

- Failure signatures:
  - If test loss plateaus above target error floor, likely insufficient width m₁ or inadequate gradient steps in stage 1
  - If stage 1 converges but overall performance is poor, likely p doesn't satisfy Assumption 4 or activation σ₁ is poorly chosen
  - If training loss decreases but test loss doesn't, likely insufficient samples n or poor generalization due to high variance

- First 3 experiments:
  1. Implement stage 1 only with synthetic target h = g ◦ p where p(x) = Σᵢ₌₁^(d/3) λᵢh₃(xᵢ) and verify that u^Tσ₁(Vx + s) converges to P_≤k h
  2. Test correlation between learned feature u^Tσ₁(Vx + s) and true feature p as a function of sample size n and width m₁
  3. Run complete two-stage algorithm on hierarchical polynomial targets and measure sample complexity scaling with d and k compared to theoretical predictions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the results be generalized to all degree k polynomials p, not just the restricted class considered in the paper?
- Basis in paper: The authors conjecture in Section 6.3 that their results should still hold as long as p is homogeneous and close in distribution to a Gaussian, which should be true for more general tensors A.
- Why unresolved: The paper only proves results for a specific subclass of degree k polynomials p that can be written in a certain form with orthogonal vectors. Proving the results for all degree k polynomials would require new techniques to handle more general cases.
- What evidence would resolve it: A proof showing that the three-layer network can learn any degree k polynomial feature p, up to small error, in Õ(d^k) samples. This could involve generalizing the approximate Stein's lemma to handle more general polynomials.

### Open Question 2
- Question: Can deep networks efficiently learn targets that depend on multiple features, i.e. of the form h(x) = g(p₁(x), ..., p_R(x)) for some g : R^R → R?
- Basis in paper: The authors state in Section 6.3 that an interesting direction is to understand whether deep networks can efficiently learn such targets, beyond the single feature case considered in the paper.
- Why unresolved: The paper only considers learning targets that depend on a single hidden feature p. Extending this to multiple features would require new techniques to handle the additional complexity.
- What evidence would resolve it: A proof showing that a deep network can learn any target of the form h(x) = g(p₁(x), ..., p_R(x)), up to small error, in a sample complexity that depends polynomially on d, R, and the degrees of the p_i. This could involve extending the layerwise training procedure to handle multiple features.

### Open Question 3
- Question: How do three-layer networks trained via gradient descent on all parameters simultaneously compare to the layerwise training procedure analyzed in the paper?
- Basis in paper: The authors conduct experiments in Section 5 showing that three-layer networks trained via gradient descent on all parameters can learn the same hierarchical polynomials as the layerwise procedure. However, the theoretical analysis only covers the layerwise case.
- Why unresolved: The paper proves results for a specific layerwise training procedure, but does not analyze the more standard approach of training all parameters simultaneously via gradient descent. Understanding the relationship between these two approaches is an open question.
- What evidence would resolve it: A theoretical analysis showing that three-layer networks trained via gradient descent on all parameters can learn hierarchical polynomials with a sample complexity similar to the layerwise case. This could involve analyzing the optimization dynamics of simultaneous training and showing that it still performs effective feature learning.

## Limitations

- The theoretical guarantees require strong assumptions on the hidden feature structure (Assumption 4), which may not hold for arbitrary degree-k polynomials p
- The layerwise training approach, while theoretically sound, may not match the practical performance of end-to-end training methods used in modern deep learning
- The analysis is limited to Gaussian input distributions and may not generalize to other data distributions commonly encountered in practice

## Confidence

- High Confidence: The sample complexity bound Õ(d^k) versus Õ(d^(kq)) for kernel methods, and the general proof framework of feature extraction followed by link function learning
- Medium Confidence: The approximate Stein's lemma generalization to degree k > 2, and the specific technical conditions required for the balanced orthogonal decomposition
- Low Confidence: The practical implications of layerwise training versus end-to-end optimization, and performance on non-Gaussian input distributions

## Next Checks

1. Test the algorithm on hierarchical polynomial targets where p does not satisfy the balanced orthogonal decomposition (e.g., L << d) to identify the breaking point of the theoretical guarantees
2. Compare layerwise training performance against end-to-end training on the same hierarchical polynomial tasks to quantify the practical cost of the theoretical approach
3. Extend experimental validation to include non-Gaussian input distributions (e.g., uniform or sub-Gaussian) to assess the robustness of the sample complexity bounds