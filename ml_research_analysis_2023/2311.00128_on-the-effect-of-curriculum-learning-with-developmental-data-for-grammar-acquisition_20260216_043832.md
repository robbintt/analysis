---
ver: rpa2
title: On the effect of curriculum learning with developmental data for grammar acquisition
arxiv_id: '2311.00128'
source_url: https://arxiv.org/abs/2311.00128
tags:
- data
- curriculum
- training
- speech
- corpora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study examines whether grammar acquisition in language models
  can be improved through curriculum learning and the use of developmentally plausible
  data, specifically transcribed speech. Using BabyBERTa as a probe, the authors find
  that grammar acquisition is primarily driven by exposure to speech data, particularly
  from the AO-Childes and Open Subtitles corpora.
---

# On the effect of curriculum learning with developmental data for grammar acquisition

## Quick Facts
- arXiv ID: 2311.00128
- Source URL: https://arxiv.org/abs/2311.00128
- Authors: 
- Reference count: 3
- Key outcome: Grammar acquisition in language models is primarily driven by exposure to speech data, particularly AO-Childes and Open Subtitles corpora, with curriculum learning showing benefits mainly when high-utility speech data is scarce.

## Executive Summary
This study investigates whether curriculum learning and developmentally plausible data can improve grammar acquisition in language models. Using BabyBERTa as a probe, the authors find that grammar acquisition is primarily driven by exposure to transcribed speech data, particularly from the AO-Childes and Open Subtitles corpora. They explore various sequence complexity-based curricula, block-based learning, and curricula varying exposure to different corpora. The results show that over-exposure to these high-utility speech corpora significantly drives performance, and that the proportion of training steps assigned to high-utility data, rather than the proportion of tokens, is crucial for acquisition.

## Method Summary
The authors train a modified BabyBERTa model (8 transformer layers, hidden size 512, feed-forward dimension 2048, vocabulary size 30k) on BabyLM training corpora including transcribed speech and text data. They implement various curricula based on sequence complexity measures (entropy, unigram probability, block size) and compare these to random sampling baselines. The model is evaluated using zero-shot performance on BLiMP tasks, held out tasks, and full results from Dynabench. Training uses dynamic masking without unmasking and excludes sequences exceeding max length of 128 tokens.

## Key Results
- Transcribed speech data (AO-Childes and Open Subtitles) significantly improves BLiMP performance and reduces variance compared to text-only data
- Over-exposure to high-utility speech data drives grammar acquisition more than token proportion, with step proportion being the key factor
- Curriculum learning shows benefits primarily when high-utility speech data is scarce, with only slight improvements in the control dataset setting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Over-exposure to high-utility speech data drives grammar acquisition more than token proportion
- Mechanism: Disproportionate allocation of training steps to simpler, high-utility speech data allows more efficient grammatical knowledge acquisition
- Core assumption: Speech data contains simpler linguistic structures conducive to grammar acquisition
- Evidence anchors: [abstract] "proportion of training steps assigned to such data"; [section 6] "transcribed speech leads to improved BLiMP performance"

### Mechanism 2
- Claim: Linguistically coherent sequences improve grammar acquisition efficiency
- Mechanism: Complete utterances allow learning grammatical boundaries and dependencies within natural linguistic units
- Core assumption: Linguistic coherence provides better learning signals than arbitrary token blocks
- Evidence anchors: [section 5] "sequences as input still quite substantially outperform blocks"; [section 5.1] "linguistically coherent units as input is beneficial"

### Mechanism 3
- Claim: Curriculum learning benefits are most pronounced when high-utility speech data is scarce
- Mechanism: Ordering from simple to complex helps build foundational knowledge when high-utility data is limited
- Core assumption: Benefits are most pronounced when training data is not already weighted toward high-utility simple data
- Evidence anchors: [abstract] "proportion of training steps assigned to such data"; [section 8] "slight, but discernible improvement from using the curriculum"

## Foundational Learning

- Concept: Curriculum learning and its relationship to developmental data
  - Why needed here: Central to investigating how presenting data in order of complexity affects grammar acquisition
  - Quick check question: What is the key difference between single-phase and multi-phase curriculum learning approaches?

- Concept: Language model pretraining with limited data
  - Why needed here: BabyLM challenge focuses on pretraining with scarce, developmentally plausible data
  - Quick check question: Why might starting with speech data be beneficial for grammar acquisition?

- Concept: Zero-shot evaluation of grammar acquisition
  - Why needed here: Paper uses BLiMP and other tasks to evaluate inherent grammatical knowledge without fine-tuning
  - Quick check question: What is the primary evaluation metric used to assess grammar acquisition?

## Architecture Onboarding

- Component map: Data preprocessing → tokenization and complexity scoring → curriculum ordering → model training with dynamic masking → zero-shot grammar evaluation
- Critical path: Data preprocessing → tokenization and complexity scoring → curriculum ordering (if applicable) → model training with dynamic masking → zero-shot grammar evaluation
- Design tradeoffs: Model trades depth (12 layers in RoBERTa base vs. 8 here) for efficiency with smaller-scale data, potentially impacting performance on certain syntactic tasks
- Failure signatures: Poor BLiMP performance despite high training accuracy, high variance across seeds, catastrophic forgetting during curriculum transitions
- First 3 experiments:
  1. Baseline model with random sampling of all corpora for 120k steps
  2. Model using sequence complexity-based curricula (entropy, unigram probability, block) to test curriculum effectiveness
  3. Model using only speech data vs. only text data for 40k steps each to isolate modality effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How much disparity in token proportion between high-utility and low-utility data can be tolerated before benefits of starting small from transcribed speech are lost?
- Basis in paper: Authors suggest replacing CBT with larger proportion of Wikipedia (e.g., Wiki-103) could test this disparity
- Why unresolved: Current study doesn't explore varying ratios of high-utility versus low-utility data
- What evidence would resolve it: Experiments with datasets containing different ratios of high-utility transcribed speech to low-utility text data

### Open Question 2
- Question: Does starting large with complex data and transitioning to simpler data achieve the same results as starting small?
- Basis in paper: Authors suggest constructing a "complementary large-to-small complexity curriculum" to test this hypothesis
- Why unresolved: Study focuses on starting small and doesn't explore reverse approach
- What evidence would resolve it: Experiments comparing large-to-small vs. small-to-large curriculum performance

### Open Question 3
- Question: How does layer stacking affect grammar acquisition across different training regimes?
- Basis in paper: Authors used layer stacking in competition submission, progressively growing model through curriculum
- Why unresolved: Study doesn't provide complete picture of layer stacking effects across all regimes
- What evidence would resolve it: Experiments comparing models with and without layer stacking across different curricula

## Limitations
- Uncertain generalizability to larger-scale models or languages with different grammatical structures
- Curriculum learning benefits show only slight improvements in limited-data control setting
- Experimental design doesn't fully isolate whether step-proportional benefits are due to over-exposure or simply more frequent exposure to grammatical constructions

## Confidence
- High confidence: Speech data improves grammar acquisition performance and reduces variance (consistent across multiple experiments)
- Medium confidence: Curriculum learning benefits when high-utility speech data is scarce (modest effect size, limited comparison)
- Low confidence: Step proportion vs. token proportion mechanism (based on comparative results, needs direct ablation)

## Next Checks
1. Replicate core experiments using a morphologically rich language like Finnish or Turkish to test cross-linguistic generalizability
2. Vary model size (BabyBERTa vs. standard RoBERTa base) while keeping training data constant to isolate speech data benefits
3. Design direct comparison where identical models are trained on identical data with only difference being step-proportional vs. token-proportional allocation to high-utility speech data