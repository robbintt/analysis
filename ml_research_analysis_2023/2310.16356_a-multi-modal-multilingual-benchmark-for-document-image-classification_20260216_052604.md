---
ver: rpa2
title: A Multi-Modal Multilingual Benchmark for Document Image Classification
arxiv_id: '2310.16356'
source_url: https://arxiv.org/abs/2310.16356
tags:
- document
- languages
- dataset
- layoutxlm
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two new multilingual datasets, MULTI EURLEX-DOC
  and WIKI -DOC, for document image classification to address limitations in the existing
  RVL-CDIP dataset. MULTI EURLEX-DOC consists of European Union laws in 23 languages
  and is multi-label, while WIKI -DOC contains Wikipedia articles in diverse languages
  including non-Latin scripts.
---

# A Multi-Modal Multilingual Benchmark for Document Image Classification

## Quick Facts
- arXiv ID: 2310.16356
- Source URL: https://arxiv.org/abs/2310.16356
- Reference count: 25
- This paper introduces two new multilingual datasets for document image classification and evaluates popular Document AI models on them.

## Executive Summary
This paper addresses the limitations of the RVL-CDIP dataset by introducing two new multilingual datasets for document image classification: MULTI EURLEX-DOC (European Union laws in 23 languages, multi-label) and WIKI-DOC (Wikipedia articles in diverse languages including non-Latin scripts). The authors evaluate popular Document AI models including InfoXLM, LiLT, LayoutXLM, Donut, and DocFormer on these datasets, focusing on multi-label classification and zero-shot cross-lingual transfer. Results show that multi-modal models outperform image-only models, and cross-lingual transfer performance correlates with syntactic distance between languages.

## Method Summary
The authors introduce two multilingual datasets: MULTI EURLEX-DOC containing European Union laws in 23 languages with multi-label classification, and WIKI-DOC containing Wikipedia articles in 9 languages including non-Latin scripts with multi-class classification. Document AI models are evaluated using fine-tuning on these datasets with OCR-extracted text and layout information. The evaluation uses mean R-Precision (mRP) for multi-label tasks and macro F1 for multi-class tasks, testing both intralingual and zero-shot cross-lingual transfer capabilities.

## Key Results
- LayoutXLM achieves the best intralingual performance across both datasets
- Cross-lingual transfer performance is highly correlated with syntactic distance between source and target languages
- Donut, an image-only model, struggles with multi-label classification and cross-lingual transfer compared to multi-modal models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal models with text, layout, and image inputs perform better than image-only models for document classification requiring deep content understanding.
- Mechanism: LayoutXLM and DocFormer achieve higher accuracy than Donut because they integrate text and layout information alongside visual features, allowing them to capture semantic and structural cues necessary for multi-label classification.
- Core assumption: Multi-modal integration improves classification accuracy when document content understanding is crucial.
- Evidence anchors:
  - [abstract] "image-only models have large improvement opportunities unlike what is reported on RVL-CDIP"
  - [section 4.3.1] "the encoder only Donut results are significantly lower (36.64), which can likely be attributed to the fact that the multi-label classification task requires identifying certain spans or distribution of words that indicate a specific attribute/label of the document"
  - [corpus] Weak - no direct citations comparing multi-modal vs image-only models on similar tasks

### Mechanism 2
- Claim: Cross-lingual transfer performance is highly correlated with syntactic distance between source and target languages.
- Mechanism: Models transfer better between typologically similar languages because shared syntactic structures allow knowledge transfer, while distant languages require more language-specific adaptation.
- Core assumption: Syntactic similarity enables cross-lingual transfer in document understanding tasks.
- Evidence anchors:
  - [section 4.5] "The correlation analysis results (Table 8) show that the transfer gap is highly correlated with the syntactic cosine distance between the source and the target language"
  - [section 4.4.2] "increasing the training examples in the source language (i.e., English) hurts the accuracy of LayoutXLM in Japanese and Arabic, which have the highest syntactic distance from English"
  - [corpus] Moderate - related papers on multilingual transfer and typological features exist but not specifically for document understanding

### Mechanism 3
- Claim: OCR-free models like Donut can generalize to unseen languages without vocabulary constraints, but struggle with multi-label classification requiring text understanding.
- Mechanism: Donut avoids OCR errors and vocabulary limitations by processing images directly, but lacks text comprehension needed for multi-label classification where specific word spans indicate document attributes.
- Core assumption: Image-only processing can handle language-agnostic document features while struggling with text-dependent classification tasks.
- Evidence anchors:
  - [abstract] "image-only models have large improvement opportunities unlike what is reported on RVL-CDIP"
  - [section 4.3.1] "An image-only model like Donut is expected to struggle in capturing such nuances from the visual document structure and correlate them to a group of labels describing the document contents rather than document types, without the knowledge of the words comprising it"
  - [corpus] Weak - limited research on OCR-free document models for multilingual classification

## Foundational Learning

- Concept: Multi-modal document understanding combining text, layout, and visual features
  - Why needed here: The datasets require understanding document structure and content, not just visual patterns
  - Quick check question: Why do LayoutXLM and DocFormer outperform Donut on MULTI EURLEX-DOC despite Donut being OCR-free?

- Concept: Cross-lingual transfer and typological distance
  - Why needed here: The experiments show performance varies significantly across language pairs based on syntactic similarity
  - Quick check question: How does syntactic distance between English and target languages correlate with cross-lingual classification accuracy?

- Concept: Multi-label vs multi-class classification differences
  - Why needed here: The datasets use different classification schemes requiring different model capabilities
  - Quick check question: Why does Donut struggle more with multi-label classification than multi-class classification?

## Architecture Onboarding

- Component map: PDF → image conversion → OCR extraction → model inference → classification → evaluation
- Critical path: PDF preprocessing → OCR extraction → model inference → classification → evaluation
- Design tradeoffs:
  - Multi-modal models vs image-only: Better accuracy vs language independence
  - OCR dependency: Higher accuracy vs language coverage limitations
  - Dataset size: Full-shot vs few-shot settings affect model performance differently
- Failure signatures:
  - Low cross-lingual performance: High syntactic distance between source and target languages
  - Poor multi-label accuracy: Using image-only models for text-dependent classification
  - Inconsistent results: High variance across different random seeds in few-shot settings
- First 3 experiments:
  1. Compare LayoutXLM vs Donut on English WIKI-DOC few-shot setting to observe multi-modal vs image-only performance gap
  2. Test InfoXLM cross-lingual transfer from English to German vs Arabic to measure syntactic distance impact
  3. Evaluate LayoutXLM intralingual performance on Romanian vs Greek to investigate data size vs language family effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of image-only models like Donut compare to multi-modal models when trained on longer document excerpts (multiple pages) instead of just the first page?
- Basis in paper: [explicit] The paper mentions that using only the first page of documents may limit performance, especially for fine-grained classification tasks. It notes that "All Page results are borrowed from (Chalkidis et al., 2021)" showing better performance when using all pages.
- Why unresolved: The experiments in the paper were conducted using only the first page of each document, so the impact of using longer document excerpts on image-only model performance remains unknown.
- What evidence would resolve it: Conducting experiments with image-only models trained on multiple pages of documents and comparing their performance to multi-modal models on the same task would provide this evidence.

### Open Question 2
- Question: Would using a different source language for cross-lingual transfer, other than English, lead to improved performance on the MULTIEURLEX-DOC and WIKI-DOC datasets?
- Basis in paper: [explicit] The paper acknowledges that "the crosslingual experiments conducted in the paper uses English as the source language" and notes that "the choice of source language changes the downstream task results significantly (e.g., Lin et al. (2019))."
- Why unresolved: The paper only experiments with English as the source language for cross-lingual transfer, leaving the impact of other source languages unexplored.
- What evidence would resolve it: Conducting cross-lingual experiments with different source languages and comparing the results to those obtained with English would provide this evidence.

### Open Question 3
- Question: How do larger Document AI models (>400M parameters) perform on the newly introduced datasets compared to the smaller models evaluated in the paper?
- Basis in paper: [explicit] The paper mentions that "the size of all models benchmarked in this paper are < 400M" and that "though not focused on document image classification, we leave it to other work (e.g., (Chen et al., 2023b)) and encourage further research on this topic by the community."
- Why unresolved: The paper only evaluates smaller models, so the potential benefits of using larger models on the new datasets remain unknown.
- What evidence would resolve it: Evaluating larger Document AI models on the MULTIEURLEX-DOC and WIKI-DOC datasets and comparing their performance to the smaller models would provide this evidence.

## Limitations

- The study focuses primarily on European languages and specific document types, which may not generalize to other domains or language families
- OCR-free Donut model shows significant performance gaps in multi-label classification tasks, suggesting current image-only approaches cannot fully replace text-based methods for complex document understanding
- Cross-lingual transfer performance heavily depends on syntactic similarity between languages, with distant language pairs showing poor transfer, limiting practical applicability

## Confidence

- High Confidence: Multi-modal models (LayoutXLM, DocFormer) outperform image-only models (Donut) for multi-label document classification requiring deep text understanding
- Medium Confidence: Cross-lingual transfer performance correlates with syntactic distance between source and target languages, with stronger transfer between typologically similar languages
- Medium Confidence: OCR-free models can generalize to unseen languages but struggle with text-dependent classification tasks, limiting their effectiveness for multi-label scenarios

## Next Checks

1. **Cross-linguistic Generalization Test**: Evaluate LayoutXLM and Donut on a non-European language pair (e.g., Japanese to Korean) to validate the syntactic distance hypothesis beyond European languages.

2. **Domain Transfer Experiment**: Test model performance on different document types (e.g., invoices, scientific papers) to assess generalizability beyond legal documents and Wikipedia articles.

3. **Zero-Shot vs Few-Shot Comparison**: Systematically compare zero-shot cross-lingual transfer against few-shot learning with limited target language examples across multiple language pairs to quantify the practical trade-offs.