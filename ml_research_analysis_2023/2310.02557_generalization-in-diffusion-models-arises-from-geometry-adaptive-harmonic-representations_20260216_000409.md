---
ver: rpa2
title: Generalization in diffusion models arises from geometry-adaptive harmonic representations
arxiv_id: '2310.02557'
source_url: https://arxiv.org/abs/2310.02557
tags:
- images
- image
- denoiser
- basis
- denoising
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the generalization behavior of deep neural
  networks trained for image denoising, which are used in diffusion generative models.
  The authors show that when trained on sufficiently large datasets, two DNN denoisers
  trained on non-overlapping subsets of a dataset learn nearly the same score function
  and thus generate nearly identical samples.
---

# Generalization in diffusion models arises from geometry-adaptive harmonic representations

## Quick Facts
- arXiv ID: 2310.02557
- Source URL: https://arxiv.org/abs/2310.02557
- Reference count: 30
- Key outcome: Two DNN denoisers trained on non-overlapping subsets learn nearly identical score functions with surprisingly few training images

## Executive Summary
This paper analyzes the generalization behavior of deep neural networks trained for image denoising, which are used in diffusion generative models. The authors show that when trained on sufficiently large datasets, two DNN denoisers trained on non-overlapping subsets learn nearly the same score function and thus generate nearly identical samples. This strong generalization suggests that the inductive biases of the DNNs are well-aligned with the data density. The authors analyze the learned denoising functions and find that they perform a shrinkage operation in a geometry-adaptive harmonic basis (GAHB) that is adapted to the underlying image features. This GAHB arises not only when the network is trained on photographic images but also when it is trained on image classes supported on low-dimensional manifolds for which the harmonic basis is suboptimal. The denoising performance of the networks is near-optimal when trained on regular image classes for which the optimal basis is known to be geometry-adaptive and harmonic.

## Method Summary
The authors use a BF-CNN architecture with 21 convolution layers, batch normalization, and ReLU non-linearities to train blind denoising networks across all noise levels without providing the noise level as input. They train models on subsets of CelebA (downsampled to 40×40), LSUN bedroom (downsampled to 32×32), and synthetic geometric Cα images. The training procedure minimizes MSE for blind denoising, and generalization is assessed by comparing denoising performance on train vs test sets and by training multiple models on non-overlapping subsets to measure cosine similarity between generated samples. The Jacobian of the denoising function is eigendecomposed to analyze the learned basis and shrinkage factors.

## Key Results
- Two DNN denoisers trained on non-overlapping subsets of a dataset learn nearly identical score functions when the number of training images is sufficiently large
- The denoising function performs a shrinkage operation in a geometry-adaptive harmonic basis (GAHB) adapted to image features
- DNN denoisers achieve near-optimal performance on image classes for which the optimal basis is known to be geometry-adaptive and harmonic

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two DNN denoisers trained on non-overlapping subsets of a dataset learn nearly the same score function when the number of training images is large enough.
- Mechanism: As training data increases, the inductive biases of the DNN architecture align with the data density, causing the learned score functions to converge to the same optimal solution.
- Core assumption: The inductive biases of the DNN architecture are well-matched to the underlying distribution of photographic images.
- Evidence anchors:
  - [abstract]: "two DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, with a surprisingly small number of training images."
  - [section 2.2]: "These results provide stronger and more direct evidence of generalization than standard comparisons of average performance on train and test sets."
  - [corpus]: Weak evidence - no direct citations found for this specific claim in the corpus.
- Break condition: If the training data is too small relative to the network capacity, the networks will memorize rather than generalize, as shown in Figure 1.

### Mechanism 2
- Claim: The denoising function performs a shrinkage operation in a geometry-adaptive harmonic basis (GAHB) that is adapted to the underlying image features.
- Mechanism: The Jacobian of the denoising function can be eigendecomposed into eigenvalues and eigenvectors, where the eigenvectors form a basis adapted to the image geometry and the eigenvalues act as shrinkage factors.
- Core assumption: The Jacobian of the optimal denoiser is proportional to the posterior covariance matrix, which is symmetric and non-negative.
- Evidence anchors:
  - [section 3.1]: "The denoiser thus locally behaves as a (soft) projection on a subspace whose dimension corresponds to the rank of the Jacobian."
  - [section 3.1]: "This gives us another interpretation of the adaptive eigenvector basis as providing an optimal approximation of the unknown clean image x given the noisy observation y."
  - [corpus]: Weak evidence - no direct citations found for this specific claim in the corpus.
- Break condition: If the image class does not have harmonic structures (e.g., shuffled images), the learned basis will not be optimal, leading to poor denoising performance.

### Mechanism 3
- Claim: The inductive biases of DNN denoisers are aligned towards learning geometry-adaptive harmonic bases (GAHBs).
- Mechanism: When trained on image classes for which the optimal basis is known to be geometry-adaptive and harmonic (e.g., Cα images), DNN denoisers achieve near-optimal performance.
- Core assumption: The optimal bases for denoising photographic images are GAHBs.
- Evidence anchors:
  - [section 3.2]: "We observe that the DNN denoiser operates within a GAHB similar to a bandlet basis, also achieving near-optimal performance."
  - [section 3.2]: "This generalization performance confirms that inductive biases of DNNs are aligned towards learning GAHBs."
  - [corpus]: Weak evidence - no direct citations found for this specific claim in the corpus.
- Break condition: If the image class has optimal bases that are not GAHBs (e.g., low-dimensional manifolds), the DNN denoiser will incorporate suboptimal components, leading to reduced performance.

## Foundational Learning

- Concept: Relationship between denoising and density estimation
  - Why needed here: The paper establishes that learning the true density model is equivalent to performing optimal denoising at all noise levels.
  - Quick check question: How does the denoising error provide a bound on the density modeling error?

- Concept: Jacobian eigendecomposition and its interpretation
  - Why needed here: The paper analyzes the inductive biases of the DNN denoiser by examining the eigenvalues and eigenvectors of its Jacobian.
  - Quick check question: What is the interpretation of the Jacobian eigenvectors in terms of the denoising function?

- Concept: Optimal bases for denoising and their properties
  - Why needed here: The paper investigates the inductive biases of DNN denoisers by examining the bases in which they perform shrinkage operations.
  - Quick check question: What are the properties of the optimal bases for denoising photographic images?

## Architecture Onboarding

- Component map: Noisy images -> BF-CNN with 21 convolution layers, batch normalization, ReLU -> Denoised images
- Critical path: Training → Generalization assessment → Basis analysis
- Design tradeoffs:
  - Model capacity vs. generalization: Larger models may require more training data to avoid overfitting
  - Fixed vs. adaptive basis: Fixed bases (e.g., wavelets) may be suboptimal for complex image structures
- Failure signatures:
  - Memorization: High training performance but poor test performance (small training set)
  - Suboptimal basis: Slow decay of shrinkage factors and poor denoising performance on non-harmonic image classes
- First 3 experiments:
  1. Train two DNN denoisers on non-overlapping subsets of a dataset and compare their generated samples to assess generalization.
  2. Analyze the Jacobian of a trained DNN denoiser to examine the learned basis and shrinkage factors.
  3. Train DNN denoisers on image classes with known optimal bases (e.g., Cα images) and compare their performance to assess inductive biases.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the inductive biases of deep neural networks arise from their computational architecture?
- Basis in paper: [explicit] The authors state "We do not provide a formal mathematical definition of this larger class of GAHB bases which underlie the inductive biases of DNN, nor do we understand how they result from the DNN computational architecture."
- Why unresolved: The paper identifies geometry-adaptive harmonic bases as a key inductive bias but does not provide a formal mathematical framework for understanding how these bases emerge from the network architecture.
- What evidence would resolve it: A formal mathematical proof connecting the network architecture (e.g., convolutional layers, batch normalization, ReLU non-linearities) to the emergence of GAHB bases in the learned representations.

### Open Question 2
- Question: How does the phase transition between memorization and generalization depend on image complexity and network capacity?
- Basis in paper: [explicit] The authors mention "The amount of data needed to cross this phase transition depends on both the image complexity and the neural network capacity" but do not provide quantitative analysis.
- Why unresolved: While the paper demonstrates a phase transition exists, it does not characterize how the transition point varies with image complexity or network architecture.
- What evidence would resolve it: Systematic experiments varying image complexity (e.g., different datasets, resolutions) and network capacity (e.g., different architectures, parameter counts) to map out the phase transition boundary.

### Open Question 3
- Question: Are there image distributions for which DNN denoisers are fundamentally suboptimal, regardless of training set size?
- Basis in paper: [inferred] The authors show DNNs perform poorly on shuffled face images, suggesting limitations when optimal bases are outside the GAHB class.
- Why unresolved: The paper only examines a few synthetic distributions and does not establish whether there are fundamental limitations for broader classes of image distributions.
- What evidence would resolve it: Analysis of DNN performance across a wide range of synthetic and natural image distributions, identifying classes where performance is consistently poor despite large training sets.

## Limitations
- The theoretical justification for why inductive biases align precisely with data density remains incomplete
- Claims about GAHB optimality are limited to specific image classes (photographic images, Cα structures)
- The scaling behavior to larger models and datasets used in state-of-the-art diffusion models remains untested

## Confidence
- Confidence Level: Medium for the core claim that DNNs trained on non-overlapping subsets learn identical score functions
- Confidence Level: Medium for the GAHB analysis
- Confidence Level: Low for the claim that these findings explain generalization in large-scale diffusion models

## Next Checks
1. **Scaling Analysis**: Train DNN denoisers on progressively larger datasets (10K → 100K → 1M images) and measure how quickly the cosine similarity between models trained on different subsets converges to 1.0.

2. **Architecture Ablation**: Repeat the GAHB analysis across different architectures (U-Net variants, transformer-based denoisers) to determine if the geometry-adaptive harmonic basis emergence is architecture-specific or a general property of denoising networks.

3. **Cross-Domain Transfer**: Train DNN denoisers on one image class (e.g., faces) and evaluate their denoising performance on structurally different classes (e.g., medical images, satellite imagery).