---
ver: rpa2
title: 'Adversarial Attacks on Image Classification Models: FGSM and Patch Attacks
  and their Impact'
arxiv_id: '2307.02055'
source_url: https://arxiv.org/abs/2307.02055
tags:
- image
- adversarial
- classification
- patch
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates adversarial attacks on three pre-trained
  CNN image classification models (ResNet-34, GoogleNet, DenseNet-161) using FGSM
  and adversarial patch attacks on ImageNet dataset. The models achieve high baseline
  accuracy with DenseNet-161 showing lowest Top-1 (15.10%) and Top-5 (2.30%) errors.
---

# Adversarial Attacks on Image Classification Models: FGSM and Patch Attacks and their Impact

## Quick Facts
- **arXiv ID**: 2307.02055
- **Source URL**: https://arxiv.org/abs/2307.02055
- **Reference count**: 39
- **Primary result**: FGSM and adversarial patch attacks significantly degrade CNN image classification performance, with DenseNet-161 showing relatively better robustness than ResNet-34 and GoogleNet

## Executive Summary
This study investigates the effectiveness of two adversarial attack methods - Fast Gradient Sign Method (FGSM) and adversarial patch attacks - on three pre-trained CNN models (ResNet-34, GoogleNet, DenseNet-161) using the ImageNet dataset. The research demonstrates that both attack types can substantially reduce classification accuracy, with FGSM attacks degrading performance even at low epsilon values, and patch attacks proving more effective overall. DenseNet-161 emerges as the most robust architecture against both attack types, though the paper does not explore the architectural reasons for this advantage.

## Method Summary
The researchers evaluate baseline classification accuracy of three pre-trained CNN models on ImageNet, then apply FGSM attacks with epsilon values ranging from 0.01 to 0.10 by computing gradients with respect to input pixels and perturbing in the direction that maximizes classification loss. They also implement adversarial patch attacks using pre-computed patches of various sizes (32×32, 48×48, 64×64) placed on images, measuring classification accuracy when the target class matches the patch class. Performance is measured using Top-1 and Top-5 error rates.

## Key Results
- DenseNet-161 achieves the lowest baseline Top-1 error (15.10%) and Top-5 error (2.30%) among the three models
- FGSM attacks cause dramatic performance degradation even at low epsilon values, with ResNet-34 reaching 97.00% Top-1 error at ε=0.10
- Adversarial patch attacks prove more effective than FGSM, with attack success rates increasing with patch size
- DenseNet-161 shows relatively better robustness against both attack types compared to ResNet-34 and GoogleNet

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: FGSM attack effectiveness is strongly dependent on epsilon magnitude and model architecture complexity.
- **Mechanism**: FGSM exploits the linearity assumption in high-dimensional spaces by computing gradients of the loss function with respect to input pixels and perturbing in the direction of the sign of these gradients, increasing classification loss even with small epsilon values.
- **Core assumption**: The model's decision boundary is locally linear near clean examples.
- **Evidence anchors**:
  - [abstract] "FGSM attacks significantly degrade performance even at low epsilon values, with ResNet-34 reaching 97.00% Top-1 error and 77.68% Top-5 error at ε=0.10"
  - [section] "The FGSM attack on an image involves the following three steps: (1) compute loss, (2) compute gradients with respect to pixels, (3) perturb pixels in gradient direction to maximize loss"
- **Break condition**: If the model's loss surface is sufficiently non-linear or if adversarial training has been applied to create robustness against gradient-based attacks.

### Mechanism 2
- **Claim**: Adversarial patch attacks are more effective than FGSM due to their ability to create large, localized perturbations that bypass traditional defenses.
- **Mechanism**: Patch attacks place a pre-computed adversarial patch at a specific location on the image, designed to be class-specific and force the model to classify the entire image as the patch's target class regardless of underlying content.
- **Core assumption**: The model processes the patch as part of the image and cannot distinguish it from legitimate content.
- **Evidence anchors**:
  - [abstract] "Adversarial patch attacks prove more effective than FGSM, with attack success rates increasing with patch size"
  - [section] "An approach that is based on creating an image-independent patch and positioning it to cover a tiny area of the image was demonstrated by Brown et al."
- **Break condition**: If the model incorporates patch detection or localization mechanisms, or if the patch is placed outside the model's field of view.

### Mechanism 3
- **Claim**: DenseNet-161 shows greater robustness against both attack types compared to ResNet-34 and GoogleNet.
- **Mechanism**: DenseNet's dense connectivity pattern and feature reuse may create more distributed and redundant representations that are harder to fool with small perturbations, with the architecture's ability to maintain feature maps across layers providing inherent regularization against adversarial examples.
- **Core assumption**: Dense connectivity provides natural defense through architectural redundancy.
- **Evidence anchors**:
  - [abstract] "DenseNet-161 shows relatively better robustness against both attack types compared to other models"
  - [section] "Among the three models, DenseNet-161 looked to be the most robust against the FGSM attack" (Section 4.2)
- **Break condition**: If the attack magnitude exceeds the model's capacity to maintain robust features, or if the attack specifically targets the dense connections.

## Foundational Learning

- **Concept**: Gradient-based optimization and backpropagation
  - Why needed here: Understanding how FGSM computes gradients with respect to input pixels and uses them to create adversarial examples
  - Quick check question: How does the chain rule enable computation of gradients with respect to input pixels in a CNN?

- **Concept**: Image classification metrics (Top-1 vs Top-5 accuracy)
  - Why needed here: The paper evaluates attack impact using both Top-1 and Top-5 error rates, which are standard metrics for ImageNet-scale classification
  - Quick check question: Why might Top-5 error be more informative than Top-1 error for ImageNet with 1000 classes?

- **Concept**: Convolutional neural network architectures and their differences
  - Why needed here: The paper compares three different CNN architectures (ResNet-34, GoogleNet, DenseNet-161) and their relative robustness to attacks
  - Quick check question: What architectural differences between ResNet and DenseNet might contribute to different adversarial robustness?

## Architecture Onboarding

- **Component map**: Load pre-trained models (ResNet-34, GoogleNet, DenseNet-161) -> Load and preprocess ImageNet images -> Apply attack (FGSM or patch) -> Run inference -> Compute classification metrics -> Compare with baseline
- **Critical path**: Load pre-trained model → Load and preprocess ImageNet images → Apply attack (FGSM or patch) → Run inference → Compute classification metrics → Compare with baseline
- **Design tradeoffs**: Using pre-trained models trades computational cost for performance but limits control over architecture-specific defenses. FGSM implementation simplicity vs. effectiveness compared to more sophisticated attacks like PGD.
- **Failure signatures**: Unexpectedly low baseline accuracy might indicate data loading issues. Attack not working might indicate gradient computation errors. High variance in results might indicate insufficient sample size.
- **First 3 experiments**:
  1. Verify baseline classification accuracy on clean ImageNet images for all three models
  2. Implement and test FGSM attack with ε=0.01 on a single image, visualizing both clean and adversarial examples
  3. Implement adversarial patch attack with 32×32 patches, testing on the same single image to observe classification changes

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The paper does not explore the architectural reasons why DenseNet-161 shows superior robustness against adversarial attacks
- Adversarial patch attack methodology lacks specification of patch placement strategy, blending techniques, and target class determination
- No statistical significance testing is reported for the observed performance differences between models

## Confidence
- **High confidence**: Baseline accuracy measurements and general FGSM attack effectiveness
- **Medium confidence**: Comparative robustness analysis between models
- **Low confidence**: Specific claims about DenseNet's architectural advantages and patch attack effectiveness metrics

## Next Checks
1. Implement statistical significance testing (e.g., paired t-tests) on classification accuracy differences between models under various attack conditions
2. Replicate experiments with additional epsilon values (0.05, 0.15) to better characterize the attack effectiveness curve
3. Test alternative patch placement strategies (corner vs center, multiple patches) to determine if current results are placement-dependent