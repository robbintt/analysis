---
ver: rpa2
title: 'ViCo: Engaging Video Comment Generation with Human Preference Rewards'
arxiv_id: '2308.11171'
source_url: https://arxiv.org/abs/2308.11171
tags:
- comments
- video
- reward
- engagement
- comment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the challenge of generating engaging video
  comments, which are subjective and diverse unlike objective video captions. The
  proposed ViCo model uses three novel designs: (1) Using the number of "likes" as
  a proxy for comment engagement after debiasing for temporal bias, (2) Training a
  reward model to automatically evaluate comment engagement based on the likes proxy,
  and (3) Using the reward model to guide an initial generator trained on noisy data
  to produce more engaging comments.'
---

# ViCo: Engaging Video Comment Generation with Human Preference Rewards

## Quick Facts
- arXiv ID: 2308.11171
- Source URL: https://arxiv.org/abs/2308.11171
- Reference count: 8
- The paper addresses the challenge of generating engaging video comments, which are subjective and diverse unlike objective video captions.

## Executive Summary
This paper tackles the problem of generating engaging video comments that capture subjective audience sentiment rather than just objective descriptions. The authors propose ViCo, a model that uses likes as a proxy for engagement after temporal debiasing, trains a reward model to evaluate comment engagement, and uses this reward to guide a generator toward producing more engaging comments. The approach is validated on a new dataset (ViCo-20k) containing 20k videos and 3M comments, showing significant improvements in comment relevance, diversity, and engagement compared to baselines, with human evaluators preferring the generated comments.

## Method Summary
ViCo employs three key innovations: (1) temporal debiasing of like counts to create an engagement proxy, (2) training a reward model via pairwise ranking on debiased comparisons, and (3) optimizing an initial generator with uniqueness-guided sampling followed by reinforcement learning using the reward model. The model architecture combines a Video Perceiver for spatiotemporal feature extraction with a large language model for comment generation. Training involves 100 epochs with uniqueness-guided sampling, followed by 5 epochs of PPO fine-tuning using reward feedback. The approach is evaluated on the newly collected ViCo-20k dataset using BLEU, ROUGE-L, diversity metrics, and human preference studies.

## Key Results
- ViCo significantly improves comment relevance (BLEU, ROUGE-L) and diversity (num. Bigrams, self-CIDEr) compared to baseline models
- Generated comments receive higher engagement scores from the reward model and are preferred by human annotators in A-B tests
- The reward model effectively aligns with human judgments, achieving 80.4% consistency with human preferences
- ViCo-20k dataset enables research on engaging comment generation with 20k videos and 3M comments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using the number of "likes" as a proxy for comment engagement, after temporal debiasing, provides a scalable, crowd-sourced measure of engagement that aligns with human preferences.
- Mechanism: Comments with more likes are treated as higher quality, but raw like counts are biased by publishing time. The debiasing process ensures that positive examples are comments posted later but still received more likes, mitigating temporal bias.
- Core assumption: The number of likes reflects genuine audience engagement and preference, and temporal bias is the primary confounding factor.
- Evidence anchors:
  - [abstract] "we utilize the number of 'likes' each comment receives as a proxy of human preference after an appropriate debiasing procedure"
  - [section] "We adopt a temporal-aware pair construction method to avoid the temporal bias for the number of likes"
  - [corpus] Weak evidence: related papers focus on comment generation but do not explicitly validate like-counts as engagement proxies; need domain-specific studies.
- Break condition: If likes are gamed, manipulated, or driven by factors unrelated to comment quality (e.g., popularity of commenter, platform algorithm changes), the proxy becomes unreliable.

### Mechanism 2
- Claim: Training a reward model using pairwise comparisons of debiased likes aligns the model's judgments with human perceptions of engagement.
- Mechanism: The reward model is optimized to score positive comments higher than negative ones in automatically constructed comparison pairs, mimicking human preference ranking.
- Core assumption: Human annotators agree with the relative ranking implied by debiased like counts, so the reward model can generalize to new comments.
- Evidence anchors:
  - [abstract] "we train a reward model to align its judgment to the above proxy"
  - [section] "Our user studies indicate that this reward model effectively aligns with human judgments"
  - [corpus] No direct evidence in corpus that reward models using likes generalize to unseen content; assumes alignment is stable.
- Break condition: If the reward model overfits to specific like patterns or fails to generalize to new video/comment types, its judgments will diverge from human preferences.

### Mechanism 3
- Claim: Combining uniqueness-guided sampling with reinforcement learning using the reward model generates more engaging comments than standard likelihood training.
- Mechanism: Initial generator is trained with a sampling strategy that favors unique comments (low BERT similarity to others), then optimized via RL using the reward model as feedback, pushing the model toward high-engagement outputs.
- Core assumption: Unique comments are more engaging, and the reward model's feedback can steer the generator toward these desirable outputs.
- Evidence anchors:
  - [abstract] "an initial generator is trained on readily available but noisy data to generate comments. Then the reward model is employed to offer feedback on the generated comments, thus optimizing the initial generator"
  - [section] "After optimizing the initial generator with our engagement reward model, our final model... is able to generate the most engaging comments"
  - [corpus] No evidence in corpus for uniqueness as a driver of engagement; assumption is based on dataset analysis only.
- Break condition: If uniqueness sampling leads to overly niche or incoherent comments, or if the reward model's feedback is noisy or misaligned, the generator's output quality degrades.

## Foundational Learning

- Concept: Temporal bias in engagement signals
  - Why needed here: Without correcting for publishing time, like counts unfairly favor early comments, corrupting the engagement proxy.
  - Quick check question: How does the temporal debiasing process ensure that later comments with high likes are correctly identified as high-quality?

- Concept: Reward model training via pairwise ranking
  - Why needed here: Reward models need to capture relative preferences, not just absolute scores; pairwise ranking loss is effective for this.
  - Quick check question: Why is pairwise ranking loss preferred over pointwise regression loss for training the reward model?

- Concept: Uniqueness-guided sampling for diversity
  - Why needed here: Standard sampling collapses to common phrases; uniqueness sampling encourages the model to generate more varied and engaging comments.
  - Quick check question: How is the uniqueness of a comment quantified, and what effect does it have on the sampling distribution?

## Architecture Onboarding

- Component map:
  - Video Perceiver -> LLM embeddings -> LLM + prompt -> generated comment
  - LLM + comment -> reward score (via reward head)
  - Uniqueness Metric (BERT similarity) -> guides sampling during initial training
  - Reinforcement Learning Module (PPO) -> fine-tunes generator using reward feedback

- Critical path:
  1. Video frames → Video Perceiver → LLM embeddings.
  2. LLM + prompt → generated comment (generator mode).
  3. LLM + comment → reward score (reward model mode).
  4. Uniqueness-guided sampling during initial training.
  5. RL fine-tuning with reward feedback.

- Design tradeoffs:
  - Using likes as proxy is scalable but noisy; debiasing helps but cannot eliminate all bias.
  - Pairwise reward model training is more aligned with human judgment than pointwise methods but requires more comparisons.
  - Uniqueness sampling increases diversity but may reduce coherence if pushed too far.

- Failure signatures:
  - Generator collapses to generic phrases despite uniqueness sampling.
  - Reward model outputs random or highly correlated scores unrelated to engagement.
  - Temporal debiasing fails, leading to systematic bias in training pairs.

- First 3 experiments:
  1. Ablation: Train reward model without temporal debiasing; compare alignment with human judgment.
  2. Diversity check: Compare num. Bigrams and self-CIDEr of comments generated with/without uniqueness-guided sampling.
  3. RL impact: Evaluate change in engagement scores (average reward) before and after RL fine-tuning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve the debiasing method for temporal bias in likes to further enhance the accuracy of engagement proxy?
- Basis in paper: [explicit] The paper mentions that temporal bias exists in the number of likes and uses a temporal-aware pair construction method to address it. However, it does not explore more advanced debiasing techniques.
- Why unresolved: The current debiasing method is relatively simple and may not fully eliminate the temporal bias. More sophisticated methods could potentially improve the accuracy of the engagement proxy.
- What evidence would resolve it: Experiments comparing the effectiveness of different debiasing techniques on the accuracy of the engagement proxy, using metrics such as the consistency between the proxy and human judgments.

### Open Question 2
- Question: Can the reward model be further improved to better align with human judgments of comment engagement?
- Basis in paper: [explicit] The paper shows that the reward model is well-aligned with human judgments, but there is still room for improvement as indicated by the 80.4% consistency rate.
- Why unresolved: The reward model is trained using a pairwise ranking loss, which may not capture all aspects of human judgment. Exploring alternative training objectives or incorporating additional features could potentially improve alignment.
- What evidence would resolve it: Human evaluations comparing the reward model's judgments with human judgments on a larger scale, and ablation studies on different reward model architectures and training objectives.

### Open Question 3
- Question: How can we further enhance the diversity of generated comments while maintaining their relevance and engagement?
- Basis in paper: [explicit] The paper shows that the uniqueness-guided sampling method improves the diversity of generated comments, but there is still a gap compared to human-generated comments.
- Why unresolved: The uniqueness-guided sampling method relies on a fixed set of similar videos and comments, which may not capture the full diversity of human preferences. Exploring more dynamic and adaptive sampling strategies could potentially further enhance diversity.
- What evidence would resolve it: Experiments comparing the diversity of generated comments using different sampling strategies, using metrics such as the number of unique bigrams and self-CIDEr scores, and human evaluations on the perceived diversity of generated comments.

## Limitations
- Temporal debiasing reliability: The debiasing method may not fully eliminate temporal bias, and the assumption that likes reflect engagement quality may not hold universally due to platform algorithms or cultural factors.
- Generalizability of reward model: The reward model is trained on pairwise comparisons from a specific dataset and may not transfer well to videos from different domains, platforms, or time periods.
- Uniqueness as engagement proxy: The claim that uniqueness drives engagement is weakly supported, with no direct causal evidence linking comment uniqueness to user engagement beyond correlation in the training data.

## Confidence
- **High confidence**: The technical approach (Video Perceiver + LLM architecture, pairwise reward training, uniqueness-guided sampling) is well-grounded and follows established methods, with strong evidence from ablation studies and human evaluation.
- **Medium confidence**: The engagement proxy (debiased likes) is reasonable but relies on assumptions about user behavior that may not hold universally; reward model alignment is demonstrated but on a limited evaluation set.
- **Low confidence**: The claim that uniqueness directly drives engagement is weakly supported, with no direct evidence beyond correlation in the training data.

## Next Checks
1. **Temporal robustness check**: Evaluate the reward model on comments from videos published at different times of day and days of week to verify that temporal debiasing effectively normalizes for publishing time effects.
2. **Cross-platform generalization**: Test the trained reward model and generator on comments from a different platform (e.g., TikTok or Instagram) to assess whether the engagement patterns learned from YouTube comments transfer to other contexts.
3. **Uniqueness-ablation study**: Generate comments with and without the uniqueness-guided sampling while keeping all other factors constant, then conduct user studies to measure the actual impact of uniqueness on perceived engagement quality.