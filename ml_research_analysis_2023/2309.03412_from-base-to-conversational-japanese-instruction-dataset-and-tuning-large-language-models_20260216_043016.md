---
ver: rpa2
title: 'From Base to Conversational: Japanese Instruction Dataset and Tuning Large
  Language Models'
arxiv_id: '2309.03412'
source_url: https://arxiv.org/abs/2309.03412
tags:
- instruction
- tuning
- llama
- japanese
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper constructs a Japanese instruction dataset by expanding
  and filtering existing datasets, then applies it to a Japanese pre-trained base
  model. The authors perform Low-Rank Adaptation (LoRA) tuning on both Japanese and
  English models using their instruction dataset.
---

# From Base to Conversational: Japanese Instruction Dataset and Tuning Large Language Models

## Quick Facts
- arXiv ID: 2309.03412
- Source URL: https://arxiv.org/abs/2309.03412
- Reference count: 40
- Primary result: Japanese instruction tuning improves performance on Japanese downstream tasks for both Japanese-based and English-based models

## Executive Summary
This paper constructs a Japanese instruction dataset by expanding and filtering existing datasets, then applies it to tune both Japanese pre-trained base models and English models using Low-Rank Adaptation (LoRA). The authors evaluate these models on various downstream tasks and demonstrate that instruction tuning with Japanese data improves performance across multiple benchmarks. The results confirm the effectiveness of Japanese instruction datasets and show that even with relatively small LLMs (7B parameters), performance in downstream tasks can be improved through instruction tuning.

## Method Summary
The authors construct a Japanese instruction dataset (llm-japanese-dataset-vanilla) containing approximately 2.5 million samples across 5 tasks. They perform LoRA tuning on both Japanese-based models (CALM) and English-based models (LLaMA) using this instruction dataset. The tuned models are then evaluated on Japanese Natural Language Inference (JNLI), MARC-ja, and VQA datasets to assess performance improvements. The study compares the effectiveness of Japanese instruction tuning on both Japanese and English base models.

## Key Results
- Japanese instruction tuning improves performance on Japanese downstream tasks for both Japanese-based and English-based models
- Even with relatively small LLMs (7B parameters), performance in downstream tasks improves through instruction tuning
- LoRA enables efficient fine-tuning of large language models without significantly reducing accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Japanese instruction tuning improves performance on Japanese downstream tasks for both Japanese-based and English-based models.
- Mechanism: Fine-tuning with Japanese instruction data aligns the model's output distribution with the task requirements, improving generalization.
- Core assumption: The instruction data captures the distribution of task-relevant inputs and expected outputs in Japanese.
- Evidence anchors:
  - [abstract] "The results confirm the effectiveness of Japanese instruction datasets."
  - [section IV-A] "In the evaluation by JNLI, the accuracy of Stormy was the highest across 1-shot, 2-shot, and 3-shot settings."
- Break condition: If the instruction data does not adequately represent the target tasks, or if the model lacks sufficient pre-training in the target language.

### Mechanism 2
- Claim: Even with smaller model sizes (7B parameters), instruction tuning can improve performance on downstream tasks.
- Mechanism: Instruction tuning adapts the model's parameters to the task distribution without requiring the full capacity of a larger model.
- Core assumption: The model has sufficient capacity to learn from the instruction data and adapt to the target tasks.
- Evidence anchors:
  - [abstract] "The results also indicate that even with relatively small LLMs, performances in downstream tasks would be improved through instruction tuning."
  - [section V-A] "The improvement in perplexity was particularly noticeable in the LLaMA-based models."
- Break condition: If the model is too small to capture the complexity of the target tasks, or if the instruction data is insufficient.

### Mechanism 3
- Claim: Low-Rank Adaptation (LoRA) enables efficient fine-tuning of large language models without significantly reducing accuracy.
- Mechanism: LoRA approximates the weight updates as low-rank matrices, reducing the number of trainable parameters and computational cost.
- Core assumption: The weight updates during fine-tuning can be effectively represented by low-rank matrices.
- Evidence anchors:
  - [section III-B] "In LoRA, only the difference between the initial and updated LLM parameters, represented with small-scale parameters, is calculated."
  - [section V-A] "All the instruct-tuned models showed improved performance with reduced perplexity due to tuning using instruction data."
- Break condition: If the weight updates during fine-tuning cannot be effectively represented by low-rank matrices, or if the rank is too low to capture the necessary changes.

## Foundational Learning

- Concept: Pre-training on large corpora
  - Why needed here: Pre-training provides the model with a broad understanding of language and general knowledge, which serves as a foundation for fine-tuning.
  - Quick check question: Does the model have sufficient pre-training on a large and diverse corpus before instruction tuning?

- Concept: Fine-tuning on task-specific data
  - Why needed here: Fine-tuning adapts the pre-trained model to the specific requirements of the target tasks, improving its performance on those tasks.
  - Quick check question: Is the instruction data representative of the target tasks and does it cover the necessary variations and edge cases?

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: LoRA enables efficient fine-tuning of large models by approximating the weight updates as low-rank matrices, reducing computational cost and memory requirements.
  - Quick check question: Is the rank of the LoRA matrices sufficient to capture the necessary changes in the model's weights during fine-tuning?

## Architecture Onboarding

- Component map: Pre-trained language model (Japanese or English-based) -> Japanese instruction dataset -> LoRA fine-tuning mechanism -> Evaluation datasets (JNLI, MARC-ja, VQA)
- Critical path:
  1. Pre-train the language model on a large corpus
  2. Construct the Japanese instruction dataset
  3. Fine-tune the model using LoRA on the instruction dataset
  4. Evaluate the tuned model on the target tasks
- Design tradeoffs:
  - Model size vs. computational cost: Larger models may achieve better performance but require more resources for fine-tuning.
  - Instruction data quality vs. quantity: High-quality instruction data is crucial for effective fine-tuning, but collecting and curating such data can be time-consuming and expensive.
  - LoRA rank vs. performance: Higher ranks may capture more complex changes in the model's weights but also increase computational cost and risk of overfitting.
- Failure signatures:
  - Poor performance on target tasks: Indicates issues with the instruction data, model architecture, or fine-tuning process.
  - High perplexity on evaluation datasets: Suggests the model has not adequately learned to generate coherent and contextually appropriate responses.
  - Overfitting on the instruction data: May lead to poor generalization to unseen data and tasks.
- First 3 experiments:
  1. Evaluate the pre-trained model on the target tasks to establish a baseline performance.
  2. Fine-tune the model using LoRA on a subset of the instruction data and evaluate its performance on the target tasks.
  3. Vary the rank of the LoRA matrices and observe its impact on the model's performance and computational cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Japanese-based models compare to English-based models when fine-tuned with Japanese instruction datasets?
- Basis in paper: [explicit] The paper states "The results show that tuning with Japanese instruction data improves performance in quantitative evaluations. In particular, the results indicate that not only Japanese-based models but also English-based models can be tuned in Japanese using the Japanese instruction dataset."
- Why unresolved: While the paper mentions that both Japanese and English models benefit from Japanese instruction tuning, it doesn't provide a direct comparison of their relative performance improvements.
- What evidence would resolve it: A detailed comparison of the performance gains achieved by Japanese-based and English-based models when fine-tuned with the same Japanese instruction dataset, measured on the same downstream tasks.

### Open Question 2
- Question: What is the impact of translation tasks on the performance of English-based models when fine-tuned with Japanese instruction datasets?
- Basis in paper: [explicit] The paper mentions "Using this dataset, which contains various tasks, we perform instruction tuning on both Japanese-based and English-based LLMs. For Japanese-based models, we conduct tuning using an instruction dataset without translation data, while for English-based models, we do using an instruction dataset that includes translation data."
- Why unresolved: The paper doesn't provide a detailed analysis of how the inclusion of translation tasks in the instruction dataset affects the performance of English-based models specifically.
- What evidence would resolve it: A comparison of the performance of English-based models fine-tuned with and without translation tasks in the Japanese instruction dataset, measured on relevant downstream tasks.

### Open Question 3
- Question: How does the effectiveness of instruction tuning vary with model size for non-English languages?
- Basis in paper: [explicit] The paper states "This effect was observed in both Japanese-based CALM and English-based LLaMA models. This suggests that, in non-English languages or when tuning English models to them, instruction tuning does not necessarily have negative effects for smaller models, and could even contribute to performance enhancement."
- Why unresolved: While the paper suggests that smaller models can benefit from instruction tuning in non-English languages, it doesn't provide a comprehensive analysis of how the effectiveness varies across different model sizes.
- What evidence would resolve it: A systematic evaluation of the performance gains achieved by instruction tuning across a range of model sizes (e.g., 1B, 3B, 7B, 13B) for non-English languages, measured on various downstream tasks.

### Open Question 4
- Question: What is the impact of increasing the input length of instruction tuning on the performance of LLaMA-based models?
- Basis in paper: [inferred] The paper mentions "While the LLaMA-based model itself can input up to 2,048 tokens and pre-training is performed at this length, in this study, the input length is limited to 256 tokens. Therefore, in data where long tokens are input, the effect of instruction tuning may not have been demonstrated."
- Why unresolved: The paper suggests that the limited input length of instruction tuning might have affected the performance of LLaMA-based models, but it doesn't provide a direct investigation of how increasing the input length would impact performance.
- What evidence would resolve it: An experiment comparing the performance of LLaMA-based models fine-tuned with different input lengths (e.g., 256, 512, 1024, 2048 tokens) on relevant downstream tasks.

### Open Question 5
- Question: How does the inclusion of additional task types in the Japanese instruction dataset affect the performance of fine-tuned models?
- Basis in paper: [explicit] The paper states "Compared to prior research [16], [38], there are fewer types of tasks. This might have led to a potential constraint in performance improvement. For instance, when compared to FLAN [16], tasks like simplification and correction have been newly added, while tasks like natural language inference, sentiment, and paraphrase lack."
- Why unresolved: The paper acknowledges that the current Japanese instruction dataset might be limited in task diversity, but it doesn't provide a direct investigation of how adding more task types would impact performance.
- What evidence would resolve it: An experiment comparing the performance of models fine-tuned with the current Japanese instruction dataset and an expanded version with additional task types (e.g., natural language inference, sentiment analysis, paraphrase) on relevant downstream tasks.

## Limitations

- Evaluation scope is limited to relatively narrow downstream tasks (JNLI, MARC-ja, VQA) that may not fully represent Japanese language capabilities
- Dataset construction relies heavily on filtering and expanding existing datasets rather than creating purpose-built Japanese instruction data
- Comparison between Japanese-based and English-based models uses the same instruction dataset, which may not be optimal for the English model originally trained on English data

## Confidence

- **High confidence**: The effectiveness of Japanese instruction tuning for improving performance on Japanese downstream tasks
- **Medium confidence**: The claim that relatively small LLMs (7B parameters) can achieve meaningful improvements through instruction tuning
- **Medium confidence**: The assertion that LoRA enables efficient fine-tuning without significant accuracy loss

## Next Checks

1. **Cross-task generalization test**: Evaluate the tuned models on additional Japanese language tasks not included in the original instruction dataset to assess whether improvements generalize beyond the specific evaluation benchmarks used in the paper.

2. **Instruction dataset ablation study**: Systematically remove or modify components of the instruction dataset to quantify the contribution of different data sources and task types to the observed performance improvements.

3. **Efficiency benchmarking**: Conduct comprehensive measurements of training time, memory usage, and inference latency for both full fine-tuning and LoRA approaches across different model sizes to validate the claimed computational efficiency benefits.