---
ver: rpa2
title: Why Do Students Drop Out? University Dropout Prediction and Associated Factor
  Analysis Using Machine Learning Techniques
arxiv_id: '2310.10987'
source_url: https://arxiv.org/abs/2310.10987
tags:
- data
- dropout
- students
- status
- academic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examined university dropout prediction using academic,
  demographic, socioeconomic, and macroeconomic data types. Four binary classifiers
  were trained to predict graduation or dropout status, achieving an average ROC-AUC
  score of 0.935.
---

# Why Do Students Drop Out? University Dropout Prediction and Associated Factor Analysis Using Machine Learning Techniques

## Quick Facts
- arXiv ID: 2310.10987
- Source URL: https://arxiv.org/abs/2310.10987
- Reference count: 11
- Primary result: Four binary classifiers achieved an average ROC-AUC score of 0.935 for university dropout prediction

## Executive Summary
This study investigates university dropout prediction using academic, demographic, socioeconomic, and macroeconomic data types. Four binary classifiers were trained to predict graduation versus dropout status, achieving strong performance with an average ROC-AUC score of 0.935. The analysis reveals that academic data is the most influential predictor, with ROC-AUC scores dropping significantly (from 0.935 to 0.811) when academic features are excluded. The findings suggest that while academic performance metrics are primary drivers of dropout risk, other data types provide supplementary predictive value.

## Method Summary
The study used 4424 student records from Polytechnic Institute of Portalegre (2008-2019), filtered to 3630 records after removing students with "enrolled" status. Four binary classifiers (Random Forest, Decision Tree, SVM, KNN) were trained using an 80/20 train-test split. The researchers conducted experiments excluding each data type (academic, demographic, socioeconomic, macroeconomic) to measure their relative importance. Feature importance analysis was performed on the Random Forest model to identify the most predictive individual features.

## Key Results
- Four classifiers achieved an average ROC-AUC score of 0.935 for dropout prediction
- Academic data was most influential, with ROC-AUC dropping 0.124 (to 0.811) when excluded
- Socioeconomic data was second most influential, with ROC-AUC dropping 0.013 (to 0.922)
- Macroeconomic data had negligible impact on model performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Academic data has the strongest predictive power for university dropout
- Mechanism: Academic performance metrics directly reflect student engagement and capability
- Core assumption: Academic metrics capture the most immediate factors influencing retention
- Evidence anchors: Average ROC-AUC dropped from 0.935 to 0.811 when excluding academic features; academic data contained 17 of 34 features
- Break condition: If academic metrics become less standardized or non-academic factors become primary drivers

### Mechanism 2
- Claim: Socioeconomic and demographic data contribute measurable predictive signal
- Mechanism: Financial strain and personal circumstances create barriers to continued enrollment
- Core assumption: Financial and personal circumstances are detectable through these features
- Evidence anchors: Dropout rates were highest for students with late tuition fees (94%); socioeconomic data caused ROC-AUC to drop 0.013
- Break condition: If socioeconomic disparities are mitigated through policy changes

### Mechanism 3
- Claim: Macroeconomic indicators have negligible predictive power for individual dropout outcomes
- Mechanism: Broad economic trends don't strongly correlate with individual dropout decisions
- Core assumption: Individual-level decisions are more sensitive to immediate academic performance
- Evidence anchors: Macroeconomic data had no impact on average ROC-AUC score of 0.935
- Break condition: If future datasets reveal stronger local economic impacts on dropout behavior

## Foundational Learning

- Concept: Binary classification evaluation using ROC-AUC
  - Why needed here: The study's primary performance metric is ROC-AUC
  - Quick check question: What does an ROC-AUC score of 0.935 imply about the classifier's ability to distinguish graduates from dropouts?

- Concept: Feature importance calculation in tree-based models
  - Why needed here: Random Forest feature importance identifies predictive academic metrics
  - Quick check question: How does a Random Forest determine the importance of a feature like "Curricular units 1st sem (approved)"?

- Concept: Imbalanced feature distribution across data types
  - Why needed here: Academic data contained 17 of 34 features, potentially biasing results
  - Quick check question: How might having 17 academic features versus 3 macroeconomic features affect the validity of the data type importance analysis?

## Architecture Onboarding

- Component map: Load data -> Preprocess (remove enrolled, split 80/20) -> Group features (academic, demographic, socioeconomic, macroeconomic) -> Train 4 classifiers (RF, DT, SVM, KNN) -> Evaluate ROC-AUC -> Analyze feature importance -> Exclude each data type and retrain -> Compare performance drops

- Critical path: 1. Load and clean dataset, 2. Define feature groups, 3. Train baseline model on full dataset, 4. Iteratively exclude each data type and retrain, 5. Compare performance drops to determine most influential data type, 6. Analyze feature importance of best-performing model

- Design tradeoffs: Academic data imbalance (17 vs 3 macroeconomic features) may bias results; binary classification simplifies but loses nuance from "enrolled" category; Random Forest chosen for interpretability via feature importance

- Failure signatures: If AUC scores don't drop significantly when excluding any data type, model may be overfitting; if performance drops equally across exclusions, no single data type is dominant; if feature importance rankings are unstable, model lacks robustness

- First 3 experiments: 1. Reproduce baseline AUC of 0.935 using Random Forest on full dataset, 2. Exclude academic data and confirm AUC drops to ~0.811, 3. Exclude socioeconomic data and confirm smaller AUC drop (~0.922)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of including "enrolled" students in the dataset for dropout prediction models?
- Basis in paper: [explicit] The paper explicitly mentions that "enrolled" students were excluded from the primary dataset due to ambiguous status
- Why unresolved: The paper does not provide any analysis or discussion on how the inclusion of "enrolled" students might affect the model's performance
- What evidence would resolve it: Conducting experiments with and without "enrolled" students in the dataset and comparing model performance

### Open Question 2
- Question: How does the imbalance in the number of features across different data types affect the model's performance and the interpretation of results?
- Basis in paper: [explicit] The paper acknowledges that academic data contained 17 features while macroeconomic data only included 3 features
- Why unresolved: The paper does not explore or quantify the effect of this imbalance on model performance
- What evidence would resolve it: Analyzing model performance with balanced and imbalanced feature sets across different data types

### Open Question 3
- Question: What specific socioeconomic and demographic factors are most predictive of dropout, and how do these factors vary across different student populations?
- Basis in paper: [inferred] While the paper discusses the influence of various data types, it lacks detailed analysis on specific factors
- Why unresolved: The paper provides a general overview but lacks detailed analysis on specific factors within categories
- What evidence would resolve it: Detailed feature importance analysis and subgroup analysis across different student populations

## Limitations
- Feature-to-data-type mapping is not fully specified, with only 17 academic features identified out of 34 total
- Preprocessing pipeline lacks detail regarding categorical variable handling and missing value treatment
- "Enrolled" category removal (794 students) may introduce selection bias affecting model learning patterns

## Confidence

- High confidence: The baseline ROC-AUC score of 0.935 for the four-classifier ensemble and the ranking of data type importance are directly supported by experimental results
- Medium confidence: The mechanism explaining why academic data is most predictive is logically sound but not empirically validated against alternative explanations
- Low confidence: The conclusion that macroeconomic data has "little to no impact" may be premature given the imbalance with only three macroeconomic features

## Next Checks

1. Reconstruct feature mapping: Map all 34 features to their respective data types and verify the 17/3/8/6 distribution claimed in the paper

2. Balance feature counts: Conduct experiments with equal numbers of features per data type (e.g., 6 features each) to determine if academic data dominance persists under balanced conditions

3. Validate selection bias: Analyze dropout rates and feature distributions in the excluded "enrolled" students (794 records) to assess whether their removal systematically altered the model's learning patterns