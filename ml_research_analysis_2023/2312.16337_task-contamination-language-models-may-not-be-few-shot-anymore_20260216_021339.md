---
ver: rpa2
title: 'Task Contamination: Language Models May Not Be Few-Shot Anymore'
arxiv_id: '2312.16337'
source_url: https://arxiv.org/abs/2312.16337
tags:
- task
- datasets
- data
- training
- contamination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) show high zero-shot and few-shot performance,
  but this may be due to task contamination from prior exposure to task examples in
  training data. We systematically analyze task contamination across 12 models and
  17 tasks.
---

# Task Contamination: Language Models May Not Be Few-Shot Anymore

## Quick Facts
- **arXiv ID**: 2312.16337
- **Source URL**: https://arxiv.org/abs/2312.16337
- **Reference count**: 38
- **Primary result**: Large language models show inflated zero-shot and few-shot performance due to prior exposure to task examples in training data.

## Executive Summary
Large language models demonstrate impressive zero-shot and few-shot performance, but this paper reveals that much of this capability may stem from task contamination - prior exposure to task examples during pretraining. Through systematic analysis of 12 models across 17 tasks, the authors find that models perform significantly better on datasets released before their training data collection dates compared to those released after. The study employs four complementary methods - chronological analysis, training data inspection, task example extraction, and membership inference attacks - to provide converging evidence of contamination. The findings suggest that closed-source models with instruction tuning or RLHF may have inflated performance metrics, and the authors recommend releasing training datasets to facilitate contamination detection.

## Method Summary
The study evaluates 12 language models (5 GPT-3 series and 7 open models) on 17 tasks, comparing performance on datasets released before and after LLM training data collection dates. The analysis employs four complementary methods: chronological analysis comparing performance across temporal dataset categories, training data inspection searching for task examples in pretraining data, task example extraction prompting models to generate task examples, and membership inference attacks for generation tasks. The authors systematically analyze contamination patterns across classification and semantic parsing tasks, with particular focus on detecting whether models have been exposed to specific task examples during training.

## Key Results
- Models are significantly more likely to beat majority baselines on datasets released before LLM training data collection compared to those released after
- Task example extraction reveals that training examples can be extracted from GPT-3 models, with increased extraction success tracking improved model performance
- Strong correlation (R=0.88) exists between extracted examples and model accuracy on classification tasks
- Models rarely demonstrate statistically significant improvements over majority baselines on classification tasks with no possibility of task contamination

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs show higher zero-shot and few-shot performance on datasets released before their training data collection date due to task contamination.
- **Mechanism**: The model has been exposed to task examples during pretraining, so when evaluated on these tasks it is no longer in a true zero-shot or few-shot setting.
- **Core assumption**: The model's improved performance correlates with prior exposure to task examples in its training data.
- **Evidence anchors**: [abstract] "We find that for datasets released before LLM training data collection, models are much more likely to beat majority baselines than for datasets released after."
- **Break condition**: If the correlation between dataset release date and model performance disappears when controlling for dataset difficulty, the mechanism is invalid.

### Mechanism 2
- **Claim**: Instruction-tuned models are more susceptible to task contamination than non-instruction-tuned models.
- **Mechanism**: Instruction tuning makes the model more likely to follow prompts that elicit task examples, which can then be extracted to reveal contamination.
- **Core assumption**: Instruction tuning changes the model's behavior in a way that makes task example extraction more effective.
- **Evidence anchors**: [section] "We find that training examples can be extracted from the GPT-3 models, and that the number of extractable training examples increased from each version from davinci to GPT-3.5-turbo"
- **Break condition**: If task example extraction is equally effective on non-instruction-tuned models, the mechanism is invalid.

### Mechanism 3
- **Claim**: Membership inference attacks provide strong evidence of task contamination for generation tasks.
- **Mechanism**: If the model generates content that exactly matches examples in the dataset, it must have seen those examples during training.
- **Core assumption**: Exact matches between generated and dataset content can only occur if the model has been exposed to those examples during training.
- **Evidence anchors**: [section] "We find a strong correlation (R=.88) between the number of extracted examples and the accuracy of the model on the final task."
- **Break condition**: If exact matches can occur without prior exposure (e.g., through highly constrained output spaces), the mechanism is invalid.

## Foundational Learning

- **Concept**: Statistical significance testing
  - **Why needed here**: To determine if performance differences between pre- and post-collection datasets are meaningful or due to chance
  - **Quick check question**: What p-value threshold is used to determine statistical significance in the paper? (Answer: 0.01)

- **Concept**: Chronological analysis
  - **Why needed here**: To identify patterns of possible task contamination across models and datasets by examining performance differences based on dataset release dates
  - **Quick check question**: What are the two dataset categories used in the chronological analysis? (Answer: pre-2021 and post-2021)

- **Concept**: Task example extraction
  - **Why needed here**: To detect task contamination by prompting the model to generate examples of the task
  - **Quick check question**: What is the key difference between task example extraction and membership inference? (Answer: Task example extraction does not require an exact match with training data)

## Architecture Onboarding

- **Component map**: Chronological analysis -> Training data inspection -> Task example extraction -> Membership inference attack
- **Critical path**: The most reliable evidence comes from the combination of chronological analysis showing performance differences and task example extraction revealing the presence of task examples in the model's training data.
- **Design tradeoffs**: The four methods have different precision-recall tradeoffs. Chronological analysis has high recall but low precision, while training data inspection and task example extraction have high precision but low recall.
- **Failure signatures**: If all four methods fail to find evidence of contamination for a model-dataset pair, it does not necessarily mean there is no contamination. The methods may simply not be sensitive enough to detect it.
- **First 3 experiments**:
  1. Replicate the chronological analysis on a subset of datasets to verify the performance difference between pre- and post-collection datasets.
  2. Attempt task example extraction on a known contaminated model to verify the method works as expected.
  3. Apply membership inference to a generation task to check for exact matches between generated and dataset content.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the extent of task contamination in closed-source models that have undergone instruction fine-tuning or RLHF?
- Basis in paper: [explicit] The paper states that closed-sourced models may demonstrate inflated performance in zero-shot or few-shot evaluation due to task contamination, and are therefore not trustworthy baselines in these settings, especially those including instruction fine-tuning or reinforcement learning with human feedback (RLHF).
- Why unresolved: The paper acknowledges that the extent of this contamination is still unknown.
- What evidence would resolve it: Releasing training datasets for closed-source models would allow for easier diagnosis of contamination issues. Additionally, conducting further experiments on a wider range of tasks and models would help quantify the impact of contamination on performance.

### Open Question 2
- Question: How does task contamination affect the performance of large language models on classification tasks compared to generation tasks?
- Basis in paper: [inferred] The paper discusses task contamination for both classification and generation tasks, but focuses primarily on classification tasks. It mentions that for generation tasks, a membership inference attack can be used to check if the model generated content exactly matches examples in the dataset.
- Why unresolved: The paper does not provide a direct comparison of the impact of task contamination on classification vs. generation tasks.
- What evidence would resolve it: Conducting experiments on a range of generation tasks and comparing the performance of models with and without task contamination would provide insights into the differential effects on classification and generation tasks.

### Open Question 3
- Question: What are the most effective strategies for mitigating task contamination in large language models?
- Basis in paper: [explicit] The paper mentions that Jacovi et al. (2023) proposed several strategies for mitigating testing data contamination, but does not discuss specific strategies for task contamination in zero-shot or few-shot settings.
- Why unresolved: The paper identifies the problem of task contamination but does not propose specific solutions.
- What evidence would resolve it: Developing and evaluating different strategies for mitigating task contamination, such as data augmentation, model fine-tuning, or architectural modifications, would provide insights into the most effective approaches.

## Limitations

- The analysis relies on dataset release dates as a proxy for contamination, which may miss contamination through indirect sources like web pages mentioning datasets
- Task example extraction has low recall, potentially missing contamination that exists but cannot be elicited through prompting
- The study focuses on classification and semantic parsing tasks, leaving questions about contamination in other task types unanswered

## Confidence

**High confidence**: The core finding that LLMs perform better on pre-2021 datasets than post-2021 datasets, suggesting task contamination.

**Medium confidence**: The claim that instruction-tuned models are more susceptible to task contamination than non-instruction-tuned models.

**Low confidence**: The assertion that closed-source models with instruction tuning or RLHF have systematically inflated performance due to task contamination.

## Next Checks

1. **Cross-dataset validation**: Test the chronological analysis approach on datasets from different domains (e.g., medical, legal, technical) to verify that the pre/post-2021 performance difference is not domain-specific.

2. **Alternative contamination detection**: Apply watermarking or fingerprinting techniques to detect contamination in models where task example extraction fails, to establish whether low recall represents true absence of contamination versus detection failure.

3. **Training data reconstruction**: Attempt to reconstruct partial training datasets for open models using publicly available web archives and data sources to verify that detected task examples were actually present in the training data rather than being generated through generalization.