---
ver: rpa2
title: 'SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification
  in 200+ Languages and Dialects'
arxiv_id: '2309.07445'
source_url: https://arxiv.org/abs/2309.07445
tags:
- languages
- latn
- language
- asia
- indo-european
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SIB-200, a large-scale evaluation dataset
  for topic classification across 200+ languages and dialects. The dataset leverages
  the Flores-200 machine translation corpus, with English sentences annotated for
  topics and labels extended to 203 additional languages.
---

# SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic Classification in 200+ Languages and Dialects

## Quick Facts
- arXiv ID: 2309.07445
- Source URL: https://arxiv.org/abs/2309.07445
- Authors: 
- Reference count: 20
- One-line primary result: A large-scale evaluation dataset covering 200+ languages and dialects, revealing significant performance gaps for low-resource and unseen languages in topic classification tasks.

## Executive Summary
SIB-200 is a benchmark dataset for topic classification across 200+ languages and dialects, derived from the Flores-200 machine translation corpus. English-labeled examples are extended to 203 additional languages, creating a diverse evaluation suite across 9 topic categories. The paper systematically evaluates performance using fully-supervised, cross-lingual transfer, and zero-shot LLM prompting settings. Results show high accuracy for high-resource languages but significant degradation for low-resource, unseen, and African languages, especially those from underrepresented families or with unique scripts. Targeted adaptation strategies like multilingual adaptive fine-tuning (MAFT) with synthetic data improve performance, particularly for African languages.

## Method Summary
The SIB-200 dataset is built by translating English topic-labeled sentences from Flores-200 into 203 additional languages, resulting in 2,009 sentences across 9 topics. Models evaluated include XLM-R, Glot-500, and region-specific PLMs (AfroXLMR, AfroBERTa, etc.). Experiments include fully-supervised fine-tuning, cross-lingual transfer, and zero-shot LLM prompting. MAFT is applied using NLLB-generated synthetic data for low-resource languages. Performance is analyzed across language families, regions, scripts, and pretraining coverage.

## Key Results
- XLM-R-base achieves >80% accuracy on most languages with 0.1GB of pretraining data, but accuracy drops sharply for unseen languages and scripts.
- Languages unseen during pretraining or from underrepresented families (e.g., Nilotic, Atlantic-Congo) consistently show lowest accuracy.
- AfroXLMR-75 (75 African languages) outperforms XLM-R on African languages, especially Nilotic languages, via region-specific pretraining.
- MAFT with NLLB-synthetic data improves African language accuracy by up to +5% on average for previously unseen languages.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language coverage in pretraining is the primary driver of classification accuracy for low-resource languages.
- Mechanism: Languages seen during pretraining benefit from learned representations and cross-lingual alignment; unseen languages lack these features.
- Core assumption: Pretrained representations generalize across languages sharing scripts or families.
- Evidence anchors:
  - [abstract]: "languages unseen during the pre-training of multilingual language models...often have the lowest performance"
  - [section]: "languages that are included during pre-training often have better performance"
  - [corpus]: "Weak – corpus only lists language families, not specific model training data"
- Break condition: If pretraining corpus is domain-biased (e.g., religious), unseen languages may outperform seen ones on non-matching tasks.

### Mechanism 2
- Claim: Script familiarity outweighs language family similarity for model performance.
- Mechanism: Models trained on a script can process unseen languages in that script better than seen languages in unseen scripts.
- Core assumption: Tokenization and embeddings capture script-level patterns that transfer across languages.
- Evidence anchors:
  - [section]: "languages unseen in the pre-training corpus of XLM-R...it outperforms Glot-500 in most cases as long as the written scripts are seen"
  - [section]: "The slope of XLM-R is often steeper than that of MLP-XLMR, implying the preferred script for a language also has better pre-trained representations"
  - [corpus]: "Missing – no explicit comparison of script vs family effects in corpus"
- Break condition: If a language uses multiple scripts, the less common script may yield significantly worse results.

### Mechanism 3
- Claim: Multilingual adaptive fine-tuning with synthetic data closes performance gaps for low-resource languages.
- Mechanism: MAFT adapts a multilingual model to new languages by leveraging large-scale synthetic translation data, effectively bootstrapping representations.
- Core assumption: Synthetic translations preserve semantic structure sufficiently for fine-tuning.
- Evidence anchors:
  - [section]: "Evaluation of this approach on African languages results in significant improvement (up to +5% in accuracy on average) for the previously unseen languages"
  - [section]: "To further extend to more languages with less than 10MB of data, we generate machine-translated data using NLLB for 34 African languages"
  - [corpus]: "Weak – corpus does not detail synthetic data quality or quantity"
- Break condition: If synthetic data is too noisy or domain-mismatched, fine-tuning may degrade performance.

## Foundational Learning

- Concept: Multilingual pretraining and cross-lingual transfer
  - Why needed here: The paper evaluates cross-lingual zero-shot transfer and relies on multilingual PLMs; understanding pretraining objectives and alignment is essential.
  - Quick check question: What is the difference between masked language model pretraining and autoregressive pretraining in terms of cross-lingual transfer potential?

- Concept: Script-based tokenization and subword modeling
  - Why needed here: Script choice strongly affects model performance; engineers must understand how tokenization interacts with script representation.
  - Quick check question: How does SentencePiece tokenization handle languages with different scripts, and what are the implications for unseen scripts?

- Concept: Domain adaptation and fine-tuning strategies
  - Why needed here: The dataset is news-domain; pretraining on religious or other domains may hurt performance; domain adaptation is critical.
  - Quick check question: Why might fine-tuning on synthetic data be preferable to direct pretraining for low-resource languages?

## Architecture Onboarding

- Component map:
  - Data ingestion: SIB-200 (Flores-200-based topic classification)
  - Model zoo: XLM-R, Glot-500, region-specific PLMs (AfriBERTa, AfroXLMR, IndicBERTv2, MuRIL)
  - Fine-tuning pipeline: MAFT with synthetic data generation
  - Evaluation: Full-supervised, cross-lingual transfer, zero-shot prompting
  - Analysis: Language family, region, Joshi class, script coverage breakdowns

- Critical path:
  1. Load SIB-200, split into TRAIN/DEV/TEST
  2. Select base PLM (e.g., XLM-R)
  3. Fine-tune on source language(s) for transfer
  4. Evaluate on target languages
  5. Analyze by groupings (family, region, etc.)
  6. If low-resource, apply MAFT with synthetic data

- Design tradeoffs:
  - Model size vs. coverage: Larger models (XLM-R) vs. specialized (AfroXLMR)
  - Synthetic vs. real data: Coverage vs. quality
  - Zero-shot prompting vs. fine-tuning: Cost vs. performance

- Failure signatures:
  - Low accuracy on unseen scripts despite high-resource language coverage
  - Domain mismatch: Pretraining on religious text hurts news-domain performance
  - Overfitting to high-resource languages when fine-tuning

- First 3 experiments:
  1. Full-supervised fine-tuning on a high-resource language (e.g., English) and evaluate on all languages
  2. Cross-lingual transfer from English to an unseen-script language (e.g., Amharic)
  3. MAFT on AfroXLMR-61 vs. AfroXLMR-75 and compare gains on Nilotic languages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can multilingual language models be improved to better handle languages with very small pre-training corpora (e.g., <0.1GB)?
- Basis in paper: [explicit] The paper found that XLM-R achieves >80% accuracy on most languages with as little as 0.1GB of pre-training corpus, but this is still not sufficient for some African languages.
- Why unresolved: The paper suggests that more diverse pre-training data could help, but does not provide concrete solutions for extremely low-resource languages.
- What evidence would resolve it: Experiments comparing different data augmentation, synthetic data generation, or transfer learning techniques on languages with tiny pre-training corpora.

### Open Question 2
- Question: Does the choice of written script significantly impact model performance for languages with multiple scripts, and if so, how can models be adapted to be more script-agnostic?
- Basis in paper: [explicit] The paper found that using n-gram features was more robust to script changes than using XLM-R tokenizer features, suggesting script impacts performance.
- Why unresolved: While the paper identifies the issue, it does not explore comprehensive methods to make models script-agnostic or evaluate the impact of script choice systematically.
- What evidence would resolve it: Controlled experiments comparing model performance across different scripts for the same language, and evaluating methods to normalize or adapt models to script variations.

### Open Question 3
- Question: What are the most effective strategies for cross-lingual transfer when the target language is not seen during pre-training and has a unique script?
- Basis in paper: [explicit] The paper found that cross-lingual transfer performed poorly for languages not seen during pre-training, especially when the script was also unseen.
- Why unresolved: The paper suggests that fully supervised methods can improve performance, but does not explore other transfer learning techniques that might be effective for such challenging cases.
- What evidence would resolve it: Comparative studies of various transfer learning methods (e.g., adapter-based approaches, meta-learning) for zero-resource languages with unique scripts.

### Open Question 4
- Question: How does the domain of pre-training data (e.g., religious vs. news) affect the performance of multilingual language models on downstream tasks in different domains?
- Basis in paper: [explicit] The paper suggests that Glot-500, pre-trained on religious data, underperformed compared to XLM-R on a news-based task, indicating domain mismatch.
- Why unresolved: The paper does not systematically evaluate the impact of pre-training domain on model performance across different downstream tasks.
- What evidence would resolve it: Experiments pre-training models on different domain-specific corpora and evaluating their performance on tasks from various domains.

## Limitations

- Dataset construction relies on translating English-labeled examples, which may introduce language-specific classification artifacts.
- Evaluation focuses on accuracy without analyzing false positive patterns or error distributions across topics.
- Synthetic data for MAFT comes from NLLB translation without reporting translation quality or domain alignment metrics.

## Confidence

- **High confidence**: The observed performance gaps between high-resource and low-resource languages, and the correlation between pretraining coverage and downstream accuracy, are well-supported by the experimental results.
- **Medium confidence**: The conclusion that script familiarity is more important than language family similarity is supported but could benefit from controlled experiments isolating script effects.
- **Medium confidence**: The MAFT approach shows improvements for African languages, but the magnitude of gains and generalization to other regions requires further validation.

## Next Checks

1. **Domain transfer validation**: Evaluate the same models on a non-news domain (e.g., social media or literature) to determine whether performance gaps persist across domains or are specific to news text.
2. **Synthetic data quality audit**: Compare classification accuracy using synthetic translations from different models (e.g., NLLB vs. GPT-4) to quantify the impact of translation quality on MAFT performance.
3. **Script isolation experiment**: Create a controlled test where models are evaluated on languages sharing scripts but from different families to quantify script versus family effects independently.