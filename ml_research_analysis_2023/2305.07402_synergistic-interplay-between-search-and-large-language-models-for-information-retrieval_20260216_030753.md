---
ver: rpa2
title: Synergistic Interplay between Search and Large Language Models for Information
  Retrieval
arxiv_id: '2305.07402'
source_url: https://arxiv.org/abs/2305.07402
tags:
- retrieval
- language
- knowledge
- inter
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of combining search engines and
  large language models (LLMs) for information retrieval. It proposes InteR, a novel
  framework that iteratively refines queries by leveraging the strengths of both search
  engines and LLMs.
---

# Synergistic Interplay between Search and Large Language Models for Information Retrieval

## Quick Facts
- arXiv ID: 2305.07402
- Source URL: https://arxiv.org/abs/2305.07402
- Authors: 
- Reference count: 27
- The paper proposes InteR, a framework that iteratively refines queries by leveraging both search engines and large language models for improved information retrieval.

## Executive Summary
This paper addresses the challenge of combining search engines and large language models (LLMs) for information retrieval. The authors propose InteR, a novel framework that iteratively refines queries by leveraging the strengths of both search engines and LLMs. The core idea involves using LLMs to generate summaries that enrich search queries, and then using search engines to retrieve documents based on these enriched queries. The retrieved documents are then used to further refine the prompts for LLMs, leading to more accurate retrieval. Experiments on two large-scale retrieval benchmarks demonstrate that InteR achieves superior zero-shot document retrieval performance compared to state-of-the-art methods.

## Method Summary
InteR is a framework that iteratively refines queries by using LLMs to generate summaries that enrich search queries, then using search engines to retrieve documents based on these enriched queries. The process involves two main steps: the SE step, which uses BM25 or dense retrieval to retrieve top-k documents, and the LLM step, which uses gpt-3.5-turbo-0301 to generate summaries from refined prompts containing queries and retrieved documents. The framework is repeated M times with M=2 as default, starting with empty retrieved documents for the first LLM step. The method operates in a zero-shot manner, requiring no relevance labels during training.

## Key Results
- InteR achieves superior zero-shot document retrieval performance compared to state-of-the-art methods on TREC Deep Learning 2019 and 2020 benchmarks
- Significant improvements in MAP and nDCG@10 metrics, with R@1k reaching 0.8308 and 0.8458 on DL'19 and DL'20 respectively
- The framework demonstrates effectiveness across different knowledge refinement iterations (M) and number of generated examples (h), with optimal performance at M=2 and h=10

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The iterative query refinement loop allows SE and LLM to mutually enrich each other's inputs, leading to more accurate retrieval.
- Mechanism: SE uses LLM-generated summaries to expand queries with domain-relevant knowledge, while LLM uses SE-retrieved documents as demonstrations to improve prompt formulation.
- Core assumption: Outputs of SE and LLM are complementary and can be effectively combined to improve retrieval quality.
- Evidence anchors:
  - [abstract] "This iterative refinement process augments the inputs of RMs and LLMs, leading to more accurate retrieval."
  - [section 4.1] "We hope the summary s can provide directly relevant information to the input query q and help the SEs focus on the domain or topic in user query q."
  - [corpus] Weak evidence - only 1 neighbor paper directly addresses LLM+search interplay.
- Break condition: If generated summaries are too generic or retrieved documents are irrelevant, the mutual enrichment becomes noisy and degrades performance.

### Mechanism 2
- Claim: Using dense retrieval for constructing the document set ¯D provides better semantic alignment than sparse retrieval when feeding documents to the LLM.
- Mechanism: The dense retrieval model captures semantic similarity between queries and documents, producing more relevant documents that serve as better demonstrations for the LLM.
- Core assumption: Semantic relevance is more important than lexical overlap when providing context to LLMs.
- Evidence anchors:
  - [section 4.3] "we use dense retrieval model to construct ¯D as default. It is noteworthy that we only use dense retrieval model for constructing ¯D during knowledge refinement and leave the final retrieval model as sparse BM25 for efficiency."
  - [section 5.5] "the dense model is the best strategy for InteR and even outperforms the hybrid counterpart, which is consistent with the prior works (Karpukhin et al., 2020)."
  - [corpus] No direct evidence - corpus lacks dense retrieval vs sparse retrieval comparisons for LLM input.
- Break condition: If the dense retrieval model fails to retrieve semantically relevant documents due to domain shift or poor initialization.

### Mechanism 3
- Claim: Sampling multiple examples (h=10) from the LLM provides sufficient diversity in generated summaries without introducing excessive redundancy.
- Mechanism: The LLM generates multiple candidate summaries for each query, which are concatenated to form a comprehensive knowledge collection that enriches the search query.
- Core assumption: Diversity in generated examples improves coverage of potential relevant information while avoiding redundancy beyond a certain point.
- Evidence anchors:
  - [section 5.5] "as the number of generated examples increases, the performance metrics demonstrate a gradual improvement until reaching 10 examples. Subsequently, the performance metrics stabilize."
  - [section 4.2] "For each q′, we sample h = 10 examples via LLM and simply concatenate them as summary s."
  - [corpus] Weak evidence - corpus lacks studies on optimal number of LLM-generated examples for retrieval tasks.
- Break condition: If generated examples become too similar (low diversity) or if the LLM produces hallucinations that mislead the retrieval process.

## Foundational Learning

- Concept: Dense vs Sparse Retrieval
  - Why needed here: Understanding the difference between semantic-based (dense) and keyword-based (sparse) retrieval is crucial for implementing the dual-retrieval strategy in InteR.
  - Quick check question: What is the main advantage of using dense retrieval when providing documents to an LLM as context?

- Concept: Zero-shot Learning
  - Why needed here: InteR operates without relevance labels, so understanding zero-shot approaches and their limitations is essential for proper evaluation and comparison.
  - Quick check question: How does zero-shot dense retrieval differ from traditional supervised dense retrieval in terms of training data requirements?

- Concept: Prompt Engineering for LLMs
  - Why needed here: The quality of generated summaries depends heavily on how queries and retrieved documents are formatted as prompts to the LLM.
  - Quick check question: What is the purpose of including retrieved documents in the LLM prompt format described in Section 4.2?

## Architecture Onboarding

- Component map:
  - Query Input → LLM Generator (h samples) → Summary Concatenation → SE Query Enrichment → Dense Retrieval → Document Set ¯D → LLM Prompt Enhancement → New Query → Repeat (M times) → Final BM25 Retrieval

- Critical path: The iterative loop between LLM and SE components is the core value proposition; any delay or failure in this loop directly impacts retrieval quality.

- Design tradeoffs: Dense retrieval for document selection provides better semantic alignment but is slower and more resource-intensive than sparse retrieval; using BM25 for final retrieval prioritizes efficiency over semantic precision.

- Failure signatures: Degraded MAP/nDCG scores despite increased iteration count may indicate that generated summaries or retrieved documents are introducing noise rather than useful information.

- First 3 experiments:
  1. Test single iteration (M=1) with h=1 to establish baseline performance without complexity
  2. Vary h from 1 to 20 while keeping M=1 to identify optimal number of examples before redundancy
  3. Compare dense vs sparse retrieval for constructing ¯D while keeping other parameters fixed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of knowledge refinement iterations (M) for different types of information retrieval tasks and datasets?
- Basis in paper: [explicit] The paper discusses the impact of different numbers of knowledge refinement iterations (M) on performance, finding that M=2 provides optimal results, but further increasing M does not improve performance.
- Why unresolved: The paper only explores a limited range of M values (0-3) on two specific datasets. Different tasks and datasets may have different optimal values for M.
- What evidence would resolve it: Systematic experiments varying M across diverse IR tasks and datasets, with statistical analysis to determine optimal values for different scenarios.

### Open Question 2
- Question: How does the performance of InteR scale with the size of the document corpus and the complexity of queries?
- Basis in paper: [inferred] The paper demonstrates effectiveness on two large-scale benchmarks but does not explicitly analyze how performance scales with corpus size or query complexity.
- Why unresolved: Scaling behavior is critical for practical deployment, but the paper focuses on benchmark performance without analyzing computational complexity or performance degradation with larger datasets.
- What evidence would resolve it: Experiments measuring runtime, memory usage, and retrieval quality as corpus size and query complexity increase, including analysis of bottlenecks and potential optimizations.

### Open Question 3
- Question: Can InteR be effectively adapted for real-time information retrieval scenarios where document content is constantly updating?
- Basis in paper: [explicit] The paper mentions that search engines can index up-to-date documents while LLMs are limited to their training data timeframe, suggesting a potential advantage of the hybrid approach.
- Why unresolved: The paper does not address how the system handles dynamic document updates or whether the iterative refinement process can be optimized for real-time scenarios.
- What evidence would resolve it: Experiments testing InteR's performance on streaming document collections and analysis of update frequency requirements for maintaining retrieval quality in dynamic environments.

## Limitations

- The paper lacks detailed specification of the LLM prompt templates and document preprocessing steps, which are critical for reproducing the InteR framework.
- The effectiveness of the iterative refinement loop depends heavily on the quality of these components, but the paper provides insufficient detail for exact replication.
- The optimal number of LLM-generated examples (h=10) is based on limited experimentation within the paper's scope, with no external validation or theoretical justification for why this specific number is optimal.

## Confidence

- **High confidence**: The core mechanism of iterative query refinement using LLM-generated summaries and search engine retrieval is well-supported by the experimental results showing consistent improvements across multiple metrics (MAP, nDCG@10, R@1k).
- **Medium confidence**: The claim about dense retrieval being superior to sparse retrieval for constructing the document set ¯D is supported by ablation studies, but the evidence is limited to this single paper's experiments without broader corpus support.
- **Low confidence**: The optimal number of LLM-generated examples (h=10) is based on limited experimentation within the paper's scope, with no external validation or theoretical justification for why this specific number is optimal.

## Next Checks

1. Test the impact of prompt engineering variations by systematically modifying the LLM prompt templates while keeping all other parameters constant, measuring changes in retrieval performance.
2. Conduct ablation studies comparing different document chunking strategies and indexing methods to determine how preprocessing affects the quality of retrieved documents used in the iterative loop.
3. Evaluate the framework's robustness to domain shift by testing on datasets from different domains than MS-MARCO to assess whether the synergistic interplay between search and LLMs generalizes beyond the training domain.