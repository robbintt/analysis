---
ver: rpa2
title: 'Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time
  Policy Adaptation'
arxiv_id: '2307.06333'
source_url: https://arxiv.org/abs/2307.06333
tags:
- user
- concepts
- concept
- policy
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of adapting robot policies to
  new environments where the distribution of states or the user's desired reward function
  has shifted from training. The core method is a human-in-the-loop framework called
  DFA that uses counterfactual demonstrations to help users identify task-irrelevant
  visual concepts, then performs data augmentation on those concepts to efficiently
  adapt the policy.
---

# Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation

## Quick Facts
- **arXiv ID**: 2307.06333
- **Source URL**: https://arxiv.org/abs/2307.06333
- **Reference count**: 14
- **Primary result**: Counterfactual demonstrations significantly improve users' ability to identify task-irrelevant concepts (p < .001), leading to more efficient policy adaptation.

## Executive Summary
This paper introduces DFA, a human-in-the-loop framework for adapting robot policies to new environments where the state or reward function has shifted from training. The core innovation is using counterfactual demonstrations to help users identify task-irrelevant visual concepts, then performing data augmentation on those concepts to efficiently adapt the policy. Through human experiments across three domains, counterfactuals significantly improved users' ability to identify true shifted concepts compared to behavior alone, leading to more efficient policy finetuning. The framework enables users to better understand agent failures and aligns the agent to individual preferences with less effort.

## Method Summary
The DFA framework adapts robot policies by generating counterfactual demonstrations that modify visual concepts in failed trajectories to match user demonstrations. Users provide feedback on which modified concepts are task-irrelevant, enabling the system to generate augmented demonstrations by varying those concepts. This augmented data is then used to fine-tune the policy to the new environment. The method requires a predefined visual concept space and a state editor function for modifying concepts in observations.

## Key Results
- Counterfactuals significantly improved user accuracy in identifying true shifted concepts (p < .001) across all three domains
- 79% of users correctly identified task-relevant vs irrelevant concepts for their desired reward
- More accurate user feedback led to more efficient policy finetuning
- Users reported that counterfactuals helped them better understand the cause of agent failures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual demonstrations improve human ability to identify task-irrelevant concepts
- Mechanism: By showing users a successful trajectory that differs only in the shifted concept, they can directly compare successful vs. unsuccessful behavior and identify which concept changes led to success
- Core assumption: Users can recognize which concepts impact task success when shown contrastive examples
- Evidence anchors:
  - [abstract]: "counterfactuals significantly improved users' ability to identify the true shifted concepts (p < .001) compared to behavior alone"
  - [section]: "users who received counterfactuals were significantly more accurate at identifying the correct concept shift compared to users with behavior alone (p < .001 across all domains)"
  - [corpus]: Weak - related papers focus on different applications of human-in-the-loop systems but don't specifically address counterfactual demonstrations for concept identification
- Break condition: If the counterfactual generation fails to produce a successful trajectory that matches user demonstration, or if the concept space Φ does not capture the relevant aspects of failure

### Mechanism 2
- Claim: User feedback on task-irrelevant concepts enables efficient data augmentation for policy adaptation
- Mechanism: Once users identify which concepts are task-irrelevant, the system can generate augmented demonstrations by varying those concepts, reducing the number of demonstrations needed for fine-tuning
- Core assumption: Data augmentation on task-irrelevant concepts preserves task-relevant behavior while expanding the training distribution
- Evidence anchors:
  - [abstract]: "This more accurate feedback led to more efficient policy finetuning"
  - [section]: "more accurate feedback directly results in more efficient policy performance on R′ after targeted data augmentation"
  - [corpus]: Weak - related papers discuss human feedback and data augmentation but not in the specific context of test-time policy adaptation with counterfactuals
- Break condition: If users incorrectly identify task-relevant concepts as irrelevant, or if the state editor cannot properly generate valid counterfactuals

### Mechanism 3
- Claim: The framework aligns agent behavior to individual user preferences
- Mechanism: By allowing users to specify which concepts are task-irrelevant for their desired reward function, the system can adapt the policy to match individual user preferences rather than a generic training reward
- Core assumption: Different users may have different preferences about which concepts are relevant to a task
- Evidence anchors:
  - [abstract]: "aligns the agent to individual user task preferences"
  - [section]: "79% of users correctly identifying task-relevant vs irrelevant concepts for their desired reward"
  - [corpus]: Weak - related papers discuss personalization but not in the context of identifying task-relevant vs irrelevant concepts for robotic policies
- Break condition: If users have preferences that cannot be expressed through the available concept space Φ, or if the reward function cannot capture the nuanced preferences users express

## Foundational Learning

- Concept: Distribution shift in sequential decision-making
  - Why needed here: The framework specifically addresses the problem of policies failing due to distribution shift between training and deployment environments
  - Quick check question: What are the two types of distribution shift the paper identifies, and how do they differ in terms of their impact on policy performance?

- Concept: Counterfactual explanations in sequential decision-making
  - Why needed here: The core mechanism relies on generating counterfactual demonstrations to help users identify failure causes
  - Quick check question: How does the counterfactual generation process in this framework differ from counterfactual explanations in classification tasks?

- Concept: Human-in-the-loop interactive learning
  - Why needed here: The framework requires active user participation to identify task-relevant vs irrelevant concepts and provide demonstrations
  - Quick check question: What specific type of human feedback does this framework collect, and how does it differ from traditional reward-based human feedback in reinforcement learning?

## Architecture Onboarding

- Component map: Policy πθ -> State editor f -> Concept mapper Φ -> Counterfactual generator -> User interface -> Data augmentation module -> Fine-tuning system

- Critical path: Failed trajectory → Counterfactual generation → User feedback on TI concepts → Data augmentation → Policy fine-tuning

- Design tradeoffs:
  - Exhaustive search vs. continuous optimization for counterfactual generation (computational cost vs. precision)
  - Granularity of concept space Φ (coverage vs. complexity)
  - Number of counterfactuals shown (user cognitive load vs. accuracy of feedback)

- Failure signatures:
  - No valid counterfactual can be generated (concept space insufficient)
  - Users consistently misidentify TI concepts (counterfactuals not interpretable)
  - Augmented demonstrations don't improve policy (data augmentation ineffective)
  - Policy fine-tuning diverges (overfitting to augmented data)

- First 3 experiments:
  1. Verify counterfactual generation works by checking if generated trajectories match user demonstrations
  2. Test user accuracy on TI concept identification with and without counterfactuals
  3. Measure policy performance improvement after fine-tuning with augmented data

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How do different visualization interfaces affect the quality and specificity of human feedback on task-irrelevant concepts?
- Basis in paper: [inferred] The paper discusses that users may have complex delineations of concept invariance (e.g. blue and green, but not red) and that different visualization interfaces could help users give more detailed feedback.
- Why unresolved: The paper only mentions this as a potential future direction but does not explore different interface designs or test their effectiveness.
- What evidence would resolve it: User studies comparing different visualization methods (e.g., 2D vs 3D, interactive vs static, highlighting vs masking) and measuring the accuracy and specificity of human feedback on task-irrelevant concepts across various domains.

### Open Question 2
- Question: Can continuous relaxations be effectively applied to the minimum edit counterfactual problem to replace the current brute force search?
- Basis in paper: [explicit] The paper states that the minimum edit counterfactual solution is currently solved via brute force search over object instantiations and suggests that continuous relaxations could be explored to enable gradient descent optimization.
- Why unresolved: The paper does not implement or evaluate any continuous relaxation approaches, only mentioning it as a future direction.
- What evidence would resolve it: Implementation and comparison of gradient-based counterfactual generation methods against the current brute force approach, measuring computational efficiency and counterfactual quality across multiple domains.

### Open Question 3
- Question: How does the size and complexity of the concept space impact the effectiveness of counterfactual explanations for human understanding?
- Basis in paper: [inferred] The paper shows that counterfactuals significantly improved user accuracy in identifying shifted concepts, with the largest effect in VIMA which has the largest concept space, but does not systematically study the relationship between concept space size and effectiveness.
- Why unresolved: The experiments compare domains with different concept space sizes but do not control for other variables or test intermediate sizes to establish a relationship.
- What evidence would resolve it: Controlled experiments varying the number of objects and concept instantiations while holding other factors constant, measuring human accuracy and time to identify shifted concepts across different concept space sizes.

## Limitations
- The framework's reliance on user feedback introduces potential bottlenecks in scalability and consistency
- Study sample size is small (3-4 participants per condition) and focused on novice users
- The concept space Φ requires careful engineering and may not generalize well to domains with high visual complexity

## Confidence
- **High confidence**: The counterfactual mechanism improves user identification accuracy of task-irrelevant concepts (supported by significant p-values across all domains)
- **Medium confidence**: Data augmentation on identified concepts leads to efficient policy adaptation (based on simulation results but limited human study validation)
- **Low confidence**: Framework generalizes to complex, real-world robotic tasks (current evaluation limited to controlled simulation environments)

## Next Checks
1. Conduct larger-scale user studies with diverse participant backgrounds and more complex task domains to evaluate robustness and scalability
2. Test framework performance when concept space Φ cannot fully capture relevant task distinctions or when multiple concepts interact
3. Evaluate long-term performance stability after multiple adaptation cycles and assess user fatigue effects on feedback quality