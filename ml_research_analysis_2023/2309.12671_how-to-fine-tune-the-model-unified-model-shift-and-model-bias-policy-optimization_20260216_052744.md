---
ver: rpa2
title: 'How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization'
arxiv_id: '2309.12671'
source_url: https://arxiv.org/abs/2309.12671
tags:
- shift
- performance
- learning
- bias
- term
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces USB-PO, a model-based reinforcement learning
  algorithm that addresses the challenge of excessive model updates by unifying model
  shift and model bias. Unlike prior methods that rely on fixed thresholds or ignore
  model shift, USB-PO adaptively fine-tunes model updates to guarantee performance
  improvement.
---

# How to Fine-tune the Model: Unified Model Shift and Model Bias Policy Optimization

## Quick Facts
- arXiv ID: 2309.12671
- Source URL: https://arxiv.org/abs/2309.12671
- Reference count: 40
- Key outcome: Introduces USB-PO, a model-based RL algorithm that adaptively fine-tunes model updates by unifying model shift and model bias terms using second-order Wasserstein distances, achieving state-of-the-art performance on continuous control benchmarks with improved sample efficiency and robustness to learning rate settings.

## Executive Summary
This paper addresses the challenge of excessive model updates in model-based reinforcement learning by introducing USB-PO, a unified framework that simultaneously optimizes model shift and model bias. Unlike prior methods that rely on fixed thresholds or ignore model shift entirely, USB-PO uses second-order Wasserstein distances to adaptively fine-tune model updates, ensuring performance improvement. The method is theoretically grounded and demonstrates strong empirical results on continuous control benchmarks, outperforming both model-based and model-free baselines.

## Method Summary
USB-PO employs a two-phase model learning process. Phase 1 trains an ensemble of probabilistic neural networks using maximum likelihood estimation (MLE) on real environment data. Phase 2 fine-tunes the model using an objective that minimizes the sum of second-order Wasserstein distances between the pre-update model and post-update model (model shift) and between the post-update model and the real environment (model bias). This unified optimization adaptively adjusts model updates to balance these competing objectives. The fine-tuned model is then used to generate rollouts for policy optimization, integrated with SAC. The method is robust to learning rate settings and does not require environment-specific tuning.

## Key Results
- USB-PO achieves state-of-the-art performance on continuous control benchmarks (Ant-v2, HalfCheetah-v2, Hopper-v2, Humanoid-v2, InvertedPendulum-v2, Walker2d-v2)
- Demonstrates improved sample efficiency compared to both model-based and model-free baselines
- Shows robustness to learning rate settings in phase 2 fine-tuning, unlike prior methods requiring careful threshold tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: USB-PO achieves performance improvement by adaptively fine-tuning model updates through unified optimization of model shift and model bias terms.
- Mechanism: Uses a two-phase process where phase 1 trains with MLE loss, and phase 2 fine-tunes using an objective minimizing second-order Wasserstein distances for model shift (between pre- and post-update models) and model bias (between post-update model and real environment).
- Core assumption: The performance improvement guarantee can be derived by optimizing the sum of model shift and model bias terms, where the intractable term ∆ can be ignored due to its higher-order nature compared to individual model shift and bias terms.
- Evidence anchors: [abstract] "theoretically derive an optimization objective that can unify model shift and model bias"; [section] "Eq.(3) unifies the model shift term and the model bias term in the second-order Wasserstein distance form"; [corpus] related works on distribution shift and model-based RL.
- Break condition: If the assumption that ∆ is negligible fails, or if the Wasserstein distance formulation doesn't capture the true performance impact of model shift and bias.

### Mechanism 2
- Claim: The fine-tuning process in phase 2 can potentially reduce both model shift and model bias while avoiding model overfitting.
- Mechanism: By minimizing the unified objective in phase 2, the algorithm actively adjusts the model to find appropriate updates that reduce both the discrepancy from the previous model (shift) and the discrepancy from the real environment (bias).
- Core assumption: The optimization of the unified objective leads to updates that reduce both model shift and model bias simultaneously, rather than trading one off against the other.
- Evidence anchors: [section] "we calculate the optimization objective value before and after the fine-tuning process and plot their difference... all random seeds show the consistent result"; [section] "when USB-PO actively recognizes that an improper update happens, it performs a pullback operation"; [corpus] work on mitigating distribution shift and synthetic data issues in model-based RL.
- Break condition: If the unified objective optimization doesn't actually reduce both terms simultaneously, or if reducing both leads to underfitting rather than avoiding overfitting.

### Mechanism 3
- Claim: USB-PO is robust to learning rate settings in phase 2, unlike previous methods that require carefully tuned thresholds.
- Mechanism: The adaptive nature of the unified optimization allows the algorithm to find appropriate model updates without relying on a fixed threshold parameter, making it less sensitive to the specific learning rate chosen for the fine-tuning process.
- Core assumption: The unified optimization objective provides sufficient guidance for appropriate model updates regardless of the specific learning rate magnitude, as long as it's within a reasonable range.
- Evidence anchors: [abstract] "The algorithm is robust to learning rate settings and does not require environment-specific tuning"; [section] "Unlike CMLO which is strongly dependent on a carefully chosen threshold for each environment to constrain the impacts of model shift, USB-PO is less sensitive to the learning rate of phase 2"; [corpus] work on learning rate sensitivity in RL.
- Break condition: If the learning rate is set too high or too low, causing the fine-tuning process to diverge or become ineffective despite the unified optimization.

## Foundational Learning

- Concept: Performance difference bound and return discrepancy in model-based RL
  - Why needed here: The paper's theoretical framework is built on decomposing the performance difference bound and comparing it to the simpler return discrepancy approach used in prior work
  - Quick check question: What is the key difference between optimizing the performance difference bound versus return discrepancy in model-based RL?

- Concept: Wasserstein distance and its application in RL
  - Why needed here: The unified optimization objective uses second-order Wasserstein distances to measure both model shift and model bias
  - Quick check question: How does the second-order Wasserstein distance between two Gaussian distributions relate to their means and covariances?

- Concept: Ensemble models and uncertainty estimation in MBRL
  - Why needed here: The paper uses an ensemble of probabilistic neural networks to capture aleatoric and epistemic uncertainty, following prior work
  - Quick check question: Why might using an ensemble of models be beneficial for handling uncertainty in model-based reinforcement learning?

## Architecture Onboarding

- Component map: Real data collection -> Phase 1: Model training with MLE -> Phase 2: Model fine-tuning with unified objective -> Policy optimization using model rollouts
- Critical path: Collect real data → train model with MLE → fine-tune model with unified objective → generate model rollouts → train policy
- Design tradeoffs: The unified optimization provides adaptivity but adds computational overhead for phase 2. Using Wasserstein distance captures distributional differences but may be computationally expensive compared to simpler metrics.
- Failure signatures: If phase 2 fine-tuning doesn't improve performance, it could indicate issues with the unified objective formulation or implementation. If the algorithm is sensitive to the phase 2 learning rate despite claims, it suggests the adaptivity is limited.
- First 3 experiments:
  1. Run phase 1 only (no fine-tuning) to establish baseline performance
  2. Run with different phase 2 learning rates to verify robustness claims
  3. Compare the model shift and bias terms before and after fine-tuning on a single environment to verify the mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of USB-PO scale with increasingly complex environments, such as those with higher-dimensional state spaces or longer task horizons?
- Basis in paper: [inferred] The paper tests USB-PO on several MuJoCo continuous control benchmark tasks but does not explore performance on environments with significantly higher complexity.
- Why unresolved: The experiments are limited to standard benchmark tasks, which may not fully capture the algorithm's scalability or limitations in more complex settings.
- What evidence would resolve it: Testing USB-PO on a wider range of environments, including those with higher-dimensional state spaces or longer horizons, would provide insights into its scalability and robustness.

### Open Question 2
- Question: What is the theoretical basis for the convergence of the fine-tuning process in Phase 2, and under what conditions can convergence be guaranteed?
- Basis in paper: [explicit] The paper mentions that the model can converge under the role of the fine-tuning process but does not provide a rigorous theoretical analysis of the convergence conditions or guarantees.
- Why unresolved: The paper focuses on empirical validation rather than theoretical guarantees for the convergence of the fine-tuning process.
- What evidence would resolve it: A formal theoretical analysis proving the convergence of the fine-tuning process under specific conditions would clarify its reliability and limitations.

### Open Question 3
- Question: How does the choice of the second-order Wasserstein distance as the metric for model shift and model bias affect the algorithm's performance compared to other metrics, such as the total variation distance or KL divergence?
- Basis in paper: [explicit] The paper uses the second-order Wasserstein distance to unify model shift and model bias but does not compare its effectiveness to other distance metrics.
- Why unresolved: The paper does not explore alternative metrics or their impact on the algorithm's performance, leaving the choice of Wasserstein distance unvalidated against other options.
- What evidence would resolve it: Comparative experiments using different distance metrics (e.g., total variation distance, KL divergence) would demonstrate the advantages or limitations of the Wasserstein distance in this context.

## Limitations
- Theoretical framework relies on the assumption that the ∆ term can be ignored as a higher-order term, which may not hold in all scenarios
- Experimental validation uses a relatively small number of seeds (3) and focuses primarily on continuous control tasks, leaving questions about generalization to other domains
- The claim of "no environment-specific tuning" is somewhat misleading - while the method may be less sensitive to learning rate choices than CMLO, it still requires appropriate learning rate selection and other hyperparameters

## Confidence
- **High confidence**: The mechanism of unifying model shift and bias through Wasserstein distances is well-grounded theoretically and the empirical performance improvements are clearly demonstrated
- **Medium confidence**: The claims about robustness to learning rate settings and avoiding overfitting are supported but could benefit from more extensive hyperparameter sensitivity analysis
- **Medium confidence**: The sample efficiency improvements are demonstrated on benchmark tasks but may not generalize to more complex or diverse environments

## Next Checks
1. **Theoretical validation**: Rigorously verify the assumption that ∆ can be ignored by comparing the magnitudes of all terms in the performance difference bound across different model learning scenarios
2. **Hyperparameter sensitivity**: Conduct a systematic ablation study varying the phase 2 learning rate across several orders of magnitude to quantify the actual robustness to this critical hyperparameter
3. **Generalization test**: Evaluate USB-PO on a broader set of environments including sparse reward tasks, partially observable environments, and tasks with different dynamics complexity to assess the limits of its performance guarantees