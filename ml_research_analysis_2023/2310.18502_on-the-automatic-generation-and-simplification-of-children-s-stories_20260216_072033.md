---
ver: rpa2
title: On the Automatic Generation and Simplification of Children's Stories
arxiv_id: '2310.18502'
source_url: https://arxiv.org/abs/2310.18502
tags:
- stories
- words
- children
- simplification
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examines whether large language models (LLMs) can generate\
  \ age-appropriate children\u2019s stories and how lexical simplification models\
  \ perform on children\u2019s text. It finds that LLMs struggle to produce stories\
  \ with appropriate simplicity levels, with over 17% worse readability scores compared\
  \ to human-written stories."
---

# On the Automatic Generation and Simplification of Children's Stories

## Quick Facts
- arXiv ID: 2310.18502
- Source URL: https://arxiv.org/abs/2310.18502
- Reference count: 11
- Key outcome: LLMs generate stories with 17% worse readability scores than human-written stories; lexical simplification models improve significantly when fine-tuned on child-directed data

## Executive Summary
This study investigates the capabilities of large language models (LLMs) in generating age-appropriate children's stories and examines how lexical simplification models perform on children's text. The research reveals that while LLMs can generate coherent stories, they struggle to limit vocabulary complexity to levels suitable for young readers, resulting in stories that are significantly more complex than human-written counterparts. The study introduces a new Child-Directed Simplification (CDS) dataset and demonstrates that lexical simplification models, particularly those allowing fine-tuning, show substantial improvements when trained on child-directed data.

## Method Summary
The research employed three LLM models (InstructGPT, ChatGPT, Vicuna) to generate 250 stories each using various prompts targeting different age groups. Generated stories were evaluated against the Books for Preschoolers dataset using readability metrics including Flesch Reading Ease, Gunning-Fog Index, and Age of Acquisition scores. A CDS dataset of 315 simplification instances was created by having human annotators simplify complex words from LLM-generated stories. Two lexical simplification models (UniHD and UofM&MMU) were tested on this dataset with and without fine-tuning to assess performance differences.

## Key Results
- LLM-generated stories showed 17% worse readability scores compared to human-written stories
- Lexical simplification models achieved only 30.52% accuracy on child-directed data without fine-tuning, compared to 42.89% on general data
- Fine-tuned UofM&MMU model achieved 47.37% accuracy on child-directed data, significantly outperforming its 28.95% accuracy on general data
- Age of Acquisition analysis revealed that LLM-generated stories consistently included words with higher complexity levels than appropriate for target age groups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs generate stories with vocabulary complexity exceeding target demographic comprehension levels
- Mechanism: LLMs rely on broad internet-trained token distributions that don't constrain vocabulary by age-of-acquisition thresholds
- Core assumption: The LLM's training objective prioritizes coherence and fluency over lexical simplicity control
- Evidence anchors:
  - [abstract]: "they do not yet possess the ability to limit their vocabulary to levels appropriate for younger age groups"
  - [section 4.5]: "they are unable to avoid including some words with age of acquisition levels significantly higher than their target demographic"
  - [corpus]: Weak - corpus neighbors focus on story generation capability but don't directly address AoA control
- Break condition: LLM training includes explicit age-appropriate vocabulary filtering or domain-specific fine-tuning on children's text

### Mechanism 2
- Claim: Lexical simplification models trained on adult data fail to generalize to children's text without fine-tuning
- Mechanism: Adult-oriented simplification models use different complexity metrics and context patterns than those needed for children's text
- Core assumption: The distribution of complex words and simplification patterns differs significantly between adult and child-directed corpora
- Evidence anchors:
  - [abstract]: "the strongest-performing lexical simplification models do not perform as well on material designed for children due to their reliance on large language models"
  - [section 5.4]: "UniHD performs considerably worse, achieving an accuracy of only 30.52% in comparison to the 42.89% accuracy it achieves on the TSAR-EN dataset"
  - [corpus]: Moderate - corpus includes related work on lexical simplification but lacks direct comparison of adult vs. child text generalization
- Break condition: Model architecture inherently captures age-appropriate simplification patterns or domain adaptation techniques prove universally effective

### Mechanism 3
- Claim: Fine-tuning on child-directed data significantly improves lexical simplification performance for children's text
- Mechanism: Fine-tuning adapts model parameters to the specific complexity patterns and simplification preferences found in children's text
- Core assumption: Even limited child-directed training data contains sufficient signal to shift model behavior toward age-appropriate simplification
- Evidence anchors:
  - [abstract]: "a model that performs well on general data strongly improves its performance on children-directed data with proper finetuning"
  - [section 5.4]: "the ordinarily weaker UofM&MMU model is actually able to perform better on the child-directed dataset when finetuned, achieving an accuracy of 47.37% compared to its 28.95% on TSAR"
  - [corpus]: Weak - corpus doesn't provide evidence about fine-tuning effectiveness on domain adaptation
- Break condition: Fine-tuning data quality or quantity becomes insufficient to capture domain-specific patterns

## Foundational Learning

- Concept: Age of Acquisition (AoA) metrics
  - Why needed here: Provides empirical measure of word difficulty for different age groups, essential for evaluating story appropriateness
  - Quick check question: If a word has AoA of 8, at what average age would a child typically learn this word?

- Concept: Readability metrics (Flesch, Gunning-Fog, etc.)
  - Why needed here: Quantifies text complexity through sentence structure and word difficulty, enabling comparison between human and LLM-generated stories
  - Quick check question: Which readability metric would increase if you replaced complex words with simpler synonyms while keeping sentence structure the same?

- Concept: Lexical simplification pipeline
  - Why needed here: Describes the process of identifying complex words and replacing them with simpler alternatives, critical for post-processing LLM output
  - Quick check question: What are the three key components needed to evaluate a lexical simplification model's performance?

## Architecture Onboarding

- Component map:
  LLM story generator (InstructGPT, ChatGPT, Vicuna) -> Target word selector (AoA 6-9 words) -> Readability evaluator (AoA average, Flesch, Gunning-Fog metrics) -> Lexical simplifier (UniHD, UofM&MMU, LLM-based) -> Dataset manager (human-generated stories, annotated simplification instances)

- Critical path:
  1. Generate stories with target words using LLM
  2. Evaluate readability and complexity
  3. Apply lexical simplification to overly complex words
  4. Re-evaluate readability of simplified stories

- Design tradeoffs:
  - LLM choice vs. cost/complexity (GPT-4 vs. Vicuna)
  - Simplification model complexity vs. generalization ability
  - Dataset size vs. annotation quality for fine-tuning

- Failure signatures:
  - Stories missing target words → LLM prompt engineering issue
  - High AoA words remaining → Simplification model ineffectiveness
  - Readability metrics not improving → Model fine-tuning insufficient

- First 3 experiments:
  1. Compare readability metrics between human and LLM-generated stories with identical target words
  2. Test lexical simplification models on a small sample of LLM-generated stories to identify performance gaps
  3. Fine-tune the best-performing simplification model on the child-directed dataset and measure improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning lexical simplification models on domain-specific data significantly improve their performance on children's stories?
- Basis in paper: [explicit] The study found that the UofM&MMU model performed better on the child-directed dataset when fine-tuned, achieving an accuracy of 47.37% compared to its 28.95% on TSAR.
- Why unresolved: While the study shows promising results for fine-tuning, it only tested one model (UofM&MMU) and used a limited dataset. Further research is needed to confirm these findings across multiple models and larger datasets.
- What evidence would resolve it: Conducting experiments with a variety of lexical simplification models and larger, more diverse datasets of children's stories to test the effectiveness of fine-tuning.

### Open Question 2
- Question: How do different prompting strategies affect the readability and age-appropriateness of LLM-generated children's stories?
- Basis in paper: [explicit] The study experimented with various prompts to generate stories for different age groups but found that age-specific prompt-tuning had little effect on the simplicity of the stories.
- Why unresolved: The study only tested a limited number of prompts and did not explore more nuanced or creative prompting strategies that might yield different results.
- What evidence would resolve it: Exploring a wider range of prompts, including those that provide more context or guidance on simplicity, and analyzing their impact on the readability and age-appropriateness of generated stories.

### Open Question 3
- Question: What are the limitations of using automatic readability metrics for evaluating children's stories, and how can they be improved?
- Basis in paper: [explicit] The study acknowledges that current automatic readability metrics are limited and largely consist of outdated measures that are coarse oversimplifications of language use.
- Why unresolved: The study relies on these metrics despite their limitations, indicating a need for more accurate and nuanced evaluation methods for children's stories.
- What evidence would resolve it: Developing and validating new readability metrics specifically designed for children's text, potentially incorporating factors like age of acquisition, concreteness, and imageability.

## Limitations
- Limited dataset size (315 instances) for the Child-Directed Simplification dataset may affect generalizability of fine-tuning results
- Study focuses primarily on lexical simplification rather than structural simplification, which may be equally important for age-appropriate content
- Results based on three specific LLM models may not generalize across different architectures

## Confidence
- High confidence: LLM-generated stories have significantly worse readability scores than human-written stories (17% gap)
- Medium confidence: Lexical simplification models improve with fine-tuning on child-directed data, though dataset size limits generalizability
- Medium confidence: The relationship between Age of Acquisition metrics and story appropriateness is empirically supported but could benefit from more diverse validation

## Next Checks
1. **Dataset Scaling Test:** Replicate the lexical simplification fine-tuning experiments with an expanded CDS dataset (minimum 1000 instances) to verify whether performance gains persist at scale and to test for diminishing returns.

2. **Cross-LLM Architecture Validation:** Test the story generation and simplification pipeline across additional LLM architectures beyond the three models studied (InstructGPT, ChatGPT, Vicuna) to determine if the observed limitations are architecture-specific or universal.

3. **Structural vs. Lexical Simplification Comparison:** Design a controlled experiment comparing the impact of lexical simplification alone versus combined lexical and structural simplification approaches on children's story readability, using human evaluation to measure comprehension and engagement.