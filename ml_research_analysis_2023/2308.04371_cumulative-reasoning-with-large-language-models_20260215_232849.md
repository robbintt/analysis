---
ver: rpa2
title: Cumulative Reasoning with Large Language Models
arxiv_id: '2308.04371'
source_url: https://arxiv.org/abs/2308.04371
tags:
- reasoning
- language
- arxiv
- premises
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents Cumulative Reasoning (CR), a novel method
  that enhances large language models'' problem-solving abilities by emulating human-like
  iterative and cumulative thought processes. CR decomposes complex tasks into smaller
  components and maintains a directed acyclic graph (DAG) of verified propositions
  through three LLM roles: Proposer, Verifier(s), and Reporter.'
---

# Cumulative Reasoning with Large Language Models

## Quick Facts
- arXiv ID: 2308.04371
- Source URL: https://arxiv.org/abs/2308.04371
- Authors: 
- Reference count: 40
- Key outcome: CR achieves 98.04% accuracy on FOLIO wiki (8.47% improvement), 94% on Game of 24 (20% improvement), and 4.2% accuracy increase on MATH problems

## Executive Summary
This paper introduces Cumulative Reasoning (CR), a novel method that enhances large language models' problem-solving capabilities by emulating human-like iterative and cumulative thought processes. CR decomposes complex tasks into smaller components and maintains a directed acyclic graph (DAG) of verified propositions through three LLM roles: Proposer, Verifier(s), and Reporter. The approach demonstrates significant improvements over existing methods across multiple reasoning tasks, including formal logic inference and mathematical problem-solving.

## Method Summary
CR implements a three-role framework where a Proposer generates candidate propositions, multiple Verifiers validate these propositions using formal methods or additional LLM evaluation, and a Reporter determines when sufficient information exists to answer the question. The system maintains a DAG structure to track verified propositions and their relationships, allowing for multiple reasoning paths and reuse of verified intermediate results. The method uses different few-shot prompts for each role and has been tested with various LLMs including GPT-3.5-turbo, GPT-4, and LLaMA models.

## Key Results
- Achieves 98.04% accuracy on FOLIO wiki dataset, an 8.47% improvement over existing methods
- Reaches 94% accuracy on Game of 24 puzzle, marking a 20% improvement over previous state-of-the-art methods
- Shows a 4.2% accuracy increase and 43% relative improvement on the most challenging MATH problems
- Outperforms the Program of Thought approach by 38.8% when incorporating a code environment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CR's DAG structure enables better reuse of verified intermediate results compared to chain or tree structures.
- **Mechanism**: By maintaining a Directed Acyclic Graph of verified propositions, CR avoids redundant reasoning and allows multiple derivation paths to share common verified components.
- **Core assumption**: Intermediate reasoning steps can be verified independently and reused across different problem-solving paths.
- **Evidence anchors**: [abstract], [section 3.1], [corpus] with FMR=0.477
- **Break condition**: If verification becomes computationally expensive or if propositions cannot be effectively reused across different reasoning paths.

### Mechanism 2
- **Claim**: Separation of roles (Proposer, Verifier, Reporter) improves reasoning accuracy by preventing compounding errors.
- **Mechanism**: Different LLM instances with specialized prompts handle proposal, verification, and solution reporting separately, reducing the risk of early errors propagating through the entire reasoning chain.
- **Core assumption**: Specialized role-based prompting can produce more reliable intermediate steps than monolithic reasoning approaches.
- **Evidence anchors**: [abstract], [section 3.1], [corpus] with max neighbor author h_index=27
- **Break condition**: If the communication overhead between roles exceeds the benefits, or if role specialization becomes too rigid for certain problem types.

### Mechanism 3
- **Claim**: Cumulative reasoning enables more complex problem-solving by allowing backtracking and exploration of alternative paths.
- **Mechanism**: Unlike chain or tree approaches, CR's DAG structure allows the system to explore multiple reasoning paths while maintaining a shared context of verified propositions, enabling more flexible problem-solving.
- **Core assumption**: Complex problems often require exploring multiple reasoning paths, and maintaining a shared context improves efficiency.
- **Evidence anchors**: [abstract], [section 3.2], [section 4.4]
- **Break condition**: If the DAG becomes too large to manage efficiently, or if the benefits of multiple paths don't outweigh the computational costs.

## Foundational Learning

- **Concept**: Directed Acyclic Graphs (DAGs) and their properties
  - Why needed here: CR's core innovation relies on maintaining a DAG of verified propositions, so understanding DAG properties is crucial for implementing and debugging the system
  - Quick check question: What property of DAGs ensures that there are no circular dependencies in the reasoning process?

- **Concept**: Formal logic systems (propositional and first-order logic)
  - Why needed here: The paper mentions FOLIO and AutoTNLI datasets which use formal logic, and the system needs to verify logical propositions
  - Quick check question: How does first-order logic extend propositional logic with quantification?

- **Concept**: Prompt engineering and role-based LLM prompting
  - Why needed here: CR uses three distinct LLM roles with different prompts, so understanding how to design effective role-specific prompts is crucial
  - Quick check question: What are the key differences between prompts for a Proposer versus a Verifier role?

## Architecture Onboarding

- **Component map**: Proposer → Verifier(s) → DAG Manager → Context Manager → Reporter
- **Critical path**: Proposer → Verifier(s) → DAG update → Reporter check → (repeat if needed) → Answer generation
- **Design tradeoffs**: 
  - Single vs. multiple verifiers (majority voting vs. single verification)
  - Temperature settings for exploration vs. exploitation
  - DAG size limits vs. completeness of reasoning
- **Failure signatures**:
  - Verifier rejection rate too high → Proposer may need different prompting
  - DAG grows too large without progress → Verification or reporting logic may be flawed
  - Reporter stops too early → Context evaluation criteria may be too strict
- **First 3 experiments**:
  1. Implement CR with a single LLM handling all roles to establish baseline performance
  2. Add verification step with majority voting (k=3) to test the impact of verification
  3. Implement DAG structure and test with multi-path reasoning problems to validate the core innovation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of verifiers needed for different problem types?
- Basis in paper: [explicit] The paper mentions using "one or more verifiers" but does not explore how the number of verifiers affects performance across different problem types.
- Why unresolved: The paper uses a fixed number of verifiers in experiments without investigating how varying this number impacts accuracy or efficiency.
- What evidence would resolve it: Systematic experiments varying the number of verifiers (n=1,2,3,4+) across different problem types while measuring accuracy, computational cost, and performance trade-offs.

### Open Question 2
- Question: How does Cumulative Reasoning compare to other reasoning frameworks on complex mathematical problems beyond Game of 24?
- Basis in paper: [explicit] The paper demonstrates CR's effectiveness on Game of 24 and MATH problems but does not extensively compare it to other frameworks on a broader range of mathematical problems.
- Why unresolved: While CR shows improvements on specific mathematical tasks, its relative performance compared to other reasoning frameworks across diverse mathematical domains remains unexplored.
- What evidence would resolve it: Comparative studies of CR against other reasoning frameworks (like Tree of Thoughts, Chain-of-Thought) on a wide variety of mathematical problem types including algebra, geometry, calculus, and proof-based problems.

### Open Question 3
- Question: How does the performance of CR scale with problem complexity and the number of premises?
- Basis in paper: [inferred] The paper shows CR works well on problems with 2-4 premises in FOLIO and Game of 24, but does not systematically explore how performance changes as problem complexity increases.
- Why unresolved: The paper does not investigate the limits of CR's effectiveness as problem complexity grows or how it handles problems with many interconnected premises.
- What evidence would resolve it: Experiments testing CR on increasingly complex problems with larger numbers of premises and more intricate logical dependencies, measuring accuracy degradation points and computational resource requirements.

### Open Question 4
- Question: What are the limitations of using general LLMs as proposer, verifier, and reporter compared to specialized models?
- Basis in paper: [explicit] The paper states "Ideally, the proposer should be implemented using a language model pretrained on the corresponding derivation tasks. Verifier(s) should be capable of translating the derivations to appropriate formal systems" but uses general LLMs with different prompts instead.
- Why unresolved: The paper acknowledges that specialized models would be ideal but does not explore the actual performance gap between general LLMs and specialized models for each role.
- What evidence would resolve it: Direct comparison of CR using general LLMs versus CR using specialized models for each role (proposer, verifier, reporter) on the same problem sets, measuring accuracy differences and implementation complexity trade-offs.

## Limitations

- The exact prompt templates for each role are not fully disclosed, making exact replication difficult
- The paper doesn't specify how the DAG handles proposition redundancy or cycles that might emerge in complex reasoning chains
- Computational overhead of maintaining multiple verifiers and the DAG structure is not quantified

## Confidence

- **High confidence**: The core DAG-based architecture and role separation mechanism are clearly described and theoretically sound
- **Medium confidence**: The reported accuracy improvements, as they depend on specific LLM implementations and prompt templates not fully disclosed
- **Low confidence**: The scalability claims for very complex reasoning tasks, as the paper doesn't extensively test edge cases

## Next Checks

1. **Prompt Template Verification**: Request and test the exact prompt templates used for Proposer, Verifier, and Reporter roles to establish baseline reproducibility
2. **DAG Scalability Test**: Implement CR with progressively more complex reasoning tasks to measure how DAG size and verification overhead scale
3. **Ablation Study**: Test CR variants with different numbers of verifiers (k=1, k=3, k=5) to quantify the impact of majority voting on accuracy and computational cost