---
ver: rpa2
title: 'Conversations in Galician: a Large Language Model for an Underrepresented
  Language'
arxiv_id: '2311.03812'
source_url: https://arxiv.org/abs/2311.03812
tags:
- language
- galician
- large
- alpaca
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the underrepresentation of low-resource languages
  like Galician in large language models by creating Cabuxa-7B, a Galician instruction-tuned
  version of LLaMA-7B using LoRA. The model was fine-tuned on a translated version
  of the Alpaca dataset (52,000 instruction-response pairs), with training conducted
  for 20 epochs using a batch size of 64, gradient accumulation of 32 steps, and a
  learning rate of 3e-4, resulting in decreasing loss values from 2.610 to 0.990 over
  the training period.
---

# Conversations in Galician: a Large Language Model for an Underrepresented Language

## Quick Facts
- arXiv ID: 2311.03812
- Source URL: https://arxiv.org/abs/2311.03812
- Reference count: 8
- Created Cabuxa-7B, a Galician instruction-tuned version of LLaMA-7B using LoRA fine-tuning on a translated Alpaca dataset

## Executive Summary
This paper addresses the underrepresentation of Galician in large language models by creating Cabuxa-7B, a fine-tuned version of LLaMA-7B that can understand and respond in Galician. The authors translated the Alpaca dataset to Galician (52,000 instruction-response pairs) and used LoRA (Low-Rank Adaptation) to efficiently adapt the base model to the new language. The work includes releasing the translated dataset and training code, with loss values decreasing from 2.610 to 0.990 over 20 epochs of training. The research also explores how knowledge of the closely related Portuguese language can assist in generating coherent Galician text when training resources are scarce.

## Method Summary
The authors created Cabuxa-7B by fine-tuning LLaMA-7B using LoRA on a translated version of the Alpaca dataset containing 52,000 instruction-response pairs in Galician. The training used a batch size of 64, gradient accumulation of 32 steps, and a learning rate of 3e-4 over 20 epochs. The translated dataset was created using the googletranslatepy Python package. The model architecture uses LoRA adapters that introduce trainable rank decomposition matrices into each Transformer layer while freezing the original LLaMA-7B weights, enabling efficient adaptation to Galician with minimal trainable parameters.

## Key Results
- Successfully fine-tuned LLaMA-7B to understand and respond in Galician using LoRA adaptation
- Translated Alpaca dataset of 52,000 instruction-response pairs created and released
- Training loss decreased from 2.610 to 0.990 over 20 epochs, indicating successful adaptation
- Demonstrated parameter-efficient fine-tuning approach for low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
LoRA fine-tuning enables effective instruction-following adaptation for Galician using minimal trainable parameters. LoRA introduces low-rank decomposition matrices into each Transformer layer while freezing the original LLaMA-7B weights, allowing efficient adaptation to Galician instructions with dramatically reduced computational cost. Core assumption: The original LLaMA-7B knowledge transfers effectively to Galician through parameter-efficient fine-tuning.

### Mechanism 2
Translation of the Alpaca dataset to Galician provides sufficient instruction-response pairs for effective fine-tuning. The 52,000 translated instruction-response pairs from Alpaca provide the necessary training examples for the model to learn the instruction-following pattern in Galician. Core assumption: Translation preserves the semantic structure and task complexity of the original Alpaca dataset.

### Mechanism 3
Knowledge transfer from Portuguese to Galician improves text generation when training resources are scarce. The linguistic similarity between Portuguese and Galician allows the model to leverage Portuguese knowledge during fine-tuning to generate more coherent Galician text. Core assumption: The structural and lexical similarities between Portuguese and Galician are sufficient for knowledge transfer.

## Foundational Learning

**Concept: Parameter-efficient fine-tuning (LoRA)**
- Why needed here: Enables adaptation of large models to low-resource languages without full fine-tuning
- Quick check question: How does LoRA reduce the number of trainable parameters compared to full fine-tuning?

**Concept: Instruction-following format**
- Why needed here: Provides a standardized format for training the model to follow human instructions
- Quick check question: What is the structure of an Alpaca-style instruction-response pair?

**Concept: Low-resource language challenges**
- Why needed here: Understanding the specific difficulties in developing NLP tools for underrepresented languages
- Quick check question: What makes Galician a "low-resource" language in the context of NLP?

## Architecture Onboarding

**Component map:** LLaMA-7B base model → LoRA adapters (rank decomposition matrices) → Galician instruction dataset → Training loop with gradient accumulation

**Critical path:** Data preparation (translation) → LoRA adapter initialization → Fine-tuning loop → Evaluation

**Design tradeoffs:** Translation quality vs. dataset size, LoRA rank selection vs. model performance, computational efficiency vs. adaptation quality

**Failure signatures:** Poor translation quality leading to nonsensical outputs, insufficient LoRA rank causing underfitting, gradient accumulation causing training instability

**First 3 experiments:**
1. Test the model with simple instructions to verify basic functionality
2. Compare outputs with different LoRA rank configurations
3. Evaluate translation quality impact by testing with manually verified vs. automatically translated examples

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: How does Cabuxa-7B perform on downstream NLP tasks compared to multilingual models like mBERT or XLM-R specifically for Galician?
- Basis in paper: The paper focuses on creating a Galician-specific model but does not provide comparative evaluations against existing multilingual models on benchmark tasks.
- Why unresolved: The authors explicitly state evaluation with expert linguists is planned for future work, suggesting current performance metrics are not established.
- What evidence would resolve it: Benchmarking results on standard Galician NLP tasks (POS tagging, NER, sentiment analysis) comparing Cabuxa-7B against mBERT and XLM-R using established evaluation metrics.

**Open Question 2**
- Question: What is the optimal training duration and dataset size for instruction-tuning LLaMA-7B on Galician given the diminishing returns shown in the loss curve?
- Basis in paper: The training loss table shows continued decrease from 2.610 to 0.990 over 20 epochs, suggesting potential for further optimization.
- Why unresolved: The authors did not perform a comprehensive ablation study on training duration, dataset size, or learning rate scheduling to identify the optimal configuration.
- What evidence would resolve it: Systematic experiments varying epoch count, dataset sizes (80%, 60%, 40% of translated Alpaca), and learning rate schedules with corresponding performance metrics on validation sets.

**Open Question 3**
- Question: How does Portuguese knowledge actually contribute to generating coherent Galician text during fine-tuning, and can this transfer be quantified?
- Basis in paper: The authors mention "exploration of how knowledge of a closely related language, in this case, Portuguese, can assist in generating coherent text" but do not provide quantitative analysis.
- Why unresolved: The paper does not include controlled experiments comparing models trained with/without Portuguese knowledge transfer or linguistic analysis of the generated text quality.
- What evidence would resolve it: Comparative analysis of text generation quality metrics between models trained on Galician-only data versus models leveraging Portuguese knowledge, including human evaluation of grammatical correctness and semantic coherence.

## Limitations

- Translation quality of the Alpaca dataset to Galician was not evaluated, leaving uncertainty about whether the model is learning genuine instruction-following capabilities or translation artifacts
- The claim about Portuguese-Galician knowledge transfer remains theoretical rather than empirically demonstrated
- No independent evaluation by expert linguists was conducted to validate the model's performance on real-world Galician language tasks

## Confidence

**High Confidence Claims:**
- The technical implementation of LoRA fine-tuning on LLaMA-7B is sound and follows established methodology
- The training procedure (20 epochs, batch size 64, learning rate 3e-4) is clearly specified and reproducible
- The general framework for addressing low-resource language representation through instruction-tuning is valid

**Medium Confidence Claims:**
- The translated dataset contains 52,000 instruction-response pairs in proper Alpaca format
- Loss values decreasing from 2.610 to 0.990 indicates successful fine-tuning
- The model can generate responses in Galician following the instruction format

**Low Confidence Claims:**
- The quality and representativeness of the translated Galician Alpaca dataset
- The effectiveness of knowledge transfer from Portuguese to Galician
- The model's performance on real-world Galician language tasks beyond the translated dataset

## Next Checks

1. **Dataset Quality Validation**: Conduct a human evaluation of 100 randomly selected instruction-response pairs from the translated dataset, comparing them against the original English Alpaca pairs to assess translation fidelity and task preservation.

2. **Cross-Lingual Transfer Experiment**: Train an identical model architecture on a Portuguese version of the Alpaca dataset and compare generation quality on Galician test prompts versus the Galician-trained model.

3. **Expert Linguistic Evaluation**: Engage 3-5 native Galician speakers with linguistic expertise to evaluate model outputs across different prompt types (creative writing, factual questions, task instructions) using standardized rubrics for fluency, grammaticality, and instruction-following accuracy.