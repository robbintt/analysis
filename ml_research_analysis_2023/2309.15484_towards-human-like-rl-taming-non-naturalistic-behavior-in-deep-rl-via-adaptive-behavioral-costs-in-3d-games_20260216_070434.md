---
ver: rpa2
title: 'Towards Human-Like RL: Taming Non-Naturalistic Behavior in Deep RL via Adaptive
  Behavioral Costs in 3D Games'
arxiv_id: '2309.15484'
source_url: https://arxiv.org/abs/2309.15484
tags:
- costs
- learning
- shaking
- policy
- human-like
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of training deep reinforcement
  learning agents that exhibit human-like behavior, specifically reducing non-naturalistic
  actions like shaking and spinning in 3D games, while maintaining competitive performance.
  The proposed method, Adaptive Behavioral Costs in Reinforcement Learning (ABC-RL),
  augments reinforcement learning with dynamically adjusted weights for behavioral
  costs.
---

# Towards Human-Like RL: Taming Non-Naturalistic Behavior in Deep RL via Adaptive Behavioral Costs in 3D Games

## Quick Facts
- arXiv ID: 2309.15484
- Source URL: https://arxiv.org/abs/2309.15484
- Reference count: 4
- Primary result: Achieves performance comparable to unconstrained agents while significantly reducing shaking and spinning behaviors in 3D games

## Executive Summary
This paper addresses the problem of training deep reinforcement learning agents that exhibit human-like behavior by reducing non-naturalistic actions like shaking and spinning in 3D games. The proposed method, Adaptive Behavioral Costs in Reinforcement Learning (ABC-RL), augments reinforcement learning with dynamically adjusted weights for behavioral costs using an augmented Lagrangian approach. Experiments on DMLab-30 and Unity ML-Agents Toolkit demonstrate that ABC-RL achieves competitive performance while significantly reducing undesirable shaking and spinning behaviors.

## Method Summary
ABC-RL modifies the standard RL objective by introducing behavioral costs (shaking and spinning) that are penalized with dynamically adjusted weights. The method minimizes these behavioral costs subject to a constraint on the value function, using a sigmoid-based approximation of the augmented Lagrangian penalty weight. This formulation allows agents some freedom to take non-human-like actions when necessary for performance, while discouraging excessive shaking and spinning. The approach was tested with both PPO (for Unity ML-Agents) and V-trace (for DMLab-30) algorithms.

## Key Results
- In Banana Collector, ABC-RL reduced shaking and spinning costs to near zero while maintaining similar cumulative rewards compared to unconstrained agents
- Across DMLab-30 games, ABC-RL consistently achieved near-zero shaking and spinning costs while preserving competitive performance
- The method successfully balanced the trade-off between human-like behavior and task performance across multiple 3D game environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ABC-RL reduces non-human-like behaviors while maintaining competitive performance through dynamic weight adjustment
- Mechanism: Uses sigmoid function to calculate penalty weight based on performance threshold; when performance is below threshold, weight approaches zero to prioritize learning, then increases to penalize non-human-like behaviors
- Core assumption: Sigmoid approximation adequately represents theoretically derived penalty weight for smooth transition between learning phases
- Evidence anchors: Abstract mentions dynamically adjusted weights; section discusses weight calculation; corpus neighbors focus on different domains
- Break condition: Unstable weight adjustments occur when value function fluctuates dramatically around threshold, causing oscillating behavior

### Mechanism 2
- Claim: Constrained formulation allows necessary non-human-like actions while discouraging excessive ones
- Mechanism: Minimizes behavioral costs subject to value constraint (vs. traditional CPO's reward maximization with constraints), giving agents flexibility to violate constraints temporarily when needed
- Core assumption: Agents can learn to distinguish between necessary and unnecessary non-human-like behaviors through this constraint structure
- Evidence anchors: Abstract mentions minimizing costs subject to value constraint; section discusses learning competitive policy; corpus lacks direct evidence
- Break condition: If value threshold is too low, agents never achieve performance needed to activate meaningful behavioral cost penalties

### Mechanism 3
- Claim: Augmented Lagrangian approach handles performance-human-likeness trade-off more effectively than pure penalty methods
- Mechanism: Maintains and updates Lagrange multiplier that increases when constraints are violated, creating self-adjusting penalty system that becomes more stringent as agent approaches performance threshold
- Core assumption: Dual update rule provides appropriate feedback to guide behavior toward human-like patterns
- Evidence anchors: Abstract mentions augmented Lagrangian approximation; section discusses Lagrange multiplier updates; corpus lacks direct evidence
- Break condition: If penalty parameter is too large, Lagrange multiplier increases too rapidly, causing premature convergence to suboptimal policies that avoid necessary exploration

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: RL problem framework with states, actions, rewards, and transition probabilities; ABC-RL modifies reward structure with behavioral costs
  - Quick check question: What are the five components of an MDP and how does ABC-RL modify the reward component?

- Concept: Constrained Policy Optimization (CPO)
  - Why needed here: ABC-RL is presented as alternative to traditional CPO approaches; understanding CPO helps grasp why ABC-RL formulation is novel
  - Quick check question: How does traditional CPO differ from ABC-RL's approach of minimizing costs subject to value constraints?

- Concept: Augmented Lagrangian methods
  - Why needed here: Paper uses augmented Lagrangian optimization to handle constrained optimization problem, more sophisticated than simple penalty methods
  - Quick check question: What is the key advantage of augmented Lagrangian methods over pure penalty methods in constrained optimization?

## Architecture Onboarding

- Component map: Policy network (parameterized by θ) → Behavioral cost calculator (C(t) = Csh(t) + αCsp(t)) → Weight calculator (Λ = W · Sigmoid((Vavg − Vth)/h)) → Adjusted reward r′t = rt − Λ · C(t) → RL algorithm (PPO or V-trace) → Updated policy
- Critical path: The flow from observed state → action selection → environment feedback → behavioral cost calculation → weight adjustment → reward modification → policy update is the core loop that must function correctly for ABC-RL to work
- Design tradeoffs: Sigmoid approximation vs. exact Lagrangian penalty provides smoother training but may sacrifice precision; PPO vs. V-trace affects constraint handling capability
- Failure signatures: Persistent high shaking/spinning costs indicate weight calculation issues; performance degradation suggests weights are too high; fluctuating performance with oscillations suggests threshold or sigmoid parameters are misconfigured
- First 3 experiments:
  1. Implement ABC-RL on Banana Collector with baseline PPO to verify shaking/spinning reduction while maintaining rewards
  2. Test different threshold values (Vth) to find optimal balance between performance and behavioral costs
  3. Compare sigmoid surrogate vs. exact Lagrangian penalty implementation to validate approximation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can behavioral costs be defined to prevent the issue where agents avoid necessary actions like shaking and spinning for performance in certain games?
- Basis in paper: The paper mentions this as an interesting observation in the watermaze game where ABC-RL agents struggled to find hidden platforms due to avoiding shaking and spinning actions, which are critical for receiving rewards in this game
- Why unresolved: The paper acknowledges this as a limitation but doesn't provide a solution, stating "Whether there is a way of defining the cost to prevent this case is not in the scope of this paper"
- What evidence would resolve it: Experimental results showing a new behavioral cost definition that allows necessary actions while still discouraging excessive non-human-like behavior, or theoretical analysis proving why this trade-off is unavoidable

### Open Question 2
- Question: Can the sigmoid surrogate function be replaced with other smooth functions to potentially improve the transition between low-penalty and high-penalty regimes?
- Basis in paper: The paper discusses using a sigmoid function as a surrogate for the penalty weight but mentions it's one possible approach without exploring alternatives
- Why unresolved: The paper presents the sigmoid function as effective but doesn't compare it to other potential surrogate functions or provide theoretical justification for why sigmoid is optimal
- What evidence would resolve it: Comparative experiments testing different smooth functions (e.g., tanh, softplus) as penalty weight surrogates across various environments, or mathematical analysis showing properties of different functions in this context

### Open Question 3
- Question: How does the choice of window size w for shaking cost calculation affect the human-likeness of agent behavior across different types of 3D games?
- Basis in paper: The paper uses a fixed window size of 8 for shaking cost calculation but doesn't explore how different window sizes might affect the results or whether different games might require different window sizes
- Why unresolved: The paper uses a single window size without justification for why this particular size is appropriate across different game types and scenarios
- What evidence would resolve it: Systematic experiments varying the window size parameter across different game types and analyzing how it affects both performance and human-likeness metrics, or theoretical analysis of optimal window sizing for different game dynamics

## Limitations

- The approach focuses specifically on shaking and spinning behaviors, with uncertain generalizability to other types of non-naturalistic behaviors
- The choice of performance threshold (Vth) appears critical but may require task-specific tuning, raising questions about scalability to new environments
- Computational overhead introduced by the adaptive cost mechanism and its impact on training efficiency is not fully characterized

## Confidence

- **High Confidence**: The core mechanism of using adaptive weights for behavioral costs is technically sound and the experimental results showing reduced shaking/spinning are well-supported by the data
- **Medium Confidence**: The claim that ABC-RL maintains competitive performance while reducing non-naturalistic behaviors is supported but could benefit from more extensive testing across diverse environments and tasks
- **Low Confidence**: The assertion that this approach generalizes to all types of human-like behavior modeling requires additional validation beyond the specific behaviors tested

## Next Checks

1. Test ABC-RL on environments with different types of non-naturalistic behaviors (e.g., excessive backtracking, unnatural object interactions) to assess generalizability
2. Conduct ablation studies to quantify the impact of each component (sigmoid approximation, augmented Lagrangian) on performance and behavior quality
3. Measure and compare training time and sample efficiency between ABC-RL and baseline methods to evaluate practical deployment considerations