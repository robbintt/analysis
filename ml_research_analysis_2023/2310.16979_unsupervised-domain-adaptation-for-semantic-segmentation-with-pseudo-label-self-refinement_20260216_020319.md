---
ver: rpa2
title: Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label
  Self-Refinement
arxiv_id: '2310.16979'
source_url: https://arxiv.org/abs/2310.16979
tags:
- domain
- target
- segmentation
- semantic
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of domain shift in semantic segmentation,
  where models trained on one dataset (source) perform poorly on a different dataset
  (target). The core idea is to use a pseudo-label refinement network (PRN) that improves
  the quality of pseudo-labels generated by a teacher model and identifies noisy labels.
---

# Unsupervised Domain Adaptation for Semantic Segmentation with Pseudo Label Self-Refinement

## Quick Facts
- arXiv ID: 2310.16979
- Source URL: https://arxiv.org/abs/2310.16979
- Reference count: 40
- Key outcome: Achieves 71.3% mIoU on GTA→Cityscapes benchmark, outperforming previous best of 68.2%

## Executive Summary
This paper addresses the challenge of domain shift in semantic segmentation by proposing a pseudo-label refinement network (PRN) that improves the quality of pseudo-labels generated by a teacher model and identifies noisy labels. The PRN is trained using a novel FFT-based perturbation strategy to handle the domain gap, enabling the model to learn style-invariant semantic representations. The approach is evaluated on three benchmark datasets with different domain shifts and consistently outperforms state-of-the-art methods, achieving significant improvements in mean IoU (mIoU) across all datasets.

## Method Summary
The method trains a student model using self-training with pseudo-labels from a teacher model, enhanced by a PRN that refines noisy labels and predicts a noise mask. The PRN is trained using FFT-based perturbations on both source and target segmentation logits, where low-frequency components of the source logits' amplitude are replaced by target-style amplitudes. The student model is trained on source data with ground truth labels and target data with PRN-refined pseudo-labels (excluding noisy pixels), while the teacher model is updated via exponential moving average (EMA). Contrastive learning with PRN output improves feature discrimination across domains.

## Key Results
- Achieves 71.3% mIoU on GTA→Cityscapes benchmark, outperforming previous best of 68.2%
- Consistently outperforms state-of-the-art methods across three benchmark datasets
- Demonstrates significant improvements in mean IoU (mIoU) across all datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The PRN improves pseudo-label quality by learning to refine logits with FFT-based perturbations.
- Mechanism: FFT perturbation replaces low-frequency components of source logits with target-style amplitudes, forcing the PRN to learn style-invariant semantic representations that generalize to noisy target pseudo-labels.
- Core assumption: The phase component of logits preserves semantic structure while amplitude encodes domain style; preserving phase ensures semantic content remains intact during perturbation.
- Evidence anchors:
  - [abstract] "We perturb the amplitude of source image segmentation logits using the amplitude of a random target image to effectively introduce noise while preserving object structure."
  - [section 3.3] "By refining source segmentation logits perturbed in this process, we hypothesize our PRN model will learn to refine some characteristics of target pseudo-label noise."
- Break condition: If the semantic content encoded in the phase is not invariant across domains, the perturbation will destroy rather than preserve structure.

### Mechanism 2
- Claim: Noise mask prediction allows the model to avoid propagating errors by selectively filtering unreliable pseudo-labels.
- Mechanism: The PRN predicts a binary mask identifying pixels likely to have incorrect pseudo-labels; these pixels are excluded from the loss calculation, preventing confirmation bias during training.
- Core assumption: The noise mask can be predicted accurately enough to reduce the impact of noisy labels on the student model.
- Evidence anchors:
  - [abstract] "Our proposed pseudo-label refinement network (PRN) is trained to serve two main purposes: it refines noisy pseudo labels, improving their quality, and localizes potential errors in pseudo labels by predicting a binary mask for challenging pixels."
  - [section 3.3] "Based on the predicted target noise mask μi T, LT (i) ce calculation can avoid the difficult pixels for which the pseudo-label is predicted to be noisy."
- Break condition: If the noise mask prediction is too noisy itself, the model may exclude correct labels or include incorrect ones, negating the benefit.

### Mechanism 3
- Claim: Contrastive learning with PRN output improves feature discrimination across domains.
- Mechanism: Positive and negative pixel pairs are defined using PRN-refined labels, ensuring that contrastive loss is based on more reliable pseudo-labels, which leads to better feature alignment between source and target domains.
- Core assumption: PRN-refined labels are more reliable than raw teacher predictions, enabling more accurate positive/negative pair construction.
- Evidence anchors:
  - [section 3.4] "We utilize GT labels for source samples and refined labels from the PRN network for target samples to find the positive and negative samples."
  - [section 3.5] "Pixel-pixel contrastive loss has been shown in prior works to improve the training of semantic segmentation models."
- Break condition: If PRN refinement is not sufficiently accurate, the contrastive loss may reinforce incorrect feature associations.

## Foundational Learning

- Concept: Fast Fourier Transform (FFT) and its decomposition into amplitude and phase.
  - Why needed here: FFT perturbation strategy relies on manipulating amplitude (style) while preserving phase (semantics) to generate realistic perturbations.
  - Quick check question: What does the amplitude component of an FFT represent, and why is it replaced during perturbation?

- Concept: Cross-entropy loss and its role in supervised learning.
  - Why needed here: The model is trained using cross-entropy loss on both source (ground truth) and target (pseudo-labels) data to adapt to the target domain.
  - Quick check question: How does the cross-entropy loss function handle class imbalance, and what modifications are used in this paper?

- Concept: Teacher-student framework and exponential moving average (EMA).
  - Why needed here: The teacher model generates pseudo-labels for the target domain, and EMA stabilizes the teacher by averaging student weights.
  - Quick check question: What is the purpose of EMA in the teacher-student framework, and how does it prevent confirmation bias?

## Architecture Onboarding

- Component map:
  - Student model (SegFormer) -> Encoder (Eθ) + Decoder (Dθ) for segmentation
  -> Teacher model (EMA-updated student)
  -> PRN (Dσ) -> Takes teacher encoder features + teacher decoder logits, outputs refined logits and noise mask
  -> Augmentation pipeline (Color jitter, Gaussian blur, ClassMix)
  -> FFT perturbation module -> Generates perturbed logits for PRN training

- Critical path:
  1. Student model is trained on source data with GT labels
  2. Teacher model generates pseudo-labels for target data
  3. PRN refines pseudo-labels and predicts noise masks
  4. Student model is trained on source data + refined target data (excluding noisy pixels)
  5. Teacher model is updated via EMA

- Design tradeoffs:
  - Using FFT perturbation adds complexity but enables PRN training without target GT labels
  - Noise mask prediction adds a second output head to PRN, increasing model size
  - Contrastive learning with PRN output improves alignment but requires careful pair selection

- Failure signatures:
  - If PRN noise mask is inaccurate, the model may exclude correct labels or include incorrect ones
  - If FFT perturbation is too aggressive, semantic content may be destroyed, leading to poor refinement
  - If EMA update rate is too high, the teacher may not stabilize, leading to noisy pseudo-labels

- First 3 experiments:
  1. Verify FFT perturbation preserves semantic structure by visualizing perturbed logits
  2. Test PRN noise mask accuracy by comparing predicted masks to actual label differences
  3. Measure the impact of contrastive learning by training with and without PRN-refined labels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal ratio of Fourier perturbation amplitude between source and target domains for maximizing pseudo-label refinement performance?
- Basis in paper: [explicit] The paper describes using Fast Fourier Transform (FFT) based perturbations where low-level frequencies of the amplitude of source segmentation logits are replaced by that of the target domain image. The perturbation strength is controlled by a hyperparameter ϵ, which is set to a random value between 0.05 and 0.2 in experiments.
- Why unresolved: The paper uses a fixed range for ϵ (0.05 to 0.2) but does not explore the full range of possible values or perform a sensitivity analysis to determine if this range is optimal.
- What evidence would resolve it: A comprehensive ablation study varying ϵ across a wider range (e.g., 0.01 to 0.5) and analyzing the impact on pseudo-label refinement quality and final segmentation performance would identify the optimal perturbation ratio.

### Open Question 2
- Question: How does the performance of the proposed method scale with the size of the target domain dataset?
- Basis in paper: [inferred] The paper evaluates the method on datasets with varying numbers of target images (e.g., 8,779 images for Dark Zurich, 24,966 for GTA5) but does not explicitly study how performance changes with dataset size.
- Why unresolved: The relationship between target domain dataset size and adaptation performance is not investigated, leaving uncertainty about the method's effectiveness in scenarios with limited target data.
- What evidence would resolve it: Conducting experiments with different fractions of the target dataset (e.g., 10%, 25%, 50%, 75%, 100%) and measuring the resulting mIoU would reveal how performance scales with dataset size.

### Open Question 3
- Question: Can the proposed pseudo-label refinement network (PRN) be effectively applied to other domain adaptation tasks beyond semantic segmentation, such as object detection or instance segmentation?
- Basis in paper: [explicit] The paper focuses on unsupervised domain adaptation for semantic segmentation and proposes a PRN specifically designed for this task, taking segmentation logits and image features as input and outputting refined logits and noise masks.
- Why unresolved: The paper does not explore the generalizability of the PRN architecture to other computer vision tasks that also suffer from domain shift and noisy pseudo-labels.
- What evidence would resolve it: Applying the PRN architecture to other domain adaptation tasks (e.g., adapting object detectors from synthetic to real images) and evaluating its impact on performance would demonstrate its broader applicability.

## Limitations

- The exact implementation details of FFT-based perturbation are not fully specified, which could affect reproducibility.
- The noise mask prediction adds another layer of potential error, as inaccurate masks could lead to excluding correct labels or including incorrect ones.
- The contrastive learning component depends heavily on the accuracy of PRN-refined labels, which may not always be reliable.

## Confidence

- High Confidence: The paper's overall approach and its effectiveness in improving mIoU on benchmark datasets are well-supported by experimental results.
- Medium Confidence: The mechanism of FFT perturbation and its role in preserving semantic structure while introducing domain-specific noise is plausible but requires further validation.
- Medium Confidence: The noise mask prediction and its ability to filter out unreliable pseudo-labels are supported by the paper's claims but may be sensitive to hyperparameter choices.

## Next Checks

1. **FFT Perturbation Validation**: Implement and test the FFT-based perturbation strategy to verify that it preserves semantic structure while introducing realistic domain-specific noise. Visualize the perturbed logits to ensure semantic content is intact.

2. **Noise Mask Accuracy**: Evaluate the accuracy of the PRN's noise mask predictions by comparing them to actual label differences on a held-out validation set. This will help determine if the noise mask is effectively identifying unreliable pseudo-labels.

3. **Contrastive Learning Impact**: Conduct ablation studies to measure the impact of contrastive learning with PRN-refined labels. Train models with and without PRN-refined labels in the contrastive loss to assess the improvement in feature alignment across domains.