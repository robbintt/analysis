---
ver: rpa2
title: Proximity to Losslessly Compressible Parameters
arxiv_id: '2306.02834'
source_url: https://arxiv.org/abs/2306.02834
tags:
- points
- rank
- units
- problem
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies lossless network compressibility in single-hidden-layer
  hyperbolic tangent networks, defining the rank of a parameter as the minimum number
  of hidden units needed to implement the same function. The author provides efficient
  algorithms for optimal lossless compression and rank computation (Algorithm 4.1
  and 4.2), showing these problems are solvable in O(h log h) time.
---

# Proximity to Losslessly Compressible Parameters

## Quick Facts
- arXiv ID: 2306.02834
- Source URL: https://arxiv.org/abs/2306.02834
- Reference count: 40
- One-line primary result: Optimal lossless compression and rank computation for single-hidden-layer tanh networks are solvable in O(h log h) time, while bounding proximate rank is NP-complete

## Executive Summary
This paper establishes fundamental computational complexity results for lossless network compressibility in single-hidden-layer hyperbolic tangent networks. The author defines the rank of a parameter as the minimum number of hidden units needed to implement the same function, and proves that both optimal lossless compression and rank computation are solvable in O(h log h) time. However, the paper shows that detecting parameters with low rank within a small L∞ neighborhood is NP-complete, via a reduction from Boolean satisfiability through a geometric clustering problem. The work provides efficient algorithms for compression and rank computation while highlighting the computational intractability of measuring proximity to highly-compressible parameters.

## Method Summary
The paper develops theoretical algorithms for analyzing lossless compressibility in single-hidden-layer tanh networks. Algorithm 4.1 (COMPRESS) eliminates redundant units by exploiting reducibility conditions, while Algorithm 4.2 (RANK) computes the minimum number of hidden units needed. For proximate rank, Algorithm 5.1 provides a greedy upper bound through approximate partitioning. The NP-completeness proof (Theorem 6.2) establishes hardness via reduction from Boolean satisfiability to a geometric point covering problem. The theoretical framework provides both efficient algorithms for lossless compression and complexity-theoretic limits on detecting compressible parameters.

## Key Results
- Optimal lossless compression and rank computation solvable in O(h log h) time
- Greedy algorithm provides upper bounds on proximate rank
- Bounding proximate rank is NP-complete via reduction from Boolean satisfiability
- Reducibility conditions characterize all opportunities for lossless compression in single-hidden-layer tanh networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal lossless compression and rank computation for single-hidden-layer tanh networks are solvable in O(h log h) time.
- Mechanism: The algorithm exploits reducibility conditions that identify units that can be eliminated or merged while preserving the function. By partitioning units based on sign(bi)·(bi,ci) and eliminating redundant units, the algorithm constructs a minimal equivalent network.
- Core assumption: The reducibility conditions (Sussmann, 1992) correctly characterize all opportunities for lossless compression in this architecture.
- Evidence anchors:
  - [abstract]: "We give efficient formal algorithms for optimal lossless compression and computing the rank of a parameters—the minimum number of hidden units required to implement the same function."
  - [section]: "Define the rank of a neural network parameter w ∈ W h, denoted rank(w), as the minimum number of hidden units required to implement fw"
  - [corpus]: Found 25 related papers with average FMR=0.404, indicating relevant literature on neural network compressibility exists but is not yet highly cited.

### Mechanism 2
- Claim: Bounding proximate rank is NP-complete, making detection of highly-compressible nearby parameters computationally intractable.
- Mechanism: The reduction from Boolean satisfiability via uniform point cover shows that clustering units with similar incoming weights and biases for merging is fundamentally a hard geometric problem. This complexity arises because optimal grouping requires solving an NP-complete problem.
- Core assumption: The geometric problem of covering points with small squares is NP-complete, which the paper proves via reduction from restricted Boolean satisfiability.
- Evidence anchors:
  - [abstract]: "We show that bounding the proximate rank below a given value... is an NP-complete decision problem."
  - [section]: "we show that optimally bounding the proximate rank, or, equivalently, detecting proximity to low-rank parameters, is NP-complete, by reduction from Boolean satisfiability via a novel hard clustering problem."
  - [corpus]: Corpus contains papers on neural network symmetries and identifiability, supporting the relevance of this theoretical result to understanding network complexity.

### Mechanism 3
- Claim: Greedy algorithms can efficiently provide upper bounds on proximate rank, though not tight bounds.
- Mechanism: The greedy algorithm relaxes the exact clustering problem by allowing approximate merges within ε distance, providing a certificate that nearby parameters exist with bounded rank. This creates a one-sided test for compressibility.
- Core assumption: Approximate merging within ε tolerance captures the essence of proximate rank without requiring exact optimal clustering.
- Evidence anchors:
  - [abstract]: "We give a greedy algorithm for bounding the proximate rank of a parameter, and show that the problem of tightly bounding the proximate rank is NP-complete."
  - [section]: "We give a greedy algorithm for bounding this value" referring to proximate rank.
  - [corpus]: Limited corpus evidence on greedy approaches to network compressibility, suggesting this is an open area for practical implementation.

## Foundational Learning

- Concept: Computational complexity theory (P vs NP, decision problems, reductions)
  - Why needed here: The paper's main theoretical contribution relies on proving NP-completeness via reduction from Boolean satisfiability
  - Quick check question: What is the difference between a decision problem and an optimization problem in computational complexity?

- Concept: Functional equivalence in neural networks
  - Why needed here: The rank definition and lossless compression rely on understanding when two parameters implement the same function
  - Quick check question: According to Sussmann (1992), what operations preserve functional equivalence in single-hidden-layer tanh networks?

- Concept: Geometric clustering problems
  - Why needed here: The NP-hardness proof reduces to a geometric point covering problem, requiring understanding of how geometric problems relate to network parameter clustering
  - Quick check question: How does the uniform point cover problem differ from k-means clustering in terms of computational complexity?

## Architecture Onboarding

- Component map:
  - Core: Single-hidden-layer tanh network with n inputs, m outputs, h hidden units
  - Parameters: Outgoing weights (ai ∈ Rm), incoming weights (bi ∈ Rn), biases (ci ∈ R), output biases (d ∈ Rm)
  - Functions: fw(x) = d + Σ ai·tanh(bi·x + ci)
  - Algorithms: COMPRESS (w), RANK(w), BOUND(ε,w), APPROX PARTITION(ε,points)

- Critical path:
  1. Implement COMPRESS to verify lossless compression works
  2. Implement RANK to verify rank computation matches compression
  3. Implement BOUND to verify greedy upper bound on proximate rank
  4. Test with known compressible and incompressible parameters

- Design tradeoffs:
  - Exact vs approximate proximate rank: NP-hardness suggests exact computation is intractable for large networks
  - Single vs multi-dimensional: Generalization to n,m > 1 requires lexicographic sorting and vector operations
  - Practical vs theoretical: Greedy bounds provide actionable insights despite not being tight

- Failure signatures:
  - COMPRESS returns non-incompressible output: Indicates reducibility conditions not properly implemented
  - RANK returns incorrect value: Check if COMPRESS properly eliminates all redundant units
  - BOUND returns value larger than h: Verify the ε threshold and partitioning logic

- First 3 experiments:
  1. Test COMPRESS on parameters with known reducibility conditions (e.g., bi=0 for some i)
  2. Verify RANK equals unit count after COMPRESS on random compressible parameters
  3. Compare BOUND results with exact proximate rank on small networks (h ≤ 10) to validate the greedy approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How prevalent are losslessly compressible parameters in practical deep learning models, and can their existence explain aspects of neural network generalization?
- Basis in paper: Explicit - The paper discusses that losslessly compressible parameters are atypical but may be highly relevant to deep learning due to the learning process exerting non-random selection pressure on parameters
- Why unresolved: The paper focuses on theoretical foundations and complexity analysis rather than empirical investigation of learned networks
- What evidence would resolve it: Empirical studies analyzing the proximate rank of parameters in trained neural networks across various architectures and datasets

### Open Question 2
- Question: Are there efficient approximation algorithms for bounding proximate rank that could be practically useful in analyzing neural network compressibility?
- Basis in paper: Explicit - The paper notes that N P-completeness does not preclude efficient approximation algorithms and that Algorithm 5.1 provides only a naive approximation
- Why unresolved: The paper provides a greedy approximation but does not explore more sophisticated approximation methods or their practical performance
- What evidence would resolve it: Development and evaluation of approximation algorithms with provable guarantees and comparison of their performance on realistic network parameters

### Open Question 3
- Question: How does the structure of losslessly compressible parameter neighborhoods differ across various neural network architectures (e.g., different nonlinearities, multiple hidden layers)?
- Basis in paper: Explicit - The paper discusses that parts of the theory are generic to feed-forward architectures but acknowledges that more complex architectures will have additional opportunities for compression
- Why unresolved: The paper restricts its analysis to single-hidden-layer hyperbolic tangent networks and does not extend the theoretical framework to other architectures
- What evidence would resolve it: Theoretical characterization of compressible parameter structures in various architectures, including different activation functions and multi-layer networks

## Limitations
- Theoretical results apply specifically to single-hidden-layer tanh networks, limiting practical applicability
- Greedy algorithm provides only upper bounds, making it difficult to assess approximation quality
- NP-completeness proof relies on specific geometric reduction that may not capture all practical scenarios

## Confidence

- **High confidence**: The O(h log h) complexity for lossless compression and rank computation
- **High confidence**: The NP-completeness proof for bounding proximate rank
- **Medium confidence**: The practical utility of the greedy approximation algorithm
- **Medium confidence**: The assumption that reducibility conditions fully characterize lossless compression opportunities

## Next Checks

1. **Empirical validation**: Implement the greedy algorithm on synthetic parameters with known proximate rank to measure approximation accuracy and runtime scaling.

2. **Architecture extension**: Test whether the reducibility conditions and O(h log h) guarantees hold for networks with ReLU activation or multiple hidden layers.

3. **Practical relevance**: Compare the greedy proximate rank bounds against actual network compression rates achieved through standard pruning or quantization techniques.