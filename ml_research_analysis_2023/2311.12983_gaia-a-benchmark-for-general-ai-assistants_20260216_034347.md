---
ver: rpa2
title: 'GAIA: a benchmark for General AI Assistants'
arxiv_id: '2311.12983'
source_url: https://arxiv.org/abs/2311.12983
tags:
- questions
- answer
- gaia
- question
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GAIA is a benchmark for General AI Assistants featuring 466 real-world
  questions that require fundamental abilities such as reasoning, multi-modality handling,
  and web browsing. Human annotators achieve 92% accuracy on GAIA, while advanced
  LLMs like GPT-4 with plugins reach only 15%.
---

# GAIA: a benchmark for General AI Assistants

## Quick Facts
- arXiv ID: 2311.12983
- Source URL: https://arxiv.org/abs/2311.12983
- Reference count: 24
- Human annotators achieve 92% accuracy while GPT-4 with plugins reaches only 15%

## Executive Summary
GAIA is a benchmark designed to evaluate General AI Assistants on 466 real-world questions that require fundamental abilities like reasoning, multi-modality handling, and web browsing. The benchmark distinguishes itself by focusing on conceptually simple tasks that are challenging for current AI systems, contrasting with existing benchmarks that test specialized knowledge. GAIA questions are answered in zero-shot with unambiguous factoid answers, enabling automated evaluation while preventing gaming through memorization or pattern recognition.

## Method Summary
GAIA employs zero-shot prompting with a prefix prompt specifying answer format requirements. The benchmark consists of 466 questions with factoid, unambiguous answers that cannot be easily brute-forced from training data. Questions are categorized into three difficulty levels based on the number of steps and tools required. Evaluation uses exact match between model responses and ground truth answers, normalized for different answer types (string, number, or comma-separated list). The benchmark tests fundamental capabilities including reasoning, multi-modality understanding, web browsing, and tool use proficiency.

## Key Results
- Human annotators achieve 92% accuracy on GAIA questions
- GPT-4 with plugins achieves only 15% accuracy, demonstrating the benchmark's difficulty
- Performance correlates with the proposed three-level difficulty structure based on steps and tools required
- Current advanced models struggle with conceptually simple tasks that require complex execution sequences

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GAIA evaluates fundamental AI capabilities rather than specialized knowledge
- **Mechanism**: Questions are conceptually simple for humans yet require complex sequences of actions, tool use, and reasoning that current AI systems struggle with
- **Core assumption**: Simple tasks requiring robust execution are better indicators of general intelligence than complex specialized tasks
- **Evidence anchors**:
  - [abstract] "GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92% vs. 15% for GPT-4 equipped with plugins"
  - [section 1] "Alternatively to tasks that are harder for humans, AI systems could be asked to solve conceptually simple tasks yet that require accurate execution of complex sequences of actions"

### Mechanism 2
- **Claim**: The benchmark prevents gaming through unambiguous factoid answers
- **Mechanism**: Each question has a single correct answer that cannot be easily brute-forced or memorized from training data
- **Core assumption**: Ambiguity in answers allows for gaming strategies that don't reflect true capability
- **Evidence anchors**:
  - [section 1] "To complete a task, a system has to plan and successfully complete some number of steps since the resulting answer is absent by design in plain text from current pre-training data"
  - [section 3.1] "Our questions are meant to be answered in zero shot, limiting the influence of the evaluation setup"

### Mechanism 3
- **Claim**: The three-level difficulty structure correlates with increasing capabilities
- **Mechanism**: Level 1 requires minimal tools and steps, Level 2 requires more complex tool combinations, and Level 3 requires arbitrary-length sequences and diverse tool use
- **Core assumption**: The number of steps and tools required to solve a question is a valid proxy for its difficulty
- **Evidence anchors**:
  - [section 3.3] "The questions can be sorted into three levels of increasing difficulty depending on the number of steps required to solve the questions, and the number of different tools needed to answer the question"
  - [section 4] "Our proposed levels of difficulty, loosely defined in terms of number of steps and number of different capabilities used, are correlated with the performance of current models"

## Foundational Learning

- **Concept**: Zero-shot prompting
  - **Why needed here**: GAIA questions are designed to be answered without any task-specific fine-tuning or examples
  - **Quick check question**: Can you design a prompt that gets a correct answer to a GAIA question without showing any examples?

- **Concept**: Tool use proficiency
  - **Why needed here**: Many questions require browsing, reading files, or other tool interactions to gather information
  - **Quick check question**: What tools would you need to answer "What was the actual enrollment count of the clinical trial on H. pylori in acne vulgaris patients from Jan-May 2018 as listed on the NIH website?"

- **Concept**: Multi-modal understanding
  - **Why needed here**: Some questions come with images, spreadsheets, or other non-text files that must be processed
  - **Quick check question**: How would you extract information from an Excel file attached to a GAIA question?

## Architecture Onboarding

- **Component map**: Question database → LLM integration layer → Answer generation → Answer verification engine → Ground truth comparison
- **Critical path**: Question → LLM with appropriate tools → Answer generation → Answer verification against ground truth
- **Design tradeoffs**: Quality vs. quantity of questions (curated 466 vs. massive datasets), ambiguity prevention vs. natural question formulation
- **Failure signatures**: Incorrect answers due to tool misuse, failure to follow format specifications, attempting to answer without required tools
- **First 3 experiments**:
  1. Test a simple GAIA question with GPT-4 without plugins to establish baseline performance
  2. Test the same question with GPT-4 and web browsing plugin to measure tool impact
  3. Test a multi-modal question with file attachment to evaluate file reading capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the GAIA benchmark be extended to evaluate multi-step tool usage in real-world environments while preventing spamming of websites?
- Basis in paper: [explicit] The paper mentions that GAIA does not specify possible APIs and relies on interactions with the real world, but testing capabilities like uploading files or booking meetings in real environments requires careful consideration to avoid spamming websites.
- Why unresolved: The paper acknowledges this as a limitation and leaves it for future work, suggesting that closed environments for LLM agents could be a solution.
- What evidence would resolve it: A framework or methodology for safely evaluating multi-step tool usage in real-world environments without causing harm or spamming, potentially involving controlled testing environments or simulated scenarios.

### Open Question 2
- Question: How can the GAIA benchmark be adapted to evaluate AI assistants' performance across multiple languages and cultures?
- Basis in paper: [explicit] The paper acknowledges the lack of linguistic and cultural diversity in GAIA, noting that all questions are in English and rely on English web pages, which limits the benchmark's applicability to non-English speakers.
- Why unresolved: The paper states this as a limitation and hopes to address it in future work or through community involvement, but does not provide a concrete plan or methodology.
- What evidence would resolve it: A comprehensive strategy for translating GAIA questions into multiple languages, ensuring cultural relevance, and validating the benchmark's effectiveness across diverse linguistic and cultural contexts.

### Open Question 3
- Question: How can the GAIA benchmark be used to evaluate the interpretability of AI assistants' reasoning traces?
- Basis in paper: [explicit] The paper mentions that GAIA questions are designed to be conceptually simple, allowing for easy interpretation of a model's reasoning trace, but it does not currently evaluate the trace leading to the answer.
- Why unresolved: The paper suggests that human and model-based evaluations could be used to assess reasoning traces, but it leaves this aspect for future work.
- What evidence would resolve it: A framework or methodology for evaluating the interpretability of AI assistants' reasoning traces, potentially involving human annotators or automated tools to assess the clarity and correctness of the reasoning process.

## Limitations

- The benchmark's focus on factoid answers may create a moving target as models become better at information retrieval
- Questions are limited to English and English web pages, lacking linguistic and cultural diversity
- The three-level difficulty structure may not capture the full spectrum of reasoning complexity needed for AGI
- Evaluating multi-step tool usage in real-world environments requires careful consideration to avoid spamming websites

## Confidence

**High Confidence**: GAIA tests fundamental capabilities that current AI systems struggle with (92% human vs 15% GPT-4 with plugins performance gap)

**Medium Confidence**: The benchmark prevents gaming through unambiguous factoid answers, though models may develop strategies to identify question patterns

**Medium Confidence**: The three-level difficulty structure correlates with model capabilities, but may not remain meaningful as AI advances

## Next Checks

1. **Temporal Validity Test**: Re-run the benchmark with newly released models every 6 months to verify that performance gaps persist and difficulty levels remain meaningful predictors of capability.

2. **Information Retrieval Analysis**: Conduct ablation studies to determine what fraction of questions can be answered through direct information retrieval versus requiring genuine reasoning and tool orchestration.

3. **Cross-cultural Robustness**: Test the benchmark with human annotators from different cultural backgrounds to verify that questions remain unambiguous and "conceptually simple" across diverse populations.