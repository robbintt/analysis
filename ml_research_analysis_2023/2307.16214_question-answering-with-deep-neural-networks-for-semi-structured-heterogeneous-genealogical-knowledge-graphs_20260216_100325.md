---
ver: rpa2
title: Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous
  Genealogical Knowledge Graphs
arxiv_id: '2307.16214'
source_url: https://arxiv.org/abs/2307.16214
tags:
- question
- genealogical
- questions
- graph
- answering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes an end-to-end pipeline for answering natural
  questions using semi-structured heterogeneous genealogical knowledge graphs. It
  introduces a dedicated graph traversal algorithm adapted to genealogical semantics,
  automatically converts genealogical graphs to text, and fine-tunes a transformer-based
  model for the genealogical domain.
---

# Question Answering with Deep Neural Networks for Semi-Structured Heterogeneous Genealogical Knowledge Graphs

## Quick Facts
- arXiv ID: 2307.16214
- Source URL: https://arxiv.org/abs/2307.16214
- Authors: [Not specified in source]
- Reference count: 40
- Key outcome: Uncle-BERT, a BERT-based model fine-tuned on genealogical data, significantly outperforms state-of-the-art open-domain models on genealogical questions using a simplified pipeline that converts genealogical knowledge graphs to text.

## Executive Summary
This study presents an end-to-end pipeline for answering natural questions using semi-structured heterogeneous genealogical knowledge graphs. The approach introduces a dedicated graph traversal algorithm (Gen-BFS) adapted to genealogical semantics, automatically converts genealogical graphs to text, and fine-tunes a BERT-based model (Uncle-BERT) for the genealogical domain. Evaluation on a large corpus of 1.8 million individuals demonstrates that the proposed model significantly outperforms state-of-the-art open-domain models on genealogical questions while simplifying complexity compared to knowledge graph-based pipelines. The results indicate that genealogical question answering requires dedicated datasets and models, and that question type and relation scope impact accuracy.

## Method Summary
The methodology converts GEDCOM files into CIDOC-CRM knowledge graphs, then applies a custom Gen-BFS traversal algorithm to extract subgraphs up to 2 relation degrees. These subgraphs are converted to text using a knowledge-graph-to-text model, generating SQuAD-style questions and answers to create the Gen-SQuAD dataset. A BERT-based model (Uncle-BERT) is fine-tuned on this auto-generated genealogical training data for each relation depth (0, 1, and 2). The pipeline simplifies traditional KGQA by converting graphs to text, enabling use of powerful BERT contextual embeddings without complex graph traversal models. The approach is evaluated on a corpus of 1.8 million individuals, comparing performance against open-domain models and a DELFT adaptation.

## Key Results
- Uncle-BERT trained on genealogical data significantly outperforms state-of-the-art open-domain models on genealogical questions
- Knowledge-graph-to-text methodology reduces pipeline complexity compared to direct KGQA approaches
- Question type and relation scope significantly impact accuracy, with dedicated datasets required for genealogical QA
- The methodology shows promise for extension to other NLP tasks in the genealogical domain

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-to-text conversion with Gen-BFS preserves genealogical semantics better than raw graph traversal
- Mechanism: Gen-BFS reduces traversal depth by treating family nodes as equivalent to edges, preserving semantic relation degrees (parents, children, siblings) in resulting text
- Core assumption: Genealogical relation degrees can be encoded as sentence templates the model can learn
- Evidence anchors: Gen-BFS algorithm handles Person/Family nodes differently to preserve genealogical meaning
- Break condition: If relation types cannot be reliably mapped to sentence templates or template generation fails to cover edge cases

### Mechanism 2
- Claim: Fine-tuning BERT-based model on domain-specific genealogical data outperforms generic QA models
- Mechanism: Domain-specific training data provides contextual embeddings that align with genealogical entities and relation types, enabling better span prediction
- Core assumption: Genealogical questions have unique semantic features not present in open-domain datasets
- Evidence anchors: Uncle-BERT fine-tuned on auto-generated genealogical dataset shows improved performance
- Break condition: If domain gap is too large or training data insufficient to capture genealogical semantics

### Mechanism 3
- Claim: Knowledge-graph-to-text methodology reduces complexity compared to direct KGQA while improving accuracy
- Mechanism: Converting KG to text allows use of powerful BERT contextual embeddings without needing complex graph traversal models
- Core assumption: Textual representation can encode necessary relational information for QA
- Evidence anchors: Pipeline simplifies task by converting genealogical knowledge graph into text
- Break condition: If KG-to-text conversion loses critical relational information needed for answering questions

## Foundational Learning

- Concept: Graph traversal algorithms (BFS, DFS) and their adaptations
  - Why needed here: Understanding Gen-BFS requires knowledge of how BFS works and how it can be modified for domain-specific semantics
  - Quick check question: How does Gen-BFS differ from standard BFS in handling family nodes?

- Concept: BERT and transformer architecture
  - Why needed here: Uncle-BERT is a fine-tuned BERT model; understanding its pre-training and fine-tuning process is crucial
  - Quick check question: What are the key differences between BERT's pre-training objectives (MLM, NSP) and its fine-tuning for QA?

- Concept: Knowledge graph representation and CIDOC-CRM
  - Why needed here: Genealogical data is modeled as a knowledge graph using CIDOC-CRM, which affects how entities and relations are represented
  - Quick check question: How does CIDOC-CRM's event-based approach differ from traditional binary relation KGs?

## Architecture Onboarding

- Component map: GEDCOM parser → CIDOC-CRM knowledge graph → Gen-BFS traversal → KG-to-text generation → SQuAD JSON format → Uncle-BERT fine-tuning → QA inference pipeline
- Critical path: Gen-BFS → KG-to-text → Uncle-BERT training → Inference with user query
- Design tradeoffs:
  - KG-to-text vs. direct KGQA: Simplicity vs. potential loss of relational structure
  - Scope of Gen-BFS traversal: More context vs. computational cost and noise
  - Template-based vs. DNN KG-to-text: Coverage vs. flexibility
- Failure signatures:
  - Low F1 scores: Possible issues with KG-to-text conversion or Uncle-BERT fine-tuning
  - Unanswerable questions: Template generation may have missed certain question types
  - Incorrect relation handling: Gen-BFS may not properly preserve genealogical semantics
- First 3 experiments:
  1. Validate Gen-BFS produces correct genealogical relation degrees by comparing output to ground truth in sample GEDCOM files
  2. Test KG-to-text generation quality by checking if generated sentences accurately represent the input sub-graph
  3. Verify Uncle-BERT fine-tuning by measuring performance on a held-out validation set from Gen-SQuAD

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal traversal depth for genealogical knowledge graphs to balance information richness and computational efficiency?
- Basis in paper: The paper discusses using Gen-BFS with different depth levels (0, 1, 2) and shows varying performance, but does not determine the optimal depth
- Why unresolved: Study only tests three specific depth levels and shows trade-offs between accuracy and context complexity, but does not identify optimal depth for different types of questions or family tree structures
- What evidence would resolve it: Systematic evaluation of multiple depth levels (e.g., 0-3) across diverse genealogical datasets with varying tree sizes and structures, measuring both accuracy and computational efficiency

### Open Question 2
- Question: How do different types of unstructured genealogical texts (e.g., biographical notes vs. event descriptions) affect the performance of graph-to-text conversion models?
- Basis in paper: Paper mentions combining structured and unstructured data but does not analyze how different text types impact model performance
- Why unresolved: Study uses combined approach for text generation but does not isolate or compare impact of different text types on model accuracy or training efficiency
- What evidence would resolve it: Comparative analysis of graph-to-text models trained separately on different text types (biographical, event-based, relationship descriptions) and evaluation of their individual and combined performance

### Open Question 3
- Question: Can the proposed genealogical question answering pipeline be effectively extended to handle multi-hop reasoning questions that require combining information from multiple family members?
- Basis in paper: Paper addresses single-hop questions but mentions multi-hop reasoning as a challenge, suggesting potential for extension
- Why unresolved: Current methodology simplifies task by converting graphs to text, but does not explicitly address how to handle questions requiring inference across multiple family members or generations
- What evidence would resolve it: Development and evaluation of extended pipeline that incorporates multi-hop reasoning capabilities, tested on genealogical questions requiring cross-generational inference

## Limitations

- Evaluation restricted to single museum corpus of GEDCOM data, raising questions about generalizability to other genealogical datasets
- KG-to-text conversion methodology may lose fine-grained relational information that direct KGQA approaches preserve
- Absence of publicly available code and datasets prevents independent verification of Gen-BFS traversal algorithm and KG-to-text generation components
- Limited sample size for some question categories and relation types in performance analysis

## Confidence

- **High Confidence**: Uncle-BERT outperforms open-domain models on Gen-SQuAD2 dataset (supported by F1 score comparisons)
- **Medium Confidence**: Knowledge-graph-to-text methodology simplifies complexity while maintaining accuracy (mechanism plausible but not empirically validated against direct KGQA)
- **Medium Confidence**: Genealogical relation types and question categories significantly impact QA performance (analysis based on error patterns, but limited sample size for some categories)

## Next Checks

1. Replicate Gen-BFS traversal on a different genealogical dataset to verify semantic preservation across data sources
2. Implement a direct KGQA baseline using the same genealogical data to quantify information loss from KG-to-text conversion
3. Conduct ablation studies on template coverage to determine which genealogical relation types are most critical for QA accuracy