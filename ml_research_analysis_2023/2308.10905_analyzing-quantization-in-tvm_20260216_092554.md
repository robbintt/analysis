---
ver: rpa2
title: Analyzing Quantization in TVM
arxiv_id: '2308.10905'
source_url: https://arxiv.org/abs/2308.10905
tags:
- memory
- quantization
- int8
- fp32
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the underperformance of 8-bit quantization
  in TVM for deep learning inference. The authors identified and fixed a bug in graph
  building that was causing quantized models to run slower than full-precision versions.
---

# Analyzing Quantization in TVM

## Quick Facts
- arXiv ID: 2308.10905
- Source URL: https://arxiv.org/abs/2308.10905
- Reference count: 0
- Primary result: 163.88% faster inference for compute-bound tasks and 194.98% for memory-bound tasks with 8-bit quantization in TVM

## Executive Summary
This paper addresses the underperformance of 8-bit quantization in TVM for deep learning inference. The authors identified and fixed a bug in graph building that was causing quantized models to run slower than full-precision versions. They then systematically analyzed and optimized quantization for both computation-bound (batch size 1) and memory-bound (larger batch sizes) tasks. Through comprehensive experimentation with different layouts and schedules, they achieved significant improvements: 163.88% faster inference for compute-bound tasks and 194.98% for memory-bound tasks compared to the baseline TVM compiled full-precision model.

## Method Summary
The study involved fixing a graph executor bug in TVM's quantization pipeline, then conducting systematic experiments to optimize quantization performance. The authors tested different optimization strategies including spatial packing layouts, SIMD optimizations, and NHWC quantized interleaved layouts across various batch sizes (1, 64, 256). They compared int8 versus fp32 performance using ResNet18 inference model compiled by TVM, measuring inference time improvements as the primary metric.

## Key Results
- Fixed TVM quantization bug that caused quantized models to run slower than full-precision versions
- Achieved 163.88% faster inference for compute-bound tasks with batch size 1
- Achieved 194.98% faster inference for memory-bound tasks with larger batch sizes
- Demonstrated that proper optimization makes quantization deliver substantial performance gains in TVM

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Quantization improves performance by reducing memory bandwidth usage in memory-bound tasks
- Mechanism: When batch size increases, larger intermediate results require more memory transfers. Reducing precision from FP32 to INT8 decreases data size by 4x, directly reducing memory bandwidth requirements and improving throughput
- Core assumption: Memory bandwidth is the primary bottleneck for large batch sizes
- Evidence anchors:
  - [abstract] "The memory bandwidth required to transfer the data between memory and processing units (e.g., CPU, GPU) is also reduced, which contributes to improved memory utilization."
  - [section] "In TVM, the performance improvement with int8 precision primarily stems from the reduction in memory bandwidth."
  - [corpus] Weak evidence - no directly relevant citations about memory bandwidth improvements in quantization
- Break condition: When computation becomes the bottleneck rather than memory access, or when quantization overhead exceeds memory savings

### Mechanism 2
- Claim: Spatial packing optimization improves computation efficiency by enabling better vectorization and parallelization
- Mechanism: Converting from NCHW to NCHW16c format allows 16-channel blocking, enabling 4x operations for 64 channels and parallelizing the H dimension by factor of 4, maximizing hardware utilization
- Core assumption: Hardware supports efficient vectorized operations on spatially packed data layouts
- Evidence anchors:
  - [section] "One way to increase the efficiency of memory accesses when parallelizing work is through spatial packing. This technique involves converting kernel arrays from a 2D NCHW format to a 4D NCHWnc packed layout."
  - [section] "In our experiment, the converted model follows the NCHW16c format, which is widely used on AVX512+ systems."
  - [corpus] Weak evidence - no directly relevant citations about spatial packing benefits
- Break condition: When hardware lacks support for the specific packed format or when data dimensions don't align with blocking factors

### Mechanism 3
- Claim: Using Graph Executor instead of VM Executor eliminates unnecessary model partitioning overhead
- Mechanism: Graph Executor executes pre-optimized computation graphs statically without runtime partitioning, avoiding the overhead of prefix, middle, and suffix function conversions between quantized and dequantized spaces
- Core assumption: The model doesn't require dynamic operations or runtime code generation
- Evidence anchors:
  - [section] "We discovered that TVM provides two types of executors: Graph Executor and VM (Virtual Machine) Executor... By default, TVM's quantization code sets the executor to VM, allowing the model to be potentially partitioned into three modules"
  - [section] "we resolved this issue by using a static graph relay executor"
  - [corpus] Weak evidence - no directly relevant citations about executor choice impact
- Break condition: When dynamic operations are actually needed for the model architecture

## Foundational Learning

- Concept: Quantization in deep learning
  - Why needed here: Understanding how reducing precision from FP32 to INT8 affects both accuracy and performance is fundamental to the entire optimization effort
  - Quick check question: Why does quantization typically reduce memory usage by 4x when converting from FP32 to INT8?

- Concept: Memory-bound vs computation-bound tasks
  - Why needed here: The optimization strategies differ significantly between these two scenarios, affecting which quantization benefits are most relevant
  - Quick check question: How does increasing batch size typically shift a task from computation-bound to memory-bound?

- Concept: TVM optimization layers and scheduling
  - Why needed here: Understanding how TVM applies different schedules and optimizations at graph and tensor levels is crucial for interpreting the performance results
  - Quick check question: What are the two main optimization layers in TVM and how do they differ in their approach to improving performance?

## Architecture Onboarding

- Component map:
  Input preprocessing → Model graph (ResNet18) → Quantization layer → TVM compiler → Optimized schedule selection → Hardware execution → Output
  Key components: Graph Executor/VM Executor, spatial packing layout converter, quantization operators, SIMD optimization units

- Critical path:
  1. Model loading and quantization configuration
  2. Graph-level optimization (executor selection)
  3. Tensor-level scheduling (layout and optimization selection)
  4. Compilation to target hardware
  5. Inference execution

- Design tradeoffs:
  - Executor choice: Graph Executor provides static optimization but lacks flexibility; VM Executor enables dynamic operations but adds overhead
  - Layout selection: NCHW spatial packing maximizes parallelization but requires specific hardware support; NHWC quantized interleaved optimizes for different hardware characteristics
  - Precision vs accuracy: INT8 provides better performance but may impact model accuracy depending on the task

- Failure signatures:
  - Quantized model runs slower than FP32 baseline → likely executor or layout configuration issue
  - Unexpected memory usage patterns → quantization operators may be storing intermediate results in FP32
  - Poor parallelization → spatial packing format may not match hardware capabilities
  - Inconsistent performance across batch sizes → optimization may be tuned for specific workload type

- First 3 experiments:
  1. Test quantization performance with Graph Executor vs VM Executor to identify executor-related bottlenecks
  2. Compare NCHW spatial packing vs NHWC quantized interleaved layouts for batch size 1 vs batch size 64
  3. Measure memory bandwidth usage during inference to quantify quantization's impact on memory-bound scenarios

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific optimizations are possible beyond the schedule and layout optimizations explored in the paper?
- Basis in paper: [inferred] The paper explores various layout and schedule optimizations but acknowledges that there may be additional optimization opportunities.
- Why unresolved: The paper focuses on a specific set of optimizations and does not exhaustively explore all possible optimization strategies.
- What evidence would resolve it: A comprehensive analysis of additional optimization techniques and their impact on quantization performance would provide clarity.

### Open Question 2
- Question: How does the performance of 8-bit quantization in TVM compare to other deep learning frameworks?
- Basis in paper: [explicit] The paper mentions that quantization is expected to achieve around 50% of the full-precision inference time, but in TVM, the quantized version performs worse.
- Why unresolved: The paper only compares TVM's performance with its own baseline and does not provide a comparison with other frameworks.
- What evidence would resolve it: Benchmarking TVM's quantization performance against other frameworks would provide insights into its relative effectiveness.

### Open Question 3
- Question: How does the performance of quantization vary across different hardware architectures?
- Basis in paper: [inferred] The paper mentions that TVM aims to optimize and deploy models on various hardware platforms, suggesting that performance may vary across different architectures.
- Why unresolved: The paper only provides results for a specific hardware architecture and does not explore the impact of different hardware on quantization performance.
- What evidence would resolve it: Conducting experiments on different hardware architectures and comparing the performance of quantization would provide insights into its generalizability.

## Limitations

- Focus on single model architecture (ResNet18) and specific hardware platform limits generalizability
- No evaluation of quantization's impact on model accuracy, which is critical for practical deployment
- Lack of direct citations for memory bandwidth and spatial packing mechanisms suggests these explanations may not be empirically validated within this work

## Confidence

- Bug fix and basic quantization performance improvements: High
- Memory bandwidth mechanism for large batch sizes: Medium
- Spatial packing and layout optimization benefits: Medium
- Executor choice impact on quantization performance: Medium

## Next Checks

1. Test the identified optimizations across multiple model architectures (e.g., MobileNet, BERT) to validate generalization
2. Measure model accuracy degradation when applying quantization to verify the precision-accuracy tradeoff
3. Profile memory bandwidth utilization during inference to empirically confirm the memory-bound performance improvements