---
ver: rpa2
title: Text Similarity from Image Contents using Statistical and Semantic Analysis
  Techniques
arxiv_id: '2308.12842'
source_url: https://arxiv.org/abs/2308.12842
tags:
- plagiarism
- image
- images
- text
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a system for detecting plagiarism in image contents,
  such as figures, graphs, and tables, by combining statistical and semantic analysis
  techniques. The system extracts textual information from images using Microsoft
  Vision API and then applies algorithms like Jaccard, Cosine, LSA, BERT, and WordNet
  to identify plagiarism.
---

# Text Similarity from Image Contents using Statistical and Semantic Analysis Techniques

## Quick Facts
- arXiv ID: 2308.12842
- Source URL: https://arxiv.org/abs/2308.12842
- Reference count: 18
- Primary result: Semantic algorithms (LSA, BERT) outperform statistical methods for detecting plagiarism in image contents

## Executive Summary
This paper presents a system for detecting plagiarism in image contents such as figures, graphs, and tables by combining statistical and semantic analysis techniques. The system extracts textual information from images using Microsoft Vision API and applies algorithms like Jaccard, Cosine, LSA, BERT, and WordNet to identify plagiarism. Results demonstrate that semantic algorithms, particularly LSA and BERT, outperform statistical methods in detecting accurate plagiarism, with plagiarism percentages ranging from 0.67% to 5.44% when using BERT with named entities excluded.

## Method Summary
The system processes image files containing textual information through Microsoft Vision API to extract readable text. This extracted text undergoes preprocessing including tokenization, stopword removal, lemmatization, and named entity recognition. The processed text is then compared against a corpus of reference images using multiple similarity algorithms including statistical methods (Jaccard, Cosine) and semantic methods (LSA, BERT, WordNet). The system supports various image formats such as jpg, jpeg, png, and bmp, and produces plagiarism detection reports with percentage scores indicating the likelihood of copied content.

## Key Results
- Semantic algorithms (LSA, BERT) outperform statistical methods in plagiarism detection accuracy
- Excluding named entities provides more accurate and efficient plagiarism detection results
- BERT algorithm with named entities excluded achieved plagiarism percentages between 0.67% and 5.44% for sample figures
- The system effectively addresses the challenge of plagiarism in non-textual content like figures, graphs, and tables

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Microsoft Vision API provides sufficient text extraction accuracy from images to enable plagiarism detection in non-textual content
- Mechanism: The API processes image files and outputs recognized text, which is then fed into similarity algorithms
- Core assumption: The extracted text preserves enough semantic content for meaningful comparison
- Evidence anchors:
  - [abstract] "The system extracts textual information from images using Microsoft Vision API"
  - [section] "Microsoft Vision API is also performing best when the images are in proper readable format"
  - [corpus] Weak evidence; no direct comparison to OCR baselines
- Break condition: OCR errors from skewed or noisy images cause loss of key semantic tokens, breaking similarity detection

### Mechanism 2
- Claim: Semantic algorithms (LSA, BERT) outperform statistical ones (Jaccard, Cosine) for image-based plagiarism detection
- Mechanism: LSA/BERT capture contextual meaning while Jaccard/Cosine rely on exact token overlap
- Core assumption: Semantic representation generalizes better across paraphrased or transformed image text
- Evidence anchors:
  - [abstract] "introducing semantic algorithms such as LSA, BERT, WordNet outperformed in detecting efficient and accurate plagiarism"
  - [section] "LSA and BERT algorithms have shown superior performance in graph plagiarism detection"
  - [corpus] No peer-verified numbers; only internal comparisons cited
- Break condition: When image text is short or low in semantic density, LSA/BERT advantages diminish

### Mechanism 3
- Claim: Excluding named entities improves plagiarism detection accuracy by reducing false positives
- Mechanism: Named entity removal eliminates proper nouns that may vary across citations or translations
- Core assumption: Named entities rarely carry the substantive meaning that determines plagiarism
- Evidence anchors:
  - [section] "it becomes evident that excluding named entities provides satisfactory results for plagiarism detection"
  - [section] "exclusion of named entities gives more accurate and efficient results in plagiarism detection"
  - [corpus] No external validation; internal claim only
- Break condition: If core ideas are encoded in entity references (e.g., specific algorithms), removal harms detection

## Foundational Learning

- Concept: Text extraction from images (OCR fundamentals)
  - Why needed here: System relies on Microsoft Vision API to convert image content into searchable text
  - Quick check question: What are the main sources of OCR error when processing figures and graphs?

- Concept: Similarity metrics (Jaccard, Cosine, TF-IDF)
  - Why needed here: These quantify overlap between extracted image text and corpus documents
  - Quick check question: How does token frequency affect Cosine similarity vs. Jaccard?

- Concept: Semantic embeddings (LSA, BERT)
  - Why needed here: LSA/BERT capture meaning beyond exact token matches, improving plagiarism detection
  - Quick check question: What is the difference between LSA's latent semantic space and BERT's contextual embeddings?

## Architecture Onboarding

- Component map:
  Image ingestion -> Microsoft Vision API -> Text extraction -> Preprocessing (tokenization, stopword removal, NER, lemmatization) -> Similarity computation (statistical + semantic algorithms) -> Plagiarism report

- Critical path:
  Image -> OCR -> Preprocess -> LSA/BERT scoring -> Result aggregation

- Design tradeoffs:
  Speed vs. accuracy: Microsoft Vision API is faster than alternatives but less accurate on noisy images
  Complexity vs. interpretability: Semantic algorithms improve detection but reduce transparency compared to statistical metrics

- Failure signatures:
  High false positives when named entities are excluded but plagiarism relies on them
  Degradation when image text is too short for meaningful semantic comparison

- First 3 experiments:
  1. Compare OCR output quality across different image qualities using Microsoft Vision vs. Tesseract
  2. Benchmark Jaccard/Cosine vs. LSA/BERT on a set of paraphrased image text pairs
  3. Measure impact of named entity inclusion/exclusion on known plagiarism examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the accuracy of the plagiarism detection system change when processing images with different text extraction challenges, such as skewed text or low contrast, and what specific improvements could be made to handle these scenarios?
- Basis in paper: [inferred] The paper mentions that Microsoft Vision API performs well with clear images but struggles with skewed or blurred text, indicating a potential limitation in handling challenging image conditions
- Why unresolved: The paper does not provide specific accuracy metrics or comparisons for images with text extraction challenges, leaving the extent of the system's robustness unclear
- What evidence would resolve it: Testing the system with a diverse dataset of images containing various text extraction challenges (e.g., skewed text, low contrast) and comparing the accuracy metrics with clear images would provide insights into the system's robustness and potential areas for improvement

### Open Question 2
- Question: What are the computational efficiency and resource requirements of the system when processing large datasets of images, and how does this impact its scalability for real-world applications?
- Basis in paper: [inferred] The paper discusses the system's ability to handle diverse image formats and perform plagiarism detection, but it does not address the computational efficiency or resource requirements for processing large datasets
- Why unresolved: The paper lacks information on the system's performance in terms of processing time, memory usage, and scalability when dealing with large volumes of images, which is crucial for real-world applications
- What evidence would resolve it: Conducting performance tests with varying dataset sizes and measuring the system's processing time, memory usage, and scalability would provide valuable insights into its efficiency and applicability in real-world scenarios

### Open Question 3
- Question: How does the system's performance compare to existing plagiarism detection tools specifically designed for textual content, and what are the advantages and limitations of using image-based plagiarism detection?
- Basis in paper: [explicit] The paper highlights the importance of detecting plagiarism in non-textual content like images, but it does not compare the system's performance with existing textual plagiarism detection tools
- Why unresolved: The paper focuses on the system's capabilities for image content plagiarism detection but does not provide a comparative analysis with established textual plagiarism detection methods, leaving the system's relative effectiveness unclear
- What evidence would resolve it: Conducting a comparative study between the image-based plagiarism detection system and existing textual plagiarism detection tools, evaluating their accuracy, efficiency, and applicability, would provide insights into the system's advantages and limitations

## Limitations

- System performance heavily depends on Microsoft Vision API's text extraction accuracy, which degrades significantly on low-quality or skewed images
- No peer-verified quantitative benchmarks are provided for algorithm comparisons
- Effectiveness of named entity exclusion is claimed but not empirically validated across diverse datasets

## Confidence

- **High Confidence**: Microsoft Vision API serves as a functional text extraction layer for clear, readable images
- **Medium Confidence**: Semantic algorithms (LSA, BERT) show superior performance compared to statistical methods in the presented internal comparisons
- **Low Confidence**: Claims about named entity exclusion improving accuracy lack external validation and may not generalize across domains

## Next Checks

1. Conduct controlled experiments comparing Microsoft Vision API against established OCR engines (Tesseract, Google Vision) across varying image quality levels
2. Perform ablation studies measuring the impact of named entity inclusion/exclusion on both true positive and false positive rates
3. Test the system on a diverse benchmark dataset containing images with different text densities, languages, and layout complexities