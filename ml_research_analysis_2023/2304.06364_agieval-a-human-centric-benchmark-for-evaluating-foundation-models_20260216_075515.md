---
ver: rpa2
title: 'AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models'
arxiv_id: '2304.06364'
source_url: https://arxiv.org/abs/2304.06364
tags:
- reasoning
- tasks
- performance
- language
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AGIEval, a human-centric benchmark designed
  to evaluate foundation models' capabilities in tackling human-level tasks. Unlike
  traditional benchmarks that rely on artificial datasets, AGIEval focuses on standardized
  exams such as college entrance exams, law school admission tests, math competitions,
  and lawyer qualification tests.
---

# AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models

## Quick Facts
- arXiv ID: 2304.06364
- Source URL: https://arxiv.org/abs/2304.06364
- Reference count: 12
- Key outcome: Introduces AGIEval benchmark showing GPT-4 surpasses average human performance on standardized exams but struggles with complex reasoning and domain-specific knowledge

## Executive Summary
This paper introduces AGIEval, a human-centric benchmark designed to evaluate foundation models' capabilities in tackling human-level tasks. Unlike traditional benchmarks that rely on artificial datasets, AGIEval focuses on standardized exams such as college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. The authors evaluate state-of-the-art models including GPT-4, ChatGPT, and Text-Davinci-003 on 20 human-centric tasks across various subjects in both English and Chinese. Results show that GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, achieving 95% accuracy on SAT Math and 92.5% on Chinese Gaokao English. However, models still struggle with tasks requiring complex reasoning or specific domain knowledge. The study provides valuable insights into model strengths and limitations, highlighting the need for future research to enhance general capabilities. AGIEval offers a more meaningful evaluation framework for foundation models in real-world scenarios.

## Method Summary
The paper introduces AGIEval, a human-centric benchmark based on standardized exams like college entrance exams, law school admission tests, math competitions, and lawyer qualification tests. The benchmark consists of 20 tasks across various subjects in both English and Chinese, totaling 8,062 questions. The authors evaluate three state-of-the-art models (GPT-4, ChatGPT, and Text-Davinci-003) under zero-shot, few-shot, and chain-of-thought prompting settings. Models are assessed on task-specific accuracy for multiple-choice questions and Exact Match (EM) for fill-in-the-blank questions. The chain-of-thought approach involves generating an explanation before answering the question. Model outputs and code are publicly released on GitHub for reproducibility.

## Key Results
- GPT-4 surpasses average human performance on SAT, LSAT, and math competitions, achieving 95% accuracy on SAT Math and 92.5% on Chinese Gaokao English
- Models still struggle with tasks requiring complex reasoning or specific domain knowledge
- Chain-of-Thought prompting substantially enhances performance in English mathematical exams, including MATH, AQuA-RAT, and SAT-Math
- Performance varies significantly across different tasks and languages, indicating inconsistent model generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AGIEval leverages standardized human exams to create a robust, real-world-aligned benchmark for foundation models.
- Mechanism: By using official, high-stakes exams (e.g., SAT, LSAT, Gaokao) designed for human test-takers, the benchmark captures complex reasoning and domain-specific knowledge that artificial datasets often miss.
- Core assumption: Tasks from standardized exams reliably reflect human cognitive abilities and reasoning skills.
- Evidence anchors:
  - [abstract]: "Traditional benchmarks, which rely on artificial datasets, may not accurately represent human-level capabilities."
  - [section 2.2]: "These benchmarks primarily focus on simpler textual understanding rather than complex reasoning abilities aligned with real-world applicability."
  - [corpus]: Weak. While related papers cite human-centric evaluations, none provide direct evidence for exam-based benchmarks as superior.
- Break condition: If exam content does not generalize well to AI reasoning, or if exams emphasize memorization over reasoning, benchmark validity may degrade.

### Mechanism 2
- Claim: Chain-of-Thought (CoT) prompting improves model performance on complex reasoning tasks by decomposing problems into intermediate steps.
- Mechanism: CoT forces models to generate explicit reasoning steps before answering, simulating human problem-solving strategies and improving answer accuracy.
- Core assumption: Breaking down complex reasoning into smaller, manageable steps improves model accuracy.
- Evidence anchors:
  - [section 4.2.2]: "Chain-of-Thought (CoT) prompting enables large language models to break down a complex question to a series of decomposed reasoning steps."
  - [section 4.4]: "CoT substantially enhances performance in English mathematical exams, including MATH, AQuA-RAT, and SAT-Math."
  - [corpus]: Weak. No direct corpus evidence supports CoT effectiveness; inference is from experimental results.
- Break condition: If models generate flawed intermediate reasoning, final answers may be misleading or incorrect.

### Mechanism 3
- Claim: Bilingual evaluation (English and Chinese) broadens the scope of model capability assessment across languages and cultures.
- Mechanism: By including exams in both languages, AGIEval tests model adaptability and generalization across linguistic contexts.
- Core assumption: Performance differences between languages reveal model biases or gaps in multilingual reasoning.
- Evidence anchors:
  - [section 3.2]: "Our benchmark is bilingual, containing both English and Chinese tasks, which allows for a more comprehensive assessment of language models across different languages and cultures."
  - [section 4.4]: "We also observed that the effects of CoT vary across different tasks, indicating that its impact on model performance is not uniform."
  - [corpus]: Weak. No corpus evidence on bilingual benchmarks; inference is from experimental design.
- Break condition: If model translation quality or cultural context differences introduce bias, results may not be comparable.

## Foundational Learning

- Concept: Standardized exam design and psychometrics
  - Why needed here: Understanding how exam validity, reliability, and difficulty are measured ensures the benchmark meaningfully reflects human capabilities.
  - Quick check question: How does the selection of high-participation exams (e.g., 12M Gaokao takers) strengthen the benchmark's representativeness?

- Concept: Prompt engineering and zero-shot learning
  - Why needed here: Effective prompts without task-specific examples test the model's innate reasoning and knowledge.
  - Quick check question: What is the difference between zero-shot and few-shot prompting, and why does few-shot show limited improvement in this study?

- Concept: Multi-dimensional evaluation metrics
  - Why needed here: Combining accuracy, Exact Match (EM), and F1 scores allows nuanced assessment of different question types.
  - Quick check question: Why might EM be preferred over F1 for fill-in-the-blank questions in this context?

## Architecture Onboarding

- Component map: Benchmark construction → Data collection (official exam sources) → Task filtering (objective questions only) → Model evaluation (zero-shot, few-shot, CoT) → Qualitative analysis (understanding, knowledge, reasoning, calculation) → Results reporting
- Critical path: Data collection → Task filtering → Model API integration → Prompt template creation → Evaluation execution → Result aggregation → Qualitative review
- Design tradeoffs: Objective-only questions simplify evaluation but exclude subjective reasoning; bilingual scope increases coverage but adds translation complexity; CoT improves reasoning but may introduce step-by-step errors
- Failure signatures: Low accuracy on CoT vs. non-CoT indicates flawed reasoning generation; large performance gaps between languages suggest multilingual bias; high variance across tasks signals inconsistent model generalization
- First 3 experiments:
  1. Run zero-shot CoT on SAT-Math to confirm CoT effectiveness on math reasoning
  2. Compare zero-shot vs. few-shot performance on LSAT-LR to measure adaptation limits
  3. Evaluate understanding capability on Gaokao-English using qualitative scoring

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively integrate external knowledge sources, such as formulas and domain-specific knowledge, into large foundation models to improve their performance in mathematical and knowledge-intensive tasks?
- Basis in paper: [explicit] The authors discuss the importance of enriching models with external knowledge sources and specialized domain knowledge bases to enhance their performance in mathematical and knowledge-intensive tasks.
- Why unresolved: While the paper acknowledges the significance of integrating external knowledge, it does not provide specific methods or techniques for achieving this integration effectively.
- What evidence would resolve it: Research and development of methods for incorporating structured knowledge repositories, mathematical and scientific concepts, and specialized domain knowledge into large foundation models through pre-training or knowledge-enhanced prompting techniques.

### Open Question 2
- Question: How can we improve the robustness of large foundation models' reasoning capabilities across different languages and contexts?
- Basis in paper: [explicit] The authors mention that the reasoning capabilities of models vary across different languages and that improving multilingual reasoning generalization is essential for their applicability in diverse real-world scenarios.
- Why unresolved: The paper does not provide specific strategies or techniques for enhancing the multilingual reasoning capabilities of foundation models.
- What evidence would resolve it: Development and evaluation of techniques that improve the multilingual reasoning generalization of foundation models, ensuring consistent reasoning performance across various languages and contexts.

### Open Question 3
- Question: How can we develop more robust and meaningful automatic evaluation metrics for human-centric tasks that accurately capture the models' understanding, knowledge, and reasoning abilities?
- Basis in paper: [explicit] The authors emphasize the need for developing more robust and meaningful automatic evaluation metrics to objectively assess large language models' performance on human-centric tasks.
- Why unresolved: The paper does not propose specific evaluation metrics or methods for capturing the nuances and complexities of real-world tasks in the assessment of model performance.
- What evidence would resolve it: Research and development of evaluation metrics that accurately measure the models' understanding, knowledge, and reasoning abilities, while considering the intricacies of human-centric tasks and real-world scenarios.

## Limitations

- The benchmark relies solely on objective exam questions, excluding subjective reasoning and open-ended responses that may better capture complex cognitive abilities
- The bilingual design introduces potential confounds from translation quality and cultural differences between English and Chinese exams
- The evaluation focuses primarily on accuracy metrics without extensive qualitative analysis of why models fail on certain tasks

## Confidence

- High confidence: The benchmark construction methodology and task selection process are well-documented and transparent
- Medium confidence: The performance comparisons between models are reliable, but the interpretation of what these results mean for general capabilities requires more nuance
- Low confidence: The transferability of exam performance to real-world applicability is assumed but not empirically validated

## Next Checks

1. **Test model generalization beyond exam contexts**: Evaluate the same models on real-world problem-solving tasks that require similar reasoning but in applied settings (e.g., technical troubleshooting, strategic planning) to validate whether exam performance translates to practical capabilities

2. **Analyze CoT reasoning quality**: Manually examine a sample of chain-of-thought outputs to identify patterns of logical errors, hallucinations, or circular reasoning that could explain performance variations and assess whether the reasoning steps actually contribute to correct answers

3. **Cross-cultural validation study**: Conduct a follow-up study with bilingual human experts to evaluate whether the Chinese and English exam questions are truly equivalent in difficulty and cognitive demand, controlling for cultural and linguistic factors that might bias model comparisons