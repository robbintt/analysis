---
ver: rpa2
title: 'Learning a Low-Rank Feature Representation: Achieving Better Trade-Off between
  Stability and Plasticity in Continual Learning'
arxiv_id: '2312.08740'
source_url: https://arxiv.org/abs/2312.08740
tags:
- tasks
- learning
- space
- network
- plasticity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the stability-plasticity trade-off in continual
  learning by proposing a novel algorithm called LRFR (Learning a Low-Rank Feature
  Representation). The key idea is to enhance plasticity without sacrificing stability
  by optimizing network parameters in the null space of the past tasks' feature representation
  matrix.
---

# Learning a Low-Rank Feature Representation: Achieving Better Trade-Off between Stability and Plasticity in Continual Learning

## Quick Facts
- arXiv ID: 2312.08740
- Source URL: https://arxiv.org/abs/2312.08740
- Reference count: 0
- LRFR consistently outperforms state-of-the-art methods on CIFAR-100 and TinyImageNet benchmarks

## Executive Summary
This paper addresses the stability-plasticity trade-off in continual learning by proposing LRFR (Learning a Low-Rank Feature Representation). The method enhances plasticity without sacrificing stability by optimizing network parameters in the null space of past tasks' feature representation matrices. By judiciously selecting a subset of neurons in each layer to learn a low-rank feature representation matrix, LRFR increases the null space dimension for subsequent tasks. Experimental results demonstrate consistent outperformance over state-of-the-art methods across all benchmarks.

## Method Summary
LRFR is a three-stage training algorithm for continual learning. First, it updates the past tasks' feature representation matrix. Second, it pretrains with a sparsity penalty on batch normalization parameters to select neurons. Third, it performs official training with the selected neurons disabled. The key innovation is learning a low-rank past tasks' feature representation matrix, which increases the null space dimension for subsequent tasks while maintaining stability by ensuring parameter updates lie in the null space of the actual feature representation matrix.

## Key Results
- On 10-split CIFAR-100: 81.30% ACC with 0.11% BWT
- Outperforms NSCL (73.77% ACC, -1.60% BWT) and AdNS (77.21% ACC, -2.32% BWT)
- Consistently achieves higher average accuracy and backward transfer across all benchmarks
- Demonstrates effectiveness of learned low-rank past tasks' feature representation matrices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LRFR achieves better stability-plasticity trade-off by learning a low-rank past tasks' feature representation matrix
- Mechanism: Selecting a subset of neurons reduces the rank of the feature representation matrix, increasing null space dimension for subsequent tasks while maintaining stability through updates in the null space
- Core assumption: Lower rank directly increases null space dimension, enhancing plasticity without sacrificing stability
- Evidence anchors:
  - [abstract] "judiciously select only a subset of neurons in each layer...to learn the past tasks' feature representation matrix in low-rank"
  - [section 3.1] "Equation (3) suggests that a reduced rank of F_bar^l_t-1 enhances the plasticity"
  - [corpus] Found related papers discussing low-rank adaptation and stability-plasticity trade-offs

### Mechanism 2
- Claim: The method maintains stability by ensuring parameter updates lie in the null space of the actual past tasks' feature representation matrix
- Mechanism: Unlike methods using low-rank approximations to expand gradient projection space, LRFR increases null space dimension of the actual feature representation matrix by learning it in low-rank form from the beginning
- Core assumption: Stability condition requires parameter updates to lie in null space, which can be satisfied by learning low-rank representation directly
- Evidence anchors:
  - [section 2.2] "If at each training step of task t, ∆W_l^t,s lies in the null space of the past tasks' feature representation matrix"
  - [section 3.2] "Diverging from existing methods that utilize low-rank approximations to expand the gradient projection space outside the null space"
  - [corpus] Related papers discuss null space projection methods

### Mechanism 3
- Claim: The method enhances plasticity by judiciously selecting neurons through a sparsity penalty on batch normalization parameters
- Mechanism: An extra penalty term on batch normalization layer parameters enforces sparsity during pretraining, indicating which neurons to disable based on lottery ticket hypothesis
- Core assumption: Sub-networks with fewer neurons can match or surpass performance of original larger network, allowing safe neuron disabling
- Evidence anchors:
  - [section 3.2] "Leveraging the lottery ticket hypothesis, which indicates sub-networks with fewer neurons can match or surpass the performance of the original larger network"
  - [section 3.2] "Minimizing (5) during pretraining enforces sparsity in the scale parameters of batch normalization layers"
  - [corpus] Found related papers discussing lottery ticket hypothesis and sparse networks

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and low-rank matrix approximation
  - Why needed here: The method relies on understanding how rank reduction affects matrix properties like null space dimension
  - Quick check question: If a matrix has rank r and dimension n×n, what is the dimension of its null space?

- Concept: Null space and range space of matrices
  - Why needed here: The stability condition requires parameter updates to lie in the null space of the past tasks' feature representation matrix
  - Quick check question: What is the relationship between the rank of a matrix and the dimension of its null space?

- Concept: Catastrophic forgetting and continual learning
  - Why needed here: The paper addresses the stability-plasticity trade-off in continual learning, which is fundamentally about preventing catastrophic forgetting
  - Quick check question: What is catastrophic forgetting, and why does it occur in neural networks trained on sequential tasks?

## Architecture Onboarding

- Component map: ResNet-18 backbone (shared across tasks) -> Separate classifiers (per task) -> Separate batch normalization parameters (per task)
- Critical path: Three-stage training process for each task: 1) Update past tasks' feature representation matrix, 2) Pretrain with sparsity penalty to select neurons, 3) Official training with disabled neurons
- Design tradeoffs: Trading off between rank reduction (for plasticity) and information retention (for stability)
- Failure signatures: 1) Insufficient rank reduction leading to poor plasticity, 2) Excessive rank reduction causing performance degradation, 3) Poor neuron selection resulting in loss of task-relevant information
- First 3 experiments:
  1. Implement the three-stage training process on a simple dataset (e.g., split MNIST) to verify basic functionality
  2. Compare null space dimension and rank of feature representation matrix with and without LRFR method
  3. Evaluate stability-plasticity trade-off on a multi-task benchmark

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the rank of the past tasks' feature representation matrix relate to catastrophic forgetting in continual learning?
- Basis in paper: [explicit] The paper states that a lower rank enhances plasticity and that a larger null space dimension allows for parameter updates in a higher-dimensional vector space
- Why unresolved: The paper demonstrates the relationship between rank and plasticity but does not explicitly quantify the impact of different rank levels on catastrophic forgetting
- What evidence would resolve it: Experimental results showing correlation between rank of feature representation matrix and extent of catastrophic forgetting across various continual learning tasks

### Open Question 2
- Question: What is the optimal percentage of neurons to disable for maximizing plasticity while maintaining stability?
- Basis in paper: [inferred] The paper mentions disabling k = 50% neurons but does not explore the impact of different percentages on performance
- Why unresolved: The paper uses a fixed percentage (50%) without exploring effects of varying this parameter
- What evidence would resolve it: A sensitivity analysis showing how different percentages of disabled neurons affect the stability-plasticity trade-off

### Open Question 3
- Question: How does the LRFR algorithm perform compared to other methods on more diverse and complex datasets?
- Basis in paper: [explicit] The paper evaluates LRFR on CIFAR-100 and TinyImageNet datasets
- Why unresolved: Experiments are limited to specific datasets, unclear how algorithm generalizes to other types of data
- What evidence would resolve it: Comparative studies on a broader range of datasets with different characteristics and complexities

## Limitations
- Limited empirical validation of the stability condition through direct measurement of parameter updates' alignment with the null space
- No ablation studies examining the trade-off between rank reduction and performance
- Fixed neuron disabling percentage (50%) without exploring sensitivity to this hyperparameter

## Confidence
- **High confidence**: Experimental results showing LRFR outperforming baseline methods on CIFAR-100 and TinyImageNet benchmarks
- **Medium confidence**: Theoretical framework connecting low-rank feature representations to null space dimension and plasticity
- **Low confidence**: Practical effectiveness of batch normalization parameter sparsity as a reliable neuron selection criterion across diverse tasks

## Next Checks
1. **Null Space Validation**: Measure actual alignment of parameter updates with the null space of past tasks' feature representation matrices during training to verify the stability condition is being maintained
2. **Neuron Selection Robustness**: Conduct sensitivity analysis on the neuron selection process by varying initialization seeds and examining consistency of selected neurons across runs
3. **Rank-Plasticity Trade-off**: Perform systematic ablation studies varying the rank reduction percentage to quantify the relationship between null space dimension and both forward/backward transfer performance