---
ver: rpa2
title: Harnessing LLM to Attack LLM-Guarded Text-to-Image Models
arxiv_id: '2312.07130'
source_url: https://arxiv.org/abs/2312.07130
tags:
- sensitive
- prompt
- prompts
- text
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to generate adversarial prompts that
  bypass safety filters in text-to-image models like DALL-E 3. The core idea is to
  use LLMs to break down a sensitive prompt into harmless component descriptions,
  then reassemble them into an adversarial prompt.
---

# Harnessing LLM to Attack LLM-Guarded Text-to-Image Models
## Quick Facts
- arXiv ID: 2312.07130
- Source URL: https://arxiv.org/abs/2312.07130
- Authors: 
- Reference count: 40
- Key outcome: LLM-powered adversarial prompts achieve 76.7% bypass rate for DALL-E 3's safety filters while maintaining semantic similarity to original sensitive content

## Executive Summary
This paper introduces a novel attack methodology that uses LLMs to generate adversarial prompts capable of bypassing safety filters in text-to-image models like DALL-E 3. The approach leverages LLM capabilities to decompose sensitive prompts into harmless component descriptions, then reassembles them into adversarial prompts that maintain semantic similarity while evading detection. The method achieves high bypass rates (76.7% one-time, 98% re-use) across multiple LLM backbones while remaining cost-effective compared to previous approaches.

## Method Summary
The attack uses LLM-powered Divide-and-Conquer methodology where LLMs decompose sensitive prompts into individual visual components (characters, actions, props, scenes) and rephrase each into harmless language. These components are then reassembled into adversarial prompts that bypass safety filters while preserving semantic similarity. The approach employs different helper prompt strategies (All-in-One-Go and Step-wise) depending on prompt complexity, and evaluates effectiveness across six LLM backbones using metrics like bypass rate, semantic similarity (CLIP embeddings), and token cost.

## Key Results
- Achieved 76.7% one-time bypass rate and 98% re-use bypass rate for DALL-E 3 safety filters
- Maintained semantic similarity to original sensitive prompts while evading detection
- Demonstrated cost-effectiveness with as low as $0.035 per attack using GPT-4
- Showed consistent effectiveness across multiple LLM backbones (GPT-4, GPT-3.5-turbo, smaller models)

## Why This Works (Mechanism)
### Mechanism 1
- LLMs can decompose sensitive prompts into semantically equivalent non-sensitive descriptions that bypass safety filters
- Safety filters operate on semantic meaning of full prompts; harmless individual components don't trigger filters when described separately
- Core assumption: filters analyze individual components rather than holistic semantic intent

### Mechanism 2
- Cost-effectiveness of using LLMs enables large-scale adversarial prompt generation
- API pricing for LLMs is low enough to enable mass generation without prohibitive cost
- Core assumption: token usage per attack remains reasonable and API pricing stays accessible

### Mechanism 3
- Re-use attack capability extends impact of successful adversarial prompts over multiple generations
- Once adversarial prompt bypasses safety filter, it continues to bypass in subsequent uses
- Core assumption: text-to-image model maintains semantic consistency across multiple generations

## Foundational Learning
- **Text-to-image model safety filters**
  - Why needed: Understanding safety filter operation is crucial for designing effective bypass methods
  - Quick check: What are the two criteria an adversarial prompt must fulfill to be effective against a safety filter?

- **Large Language Model capabilities for text transformation**
  - Why needed: Attack relies on LLMs to decompose and rephrase sensitive prompts
  - Quick check: Why does the attack use different helper prompt types for different sensitive image categories?

- **Semantic similarity measurement (CLIP embeddings)**
  - Why needed: Evaluating attack success requires measuring semantic similarity between generated and original images
  - Quick check: How does the paper use CLIP embeddings to establish semantic similarity baseline?

## Architecture Onboarding
- **Component map**: Sensitive prompt dataset -> LLM backbones -> Helper prompts -> Text-to-image model -> Evaluation metrics
- **Critical path**: Input prompt → LLM processing with helper prompts → Adversarial prompt generation → Text-to-image model → Image generation → Evaluation
- **Design tradeoffs**: More capable LLMs yield higher bypass rates but at higher cost; simpler prompts are cheaper but less effective for complex images
- **Failure signatures**: Low bypass rate indicates decomposition failure; low semantic similarity indicates lost original intent; high cost indicates inefficiency
- **First 3 experiments**: 1) Test All-in-One-Go on simple character copyright violation, 2) Test Step-wise on complex violent scene, 3) Compare GPT-4 vs Qwen-14B bypass rates and costs

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How would DACA perform against complex multi-theme sensitive prompts combining different sensitive elements?
- Basis: Paper evaluates straightforward single-category prompts without exploring compound scenarios
- Why unresolved: Evaluation focuses on single-category prompts without testing complex multi-theme combinations
- Evidence needed: Testing on prompts combining multiple sensitive themes with corresponding bypass rate and semantic coherence measurements

### Open Question 2
- Question: What is the exact mechanism by which DALL-E 3's safety filter detects sensitive prompts?
- Basis: Paper acknowledges safety filter uses GPT-4 but doesn't detail specific detection mechanisms
- Why unresolved: Safety filter described as black box without exploring underlying detection mechanisms
- Evidence needed: Detailed analysis of safety filter decision boundaries and prompt modification effects

### Open Question 3
- Question: How does semantic similarity threshold affect bypass rate and what is optimal balance?
- Basis: Paper measures semantic similarity but doesn't systematically explore different preservation levels
- Why unresolved: Relationship between semantic similarity and bypass rate not explicitly studied or optimized
- Evidence needed: Experiments varying semantic preservation degrees and measuring corresponding bypass rates

## Limitations
- Attack methodology assumes safety filters analyze individual components rather than holistic semantic intent
- Evaluation limited to DALL-E 3 without testing other text-to-image models or safety architectures
- Re-use attacks may become unsustainable as safety filters could track and block known adversarial prompts
- Relatively small dataset of 40 prompts may not capture full diversity of potential sensitive content

## Confidence
- **High Confidence**: Bypass rates (76.7% one-time, 98% re-use) - clearly specified methodology with consistent results across multiple LLM backbones
- **Medium Confidence**: Cost-effectiveness - token-based calculations provided but real-world pricing can fluctuate
- **Low Confidence**: Generalizability - limited testing to DALL-E 3 without evaluation on other models or safety filter architectures

## Next Checks
1. Apply attack methodology to other text-to-image models (Midjourney, Stable Diffusion) to verify consistent bypass rates
2. Implement adaptive safety filter that tracks and blocks previously successful adversarial prompts, then measure attack failure rate
3. Expand sensitive prompt dataset to 400+ prompts across additional categories to assess effectiveness with greater semantic diversity