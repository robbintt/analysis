---
ver: rpa2
title: Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models
  for Learning Conceptual Spaces
arxiv_id: '2310.05481'
source_url: https://arxiv.org/abs/2310.05481
tags:
- conceptual
- spaces
- which
- mass
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates whether large language models can learn
  meaningful conceptual space representations, which are cognitive-linguistic frameworks
  for representing concepts in terms of primitive perceptual features. The authors
  evaluate this in two domains: taste (with 590 food items rated across 6 dimensions)
  and physical properties (mass, height, size with various datasets).'
---

# Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces

## Quick Facts
- arXiv ID: 2310.05481
- Source URL: https://arxiv.org/abs/2310.05481
- Reference count: 10
- Fine-tuned 350M parameter DeBERTa model outperforms 175B parameter GPT-3 davinci for conceptual space learning

## Executive Summary
This paper investigates whether large language models can learn meaningful conceptual space representations - cognitive-linguistic frameworks for representing concepts through primitive perceptual features. The authors evaluate this capability across two domains: taste (590 food items across 6 dimensions) and physical properties (mass, height, size with various datasets). Surprisingly, their experiments show that fine-tuned smaller models (DeBERTa, 350M parameters) consistently outperform much larger LLMs (GPT-3 davinci, 175B parameters) across most tasks, with DeBERTa achieving 69.1% Spearman correlation for sweetness prediction compared to GPT-3's 55.0%. The paper demonstrates that a bi-encoder trained on ChatGPT-generated data performs competitively, suggesting efficient alternatives to massive LLMs for learning conceptual spaces.

## Method Summary
The authors evaluated conceptual space learning using three approaches: (1) multiple GPT-3 models (350M to 175B parameters) using conditional probabilities and perplexity scoring with carefully crafted prompts, (2) DeBERTa-v3-large fine-tuned on McRae and CSLB datasets, and (3) a BERT-large bi-encoder trained on Microsoft Concept Graph, GenericsKB, and ChatGPT-generated data. They used Spearman rank correlation to evaluate taste predictions and accuracy for pairwise judgments in physical domains. The key innovation was leveraging ChatGPT to construct a high-quality dataset of 109K (concept, property) pairs, filtering for properties shared by multiple concepts to ensure meaningful representations.

## Key Results
- Fine-tuned DeBERTa (350M parameters) achieved 69.1% Spearman correlation for sweetness prediction vs GPT-3 davinci's 55.0%
- BERT-large bi-encoder trained on ChatGPT-generated data performed competitively with larger models
- GPT-4 achieved 83.7% accuracy on mass predictions while DeBERTa failed completely (negative correlation)
- Performance varied significantly across domains, with DeBERTa excelling in taste but failing on mass properties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can learn meaningful conceptual space representations when trained on high-quality, diverse concept-property pairs.
- Mechanism: The models map concepts into a semantic space where proximity corresponds to shared perceptual features, enabling ranking and comparison across dimensions like sweetness or mass.
- Core assumption: The training data is sufficiently rich and diverse to capture the underlying perceptual structure without requiring explicit grounding in sensory input.
- Evidence anchors:
  - [abstract]: "Our experiments show that LLMs can indeed be used for learning meaningful representations to some extent."
  - [section]: "We used ChatGPT to construct a dataset of 109K (concept,property) pairs... The key to obtain high-quality examples was to ask the model to suggest properties that are shared by several concepts..."
  - [corpus]: Weak - No direct citation to similar conceptual space work in the neighbor papers; most are on cache eviction or kernel design.
- Break condition: If the dataset lacks coverage of rare or nuanced properties, the model cannot generalize beyond the training distribution, leading to poor performance on edge cases.

### Mechanism 2
- Claim: Fine-tuned smaller models (DeBERTa, BERT bi-encoders) can outperform much larger LLMs because they leverage targeted, high-quality training data.
- Mechanism: By fine-tuning on curated datasets (McRae, CSLB, ChatGPT-generated pairs), the models internalize a structured representation of concepts that generalizes better to ranking tasks than raw LLM outputs.
- Core assumption: The curated datasets capture enough commonsense knowledge to compensate for the reduced model size and lack of general pretraining breadth.
- Evidence anchors:
  - [abstract]: "fine-tuned models of the BERT family are able to match or even outperform the largest GPT-3 model, despite being 2 to 3 orders of magnitude smaller."
  - [section]: "Surprisingly, we find that the DeBERTa model outperforms davinci in most cases, and often by a substantial margin."
  - [corpus]: Weak - No neighbor papers discuss fine-tuning or conceptual spaces.
- Break condition: If the curated datasets are noisy or incomplete, the fine-tuned models will overfit and underperform relative to LLMs that have broader pretraining.

### Mechanism 3
- Claim: The effectiveness of LLMs for conceptual spaces depends critically on prompt engineering and the choice of conditional probability versus perplexity-based scoring.
- Mechanism: Well-crafted prompts that contextualize the property (e.g., "it is known that [food item] tastes [sweet]") improve the signal-to-noise ratio in LLM outputs, enabling more accurate rankings.
- Core assumption: The language model's internal probability estimates are sensitive to prompt phrasing and can be leveraged for meaningful comparisons.
- Evidence anchors:
  - [abstract]: "For the LMs, we experimented with two prompts... The results in Table 1 show a strong correlation, for the GPT-3 models, between model size and performance..."
  - [section]: "Prompt 1 is of the form '[food item] tastes [sweet]', e.g. 'apple tastes sweet'. Prompt 2 is of the form 'it is known that [food item] tastes [sweet]'."
  - [corpus]: Weak - No neighbor papers address prompt engineering or conditional probability usage.
- Break condition: If prompts are ambiguous or misaligned with the task, conditional probabilities become unreliable and rankings degrade to near-random performance.

## Foundational Learning

- Concept: Spearman rank correlation
  - Why needed here: The paper evaluates how well model-generated rankings align with human-annotated orderings of food items or objects across perceptual dimensions.
  - Quick check question: If a model ranks 5 items as [A,B,C,D,E] and the ground truth is [A,C,B,E,D], what is the Spearman correlation? (Answer: 0.9)
- Concept: Conditional probability extraction from LLMs
  - Why needed here: The experiments rely on retrieving P(word|context) to infer how strongly a concept exhibits a given property.
  - Quick check question: What is the difference between using conditional probability vs. perplexity for ranking? (Answer: Conditional probability measures the likelihood of a property given a concept; perplexity measures overall sentence fit, which can be less discriminative for property presence.)
- Concept: Fine-tuning vs. zero-shot prompting
  - Why needed here: The paper compares zero-shot LLM usage to fine-tuned BERT-family models to determine the optimal strategy for learning conceptual spaces.
  - Quick check question: Why might a fine-tuned 350M parameter model outperform a 175B parameter zero-shot LLM? (Answer: Targeted training data can encode task-specific structure more effectively than general pretraining.)

## Architecture Onboarding

- Component map: Data collection -> Prompt engineering -> Model inference -> Evaluation (Spearman / accuracy)
- Critical path:
  1. Generate or load concept-property pairs
  2. Construct prompts for each model type
  3. Extract conditional probabilities or perplexity scores
  4. Rank items and compute Spearman correlation
- Design tradeoffs:
  - Model size vs. data quality: Larger LLMs can generalize better but are less efficient; smaller fine-tuned models excel with curated data.
  - Prompt complexity vs. reliability: Complex prompts may improve signal but increase variance across runs.
  - Data coverage vs. noise: More diverse pairs improve robustness but risk introducing irrelevant properties.
- Failure signatures:
  - Low Spearman correlation with high variance across prompts → prompt sensitivity
  - Consistently poor performance on a property → dataset bias or missing property representation
  - Negative correlation → inverted scoring or data leakage
- First 3 experiments:
  1. Run DeBERTa on the taste dataset with filtered training to reproduce Table 5 results.
  2. Compare conditional probability vs. perplexity scoring on the mass dataset using GPT-3 babbage.
  3. Generate a small ChatGPT-based concept-property dataset and train the bi-encoder to match or exceed Table 1 results.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal prompt format for eliciting accurate conceptual space representations from LLMs across different perceptual domains?
- Basis in paper: [explicit] The paper extensively discusses how different prompts (e.g., "[food item] tastes [property]" vs "it is known that [food item] tastes [property]") yielded different results, and notes that "previous work has found that performance may dramatically differ depending on the prompt which is used."
- Why unresolved: The paper only tested a limited number of prompt variations and found that even with reasonable choices, the performance varied significantly. The authors acknowledge they "attempted to make reasonable choices when deciding on the considered prompts" but note this "is not a guarantee that our prompts are close to being optimal."
- What evidence would resolve it: Systematic ablation studies testing numerous prompt variations across multiple domains, measuring both accuracy and consistency of conceptual space representations.

### Open Question 2
- Question: How does the performance gap between fine-tuned models and LLMs scale with model size and training data quality across different perceptual domains?
- Basis in paper: [explicit] The paper shows that "fine-tuned models of the BERT family are able to match or even outperform the largest GPT-3 model, despite being 2 to 3 orders of magnitude smaller" for taste predictions, but DeBERTa failed completely on mass predictions while GPT-4 performed exceptionally well.
- Why unresolved: The paper only tested a limited set of domains (taste, mass, height, size) with specific datasets, and the performance relationship between fine-tuned models and LLMs appears inconsistent across these domains. The authors note this creates uncertainty about when each approach is superior.
- What evidence would resolve it: Comparative experiments across a broader range of perceptual domains (texture, sound, temperature, etc.) with varying dataset sizes and qualities, measuring performance relative to model size for both approaches.

### Open Question 3
- Question: What is the theoretical limit of how much perceptual grounding can be learned from text alone, and at what point do reporting biases fundamentally constrain LLM performance?
- Basis in paper: [explicit] The authors position their work in "the ongoing debate about the extent to which Language Models (LMs) can learn perceptually grounded representations" and note that "reporting bias" affects color predictions, while their taste domain experiments showed GPT-3 davinci failed to identify sweetness in common foods while overestimating it in cheeses and vegetables.
- Why unresolved: While the paper demonstrates that LLMs can learn some meaningful representations, it doesn't establish clear boundaries for what cannot be learned from text alone. The authors' finding that fine-tuned models sometimes outperform much larger LLMs suggests limitations in text-based learning that aren't fully characterized.
- What evidence would resolve it: Comparative studies measuring LLM performance against ground truth data from domains with varying degrees of reporting bias, establishing correlation between reporting bias levels and prediction accuracy.

## Limitations

- Performance inconsistency across domains suggests the approach may not generalize well to all perceptual properties
- Reliance on proprietary GPT-3 models and custom-trained baselines limits reproducibility
- Potential circularity in using ChatGPT-generated data for training while evaluating conceptual space learning capabilities

## Confidence

- **High confidence**: The experimental methodology and statistical evaluation (Spearman correlation) are sound and well-documented
- **Medium confidence**: The comparative performance results between different model families, as these are directly measurable from the experiments
- **Low confidence**: The generalizability of findings to other conceptual domains beyond taste, mass, height, and size, as the paper only explores these four specific domains

## Next Checks

1. **Cross-dataset validation**: Test the best-performing models (DeBERTa and bi-encoder) on a completely different conceptual domain (e.g., texture, temperature, or emotional valence) to assess domain transfer
2. **Human evaluation study**: Conduct a small-scale human judgment study comparing LLM-generated rankings with human intuitions for the same concept-property pairs to validate the Spearman correlation results
3. **Ablation on data quality**: Systematically vary the quality and diversity of the concept-property training data to quantify the relationship between data curation effort and model performance improvements