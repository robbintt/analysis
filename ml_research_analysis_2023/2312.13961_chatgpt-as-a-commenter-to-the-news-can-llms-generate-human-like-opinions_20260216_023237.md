---
ver: rpa2
title: 'ChatGPT as a commenter to the news: can LLMs generate human-like opinions?'
arxiv_id: '2312.13961'
source_url: https://arxiv.org/abs/2312.13961
tags:
- comments
- gpt-3
- article
- human
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how well GPT-3.5 can generate human-like comments
  on Dutch news articles by comparing machine-generated text to human comments. Using
  a fine-tuned BERT model, the authors find that GPT-3.5 outputs are easily distinguishable
  from human-written comments, with F1 scores above 90% across multiple prompting
  techniques (zero-shot, few-shot, context-based).
---

# ChatGPT as a commenter to the news: can LLMs generate human-like opinions?

## Quick Facts
- arXiv ID: 2312.13961
- Source URL: https://arxiv.org/abs/2312.13961
- Reference count: 18
- Primary result: GPT-3.5-generated Dutch news comments are easily distinguishable from human-written ones, with F1 scores above 90% across prompting techniques.

## Executive Summary
This paper investigates whether GPT-3.5 can generate human-like comments on Dutch news articles. Using a fine-tuned Dutch BERT model (RobBERT), the authors compare machine-generated text to human comments across three prompting techniques: zero-shot, few-shot, and context-based. Despite GPT-3.5's fluency, the generated comments consistently show lower lexical diversity and lack emotional tone compared to human-written comments. The study concludes that GPT-3.5 struggles to replicate authentic, opinionated human commentary, though future work with larger models like GPT-4 may yield different results.

## Method Summary
The study collected 10 Dutch news articles with at least 100 human comments each from NU.nl. For each article, 100 comments were generated using GPT-3.5 with three prompting techniques and two personas. A fine-tuned RobBERT-v2-dutch-base model was trained using 10-fold cross-validation to classify comments as human or AI-generated. Lexical diversity was measured using Corrected Type-Token Ratio (CTTR), and SHAP analysis was used to interpret misclassifications. The evaluation focused on F1 score, precision, and recall metrics across different prompt settings.

## Key Results
- GPT-3.5-generated comments achieved F1 scores above 90% in classification as non-human, regardless of prompting technique
- Human comments consistently showed higher CTTR values, indicating greater lexical diversity
- Generated comments exhibited formal language and lacked the emotional tone and slang characteristic of human opinions

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning RobBERT with balanced human/GPT-3.5 comment pairs enables high F1 classification because the Dutch language representation is already robust and domain-adapted. RobBERT was pre-trained on Dutch text using RoBERTa architecture and then fine-tuned with balanced train/test splits per article, which prevents overfitting to article-specific style while preserving the general lexical patterns that distinguish human from machine comments.

### Mechanism 2
Zero-shot, few-shot, and context prompts yield similarly high lexical diversity deficits because GPT-3.5's output distribution is inherently constrained, regardless of input format. Even with few-shot examples or article context, the model samples from a fixed token distribution that favors high-frequency, formal phrasing, resulting in consistently lower Corrected Type-Token Ratio (CTTR) compared to human comments.

### Mechanism 3
GPT-3.5 persona injection creates superficially personalized tone but fails to generate emotionally charged opinions because the persona text is too brief and generic. Short, descriptive persona prompts only set a shallow voice, which the model partially mimics but does not integrate deeply into opinion content, resulting in formal, less opinionated text.

## Foundational Learning

- **Tokenization and token limits**: Why needed here - GPT-3.5 has a 4,096 token limit, which constrains how much article context or how many few-shot examples can be included, affecting output quality. Quick check: If a prompt uses 500 tokens for context and 100 tokens for examples, how many tokens remain for the model to generate a comment?

- **Type-Token Ratio (TTR) vs. Corrected TTR (CTTR)**: Why needed here - TTR alone inflates with longer texts; CTTR normalizes by text length, making cross-sample lexical diversity comparisons valid. Quick check: If a text has 50 unique words out of 200 total, what is its CTTR?

- **10-fold cross-validation**: Why needed here - Ensures the classifier is evaluated on unseen articles, preventing overfitting to article-specific language quirks. Quick check: If you have 10 articles, how many times is each article used as the test set in 10-fold CV?

## Architecture Onboarding

- **Component map**: HTML scraper -> BeautifulSoup parser -> manual correction -> CSV writer -> GPT-3.5 API (gpt-3.5-turbo) with persona prompts -> HuggingFace Trainer with RobBERT-v2 -> 10-fold CV evaluation -> CTTR calculation -> SHAP analysis

- **Critical path**: 1. Collect 10 articles, each â‰¥100 human comments; 2. Generate 100 GPT comments per article, per prompt type, per persona; 3. Balance datasets, split by article for 10-fold CV; 4. Fine-tune RobBERT, record metrics; 5. Compute CTTR, analyze misclassifications with SHAP

- **Design tradeoffs**: Using 4-shot examples limits context but keeps within token budget; more examples would require batching or truncating articles. Balanced sampling per article avoids bias but reduces total data; unbalanced data might improve overall F1 but hurt article-level generalization. Manual HTML correction introduces noise but is necessary due to inconsistent site structure.

- **Failure signatures**: Low F1 variance across settings suggests the model has found a stable distinction; large variance would indicate overfitting to prompt style. CTTR differences close to zero would imply no lexical diversity gap; in practice, human CTTR > GPT-3.5 CTTR. SHAP showing random token importance indicates poor feature learning; consistent patterns (e.g., punctuation, informal words) indicate robust classification.

- **First 3 experiments**: 1. Run each prompt type with persona 1 only, measure F1 per article to check if personas matter; 2. Double the number of few-shot examples (if token budget allows) and compare CTTR and F1 to baseline; 3. Replace GPT-3.5 with GPT-4 (25k token context) on a subset of articles to see if lexical diversity and F1 improve.

## Open Questions the Paper Calls Out

### Open Question 1
How does the human-likeness of GPT-3.5-generated comments compare to those generated by GPT-4 or other larger language models? The authors note that future work may explore larger models like GPT-4 or open-source alternatives with more context capacity, suggesting potential improvements in human-likeness. Comparative studies using GPT-4 or other LLMs would resolve this question.

### Open Question 2
Does the pre-training date of GPT-3.5 influence its ability to generate human-like comments on topics published before and after its training cutoff? The authors observed that article 1, published before GPT-3.5's pre-training cutoff, resulted in the lowest F1 scores, suggesting familiarity with the topic may affect human-likeness. A larger dataset of articles published both before and after GPT-3.5's pre-training cutoff would enable generalization of these findings.

### Open Question 3
How does increasing the diversity and detail of personas affect the human-likeness of generated comments? The authors note that the generated personas were narrowly described, limiting the model's understanding of beliefs and motivations, which may impact the human-likeness of comments. Experiments with a larger number of diverse and detailed personas would test this relationship.

## Limitations
- Results are specific to Dutch language and may not generalize to other languages or larger models like GPT-4
- GPT-3.5's 4,096 token limit constrained article context inclusion, potentially affecting output quality
- Manual HTML correction introduced potential bias due to inconsistent NU.nl comment structure

## Confidence
**High Confidence**: Classification results showing F1 scores above 90% for distinguishing GPT-3.5 from human comments are highly reliable, given consistent performance across multiple prompting techniques and robust Dutch language representation in RobBERT.

**Medium Confidence**: Conclusion that GPT-3.5 struggles to replicate authentic, opinionated human commentary is supported by lexical diversity metrics and qualitative analysis, but may not hold for larger models with greater context capacity.

**Low Confidence**: Assertion that persona injection creates only superficial personalization requires further validation, as the study used relatively brief persona descriptions without testing longer, more detailed persona narratives.

## Next Checks
1. Cross-linguistic validation: Repeat experiment with English news articles using BERT-based models trained on English corpora to determine if human-machine distinction holds across languages.

2. Model scaling experiment: Compare GPT-3.5 results against GPT-4 outputs on the same Dutch news articles, measuring changes in F1 classification scores, CTTR values, and qualitative differences in emotional engagement.

3. Sampling strategy analysis: Systematically vary temperature and top-p parameters during GPT-3.5 generation to determine relationship between sampling strategy and lexical diversity, testing whether more stochastic sampling produces outputs closer to human CTTR values.