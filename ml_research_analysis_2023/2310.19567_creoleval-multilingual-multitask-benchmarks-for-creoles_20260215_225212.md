---
ver: rpa2
title: 'CreoleVal: Multilingual Multitask Benchmarks for Creoles'
arxiv_id: '2310.19567'
source_url: https://arxiv.org/abs/2310.19567
tags:
- creole
- creoles
- languages
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CreoleVal addresses the lack of benchmark datasets for Creole languages
  by introducing new development data for three NLP tasks and aggregating existing
  datasets for eight tasks across up to 28 Creole languages. The work presents zero-shot
  transfer learning baselines using multilingual models like mBERT, XLM-R, mT5, and
  mBART-50 to assess the viability of transfer learning from related higher-resourced
  languages.
---

# CreoleVal: Multilingual Multitask Benchmarks for Creoles

## Quick Facts
- arXiv ID: 2310.19567
- Source URL: https://arxiv.org/abs/2310.19567
- Reference count: 23
- Key outcome: CreoleVal introduces new datasets for 3 tasks and aggregates existing data for 8 tasks across up to 28 Creole languages, establishing zero-shot transfer learning baselines using multilingual models to assess viability of transfer learning from related higher-resourced languages.

## Executive Summary
CreoleVal addresses the critical gap in benchmark datasets for Creole languages by providing new development data for machine comprehension, relation classification, and machine translation tasks, while aggregating existing datasets for eight NLP tasks across up to 28 Creole languages. The work establishes zero-shot transfer learning baselines using multilingual models like mBERT, XLM-R, mT5, and mBART-50 to evaluate the potential of leveraging genealogical ties between Creoles and higher-resourced ancestor languages. Experiments show varied performance across tasks and languages, highlighting both the promise and limitations of transfer learning approaches for these under-resourced languages.

## Method Summary
CreoleVal creates benchmark datasets for eight NLP tasks across up to 28 Creole languages, including new datasets for machine comprehension (MCTest160 in Haitian and Mauritian Creole), relation classification for five Creoles, and machine translation corpora for 26 Creoles (Bible-based) and Haitian Creole (MIT-Haiti, education domain). The method employs zero-shot transfer learning using mBERT, XLM-R, mT5, and mBART-50 models, fine-tuning on English data and evaluating on Creole test sets. For translation tasks, both mBART-50 and mBART-50-MT models are evaluated, with the latter showing improvements when fine-tuned on CreoleM2M corpus. Cultural adaptation is implemented through localized translations that replace culturally specific references with locally relevant equivalents.

## Key Results
- mBERT slightly outperforms XLM-R on machine comprehension and relation classification tasks for Creoles
- Fine-tuning mBART-50-MT improves translation quality by up to 19.2 BLEU and 17.3 chrF for Creole-to-English translation compared to scratch models
- Bible-based translation corpora provide reasonable seed data within domain but show limited cross-domain generalization to education data
- Transfer learning effectiveness varies significantly across language pairs and tasks, with some Creoles showing better performance than others

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning from genealogically related languages can bootstrap performance on Creole languages
- Mechanism: Multilingual LMs pre-trained on ancestor languages (e.g., English, French, Spanish) carry lexical and syntactic knowledge that can be leveraged via zero-shot transfer to structurally similar but lower-resourced Creole languages
- Core assumption: Shared subword overlap and typological similarity between Creoles and their ancestor languages is sufficient for transfer without additional fine-tuning
- Evidence anchors: [abstract] "While the genealogical ties between Creoles and a number of highly-resourced languages imply a significant potential for transfer learning, this potential is hampered due to this lack of annotated data."
- Break condition: Transfer fails if subword vocabularies diverge or if syntactic/semantic differences between ancestor and Creole languages are too large for zero-shot adaptation

### Mechanism 2
- Claim: Domain-specific translation corpora (e.g., Bible, educational texts) can provide strong seed data for MT even if limited in size
- Mechanism: Parallel corpora from a fixed domain (Bible) combined with fine-tuning on larger multilingual MT models (mBART-50-MT) produce usable translation quality for multiple Creole languages
- Core assumption: The fixed domain provides sufficient lexical and phrase-level correspondences that can generalize within the domain and, to a lesser extent, to related domains
- Evidence anchors: [section 4.1.1] "fine-tuning the mBART-50-MT model leads to significant improvements in translation quality by up to 19.2 BLEU and 17.3 chrF for Creole to English translation."
- Break condition: Cross-domain transfer fails when source domain (religious) has minimal lexical overlap with target domain (e.g., education)

### Mechanism 3
- Claim: Creating localized translations (culturally adapted) improves comprehension task performance relative to direct translations
- Mechanism: Adapting dataset content to local cultural references (e.g., changing "ice cream truck" to "machann fresko") increases relevance and ease of understanding for target language speakers
- Core assumption: Cultural adaptation preserves semantic intent while increasing engagement and interpretability for target audience
- Evidence anchors: [section 3.1] "the localized version is a culturally-sensitive translation, with minor changes to include names, places, and activities that are directly pertinent to a Haitian audience."
- Break condition: Localization fails if changes alter semantic content or introduce unintended bias

## Foundational Learning

- Concept: Zero-shot transfer learning
  - Why needed here: Many Creole languages have no annotated data, so models must rely on pre-trained multilingual knowledge without task-specific fine-tuning
  - Quick check question: What is the difference between zero-shot and few-shot transfer learning in NLP?

- Concept: Cross-lingual representation alignment
  - Why needed here: Effective transfer depends on models mapping semantically equivalent phrases across languages into similar vector spaces
  - Quick check question: How do multilingual sentence transformers achieve cross-lingual alignment without parallel data?

- Concept: Domain adaptation in MT
  - Why needed here: Most available Creole data is religious (Bible); models must adapt to other domains (education) to be broadly useful
  - Quick check question: What is the typical performance drop when adapting an MT model from one domain to another?

## Architecture Onboarding

- Component map: Data acquisition -> Tokenizer creation -> Model fine-tuning/evaluation -> Benchmark aggregation
- Critical path: Data acquisition → Tokenizer creation → Model fine-tuning/evaluation → Benchmark aggregation
- Design tradeoffs:
  - Use fixed-domain data (Bible) for scale vs. domain bias
  - Localized vs. direct translations for comprehension tasks
  - Shared vs. separate tokenizers for multilingual MT
- Failure signatures:
  - BLEU/chrF scores plateau or drop sharply for low-resource Creoles
  - High variance in zero-shot performance across tasks
  - Tokenizer vocabulary mismatch causing out-of-vocabulary tokens
- First 3 experiments:
  1. Fine-tune mBERT on English MCTest160, evaluate on Haitian Creole (direct vs. localized) to compare cultural adaptation
  2. Fine-tune mBART-50-MT on CreoleM2M Bible corpus, evaluate on MIT-Haiti educational data to measure domain adaptation
  3. Train relation classification model on English UKP dataset, evaluate on Bislama, Chavacano, Jamaican, Pitkern, Tok Pisin to test cross-lingual transfer for semantic relations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective transfer learning strategies for Creoles given their unique linguistic characteristics?
- Basis in paper: [explicit] The paper highlights that Creoles' genealogical ties to higher-resourced languages suggest potential for transfer learning, but previous research has shown mixed results
- Why unresolved: The paper indicates that while Creoles are related to higher-resourced languages, transfer learning from these languages has not always been straightforward, as seen in studies like de Vries et al. (2022) and Lent et al. (2022a)
- What evidence would resolve it: Comparative studies evaluating different transfer learning methods (e.g., fine-tuning vs. feature extraction, multi-source transfer) across various Creole languages and their ancestor languages

### Open Question 2
- Question: How does cultural adaptation impact the performance of NLP models for Creoles, and what are the best practices for achieving it?
- Basis in paper: [explicit] The paper discusses the importance of culturally sensitive translations, as seen in the MCTest160 dataset where a localized Haitian Creole version was created
- Why unresolved: While the paper introduces culturally sensitive translations, it does not provide a comprehensive framework for cultural adaptation or assess its impact on model performance across different tasks and Creole languages
- What evidence would resolve it: Empirical studies comparing model performance on culturally adapted vs. direct translations, and the development of guidelines or tools for cultural adaptation in NLP for Creoles

### Open Question 3
- Question: What are the challenges and solutions for developing language identification tools for Creoles, and how would this impact resource development?
- Basis in paper: [explicit] The paper mentions that "One reason for the difficulty in obtaining Creole corpora from the web is that there are extremely limited language identification (LID) tools for Creoles"
- Why unresolved: The paper highlights the lack of LID tools as a barrier to resource development but does not explore the specific challenges or propose solutions for creating effective LID systems for Creoles
- What evidence would resolve it: Development and evaluation of LID models specifically trained on Creole languages, along with analysis of their impact on corpus collection and resource development

## Limitations
- Weak corpus evidence with only 5 neighbor papers averaging 0 citations, suggesting limited external validation of transfer learning claims
- No baseline results reported for models trained on Creole data, making it impossible to quantify actual improvement from transfer learning
- Domain adaptation only partially addressed, with Bible data shown to work within-domain but untested for cross-domain generalization

## Confidence
- **High confidence**: The CreoleVal benchmark creation and dataset aggregation are well-documented and reproducible
- **Medium confidence**: Zero-shot transfer learning baselines are reported, but without Creole-specific training baselines for comparison
- **Low confidence**: Claims about transfer learning potential are based on limited empirical evidence and theoretical reasoning

## Next Checks
1. Train monolingual Creole models from scratch (where data permits) and compare performance to zero-shot transfer baselines to quantify actual transfer benefit
2. Systematically evaluate domain adaptation by testing Bible-trained models on education and other domains to measure cross-domain generalization
3. Conduct controlled experiments comparing localized vs. direct translations on comprehension tasks with multiple Creole languages to validate cultural adaptation claims