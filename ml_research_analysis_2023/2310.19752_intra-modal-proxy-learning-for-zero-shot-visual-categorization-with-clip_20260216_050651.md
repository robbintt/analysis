---
ver: rpa2
title: Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP
arxiv_id: '2310.19752'
source_url: https://arxiv.org/abs/2310.19752
tags:
- proxy
- vision
- text
- learning
- clip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Intra-Modal Proxy Learning (InMaP) to improve
  zero-shot visual categorization with CLIP. It theoretically shows that the modality
  gap between text and vision space in CLIP can't be sufficiently reduced, leading
  to sub-optimal performance.
---

# Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP

## Quick Facts
- arXiv ID: 2310.19752
- Source URL: https://arxiv.org/abs/2310.19752
- Reference count: 40
- Primary result: InMaP achieves 80.21% accuracy on ImageNet with ViT-L/14@336, outperforming CLIP's 77.02% and other methods while being efficient (1 min on single GPU)

## Executive Summary
This paper addresses the challenge of zero-shot visual categorization using CLIP models, which suffer from a modality gap between text and vision embeddings that limits performance. The authors propose Intra-Modal Proxy Learning (InMaP), which directly learns vision proxies in the vision space rather than relying on cross-modal proxies. By leveraging pseudo labels from text proxies and refining them through optimal transport and thresholding, InMaP recovers optimal vision proxies that better capture vision data distributions. Experiments on 14 tasks demonstrate significant performance improvements while maintaining computational efficiency.

## Method Summary
InMaP addresses CLIP's modality gap by learning vision proxies directly in the vision space rather than using cross-modal proxies. The method uses pseudo labels generated from text proxies to optimize vision proxies via convex loss minimization. These pseudo labels are refined using Sinkhorn distance to balance class distributions and thresholding to convert high-confidence soft labels to hard labels. The approach leverages pre-trained CLIP encoders (frozen) and requires only unlabeled target vision data and class names. The entire process is efficient, completing in about 1 minute on a single GPU.

## Key Results
- Achieves 80.21% accuracy on ImageNet with ViT-L/14@336
- Outperforms CLIP baseline (77.02%) and other competing methods
- Demonstrates efficiency with 1 minute runtime on single GPU
- Shows consistent improvements across 14 different vision tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP's contrastive loss cannot sufficiently reduce the modality gap between text and vision embeddings.
- Mechanism: The small temperature parameter (τ ≈ 0.01) amplifies differences between paired embeddings, keeping them far apart in feature space. This preserves a gap that prevents text proxies from adequately representing vision distributions.
- Core assumption: The temperature in CLIP's contrastive loss is fixed and small, causing embeddings to remain distinct.
- Evidence anchors:
  - [abstract] "the modality gap between the text and vision space can result in a sub-optimal performance"
  - [section 3.1] Proposition 1 shows that with small τ, the absolute distance between modalities partially depends on τ, keeping embeddings far apart.
  - [corpus] Related work "Cross the Gap" confirms that the modality gap remains significant after CLIP training.
- Break condition: If CLIP uses a much larger τ or uses a different alignment loss that explicitly pulls modalities together, the gap might be reduced.

### Mechanism 2
- Claim: The optimal proxy for vision tasks lies only in the vision space, not in the cross-modal space.
- Mechanism: By Proposition 2, the optimal vision proxy is a weighted combination of vision features only, so proxies in the text space cannot fully capture vision data distribution.
- Core assumption: Vision classification accuracy depends on having proxies that align well with vision feature geometry.
- Evidence anchors:
  - [abstract] "the optimal proxy for vision tasks may reside only in the vision space"
  - [section 3.2] Proposition 2 explicitly shows that optimal vision proxies are constructed from vision data only.
  - [corpus] Weak, no direct neighbor paper discusses optimal proxy location.
- Break condition: If the vision and text feature spaces overlap perfectly, text proxies could become optimal for vision tasks.

### Mechanism 3
- Claim: Intra-modal proxy learning recovers the optimal vision proxy using pseudo labels derived from text proxies.
- Mechanism: InMaP optimizes a convex loss in the vision space using pseudo labels from text proxies, refining labels via Sinkhorn distance and thresholding to improve accuracy.
- Core assumption: Pseudo labels from text proxies are sufficiently accurate to guide vision proxy learning, and label refinement improves proxy quality.
- Evidence anchors:
  - [abstract] "propose to learn the vision proxy directly with the help from the text proxy for zero-shot transfer"
  - [section 3.3] Sinkhorn distance and thresholding refine pseudo labels, improving proxy recovery.
  - [section 4.1] Ablation confirms that both proxy learning and label refinement contribute to performance gains.
- Break condition: If pseudo labels are too noisy or the vision/text spaces are completely disjoint, proxy recovery will fail.

## Foundational Learning

- Concept: Contrastive learning with temperature scaling
  - Why needed here: CLIP's loss uses temperature τ to shape similarity distributions; small τ causes modality separation.
  - Quick check question: If τ is doubled from 0.01 to 0.02, does the cross-modal similarity increase or decrease?
- Concept: Convex optimization in feature space
  - Why needed here: InMaP's proxy learning problem is convex, solvable efficiently without gradient through encoders.
  - Quick check question: Does adding more classes (larger C) change the convexity of the proxy learning objective?
- Concept: Optimal transport and Sinkhorn distance
  - Why needed here: Used to refine pseudo labels by matching label distribution to target class proportions.
  - Quick check question: If the class distribution is balanced, does the Sinkhorn refinement still change the labels?

## Architecture Onboarding

- Component map: CLIP encoders (frozen) -> Pseudo label generator -> Sinkhorn refiner -> Threshold filter -> Vision proxy optimizer -> Classifier
- Critical path: 1. Extract features from CLIP. 2. Generate initial pseudo labels. 3. Refine labels via Sinkhorn + threshold. 4. Optimize vision proxies. 5. Classify using learned vision proxies.
- Design tradeoffs:
  - Small τ in CLIP preserves discriminative power but widens modality gap.
  - Large τ in InMaP proxy learning reduces modality gap but risks oversmoothing.
  - More Sinkhorn iterations improve label quality but increase runtime.
- Failure signatures:
  - Low improvement over baseline → pseudo labels too noisy or vision/text spaces too far apart.
  - Proxy learning diverges → learning rate too high or labels too noisy.
  - Extremely slow convergence → insufficient Sinkhorn iterations or bad initial learning rate.
- First 3 experiments:
  1. Verify baseline zero-shot accuracy on ImageNet with ResNet-50.
  2. Test proxy learning with fixed pseudo labels from text proxy (no Sinkhorn).
  3. Add Sinkhorn refinement and measure pseudo label accuracy before/after.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the modality gap in CLIP evolve during the training process, and at which point does it become most significant?
- Basis in paper: [inferred] The paper mentions that the modality gap between text and vision space in CLIP cannot be reduced sufficiently by minimizing the contrastive loss, but does not provide a detailed analysis of how this gap evolves during training.
- Why unresolved: The paper does not include an empirical study or theoretical analysis of the training dynamics that would reveal the point at which the modality gap becomes most significant.
- What evidence would resolve it: A study tracking the modality gap at different stages of CLIP's training process, possibly using a measure of distance between the text and vision embeddings, would provide insights into the evolution of the gap.

### Open Question 2
- Question: Can the proposed InMaP method be extended to handle more than two modalities, such as incorporating audio or other sensory data?
- Basis in paper: [inferred] The paper focuses on the text-vision modality gap in CLIP, but the concept of intra-modal proxy learning could potentially be extended to other modalities.
- Why unresolved: The paper does not explore the application of InMaP to modalities beyond text and vision, nor does it discuss the theoretical implications of such an extension.
- What evidence would resolve it: Experiments applying InMaP to tasks involving additional modalities, along with a theoretical analysis of the method's behavior in a multi-modal context, would demonstrate its potential for broader application.

### Open Question 3
- Question: What is the impact of the temperature parameter τI on the stability and convergence of the intra-modal proxy learning process?
- Basis in paper: [explicit] The paper discusses the importance of the temperature parameter τI for calibrating the magnitude of the distribution in the vision proxy learning, but does not provide a detailed analysis of its impact on stability and convergence.
- Why unresolved: The paper does not include experiments or theoretical analysis that specifically address the role of τI in the stability and convergence of the learning process.
- What evidence would resolve it: A systematic study of the learning process with varying values of τI, including an analysis of the stability and convergence rates, would provide insights into the optimal settings for this parameter.

### Open Question 4
- Question: How does the performance of InMaP compare to other methods that aim to reduce the modality gap in vision-language models, such as those that use additional large language models or fine-tuning strategies?
- Basis in paper: [explicit] The paper compares InMaP to several methods, including those that use external large models, but does not provide a comprehensive comparison with all existing methods that target the modality gap.
- Why unresolved: The paper focuses on a specific set of methods for comparison and does not explore the full landscape of techniques aimed at reducing the modality gap in vision-language models.
- What evidence would resolve it: A thorough comparison of InMaP with a wide range of methods that address the modality gap, including those that use fine-tuning or other large models, would provide a clearer picture of its relative effectiveness.

### Open Question 5
- Question: Can the pseudo label refinement strategies proposed in InMaP be applied to other semi-supervised learning scenarios beyond zero-shot visual categorization?
- Basis in paper: [explicit] The paper introduces strategies for refining pseudo labels, such as using Sinkhorn distance and thresholding, which are motivated by semi-supervised learning principles.
- Why unresolved: The paper does not explore the application of these refinement strategies to other semi-supervised learning tasks or discuss their potential benefits in different contexts.
- What evidence would resolve it: Experiments applying the pseudo label refinement strategies to a variety of semi-supervised learning tasks, along with an analysis of their impact on performance, would demonstrate their broader applicability.

## Limitations

- Theoretical analysis of modality gap relies on idealized assumptions about temperature parameter that may not fully capture real CLIP models.
- Empirical evidence for optimal proxy location is supported by theory but lacks extensive ablation studies on different CLIP architectures.
- Method's reliance on pseudo labels introduces potential noise propagation, though robustness to label noise is not thoroughly investigated.

## Confidence

- **High Confidence**: The effectiveness of InMaP on ImageNet and its efficiency (1 min on single GPU) are well-supported by empirical results.
- **Medium Confidence**: The theoretical analysis of the modality gap and optimal proxy location is sound but based on assumptions that may not hold in all cases.
- **Low Confidence**: The robustness of InMaP to noisy pseudo labels and its performance on highly imbalanced datasets are not thoroughly validated.

## Next Checks

1. **Theoretical Validation**: Conduct a more detailed analysis of the modality gap across different CLIP architectures and temperature settings to verify the theoretical assumptions.
2. **Empirical Validation**: Perform extensive ablation studies on the impact of pseudo label quality and Sinkhorn refinement on InMaP's performance.
3. **Robustness Testing**: Evaluate InMaP's performance on highly imbalanced datasets and compare it with baseline methods to assess its robustness to label noise and distribution shifts.