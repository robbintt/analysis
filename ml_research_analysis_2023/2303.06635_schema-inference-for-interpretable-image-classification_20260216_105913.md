---
ver: rpa2
title: Schema Inference for Interpretable Image Classification
arxiv_id: '2303.06635'
source_url: https://arxiv.org/abs/2303.06635
tags:
- visual
- conference
- graph
- vertex
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel schema inference paradigm for interpretable
  image classification. The key idea is to reformulate the traditional deep neural
  network (DNN) inference pipeline into a graph matching process that associates extracted
  visual concepts of an input image with pre-computed category-level imaginations.
---

# Schema Inference for Interpretable Image Classification

## Quick Facts
- arXiv ID: 2303.06635
- Source URL: https://arxiv.org/abs/2303.06635
- Reference count: 40
- Achieves 95.96% top-1 accuracy on CIFAR-10 and 78.45% on CIFAR-100

## Executive Summary
This paper proposes a novel schema inference paradigm for interpretable image classification that reformulates traditional deep neural network inference into a graph matching process. The method models visual semantics and category imaginations as topological relational graphs, using a Feat2Graph scheme to establish rich interaction information. The approach achieves state-of-the-art performance on CIFAR-10/100, Caltech-101, and ImageNet while providing clear insight into the deductive reasoning process through visualizations of ingredient graphs and the matching procedure.

## Method Summary
The schema inference method uses a pre-trained vision transformer backbone to extract features, which are then discretized into visual words using k-means clustering. These visual words form vertices in instance-level IR-Graphs, with edges established based on self-attention and spatial adjacency relationships. The method builds a category-level IR-Atlas containing imagination graphs for each class, and a graph matcher with shallow GCN layers finds the most similar category graph for each input. The learned knowledge is stored in the IR-Atlas rather than the matcher, enabling knowledge transfer without fine-tuning. Training uses cross-entropy loss with entropy regularization on IR-Atlas complexity.

## Key Results
- Achieves 95.96% top-1 accuracy on CIFAR-10 and 78.45% on CIFAR-100
- Outperforms state-of-the-art interpretable approaches on multiple benchmarks
- Demonstrates knowledge transfer capability from CIFAR-100 to ImageNet without fine-tuning
- Provides clear visualizations of the deductive reasoning process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Schema inference achieves interpretable image classification by reformulating inference into graph matching that associates visual concepts with category-level imaginations
- Mechanism: Visual semantics and category imaginations are modeled as topological relational graphs, with Feat2Graph establishing relational graphs containing interaction information, then graph matching derives predictions
- Core assumption: Visual concepts can be effectively discretized into a vocabulary and represented as graph vertices with interaction-weighted edges
- Evidence anchors:
  - [abstract]: reformulate conventional model inference pipeline into graph matching policy
  - [section 2.2]: main purpose of feature discretization is to mine common patterns corresponding to human understandable semantic
  - [corpus]: Weak - no direct evidence found in related papers
- Break condition: If visual vocabulary discretization fails to capture meaningful semantic concepts, or if graph matching cannot effectively capture relationships

### Mechanism 2
- Claim: Learned category knowledge is stored in IR-Atlas rather than the matcher, exhibiting high interpretability
- Mechanism: IR-Atlas contains category-level IR-Graphs representing imagination of all categories, while matcher only captures local structure for gathering class evidence
- Core assumption: Category-specific knowledge can be effectively represented and learned in graph form within IR-Atlas
- Evidence anchors:
  - [abstract]: learned category knowledge is stored in IR-Atlas rather than the matcher
  - [section 2.3]: matcher finds most similar category-level graph in IR-Atlas
  - [corpus]: Weak - no direct evidence found in related papers
- Break condition: If IR-Atlas fails to capture sufficient category-specific knowledge, or if matcher cannot effectively utilize this knowledge

### Mechanism 3
- Claim: Schema inference yields results superior to state-of-the-art interpretable approaches while providing clear insight into reasoning procedure
- Mechanism: Compositional contributions of visual semantics through graph interactions rather than simple aggregation captures richer relationships between visual elements
- Core assumption: Considering visual word interactions is critical for both human and DNN inference
- Evidence anchors:
  - [abstract]: theoretical analysis and experimental results demonstrate encouraging performance and clear picture of deductive process
  - [section 2.2]: interaction between vertices quantified by similarity at semantic-level and adjacency at spatial-level
  - [corpus]: Weak - no direct evidence found in related papers
- Break condition: If graph structure fails to capture meaningful interactions, or if performance advantage is not maintained

## Foundational Learning

- Concept: Visual concept discretization
  - Why needed here: Method relies on converting continuous visual features into discrete visual words representing human-understandable semantics
  - Quick check question: Can you explain how k-means clustering is used to create a visual vocabulary from deep features?

- Concept: Graph neural networks and message passing
  - Why needed here: Method uses GCNs to capture relationships between visual concepts represented as graph vertices
  - Quick check question: Can you describe how GraphConv layers aggregate information from neighboring vertices?

- Concept: Attention mechanisms in vision transformers
  - Why needed here: Method leverages self-attention matrices from ViTs to establish relationships between visual tokens
  - Quick check question: Can you explain how the attention matrix represents relationships between visual tokens in a transformer?

## Architecture Onboarding

- Component map: Feat2Graph module (discretizes features → creates vertices → establishes edges) → Matcher (GCN + similarity computation) → IR-Atlas (stores category graphs)
- Critical path: Input image → Backbone feature extraction → Feat2Graph → Graph matching with IR-Atlas → Prediction
- Design tradeoffs: Using graphs instead of flat features adds complexity but captures richer relationships; storing knowledge in IR-Atlas enables transferability but requires more memory
- Failure signatures: Poor discretization leading to meaningless visual words; insufficient graph connectivity preventing effective message passing; incorrect vertex weighting leading to irrelevant features dominating predictions
- First 3 experiments:
  1. Test Feat2Graph module independently with synthetic features to verify vertex creation and edge establishment
  2. Test graph matcher with pre-computed IR-Graphs to verify similarity computation works correctly
  3. Run end-to-end pipeline on a small dataset with known ground truth to verify overall system behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Feat2Graph module's performance scale with increasing visual vocabulary size, and what is the computational bottleneck that prevents parallelization?
- Basis in paper: [explicit] Paper mentions Feat2Graph implemented in C++ for better random access performance, but running time is still significantly longer than other components (about 1.75 ms per image) and does not benefit from GPU parallel acceleration
- Why unresolved: Paper does not provide detailed analysis of how performance scales with different visual vocabulary sizes or why it cannot be parallelized
- What evidence would resolve it: Detailed profiling of Feat2Graph performance with varying visual vocabulary sizes and analysis of algorithmic steps preventing parallelization

### Open Question 2
- Question: How does the proposed schema inference paradigm perform on more complex vision tasks beyond image classification, such as visual question answering or image captioning?
- Basis in paper: [inferred] Paper mentions implementing schema inference for more complicated vision tasks like visual question answering would enable linking visual semantics to phrases in human language
- Why unresolved: Paper only evaluates method on image classification benchmarks (CIFAR-10/100, Caltech-101, ImageNet)
- What evidence would resolve it: Experiments evaluating schema inference paradigm on visual question answering, image captioning, or other complex vision tasks

### Open Question 3
- Question: How does the proposed method handle fine-grained classification tasks where distinctions between classes are subtle and may require more detailed visual semantics analysis?
- Basis in paper: [inferred] Paper mentions learned edge weights tend to connect object parts to adjacent background and other object parts while ignoring background connections, which helps distinguish fine-grained categories
- Why unresolved: Paper does not evaluate method on fine-grained classification datasets or provide detailed analysis of handling subtle distinctions
- What evidence would resolve it: Experiments on fine-grained classification datasets like CUB-200-2011 or Stanford Cars

## Limitations
- Dependency on pre-trained vision transformer backbone adds computational overhead and may not be accessible for all domains
- K-means discretization introduces hyperparameter (visual vocabulary size M) that requires careful tuning and may not generalize across datasets
- Computational complexity of fully connected IR-Graphs requires sparsification, potentially losing some relationship information

## Confidence
- **High confidence**: Core graph matching methodology and overall architecture design supported by experimental results showing superior performance on multiple benchmarks
- **Medium confidence**: Interpretability claims supported by visualizations, but subjective nature makes quantitative validation challenging
- **Medium confidence**: Knowledge transfer capability supported by CIFAR-100→ImageNet experiments, but extent and practical utility needs more exploration

## Next Checks
1. **Ablation study on visual vocabulary size**: Systematically evaluate impact of different M values (128, 512, 1024, 2048) on both accuracy and interpretability across all datasets to identify optimal configurations and understand trade-offs.

2. **Cross-dataset generalization test**: Train model on one dataset (e.g., CIFAR-10) and evaluate directly on completely different datasets (e.g., ImageNet) without fine-tuning to validate claimed knowledge transferability and identify failure modes.

3. **Comparison with alternative discretization methods**: Replace k-means clustering with alternative approaches like vector quantization or end-to-end learned discretization to assess whether performance gains are specifically due to schema inference framework or could be achieved through simpler modifications.