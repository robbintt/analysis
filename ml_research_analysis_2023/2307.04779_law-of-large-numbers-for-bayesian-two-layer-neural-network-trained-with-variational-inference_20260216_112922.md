---
ver: rpa2
title: Law of Large Numbers for Bayesian two-layer Neural Network trained with Variational
  Inference
arxiv_id: '2307.04779'
source_url: https://arxiv.org/abs/2307.04779
tags:
- proof
- limit
- theorem
- neural
- variational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper analyzes the training of Bayesian neural networks using
  variational inference in the two-layer and infinite-width case. It considers a regression
  problem with a regularized evidence lower bound (ELBO) and proves a law of large
  numbers for three training schemes: exact estimation, Bayes by Backprop, and a new
  computationally cheaper algorithm called Minimal VI.'
---

# Law of Large Numbers for Bayesian two-layer Neural Network trained with Variational Inference

## Quick Facts
- arXiv ID: 2307.04779
- Source URL: https://arxiv.org/abs/2307.04779
- Reference count: 40
- Key outcome: Proves law of large numbers for three variational inference training schemes in infinite-width Bayesian neural networks, showing they converge to the same mean-field limit

## Executive Summary
This paper analyzes the training of Bayesian neural networks using variational inference in the two-layer and infinite-width regime. The authors consider a regression problem with a regularized evidence lower bound (ELBO) and prove a law of large numbers for three training schemes: exact estimation (idealized SGD), Bayes by Backprop (Bayes-by-Backprop SGD), and a new computationally cheaper algorithm called Minimal VI. The key finding is that all three methods converge to the same mean-field limit, providing theoretical foundations for variational inference in Bayesian neural networks and introducing an efficient new algorithm.

## Method Summary
The paper studies a two-layer neural network with N neurons trained using variational inference with a regularized ELBO objective. Three training schemes are analyzed: idealized SGD with exact integral computation, Bayes-by-Backprop SGD using Monte Carlo sampling, and Minimal VI SGD with reduced computational complexity. The analysis proves convergence of the scaled empirical distribution of network parameters to a deterministic limit as N → ∞. The key innovation is showing that all three algorithms converge to the same mean-field limit, with Minimal VI achieving this at lower computational cost by retaining only non-interacting terms from the limit equation.

## Key Results
- All three training schemes (idealized SGD, Bayes-by-Backprop SGD, and Minimal-VI SGD) converge to the same mean-field limit in the infinite-width regime
- Minimal-VI algorithm reduces computational complexity from O(N) to O(1) random variables per update while maintaining the same asymptotic behavior
- Appropriate scaling of the KL divergence term (by 1/N) ensures balanced asymptotic behavior between the integrated loss and KL terms
- The convergence proof holds under assumptions of smooth, bounded activation functions and Gaussian priors/variational distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Three different training schemes converge to the same mean-field limit
- Mechanism: Law of large numbers for empirical distribution of parameters shows idealized SGD, Bayes-by-Backprop SGD, and Minimal-VI SGD all converge to the same deterministic solution of a measure-valued evolution equation
- Core assumption: Activation function is smooth and bounded (C∞b), prior and variational distributions are Gaussian with appropriate KL divergence weighting
- Evidence anchors: [abstract] "With an appropriate weighting of the KL, we prove a law of large numbers for three different training schemes"; [section] "The key finding is that all methods converge to the same mean-field limit"
- Break condition: If activation function is not smooth or KL weighting is not appropriately scaled, convergence to same limit may not hold

### Mechanism 2
- Claim: Minimal-VI algorithm is computationally cheaper while maintaining same asymptotic behavior
- Mechanism: By analyzing limit equation, only certain terms contribute to asymptotic behavior; Minimal-VI retains only non-interacting terms, reducing random variables from O(N) to 2 per step
- Core assumption: Limit equation can be decomposed into interacting and non-interacting terms, with only non-interacting terms affecting asymptotic behavior
- Evidence anchors: [abstract] "a new and computationally cheaper algorithm which we introduce as Minimal VI"; [section] "This scheme is the true mean-field algorithmic approach, as only deriving from non-interacting terms"
- Break condition: If decomposition of limit equation is incorrect or non-interacting terms don't fully capture asymptotic behavior, Minimal-VI may not converge to same limit

### Mechanism 3
- Claim: Appropriate scaling of KL divergence prevents overfitting and ensures balanced asymptotic behavior
- Mechanism: Multiplying KL term by factor scaled by 1/N balances effects of integrated loss and KL terms in limit N → ∞
- Core assumption: Scaling factor 1/N correctly balances KL and loss terms in infinite-width limit
- Evidence anchors: [abstract] "With an appropriate weighting of the KL, we prove a law of large numbers"; [section] "we prove a law of large numbers for three different training schemes"
- Break condition: If scaling factor is not 1/N or balance between KL and loss terms is not achieved, asymptotic behavior may be dominated by one term

## Foundational Learning

- Concept: Law of Large Numbers (LLN) for mean-field interacting particle systems
  - Why needed here: Paper relies on LLN results to prove empirical distribution of parameters converges to deterministic limit as neurons → infinity
  - Quick check question: What is the main difference between LLN for idealized SGD and Bayes-by-Backprop SGD in this paper?

- Concept: Wasserstein spaces and distances
  - Why needed here: Paper uses Wasserstein spaces to handle convergence of empirical parameter distribution, especially when parameters are unbounded
  - Quick check question: Why does paper switch from using P(ΘT) to Pγ0(Rd+1) for Bayes-by-Backprop SGD?

- Concept: Variational Inference and Evidence Lower Bound (ELBO)
  - Why needed here: Paper analyzes training of Bayesian neural networks using variational inference, where goal is to maximize ELBO
  - Quick check question: What is purpose of multiplying KL term in ELBO by 1/N?

## Architecture Onboarding

- Component map: Idealized SGD (exact integrals) -> Bayes-by-Backprop SGD (Monte Carlo sampling) -> Minimal-VI SGD (non-interacting terms only)
- Critical path: 1) Prove LLN for idealized SGD; 2) Show Bayes-by-Backprop SGD has same limit; 3) Derive Minimal-VI algorithm from limit equation; 4) Validate convergence numerically
- Design tradeoffs: Computational complexity vs. variance (Minimal-VI cheaper but may have higher variance); Exactness vs. tractability (idealized exact but intractable, Bayes-by-Backprop approximate but tractable); Strong assumptions vs. generality (Gaussian priors, smooth activations may not hold in all cases)
- Failure signatures: Divergence of parameters (if parameters not bounded, convergence may fail); Incorrect scaling of KL (if KL term not scaled by 1/N, asymptotic behavior may be unbalanced); Non-smooth activations (if activation function not smooth, limit equation may not hold)
- First 3 experiments: 1) Verify convergence of three algorithms to same limit on toy regression problem; 2) Compare computational cost and variance of three algorithms; 3) Test robustness of results to different activation functions and prior distributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise rate of convergence of the Minimal-VI algorithm compared to the idealized and Bayes-by-Backprop SGD algorithms?
- Basis in paper: [inferred] Paper states Minimal-VI is computationally cheaper but may exhibit larger variance than other algorithms, suggesting convergence rate might differ
- Why unresolved: Paper doesn't provide detailed analysis of convergence rates of different algorithms, only mentions Minimal-VI is cheaper and may have larger variance
- What evidence would resolve it: Rigorous comparison of convergence rates of three algorithms through theoretical analysis or extensive numerical experiments

### Open Question 2
- Question: How does scaling of KL divergence term in ELBO affect calibration of predictive uncertainty in Bayesian neural networks?
- Basis in paper: [explicit] Paper discusses importance of balancing integrated loss and KL terms for asymptotic stability but doesn't explore impact on predictive uncertainty calibration
- Why unresolved: Paper focuses on theoretical analysis of training algorithms and convergence properties, not practical implications for uncertainty quantification
- What evidence would resolve it: Experiments comparing predictive uncertainty calibration of BNNs trained with different KL scaling factors on benchmark datasets or real-world applications

### Open Question 3
- Question: How does Minimal-VI algorithm perform on deep neural networks with multiple hidden layers?
- Basis in paper: [inferred] Paper analyzes two-layer case and introduces Minimal-VI for this setting; unclear whether algorithm can be extended to deeper architectures and how it would perform
- Why unresolved: Paper doesn't provide analysis or experiments on deep neural networks; theoretical results and algorithm are specific to two-layer case
- What evidence would resolve it: Implementation and evaluation of Minimal-VI algorithm on deep neural networks with varying depths and architectures to assess scalability and performance

## Limitations
- Analysis relies on specific assumptions about activation functions (C∞b smoothness) and Gaussian priors that may not hold in practical applications
- Paper assumes access to exact integrals in idealized case, which is generally intractable for real-world problems
- Doesn't fully characterize variance behavior around the limit, which could impact practical implementation

## Confidence

**Confidence levels:**
- High confidence: Convergence proof for idealized SGD algorithm under stated assumptions
- Medium confidence: Extension of convergence results from idealized to practical algorithms (Bayes-by-Backprop and Minimal-VI)
- Medium confidence: Computational efficiency claims for Minimal-VI algorithm, particularly regarding variance behavior

## Next Checks

1. **Empirical validation of variance bounds**: Conduct numerical experiments comparing variance of parameter updates across all three algorithms, particularly examining whether Minimal-VI's variance remains bounded and acceptable for practical use

2. **Activation function robustness**: Test convergence properties with different activation functions (ReLU, tanh, etc.) to understand how sensitive results are to smoothness assumptions

3. **Prior distribution sensitivity**: Evaluate how convergence and computational efficiency of algorithms change when using different prior distributions beyond Gaussian, such as heavy-tailed or hierarchical priors