---
ver: rpa2
title: 'Democratizing Reasoning Ability: Tailored Learning from Large Language Model'
arxiv_id: '2310.13332'
source_url: https://arxiv.org/abs/2310.13332
tags:
- student
- reasoning
- learning
- answer
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for distilling reasoning ability from
  large language models (LLMs) to smaller language models through a tailored multi-round
  learning paradigm. The approach leverages an interactive feedback loop where the
  student model's errors are used to generate customized training data from the teacher
  LLM, while also incorporating self-reflection learning to help the student distinguish
  correct from incorrect reasoning paths.
---

# Democratizing Reasoning Ability: Tailored Learning from Large Language Model

## Quick Facts
- **arXiv ID**: 2310.13332
- **Source URL**: https://arxiv.org/abs/2310.13332
- **Reference count**: 40
- **Primary result**: Student model's GSM8K accuracy increased from 2.7% to 32.0% using multi-round tailored learning

## Executive Summary
This paper introduces a novel approach to distill reasoning ability from large language models (LLMs) to smaller language models through a tailored multi-round learning paradigm. The method leverages an interactive feedback loop where the student model's errors are used to generate customized training data from the teacher LLM, while also incorporating self-reflection learning to help the student distinguish correct from incorrect reasoning paths. Experiments on mathematical and commonsense reasoning tasks demonstrate significant improvements in the student model's reasoning performance compared to existing distillation methods.

## Method Summary
The approach uses a multi-round learning paradigm where a smaller student LM learns reasoning from a larger teacher LLM through customized feedback. In each round, the student provides its learning status (mistakes) to the teacher, who generates tailored rationales addressing those specific errors. The student then learns from this customized training data and performs self-reflection learning using contrastive samples from its own mistakes. This iterative process continues until the student's performance plateaus.

## Key Results
- Student model accuracy on GSM8K increased from 2.7% to 32.0% after multi-round learning
- Significant improvements on other reasoning benchmarks including MultiArith, SVAMP, CSQA, and StrategyQA
- Strong performance compared to existing distillation methods while using smaller model sizes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Interactive multi-round learning enables teacher LLM to generate rationales tailored to student's specific reasoning weaknesses.
- **Mechanism**: Student's incorrect reasoning paths are fed back to the teacher LLM, which generates customized correct rationales addressing those specific errors.
- **Core assumption**: Teacher LLM can analyze student's mistakes and generate targeted reasoning steps addressing deficiencies.
- **Evidence anchors**: [abstract] states teacher provides "customized training data" based on student's deficiencies; [section] describes teacher generating rationales as feedback to student.
- **Break condition**: Teacher fails to generate correct/relevant rationales when given student's mistakes.

### Mechanism 2
- **Claim**: Self-reflection learning helps student distinguish correct from incorrect reasoning paths.
- **Mechanism**: Wrong reasoning paths serve as negative contrastive samples against correct paths, teaching the model to recognize differences.
- **Core assumption**: Student can learn to represent correct and incorrect reasoning paths differently in latent space.
- **Evidence anchors**: [abstract] mentions self-reflection motivates student to learn from self-made mistakes; [section] describes using mistakes to teach distinction between good and bad reasoning steps.
- **Break condition**: Student fails to learn meaningful representations from contrastive samples.

### Mechanism 3
- **Claim**: Multi-round learning progressively improves student's reasoning ability.
- **Mechanism**: After each training round, new student mistakes are collected to generate updated customized training data for next round.
- **Core assumption**: Student's reasoning ability improves sufficiently each round to generate new meaningful mistakes.
- **Evidence anchors**: [abstract] notes learning is "tailored to student's learning status" through multi-round paradigm; [section] describes iterative cultivation of reasoning ability.
- **Break condition**: Student plateaus early and stops generating meaningful new mistakes.

## Foundational Learning

- **Concept**: Chain-of-Thought (CoT) prompting
  - **Why needed here**: Enables LLMs to generate intermediate reasoning steps essential for teaching smaller models to reason.
  - **Quick check question**: What is the key difference between standard prompting and CoT prompting in terms of model output?

- **Concept**: Knowledge distillation
  - **Why needed here**: Entire approach is knowledge distillation transferring reasoning from larger teacher to smaller student.
  - **Quick check question**: How does this approach differ from traditional knowledge distillation in terms of feedback mechanism?

- **Concept**: Contrastive learning
  - **Why needed here**: Self-reflection learning uses contrastive learning to distinguish correct from incorrect reasoning paths.
  - **Quick check question**: What is the role of the margin (ρ) in the triplet loss used for self-reflection learning?

## Architecture Onboarding

- **Component map**: Student model -> Mistake collector -> Prompt template generator -> Teacher model -> Training data curator -> Student model (iterative loop)

- **Critical path**:
  1. Student model generates outputs on training data
  2. Mistakes are collected and formatted as feedback
  3. Teacher model generates customized rationales using feedback
  4. Student model is trained on combined correct rationales and self-reflection loss
  5. Process repeats for multiple rounds until convergence

- **Design tradeoffs**:
  - Number of rationales generated per sample (4) vs. cost and diversity
  - Weight of self-reflection loss (λ=0.5) vs. main training loss
  - Number of rounds (3) vs. diminishing returns
  - Sampling vs. greedy decoding for mistake collection

- **Failure signatures**:
  - Student model accuracy plateaus or decreases
  - Teacher model fails to generate correct rationales consistently
  - Self-reflection loss becomes unstable or diverges
  - Multi-round training shows no improvement after several rounds

- **First 3 experiments**:
  1. Single round distillation without self-reflection on GSM8K - verify basic knowledge transfer works
  2. Add self-reflection learning with λ=0.5 - test if contrastive learning improves performance
  3. Implement multi-round learning for 2-3 rounds - verify iterative improvement and check for diminishing returns

## Open Questions the Paper Calls Out

1. **Optimal balance between self-reflection and LLM feedback**: The paper experiments with limited λ values (0.0, 0.25, 0.5, 0.75, 1.0) but doesn't explore the full spectrum or provide systematic methods for determining optimal balance.

2. **Performance variation across different LLM architectures**: The paper primarily uses ChatGPT as teacher and mentions effectiveness should be validated with more powerful LMs like LLaMA, but doesn't conduct experiments with different architectures or sizes.

3. **Effective evaluation of rationale quality**: The paper acknowledges current evaluation is mainly based on final answer and calls for developing more trustworthy criteria, suggesting GPT-4 or process reward models but not implementing them.

## Limitations

- Approach heavily depends on teacher LLM's ability to generate high-quality, targeted rationales from student mistakes
- Self-reflection component assumes contrastive learning from mistakes will be effective without guarantee
- Multi-round mechanism relies on student generating sufficiently different mistakes each round, which may not hold if reasoning ability plateaus early

## Confidence

- **High confidence**: Core distillation framework is technically sound and GSM8K improvement from 2.7% to 32.0% is clearly reported
- **Medium confidence**: Self-reflection learning mechanism is theoretically justified but lacks direct empirical validation
- **Low confidence**: Effectiveness of multi-round paradigm beyond 2-3 rounds is not demonstrated, no analysis of when/why iterative process might fail

## Next Checks

1. **Ablation study on self-reflection**: Remove self-reflection loss entirely and compare performance on GSM8K across 1-3 rounds to isolate its contribution versus standard distillation.

2. **Error analysis on teacher feedback**: Manually evaluate 50 teacher-generated rationales for correctness and relevance to student mistakes to quantify feedback quality degradation.

3. **Round-by-round convergence plot**: Track student accuracy and mistake diversity after each round on a held-out validation set to identify when multi-round approach stops providing meaningful improvements.