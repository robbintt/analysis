---
ver: rpa2
title: 'Elo Uncovered: Robustness and Best Practices in Language Model Evaluation'
arxiv_id: '2311.17295'
source_url: https://arxiv.org/abs/2311.17295
tags:
- ratings
- rating
- language
- evaluation
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates the robustness of Elo ratings for evaluating
  large language models (LLMs) using human feedback. The authors study two key axioms:
  reliability and transitivity, and demonstrate that Elo ratings are highly sensitive
  to the order of comparisons and the choice of hyperparameters, particularly the
  K-factor.'
---

# Elo Uncovered: Robustness and Best Practices in Language Model Evaluation

## Quick Facts
- **arXiv ID:** 2311.17295
- **Source URL:** https://arxiv.org/abs/2311.17295
- **Reference count:** 21
- **Primary result:** Elo ratings for LLMs are highly sensitive to comparison order and K-factor, with transitivity not always guaranteed.

## Executive Summary
This paper investigates the robustness of Elo ratings for evaluating large language models (LLMs) using human feedback. The authors study two key axioms—reliability and transitivity—and demonstrate that Elo ratings are highly sensitive to the order of comparisons and the choice of hyperparameters, particularly the K-factor. Through extensive experiments using synthetic data and real-world human feedback, the study reveals that desirable properties like transitivity are not always satisfied. The authors provide empirical guidelines for improving the reliability of Elo-based evaluation, including recommendations for the number of permutations, K-factor selection, and caution regarding transitivity assumptions.

## Method Summary
The study employs synthetic paired comparison data generated from Bernoulli processes to simulate human feedback with controlled win probabilities between two models. Elo ratings are computed for each permutation of the comparison sequence using different K-factor values and numbers of permutations. The stability and transitivity of Elo ratings are analyzed by examining the variation in ratings across permutations and K-factors. Findings are validated using real-world human feedback data comparing Flan and Dolly model families. The method focuses on evaluating the reliability (order sensitivity) and transitivity axioms of the Elo rating system.

## Key Results
- Elo ratings are highly sensitive to the order of comparisons, especially when models have similar performance levels.
- Higher K-factors can both destabilize and stabilize Elo ratings depending on the number of permutations and model performance disparity.
- Transitivity is not guaranteed in Elo ratings for LLMs, particularly when models have similar performance levels.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Elo ratings for LLMs are highly sensitive to the order of comparisons, violating the reliability axiom.
- Mechanism: The Elo update rule accumulates rating changes sequentially, so early wins or losses disproportionately affect final scores, especially when models are closely matched (win probability near 0.5).
- Core assumption: Human feedback is independent and identically distributed across prompts, but the sequence of comparisons matters because Elo is not a static scoring method.
- Evidence anchors:
  - [abstract]: "Elo ratings are highly sensitive to the order of comparisons and the choice of hyperparameters, particularly the K-factor."
  - [section 5.1.1]: "we compute average Elo ratings per match across all Nperms permutations, ensuring a robust analysis that takes into account the full range of possible match-up orders."
  - [corpus]: Weak - no direct corpus citations, but the study's focus on permutations is novel.
- Break condition: If win probabilities are highly skewed (e.g., >0.65), order sensitivity diminishes because rating updates consistently move in one direction.

### Mechanism 2
- Claim: Higher K-factors can both destabilize and stabilize Elo ratings depending on the number of permutations and the degree of model performance disparity.
- Mechanism: K-factor controls the magnitude of rating adjustments; with a single permutation, high K amplifies noise from any single ordering, but with many permutations, high K accelerates convergence to true skill levels.
- Core assumption: The true performance hierarchy is stable across different prompt sets and does not change during evaluation.
- Evidence anchors:
  - [abstract]: "Elo ratings... are highly sensitive to... the choice of hyperparameters, particularly the K-factor."
  - [section 5.2.1]: "We extend our previous approach by conducting tests across a range of winning probabilities and multiple K-factor values (1, 8, 16, 32, 64)."
  - [corpus]: Weak - no direct corpus citations, but the finding is consistent with general Elo literature.
- Break condition: If K is too high relative to the variance in outcomes, ratings can oscillate or overshoot true skill levels.

### Mechanism 3
- Claim: Transitivity is not guaranteed in Elo ratings for LLMs, especially when models have similar performance levels.
- Mechanism: Elo updates are pairwise and local; even if A beats B and B beats C in separate sequences, the cumulative rating differences may not preserve the expected A > C relationship due to the non-linear nature of the Elo update function.
- Core assumption: Elo's design assumes transitivity, but this is not enforced in the update algorithm.
- Evidence anchors:
  - [abstract]: "desirable properties such as transitivity are not always guaranteed."
  - [section 3.1]: "If the ranking of large language models exhibits transitivity, we can deduce their comparative performance without the need for direct head-to-head evaluations."
  - [corpus]: Weak - no direct corpus citations, but the study's synthetic and real-world experiments demonstrate this empirically.
- Break condition: If win probabilities are very high (>0.75) and models are clearly distinguishable, transitivity is more likely to hold.

## Foundational Learning

- Concept: Bernoulli process and binomial distribution for modeling pairwise comparisons.
  - Why needed here: To generate synthetic human feedback data that mimics real-world win/loss outcomes in a controlled way.
  - Quick check question: If you have 10 pairwise comparisons between two models with a 60% win probability for one, what is the probability of exactly 7 wins for that model?

- Concept: Elo rating update equation and the role of the K-factor.
  - Why needed here: To understand how ratings evolve and why hyperparameter choices affect stability.
  - Quick check question: In the Elo update formula, what happens to the rating change if the K-factor is doubled?

- Concept: Axiomatic reasoning (reliability and transitivity) in evaluation systems.
  - Why needed here: To frame the study's investigation of Elo's suitability for LLMs and to identify when the method breaks down.
  - Quick check question: If a rating system satisfies transitivity, and A > B and B > C, what must be true about A and C?

## Architecture Onboarding

- Component map:
  - Synthetic data generator (Bernoulli/binomial processes)
  - Elo rating engine (update function, K-factor control)
  - Permutation sampler (reshuffling match sequences)
  - Analysis pipeline (averaging, stability metrics, transitivity checks)
  - Real-world feedback loader (human evaluation data)

- Critical path:
  1. Generate or load pairwise comparison data.
  2. Initialize Elo ratings.
  3. For each permutation:
     a. Shuffle match sequence.
     b. Update Elo ratings sequentially.
  4. Aggregate results across permutations.
  5. Evaluate reliability (order sensitivity) and transitivity.

- Design tradeoffs:
  - More permutations increase reliability but also computational cost.
  - Higher K-factor speeds convergence for clear winners but increases instability for close matches.
  - Synthetic data allows controlled experiments but may not capture all real-world nuances.

- Failure signatures:
  - Large variance in Elo scores across permutations (order sensitivity).
  - Elo ratings do not align with expected win rates (hyperparameter misconfiguration).
  - Violation of transitivity in synthetic or real data (method limitation).

- First 3 experiments:
  1. Compare Elo scores for two models with P_win = 0.55 across Nperms = 1 and Nperms = 100.
  2. Test Elo stability with K = 1 vs. K = 32 for models with P_win = 0.8.
  3. Validate transitivity by setting up three models with A > B > C and checking if Elo preserves A > C.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Elo ratings perform when evaluating LLMs in scenarios with multiple possible outcomes beyond simple win/loss, such as ties or partial victories?
- Basis in paper: The paper mentions that current Elo ratings assume binary outcomes and suggests extending to scenarios with multiple outcomes using multinomial distributions.
- Why unresolved: The study focuses on binary outcomes, and while the authors mention the possibility of extending to multinomial distributions, they do not conduct experiments or provide empirical evidence for such scenarios.
- What evidence would resolve it: Conducting experiments using multinomial distributions to simulate various outcomes (e.g., ties, partial victories) and comparing the performance of Elo ratings in these scenarios against the current binary outcome approach.

### Open Question 2
- Question: How does the choice of K-factor impact the stability and reliability of Elo ratings in real-world LLM evaluations, especially when dealing with models of varying performance levels?
- Basis in paper: The paper discusses the sensitivity of Elo ratings to the K-factor and suggests that different K-factors may be needed for different scenarios, but does not provide specific guidelines for selecting the optimal K-factor in real-world evaluations.
- Why unresolved: The study provides theoretical insights into the impact of K-factor on Elo ratings but does not conduct experiments to determine the optimal K-factor for different LLM evaluation scenarios.
- What evidence would resolve it: Conducting experiments with various K-factors in real-world LLM evaluations and analyzing the impact on the stability and reliability of Elo ratings for different model performance levels.

### Open Question 3
- Question: How does the transitivity property of Elo ratings hold up in real-world LLM evaluations, especially when dealing with models that have closely matched performance levels?
- Basis in paper: The paper mentions that the transitivity property of Elo ratings is not always guaranteed, particularly when dealing with models of similar performance levels, but does not provide empirical evidence for this claim in real-world scenarios.
- Why unresolved: The study focuses on synthetic data and provides theoretical insights into the transitivity property of Elo ratings, but does not conduct experiments to validate these findings in real-world LLM evaluations.
- What evidence would resolve it: Conducting experiments in real-world LLM evaluations with models of varying performance levels and analyzing the transitivity property of Elo ratings, especially in scenarios where models have closely matched performance.

## Limitations
- Reliance on synthetic data for controlled experiments, which may not fully represent the complexity of real human feedback.
- Focus on pairwise comparisons, which may not scale to large-scale LLM evaluation.
- Lack of a direct comparison to alternative evaluation methods.

## Confidence
- **High:** Elo ratings for LLMs are highly sensitive to comparison order and K-factor choice, particularly for closely matched models.
- **Medium:** The generalizability of the transitivity violation findings, as the synthetic experiments may not fully capture all real-world evaluation scenarios.

## Next Checks
1. Conduct a large-scale human evaluation experiment with a diverse set of LLM prompts and model pairs to validate the order sensitivity and transitivity findings in real-world settings.
2. Compare the stability and reliability of Elo ratings with other pairwise comparison-based evaluation methods, such as Bradley-Terry or Plackett-Luce models, under similar experimental conditions.
3. Investigate the impact of different rating update algorithms (e.g., Bayesian update rules) on the robustness of LLM evaluation, and compare their performance to the standard Elo rating system.