---
ver: rpa2
title: Self-Consistency of Large Language Models under Ambiguity
arxiv_id: '2310.13439'
source_url: https://arxiv.org/abs/2310.13439
tags:
- sequence
- consistency
- answers
- lambda
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates self-consistency of large language models
  (LLMs) on ambiguous tasks where multiple answers are correct. The authors create
  a dataset of ambiguous integer sequences and conduct behavioral experiments using
  OpenAI models.
---

# Self-Consistency of Large Language Models under Ambiguity

## Quick Facts
- arXiv ID: 2310.13439
- Source URL: https://arxiv.org/abs/2310.13439
- Authors: 
- Reference count: 11
- Key outcome: Self-consistency in LLMs increases with model capability without explicit training, with cross-context consistency ranging from 67% to 82% on ambiguous integer sequences.

## Executive Summary
This paper investigates how large language models maintain self-consistency when faced with ambiguous tasks that have multiple correct answers. The authors create a dataset of ambiguous integer sequences and conduct behavioral experiments using OpenAI models to measure cross-context consistency. They find that models exhibit substantial self-consistency (67-82%) even without being trained specifically for this capability, and this consistency increases with model capability. However, models are poorly calibrated in judging their own consistency and often place significant probability on alternative answers despite showing high self-consistency.

## Method Summary
The study uses behavioral experiments with OpenAI models (text-davinci-003, gpt-3.5-turbo, gpt-4) to evaluate self-consistency on ambiguous integer sequence tasks. The authors generate 140 unambiguous and 57 ambiguous integer sequences from 197 possible functions, then prompt models for sequence completion and explanation tasks using greedy decoding (temperature set to 0). Cross-context consistency is measured by comparing model responses across separate context windows, and probability distributions are analyzed to understand how models allocate weight to alternative answers. The study also evaluates models' ability to judge their own consistency and investigates effects of speaker changes and sequence length modifications.

## Key Results
- Cross-context consistency ranges from 67% to 82%, far exceeding random consistency levels
- Model consistency increases with capability (text-davinci-003 to gpt-4)
- Models place significant probability on alternative answers despite high self-consistency
- Models are uncalibrated when judging their own consistency, showing both over- and under-confidence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-consistency in LLMs increases with model capability without explicit training.
- Mechanism: Larger models have more internal representations and computational capacity to maintain coherence across contexts, even when multiple correct answers exist.
- Core assumption: Model capability scales with the ability to internally represent and select consistent answers.
- Evidence anchors:
  - [abstract] "average consistency ranges from 67% to 82%, far higher than would be predicted if a model's consistency was random, and increases as model capability improves"
  - [section 3.2] "models improve in consistency as they improve in arithmetical capability from text-davinci-003 to gpt-4"
  - [corpus] No direct evidence, but related works like "Self-Consistency Preference Optimization" suggest capability scaling effects
- Break condition: If capability scaling does not translate to internal consistency mechanisms, or if consistency plateaus at some capability threshold.

### Mechanism 2
- Claim: Models assign non-trivial probability mass to alternative correct answers even when showing high self-consistency.
- Mechanism: Internal representation of multiple valid continuations persists despite greedy decoding selecting one answer, as evidenced by probability distributions.
- Core assumption: Output probability distributions reflect internal representation of multiple possible answers.
- Evidence anchors:
  - [abstract] "models usually place significant weight on alternative, inconsistent answers"
  - [section 5.1] "despite increases in self-consistency, models usually place significant weight on alternative, inconsistent answers"
  - [corpus] "It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning" suggests models struggle with considering alternatives
- Break condition: If probability distributions are not well-calibrated or if models have learned to suppress alternative representations during training.

### Mechanism 3
- Claim: Models are poorly calibrated in judging their own consistency across contexts.
- Mechanism: Models cannot accurately assess whether their separate context answers align, leading to over- and under-confidence.
- Core assumption: Self-evaluation of consistency requires meta-cognitive capabilities not present in current models.
- Evidence anchors:
  - [abstract] "models are uncalibrated when judging their own consistency, with models displaying both over- and under-confidence"
  - [section 3.2] "models tend to consider their answers consistent when they are not, except for gpt-4 which underestimates its own consistency"
  - [corpus] No direct evidence, but calibration issues are well-documented in LLM research
- Break condition: If models develop better self-awareness or if consistency checking is externalized to separate evaluation models.

## Foundational Learning

- Concept: Ambiguous integer sequences
  - Why needed here: The task requires understanding how models handle multiple valid answers in a controlled arithmetic setting
  - Quick check question: What makes an integer sequence "ambiguous" in the context of this paper?

- Concept: Cross-context consistency
  - Why needed here: The paper measures consistency between separate context windows for completion and explanation tasks
  - Quick check question: How is cross-context consistency measured between completion and explanation responses?

- Concept: Probability distribution analysis
  - Why needed here: Understanding how models allocate probability mass to alternative answers requires analyzing output distributions
  - Quick check question: What does the distribution of log probabilities tell us about model internal representations?

## Architecture Onboarding

- Component map: Prompt generation -> API call with greedy decoding -> Parse response -> Check consistency -> Aggregate results across runs
- Critical path: Prompt generation → API call with greedy decoding → Parse response → Check consistency → Aggregate results across runs
- Design tradeoffs: Using greedy decoding provides best-case consistency analysis but doesn't capture sampling behavior; API access limits control over model internals
- Failure signatures: Inconsistent results across runs may indicate model non-determinism; low precision in verbalization tasks suggests difficulty representing alternatives
- First 3 experiments:
  1. Test basic cross-context consistency on unambiguous sequences to establish baseline capability
  2. Run consistency analysis with base-2 sequences to check robustness to difficulty changes
  3. Perform speaker change experiments to verify consistency isn't prompt artifact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of model training (e.g., data quality, RLHF, architecture) contribute most to the emergence of self-consistency in LLMs?
- Basis in paper: [explicit] The paper notes that self-consistency emerges without specific training for it, and suggests that the increase in cross-context consistency results from RLHF and pre-training, but cannot definitively rule out the impact of other factors due to the closed-source nature of the models.
- Why unresolved: The study uses closed-source models, preventing direct analysis of training data and processes.
- What evidence would resolve it: Access to model internals and training data, or experiments with open-source models trained with varying techniques, would clarify the contributions of different factors.

### Open Question 2
- Question: How does self-consistency in LLMs under ambiguity generalize to more complex, real-world tasks involving natural language?
- Basis in paper: [explicit] The paper acknowledges that ambiguous integer sequences are an idealized domain and results may not generalize to natural language tasks.
- Why unresolved: The current study focuses on a simplified, mathematical domain, which may not capture the complexities of natural language.
- What evidence would resolve it: Applying the consistency framework to tasks involving natural language ambiguity (e.g., question answering, summarization) and comparing results would provide insights.

### Open Question 3
- Question: To what extent do LLMs' probability distributions over multiple possible answers reflect their internal reasoning processes?
- Basis in paper: [explicit] The paper finds that even highly self-consistent models place significant probability on alternative answers, suggesting they internally compute multiple possible responses. However, the paper also notes that RLHF may encourage models to allocate probability mass narrowly.
- Why unresolved: The study only has access to the log probabilities of one model and cannot directly observe internal reasoning.
- What evidence would resolve it: Analyzing the probability distributions and token-level activations of models across a range of ambiguous tasks could reveal how models represent and reason about multiple possibilities.

## Limitations

- Dataset Representativeness: The ambiguous integer sequences represent a constrained mathematical space that may not capture the full complexity of real-world ambiguity.
- API-based Methodology: Using OpenAI's API with greedy decoding limits experimental control and may not reflect sampling-based behavior.
- Self-Evaluation Calibration: The exact nature of models' miscalibration in judging their own consistency and its practical implications remain unclear.

## Confidence

- High Confidence: The finding that cross-context consistency ranges from 67% to 82% and increases with model capability is well-supported by the experimental design and results.
- Medium Confidence: The claim that models place significant probability on alternative answers is supported by probability distribution analysis, but interpretation requires additional assumptions.
- Low Confidence: The assertion that apparent consistency does not indicate internal consistency is primarily inferential and would require additional experiments to verify.

## Next Checks

- Validation Check 1: Test consistency under varying temperature settings (e.g., 0.1, 0.5, 1.0) to determine whether the observed consistency patterns hold under sampling-based decoding.
- Validation Check 2: Apply the consistency evaluation framework to a different domain of ambiguous tasks (e.g., natural language reasoning, visual reasoning) to assess whether findings generalize beyond arithmetic sequences.
- Validation Check 3: Design experiments to probe whether models can explicitly recognize and articulate the ambiguity in sequences when prompted appropriately.