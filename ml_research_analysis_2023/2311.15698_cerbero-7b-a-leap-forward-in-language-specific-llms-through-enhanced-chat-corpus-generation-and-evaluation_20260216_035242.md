---
ver: rpa2
title: 'Cerbero-7B: A Leap Forward in Language-Specific LLMs Through Enhanced Chat
  Corpus Generation and Evaluation'
arxiv_id: '2311.15698'
source_url: https://arxiv.org/abs/2311.15698
tags:
- corpus
- language
- italian
- corpora
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel method for generating high-quality,
  language-specific chat corpora through self-chat mechanisms using open-source models.
  The approach combines a generator LLM for creating new samples and an embedder LLM
  to ensure diversity, and introduces a new Masked Language Modeling (MLM)-based quality
  assessment metric for evaluating and filtering corpora.
---

# Cerbero-7B: A Leap Forward in Language-Specific LLMs Through Enhanced Chat Corpus Generation and Evaluation

## Quick Facts
- arXiv ID: 2311.15698
- Source URL: https://arxiv.org/abs/2311.15698
- Reference count: 29
- Key outcome: Introduces self-chat generation with MLM-based quality assessment to create cerbero-7B, achieving 72.55% F1 and 55.6% EM on SQuAD-it, surpassing other Italian LLMs.

## Executive Summary
This paper presents a novel method for generating high-quality, language-specific chat corpora using self-chat mechanisms with open-source models. The approach combines a generator LLM (llama2-70b) for creating new samples and an embedder LLM (multilingual sentence transformer) to ensure diversity, along with a new Masked Language Modeling (MLM)-based quality assessment metric. The authors generate an Italian chat corpus by refining the Fauno corpus (translated English ChatGPT self-chat data), resulting in cerbero-7B, a state-of-the-art Italian LLM with enhanced language comprehension and question-answering skills.

## Method Summary
The method involves self-chat corpus generation using llama2-70b as generator and multilingual sentence transformer as embedder, applying diversity filtering with cosine similarity threshold of 0.9. The corpus quality is evaluated using an MLM-based NLL metric, and the refined corpora are used to fine-tune mistral-7b. The approach combines the Generated corpus with the refined Fauno corpus for fine-tuning, resulting in cerbero-7B.

## Key Results
- cerbero-7B achieves 72.55% F1 and 55.6% EM on SQuAD-it, outperforming other Italian LLMs
- The generated corpus shows similar NLL distribution to OASST, indicating successful emulation of high-quality conversation patterns
- Fine-tuning with combined Generated + Fauno corpus yields better performance than either individually on SQuAD-it and EVALITA benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-chat generation combined with diversity filtering produces higher quality corpora than direct translation of existing chat data
- Core assumption: A generative model can create more linguistically diverse and contextually appropriate conversations than translation when combined with diversity filtering
- Evidence anchors:
  - [abstract] "We combine a generator LLM for creating new samples and an embedder LLM to ensure diversity"
  - [section] "Messages displaying a cosine similarity above 0.9 with any vector in the database were excluded. This diversity filter ensured that only unique and varied linguistic elements were added to the corpus"
  - [corpus] Strong - The corpus analysis shows the Generated corpus has similar NLL distribution to OASST, indicating successful emulation of high-quality conversation patterns
- Break condition: If the generator produces repetitive or nonsensical outputs that pass the diversity filter, or if the embedder fails to capture meaningful semantic differences between messages

### Mechanism 2
- Claim: Masked Language Modeling-based quality assessment can effectively evaluate and compare corpus quality
- Core assumption: Sentence predictability by a pre-trained Italian BERT model correlates with human judgments of corpus quality
- Evidence anchors:
  - [abstract] "A new Masked Language Modelling (MLM) model-based quality assessment metric is proposed for evaluating and filtering the corpora"
  - [section] "The computed NLL scores are utilized to evaluate and compare the quality of the original corpus, the filtered corpus, and the generated one"
  - [corpus] Strong - The histogram shows clear differentiation between Original Fauno (high NLL, broad distribution), Filtered Fauno (intermediate), and Generated/OASST (low NLL, narrow distribution)
- Break condition: If the MLM metric fails to correlate with downstream task performance, or if it systematically favors certain types of sentences over others

### Mechanism 3
- Claim: Combining generated corpus with refined existing corpus yields better performance than either alone
- Core assumption: The strengths of different corpus types are complementary and their combination improves model performance
- Evidence anchors:
  - [section] "we conduct a fine-tuning process using a combination of the Fauno corpus and our newly generated corpus"
  - [corpus] Moderate - The fine-tuned model with Full corpus (Generated + Fauno) shows better performance than either individually on SQuAD-it and EVALITA benchmarks
- Break condition: If the combination introduces conflicting training signals or if one corpus type dominates and negates the benefits of the other

## Foundational Learning

- Concept: Self-supervised learning through masked language modeling
  - Why needed here: The quality assessment relies on MLM to evaluate corpus quality by measuring how well the model can predict masked tokens
  - Quick check question: How does masking tokens and computing NLL help assess whether a sentence is well-formed and contextually appropriate?

- Concept: Vector embeddings and cosine similarity for diversity measurement
  - Why needed here: The corpus generation uses sentence embeddings to ensure diversity by filtering out messages too similar to existing ones
  - Quick check question: Why would messages with cosine similarity > 0.9 be considered too similar and potentially redundant?

- Concept: Fine-tuning vs. pre-training in LLMs
  - Why needed here: The study focuses on fine-tuning a base model (mistral-7b) with domain-specific corpora rather than pre-training from scratch
  - Quick check question: What are the key differences between fine-tuning an existing LLM and training one from scratch, particularly regarding data requirements and computational costs?

## Architecture Onboarding

- Component map: Generator LLM (llama2-70b) → Diversity Filter → Quality Assessment (MLM) → Fine-tuning pipeline (mistral-7b)
- Critical path: Generator → Diversity Filter → Quality Assessment → Fine-tuning → Evaluation
- Design tradeoffs:
  - Higher diversity threshold (lower cosine similarity) → more unique content but potentially fewer valid conversations
  - More aggressive MLM filtering → higher quality but smaller corpus size
  - Combining corpora → better performance but more complex data management
- Failure signatures:
  - Generator produces repetitive patterns → diversity filter becomes ineffective
  - MLM quality assessment doesn't correlate with downstream performance → metric is flawed
  - Fine-tuning overfits to specific corpus → poor generalization on benchmarks
- First 3 experiments:
  1. Generate 100 conversations with default parameters and visualize the diversity distribution to verify the filtering works
  2. Compute NLL scores for a small sample of sentences and manually verify if high/low scores correspond to quality
  3. Fine-tune mistral-7b with only the Generated corpus and test on a small subset of SQuAD-it to establish baseline performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MLM-based quality assessment metric perform across different languages, and what are its limitations in cross-linguistic contexts?
- Basis in paper: [explicit] The paper introduces a novel MLM-based quality assessment metric for evaluating corpora but does not extensively discuss its performance across different languages.
- Why unresolved: The metric's effectiveness in languages other than Italian, especially those with different linguistic structures, is not explored.
- What evidence would resolve it: Comparative studies using the metric on corpora in multiple languages, highlighting its adaptability and limitations.

### Open Question 2
- Question: What are the computational trade-offs between using llama2-70b and other potential generator models for corpus generation in terms of quality and efficiency?
- Basis in paper: [inferred] The paper uses llama2-70b as the generator model but does not compare it with other models in terms of computational efficiency and output quality.
- Why unresolved: The choice of generator model impacts both the quality of the generated corpus and the computational resources required, which are not fully analyzed.
- What evidence would resolve it: Empirical comparisons of corpus quality and resource usage across different generator models.

### Open Question 3
- Question: How does the refinement process impact the diversity of the corpus, and what is the optimal balance between diversity and coherence?
- Basis in paper: [explicit] The paper discusses refining the Fauno corpus using structural assertions and NLP techniques but does not quantify the impact on diversity.
- Why unresolved: The trade-off between maintaining diversity and ensuring coherence in the refined corpus is not fully explored.
- What evidence would resolve it: Studies measuring corpus diversity before and after refinement, alongside coherence assessments, to identify an optimal balance.

## Limitations

- The evaluation relies heavily on a single masked language modeling metric (NLL-based MLM) to assess corpus quality, which may not fully capture conversational naturalness or domain relevance
- The fine-tuning methodology lacks details on hyperparameter optimization and training stability measures
- While cerbero-7b achieves state-of-the-art results on Italian benchmarks, the comparison pool of Italian LLMs is relatively small, and generalizability to other Italian language tasks remains uncertain

## Confidence

- High Confidence: The generation and diversity filtering pipeline (Mechanism 1) is well-documented with clear implementation details and observable quality improvements in corpus NLL scores
- Medium Confidence: The MLM-based quality assessment metric (Mechanism 2) shows clear differentiation between corpora but requires validation that NLL scores correlate with human judgments of quality
- Medium Confidence: The combined corpus approach (Mechanism 3) demonstrates performance improvements but lacks ablation studies to isolate the contribution of each corpus component

## Next Checks

1. **Human Evaluation of Corpus Quality**: Recruit native Italian speakers to rate a sample of 100 conversations from Original Fauno, Filtered Fauno, and Generated corpora on naturalness, coherence, and relevance scales. Compare these subjective ratings against the NLL scores to validate the MLM quality metric.

2. **Ablation Study on Corpus Components**: Fine-tune separate models using only the Generated corpus, only the Filtered Fauno corpus, and the combined corpus. Compare performance across all evaluation metrics to quantify the contribution of each component and determine if the combination is synergistic or dominated by one source.

3. **Cross-Lingual Generalization Test**: Evaluate cerbero-7b on English-Italian translation tasks or multilingual benchmarks to assess whether the quality improvements extend beyond purely Italian language understanding, providing evidence for the robustness of the corpus generation approach.