---
ver: rpa2
title: Machine Learning Clifford invariants of ADE Coxeter elements
arxiv_id: '2310.00041'
source_url: https://arxiv.org/abs/2310.00041
tags:
- invariants
- order
- coxeter
- each
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies machine learning to analyze geometric invariants
  of Coxeter elements in ADE root systems (A8, D8, E8) using Clifford algebras. By
  exhaustively computing all Coxeter transformations via high-performance computing,
  the researchers generate datasets of characteristic multivectors (SOCM) for 40320
  permutations of simple roots.
---

# Machine Learning Clifford invariants of ADE Coxeter elements

## Quick Facts
- arXiv ID: 2310.00041
- Source URL: https://arxiv.org/abs/2310.00041
- Reference count: 40
- Key outcome: Machine learning successfully classifies and predicts geometric invariants of Coxeter elements in ADE root systems with near-perfect accuracy

## Executive Summary
This study demonstrates that machine learning can effectively capture the structure of geometric invariants in Coxeter elements of ADE root systems using Clifford algebras. By exhaustively computing all Coxeter transformations via high-performance computing, the researchers generate datasets of characteristic multivectors (SOCM) for 40320 permutations of simple roots. Neural networks successfully classify and predict these invariants with near-perfect accuracy (0.90-0.99), revealing distinct patterns between root systems and invariant orders through principal component analysis. The study validates the experimental mathematics approach of combining computational algebra with data science to uncover analytical insights in Clifford geometric invariants.

## Method Summary
The study uses Python scripts with the galgebra package to compute Coxeter versors and their characteristic multivectors (SOCM) for all 40320 permutations of simple roots in A8, D8, and E8 root systems. Dense neural networks with ReLU activation, Adam optimizer, and MSE loss are trained on 80% of the data with 5-fold cross-validation to predict invariants from permutations. Fake invariant datasets are created by sampling from empirical distributions to test classification performance. Gradient saliency analysis and PCA are applied to interpret the neural network decisions and reveal structural patterns in the invariant space.

## Key Results
- Neural networks achieve classification accuracies of 0.90-0.99 for distinguishing real vs fake invariants and A8 vs D8 vs E8 root systems
- Near-perfect regression accuracy (0.9996-0.9999) for predicting individual invariant coefficients from permutations
- PCA reveals distinct clustering patterns between root systems and shows mirror symmetry between invariant orders
- Gradient saliency analysis identifies which permutation elements most influence specific invariant coefficients

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The high performance of neural networks in predicting Clifford invariants stems from the combinatorial structure of Coxeter elements mapping permutations to algebraic outputs.
- **Mechanism:** The neural network learns the underlying algebraic relationships between permutation order and resulting invariant coefficients, despite the large input space (40320 permutations) collapsing to only 128 unique outputs due to degeneracy.
- **Core assumption:** The mapping from permutations to invariants has sufficient regularity and structure that can be captured by learned function approximation.
- **Evidence anchors:**
  - [section] "The 40320 input permutations are a large enough number to make these examples accessible to data science techniques such as machine learning classification tasks"
  - [section] "We see that the datasets can be machine learned to very high accuracy"
- **Break condition:** If the algebraic structure between permutations and invariants is too irregular or discontinuous, the neural network would fail to generalize beyond the training data.

### Mechanism 2
- **Claim:** Principal Component Analysis reveals distinct patterns between root systems (A8, D8, E8) and invariant orders through variance decomposition.
- **Mechanism:** PCA identifies the principal directions of variance in the high-dimensional invariant space, revealing structural differences between root systems and the mirror symmetry between invariant orders.
- **Core assumption:** The variance structure in the invariant data contains meaningful information about the underlying algebraic and geometric relationships.
- **Evidence anchors:**
  - [section] "The results for the 9 individual invariants of A8, D8 and E8 are shown in Figures 11, 12 and 13 respectively and the PCA results on the combined invariant data are shown in Figure 8"
  - [section] "The explained variance ratio is a measure of the proportion of the total variance in the original dataset that is explained by each principal component"
- **Break condition:** If the variance structure is dominated by noise or irrelevant factors, PCA would not reveal meaningful patterns about the root systems or invariant relationships.

### Mechanism 3
- **Claim:** Gradient saliency analysis identifies which permutation elements most influence the computed invariants, revealing structural dependencies.
- **Mechanism:** By computing gradients of the neural network output with respect to permutation inputs, we identify which positions in the permutation vector are most critical for determining specific invariant coefficients.
- **Core assumption:** The neural network has learned meaningful relationships between input permutations and output invariants that can be interpreted through gradient analysis.
- **Evidence anchors:**
  - [section] "To better interpret the decision-making of our NN models – which are black-box models – we also performed gradient saliency analysis [68]"
  - [section] "The magnitude of the gradient indicates how sensitive the output is to a change in the input variable"
- **Break condition:** If the neural network has memorized the training data rather than learned generalizable relationships, gradient saliency would reveal spurious or non-robust dependencies.

## Foundational Learning

- **Concept:** Clifford algebras and geometric products
  - Why needed here: The entire study relies on computing invariants using Clifford algebraic operations like versors and simplicial derivatives
  - Quick check question: What is the geometric product and how does it relate to the inner and outer products?

- **Concept:** Root systems and Coxeter groups
  - Why needed here: The study focuses on Coxeter transformations in specific root systems (A8, D8, E8), requiring understanding of simple roots, reflections, and Coxeter elements
  - Quick check question: How does a Coxeter element relate to the simple reflections in a root system?

- **Concept:** Neural network training and optimization
  - Why needed here: The machine learning component uses neural networks with specific architectures, loss functions, and optimization algorithms to learn the invariant relationships
  - Quick check question: What is the role of the activation function in a neural network layer?

## Architecture Onboarding

- **Component map:** Permutation generation -> Clifford invariant computation -> Neural network training -> PCA analysis -> Gradient saliency visualization
- **Critical path:** 1) Generate all 40320 permutations of simple roots 2) Compute Coxeter versors and invariants using Clifford algebra 3) Train neural networks to predict invariants 4) Analyze results using PCA and gradient saliency 5) Validate against theoretical expectations
- **Design tradeoffs:**
  - Computational cost vs. accuracy: Using 8-dimensional root systems balances computational tractability with sufficient complexity
  - Model complexity vs. interpretability: Dense neural networks provide good performance but require gradient saliency for interpretability
  - Data augmentation vs. overfitting: Including fake data helps prevent overfitting but may introduce noise
- **Failure signatures:**
  - Perfect training accuracy but poor test performance indicates overfitting
  - Random gradient saliency patterns suggest the model has not learned meaningful relationships
  - PCA results showing no separation between root systems suggest insufficient discriminative power
- **First 3 experiments:**
  1. Train a simple neural network (1 hidden layer, 32 units) on A8 data to predict invariants from permutations, measure training and test accuracy
  2. Apply PCA to the A8 invariant dataset and visualize the first two principal components to check for clustering patterns
  3. Compute gradient saliency for the trained model to identify which permutation positions most influence the predicted invariants

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific analytical relationships between the characteristic multivectors and other well-known geometric invariants like the characteristic polynomial and exponents?
- Basis in paper: Explicit - The paper mentions "systematically related to geometric invariant spaces of the linear transformation and the coefficients in the characteristic polynomial and Cayley-Hamilton theorem" and discusses connections to exponents.
- Why unresolved: The paper states these relationships are being explored more fully in a companion paper, indicating the analytical work is ongoing.
- What evidence would resolve it: A mathematical proof or derivation showing the exact formulas relating characteristic multivectors to characteristic polynomial coefficients and Coxeter exponents.

### Open Question 2
- Question: How does the connectivity structure of bivector subinvariants relate to the underlying root system geometry?
- Basis in paper: Explicit - The paper discusses analyzing the "connectivity structure of the bivector invariants" and finding patterns in "how these different order subinvariants span different subspaces."
- Why unresolved: While the paper identifies patterns and differences between root systems, it doesn't provide a complete theoretical explanation for why these patterns exist.
- What evidence would resolve it: A mathematical framework explaining why certain bivector graphs have specific connectivity patterns based on the root system's Dynkin diagram structure.

### Open Question 3
- Question: What is the optimal machine learning architecture for predicting characteristic multivectors from Coxeter element permutations?
- Basis in paper: Inferred - The paper shows successful results with dense neural networks but notes "the actual number of Coxeter versors will be lower" due to degeneracy, suggesting potential for optimization.
- Why unresolved: The paper demonstrates feasibility but doesn't explore whether more specialized architectures could improve prediction accuracy or efficiency.
- What evidence would resolve it: Comparative studies showing whether alternative ML architectures (e.g., graph neural networks, attention mechanisms) outperform dense networks on this specific task.

## Limitations
- The dataset size (40,320 permutations per root system) is relatively small for deep learning applications, potentially limiting the model's ability to generalize to more complex scenarios
- The study focuses exclusively on 8-dimensional root systems (A8, D8, E8), limiting applicability to other Coxeter systems
- Perfect classification accuracy for trivial invariants (1.0000) may indicate data leakage or overly simplistic patterns

## Confidence
- **High Confidence:** Neural network classification and regression performance metrics are well-documented and reproducible
- **Medium Confidence:** PCA results showing distinct patterns between root systems, as these depend on specific implementation details of the visualization and analysis
- **Medium Confidence:** Gradient saliency interpretations, as the meaning of "important features" can vary depending on the specific gradient computation method

## Next Checks
1. Test model performance on synthetic perturbations of the permutation data to assess robustness to noise and verify that the learned relationships are not memorizing specific input patterns
2. Implement k-fold cross-validation with k>5 to better estimate model generalization performance and identify potential overfitting
3. Compare neural network predictions against analytical solutions for smaller root systems (A2, A3) where closed-form expressions for invariants are available