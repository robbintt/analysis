---
ver: rpa2
title: 'Routing to the Expert: Efficient Reward-guided Ensemble of Large Language
  Models'
arxiv_id: '2311.08692'
source_url: https://arxiv.org/abs/2311.08692
tags:
- reward
- llms
- zooter
- routing
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently assembling large
  language models (LLMs) with heterogeneous expertise to achieve better performance
  across diverse tasks. The core method idea is ZOOTER, a reward-guided query routing
  method that uses silver supervision from off-the-shelf reward models to train a
  routing function.
---

# Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models

## Quick Facts
- arXiv ID: 2311.08692
- Source URL: https://arxiv.org/abs/2311.08692
- Reference count: 4
- This paper addresses the problem of efficiently assembling large language models (LLMs) with heterogeneous expertise to achieve better performance across diverse tasks.

## Executive Summary
This paper proposes ZOOTER, a reward-guided query routing method that efficiently assembles large language models (LLMs) with heterogeneous expertise. The core idea is to use silver supervision from off-the-shelf reward models to train a routing function that distributes each query to the most expert LLM, avoiding the significant computation overhead of reward model ranking methods. ZOOTER also integrates a tag-based label enhancement to mitigate noise from uncertainty when using rewards as silver supervision. The method achieves better average performance than the best single model and ranks first on 44% of tasks, while introducing only minor computation overhead compared to reward model ranking methods.

## Method Summary
ZOOTER is a reward-guided routing method that distills rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it. The method uses off-the-shelf aligned LLMs as candidates and silver supervision from off-the-shelf reward models. A tag-based label enhancement is integrated to mitigate noise from uncertainty when using rewards as silver supervision. The routing function is trained via knowledge distillation using normalized reward distributions as supervision, capturing latent expertise patterns among candidate LLMs.

## Key Results
- ZOOTER outperforms the best single model on average and ranks first on 44% of tasks.
- ZOOTER surpasses multiple reward model ranking methods while introducing only minor computation overhead.
- The method demonstrates computational efficiency in inference by routing to a single expert LLM rather than generating from all candidates.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reward-guided routing leverages silver supervision from reward models to train a routing function that distributes queries to the most expert LLM.
- Mechanism: The routing function is trained via knowledge distillation using normalized reward distributions as supervision, capturing latent expertise patterns among candidate LLMs.
- Core assumption: Off-the-shelf aligned LLMs have heterogeneous expertise across different domains and tasks, which can be revealed through reward model rankings.
- Evidence anchors:
  - [abstract] "Zooter, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it."
  - [section] "Therefore, the normalized reward can be used as a silver supervision for the routing function"
  - [corpus] Found 25 related papers - weak evidence for this specific mechanism, but shows active research in reward-guided routing.
- Break condition: If reward models cannot accurately distinguish expertise differences between LLMs, or if candidate LLMs have overlapping expertise that makes routing ambiguous.

### Mechanism 2
- Claim: Tag-based label enhancement mitigates noise from reward model uncertainty during routing function training.
- Mechanism: Instruction tags are used to aggregate rewards at a coarse-grained level, creating a smoothed reward signal that reduces the impact of individual reward model noise.
- Core assumption: Reward models exhibit uncertainty in their scalar rewards, which can be partially smoothed by leveraging common patterns across similarly tagged queries.
- Evidence anchors:
  - [abstract] "We also integrate a tag-based label enhancement to mitigate noise from uncertainty when using rewards as silver supervision."
  - [section] "samples with lower reward entropy tend to have high MT-bench scores" and "We interpret this observation as higher reward entropy reveals more uncertainty in the reward."
  - [corpus] Weak evidence - related papers focus on reward-guided methods but don't specifically address tag-based enhancement.
- Break condition: If tag distribution doesn't correlate with task similarity, or if tagger quality is poor, the smoothing may introduce bias rather than reduce noise.

### Mechanism 3
- Claim: ZOOTER achieves computational efficiency by routing to a single expert LLM rather than generating from all candidates.
- Mechanism: During inference, the routing function categorizes each input query and selects the most appropriate expert LLM, avoiding the need to generate outputs from all candidate models.
- Core assumption: The routing function can accurately predict which LLM has expertise for a given query, making single-model inference sufficient.
- Evidence anchors:
  - [abstract] "ZOOTER shows computation efficiency in inference as it introduces only a minor computation overhead of a routing function compared with reward model ranking methods."
  - [section] "The routing function categorizes the input query to an LLM with the strongest expertise potential in this query, and the LLM will generate an expert response."
  - [corpus] Found 25 related papers - several discuss efficient routing/ensemble methods, supporting the premise of computational efficiency.
- Break condition: If routing accuracy is poor, the efficiency gain is negated by performance degradation from incorrect model selection.

## Foundational Learning

- Concept: Knowledge distillation
  - Why needed here: The routing function learns from reward distributions using KL divergence loss, which is a form of knowledge distillation where the reward model's preferences are transferred to the router.
  - Quick check question: How does knowledge distillation enable the routing function to learn latent expertise patterns without requiring labeled training data?

- Concept: Reward modeling for preference learning
  - Why needed here: Off-the-shelf reward models provide scalar scores that indicate LLM quality on specific queries, serving as the silver supervision for routing function training.
  - Quick check question: What are the limitations of using reward model scores as supervision, and how does the tag-based enhancement address these limitations?

- Concept: Ensemble methods and complementary expertise
  - Why needed here: The fundamental assumption is that combining LLMs with heterogeneous expertise yields better performance than any single model, motivating the routing approach.
  - Quick check question: How does the concept of complementary expertise differ from traditional ensemble methods that combine model outputs?

## Architecture Onboarding

- Component map:
  - Candidate LLMs (6 × 13B parameter models)
  - Diverse training query set (DIVINSTRUCT with 47,986 queries)
  - Off-the-shelf reward model (QwenRM used in experiments)
  - Tag-based label enhancement module (with hyperparameter β)
  - Routing function (trained from mdeberta-v3-base)
  - Evaluation benchmarks (26 subsets across 4 benchmark groups)

- Critical path: Training queries → Reward model scoring → Tag-based enhancement → Routing function training → Inference routing → Expert LLM generation

- Design tradeoffs:
  - Router model size (86M parameters) vs. routing accuracy
  - β hyperparameter in label enhancement (balancing sample-level vs. tag-level rewards)
  - Choice of reward model (QwenRM chosen for best performance vs. parameter efficiency)
  - Diversity of training queries vs. computation cost for reward scoring

- Failure signatures:
  - Poor routing accuracy on validation set (router not learning expertise patterns)
  - High reward entropy in training data (indicating reward model uncertainty)
  - Performance degradation compared to best single model (router making incorrect selections)
  - Sensitivity to β hyperparameter (overfitting to either sample-level or tag-level rewards)

- First 3 experiments:
  1. Train routing function without tag-based enhancement (β=1) and evaluate routing accuracy on held-out validation queries
  2. Sweep β hyperparameter (0, 0.3, 0.5, 0.7, 0.9, 1.0) and measure impact on MTR across all benchmarks
  3. Compare routing function performance using different reward models (QwenRM, UltraRM, Auto-J) to assess sensitivity to reward model choice

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the uncertainty in reward model scores affect the quality of the silver supervision used for training the routing function in ZOOTER?
- Basis in paper: [explicit] The paper discusses that reward models introduce uncertainty in their scalar rewards, which may introduce noise in the routing function training.
- Why unresolved: The paper acknowledges the existence of this uncertainty but does not provide a detailed analysis of its impact on the quality of the silver supervision or how it affects the performance of the routing function.
- What evidence would resolve it: Empirical results comparing the performance of ZOOTER when using rewards with different levels of uncertainty, or a theoretical analysis of how reward uncertainty propagates through the routing function training process.

### Open Question 2
- Question: What is the optimal balance between fine-grained sample-level rewards and coarse-grained tag-level rewards in the tag-based label enhancement method proposed in ZOOTER?
- Basis in paper: [explicit] The paper mentions a hyperparameter β that represents the trade-off between sample-level and tag-level rewards in the tag-based label enhancement, but does not provide a definitive answer on the optimal value of β.
- Why unresolved: The paper presents a range of β values and their corresponding performance, but does not determine the optimal value or provide a clear rationale for why a particular value is best.
- What evidence would resolve it: A more extensive ablation study with a wider range of β values and additional benchmarks to determine the optimal balance between sample-level and tag-level rewards.

### Open Question 3
- Question: How does the performance of ZOOTER compare to other ensemble methods that use different strategies for combining the outputs of multiple language models?
- Basis in paper: [inferred] The paper compares ZOOTER to reward model ranking methods and a best single model, but does not explore other ensemble strategies such as weighted averaging or stacking.
- Why unresolved: The paper focuses on comparing ZOOTER to reward model ranking and the best single model, but does not provide a comprehensive comparison with other ensemble methods.
- What evidence would resolve it: Experimental results comparing ZOOTER to other ensemble methods such as weighted averaging, stacking, or other model combination strategies on the same benchmarks used in the paper.

## Limitations
- The effectiveness of ZOOTER depends critically on the quality and coverage of the training query set, which may not fully capture the breadth of real-world tasks.
- The tag-based label enhancement introduces an additional hyperparameter (β) that requires careful tuning and may be less robust than claimed.
- The method assumes that off-the-shelf reward models can reliably distinguish between LLM expertise levels, which may not hold for highly specialized domains or emerging task types.

## Confidence
- **High Confidence**: The computational efficiency claims are well-supported by the architecture - routing to a single expert model rather than generating from all candidates is inherently more efficient.
- **Medium Confidence**: The mechanism claims about knowledge distillation and reward-guided learning are theoretically sound, but the paper lacks detailed analysis of routing accuracy on held-out validation sets.
- **Low Confidence**: The generalizability of the tag-based enhancement mechanism is uncertain, as the paper demonstrates effectiveness on a specific dataset but doesn't establish whether this approach transfers well to different query distributions or tagging schemes.

## Next Checks
1. **Routing Accuracy Validation**: Evaluate the trained routing function on a held-out validation set with known ground truth (e.g., human-annotated expertise labels) to measure routing precision and recall, independent of downstream performance metrics.
2. **Reward Model Sensitivity Analysis**: Systematically test ZOOTER with different reward models (UltraRM, Auto-J, and others) to quantify the sensitivity of routing performance to reward model choice, and identify whether the method is robust across different reward model architectures.
3. **Tag Distribution Robustness**: Test the tag-based enhancement with synthetic tag distributions that vary in correlation with task similarity (e.g., random tags, highly correlated tags, moderately correlated tags) to establish the conditions under which the enhancement helps versus harms performance.