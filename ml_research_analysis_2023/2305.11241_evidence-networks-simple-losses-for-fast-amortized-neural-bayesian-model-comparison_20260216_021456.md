---
ver: rpa2
title: 'Evidence Networks: simple losses for fast, amortized, neural Bayesian model
  comparison'
arxiv_id: '2305.11241'
source_url: https://arxiv.org/abs/2305.11241
tags:
- evidence
- data
- bayes
- network
- factor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Evidence Networks enable fast amortized neural Bayesian model comparison
  even when nested sampling fails and likelihoods or priors are intractable or unknown.
  The method trains neural networks to directly estimate convenient functions of the
  Bayes factor (e.g., log Bayes factor) using bespoke loss functions.
---

# Evidence Networks: simple losses for fast, amortized, neural Bayesian model comparison

## Quick Facts
- arXiv ID: 2305.11241
- Source URL: https://arxiv.org/abs/2305.11241
- Reference count: 20
- One-line primary result: Evidence Networks enable fast amortized neural Bayesian model comparison even when nested sampling fails and likelihoods or priors are intractable or unknown

## Executive Summary
Evidence Networks provide a novel approach to Bayesian model comparison that bypasses the need for explicit likelihood or prior knowledge. By training neural networks with bespoke loss functions, the method directly estimates convenient functions of the Bayes factor from data alone. This allows for fast, amortized model comparison in high-dimensional settings where traditional methods like nested sampling fail or become computationally intractable.

The key innovation lies in the design of specific loss functions, such as the l-POP-Exponential loss, which yield neural network outputs that are consistent estimators of the desired function of the Bayes factor. The approach is validated using a blind coverage test when ground truth Bayes factors are unavailable, demonstrating robust performance across several orders of magnitude in Bayes factor values. Evidence Networks show particular promise for applications in fields where model comparison is essential but traditional methods are computationally prohibitive.

## Method Summary
Evidence Networks train neural networks to directly estimate functions of the Bayes factor using bespoke loss functions, bypassing the need for explicit likelihood or prior knowledge. The method generates labeled training data from competing models, then optimizes a network to map data to Bayes factor estimates. The l-POP-Exponential loss function is key, providing stable gradients across many orders of magnitude in Bayes factor values. After training, the network can rapidly estimate Bayes factors for new data, with validation performed via blind coverage testing when ground truth is unavailable.

## Key Results
- Accurate Bayes factor estimation over many orders of magnitude with root-mean-square error ≈ 0.02 on a 100-dimensional synthetic time series problem
- Robust performance independent of parameter space dimensionality, successfully applied to real Dark Energy Survey gravitational lensing data
- Outperforms nested sampling in high dimensions and avoids the curse of dimensionality that plagues alternative density estimation methods

## Why This Works (Mechanism)

### Mechanism 1
Evidence Networks bypass the need to evaluate high-dimensional parameter integrals by estimating functions of the Bayes factor directly. Instead of computing the marginal likelihood via high-dimensional integration over model parameters θ, Evidence Networks train on simulated data samples paired with model labels, optimizing loss functions that yield estimates of log Bayes factors or posterior probabilities.

Core assumption: The Bayes factor can be estimated accurately from data alone without needing to know or estimate the likelihood or prior explicitly.

Break condition: If the model classes are not distinguishable by data, or if the posterior probability is too complex, the network may fail to converge to the correct Bayes factor.

### Mechanism 2
Specific loss functions, such as the l-POP-Exponential loss, can be designed to yield optimal neural network outputs that directly estimate convenient functions of the Bayes factor. By defining loss functions that optimize for specific transformations of the Bayes factor (e.g., log Bayes factor), the network learns to map data to these quantities without reference to model parameters.

Core assumption: There exists a loss function that yields a neural network output that is a consistent estimator of the desired function of the Bayes factor.

Break condition: If the loss function is poorly chosen, the network may not converge to the correct Bayes factor estimate.

### Mechanism 3
Blind coverage testing provides a validation method for Evidence Networks when the true Bayes factor is unknown. By comparing the estimated model posterior probabilities to the empirical fraction of validation data with each model label, the accuracy of the Bayes factor estimates can be assessed without knowing the ground truth.

Core assumption: The network, if optimally trained, will recover the true posterior probabilities, and the empirical model label frequencies in validation data will approximate these posteriors.

Break condition: If the validation data is not representative or if the network is not optimally trained, the coverage test may give misleading results.

## Foundational Learning

- Concept: Bayes factor as a ratio of model evidences.
  - Why needed here: Understanding the Bayes factor is crucial for grasping the goal of Evidence Networks, which is to estimate this quantity directly.
  - Quick check question: What is the relationship between the Bayes factor and the posterior odds under equal model priors?

- Concept: Likelihood-free inference and simulation-based inference.
  - Why needed here: Evidence Networks operate in a setting where likelihoods and priors are often unknown, relying on simulated data instead.
  - Quick check question: How does likelihood-free inference differ from traditional Bayesian inference?

- Concept: Neural density estimation and its limitations.
  - Why needed here: The paper contrasts Evidence Networks with neural density estimation methods, highlighting the curse of dimensionality as a key limitation.
  - Quick check question: Why does neural density estimation suffer from the curse of dimensionality in high-dimensional settings?

## Architecture Onboarding

- Component map: Data simulation -> Network training with bespoke loss -> Bayes factor estimation on new data -> Validation via blind coverage testing
- Critical path: Data simulation → Network training with bespoke loss → Bayes factor estimation on new data → Validation via blind coverage testing
- Design tradeoffs: Simpler networks with fewer parameters may suffice for reliable Bayes factor estimates, but more complex networks may be needed for highly complex posteriors. The choice of loss function impacts accuracy and convergence.
- Failure signatures: Systematic errors in Bayes factor estimates, poor convergence during training, or failure of the blind coverage test indicate potential issues.
- First 3 experiments:
  1. Train an Evidence Network on synthetic data with known Bayes factors (e.g., the time series example) to verify accuracy.
  2. Compare Evidence Network performance to nested sampling on a high-dimensional problem where nested sampling fails.
  3. Apply the Evidence Network to real-world data (e.g., gravitational lensing data) and validate using blind coverage testing.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Evidence Networks scale with the dimensionality of the data (dim(x)) compared to the dimensionality of the parameter space (dim(θ))?
Basis in paper: [explicit] The paper states that Evidence Networks are "explicitly independent of dimensionality of the parameter space" and "scale mildly with the complexity of the posterior probability density function."
Why unresolved: The paper only provides a limited comparison with density estimation methods for a single case with dim(x) = 20 and dim(θ) = 20, but does not explore the full scaling relationship between data and parameter dimensionality.
What evidence would resolve it: A systematic study varying both dim(x) and dim(θ) independently, comparing Evidence Networks to alternative methods like nested sampling and density estimation across a wide range of dimensionalities.

### Open Question 2
What is the theoretical justification for why the l-POP transform outperforms other monotonic transformations like sinh for constructing loss functions?
Basis in paper: [explicit] The paper states that "we have considered the sinh function in the place of Jα, but find the exponential scaling for high K leads to unreliable performance."
Why unresolved: The paper provides empirical evidence for the l-POP transform's superiority but does not offer a theoretical explanation for why this specific transformation works better than alternatives.
What evidence would resolve it: A mathematical analysis of the optimization landscape created by different monotonic transformations, explaining why l-POP maintains stable gradients and avoids numerical issues that affect other choices.

### Open Question 3
How do Evidence Networks perform when the true Bayes factor lies outside the range represented in the training data?
Basis in paper: [inferred] The paper mentions that the l-POP transform "provides the reweighting of errors for high Bayes factor values, ensuring accuracy across many orders of magnitude," but does not test extrapolation beyond training ranges.
Why unresolved: The validation uses blind coverage testing within the range of training data, but the paper does not address what happens when the network encounters data with Bayes factors far outside the training distribution.
What evidence would resolve it: Experiments where the network is trained on data with restricted Bayes factor ranges, then tested on data with systematically shifted or expanded Bayes factor distributions to measure extrapolation performance.

## Limitations
- Generalizability beyond tested examples remains uncertain, particularly for problems with vastly different data structures or complex, non-smooth likelihoods
- Computational cost of generating large training datasets may offset speed advantages in certain applications
- Blind coverage test may not detect all types of systematic errors, particularly those arising from model misspecification or unrepresentative training data

## Confidence
- High Confidence: The core mechanism of using bespoke loss functions to train networks for direct Bayes factor estimation is well-supported by both theoretical derivation and empirical results on synthetic data.
- Medium Confidence: The claim that Evidence Networks outperform nested sampling in high dimensions is supported by the time series example, but may not hold universally across all problem types.
- Medium Confidence: The blind coverage test is a valid validation method in principle, but its sensitivity to various types of model and data misspecification remains to be fully characterized.

## Next Checks
1. Apply Evidence Networks to a diverse set of synthetic problems with known Bayes factors, including those with non-Gaussian posteriors and multimodal distributions, to assess robustness across different data structures.

2. Systematically compare Evidence Network performance against nested sampling on a suite of problems spanning low to high dimensions, including cases where nested sampling is known to struggle, to quantify the conditions under which Evidence Networks provide a clear advantage.

3. Conduct a thorough sensitivity analysis of the blind coverage test by introducing controlled amounts of model misspecification and data contamination to determine the test's ability to detect various types of errors in Bayes factor estimation.