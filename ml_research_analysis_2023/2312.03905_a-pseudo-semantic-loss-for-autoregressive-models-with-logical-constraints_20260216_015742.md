---
ver: rpa2
title: A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints
arxiv_id: '2312.03905'
source_url: https://arxiv.org/abs/2312.03905
tags:
- constraint
- distribution
- neural
- logical
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incorporating logical constraints
  into autoregressive models, such as large language models, which is computationally
  intractable under standard formulations. The authors propose a novel approach that
  approximates the likelihood of a constraint with its probability in a local pseudolikelihood
  distribution centered around a model sample.
---

# A Pseudo-Semantic Loss for Autoregressive Models with Logical Constraints

## Quick Facts
- arXiv ID: 2312.03905
- Source URL: https://arxiv.org/abs/2312.03905
- Reference count: 40
- The paper proposes a pseudo-semantic loss that enables efficient incorporation of logical constraints into autoregressive models by using a local pseudolikelihood approximation, achieving state-of-the-art results on Sudoku, shortest-path prediction, and LLM detoxification tasks.

## Executive Summary
This paper addresses the fundamental challenge of incorporating logical constraints into autoregressive models, where computing the likelihood of constraints under the full model distribution is #P-hard. The authors propose a novel approach that approximates constraint likelihoods using a local pseudolikelihood distribution centered around model samples, enabling tractable computation through knowledge compilation techniques. The method is evaluated on three tasks—Sudoku solving, shortest-path prediction, and LLM detoxification—demonstrating significant improvements in constraint satisfaction while maintaining model performance. The approach achieves state-of-the-art detoxification results by steering language model outputs away from toxic content using a simple word exclusion constraint.

## Method Summary
The proposed method addresses the intractability of enforcing logical constraints under autoregressive distributions by approximating the full likelihood with a local pseudolikelihood distribution centered around model samples. This factorized approximation enables efficient computation using knowledge compilation, where logical constraints are compiled into tractable circuits that support probabilistic queries. The pseudo-semantic loss is computed by sampling from the model, constructing a local distribution of perturbations around the sample, and evaluating the probability of constraint satisfaction within this neighborhood. This approach maintains computational efficiency while providing high-fidelity constraint enforcement, as demonstrated on Sudoku puzzles, Warcraft II pathfinding tasks, and LLM detoxification using GPT-2.

## Key Results
- The method improves Sudoku solving accuracy and consistency compared to baselines while maintaining computational tractability
- Achieves state-of-the-art LLM detoxification performance by reducing toxic generations through simple word exclusion constraints
- Demonstrates low-entropy, high-fidelity approximations of true distributions in the neighborhood of model samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed approach makes logical constraint enforcement tractable under autoregressive models by approximating the full likelihood with a local pseudolikelihood distribution.
- Mechanism: The method centers a factorized pseudolikelihood distribution around a model sample and computes the probability of the constraint only within this local neighborhood, rather than over the entire output distribution. This approximation enables reuse of solutions to sub-problems through knowledge compilation techniques.
- Core assumption: The local pseudolikelihood distribution provides a high-fidelity approximation of the true likelihood around the sampled point, capturing the essential behavior needed for effective constraint enforcement.
- Evidence anchors:
  - [abstract] "Instead of attempting to enforce the constraint on the entire output distribution, we propose to do so on a random, local approximation thereof."
  - [section 3] "We approximate the likelihood of the constraint w.r.t. the autoregressive distribution with its probability in a local pseudolikelihood distribution—a product of conditionals—centered around a model sample."
  - [corpus] Weak evidence - corpus doesn't directly discuss the local approximation mechanism.
- Break condition: If the local pseudolikelihood distribution deviates significantly from the true distribution around the sample, the constraint enforcement may become ineffective or lead to poor model behavior.

### Mechanism 2
- Claim: The approach achieves state-of-the-art LLM detoxification by effectively steering the model's distribution away from toxic generations while maintaining language modeling capabilities.
- Mechanism: By using a simple constraint disallowing a list of toxic words and optimizing the pseudo-semantic loss, the model learns to avoid generating toxic content. The method maintains perplexity close to the base model, indicating preserved language modeling ability.
- Core assumption: The list of toxic words provides sufficient coverage to effectively reduce toxic generations across diverse prompts and contexts.
- Evidence anchors:
  - [abstract] "Using a simple constraint disallowing a list of toxic words, we are able to steer the model's outputs away from toxic generations, achieving SoTA detoxification compared to previous approaches."
  - [section 5] "We also evaluate on the task of large language models (LLMs) detoxification... we are able to steer the model's distribution away from toxic generations, and toward non-toxic ones, all while maintaining its original ability to model text."
  - [corpus] Weak evidence - corpus doesn't directly discuss the detoxification mechanism.
- Break condition: If the toxic word list is incomplete or if toxic content can be generated without using explicitly listed words, the detoxification may be less effective.

### Mechanism 3
- Claim: The method provides low-entropy, high-fidelity approximations of the true distribution in the neighborhood of model samples.
- Mechanism: The local pseudolikelihood distribution allocates most of its mass around the sample point, resulting in lower entropy compared to the full model distribution. This concentration of probability mass indicates that the approximation is focused on relevant perturbations.
- Core assumption: The local neighborhood around the sample contains sufficient information about the constraint satisfaction for effective learning.
- Evidence anchors:
  - [abstract] "Moreover, it is a local, high-fidelity approximation of the likelihood, exhibiting low entropy and KL-divergence around the model sample."
  - [section 5] "We also evaluated the fidelity of our approximation... We find the entropy of GPT-2 is 80.89 bits while the entropy of our approximation is, on average, 35.08 bits."
  - [corpus] Weak evidence - corpus doesn't directly discuss the entropy or fidelity measures.
- Break condition: If the local neighborhood doesn't capture the essential variations needed for constraint satisfaction, the low-entropy approximation may miss important modes of the true distribution.

## Foundational Learning

- Concept: Autoregressive distributions and their computational complexity
  - Why needed here: Understanding why computing constraint likelihoods is #P-hard under autoregressive models is crucial for appreciating the need for approximation.
  - Quick check question: Why is computing the probability of a constraint under an autoregressive distribution #P-hard, even for simple constraints?

- Concept: Knowledge compilation and tractable circuits
  - Why needed here: The approach relies on compiling logical constraints into circuits that support efficient probabilistic queries.
  - Quick check question: What properties must a logical circuit have to enable tractable computation of constraint probabilities under factorized distributions?

- Concept: Pseudolikelihood and local approximations
  - Why needed here: The core of the method is using pseudolikelihood as a surrogate for full likelihood, and understanding how local approximations work.
  - Quick check question: How does pseudolikelihood differ from true likelihood, and why is it useful for tractable computation?

## Architecture Onboarding

- Component map:
  - Model sampler -> Local distribution constructor -> Circuit compiler -> Probability calculator -> Loss function

- Critical path:
  1. Sample from the model
  2. Expand sample to create local perturbations
  3. Compute joint probabilities of perturbations
  4. Normalize to get conditionals
  5. Feed conditionals into compiled circuit
  6. Compute constraint probability
  7. Calculate loss and backpropagate

- Design tradeoffs:
  - Local vs global approximation: Local provides tractability but may miss global structure
  - Sample efficiency: More samples provide better approximation but increase computation
  - Circuit complexity: More complex constraints require larger circuits, affecting runtime

- Failure signatures:
  - High perplexity despite low toxicity: Indicates over-constraining or poor approximation
  - No improvement in constraint satisfaction: Suggests inadequate local approximation
  - Memory errors during circuit compilation: Indicates overly complex constraints

- First 3 experiments:
  1. Sudoku task with simple RNN model to verify basic functionality
  2. Warcraft pathfinding with CNN-LSTM to test on structured output
  3. LLM detoxification with GPT-2 to validate on large-scale language model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the pseudo-semantic loss approach be extended to handle soft constraints that only hold in expectation, rather than hard symbolic constraints?
- Basis in paper: [inferred] The paper mentions that the approach assumes access to hard symbolic knowledge and currently only supports hard constraints, while noting that often times we might be interested in distributional soft constraints.
- Why unresolved: The paper does not explore or propose methods for extending the approach to handle soft constraints.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of the pseudo-semantic loss approach when applied to soft constraints, showing improvements in consistency and accuracy compared to baselines.

### Open Question 2
- Question: What are the theoretical guarantees for the fidelity of the local approximation used in the pseudo-semantic loss approach, and how does it compare to the true distribution in the neighborhood of the model sample?
- Basis in paper: [explicit] The paper evaluates the fidelity of the approximation by comparing the entropy of the approximate distribution to the true distribution and measuring the KL-divergence between them.
- Why unresolved: The paper provides empirical evidence of the approximation's fidelity but does not offer theoretical guarantees or a comprehensive analysis of how the approximation compares to the true distribution.
- What evidence would resolve it: Theoretical analysis and mathematical proofs establishing the conditions under which the local approximation is faithful to the true distribution, along with experimental results validating the theoretical claims.

### Open Question 3
- Question: How does the computational complexity of the pseudo-semantic loss approach scale with the size of the logical constraint and the dimensionality of the output space, and are there more efficient algorithms or optimizations that can be applied?
- Basis in paper: [inferred] The paper mentions that the approach requires a sufficient amount of memory to construct the local distribution centered around the model sample and that the computational complexity might be a limitation.
- Why unresolved: The paper does not provide a detailed analysis of the computational complexity or explore potential optimizations to improve efficiency.
- What evidence would resolve it: Experimental results demonstrating the scalability of the approach with varying sizes of logical constraints and output dimensions, along with comparisons to alternative algorithms or optimizations that improve computational efficiency.

## Limitations

- The method's effectiveness depends on the quality of the local pseudolikelihood approximation, which may fail when the true distribution deviates significantly from the approximation around sampled points
- The computational efficiency gains rely on effective knowledge compilation, which can degrade with increasingly complex constraint structures
- The LLM detoxification results depend on a predefined list of toxic words, which may not capture context-dependent or implicit toxicity

## Confidence

- **High Confidence**: The core algorithmic contribution (pseudo-semantic loss with local pseudolikelihood approximation) is technically sound and the mathematical formulation is rigorous. The empirical improvements on Sudoku and Warcraft tasks are well-supported with clear metrics and ablation studies.

- **Medium Confidence**: The LLM detoxification results, while promising, rely on a relatively simple constraint (word blacklist) and comparisons are made primarily against SOTA methods without extensive ablation of the pseudo-semantic loss contribution. The claim of "SoTA" detoxification would benefit from comparison against a broader set of baselines.

- **Medium Confidence**: The theoretical claims about low-entropy approximations and their fidelity are supported by entropy measurements, but the analysis is limited to a single sample distribution and doesn't explore the full distribution of approximation quality across different contexts.

## Next Checks

1. **Robustness Analysis**: Test the approach on Sudoku and Warcraft tasks with intentionally corrupted logical circuits or perturbed local distributions to quantify how sensitive the constraint satisfaction is to approximation quality.

2. **Generalization Testing**: Apply the method to a broader range of LLM constraints beyond word blacklists (e.g., syntactic constraints, factual consistency constraints) to assess whether the approach generalizes beyond the demonstrated cases.

3. **Computational Scaling Study**: Systematically evaluate how the method's runtime and memory requirements scale with sequence length and constraint complexity, particularly for the top-k approximation mentioned for longer sequences.