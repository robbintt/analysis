---
ver: rpa2
title: 'AutoTrial: Prompting Language Models for Clinical Trial Design'
arxiv_id: '2305.11366'
source_url: https://arxiv.org/abs/2305.11366
tags:
- trial
- criteria
- clinical
- autotrial
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AutoTrial uses language models to aid clinical trial design by
  generating eligibility criteria from trial synopses and instructions. It combines
  discrete and neural prompting for controllable generation, retrieval-augmented exemplars
  for scalable knowledge incorporation, and multi-step reasoning for transparency.
---

# AutoTrial: Prompting Language Models for Clinical Trial Design

## Quick Facts
- arXiv ID: 2305.11366
- Source URL: https://arxiv.org/abs/2305.11366
- Reference count: 18
- Key outcome: Achieves 0.91 F1 clinical accuracy and 60% win rate against GPT-3.5 in human evaluations for clinical trial eligibility criteria generation

## Executive Summary
AutoTrial introduces a novel approach to automating clinical trial eligibility criteria generation using language models. The system combines hybrid prompting (discrete and neural), retrieval-augmented exemplars, and multi-step reasoning to produce high-quality clinical criteria from trial synopses and instructions. Evaluated on over 70,000 clinical trials, AutoTrial demonstrates superior performance compared to baseline models while maintaining transparency through explicit reasoning chains.

## Method Summary
AutoTrial uses a decoder-based language model (GPT-2/T5 backbone) enhanced with hybrid prompting that combines discrete tokens and neural embeddings for instruction control. The system retrieves relevant exemplar criteria from a knowledge store and generates eligibility criteria through multi-step reasoning supervision. Training involves two stages: pretraining on unlabeled trial documents with reasoning supervision, followed by finetuning on instruction-criteria pairs using MLE and contrastive loss. The model supports incremental learning by updating only instruction embeddings while keeping other parameters frozen.

## Key Results
- Achieves 0.91 F1 clinical accuracy on eligibility criteria generation
- 60% win rate against GPT-3.5 in human evaluations
- Outperforms baseline models on both automatic NLG metrics and clinical accuracy
- Demonstrates effective knowledge incorporation through retrieval-augmented exemplars

## Why This Works (Mechanism)

### Mechanism 1
Hybrid prompting combines discrete exemplars with continuous neural prompts to enable both knowledge retrieval and fine-grained instruction control. The model uses external exemplars to provide structured reasoning templates while neural prompts parameterize specific instructions, allowing modular adaptation without full retraining.

### Mechanism 2
Multi-step reasoning supervision improves generation coherence by explicitly modeling intermediate reasoning steps. The model generates intermediate rationales (e.g., <inc> tags) before producing final criteria, creating a traceable reasoning chain that improves alignment with instructions.

### Mechanism 3
Incremental learning enables knowledge updates without catastrophic forgetting through frozen parameters and selective neural prompt updates. When new trials/instructions arrive, only the instruction embedding matrix Er is updated while other parameters remain frozen, preserving previously learned knowledge.

## Foundational Learning

- Concept: Multi-step reasoning and chain-of-thought prompting
  - Why needed here: Clinical eligibility criteria require logical progression from trial characteristics to specific patient requirements
  - Quick check question: Can you trace how a BMI criterion logically follows from trial requirements?

- Concept: In-context learning and retrieval-augmented generation
  - Why needed here: Clinical trials have massive, growing databases requiring efficient knowledge retrieval for protocol design
  - Quick check question: How would you retrieve relevant exemplars for a new oncology trial?

- Concept: Instruction tuning and parameter-efficient adaptation
  - Why needed here: Different trials need different criteria types (inclusion vs exclusion, age vs BMI) requiring fine-grained control
  - Quick check question: What happens to model performance when adding new instruction types?

## Architecture Onboarding

- Component map: Trial setup + exemplars → Hybrid encoding (discrete tokens + neural embeddings) → Multi-step reasoning head → Retrieval module → Output generator
- Critical path: Input trial setup + exemplars → Hybrid encoding → Multi-step generation → Clustering + ranking → Final criteria output
- Design tradeoffs: Larger exemplar space improves coverage but increases retrieval latency; more neural prompt dimensions improve instruction capture but risk overfitting
- Failure signatures: Low clinical accuracy → exemplar retrieval failing or instruction embedding misaligned; poor coherence → multi-step reasoning supervision not working; high hallucination → neural prompting overriding exemplar constraints
- First 3 experiments:
  1. Verify exemplar retrieval accuracy on held-out trials
  2. Test instruction embedding quality with ablation on neural prompts
  3. Validate multi-step generation coherence through intermediate step evaluation

## Open Questions the Paper Calls Out

### Open Question 1
How does AutoTrial's performance compare when using different backbone models (e.g., GPT-3, GPT-4, or other transformer architectures)? The paper focuses on specific backbone models without exploring alternatives.

### Open Question 2
Can AutoTrial handle more complex clinical trial designs, such as adaptive trials or trials with multiple arms and interventions? The current evaluation focuses on simpler trial structures.

### Open Question 3
How does AutoTrial's performance vary across different medical specialties or therapeutic areas? The paper doesn't provide detailed analysis of performance across medical domains.

## Limitations
- Clinical accuracy validated only on ClinicalTrials.gov dataset without external validation
- Human evaluation methodology lacks detail on rater selection and inter-rater reliability
- Incremental learning claims lack quantitative ablation comparing to full retraining approaches

## Confidence

- **High confidence**: Hybrid prompting architecture and multi-step reasoning mechanism are well-specified
- **Medium confidence**: Clinical accuracy results are internally consistent but limited to one dataset
- **Low confidence**: Incremental learning effectiveness claims are weakly supported

## Next Checks

1. **External validation**: Test AutoTrial on clinical trial datasets from different registries (EU Clinical Trials Register, WHO ICTRP) to verify generalization beyond ClinicalTrials.gov

2. **Incremental learning ablation**: Conduct quantitative comparison of incremental vs full retraining approaches across multiple rounds of instruction additions, measuring performance on both new and previously seen instruction types

3. **Human evaluation robustness**: Replicate human evaluation with multiple rounds, different rater pools, and explicit inter-rater reliability metrics to confirm the 60% win rate against GPT-3.5