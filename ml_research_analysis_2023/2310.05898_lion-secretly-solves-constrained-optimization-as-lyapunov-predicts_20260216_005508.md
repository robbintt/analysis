---
ver: rpa2
title: 'Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts'
arxiv_id: '2310.05898'
source_url: https://arxiv.org/abs/2310.05898
tags:
- lion
- have
- domk
- function
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the theoretical foundation of Lion (Evolved
  Sign Momentum), an optimizer discovered through program search that performs comparably
  to AdamW with better memory efficiency. The authors show that Lion can be understood
  as a principled approach for solving constrained optimization problems by incorporating
  decoupled weight decay.
---

# Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts

## Quick Facts
- arXiv ID: 2310.05898
- Source URL: https://arxiv.org/abs/2310.05898
- Reference count: 40
- Key outcome: Lion optimizer is shown to solve constrained optimization problems through weight decay, with a theoretical framework using Lyapunov functions to prove convergence to stationary points.

## Executive Summary
This paper provides a theoretical foundation for Lion (Evolved Sign Momentum), an optimizer discovered through program search that performs comparably to AdamW with better memory efficiency. The authors demonstrate that Lion can be understood as a principled approach for solving constrained optimization problems by incorporating decoupled weight decay. They introduce a general family of Lion-κ algorithms where the sign operator is replaced by the subgradient of a convex function κ, leading to the solution of composite optimization problems. The key insight is the development of a new Lyapunov function that characterizes the dynamics of Lion-κ, proving its convergence to stationary points of the constrained objective.

## Method Summary
The paper analyzes Lion optimizer through a theoretical lens, showing it solves constrained optimization problems. The method introduces Lion-κ, a family of algorithms where the sign function is replaced by subgradients of convex functions. A new Lyapunov function H(x,m) is developed to prove convergence properties. The approach uses continuous-time ODE analysis and discrete-time Lyapunov functions to characterize the algorithm's behavior and convergence to stationary points of constrained objectives.

## Key Results
- Lion implicitly enforces infinity norm constraints through weight decay
- Lion-κ family generalizes constraint enforcement to other convex norms
- New Lyapunov function proves convergence to stationary points of constrained problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lion secretly solves constrained optimization by enforcing an infinity norm bound through weight decay.
- Mechanism: The weight decay coefficient λ in Lion's update equation implicitly imposes the constraint ||x||_∞ ≤ 1/λ. When parameters exceed this bound, the decay term λx in the update pushes them back within the feasible region.
- Core assumption: The Lion dynamics converge to stationary points that satisfy the infinity norm constraint.
- Evidence anchors:
  - [abstract] "Lion is a theoretically novel and principled approach for minimizing a general loss function f(x) while enforcing a bound constraint ||x||_∞ ≤ 1/λ."
  - [section] "Our analysis is made possible by the development of a new Lyapunov function for the Lion updates... Lion achieves this through the incorporation of decoupled weight decay, where λ represents the weight decay coefficient."
- Break condition: If λ becomes too small, the constraint becomes too loose and Lion loses its constrained optimization properties. If λ is too large, the constraint may be overly restrictive and prevent effective optimization.

### Mechanism 2
- Claim: Lion's sign function acts as a normalization that enables convergence to the constraint boundary.
- Mechanism: The sign function in Lion's update normalizes the gradient direction, preventing large parameter updates that would violate the infinity norm constraint. This normalization, combined with weight decay, creates a dynamics that converges to the boundary of the feasible region.
- Core assumption: The sign function effectively normalizes updates in a way that respects the infinity norm constraint.
- Evidence anchors:
  - [abstract] "The use of the sign(·) function for update... can be viewed as an extreme way of normalizing the magnitude of the coordinate-wise updates."
  - [section] "It is closed related to normalized gradient [19, 24] and adaptive gradient methods such as Adam [14] and RMSprop [35]."
- Break condition: If the learning rate is too large, the normalization effect of the sign function may be overwhelmed, leading to constraint violations.

### Mechanism 3
- Claim: The Lion-κ family generalizes the constraint enforcement to other convex norms through the choice of K.
- Mechanism: By replacing the sign function with the subgradient of a convex function K, Lion-κ can enforce different types of constraints. The conjugate function K* determines the feasible region, and the dynamics converge to the boundary of this region.
- Core assumption: Different choices of K lead to different convex constraints that can be effectively enforced through the Lion-κ dynamics.
- Evidence anchors:
  - [abstract] "Our analysis is made possible by the development of a new Lyapunov function for the Lion updates. It applies to a broader family of Lion-κ algorithms, where the sign(·) operator in Lion is replaced by the subgradient of a convex function K, leading to the solution of a general composite optimization problem of min_x f(x) + K*(x)."
  - [section] "Lion-K includes a broad set of algorithms as special cases... using the ℓp norm K(x) = ||x||_p yields a constraint on the dual norm ||x||_q ≤ 1/λ."
- Break condition: If K is not convex or not properly chosen, the constraint enforcement may fail or lead to unintended optimization behavior.

## Foundational Learning

- Concept: Lyapunov functions and their role in proving convergence of dynamical systems.
  - Why needed here: The paper introduces a new Lyapunov function H(x, m) to prove that Lion-K dynamics converge to stationary points of the constrained optimization problem.
  - Quick check question: What are the key properties a function must have to be a valid Lyapunov function for proving convergence?

- Concept: Convex conjugate functions and Fenchel-Young inequality.
  - Why needed here: The paper uses convex conjugate functions K* to characterize the feasible regions and constraints in the optimization problems solved by Lion-K.
  - Quick check question: How does the Fenchel-Young inequality relate the primal and dual variables in convex optimization?

- Concept: Subgradients and their properties for non-differentiable convex functions.
  - Why needed here: The Lion-κ family uses subgradients of convex functions K, which may not be differentiable everywhere. Understanding subgradient properties is crucial for analyzing the algorithm's behavior.
  - Quick check question: What are the key properties of subgradients that distinguish them from regular gradients?

## Architecture Onboarding

- Component map: Lion-K update equations with momentum m, learning rate ε, weight decay λ, and convex function K -> Lyapunov function H(x,m) -> Convergence to stationary points -> Satisfaction of the infinity norm constraint
- Critical path: The Lion-K update equations -> Convergence to stationary points -> Satisfaction of the infinity norm constraint
- Design tradeoffs: The choice of K determines the type of constraint enforced. Different choices of K lead to different feasible regions and optimization behaviors.
- Failure signatures: If the algorithm fails to converge, check if the Lyapunov function is decreasing monotonically. If the constraint is violated, check if λ is too small or the learning rate is too large.
- First 3 experiments:
  1. Implement Lion-K with K(x) = ||x||_1 (recovering the original Lion algorithm) and verify that it converges to solutions satisfying ||x||_∞ ≤ 1/λ.
  2. Experiment with different values of λ and observe how the constraint tightness affects the optimization performance.
  3. Implement Lion-K with a different K, such as K(x) = ||x||_2^2/2, and verify that it enforces the corresponding ℓ2 norm constraint.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of the search space design in Chen et al. [6] lead to the discovery of theoretically sound algorithms like Lion through random search?
- Basis in paper: [explicit] The paper states: "It might be possible to find novel accelerated algorithms based on the Lion-K family" and "The reasons for this remain elusive, whether it is a coincidence or due to some inherent necessity."
- Why unresolved: The paper acknowledges that the reasons behind the discovery of theoretically sound algorithms through random search are unclear and calls for further investigation.
- What evidence would resolve it: Analyzing the structure and properties of the search space that led to the discovery of Lion, and comparing it with other search spaces that did not yield similar results.

### Open Question 2
- Question: Can the Lion-K family of algorithms exhibit acceleration phenomena, similar to Nesterov momentum and accelerated mirror descent?
- Basis in paper: [explicit] The paper mentions: "The connection between Lion-K and Nesterov momentum and accelerated mirror descent suggests the possibility of acceleration phenomena in variants of Lion-K, which opens an exciting avenue for future exploration and research."
- Why unresolved: While the paper identifies a potential connection to acceleration phenomena, it does not provide a definitive answer or proof of acceleration in the Lion-K family.
- What evidence would resolve it: Developing and analyzing variants of Lion-K that provably exhibit acceleration, and comparing their performance with existing accelerated algorithms.

### Open Question 3
- Question: How can the Lion-K framework be extended to handle more complex constraint optimization problems beyond bound constraints and ℓp norms?
- Basis in paper: [inferred] The paper discusses various examples of K functions and their corresponding optimization problems, suggesting the potential for extending the framework to more complex constraints.
- Why unresolved: The paper focuses on building the basic theoretical framework and leaves the exploration of practical applications and extensions as future directions.
- What evidence would resolve it: Developing and analyzing new K functions and their corresponding optimization problems, and demonstrating their effectiveness in solving complex constraint optimization problems.

## Limitations
- Theoretical analysis relies on continuous-time ODE approximations with limited validation of convergence rates in discrete-time implementations
- Role of momentum parameters (β1, β2) in constraint enforcement is not fully characterized
- Extension to non-convex K functions remains unexplored

## Confidence
- High confidence: The core claim that Lion solves constrained optimization via weight decay and infinity norm constraints
- Medium confidence: The generalization to Lion-κ family and its convergence properties for different convex K
- Low confidence: Practical implications for hyperparameter tuning and constraint violation handling

## Next Checks
1. Implement finite-time convergence rate bounds for discrete-time Lion-κ updates and compare against theoretical predictions
2. Experimentally verify constraint satisfaction across different learning rates and weight decay values on benchmark optimization problems
3. Test Lion-κ with non-smooth convex K functions (e.g., ℓ1 norm) to validate the subgradient-based convergence proof