---
ver: rpa2
title: 'DistriBlock: Identifying adversarial audio samples by leveraging characteristics
  of the output distribution'
arxiv_id: '2305.17000'
source_url: https://arxiv.org/abs/2305.17000
tags:
- adversarial
- data
- audio
- benign
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We propose DistriBlock, a detection method for adversarial audio
  examples in ASR systems. It analyzes the output probability distributions over tokens
  across time steps, computing characteristics like median, max/min, entropy, and
  Jensen-Shannon divergence.
---

# DistriBlock: Identifying adversarial audio samples by leveraging characteristics of the output distribution

## Quick Facts
- arXiv ID: 2305.17000
- Source URL: https://arxiv.org/abs/2305.17000
- Authors: 
- Reference count: 28
- Primary result: AUROC >99% for standard adversarial attacks, >96% against adaptive attacks on Librispeech

## Executive Summary
DistriBlock is a detection method for adversarial audio examples in automatic speech recognition (ASR) systems. It analyzes the output probability distributions over tokens across time steps, computing characteristics like median, max/min, entropy, and Jensen-Shannon divergence. Binary classifiers are then trained on these characteristics to distinguish benign from adversarial data. Evaluated on CTC/attention and transducer models using Librispeech, DistriBlock achieves AUROC >99% for standard adversarial attacks and >96% against adaptive attacks. Adaptive adversarial examples are noisier (SNR ~0.38 dB), making them easier to detect via filtering. This method does not require adversarial training or model fine-tuning and outperforms temporal dependency-based detectors.

## Method Summary
DistriBlock analyzes ASR output probability distributions by computing distribution characteristics (median, min/max, entropy, and Jensen-Shannon divergence) across time steps. These characteristics are aggregated at the utterance level and used to fit Gaussian distributions for benign data. New samples are classified as adversarial if their likelihood under the Gaussian model falls below a threshold. The method works with any ASR system that predicts probability distributions over output tokens and does not require model fine-tuning or adversarial training.

## Key Results
- AUROC >99% for standard adversarial attacks (Carlini-Wagner, psychoacoustic)
- AUROC >96% against adaptive attacks that specifically target the detector
- Adaptive attacks produce noisier adversarial examples (SNR ~0.38 dB), making them detectable through filtering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial perturbations cause distinct statistical shifts in ASR output distributions
- Mechanism: Attackers optimize input perturbations to force specific token sequences, but this optimization changes the confidence and entropy patterns in the ASR's per-timestep token probability distributions
- Core assumption: Adversarial examples will consistently deviate from the statistical properties of clean data across multiple distribution characteristics (median, max/min, entropy, Jensen-Shannon divergence)
- Evidence anchors:
  - [abstract] "We measure a set of characteristics of this distribution: the median, maximum, and minimum over the output probabilities, the entropy, and the Jensen-Shannon divergence of the distributions of subsequent time steps."
  - [section] "We compute the following quantities of this distribution for all ts of an utterance of length T"
  - [corpus] Weak evidence - only 8 related papers found, but none specifically analyze ASR output distribution characteristics for detection
- Break condition: Attackers discover and optimize perturbations specifically to mimic clean distribution characteristics, or use adaptive attacks that force adversarial examples to match clean data's statistical profile

### Mechanism 2
- Claim: Gaussian modeling of clean data characteristics provides effective binary classification
- Mechanism: By fitting Gaussian distributions to benign data's distribution characteristics, new samples can be classified based on likelihood scores under these models
- Core assumption: Clean and adversarial data occupy sufficiently separate regions in the characteristic space that Gaussian modeling captures the separation effectively
- Evidence anchors:
  - [abstract] "we fit a Gaussian distribution to the characteristics observed for benign data. If the probability of new audio samples is below a chosen threshold, these examples are classified as adversarial"
  - [section] "To obtain a classifier, we fitted a Gaussian distribution to the scores computed for the utterances from a held-out set of benign data"
  - [corpus] No direct evidence found in corpus - this appears to be a novel contribution
- Break condition: The Gaussian assumption fails (e.g., multimodal distributions) or adversarial examples cluster sufficiently close to clean data in characteristic space

### Mechanism 3
- Claim: Adaptive attacks that circumvent detection produce noisier adversarial examples
- Mechanism: When attackers optimize perturbations to fool the distribution-based detector, they must sacrifice stealthiness, resulting in higher SNR and making them detectable through other means
- Core assumption: There exists a fundamental tradeoff between evading distribution-based detection and maintaining low perceptibility
- Evidence anchors:
  - [abstract] "Adaptive adversarial examples are noisier (SNR ~0.38 dB), making them easier to detect via filtering"
  - [section] "To assess the robustness of our method we build adaptive attacks. This reduces the AUROC to 0.96 but results in more noisy adversarial clips"
  - [corpus] No supporting evidence found in corpus - this appears to be an original finding
- Break condition: Attackers discover optimization strategies that simultaneously minimize both detection score differences and perceptibility

## Foundational Learning

- Concept: Probability distributions over discrete tokens
  - Why needed here: The core detection mechanism relies on analyzing ASR output distributions, requiring understanding of softmax outputs and categorical distributions
  - Quick check question: Given a softmax vector [0.1, 0.7, 0.2] over three tokens, what is the entropy and which token has maximum probability?

- Concept: Information theory metrics (entropy, KL divergence, Jensen-Shannon divergence)
  - Why needed here: These metrics quantify the "shape" of output distributions and temporal consistency, forming the detection features
  - Quick check question: For two distributions p=[0.8, 0.2] and q=[0.2, 0.8], compute the Jensen-Shuman divergence

- Concept: Gaussian distribution modeling and likelihood scoring
  - Why needed here: The detection system uses Gaussian models fitted to clean data characteristics for classification
  - Quick check question: If a Gaussian model has mean=0.5 and std=0.1, what is the z-score of an observation with value 0.7?

## Architecture Onboarding

- Component map:
  - ASR system (CTC/attention or transducer model) → produces token probability distributions
  - Distribution characteristic extractor → computes median, min/max, entropy, JS divergence
  - Aggregator → combines timestep values into utterance-level scores
  - Gaussian classifier → compares scores against clean data models
  - Binary decision → flags samples with low likelihood as adversarial

- Critical path: Input audio → ASR decoding → distribution extraction → characteristic computation → Gaussian scoring → adversarial classification

- Design tradeoffs:
  - Computational cost vs. detection accuracy: More characteristics improve detection but increase computation
  - Model complexity vs. generalizability: Complex models may overfit to specific ASR architectures
  - False positive tolerance vs. security: Stricter thresholds reduce false positives but may miss subtle attacks

- Failure signatures:
  - High false positive rate: Clean data distribution characteristics vary too much or Gaussian models poorly fit the data
  - High false negative rate: Adversarial examples successfully mimic clean distribution characteristics
  - System instability: Inconsistent characteristic values across similar inputs suggest numerical instability

- First 3 experiments:
  1. Run clean audio through ASR and visualize distribution characteristics to verify expected statistical patterns
  2. Generate adversarial examples using Carlini-Wagner attack and compare characteristic distributions to clean data
  3. Train Gaussian classifiers on clean data and test detection performance on both clean and adversarial examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DistriBlock perform on extremely long audio files (e.g., audiobooks or multi-minute recordings) where temporal dependencies become more complex?
- Basis in paper: [inferred] The paper notes that temporal dependency-based detectors require "long enough" audio streams, implying potential limitations with very long sequences, but does not evaluate DistriBlock on such data.
- Why unresolved: The evaluation uses LibriSpeech test sets with relatively short utterances; performance on extended audio sequences remains untested.
- What evidence would resolve it: Testing DistriBlock on multi-minute audio files with varying speech rates and background conditions to measure detection accuracy and computational efficiency.

### Open Question 2
- Question: Can the characteristics used by DistriBlock (e.g., entropy, JS divergence) be reliably adapted for multilingual ASR systems or languages with different token vocabularies and structures?
- Basis in paper: [inferred] The method is described as applicable to "any ASR system that predicts a probability distribution over output tokens," but experiments are limited to English LibriSpeech data.
- Why unresolved: No experiments were conducted on non-English datasets or languages with different tokenizations (e.g., character-based vs. word-piece models).
- What evidence would resolve it: Evaluating DistriBlock across multiple languages with varying vocabulary sizes and tokenization schemes to confirm consistent performance.

### Open Question 3
- Question: What is the minimum signal-to-noise ratio (SNR) threshold below which DistriBlock's detection capability degrades significantly, and how does this compare to human perception thresholds?
- Basis in paper: [explicit] The paper reports SNR values for adversarial examples (e.g., 0.38 dB for adaptive attacks) but does not analyze the detection threshold or compare it to perceptual thresholds.
- Why unresolved: While SNR is measured, the paper does not establish a critical SNR below which detection fails or becomes unreliable.
- What evidence would resolve it: Systematic testing of DistriBlock across a range of SNR values from clean audio to heavily degraded speech to identify the operational limits of detection.

### Open Question 4
- Question: How does DistriBlock handle streaming ASR scenarios where probability distributions are computed incrementally rather than on complete utterances?
- Basis in paper: [inferred] The method computes characteristics over the full output sequence (T steps), implying batch processing, but real-time ASR often requires incremental decoding.
- Why unresolved: No experiments or discussion of streaming/online detection performance are provided.
- What evidence would resolve it: Implementing and testing DistriBlock in an online ASR pipeline with incremental updates to distribution characteristics and measuring detection latency and accuracy.

## Limitations
- Evaluation limited to SpeechBrain ASR architecture and Librispeech dataset, limiting generalizability
- Adaptive attacks reduce AUROC from 99% to 96%, indicating imperfect robustness
- Gaussian modeling assumption may not hold for all ASR architectures or data distributions

## Confidence
- **High confidence**: The core detection mechanism (analyzing ASR output distribution characteristics) is technically sound and well-supported by the results
- **Medium confidence**: The adaptive attack results showing increased perceptibility are promising but represent a narrow adversarial strategy
- **Low confidence**: Claims about generalizability to other ASR architectures, datasets, and attack types are not sufficiently supported

## Next Checks
1. Test DistriBlock on ASR systems beyond SpeechBrain (e.g., DeepSpeech, wav2vec 2.0) to assess architecture dependence
2. Design and evaluate adaptive attacks that specifically optimize to minimize changes in distribution characteristics while maintaining adversarial effectiveness
3. Evaluate detection performance on audio with varying recording conditions, background noise, and compression artifacts to assess practical robustness