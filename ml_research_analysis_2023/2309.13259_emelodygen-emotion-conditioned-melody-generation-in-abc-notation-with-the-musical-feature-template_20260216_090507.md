---
ver: rpa2
title: 'EMelodyGen: Emotion-Conditioned Melody Generation in ABC Notation with the
  Musical Feature Template'
arxiv_id: '2309.13259'
source_url: https://arxiv.org/abs/2309.13259
tags:
- music
- wikimt
- emotion
- dataset
- lyrics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: WikiMT++ expands the original WikiMT dataset to 1010 lead sheets
  in ABC notation with additional attributes including album names, complete lyrics,
  music videos, and two types of emotion labels (12 emotion adjectives and 4 basic
  emotions). The dataset supports various tasks including music information retrieval,
  music generation, automatic lyrics generation, genre recognition, and emotion classification.
---

# EMelodyGen: Emotion-Conditioned Melody Generation in ABC Notation with the Musical Feature Template

## Quick Facts
- arXiv ID: 2309.13259
- Source URL: https://arxiv.org/abs/2309.13259
- Reference count: 0
- Key outcome: Introduces WikiMT++, a multi-modal music dataset with 1010 lead sheets in ABC notation supporting various music analysis and generation tasks

## Executive Summary
WikiMT++ is a comprehensive multi-modal music dataset that expands the original WikiMT dataset to 1010 lead sheets in ABC notation. The dataset includes additional attributes such as album names, complete lyrics, music video links, and dual emotion labels (12 emotion adjectives and 4 basic emotions). It supports various tasks including music information retrieval, music generation, automatic lyrics generation, genre recognition, and emotion classification. The dataset was created through a systematic process of filtering and processing musical scores from Wikifonia, collecting additional data through Amazon Mechanical Turk, and verifying the results with students.

## Method Summary
The dataset was created by first filtering and processing musical scores from Wikifonia to ensure complete descriptive information, then converting them to ABC notation format. Additional attributes were collected through Amazon Mechanical Turk surveys, including album names, lyrics, and music video links. The CLaMP model was implemented to automatically label genre information and correct attributes inherited from WikiMT. The collected data underwent manual verification by music students, and the final dataset was evaluated using the MARBLE benchmark framework.

## Key Results
- 1010 lead sheets in ABC notation with 17 comprehensive attributes
- Dual emotion labeling system with 12 emotion adjectives and 4 basic emotions
- Support for 8 genres and multiple task types including MIR, generation, and emotion classification
- Integration of multi-modal data including lyrics, music videos, and mel spectrograms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dataset provides comprehensive multi-modal music data that enables multiple task types including MIR, generation, and emotion classification.
- Mechanism: By incorporating objective attributes (album, lyrics, video) and subjective emotion labels (12 emotions, 4 basic emotions), the dataset creates rich feature spaces for different learning tasks.
- Core assumption: Multi-modal and multi-label data improves model performance across diverse music tasks compared to single-modality datasets.
- Evidence anchors:
  - [abstract] "The dataset supports various tasks including music information retrieval, music generation, automatic lyrics generation, genre recognition, and emotion classification."
  - [section] "WikiMT++ offers a versatile platform with a wide spectrum of supported tasks that encompass various aspects of music analysis and generation."
  - [corpus] Weak evidence - corpus neighbors focus on music generation and emotion tasks but don't directly validate the multi-modal approach.
- Break condition: If multi-modal data introduces noise that outweighs the benefits, or if task-specific models perform better with domain-specific data.

### Mechanism 2
- Claim: The combination of high-dimensional (12 emotions) and low-dimensional (4 basic emotions) emotion labels enables both fine-grained and coarse-grained emotion analysis.
- Mechanism: Russell's 4Q framework provides a structured dimensional approach while the 12 emotion adjectives allow for more nuanced classification.
- Core assumption: Dimensional emotion models capture the underlying structure of emotional responses to music better than categorical models alone.
- Evidence anchors:
  - [section] "Music has the capacity to evoke, convey, and regulate emotions... Thus, music emotion classification is an important area to be explored."
  - [section] "We do both high dimension and low dimension labeling, respectively the emotion and emo_4q."
  - [corpus] Weak evidence - corpus shows emotion-conditioned models but doesn't validate the dual-label approach.
- Break condition: If the dimensional labels don't align well with human perception or if the 12 emotions are too granular to be useful.

### Mechanism 3
- Claim: Using CLaMP for attribute correction improves dataset quality and reduces errors from original data collection.
- Mechanism: Contrastive Language-Music Pre-training model leverages cross-modal understanding to validate and correct genre and emotion labels.
- Core assumption: Large language models trained on music data can accurately validate and correct human-labeled attributes.
- Evidence anchors:
  - [section] "we update these attributes through CLaMP [1]. Album names are also taken into consideration because songs from the same album sometimes have identical genre or emotion."
  - [section] "CLaMP is implemented to correct the attributes inherited from WikiMT to reduce errors introduced during original data collection."
  - [corpus] Weak evidence - CLaMP 2 appears in corpus but doesn't directly validate the correction approach.
- Break condition: If CLaMP's corrections introduce systematic biases or if the model's understanding of music-specific attributes is insufficient.

## Foundational Learning

- Concept: ABC notation structure and syntax
  - Why needed here: The dataset uses ABC notation for music representation, which is crucial for understanding how music data is structured and processed
  - Quick check question: What are the basic components of ABC notation and how do they represent musical elements like pitch, duration, and key?

- Concept: Russell's circumplex model of affect (Russell 4Q)
  - Why needed here: The dataset uses both 4 basic emotions (Russell 4Q) and 12 emotion adjectives, requiring understanding of dimensional emotion models
  - Quick check question: How does the Russell 4Q model map emotional states in a two-dimensional space of valence and arousal?

- Concept: Music information retrieval (MIR) fundamentals
  - Why needed here: The dataset supports MIR tasks, requiring understanding of how musical features are extracted and used for retrieval
  - Quick check question: What are the key musical features used in MIR tasks and how are they typically extracted from symbolic music representations?

## Architecture Onboarding

- Component map: Wikifonia → Python filtering → music21 extraction → CLaMP labeling → Amazon Mechanical Turk surveys → student verification → dataset publication
- Critical path: Data collection → CLaMP labeling → multi-modal augmentation → quality verification → dataset publication
- Design tradeoffs: Manual curation vs. automated labeling (CLaMP provides scalability but may introduce model biases)
- Failure signatures: Inconsistent emotion labels across similar tracks, missing multi-modal data for certain entries, format conversion errors
- First 3 experiments:
  1. Test CLaMP's genre labeling accuracy on a subset of manually verified tracks
  2. Validate the correlation between 12 emotion adjectives and 4 basic emotions using human annotators
  3. Measure music21 parsing rates for ABC files with varying complexity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the addition of emotion labels (emotion and emo_4q) impact the performance of music information retrieval tasks compared to using only genre labels?
- Basis in paper: [explicit] The paper states that WikiMT++ includes emotion labels to increase the capacity of the dataset for emotion classification tasks and mentions its utility for music information retrieval.
- Why unresolved: The paper does not provide empirical results or comparisons showing how the emotion labels specifically enhance music information retrieval tasks.
- What evidence would resolve it: Empirical studies comparing the performance of music information retrieval algorithms using WikiMT++ with and without emotion labels, demonstrating any improvements or differences in retrieval accuracy.

### Open Question 2
- Question: What are the specific benefits of including multi-modal data (lyrics, music videos, mel spectrogram, music score images) in WikiMT++ for music generation tasks?
- Basis in paper: [explicit] The paper mentions that multi-modal data is included to prepare for multi-modal tasks and enhance the dataset's capacity for various applications, including music generation.
- Why unresolved: The paper does not detail how these multi-modal attributes specifically benefit music generation tasks or provide examples of improved generation outcomes.
- What evidence would resolve it: Comparative studies or experiments showing the impact of multi-modal data on the quality and creativity of music generated using WikiMT++.

### Open Question 3
- Question: How effective is the CLaMP model in correcting and enhancing the accuracy of attributes inherited from WikiMT, and what are the limitations of this approach?
- Basis in paper: [explicit] The paper states that CLaMP is implemented to correct attributes inherited from WikiMT to reduce errors and enhance accuracy.
- Why unresolved: The paper does not provide quantitative results or discuss the limitations of using CLaMP for attribute correction.
- What evidence would resolve it: Detailed evaluation results showing the accuracy improvements achieved by CLaMP, along with a discussion of any remaining limitations or challenges in attribute correction.

## Limitations

- Reliance on automated CLaMP labeling for genre and emotion attributes may introduce systematic biases
- Amazon Mechanical Turk data collection may result in variable data quality for subjective attributes
- Focus on lead sheets in ABC notation limits applicability to full arrangements or different musical representations

## Confidence

- High Confidence: The dataset creation methodology and basic structure (1010 lead sheets with 17 attributes in ABC notation) are clearly specified and reproducible
- Medium Confidence: The effectiveness of the dual emotion labeling system (12 adjectives + 4 basic emotions) for various tasks is theoretically sound but lacks direct empirical validation
- Medium Confidence: The CLaMP-based attribute correction mechanism is described but its accuracy and impact on dataset quality are not quantitatively evaluated

## Next Checks

1. Conduct a blind evaluation comparing CLaMP's genre labeling accuracy against a ground truth set of manually verified tracks to quantify potential systematic biases

2. Perform inter-rater reliability analysis on the 12 emotion adjectives and 4 basic emotions using independent human annotators to validate the dual-label approach

3. Test music21 parsing success rates across different complexity levels of ABC notation to identify potential format conversion issues that could affect dataset usability