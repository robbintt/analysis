---
ver: rpa2
title: Can We Trust the Similarity Measurement in Federated Learning?
arxiv_id: '2311.03369'
source_url: https://arxiv.org/abs/2311.03369
tags:
- faker
- local
- similarity
- attacks
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uncovers a significant security vulnerability in federated
  learning (FL) systems that rely on similarity metrics, such as L2 norm, Euclidean
  distance, and cosine similarity, to evaluate the reliability of local models. The
  authors demonstrate that these metrics can be manipulated to allow malicious models
  to be evaluated as similar to benign ones while having significantly different parameter
  values.
---

# Can We Trust the Similarity Measurement in Federated Learning?

## Quick Facts
- arXiv ID: 2311.03369
- Source URL: https://arxiv.org/abs/2311.03369
- Reference count: 40
- This paper uncovers significant security vulnerabilities in similarity-based defenses in federated learning systems.

## Executive Summary
This paper demonstrates that similarity metrics commonly used in federated learning (L2 norm, Euclidean distance, cosine similarity) can be exploited by attackers to pass detection while poisoning the global model. The authors introduce Faker, a novel untargeted model poisoning attack that generates poisoned models appearing similar to benign ones while significantly altering parameter values. Through extensive experiments across seven datasets and eight defenses, Faker shows substantial improvements in attack effectiveness and efficiency compared to existing methods. The work highlights critical security concerns with similarity-based evaluation and proposes a partial parameter similarity defense to mitigate these attacks.

## Method Summary
The Faker attack exploits vulnerabilities in similarity metrics by optimizing for both high similarity scores and large parameter differences. The method transforms model poisoning into an optimization problem where poisoned models are generated using scalar multipliers for each parameter group, maintaining high similarity while altering model behavior. The attack is designed to work against various similarity-based defenses including FLTrust, Krum, and norm-clipping. For defense, the authors propose Similarity of Partial Parameters (SPP), which randomly evaluates only a subset of parameters rather than the full model, making it harder for attackers to predict which parameters will be assessed.

## Key Results
- Faker outperforms existing attacks by 1.1-9.0X in reducing global model accuracy
- Faker achieves 1.2-8.0X improvements in computational efficiency over baseline attacks
- Even with a single malicious client and limited knowledge, Faker successfully poisons models
- The proposed SPP defense shows effectiveness in mitigating Faker attacks across multiple datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Similarity metrics in federated learning are vulnerable because high-dimensional models can have identical similarity scores while differing significantly in parameter values.
- **Mechanism**: The vulnerability arises because similarity calculations aggregate across all parameters. This allows attackers to carefully adjust individual parameters to maintain the same similarity score while altering the overall model behavior.
- **Core assumption**: The server cannot verify individual parameter values, only aggregated similarity scores.
- **Evidence anchors**:
  - [abstract] "We first uncover the deficiencies of similarity metrics that high-dimensional local models, including benign and poisoned models, may be evaluated to have the same similarity while being significantly different in the parameter values."
  - [section 4.1] "We demonstrate the vulnerabilities of similarity metrics in Figure 2... revealing that multiple local models, which are significantly different, could be treated as qualified local models since their evaluated similarities are within the allowed range."
  - [corpus] "Average neighbor FMR=0.346, average citations=0.0" (weak corpus support for similarity metric vulnerabilities specifically)

### Mechanism 2
- **Claim**: Faker transforms model poisoning into an optimization problem that maximizes both similarity to benign models and difference in parameter values.
- **Mechanism**: Faker generates poisoned models by finding optimal scalar multipliers for each parameter. These scalars maintain high similarity scores while creating significant parameter differences, allowing poisoned models to pass detection while harming the global model.
- **Core assumption**: The attacker knows the defense mechanism and can approximate the reference model using their own local model.
- **Evidence anchors**:
  - [abstract] "Faker, which launches the attack by simultaneously maximizing the evaluated similarity of the poisoned local model and the difference in the parameter values."
  - [section 5.1] "The core idea of Faker is to find an effective vector of scalars... for the attacker to generate the poisoned local model wi based on wi."
  - [section 5.2.1] Detailed formulations for FLTrust, Krum, and norm-clipping defenses.

### Mechanism 3
- **Claim**: Faker achieves computational efficiency by grouping parameters and solving reduced-dimension optimization problems.
- **Mechanism**: Instead of optimizing J parameters individually (O(J) complexity), Faker groups parameters (e.g., by neural network layer) and shares scalars within groups, reducing complexity to O(T) where T << J.
- **Core assumption**: Parameters within the same group can share similar scaling factors without losing attack effectiveness.
- **Evidence anchors**:
  - [section 5.3] "We can divide the scalars into T â‰ª J groups and allow the scalars in the same group to share the same value, thus the time complexity can be reduced to O(T)."
  - [section 5.2] Demonstrates specific formulations for different defenses with grouped parameters.
  - [section 6.2] "Table 13: Time cost of different values of T" shows reduced time with grouped parameters.

## Foundational Learning

- **Concept: Similarity metrics in high-dimensional spaces**
  - Why needed here: Understanding why L2 norm, Euclidean distance, and cosine similarity can be manipulated in federated learning requires grasping how these metrics behave in high-dimensional spaces.
  - Quick check question: Why do high-dimensional models allow multiple parameter configurations to yield identical similarity scores?

- **Concept: Optimization with constraints**
  - Why needed here: Faker formulates model poisoning as a constrained optimization problem (maximize similarity while maintaining difference), requiring knowledge of optimization techniques.
  - Quick check question: How does Faker balance maximizing similarity and parameter differences within defense constraints?

- **Concept: Federated learning architecture and defense mechanisms**
  - Why needed here: Understanding how federated learning works, including aggregation methods and defense mechanisms, is essential to comprehend attack vectors and countermeasures.
  - Quick check question: What information does an attacker need to successfully launch Faker against a specific defense?

## Architecture Onboarding

- **Component map**:
  Clients -> Server (aggregates models, applies defenses) -> Global model

- **Critical path**:
  1. Client trains local model
  2. Attacker applies Faker to generate poisoned model
  3. Server evaluates submitted models using similarity metrics
  4. Server aggregates models (selecting/rejecting based on evaluation)
  5. Global model updates and testing

- **Design tradeoffs**:
  - Computational efficiency vs. attack effectiveness: Grouping parameters reduces computation but may slightly reduce attack precision
  - Defense strength vs. performance overhead: SPP provides stronger defense but requires more computation than full similarity evaluation
  - Knowledge requirements: Faker requires attacker knowledge of defense mechanisms, limiting applicability

- **Failure signatures**:
  - Attack fails: Defense detects poisoned models (similarity scores outside acceptable ranges)
  - Defense fails: Global model accuracy degrades despite similarity-based evaluation
  - SPP fails: Attacker successfully predicts which parameters will be evaluated

- **First 3 experiments**:
  1. Implement Faker against a simple similarity-based defense (e.g., norm-clipping) with a basic neural network on MNIST to verify basic attack mechanism
  2. Test SPP defense by varying the number of partial parameters evaluated and measuring attack success rates
  3. Compare Faker's computational efficiency against benchmark attacks (LA, MB) on larger models/datasets (e.g., CIFAR-10 with AlexNet)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of Faker change when attacking federated learning systems with different model architectures, such as transformers or graph neural networks, compared to traditional CNNs and MLPs?
- Basis in paper: [inferred] The paper evaluates Faker on several datasets using CNN and MLP models but does not explore other model architectures like transformers or graph neural networks.
- Why unresolved: The paper's focus on CNNs and MLPs leaves a gap in understanding Faker's performance across diverse model architectures.
- What evidence would resolve it: Experiments testing Faker on federated learning systems using transformer or graph neural network architectures, comparing the attack's success rates and time efficiency across different model types.

### Open Question 2
- Question: Can the similarity of partial parameters (SPP) strategy be further optimized to improve its efficiency in defending against Faker, especially when dealing with high-dimensional models or models with non-uniform parameter importance?
- Basis in paper: [explicit] The paper introduces SPP as a defense strategy but notes that further optimization might be needed, especially for high-dimensional models or models with non-uniform parameter importance.
- Why unresolved: The current implementation of SPP may not be optimal for all scenarios, particularly for models with varying parameter importance or extremely high dimensions.
- What evidence would resolve it: Studies that test SPP against Faker on high-dimensional models and models with non-uniform parameter importance, comparing the defense's effectiveness and computational efficiency to the current implementation.

### Open Question 3
- Question: How does Faker perform in federated learning environments with heterogeneous data distributions, where clients have significantly different local datasets, and what adaptations are necessary for the attack to remain effective?
- Basis in paper: [inferred] The paper tests Faker on IID and non-IID data distributions but does not specifically address the challenges posed by highly heterogeneous data distributions among clients.
- Why unresolved: Federated learning often involves clients with diverse and heterogeneous data, which may impact the effectiveness of Faker.
- What evidence would resolve it: Experiments evaluating Faker's performance in federated learning systems with highly heterogeneous data distributions, identifying any necessary adaptations for the attack to maintain its effectiveness.

## Limitations
- Attack effectiveness depends on attacker's knowledge of the defense mechanism and threshold values
- Computational efficiency gains may vary depending on implementation details and hardware configurations
- Results are primarily validated on image classification tasks and may not generalize to other domains

## Confidence

**High confidence** in the fundamental vulnerability discovery of similarity metrics in federated learning. The core claims are supported by strong theoretical analysis and experimental evidence across multiple datasets and defenses.

**Medium confidence** in the Faker attack mechanism and its computational efficiency improvements. While well-documented, exact performance gains may vary depending on implementation details and hardware configurations.

**Medium confidence** in the proposed SPP defense. Shows promise but requires further validation across more diverse attack scenarios and model architectures.

## Next Checks

1. Implement SPP defense with varying numbers of partial parameters (k) and measure how quickly attack success rates degrade as k increases, particularly on non-IID data distributions.

2. Test Faker's computational efficiency scaling on larger models (ResNet-50 on ImageNet) to validate the O(T) complexity claims with T << J parameters.

3. Evaluate the attack's effectiveness when the attacker has incomplete knowledge of the defense mechanism (e.g., knowing defense type but not exact threshold values).