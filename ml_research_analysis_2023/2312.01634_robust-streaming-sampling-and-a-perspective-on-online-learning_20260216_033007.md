---
ver: rpa2
title: Robust Streaming, Sampling, and a Perspective on Online Learning
arxiv_id: '2312.01634'
source_url: https://arxiv.org/abs/2312.01634
tags:
- algorithm
- online
- stream
- learning
- adversarial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive study of the relationship between
  adversarial streaming, sampling, and online learning. It introduces the concept
  of adversarial robustness in streaming and sampling settings, where an adversary
  can adaptively choose the input stream based on the algorithm's output history.
---

# Robust Streaming, Sampling, and a Perspective on Online Learning

## Quick Facts
- arXiv ID: 2312.01634
- Source URL: https://arxiv.org/abs/2312.01634
- Reference count: 21
- Primary result: Establishes equivalence between online learnability and adversarial uniform law of large numbers, characterized by finite Littlestone dimension

## Executive Summary
This paper presents a comprehensive framework unifying adversarial streaming, sampling, and online learning through the concept of adversarial robustness. The authors introduce sketch switching techniques for robust streaming algorithms and establish fundamental connections between sampling complexity and online learnability. By proving that a hypothesis class is online learnable if and only if it admits an adversarial uniform law of large numbers, the work bridges classical PAC learning theory with modern online learning frameworks. The theoretical results demonstrate that Littlestone dimension serves as the central measure of complexity across these seemingly disparate domains.

## Method Summary
The paper develops robust streaming algorithms through sketch switching, maintaining multiple independent copies of strong tracking algorithms and switching between them when estimates fall outside acceptable ranges. For adversarial sampling, the authors analyze Bernoulli sampling under adaptive adversaries and establish bounds on required sample sizes using reductions to online combinatorial discrepancy. The theoretical framework employs Sequential Rademacher Complexity to connect adversarial sampling performance to Littlestone dimension, ultimately proving equivalence between online learnability and adversarial uniform convergence.

## Key Results
- Sketch switching achieves robustness against adaptive adversaries with space complexity depending on the flip number
- Adversarial sampling requires sample sizes characterized by Littlestone dimension rather than VC-dimension
- Establishes equivalence between online learnability, adversarial uniform law of large numbers, and finite Littlestone dimension
- Provides quantitative bounds on sample complexity and regret that demonstrate deep connections between sampling theory and online learning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Adversarial Uniform Law of Large Numbers characterizes online learnability in the same way that standard Uniform Law of Large Numbers characterizes offline learnability.
- **Mechanism:** By establishing equivalence between online learnability, adversarial uniform convergence, and finite Littlestone dimension, the framework creates a unified foundation where sampling theory and online learning theory are deeply interconnected.
- **Core assumption:** The adversarial setting (where the stream is adaptively chosen based on algorithm outputs) can be modeled as a two-player game between an adversary and an algorithm, and this adversarial model captures the essential challenges of online learning.
- **Evidence anchors:**
  - [abstract] "unifies often disjoint theorems in a shared framework"
  - [section 3.1] "A hypothesis class R admits an Adversarial Uniform Law of Large Numbers if there exists a function mR : (0, 1)² → N and a sampler such that for any adversarially-prepared stream X over U and any tolerances ǫ, δ ∈ (0, 1), the sampler chooses at most mR(ǫ, δ) samples which form an ǫ-approximation of X with probability at least 1−δ"
  - [corpus] Weak evidence - only 5/8 related papers mention "online learning" or "streaming" directly
- **Break condition:** If the adversarial model fails to capture real-world streaming scenarios where data arrives too quickly for adaptive responses, or if there exist online learnable classes that cannot be characterized by adversarial uniform convergence.

### Mechanism 2
- **Claim:** Sketch switching technique converts static streaming algorithms into adversarially robust ones by maintaining multiple independent copies.
- **Mechanism:** When the current sketch's estimate falls outside the acceptable range, the algorithm switches to a fresh copy with independent randomness, preventing the adversary from learning the internal state.
- **Core assumption:** Multiple independent copies of a strong-tracking algorithm can handle adversarial streams if the number of required switches (flip number) is bounded.
- **Evidence anchors:**
  - [section 2.2.2] "Sketch switching achieves robustness by keeping multiple copies of strong tracking algorithms"
  - [section 2.2.2] "As long as the previous output based on (at−1, ∆t−1) is a good multiplicative approximation of the estimate of (at, ∆t) by the copy in use, ALGORITHM outputs the previous output"
  - [corpus] No direct evidence about sketch switching techniques
- **Break condition:** If the flip number λ grows too quickly with stream length m, making the space complexity prohibitive, or if the adversary can force more switches than the theoretical bounds predict.

### Mechanism 3
- **Claim:** The sample complexity bounds for adversarial ǫ-approximation match the form of VC-theory bounds, with Littlestone dimension replacing VC-dimension.
- **Mechanism:** By reducing the adversarial sampling problem to online combinatorial discrepancy through double sampling, the problem connects to Sequential Rademacher Complexity, which can be bounded in terms of Littlestone dimension.
- **Core assumption:** The reduction from adversarial sampling to online discrepancy preserves the essential complexity characteristics that can be measured by Sequential Rademacher Complexity.
- **Evidence anchors:**
  - [section 3.2] "By doing so, the minimization of the ǫ-approximation error reduces to minimizing the discrepancy in a well known online combinatorial game"
  - [section 3.2] "This reduction allows us to connect performance in combinatorial discrepancy... to a measure of complexity known as Sequential Rademacher Complexity"
  - [corpus] No direct evidence about Sequential Rademacher Complexity connections
- **Break condition:** If the reduction to online discrepancy introduces approximation errors that grow with problem size, or if Sequential Rademacher Complexity cannot be effectively bounded by Littlestone dimension in all cases.

## Foundational Learning

- **Concept: VC-dimension and shattering**
  - Why needed here: Understanding VC-dimension is essential for grasping the classical PAC learning framework and its relationship to sample complexity bounds
  - Quick check question: Can you explain why a hypothesis class that can shatter sets of arbitrarily large size cannot be PAC-learnable?

- **Concept: Littlestone dimension and mistake trees**
  - Why needed here: Littlestone dimension is the online learning analogue of VC-dimension and characterizes online learnability
  - Quick check question: How does the mistake tree framework capture the sequential nature of online learning that VC-dimension does not?

- **Concept: Martingale concentration inequalities**
  - Why needed here: The adversarial sampling analysis relies on martingale concentration rather than standard Chernoff bounds due to dependence between samples
  - Quick check question: Why can't we use standard concentration inequalities like Chernoff bounds in the adversarial setting?

## Architecture Onboarding

- **Component map:** Core framework -> Sketch switching technique -> Adversarial sampling analysis -> Reduction to online discrepancy -> Sequential Rademacher Complexity -> Equivalence theorems

- **Critical path:**
  1. Understand the adversarial streaming model and its challenges
  2. Learn sketch switching technique and its guarantees
  3. Grasp the adversarial sampling problem and its complexity
  4. Follow the reduction from sampling to online discrepancy
  5. Connect to Sequential Rademacher Complexity and Littlestone dimension
  6. Understand the equivalence theorems and their implications

- **Design tradeoffs:**
  - Space vs. robustness: More copies in sketch switching increases robustness but also space usage
  - Sample size vs. approximation quality: Larger samples provide better approximation but higher computational cost
  - Theoretical elegance vs. practical implementation: The reductions and equivalences are theoretically beautiful but may be complex to implement

- **Failure signatures:**
  - Sketch switching failing: Algorithm makes too many switches, space usage explodes, or estimates become inaccurate
  - Adversarial sampling failing: Sample fails to be ǫ-approximation despite meeting theoretical bounds
  - Reduction breaking: The connection between sampling and discrepancy fails to preserve essential properties

- **First 3 experiments:**
  1. Implement sketch switching on a simple streaming problem (like F2 estimation) and measure the number of switches under adversarial input
  2. Test adversarial sampling with Bernoulli sampler on a hypothesis class with known VC-dimension and verify the sample complexity bounds
  3. Implement the reduction from adversarial sampling to online discrepancy on a small example and verify the equivalence of the optimization problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can non-oblivious samplers created using robustification methods from Ben-Eliezer et al. [6] and Woodruff et al. [19] achieve better performance on adversarial ε-approximation than oblivious samplers?
- Basis in paper: [explicit] The paper suggests that robustification methods for mechanically turning oblivious streaming algorithms into adversarially-robust ones could result in new sampling schemas with better performance on adversarial ε-approximation.
- Why unresolved: The paper does not provide specific results or comparisons between non-oblivious and oblivious samplers in adversarial settings.
- What evidence would resolve it: Empirical studies comparing the performance of non-oblivious samplers created using robustification methods with traditional oblivious samplers in adversarial ε-approximation tasks.

### Open Question 2
- Question: Is there a tighter lower bound on the adversarial sampling sample size complexity that matches the upper bound with the Littlestone dimension?
- Basis in paper: [explicit] The paper mentions that while there is a tighter lower bound for ε-nets, neither bound matches the tightness seen in the quantitative results for ofﬂine learning, leaving room for further research to improve the lower bound on adversarial sampling complexity.
- Why unresolved: The current lower bounds do not achieve the same level of tightness as the upper bounds, indicating potential for improvement.
- What evidence would resolve it: A mathematical proof or counterexample demonstrating a tighter lower bound on the adversarial sampling sample size complexity that matches the upper bound with the Littlestone dimension.

### Open Question 3
- Question: How does the performance of sampling algorithms in adversarial settings relate to their Sequential Rademacher Complexity?
- Basis in paper: [explicit] The paper discusses the connection between the Sequential Rademacher Complexity and the optimal regret of agnostic online learners, suggesting that understanding this relationship could enlighten the design of sampling algorithms.
- Why unresolved: While the paper hints at this relationship, it does not provide a comprehensive analysis or direct implications for sampling algorithm design.
- What evidence would resolve it: A detailed study or theoretical analysis linking the performance of sampling algorithms in adversarial settings to their Sequential Rademacher Complexity, potentially leading to new insights for algorithm design.

## Limitations

- The theoretical framework relies heavily on abstract reductions between online learning, streaming, and sampling problems, with limited empirical validation
- Sketch switching performance depends critically on the flip number, but practical scaling with different query functions and adversarial strategies is not fully characterized
- The adversarial sampling analysis assumes specific attack models that may not capture all real-world streaming scenarios

## Confidence

- **High Confidence:** The foundational definitions and basic theorems connecting Littlestone dimension to online learnability
- **Medium Confidence:** The sketch switching technique and its space-robustness tradeoff analysis
- **Low Confidence:** The practical performance of adversarial sampling algorithms in realistic streaming scenarios

## Next Checks

1. **Empirical Flip Number Analysis:** Implement sketch switching on standard streaming queries (F2, frequency moments) and measure the actual flip number under various adversarial strategies to validate theoretical bounds.

2. **Adversarial Sampling Benchmark:** Create a comprehensive benchmark comparing Bernoulli sampling, deterministic sampling, and hybrid approaches under different adversarial models, measuring both approximation quality and sample efficiency.

3. **Littlestone Dimension Computation:** Develop algorithms to compute or estimate Littlestone dimension for common hypothesis classes and verify that the sample complexity bounds accurately predict performance in adversarial sampling scenarios.