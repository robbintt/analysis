---
ver: rpa2
title: 'Mobiprox: Supporting Dynamic Approximate Computing on Mobiles'
arxiv_id: '2303.11291'
source_url: https://arxiv.org/abs/2303.11291
tags:
- mobile
- mobiprox
- approximation
- network
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mobiprox, a framework that enables dynamic
  approximation of deep learning operations on mobile devices. Mobiprox addresses
  the challenge of resource-efficient deep learning on mobile devices by implementing
  tunable approximations of tensor operations and enabling runtime adaptation of individual
  network layers.
---

# Mobiprox: Supporting Dynamic Approximate Computing on Mobiles

## Quick Facts
- arXiv ID: 2303.11291
- Source URL: https://arxiv.org/abs/2303.11291
- Reference count: 40
- Primary result: Framework enables up to 15% system-wide energy savings on mobile devices with minimal impact on inference accuracy

## Executive Summary
Mobiprox is a framework that enables dynamic approximation of deep learning operations on mobile devices, addressing the challenge of resource-efficient deep learning on constrained hardware. The framework implements tunable approximations of tensor operations and enables runtime adaptation of individual network layers to balance accuracy and energy consumption. Mobiprox employs a profiler and tuner to identify optimal approximation configurations and uses three adaptation strategies to dynamically adjust approximation levels based on contextual factors like input data difficulty.

## Method Summary
Mobiprox extends the ApproxHPVM compiler infrastructure to support compilation and tuning of Android applications, implementing low-level support for approximate computing on mobile CPUs and GPUs through OpenCL-based tensor operations. The framework integrates heterogeneous compilation infrastructure, approximate configuration search framework, and a novel profiler into an Android-ready pipeline. It enables context-dependent runtime adaptation of approximation levels through three strategies: naive, state-driven, and confidence-driven approaches that select configurations based on input data difficulty or classifier confidence.

## Key Results
- Achieves up to 15% system-wide energy savings on mobile devices
- Maintains minimal impact on inference accuracy while reducing resource usage
- Demonstrates effectiveness across three application domains: human activity recognition, image recognition, and spoken keyword detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mobiprox dynamically adjusts DNN approximation levels to balance accuracy and energy consumption on mobile devices
- Mechanism: Runtime adaptation strategies (naive, state-driven, confidence-driven) select approximation configurations based on input data difficulty or classifier confidence
- Core assumption: Input data difficulty or classifier confidence correlates with optimal approximation level for minimizing energy while preserving accuracy
- Evidence anchors:
  - [abstract]: "Mobiprox identifies the most promising neural network approximation configurations leading to the desired inference quality with the minimal use of resources"
  - [section]: "we harness the natural temporal dependence of the instances of sensed data... and devise three strategies demonstrating that a widely applicable goal of energy minimization can be met with Mobiprox"
  - [corpus]: No direct evidence; only general mobile DL adaptation context is present
- Break condition: If input data difficulty or classifier confidence does not correlate with optimal approximation level, adaptation strategies may not effectively minimize energy while preserving accuracy

### Mechanism 2
- Claim: Mobiprox implements low-level support for approximate computing on mobile CPUs and GPUs through compiler-level primitives
- Mechanism: Extends HPVM and ApproxHPVM to support OpenCL-based tensor operations on Android devices, enabling tunable approximations of tensor operations
- Core assumption: OpenCL-based tensor operations can be efficiently implemented on mobile devices to support approximate computing
- Evidence anchors:
  - [section]: "We implement a mechanism for turn-taking interplay of the ApproxHPVM and Android NDK LLVM compiler toolchains... while using ApproxHPVM's HPVM-IR transformations for the internal part of the compilation pipeline in order to insert the description of the desired approximate tensor operations"
  - [section]: "To enable an enhanced control over low-level concepts (such as memory allocation), we implemented the HPVM Tensor Runtime for Android using CLBlast [ 35], an OpenCL implementation of basic linear algebra subprograms (BLAS)"
  - [corpus]: No direct evidence; only general mobile DL compilation context is present
- Break condition: If OpenCL-based tensor operations are not efficiently implemented on mobile devices, approximate computing support may not be effective

### Mechanism 3
- Claim: Mobiprox provides an end-to-end framework for dynamic approximation of mobile deep learning, enabling energy savings while preserving accuracy
- Mechanism: Integrates heterogeneous compilation infrastructure, approximate configuration search framework, and novel profiler into an Android-ready pipeline
- Core assumption: End-to-end framework can effectively integrate various components to enable dynamic approximation of mobile deep learning
- Evidence anchors:
  - [abstract]: "Mobiprox introduces significant extensions to the ApproxHPVM compiler infrastructure to support compilation and tuning of Android applications"
  - [section]: "In this paper we present Mobiprox, a framework enabling flexible-accuracy on-device deep learning. Mobiprox implements tunable approximations of tensor operations and enables runtime-adaptable approximation of individual network layers"
  - [corpus]: No direct evidence; only general mobile DL framework context is present
- Break condition: If integration of various components is not effective, end-to-end framework may not enable dynamic approximation of mobile deep learning

## Foundational Learning

- Concept: Dynamic Neural Network Compression
  - Why needed here: To understand need for Mobiprox and its approach to enabling dynamic adaptation of DNN approximation levels
  - Quick check question: What are limitations of static neural network compression techniques, and how does dynamic compression address these limitations?

- Concept: Approximate Computing
  - Why needed here: To understand underlying principles of Mobiprox's approach to enabling approximate computing on mobile devices
  - Quick check question: What are key techniques for approximate computing, and how do they differ in terms of hardware and software requirements?

- Concept: Mobile Deep Learning Compilation
  - Why needed here: To understand challenges and approaches to compiling and optimizing DNNs for mobile devices
  - Quick check question: What are key considerations when compiling DNNs for mobile devices, and how do compiler-level primitives enable approximate computing?

## Architecture Onboarding

- Component map: HPVM -> ApproxHPVM -> HPVM Tensor Runtime for Android -> Mobiprox Profiler for Android -> Runtime adaptation strategies
- Critical path: DNN model -> ApproxHPVM compilation -> HPVM Tensor Runtime for Android -> Mobiprox Profiler for Android -> Runtime adaptation strategies
- Design tradeoffs:
  - Precision vs. energy consumption
  - Runtime adaptation complexity vs. energy savings
  - Mobile device compatibility vs. approximation technique availability
- Failure signatures:
  - Inaccurate approximation configuration identification
  - Inefficient OpenCL-based tensor operations
  - Ineffective integration of framework components
- First 3 experiments:
  1. Evaluate energy savings and accuracy preservation of Mobiprox on benchmark DNN model (e.g., MobileNet) for mobile deep learning task (e.g., image classification)
  2. Compare performance of Mobiprox's adaptation strategies (naive, state-driven, confidence-driven) in terms of energy savings and accuracy preservation
  3. Assess compatibility and efficiency of Mobiprox on different mobile device architectures (e.g., ARM-based ABIs)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ordering of approximate configurations in QoS loss – Speedup space vary across different mobile devices with same architecture?
- Basis in paper: [explicit] Paper mentions that while absolute speedup values may vary across devices, ordering of configurations remains consistent across devices tested (Samsung Galaxy S21, Samsung Galaxy M21, Xiaomi Pocophone F1, and ASUS Tinkerboard S)
- Why unresolved: Paper only tested limited number of devices, and authors suggest testing with very large number of devices would be needed to confirm this assumption
- What evidence would resolve it: Extensive testing of Mobiprox on wide range of Android devices with same architecture (armeabi-v7a and arm64-v8a) to confirm ordering of approximate configurations remains consistent

### Open Question 2
- Question: What is maximum speedup Mobiprox can achieve on mobile device, and how does this compare to speedup achieved on server computer?
- Basis in paper: [explicit] Paper states maximum speedup Mobiprox achieves on mobile device is relatively modest at 1.25×, while same network architecture achieves twice the speedup on server computer
- Why unresolved: Paper suggests this discrepancy likely stems from lack of optimized support for running (approximate) deep learning on mobile devices
- What evidence would resolve it: Integration of Mobiprox with mobile DL compiler stacks that are already hand-optimized by large engineering teams in production environments, such as TVM, Pytorch Mobile, or TF Lite, to determine if improved speedup gains can be achieved

### Open Question 3
- Question: How can Mobiprox be extended to support dynamic pruning of fully-connected layers in addition to convolutional layers?
- Basis in paper: [inferred] Paper mentions Mobiprox specifically targets convolutional layers with convolution perforation and filter sampling approximations, as these layers tend to consume majority of computational time and energy in mobile neural networks. However, it also notes that for non-convolutional layers, Mobiprox allows only one type of approximation – half-precision quantization – leading to at most 2^L possible approximation configurations in an L-layer network
- Why unresolved: Paper suggests to expand range of approximation techniques, integration of dynamic pruning [27] of fully-connected layers in Mobiprox should be investigated
- What evidence would resolve it: Development and implementation of method for dynamically pruning fully-connected layers in Mobiprox, and evaluation of resulting approximation configurations and energy savings

## Limitations

- Implementation relies heavily on specific compiler toolchains (HPVM/ApproxHPVM) and OpenCL-based tensor operations that may not be universally available across all mobile devices
- Evaluation limited to three specific application domains (HAR, image recognition, keyword spotting) with relatively small models (ResNet-20, VGG-16), raising questions about scalability to larger, more complex models
- Does not address potential accuracy degradation under prolonged use or varying environmental conditions

## Confidence

- **High Confidence**: Core mechanism of using approximation techniques (perforation, sampling, quantization) to reduce computational load is well-established in approximate computing literature; compilation pipeline using HPVM and ApproxHPVM is technically sound
- **Medium Confidence**: Claim that runtime adaptation strategies can achieve reported energy savings while preserving accuracy is supported by experiments, but generalizability across diverse mobile workloads and device architectures remains uncertain
- **Low Confidence**: Assertion that input data difficulty or classifier confidence reliably indicates optimal approximation levels is based on relatively limited empirical evidence and may not hold for all application scenarios

## Next Checks

1. **Cross-Platform Verification**: Test Mobiprox on diverse set of Android devices with different GPU architectures and Android versions to validate framework's portability claims

2. **Large-Scale Model Testing**: Evaluate framework's performance and energy savings on larger, more complex models (e.g., MobileNetV2, EfficientNet) commonly used in production mobile applications

3. **Longitudinal Accuracy Assessment**: Conduct extended runtime experiments (24+ hours) to measure accuracy stability and energy savings consistency under varying workloads and thermal conditions