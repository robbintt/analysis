---
ver: rpa2
title: Adaptive parameter sharing for multi-agent reinforcement learning
arxiv_id: '2312.09009'
source_url: https://arxiv.org/abs/2312.09009
tags:
- uni00000013
- agents
- sharing
- parameter
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Adaptive Parameter Sharing (AdaPS), a novel
  parameter sharing method for multi-agent reinforcement learning. AdaPS uses a variational
  autoencoder to encode agent identities, clusters agents based on their identities,
  and generates unique subnetworks for each cluster using a fixed mapping network.
---

# Adaptive parameter sharing for multi-agent reinforcement learning

## Quick Facts
- arXiv ID: 2312.09009
- Source URL: https://arxiv.org/abs/2312.09009
- Reference count: 0
- This paper proposes Adaptive Parameter Sharing (AdaPS), a novel parameter sharing method for multi-agent reinforcement learning that uses VAE encoding, clustering, and masking to achieve better performance than existing methods while maintaining the same parameter size.

## Executive Summary
This paper addresses the challenge of parameter sharing in multi-agent reinforcement learning with heterogeneous agents. The proposed Adaptive Parameter Sharing (AdaPS) method uses a Variational Autoencoder (VAE) to encode agent identities, clusters agents based on these encodings, and generates unique subnetworks for each cluster using a fixed mapping network. This approach allows agents in the same cluster to share experiences while maintaining strategy diversity across clusters. Experiments on three environments demonstrate that AdaPS outperforms existing parameter sharing methods including Full Parameter Sharing, Selective Parameter Sharing, and No Parameter Sharing, while maintaining the same parameter size as Full Parameter Sharing.

## Method Summary
AdaPS first trains a VAE on trajectories collected by an initial shared policy to encode agent identities into vectors. These vectors are clustered using K-means into K groups, where K < N (total number of agents). A fixed, randomly initialized mapping network generates unique binary masks for each cluster center by passing them through a sigmoid function and applying a drop threshold λ. These masks are applied to a shared policy network to create distinct subnetworks for each cluster. During reinforcement learning training with A2C, agents in the same cluster use the same masked subnetwork while agents in different clusters use different subnetworks, enabling both experience sharing within clusters and strategy diversity across clusters.

## Key Results
- AdaPS achieves higher sample efficiency and performance compared to baselines (NoPS, FuPS, FuPS+id, SePS, SNP-PS) while maintaining the same parameter size as Full Parameter Sharing
- The method successfully handles heterogeneous agents with different reward functions and transition models without increasing model size
- Experiments on Blind-particle Spread, Coloured Multi-Robot Warehouse, and Level-based Foraging environments demonstrate consistent performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Agents with similar identities are grouped together and share the same subnetwork, allowing them to learn faster through experience sharing while maintaining strategy diversity across groups.
- Mechanism: The method uses a VAE to encode agent identities into vectors, then applies K-means clustering to group similar agents. A fixed mapping network generates unique binary masks for each cluster center, which are applied to a shared policy network to create distinct subnetworks for each group.
- Core assumption: Agents with similar identity vectors have sufficiently similar reward functions and transition models that they can benefit from sharing experiences without hindering performance.
- Evidence anchors: [abstract] "Agents in one cluster can share the experience to promote learning due to they have the same network structure"; [section] "Through this masking process, agents in one cluster can get the same structures, enabling the sharing of experiences to facilitate learning"
- Break Condition: If agents within a cluster have significantly different reward functions or transition dynamics, sharing experiences will slow learning rather than accelerate it.

### Mechanism 2
- Claim: Using a fixed, untrained mapping network to generate masks eliminates the need for additional training parameters while still creating diverse subnetworks.
- Mechanism: The mapping network is randomly initialized and never trained. Its output passes through a sigmoid function to produce activation probabilities, which are then thresholded to create binary masks that select different subsets of neurons in the shared network.
- Core assumption: Random but fixed mappings can produce sufficiently diverse and useful subnetworks without optimization.
- Evidence anchors: [section] "We choose a randomly initialized and fixed network as the mapping network [23], since it is capable of generating uniform mappings without the need for training"
- Break Condition: If the random masks produce subnetworks that are too similar or too different from what would be optimal, performance will degrade compared to trained approaches.

### Mechanism 3
- Claim: Pre-training the VAE on initial trajectories before reinforcement learning allows the identity encoding to capture meaningful agent characteristics that correlate with behavior.
- Mechanism: The VAE is trained on trajectories collected by an initial shared policy to learn a latent space where agents with similar intrinsic characteristics (reward functions, transition models) are close together.
- Core assumption: The initial shared policy generates diverse enough trajectories that the VAE can learn meaningful distinctions between agent types.
- Evidence anchors: [section] "We first train the V AE with the trajectories collected by the initial shared policy, so that it can encode the agents' identity information"
- Break Condition: If the initial policy is too poor or too uniform, the VAE may not learn meaningful distinctions, leading to poor clustering and suboptimal masks.

## Foundational Learning

- Concept: Variational Autoencoders (VAEs)
  - Why needed here: VAEs are used to encode agent identity information into a latent space where similar agents are close together, enabling effective clustering
  - Quick check question: What is the difference between a VAE and a standard autoencoder, and why is this difference important for capturing agent identity?

- Concept: K-means clustering
  - Why needed here: K-means is used to partition agents into groups based on their identity vectors so that similar agents can share experiences
  - Quick check question: How does the choice of K (number of clusters) affect the tradeoff between diversity and sample efficiency in this method?

- Concept: Network masking and pruning
  - Why needed here: Masking is used to create distinct subnetworks from a shared network without increasing parameter count, allowing different agent groups to have different expressive capabilities
  - Quick check question: What is the relationship between the drop threshold λ and the diversity of strategies across clusters?

## Architecture Onboarding

- Component map: VAE encoder (fe) -> K-means clustering -> Fixed mapping network (fm) -> Shared policy network -> Actor-critic RL modules
- Critical path: Trajectory collection → VAE pre-training → Identity vector extraction → Clustering → Mask generation → RL training with masked networks
- Design tradeoffs:
  - Fixed vs. learned mapping network: Fixed saves parameters but may be suboptimal
  - Number of clusters K: More clusters increase diversity but reduce sample efficiency per cluster
  - Drop threshold λ: Higher values increase diversity but may remove important neurons
  - VAE capacity: Larger VAEs can capture more complex identity relationships but require more data
- Failure signatures:
  - Poor performance across all agents: Likely issues with VAE training or mask generation
  - Some agents perform well, others poorly: Clusters may be poorly formed or masks may be inappropriate for certain agent types
  - No improvement over full parameter sharing: Masking may be too conservative (λ too low) or clustering may be ineffective
  - Instability during training: May indicate poor initial policy for VAE pre-training or inappropriate λ value
- First 3 experiments:
  1. Ablation study: Compare AdaPS with and without the VAE pre-training step to measure its impact on performance
  2. Sensitivity analysis: Vary the number of clusters K and drop threshold λ to find optimal values for a given environment
  3. Visualization study: Plot agent trajectories in the VAE latent space before and after clustering to verify that similar agents are grouped together

## Open Questions the Paper Calls Out
No open questions are explicitly stated in the paper.

## Limitations
- The paper does not provide guidance on selecting the optimal number of clusters K or drop threshold λ for different environments
- No comparison with alternative clustering algorithms or identity encoding methods is provided
- The method's scalability to environments with a large number of agent types is not demonstrated

## Confidence
- High confidence: Core concept that adaptive parameter sharing can improve performance in heterogeneous multi-agent systems
- Medium confidence: Specific VAE + clustering + masking mechanism, as evidence relies heavily on authors' own experiments
- Low confidence: Choice of fixed random mapping network, as cited reference [23] addresses exploration rather than parameter sharing

## Next Checks
1. **Ablation study**: Implement AdaPS without the VAE pre-training step to quantify the contribution of learned identity vectors versus random initialization
2. **Hyperparameter sensitivity**: Systematically vary K (number of clusters) and λ (drop threshold) across multiple random seeds to identify robust operating ranges
3. **Visualization analysis**: Plot agent trajectories in the VAE latent space before and after clustering to verify that similar agents are properly grouped and that cluster separation is meaningful