---
ver: rpa2
title: 'Explainable AI applications in the Medical Domain: a systematic review'
arxiv_id: '2308.05411'
source_url: https://arxiv.org/abs/2308.05411
tags:
- medical
- learning
- explainable
- explanations
- explainability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a systematic literature review of Explainable
  AI (XAI) applications in the medical domain, analyzing 198 articles published between
  2019 and October 2022. The review aimed to understand which XAI techniques are used
  for specific medical tasks, whether there is a connection between techniques and
  medical data, if explanations are evaluated by physicians, and to what extent explainability
  challenges are addressed.
---

# Explainable AI applications in the Medical Domain: a systematic review

## Quick Facts
- arXiv ID: 2308.05411
- Source URL: https://arxiv.org/abs/2308.05411
- Reference count: 40
- Primary result: Systematic review of 198 XAI medical articles (2019-Oct 2022) shows SHAP and feature attribution dominate, with limited physician involvement and evaluation

## Executive Summary
This systematic literature review analyzes 198 articles on Explainable AI (XAI) applications in medicine, covering publications from 2019 to October 2022. The review examines which XAI techniques are applied to specific medical tasks, their relationship with medical data types, physician evaluation practices, and how well explainability challenges are addressed. The findings reveal that model-agnostic techniques, particularly SHAP, and feature attribution methods on deep learning architectures are most commonly used for medical diagnosis and decision support.

## Method Summary
The review employed the PRISMA framework to systematically search and screen medical-related articles from Scopus, IEEE Xplore, and PubMed databases. After applying inclusion/exclusion criteria to 477 initial articles, 198 relevant studies were selected for analysis. The methodology involved high-level analysis of technical characteristics across all studies, followed by detailed evaluation of representative studies that incorporated domain expert involvement. Explainability assessment metrics were applied to studies featuring human-in-the-loop approaches.

## Key Results
- Model-agnostic techniques (mainly SHAP) and feature attribution methods on deep learning architectures are most commonly applied for medical diagnosis and decision support
- Deep learning models are utilized more than other ML models, with 75% of studies proposing explainable solutions based on deep learning
- Explainability is primarily applied to promote trust, though only 30% of studies involved domain experts in the development cycle

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Model-agnostic techniques like SHAP are most widely used for explaining predictions in medical AI because they can be applied across multiple model types without altering the underlying model architecture.
- **Mechanism:** SHAP uses game-theoretic concepts (Shapley values) to fairly distribute the contribution of each feature across all possible feature coalitions, producing both local and global explanations. This generality allows SHAP to be applied to both deep learning and ensemble models commonly used in medical prediction tasks.
- **Core assumption:** Feature importance derived from game-theoretic principles provides meaningful explanations that physicians can understand and trust.
- **Evidence anchors:**
  - [abstract] "model-agnostic techniques (mainly SHAP) and feature attribution methods on deep learning architectures are most commonly applied"
  - [section] "Shapley values (SHAP) is the technique most utilized in this category (~70%)... It has been applied mainly on top of deep learning architectures [142] and ensemble models [143]"
  - [corpus] Weak - The corpus contains only related papers without specific evidence about SHAP's effectiveness in medical AI

### Mechanism 2
- **Claim:** Visual and interactive user interfaces significantly improve physicians' understanding of AI explanations compared to textual explanations alone.
- **Mechanism:** Visual representations (heatmaps, attention maps, decision trees) allow physicians to quickly identify which regions of medical images or which features of patient data are most relevant to predictions, enabling faster clinical reasoning and verification.
- **Core assumption:** Physicians process visual information more efficiently than numerical feature importance scores when evaluating medical AI recommendations.
- **Evidence anchors:**
  - [abstract] "Visual and interactive user interfaces are found to be more useful for understanding explanations and recommendations"
  - [section] "The explanation of the proposed approach is straightforward as the system is transparent by design... An example is illustrated in Fig. 5 (e)" and "An example of these visual results is illustrated in Fig. 6 (d)"
  - [corpus] Weak - The corpus contains only related papers without specific evidence about visual interface effectiveness

### Mechanism 3
- **Claim:** The combination of opaque baseline models (deep learning) with model-agnostic explanation techniques creates a practical tradeoff between predictive performance and interpretability in medical AI systems.
- **Mechanism:** Deep learning models provide superior predictive accuracy for complex medical tasks like image analysis and disease prediction, while model-agnostic techniques like SHAP can be applied post-hoc to explain these models' predictions without sacrificing performance.
- **Core assumption:** The accuracy gains from deep learning models outweigh the interpretability limitations, and post-hoc explanations can sufficiently bridge the trust gap for clinical adoption.
- **Evidence anchors:**
  - [abstract] "deep learning models are utilized more than other ML models" and "explainability is primarily applied to promote trust"
  - [section] "seventy five percent (75%) of the articles examined proposed an explainable solution based on a deep learning architecture" and "We did not identify any pattern between the XAI technique and the medical task"
  - [corpus] Weak - The corpus contains only related papers without specific evidence about this tradeoff

## Foundational Learning

- **Concept: Shapley values in cooperative game theory**
  - Why needed here: Understanding the mathematical foundation of SHAP is crucial for evaluating whether feature contributions fairly represent each feature's contribution to predictions
  - Quick check question: How does the Shapley value calculation ensure that feature contributions sum to the difference between the model prediction and the baseline?

- **Concept: Feature attribution vs feature interaction**
  - Why needed here: Distinguishing between simple feature importance and understanding how features interact is essential for interpreting medical AI explanations
  - Quick check question: What is the key difference between SHAP's feature attribution and PDP's (Partial Dependence Plots) approach to feature interaction?

- **Concept: Local vs global explanations**
  - Why needed here: Medical practitioners need both patient-specific explanations and understanding of overall model behavior for different use cases
  - Quick check question: When would a physician prefer a local explanation versus a global explanation for a medical AI recommendation?

## Architecture Onboarding

- **Component map:**
  - Data ingestion → Model training (baseline) → Explanation generation → Visualization interface → User feedback loop
  - Key components: Medical data preprocessing, ML model (CNN/RNN/ensemble), XAI technique (SHAP/LIME), UI framework, feedback collection

- **Critical path:**
  1. Medical data preparation and preprocessing
  2. Baseline model training and validation
  3. XAI technique application (post-hoc or ante-hoc)
  4. Explanation visualization and interface development
  5. Physician evaluation and feedback integration

- **Design tradeoffs:**
  - Model complexity vs interpretability: Deep learning offers better accuracy but requires post-hoc explanations
  - Real-time vs batch processing: Interactive interfaces need fast explanation generation
  - General vs specific explanations: Balance between global model understanding and patient-specific insights

- **Failure signatures:**
  - Explanations that contradict domain knowledge or clinical guidelines
  - High computational latency preventing real-time use in clinical settings
  - Poor user interface design leading to misunderstanding of explanations
  - Lack of physician involvement resulting in explanations that don't address clinical needs

- **First 3 experiments:**
  1. Implement SHAP on a simple medical dataset (e.g., diabetes prediction) to verify feature importance calculations match domain expertise
  2. Compare visual explanations (heatmaps) vs textual explanations for a medical image classification task with physician users
  3. Evaluate explanation comprehension using pre/post tests with medical practitioners for different explanation types

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can XAI systems in medicine be designed to provide contrastive explanations that effectively answer "why not" questions for physicians?
- Basis in paper: [explicit] The review identified that only 2 out of 10 studies offered contrastive explanation features, and physicians expressed the need for explanations that address why a different prediction wasn't made.
- Why unresolved: The technical challenges of implementing contrastive explanations that are both accurate to the model and understandable to physicians remain poorly understood. The social interaction aspect of explanations also needs further investigation.
- What evidence would resolve it: Studies demonstrating effective contrastive explanation implementations in medical XAI systems, showing physician comprehension and trust, and validating the explanations against ground truth clinical reasoning.

### Open Question 2
- Question: What are the optimal evaluation metrics and frameworks for assessing the quality of explanations in medical XAI systems?
- Basis in paper: [explicit] The review found that while various evaluation approaches exist, there is no standardized framework for XAI solution assessment in medicine, particularly for explanation quality.
- Why unresolved: The field lacks established methods for evaluating explanation quality, with most studies using ad-hoc approaches. There is a need to connect design goals with evaluation methods and develop domain-specific metrics.
- What evidence would resolve it: Development and validation of comprehensive evaluation frameworks that incorporate both technical accuracy metrics and physician-centered usability assessments, with demonstrated application across different medical domains.

### Open Question 3
- Question: How can hybrid neuro-symbolic approaches be effectively integrated into medical XAI systems to provide explanations that align with medical expertise and guidelines?
- Basis in paper: [explicit] The review mentioned hybrid approaches combining machine learning with symbolic AI methods, but noted that current methods primarily derive features from models rather than incorporating medical theory and guidelines.
- Why unresolved: The technical challenges of integrating symbolic reasoning with deep learning, and mapping medical expertise into interpretable models, remain significant. The cognitive compatibility with expert users needs further exploration.
- What evidence would resolve it: Successful implementations of neuro-symbolic medical XAI systems that demonstrate improved explanation quality and physician acceptance compared to traditional approaches, with validation against clinical outcomes.

## Limitations
- Limited physician involvement in 70% of studies, with only 30% involving domain experts in development cycles
- Weak direct evidence for key mechanisms, with corpus analysis relying on abstract-level patterns rather than detailed methodological analysis
- Potential publication bias toward successful implementations with limited visibility into failed attempts

## Confidence
- General trends in XAI medical applications: Medium
- Specific clinical impact claims: Low

## Next Checks
1. Conduct a randomized controlled trial comparing physician decision-making accuracy and trust when using different XAI explanation types (visual vs textual vs combined) for the same medical AI system
2. Perform computational benchmarking of SHAP explanation generation latency on high-dimensional medical imaging data to verify real-time clinical feasibility
3. Implement a longitudinal study tracking physician adoption and sustained use of XAI-augmented medical systems in clinical workflows, measuring both quantitative performance metrics and qualitative trust assessments