---
ver: rpa2
title: 'Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO'
arxiv_id: '2309.06132'
source_url: https://arxiv.org/abs/2309.06132
tags:
- vagueness
- vago
- sentences
- factual
- vago-n
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present VAGO, a hybrid symbolic-neural system for measuring
  vagueness and subjectivity in text. The symbolic VAGO uses a lexicon-based approach
  with four categories of vague terms (approximation, generality, degree, combinatorial)
  to assign scores of factual vagueness, subjective vagueness, and detail/vagueness
  to French and English sentences and documents.
---

# Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO

## Quick Facts
- arXiv ID: 2309.06132
- Source URL: https://arxiv.org/abs/2309.06132
- Reference count: 27
- Authors classify satirical articles as significantly more vague and subjective than regular press articles

## Executive Summary
This paper presents VAGO, a hybrid symbolic-neural system for measuring vagueness and subjectivity in French and English texts. The symbolic VAGO uses a lexicon-based approach with four categories of vague terms (approximation, generality, degree, combinatorial) to assign scores of factual vagueness, subjective vagueness, and detail/vagueness to sentences and documents. The authors validate their system on fact vs. opinion statements and a French press corpus, then create a neural clone VAGO-N using RoBERTa that replicates the symbolic system's performance with R² > 0.85. They demonstrate that LIME analysis can identify new vague terms for lexicon enrichment and show that the neural approach enables multilingual versions through translation and score preservation.

## Method Summary
The authors develop VAGO as a lexicon-based expert system that detects vague terms categorized into approximation, generality, degree, and combinatorial vagueness. The system calculates four scores: vagueness (ratio of vague words to total words), subjectivity (ratio of subjective vague expressions to total words), factual vagueness (ratio of factual vague expressions to total words), and detail/vagueness (ratio of named entities to the sum of named entities and vague terms). VAGO-N is implemented as a RoBERTa-based neural model trained on VAGO scores using MSE loss to predict vagueness scores. The authors use LIME for interpretability to identify key terms contributing to predictions and explore multilingual capabilities through translation and score preservation.

## Key Results
- VAGO correctly classifies 8 of 10 fact vs. opinion statements as subjective, with the two factual statements receiving the highest detail/vagueness scores
- Satirical articles are significantly more vague (mean 0.08 vs 0.07), more subjective (mean 0.03 vs 0.02), and less detailed (mean 0.75 vs 0.79) than regular press articles
- VAGO-N replicates symbolic VAGO performance with R² > 0.85 and MAE < 0.03 on the FreSaDa corpus
- LIME analysis identifies additional vague terms not in the original lexicon, demonstrating potential for lexicon enrichment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VAGO-N successfully replicates the symbolic VAGO's performance by training on the scores generated by VAGO, enabling the neural model to learn the same linguistic cues without explicit lexicon access.
- Mechanism: The neural model uses regression to map sentences to the VAGO scores, capturing the underlying patterns that associate lexical vagueness markers with numeric scores.
- Core assumption: The symbolic VAGO's scoring function is consistent and stable enough that a neural model can approximate it with sufficient training data.
- Evidence anchors:
  - [abstract] "we build a neural clone of VAGO, based on a BERT-like architecture, trained on the symbolic VAGO scores obtained on FreSaDa"
  - [section III-A] "VAGO-N, based on combining a BERT [6] architecture with a regression layer and an MSE loss function to predict a score of vagueness for sentences"
  - [corpus] Weak - no direct comparison to other neural models trained on different objectives
- Break condition: If the symbolic scoring rules change frequently or are inconsistent across contexts, the neural model's approximation will degrade.

### Mechanism 2
- Claim: LIME analysis validates that VAGO-N uses the same lexical cues as the symbolic VAGO while identifying additional relevant terms for lexicon enrichment.
- Mechanism: LIME identifies the contribution of each token to the neural model's vagueness score, allowing comparison with the symbolic lexicon and discovery of new vague terms.
- Core assumption: The neural model's internal representation aligns with the symbolic system's explicit categorization rules.
- Evidence anchors:
  - [abstract] "Using explainability tools (LIME), we show the interest of this neural version for the enrichment of the lexicons of the symbolic version"
  - [section IV-A] "LIME applied to VAGO-N identifies adjectives and terms of excess that were already present in the extant VAGO lexicon... But LIME also identifies other adjectives carrying combinatorial vagueness that are not yet in the lexicon"
  - [corpus] Weak - no mention of how LIME performance compares to other interpretability methods
- Break condition: If the neural model develops different internal representations than the symbolic rules, LIME explanations may not reflect actual decision-making processes.

### Mechanism 3
- Claim: VAGO-N enables multilingual versions by assuming vagueness scores are preserved across languages when translating the training corpus.
- Mechanism: The French corpus is translated to English, and VAGO-N is trained on the translated text with preserved scores, assuming the same vagueness markers will be identified post-translation.
- Core assumption: Vague expressions and their scoring functions are language-agnostic, and translation preserves the vagueness characteristics of the original text.
- Evidence anchors:
  - [abstract] "The neural approach also enabled multilingual versions by leveraging translation and score preservation"
  - [section IV-B] "it is possible to translate the VAGO-N training set by relying on the following assumption: vague-ness scores, in particular subjective vagueness and factual vagueness scores, are preserved from the source language into the target language"
  - [corpus] Weak - no validation that translation preserves vagueness characteristics compared to training separate models per language
- Break condition: If translation alters the context or meaning of vague expressions, the preserved scores will no longer correspond to the linguistic reality of the target language.

## Foundational Learning

- Concept: Vagueness categorization (approximation, generality, degree, combinatorial)
  - Why needed here: The VAGO system relies on this four-category framework to identify vague terms and assign appropriate scores
  - Quick check question: Can you explain why "grand" (big) would be categorized as degree vagueness while "beau" (beautiful) is combinatorial vagueness?

- Concept: Named entity recognition for detail scoring
  - Why needed here: VAGO uses the ratio of named entities to vague terms to calculate detail/vagueness scores
  - Quick check question: How would you compute the detail/vagueness score for a sentence containing 3 named entities and 5 vague terms out of 20 total words?

- Concept: BERT/RoBERTa architecture for sequence classification
  - Why needed here: VAGO-N uses these transformer architectures as the base for predicting vagueness scores
  - Quick check question: What is the key difference between BERT and RoBERTa that might affect performance in this task?

## Architecture Onboarding

- Component map: VAGO (symbolic expert system with lexicon and rules) -> VAGO-N (neural clone with BERT/RoBERTa + regression layer) -> LIME (interpretability) -> FreSaDa corpus (training data)
- Critical path: FreSaDa -> VAGO scoring -> VAGO-N training -> VAGO-N prediction -> LIME analysis -> Lexicon enrichment
- Design tradeoffs: Symbolic approach provides interpretability but limited scalability; neural approach scales better but requires explainability tools; translation approach enables multilingual expansion but assumes score preservation
- Failure signatures: Low R² between VAGO and VAGO-N indicates poor replication; inconsistent LIME explanations suggest neural model divergence; poor multilingual performance indicates translation issues
- First 3 experiments:
  1. Train VAGO-N on FreSaDa and measure R² against VAGO scores to verify replication
  2. Apply LIME to VAGO-N outputs and compare identified tokens with VAGO lexicon to validate explainability
  3. Translate FreSaDa to English and train VAGO-N to verify multilingual capability and score preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the VAGO system be extended to include more contextual features, such as direct vs. reported discourse, to improve its accuracy in identifying subjective and objective information?
- Basis in paper: [explicit] The authors mention that they plan to update the VAGO interface to help users have a reliable grasp of levels of objectivity and subjectivity in the texts they read in the media. They also mention the possibility of extending VAGO with additional markers of subjectivity beyond adjectives, such as explicit markers (first-person pronouns, exclamation marks) and more contextual features (direct vs. reported discourse).
- Why unresolved: The paper does not provide any information on how the VAGO system could be extended to include more contextual features or the potential impact of such extensions on its accuracy.
- What evidence would resolve it: Conducting experiments to evaluate the impact of including additional contextual features on the accuracy of the VAGO system in identifying subjective and objective information.

### Open Question 2
- Question: How can the VAGO-N neural clone be used to generate lexicons for symbolic versions of VAGO in other languages, and what are the potential challenges and limitations of this approach?
- Basis in paper: [explicit] The authors mention that once trained, VAGO-N can be used to complete the extant lexicons of the symbolic VAGO and to produce neural versions of VAGO in other languages and to generate lexicons for symbolic versions in these languages.
- Why unresolved: The paper does not provide any information on how the VAGO-N neural clone can be used to generate lexicons for symbolic versions of VAGO in other languages or the potential challenges and limitations of this approach.
- What evidence would resolve it: Conducting experiments to evaluate the effectiveness of using VAGO-N to generate lexicons for symbolic versions of VAGO in other languages and identifying the potential challenges and limitations of this approach.

### Open Question 3
- Question: How can the VAGO system be adapted to handle different domains and genres of text, such as scientific articles, legal documents, or social media posts, and what are the potential challenges and limitations of such adaptations?
- Basis in paper: [inferred] The paper does not explicitly mention the adaptation of the VAGO system to different domains and genres of text, but it is a logical extension of the system's capabilities.
- Why unresolved: The paper does not provide any information on how the VAGO system can be adapted to handle different domains and genres of text or the potential challenges and limitations of such adaptations.
- What evidence would resolve it: Conducting experiments to evaluate the effectiveness of adapting the VAGO system to different domains and genres of text and identifying the potential challenges and limitations of such adaptations.

## Limitations

- The validation on the small benchmark (10 fact/opinion statements) represents weak evidence for VAGO's effectiveness
- Claims about satirical articles being "significantly more vague" lack statistical rigor with no p-values or confidence intervals reported
- The multilingual approach relies on the untested assumption that vagueness scores are preserved across languages
- LIME analysis shows promise but lacks validation that identified additional terms actually improve performance

## Confidence

- High confidence: The core mechanism of using a lexicon-based approach with four vagueness categories is well-established and clearly implemented
- Medium confidence: The neural replication performance (R² > 0.85) is reported but lacks comparison to baseline neural models trained on alternative objectives
- Low confidence: Claims about multilingual score preservation and the practical utility of lexicon enrichment through LIME analysis lack direct empirical validation

## Next Checks

1. Conduct statistical significance testing on the differences between satirical and regular article scores (t-tests with confidence intervals) to verify the "significant" claims
2. Evaluate VAGO-N's multilingual performance by training separate language-specific models and comparing their predictions against the translated model to validate the score preservation assumption
3. Implement an ablation study where VAGO's lexicon is enriched with LIME-identified terms and measure whether this improves classification accuracy on a held-out test set