---
ver: rpa2
title: 'From Text to Structure: Using Large Language Models to Support the Development
  of Legal Expert Systems'
arxiv_id: '2311.04911'
source_url: https://arxiv.org/abs/2311.04911
tags:
- legal
- pathways
- articles
- generated
- pathway
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates using GPT-4 to automatically extract structured
  legal pathways from legislative text. The method prompts GPT-4 to analyze a legal
  article and output a pathway in JSON format compatible with a legal expert system.
---

# From Text to Structure: Using Large Language Models to Support the Development of Legal Expert Systems

## Quick Facts
- arXiv ID: 2311.04911
- Source URL: https://arxiv.org/abs/2311.04911
- Reference count: 34
- Key outcome: GPT-4 extracts structured legal pathways from legislative text with 92.5% textual accuracy; 60% of generated pathways rated equivalent or better than manual ones in blind comparison.

## Executive Summary
This paper investigates using GPT-4 to automatically extract structured legal pathways from legislative text. The method prompts GPT-4 to analyze a legal article and output a pathway in JSON format compatible with a legal expert system. The pathways are then evaluated by human annotators and compared to manually created ones in a blind study. Results show 60% of generated pathways were rated equivalent or better than manual ones. 40% were directly usable and 90% needed only slight adjustment. The approach shows promise for supporting legal experts in creating structured legal representations to build transparent expert systems.

## Method Summary
The authors selected 40 standalone articles from the Quebec Civil Code and manually created pathways as ground truth. They used GPT-4 with a carefully crafted prompt to generate pathways from the articles, converting the JSON output to JusticeCreator format. Four annotators then rated the automatically generated pathways and compared them to the manual ones in a blind study, evaluating textual accuracy, logical completeness, and hallucination.

## Key Results
- 92.5% textual accuracy in generated pathways
- 60% of generated pathways rated equivalent or better than manual ones in blind comparison
- 40% of generated pathways were directly usable, 90% needed only slight adjustment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM can extract structured legal pathways from legislative text with high textual accuracy.
- Mechanism: The prompt constrains the LLM to stick closely to the legislative text by providing the full text in the prompt and instructing the model to only use information contained within it. This reduces hallucination and ensures textual alignment.
- Core assumption: The legislative text is self-contained and complete for the task at hand; no external knowledge is needed to determine the pathway structure.
- Evidence anchors:
  - [abstract] states "60% of generated pathways being rated as equivalent or better than manually created ones in a blind comparison" and "textual accuracy is very high at 92.5%."
  - [section 6.1] notes "The textual accuracy of the generated pathways is high, with 92.5% of them being rated as textually accurate. This shows that our prompt asking the model to stick to the legislative text and providing the legislative text in the prompt seems like a viable method to constrain the textual output to the targeted legislative text."
  - [corpus] shows related work on legal text processing and retrieval, supporting the premise that structured extraction from legal text is a studied problem.
- Break condition: If the legislative text references other articles or requires external legal doctrine to interpret, the assumption fails and the model may hallucinate or omit necessary elements.

### Mechanism 2
- Claim: The LLM can produce pathways that are useful as draft inputs for legal expert systems, even when not perfectly matching manual ones.
- Mechanism: The LLM captures the logical structure of legal criteria and conclusions, encoding them in a JSON format compatible with the JusticeCreator. Even if the exact mapping differs from manual pathways, the logical content remains valid and editable.
- Core assumption: There is no single "correct" pathway structure for a given article; multiple valid encodings exist, and the LLM's version is one of them.
- Evidence anchors:
  - [abstract] reports "40% were directly usable and 90% needed only slight adjustment."
  - [section 6.1] explains "Even if the article was clear, we noticed that there was often no single 'right' way to split criteria and conclusions into different elements. While some of these may be preferable, the other ones are not technically wrong."
  - [corpus] includes related papers on legal knowledge graphs and automated norm extraction, indicating precedent for structured legal representations.
- Break condition: If the article's structure is highly ambiguous or the LLM systematically misunderstands a class of legal constructions, the draft becomes less useful and requires major revision.

### Mechanism 3
- Claim: Human annotators benefit from LLM-generated drafts by discovering logical errors in their own reasoning and saving time.
- Mechanism: The draft acts as an augmented intelligence tool: it suggests a plausible pathway that the expert reviews, corrects, and finalizes. The comparison process surfaces mistakes or alternative interpretations the expert missed.
- Core assumption: Annotators are open to reviewing and improving AI-generated content, and the draft is good enough to trigger meaningful review rather than being ignored or discarded.
- Evidence anchors:
  - [abstract] notes "In the blind test we conducted, 60% of the generated pathways were rated as equivalent or better than manually created ones."
  - [section 6.3] states "the model can capture nuances or logical particularities that the humans missed. This phenomenon is also captured in the comments left by the annotators, where in five instances human annotators discovered logical errors in their own reasoning after reading the automatically generated pathway."
  - [corpus] shows related work on computer-assisted annotation and explainable legal prediction, supporting the role of AI in supporting human legal analysis.
- Break condition: If annotators distrust or dismiss the AI output without review, or if the draft quality is too low to be useful, the augmentation benefit disappears.

## Foundational Learning

- Concept: Legal criteria and conclusions structure in statutes
  - Why needed here: The LLM must identify and encode these elements into a pathway; understanding their nature is essential to evaluate and improve the output.
  - Quick check question: What distinguishes a legal criterion from a legal conclusion in a statute, and how are they linked in a decision pathway?

- Concept: Rule-based reasoning and formal representations
  - Why needed here: The output JSON must conform to the JusticeCreator's expected format, which relies on rule-based reasoning concepts like questions, answers, and logical links.
  - Quick check question: How does a question block in the JusticeCreator represent a legal criterion, and how does it connect to outcomes?

- Concept: Prompt engineering for constrained generation
  - Why needed here: The LLM's performance depends on how the prompt is crafted to limit hallucination and enforce textual fidelity.
  - Quick check question: What prompt elements are most effective at ensuring the LLM only uses information from the provided legislative text?

## Architecture Onboarding

- Component map: Input article text -> GPT-4 via OpenAI API with prompt -> JSON pathway output -> Parser validates JSON -> JCAPG interface for legal experts to review and export

- Critical path: 1. Load article text into JCAPG 2. JCAPG constructs prompt and sends to GPT-4 3. Receive JSON output 4. Parse and validate JSON 5. Export for JusticeCreator import

- Design tradeoffs:
  - Using GPT-4 (closed model) vs. open-source models: higher accuracy but less control and cost
  - Providing full text in prompt vs. external retrieval: simpler but may hit context limits
  - Focusing on standalone articles vs. multi-article dependencies: easier but less general

- Failure signatures:
  - JSON parse errors: indicates model output not conforming to expected schema
  - Missing criteria or conclusions: indicates incomplete extraction or hallucination
  - Recursive or disconnected blocks: indicates logical structure errors in model output

- First 3 experiments:
  1. Run JCAPG on a set of 5 easy, standalone articles and measure textual accuracy and usability scores from annotators.
  2. Compare LLM-generated pathways to manual ones for a set of 5 moderately complex articles, noting structural differences and annotator preference.
  3. Test the effect of prompt variations (e.g., adding instructions to avoid recursion, or to list all criteria first) on output quality for a sample of 3 articles.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the generated pathways compare to human-created ones for more complex legal articles that reference other articles?
- Basis in paper: [explicit] The authors note they deliberately selected standalone articles and excluded overly complicated articles, stating "The results may not replicate to very complex articles, or articles that can only be read in conjunction with other articles" as a limitation.
- Why unresolved: The current study only evaluated standalone articles, leaving open the question of how the model performs on more complex legislation.
- What evidence would resolve it: Testing the approach on articles that reference other articles and evaluating the generated pathways against human-created ones.

### Open Question 2
- Question: Can the generated pathways be used as drafts to make the creation of legal expert systems more efficient?
- Basis in paper: [explicit] The authors suggest the generated pathways could serve as a starting point for legal experts to verify and adjust, potentially making the process more efficient. They note 90% of generated pathways were rated as correct or needing only slight adjustment.
- Why unresolved: While the authors suggest this potential efficiency gain, they did not actually measure the time or effort required to use the generated pathways as drafts compared to creating them from scratch.
- What evidence would resolve it: Conducting a study measuring the time and effort required for legal experts to create expert systems using the generated pathways as drafts versus creating them entirely from scratch.

### Open Question 3
- Question: How can the approach be extended to integrate case law and doctrine to resolve ambiguities in legislative text?
- Basis in paper: [explicit] The authors state this as a direction for future work, noting "we aim to... potentially integrate case law and/or doctrine to resolve ambiguities" in the discussion section.
- Why unresolved: The current approach only uses the legislative text itself, without leveraging additional legal sources that could help resolve ambiguities.
- What evidence would resolve it: Developing and testing an extended approach that integrates case law and doctrine, and evaluating how it impacts the accuracy and completeness of the generated pathways.

## Limitations
- The method was only tested on standalone legal articles, not those referencing other legislation.
- No evaluation of long-term reliability or consistency of LLM-generated pathways across different sessions or model versions.
- The exact prompt text and system message used with GPT-4 are not provided, making it difficult to reproduce or assess the full impact of prompt engineering.

## Confidence

**Confidence Labels:**
- **High confidence** in the claim that GPT-4 can extract structured legal pathways from legislative text with high textual accuracy (92.5%). This is supported by robust annotator ratings and clear methodology.
- **Medium confidence** in the claim that LLM-generated pathways are useful as drafts for legal expert systems. While 40% were directly usable and 90% needed only slight adjustment, this is based on a single set of annotators and a specific legal domain (Quebec Civil Code).
- **Low confidence** in generalizability beyond standalone legal articles. The study explicitly excluded articles referencing other legislation, so the method's performance on more complex, interdependent legal texts is unknown.

**Major Uncertainties:**
- The exact prompt text and system message used with GPT-4 are not provided, making it difficult to reproduce or assess the full impact of prompt engineering.
- The study relies on a small number of annotators (4) and a single legal domain, limiting external validity.
- No evaluation of the long-term reliability or consistency of LLM-generated pathways across different sessions or model versions.

## Next Checks
1. Test the method on legal articles that reference other statutes or require external legal doctrine to interpret, to assess robustness outside the "standalone" subset.
2. Conduct a blind study with a larger, more diverse group of legal experts and across multiple legal domains (e.g., criminal law, administrative law) to evaluate generalizability.
3. Perform a longitudinal study to assess the consistency and reliability of LLM-generated pathways over time and across different GPT-4 model versions.