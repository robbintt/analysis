---
ver: rpa2
title: Instilling Inductive Biases with Subnetworks
arxiv_id: '2310.10899'
source_url: https://arxiv.org/abs/2310.10899
tags:
- subtask
- subnetwork
- induction
- inductive
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Subtask Induction, a method for instilling
  inductive biases into neural networks by discovering and transferring functional
  subnetworks from trained models. The approach works in two stages: first, it identifies
  a subnetwork that implements a specific subtask within a trained model using continuous
  sparsification; second, it transfers these subnetwork weights to a randomly initialized
  model, leaving the remaining weights trainable.'
---

# Instilling Inductive Biases with Subnetworks

## Quick Facts
- arXiv ID: 2310.10899
- Source URL: https://arxiv.org/abs/2310.10899
- Authors: 
- Reference count: 18
- This paper introduces Subtask Induction, a method for instilling inductive biases into neural networks by discovering and transferring functional subnetworks from trained models.

## Executive Summary
This paper proposes Subtask Induction, a novel approach to instill inductive biases in neural networks by discovering and transferring functional subnetworks from trained models. The method works in two stages: first, it identifies a subnetwork that implements a specific subtask within a trained model using continuous sparsification; second, it transfers these subnetwork weights to a randomly initialized model, leaving the remaining weights trainable. The approach is evaluated on arithmetic and image classification tasks, demonstrating significant improvements in sample efficiency and task-specific performance.

## Method Summary
Subtask Induction discovers functional subnetworks within trained models using continuous sparsification with binary masks. A binary mask γ is applied to model parameters θ to create a subnetwork θsub = θ ⊙ γ. The mask is optimized using sigmoid re-parameterization and annealing, with a scale coefficient β that increases during training. Discovered subnetworks are transferred to new models with remaining parameters randomly initialized and trainable, while subnetwork weights are frozen. The method is evaluated on modular arithmetic tasks (T1: a + ab mod 7, T2: a² + ab mod 7) and image classification (16-class ImageNet), showing improved sample efficiency and task-specific performance.

## Key Results
- For arithmetic tasks, transferring subnetworks reduced required training data by up to 50× compared to training from scratch, achieving near-perfect generalization with only 1% of total possible combinations.
- On image classification, transferring shape-related subnetworks significantly increased shape bias (18.8% accuracy improvement on mean-pooled ImageNet for ResNet18) and improved data efficiency compared to training from scratch.
- The method produced more robust models on cue-conflict datasets where texture and shape cues are dissociated.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subtask Induction localizes functional subnetworks that implement specific subtasks within trained models and transfers them to new models to instill inductive biases toward solutions utilizing those subtasks.
- Mechanism: The method uses continuous sparsification to train a binary mask that isolates parameters responsible for a subtask, then freezes these parameters in a new model while training remaining parameters on a related task.
- Core assumption: Trained neural networks contain modular subnetworks that can be isolated and transferred without loss of functionality.
- Evidence anchors:
  - [abstract] "Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask."
  - [section 3.1] "Given a trained neural network Mθ with parameters θ, we define a subnetwork as a model where a binary mask γ ∈ {0, 1}|θ| is applied over the original model parameters, such that θsub = θ ⊙ γ."
- Break condition: If trained models do not decompose into modular subnetworks, the isolation and transfer process would fail.

### Mechanism 2
- Claim: Continuous sparsification enables efficient discovery of functional subnetworks by approximating binary masks through sigmoid re-parameterization and annealing.
- Mechanism: Binary masks are re-parameterized using sigmoid functions and scaled with coefficient β that increases during training, allowing gradient-based optimization of mask weights and discretization at test time.
- Core assumption: Continuous approximation of binary masks through sigmoid functions can effectively identify functional subnetworks.
- Evidence anchors:
  - [section 3.1] "Continuous sparsification re-parameterizes a binary mask with element-wise sigmoid functions and schedules a scale coefficient β that increases through training to 'anneal' a soft mask to a hard one."
- Break condition: If the sigmoid approximation fails to converge to meaningful binary masks, the subnetwork discovery process would not isolate functional components.

### Mechanism 3
- Claim: Transferring discovered subnetworks to new models provides inductive biases that increase sample efficiency and generalization on related tasks.
- Mechanism: The transferred subnetwork weights are frozen while remaining parameters are trained, creating a soft inductive bias toward solutions that utilize the transferred computation.
- Core assumption: Freezing specific subnetworks in new models creates meaningful inductive biases that improve learning on related tasks.
- Evidence anchors:
  - [abstract] "We demonstrate the effectiveness of Subtask Induction on an arithmetic task, showing that Subtask Induction provides a preference for learning a particular solution with minimal training signal and significantly reduces the amount of data required for generalization."
- Break condition: If transferred subnetworks do not provide meaningful inductive biases, new models would perform similarly to random initialization.

## Foundational Learning

- Concept: Modular decomposition of neural networks into functional subnetworks
  - Why needed here: Understanding that trained models contain separable functional components is fundamental to the subnetwork transfer approach.
  - Quick check question: Can you identify which parts of a trained model are responsible for specific subtasks versus general computation?

- Concept: Continuous sparsification for binary mask optimization
  - Why needed here: This technique enables efficient discovery of functional subnetworks by approximating the intractable binary mask optimization problem.
  - Quick check question: How does the sigmoid re-parameterization and annealing process convert the discrete mask optimization into a continuous problem?

- Concept: Inductive bias and its role in machine learning
  - Why needed here: The entire approach relies on understanding how to instill preferences for certain solutions over others in neural networks.
  - Quick check question: What is the difference between hard architectural constraints and soft inductive biases in neural network design?

## Architecture Onboarding

- Component map: Train model → Discover subnetwork via continuous sparsification → Transfer subnetwork weights → Train remaining parameters on new task while freezing subnetwork
- Critical path: Train model → Discover subnetwork via continuous sparsification → Transfer subnetwork weights → Train remaining parameters on new task while freezing subnetwork
- Design tradeoffs: The method trades computational cost of mask training for potential gains in sample efficiency and generalization. It also requires careful design of subtask-specific training objectives for subnetwork discovery.
- Failure signatures: Poor performance on both original and transferred tasks, inability to discover functional subnetworks, or random initialization performing similarly to subnetwork transfer would indicate failures.
- First 3 experiments:
  1. Train a simple GPT-2 model on modular arithmetic task (a + ab mod p) to establish baseline performance.
  2. Apply continuous sparsification to discover subnetwork for subtask S1 (ab mod p) and verify it achieves the intended computation.
  3. Transfer discovered subnetwork to new task (a² + ab mod p) with minimal disambiguation data and measure sample efficiency gains.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Subtask Induction be made more data-efficient by reducing the need for supervised training of the binary mask during subnetwork discovery?
- Basis in paper: [explicit] The paper notes that "subtask induction requires supervised training of a binary mask to perform subnetwork discovery, which requires constructing custom designed datasets" as a limitation.
- Why unresolved: Current implementation requires creating custom datasets to train the binary mask, which adds overhead and limits applicability.
- What evidence would resolve it: Demonstration of successful subnetwork discovery using unsupervised or weakly supervised methods that don't require custom datasets.

### Open Question 2
- Question: How can the magnitude of the inductive bias conferred by Subtask Induction be controlled or tuned?
- Basis in paper: [explicit] The authors note that "Subtask Induction provides a soft inductive bias — there is currently no way to increase or decrease the magnitude of the inductive bias we confer upon the model."
- Why unresolved: The current method provides an all-or-nothing transfer of subnetworks without fine-grained control over the strength of the bias.
- What evidence would resolve it: Methods that allow graduated transfer of subnetworks or dynamic adjustment of the subnetwork's influence during training.

### Open Question 3
- Question: What is the relationship between model depth and the effectiveness of Subtask Induction?
- Basis in paper: [explicit] The arithmetic experiment showed "The effect of Subtask Induction is constant across different number of layers, suggesting that the inductive bias the model gains does not seem to depend on depth of the model, at least in this arithmetic task."
- Why unresolved: The paper only tested this relationship on one task type (arithmetic), and the conclusion may not generalize to more complex tasks like image classification.
- What evidence would resolve it: Systematic experiments varying model depth across multiple task types to establish whether the relationship holds or breaks down in different domains.

## Limitations

- The method's effectiveness depends heavily on the existence of modular subnetworks within trained models, which may not hold for all architectures or tasks.
- The continuous sparsification approach requires careful hyperparameter tuning (β scheduling, l0 penalty), and the quality of discovered subnetworks can be sensitive to these choices.
- Vision task results show directional improvements but limited absolute performance gains (18.8% on mean-pooled ImageNet), suggesting the approach may have narrower applicability than claimed.

## Confidence

- **High Confidence:** The core algorithmic framework of continuous sparsification and subnetwork transfer is technically sound and well-implemented. The methodology for isolating functional subnetworks through binary mask optimization is clearly specified.
- **Medium Confidence:** The empirical results on arithmetic tasks are convincing, showing consistent sample efficiency gains. However, the vision task results, while showing directional improvements, have more modest absolute performance gains that may not generalize to more complex scenarios.
- **Low Confidence:** The claim that this approach can "significantly reduce" data requirements in general settings is overstated based on the evidence. The vision experiments show improvements in shape bias but limited gains in overall accuracy.

## Next Checks

1. **Ablation Study on Sparsity Levels:** Systematically vary the proportion of frozen parameters in transferred subnetworks (e.g., 1%, 5%, 10%, 25%) to determine the optimal trade-off between inductive bias strength and remaining model capacity across different tasks.

2. **Cross-Domain Transferability:** Test whether subnetworks discovered in one domain (e.g., arithmetic) can transfer meaningful inductive biases to completely different domains (e.g., vision), which would validate the generality of the approach beyond task-specific adaptations.

3. **Comparison with Alternative Bias-Instillation Methods:** Benchmark against other approaches for instilling inductive biases, such as architectural modifications (e.g., convolution-first layers for shape bias) or meta-learning techniques, to establish whether subnetwork transfer offers unique advantages.