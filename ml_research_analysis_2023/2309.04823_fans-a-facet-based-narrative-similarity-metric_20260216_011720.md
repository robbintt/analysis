---
ver: rpa2
title: 'FaNS: a Facet-based Narrative Similarity Metric'
arxiv_id: '2309.04823'
source_url: https://arxiv.org/abs/2309.04823
tags:
- similarity
- facets
- narratives
- narrative
- level
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FaNS, a facet-based narrative similarity
  metric that measures the similarity between narratives by extracting and comparing
  the 5W1H facets (Who, What, When, Where, Why, How) using large language models.
  Unlike existing metrics that rely on overall lexical/semantic matching, FaNS provides
  granular matching along six different facets independently and then combines them.
---

# FaNS: a Facet-based Narrative Similarity Metric

## Quick Facts
- arXiv ID: 2309.04823
- Source URL: https://arxiv.org/abs/2309.04823
- Reference count: 23
- Primary result: FaNS achieves 37% higher correlation with ground truth narrative similarity labels compared to traditional metrics like ROUGE and BERTScore

## Executive Summary
This paper introduces FaNS (Facet-based Narrative Similarity), a novel metric that measures narrative similarity by decomposing narratives into 6W1H facets (Who, What, When, Where, Why, How) and comparing them independently before aggregation. Unlike traditional metrics that rely on overall lexical or semantic matching, FaNS provides granular facet-level matching using large language models for extraction and weighted combination of results. Experiments on news articles from the AllSides dataset demonstrate that FaNS achieves significantly higher correlation with human-labeled ground truth similarity scores compared to baseline metrics.

## Method Summary
FaNS extracts 5W1H facets from narratives using large language models (ChatGPT and Bard) through three levels of prompts, then computes similarity for each facet independently using techniques like fuzzy matching, semantic matching, and entity recognition. The facet similarities are combined using weighted linear aggregation with α=0.2 for entity-specific facets (Who, When, Where) and 0.8 for descriptive facets (What, Why, How). The metric is evaluated on news narratives from AllSides, measuring correlation with ground truth similarity labels using Kendall τ.

## Key Results
- FaNS achieves 37% higher correlation with ground truth labels than traditional metrics like ROUGE and BERTScore
- Facet weighting with α=0.2 provides optimal correlation, with the What facet showing the strongest individual correlation (41%) with ground truth
- LLMs show variable performance in facet extraction, with ChatGPT succeeding on 96% of narratives versus Bard's 63% success rate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FaNS improves narrative similarity by decomposing narratives into 5W1H facets and comparing them independently before aggregation
- Mechanism: The metric extracts Who, What, When, Where, Why, and How facets using LLMs, then computes similarity for each facet separately (e.g., entity matching for Who, temporal matching for When), and finally combines facet similarities with weighted averaging
- Core assumption: Different facets capture different aspects of narrative meaning, and granular comparison across facets reveals similarity patterns that holistic metrics miss
- Evidence anchors: [abstract] "Unlike existing similarity metrics that only focus on overall lexical/semantic match, FaNS provides a more granular matching along six different facets independently and then combines them"
- Break condition: If facet extraction quality degrades (e.g., LLM failures increase from 20% to 50%+), the mechanism breaks down as facet comparisons become unreliable

### Mechanism 2
- Claim: Assigning different weights to entity-specific versus descriptive facets improves correlation with ground truth
- Mechanism: The metric weights Who/When/Where facets (entity-specific) differently from What/Why/How facets (descriptive) based on their relative importance for narrative similarity, with optimal weighting found empirically (α ≈ 0.2)
- Core assumption: Entity-specific facets capture the structural backbone of narratives while descriptive facets capture semantic content, and their relative importance varies by context
- Evidence anchors: [section 5.3.1] "For α ≥ 0.5, narrative similarity using Level-2 prompts showed a better correlation than Level-3 prompts"
- Break condition: If the optimal α shifts significantly (e.g., from 0.2 to 0.8) across different datasets, the weighting mechanism loses generalizability

### Mechanism 3
- Claim: Using LLMs for facet extraction enables robust extraction of semantic facets that traditional NLP tools cannot capture
- Mechanism: LLMs like ChatGPT and Bard parse narratives to identify facet-relevant entities and relationships, handling ambiguity and context that rule-based or statistical methods miss
- Core assumption: Modern LLMs have sufficient world knowledge and reasoning capability to extract facet-relevant information from unstructured text
- Evidence anchors: [section 4.1] "we prompted LLMs to extract 5W1H facets from narratives rather than asking them to provide similarity scores directly"
- Break condition: If LLM API availability changes or costs increase substantially (>10x), the mechanism becomes impractical to deploy

## Foundational Learning

- Concept: 5W1H framework for event representation
  - Why needed here: Provides the theoretical foundation for decomposing narratives into comparable components
  - Quick check question: Can you list all six facets and give one example of what information each captures in a news article?

- Concept: Semantic versus lexical similarity metrics
  - Why needed here: Understanding why traditional metrics like ROUGE and BERTScore are insufficient for narrative comparison
  - Quick check question: What's the key difference between how ROUGE and BERTScore measure similarity compared to how FaNS measures it?

- Concept: Large Language Model prompting strategies
  - Why needed here: Critical for reliable facet extraction, which is the foundation of the entire metric
  - Quick check question: What's the difference between Level 1, 2, and 3 prompts in the TELeR taxonomy used for facet extraction?

## Architecture Onboarding

- Component map: LLM Facet Extractor → Facet Similarity Calculators (Who, When, Where, What, Why, How) → Weighted Aggregator → Final Similarity Score
- Critical path: LLM extraction → facet matching → weighted combination
- Design tradeoffs: Granularity versus complexity (6 facets vs. single holistic score), LLM dependency versus traditional NLP approaches
- Failure signatures: Low correlation with ground truth (below 0.3), high variance across runs, facet extraction failures from LLMs
- First 3 experiments:
  1. Test facet extraction accuracy by comparing LLM output against manually labeled facets on 10 sample narratives
  2. Validate facet weighting by computing correlation with ground truth across different α values (0.0 to 1.0 in 0.1 increments)
  3. Benchmark against baseline metrics (ROUGE, BERTScore) on the same narrative pairs to quantify improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the FaNS metric perform on narrative domains outside of news articles, such as fiction, scientific literature, or social media posts?
- Basis in paper: [inferred] The paper states that "further investigation is needed to explore the applicability and effectiveness of our approach in other domains" and notes that the experiments were solely conducted within the news domain.
- Why unresolved: The paper does not provide any empirical results or analysis of FaNS performance on non-news narrative domains. The authors acknowledge this as a limitation but do not explore it further.
- What evidence would resolve it: Experimental results comparing FaNS performance on a diverse set of narrative domains against baseline metrics, with analysis of any domain-specific challenges or advantages.

### Open Question 2
- Question: What is the impact of different LLM architectures (e.g., GPT-3, GPT-4, Claude) on the quality and consistency of 5W1H facet extraction for FaNS?
- Basis in paper: [explicit] The paper mentions using ChatGPT and Bard for facet extraction and notes that "the accuracy greatly depends on the quality of facets extracted by the LLMs," but does not systematically compare different LLM models.
- Why unresolved: While the paper acknowledges LLM quality affects FaNS accuracy, it only experiments with two specific LLMs and does not explore how different architectures might impact results.
- What evidence would resolve it: Comparative study of FaNS performance using multiple LLM architectures for facet extraction, with quantitative analysis of consistency and quality differences.

### Open Question 3
- Question: How does the performance of FaNS compare to human annotators in terms of correlation with ground truth narrative similarity labels?
- Basis in paper: [inferred] The paper extensively validates FaNS against automated metrics like BERTScore and ROUGE, but does not include human evaluation or comparison with human judgment.
- Why unresolved: The paper establishes that FaNS correlates better with ground truth labels than traditional metrics, but does not establish whether this correlation is comparable to human judgment or whether humans would agree with FaNS assessments.
- What evidence would resolve it: Human evaluation study where annotators rate narrative similarity pairs and results are compared against both FaNS scores and automated baseline metrics.

### Open Question 4
- Question: What is the optimal weighting scheme for combining individual facet similarities in FaNS, and how sensitive is the metric to these weights?
- Basis in paper: [explicit] The paper experiments with two weighting approaches (fixed 20/80 split and data-driven weights) but notes that "we conducted extensive experiments with the FaNS metric" without exploring the full parameter space.
- Why unresolved: While the paper identifies some weight combinations that work well, it does not perform systematic sensitivity analysis or optimization to determine if better weightings exist.
- What evidence would resolve it: Comprehensive sensitivity analysis of FaNS performance across the full range of weighting parameters, potentially using optimization techniques to identify optimal weight combinations.

### Open Question 5
- Question: How does FaNS handle narratives that are highly similar in content but differ significantly in perspective or framing (e.g., politically biased vs. neutral reporting)?
- Basis in paper: [inferred] The paper uses AllSides data which includes politically biased news articles, but does not specifically analyze how FaNS handles perspective differences versus content similarities.
- Why unresolved: The paper demonstrates that FaNS can capture content similarity but does not investigate whether it appropriately weights perspective differences or whether it might overemphasize factual overlaps while missing important framing distinctions.
- What evidence would resolve it: Analysis of FaNS scores on narrative pairs with similar facts but different perspectives, comparing results to human judgments about narrative similarity that account for both content and framing.

## Limitations
- Heavy reliance on LLM performance for facet extraction, with significant extraction failures observed (especially with Bard)
- Optimal weighting parameter α=0.2 was empirically determined on one dataset and may not generalize across different narrative types or domains
- Study used only news articles from the AllSides dataset, limiting generalizability to other narrative domains

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Core mechanism of facet decomposition improving granularity | High |
| Optimal α=0.2 weighting for the tested dataset | Medium |
| LLM robustness for production deployment | Low |

## Next Checks
1. Test FaNS on cross-domain narrative datasets (scientific abstracts, social media narratives, literary texts) to validate generalizability of the 0.2 weighting parameter
2. Implement an ablation study removing individual facets to quantify their relative contribution and identify the most critical components
3. Develop a fallback mechanism for when LLMs fail to extract facets, such as using traditional NLP tools as a backup to improve robustness