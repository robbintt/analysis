---
ver: rpa2
title: Designing Decision Support Systems Using Counterfactual Prediction Sets
arxiv_id: '2306.03928'
source_url: https://arxiv.org/abs/2306.03928
tags:
- prediction
- experts
- label
- expert
- counterfactual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of designing decision support
  systems that provide human experts with sets of label predictions (prediction sets)
  rather than single label predictions, and ask the experts to predict a label from
  within the set. The core method idea is to use counterfactual prediction sets and
  counterfactual inference to efficiently explore the space of possible prediction
  sets using multi-armed bandit algorithms.
---

# Designing Decision Support Systems Using Counterfactual Prediction Sets

## Quick Facts
- arXiv ID: 2306.03928
- Source URL: https://arxiv.org/abs/2306.03928
- Reference count: 40
- Primary result: Counterfactual algorithms achieve exponential improvement in regret and outperform vanilla bandits in human subject studies

## Executive Summary
This paper addresses the problem of designing decision support systems that provide human experts with prediction sets rather than single label predictions. The core innovation is the use of counterfactual prediction sets and counterfactual inference to efficiently explore the space of possible prediction sets using multi-armed bandit algorithms. The authors introduce counterfactual successive elimination and counterfactual UCB1 algorithms that leverage the nested structure of prediction sets and a counterfactual monotonicity assumption to achieve exponential improvement in regret compared to vanilla bandit algorithms. Human subject study results demonstrate that the counterfactual monotonicity assumption holds and counterfactual algorithms outperform their vanilla counterparts, while a strict implementation of the decision support system that limits experts' agency offers greater performance than a lenient implementation.

## Method Summary
The authors design decision support systems that use conformal prediction to create prediction sets for human experts to choose from. They frame the problem of finding the optimal conformal predictor as a multi-armed bandit problem, where each arm corresponds to a different prediction set size. To improve efficiency, they introduce counterfactual successive elimination and counterfactual UCB1 algorithms that exploit the nested structure of prediction sets and a counterfactual monotonicity assumption. The counterfactual monotonicity assumption states that if an expert succeeds at predicting the ground truth label from a prediction set, they would have also succeeded had the prediction set been smaller but still contained the ground truth label. The authors validate their approach through a large-scale human subject study on the ImageNet16H dataset, where human subjects make predictions on images from prediction sets generated by the system.

## Key Results
- Counterfactual successive elimination achieves exponential improvement in regret compared to vanilla successive elimination by exploiting the nested structure of prediction sets and counterfactual monotonicity assumption.
- Human subject study results show that the counterfactual monotonicity assumption holds and counterfactual algorithms outperform their vanilla counterparts.
- Strict implementation of the decision support system (forcing experts to predict from prediction sets) offers greater performance than a lenient implementation that allows experts to exercise their own agency.
- Counterfactual algorithms find near-optimal prediction sets using the same amount of expert data as a previous algorithm.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Counterfactual successive elimination achieves exponential improvement in regret compared to vanilla successive elimination.
- Mechanism: The nested structure of prediction sets and counterfactual monotonicity assumption allow the algorithm to counterfactually infer rewards for half of the arms in each round, reducing the number of actual pulls needed.
- Core assumption: Counterfactual monotonicity assumption holds - if an expert succeeds (fails) at predicting the ground truth label from a prediction set, they would have also succeeded (failed) had the prediction set been smaller (larger) but still contained the ground truth label.
- Evidence anchors:
  - [abstract] "Our methodology leverages the nested structure of the prediction sets provided by any conformal predictor and a natural counterfactual monotonicity assumption on the experts' predictions over the prediction sets to achieve an exponential improvement in regret in comparison with vanilla bandit algorithms."
  - [section 3] "Under this view, we can formally reason about the predictionsˆYCα made by a human expert under different support systemsCα."
  - [corpus] Weak evidence - no direct mentions of counterfactual monotonicity or regret improvement in related papers.
- Break condition: If the counterfactual monotonicity assumption does not hold, the algorithm would lose its efficiency gains and perform similarly to vanilla successive elimination.

### Mechanism 2
- Claim: Limiting experts' agency leads to greater performance than allowing experts to always exercise their own agency.
- Mechanism: By forcefully asking experts to predict from prediction sets, the system can precisely control the trade-off between how frequently the expert is misled and the difficulty of the classification task they need to solve.
- Core assumption: Experts benefit from reduced choice complexity when forced to predict from smaller sets.
- Evidence anchors:
  - [abstract] "Our results also demonstrate that, for the strict implementation of our system, the experts' predictions seem to satisfy the counterfactual monotonicity assumption and, as a consequence, the conformal predictor found by our counterfactual successive elimination algorithm offers greater performance than that found by the algorithm introduced by Straitouri et al. [16]."
  - [section 2] "Here, following Straitouri et al. [16], we opt for conformal predictors [17, 18] over alternatives since they allow for a precise control of the trade-off between how frequently the expert is misled by the system and the difficulty of the classification task she needs to solve."
  - [corpus] Moderate evidence - related papers mention "Controlling Counterfactual Harm in Decision Support Systems Based on Prediction Sets" but lack detailed mechanism discussion.
- Break condition: If experts consistently perform better when given more choice (lenient implementation), this mechanism would break down.

### Mechanism 3
- Claim: Counterfactual rewards improve the performance of bandit algorithms.
- Mechanism: By counterfactually inferring rewards for arms not pulled, the algorithm can update confidence bounds for multiple arms with fewer actual pulls, leading to faster convergence.
- Core assumption: The counterfactual rewards inferred are accurate representations of what would have happened if those arms were pulled.
- Evidence anchors:
  - [section 4] "In the nextO(log m) rounds, it repeats the same reasoning, it pulls the arm whose parameter value is the median of the remaining (at most)half of the arms whose reward has not yet observed or counterfactually inferred, until it has observed or counterfactually inferred the reward of all arms at least once."
  - [section 4] "Remarkably, our experimental results demonstrate that counterfactual UCB1, i.e., UCB1 using counterfactual rewards, achieves lower expected regret than Algorithm 1."
  - [corpus] Moderate evidence - "Conformal Prediction and Human Decision Making" mentions conformal prediction but doesn't discuss counterfactual rewards.
- Break condition: If the counterfactual inferences are systematically incorrect, the algorithm would make poor decisions based on false confidence bounds.

## Foundational Learning

- Concept: Conformal prediction and its PAC coverage guarantees.
  - Why needed here: Understanding how prediction sets are constructed and their coverage properties is crucial for designing the decision support system and interpreting results.
  - Quick check question: How does the parameter α control the trade-off between the probability that the ground truth label is not in the prediction set and the size of the prediction set?

- Concept: Structural causal models (SCMs) and counterfactual inference.
  - Why needed here: SCMs provide the formal framework for reasoning about experts' predictions under different prediction sets and deriving the counterfactual monotonicity assumption.
  - Quick check question: How does the counterfactual monotonicity assumption relate to the nested structure of prediction sets?

- Concept: Multi-armed bandit algorithms and regret analysis.
  - Why needed here: The problem of finding the optimal conformal predictor is framed as a bandit problem, and understanding regret guarantees is essential for evaluating algorithm performance.
  - Quick check question: How does counterfactual successive elimination achieve exponential improvement in regret compared to vanilla successive elimination?

## Architecture Onboarding

- Component map: Conformal predictor -> Decision support system -> Bandit algorithm -> Counterfactual inference engine

- Critical path:
  1. Pre-train classifier on available data.
  2. Construct calibration set and conformal predictor.
  3. Initialize bandit algorithm with conformal predictors as arms.
  4. At each round, show prediction set from selected arm to expert, collect prediction and ground truth.
  5. Update bandit algorithm with observed reward and counterfactually inferred rewards.
  6. Repeat until convergence or maximum rounds reached.

- Design tradeoffs:
  - Strict vs lenient implementation: Strict implementation forces experts to predict from sets, enabling precise control but potentially limiting expert knowledge. Lenient implementation allows more flexibility but may lead to worse performance.
  - Counterfactual vs observed rewards: Using counterfactual rewards can speed up convergence but relies on the validity of the counterfactual monotonicity assumption.

- Failure signatures:
  - If experts consistently perform worse under the strict implementation, it may indicate that the counterfactual monotonicity assumption doesn't hold or that experts benefit from more flexibility.
  - If the bandit algorithm doesn't converge or finds suboptimal conformal predictors, it may indicate issues with the counterfactual inference engine or the reward structure.

- First 3 experiments:
  1. Validate the counterfactual monotonicity assumption: Measure experts' success rates on different prediction set sizes and check if smaller sets lead to higher success rates when they contain the ground truth label.
  2. Compare counterfactual vs vanilla bandit algorithms: Run both algorithms on the same data and measure their regret to confirm the exponential improvement claim.
  3. Test strict vs lenient implementation: Run the system with both implementations and measure experts' success rates to confirm the superiority of the strict implementation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do counterfactual UCB1 algorithms perform compared to counterfactual successive elimination algorithms in terms of regret and computational efficiency?
- Basis in paper: [explicit] The paper mentions that counterfactual UCB1 achieves lower expected regret than counterfactual successive elimination but does not provide formal regret guarantees for counterfactual UCB1.
- Why unresolved: The paper only provides empirical evidence for the superiority of counterfactual UCB1 but lacks theoretical analysis.
- What evidence would resolve it: Formal regret guarantees and computational complexity analysis for counterfactual UCB1 compared to counterfactual successive elimination.

### Open Question 2
- Question: How does the performance of decision support systems based on prediction sets change when using biased classifiers?
- Basis in paper: [inferred] The paper mentions that it would be important to investigate the impact of biased classifiers on decision support systems but does not explore this aspect.
- Why unresolved: The paper focuses on decision support systems using conformal predictors and does not consider the potential effects of classifier bias.
- What evidence would resolve it: Experimental results comparing the performance of decision support systems using biased and unbiased classifiers.

### Open Question 3
- Question: How does the counterfactual monotonicity assumption hold in real-world domains with domain experts?
- Basis in paper: [explicit] The paper suggests that the counterfactual monotonicity assumption may hold in practice based on empirical results but cannot validate it using observational nor interventional experiments.
- Why unresolved: The paper relies on a human subject study to provide evidence for the assumption but acknowledges that it cannot be directly validated.
- What evidence would resolve it: Experimental results from real-world domains with domain experts to verify the counterfactual monotonicity assumption.

## Limitations

- The counterfactual monotonicity assumption, while empirically validated, is not theoretically proven across all possible expert behaviors and may not generalize to other domains or expert populations.
- The exponential regret improvement claim is mathematically sound under the assumed conditions, but real-world performance may be affected by noise in human predictions and potential violations of the nested structure assumption.
- The superiority of the strict implementation over the lenient implementation may not hold for all types of classification tasks or expert populations, as some experts may benefit from more flexibility.

## Confidence

- Counterfactual monotonicity assumption validity: Medium
- Exponential regret improvement: High (theoretical) / Medium (empirical)
- Strict implementation superiority: Medium
- Algorithm convergence and near-optimality: High

## Next Checks

1. Test counterfactual monotonicity across diverse datasets and expert populations to assess generalizability
2. Implement robustness checks for the counterfactual inference engine against noisy expert predictions
3. Conduct ablation studies to quantify the impact of strict vs lenient implementation on different types of classification tasks