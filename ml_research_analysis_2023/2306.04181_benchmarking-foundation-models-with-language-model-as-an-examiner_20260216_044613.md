---
ver: rpa2
title: Benchmarking Foundation Models with Language-Model-as-an-Examiner
arxiv_id: '2306.04181'
source_url: https://arxiv.org/abs/2306.04181
tags:
- question
- answer
- questions
- evaluation
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a novel benchmarking framework, Language-Model-as-an-Examiner
  (LMExam), where a language model acts as an examiner that generates questions and
  evaluates responses in a reference-free manner. To ensure comprehensive and equitable
  evaluation, the framework employs three strategies: increasing knowledge breadth
  and depth through multi-round question generation across diverse domains, using
  both scoring and ranking measurements for reliable evaluation, and introducing a
  decentralized peer-examination method to mitigate bias from a single examiner.'
---

# Benchmarking Foundation Models with Language-Model-as-an-Examiner

## Quick Facts
- arXiv ID: 2306.04181
- Source URL: https://arxiv.org/abs/2306.04181
- Reference count: 40
- Primary result: Introduces LMExam, a framework using language models as examiners for fairer, reference-free evaluation of foundation models

## Executive Summary
This paper introduces LMExam, a novel benchmarking framework that uses language models as examiners to generate questions and evaluate responses in a reference-free manner. The framework addresses key challenges in foundation model evaluation including testing leakage, evaluation automation, and fairness concerns. Through experiments with 8 foundation models and 4 peer examiners, the authors demonstrate that LMExam achieves high correlation with human annotations while providing more equitable assessment through decentralized peer-examination.

## Method Summary
LMExam employs a language model as an examiner that generates diverse questions across multiple domains and evaluates responses without requiring ground truth answers. The framework uses three key strategies: multi-round question generation to assess depth of understanding, both scoring and ranking measurements for reliable evaluation, and a decentralized peer-examination method where multiple LMs evaluate each other to mitigate individual bias. The approach eliminates testing leakage by drawing questions from the examiner's knowledge rather than fixed datasets, and provides comprehensive assessment through follow-up questions that probe understanding beyond surface-level responses.

## Key Results
- LMExam achieves Spearman's Ï = 0.84 correlation with human annotations on 100 sampled responses
- Peer-examination leads to more diverse questions and balanced evaluation outcomes compared to centralized evaluation
- Multi-round follow-up questions reveal performance drops in most models, suggesting surface-level understanding in single-round assessments
- The framework successfully benchmarks 8 foundation models across diverse domains with improved fairness metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using a language model as both question generator and evaluator eliminates testing leakage and provides reference-free evaluation
- Mechanism: The LM examiner draws questions from its own knowledge rather than fixed datasets, ensuring questions are always novel and not subject to memorization
- Core assumption: The LM examiner's knowledge base is sufficiently comprehensive and unbiased to generate valid questions and make reliable judgments
- Evidence anchors: [abstract] "formulates questions based on its knowledge and evaluates responses in a reference-free manner"; [section 3.1] "generates diversifying and high-quality questions across various domains"
- Break condition: If the LM examiner lacks knowledge in critical domains or develops systematic biases, the evaluation becomes unreliable or unfair

### Mechanism 2
- Claim: Multi-round follow-up questions enable deeper assessment of model understanding
- Mechanism: After the initial question, the examiner generates follow-up questions based on the model's response, probing the depth of understanding beyond surface-level recall
- Core assumption: Follow-up questions can be generated that meaningfully extend the initial topic and that responses to these questions indicate genuine comprehension
- Evidence anchors: [section 3.1] "evaluation procedure involving multiple rounds of follow-up inquiries"; [section 4.2] "all examinee models exhibit a notable decrease in performance in the second round"
- Break condition: If follow-up questions deviate significantly from the original topic or if models can provide correct answers through superficial reasoning without true understanding

### Mechanism 3
- Claim: Decentralized peer-examination reduces individual evaluator bias and increases question diversity
- Mechanism: Multiple LMs serve as examiners, each generating questions and evaluating others. Results are combined through voting, balancing individual biases and expanding the knowledge domains covered
- Core assumption: Different LMs have complementary strengths and biases that average out when combined, rather than reinforcing each other's weaknesses
- Evidence anchors: [abstract] "We additionally propose a decentralized Peer-examination method to address the biases in a single examiner"; [section 4.4] "Combining them within a peer-examination framework can balance their individual biases"
- Break condition: If peer examiners share common biases or if voting mechanisms fail to properly aggregate diverse evaluations

## Foundational Learning

- Concept: Reference-free evaluation in NLP
  - Why needed here: Traditional metrics require reference answers, but open-ended QA has many valid responses
  - Quick check question: What is the fundamental limitation of using exact match metrics for open-ended question answering evaluation?

- Concept: Bloom's taxonomy of cognitive levels
  - Why needed here: The framework explicitly categorizes questions by cognitive complexity to ensure comprehensive assessment
  - Quick check question: How does the framework ensure coverage of higher-order cognitive skills beyond simple recall?

- Concept: Bias detection and mitigation in AI systems
  - Why needed here: The peer-examination approach specifically addresses known issues with individual evaluator bias in LLM-based assessment
  - Quick check question: What experimental evidence demonstrates that centralized evaluation introduces systematic bias?

## Architecture Onboarding

- Component map: Question Generation Module -> Multi-round Extension -> Evaluation Engine -> Peer-Examination Coordinator -> Human Annotation Interface
- Critical path: 1) Select examiner LM(s) and define domain taxonomy; 2) Generate initial question set; 3) For each foundation model, generate responses; 4) Apply multi-round questioning; 5) Centralized evaluation; 6) Peer-examination; 7) Aggregate and analyze results
- Design tradeoffs: Single vs. multiple examiners (simplicity vs. bias); Depth vs. breadth (coverage vs. specificity); Scoring vs. ranking (absolute vs. relative measures)
- Failure signatures: Low correlation with human annotations indicates misaligned metrics; significant performance drops in multi-round suggest surface-level understanding; consistent ranking across examiners suggests evaluator bias
- First 3 experiments: 1) Baseline correlation test comparing GPT-4 scores against human annotations; 2) Single-round benchmarking of all 8 models with GPT-4 as examiner; 3) Peer-examination validation with 4 capable models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does peer-examination perform when evaluating models with significantly different capabilities?
- Basis in paper: The paper mentions using peer-examination with models like ChatGPT, Claude, Bard, and Vicuna-13B, but doesn't extensively explore performance differences when examiners have vastly different capabilities
- Why unresolved: Experiments focused on relatively similar models, and the paper acknowledges that finding models capable of reliable peer-examination is challenging
- What evidence would resolve it: Experiments comparing evaluation consistency and bias when using examiners with different capability levels on the same set of responses

### Open Question 2
- Question: What is the optimal number of rounds for multi-round follow-up questions before topic deviation becomes problematic?
- Basis in paper: The paper limits follow-up questions to k=2 rounds due to API costs, but acknowledges topic deviation as a concern for longer sessions
- Why unresolved: The paper didn't explore how question relevance and answer quality change with more follow-up rounds
- What evidence would resolve it: Systematic experiments varying k and measuring topic coherence, answer accuracy, and examiner understanding across rounds

### Open Question 3
- Question: How does the centralized examiner's bias manifest across different knowledge domains and question types?
- Basis in paper: The paper discusses potential biases in centralized examination, particularly linguistic style preferences, but doesn't provide domain-specific bias analysis
- Why unresolved: The bias analysis focused on general linguistic preferences rather than domain-specific evaluation tendencies
- What evidence would resolve it: Detailed analysis of GPT-4's evaluation patterns across different domains, showing where it might be more lenient or strict based on subject matter

## Limitations
- The framework relies on the examiner's internal knowledge, which may contain undocumented biases or gaps that propagate to evaluation outcomes
- Multi-round questioning shows performance decreases but doesn't definitively prove deeper assessment versus compounding error propagation
- Peer-examination improves fairness but introduces new complexity in aggregating diverse judgments and potential examiner collusion effects

## Confidence

**Confidence labels:**
- **High confidence**: The peer-examination methodology and its ability to reduce bias (supported by quantitative comparisons showing balanced rankings)
- **Medium confidence**: The multi-round questioning mechanism's ability to assess deeper understanding (supported by observed performance drops in round 2)
- **Low confidence**: The claim of eliminating testing leakage entirely (while questions are generated on-the-fly, the examiner's pre-existing knowledge may still introduce implicit biases)

## Next Checks

1. Conduct blind human evaluation on 500 responses across all models to verify the claimed correlation holds at scale and isn't inflated by cherry-picking
2. Test examiner consistency by having the same model serve as examiner multiple times with different random seeds to measure variance in scoring
3. Implement controlled experiments where foundation models are given access to the examiner's knowledge base to quantify the actual testing leakage versus claimed elimination