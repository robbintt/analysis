---
ver: rpa2
title: Human Transcription Quality Improvement
arxiv_id: '2309.14372'
source_url: https://arxiv.org/abs/2309.14372
tags:
- transcription
- speech
- data
- error
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of collecting high-quality human
  transcriptions for training automatic speech recognition (ASR) systems, as existing
  industry pipelines are expensive and crowdsourced transcription quality is often
  low. The authors propose a new ML-in-the-loop data collection mechanism that combines
  confidence estimation-based reprocessing during the labeling stage with automatic
  word error correction at the post-labeling stage.
---

# Human Transcription Quality Improvement

## Quick Facts
- arXiv ID: 2309.14372
- Source URL: https://arxiv.org/abs/2309.14372
- Reference count: 0
- Primary result: Proposed ML-in-the-loop approach reduces TWER by 50% from 10.91% to 4.94%

## Executive Summary
This paper addresses the challenge of collecting high-quality human transcriptions for training automatic speech recognition (ASR) systems, as existing industry pipelines are expensive and crowdsourced transcription quality is often low. The authors propose a new ML-in-the-loop data collection mechanism that combines confidence estimation-based reprocessing during the labeling stage with automatic word error correction at the post-labeling stage. They collect and release LibriCrowd, a large-scale crowdsourced dataset of 100 hours of English speech transcriptions. Experiments show their approach reduces Transcription Word Error Rate (TWER) by over 50%, from 10.91% to 4.94%. Further analysis reveals a strong correlation between TWER and the WER of downstream ASR models trained on the transcriptions. The improved transcription quality provides over 10% relative WER reduction for ASR models. The dataset and code are released to benefit the research community.

## Method Summary
The authors propose a two-stage ML-in-the-loop approach for improving human transcription quality. First, a Confidence Estimation Model (CEM) identifies low-quality transcriptions during the labeling stage by analyzing audio-text alignment embeddings and fine-tuned ELECTRA error detection. Second, an Error Correction Model (ECM) applies automatic word error correction at the post-labeling stage using a weighted voting scheme that combines word frequency with individual confidence scores. The method was evaluated on LibriCrowd, a 100-hour crowdsourced English speech dataset, where it reduced TWER by 50% compared to baseline human transcriptions.

## Key Results
- TWER reduced by 50% from 10.91% to 4.94% on LibriCrowd test sets
- Strong correlation found between TWER and downstream ASR WER
- Over 10% relative WER reduction achieved for ASR models trained on improved transcriptions
- Proposed ECM outperforms five baseline methods by over 30%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Confidence Estimation Models (CEMs) improve transcription quality by identifying low-confidence words for relabeling.
- Mechanism: CEMs use audio-text alignment embeddings combined with a fine-tuned ELECTRA error detection model to assign confidence scores to each word. Words with low scores trigger relabeling.
- Core assumption: Word-level confidence scores from alignment and semantic consistency are reliable indicators of transcription errors.
- Evidence anchors:
  - [abstract] "confidence estimation based reprocessing at labeling stage"
  - [section] "Compared to the ELECTRA baseline, having the alignment embedding can improve F1 score by 9% on LibriCrowd."
- Break condition: If alignment scores fail to capture true audio-text mismatches, confidence scores will not reliably identify errors.

### Mechanism 2
- Claim: Error Correction Models (ECMs) reduce Transcription WER by combining word frequency voting with confidence scores.
- Mechanism: ECMs align multiple transcriptions and use a weighted vote where the weight combines word frequency and individual confidence scores. The trade-off parameter α balances these factors.
- Core assumption: Multiple transcriptions of the same audio contain complementary information, and combining them with confidence weights yields better output than any single transcription.
- Evidence anchors:
  - [abstract] "automatic word error correction at post-labeling stage"
  - [section] "Our model outperforms the baselines and RescoreBERT by over 30%."
- Break condition: If transcribers make systematic errors on the same words, voting will amplify rather than correct errors.

### Mechanism 3
- Claim: Transcription quality directly impacts downstream ASR model performance through a strong correlation.
- Mechanism: Lower Transcription WER in training data leads to lower Word Error Rate in ASR models trained on that data. The correlation is quantified by controlled experiments mixing noisy and clean transcriptions.
- Core assumption: ASR models learn primarily from correctly transcribed examples, and errors in training transcriptions propagate to model predictions.
- Evidence anchors:
  - [abstract] "Experiment shows the Transcription WER is reduced by over 50%... We further investigate the impact of transcription error on ASR model performance and found a strong correlation."
  - [section] "We find a strong correlation between TWER and WER of downstream ASR models."
- Break condition: If ASR models develop robustness to label noise through regularization or architecture, the correlation may weaken.

## Foundational Learning

- Concept: Word-level confidence estimation from audio-text alignment
  - Why needed here: Provides a surrogate for ASR confidence scores that humans lack, enabling automated quality assessment
  - Quick check question: What two components are combined in the confidence estimation model for human transcriptions?

- Concept: Error correction through weighted voting across multiple transcriptions
  - Why needed here: Aggregating information from multiple annotators reduces individual errors while maintaining data diversity
  - Quick check question: What parameter controls the trade-off between word frequency and confidence score in the error correction algorithm?

- Concept: Correlation between training data quality and model performance
  - Why needed here: Quantifies the value of quality improvement efforts and guides resource allocation for data collection
  - Quick check question: What experimental method was used to establish the correlation between transcription quality and ASR performance?

## Architecture Onboarding

- Component map: Audio collection → MTurk transcription → Confidence Estimation → Relabeling (if needed) → Error Correction → Final dataset → ASR training
- Critical path: MTurk transcription → CEM alignment and scoring → ECM aggregation → ASR training and evaluation
- Design tradeoffs: More workers per utterance increases quality but raises cost; higher α in ECM favors frequency over confidence
- Failure signatures: High deletion rates indicate UI issues; systematic substitution errors suggest model bias; weak correlation suggests ASR robustness
- First 3 experiments:
  1. Run CEM on test data and measure precision/recall of error detection
  2. Compare ECM with different α values on validation set
  3. Train ASR model on data with controlled TWER levels and measure WER correlation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed transcription quality improvement method scale with increasing amounts of training data for the confidence estimation and error correction models?
- Basis in paper: [explicit] The authors mention that the CEM and ECM are trained on 10 hours of speech (train-other-10h) and evaluate their performance on the entire 100-hour LibriCrowd dataset.
- Why unresolved: The paper does not investigate how the performance of the quality improvement method changes when trained on larger amounts of data or when applied to datasets with different characteristics (e.g., different languages, domains, or noise levels).
- What evidence would resolve it: Experiments training the CEM and ECM on progressively larger amounts of data and evaluating their performance on diverse datasets would provide insights into the scalability of the proposed method.

### Open Question 2
- Question: Can the proposed transcription quality improvement method be extended to handle non-English languages or speech from diverse speakers and accents?
- Basis in paper: [inferred] The paper focuses on English speech transcribed by a diverse group of 4433 human transcribers, but it does not explore the generalizability of the method to other languages or speaker groups.
- Why unresolved: The authors do not investigate the performance of the CEM and ECM on non-English speech or speech from speakers with different accents or dialects.
- What evidence would resolve it: Evaluating the proposed method on speech corpora in other languages and from speakers with diverse backgrounds would provide insights into its generalizability and potential limitations.

### Open Question 3
- Question: How does the proposed transcription quality improvement method compare to other state-of-the-art approaches for speech transcription error correction?
- Basis in paper: [explicit] The authors compare their proposed ECM to five baseline methods (Random, Longest, Best worker, Oracle, and RescoreBERT) and demonstrate superior performance.
- Why unresolved: The paper does not compare the proposed method to other recent approaches for speech transcription error correction, such as those based on large language models or advanced neural architectures.
- What evidence would resolve it: Conducting experiments comparing the proposed method to other state-of-the-art approaches on the same datasets would provide a more comprehensive understanding of its relative performance and potential areas for improvement.

## Limitations

- Performance may not generalize beyond LibriVox domain due to reliance on alignment scores
- Cost increases significantly with requirement for multiple diverse transcriptions per utterance
- Correlation between TWER and ASR WER only tested on limited ASR architectures (Wav2Vec2 and WavLM)
- Effectiveness on non-English languages and diverse speech domains remains unproven

## Confidence

- High confidence: TWER reduction from 10.91% to 4.94% (measured on controlled test sets)
- Medium confidence: Correlation between TWER and downstream ASR WER (correlation established but mechanism not fully explored)
- Medium confidence: Relative WER reduction of >10% for ASR models (based on limited ASR architecture experiments)

## Next Checks

1. Cross-domain validation: Test the CEM and ECM models on speech from different domains (news, conversational speech, non-LibriVox audiobooks) to assess generalization beyond the training corpus.

2. Single-transcription baseline: Evaluate ASR performance when using only the best single transcription per utterance (without error correction) to quantify the actual contribution of the ECM component.

3. Cost-effectiveness analysis: Calculate the total cost per hour of transcription including worker payments, CEM/ECM training, and relabeling, then compare against industry transcription services to determine economic viability.