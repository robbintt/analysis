---
ver: rpa2
title: 'Impressions: Understanding Visual Semiotics and Aesthetic Impact'
arxiv_id: '2310.17887'
source_url: https://arxiv.org/abs/2310.17887
tags:
- image
- visual
- aesthetic
- impressions
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Impressions, a novel dataset for training
  multimodal models to understand visual semiotics and aesthetic impact in images.
  The dataset contains 1,440 images with 4,320 unique annotations exploring image
  description, impressions, and aesthetic evaluation.
---

# Impressions: Understanding Visual Semiotics and Aesthetic Impact

## Quick Facts
- arXiv ID: 2310.17887
- Source URL: https://arxiv.org/abs/2310.17887
- Reference count: 40
- Key outcome: Impressions dataset improves multimodal models' ability to generate human-like impressions by training on rich, connotation-rich annotations

## Executive Summary
This paper introduces Impressions, a novel dataset for training multimodal models to understand visual semiotics and aesthetic impact in images. The dataset contains 1,440 images with 4,320 unique annotations exploring image description, impressions, and aesthetic evaluation. Each annotation consists of three free-form responses grounded in image analysis techniques from visual arts and media studies. The authors show that existing state-of-the-art image captioning and conditional generation models struggle to simulate plausible human responses to images. However, fine-tuning and few-shot adaptation on the Impressions dataset significantly improves their ability to model impressions and aesthetic evaluations. In human evaluation tasks, fine-tuned models were preferred over 80% of the time across all caption categories. The dataset also enables investigating variations in human perceptions by training persona-specific models.

## Method Summary
The paper collects 1,440 images from photojournalistic sources and semantically related images from Google search, then annotates them with three types of responses (description, perception, aesthetic evaluation) for each image. The authors fine-tune or few-shot adapt four models (GIT, BLIP, OpenFlamingo, LLaVA) on this dataset, using separate training for each annotation type or combined instruction tuning. They evaluate the models using both automatic metrics (BLEU, METEOR, ROUGE) and human evaluation tasks where annotators compare generations from fine-tuned vs. base models.

## Key Results
- Fine-tuned models were preferred over 80% of the time in human evaluation across all caption categories
- Persona-specific models show distinct generation patterns with statistical significance (p=0.026 for introverts vs extroverts, p=0.012 for art experience differences)
- Existing state-of-the-art models struggle to simulate plausible human responses to images without Impressions dataset training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Impressions dataset improves multimodal models' ability to generate human-like impressions by training on rich, connotation-rich annotations.
- Mechanism: Fine-tuning on annotations that include image description, viewer impressions, and aesthetic evaluation grounds the model in the link between visual features and perceived meaning.
- Core assumption: Models can learn to associate specific aesthetic elements with the impressions they inspire when trained on human-generated examples that explicitly make this connection.
- Evidence anchors: Abstract states dataset "significantly improves their ability to model impressions and aesthetic evaluations"; section 4 shows architectures attain this ability through fine-tuning.

### Mechanism 2
- Claim: Persona-specific fine-tuning reveals that models can generate impressions reflecting the traits of different annotator groups.
- Mechanism: By fine-tuning on annotations from specific personality or demographic groups, the model learns to generate text that reflects typical response patterns of those groups.
- Core assumption: Variation in annotations across personality and demographic groups is meaningful and reflects real differences in perception that can be modeled.
- Evidence anchors: Section 6 finds distinct distributions between extrovert/introvert models (p=0.026) and art experience/no experience models (p=0.012).

### Mechanism 3
- Claim: Targeted prompts modeled after visual arts techniques elicit complex commentary on perlocution and aesthetic elements.
- Mechanism: Annotation task designed to go beyond literal description by asking annotators to infer meaning, discuss impressions, and connect aesthetic elements to those impressions.
- Core assumption: Prompts asking about impressions and aesthetic elements will produce more varied and meaningful annotations than prompts asking only for literal description.
- Evidence anchors: Section 3.2 describes scaffolding discussion for individuals with little to no experience in media or visual arts; section 4 shows annotators preferred outputs from fine-tuned models 76% of the time.

## Foundational Learning

- Concept: Difference between denotation and connotation in visual media
  - Why needed here: Paper argues existing image captioning datasets focus on denotation (literal description) while Impressions focuses on connotation (inferred meaning and impressions)
  - Quick check question: If an image shows a person smiling in a park, what would be an example of a denotational caption vs a connotational caption?

- Concept: Perlocutionary force in communication
  - Why needed here: Paper uses concept of perlocutionary force (effect an utterance has on the listener) to describe impact of images on viewers
  - Quick check question: How is perlocutionary force different from the literal meaning of an image or text?

- Concept: Aesthetic elements and their communicative role
  - Why needed here: Paper argues aesthetic elements are not just about beauty but about communication
  - Quick check question: Give an example of how a photographer might use lighting to change the perceived meaning of a subject

## Architecture Onboarding

- Component map: Dataset (1,440 images with 4,320 annotations) -> Annotation task structure (description, impression, aesthetic evaluation) -> Models being fine-tuned (GIT, BLIP, OpenFlamingo, LLaVA)
- Critical path: Collect images → Annotate images with impressions → Fine-tune/few-shot adapt models on annotations → Evaluate model outputs against human preferences
- Design tradeoffs: Dataset is relatively small (1,440 images) compared to standard image captioning datasets, but contains richer annotations; annotation task is more complex and time-consuming than simple description, but yields more meaningful data
- Failure signatures: Models generate generic or overly literal captions despite fine-tuning; models generate captions that don't match image content; persona-specific models don't show distinct behaviors
- First 3 experiments:
  1. Fine-tune a standard image captioning model (e.g., BLIP) on the Impressions dataset and evaluate on a held-out set using both automatic metrics and human evaluation
  2. Perform few-shot adaptation of a VQA model (e.g., OpenFlamingo) on the Impressions dataset and compare performance to zero-shot
  3. Fine-tune persona-specific models (e.g., introvert vs extrovert) and analyze differences in their generated captions using both quantitative metrics and qualitative analysis

## Open Questions the Paper Calls Out

1. How can future synthetic visual instruction-tuning datasets be improved to better capture connotation grounded in style? (Basis: Explicit recommendation to incorporate aesthetic features beyond bounding boxes)

2. How can the Impressions dataset be used to investigate potential bias resolution? (Basis: Authors acknowledge risk of introducing harmful, biased ideas through inference in annotation task)

3. How can persona-specific generation be further explored to understand variation in human perceptions of images? (Basis: Authors note more distinctive behaviors may arise with larger training/evaluation sets)

## Limitations

- Dataset size (1,440 images) represents a significant limitation for training robust multimodal models
- Free-form annotation nature introduces variability in how different annotators interpret and respond to prompts
- Practical significance of persona-specific differences in visual perception remains unclear

## Confidence

- High Confidence: Core finding that fine-tuning improves model performance on impression and aesthetic evaluation tasks is well-supported by automatic metrics and human evaluation
- Medium Confidence: Identification of distinct persona-specific generation patterns is supported by statistical tests but requires further validation of practical implications
- Low Confidence: Claim that existing state-of-the-art models "struggle" with simulating human responses is primarily based on comparison to Impressions dataset outputs

## Next Checks

1. Test whether observed improvements scale with dataset size by evaluating models fine-tuned on progressively larger subsets of the Impressions dataset

2. Evaluate fine-tuned models on established image captioning benchmarks (COCO, Flickr30k) to assess potential trade-offs between impression modeling and traditional captioning performance

3. Calculate and report inter-annotator agreement scores for each annotation type, and assess whether persona labels significantly predict annotation similarity beyond demographic factors