---
ver: rpa2
title: 'A Meta-Evaluation of C/W/L/A Metrics: System Ranking Similarity, System Ranking
  Consistency and Discriminative Power'
arxiv_id: '2307.02936'
source_url: https://arxiv.org/abs/2307.02936
tags:
- system
- ranking
- metrics
- power
- discriminative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the statistical stability of C/W/L/A metrics
  from three perspectives: system ranking similarity, system ranking consistency and
  discriminative power. We combine different aggregation functions with the browsing
  model of Precision, DCG, RBP, INST, AP and ERR and conduct experiments on two offline
  test collections.'
---

# A Meta-Evaluation of C/W/L/A Metrics: System Ranking Similarity, System Ranking Consistency and Discriminative Power

## Quick Facts
- arXiv ID: 2307.02936
- Source URL: https://arxiv.org/abs/2307.02936
- Reference count: 40
- Key outcome: ERG aggregation shows outstanding performance in system ranking consistency and discriminative power across C/W/L/A metrics

## Executive Summary
This study systematically evaluates the statistical stability of C/W/L/A metrics by examining system ranking similarity, consistency, and discriminative power. The authors investigate how different aggregation functions (ERG, ETG, avg, max, fin, PE, ERR) perform when combined with various browsing models (Precision, DCG, RBP, INST, AP, and ERR). Their experiments on two offline test collections reveal that ERG consistently outperforms other aggregation functions, particularly in system ranking consistency and discriminative power, while also showing that replacing ERR's canonical aggregation with ERG can strengthen its discriminative power without significantly altering system rankings.

## Method Summary
The study combines six different aggregation functions with seven browsing models and evaluates their performance using three metrics: system ranking similarity (Kendall's tau), system ranking consistency (mean tau across topic subsets), and discriminative power (ASL curves). The experiments use two test collections (NTCIR-15 WWW-3 and TREC 2019 Deep Learning) with randomized Tukey HSD significance testing using 1000 topic subset splits and 2000 trials for significance testing.

## Key Results
- ERG aggregation consistently outperforms other aggregations in system ranking consistency and discriminative power
- Maximum relevance aggregation (Amax) shows insufficient performance due to its inability to distinguish between runs with different lower-ranked documents
- ERR with ERG aggregation maintains similar system rankings while improving discriminative power compared to ERR's canonical aggregation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ERG aggregation consistently improves both system ranking consistency and discriminative power
- Mechanism: ERG accumulates user gain at each position weighted by continuation probability and reciprocal of total inspection probability, incorporating both relevance and user behavior patterns
- Core assumption: Continuation probability and gain at each position are reliable indicators of user satisfaction
- Evidence anchors: Abstract finding on ERG's outstanding performance; explanation that ERG uses information from all relevance scores and inspection probabilities
- Break condition: If continuation probabilities don't accurately model user behavior or relevance scores are unreliable

### Mechanism 2
- Claim: Maximum relevance aggregation performs poorly because it only uses the highest relevance score
- Mechanism: Amax creates metric scores that plateau once maximum relevance is found, making it impossible to distinguish between runs with different lower-ranked documents
- Core assumption: User satisfaction depends primarily on single best result encountered
- Evidence anchors: Abstract finding on insufficient performance of maximum relevance; explanation of why Amax scores become similar and impair discrimination
- Break condition: If user satisfaction is truly dominated by single best result regardless of others

### Mechanism 3
- Claim: ERR with ERG aggregation maintains similar rankings while improving discriminative power
- Mechanism: ERR's canonical aggregation assumes user dissatisfaction increases with position regardless of document quality; ERG preserves browsing model while adding discriminative information from all positions
- Core assumption: ERR browsing model is appropriate but aggregation function can be improved
- Evidence anchors: Abstract finding on ERR with ERG strengthening discriminative power; section explanation of replacing A_ERR with A_ERG
- Break condition: If ERR browsing model itself is flawed

## Foundational Learning

- Concept: C/W/L/A framework (Continuation, Weight, Last probability, Aggregation)
  - Why needed here: Understanding how user browsing behavior is modeled and how different aggregations combine with these models is fundamental to interpreting results
  - Quick check question: What are the three components of the C/W/L framework and how are they related mathematically?

- Concept: System ranking consistency and discriminative power
  - Why needed here: These are the two statistical stability metrics used to evaluate aggregation functions
  - Quick check question: How does the randomized Tukey HSD test measure system ranking consistency across topic sets?

- Concept: Significance testing and p-values
  - Why needed here: The study uses ASL curves and significance testing to evaluate discriminative power
  - Quick check question: What does it mean when a metric has an ASL curve closer to the origin in terms of discriminative power?

## Architecture Onboarding

- Component map: Data layer (two test collections) -> Metric computation layer (C/W/L/A metrics with aggregations) -> Evaluation layer (ranking similarity, consistency, discriminative power) -> Statistical testing layer (randomized Tukey HSD)

- Critical path: Load test collection data → Compute metric scores for all combinations → Calculate system ranking similarity (Kendall's tau) → Perform consistency analysis with B=1000 random splits → Generate ASL curves → Apply significance testing

- Design tradeoffs: Using randomized Tukey HSD vs. parametric tests (distribution-free but computationally expensive); choosing B=1000 splits for consistency vs. computational cost; linear vs. exponential relevance mapping

- Failure signatures: Zero variance in metric scores (aggregation issues); inconsistent rankings across topic splits (low consistency); flat ASL curves (poor discriminative power)

- First 3 experiments: 1) Verify ERG and ETG produce identical rankings for metrics with constant C(i); 2) Compare system ranking consistency of ERG vs. max relevance; 3) Generate ASL curves for ERR with both ERR and ERG aggregations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do aggregation functions perform when combined with browsing models not examined in this study?
- Basis in paper: The paper only examines specific browsing models, though the C/W/L/A framework allows for flexible combinations
- Why unresolved: The paper focuses on a specific set of browsing models without exploring other potential combinations
- What evidence would resolve it: Experiments combining aggregation functions with a wider range of browsing models

### Open Question 2
- Question: How does the choice of aggregation function affect stability across different types of search tasks?
- Basis in paper: The paper examines stability across two collections but doesn't investigate task type effects
- Why unresolved: The paper doesn't analyze the effect of task type on aggregation function stability
- What evidence would resolve it: Comparative experiments across diverse search task types

### Open Question 3
- Question: Can stability be further improved by incorporating additional factors like user interaction patterns or query difficulty?
- Basis in paper: The paper focuses on statistical stability without exploring additional influencing factors
- Why unresolved: The paper doesn't investigate the role of user interaction patterns or query difficulty in stability
- What evidence would resolve it: Experiments incorporating user interaction data and query difficulty measures

## Limitations
- Reliance on two specific test collections may limit generalizability to other domains
- C/W/L/A framework assumes specific user browsing behaviors that may not capture all real-world scenarios
- Significance testing approach may have limited statistical power with available topics and runs

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Experimental design and methodology are sound | High Confidence |
| ERG's superior performance within tested frameworks | Medium Confidence |
| ERR with ERG aggregation improves discriminative power | Medium Confidence |

## Next Checks

1. **Replication across collections**: Apply methodology to additional test collections to verify consistency of ERG findings
2. **Behavioral validation**: Conduct user studies to verify C/W/L/A framework assumptions about browsing behavior
3. **Statistical power analysis**: Perform additional significance testing with varying splits and trials to determine minimum requirements for reliable consistency measurements