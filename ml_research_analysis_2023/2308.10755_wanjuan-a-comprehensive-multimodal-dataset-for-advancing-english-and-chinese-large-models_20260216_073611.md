---
ver: rpa2
title: 'WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese
  Large Models'
arxiv_id: '2308.10755'
source_url: https://arxiv.org/abs/2308.10755
tags:
- data
- text
- dataset
- image-text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WanJuan, a large-scale multimodal dataset
  for training large language models and multimodal large language models. WanJuan
  includes text, image-text, and video data in both Chinese and English, totaling
  over 2TB.
---

# WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models

## Quick Facts
- arXiv ID: 2308.10755
- Source URL: https://arxiv.org/abs/2308.10755
- Reference count: 19
- Key outcome: WanJuan is a large-scale multimodal dataset for training large language models and multimodal large language models, totaling over 2TB and used to train InternLM with significant multi-dimensional evaluation advantages.

## Executive Summary
This paper introduces WanJuan, a comprehensive multimodal dataset designed for training large language models (LLMs) and multimodal large language models (MLLMs) in both Chinese and English. The dataset aggregates over 2TB of text, image-text, and video data from diverse web sources and applies rigorous cleaning and filtering to ensure high quality and safety. WanJuan was used to pretrain InternLM, a model that demonstrated significant performance advantages compared to models of similar scale across multiple evaluation dimensions.

## Method Summary
WanJuan was constructed by collecting multimodal data from various web sources, including web pages, encyclopedias, books, patents, textbooks, and video footage from China Media Group and Shanghai Media Group. The data underwent a multi-stage cleaning pipeline involving text extraction, language detection, corpus filtering, deduplication (using MinHashLSH and n-grams), and safety filtering (via FastText models for content safety and data quality). The processed data was serialized in a unified JSON format and made accessible via download tools and documentation.

## Key Results
- Dataset totals over 2TB across text (>600M documents, 1TB+), image-text (>22M documents, 200GB+), and video (>1000 videos, 900GB+) modalities.
- WanJuan was used to train InternLM, which demonstrated significant advantages in multi-dimensional evaluations compared to models of a similar scale.
- The dataset ensures data safety, high quality, and value alignment through algorithmic processing and manual verification, filtering out pornography, violence, and bias.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large-scale pretraining with diverse multimodal data enables strong generalization and zero-shot capabilities in LLMs and MLLMs.
- Mechanism: Pretraining on a broad corpus covering text, image-text, and video modalities in both Chinese and English allows models to learn rich representations that transfer across tasks and languages.
- Core assumption: Data diversity and volume are the primary drivers of model performance, outweighing architectural innovations.
- Evidence anchors:
  - [abstract]: "These cutting-edge models owe their remarkable performance to high-quality data."
  - [section]: "Based on large-scale pretraining, using a small amount of SFT fine-tuning and RLHF fine-tuning, some tasks even exceed the average human level."
  - [corpus]: Weak - no direct performance metrics, but high FMR scores suggest relevance to related works.
- Break condition: If the dataset contains significant noise, duplicates, or safety issues, performance gains may be negated.

### Mechanism 2
- Claim: Rigorous data cleaning and filtering ensures high data quality and safety, which is critical for effective pretraining.
- Mechanism: The dataset employs multiple filtering stages including content safety models, quality assessment, deduplication, and format validation to remove harmful or low-quality data.
- Core assumption: Removing toxic, irrelevant, or duplicate content during data preparation is more effective than post-hoc filtering or fine-tuning.
- Evidence anchors:
  - [section]: "In the construction process of the WanJuan dataset, we ensure data safety, high quality, and value alignment (filtering out pornography, violence, and bias) through algorithmic processing and manual verification."
  - [section]: "We trained content safety models for pornography, violence, gambling, attacks, and other toxic themes using the FastText model, separately for Chinese and English, to filter out potentially toxic data."
  - [corpus]: Weak - no explicit corpus-level safety metrics, but related datasets like WanJuan-CC mention safety as a key focus.
- Break condition: If filtering rules are too aggressive, they may remove valuable data; if too lax, safety and quality may be compromised.

### Mechanism 3
- Claim: A unified JSON format and accessible tooling accelerates dataset adoption and integration into training pipelines.
- Mechanism: Standardized data structure and tooling lower the barrier to entry, enabling more researchers and engineers to use the dataset effectively.
- Core assumption: Ease of use and accessibility directly correlate with dataset impact and community adoption.
- Evidence anchors:
  - [abstract]: "We provide a unified JSON format processing, dataset download tool, and supporting documentation to facilitate users in quickly applying large model training."
  - [section]: "All data can be accessed at https://opendatalab.org.cn/WanJuan1.0."
  - [corpus]: Weak - no direct corpus evidence, but the presence of related open-source datasets supports this claim.
- Break condition: If tooling is incomplete or documentation is unclear, adoption may be hindered.

## Foundational Learning

- Concept: Multimodal pretraining and transfer learning
  - Why needed here: Understanding how models learn from diverse modalities (text, image-text, video) and transfer that knowledge to downstream tasks is central to leveraging WanJuan effectively.
  - Quick check question: How does pretraining on interleaved image-text data improve performance on pure text tasks?

- Concept: Data cleaning and quality assurance in ML pipelines
  - Why needed here: The dataset's value hinges on rigorous cleaning processes; engineers must understand how to implement or adapt similar pipelines.
  - Quick check question: What are the trade-offs between aggressive filtering (removing more noise) and data retention (keeping more signal)?

- Concept: JSON-based data serialization and large-scale dataset handling
  - Why needed here: The unified JSON format is key to the dataset's usability; familiarity with parsing and streaming large JSON files is essential.
  - Quick check question: How would you efficiently stream and batch-process a 2TB JSON dataset for training?

## Architecture Onboarding

- Component map: Raw web data -> Cleaning/filtering pipeline -> JSON serialization -> Download tool -> Model training loop -> Evaluation framework
- Critical path: Raw web data -> Cleaning -> Deduplication -> JSON formatting -> Download tool -> Training
- Design tradeoffs: Balancing data volume vs. quality, multilingual coverage vs. depth, open access vs. safety
- Failure signatures: High duplicate rate, low data diversity, safety violations in output, slow download speeds
- First 3 experiments:
  1. Load a small subset of the JSON data and validate structure and content.
  2. Run a quick deduplication check using MinHashLSH on a sample.
  3. Train a small LLM on the text portion and evaluate on a multilingual benchmark.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of models trained on WanJuan compare to those trained on other large multimodal datasets like LAION-5B or Conceptual Captions?
- Basis in paper: [explicit] The paper mentions that WanJuan was used to train InternLM, which demonstrated significant advantages in multi-dimensional evaluations compared to models of a similar scale.
- Why unresolved: The paper does not provide direct comparisons with other large multimodal datasets.
- What evidence would resolve it: Benchmarking InternLM against models trained on other large multimodal datasets using standardized evaluation metrics.

### Open Question 2
- Question: What is the optimal ratio of Chinese to English data in the WanJuan dataset for training models that perform well in both languages?
- Basis in paper: [explicit] The dataset contains both Chinese and English data, with text data being 61.4% English and 35.3% Chinese.
- Why unresolved: The paper does not explore the impact of different language ratios on model performance.
- What evidence would resolve it: Training multiple models with varying ratios of Chinese to English data and comparing their performance in both languages.

### Open Question 3
- Question: How does the inclusion of video data in the WanJuan dataset affect the performance of multimodal models compared to those trained only on text and image data?
- Basis in paper: [explicit] The dataset includes video data from high-quality sources, which was used in the training of InternLM.
- Why unresolved: The paper does not isolate the impact of video data on model performance.
- What evidence would resolve it: Training models with and without video data and comparing their performance on tasks that benefit from video understanding.

## Limitations

- The paper lacks quantitative validation of the cleaning and filtering processes, such as duplicate removal rates or false positive rates in safety filtering.
- The causal link between dataset composition and model performance is asserted but not empirically validated through ablation studies or controlled experiments.
- No direct comparisons are provided with other large multimodal datasets to benchmark WanJuan's relative effectiveness.

## Confidence

- **High confidence** in the dataset construction methodology and the technical feasibility of the described pipeline. The steps for data collection, cleaning, filtering, and serialization are clearly specified and align with standard practices in the field.
- **Medium confidence** in the claims about dataset quality and safety. While the filtering mechanisms are described, there is no direct evidence (e.g., sample statistics, safety audit results) to verify that harmful content was successfully removed or that valuable data was not over-filtered.
- **Low confidence** in the causal link between WanJuan pretraining and InternLM's performance gains. Without controlled experiments or detailed performance breakdowns, it's unclear whether the dataset itself was a key differentiator.

## Next Checks

1. **Sample Audit:** Randomly sample 100 documents from each data type (text, image-text, video) and manually verify content quality, relevance, and safety against the claimed filtering standards.
2. **Duplicate Analysis:** Run a deduplication check on a 10% sample of the text corpus using MinHashLSH and report the estimated duplicate rate and impact on corpus size.
3. **Multilingual Coverage Test:** Use language detection on a stratified sample to quantify the actual distribution of Chinese vs. English content and assess whether it matches the intended balance for bilingual pretraining.