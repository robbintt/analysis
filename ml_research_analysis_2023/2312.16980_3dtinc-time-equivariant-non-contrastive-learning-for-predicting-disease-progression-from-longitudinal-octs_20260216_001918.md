---
ver: rpa2
title: '3DTINC: Time-Equivariant Non-Contrastive Learning for Predicting Disease Progression
  from Longitudinal OCTs'
arxiv_id: '2312.16980'
source_url: https://arxiv.org/abs/2312.16980
tags:
- learning
- representations
- loss
- contrastive
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces 3DTINC, a self-supervised learning method
  designed to predict disease progression from longitudinal 3D OCT scans of the retina.
  The method extends non-contrastive learning to handle volumetric OCT data and incorporates
  temporal information by aligning representations of scans taken at different times,
  encouraging time-equivariant features.
---

# 3DTINC: Time-Equivariant Non-Contrastive Learning for Predicting Disease Progression from Longitudinal OCTs

## Quick Facts
- arXiv ID: 2312.16980
- Source URL: https://arxiv.org/abs/2312.16980
- Reference count: 40
- Primary result: Achieves state-of-the-art performance in predicting wet AMD conversion using self-supervised learning on longitudinal OCTs

## Executive Summary
This paper introduces 3DTINC, a self-supervised learning method designed to predict disease progression from longitudinal 3D OCT scans of the retina. The method extends non-contrastive learning to handle volumetric OCT data and incorporates temporal information by aligning representations of scans taken at different times, encouraging time-equivariant features. This is achieved via a novel time-aware similarity loss term, enabling the model to capture disease progression patterns. Experiments on two large-scale datasets for predicting conversion to wet AMD demonstrate that 3DTINC outperforms state-of-the-art non-contrastive and contrastive baselines in linear evaluation and fine-tuning tasks, with robust performance even under domain shifts between different OCT scanners.

## Method Summary
3DTINC is a self-supervised pretraining method that learns time-equivariant representations from longitudinal 3D OCT scans. It uses a Siamese architecture with a Channel-Separated Convolutional Network (CSN) backbone to encode two OCT volumes from the same patient acquired at different time points. The method employs a time-aware non-contrastive loss (ℓTINC) that enforces a margin-based similarity between representations that grows with the time interval between scans. This encourages the model to capture both patient-specific anatomy and disease progression patterns. The learned representations are then fine-tuned for downstream tasks such as predicting conversion to wet AMD.

## Key Results
- 3DTINC achieves superior performance compared to state-of-the-art non-contrastive and contrastive baselines in linear evaluation and fine-tuning tasks on wet AMD conversion prediction
- The method demonstrates robust performance under domain shifts between different OCT scanners (Cirrus vs Topcon)
- 3DTINC effectively captures temporal information, enabling accurate disease progression prediction from longitudinal OCT data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 3DTINC learns representations that are equivariant to time by enforcing a margin-based similarity loss that grows with the time interval between OCT scans.
- Mechanism: The time-aware similarity term ℓTINC in the loss function pulls representations of scans from the same patient closer together only if their distance exceeds a margin proportional to the normalized time difference ∆t. This forces the network to encode time progression implicitly.
- Core assumption: Anatomical changes in OCT volumes are monotonic with disease progression, and the rate of change is consistent enough that distance in representation space can reflect time intervals.
- Evidence anchors:
  - [abstract] "We introduce a new non-contrastive similarity loss term that learns temporal information implicitly from intra-patient scans acquired at different times."
  - [section] "We hypothesize that as the time difference ∆t increases, the distance between z and z ′ should be within a margin proportional to ∆t."
  - [corpus] Weak evidence: Corpus contains no papers explicitly discussing margin-based time-equivariant similarity losses in non-contrastive learning; this appears to be a novel contribution.
- Break condition: If disease progression is non-monotonic or if OCT scan quality varies enough to mask anatomical changes, the distance-to-time mapping may fail.

### Mechanism 2
- Claim: Using two different OCT volumes from the same patient as input views, instead of two augmented views of the same scan, provides richer temporal context for representation learning.
- Mechanism: By sampling two OCTs at different time points and treating them as a pair, the model learns to embed both patient-specific anatomical structure and disease progression patterns into the representation space.
- Core assumption: The anatomical differences between two visits are small enough to be considered within the same patient identity but large enough to capture disease progression.
- Evidence anchors:
  - [abstract] "We introduce a new non-contrastive similarity loss term that learns temporal information implicitly from intra-patient scans acquired at different times."
  - [section] "inspired by [23], instead of creating two views from a single OCT, the input views are created from two OCTs of a patient acquired at different times acting as an additional transformation."
  - [corpus] Moderate evidence: Several papers in the corpus (e.g., L-MAE, LaTiM) use longitudinal data in SSL, but none explicitly use two time-point OCTs as input views for non-contrastive learning.
- Break condition: If the time interval between scans is too large or too small, the anatomical similarity may break down, harming learning.

### Mechanism 3
- Claim: Domain-specific augmentations for 3D OCT volumes (e.g., slice shifting, bounded cropping) improve the quality of learned representations compared to generic natural image augmentations.
- Mechanism: Augmentations tailored to OCT characteristics reduce noise from uninformative crops and maintain relevant anatomical context, leading to more robust representations.
- Core assumption: OCT images have different statistical properties and spatial layouts than natural images, so augmentations must be adapted accordingly.
- Evidence anchors:
  - [section] "The standardized view and the noisy nature of the OCT could result in an uninformative and extremely noisy crops. In this respect, medical OCT volumes require a distinct set of transformations..."
  - [section] "We selected the cropping ratio to be between 40-80% of B-scans...we changed solarization threshold to 0.42 to calibrate with respect to the intensity distribution of OCT volumes."
  - [corpus] Moderate evidence: One paper (Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT) discusses modality-specific adaptations, but none detail OCT-specific augmentation design.
- Break condition: If augmentations are too conservative, they may not provide enough variability for effective SSL; if too aggressive, they may remove discriminative features.

## Foundational Learning

- Concept: Contrastive vs. Non-contrastive Self-Supervised Learning
  - Why needed here: Understanding the difference explains why 3DTINC avoids large batch sizes and hard negative mining, making it feasible for 3D OCT data.
  - Quick check question: What is the main advantage of non-contrastive methods like VICReg over contrastive methods like SimCLR for 3D medical imaging?

- Concept: Equivariance in Representation Learning
  - Why needed here: Time-equivariance is the core property 3DTINC enforces, so understanding its mathematical formulation is essential to grasp the ℓTINC loss.
  - Quick check question: In the context of 3DTINC, what does it mean for a representation to be time-equivariant?

- Concept: Siamese Network Architecture
  - Why needed here: 3DTINC uses a Siamese encoder with shared weights; knowing how Siamese nets work clarifies the pretraining pipeline.
  - Quick check question: Why are the two branches of the Siamese network in 3DTINC initialized with the same weights?

## Architecture Onboarding

- Component map:
  Input OCT volumes -> CSN backbone -> MLP projector -> Embedding space

- Critical path:
  1. Load two OCT volumes from same patient, different visits
  2. Apply OCT-specific augmentations (crop, shift, flip, etc.)
  3. Encode both volumes with shared CSN
  4. Project embeddings with MLP
  5. Compute ℓTINC, variance, covariance losses
  6. Backpropagate to update encoder weights

- Design tradeoffs:
  - Batch size vs. memory: Small batches (32) make VICReg stable but limit negative sampling
  - Augmentation strength vs. anatomical fidelity: Too aggressive cropping may lose disease cues
  - Projector depth vs. collapse risk: Deeper projector helps prevent dimensional collapse but adds complexity

- Failure signatures:
  - Representations not time-equivariant: Distances between visits do not scale with time intervals
  - Overfitting to trivial solutions: All embeddings collapse to a single point
  - Poor cross-domain performance: Representations fail to generalize to different OCT scanners

- First 3 experiments:
  1. Train 3DTINC on synthetic OCT-like volumes with known time progression; verify that embedding distances scale with synthetic time labels.
  2. Replace ℓTINC with standard VICReg invariance loss; compare downstream ROCAUC to measure impact of temporal awareness.
  3. Train with and without OCT-specific augmentations; evaluate effect on linear evaluation performance.

## Open Questions the Paper Calls Out
- How does 3DTINC's performance compare to other state-of-the-art self-supervised methods when applied to different retinal diseases or imaging modalities?
- What is the optimal batch size for 3DTINC, and how does it impact the model's performance and training efficiency?
- How does 3DTINC's time-equivariant property generalize to different time scales and disease progression patterns?
- Can 3DTINC's representations be further improved by incorporating additional information, such as patient demographics or genetic data?
- How does 3DTINC's performance compare to supervised learning methods when large amounts of labeled data are available?

## Limitations
- The time-equivariant assumption may not hold for all disease trajectories or patients with variable progression rates
- OCT-specific augmentations introduce additional hyperparameters that could affect reproducibility and may not transfer well to other imaging modalities
- The fixed 6-month prediction window limits applicability to other clinical scenarios requiring different temporal horizons

## Confidence
- High confidence: The core mechanism of using two time-point OCT volumes as input views for non-contrastive learning
- Medium confidence: The OCT-specific augmentation strategy, as it's well-motivated but lacks extensive ablation
- Medium confidence: The time-aware similarity loss design, though the underlying assumption of monotonic progression is reasonable for AMD

## Next Checks
1. Test 3DTINC on synthetic OCT-like volumes with known time progression to verify that embedding distances scale linearly with synthetic time labels
2. Replace the ℓTINC loss with standard VICReg invariance loss and measure the degradation in downstream ROCAUC to quantify the impact of temporal awareness
3. Conduct ablation studies with varying time intervals between input OCT pairs to determine the optimal temporal window for representation learning