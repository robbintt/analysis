---
ver: rpa2
title: 'Routing Arena: A Benchmark Suite for Neural Routing Solvers'
arxiv_id: '2310.04140'
source_url: https://arxiv.org/abs/2310.04140
tags:
- routing
- methods
- solution
- time
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Routing Arena provides a benchmark suite for neural routing
  solvers with consistent evaluation protocols and a novel metric, Weighted Relative
  Average Performance (WRAP), that quantifies runtime efficiency by comparing solution
  trajectories to a base solver and Best Known Solution. The suite integrates over
  15 datasets and nine machine learning- and five operations research-based solvers,
  employing hardware normalization via PassMark scores for fair runtime comparisons.
---

# Routing Arena: A Benchmark Suite for Neural Routing Solvers

## Quick Facts
- **arXiv ID**: 2310.04140
- **Source URL**: https://arxiv.org/abs/2310.04140
- **Reference count**: 40
- **Key outcome**: HGS-CVRP achieves superior performance in solution quality and runtime efficiency for capacitated vehicle routing problems, while hybrid ML-OR approaches like NeuroLKH demonstrate competitive advantages.

## Executive Summary
The Routing Arena provides a benchmark suite for neural routing solvers with consistent evaluation protocols and a novel metric, Weighted Relative Average Performance (WRAP), that quantifies runtime efficiency by comparing solution trajectories to a base solver and Best Known Solution. The suite integrates over 15 datasets and nine machine learning- and five operations research-based solvers, employing hardware normalization via PassMark scores for fair runtime comparisons. Experiments show that state-of-the-art operations research solvers achieve superior performance, while hybrid ML-OR approaches demonstrate competitive advantages, suggesting future neural routing solvers should incorporate hybrid strategies.

## Method Summary
The Routing Arena evaluates solvers on capacitated vehicle routing problems using two main protocols: fixed-budget comparison of final solutions against Best Known Solutions, and anytime evaluation tracking full solution trajectories via Primal Integral and WRAP metrics. Hardware normalization uses PassMark scores to standardize runtimes across different machines. The benchmark integrates multiple datasets including CVRPLib instances, ML benchmarks (Nazari et al., Queiroga et al.), and generated uniform, Gaussian mixture, and X-type distributions. Solvers include both operations research methods (HGS-CVRP, LKH, FILO, Savings+SA, OR-Tools) and neural approaches with hybrid ML-OR strategies.

## Key Results
- HGS-CVRP achieves superior performance in solution quality and runtime efficiency for capacitated vehicle routing problems
- Hybrid ML-OR approaches like NeuroLKH demonstrate competitive advantages in both anytime and final solution quality
- WRAP metric successfully differentiates solver performance by accounting for both solution quality and computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: WRAP quantifies runtime efficiency by normalizing incumbent solution improvements against a base solver trajectory
- Mechanism: WRAP computes a relative performance index that stays at 1 until a method finds a solution better than the base solver, then decreases proportionally to the gap improvement relative to the best known solution. The metric integrates this RPI over the entire time horizon, normalizing by total time.
- Core assumption: The base solver (Savings+SA) provides a consistent, simple baseline trajectory that reflects minimal effective performance across all problem instances.
- Evidence anchors:
  - [abstract]: "by setting the solution trajectory in perspective to a Best Known Solution and a Base Solver's solutions trajectory"
  - [section 5.2]: "Based on the relative performance improvement (RPI) over a base solver. In the current version of the Routing Arena, the Clark and Wright Savings Algorithm together with a SA-guided ORT search is chosen as base solver"

### Mechanism 2
- Claim: Hardware normalization via PassMark scores ensures fair runtime comparisons across heterogeneous machines
- Mechanism: Raw runtimes are multiplied by the ratio of the reference machine's PassMark score to the actual machine's PassMark score, both for CPU-only and GPU-inclusive setups, producing normalized times as if executed on the reference hardware.
- Core assumption: PassMark scores are proportional to actual computational throughput for the algorithms tested, and hardware differences dominate runtime variance.
- Evidence anchors:
  - [section 4]: "All metric evaluations incorporate hardware information about resources used... by multiplying with the ratio of the machines PassMark score s and a reference machines PassMark score sbase"
  - [section 4]: "The run-times for evaluation are standardized to a particular reference machine"

### Mechanism 3
- Claim: Fixed-budget and anytime evaluation protocols isolate quality and efficiency aspects of solvers respectively
- Mechanism: Fixed-budget compares final solutions against BKS at a fixed time budget; anytime tracks full solution trajectories via PI and WRAP, rewarding early good solutions.
- Core assumption: The two metrics capture orthogonal aspects: absolute solution quality vs dynamic improvement over time, allowing clearer assessment of solver strengths.
- Evidence anchors:
  - [section 5.1]: "Fixed Budget... compares algorithms for an a priori fixed time budget... Anytime... assesses how well a method performs over its full solution trajectory"
  - [section 6.2]: "From the results above, we see that the neural solvers that deliver better any-time performance on average and in a more stable fashion are hybrid ML-OR algorithms"

## Foundational Learning

- **Concept: Primal Integral (PI)** - Measures how quickly a solver improves its incumbent solution over time, providing a single scalar summarizing anytime efficiency. *Quick check: In PI, does a lower value indicate better performance or worse?*
- **Concept: Hardware normalization using PassMark scores** - Ensures fair cross-hardware comparisons; without it, solvers on faster machines appear artificially superior. *Quick check: What is the reference machine's PassMark score used for single-threaded CPU normalization?*
- **Concept: Hybrid ML-OR solver integration** - Combines learned candidate generation with proven OR search, yielding competitive anytime and final solution quality. *Quick check: What base OR heuristic does NeuroLKH augment?*

## Architecture Onboarding

- **Component map**: `data/` → `models/` → `outputs/` → `config/` → `run_*.py`
- **Critical path**: 1. Load CVRPInstance → preprocess per solver requirements 2. Execute solver with given time budget 3. Capture incumbent solutions and times 4. Normalize runtimes using PassMark 5. Compute WRAP/PI and store results
- **Design tradeoffs**: Single-instance vs batch inference: single-instance matches evaluation protocol but may limit parallelism. Normalization granularity: per-instance vs per-dataset affects fairness and computational overhead.
- **Failure signatures**: Missing incumbent solutions → WRAP/PI cannot be computed. Out-of-memory during large instance inference → solver crash. Inconsistent data formats → loader errors.
- **First 3 experiments**: 1. Run Savings+SA baseline on Unif100 to verify baseline pipeline 2. Run NeuroLKH on XML_small and check WRAP computation 3. Compare HGS-CVRP vs FILO on XE1 dataset for anytime performance

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the WRAP metric's comparison to a base solver affect its ability to differentiate between methods when the base solver's performance is already close to optimal?
- **Open Question 2**: What is the optimal normalization scheme for combining CPU and GPU performance metrics in the PassMark-based standardization?
- **Open Question 3**: How does the single-instance vs batch-mode distinction affect the fairness of comparing NLNS to other methods?
- **Open Question 4**: What is the relationship between training distribution difficulty and final method performance across different benchmark sets?
- **Open Question 5**: How does the inclusion of SGBS+EAS on larger instances with adjusted sampling sizes affect its relative performance ranking?

## Limitations

- WRAP metric assumes the Savings+SA baseline trajectory is representative across all CVRP distributions; performance could be skewed if this baseline underperforms on certain problem structures
- Hardware normalization via PassMark scores may not perfectly capture actual solver throughput differences, particularly for memory-bound algorithms
- Evaluation focuses exclusively on CVRP, limiting generalizability to other routing problem types

## Confidence

- **High Confidence**: Hardware normalization methodology and fixed-budget evaluation protocol (well-specified and directly implementable)
- **Medium Confidence**: WRAP metric design and interpretation (clear mechanism but dependent on baseline representativeness)
- **Medium Confidence**: Hybrid ML-OR performance claims (competitive results shown but may not generalize beyond tested CVRP instances)

## Next Checks

1. **Baseline Sensitivity**: Test WRAP with alternative base solvers (e.g., OR-Tools LNS) to assess metric sensitivity to baseline choice
2. **Hardware Normalization Verification**: Compare PassMark-normalized runtimes against actual measurements on multiple hardware configurations to validate normalization accuracy
3. **Distribution Generalization**: Evaluate top-performing solvers (HGS-CVRP, NeuroLKH) on additional CVRP distributions beyond those tested to verify consistent performance advantages