---
ver: rpa2
title: Locally-Minimal Probabilistic Explanations
arxiv_id: '2312.11831'
source_url: https://arxiv.org/abs/2312.11831
tags:
- explanations
- probabilistic
- features
- marques-silva
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of computing high-quality, small-size
  explanations for machine learning models, specifically for probabilistic abductive
  explanations (PAXps). PAXps offer strong theoretical guarantees but are computationally
  complex to compute exactly.
---

# Locally-Minimal Probabilistic Explanations

## Quick Facts
- arXiv ID: 2312.11831
- Source URL: https://arxiv.org/abs/2312.11831
- Authors: 
- Reference count: 9
- Primary result: Novel algorithms compute high-quality, small-size explanations for ML models with precision 0.97-0.99 and size reductions up to 86.6% compared to abductive explanations

## Executive Summary
This paper addresses the challenge of computing high-quality, small-size explanations for machine learning models, specifically for probabilistic abductive explanations (PAXps). PAXps offer strong theoretical guarantees but are computationally complex to compute exactly. The authors propose two new algorithms for efficiently computing locally-minimal PAXps, which are high-quality approximations of PAXps. The first algorithm uses approximate model counting, while the second uses Monte Carlo sampling with probabilistic guarantees.

## Method Summary
The paper introduces two algorithms for computing locally-minimal probabilistic abductive explanations (LmPAXps). The first uses approximate model counting (PAC guarantees) to estimate precision thresholds while iteratively refining feature subsets. The second employs Monte Carlo sampling to achieve similar results with probabilistic guarantees. Both algorithms use linear search strategies (deletion-based and progression-based) to efficiently explore the feature space without exhaustive enumeration.

## Key Results
- For random forests, algorithms provide explanations 86.6% and 73.9% smaller than abductive explanations with precision 0.97 and 0.98
- For binarized neural networks, explanations are 33.6% to 68.2% smaller with precision 0.99
- Progression-based algorithm outperforms deletion-based in runtime and explanation size
- Algorithms demonstrate practical efficiency on UCI Machine Learning Repository and Penn Machine Learning Benchmarks datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Approximate model counting with PAC guarantees enables scalable computation of probabilistic abductive explanations.
- Mechanism: The algorithm uses approximate model counting to estimate the number of feature assignments that satisfy the classifier's prediction condition. By iteratively removing or adding features and checking if the precision threshold is maintained, it identifies locally-minimal sets without requiring exact counting.
- Core assumption: Approximate model counting provides sufficiently accurate estimates for determining whether a feature subset maintains the required precision threshold.
- Evidence anchors:
  - [abstract]: "The paper proposes two new algorithms for efficiently computing locally-minimal PAXps, which are high-quality approximations of PAXps."
  - [section]: "We formulate this as SAT (or pseudo-Boolean) formula... and instrument an oracle call to a (probably approximately correct (or PAC)) counter ApproxMC2..."
  - [corpus]: Weak evidence - no direct mentions of approximate model counting in corpus.
- Break condition: If the approximation error becomes too large relative to the precision threshold, the algorithm may incorrectly classify a subset as maintaining precision when it doesn't.

### Mechanism 2
- Claim: Monte Carlo sampling provides a scalable alternative to approximate model counting for robust probabilistic explanations.
- Mechanism: The algorithm generates random samples from the feature space, tests the classifier on each sample, and counts how many maintain the correct prediction. This count is used to estimate the precision of feature subsets.
- Core assumption: Random sampling from the feature space provides an unbiased estimate of the precision of feature subsets.
- Evidence anchors:
  - [abstract]: "The second uses Monte Carlo sampling with probabilistic guarantees."
  - [section]: "Our second proposed approach is Monte Carlo sampling over xS ~ F (uniformly sample from F restricted to variables of F\S such that V i∈S xi = vi)..."
  - [corpus]: Weak evidence - no direct mentions of Monte Carlo sampling in corpus.
- Break condition: If the number of samples is insufficient, the sampling estimate may have high variance and lead to incorrect decisions about feature subset precision.

### Mechanism 3
- Claim: Linear search algorithms (deletion-based and progression-based) efficiently explore the feature space to find locally-minimal explanations.
- Mechanism: The algorithms iteratively add or remove features while checking precision thresholds, using the counting/sampling oracles to evaluate precision at each step. This avoids exhaustive enumeration of all feature subsets.
- Core assumption: The locally-minimal property can be verified by checking only single-feature additions/removals rather than all possible subsets.
- Evidence anchors:
  - [abstract]: "This paper proposes novel efficient algorithms for the computation of locally-minimal PXAps, which offer high-quality approximations of PXAps in practice."
  - [section]: "Algorithm 1 depicts a (deletion-based) linear search method for computing a locally-minimal PAXp... Algorithm 2 outlines the progression-based linear search approach..."
  - [corpus]: Weak evidence - no direct mentions of linear search algorithms in corpus.
- Break condition: If the precision function has complex non-monotonic behavior, linear search may miss globally optimal solutions.

## Foundational Learning

- Concept: PAC (Probably Approximately Correct) learning
  - Why needed here: The algorithms rely on PAC guarantees to provide confidence bounds on the approximation quality of computed explanations.
  - Quick check question: What does the (ε, δ) notation represent in the context of approximate model counting?

- Concept: Model counting and SAT solving
  - Why needed here: The algorithms encode the classifier and explanation conditions as logical formulas that need to be solved or counted.
  - Quick check question: How does the approximate model counter differ from an exact model counter in terms of guarantees and computational complexity?

- Concept: Probabilistic abductive explanations
  - Why needed here: Understanding the definition and properties of PAXps is essential for implementing the precision checking logic.
  - Quick check question: Why is the precision threshold T important in the definition of probabilistic abductive explanations?

## Architecture Onboarding

- Component map: Classifier encoding -> Precision evaluation (approximate counting/sampling) -> Linear search module -> Locally-minimal explanation
- Critical path: 1) Encode classifier and instance into logical formula 2) Use precision evaluation (counting/sampling) to check precision thresholds 3) Apply linear search to find locally-minimal feature sets
- Design tradeoffs: Approximate counting provides stronger theoretical guarantees but may be slower; sampling is faster but has weaker guarantees. Deletion-based search is simpler but may be less efficient than progression-based search.
- Failure signatures: Timeouts during precision evaluation indicate classifier complexity is too high; low precision in computed explanations suggests threshold is too high or approximation error is significant.
- First 3 experiments:
  1. Implement and test classifier encoding for a simple random forest on a small dataset
  2. Compare approximate counting vs sampling precision estimates on a fixed feature subset
  3. Test deletion-based vs progression-based search on a simple classifier to compare explanation quality and runtime

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of different feature selection heuristics on the quality and runtime of locally-minimal probabilistic explanations?
- Basis in paper: [explicit] The paper mentions using a heuristic to select the most likely influential feature at each iteration of the progression-based algorithm.
- Why unresolved: The paper only reports results using one specific heuristic, and does not explore the impact of other potential heuristics.
- What evidence would resolve it: Comparative analysis of different heuristics (e.g., greedy, random, information gain-based) on explanation quality and runtime across various datasets and models.

### Open Question 2
- Question: How do the proposed algorithms scale to larger datasets and more complex models?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the algorithms on a limited number of datasets and models. Scalability is a key concern in practical applications.
- Why unresolved: The paper does not provide comprehensive scaling analysis or theoretical bounds on the algorithm's complexity.
- What evidence would resolve it: Extensive experiments on larger datasets (e.g., ImageNet) and more complex models (e.g., deep neural networks) with varying numbers of features and classes.

### Open Question 3
- Question: What are the trade-offs between the quality of explanations and their computational cost for different applications?
- Basis in paper: [explicit] The paper introduces two algorithms with different trade-offs: Monte Carlo sampling for robust explanations and approximate model counting for high-precision explanations.
- Why unresolved: The paper does not provide a systematic analysis of the trade-offs between explanation quality and computational cost for different application domains.
- What evidence would resolve it: Empirical studies comparing the quality and computational cost of explanations for different applications (e.g., medical diagnosis, fraud detection) with varying requirements for precision and robustness.

## Limitations

- Limited empirical validation to specific model types (RFs and BNNs) and datasets
- Potential generalization issues to other model architectures or data domains
- Scalability concerns for larger, more complex models and datasets with thousands of features
- Lack of comprehensive comparison with other explanation methods beyond standard abductive explanations

## Confidence

**High confidence** in the algorithmic framework and theoretical foundations. The use of approximate model counting and Monte Carlo sampling is well-established in the literature, and the paper provides clear pseudocode and complexity analysis.

**Medium confidence** in the empirical results. While the reported precision values (0.97-0.99) are impressive, the evaluation setup lacks details about hyperparameter tuning and comparison baselines beyond standard abductive explanations.

**Low confidence** in the scalability claims. The paper doesn't adequately address how the algorithms would perform on larger, more complex models or datasets with thousands of features.

## Next Checks

1. Cross-architecture validation: Test the algorithms on transformer-based models and other non-traditional ML architectures to assess generalizability.

2. Robustness analysis: Evaluate explanation stability under different random seeds and precision thresholds to quantify variance in the results.

3. Complexity benchmarking: Measure actual wall-clock time and memory usage on progressively larger datasets to validate the claimed scalability.