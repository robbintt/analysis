---
ver: rpa2
title: Competitions in AI -- Robustly Ranking Solvers Using Statistical Resampling
arxiv_id: '2308.05062'
source_url: https://arxiv.org/abs/2308.05062
tags:
- competition
- solvers
- solver
- bootstrap
- track
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how sensitive competition rankings are to
  changes in benchmark instances using statistical resampling. It shows that standard
  competition rankings can be unstable even with minor changes to the benchmark set.
---

# Competitions in AI -- Robustly Ranking Solving Using Statistical Resampling

## Quick Facts
- arXiv ID: 2308.05062
- Source URL: https://arxiv.org/abs/2308.05062
- Authors: 
- Reference count: 3
- Primary result: Shows competition rankings are sensitive to benchmark instance sets and proposes bootstrap resampling method to produce statistically robust solver rankings with confidence intervals

## Executive Summary
This paper investigates the stability of competition rankings when benchmark instances change, demonstrating that standard rankings can be highly sensitive to minor benchmark modifications. The authors propose a robust ranking method using bootstrap resampling and hypothesis testing to identify statistically tied solvers with bounded error. Applied to SAT, planning, and vision competitions, the method reveals frequent statistical ties and some rank inversions compared to official results. For example, in SAT 2016 main track, 17 solvers were statistically tied for first place.

## Method Summary
The method uses bootstrap resampling with 10,000 iterations to approximate drawing new samples from the underlying problem instance distribution. For each bootstrap sample, the competition scoring mechanism is applied to all solvers. Confidence intervals are calculated using the percentile method, and pairwise bootstrap hypothesis tests determine statistically significant performance differences. Holm-Bonferroni correction controls family-wise error across multiple comparisons, and solvers are grouped into statistically tied clusters through iterative significance testing.

## Key Results
- Competition rankings can be very sensitive to even minor changes in benchmark instance sets
- In SAT 2016 main track, 17 solvers were statistically tied for first place using robust ranking
- Method reveals frequent statistical ties and some inversions compared to official results
- Produces confidence intervals for competition scores with bounded error rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Competition rankings are sensitive to even small changes in benchmark instance sets
- Mechanism: Bootstrap resampling approximates drawing new samples from the underlying problem instance distribution. By resampling the original benchmark set with replacement, the method creates pseudo-replicates that capture the variability inherent in the instance distribution.
- Core assumption: The original benchmark set is a representative sample from the underlying distribution D, so resampling it mimics drawing from D.
- Evidence anchors:
  - [abstract] "We show that the rankings resulting from the standard interpretation of competition results can be very sensitive to even minor changes in the benchmark instance set"
  - [section] "we measured the effect of removing a single problem instance... In almost all tracks, there were multiple problem instances whose removal caused changes in the competition rankings"
  - [corpus] Weak - the corpus doesn't provide direct evidence for sensitivity to minor changes, but this is the core premise of the paper
- Break condition: If the original benchmark set is not representative of D, or if the instance distribution has high variance, the bootstrap approximation may not capture the true ranking variability.

### Mechanism 2
- Claim: Bootstrap hypothesis testing can identify statistically tied solvers with bounded error.
- Mechanism: For each pair of solvers, the method counts bootstrap replicates where one solver outperforms the other. If the proportion is below significance threshold α, the null hypothesis (that the first solver is no better) is rejected. Holm-Bonferroni correction controls family-wise error across multiple comparisons.
- Core assumption: The bootstrap distribution of solver scores is a valid approximation of the true sampling distribution of scores from D.
- Evidence anchors:
  - [abstract] "Our approach produces confidence intervals of competition scores as well as statistically robust solver rankings with bounded error"
  - [section] "We use the Holm-Bonferroni multiple-testing correction to ensure an upper bound of α := 0.05 for the overall (family-wise) false rejection probability"
  - [corpus] No direct evidence - this is a methodological claim not supported by corpus neighbors
- Break condition: If the bootstrap samples don't adequately represent the true sampling distribution, or if the Holm-Bonferroni correction is too conservative for the problem context.

### Mechanism 3
- Claim: Resampling reveals frequent statistical ties and rank inversions compared to official results.
- Mechanism: By generating 10,000 bootstrap samples, the method creates a distribution of possible rankings. Solvers are grouped into statistically tied clusters based on pairwise significance tests. This often reveals ties where official rankings show clear winners.
- Core assumption: The bootstrap ranking distribution is stable enough to identify true performance equivalence between solvers.
- Evidence anchors:
  - [abstract] "our analysis reveals frequent statistical ties in solver performance as well as some inversions of ranks compared to the official results"
  - [section] "In the main track of the 2016 SAT Competition, 17 solvers are tied for first place in our robust competition rankings"
  - [corpus] No direct evidence - corpus doesn't discuss rank inversions or statistical ties
- Break condition: If the bootstrap distribution is too noisy, or if the significance threshold is too stringent, the method may either over-group solvers or fail to detect true performance differences.

## Foundational Learning

- Concept: Bootstrap resampling and confidence intervals
  - Why needed here: The method relies on bootstrap sampling to estimate the variability of solver scores and create confidence intervals. Understanding how bootstrap works is crucial to grasp why the method produces the results it does.
  - Quick check question: If you have 100 data points and generate 1000 bootstrap samples by resampling with replacement, what's the expected number of unique data points in each bootstrap sample?

- Concept: Hypothesis testing and p-values
  - Why needed here: The method uses bootstrap hypothesis tests to determine if performance differences between solvers are statistically significant. Understanding p-values and significance thresholds is essential to interpret the results.
  - Quick check question: If a hypothesis test yields a p-value of 0.03 with significance threshold α=0.05, do we reject the null hypothesis? What if α=0.01?

- Concept: Multiple testing correction
  - Why needed here: When performing many pairwise comparisons between solvers, the chance of false positives increases. The Holm-Bonferroni correction controls this family-wise error rate, which is crucial for the method's validity.
  - Quick check question: If you perform 10 independent hypothesis tests at α=0.05, what's the probability of at least one false positive without correction? How does Holm-Bonferroni reduce this?

## Architecture Onboarding

- Component map:
  Data input -> Bootstrap generator -> Scoring engine -> Confidence interval calculator -> Hypothesis tester -> Ranking grouper -> Output formatter

- Critical path:
  1. Load competition data
  2. Generate bootstrap samples (10,000 iterations)
  3. Score each bootstrap sample for all solvers
  4. Calculate confidence intervals for each solver's score
  5. Perform pairwise bootstrap hypothesis tests
  6. Apply Holm-Bonferroni correction
  7. Generate robust ranking with tied groups
  8. Format and output results

- Design tradeoffs:
  - Number of bootstrap samples: More samples increase accuracy but also computation time (10,000 chosen as balance)
  - Significance threshold α: Lower α reduces false positives but may increase false negatives in tied groups
  - Multiple testing correction: Holm-Bonferroni is conservative but simple; other methods might be more powerful
  - Stratification: Domain-stratified sampling preserves competition structure but adds complexity

- Failure signatures:
  - Extremely wide confidence intervals: Indicates high variability in benchmark instances or solver performance
  - Large number of tied groups: May suggest benchmark set is too small or not diverse enough
  - Rank inversions: Could indicate noise in the data or that official ranking method doesn't capture true performance
  - Computation time issues: 10,000 bootstrap samples can be computationally intensive for large competitions

- First 3 experiments:
  1. Apply method to a simple competition with known ground truth (e.g., simulated data) to verify it correctly identifies true ties and differences
  2. Vary the number of bootstrap samples (100, 1000, 10000) to assess impact on result stability and computation time
  3. Apply the method to competition data where official rankings are known to be stable vs. unstable to validate its sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the statistical significance of competition results be maintained while reducing the computational burden of competitions?
- Basis in paper: [explicit] The paper suggests that the methodology could help reduce the computational burden by allowing competitions to be run in a racing-like fashion, where solvers are iteratively evaluated on incrementally larger benchmark sets and eliminated as soon as their performance is shown to fall statistically significantly below that of the current front runner.
- Why unresolved: The paper does not provide a detailed implementation or testing of this racing-like approach to see how well it maintains statistical significance while reducing computational costs.
- What evidence would resolve it: Experimental results showing the effectiveness and efficiency of the racing-like approach in various competition settings, with comparisons to traditional methods.

### Open Question 2
- Question: What are the implications of using different bootstrapping techniques or hypothesis tests on the robustness of competition rankings?
- Basis in paper: [explicit] The paper mentions that robust rankings can be determined using arbitrary tests for pairwise performance differences and that they have also explored the use of a permutation test for the same null hypothesis, obtaining very similar results.
- Why unresolved: The paper does not explore the implications of using different bootstrapping techniques or hypothesis tests in detail, nor does it compare their effectiveness in maintaining robust rankings.
- What evidence would resolve it: Comparative studies of different bootstrapping techniques and hypothesis tests applied to competition data, with analysis of their impact on the robustness and reliability of rankings.

### Open Question 3
- Question: How can benchmark set construction be optimized to minimize performance ties among top-ranked solvers?
- Basis in paper: [explicit] The paper suggests that the methodology could be used in combination with other methods to facilitate the construction of benchmark sets that minimize performance ties of top-ranked solvers from previous competitions.
- Why unresolved: The paper does not provide a detailed approach or empirical evidence on how to construct such benchmark sets or how effective this approach would be in practice.
- What evidence would resolve it: Development and testing of benchmark set construction methods that incorporate the proposed statistical analysis, with evaluation of their effectiveness in reducing ties among top solvers.

## Limitations
- Bootstrap method assumes benchmark set is representative of true problem instance distribution
- Holm-Bonferroni correction may be overly conservative, creating too many tied groups
- Computationally intensive for large competitions with 10,000 bootstrap samples

## Confidence
- Competition rankings are sensitive to benchmark changes: High confidence
- Bootstrap resampling produces valid confidence intervals: Medium confidence
- Method reveals frequent statistical ties: High confidence
- Rank inversions compared to official results: Medium confidence

## Next Checks
1. **Benchmark size sensitivity analysis**: Systematically vary the number of benchmark instances (50%, 75%, 100%, 125% of original) and measure how confidence interval widths and tied group sizes change to quantify the impact of benchmark representativeness
2. **Cross-validation with simulated ground truth**: Create synthetic competition data with known true performance differences and evaluate how often the method correctly identifies ties vs. differences at different significance thresholds
3. **Alternative multiple testing comparison**: Apply the same bootstrap framework using FDR-based corrections (Benjamini-Hochberg) and compare the number and composition of tied groups to assess whether Holm-Bonferroni is overly conservative