---
ver: rpa2
title: 'Adversarial Examples Might be Avoidable: The Role of Data Concentration in
  Adversarial Robustness'
arxiv_id: '2309.16096'
source_url: https://arxiv.org/abs/2309.16096
tags:
- robust
- classifier
- data
- adversarial
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work examines whether adversarial examples are inherently\
  \ unavoidable by investigating the relationship between data distribution concentration\
  \ and classifier robustness. It introduces the notion of (\u03F5, \u03B4)-concentration,\
  \ showing that this property is necessary for the existence of robust classifiers\
  \ against \u21132-bounded perturbations."
---

# Adversarial Examples Might be Avoidable: The Role of Data Concentration in Adversarial Robustness

## Quick Facts
- arXiv ID: 2309.16096
- Source URL: https://arxiv.org/abs/2309.16096
- Reference count: 40
- Key outcome: Data concentration on small-volume subsets is necessary and sufficient for robust classifiers; low-dimensional subspace structure enables norm-independent polyhedral certificates

## Executive Summary
This paper investigates whether adversarial examples are inherently unavoidable by examining the relationship between data distribution concentration and classifier robustness. The authors introduce the notion of (ϵ, δ)-concentration, proving it is necessary for robust classifiers against ℓ2-bounded perturbations, and define a stronger concentration notion that is sufficient for constructing robust classifiers. Focusing on data distributed near low-dimensional linear subspaces, they develop a geometric analysis of the dual optimization problem, yielding norm-independent polyhedral robustness certificates. Empirically, on MNIST, their method provides certificates complementary to Randomized Smoothing, demonstrating robustness to larger adversarial perturbations in certain regions of the input space.

## Method Summary
The paper introduces (ϵ, δ)-concentration to characterize when robust classifiers exist, proving concentration is necessary for adversarial robustness using the Brunn-Minkowski theorem. They define (ϵ, δ, γ)-strong concentration as sufficient for robust classifier existence. For data concentrated near low-dimensional linear subspaces, they exploit this structure through sparse representation and dual optimization to construct classifiers with polyhedral certificates. The method solves a dual optimization problem to find active constraints from training data, then uses majority voting for classification and computes certified regions. Empirical evaluation on MNIST demonstrates complementary performance to Randomized Smoothing.

## Key Results
- Data concentration on small-volume subsets is necessary for adversarial robustness
- Strong concentration is sufficient for constructing robust classifiers
- Polyhedral certificates derived from subspace structure complement Randomized Smoothing
- The method provides larger certified regions than RS in certain input space regions
- Combined approach yields improved ℓ2 robustness on MNIST

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Data concentration is a necessary condition for adversarial robustness
- **Mechanism**: If a classifier is robust to adversarial perturbations, the underlying data distribution must concentrate on small-volume subsets of the input space. This follows from the Brunn-Minkowski theorem applied to classification regions.
- **Core assumption**: The data domain is normalized (X = Bℓ2(0, 1)) and adversarial perturbations have bounded ℓ2 norm
- **Evidence anchors**:
  - [abstract] "a key property of the data distribution – concentration on small-volume subsets of the input space – determines whether a robust classifier exists"
  - [section 2] "If there exists an (ϵ, δ)-robust classifier f for a data distribution p, then at least one of the class conditionals q1, q2, ..., qK must be (ϵ, δ)–concentrated"
- **Break condition**: If the data distribution is uniform or spread across large regions of the input space, no robust classifier can exist

### Mechanism 2
- **Claim**: Strong concentration is sufficient for robust classifier existence
- **Mechanism**: When class conditionals concentrate on non-overlapping subsets of the input space (with bounded intersection), a robust classifier can be constructed by predicting the majority label among points in the expanded concentration regions
- **Core assumption**: The data distribution is (ϵ, δ, γ)-strongly-concentrated, meaning class conditionals concentrate on subsets that don't overlap too much after expansion
- **Evidence anchors**:
  - [abstract] "we formally proving that a successful defense exists only when the data distribution concentrates on an exponentially small volume of the input space"
  - [section 3] "If the data distribution p is (ϵ, δ, γ)-strongly-concentrated, then there exists an (ϵ, δ + γ)-robust classifier for p"
- **Break condition**: If the expanded concentration regions of different classes significantly overlap, the classifier cannot distinguish between classes and robustness breaks down

### Mechanism 3
- **Claim**: Low-dimensional subspace structure enables norm-independent polyhedral certificates
- **Mechanism**: When data concentrates near a union of low-dimensional linear subspaces, the dual optimization problem for sparse representation yields stable active constraint sets, enabling robust classification with polyhedral certificates that are larger than traditional ℓ2 balls
- **Core assumption**: Data lies near a union of low-dimensional linear subspaces, and the training data matrix S is sufficiently large to span these subspaces
- **Evidence anchors**:
  - [abstract] "for a data distribution concentrated on a union of low-dimensional linear subspaces, exploiting data structure naturally leads to classifiers that enjoy good robustness guarantees"
  - [section 4] "Our analysis results in polyhedral certified regions whose faces and extreme rays are described by selected points in the training data"
- **Break condition**: If the data does not lie near low-dimensional subspaces, or if the subspaces are not well-separated, the polyhedral certificates may not provide significant advantage over traditional methods

## Foundational Learning

- **Concept**: Concentration of measure
  - Why needed here: The paper's core theoretical results rely on understanding how probability distributions concentrate on subsets of the input space
  - Quick check question: If a distribution assigns probability 0.99 to a set of volume 0.001, is it considered concentrated? (Yes, per the paper's definition)

- **Concept**: Subspace clustering and sparse representation
  - Why needed here: The practical construction of robust classifiers relies on representing data points as sparse combinations of basis vectors from training data
  - Quick check question: What is the dual problem to the ℓ1 minimization problem used for sparse representation? (The projection problem onto the polar of the convex hull of ±S)

- **Concept**: Convex geometry and Brunn-Minkowski theorem
  - Why needed here: The proof that concentration is necessary for robustness uses the Brunn-Minkowski theorem to bound the volume of classification regions
  - Quick check question: What does the Brunn-Minkowski theorem state about the volume of Minkowski sums of sets? (It provides a lower bound on the volume of E + F in terms of the volumes of E and F)

## Architecture Onboarding

- **Component map**:
  - Data preprocessing and normalization
  - Subspace structure exploitation
  - Dual optimization solver
  - Certificate generation module
  - Integration with existing methods

- **Critical path**:
  1. Input data preprocessing and normalization
  2. Dual optimization problem solving to find active constraints
  3. Polyhedral certificate computation from active constraints
  4. Classification using majority voting among active constraints
  5. Certificate validation and comparison with baseline methods

- **Design tradeoffs**:
  - Computational complexity vs. certificate tightness: The exact ℓ2 certificate requires solving an optimization problem with potentially many constraints
  - Data structure assumption vs. general applicability: The method works best when data lies near low-dimensional subspaces
  - Certificate type vs. attack model: Polyhedral certificates are better against certain attacks but may be weaker against others

- **Failure signatures**:
  - Active constraint set Aλ(x) contains points from multiple classes (indicates poor separation)
  - Polyhedral certificate contains very few training points (indicates weak structure exploitation)
  - Certificate size is much smaller than Randomized Smoothing (indicates limited advantage)

- **First 3 experiments**:
  1. Verify concentration property on MNIST: Measure the volume of the support of each class conditional distribution
  2. Test dual optimization stability: For random points in the MNIST test set, check if the active constraint set remains stable under small perturbations
  3. Compare certificate sizes: For a set of test points, compute both the polyhedral certificate and the Randomized Smoothing certificate, and compare their volumes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational inefficiency of calculating the exact ℓ2 certified radius for the classifier gλ be addressed, particularly for high-dimensional data?
- Basis in paper: [explicit] The paper discusses the computational difficulty of solving an optimization problem with a large number of constraints, which corresponds to the number of extreme points of the polyhedron F(x), to compute the exact ℓ2 certified radius for the classifier gλ.
- Why unresolved: The exact calculation of the certified radius is computationally intensive due to the large number of constraints, especially in high dimensions, making it impractical for real-world applications.
- What evidence would resolve it: Developing more efficient algorithms or approximation methods to calculate the certified radius, or demonstrating the effectiveness of such methods in practice.

### Open Question 2
- Question: Can the framework for constructing robust classifiers based on data concentration be extended to data distributions that do not lie on low-dimensional linear subspaces, such as general manifolds or more complex structures?
- Basis in paper: [inferred] The paper focuses on data concentrated on low-dimensional linear subspaces and constructs robust classifiers for such distributions. However, real-world data often lies on more complex manifolds or structures.
- Why unresolved: The current theoretical framework is limited to low-dimensional linear subspaces, and extending it to more general cases is an open challenge.
- What evidence would resolve it: Extending the theoretical framework to handle general manifolds or complex structures, and demonstrating its effectiveness on real-world datasets with such distributions.

### Open Question 3
- Question: How can the performance of the proposed method be improved by incorporating techniques developed for improving Randomized Smoothing (RS) performance, such as better noise distributions or more efficient sampling strategies?
- Basis in paper: [explicit] The paper mentions that the objective in combining their method with RS was simply to explore RS as a method for obtaining an ℓ2 certificate, and they did not tune their method or RS for performance. They also suggest that various tricks developed in the literature for improving RS performance could be employed to improve the results.
- Why unresolved: The potential improvements in performance by incorporating RS techniques are not explored in the paper, leaving room for further investigation.
- What evidence would resolve it: Conducting experiments that incorporate various RS techniques and comparing the results to the current method, demonstrating the effectiveness of the improvements.

## Limitations

- Theoretical results rely on strong assumptions about data concentration and subspace structure that may not hold in practice
- Empirical evaluation is limited to MNIST, a relatively simple dataset with well-separated classes
- Computational complexity of solving the dual optimization problem could be prohibitive for large-scale applications
- Method's performance on more complex datasets with natural image distributions remains unknown

## Confidence

**High Confidence**: The theoretical characterization that concentration is necessary for robustness is mathematically sound given the stated assumptions. The geometric analysis of the dual problem and the connection to polyhedral certificates follows established convex optimization principles.

**Medium Confidence**: The sufficiency result for strong concentration relies on ideal conditions (non-overlapping expanded regions) that may be difficult to achieve in practice. The empirical results on MNIST are promising but limited in scope.

**Low Confidence**: The claim that data lying near low-dimensional subspaces naturally leads to robust classifiers is theoretically interesting but practically uncertain, as real-world data distributions rarely conform to such idealized structures.

## Next Checks

1. **Generalization to Complex Datasets**: Test the method on CIFAR-10 or ImageNet to evaluate whether the concentration and subspace assumptions hold for natural images, and measure the degradation in certification performance compared to MNIST.

2. **Computational Scalability Analysis**: Profile the computational cost of solving the dual optimization problem for varying numbers of training points and constraint sets, and identify practical limits on dataset size.

3. **Attack Model Complementarity**: Design experiments to quantify the complementary nature of polyhedral vs. Randomized Smoothing certificates across different attack models (ℓ2, ℓ∞, PGD), measuring both coverage and robustness gains in regions where each method excels.