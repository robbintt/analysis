---
ver: rpa2
title: 'CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking
  on HumanEval-X'
arxiv_id: '2303.17568'
source_url: https://arxiv.org/abs/2303.17568
tags:
- codegeex
- code
- generation
- language
- pass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodeGeeX is a multilingual code generation model with 13 billion
  parameters, pre-trained on 850 billion tokens of 23 programming languages. It demonstrates
  consistent outperformance over multilingual baselines of similar scale for both
  code generation and translation tasks on the HumanEval-X benchmark.
---

# CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X

## Quick Facts
- arXiv ID: 2303.17568
- Source URL: https://arxiv.org/abs/2303.17568
- Reference count: 34
- Key outcome: CodeGeeX is a multilingual code generation model with 13 billion parameters, pre-trained on 850 billion tokens of 23 programming languages, demonstrating consistent outperformance over multilingual baselines on HumanEval-X benchmark.

## Executive Summary
CodeGeeX is a large-scale multilingual code generation model designed to address the limitations of monolingual approaches by supporting 23 programming languages. Pre-trained on an extensive corpus of 850 billion tokens, the model demonstrates superior performance on both code generation and translation tasks, outperforming comparable multilingual baselines on the HumanEval-X benchmark. The model has been successfully deployed in popular development environments including VS Code, JetBrains, and Cloud Studio, serving tens of thousands of active users weekly. User studies indicate that 83.4% of users report improved coding efficiency when using CodeGeeX.

## Method Summary
CodeGeeX follows the GPT architecture with 13 billion parameters, utilizing a 39-layer transformer decoder with 5120 hidden size and 40 attention heads. The model was pre-trained on 850 billion tokens from 23 programming languages using a combination of Pile and CodeParrot datasets. Training employed an 8-way model parallel and 192-way data parallel configuration on 1,536 Ascend 910 AI processors, with a batch size of 3,072 and Adam optimizer with cosine learning rate decay. Language-specific tags were prepended to code segments to help the model distinguish between different programming languages during training.

## Key Results
- CodeGeeX achieves pass@1 scores of 34.7% and pass@10 scores of 58.9% on the HumanEval-X benchmark for code generation
- For code translation, CodeGeeX reaches pass@1 of 47.5% and pass@10 of 71.5% on HumanEval-X
- User studies show 83.4% of developers report improved coding efficiency when using CodeGeeX
- The model demonstrates consistent outperformance over multilingual baselines of similar scale across all evaluated tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on a large, multilingual code corpus enables CodeGeeX to develop generalized code understanding and generation capabilities across 23 programming languages.
- Mechanism: By processing over 850 billion tokens from diverse programming languages, the model learns universal programming patterns, syntactic structures, and semantic relationships that transfer across languages.
- Core assumption: Code patterns and problem-solving approaches are transferable across programming languages at the semantic level.
- Evidence anchors:
  - [abstract] "CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022."
  - [section 2.2] "The training corpus contains two parts... CodeParrot is another public Python dataset from BigQuery."
- Break condition: If the model shows significant performance degradation on languages with different paradigms (e.g., functional vs. imperative), the cross-language generalization assumption may be invalid.

### Mechanism 2
- Claim: The large-scale transformer architecture with 13 billion parameters is essential for capturing the complexity of multilingual code generation tasks.
- Mechanism: The 39-layer transformer decoder with 5120 hidden size and 40 attention heads provides sufficient capacity to model the diverse syntax and semantics across 23 programming languages simultaneously.
- Core assumption: Multilingual code generation requires substantially more parameters than monolingual approaches due to the increased complexity of modeling multiple language paradigms.
- Evidence anchors:
  - [abstract] "CodeGeeX is a multilingual code generation model with 13 billion parameters"
  - [section 2.1] "CodeGeeX follows the generative pre-training (GPT) architecture... The core architecture of CodeGeeX is a 39-layer transformer decoder."
- Break condition: If performance plateaus or degrades when scaling to more languages, it may indicate that the model capacity is insufficient or that architectural changes are needed.

### Mechanism 3
- Claim: Language-specific tags prepended to code segments help the model distinguish and generate appropriate code for different programming languages.
- Mechanism: By adding tags like "# language: Python" before each code segment, the model learns to associate specific syntactic and semantic patterns with their corresponding languages, improving generation accuracy.
- Core assumption: Explicit language identification improves the model's ability to maintain language-specific conventions and avoid cross-language contamination.
- Evidence anchors:
  - [section 2.2] "To help the model distinguish between multiple languages, we add a language-specific tag before each segment in the form of [Comment sign]language: [LANG]"
  - [section 4.2] "For all baselines, we use t = 0.2,p = 0.95 for pass@1, t = 0.8,p = 0.95 for pass@10 and pass@100"
- Break condition: If the model shows improved performance when language tags are removed, it may indicate that the model has learned to infer language context from the code itself.

## Foundational Learning

- Concept: Tokenization and vocabulary design
  - Why needed here: Proper tokenization is critical for handling the diverse vocabulary across 23 programming languages and ensuring efficient model training and inference.
  - Quick check question: How does CodeGeeX handle whitespace and special characters across different programming languages?

- Concept: Cross-entropy loss optimization for autoregressive generation
  - Why needed here: Understanding how the model is trained to predict the next token sequentially is essential for debugging generation issues and improving model performance.
  - Quick check question: What is the relationship between the cross-entropy loss and the model's ability to generate syntactically correct code?

- Concept: Budget allocation strategies for multilingual generation
  - Why needed here: Effective allocation of generation budgets across multiple languages can significantly improve problem-solving performance compared to single-language approaches.
  - Quick check question: How does the weighted budget allocation strategy differ from uniform allocation, and when should each be used?

## Architecture Onboarding

- Component map:
  Input tokens -> GPT-2 BPE tokenizer (52,224 vocab) -> Embedding layer -> 39-layer transformer decoder (5120 hidden size, 40 attention heads, FastGELU) -> Top query layer -> Output probability distribution -> Sampling/decoding -> Detokenization

- Critical path:
  1. Input tokenization and embedding
  2. 39-layer transformer processing
  3. Top query layer attention
  4. Output probability calculation
  5. Decoding (sampling/greedy/beam search)
  6. Detokenization to code

- Design tradeoffs:
  - Model size vs. inference efficiency: 13B parameters provide strong performance but require quantization and acceleration techniques for practical deployment
  - Tokenization granularity: BPE tokenization balances vocabulary size with ability to handle rare tokens across languages
  - Language tag inclusion: Improves language-specific generation but adds preprocessing overhead

- Failure signatures:
  - Syntax errors: May indicate tokenizer issues or insufficient language-specific training data
  - Runtime errors: Could suggest logical errors in generated code or inadequate test case coverage
  - Semantic errors: Might indicate the model learned incorrect patterns from training data

- First 3 experiments:
  1. Test generation performance on a simple Python function with different temperature settings (0.2, 0.5, 0.8) to understand the exploration-exploitation tradeoff
  2. Evaluate code generation accuracy for a single language (e.g., Python) vs. multilingual generation to measure cross-language impact
  3. Measure inference speed and memory usage with and without quantization on different GPU types (RTX 3090 vs. A100) to understand deployment requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does multilingual pre-training impact the model's ability to generalize across programming languages, and what are the specific challenges in achieving high-level programming abstraction?
- Basis in paper: [explicit] The paper discusses the potential of multilingual models to solve problems across different languages and the challenges in achieving high-level programming abstraction.
- Why unresolved: While the paper shows that multilingual models like CodeGeeX can perform well across languages, it doesn't fully explore the mechanisms behind this generalization or the specific challenges in achieving programming abstraction.
- What evidence would resolve it: Experiments comparing the performance of monolingual and multilingual models on tasks requiring high-level programming abstraction, along with ablation studies to identify key factors contributing to generalization.

### Open Question 2
- Question: What are the trade-offs between exploration and exploitation in code generation under fixed budgets, and how do different budget allocation strategies impact the model's performance?
- Basis in paper: [explicit] The paper discusses the trade-off between exploration and exploitation in code generation and compares different budget allocation strategies.
- Why unresolved: The paper provides insights into the trade-off and compares strategies, but a comprehensive analysis of the impact of different strategies on various tasks and datasets is needed.
- What evidence would resolve it: Extensive experiments evaluating the performance of different budget allocation strategies on diverse code generation tasks and datasets, along with theoretical analysis of the trade-offs involved.

### Open Question 3
- Question: How does the model's capacity affect its multilingual programming ability, and what is the optimal model size for balancing performance and computational efficiency?
- Basis in paper: [inferred] The paper suggests that model capacity is essential for multilingual programming ability, as evidenced by the comparison between CodeGeeX and larger models like PaLM-Coder.
- Why unresolved: While the paper indicates the importance of model capacity, it doesn't provide a detailed analysis of the relationship between model size and performance, or the optimal size for balancing performance and efficiency.
- What evidence would resolve it: Systematic experiments varying the model size and evaluating its impact on multilingual programming performance, along with analysis of computational requirements and efficiency trade-offs.

## Limitations

- Evaluation relies heavily on the HumanEval-X benchmark, which may not fully capture real-world coding scenarios across all 23 supported languages
- Languages with smaller representation in training data (Fortran, R, CUDA) show lower performance due to insufficient data coverage
- Effectiveness in handling complex multi-file projects or large-scale software engineering tasks remains untested

## Confidence

**High Confidence**: The core architectural claims regarding the 13B parameter transformer decoder and its training methodology are well-supported by the detailed technical specifications provided in section 2.1 and 2.2.

**Medium Confidence**: The claimed performance improvements over baseline models are supported by the HumanEval-X benchmark results, but the lack of ablation studies makes it difficult to isolate which specific design choices contribute most to the observed improvements.

**Low Confidence**: The user study findings (83.4% reporting improved coding efficiency) are based on subjective self-reporting without standardized measurement protocols, making the quantitative claims less reliable.

## Next Checks

1. **Ablation Study on Language Tags**: Remove language-specific tags from the input preprocessing pipeline and evaluate pass@k scores on HumanEval-X to determine whether the tags genuinely improve multilingual generation or if the model has learned to infer language context from the code itself.

2. **Performance Scaling Analysis**: Systematically evaluate model performance as the number of supported languages increases from 5 to 23, measuring pass@k scores and inference latency to identify the point of diminishing returns for multilingual training.

3. **Real-World Usage Validation**: Conduct a controlled study comparing CodeGeeX-assisted coding sessions against unassisted coding for complex programming tasks (multi-file projects, API integration, algorithm implementation) with objective metrics like completion time, bug count, and code quality scores.