---
ver: rpa2
title: Art Authentication with Vision Transformers
arxiv_id: '2307.03039'
source_url: https://arxiv.org/abs/2307.03039
tags:
- contrast
- authentication
- swin
- standard
- gogh
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study compares Convolutional Neural Networks (CNNs) and Vision
  Transformers (ViTs) for art authentication, using Vincent van Gogh paintings as
  a case study. The researchers compiled a dataset of 654 authentic van Gogh paintings
  and two contrast sets: one with imitations and proxies, and another with imitations
  only.'
---

# Art Authentication with Vision Transformers

## Quick Facts
- **arXiv ID:** 2307.03039
- **Source URL:** https://arxiv.org/abs/2307.03039
- **Reference count:** 40
- **Primary result:** Vision Transformers match or exceed CNNs for art authentication, with Swin-Tiny showing superior performance on imitation detection.

## Executive Summary
This study compares Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) for art authentication using Vincent van Gogh paintings as a case study. The researchers compiled a dataset of 654 authentic van Gogh paintings and two contrast sets: one with imitations and proxies, and another with imitations only. They evaluated EfficientNetB5, ResNet101, Swin-Tiny, and Swin-Base architectures. With the standard contrast set, EfficientNetB5 achieved the best overall performance (accuracy 0.925 on paintings). However, with the refined contrast set containing only imitations, Swin-Tiny showed superior performance (accuracy 0.858 on paintings, precision 0.818, recall 0.802). The results demonstrate that Vision Transformers are at least as viable for art authentication as CNNs, with potential advantages in detecting artistic imitations.

## Method Summary
The study employs transfer learning with pre-trained CNN and ViT models fine-tuned on a dataset of van Gogh paintings. Images are processed into 256×256 patches, with some models requiring bicubic downsampling to 224×224 input size. Four architectures are evaluated: EfficientNetB5, ResNet101 (CNNs), and Swin-Tiny and Swin-Base (ViTs). Training uses binary cross-entropy loss with optional imitation sample weighting (wim=10) to address class imbalance. The Adam optimizer is used with learning rate 0.0001 and early stopping (patience=20, min_delta=0.001). Evaluation includes both patch-level and painting-level accuracy, with painting predictions derived from the mean of patch predictions.

## Key Results
- EfficientNetB5 achieved the best overall performance (accuracy 0.925 on paintings) when proxies were included in the contrast set.
- Swin-Tiny showed superior performance on the imitation-only contrast set (accuracy 0.858 on paintings, precision 0.818, recall 0.802).
- Swin-Base did not outperform Swin-Tiny despite larger pre-training on 22K ImageNet classes.
- ViTs matched or exceeded CNN performance for imitation detection, demonstrating their viability for art authentication tasks.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Vision Transformers (ViTs) outperform CNNs in detecting fine-grained artistic imitations when trained on a contrast set that excludes stylistically dissimilar proxies.
- **Mechanism:** ViTs' self-attention layers allow long-range contextual dependencies to be captured across the image, which is crucial for distinguishing subtle stylistic differences in imitations versus authentic works.
- **Core assumption:** Imitations contain visual patterns that are closer to the authentic style than proxies, so the contrast task requires higher sensitivity to local and global compositional features.
- **Evidence anchors:**
  - [abstract] "With a contrast set that only consists of imitations, we find the Swin Transformer to be superior to EfficientNet by achieving an authentication accuracy of over 85%."
  - [section] "Clearly, art authentication requires a fine distinction between imitations and authentic art... the performance on the imitations is considerably lower, despite the use of sample weights."
- **Break condition:** If the imitation dataset contains too few stylistic variations, ViTs may overfit to minor artifacts and fail to generalize.

### Mechanism 2
- **Claim:** CNNs (EfficientNetB5) perform better than ViTs when the contrast set includes proxies from related artistic movements, because CNNs can effectively learn broad stylistic separations.
- **Mechanism:** CNNs' hierarchical feature extraction naturally separates artworks by coarser stylistic cues (brushwork, composition), making them strong at distinguishing van Gogh from his contemporaries.
- **Core assumption:** Proxies differ enough in style from van Gogh that a CNN can learn a discriminative boundary without needing the fine-grained attention of ViTs.
- **Evidence anchors:**
  - [section] "Both Swin architectures yield a considerable improvement in performance regarding ResNet101... but EfficientNetB5 yields the best art-authentication performance" when proxies are present.
  - [section] "These results show the performances obtained by all architectures mainly reflect a successful separation of authentic paintings and artworks by proxies."
- **Break condition:** If proxies are too stylistically similar to van Gogh, CNNs' coarser separation will fail and ViTs' finer-grained attention will dominate.

### Mechanism 3
- **Claim:** The choice of pre-training dataset size matters: Swin-Base (trained on 22K ImageNet classes) does not outperform Swin-Tiny (1K classes) in this domain.
- **Mechanism:** Domain-specific fine-tuning on art images overrides benefits from larger pre-training vocabularies, suggesting that architectural inductive bias matters more than raw pre-training scale for this task.
- **Core assumption:** The distribution shift between natural images and artworks is large enough that the extra pre-training classes provide diminishing returns.
- **Evidence anchors:**
  - [section] "Swin-Base is pre-trained on the 22K version of ImageNet... Although the Swin-Base Transformer performs marginally better than the Swin-Tiny Transformer on patches, it does not result in a better performance on paintings."
- **Break condition:** If the art dataset were much larger or more diverse, the 22K pre-training might begin to show advantages.

## Foundational Learning

- **Concept:** Convolutional neural network (CNN) architecture and feature hierarchy
  - Why needed here: CNNs like ResNet101 and EfficientNetB5 rely on hierarchical convolutional layers to extract increasingly abstract visual features; understanding this helps explain why they excel at separating broad stylistic groups.
  - Quick check question: What is the main difference between a standard CNN and a residual CNN like ResNet?

- **Concept:** Vision Transformer self-attention mechanism
  - Why needed here: ViTs like Swin use self-attention over shifted windows to capture long-range dependencies; knowing this clarifies why they can outperform CNNs on imitation detection.
  - Quick check question: How does a Swin Transformer's shifted window attention differ from standard self-attention?

- **Concept:** Transfer learning and fine-tuning strategies
  - Why needed here: All models are pre-trained on ImageNet and fine-tuned on art data; understanding freezing vs unfreezing layers is key to replicating the results.
  - Quick check question: What is the difference between freezing the base model and fine-tuning all layers during transfer learning?

## Architecture Onboarding

- **Component map:** Image preprocessing (256×256 patches) → Normalization → Model zoo (EfficientNetB5/ResNet101/Swin-Tiny/Swin-Base) → Dense layer with He normal init → Binary cross-entropy loss → Adam optimizer → Early stopping → Evaluation (patch-level and painting-level metrics)

- **Critical path:**
  1. Prepare and patch dataset (split paintings into patches, assign all patches of a painting to same partition).
  2. Load pre-trained model, define top dense layer with He normal init.
  3. Train with binary cross-entropy, sample weighting if using standard contrast set.
  4. Evaluate on test patches and aggregate to painting predictions.
  5. Compare per-class accuracy (authentic vs imitations vs proxies).

- **Design tradeoffs:**
  - Using patches gives fine-grained brushstroke features but requires aggregation for painting-level decisions.
  - Sample weighting helps when proxies outnumber imitations but can bias the model if not tuned.
  - Larger ViT (Swin-Base) adds parameters but may not improve painting-level performance due to domain shift.

- **Failure signatures:**
  - High patch accuracy but low painting accuracy → patch aggregation method flawed.
  - Strong performance on proxies but weak on imitations → dataset imbalance or proxy-proxy confusion.
  - Overfitting to sample weights → check validation loss trend.

- **First 3 experiments:**
  1. Train EfficientNetB5 on standard contrast set with wim=10, report patch and painting accuracy.
  2. Train Swin-Tiny on same data, compare patch-level performance to EfficientNetB5.
  3. Train Swin-Tiny on refined contrast set (imitations only), evaluate imitation accuracy improvement.

## Open Questions the Paper Calls Out

- **Question:** Do Vision Transformers generalize better than CNNs for art authentication across different artists and art styles?
  - **Basis in paper:** [explicit] The authors note that further tests should be carried out to determine the generalizability of their results to other artists' datasets.
  - **Why unresolved:** The study focused solely on Vincent van Gogh paintings. The authors explicitly call for testing on other artists to validate if the observed advantages of ViTs are universal or artist-specific.
  - **What evidence would resolve it:** Conducting similar experiments with artworks from different artists, periods, and styles to compare the performance of CNNs and ViTs across diverse datasets.

- **Question:** How do different image acquisition conditions (resolution, lighting, scale) affect the performance of ViTs and CNNs in art authentication?
  - **Basis in paper:** [inferred] The authors mention limitations arising from the digital nature of training images and stress the importance of developing methodologies that can achieve invariance to different camera acquisitions, resolutions, and scales.
  - **Why unresolved:** The current study does not address how variations in image quality and acquisition conditions impact model performance, which is crucial for real-world applications.
  - **What evidence would resolve it:** Evaluating model performance on datasets with controlled variations in resolution, lighting, and scale to determine robustness to image acquisition differences.

- **Question:** Can incorporating contextual information (e.g., provenance, historical context, scientific analysis) improve the accuracy of art authentication models?
  - **Basis in paper:** [inferred] The authors suggest exploring incorporating contextual information into the models, potentially leveraging multi-modality and textual guidance.
  - **Why unresolved:** The current study relies solely on optical information from images, while human connoisseurs consider contextual information. The potential benefits of integrating such information into machine learning models are unexplored.
  - **What evidence would resolve it:** Developing and testing multimodal models that incorporate both visual and contextual data to assess improvements in authentication accuracy compared to image-only models.

## Limitations

- The study is limited to a single artist (van Gogh), making it unclear if results generalize to other artists or art styles.
- Dataset construction methodology is not fully detailed, making exact replication challenging.
- Performance on imitations is notably lower than on proxies, suggesting potential class imbalance or proxy-proxy confusion.

## Confidence

- **High confidence:** CNNs outperform ViTs when proxies are present in the contrast set; ViTs match or exceed CNNs for imitation-only detection.
- **Medium confidence:** Swin-Tiny consistently outperforms Swin-Base despite larger pre-training, suggesting architectural bias matters more than pre-training scale for this task.
- **Low confidence:** Generalization of these findings to other artists or art styles beyond van Gogh.

## Next Checks

1. Replicate the study with a multi-artist dataset (e.g., Impressionists including van Gogh, Monet, and Renoir) to test cross-style generalization.
2. Perform ablation studies on patch aggregation methods to isolate whether painting-level performance gains come from the model or the ensemble strategy.
3. Test the same models on a held-out imitation dataset with known provenance to verify that sample weighting doesn't artificially inflate imitation detection metrics.