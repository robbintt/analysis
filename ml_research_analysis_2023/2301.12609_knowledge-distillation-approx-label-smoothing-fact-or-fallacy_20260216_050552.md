---
ver: rpa2
title: 'Knowledge Distillation $\approx$ Label Smoothing: Fact or Fallacy?'
arxiv_id: '2301.12609'
source_url: https://arxiv.org/abs/2301.12609
tags:
- training
- entropy
- self-kd
- reverse
- teacher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares knowledge distillation (KD) and label smoothing
  (LS) in the context of text classification. The study investigates whether KD is
  a form of regularization, as suggested by its similarities with LS.
---

# Knowledge Distillation $\approx$ Label Smoothing: Fact or Fallacy?

## Quick Facts
- arXiv ID: 2301.12609
- Source URL: https://arxiv.org/abs/2301.12609
- Authors: 
- Reference count: 20
- Key outcome: KD reduces model entropy while LS increases it, contradicting claims that KD is merely a regularization technique

## Executive Summary
This paper investigates whether knowledge distillation (KD) functions similarly to label smoothing (LS) as a regularization technique in text classification. Through experiments on four GLUE tasks using BERT and DistilBERT models, the authors demonstrate that KD and LS have fundamentally different effects on model uncertainty (entropy). While LS consistently increases entropy by smoothing targets toward uniform distributions, KD typically reduces entropy by transferring the teacher's confidence to the student. The study reveals that in KD, the student not only inherits knowledge but also the teacher's predictive uncertainty, reinforcing the knowledge transfer view rather than a regularization perspective.

## Method Summary
The study compares ERM, KD, and LS across four GLUE text classification tasks (MRPC, QNLI, SST-2, MNLI) using BERT-base and DistilBERT-base architectures. For each task, models are trained with exhaustive hyperparameter search over learning rates, epochs, and α values. Three KD variants are tested: standard (BERT→DistilBERT), self-KD, and reverse KD (DistilBERT→BERT). Posterior entropy and accuracy are evaluated on test sets, with validation-based hyperparameter selection. The paper examines entropy trends across training epochs and compares how KD and LS affect model confidence differently.

## Key Results
- KD reduces posterior entropy in most settings while LS increases it, contradicting the regularization hypothesis
- In KD, students inherit not only knowledge but also predictive uncertainty from teachers
- Reverse KD (smaller teacher→larger student) generally increases entropy compared to ERM
- KD and LS drive model uncertainty in completely opposite directions despite both smoothing target distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KD reduces posterior entropy by transferring teacher confidence, while LS increases entropy through uniform smoothing
- Core assumption: Teacher's predictive uncertainty is lower than student's initial uncertainty and can be transferred
- Evidence anchors:
  - [abstract] "In most settings, KD and LS drive model uncertainty (entropy) in completely opposite directions"
  - [section] "Both KD variants...bring entropy down; the only exception to this is self-KD on MNLI"
  - [corpus] Weak: No direct evidence about entropy reduction in corpus

### Mechanism 2
- Claim: Student's predictive uncertainty in KD is a direct function of teacher's uncertainty, reinforcing knowledge transfer
- Core assumption: Teacher's posterior distribution captures task-specific knowledge that influences student confidence
- Evidence anchors:
  - [abstract] "In KD, the student inherits not only its knowledge but also its confidence from the teacher"
  - [section] "we see in the above results that it also inherits its predictive uncertainty"
  - [corpus] Weak: No direct evidence about inheritance of confidence in corpus

### Mechanism 3
- Claim: Reverse KD increases entropy because student learns from a less confident teacher
- Core assumption: Teacher uncertainty is higher than student's initial uncertainty when teacher is smaller
- Evidence anchors:
  - [section] "when trained using ERM, BERT classifiers have a lower posterior entropy than their smaller DistilBERT counterparts"
  - [section] "Reverse KD...generally results in a higher entropy than ERM"
  - [corpus] Weak: No direct evidence about capacity mismatch in corpus

## Foundational Learning

- Concept: Entropy as a measure of predictive uncertainty
  - Why needed here: The paper's core claim compares how KD and LS affect model uncertainty via entropy changes
  - Quick check question: What happens to entropy when a model's predictions become more uniform across classes?

- Concept: Kullback-Leibler (KL) divergence in loss functions
  - Why needed here: Both KD and LS use KL divergence terms that determine how student/posterior distributions are shaped
  - Quick check question: In the KD loss, what does minimizing DKL(pt,p) achieve when pt is more confident than p?

- Concept: Temperature scaling in KD
  - Why needed here: The paper tests KD at τ=1 but mentions temperature as a hyperparameter that could affect smoothing
  - Quick check question: How does increasing temperature affect the sharpness of teacher probabilities in KD?

## Architecture Onboarding

- Component map: GLUE datasets -> BERT-base (teacher), DistilBERT-base (student) -> ERM/KD/LS training loops -> Posterior entropy and accuracy evaluation

- Critical path:
  1. Load dataset splits (train/validation/test)
  2. Train teacher with ERM
  3. For each method, run hyperparameter search over learning rate, epochs, α (and τ for KD)
  4. Select best model on validation set
  5. Evaluate entropy and accuracy on test set
  6. Compare entropy trends across training epochs

- Design tradeoffs:
  - Single seed vs. multiple seeds: Paper uses single seed for scalability but acknowledges reliability trade-off
  - Temperature inclusion: Tested τ=1 primarily, with extended experiments showing qualitative robustness
  - Model capacity pairing: Standard KD (BERT→DistilBERT) vs. Reverse KD (DistilBERT→BERT) reveals entropy behavior differences

- Failure signatures:
  - If KD increases entropy instead of decreasing it, likely due to teacher having higher uncertainty than student
  - If LS does not increase entropy as expected, check implementation of smoothing (α values, correct target distribution)
  - If KD performs worse than ERM, may indicate insufficient teacher quality or inappropriate hyperparameter settings

- First 3 experiments:
  1. Reproduce MRPC entropy comparison: ERM vs. LS vs. KD with DistilBERT, verify LS increases entropy ~3-6× while KD reduces it
  2. Test reverse KD setup: DistilBERT teacher → BERT student, check if entropy increases as predicted
  3. Vary teacher capacity: Compare KD from BERT vs. DistilBERT into same student size, observe entropy differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does knowledge distillation reduce or increase model entropy, and how do teacher-student capacity ratios influence this relationship?
- Basis in paper: [explicit] The paper shows that KD's effect on entropy varies depending on the relative capacities of the teacher and student models
- Why unresolved: The paper demonstrates the phenomenon but doesn't provide a theoretical framework explaining why these capacity ratios lead to different entropy outcomes
- What evidence would resolve it: Systematic experiments varying teacher-student capacity ratios across multiple tasks, combined with theoretical analysis of how information flow affects entropy in KD

### Open Question 2
- Question: How does the temperature parameter τ in knowledge distillation affect the relationship between student uncertainty and teacher uncertainty across different tasks and model architectures?
- Basis in paper: [explicit] The paper includes experiments with varying τ values but notes that results remain qualitatively similar
- Why unresolved: The paper shows that optimal τ varies by task and method but doesn't explore how τ systematically affects the entropy relationship between teachers and students
- What evidence would resolve it: Extensive experiments varying τ across a wider range of values, tasks, and architectures, with analysis of how it affects the student's ability to match the teacher's uncertainty profile

### Open Question 3
- Question: What is the fundamental mechanism by which knowledge distillation reduces overfitting compared to label smoothing, given that both methods smooth target distributions?
- Basis in paper: [explicit] The paper shows that while both KD and LS smooth targets, only LS consistently increases entropy
- Why unresolved: The paper demonstrates that KD and LS have opposite effects on entropy but doesn't explain the underlying mechanism that makes KD more effective at preventing overfitting
- What evidence would resolve it: Controlled experiments isolating the information content in teacher predictions versus the smoothing effect, combined with analysis of how this information guides learning versus simply regularizing confidence

## Limitations
- Entropy comparisons rely on a single random seed, introducing uncertainty about result stability
- Reverse KD experiments show weaker patterns than standard KD, suggesting the entropy-increase claim may be more nuanced
- Study focuses on BERT-family models in text classification, limiting generalizability to other architectures or domains

## Confidence
- High Confidence: KD reduces posterior entropy compared to ERM in standard teacher-student settings (BERT→DistilBERT)
- Medium Confidence: LS increases entropy compared to ERM, as this pattern holds across experiments
- Medium Confidence: The student inherits predictive uncertainty from the teacher in KD, though the reverse KD results suggest this relationship may be more complex

## Next Checks
1. Run each experiment with 3-5 different random seeds to establish confidence intervals for entropy measurements and verify that KD consistently reduces entropy across seeds
2. Test KD with temperature scaling (τ > 1) to verify that entropy reduction is not an artifact of τ=1 configuration and to explore the full parameter space
3. Evaluate KD with teacher-student pairs of similar capacity (e.g., BERT-base → BERT-small) to test whether the entropy reduction pattern holds when capacity differences are smaller