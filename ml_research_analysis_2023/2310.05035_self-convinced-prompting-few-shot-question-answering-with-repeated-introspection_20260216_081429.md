---
ver: rpa2
title: 'Self-Convinced Prompting: Few-Shot Question Answering with Repeated Introspection'
arxiv_id: '2310.05035'
source_url: https://arxiv.org/abs/2310.05035
tags:
- answer
- have
- choices
- then
- convincer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Self-Convinced Prompting introduces a framework that iteratively
  improves the reasoning and answers of large language models by using the model itself
  to detect errors, analyze mistakes, and generate corrected solutions. The method
  processes the output of a chain-of-thought prompt through three modules: Normal
  CoT, a Convincer, and an Answerer.'
---

# Self-Convinced Prompting: Few-Shot Question Answering with Repeated Introspection

## Quick Facts
- arXiv ID: 2310.05035
- Source URL: https://arxiv.org/abs/2310.05035
- Authors: [Authors not specified]
- Reference count: 40
- Key outcome: Self-Convinced Prompting introduces a framework that iteratively improves the reasoning and answers of large language models by using the model itself to detect errors, analyze mistakes, and generate corrected solutions.

## Executive Summary
Self-Convinced Prompting is a framework for improving few-shot question answering by iteratively refining the reasoning and answers of large language models (LLMs). The method builds on chain-of-thought prompting, adding two additional modules—Convincer and Answerer—that allow the model to detect errors in its own reasoning and provide hints for correction. The framework is tested on seven diverse datasets and shows substantial accuracy improvements over baseline methods. Experimental results indicate that the iterative process is especially effective for harder problems.

## Method Summary
The framework processes the output of a typical few-shot chain-of-thought prompt through three modules: Normal CoT, a Convincer, and an Answerer. The Normal CoT module generates an initial reasoning path and answer. The Convincer module inspects this output, detects the first error in the reasoning, and provides a corrected reasoning path truncated at the error point. The Answerer then generates a type hint (e.g., "Algebraic Equation") and an intermediate answer, which are prepended to the input for the next Normal CoT iteration. This loop repeats until the answer is deemed correct or a maximum number of iterations is reached. The method relies on in-context learning and avoids explicit model training.

## Key Results
- Self-Convince achieves state-of-the-art performance on AQuA and SV AMP datasets.
- Iterative refinement yields substantial accuracy improvements over baseline CoT methods across seven diverse benchmarks.
- Ablation studies demonstrate the importance of each module, with larger gains observed on harder tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-Convince improves LLM reasoning by iteratively identifying and correcting the first error in a reasoning chain, then rebuilding the solution.
- Mechanism: The Convincer module inspects the Normal CoT output, detects the first incorrect step, and outputs a corrected reasoning path truncated at that point. The Answerer then provides additional context (e.g., problem type), and a new Normal CoT completes the reasoning. This loop repeats until correctness is achieved.
- Core assumption: LLMs can reliably detect and fix their own reasoning errors when explicitly prompted to introspect.
- Evidence anchors:
  - [abstract]: "processes the output of a typical few-shot chain-of-thought prompt, assesses the correctness of the response, scrutinizes the answer, refines the reasoning, and ultimately produces a new solution."
  - [section]: "The Convincer possesses the ability to identify errors within the original reasoning path r, conduct an analysis of the mistakes, and subsequently rectify them."
  - [corpus]: Weak - no corpus entries directly address introspection-based error correction; assumption not externally validated.
- Break condition: If the Convincer fails to detect errors or consistently produces incorrect corrections, the loop cannot converge and performance degrades.

### Mechanism 2
- Claim: Providing type hints (e.g., "Algebraic Equation") via the Answerer improves subsequent reasoning accuracy.
- Mechanism: After the Convincer outputs a corrected reasoning path, the Answerer generates a "Type" label and an intermediate answer. This type hint is then prepended to the input of the next Normal CoT, guiding the model toward the correct problem-solving approach.
- Core assumption: Explicit type labels help LLMs focus on the relevant reasoning pattern for the problem.
- Evidence anchors:
  - [abstract]: "The Answerer then provides an answer based on the rectified reasoning path, along with self-hinted question type information."
  - [section]: "The Answerer module appends pertinent information, such as 'Type: Algebraic Equation', following the answer derived from the previous step."
  - [corpus]: Weak - no corpus entries directly support the effectiveness of type hints in iterative prompting; assumption not externally validated.
- Break condition: If the model ignores or misinterprets the type hint, the guidance becomes ineffective and may even mislead reasoning.

### Mechanism 3
- Claim: Iterative refinement converges to correct answers faster on harder problems than on simpler ones.
- Mechanism: The framework's iterative loop (Convincer → Answerer → Normal CoT) is applied repeatedly. Harder problems (e.g., AQuA) show larger accuracy gains because the Convincer is more likely to detect errors that the model can then correct.
- Core assumption: LLMs are more likely to make detectable errors on complex reasoning tasks, and these errors are correctable via self-introspection.
- Evidence anchors:
  - [abstract]: "achieving the state-of-the-art performance on AQuA and SV AMP datasets."
  - [section]: "Table 1 reveals that, for arithmetic benchmarks, the improvement relative to Manual CoT becomes more pronounced as the task difficulty increases."
  - [corpus]: Weak - no corpus entries provide comparative difficulty analysis for iterative prompting; assumption based on internal ablation results.
- Break condition: If errors are too subtle for the Convincer to detect, or if the model's self-correction is insufficient, iterations will not improve accuracy.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Self-Convince builds on CoT by adding introspection and correction layers; understanding CoT is essential to grasp how the framework modifies reasoning.
  - Quick check question: What is the primary purpose of CoT prompting in few-shot learning?
- Concept: In-Context Learning (ICL)
  - Why needed here: ICL enables LLMs to perform tasks using few examples without parameter updates; Self-Convince relies on ICL-style prompts for all modules.
  - Quick check question: How does ICL differ from fine-tuning in LLM adaptation?
- Concept: Error detection and correction in reasoning
  - Why needed here: The Convincer's core function is to detect and fix reasoning errors; understanding this mechanism is key to implementing or debugging Self-Convince.
  - Quick check question: What are the two main outputs of the Convincer module?

## Architecture Onboarding

- Component map: Normal CoT -> Convincer -> Answerer -> Normal CoT (loop)
- Critical path: Normal CoT → Convincer → Answerer → Normal CoT (loop)
- Design tradeoffs:
  - Accuracy vs. iteration count: More iterations can improve accuracy but increase latency.
  - Prompt complexity vs. reliability: Complex prompts may improve error detection but risk model confusion.
  - Type hints vs. generality: Type hints help in narrow domains but may not transfer well to unseen problem types.
- Failure signatures:
  - Convincer always outputs "Correct" even for wrong answers (model overconfidence).
  - Answerer fails to generate type hints or intermediate answers.
  - Iterations do not improve accuracy (loop convergence failure).
- First 3 experiments:
  1. Run Self-Convince on a simple arithmetic problem (e.g., GSM8K) and observe if the Convincer detects and corrects a known error.
  2. Remove the Answerer and run the same problem to see if accuracy drops, confirming the importance of type hints.
  3. Increase the number of iterations on a hard problem (e.g., AQuA) and plot accuracy vs. iteration count to identify convergence behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Self-Convinced framework perform on tasks beyond arithmetic and commonsense reasoning, such as natural language understanding or generation tasks?
- Basis in paper: [inferred] The paper mentions that the framework was tested on seven diverse datasets, including English arithmetic, commonsense problems, and Chinese arithmetic problems, but does not provide results on other types of tasks.
- Why unresolved: The paper does not provide experimental results or analysis on tasks beyond arithmetic and commonsense reasoning, leaving the framework's performance on other types of tasks unexplored.
- What evidence would resolve it: Conducting experiments on a wider range of tasks, such as natural language understanding or generation tasks, and comparing the results with state-of-the-art methods for those tasks.

### Open Question 2
- Question: How does the framework's performance scale with the size of the language model?
- Basis in paper: [inferred] The paper does not discuss the impact of model size on the framework's performance, nor does it provide results using different model sizes.
- Why unresolved: The paper does not provide any analysis or experimental results on how the framework's performance changes with different model sizes, leaving the scalability of the framework unexplored.
- What evidence would resolve it: Conducting experiments using different model sizes and analyzing the performance of the framework on each, to determine the impact of model size on the framework's effectiveness.

### Open Question 3
- Question: How does the framework handle tasks with more complex reasoning requirements, such as multi-hop reasoning or reasoning over longer contexts?
- Basis in paper: [inferred] The paper mentions that the framework is designed to iteratively refine reasoning and answers, but does not provide specific results or analysis on tasks with more complex reasoning requirements.
- Why unresolved: The paper does not provide any experimental results or analysis on the framework's performance on tasks with more complex reasoning requirements, such as multi-hop reasoning or reasoning over longer contexts.
- What evidence would resolve it: Conducting experiments on tasks with more complex reasoning requirements and analyzing the framework's performance, to determine its effectiveness in handling such tasks.

## Limitations
- The framework's reliance on self-correction assumes LLMs can introspect and fix errors, but this has not been validated beyond the paper's experiments.
- Performance is tightly coupled to the quality of prompt templates for each module; small prompt variations may lead to significant accuracy swings.
- The method depends on carefully crafted few-shot examples, but the paper does not disclose how these exemplars were selected or whether they introduce bias.

## Confidence
- **High confidence**: Iterative refinement improves accuracy over baseline CoT on tested datasets.
- **Medium confidence**: The Convincer reliably detects and corrects the first reasoning error.
- **Medium confidence**: Type hints (e.g., "Algebraic Equation") improve reasoning accuracy.
- **Low confidence**: Self-Convince generalizes to unseen problem types or model architectures.

## Next Checks
1. Apply Self-Convince to a new, unseen QA dataset (e.g., DROP or HotpotQA) and measure accuracy gains over baseline CoT.
2. Systematically vary the Convincer and Answerer prompts and measure the impact on accuracy to quantify robustness to prompt engineering choices.
3. Manually inject known reasoning errors into a subset of problems and measure the Convincer's ability to detect and correct them.