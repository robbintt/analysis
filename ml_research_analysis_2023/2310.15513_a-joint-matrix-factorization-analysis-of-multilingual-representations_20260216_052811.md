---
ver: rpa2
title: A Joint Matrix Factorization Analysis of Multilingual Representations
arxiv_id: '2310.15513'
source_url: https://arxiv.org/abs/2310.15513
tags:
- languages
- language
- multilingual
- number
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a joint matrix factorization tool for analyzing
  multilingual and monolingual model representations. By jointly decomposing covariance
  matrices between multilingual and monolingual representations, the method reveals
  how morphosyntactic features are encoded across layers, with variations influenced
  by language-specific properties like writing systems.
---

# A Joint Matrix Factorization Analysis of Multilingual Representations

## Quick Facts
- arXiv ID: 2310.15513
- Source URL: https://arxiv.org/abs/2310.15513
- Reference count: 40
- Primary result: Joint matrix factorization reveals layer-wise encoding patterns in multilingual models, with middle layers capturing language-neutral subspaces and correlations to cross-lingual task performance

## Executive Summary
This work introduces a joint matrix factorization tool for analyzing multilingual and monolingual model representations. By jointly decomposing covariance matrices between multilingual and monolingual representations using PARAFAC2, the method reveals how morphosyntactic features are encoded across layers, with variations influenced by language-specific properties like writing systems. The factorization outputs correlate with cross-lingual task performance and yield phylogenetic tree structures that align with linguistic trees. Experiments on 33 languages and 17 morphosyntactic categories show that middle layers capture language-neutral subspaces, while upper layers become task-specific.

## Method Summary
The method involves pre-training XLM-R and RoBERTa models on downsampled Wikipedia data for 33 languages, extracting representations for words in Universal Dependencies treebanks, calculating covariance matrices between multilingual and monolingual representations, and applying PARAFAC2 joint matrix factorization. The factorization produces signature vectors that capture language-specific transformations, which are then analyzed using Pearson correlation with cross-lingual task performance and hierarchical clustering to generate phylogenetic trees.

## Key Results
- Middle layers of multilingual models capture language-neutral subspaces, while upper layers become task-specific
- Factorization outputs exhibit strong associations with cross-lingual task performance across different benchmarks
- Phylogenetic trees generated from signature vectors align with linguistic phylogenetic trees
- Logographic languages (Chinese, Japanese) show distinct encoding patterns that are mitigated by romanization

## Why This Works (Mechanism)

### Mechanism 1
Joint matrix factorization reveals language-specific encoding patterns in multilingual models. By decomposing covariance matrices between multilingual and monolingual representations, PARAFAC2 identifies language-specific signatures that capture how each language transforms multilingual representations into its own space. The diagonal values of Σℓ act as intensity measures of this transformation.

### Mechanism 2
Layer position correlates with language neutrality vs. task-specificity in multilingual representations. Lower layers show higher average signature values (indicating strong language-specific transformations), middle layers show compressed distributions (indicating language-neutral subspaces), and upper layers show increasing variance (indicating task-specific specialization).

### Mechanism 3
Factorization outputs predict cross-lingual task performance. The signature vectors capture the quality of language representations in the multilingual model, which directly correlates with downstream task performance across different cross-lingual benchmarks.

## Foundational Learning

- Concept: Matrix factorization (specifically PARAFAC2)
  - Why needed here: The method requires understanding how joint factorization differs from standard SVD and how it can decompose multiple related matrices simultaneously.
  - Quick check question: What constraint does PARAFAC2 impose on Uℓ matrices that distinguishes it from standard SVD?

- Concept: Covariance matrix computation and interpretation
  - Why needed here: The analysis depends on correctly computing and interpreting cross-covariance between multilingual and monolingual representations.
  - Quick check question: How is the covariance matrix Ωℓ defined between Zℓ and Yℓ, and what does each element represent?

- Concept: Hierarchical clustering and phylogenetic tree construction
  - Why needed here: The method uses cosine distance matrices and UPGMA clustering to create phylogenetic trees from signature vectors.
  - Quick check question: What distance metric is used to compare signature vectors, and which clustering algorithm constructs the phylogenetic tree?

## Architecture Onboarding

- Component map: Data preprocessing → Model training (multilingual + monolingual) → Representation extraction → Covariance matrix computation → PARAFAC2 factorization → Signature analysis → Correlation with tasks/linguistic properties
- Critical path: The factorization step is critical - without meaningful decomposition, subsequent analyses cannot proceed.
- Design tradeoffs: Using downsampled data ensures fair comparison but may not reflect full-scale model behavior; using the same tokenizer/vocabulary across models ensures comparability but may introduce biases.
- Failure signatures: Low variance in signature values across languages suggests the factorization is not capturing meaningful differences; inconsistent trends across layers suggest architectural assumptions may be violated.
- First 3 experiments:
  1. Verify PARAFAC2 factorization produces expected dimensionality reduction by checking that Σℓ matrices have the expected k non-zero values
  2. Confirm the hypothesized layer-wise trend (decreasing mean, compressed middle, increasing variance) holds on a subset of languages
  3. Test correlation between signature values and a simple downstream metric (like POS tagging accuracy) on one language pair before scaling to full analysis

## Open Questions the Paper Calls Out

- Question: How do logographic writing systems specifically impact multilingual model representations compared to phonographic systems?
  - Basis in paper: The paper found Chinese and Japanese had lower signature values and showed distinct encoding patterns, which were mitigated when data was romanized.
  - Why unresolved: The analysis only examined romanization as an ablation study. The underlying computational processes and architectural factors that make logographic systems challenging remain unclear.
  - What evidence would resolve it: Controlled experiments comparing models trained on logographic vs phonographic systems with matched vocabulary sizes and linguistic properties, plus analysis of attention patterns and subword tokenization effects.

- Question: What is the relationship between morphosyntactic feature frequency and their encoding across multilingual model layers?
  - Basis in paper: The paper found some features showed consistent distributions while others varied across layers, but didn't systematically analyze frequency effects.
  - Why unresolved: While the paper examined correlation between average signature values and data size, it didn't explore how feature frequency specifically interacts with layer-wise encoding.
  - What evidence would resolve it: Controlled experiments manipulating feature frequency while holding other factors constant, combined with layer-wise correlation analysis.

- Question: How transferable are the factorization analysis results across different multilingual model architectures?
  - Basis in paper: The paper validated results on a public XLM-R checkpoint but noted computational limitations prevented testing other architectures like BERT or T5.
  - Why unresolved: The analysis was restricted to XLM-R and RoBERTa architectures, leaving open whether results generalize to other architectures with different training objectives or tokenization strategies.
  - What evidence would resolve it: Systematic comparison of factorization results across multiple architectures (BERT, mBERT, T5, mT5) trained on identical data with controlled hyperparameters.

## Limitations
- The analysis relies heavily on the quality of covariance matrices computed between multilingual and monolingual representations
- The assumption that middle layers serve as a language-neutral interlingua may not hold for all multilingual model architectures
- The study only examined XLM-R and RoBERTa architectures, leaving open whether results generalize to other architectures

## Confidence
- High confidence: Layer-wise trends in signature values (middle layers show compressed distributions)
- Medium confidence: Correlation between factorization outputs and cross-lingual task performance
- Medium confidence: Phylogenetic tree alignment with linguistic trees

## Next Checks
1. Validate that PARAFAC2 decomposition produces stable results across multiple random seeds and different downsampling ratios
2. Test whether the layer-wise signature trends persist when using alternative dimensionality reduction methods (e.g., PCA) as a comparison baseline
3. Evaluate the factorization outputs on additional downstream tasks not used in the original correlation analysis to assess generalizability