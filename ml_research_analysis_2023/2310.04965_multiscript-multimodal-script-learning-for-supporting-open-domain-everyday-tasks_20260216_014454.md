---
ver: rpa2
title: 'MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday
  Tasks'
arxiv_id: '2310.04965'
source_url: https://arxiv.org/abs/2310.04965
tags:
- video
- step
- task
- steps
- script
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces M ULTISCRIPT, a new benchmark for multimodal
  script learning with two novel tasks: multimodal script generation and subsequent
  step prediction. Built on WikiHow, the dataset contains over 6,655 everyday tasks
  across 19 domains, each paired with video demonstrations and text descriptions.'
---

# MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain Everyday Tasks

## Quick Facts
- arXiv ID: 2310.04965
- Source URL: https://arxiv.org/abs/2310.04965
- Reference count: 22
- Key outcome: Introduces a multimodal script learning benchmark with knowledge-guided generation that outperforms baselines by up to 30% on generation metrics.

## Executive Summary
This paper introduces MULTISCRIPT, a new benchmark for multimodal script learning with two novel tasks: multimodal script generation and subsequent step prediction. Built on WikiHow, the dataset contains over 6,655 everyday tasks across 19 domains, each paired with video demonstrations and text descriptions. The tasks require generating complete structured step-by-step scripts from videos or predicting the next logical step given partial video input. To address these challenges, the authors propose a knowledge-guided generative framework that leverages large language models like Vicuna to prompt task-specific instructional knowledge and adaptively incorporate it into the generation process. Experiments show that their approach significantly outperforms competitive baselines, with substantial gains in BLEU, METEOR, ROUGE-L, and BERTScore metrics.

## Method Summary
MULTISCRIPT addresses multimodal script learning through a knowledge-guided generative framework. The method extracts keyframes from videos, captions them with OFA, and uses UniVL as the multimodal encoder-decoder. For script generation, it prompts Vicuna to generate task-specific instructional knowledge, encodes this alongside video and captions, and decodes the complete script. For subsequent step prediction, it employs two UniVL encoders - one with and one without instructional knowledge - and uses a Deberta-based NLI model to compute confidence scores for selectively incorporating the knowledge. The knowledge prompter uses a template to elicit structured procedural knowledge from LLMs, while the NLI selector filters and weights knowledge based on alignment with preceding steps.

## Key Results
- The knowledge-guided approach achieves 26.5% absolute improvement in BLEU-4 over baseline models on script generation.
- For subsequent step prediction, the selective knowledge incorporation improves accuracy by 18.3% compared to knowledge-free generation.
- The proposed method demonstrates strong generalization across 19 diverse domains with consistent performance improvements over all baselines.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incorporating task-specific instructional knowledge from large language models improves generation quality by providing relevant procedural context.
- Mechanism: The knowledge prompter sends the task goal to an LLM (Vicuna) and receives a sequence of step descriptions that, while not perfectly aligned with the video, provide a structured workflow relevant to the task. This workflow is then encoded and fed into the generator alongside video and keyframe captions, enriching the context for generation.
- Core assumption: The LLM's prompted steps, even if not perfectly aligned, contain sufficient semantic overlap with the target task to guide generation toward correct procedural descriptions.
- Evidence anchors:
  - [abstract]: "knowledge-guided generative framework that incorporates the task-related knowledge prompted from large language models such as Vicuna to prompt task-specific instructional knowledge and adaptively incorporate it into the generation process."
  - [section]: "we leverage an LLM, such as Vicuna...to produce task-specific instructional knowledge."
- Break condition: If the LLM generates generic or hallucinated steps that are completely misaligned with the video content, the added noise could degrade performance rather than help.

### Mechanism 2
- Claim: Selective incorporation of instructional knowledge based on natural language inference (NLI) confidence scores prevents misalignment errors during generation.
- Mechanism: For subsequent step prediction, two probability distributions are generated: one using only preceding steps and video, the other also including instructional knowledge. An NLI model scores how well each step in the instructional knowledge aligns with the preceding steps, and the highest score weights the fusion of the two distributions. This prevents irrelevant knowledge from overwhelming the correct context.
- Core assumption: The NLI model's confidence score reliably indicates the alignment between instructional knowledge and the actual demonstrated steps.
- Evidence anchors:
  - [abstract]: "develop a natural language inference-based selector to selectively incorporate the prompt knowledge into the generation process."
  - [section]: "We further employ the pre-trained Deberta...to predict a probability for the Entailment label, which is then used as the confidence score."
- Break condition: If the NLI model fails to distinguish between relevant and irrelevant steps (e.g., due to limited training examples), the fusion could degrade output quality.

### Mechanism 3
- Claim: Extracting and captioning keyframes provides essential visual grounding that bridges the gap between raw video and structured text generation.
- Mechanism: Katna extracts a sequence of key frames from the video, preserving temporal order. The OFA model generates captions for each frame, producing a concise textual representation of the key actions. These captions are then encoded and used as an input modality alongside video features.
- Core assumption: The keyframe extraction preserves the essential actions needed for accurate script generation, and the captioning model produces faithful textual descriptions.
- Evidence anchors:
  - [section]: "we employ Katna...to extract a set of keyframes from V and order them based on their timestamp...For each keyframe Ki, we further employ the pre-trained OFA model...to generate a caption."
- Break condition: If the keyframe extraction misses critical actions or the captioning model hallucinates details, the textual grounding will be insufficient for accurate script generation.

## Foundational Learning

- Concept: Multimodal video understanding and encoding
  - Why needed here: The model must fuse visual features from video with textual task descriptions and instructional knowledge to generate coherent scripts.
  - Quick check question: What model is used to encode the video into spatial and temporal features, and what pre-training dataset supports it?

- Concept: Natural language inference (NLI) for confidence scoring
  - Why needed here: NLI scores determine how well instructional knowledge aligns with preceding steps, enabling selective fusion in generation.
  - Quick check question: Which NLI model backbone is used and how is it fine-tuned for this task?

- Concept: Large language model prompting for knowledge injection
  - Why needed here: LLMs provide task-specific procedural knowledge that is not present in the video or keyframe captions alone.
  - Quick check question: What template prompt structure is used to elicit instructional knowledge from Vicuna?

## Architecture Onboarding

- Component map: UniVL encoder-decoder (text and video encoders, cross encoder, decoder) → Step extractor (Katna + OFA) → Knowledge prompter (Vicuna) → NLI-based selector (Deberta) → Generator (UniVL)
- Critical path: Task goal → Knowledge prompter → UniVL encoders → NLI selector → Decoder → Output script/step
- Design tradeoffs: Using keyframe captions instead of full video reduces computation but may lose fine-grained details; using NLI selector adds complexity but improves alignment.
- Failure signatures: Low BLEU scores with high semantic similarity scores suggest misalignment; high variance in confidence scores indicates NLI model uncertainty.
- First 3 experiments:
  1. Ablation: Remove instructional knowledge input and measure drop in BLEU/ROUGE scores.
  2. Ablation: Replace NLI selector with uniform weighting of distributions and observe changes in subsequent step accuracy.
  3. Ablation: Use raw video instead of keyframe captions as input to evaluate the benefit of frame extraction and captioning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed model change when using different large language models (LLMs) as the knowledge source?
- Basis in paper: [explicit] The paper mentions using Vicuna as an example LLM for knowledge prompting, but does not explore other models.
- Why unresolved: The authors only experimented with Vicuna, leaving the impact of other LLMs unexplored.
- What evidence would resolve it: Comparative results using different LLMs (e.g., GPT-4, Claude) as knowledge sources.

### Open Question 2
- Question: How robust is the proposed method to noisy or incorrect video demonstrations?
- Basis in paper: [inferred] The paper acknowledges that video keyframes may miss fine-grained details and that video demonstrations can be noisy.
- Why unresolved: The paper does not explicitly test the model's robustness to corrupted or misleading video inputs.
- What evidence would resolve it: Experiments with degraded video quality or intentionally misleading demonstrations.

### Open Question 3
- Question: Can the proposed framework generalize to domains beyond the 19 covered in WikiHow?
- Basis in paper: [explicit] The paper notes that MULTISCRIPT covers 19 diverse domains but does not test generalization to unseen domains.
- Why unresolved: The evaluation is limited to the domains present in the dataset.
- What evidence would resolve it: Testing the model on tasks from completely new domains not present in the training data.

## Limitations
- The evaluation primarily relies on generation quality metrics without human evaluation of script coherence or correctness.
- The claim of being the first multimodal script learning benchmark is limited by the narrow definition of "everyday tasks."
- The selective incorporation mechanism depends heavily on the NLI model's ability to judge step relevance, but this is not fully validated.

## Confidence

**High Confidence**: The experimental results showing performance improvements over baselines are well-supported by quantitative metrics. The architecture description and dataset construction methodology are detailed enough for reproducibility.

**Medium Confidence**: The effectiveness of the knowledge-guided framework depends on assumptions about LLM-generated knowledge quality and NLI selector reliability that are not fully validated through ablation studies or error analysis.

**Low Confidence**: Claims about the novelty of addressing "open domain everyday tasks" and the dataset being "the first" multimodal script learning benchmark lack comparative analysis with existing script learning or procedural video understanding datasets.

## Next Checks

1. Conduct human evaluation studies to assess script coherence and correctness beyond automated metrics, particularly focusing on cases where semantic similarity is high but procedural accuracy may be low.

2. Perform detailed error analysis on the NLI selector to quantify its false positive and false negative rates in filtering relevant versus irrelevant instructional knowledge steps.

3. Evaluate model robustness by testing on videos with varying quality, missing steps, or non-standard execution orders to assess generalization beyond the curated WikiHow dataset.