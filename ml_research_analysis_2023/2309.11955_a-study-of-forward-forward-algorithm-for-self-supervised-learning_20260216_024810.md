---
ver: rpa2
title: A Study of Forward-Forward Algorithm for Self-Supervised Learning
arxiv_id: '2309.11955'
source_url: https://arxiv.org/abs/2309.11955
tags:
- forward-forward
- layer
- learning
- network
- trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the performance of the forward-forward
  algorithm for self-supervised learning, comparing it to the backpropagation approach.
  The authors benchmark four datasets (MNIST, F-MNIST, SVHN, CIFAR-10) and three self-supervised
  representation learning techniques (rotation, flip, jigsaw).
---

# A Study of Forward-Forward Algorithm for Self-Supervised Learning

## Quick Facts
- arXiv ID: 2309.11955
- Source URL: https://arxiv.org/abs/2309.11955
- Reference count: 38
- Key outcome: Forward-forward algorithm performs worse than backpropagation for transfer learning in self-supervised settings

## Executive Summary
This study investigates the forward-forward algorithm for self-supervised learning (SSL) by comparing it to backpropagation across four image datasets and three SSL tasks. The authors find that while forward-forward performs comparably to backpropagation on supervised training, it significantly underperforms on transfer learning tasks. This is attributed to the algorithm's tendency to focus on task-specific decision boundaries and discard information not directly relevant to the immediate objective, which harms the goal of learning rich, generalizable representations.

## Method Summary
The study evaluates forward-forward against backpropagation using four datasets (MNIST, F-MNIST, SVHN, CIFAR-10) and three SSL pretext tasks (rotation prediction, flip prediction, jigsaw puzzle). Both algorithms use a 4-layer ReLU MLP with 2000 neurons per layer, normalization between layers, and Adam optimizer with learning rate 0.0001. For SSL, labels are embedded as one-hot encodings in the first n pixels of input images. Transfer performance is measured by training a linear classifier on frozen features from the pretrained network.

## Key Results
- Forward-forward performs comparably to backpropagation on supervised training tasks
- Forward-forward shows significantly worse transfer performance across all datasets and SSL tasks
- Using per-layer goodness loss with backpropagation yields similar transfer performance degradation as forward-forward

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Forward-forward algorithm uses separate loss functions per layer to distinguish positive from negative samples, which works well for supervised tasks but harms SSL transfer
- Mechanism: Each layer independently maximizes activation for "positive" data (correct label) and minimizes for "negative" (random label). This encourages layers to specialize on the immediate classification boundary, discarding information not directly useful for that decision
- Core assumption: Layer-wise independent training is sufficient for learning general representations
- Evidence anchors:
  - [abstract]: "the forward-forward algorithm focuses more on the boundaries and drops part of the information unnecessary for making decisions which harms the representation learning goal"
  - [section 2.1]: Describes the per-layer goodness loss and normalization requirement
- Break condition: If the SSL task can be solved without retaining much general feature information (e.g., easy tasks like horizontal flip on MNIST), transfer may be acceptable; otherwise, transfer performance degrades

### Mechanism 2
- Claim: The embedding of labels directly into the input image creates a narrow supervisory signal that limits representation learning
- Mechanism: Labels are one-hot encoded and replace the first n pixels. Layers learn to respond to these embedded labels rather than extracting rich, generalizable features from the image itself. This design biases learning toward label detection rather than semantic understanding
- Core assumption: Embedding labels in the input is an effective way to provide supervision without changing network architecture
- Evidence anchors:
  - [section 2.1]: Explains the embedding process and supervised learning variant
  - [section 6.3]: Notes that unsupervised forward-forward (no label embedding) performs much better for transfer
- Break condition: If labels are provided in a way that forces the network to process the full image (e.g., auxiliary tasks that require full-image understanding), the negative effect may be mitigated

### Mechanism 3
- Claim: Forward-forward's per-layer loss function conflicts with the goal of SSL, which is to learn rich representations for downstream tasks
- Mechanism: SSL tasks like rotation, flip, and jigsaw require learning features useful for multiple potential classification objectives. Forward-forward's layer-wise goodness loss optimizes for immediate task accuracy, not for preserving information useful for later transfer. In contrast, BP's end-to-end loss propagates gradients that encourage feature reuse
- Core assumption: Representation learning benefits from a global loss that encourages features useful across tasks
- Evidence anchors:
  - [section 5.3]: Shows that using goodness loss with BP (layer-wise) also hurts transfer, mirroring FF behavior
  - [section 6]: t-SNE visualizations show worse clustering in FF embeddings compared to BP
- Break condition: If the SSL task is so simple that it can be solved with minimal feature extraction (e.g., easy MNIST flips), the negative impact on transfer is reduced

## Foundational Learning

- Concept: Backpropagation and gradient flow
  - Why needed here: Understanding how BP propagates error signals backward is key to grasping why FF's layer-wise training fails to retain useful features
  - Quick check question: What is the main difference between how BP and FF update weights during training?

- Concept: Self-supervised learning (SSL) and pretext tasks
  - Why needed here: The paper evaluates FF on rotation, flip, and jigsaw tasks—common SSL pretext tasks. Knowing their goals clarifies why FF's narrow focus is problematic
  - Quick check question: What is the purpose of using rotation prediction as an SSL task?

- Concept: Representation learning and transfer learning
  - Why needed here: The core issue is that FF learns task-specific representations, not general ones. Understanding transfer learning helps explain why this matters
  - Quick check question: Why is transfer performance a key metric for evaluating SSL methods?

## Architecture Onboarding

- Component map:
  Input layer (784 or 3072 neurons) -> Hidden layers (4x2000 ReLU) -> Normalization layers (1 per hidden) -> Output layer (not used directly)

- Critical path:
  1. Embed SSL label into first n pixels of input
  2. Forward pass through 4-layer MLP
  3. Compute per-layer goodness loss (separate for each layer)
  4. Update weights for each layer independently (no backpropagation)
  5. Evaluate transfer by training linear classifier on frozen features

- Design tradeoffs:
  - FF: No backward pass → simpler hardware, but layer-wise independence can hurt representation quality
  - BP: Global loss → better transfer, but requires backward gradients and more memory
  - Label embedding: Simple to implement, but may bias learning toward label detection

- Failure signatures:
  - Poor transfer accuracy despite good SSL-task accuracy
  - t-SNE embeddings show poor clustering of classes in FF-trained networks
  - Overfitting on complex datasets (e.g., CIFAR-10) with MLP architecture

- First 3 experiments:
  1. Train FF on MNIST rotation task; evaluate SSL accuracy and transfer to classification
  2. Repeat with flip task; compare transfer performance to rotation
  3. Switch to BP with per-layer goodness loss; compare embeddings and transfer to FF results

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Evaluated only on shallow MLP architecture, which may not reflect forward-forward performance on deeper networks
- Label embedding in pixel space may artificially constrain representation learning capacity
- Normalization requirement between layers introduces architectural constraints not present in standard deep learning

## Confidence
- High confidence: The core finding that forward-forward performs worse than backpropagation for transfer learning across multiple datasets and SSL tasks
- Medium confidence: The mechanism explanation linking layer-wise goodness loss to reduced transfer performance, though supported by ablation studies, requires further validation on deeper architectures
- Medium confidence: The claim that label embedding in pixel space biases learning toward task-specific boundaries, based on comparison with unsupervised variants

## Next Checks
1. **Architecture scaling test**: Evaluate forward-forward on deeper convolutional networks (e.g., ResNet-18) to determine if performance gaps persist with architectural improvements
2. **Loss function ablation**: Compare forward-forward with alternative goodness functions (e.g., contrastive losses) to isolate whether the specific loss formulation or layer-wise training is responsible for poor transfer
3. **Downstream task diversity**: Test transfer to non-image classification tasks (e.g., object detection, segmentation) to verify if transfer limitations generalize beyond linear classification scenarios