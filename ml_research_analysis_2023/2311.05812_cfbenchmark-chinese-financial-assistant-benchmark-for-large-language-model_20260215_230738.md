---
ver: rpa2
title: 'CFBenchmark: Chinese Financial Assistant Benchmark for Large Language Model'
arxiv_id: '2311.05812'
source_url: https://arxiv.org/abs/2311.05812
tags:
- financial
- text
- llms
- arxiv
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CFBenchmark-Basic, a Chinese financial assistant
  benchmark designed to evaluate the performance of large language models (LLMs) in
  financial text processing tasks. The benchmark covers three aspects - recognition,
  classification, and generation - across eight tasks, including company and product
  recognition, sentiment analysis, sector classification, event detection, content
  summary, investment suggestions, and risk alerts.
---

# CFBenchmark: Chinese Financial Assistant Benchmark for Large Language Model

## Quick Facts
- arXiv ID: 2311.05812
- Source URL: https://arxiv.org/abs/2311.05812
- Reference count: 9
- Primary result: 22 LLMs evaluated on Chinese financial text processing tasks show significant room for improvement

## Executive Summary
This paper introduces CFBenchmark-Basic, a Chinese financial assistant benchmark designed to evaluate large language models' capabilities in financial text processing. The benchmark covers three dimensions - recognition, classification, and generation - across eight tasks using 3,917 Chinese financial texts. Experiments with 22 renowned LLMs reveal that while some models excel in specific tasks, overall performance remains suboptimal, indicating substantial opportunities for improvement in financial text processing.

## Method Summary
The benchmark evaluates 22 LLMs on 8 financial text processing tasks across 3,917 Chinese financial documents. Tasks are organized into three aspects: recognition (company/product recognition), classification (sector/event/sentiment classification), and generation (summary/suggestion/risk alert generation). The evaluation uses zero-shot and few-shot modes with F1-score for recognition/classification tasks and cosine similarity for generation tasks. A domain-specific model (CFGPT) was also developed and tested against baseline models.

## Key Results
- Existing LLMs show significant performance gaps in basic financial text processing tasks
- CFGPT demonstrates superior performance on several financial tasks compared to general LLMs
- No publicly available method achieves 0.6 F1-score across all three financial processing aspects
- Longer financial texts (up to 1,800 characters) reveal LLM limitations not apparent in shorter benchmarks

## Why This Works (Mechanism)

### Mechanism 1
The benchmark evaluates LLMs across three progressively complex financial processing dimensions (recognition → classification → generation). By decomposing financial reasoning into hierarchical subtasks, the benchmark isolates specific LLM weaknesses and provides granular performance diagnostics.

### Mechanism 2
Longer financial texts (50-1,800 characters) reveal LLM limitations that shorter benchmarks miss. Extended context requires sustained attention and reasoning across multiple financial entities and relationships, stressing LLM capabilities beyond single-sentence comprehension.

### Mechanism 3
Domain-specific pretraining and fine-tuning (CFGPT) significantly improves financial task performance. Financial domain adaptation provides specialized vocabulary, entity relationships, and reasoning patterns that general LLMs lack.

## Foundational Learning

- **Financial entity recognition**
  - Why needed here: Forms the foundation for all higher-level financial reasoning tasks
  - Quick check question: Can you identify company and product names in a financial news article without understanding their relationships?

- **Hierarchical sector classification**
  - Why needed here: Financial texts require understanding both broad industry categories and specific subsectors
  - Quick check question: Given a financial company, can you classify it into both first-level (e.g., "Manufacturing") and second-level (e.g., "Electrical Equipment") categories?

- **Financial sentiment analysis**
  - Why needed here: Financial sentiment differs from general sentiment and affects market behavior
  - Quick check question: Does "reducing staff" indicate positive or negative sentiment in financial contexts?

## Architecture Onboarding

- **Component map**: Data collection pipeline → annotation process → benchmark construction → evaluation framework → LLM testing interface
- **Critical path**: Text collection → expert annotation → benchmark assembly → metric definition → LLM evaluation
- **Design tradeoffs**: Longer texts provide realism but challenge LLM context windows; multi-task evaluation provides comprehensiveness but increases complexity
- **Failure signatures**: Poor entity recognition cascades to classification errors; generation tasks fail when underlying recognition/classification is weak
- **First 3 experiments**:
  1. Run zero-shot evaluation on all 22 LLMs to establish baseline capabilities
  2. Compare performance across task types to identify specific LLM weaknesses
  3. Evaluate domain-specific models (CFGPT) against general LLMs to measure adaptation benefits

## Open Questions the Paper Calls Out

### Open Question 1
What specific performance improvements would be needed for LLMs to reach 0.6 F1-score across all three financial text processing aspects (recognition, classification, generation) in CFBenchmark-Basic? The paper identifies the performance gap but doesn't specify what exact improvements would be needed to reach the 0.6 threshold.

### Open Question 2
How would incorporating more domain-specific knowledge beyond financial text processing (e.g., financial calculations, compositional reasoning) impact LLM performance in financial assistant tasks? The benchmark only evaluates text processing, leaving the impact of additional financial reasoning capabilities unexplored.

### Open Question 3
What is the relationship between text length and LLM performance in financial text processing tasks, and at what point does increased length become detrimental? While the paper acknowledges the importance of text length, it doesn't analyze how performance varies with different text lengths.

## Limitations

- Limited transparency in annotation quality and reliability procedures
- Unknown dataset representativeness across financial sectors and market conditions
- Unclear effectiveness of cosine similarity for evaluating generation task quality

## Confidence

- **High Confidence**: Multi-task structure is technically sound; observation of LLM performance gaps is empirically supported; CFGPT performance advantage is validated
- **Medium Confidence**: Claims about longer texts revealing limitations need more validation; domain-specific pretraining benefits require more systematic studies
- **Low Confidence**: Unique gap filling claim lacks strong comparative analysis; scalability assertions are speculative; few-shot prompting effectiveness needs systematic exploration

## Next Checks

1. Conduct statistical analysis of text corpus to verify balanced representation across financial sectors, market conditions, and temporal periods
2. Implement inter-annotator agreement studies and calculate Cohen's kappa for classification tasks to establish ground truth reliability
3. Design controlled experiments to evaluate whether cosine similarity effectively captures generation quality compared to human evaluation