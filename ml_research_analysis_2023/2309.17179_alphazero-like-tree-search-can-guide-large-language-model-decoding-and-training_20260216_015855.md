---
ver: rpa2
title: Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training
arxiv_id: '2309.17179'
source_url: https://arxiv.org/abs/2309.17179
tags:
- search
- value
- tree
- training
- mcts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Tree-of-Thought and Reasoning-via-Planning augment LLM reasoning
  using tree-search, but they rely on prompting a pre-trained model to serve as a
  value function and are limited to low search depth. To address these limitations,
  we present an AlphaZero-like tree-search learning framework for LLMs (TS-LLM) that
  uses a learned value function to guide LLM decoding and training.
---

# Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training

## Quick Facts
- arXiv ID: 2309.17179
- Source URL: https://arxiv.org/abs/2309.17179
- Reference count: 39
- One-line primary result: TS-LLM framework uses learned value functions to guide LLM decoding and training, achieving state-of-the-art performance on reasoning tasks with trees up to depth 64.

## Executive Summary
This paper presents TS-LLM, an AlphaZero-like framework that uses learned value functions to guide large language model (LLM) decoding and training through tree search. Unlike previous approaches that rely on prompting pre-trained models for value estimation, TS-LLM trains a learned value network to provide intermediate rewards during search. The framework is designed to be generally applicable across tasks, LLM sizes, and search depths, and can guide LLMs during both inference and training through iterative improvement.

## Method Summary
The TS-LLM framework treats LLM generation as a Markov Decision Process and applies tree search algorithms (MCTS-α, MCTS-Rollout, MCTS, BFS, DFS) using learned value functions and outcome reward models. The method uses token-level action nodes to enable deep search trees, with a learned value network providing intermediate reward estimates. The framework supports both inference-time search and iterative training where tree search trajectories are used to fine-tune the LLM policy, value function, and reward model in an AlphaZero-style loop.

## Key Results
- TS-LLM outperforms existing tree-of-thought and reasoning-via-planning approaches on GSM8k, Game24, PrOntoQA, and RLHF alignment tasks
- The framework can handle search trees with depth up to 64 using token-level action spaces
- Iterative training with tree search augmented data leads to improved LLM performance over baseline fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A learned value function can replace prompt-based evaluation for tree-search in LLMs.
- **Mechanism:** The paper replaces reliance on prompting large LLMs (like GPT-4) for value estimation with a learned value network trained via supervised learning on sampled trajectories. This network provides intermediate reward signals during search.
- **Core assumption:** A small-to-medium LLM can learn to approximate task-specific rewards if trained on sufficient intermediate reasoning steps with known outcomes.
- **Evidence anchors:**
  - [abstract]: "To address these limitations, we present an AlphaZero-like tree-search learning framework for LLMs (TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLM decoding."
  - [section]: "To make the tree search algorithm generally applicable, our method leverages a learned value function vθ(s) conditioned on state s and a learned final-step outcome reward model (ORM) ˆrθ"
  - [corpus]: Weak - the corpus lists related works like "OVM, Outcome-supervised Value Models for Planning in Mathematical Reasoning" but no direct evidence that learned value functions consistently outperform prompt-based ones across tasks.
- **Break condition:** If the value network cannot be trained to approximate rewards accurately, the tree search will prune suboptimal paths incorrectly, degrading performance.

### Mechanism 2
- **Claim:** Tree search with learned value function scales to deep trees (depth 64) in token-level action spaces.
- **Mechanism:** By treating each token as an action node, the method enables search trees with much greater depth than sentence-level approaches. The learned value function provides intermediate estimates, allowing search to proceed without completing full sequences.
- **Core assumption:** The learned value function can generalize across long sequences and provide useful intermediate signals even deep in the tree.
- **Evidence anchors:**
  - [abstract]: "Empirical results across reasoning, planning, alignment, and decision-making tasks show that TS-LLM outperforms existing approaches and can handle trees with a depth of 64."
  - [section]: "For token-level action nodes, though it can get rid of the search space discrepancy and extra computational burdens, it greatly increases the depth of the tree, making tree-search more challenging."
  - [corpus]: Missing - no corpus entry directly supports scaling to depth 64 with token-level actions.
- **Break condition:** If the value function's accuracy degrades with depth, the search may become unreliable and fail to prune effectively.

### Mechanism 3
- **Claim:** Tree search can serve as a policy improvement operator to iteratively train LLMs.
- **Mechanism:** The framework treats tree search as a way to generate high-reward trajectories, which are then used to fine-tune both the policy (LLM) and value function in an iterative AlphaZero-style loop.
- **Core assumption:** The tree search can consistently find better trajectories than the current policy, providing useful training signal.
- **Evidence anchors:**
  - [abstract]: "Our approach can guide LLMs during both inference and training, iteratively improving the LLM."
  - [section]: "By treating the tree-search operation as a policy improvement operator, we can conduct iterative processes of policy improvement and evaluation, to further train the LLM."
  - [corpus]: Weak - related work like "Offline Reinforcement Learning for LLM Multi-Step Reasoning" exists but no direct evidence of iterative improvement in this exact framework.
- **Break condition:** If tree search fails to find better trajectories, the training loop will not improve the policy and may even degrade it.

## Foundational Learning

- **Concept:** Markov Decision Process (MDP) formulation of language generation.
  - **Why needed here:** The paper frames LLM generation as an MDP to apply tree search algorithms, which require states, actions, rewards, and transition dynamics.
  - **Quick check question:** What are the states and actions in the token-level action space formulation?

- **Concept:** Monte Carlo Tree Search (MCTS) and its variants.
  - **Why needed here:** The core algorithm uses MCTS variants (MCTS-α, MCTS-Rollout) to explore the search tree efficiently.
  - **Quick check question:** How does MCTS-α differ from classic MCTS in terms of value backpropagation?

- **Concept:** Policy improvement via imitation learning.
  - **Why needed here:** The iterative training loop uses tree search trajectories as supervision to improve the policy.
  - **Quick check question:** What loss function is used to train the policy from tree search trajectories?

## Architecture Onboarding

- **Component map:** LLM policy (πθ) -> Value network (vθ) -> Outcome Reward Model (ORM) (ˆrθ) -> Tree search algorithm -> Data collection pipeline
- **Critical path:**
  1. Sample trajectories using current policy
  2. Train value network and ORM on these trajectories
  3. Use tree search with learned value to generate new trajectories
  4. Fine-tune policy on new trajectories
  5. Repeat
- **Design tradeoffs:**
  - Sentence-level vs token-level action nodes: sentence-level has smaller depth but larger branching factor and computational cost; token-level has larger depth but cheaper expansion.
  - Learned value vs prompt-based evaluation: learned value generalizes to smaller models but requires training data; prompt-based works zero-shot but needs large models.
  - Intra-tree vs inter-tree aggregation: intra-tree reuses tree structure (faster) but may have less diversity; inter-tree builds new trees (slower) but more diverse.
- **Failure signatures:**
  - Value network predictions are flat or random → search degenerates to random exploration
  - Tree search takes too long → computational cost outweighs benefits
  - Policy overfits to tree search trajectories → poor generalization to direct decoding
- **First 3 experiments:**
  1. Train value network on sampled trajectories and evaluate prediction accuracy on held-out data.
  2. Run MCTS-α with learned value on a simple task (e.g., Game24) and compare to CoT baseline.
  3. Perform one iteration of policy improvement and measure performance gain on direct decoding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the learned value function performance scale with the amount and diversity of training data?
- Basis in paper: [inferred] The paper mentions investigating data collection and training paradigms for value function and ORM in TS-LLM, and concludes that collecting a diverse dataset as much as possible is better for TS-LLM's value function and ORM training.
- Why unresolved: The paper only provides a qualitative analysis and does not quantify the relationship between value function performance and dataset size/diversity.
- What evidence would resolve it: Empirical results showing value function performance (e.g., accuracy on a held-out validation set) as a function of dataset size and diversity would provide insights into the scaling behavior.

### Open Question 2
- Question: How does the performance of different tree search algorithms (MCTS-α, MCTS-Rollout, MCTS, BFS, DFS) compare across various reasoning and planning tasks with different search space characteristics?
- Basis in paper: [explicit] The paper presents empirical results comparing the performance of these algorithms on four tasks with different search widths and depths (GSM8k, Game24, PrOntoQA, and RLHF alignment).
- Why unresolved: While the paper provides results for specific tasks, it does not offer a comprehensive analysis of how algorithm performance generalizes across different search space characteristics (e.g., depth, width, node size).
- What evidence would resolve it: A systematic study evaluating the performance of different algorithms across a range of tasks with varying search space characteristics would provide insights into their strengths and limitations.

### Open Question 3
- Question: How does the iterative training process in TS-LLM impact the performance of the LLM policy, value function, and ORM over multiple iterations?
- Basis in paper: [inferred] The paper describes an iterative training process where the LLM policy, value function, and ORM are updated based on tree search augmented data. However, it only presents results for a single iteration and mentions plans for further exploration.
- Why unresolved: The long-term impact of iterative training on the performance of all three components is not yet known.
- What evidence would resolve it: Experimental results showing the performance of the LLM policy, value function, and ORM after multiple iterations of training would provide insights into the effectiveness and convergence of the iterative process.

## Limitations

- The paper lacks ablation studies to isolate the contribution of the learned value function from other components
- Limited evidence is provided to support the claimed scalability to trees of depth 64
- No detailed computational cost analysis comparing tree search methods to baseline approaches

## Confidence

**High confidence** in the basic methodology and framework design - the paper clearly describes how to implement tree search with learned value functions for LLMs, and the core concepts are well-established in reinforcement learning literature.

**Medium confidence** in the empirical results - while the paper reports performance improvements on multiple tasks, the lack of detailed ablation studies and computational cost analysis makes it difficult to fully assess the practical significance of the approach.

**Low confidence** in the scalability claims - the paper mentions handling trees of depth 64 but provides limited evidence about actual search depth achieved and how performance scales with tree depth.

## Next Checks

1. **Ablation study on value function accuracy**: Measure how tree search performance degrades when the value function predictions are corrupted with increasing levels of noise. This would validate whether the learned value function is truly necessary for the approach to work, as claimed in Mechanism 1.

2. **Search depth analysis**: Report the actual maximum search depth achieved on token-level tasks and analyze how performance varies with search depth. This would verify the depth-64 claim and test the scaling assumption in Mechanism 2.

3. **Computational cost comparison**: Measure and compare the wall-clock time for tree search inference versus baseline methods (CoT, CoT-SC) on representative tasks. This would validate whether the approach is practical given the computational overhead of tree search.