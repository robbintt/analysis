---
ver: rpa2
title: GPT-4 Can't Reason
arxiv_id: '2308.03762'
source_url: https://arxiv.org/abs/2308.03762
tags:
- aunt
- gpt-4
- agatha
- holds
- butler
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates GPT-4\u2019s reasoning capabilities through\
  \ 21 qualitative problems spanning logic, math, common sense, and spatial reasoning.\
  \ Despite GPT-4\u2019s improvements over earlier models, it consistently fails on\
  \ basic tasks: incorrect arithmetic, miscounting negations, flawed logical proofs,\
  \ and inability to solve simple scheduling or graph coloring."
---

# GPT-4 Can't Reason

## Quick Facts
- arXiv ID: 2308.03762
- Source URL: https://arxiv.org/abs/2308.03762
- Authors: 
- Reference count: 1
- Key outcome: GPT-4 consistently fails on basic reasoning tasks including arithmetic, logic, and spatial reasoning, making it unsuitable for critical applications without external verification.

## Executive Summary
This paper evaluates GPT-4's reasoning capabilities through 21 qualitative problems spanning logic, math, common sense, and spatial reasoning. Despite GPT-4's improvements over earlier models, it consistently fails on basic tasks: incorrect arithmetic, miscounting negations, flawed logical proofs, and inability to solve simple scheduling or graph coloring. Errors include internal inconsistency, misunderstanding of fundamental concepts like logical entailment, and rote generation of plausible but wrong answers. The analysis concludes GPT-4 cannot reliably reason, making it unsuitable for critical applications in science, engineering, or software development without rigorous external verification.

## Method Summary
The study evaluates GPT-4 through manual qualitative analysis of 21 reasoning problems covering arithmetic, logic, graph coloring, scheduling, and spatial reasoning. For each problem, researchers document GPT-4's responses, analyze explanations and proof attempts, and identify errors, misconceptions, and instances of internal inconsistency. The evaluation focuses on correctness, logical coherence, and the model's ability to maintain consistent reasoning across related problems.

## Key Results
- GPT-4 produces incorrect answers on basic arithmetic and counting problems despite being able to generate correct code
- The model exhibits internal inconsistency, making contradictory statements within single responses
- GPT-4 fails to understand fundamental logical concepts like entailment and cannot construct valid proofs
- The model cannot solve simple graph coloring or scheduling problems that require basic reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: GPT-4's reasoning failures stem from pattern matching without logical inference capability.
- **Mechanism**: The model memorizes surface forms of problems and associated responses but lacks true understanding of logical entailment and inference rules.
- **Core assumption**: Reasoning requires constructing and validating logical chains, not just recalling similar examples.
- **Evidence anchors**:
  - [abstract] "GPT-4 cannot reliably reason, making it unsuitable for critical applications in science, engineering, or software development without rigorous external verification."
  - [section] "GPT-4 consistently fails on basic tasks: incorrect arithmetic, miscounting negations, flawed logical proofs, and inability to solve simple scheduling or graph coloring."
  - [corpus] Weak - the corpus provides related work but not direct evidence for this specific mechanism.
- **Break condition**: If GPT-4 could be shown to construct valid logical proofs from first principles rather than pattern matching, this mechanism would break.

### Mechanism 2
- **Claim**: GPT-4 exhibits internal inconsistency because it generates responses based on statistical patterns rather than coherent logical models.
- **Mechanism**: The model produces contradictory statements within the same response because it lacks a unified mental model to maintain consistency.
- **Core assumption**: Logical reasoning requires maintaining a consistent model of the problem domain throughout the reasoning process.
- **Evidence anchors**:
  - [section] "GPT-4 recognizes that P (x) does not in fact imply Q(x) under the interpretation it gave, but it nevertheless dismisses this inconvenient consideration."
  - [section] "GPT-4 has chosen to consider two randomly selected worlds w1 and w2... and reach a conclusion on the basis of those two particular worlds."
  - [corpus] Weak - corpus neighbors discuss related reasoning problems but don't directly address internal inconsistency mechanisms.
- **Break condition**: If GPT-4 could maintain consistent reasoning across multiple related problems without contradiction, this mechanism would break.

### Mechanism 3
- **Claim**: GPT-4's inability to handle novel problems stems from its reliance on memorized examples rather than general reasoning principles.
- **Mechanism**: The model cannot abstract general principles from specific examples, so it fails when faced with problems that don't match its training data.
- **Core assumption**: True reasoning requires the ability to generalize from specific instances to abstract principles.
- **Evidence anchors**:
  - [section] "GPT-4's response verges on nonsensical... GPT-4 has defined B (in a roundabout way) as the empty set ∅, so that Q is always false."
  - [section] "When we ask GPT-4 to elaborate on why it thinks that P (x) implies Q(x), GPT-4's response is revealing... After multiple unsuccessful attempts to find a countermodel, GPT-4 acknowledges that such a counter-model might not exist."
  - [corpus] Weak - corpus provides related work but not direct evidence for this generalization failure mechanism.
- **Break condition**: If GPT-4 could successfully solve novel problems by applying general reasoning principles rather than pattern matching, this mechanism would break.

## Foundational Learning

- **Concept**: Logical entailment
  - Why needed here: Understanding when conclusions follow necessarily from premises is fundamental to reasoning.
  - Quick check question: If all A are B, and all B are C, can we conclude all A are C? Why or why not?

- **Concept**: Proof construction
  - Why needed here: Reasoning requires building valid chains of inference from premises to conclusions.
  - Quick check question: What makes a proof valid? Can a proof with a single logical error still be considered valid?

- **Concept**: Countermodel construction
  - Why needed here: Disproving claims requires constructing examples that satisfy premises while violating conclusions.
  - Quick check question: How would you construct a countermodel to disprove "All birds can fly"?

## Architecture Onboarding

- **Component map**: Input processing -> Pattern matching layer -> Response generation -> Output
- **Critical path**: Input → Pattern matching → Response generation → Output
  - The critical failure occurs at pattern matching where logical relationships are not properly understood
- **Design tradeoffs**:
  - Scale vs. reasoning: Larger models show better pattern matching but no improvement in logical reasoning
  - Memorization vs. generalization: Model favors memorized responses over novel reasoning
  - Speed vs. accuracy: Fast pattern matching sacrifices logical correctness
- **Failure signatures**:
  - Internal contradictions within single responses
  - Pattern matching to similar but logically different problems
  - Inability to handle novel problem variations
  - Superficial understanding of logical relationships
- **First 3 experiments**:
  1. Test with logically equivalent but syntactically different problems to see if pattern matching fails
  2. Provide countermodels and ask model to explain why they work
  3. Test with problems that require chaining multiple logical steps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can GPT-4 reliably perform basic arithmetic operations like addition and multiplication?
- Basis in paper: Explicit
- Why unresolved: The paper demonstrates multiple instances where GPT-4 fails at basic arithmetic despite being able to generate code that performs the operations correctly.
- What evidence would resolve it: Systematic testing of GPT-4 on large numbers of arithmetic problems with varying difficulty levels.

### Open Question 2
- Question: Is GPT-4's reasoning ability fundamentally limited by computational complexity constraints?
- Basis in paper: Explicit
- Why unresolved: The paper argues that reasoning is computationally hard, but doesn't definitively prove that GPT-4's architecture can't overcome these limitations.
- What evidence would resolve it: Demonstration of GPT-4 solving previously intractable reasoning problems, or formal proof of computational complexity bounds.

### Open Question 3
- Question: Can GPT-4's reasoning errors be eliminated through improved training data or fine-tuning?
- Basis in paper: Inferred
- Why unresolved: The paper shows GPT-4 has systematic reasoning errors, but doesn't explore whether these could be fixed through better training.
- What evidence would resolve it: Comparative studies of GPT-4's reasoning performance before and after different training/fine-tuning approaches.

## Limitations

- The evaluation relies entirely on qualitative assessment rather than systematic quantitative testing with statistical significance
- The 21 problems represent a relatively small sample that may not capture the full range of GPT-4's reasoning capabilities or limitations
- The study does not specify the exact prompt format or model configuration used, making exact reproduction difficult

## Confidence

- High confidence: GPT-4 exhibits significant reasoning failures (multiple concrete examples with detailed error analysis)
- Medium confidence: GPT-4 "cannot reason" at all (categorical statement may be too strong given limited testing scope)
- Medium confidence: Mechanism explanations (plausible but not definitively proven by available evidence)

## Next Checks

1. **Systematic replication**: Conduct a larger-scale quantitative study using hundreds of reasoning problems with controlled prompting and multiple model versions to establish statistical significance of the observed failures.

2. **Cross-model comparison**: Test GPT-4 alongside other frontier models (GPT-3.5, Claude, Gemini) on the same problem set to determine whether these reasoning failures are unique to GPT-4 or common across large language models.

3. **Adversarial prompt engineering**: Systematically vary prompts, temperature settings, and instruction framing to determine whether GPT-4's reasoning failures can be mitigated through better prompting techniques or whether they represent fundamental architectural limitations.