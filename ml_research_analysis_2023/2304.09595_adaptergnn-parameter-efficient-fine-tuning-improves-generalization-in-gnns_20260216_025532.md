---
ver: rpa2
title: 'AdapterGNN: Parameter-Efficient Fine-Tuning Improves Generalization in GNNs'
arxiv_id: '2304.09595'
source_url: https://arxiv.org/abs/2304.09595
tags:
- tuning
- delta
- gnns
- adaptergnn
- ne-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies parameter-efficient fine-tuning (PEFT) in graph
  neural networks (GNNs) and proposes AdapterGNN, a novel PEFT method specifically
  designed for GNNs. Unlike previous methods that primarily focus on prompt tuning,
  AdapterGNN leverages task-specific knowledge in tunable adapters with task-agnostic
  knowledge in the frozen pre-trained model.
---

# AdapterGNN: Parameter-Efficient Fine-Tuning Improves Generalization in GNNs

## Quick Facts
- arXiv ID: 2304.09595
- Source URL: https://arxiv.org/abs/2304.09595
- Reference count: 40
- AdapterGNN achieves higher evaluation performance and lower generalization gaps compared to full fine-tuning while tuning only 5% of parameters

## Executive Summary
This paper addresses the challenge of fine-tuning large pre-trained Graph Neural Networks (GNNs) by proposing AdapterGNN, a parameter-efficient fine-tuning (PEFT) method specifically designed for GNNs. Unlike previous PEFT approaches that focus primarily on prompt tuning, AdapterGNN leverages task-specific knowledge through tunable adapters while preserving task-agnostic knowledge in a frozen pre-trained model. The method demonstrates significant improvements in both evaluation performance and generalization ability compared to full fine-tuning, while only tuning 5% of the total parameters. The authors also provide theoretical justification showing how reducing the tunable parameter space improves generalization bounds.

## Method Summary
AdapterGNN inserts bottleneck adapter modules in parallel with the frozen pre-trained GNN layers. These adapters process the input and output of each message passing layer separately, with outputs combined through learnable scaling parameters. The adapters consist of a linear layer, ReLU activation, and batch normalization, with bottleneck dimension much smaller than the original layer dimension. A key innovation is the learnable scaling parameter that starts from a small value (0.01) to prevent catastrophic forgetting while gradually allowing the adapter outputs to contribute more to the final embedding. This architecture preserves the pre-trained knowledge while enabling effective adaptation to downstream tasks.

## Key Results
- AdapterGNN achieves 5-8% higher ROC-AUC scores compared to full fine-tuning on molecular datasets
- The method reduces the generalization gap by 30-40% compared to full fine-tuning
- Only 5% of parameters are tuned during fine-tuning, significantly reducing computational cost
- Performance improvements are consistent across 8 molecular datasets and 88K PPI protein ego-networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AdapterGNN preserves pre-trained knowledge while adapting to downstream tasks by using tunable adapters that operate in parallel with frozen GNN layers.
- Mechanism: By keeping most parameters frozen and only tuning a small bottleneck adapter module, the model retains task-agnostic knowledge from pre-training while acquiring task-specific knowledge through the adapter. The parallel architecture ensures that the original GNN outputs are preserved and combined with adapter outputs via learnable scaling.
- Core assumption: The bottleneck dimension is small enough to reduce overfitting but large enough to maintain expressivity.
- Evidence anchors:
  - [abstract] "AdapterGNN preserves the knowledge of the large pre-trained model and leverages highly expressive adapters for GNNs, which can adapt to downstream tasks effectively with only a few parameters, while also improving the model's generalization ability"
  - [section] "AdapterGNN combines task-specific knowledge in tunable adapters with task-agnostic knowledge in the frozen pre-trained model"
  - [corpus] Weak evidence - corpus contains related PEFT work but no direct validation of AdapterGNN's parallel architecture preservation mechanism
- Break condition: If bottleneck dimension is too small, expressivity is lost and performance degrades. If too large, overfitting occurs and generalization benefits disappear.

### Mechanism 2
- Claim: The learnable scaling parameter automatically controls the trade-off between preserving pre-trained knowledge and acquiring new task-specific knowledge.
- Mechanism: Starting from a small initial value (0.01), the learnable scaling parameter s gradually increases during training, allowing the adapter outputs to contribute more to the final embedding while the frozen pre-trained knowledge remains stable. This prevents catastrophic forgetting by not immediately overwhelming the frozen parameters.
- Core assumption: The optimization process will find an appropriate balance between the two knowledge sources.
- Evidence anchors:
  - [section] "To maintain consistency with the original output, we include tunable batch normalization in each adapter. Also, to maintain consistency with the original output, we include tunable batch normalization in each adapter"
  - [section] "We propose using a learnable s, which we train from a very small starting point to avoid such issues"
  - [corpus] Weak evidence - corpus contains related adapter work but no direct evidence of this specific learnable scaling strategy
- Break condition: If initialization is too large, catastrophic forgetting occurs immediately. If too small, adaptation is too slow and downstream performance suffers.

### Mechanism 3
- Claim: Reducing the size of tunable parameter space improves generalization by lowering the generalization gap bounds.
- Mechanism: By applying generalization bounds theory, the paper shows that reducing the number of tunable parameters from full fine-tuning (|P|) to delta tuning (|ψ|) decreases the O(√|P|/n) term in the generalization bound, leading to tighter bounds and better generalization, especially when the dataset is small.
- Core assumption: The reduction in parameter space doesn't compromise expressivity below the level needed for the downstream task.
- Evidence anchors:
  - [abstract] "Building upon this, we provide a theoretical justification for PEFT can improve generalization of GNNs by applying generalization bounds"
  - [section] "Delta tuning reduces the size of the tunable parameter space. This is especially important in scenarios where the dataset is limited"
  - [corpus] Weak evidence - corpus contains related generalization theory but no direct validation of this specific application to GNNs
- Break condition: If the reduction is too aggressive, underfitting occurs and test error increases despite theoretical benefits.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message passing mechanism
  - Why needed here: AdapterGNN is specifically designed for GNNs, so understanding how GNNs aggregate neighborhood information is crucial for understanding where adapters are inserted
  - Quick check question: What are the two key inputs to each GNN layer that AdapterGNN processes with separate adapters?

- Concept: Generalization bounds and the bias-variance tradeoff
  - Why needed here: The paper's theoretical justification relies on understanding how generalization bounds relate to model complexity and parameter space size
  - Quick check question: According to the paper's theory, what happens to test error as model size increases in the classical regime?

- Concept: Adapter modules and bottleneck architecture
  - Why needed here: AdapterGNN's core innovation is its adapter design, so understanding how bottleneck adapters work is essential
  - Quick check question: What are the three components of the adapter module described in the paper?

## Architecture Onboarding

- Component map: Pre-trained GNN backbone (frozen) → Message Passing Layer → Two parallel adapters (one before MP, one after MP) → Batch Normalization → Learnable scaling → Output embedding
- Critical path: Input graph → GNN layers (frozen) → Adapter modules (tuned) → Classifier → Prediction
- Design tradeoffs: Expressivity vs parameter efficiency (larger bottleneck = more expressive but less efficient), knowledge preservation vs adaptation speed (learnable scaling = gradual adaptation but slower convergence)
- Failure signatures: Poor performance with very small bottleneck dimensions (underfitting), catastrophic forgetting with large initial scaling values, unstable training with inappropriate learning rates
- First 3 experiments:
  1. Run AdapterGNN with default settings on a small molecular dataset and compare ROC-AUC to frozen baseline
  2. Vary bottleneck dimension (5, 15, 30) and observe performance trade-offs
  3. Test different initial scaling values (0.01, 0.05, 0.1) to understand catastrophic forgetting dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific molecular and biological properties does AdapterGNN struggle to capture, and how could the adapter architecture be modified to address these limitations?
- Basis in paper: [inferred] The paper demonstrates AdapterGNN's effectiveness on graph-level classification tasks in molecular and biological domains but does not explore its performance on other task types or GNN architectures.
- Why unresolved: The authors only evaluate AdapterGNN on graph-level classification tasks using GIN and leave other tasks and GNN architectures for future exploration.
- What evidence would resolve it: Extensive experiments comparing AdapterGNN's performance on node-level and graph-level regression tasks, as well as with other GNN architectures like GAT and GraphSAGE.

### Open Question 2
- Question: What is the theoretical relationship between the bottleneck dimension in AdapterGNN and its generalization ability, and how can this relationship be quantified?
- Basis in paper: [inferred] The paper discusses the importance of the bottleneck dimension in reducing the size of the tunable parameter space and improving generalization, but does not provide a theoretical justification for this relationship.
- Why unresolved: The authors only empirically demonstrate the effectiveness of the bottleneck dimension and do not provide a theoretical explanation for its impact on generalization.
- What evidence would resolve it: A theoretical analysis of the relationship between the bottleneck dimension and generalization ability, supported by empirical evidence.

### Open Question 3
- Question: How does AdapterGNN's performance compare to other state-of-the-art GNN models on the same datasets, and what are the key factors contributing to its success?
- Basis in paper: [inferred] The paper compares AdapterGNN to other delta tuning methods and full fine-tuning, but does not compare it to other state-of-the-art GNN models.
- Why unresolved: The authors only focus on comparing AdapterGNN to delta tuning methods and do not consider other GNN models that may achieve similar or better performance.
- What evidence would resolve it: A comprehensive comparison of AdapterGNN's performance to other state-of-the-art GNN models on the same datasets, along with an analysis of the key factors contributing to its success.

## Limitations

- The theoretical generalization analysis relies on standard PAC-Bayes bounds that may not fully capture the specific advantages of AdapterGNN's architecture
- The paper only evaluates AdapterGNN on graph-level classification tasks using GIN architecture, leaving other task types and GNN architectures unexplored
- The mechanism claims about learnable scaling preventing catastrophic forgetting lack systematic empirical validation

## Confidence

**High Confidence:** The core empirical findings showing AdapterGNN outperforms full fine-tuning and other PEFT methods on downstream tasks. The experimental setup is well-defined and results are consistently positive across multiple datasets.

**Medium Confidence:** The theoretical generalization argument connecting parameter reduction to improved bounds. While the mathematical derivation follows standard approaches, the direct applicability to AdapterGNN's specific architecture needs more rigorous validation.

**Low Confidence:** The mechanism claims about how learnable scaling prevents catastrophic forgetting and optimizes the knowledge trade-off. These are presented as intuitive explanations but lack systematic empirical validation.

## Next Checks

1. **Mechanistic ablation:** Systematically vary the learnable scaling initialization and bottleneck dimensions across a wider range to quantify their impact on both performance and generalization gap.

2. **Architecture generalization:** Test AdapterGNN with different GNN backbones (GAT, GCN, GraphSAGE) to verify that generalization benefits aren't specific to the GIN architecture.

3. **Theory-experiment alignment:** Design experiments that directly test the generalization bound predictions, such as measuring how generalization gap changes with dataset size and parameter count to validate the theoretical scaling relationships.