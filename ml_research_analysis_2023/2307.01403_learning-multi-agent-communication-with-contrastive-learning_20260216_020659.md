---
ver: rpa2
title: Learning Multi-Agent Communication with Contrastive Learning
arxiv_id: '2307.01403'
source_url: https://arxiv.org/abs/2307.01403
tags:
- learning
- agents
- communication
- messages
- cacl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a contrastive learning approach (CACL) for
  multi-agent communication in partially observable environments. The key idea is
  to treat messages as incomplete "views" of the environment state, and use contrastive
  learning to maximize mutual information between messages from similar states.
---

# Learning Multi-Agent Communication with Contrastive Learning

## Quick Facts
- **arXiv ID:** 2307.01403
- **Source URL:** https://arxiv.org/abs/2307.01403
- **Reference count:** 21
- **Primary result:** CACL achieves 66.67% capture rate in Predator-Prey vs 51.67% for next best method

## Executive Summary
This paper introduces CACL, a contrastive learning approach for multi-agent communication in partially observable environments. The key innovation treats messages as incomplete "views" of the environment state and uses contrastive learning to maximize mutual information between messages from similar states. By pushing messages from similar states closer together and dissimilar states further apart, CACL learns more effective communication protocols. The method is evaluated on Traffic-Junction, Predator-Prey, and Find-Goal environments, consistently outperforming state-of-the-art baselines in both final performance and learning speed.

## Method Summary
CACL frames multi-agent communication as a multi-view representation learning problem, treating each agent's message as an incomplete view of the underlying state. The method uses a sliding window of 5 timesteps to identify positive samples (messages from similar states) and maximizes their mutual information using a SupCon-based contrastive objective. The overall loss combines standard RL loss with the contrastive loss (L = LRL + κLCACL), where κ controls the contribution of contrastive learning. Agents encode observations and messages through a GRU-based architecture, with messages being continuous vectors of dimensionality 4. The contrastive loss aligns the message space by pulling together messages from similar states and pushing apart messages from dissimilar states.

## Key Results
- CACL achieves 66.67% capture rate in Predator-Prey (vs 51.67% for next best method)
- CACL significantly reduces episode length in Find-Goal, achieving 12.17 steps vs 13.03 for baselines
- Protocol symmetry reaches 0.95 for CACL vs 0.89 for next best method
- Representation probing shows CACL messages better capture spatial information for navigation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CACL aligns message space by pushing messages from similar states closer together and messages from dissimilar states further apart
- Mechanism: Contrastive learning maximizes mutual information between messages from similar states using a sliding window as positive samples
- Core assumption: Messages from the same trajectory within a short time window represent similar underlying states
- Evidence anchors:
  - [abstract] "treat messages as incomplete 'views' of the environment state"
  - [section 4] "a window of timesteps within a trajectory to be all similar states"
  - [corpus] Weak - no direct corpus evidence of this specific contrastive mechanism
- Break condition: If message similarity doesn't correlate with state similarity, or if contrastive learning fails to learn meaningful representations

### Mechanism 2
- Claim: CACL induces more symmetric communication than baseline methods
- Mechanism: By treating messages as encoded views and aligning them through contrastive learning, agents learn to communicate more consistently when faced with the same observations
- Core assumption: Consistent communication patterns emerge when agents learn from each other's messages as representations of shared state information
- Evidence anchors:
  - [section 5.5] "CACL's protocol is very highly symmetric, clearly outperforming all others"
  - [section 4] "casting the problem in the multi-view perspective and implicitly aligning agents' messages"
  - [corpus] Weak - no direct corpus evidence of this specific symmetry mechanism
- Break condition: If contrastive alignment doesn't lead to consistent communication patterns, or if symmetry metrics don't correlate with performance

### Mechanism 3
- Claim: CACL messages capture global state information better than baselines
- Mechanism: By leveraging contrastive learning across multiple agents' views, CACL learns to encode more comprehensive state representations that include spatial and semantic information
- Core assumption: Multiple incomplete views of the same state contain complementary information that can be combined through contrastive learning
- Evidence anchors:
  - [section 5.6] "CACL learns to compress meaningful, global state information in messages"
  - [section 5.6] "CACL's protocol significantly outperforms baselines" in goal location classification
  - [corpus] Weak - no direct corpus evidence of this specific information capture mechanism
- Break condition: If contrastive learning doesn't improve representation quality, or if global information isn't captured in messages

## Foundational Learning

- Concept: Contrastive learning and mutual information maximization
  - Why needed here: CACL uses contrastive learning to align message space by maximizing mutual information between messages from similar states
  - Quick check question: What is the key difference between CACL's contrastive approach and traditional autoencoding methods?

- Concept: Multi-view representation learning
  - Why needed here: CACL treats agents' messages as different incomplete views of the same underlying state, similar to multi-view learning in computer vision
  - Quick check question: How does the multi-view perspective justify using messages from the same trajectory as positive samples?

- Concept: Representation probing and evaluation
  - Why needed here: CACL uses message clustering and classification to evaluate how much semantic and spatial information is captured in the learned communication protocol
  - Quick check question: Why is the goal location classification task more difficult than the goal visibility classification task?

## Architecture Onboarding

- Component map: Observation → Message encoder → Communication head → Contrastive learning loss → Message → GRU → Policy/Value heads
- Critical path: Observation → Message encoder → Communication head → Contrastive learning loss → Message → GRU → Policy/Value heads
- Design tradeoffs:
  - Continuous vs discrete messages: Continuous messages are standard in contrastive learning but may be harder to interpret
  - Sliding window size: Balances positive sample quality vs computational efficiency
  - Message dimensionality: Higher dimensions may capture more information but increase computational cost
- Failure signatures:
  - Poor performance despite learning: May indicate contrastive loss isn't effective or messages aren't informative
  - Unstable training: May indicate learning rate or temperature hyperparameter issues
  - Low protocol symmetry: May indicate messages aren't aligning properly across agents
- First 3 experiments:
  1. Ablation study: Remove contrastive loss to verify its importance for performance
  2. Window size variation: Test different sliding window sizes to find optimal balance
  3. Cross-play evaluation: Test trained agents with novel partners to assess zero-shot communication robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can contrastive learning be combined with reward-oriented gradients to improve multi-agent communication without compromising the stability of the learned protocols?
- Basis in paper: [explicit] The paper mentions that augmenting CACL with DIAL (which involves reward-oriented gradients) generally performs worse, except in Find-Goal, where performances are similar but not better.
- Why unresolved: The paper notes that the phenomenon of decentralized DIAL being a complex, high-variance optimization that is difficult to stabilize is well known but not fully understood.
- What evidence would resolve it: Experimental results showing a stable method that successfully combines contrastive learning with reward-oriented gradients, demonstrating improved performance over either method alone.

### Open Question 2
- Question: How can the learned communication protocols be made robust to zero-shot coordination with unseen partners?
- Basis in paper: [explicit] The paper discusses the challenge of zero-shot communication, an extension of zero-shot cooperation, where agents must communicate effectively with novel partners unseen during training. The results show that existing methods perform poorly in this setting.
- Why unresolved: The paper indicates that zero-shot linguistic communication is incredibly difficult and far from optimal, and existing methods are not effective.
- What evidence would resolve it: Development and experimental validation of a method that significantly improves zero-shot cross-play performance, demonstrating the ability to communicate effectively with unseen partners.

### Open Question 3
- Question: What is the optimal design for the contrastive learning objective in multi-agent communication to balance exploration and exploitation?
- Basis in paper: [explicit] The paper discusses the importance of design decisions behind CACL, such as the choice of sliding window size and the use of SupCon, which were crucial for its success.
- Why unresolved: While the paper shows the importance of these design choices, it does not explore a systematic approach to balancing exploration and exploitation in the contrastive learning objective.
- What evidence would resolve it: A comprehensive study that explores various configurations of the contrastive learning objective, including different window sizes, positive sample selection strategies, and temperature settings, to determine the optimal balance for multi-agent communication tasks.

## Limitations
- Contrastive learning relies on the assumption that nearby timestep messages represent similar states, which may not hold in rapidly changing environments
- The 5-timestep sliding window is somewhat arbitrary and may need tuning for different environment dynamics
- Analysis doesn't deeply explore potential failure modes or edge cases where the method might break down

## Confidence
- **High confidence**: CACL's superior task performance (capture rate, episode length, success rate) across all three tested environments
- **Medium confidence**: Protocol symmetry improvements, as this metric may be sensitive to implementation details and normalization
- **Medium confidence**: Representation probing results showing CACL captures more global state information, though the probing tasks themselves may have limitations

## Next Checks
1. **Ablation study**: Remove the contrastive learning component to verify that performance improvements are directly attributable to the contrastive loss and not other architectural choices.

2. **Cross-play robustness**: Test whether CACL-trained agents can effectively communicate with agents trained using different methods or with novel partners to assess zero-shot communication capabilities.

3. **Dynamic environment testing**: Evaluate CACL in environments with rapidly changing states or longer time horizons to test the robustness of the 5-timestep sliding window assumption.