---
ver: rpa2
title: 'MMHQA-ICL: Multimodal In-context Learning for Hybrid Question Answering over
  Text, Tables and Images'
arxiv_id: '2309.04790'
source_url: https://arxiv.org/abs/2309.04790
tags:
- question
- table
- questions
- image
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles multimodal hybrid question answering (MMHQA)
  over text, tables, and images using an end-to-end in-context learning (ICL) approach.
  The MMHQA-ICL framework features a stronger heterogeneous data retriever, an image
  captioning module using LLaVA, and a Type-specific In-context Learning Strategy
  that selects different prompting strategies based on question type.
---

# MMHQA-ICL: Multimodal In-context Learning for Hybrid Question Answering over Text, Tables and Images

## Quick Facts
- arXiv ID: 2309.04790
- Source URL: https://arxiv.org/abs/2309.04790
- Authors: 
- Reference count: 8
- Key outcome: MMHQA-ICL achieves 65.8 F1 and 54.8 EM on MultimodalQA, outperforming SKURG by 8.7% F1 and 3.8% EM using few-shot in-context learning

## Executive Summary
This paper addresses multimodal hybrid question answering over text, tables, and images using an end-to-end in-context learning approach. The framework eliminates intermediate SQL generation errors by directly prompting large language models with carefully constructed examples. Through type-specific prompting strategies and LLaVA-based image captioning, the system achieves state-of-the-art performance on the MultimodalQA dataset under few-shot settings, demonstrating significant improvements over previous methods that required full dataset training.

## Method Summary
The framework implements end-to-end in-context learning for multimodal question answering, bypassing SQL generation to avoid syntax errors. It uses a DeBERTa-large classifier for question type detection and document retrieval, LLaVA for rich image captioning, and type-specific prompting strategies that select between Chain-of-Thought and direct prompting based on question modality. The system constructs prompts using retrieved evidence from text, tables, and images, then generates answers through the text-davinci-003 API without fine-tuning.

## Key Results
- Achieves 65.8 F1 and 54.8 EM on MultimodalQA development set
- Outperforms SKURG by 8.7% F1 and 3.8% EM despite SKURG being trained on full dataset
- Shows 5.7% F1 improvement on text-only questions compared to SKURG
- Demonstrates effectiveness of type-specific prompting across different question modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: End-to-end LLM prompting avoids errors from intermediate SQL generation
- Mechanism: Direct answer generation through LLM prompting eliminates syntax errors inherent in generated SQL
- Core assumption: LLM-generated SQL queries frequently contain syntax errors that propagate to final answers
- Evidence anchors: [abstract]: "our framework adopts an end-to-end approach that generates the answer directly without generating intermediate SQL queries." [section]: "Binder (Cheng et al., 2022) develops a new text-to-SQL pipeline... However, this approach often fails to answer questions due to SQL syntax errors."

### Mechanism 2
- Claim: Type-specific in-context learning strategy improves performance by matching prompt style to question type
- Mechanism: Different prompting strategies (CoT vs noCoT) are selected based on question type, optimizing token usage and reasoning approach
- Core assumption: Not all question types benefit equally from chain-of-thought prompting; some require simpler, more direct approaches
- Evidence anchors: [section]: "We leverage Chain-of-Thought (CoT) (Wei et al., 2022) for all questions at the beginning. However, in subsequent experiments, we found that this technology is not universally applicable... Thus, we concluded that not all types of questions are suitable for using the chain of thought technology."

### Mechanism 3
- Claim: LLaVA-based premium captioning provides richer semantic information than traditional captioning methods
- Mechanism: LLaVA generates captions containing more features (colors, characters, movements, objects) compared to traditional match-based metrics
- Core assumption: Traditional image captioning methods produce limited semantic content that doesn't capture the diverse features needed for multimodal reasoning
- Evidence anchors: [section]: "Traditional image captioning models... Consequently, we introduce a LLaVA-based Premium Captioning Module which can generate rich caption texts through LLaVA (Liu et al., 2023a)." [section]: "LLaV A is a large Language and Vision Assistant... For example, the picture containing a palm tree in Figure 1 is captioned into text 'The image features a blue flag with a white palmetto tree on it, which represents the state of South Carolina.'"

## Foundational Learning

- Concept: Multimodal reasoning over heterogeneous data sources
  - Why needed here: The framework must integrate information from text, tables, and images to answer complex questions requiring multi-hop reasoning
  - Quick check question: How does the framework handle questions that require information from all three modalities simultaneously?

- Concept: In-context learning and prompt engineering
  - Why needed here: The framework relies on few-shot prompting with LLMs rather than fine-tuning, requiring careful selection of demonstration examples
  - Quick check question: What are the key differences between the prompt strategies used for image-only questions versus composite questions?

- Concept: Retrieval-augmented generation
  - Why needed here: The framework must retrieve relevant documents from multiple modalities before generating answers
  - Quick check question: How does the retrieval module handle the different representations of image captions versus text passages?

## Architecture Onboarding

- Component map: Image Captioning: LLaVA-based Premium Captioning Module -> Document Retrieval: Classifier and Retriever Module (DeBERTa-large) -> Prompt Generation: Type-specific In-context Learning Strategy -> Answer Generation: LLM (text-davinci-003 API)
- Critical path: Question → Classification → Retrieval → Prompt Generation → LLM Answer
- Design tradeoffs:
  - End-to-end prompting vs. intermediate representation (SQL): Trade simplicity and robustness for potential precision
  - Type-specific vs. universal prompting: Trade performance optimization for implementation complexity
  - LLaVA captioning vs. traditional methods: Trade caption richness for computational overhead
- Failure signatures:
  - Classification errors → Wrong document retrieval → Poor answers
  - Retrieval recall below 80% → Missing critical evidence
  - Token limit exceeded in prompts → Truncated context → Incorrect reasoning
  - LLM API failures → System downtime
- First 3 experiments:
  1. Verify classification accuracy on validation set (target: >95%)
  2. Test retrieval recall with oracle document selection (target: >99% for passages, >80% for images)
  3. Compare performance with/without LLaVA captioning on image-related questions (target: significant improvement with LLaVA)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Type-specific In-context Learning Strategy perform on other multimodal question answering datasets beyond MultimodalQA?
- Basis in paper: [explicit] The paper mentions that the model achieves state-of-the-art results on the MultimodalQA dataset, but does not provide evidence of performance on other datasets.
- Why unresolved: The paper only focuses on one dataset, so the generalizability of the approach to other multimodal QA tasks is unclear.
- What evidence would resolve it: Testing the model on other multimodal QA datasets like HybridQA or OTTQA and comparing its performance to other state-of-the-art methods would provide insights into its generalizability.

### Open Question 2
- Question: What is the impact of using different image captioning models (e.g., OFA, BLIP) on the performance of the proposed framework?
- Basis in paper: [explicit] The paper introduces a LLaVA-based Premium Captioning Module and compares its performance with traditional captioning methods, but does not explore other captioning models.
- Why unresolved: The paper does not provide a comprehensive comparison of different image captioning models and their impact on the overall performance of the framework.
- What evidence would resolve it: Conducting experiments with different image captioning models and comparing their performance in terms of retrieval and reasoning capabilities would provide insights into the importance of the captioning model choice.

### Open Question 3
- Question: How does the proposed framework handle questions that require reasoning over multiple tables or combining information from multiple tables and images?
- Basis in paper: [explicit] The paper mentions that the framework can handle questions that require reasoning over text, tables, and images, but does not provide specific examples or performance metrics for questions involving multiple tables or combining tables and images.
- Why unresolved: The paper does not provide evidence of the framework's ability to handle more complex questions involving multiple tables or combining tables and images.
- What evidence would resolve it: Creating a new dataset or subset of questions that specifically require reasoning over multiple tables or combining tables and images, and testing the framework's performance on these questions, would provide insights into its capabilities in handling such complex scenarios.

## Limitations

- Reliance on specific API access (text-davinci-003) and proprietary models (LLaVA) creates significant reproducibility barriers
- Type-specific In-context Learning Strategy requires manual classification of question types without clear decision boundaries
- Performance gains measured only on MultimodalQA dataset, raising questions about external validity across different domains

## Confidence

- High Confidence: End-to-end LLM prompting avoids SQL syntax errors, well-supported by literature and empirical results
- Medium Confidence: Type-specific In-context Learning Strategy's effectiveness supported by ablation studies but limited by manual classification
- Low Confidence: Superiority of LLaVA-based captioning asserted but not empirically validated against alternative approaches

## Next Checks

1. **Cross-dataset generalization test**: Evaluate MMHQA-ICL on at least two additional multimodal QA datasets (e.g., HybridQA, TAT-QA) to assess whether the 8.7% F1 improvement over SKURG generalizes beyond MultimodalQA, using identical few-shot settings.

2. **Ablation on captioning alternatives**: Replace LLaVA with three alternative captioning methods (traditional transformer, BLIP, and GPT-4V) while keeping all other components constant, measuring both caption quality (via standard metrics) and downstream QA performance to quantify the actual contribution of premium captioning.

3. **Automated vs. manual classification comparison**: Implement an automated question type classifier and compare its classification accuracy and resulting QA performance against the manual classification approach described in the paper, measuring the sensitivity of the Type-specific ICL Strategy to classification errors.