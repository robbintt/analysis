---
ver: rpa2
title: An intelligent tutor for planning in large partially observable environments
arxiv_id: '2302.02785'
source_url: https://arxiv.org/abs/2302.02785
tags:
- planning
- tutor
- strategy
- people
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We developed the first intelligent tutor for planning in partially
  observable environments, combining a new metareasoning algorithm for discovering
  optimal planning strategies and scaffolding the learning process through incremental
  complexity. The algorithm, MGPO, outperformed state-of-the-art methods with higher
  resource-rationality scores while being substantially faster (0.03 seconds vs.
---

# An intelligent tutor for planning in large partially observable environments

## Quick Facts
- arXiv ID: 2302.02785
- Source URL: https://arxiv.org/abs/2302.02785
- Authors: 
- Reference count: 40
- Primary result: MGPO algorithm discovered planning strategies that outperformed baselines while being 100-500x faster, and when taught through an intelligent tutor, human performance matched algorithmic performance

## Executive Summary
This paper presents the first intelligent tutor for planning in partially observable environments, combining a new meta-reasoning algorithm (MGPO) with a cognitive tutoring system. The MGPO algorithm efficiently discovers resource-rational planning strategies through a myopic approximation of value of computation, outperforming state-of-the-art methods while being substantially faster. A preregistered experiment with 330 participants demonstrated that people's intuitive planning strategies are highly suboptimal, but training with the intelligent tutor significantly improved decision-making to match algorithmic performance. This work bridges the gap between algorithmic planning and human decision-making, showing that AI-discovered strategies can be effectively taught to humans through structured tutoring.

## Method Summary
The method combines MGPO for strategy discovery with a cognitive tutor for human training. MGPO uses a myopic approximation of value of computation to efficiently balance information gathering and planning termination in partially observable environments. The cognitive tutor implements a shaping methodology, starting with simple environments and gradually increasing complexity, combined with video demonstrations and binary feedback based on MGPO's VOC calculations. The system was tested through an online experiment with 330 participants across three conditions (Choice Tutor, No Tutor, and Dummy Tutor) using benchmark planning environments with 2-5 blocks of 18 nodes each.

## Key Results
- MGPO achieved higher resource-rationality scores than baseline methods while being 100-500x faster (0.03 seconds vs 3-14 seconds)
- Human participants in the Choice Tutor condition achieved similar resource-rationality scores to MGPO itself
- People's intuitive planning strategies were highly suboptimal, with significant improvements only seen in tutored participants
- The scaffolding approach enabled effective learning of complex planning strategies in partially observable environments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The intelligent tutor improves human planning by teaching resource-rational strategies discovered by MGPO.
- Mechanism: MGPO discovers planning strategies through meta-greedy approximation of value of computation in partially observable environments, which are then taught to humans through iterative feedback and scaffolding.
- Core assumption: The MGPO-discovered strategies are superior to human intuitive strategies and can be effectively transferred to humans through tutoring.
- Evidence anchors:
  - [abstract]: "people's intuitive planning strategies were highly suboptimal, but training with the intelligent tutor significantly improved decision-making, with tutored participants achieving similar resource-rationality scores to the algorithm itself"
  - [section]: "Our experiment is one of the first to demonstrate that people do not make fully rational use of their limited cognitive resources"
  - [corpus]: Weak - The corpus papers focus on intelligent tutor platforms and applications but don't address the specific mechanism of teaching resource-rational planning strategies in partially observable environments.
- Break condition: If human learners cannot effectively transfer the discovered strategies to novel problems, or if the feedback mechanism fails to provide actionable guidance.

### Mechanism 2
- Claim: The MGPO algorithm outperforms baseline methods in discovering resource-rational planning strategies.
- Mechanism: MGPO uses a myopic value of computation approximation that efficiently balances information gathering and planning termination in partially observable environments.
- Core assumption: The myopic approximation of VOC is sufficient to discover near-optimal planning strategies without the computational burden of full meta-planning.
- Evidence anchors:
  - [abstract]: "The algorithm, MGPO, outperformed state-of-the-art methods with higher resource-rationality scores while being substantially faster (0.03 seconds vs. 3-14 seconds)"
  - [section]: "Our results show that our method is consistently better than alternative methods with a similar computational budget and much faster"
  - [corpus]: Weak - Corpus papers don't address meta-reasoning algorithms for partially observable environments or their computational efficiency.
- Break condition: If the myopic approximation fails to capture important long-term planning considerations, leading to suboptimal strategies.

### Mechanism 3
- Claim: The cognitive tutor's scaffolding approach enables effective learning of complex planning strategies.
- Mechanism: The tutor uses shaping methodology, starting with simple environments and gradually increasing complexity, combined with video demonstrations and binary feedback.
- Core assumption: Humans can learn complex planning strategies through incremental exposure and feedback, even in partially observable environments.
- Evidence anchors:
  - [abstract]: "scaffolding the learning process through incremental complexity"
  - [section]: "we introduced an improved teaching methodology and training schedule to the cognitive tutor... starting with a smaller version of the decision-task and incrementally add complexity"
  - [corpus]: Weak - While corpus papers discuss intelligent tutoring systems, they don't specifically address scaffolding approaches for teaching planning strategies in partially observable environments.
- Break condition: If learners become frustrated with the incremental approach or fail to generalize strategies to more complex scenarios.

## Foundational Learning

- Concept: Meta-level Markov Decision Processes (MDPs)
  - Why needed here: The problem of discovering planning strategies is formalized as a meta-level MDP where actions are computations that reveal information about the environment.
  - Quick check question: What is the difference between the object-level MDP (the planning problem) and the meta-level MDP (the strategy discovery problem)?

- Concept: Value of Computation (VOC)
  - Why needed here: VOC represents the expected benefit of performing a computation, which is crucial for both MGPO's strategy discovery and the tutor's feedback mechanism.
  - Quick check question: How does the myopic approximation of VOC differ from a full meta-planning approach?

- Concept: Resource-rationality
  - Why needed here: This formal measure evaluates planning strategies by considering both the expected utility of plans and the cost of computations, providing a metric for comparing human and algorithmic performance.
  - Quick check question: Why is resource-rationality a more appropriate measure than traditional expected utility in planning problems with cognitive constraints?

## Architecture Onboarding

- Component map: MGPO algorithm -> VOC calculation -> Cognitive tutor interface -> Human learner -> Partially observable environment simulator
- Critical path: Strategy discovery (MGPO) → Tutor training (scaffolding + feedback) → Human learning (practice trials) → Evaluation (test trials)
- Design tradeoffs: Computational efficiency vs. planning optimality (MGPO uses myopic approximation), simplicity of feedback vs. precision (binary feedback instead of exact Q-values), complexity of environments vs. learning effectiveness (scaffolding approach)
- Failure signatures: Poor transfer of strategies to novel problems, frustration with incremental complexity, computational bottlenecks in real-time feedback, or ineffective feedback mechanisms
- First 3 experiments:
  1. Implement MGPO algorithm on a simple 2-goal environment and compare VOC calculations to ground truth
  2. Build the tutor interface with binary feedback and test on the 8-node environment
  3. Conduct a pilot study with 20 participants to test the scaffolding approach and gather initial learning curves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MGPO's myopic value of computation (VOC) approximation perform compared to methods that consider multiple future computation steps?
- Basis in paper: [explicit] The paper notes that MGPO's myopic approach may underestimate VOC by not considering future information value, and suggests this could be improved by planning for a fixed number of computations or using approximation methods.
- Why unresolved: The paper chose computational efficiency over looking multiple steps ahead, but didn't test how much better performance could be achieved with more lookahead.
- What evidence would resolve it: Head-to-head comparison of MGPO's myopic VOC with versions that look 2-5 computation steps ahead, measuring both performance and computation time.

### Open Question 2
- Question: Can people transfer the planning strategies learned in the flight planning task to real-world decision-making contexts?
- Basis in paper: [inferred] The paper discusses potential real-world applications but only tested transfer within similar artificial planning tasks, not actual real-world domains.
- Why unresolved: The experiment only tested performance on similar artificial planning problems, not whether people can apply the learned strategies to their actual lives.
- What evidence would resolve it: Longitudinal studies tracking whether tutored participants make better decisions in real-life planning contexts (career planning, financial decisions, etc.) compared to control groups.

### Open Question 3
- Question: What are the cognitive mechanisms people use to approximate rational metareasoning, and how can they be improved?
- Basis in paper: [explicit] The paper suggests people's metareasoning might be simpler than the meta-greedy approximation and could be improved through feedback and shaping.
- Why unresolved: The paper proposes model-free reinforcement learning as a possibility but doesn't empirically investigate the actual cognitive mechanisms people use.
- What evidence would resolve it: Cognitive modeling studies comparing human behavior to different metareasoning approximations, combined with neuroimaging to identify the neural mechanisms involved.

## Limitations

- The binary feedback mechanism may not provide sufficient granularity for learners to understand precise value differences between planning actions
- The controlled experimental conditions may not capture the complexity and variability of real-world planning scenarios
- The computational superiority of MGPO is demonstrated but may not generalize to domains with significantly different reward structures

## Confidence

- High confidence: The computational superiority of MGPO over baseline methods (0.03s vs 3-14s) and its improved resource-rationality scores are well-supported by systematic benchmarking across multiple environments.
- Medium confidence: The transfer of algorithmic strategies to human learners is demonstrated but could be influenced by experimental artifacts such as practice effects or task familiarity rather than genuine strategy adoption.
- Low confidence: The generalizability of the scaffolding approach to more complex real-world planning domains remains untested, as the current work focuses on relatively structured benchmark environments.

## Next Checks

1. Conduct a replication study with a delayed posttest (e.g., one week later) to assess whether learning gains persist and whether they stem from genuine strategy acquisition rather than short-term memory effects.

2. Implement a more granular feedback system that provides approximate Q-value differences rather than binary feedback, then compare learning outcomes to determine if the simplified feedback mechanism limits strategy transfer.

3. Test the intelligent tutor approach on a real-world planning domain (such as logistics routing or project scheduling) with professional planners to evaluate external validity and practical applicability beyond controlled experimental settings.