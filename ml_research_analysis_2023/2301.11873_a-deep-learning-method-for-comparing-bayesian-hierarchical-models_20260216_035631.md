---
ver: rpa2
title: A Deep Learning Method for Comparing Bayesian Hierarchical Models
arxiv_id: '2301.11873'
source_url: https://arxiv.org/abs/2301.11873
tags:
- data
- hierarchical
- network
- bayesian
- sets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of Bayesian model comparison
  for complex hierarchical models, which is analytically intractable due to high-dimensional
  parameter spaces. The authors propose a deep learning method using hierarchical
  neural networks to approximate posterior model probabilities for any set of hierarchical
  models, including those with intractable likelihoods.
---

# A Deep Learning Method for Comparing Bayesian Hierarchical Models

## Quick Facts
- **arXiv ID**: 2301.11873
- **Source URL**: https://arxiv.org/abs/2301.11873
- **Reference count**: 40
- **Key outcome**: Deep learning method for Bayesian model comparison using hierarchical neural networks, validated on complex cognitive models with missing data

## Executive Summary
This paper addresses the challenge of Bayesian model comparison (BMC) for complex hierarchical models, which is analytically intractable due to high-dimensional parameter spaces. The authors propose a deep learning method using hierarchical neural networks to approximate posterior model probabilities (PMPs) for any set of hierarchical models, including those with intractable likelihoods. The method leverages permutation invariance and amortization for efficient inference, enabling rapid re-estimation of PMPs after an initial training phase. Extensive validation studies show excellent calibration (median Expected Calibration Error < 0.012) and competitive performance against bridge sampling, with accurate recovery (median accuracy > 0.88) even with missing data.

## Method Summary
The method uses a hierarchical invariant neural network architecture that encodes permutation invariance to respect the exchangeability of hierarchical models. The network consists of deep invariant modules that perform equivariant transformations followed by pooling at each hierarchical level. An inference network maps the learned summary representation to PMPs using a softmax classifier. The method is trained on simulated data from competing models using backpropagation and the Adam optimizer with logarithmic loss. Transfer learning is employed to accelerate training by pre-training on simpler data sets and fine-tuning on target data. The trained network can instantly compute PMPs for any new data set without refitting models or performing expensive computations.

## Key Results
- Excellent calibration with median Expected Calibration Error < 0.012 across simulation studies
- High accuracy in model recovery with median accuracy > 0.88, competitive with bridge sampling
- Robust performance on real cognitive data, including models with intractable likelihoods and missing data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The hierarchical invariant neural network architecture encodes the probabilistic symmetry (exchangeability) of hierarchical models, enabling efficient amortization.
- **Mechanism**: The architecture uses deep invariant modules that respect the nested structure of hierarchical models. Each module performs equivariant transformations followed by pooling, ensuring that the output is independent of the ordering of exchangeable units at each level.
- **Core assumption**: The data at each hierarchical level is exchangeable, meaning the joint probability distribution is invariant to permutations of units within that level.
- **Evidence anchors**: [abstract] "The method leverages permutation invariance and amortization for efficient inference."
- **Break condition**: If the data at any hierarchical level violates exchangeability (e.g., time series with temporal dependencies), the invariant architecture would no longer be appropriate.

### Mechanism 2
- **Claim**: Amortized inference allows rapid re-estimation of posterior model probabilities for any new data set after a potentially high computational investment in training.
- **Mechanism**: The neural network is trained on a large number of simulated data sets from the competing models. Once trained, it can instantly compute PMPs for any new data set without needing to refit the models or perform expensive computations like bridge sampling.
- **Core assumption**: The trained network generalizes well to new data sets from the same model family and can accurately approximate the PMP mapping function.
- **Evidence anchors**: [abstract] "Since our method enables amortized inference, it allows efficient re-estimation of posterior model probabilities and fast performance validation prior to any real-data application."
- **Break condition**: If the training data does not adequately represent the range of data sets the network will encounter, or if the models are too complex for the network to learn the PMP mapping accurately.

### Mechanism 3
- **Claim**: Transfer learning accelerates training by reusing representations learned from simpler or smaller data sets.
- **Mechanism**: The network is first pre-trained on simulated data sets with reduced complexity (e.g., fewer observations per group). It is then fine-tuned on data sets that match the characteristics of the target empirical data. This leverages the initial learning and reduces the amount of data and computation needed for the final training.
- **Core assumption**: The simpler or smaller data sets share relevant features with the target data, allowing the pre-trained network to serve as a good starting point.
- **Evidence anchors**: [abstract] "Additionally, we demonstrate how transfer learning can be leveraged to enhance training efficiency."
- **Break condition**: If the pre-training data is too dissimilar from the target data, transfer learning may not provide significant benefits and could even hinder learning.

## Foundational Learning

- **Concept**: Permutation invariance and exchangeability
  - Why needed here: Hierarchical models assume exchangeable data at each level, and the neural network architecture must respect this symmetry to accurately approximate PMPs.
  - Quick check question: If we permute the order of observations within a group in a hierarchical model, should the PMP approximation change? (Answer: No, due to exchangeability)

- **Concept**: Amortized inference
  - Why needed here: BMC is computationally expensive, especially for complex hierarchical models. Amortization allows for rapid inference after an initial training phase, enabling extensive validation and robustness checks.
  - Quick check question: What is the key difference between amortized and non-amortized methods in terms of computational cost per data set? (Answer: Amortized methods have a high upfront cost but low per-data-set cost, while non-amortized methods have high cost per data set)

- **Concept**: Transfer learning
  - Why needed here: Training a neural network for complex hierarchical model comparison can be computationally intensive. Transfer learning can significantly reduce this cost by reusing learned representations from related tasks.
  - Quick check question: What is the main benefit of pre-training a network on simpler data sets before fine-tuning it on the target data? (Answer: It reduces the amount of data and computation needed for the final training phase)

## Architecture Onboarding

- **Component map**: Hierarchical summary network (deep invariant modules) -> Inference network (softmax classifier) -> Training loop (simulation, backpropagation)

- **Critical path**:
  1. Design and implement the hierarchical invariant network architecture.
  2. Set up the training pipeline to simulate data and compute true PMPs.
  3. Train the network using backpropagation and the Adam optimizer.
  4. Validate the trained network on held-out data and assess calibration and accuracy.
  5. Apply the trained network to empirical data for model comparison.

- **Design tradeoffs**:
  - Network capacity vs. overfitting: A larger network may be more expressive but also more prone to overfitting, especially with limited training data.
  - Training time vs. amortization gains: A longer training phase may yield better approximations but also increase the upfront computational cost.
  - Pre-training vs. direct training: Pre-training can accelerate learning but requires additional design choices and may not always be beneficial.

- **Failure signatures**:
  - Poor calibration: The network consistently over- or under-estimates the PMPs, indicating a failure to learn the correct mapping.
  - Low accuracy: The network's predictions do not match the true model underlying the data, suggesting a lack of discriminative power.
  - Slow convergence: The network takes a long time to train or does not converge at all, indicating issues with the architecture or training process.

- **First 3 experiments**:
  1. Implement a simple hierarchical invariant network and train it on a toy model comparison task (e.g., nested normal models). Assess calibration and accuracy.
  2. Extend the network to handle variable data set sizes and test its ability to amortize over different numbers of observations and groups.
  3. Implement transfer learning by pre-training the network on simpler data sets and fine-tuning it on the target data. Compare the training time and performance to direct training.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the method perform for models with more than two hierarchical levels?
- **Basis in paper**: [explicit] The authors state "we hold that any method which does not rely on ad hoc summary statistics should take this probabilistic symmetry (e.g., exchangeability) into account" and discuss extending the architecture to handle non-exchangeable models, but do not provide empirical validation for more than two levels.
- **Why unresolved**: The method is currently only validated on two-level hierarchical models. The authors mention that the modularity of the method allows easy adaptation to handle non-exchangeable models and potentially more hierarchical levels, but they do not provide empirical evidence for this claim.
- **What evidence would resolve it**: Empirical validation studies comparing the method's performance on models with three or more hierarchical levels against existing methods like bridge sampling or other BMC approaches.

### Open Question 2
- **Question**: What is the impact of the choice of equivariant module depth (K) and invariant module depth (K') on the method's performance?
- **Basis in paper**: [explicit] The authors mention that "the number of equivariant modules K' for level 2 can differ from the number of equivariant modules K for level 1" and that "the performance of the networks is largely insensitive to the choice of K or K'", but they do not provide a detailed analysis of how these hyperparameters affect the results.
- **Why unresolved**: The authors state that the performance is "largely insensitive" to these choices, but they do not provide a thorough exploration of the hyperparameter space or a sensitivity analysis.
- **What evidence would resolve it**: A comprehensive sensitivity analysis exploring different combinations of K and K' values and their impact on the method's calibration, accuracy, and computational efficiency.

### Open Question 3
- **Question**: How does the method handle hierarchical models with non-Gaussian likelihoods?
- **Basis in paper**: [inferred] The authors demonstrate the method's applicability to models with intractable likelihoods, but they do not explicitly test it on models with non-Gaussian likelihoods.
- **Why unresolved**: While the method is designed to work with any hierarchical model that can be instantiated as a probabilistic program, the authors only validate it on models with Gaussian likelihoods or those that can be approximated as such.
- **What evidence would resolve it**: Empirical validation studies comparing the method's performance on hierarchical models with non-Gaussian likelihoods (e.g., Poisson, binomial, or heavy-tailed distributions) against existing methods or analytical solutions when available.

## Limitations
- Method requires careful architectural design to respect permutation invariance, which may not hold for non-exchangeable data structures
- Network capacity and training data requirements scale with model complexity, potentially limiting applicability to extremely high-dimensional models
- Approach depends on quality simulated training data, and poor priors or simulator settings could lead to biased approximations

## Confidence
- **High confidence**: Calibration results (ECE < 0.012) and accuracy metrics (> 0.88) across simulation studies
- **Medium confidence**: Transfer learning benefits and missing data handling claims, as these are demonstrated but not extensively validated
- **Medium confidence**: Claims about computational efficiency gains relative to bridge sampling, as these depend on specific implementation choices

## Next Checks
1. Test the method on non-exchangeable hierarchical models (e.g., time series data) to verify break conditions for the permutation invariance assumption
2. Systematically vary network architecture parameters (depth, width) to establish scaling relationships with model complexity
3. Conduct ablation studies removing transfer learning to quantify its actual contribution to training efficiency