---
ver: rpa2
title: 'Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement
  Learning Approach'
arxiv_id: '2306.03604'
source_url: https://arxiv.org/abs/2306.03604
tags:
- agent
- planner
- llms
- when
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an RL-based method called When2Ask to enable
  efficient interaction between an embodied agent and a large language model (LLM)
  planner. The approach learns a mediator policy that decides when to query the LLM
  for high-level instructions during task execution, aiming to reduce communication
  costs while maintaining task performance.
---

# Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach

## Quick Facts
- arXiv ID: 2306.03604
- Source URL: https://arxiv.org/abs/2306.03604
- Reference count: 3
- Primary result: Learned mediator policy achieves competitive task success with significantly fewer LLM interactions compared to always-asking baselines

## Executive Summary
This paper introduces When2Ask, a reinforcement learning-based method that enables an embodied agent to efficiently interact with a large language model (LLM) planner. The approach learns a mediator policy that decides when to query the LLM for high-level instructions during task execution, aiming to reduce communication costs while maintaining task performance. The agent is tested on four partially observable MiniGrid environments requiring exploration and planning. Results show that When2Ask achieves competitive task success rates with significantly fewer interactions with the LLM compared to baseline methods that always query the LLM or use hard-coded interaction rules. The learned mediator also demonstrates improved robustness in handling uncertainties during execution, such as unexpected errors or newly acquired information.

## Method Summary
When2Ask uses a three-component framework: a pre-trained LLM planner generates high-level plans, an actor executes low-level actions following the planned options, and a mediator (trained via PPO) decides when to query the LLM. The mediator receives the difference between current and previous observations as input, encouraging it to ask only when something changes in the environment. The reward function includes a penalty for non-informative interactions, incentivizing the mediator to balance task performance and communication cost. The method is evaluated on four MiniGrid-based environments with partial observability and procedural generation, comparing performance against baseline methods including hard-coded, always-asking, and random interaction policies.

## Key Results
- When2Ask achieves competitive task success rates with 40-70% fewer LLM interactions across all four environments
- The learned mediator demonstrates improved robustness to uncertainties in other framework components
- Performance is maintained across different LLM sizes (Vicuna-7b for simpler environments, Vicuna-13b for more complex tasks)
- The approach outperforms hard-coded baselines that require domain expertise to determine when to ask

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The mediator learns to ask the LLM only when observations change significantly, reducing non-informative interactions.
- Mechanism: The mediator policy receives difference between current and previous observations as input, encouraging it to ask only when something changes in the environment.
- Core assumption: Changes in observations indicate potentially new information that might require LLM re-planning.
- Evidence anchors:
  - [abstract] "The agent is tested on four partially observable MiniGrid environments requiring exploration and planning."
  - [section 3.2] "We takes the difference between two frames before passing it to the network, so that it is encouraged to take the ask action only when something changes in the environment."
  - [corpus] Weak evidence - related papers focus on LLM-agent interactions but don't specifically address observation-difference-based asking policies.
- Break condition: If the environment changes are subtle or if relevant information doesn't manifest as observable changes, the mediator might miss important asking opportunities.

### Mechanism 2
- Claim: The RL-based mediator can adapt to imperfections in other components (translator, LLM, actor) by learning when to interact.
- Mechanism: Through reinforcement learning with a penalty for non-informative interactions, the mediator learns to compensate for component imperfections by selectively asking the LLM.
- Core assumption: The learning process can discover patterns in component failures and adjust asking behavior accordingly.
- Evidence anchors:
  - [abstract] "The learned mediator also demonstrates improved robustness in handling uncertainties during execution, such as unexpected errors or newly acquired information."
  - [section 5.3] "This outcome suggests that the learned mediator is capable of learning about the behaviors of the other components within the framework, leading to more robust performances in complex environments."
  - [corpus] No direct evidence - related papers don't explore component imperfection handling through adaptive interaction policies.
- Break condition: If component imperfections are too severe or too variable, the mediator might not be able to learn effective compensation strategies.

### Mechanism 3
- Claim: The learned asking policy achieves competitive task performance with fewer LLM interactions compared to always-asking baselines.
- Mechanism: The mediator learns to balance task performance and interaction cost through the reward function that penalizes unnecessary LLM calls.
- Core assumption: The LLM's planning ability is valuable but expensive, and selective use preserves performance while reducing cost.
- Evidence anchors:
  - [abstract] "Results show that When2Ask achieves competitive task success rates with significantly fewer interactions with the LLM compared to baseline methods that always query the LLM."
  - [section 5.1] "Figure 5 summarizes both the communication costs (top row) and task performances (bottom row) across all four environments. As shown, using our approach, the number of interactions with the LLM is reduced while maintaining the task performance across all four environments."
  - [corpus] Weak evidence - related papers explore LLM-agent interactions but don't provide direct comparison of selective vs. always-asking approaches.
- Break condition: If the LLM is only marginally helpful or if interaction costs are negligible, the benefits of selective asking diminish.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The environments are partially observable, requiring the agent to explore and gather information before making decisions.
  - Quick check question: What is the key difference between an MDP and a POMDP, and how does this affect planning strategies?

- Concept: Options framework in reinforcement learning
  - Why needed here: The agent uses temporally extended actions (options) where each option has its own policy and termination condition, requiring planning at multiple time scales.
  - Quick check question: How does the options framework extend the traditional MDP formulation, and what are the three components of an option?

- Concept: Reinforcement learning with sparse rewards
  - Why needed here: The environments provide rewards only upon task completion, requiring the agent to learn effective exploration and planning strategies from limited feedback.
  - Quick check question: What challenges arise in RL when rewards are sparse, and what techniques can help address these challenges?

## Architecture Onboarding

- Component map:
  - Environment -> Mediator -> Translator (if asking) -> LLM Planner -> Actor -> Environment
  - Environment -> Actor (if not asking) -> Environment

- Critical path:
  1. Environment provides observation to mediator
  2. Mediator decides whether to ask LLM or continue current plan
  3. If asking, translator converts observation to text
  4. LLM generates plan
  5. Actor executes actions based on plan
  6. Repeat until task completion

- Design tradeoffs:
  - Hard-coded vs. learned asking policy: Hard-coded requires domain expertise and may not adapt to component imperfections
  - Always-asking vs. selective asking: Always-asking guarantees latest information but is expensive; selective asking balances cost and performance
  - Simple vs. complex translator: Simple translator may fail to encode important information; complex translator may be difficult to train

- Failure signatures:
  - Mediator asks too frequently: High interaction count with minimal performance improvement
  - Mediator asks too rarely: Task failures due to outdated plans or missed opportunities for re-planning
  - Translator fails to encode information: LLM provides irrelevant plans despite relevant observations
  - Actor fails to execute plans: Task failures despite good planning from LLM

- First 3 experiments:
  1. Implement the SimpleDoorKey environment and verify basic functionality of all components
  2. Test the hard-coded baseline to establish a performance and interaction count reference point
  3. Implement and test the learned mediator policy to verify it can learn to ask appropriately and improve over the hard-coded baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of When2Ask scale with increasingly complex embodied environments that require more sophisticated planning and reasoning?
- Basis in paper: [explicit] The authors mention that future work includes testing with larger LLMs and more complex embodied tasks, and that they only tested on four MiniGrid environments.
- Why unresolved: The current experiments are limited to relatively simple MiniGrid environments, and it's unclear if the approach would maintain efficiency and effectiveness in more complex scenarios.
- What evidence would resolve it: Experiments showing When2Ask's performance (interaction cost, success rate) on more complex embodied tasks like Habitat or ALFRED, with comparison to other approaches.

### Open Question 2
- Question: What is the impact of imperfect translation between observations and text descriptions on the overall system performance, and how can this component be improved?
- Basis in paper: [explicit] The authors mention using an "expert translator" but acknowledge that imperfect translation can lead to failures, as shown in Figure 6b where the translator fails to encode relative positions between the agent and objects.
- Why unresolved: The paper uses an idealized translator for experiments, but real-world translation from observations to text is likely to be imperfect, and its impact on the system is not fully explored.
- What evidence would resolve it: Experiments comparing performance with different translation methods (learned vs. hard-coded), and analysis of how translation errors propagate through the system.

### Open Question 3
- Question: How does the learned asking policy generalize to different LLM planners with varying capabilities and reasoning abilities?
- Basis in paper: [inferred] The authors use different Vicuna models (7b and 13b) but don't explore how the asking policy adapts to planners with different reasoning strengths or output formats.
- Why unresolved: The current approach is trained with specific LLM planners, but it's unclear if the asking policy would need retraining or adaptation for different LLMs with varying capabilities.
- What evidence would resolve it: Experiments showing performance of the learned asking policy when transferred to different LLM planners (e.g., GPT-3.5, GPT-4, or smaller models), or when the LLM's reasoning ability is systematically varied.

## Limitations

- The approach assumes observation differences are meaningful indicators for asking, which may not hold in environments with subtle or non-observable relevant changes
- Performance depends on the quality of the translator component, which is treated as an expert system rather than learned in this work
- The method hasn't been tested on more complex environments that require sophisticated planning and reasoning beyond the four MiniGrid tasks

## Confidence

**High confidence:** The claim that the learned mediator achieves competitive task performance with fewer LLM interactions compared to always-asking baselines.

**Medium confidence:** The claim about improved robustness to component imperfections and the mechanism claim about learning to ask based on observation differences.

## Next Checks

1. **Component failure analysis:** Systematically test the mediator's performance when individual components (translator, LLM, actor) are intentionally degraded or fail. Measure how well the learned policy adapts to different failure modes compared to baseline methods.

2. **Mechanism ablation study:** Disable the observation-difference input to the mediator and retrain the policy. Compare the asking behavior and performance to determine if the observation-difference mechanism is essential or if the policy learns alternative asking strategies.

3. **Cross-environment generalization test:** Train the mediator on one environment (e.g., SimpleDoorKey) and evaluate it on another (e.g., ColoredDoorKey) without fine-tuning. Measure whether the learned asking patterns transfer across environments with different characteristics and complexities.