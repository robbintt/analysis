---
ver: rpa2
title: The Moral Machine Experiment on Large Language Models
arxiv_id: '2309.05958'
source_url: https://arxiv.org/abs/2309.05958
tags:
- llms
- preferences
- human
- scenarios
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates the moral judgments of large language\
  \ models (LLMs) using the Moral Machine framework, comparing their ethical decision-making\
  \ tendencies with human preferences in autonomous driving scenarios. The research\
  \ evaluates four prominent LLMs\u2014GPT-3.5, GPT-4, PaLM 2, and Llama 2\u2014across\
  \ 50,000 scenarios that present moral dilemmas involving trade-offs between saving\
  \ different types of characters (e.g., humans vs."
---

# The Moral Machine Experiment on Large Language Models

## Quick Facts
- arXiv ID: 2309.05958
- Source URL: https://arxiv.org/abs/2309.05958
- Authors: 
- Reference count: 0
- Key outcome: This study investigates the moral judgments of large language models (LLMs) using the Moral Machine framework, comparing their ethical decision-making tendencies with human preferences in autonomous driving scenarios.

## Executive Summary
This study evaluates the moral decision-making of four prominent LLMs (GPT-3.5, GPT-4, PaLM 2, and Llama 2) using the Moral Machine framework across 50,000 autonomous driving scenarios. The research quantifies LLM preferences through conjoint analysis and compares them to human moral judgments, revealing both alignment and significant quantitative differences. While LLMs broadly align with human preferences for prioritizing humans over pets and saving more lives, they demonstrate stronger, more uncompromising preferences for certain attributes. GPT models show the closest alignment with human moral preferences, while PaLM 2 and Llama 2 exhibit more pronounced deviations. The findings highlight important considerations for deploying AI systems in contexts requiring ethical decision-making.

## Method Summary
The study employs the Moral Machine framework to generate 50,000 autonomous driving scenarios testing nine moral attributes through trade-offs between two cases. Four LLMs were prompted to select either Case 1 or Case 2 for each scenario. Conjoint analysis was applied to calculate Average Marginal Component Effects (AMCE) for each attribute, measuring how much each moral dimension influences decision-making. The researchers then compared LLM AMCE values to human data from the original Moral Machine experiment using Euclidean distance measurements and Principal Component Analysis (PCA) to visualize similarity patterns. Statistical analysis revealed both qualitative alignment and quantitative differences between LLM and human moral preferences.

## Key Results
- GPT models (GPT-3.5 and GPT-4) show the closest alignment with human moral preferences among the tested LLMs
- LLMs demonstrate stronger quantitative preferences for certain attributes compared to humans, suggesting more uncompromising decision-making
- PaLM 2 and Llama 2 exhibit more pronounced deviations from human preferences than GPT models
- While qualitatively aligned on prioritizing humans over pets and saving more lives, LLMs show significant differences in preference strength across multiple moral dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs' moral decision-making patterns can be quantitatively measured and compared to human preferences using conjoint analysis and AMCE metrics.
- Mechanism: The study employs the Moral Machine framework with 50,000 scenarios testing nine moral attributes, then uses conjoint analysis to calculate Average Marginal Component Effects (AMCE) that represent the relative importance of each moral dimension.
- Core assumption: The conjoint analysis framework can reliably identify causal effects of moral attributes on decision-making without requiring specific modeling assumptions.
- Evidence anchors:
  - [abstract] "The analysis employs conjoint analysis to measure Average Marginal Component Effects (AMCE) for nine preference attributes"
  - [section] "we applied the conjoint analysis framework proposed in [21] (electronic supplementary material, code S1). This framework offers nonparametric and robust identification of causal effects"
  - [corpus] Weak - no direct corpus evidence about conjoint analysis methodology

### Mechanism 2
- Claim: GPT models demonstrate closer alignment with human moral preferences than PaLM 2 and Llama 2 models.
- Mechanism: The study measures Euclidean distances between LLM AMCE values and human AMCE values, finding that ChatGPT models (GPT-3.5 and GPT-4) have shorter distances, indicating closer alignment.
- Core assumption: Euclidean distance between AMCE vectors is a valid metric for measuring alignment between LLM and human moral preferences.
- Evidence anchors:
  - [abstract] "GPT models show the closest alignment with human preferences, while PaLM 2 and Llama 2 exhibit more pronounced deviations"
  - [section] "we calculated the Euclidean distance between the preference scores (represented by AMCE values) of humans and each LLM"
  - [corpus] Weak - no direct corpus evidence about distance metrics for moral alignment

### Mechanism 3
- Claim: LLMs make more uncompromising moral decisions than humans, as evidenced by stronger quantitative preferences for certain attributes.
- Mechanism: The study observes that while LLMs qualitatively align with human preferences (e.g., prioritizing humans over pets), their quantitative AMCE values show stronger preferences, suggesting less compromise in decision-making.
- Core assumption: Stronger quantitative preferences in AMCE values indicate more uncompromising decision-making behavior.
- Evidence anchors:
  - [abstract] "significant quantitative disparities, suggesting that LLMs might lean toward more uncompromising decisions, compared to the milder inclinations of humans"
  - [section] "While some LLM preferences aligned qualitatively with human preferences, there were quantitative divergences. The pronounced preferences of LLMs in certain scenarios, compared to the milder inclinations of humans, may indicate the models' tendency to make more uncompromising decisions"
  - [corpus] Weak - no direct corpus evidence about compromise vs. decisiveness in LLM behavior

## Foundational Learning

- Concept: Conjoint analysis methodology
  - Why needed here: The study relies on conjoint analysis to measure how different moral attributes influence LLM decision-making, making it essential to understand this statistical framework.
  - Quick check question: What does AMCE (Average Marginal Component Effect) measure in the context of conjoint analysis?

- Concept: Moral Machine experiment framework
  - Why needed here: The entire study is built on scenarios from the Moral Machine experiment, so understanding its structure and limitations is crucial for interpreting results.
  - Quick check question: How do the Moral Machine scenarios present moral dilemmas to participants?

- Concept: Euclidean distance as a similarity metric
  - Why needed here: The study uses Euclidean distance to quantify alignment between LLM and human preferences, requiring understanding of this geometric measurement.
  - Quick check question: Why might Euclidean distance be an appropriate measure for comparing AMCE vectors between LLMs and humans?

## Architecture Onboarding

- Component map: Scenario generation -> LLM API calls -> Response validation -> AMCE calculation -> Distance measurement -> Result interpretation
- Critical path: Scenario generation → LLM API calls → Response validation → AMCE calculation → Distance measurement → Result interpretation
- Design tradeoffs: Using 50,000 scenarios balances computational cost against statistical power; API usage constraints limited GPT-4 to 10,000 scenarios; default temperature settings may affect response consistency.
- Failure signatures: High invalid response rates (like Llama 2's 20% invalid rate) indicate model reluctance; inconsistent AMCE values across runs suggest instability; large Euclidean distances might indicate fundamental preference misalignment.
- First 3 experiments:
  1. Run a smaller pilot with 1,000 scenarios across all models to verify API integration and response validation logic before scaling to full dataset.
  2. Test AMCE calculation on a known dataset to ensure the conjoint analysis framework is implemented correctly and producing expected results.
  3. Validate Euclidean distance measurements by creating synthetic AMCE vectors with known relationships to confirm the similarity metric works as intended.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLMs handle neutral options in moral dilemmas, and how would this affect their decision-making compared to human preferences?
- Basis in paper: [inferred] from the paper's discussion of MM scenarios presenting binary choices and noting that neutral options in similar dilemmas led to significant proportions of human participants choosing them.
- Why unresolved: The study used MM scenarios that presented binary choices, and the paper notes that the presence or absence of neutral choices can influence conclusions.
- What evidence would resolve it: Experiments testing LLMs' responses to moral dilemmas that include neutral options, and comparing their choices to human responses in similar scenarios.

### Open Question 2
- Question: What specific aspects of LLM training data contribute to their stronger preferences for certain attributes (e.g., saving more characters, prioritizing females) compared to human tendencies?
- Basis in paper: [explicit] from the discussion on how LLMs may overemphasize cultural characteristics due to training data originating from Western sources, and their tendency to make more uncompromising decisions.
- Why unresolved: While the paper suggests a connection to training data and cultural characteristics, it does not specify which aspects of the training data lead to these stronger preferences.
- What evidence would resolve it: Detailed analysis of LLM training data to identify patterns and biases that correlate with the observed stronger preferences, and comparison with human cultural and societal factors.

### Open Question 3
- Question: How do different LLM architectures (e.g., transformer-based vs. other architectures) influence their moral decision-making in autonomous driving scenarios?
- Basis in paper: [inferred] from the study comparing four different LLMs (GPT-3.5, GPT-4, PaLM 2, and Llama 2), which are all transformer-based, but noting that Llama 2 had a lower valid response rate and different preferences.
- Why unresolved: The study only compared transformer-based LLMs and did not explore how different architectures might influence moral decision-making.
- What evidence would resolve it: Comparative studies of LLMs with different architectures (e.g., transformer-based vs. recurrent neural networks) in moral dilemma scenarios, analyzing their preferences and decision-making processes.

## Limitations

- The study is limited to autonomous driving scenarios from the Moral Machine framework, which may not capture the full complexity of real-world moral decision-making
- LLM responses were collected using default temperature settings without controlling for response variability
- Some models (particularly Llama 2) showed high rates of invalid responses that required exclusion from analysis

## Confidence

- Alignment measurement methodology: Medium-High (robust statistical framework with established precedent)
- GPT models showing closest human alignment: Medium (supported by distance metrics but based on single dataset)
- LLMs making more uncompromising decisions: Low-Medium (inferred from quantitative differences but not directly tested)

## Next Checks

1. Replicate the study using a different moral dilemma framework (such as trolley problems) to test whether findings generalize beyond autonomous driving scenarios
2. Conduct sensitivity analysis by varying temperature settings and measuring AMCE stability across multiple runs for each LLM
3. Test the predictive validity of AMCE-based alignment by having humans evaluate LLM decision rationales directly, comparing qualitative human judgments with quantitative distance metrics