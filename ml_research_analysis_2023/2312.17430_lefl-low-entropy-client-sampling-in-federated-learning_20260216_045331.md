---
ver: rpa2
title: 'LEFL: Low Entropy Client Sampling in Federated Learning'
arxiv_id: '2312.17430'
source_url: https://arxiv.org/abs/2312.17430
tags:
- clients
- client
- data
- learning
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses federated learning (FL) client sampling challenges
  caused by data heterogeneity across clients. The authors propose a method to cluster
  clients based on the similarity of their models' learned high-level features, enabling
  stratified sampling that yields client subsets with low relative entropy to the
  global data distribution.
---

# LEFL: Low Entropy Client Sampling in Federated Learning

## Quick Facts
- arXiv ID: 2312.17430
- Source URL: https://arxiv.org/abs/2312.17430
- Reference count: 40
- Improves global model accuracy by up to 7.4% and reduces communication rounds in federated learning

## Executive Summary
LEFL addresses federated learning client sampling challenges caused by data heterogeneity across clients. The method clusters clients based on similarity of their models' learned high-level features using soft-labels on a shared public dataset. This enables stratified sampling that yields client subsets with low relative entropy to the global data distribution, improving global model accuracy by up to 7.4%, reducing communication rounds by up to 3.78GB, and accelerating convergence compared to strong baselines across multiple datasets.

## Method Summary
The LEFL method involves a one-time preprocessing step where clients train initial models on local data, then predict soft-labels on a shared public dataset. The server computes pairwise KL-divergence between soft-labels to build a similarity matrix, which is used to cluster clients via KMeans. In each subsequent FL round, the server samples proportionally from each cluster (stratified sampling), aggregates parameters, and updates the global model. This approach improves the representativeness of sampled client data and reduces gradient drift during aggregation.

## Key Results
- Improves global model accuracy by up to 7.4% compared to baselines (FedAvg, FedProx, SCAFFOLD, FedNova)
- Reduces communication rounds by up to 3.78GB
- Accelerates convergence across CIFAR-10, CIFAR-100, and EMNIST datasets
- Maintains low relative entropy between sampled client data and global data distribution

## Why This Works (Mechanism)

### Mechanism 1
Clustering clients based on high-level feature similarity improves the representativeness of sampled client data. Clients train locally, predict soft-labels on a shared public dataset, and KL-divergence between soft-labels constructs a similarity matrix for KMeans clustering. Stratified sampling across clusters ensures selected clients have datasets with low relative entropy to the global distribution.

### Mechanism 2
Soft-label similarity serves as a proxy for model similarity in latent space. After initial local training, clients predict soft-labels on the same public dataset. KL-divergence between these soft-labels indicates similar latent representations, grouping clients with similar data distributions.

### Mechanism 3
Stratified sampling across clusters reduces gradient drift and improves convergence. Balanced representation of data distributions in each round reduces variance in gradient updates, leading to faster and more stable convergence by minimizing aggregation noise.

## Foundational Learning

- Concept: Federated Learning (FL)
  - Why needed here: The paper builds a sampling strategy specifically for FL systems where clients train models locally on private data and upload parameters.
  - Quick check question: In FL, who maintains the global model and orchestrates the training rounds?

- Concept: KL-divergence and relative entropy
  - Why needed here: Used to measure similarity between soft-label distributions and quantify how representative sampled client data is of the global distribution.
  - Quick check question: What does a low KL-divergence between two soft-label distributions indicate about the models that produced them?

- Concept: KMeans clustering
  - Why needed here: Groups clients based on similarity matrix derived from KL-divergence, enabling stratified sampling.
  - Quick check question: Why is an unsupervised clustering algorithm appropriate for grouping clients based on soft-label similarity?

## Architecture Onboarding

- Component map: Client trains locally → Predicts soft-labels on public dataset → Uploads soft-labels → Server builds similarity matrix → Applies KMeans clustering → Conducts stratified sampling → Aggregates parameters → Updates global model
- Critical path: Soft-label upload → Similarity matrix construction → Clustering → Stratified sampling → Model aggregation
- Design tradeoffs:
  - Public dataset size vs. computation cost: Larger public datasets improve soft-label reliability but increase upload cost
  - Number of clusters vs. representativeness: More clusters capture finer differences but may reduce sample size per cluster
  - Initial training epochs vs. feature quality: More epochs improve feature learning but delay clustering step
- Failure signatures:
  - High relative entropy in sampled datasets despite clustering
  - No improvement in convergence speed or accuracy compared to random sampling
  - Clusters are highly imbalanced or contain very few clients
- First 3 experiments:
  1. Run clustering and stratified sampling on CIFAR-10 with 30 clients and 10% sample rate; measure relative entropy and accuracy vs. random sampling
  2. Vary number of clusters (log(n) vs. fixed k) and observe impact on convergence and communication rounds
  3. Test with different public datasets (TinyImageNet vs. FashionMNIST) to evaluate robustness to public data distribution

## Open Questions the Paper Calls Out

### Open Question 1
How do the proposed clustering and sampling methods perform under extreme data heterogeneity scenarios, such as when clients have completely disjoint label sets? The paper discusses effectiveness under various data heterogeneity conditions but does not explore the extreme case of completely disjoint label sets.

### Open Question 2
Can the proposed method be adapted to handle system heterogeneity, such as varying client architectures and hardware capabilities? The paper mentions this as a potential limitation but does not provide a concrete approach or experimental results.

### Open Question 3
How does the proposed method perform in a cross-device federated learning setting with a large number of clients? The paper mentions potential scalability issues with computing learned high-level features for a large number of clients but does not include experiments in this setting.

## Limitations

- Soft-label similarity as a proxy for model similarity is asserted but not empirically validated with alternative feature similarity metrics
- One-time clustering step lacks sensitivity analysis to cluster initialization, public dataset size, or choice of k in KMeans
- Method's robustness to different degrees of data heterogeneity or client churn is not explored
- Public dataset's distribution and size effects on clustering quality are not analyzed

## Confidence

- **High**: Improvement in accuracy and convergence when using stratified sampling over strong baselines (directly supported by experimental results)
- **Medium**: Clustering clients based on soft-label similarity meaningfully groups them by data distribution (plausible but not fully validated)
- **Low**: Soft-label similarity is a reliable proxy for model latent feature similarity (mechanism not independently verified)

## Next Checks

1. Compare LEFL's clustering-induced accuracy gains to a version where clients are clustered by gradient similarity or embedding distance; check if soft-label clustering is strictly necessary
2. Perform sensitivity analysis on public dataset size and distribution; verify that soft-label similarity remains a robust clustering signal across varied public datasets
3. Test LEFL's performance under high client churn or with varying numbers of clusters; measure impact on convergence stability and relative entropy