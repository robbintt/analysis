---
ver: rpa2
title: 'Model Stitching: Looking For Functional Similarity Between Representations'
arxiv_id: '2303.11277'
source_url: https://arxiv.org/abs/2303.11277
tags:
- similarity
- representations
- stitch
- layer
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a generalization of the model stitching methodology
  to compare representations across neural networks with different architectures.
  The authors apply this method to compare intermediate representations across small
  ResNet architectures (10-18 layers) trained on CIFAR-10.
---

# Model Stitching: Looking For Functional Similarity Between Representations

## Quick Facts
- arXiv ID: 2303.11277
- Source URL: https://arxiv.org/abs/2303.11277
- Authors: 
- Reference count: 17
- Key outcome: Model stitching reveals unexpected high functional similarity between neural network layers, challenging conventional understanding of information flow in ResNets

## Executive Summary
This paper generalizes the model stitching methodology to compare representations across neural networks with different architectures. The authors apply this method to small ResNet architectures (10-18 layers) trained on CIFAR-10 and find surprising patterns in functional similarity. Stitching achieves high accuracy when connecting layers from the sender network to earlier layers in the receiver network, even when those layers are far apart. This creates an unexpected triangular pattern in similarity matrices that contradicts the standard narrative that neural networks progressively discard information. The authors propose two possible explanations: either neural networks maintain more information than previously thought, or the stitching process is "hacking" the receiver network by finding alternative representations that yield similar results.

## Method Summary
The paper introduces a generalized model stitching approach to compare intermediate representations across neural networks with different architectures. For each ordered pair of networks, the method stitches every sender layer to every receiver layer by training a transformation layer that maps the sender's representation to match the receiver's expected input dimensions. The stitched network is then evaluated on CIFAR-10 classification accuracy. The authors systematically stitch 256 network pairs across 16 Small ResNet architectures ranging from 10 to 18 layers, using strided convolutions or upsampling for dimension matching, and train stitches for 4 epochs with specific hyperparameters.

## Key Results
- Stitching achieves high accuracy when connecting layers from sender to earlier layers in receiver, forming an unexpected triangular pattern in similarity matrices
- The triangular pattern appears even with randomly initialized networks, suggesting it's a fundamental property of the stitching process
- High stitching accuracy can occur even when sender and receiver layers are far apart in their respective networks
- This finding challenges the conventional narrative that neural networks progressively discard information through layers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stitching achieves high accuracy because neural networks progressively maintain class-relevant information throughout their layers rather than discarding it.
- Mechanism: The stitch learns to map intermediate representations to a space where the receiver's subsequent layers can still extract the same class-relevant features, effectively preserving information across different architectures.
- Core assumption: Neural networks encode class-relevant information in multiple layers, not just the final layers, allowing intermediate representations to be functionally interchangeable.
- Evidence anchors:
  - [abstract] "we find that stitching, based on convolutions, for small ResNets, can reach high accuracy if those layers come later in the first (sender) network than in the second (receiver), even if those layers are far apart."
  - [section] "The second explanation we see is that the common narrative could be wrong and some neural networks may in fact be able to maintain most if not all of the granular information of the image throughout their processing of it."
  - [corpus] No direct corpus evidence supporting this mechanism specifically.

### Mechanism 2
- Claim: The stitch learns to find alternative representations that yield similar results rather than matching expected representations.
- Mechanism: The stitch acts as an adversarial transformation that generates "generic" salient features which the receiver's layers can classify in a given way, effectively "hacking" the receiver.
- Core assumption: The stitch has sufficient capacity to find alternative representations that bypass the need for exact representational matching.
- Evidence anchors:
  - [abstract] "we suggest that model stitching, naively implemented, may not necessarily always be an accurate measure of similarity."
  - [section] "The second explanation we see is that the stitch may be able to give the receiver a representation which, despite being different from that which is expected, nonetheless yields high accuracy."
  - [corpus] Weak corpus evidence - related papers discuss model stitching limitations but don't specifically address the "hacking" hypothesis.

### Mechanism 3
- Claim: The similarity triangle pattern emerges because proportionally earlier layers in different-sized networks contain similar information.
- Mechanism: When comparing networks of different lengths, layers that are proportionally earlier in each network (e.g., 25% through a 10-layer network vs 25% through 20-layer network) contain similar information about the input, allowing successful stitching.
- Core assumption: Neural network layers encode information in a way that scales proportionally with network depth across different architectures.
- Evidence anchors:
  - [section] "for small Resnets, regardless of the architecture, if we had one Resnet of length I and another of length J, then if j/J â‰¤ i/I the similarity was high"
  - [section] "However, the proportionality finding could prove to be interesting to explore in future work, since it may tell us something about Resnet length-invariant properties."
  - [corpus] No direct corpus evidence supporting this specific proportionality mechanism.

## Foundational Learning

- Concept: Functional similarity vs. statistical/geometric similarity
  - Why needed here: The paper argues that functional similarity (can representations be interchanged for the same purpose?) is more meaningful than statistical measures (how close are the representations numerically?).
  - Quick check question: If two representations have high cosine similarity but low stitching accuracy, which similarity measure would the paper consider more meaningful?

- Concept: Model stitching methodology
  - Why needed here: Understanding how model stitching works is crucial to interpreting the results - the stitch learns to map representations from one network to another while keeping both networks frozen.
  - Quick check question: In the stitching process, which parts of the network are frozen and which are trained?

- Concept: Residual network architecture
  - Why needed here: The paper specifically studies small ResNets with varying numbers of residual blocks, so understanding how residual connections work is important for interpreting the results.
  - Quick check question: How does the residual connection in a ResNet layer affect the flow of information compared to a standard convolutional layer?

## Architecture Onboarding

- Component map:
  - Sender network (frozen) -> Stitch network (trainable) -> Receiver network (frozen)

- Critical path:
  1. Forward pass through sender network to intermediate layer
  2. Stitch transformation of sender representation
  3. Input to receiver network at intermediate layer
  4. Forward pass through remainder of receiver network
  5. Compute classification loss
  6. Backpropagate only through stitch parameters

- Design tradeoffs:
  - Stitch complexity vs. generalization: More complex stitches can represent more transformations but may overfit
  - Architecture compatibility: Stitches must handle different tensor shapes between sender and receiver
  - Training efficiency: Using frozen networks allows efficient training of only the stitch

- Failure signatures:
  - High training accuracy but poor generalization on test data
  - Stitch fails to converge (training loss plateaus)
  - Receiver network accuracy drops significantly after stitching

- First 3 experiments:
  1. Stitch between corresponding layers of two identical architectures to establish baseline performance
  2. Stitch between randomly initialized networks to verify stitch capacity is not excessive
  3. Stitch between sender and receiver with significantly different architectures to test architectural generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Are the high similarities observed in the stitching experiments due to information preservation or representation hacking?
- Basis in paper: The authors discuss two main explanations for the unexpected high similarity patterns: information preservation (neural networks maintain more information than previously thought) and representation hacking (stitches find alternative representations that yield similar results).
- Why unresolved: The authors attempted numerical sanity tests comparing vanilla stitches to similarity-trained stitches, but the differences were not as large as expected to conclusively determine hacking. Image generation experiments also failed to reveal clear patterns.
- What evidence would resolve it: Systematic ablation studies varying stitch architecture and training procedures, or experiments using neural networks with known information bottlenecks, could help distinguish between preservation and hacking explanations.

### Open Question 2
- Question: Does the proportional relationship between layer positions (rather than absolute positions) in the similarity triangle generalize to larger networks and other architectures?
- Basis in paper: The authors observe that high similarity occurs when the receiver layer is before the sender layer proportionally to their respective network lengths, not necessarily in absolute terms. They note this pattern appears even with larger ResNets.
- Why unresolved: The study only examined small ResNets (10-18 layers) and a limited set of larger ResNet pairs. The phenomenon's generality across different architectures and scales remains unknown.
- What evidence would resolve it: Testing the proportional relationship across diverse architectures (VGG, DenseNet, Transformers) and scales (from small to very deep networks) would determine if this is a fundamental property of neural networks or specific to ResNets.

### Open Question 3
- Question: What is the theoretical basis for the observed triangular similarity pattern in model stitching experiments?
- Basis in paper: The authors find that stitching accuracy is high when receiver layers are proportionally before sender layers, forming a triangle in the similarity matrix. They suggest this contradicts the standard narrative that neural networks progressively discard information.
- Why unresolved: The authors only propose two possible explanations (information preservation or hacking) without providing a rigorous theoretical framework. The underlying mechanisms remain unclear.
- What evidence would resolve it: Developing information-theoretic analyses or formal proofs about how information flows through residual networks, combined with controlled experiments manipulating layer properties, could reveal the theoretical foundation of this phenomenon.

## Limitations
- Analysis restricted to small ResNet architectures on CIFAR-10, limiting generalizability
- "Hacking" explanation lacks rigorous validation despite being intuitive
- Proportionality finding remains unexplored without deeper theoretical or empirical investigation

## Confidence
- Main empirical findings: High - the triangular pattern in similarity matrices is clearly observable and reproducible
- Mechanistic explanations: Medium - while the two proposed explanations are plausible, they remain hypotheses without definitive proof

## Next Checks
1. Test stitching between networks with different architectures (e.g., ResNet vs. Vision Transformer) to determine if the triangular pattern persists across architectural families
2. Analyze the learned stitch transformations to distinguish between information-preserving mappings versus adversarial feature generation
3. Extend experiments to larger networks and other datasets to assess scalability and domain transfer of the observed patterns