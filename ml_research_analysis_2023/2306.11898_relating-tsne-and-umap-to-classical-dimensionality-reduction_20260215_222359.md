---
ver: rpa2
title: Relating tSNE and UMAP to Classical Dimensionality Reduction
arxiv_id: '2306.11898'
source_url: https://arxiv.org/abs/2306.11898
tags:
- umap
- methods
- gradient
- learning
- embedding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper explores the explainability of gradient-based dimensionality
  reduction methods like tSNE and UMAP by relating them to classical methods such
  as PCA and LLE. It shows that PCA can be framed in the ARDR framework, with its
  gradient being expressed as attractions and repulsions between points.
---

# Relating tSNE and UMAP to Classical Dimensionality Reduction

## Quick Facts
- arXiv ID: 2306.11898
- Source URL: https://arxiv.org/abs/2306.11898
- Reference count: 40
- Primary result: Shows that classical methods like PCA and LLE can be expressed in the attraction-repulsion framework used by tSNE and UMAP, enabling better interpretability

## Executive Summary
This paper establishes a theoretical framework connecting modern gradient-based dimensionality reduction methods (tSNE, UMAP) to classical approaches (PCA, LLE, MDS, ISOMAP). The authors demonstrate that PCA can be recovered exactly by applying attractions and repulsions between points in a randomly initialized embedding, and that LLE with two kernels can indistinguishably reproduce UMAP outputs. This work provides a unifying perspective that makes gradient-based methods more interpretable by relating them to well-understood classical techniques, addressing the "black box" nature of tSNE and UMAP.

## Method Summary
The paper develops a kernel-based framework for dimensionality reduction where classical methods like PCA and LLE can be expressed through attractions and repulsions between embedding points. By reformulating these methods in the ARDR (Attraction-Repulsion Dimensionality Reduction) paradigm, the authors show that PCA's gradient can be decomposed into point-to-point forces, and that LLE with appropriate kernel choices (including UMAP's kernel) produces similar gradients to modern methods. The work also explores fast low-rank approximations of kernel matrices for scalable computation and conjectures that the tSNE/UMAP gradient descent heuristics minimize the LLE objective with two kernels.

## Key Results
- PCA can be exactly recovered by applying attractions and repulsions between points in a randomly initialized embedding
- LLE with two kernels (including UMAP's kernel) can indistinguishably reproduce UMAP outputs
- Fast low-rank approximations of kernel matrices do not significantly affect PCA convergence under certain eigengap conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PCA can be recovered exactly by applying attractions and repulsions between points in a randomly initialized embedding.
- Mechanism: The gradient of the PCA objective function can be decomposed into attractions and repulsions between embedding points. By initializing a random pointset and iteratively applying these gradients, the embedding converges to the true PCA projection.
- Core assumption: The kernel on the embedding space is the linear kernel, allowing the gradient to be written purely in terms of point-to-point forces.
- Evidence anchors:
  - [abstract]: "one can fully recover methods like PCA, MDS, and ISOMAP in the modern DR paradigm: by applying attractions and repulsions onto a randomly initialized dataset."
  - [section 4.1]: "This has a natural extension to kernel methods... Then the kernel matrix rKsij " kpxi, xjq is a psd Gram matrix... admits an equivalent procedure for finding principal components as traditional PCA."
  - [corpus]: Found 25 related papers. Average neighbor FMR=0.361. Top related titles include "Dimensionality reduction for homological stability" and "ActUp: Analyzing and Consolidating tSNE and UMAP" - evidence is weak for direct mechanism support but shows the field is active.
- Break condition: The kernel on the embedding space is non-linear, which introduces a chain rule term that cannot be expressed purely as point-to-point forces.

### Mechanism 2
- Claim: LLE with two kernels can indistinguishably reproduce UMAP outputs.
- Mechanism: By replacing the linear kernel on the embedding space in LLE with UMAP's kernel, the resulting gradient has the same form as UMAP's gradient (attractions based on local neighborhoods, repulsions based on graph connectivity). This means minimizing the LLE objective with UMAP's kernels produces embeddings similar to UMAP.
- Core assumption: The constraint 1/nKY = I applies in kernel space, meaning the off-diagonals of KY must be 0.
- Evidence anchors:
  - [abstract]: "we show that, with a small change, Locally Linear Embeddings (LLE) can indistinguishably reproduce UMAP outputs."
  - [section 5.2]: "we conjecture that the tSNE/UMAP gradient descent heuristics minimize the LLE objective with two kernels" and "the most vanilla implementation of LLE with UMAP's kernels adequately reproduces ARDR embeddings."
  - [corpus]: Weak evidence - no direct corpus papers discussing this specific LLE-UMAP connection.
- Break condition: The kernel on Y is changed to something other than UMAP's kernel, or the constraint 1/nKY = I does not hold.

### Mechanism 3
- Claim: Fast low-rank approximations of the kernel matrices do not significantly affect PCA convergence.
- Mechanism: When the current embedding is a worse approximation to the true PCA than the rank-k approximation, the approximate gradient points in a similar direction to the true gradient. This allows for sublinear-time methods to approximate PCA without eigendecomposition.
- Core assumption: The eigengap between the k-th and (k+1)-th eigenvalues is sufficiently large.
- Evidence anchors:
  - [section 4.2]: "these approximations do not significantly affect the gradient descent convergence" and provides a mathematical condition for when the approximate gradient is similar to the true gradient.
  - [section 4.2]: "there are sublinear-time methods for obtaining A1 such that ||A - A1||2F is within (1 + ε)||A - Ak||2F" of the optimal rank-k approximation.
  - [corpus]: Weak evidence - no direct corpus papers discussing this specific PCA approximation technique.
- Break condition: The eigengap is too small, or the current embedding becomes a better approximation to the true PCA than the rank-k approximation.

## Foundational Learning

- Concept: Principal Component Analysis (PCA)
  - Why needed here: PCA is the foundational dimensionality reduction method being related to ARDR methods. Understanding its mechanics is crucial for seeing how it fits into the attraction-repulsion framework.
  - Quick check question: What is the objective function being minimized in PCA, and how does it relate to preserving variance in the data?

- Concept: Kernel methods and Gram matrices
  - Why needed here: Both classical and ARDR methods can be expressed using kernel matrices. Understanding how kernels define similarity measures is key to seeing the connections between methods.
  - Quick check question: How does the choice of kernel function affect the resulting embedding in PCA and LLE?

- Concept: Attraction-repulsion framework
  - Why needed here: This is the unifying framework for both classical and ARDR methods. Understanding how attractions and repulsions translate to gradients is essential for seeing the connections.
  - Quick check question: How can the gradient of a loss function be decomposed into attractions and repulsions between points?

## Architecture Onboarding

- Component map: X (input data) -> KX (input kernel) -> Y (embedding) -> KY (embedding kernel) -> Loss function -> Gradient -> Y update
- Critical path: Compute KX → Initialize Y randomly → Compute KY → Compute loss → Compute gradient → Update Y → Check convergence
- Design tradeoffs:
  - Using linear vs. non-linear kernels on Y
  - Computing full vs. approximate gradients (for scalability)
  - Choice of loss function (Frobenius vs. KL divergence)
- Failure signatures:
  - Embedding doesn't converge to PCA or LLE solution
  - Embedding has poor separation of clusters
  - Optimization is too slow for large datasets
- First 3 experiments:
  1. Implement PCA as attractions and repulsions on a small dataset (e.g., 2D Swiss roll) and verify convergence to true PCA.
  2. Implement LLE with UMAP's kernels on a small dataset and compare to UMAP output.
  3. Test fast PCA approximation on a medium-sized dataset and compare to standard PCA in terms of speed and accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can gradient-based dimensionality reduction methods like UMAP and tSNE be directly related to classical methods like PCA and LLE, enabling better interpretability?
- Basis in paper: [explicit] The paper shows that PCA and LLE can be framed in the attraction-repulsion framework and that LLE with two kernels can reproduce UMAP outputs.
- Why unresolved: While the paper demonstrates similarities between the methods, a complete understanding of how the structures in UMAP outputs relate to the input distributions is still lacking.
- What evidence would resolve it: A comprehensive framework that maps the properties of UMAP embeddings back to the input data characteristics, supported by convergence guarantees and empirical validation.

### Open Question 2
- Question: What specific properties in tSNE/UMAP embeddings guarantee the preservation of structure in the original dataset?
- Basis in paper: [explicit] The authors identify this as the primary open question and discuss the need for a metric to measure the similarity between embeddings.
- Why unresolved: The paper highlights the difficulty in quantifying the similarity between embeddings and the lack of consensus on a metric that adequately differentiates between classical and modern DR methods.
- What evidence would resolve it: A robust metric that captures the fundamental differences between classical and modern DR methods, along with empirical studies demonstrating the relationship between embedding properties and input data structure.

### Open Question 3
- Question: Can UMAP and similar ARDR methods be solved directly without iterative gradient descent, as suggested by the conjecture that they approximate LLE with two kernels?
- Basis in paper: [explicit] The paper conjectures that LLE with two kernels can emulate ARDR methods and that there exists an algorithm that can solve UMAP-like problems directly.
- Why unresolved: The conjecture is based on theoretical similarities, but empirical validation and a concrete algorithm that directly solves UMAP-like problems are needed.
- What evidence would resolve it: An algorithm that directly computes embeddings similar to UMAP without gradient descent, along with convergence guarantees and experimental comparisons demonstrating the quality of the embeddings.

## Limitations
- The conjecture that LLE with two kernels can indistinguishably reproduce UMAP outputs remains unproven and requires empirical validation
- Convergence guarantees for fast low-rank approximations depend on strong eigengap assumptions that may not hold in practice
- The paper does not address computational complexity trade-offs when scaling these methods to very large datasets

## Confidence

**High Confidence**: The mathematical derivations showing PCA as a special case of the ARDR framework (Mechanism 1) are rigorous and well-supported by the literature. The gradient decomposition into attractions and repulsions is mathematically sound.

**Medium Confidence**: The connection between LLE with UMAP's kernels and UMAP's behavior (Mechanism 2) is plausible but remains a conjecture. While the gradient forms match, empirical validation across diverse datasets is needed.

**Low Confidence**: The conditions under which fast low-rank approximations maintain convergence properties (Mechanism 3) are theoretically derived but may be fragile in practice. The dependence on eigengap assumptions limits real-world applicability.

## Next Checks
1. **Empirical validation of LLE-UMAP equivalence**: Implement LLE with UMAP's kernels on multiple benchmark datasets (MNIST, Fashion-MNIST, COIL-20) and quantitatively compare the resulting embeddings to UMAP using established metrics like trustworthiness, continuity, and stress.

2. **Stress-test fast PCA approximation**: Evaluate the proposed low-rank gradient approximation method on datasets with varying eigengap properties, measuring both computational speedup and preservation of global structure compared to exact PCA.

3. **Investigate kernel constraint violations**: Systematically relax the constraint 1/nKY = I in the LLE formulation and measure the impact on embedding quality and gradient behavior, identifying the boundary conditions where the ARDR framework breaks down.