---
ver: rpa2
title: 'Multimodal Recommender Systems: A Survey'
arxiv_id: '2302.03883'
source_url: https://arxiv.org/abs/2302.03883
tags:
- recommendation
- multimodal
- graph
- information
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey comprehensively reviews multimodal recommender systems
  (MRS), which integrate deep learning techniques to model user preferences using
  both identifier/attribute information and multimodal features like images, audio,
  and text. The paper categorizes MRS into three technical taxonomies: Feature Interaction
  (using methods like bridge, fusion, and filtration to combine multimodal data),
  Feature Enhancement (employing contrastive learning and disentangled representation
  learning to improve representations under data sparsity), and Model Optimization
  (addressing computational challenges in training complex multimodal encoders).'
---

# Multimodal Recommender Systems: A Survey

## Quick Facts
- arXiv ID: 2302.03883
- Source URL: https://arxiv.org/abs/2302.03883
- Reference count: 40
- This survey comprehensively reviews multimodal recommender systems, categorizing them into Feature Interaction, Feature Enhancement, and Model Optimization approaches.

## Executive Summary
This survey provides a comprehensive overview of multimodal recommender systems (MRS), which integrate deep learning techniques to model user preferences using both identifier/attribute information and multimodal features like images, audio, and text. The paper categorizes MRS approaches into three technical taxonomies addressing different challenges in the field. It covers recent works across various applications including video, fashion, news, and restaurant recommendations, while identifying open challenges in interpretability, computational complexity, and the need for general MRS datasets.

## Method Summary
The survey systematically reviews MRS approaches by categorizing them into three technical taxonomies: Feature Interaction (bridge, fusion, and filtration methods for combining multimodal data), Feature Enhancement (contrastive learning and disentangled representation learning to improve representations under data sparsity), and Model Optimization (addressing computational challenges in training complex multimodal encoders). The methodology involves analyzing existing MRS literature across applications, extracting common patterns and techniques, and organizing them into a coherent framework. Implementation details and specific hyperparameters for surveyed models are not provided, focusing instead on conceptual frameworks and high-level approaches.

## Key Results
- MRS can handle different modality information inherent in multimedia services and alleviate data sparsity problems
- Three major technical challenges in MRS: feature interaction, feature enhancement, and model optimization
- Open challenges include model interpretability, computational complexity, and the need for general MRS datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal recommender systems improve performance by fusing heterogeneous data representations from different modalities (text, image, audio) into unified user/item embeddings.
- Mechanism: The system encodes each modality separately using specialized encoders (e.g., ViT for images, BERT for text), then applies interaction techniques such as attention-based fusion or graph-based message passing to integrate these representations into a common space suitable for recommendation prediction.
- Core assumption: Different modalities capture complementary aspects of user preferences and item characteristics; combining them yields richer representations than single-modality approaches.
- Evidence anchors:
  - [abstract] "MRS can handle different modality information, which is inherent in multimedia services. Besides, multimodal features are also helpful in alleviating the problem of data sparsity in RS."
  - [section] "The most recent works aim to propose efficient techniques to solve these three challenges, so we categorize them into three corresponding taxonomies: Feature Interaction, Feature Enhancement and Model Optimization."
  - [corpus] Weak: The corpus neighbors do not provide direct evidence about multimodal fusion mechanisms; they focus on MRS surveys and LLM integration.

### Mechanism 2
- Claim: Disentangled representation learning and contrastive learning enhance multimodal features by separating modality-specific and modality-common factors, improving generalization under data sparsity.
- Mechanism: DRL techniques decompose multimodal representations into shared and private components, while CL maximizes agreement between positive pairs (e.g., same item across modalities) and pushes apart negative pairs, effectively regularizing the learned embeddings.
- Core assumption: The underlying user preferences and item attributes can be factorized into shared and modality-specific components; contrastive pairs can be reliably constructed to guide learning.
- Evidence anchors:
  - [abstract] "MRS can also make use of abundant multimodal information of items to alleviate the problems of data sparsity and cold start, which widely exist in recommender systems."
  - [section] "Recently, to solve this problem, some models are equipped with Disentangled Representation Learning (DRL) and Contrastive Learning (CL) to carry out feature enhancement based on interaction..."
  - [corpus] Missing: No direct corpus evidence provided for contrastive or disentangled learning in MRS; inference is based on the paper text.

### Mechanism 3
- Claim: Two-stage training (pretrain multimodal encoders, then fine-tune for recommendation) balances computational efficiency with task-specific optimization.
- Mechanism: In the first stage, large-scale multimodal encoders are pretrained on auxiliary tasks (e.g., masked feature reconstruction, graph structure reconstruction). In the second stage, only the recommendation head is updated, leveraging the rich multimodal representations without retraining the entire encoder.
- Core assumption: Pretrained multimodal encoders capture general visual/linguistic features that transfer well to recommendation tasks; freezing them during fine-tuning saves computation while retaining performance.
- Evidence anchors:
  - [section] "Compared with end-to-end pattern, two-stage training scheme can target downstream tasks better, but it requests much higher computing resources. Thus, few MRS adopts two-step training."
  - [section] "In the pretraining stage, MML [40] first trains the meta-learner through meta-learning to increase model generalization, then trains the item embedding generator in the second stage."
  - [corpus] Weak: Corpus does not discuss two-stage training; the evidence is inferred from the survey paper.

## Foundational Learning

- Concept: Graph Neural Networks (GNN)
  - Why needed here: Many MRS use GNNs to model user-item interactions and capture relational patterns across modalities.
  - Quick check question: How does a graph convolution operation aggregate information from a node's neighbors in a user-item bipartite graph?

- Concept: Attention Mechanisms
  - Why needed here: Attention layers enable the model to weigh modality contributions dynamically, focusing on the most relevant features for each user/item pair.
  - Quick check question: In a multi-head attention setup, how does the model combine information from different attention heads?

- Concept: Contrastive Learning
  - Why needed here: CL is used to align multimodal representations and enhance feature discriminativeness, especially important under data sparsity.
  - Quick check question: What is the difference between instance-wise and modality-wise contrastive loss in the context of multimodal recommendation?

## Architecture Onboarding

- Component map:
  Modality Encoders (ViT, BERT, audio encoders) → Feature Interaction Module (attention, fusion, graph layers) → Feature Enhancement Module (DRL, CL) → Recommendation Head (MLP, matrix factorization) → Loss Functions (recommendation loss + auxiliary CL/DRL losses)

- Critical path:
  1. Input raw multimodal data → 2. Encode each modality → 3. Fuse/interact features → 4. Enhance representations → 5. Generate recommendation scores → 6. Compute loss → 7. Backpropagate gradients

- Design tradeoffs:
  - End-to-end vs. two-stage training: End-to-end offers joint optimization but is computationally heavy; two-stage is efficient but may limit adaptation.
  - Fusion granularity: Coarse-grained fusion is faster but may lose fine details; fine-grained preserves details but increases complexity.
  - Modality selection: More modalities improve coverage but increase data requirements and computational cost.

- Failure signatures:
  - Overfitting to training modality patterns without generalizing to new users/items
  - Modality imbalance causing dominant modalities to overshadow others
  - Training instability due to conflicting losses (recommendation vs. CL/DRL)

- First 3 experiments:
  1. Baseline MRS without any multimodal inputs (pure collaborative filtering) to measure the impact of adding modalities.
  2. MRS with single modality (e.g., only images) to assess modality contribution before full multimodal fusion.
  3. MRS with attention-based fusion but without feature enhancement to isolate the effect of interaction techniques from representation refinement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective and efficient techniques for fusing multimodal features in recommender systems?
- Basis in paper: [explicit] The paper identifies "Challenge 1: How to fuse the modality features in different semantic spaces and get the preferences for each modality" as a major challenge.
- Why unresolved: The paper surveys various fusion techniques (Bridge, Fusion, Filtration) but doesn't provide definitive evidence about which are most effective and efficient in practice. The effectiveness likely depends on specific applications and datasets.
- What evidence would resolve it: Systematic empirical comparisons across diverse datasets and application domains, measuring both recommendation accuracy and computational efficiency of different fusion techniques.

### Open Question 2
- Question: How can multimodal recommender systems achieve better model interpretability?
- Basis in paper: [explicit] The paper lists "Model Interpretability" as a key challenge, noting that "The complexity of multimodal models can make it difficult to understand and interpret the recommendations generated by the system."
- Why unresolved: The paper mentions only a few early works on interpretability but states that it "is still need to be explored." Complex multimodal models create black-box effects that are difficult to explain.
- What evidence would resolve it: Development and validation of interpretability methods specifically designed for multimodal recommender systems, with user studies demonstrating that explanations improve user trust and understanding.

### Open Question 3
- Question: What is the optimal training scheme (end-to-end vs. two-step) for multimodal recommender systems?
- Basis in paper: [explicit] The paper discusses both end-to-end and two-step training schemes, noting that "the multimodal recommendation model can be divided into two categories during training: End-to-end training and Two-step training."
- Why unresolved: The paper presents both approaches without clear guidance on when to use each. Each has trade-offs between computational efficiency and recommendation performance that depend on specific use cases.
- What evidence would resolve it: Comprehensive comparative studies across multiple MRS applications, measuring both recommendation quality and training/inference efficiency for different training schemes.

## Limitations
- The survey lacks detailed implementation specifics and quantitative comparisons between MRS approaches
- The effectiveness of specific feature enhancement techniques (DRL, CL) in MRS context is inferred from general deep learning literature rather than MRS-specific empirical results
- The paper acknowledges that two-stage training approaches are rarely adopted due to computational constraints, suggesting a gap between proposed methods and practical deployment

## Confidence

- **High confidence**: The categorization framework and taxonomy structure are well-grounded in existing MRS literature
- **Medium confidence**: Claims about multimodal fusion improving performance under data sparsity are supported by general RS principles but lack specific quantitative evidence in this survey
- **Low confidence**: The effectiveness of specific feature enhancement techniques (DRL, CL) in MRS context is inferred from general deep learning literature rather than MRS-specific empirical results

## Next Checks

1. Implement and compare the three feature interaction methods (bridge, fusion, filtration) on a standard MRS benchmark to quantify their relative performance gains
2. Conduct ablation studies measuring the impact of modality-specific versus shared representations in DRL-based MRS models
3. Benchmark computational requirements and recommendation accuracy trade-offs between end-to-end and two-stage training approaches on representative MRS datasets