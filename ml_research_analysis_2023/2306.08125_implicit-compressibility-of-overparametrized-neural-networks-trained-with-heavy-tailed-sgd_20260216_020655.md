---
ver: rpa2
title: Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed
  SGD
arxiv_id: '2306.08125'
source_url: https://arxiv.org/abs/2306.08125
tags:
- noise
- neural
- test
- pruning
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies implicit compressibility of overparametrized
  neural networks trained with heavy-tailed SGD. The authors propose injecting heavy-tailed
  noise into SGD iterations, proving that for any compression rate, there exists a
  level of overparametrization such that the output will be compressible with high
  probability.
---

# Implicit Compressibility of Overparametrized Neural Networks Trained with Heavy-Tailed SGD

## Quick Facts
- arXiv ID: 2306.08125
- Source URL: https://arxiv.org/abs/2306.08125
- Authors: 
- Reference count: 40
- Primary result: Heavy-tailed noise injection enables significantly higher compression rates (44% column pruning vs 10% with vanilla SGD) while maintaining test accuracy on MNIST

## Executive Summary
This paper addresses the challenge of compressing overparametrized neural networks by introducing heavy-tailed noise into SGD training. The key insight is that α-stable noise (with α < 2) induces heavy-tailed distributions in network weights, enabling lossy compression through column pruning. The authors provide both theoretical guarantees and empirical validation showing that for any desired compression rate, there exists a level of overparametrization where the trained network becomes compressible with high probability. Experiments demonstrate that on MNIST with 10K hidden units, heavy-tailed SGD enables 44% column pruning while maintaining 94% test accuracy, compared to only 10% with vanilla SGD.

## Method Summary
The method involves modifying standard SGD by injecting additive α-stable noise at each iteration, where α < 2 determines the tail heaviness. The noise can be of Type-I, II, or III stable distributions, each with different tail structures. As the network size n grows and learning rate η shrinks, the SGD recursion converges to a McKean-Vlasov SDE with heavy-tailed noise. The authors prove that this limiting process has a unique stationary distribution with infinite variance, which implies compressibility through sparse approximation theory. After training, column norms of the weight matrix are computed and smallest-norm columns are pruned to achieve the desired compression ratio κ.

## Key Results
- Heavy-tailed SGD enables 44% column pruning on MNIST (n=10K) while maintaining 94% test accuracy, versus 10% with vanilla SGD
- Compressibility advantage increases as α decreases (heavier tails) and n increases
- On CIFAR10, vanilla SGD already achieves good compressibility, but heavy-tailed SGD further improves it
- Type-I noise generally performs best across datasets and compression levels
- Theoretical guarantees show that for any compression rate, there exists overparametrization level achieving it with high probability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heavy-tailed noise injection creates compressible weight matrices via propagation of chaos
- Mechanism: When SGD is perturbed with α-stable noise (α < 2), the parameter columns converge to i.i.d. heavy-tailed distributions. In heavy-tailed distributions, a small fraction of elements dominate the norm, enabling lossy compression by pruning small-norm columns.
- Core assumption: α-stable noise with infinite variance induces heavy-tailed stationary distributions for parameter columns
- Evidence anchors:
  - [abstract]: "if we inject additive heavy-tailed noise to the iterates at each iteration, for any compression rate, there exists a level of overparametrization such that the output of the algorithm will be compressible with high probability"
  - [section]: "we prove a 'propagation of chaos' result with improved rates for a class of heavy-tailed stochastic differential equations"
  - [corpus]: Missing relevant evidence (corpus contains papers about heavy-tailed SGD but not about compression guarantees)
- Break condition: If noise tails become too heavy (α → 1), training instability may dominate compression benefits

### Mechanism 2
- Claim: Mean-field limit enables analytical compressibility guarantees
- Mechanism: As n → ∞ and step-size η → 0, the SGD recursion converges to a McKean-Vlasov SDE with measure-dependent coefficients. The heavy-tailed noise in this limiting SDE ensures that the marginal distributions have infinite second-order moments, which implies compressibility.
- Core assumption: The heavy-tailed McKean-Vlasov SDE has a unique stationary distribution with infinite variance
- Evidence anchors:
  - [abstract]: "we consider the mean-field limit, where n goes to infinity and η goes to zero"
  - [section]: "we prove a 'propagation of chaos' result with improved rates for a class of heavy-tailed stochastic differential equations"
  - [corpus]: Weak evidence - corpus papers discuss heavy tails in SGD but not the specific McKean-Vlasov analysis
- Break condition: If the mean-field approximation breaks down (finite n, non-small η), the i.i.d. assumption may fail

### Mechanism 3
- Claim: Strong error estimates bridge continuous theory to practical implementation
- Mechanism: The Euler discretization of the heavy-tailed SDE maintains proximity to the continuous dynamics with quantifiable error bounds. This allows incorporating discretization error into the compressibility analysis, ensuring practical algorithms achieve theoretical compression rates.
- Core assumption: Euler discretization error scales as O(n^(-α/2-1)) with the same heavy-tailed properties
- Evidence anchors:
  - [abstract]: "we derive error estimates for their Euler discretization"
  - [section]: "we derive strong-error estimates for the Euler discretization and show that for sufficiently small η, the trajectories of the discretized process will be close to the one of the continuous-time SDE"
  - [corpus]: Missing evidence - corpus lacks papers about discretization error for heavy-tailed SDEs
- Break condition: If discretization error dominates the heavy-tailed structure, compressibility guarantees fail

## Foundational Learning

- Concept: α-stable distributions and heavy-tailed processes
  - Why needed here: Heavy tails are the core mechanism enabling compressibility; understanding stable distributions is essential for analyzing the noise injection
  - Quick check question: What happens to the variance of an α-stable distribution when α < 2? (Answer: It becomes infinite)

- Concept: Propagation of chaos and mean-field limits
  - Why needed here: The theoretical framework relies on showing that many-particle systems (SGD iterates) converge to mean-field behavior where parameters become asymptotically independent
  - Quick check question: In the mean-field limit, what happens to the correlation between different parameter columns? (Answer: They become asymptotically independent)

- Concept: ℓp-compressibility and sparse approximation theory
  - Why needed here: The compression guarantees are established using results from sparse approximation theory that characterize when i.i.d. heavy-tailed sequences are compressible
  - Quick check question: What property of heavy-tailed distributions enables ℓp-compressibility for p < α? (Answer: A small fraction of elements dominate the norm)

## Architecture Onboarding

- Component map:
  Neural network -> Modified SGD with heavy-tailed noise -> Column norm computation -> Pruning module -> Compressed network

- Critical path:
  1. Initialize network weights randomly
  2. Run modified SGD with heavy-tailed noise for T iterations
  3. Compute column norms of final weight matrix
  4. Prune smallest-norm columns to achieve κ compression
  5. Evaluate accuracy of pruned network

- Design tradeoffs:
  - α selection: Smaller α → better compressibility but potential training instability
  - Noise type: Type-I/II/III have different tail structures affecting both compression and accuracy
  - n scaling: Larger n improves theoretical guarantees but increases computational cost
  - η selection: Smaller η better approximates continuous dynamics but slows training

- Failure signatures:
  - Training divergence: Noise too heavy (α too small) or step-size too large
  - No compression gain: α too close to 2 or network too small
  - Accuracy collapse after pruning: κ too aggressive or compression error threshold too low

- First 3 experiments:
  1. Baseline: Train with vanilla SGD, measure baseline compressibility (pruning ratio ~10-15%)
  2. α exploration: Train with α = 1.75, 1.8, 1.9, measure compressibility vs accuracy tradeoff
  3. Noise type comparison: Compare Type-I, II, III noise effects on same network/dataset

## Open Questions the Paper Calls Out
The paper identifies several important directions for future work:
- Extending the analysis to deeper neural network architectures beyond single-hidden-layer models
- Understanding the precise relationship between noise type and the compressibility-generalization tradeoff
- Determining conditions under which vanilla SGD already exhibits heavy-tailed behavior that makes additional noise injection unnecessary or harmful
- Analyzing how heavy-tailed noise injection affects the convergence rate and stability of training compared to vanilla SGD

## Limitations
- The theoretical framework relies on asymptotic analysis that may not fully capture finite-sample behavior
- The assumption of i.i.d. initialization across parameter columns is critical but not empirically validated
- The analysis focuses on single-hidden-layer networks, with generalization to deeper architectures unexplored

## Confidence

**High**: Empirical demonstration of improved compressibility with heavy-tailed noise (Figures 1-3)

**Medium**: Theoretical framework connecting heavy-tailed SDEs to compression guarantees (Theorems 1-2)

**Low**: Quantitative bounds on compression-accuracy tradeoff across all dataset/architecture combinations

## Next Checks

1. **Finite-size validation**: Test whether the i.i.d. column distribution assumption holds for realistic network sizes (n = 2K, 5K, 10K) by measuring empirical correlations between weight columns after training.

2. **Architecture generalization**: Evaluate compressibility on multi-layer networks and convolutional architectures to assess whether the single-layer theory extends to practical deep learning models.

3. **Distribution characterization**: Analyze the empirical distribution of column norms in trained networks to verify the heavy-tailed structure and identify the actual α parameter governing compressibility.