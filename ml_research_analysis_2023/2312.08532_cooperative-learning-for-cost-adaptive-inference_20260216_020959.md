---
ver: rpa2
title: Cooperative Learning for Cost-Adaptive Inference
arxiv_id: '2312.08532'
source_url: https://arxiv.org/abs/2312.08532
tags:
- network
- networks
- training
- loss
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Cooperative training framework for deep neural
  networks that enables dynamic runtime adjustment of network depth to meet varying
  resource constraints during inference. The method trains two Teammate networks and
  one Leader network, along with multiple sub-networks of different depths, using
  knowledge distillation.
---

# Cooperative Learning for Cost-Adaptive Inference

## Quick Facts
- arXiv ID: 2312.08532
- Source URL: https://arxiv.org/abs/2312.08532
- Authors: 
- Reference count: 40
- Key outcome: Cooperative training framework enabling dynamic depth adjustment during inference while maintaining accuracy across model sizes

## Executive Summary
This paper introduces a Cooperative Learning framework for deep neural networks that enables dynamic runtime adjustment of network depth to meet varying resource constraints during inference. The method trains two Teammate networks and one Leader network, along with multiple sub-networks of different depths, using knowledge distillation. Unlike traditional approaches that train separate models for different resource constraints, this framework trains all model sizes simultaneously in a "package deal" rather than individually, allowing flexible scaling on the fly.

The key innovation is the cooperative training strategy where Teammate networks transfer knowledge to each other and their sub-networks, while the Leader network guides them to ensure accuracy. Experiments on CIFAR-100 and Tiny ImageNet show that the method achieves comparable accuracy to full-sized networks across various model sizes, with smoother accuracy degradation as depth decreases compared to baseline methods. The framework is architecture-agnostic and maintains stable performance regardless of feature map size.

## Method Summary
The Cooperative Learning framework consists of two Teammate networks and one Leader network trained simultaneously. The Teammate networks derive sub-networks at various depths and transfer knowledge to them using knowledge distillation, while also learning from each other through mutual knowledge distillation. The Leader network guides the Teammates to ensure accuracy by providing supervision through cross-entropy loss with true labels. The framework introduces a Scaling Factor Sensitive Loss (SFSL) that weights losses from shallower sub-networks more heavily, improving their accuracy. All networks are trained together with random scaling factors sampled each epoch, enabling flexible depth adjustment during inference without retraining.

## Key Results
- Achieves comparable accuracy to full-sized networks across scaling factors 0.2 to 1.0
- Shows smoother accuracy degradation as depth decreases compared to BranchyNet and BYOT baselines
- Demonstrates architecture-agnostic performance across different feature map sizes

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Distillation from Full Network
Knowledge distillation from full network to sub-networks improves sub-network accuracy without separate full-size training. The full network provides soft labels that capture inter-class relationships, which sub-networks learn to mimic using Kullback-Leibler divergence loss. Core assumption: Sub-networks can effectively learn from the distributional information encoded in the full network's outputs.

### Mechanism 2: Mutual Knowledge Distillation
Mutual knowledge distillation between two Teammate networks improves both networks' performance. Each Teammate network learns from the other's outputs, creating a feedback loop where both networks improve by teaching each other. Core assumption: Two networks can benefit from learning each other's output distributions, not just from ground truth labels.

### Mechanism 3: Scaling Factor Sensitive Loss (SFSL)
SFSL improves accuracy of shallower sub-networks by weighting their losses more heavily. Loss contributions from shallower sub-networks are divided by their scaling factor, giving them proportionally more weight in the overall loss function. Core assumption: Layers common to all sub-networks (i.e., shallower layers) have disproportionate impact on overall performance.

## Foundational Learning

- Concept: Knowledge distillation and Kullback-Leibler divergence
  - Why needed here: The framework relies on transferring knowledge from larger networks to smaller sub-networks using soft labels
  - Quick check question: Can you explain the difference between cross-entropy loss and KL divergence loss in the context of knowledge distillation?

- Concept: Gumbel-Softmax reparameterization trick
  - Why needed here: Enables differentiable binary masking for dynamic depth control during training
  - Quick check question: How does the Gumbel-Softmax trick allow back-propagation through discrete sampling operations?

- Concept: Multi-task learning and ensemble methods
  - Why needed here: The framework trains multiple networks (two Teammates and a Leader) simultaneously with different but related objectives
  - Quick check question: What are the potential benefits and challenges of training multiple related networks together versus individually?

## Architecture Onboarding

- Component map: Leader network (trained on true labels only) -> Two Teammate networks (trained with mutual knowledge distillation and sub-network distillation) -> Sub-networks (derived from Teammates at various depths)
- Critical path: Training flow - Leader learns from labels → Teammates learn from each other and Leader → Teammates generate sub-networks → Sub-networks learn from their parent Teammates
- Design tradeoffs: Architecture flexibility vs. training complexity, number of sub-networks vs. computational cost during training, depth granularity vs. model diversity
- Failure signatures: Poor accuracy across all model sizes (likely training issues), unstable accuracy as depth changes (masking problems), one Teammate significantly outperforming the other (imbalanced mutual learning)
- First 3 experiments:
  1. Implement single Teammate with self-learning and SFSL on CIFAR-100, verify accuracy vs. baseline
  2. Add second Teammate and verify mutual learning improves both networks
  3. Add Leader network and verify it improves overall accuracy and stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Cooperative Learning scale with the number of Teammate networks beyond the two used in the experiments?
- Basis in paper: The paper uses two Teammate networks and a Leader network, but does not explore configurations with more than two Teammates
- Why unresolved: The paper only experiments with two Teammate networks and does not provide evidence or analysis of how additional Teammate networks would affect performance
- What evidence would resolve it: Experimental results comparing the performance of the Cooperative Learning framework with varying numbers of Teammate networks (e.g., 1, 2, 3, 4) on the same datasets

### Open Question 2
- Question: How does the Cooperative Learning framework perform when applied to different neural network architectures beyond ResNet, such as Vision Transformers or EfficientNet?
- Basis in paper: The paper claims the framework is architecture-agnostic but only provides experimental results using ResNet152
- Why unresolved: The paper only validates the approach on ResNet architectures, leaving uncertainty about its effectiveness on other architectures
- What evidence would resolve it: Experimental results applying the Cooperative Learning framework to various architectures (e.g., Vision Transformers, EfficientNet, MobileNet) on standard datasets

### Open Question 3
- Question: What is the impact of the scaling factor sensitivity loss (SFSL) on networks with significantly different depths or widths compared to the full network?
- Basis in paper: The paper introduces SFSL to improve performance of sub-networks, but only tests it on depth variations with scaling factors between 0.2 and 1.0
- Why unresolved: The experiments do not explore extreme scaling factors (e.g., 0.05 or 2.0) or width variations, leaving uncertainty about SFSL's effectiveness in these scenarios
- What evidence would resolve it: Experimental results showing the performance impact of SFSL when applied to networks with extreme depth/width scaling factors or when applied to width-adaptive networks

## Limitations

- Framework effectiveness depends heavily on quality of Leader network's guidance and Teammates' ability to learn from each other
- Requires training multiple networks simultaneously, increasing computational cost compared to single-network training
- Gumbel-Softmax reparameterization introduces approximation error that may affect final model quality

## Confidence

- High confidence: Knowledge distillation from full to sub-networks improves sub-network accuracy
- Medium confidence: Mutual knowledge distillation between Teammates provides meaningful improvement
- Medium confidence: SFSL improves accuracy of shallower sub-networks compared to uniform weighting

## Next Checks

1. Conduct ablation studies to verify that each component (mutual learning, Leader guidance, SFSL) contributes positively to the overall framework performance
2. Test the framework with different backbone architectures (e.g., MobileNet, EfficientNet) to verify architecture-agnostic claims
3. Measure training time and resource requirements compared to training individual models separately to quantify the practical overhead of the cooperative approach