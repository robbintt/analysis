---
ver: rpa2
title: 'SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs'
arxiv_id: '2306.17842'
source_url: https://arxiv.org/abs/2306.17842
tags:
- image
- spae
- tokens
- generation
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPAE, a semantic pyramid auto-encoder that
  enables frozen LLMs to perform multimodal generation and understanding tasks by
  converting images or videos into interpretable lexical tokens from the LLM's vocabulary.
  The key innovation is a multi-layer pyramid of tokens that captures both high-level
  semantic concepts and fine-grained appearance details, allowing flexible adjustment
  of token length for different tasks.
---

# SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs

## Quick Facts
- **arXiv ID**: 2306.17842
- **Source URL**: https://arxiv.org/abs/2306.17842
- **Reference count**: 40
- **Primary result**: Enables frozen LLMs to perform multimodal generation and understanding by converting images/videos into interpretable lexical tokens, achieving 25%+ improvement in few-shot classification accuracy

## Executive Summary
SPAE introduces a semantic pyramid auto-encoder that enables frozen LLMs to perform multimodal tasks by converting images and videos into interpretable lexical tokens from the LLM's vocabulary. The key innovation is a multi-layer pyramid structure that captures both high-level semantic concepts and fine-grained appearance details, allowing dynamic adjustment of token length for different tasks. The approach is validated on image classification, captioning, VQA, and image generation tasks, demonstrating successful text-to-image and image-to-video generation using frozen LLMs for the first time.

## Method Summary
SPAE uses a semantic pyramid auto-encoder to convert images into multi-layer token pyramids, where upper layers capture semantic concepts and lower layers capture fine-grained appearance details. The model is trained standalone without updating the LLM, using semantic guidance from a pre-trained CLIP model to ensure tokens are semantically relevant. For generation, SPAE employs progressive in-context denoising with corrupted examples, gradually reducing corruption through a cosine schedule. The approach uses autoregressive and non-autoregressive decoding stages to generate complete images.

## Key Results
- Achieves over 25% improvement in few-shot classification accuracy compared to state-of-the-art methods
- Demonstrates successful text-to-image and image-to-video generation using frozen LLMs for the first time
- Shows flexible token length adjustment for different tasks, using fewer tokens for understanding and more for generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SPAE enables frozen LLMs to perform multimodal tasks by converting visual content into interpretable lexical tokens from the LLM's vocabulary.
- Mechanism: SPAE uses a semantic pyramid auto-encoder that maps images to multi-layer token pyramids, where upper layers capture semantic concepts and lower layers capture fine-grained appearance details. These tokens are then processed by the frozen LLM through in-context learning.
- Core assumption: The frozen LLM's lexical embeddings contain sufficient semantic knowledge to interpret and generate content in foreign modalities when provided with appropriate visual representations.
- Evidence anchors:
  - [abstract] "SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary."
  - [section] "Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks."
- Break condition: If the LLM's vocabulary lacks sufficient semantic coverage for the visual domain, or if the token conversion process fails to preserve critical visual information.

### Mechanism 2
- Claim: The semantic pyramid structure allows dynamic adjustment of token length for different tasks, enabling both understanding and generation.
- Mechanism: SPAE organizes tokens in a pyramid where upper layers (fewer tokens) focus on high-level semantics for understanding tasks, while deeper layers (more tokens) provide detailed appearance information for generation tasks. This is achieved through streaming average quantization.
- Core assumption: Semantic concepts can be effectively represented with fewer tokens than fine-grained visual details, and the LLM can process varying token lengths appropriately.
- Evidence anchors:
  - [abstract] "The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction"
  - [section] "This design enables us to dynamically adjust the token length to accommodate various tasks, such as using fewer tokens for understanding tasks and more tokens for generation tasks."
- Break condition: If the pyramid structure fails to maintain semantic coherence across layers, or if the LLM cannot effectively process the varying token lengths.

### Mechanism 3
- Claim: Progressive in-context denoising enables generation of foreign modalities by operating in a corrupted token space.
- Mechanism: The approach uses corrupted versions of images as context examples, gradually reducing corruption levels through a cosine schedule. The LLM learns to denoise and generate complete images through autoregressive and non-autoregressive decoding stages.
- Core assumption: LLMs can learn generation patterns from corrupted examples through in-context learning without parameter updates.
- Evidence anchors:
  - [abstract] "We propose a new progressive prompting method that facilitates in-context generation of long cross-modal sequences."
  - [section] "To ensure the generation is not simply copying the context, we enforce a minimal corruption rate of 20% such that no identical image from the context matches the real target image."
- Break condition: If the corruption rate is too high or too low, or if the LLM fails to learn the denoising pattern from the provided context.

## Foundational Learning

- Concept: Vector quantization and discrete latent spaces
  - Why needed here: SPAE relies on converting continuous visual embeddings into discrete tokens that can be processed by the LLM, which operates on discrete tokens.
  - Quick check question: What is the difference between learned codebooks and frozen language codebooks, and why does SPAE use the latter?

- Concept: In-context learning and few-shot learning
  - Why needed here: The entire approach depends on the LLM's ability to learn new tasks from examples without parameter updates, which is the core of in-context learning.
  - Quick check question: How does the progressive generation approach in SPAE differ from standard autoregressive decoding in LLMs?

- Concept: Multimodal representation learning
  - Why needed here: Understanding how to bridge visual and textual modalities through shared semantic spaces is crucial for SPAE's token conversion process.
  - Quick check question: What role does the CLIP model play in SPAE's semantic loss calculation, and why is it important?

## Architecture Onboarding

- Component map: Image → CNN encoder → continuous embeddings → SPAE quantizer → pyramid tokens → LLM processing → SPAE decoder → reconstructed image

- Critical path:
  1. Image → CNN encoder → continuous embeddings
  2. Continuous embeddings → SPAE quantizer → pyramid tokens
  3. Pyramid tokens → LLM processing (understanding/generation)
  4. LLM output → SPAE decoder → reconstructed image

- Design tradeoffs:
  - Pyramid vs flat token structure: Pyramid provides semantic flexibility but adds complexity
  - Frozen vs learned codebook: Frozen ensures interpretability but may limit reconstruction quality
  - AR vs NAR decoding: AR provides better quality but is slower; NAR is faster but may sacrifice quality

- Failure signatures:
  - Poor reconstruction quality: Check if semantic guidance is working or if codebook alignment is off
  - LLM fails to process tokens: Verify token format and sequence length compatibility
  - Generation quality issues: Check corruption rate settings and decoding strategy

- First 3 experiments:
  1. Test SPAE tokenization on a small image set and verify semantic relevance using CLIP scores
  2. Validate in-context learning on a simple image classification task with known examples
  3. Test progressive denoising generation on a controlled dataset with varying corruption rates

## Open Questions the Paper Calls Out
- **Scalability to higher resolutions**: The paper demonstrates SPAE on 128×128 images but does not explore higher resolutions or longer video sequences, noting the potential for better reconstruction quality with more token layers but not validating this.
- **Comparison to fine-tuned multimodal models**: The authors acknowledge a substantial gap to specialized models like Stable Diffusion that have been specifically trained on billions of text-image pairs, but provide no direct comparison.
- **Impact of different LLM architectures**: While SPAE is tested with PaLM 2 and GPT-3.5, the authors note potential future work on finetuning or adapter tuning of LLMs on large-scale text-image datasets without exploring how different architectures affect performance.

## Limitations
- The approach relies on frozen LLMs, which limits the model's ability to learn specialized multimodal representations compared to fine-tuned models trained on billions of text-image pairs
- Progressive in-context denoising introduces complexity with corruption scheduling and hybrid decoding, but the sensitivity of these hyperparameters is not thoroughly explored
- Limited validation on higher-resolution images and longer video sequences, which may face sequence length limitations in LLMs

## Confidence
- **High Confidence**: Pyramid token structure and flexible token length adjustment for different tasks is well-supported by architecture description and empirical results
- **Medium Confidence**: 25% improvement claim in few-shot classification is supported by experiments but lacks detailed ablation studies and clear baseline specifications
- **Low Confidence**: Claim about being first to achieve text-to-image and image-to-video generation with frozen LLMs requires more careful historical context and comprehensive literature review

## Next Checks
1. **Semantic Coverage Analysis**: Conduct systematic analysis of how well frozen LLM vocabulary covers visual concepts across different domains by measuring semantic similarity between ground truth visual concepts and closest available tokens
2. **Corruption Rate Sensitivity**: Perform ablation study on progressive denoising method by varying corruption rates and schedules across different datasets to understand robustness and identify optimal parameters
3. **Cross-LLM Generalization**: Test SPAE's performance when transferring trained model to different frozen LLMs (different sizes or architectures) to validate whether semantic pyramid structure is truly LLM-agnostic or overfits to specific model characteristics