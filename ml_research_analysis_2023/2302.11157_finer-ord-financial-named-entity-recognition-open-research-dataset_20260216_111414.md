---
ver: rpa2
title: 'FiNER-ORD: Financial Named Entity Recognition Open Research Dataset'
arxiv_id: '2302.11157'
source_url: https://arxiv.org/abs/2302.11157
tags:
- labeling
- entity
- dataset
- label
- financial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FiNER-ORD, the first high-quality English
  Financial Named Entity Recognition Open Research Dataset. The dataset contains 201
  news articles with manual annotations for person (PER), location (LOC), and organization
  (ORG) entities, addressing the lack of domain-specific NER data in finance.
---

# FiNER-ORD: Financial Named Entity Recognition Open Research Dataset

## Quick Facts
- arXiv ID: 2302.11157
- Source URL: https://arxiv.org/abs/2302.11157
- Reference count: 40
- Introduces the first high-quality English Financial Named Entity Recognition Open Research Dataset (FiNER-ORD) with 201 manually annotated news articles.

## Executive Summary
The paper introduces FiNER-ORD, the first high-quality English Financial Named Entity Recognition Open Research Dataset. The dataset contains 201 news articles with manual annotations for person (PER), location (LOC), and organization (ORG) entities, addressing the lack of domain-specific NER data in finance. To establish a benchmark, the authors develop FiNER, a weak-supervision framework extending Snorkel for span-level labeling, incorporating multiple heuristic labeling functions tailored to financial texts. They evaluate FiNER-ORD with various pre-trained language models, achieving a best weighted F1-score of 0.7948 using FiNER labeling functions aggregated with Snorkel's weighted majority vote. The dataset, models, and code are publicly available under a CC BY-NC 4.0 license.

## Method Summary
The authors create FiNER-ORD by collecting 201 financial news articles and manually annotating PER, LOC, and ORG entities. They develop FiNER, a weak-supervision framework that extends Snorkel for span-level NER. FiNER uses multiple heuristic labeling functions (FiNER-LFs) tailored to financial texts, which are aggregated using Snorkel's weighted majority vote to generate training labels. The framework is evaluated on FiNER-ORD using various pre-trained language models, with the best performance achieved using FiNER-LFs and Snorkel WMV.

## Key Results
- FiNER-ORD is the first high-quality English Financial NER Open Research Dataset with 201 manually annotated news articles.
- FiNER-LFs improve precision over vanilla Flair by adding domain-specific heuristics for PER, LOC, and ORG entity types.
- The best weighted F1-score of 0.7948 is achieved using FiNER-LFs aggregated with Snorkel's weighted majority vote.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weak-supervision via Snorkel framework enables efficient labeling of large financial NER datasets without requiring extensive manual annotation.
- Mechanism: Labeling functions (LFs) encode heuristic rules specific to financial texts (e.g., detecting organization entities via suffixes like "Inc", "LLC", "Ltd"). These LFs generate noisy labels for unlabeled data, which are then aggregated by Snorkel's weighted majority vote to produce high-quality training labels.
- Core assumption: Financial domain heuristics can be encoded as LFs with reasonable precision and recall.
- Evidence anchors:
  - [abstract] "We develop the first high-quality English Financial NER Open Research Dataset (FiNER-ORD)... We benchmark multiple pre-trained language models (PLMs) and large-language models (LLMs) on FiNER-ORD."
  - [section 4.1] "We rely on Snorkel, a weak-supervision framework, to generate training data programmatically."
  - [corpus] Weak, as the paper does not explicitly validate LF precision/recall against ground truth.
- Break condition: If heuristic rules fail to capture domain-specific patterns, the weak-supervision model will generate poor quality labels leading to low NER performance.

### Mechanism 2
- Claim: Extending Snorkel for span-level labeling enables FiNER to handle token-level entity recognition required for NER.
- Mechanism: The original Snorkel framework operates on sentence-level classification. FiNER subclasses BaseLFApplier and implements PandasLFApplierForNER to flatten the 3D label matrix (sentences × tokens × LFs) into a 2D format compatible with Snorkel's aggregation methods.
- Core assumption: The flattening approach preserves entity boundary information needed for accurate NER.
- Evidence anchors:
  - [section 4.1] "In order to use Snorkel's functionalities, we design a new applier called PandasLFApplierForNER which subclasses BaseLFApplier and is specifically designed for NER tasks to operate on the word/span-level."
  - [section 4.3] "Our goal is to generate a label matrix that contains the label of each word of each sentence by each labeling function."
  - [corpus] Weak, as no ablation study tests performance with/without span-level extension.
- Break condition: If flattening loses token-level context, the aggregated labels will be inaccurate, reducing NER model performance.

### Mechanism 3
- Claim: FiNER-LFs improve precision over vanilla Flair by adding domain-specific heuristics for PER, LOC, and ORG entity types.
- Mechanism: Multiple LFs per entity type capture different financial domain patterns (e.g., "label_org_heuristic_abbr" detects organization abbreviations, "label_per_heuristic_exec_title" finds persons by executive titles). Aggregation via Snorkel WMV resolves conflicts between LFs.
- Core assumption: Financial texts contain sufficient predictable patterns to encode as effective LFs.
- Evidence anchors:
  - [section 4.2] "We add more labeling functions to improve the accuracy of extracting a person, location, and organization from financial documents."
  - [table 3] Lists FiNER-LFs with descriptions of heuristics for each entity type.
  - [corpus] Weak, as no per-LF precision/recall analysis is provided.
- Break condition: If heuristics are too specific or too general, they may introduce noise or miss entities, degrading overall model performance.

## Foundational Learning

- Concept: Weak-supervision framework (Snorkel)
  - Why needed here: Manual annotation of large financial NER datasets is expensive and time-consuming; weak-supervision enables scalable data creation.
  - Quick check question: What are the three main advantages of weak-supervision over manual labeling mentioned in the paper?

- Concept: Labeling functions (LFs) and their aggregation
  - Why needed here: LFs encode domain expertise as rules; their aggregation resolves conflicts and produces high-quality labels from noisy sources.
  - Quick check question: How does FiNER handle the case where a word is split by the common tokenizer but not by an individual LF's tokenizer?

- Concept: Span-level labeling for NER
  - Why needed here: NER requires labeling individual tokens within text spans, unlike sentence-level classification; FiNER extends Snorkel to support this.
  - Quick check question: What is the shape of the label matrix before and after flattening in FiNER's implementation?

## Architecture Onboarding

- Component map: Unlabeled raw text dataset → Preprocessing (Stanza tokenizer) → Labeling functions (FiNER-LFs) → Label matrix generation → Snorkel aggregation (WMV or majority vote) → Final labeled NER dataset → Training/evaluation of NER models
- Critical path: Raw text → Preprocessing → Labeling functions → Aggregation → Labeled data
- Design tradeoffs:
  - Flexibility vs. consistency: Allowing each LF to use custom tokenization provides flexibility but requires alignment to a common scheme for aggregation.
  - Heuristic specificity vs. coverage: Highly specific LFs may have high precision but low recall; combining multiple LFs balances this tradeoff.
  - Weak-supervision noise vs. manual labeling quality: Weak-supervision is faster and cheaper but introduces noise that must be mitigated through LF design and aggregation.
- Failure signatures:
  - Low precision: Indicates LFs are too permissive or conflicts are not well-resolved by aggregation.
  - Low recall: Suggests LFs miss many entities; may need more diverse heuristics or better preprocessing.
  - Performance gap between validation and test: Indicates overfitting to LF-generated training data.
- First 3 experiments:
  1. Run Vanilla Flair (3 out-of-the-box LFs, no aggregation) on FiNER-ORD test set to establish baseline.
  2. Run FiNER-LFs + Majority Vote to test if multiple LFs per entity type improve performance without complex aggregation.
  3. Run FiNER-LFs + Snorkel WMV to evaluate if learned aggregation weights further improve performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the model better distinguish between locations and organizations when location names are part of organization names (e.g., "Google India")?
- Basis in paper: [explicit] The paper explicitly mentions this as a limitation where "Google India" has "India" as a location but is labeled as an organization.
- Why unresolved: The paper states they do not permit overlapping entity labels, making it impossible to correctly label such cases.
- What evidence would resolve it: A new labeling strategy or model architecture that can handle overlapping entity labels, tested on financial texts containing such ambiguous cases.

### Open Question 2
- Question: Would incorporating a miscellaneous (MISC) entity label improve overall NER performance in financial texts?
- Basis in paper: [explicit] The paper explicitly states they do not incorporate a MISC label and suggests researchers could expand on their work by adding it.
- Why unresolved: The current dataset and models are limited to PER, LOC, and ORG entities, potentially missing important entity types in financial texts.
- What evidence would resolve it: Extending the dataset and models to include MISC labels, then benchmarking performance improvements on financial texts.

### Open Question 3
- Question: How would allowing labeling functions to leverage knowledge from other sentences or labeling functions affect NER performance?
- Basis in paper: [inferred] The paper mentions this as a limitation inherited from Snorkel, stating labeling functions cannot leverage knowledge from sentences other than the one they are currently processing.
- Why unresolved: The current weak-supervision framework restricts each labeling function to work independently on individual sentences.
- What evidence would resolve it: Developing a modified weak-supervision framework that allows cross-sentence and cross-labeling-function knowledge sharing, then comparing performance with the current approach.

## Limitations

- The specific impact of the span-level extension on NER performance is not quantified through ablation studies.
- Per-labeling function precision and recall values are not reported, making it difficult to assess the quality and diversity of FiNER-LFs.
- The evaluation is limited to a single test set, with no cross-validation or external validation to ensure generalizability.

## Confidence

- High: The creation of FiNER-ORD as the first high-quality financial NER dataset with manual annotations.
- Medium: The development of FiNER as a weak-supervision framework extending Snorkel for span-level labeling.
- Medium: The performance improvement of FiNER-LFs over vanilla Flair through domain-specific heuristics.
- Low: The specific impact of the span-level extension on NER performance without ablation studies.
- Low: The robustness of the dataset and models to tokenization variations and annotation inconsistencies.

## Next Checks

1. Implement an ablation study comparing FiNER's performance with and without the span-level extension to quantify its contribution to NER accuracy.
2. Report per-labeling function precision and recall values to assess the quality and diversity of FiNER-LFs for each entity type.
3. Conduct cross-validation or test on an external financial NER dataset to evaluate the generalizability of FiNER-ORD and the trained models.