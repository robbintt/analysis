---
ver: rpa2
title: Reparameterized Policy Learning for Multimodal Trajectory Optimization
arxiv_id: '2307.10710'
source_url: https://arxiv.org/abs/2307.10710
tags:
- policy
- learning
- reward
- latent
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of continuous action space policy
  optimization in reinforcement learning by proposing a multimodal policy parameterization.
  The core method, Reparameterized Policy Gradient (RPG), uses latent variables to
  model a generative policy over optimal trajectories, enabling exploration of multiple
  modes in the solution space.
---

# Reparameterized Policy Learning for Multimodal Trajectory Optimization

## Quick Facts
- arXiv ID: 2307.10710
- Source URL: https://arxiv.org/abs/2307.10710
- Reference count: 30
- One-line primary result: RPG outperforms single-modal policies and previous approaches on sparse-reward environments like dexterous manipulation and block pushing.

## Executive Summary
This paper addresses the challenge of continuous action space policy optimization in reinforcement learning by proposing a multimodal policy parameterization. The core method, Reparameterized Policy Gradient (RPG), uses latent variables to model a generative policy over optimal trajectories, enabling exploration of multiple modes in the solution space. RPG combines this with a learned world model and an object-centric intrinsic reward to achieve strong exploration capabilities. The method outperforms single-modal policies and previous approaches on a range of tasks, including sparse-reward environments like dexterous manipulation and block pushing.

## Method Summary
RPG proposes a principled framework that models the continuous RL policy as a generative model of optimal trajectories. By conditioning the policy on a latent variable z, RPG derives a novel variational bound as the optimization objective, which promotes exploration of the environment. The policy is decomposed into a latent prior πθ(z|s1) and a conditional policy πθ(a|s, z). Random sampling of z before each episode effectively "reparameterizes" a single random seed into multiple distinct action distributions, each corresponding to a different mode of the optimal trajectory distribution. RPG combines this with a learned world model and an object-centric intrinsic reward to achieve strong exploration capabilities and high data efficiency.

## Key Results
- RPG outperforms single-modal policies and previous approaches on sparse-reward environments like dexterous manipulation and block pushing.
- The method shows improved success rates on tasks with local optima and achieves higher data efficiency compared to baselines.
- Multimodal policy parameterization is essential for effective exploration and optimization in continuous control problems.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reparameterized latent variables enable multimodal policy exploration by converting a unimodal distribution into a mixture over latent modes.
- **Mechanism:** The policy is decomposed into a latent prior πθ(z|s1) and a conditional policy πθ(a|s, z). Random sampling of z before each episode effectively "reparameterizes" a single random seed into multiple distinct action distributions, each corresponding to a different mode of the optimal trajectory distribution.
- **Core assumption:** The latent space Z is expressive enough to capture distinct modes in the trajectory space, and the neural network can transform a simple latent prior into complex multimodal action distributions.
- **Evidence anchors:** [abstract], [section], [corpus]
- **Break condition:** If the latent space dimensionality is too small, or if the network capacity is insufficient, the reparameterization cannot generate meaningful distinct modes, causing mode collapse.

### Mechanism 2
- **Claim:** The variational bound (ELBO) unifies reward maximization and mode consistency, preventing the policy from collapsing into a single mode.
- **Mechanism:** The ELBO contains three terms: reward maximization, cross-entropy between πθ(z|s, a) and pϕ(z|s, a) for consistency, and policy entropy for exploration. Optimizing all terms jointly ensures that the policy generates diverse trajectories that are still consistent with the learned latent-posterior mapping.
- **Core assumption:** The auxiliary distribution pϕ(z|s, a) can be learned to approximate the true posterior of z given trajectory segments, and the cross-entropy term is strong enough to enforce consistency without overwhelming the reward term.
- **Evidence anchors:** [abstract], [section], [corpus]
- **Break condition:** If β (cross-entropy weight) is too small, the policy ignores z and collapses; if too large, the policy explores too much and fails to exploit high-reward modes.

### Mechanism 3
- **Claim:** Model-based planning with latent variables enables sample-efficient exploration of distant modes without requiring many environment interactions.
- **Mechanism:** The learned world model hψ predicts future states and rewards given current state and action. Combined with the latent-conditioned policy, the agent can simulate trajectories for different z values and choose actions that maximize the estimated value function, allowing "jumps" between distant modes in imagination.
- **Core assumption:** The learned world model is accurate enough in regions relevant to exploration, and the planning horizon is sufficient to capture mode transitions.
- **Evidence anchors:** [abstract], [section], [corpus]
- **Break condition:** If the world model is inaccurate in critical regions, planning will be misled and exploration will be inefficient or unsafe.

## Foundational Learning

- **Concept:** Variational inference and ELBO maximization
  - Why needed here: The ELBO provides a tractable objective for optimizing the posterior of optimal trajectories while maintaining exploration and mode consistency.
  - Quick check question: What three terms make up the ELBO in this framework, and how does each contribute to the learning objective?

- **Concept:** Reparameterization trick for continuous latent variables
  - Why needed here: Enables gradient-based optimization through stochastic nodes by sampling z from a simple distribution and transforming it through a neural network.
  - Quick check question: How does the reparameterization trick avoid the need for marginalizing over z in the policy gradient?

- **Concept:** Maximum entropy reinforcement learning
  - Why needed here: The entropy term in the ELBO encourages exploration and prevents premature convergence to suboptimal modes.
  - Quick check question: How does maximizing entropy in the policy help with exploration in multimodal reward landscapes?

## Architecture Onboarding

- **Component map:** s1 → z ~ πθ(z|s1) → (s, a) ~ πθ(a|s, z) → st+1 ~ hψ(st, at) → r ~ Rψ(st, at) → update πθ, pϕ, hψ, Rψ, Qψ
- **Critical path:** s1 → z ~ πθ(z|s1) → (s, a) ~ πθ(a|s, z) → st+1 ~ hψ(st, at) → r ~ Rψ(st, at) → update πθ, pϕ, hψ, Rψ, Qψ
- **Design tradeoffs:**
  - Latent space dimension vs. expressiveness: Higher dimensions allow more modes but increase sample complexity
  - β weight vs. exploration-exploitation balance: Higher β increases diversity but may reduce reward optimization
  - World model horizon vs. accuracy: Longer horizons enable better planning but suffer from compounding errors
- **Failure signatures:**
  - Mode collapse: Policy ignores z and behaves like single-mode SAC
  - Over-exploration: Policy samples diverse modes but fails to converge to high-reward solutions
  - Model bias: World model inaccuracies lead to poor planning decisions
- **First 3 experiments:**
  1. Train on a 1D bandit with two reward modes to verify multimodal policy can find both optima
  2. Compare exploration efficiency on a 2D maze with sparse rewards vs. single-mode SAC
  3. Test on a dense-reward environment with local optima to verify ability to escape suboptimal modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed multimodal policy parameterization scale to even higher-dimensional continuous action spaces, such as those encountered in humanoid robotics?
- Basis in paper: [explicit] The paper demonstrates the method's effectiveness on tasks with up to 26-dimensional action spaces, but scalability to higher dimensions is not directly addressed.
- Why unresolved: The experiments focus on dexterous manipulation and locomotion tasks, which may not fully represent the complexity of humanoid control.
- What evidence would resolve it: Empirical results showing the method's performance on humanoid robotics tasks with significantly higher action dimensions.

### Open Question 2
- Question: What is the impact of different latent variable distributions (e.g., Gaussian vs. categorical) on the exploration efficiency and final performance in continuous control tasks?
- Basis in paper: [explicit] The ablation study compares Gaussian and categorical latent spaces but does not extensively explore other distributions or their effects on exploration.
- Why unresolved: The paper focuses on Gaussian and categorical distributions without a comprehensive comparison to other potential distributions.
- What evidence would resolve it: Systematic experiments comparing the proposed method with various latent variable distributions across multiple tasks.

### Open Question 3
- Question: How does the proposed method perform in partially observable environments where the agent does not have direct access to the full state?
- Basis in paper: [explicit] The method is evaluated in fully observable environments, but its performance in partially observable settings is not discussed.
- Why unresolved: The experiments assume full state observability, which may not be realistic in many real-world applications.
- What evidence would resolve it: Results from experiments in partially observable environments, such as those with partial state observations or noisy sensors.

### Open Question 4
- Question: What are the computational trade-offs of using the proposed multimodal policy parameterization compared to single-modal policies in terms of training time and sample efficiency?
- Basis in paper: [explicit] The paper mentions improved sample efficiency but does not provide a detailed analysis of computational costs.
- Why unresolved: The focus is on performance gains rather than computational resource usage.
- What evidence would resolve it: A comparative analysis of training times, memory usage, and sample efficiency between the proposed method and single-modal policies across various tasks.

## Limitations
- The core uncertainty lies in the scalability of the latent-variable reparameterization to high-dimensional action spaces.
- The computational overhead of maintaining both a learned world model and the variational inference framework is substantial.
- The assumption that a single latent variable can capture all relevant modes in complex environments may break down as task complexity increases.

## Confidence
**High Confidence:**
- RPG's ability to discover multiple modes in continuous control tasks is well-supported by ablation studies and quantitative results.
- The superiority of multimodal policies over single-mode baselines on sparse-reward environments is convincingly demonstrated.
- The general framework of combining latent-variable policies with world models for exploration is theoretically sound.

**Medium Confidence:**
- The claim that RPG achieves "strong exploration capabilities" relies heavily on synthetic environments and may not generalize to all continuous control scenarios.
- The assertion that RPG is "sample-efficient" compared to model-free methods needs more diverse benchmarks for validation.
- The practical benefits of the object-centric intrinsic reward need more empirical validation beyond the presented tasks.

**Low Confidence:**
- The long-term stability of the world model in RPG across extended training periods is not thoroughly investigated.
- The sensitivity of RPG to hyperparameters (latent dimension, β weight, learning rates) is not systematically explored.
- The claim that RPG can "model the entire distribution of optimal trajectories" may be overstated for highly complex environments.

## Next Checks
1. **Scaling Test:** Evaluate RPG on high-dimensional continuous control tasks (e.g., humanoid locomotion or multi-agent coordination) to verify the method's scalability beyond 7-DoF manipulation.
2. **Ablation Study:** Conduct a systematic ablation study varying the latent dimension, β weight, and world model horizon to identify the critical factors for successful exploration and optimization.
3. **Robustness Test:** Test RPG's performance under different reward structures (dense vs. sparse, shaped vs. sparse) and compare its exploration efficiency to state-of-the-art curiosity-driven exploration methods like RND or ICM in a standardized benchmark.