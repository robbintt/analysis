---
ver: rpa2
title: Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order
  Execution in Finance
arxiv_id: '2307.03119'
source_url: https://arxiv.org/abs/2307.03119
tags:
- execution
- communication
- agents
- order
- trading
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-agent reinforcement learning approach
  to simultaneously execute multiple financial orders while maximizing overall profit.
  The method treats each order as an individual agent that communicates its intended
  actions to others through a learnable multi-round protocol, allowing agents to refine
  decisions and avoid cash shortages.
---

# Learning Multi-Agent Intention-Aware Communication for Optimal Multi-Order Execution in Finance

## Quick Facts
- arXiv ID: 2307.03119
- Source URL: https://arxiv.org/abs/2307.03119
- Reference count: 40
- Primary result: Multi-agent reinforcement learning approach for simultaneous order execution outperforms baselines on profit gain, collaboration, and cash management

## Executive Summary
This paper introduces a multi-agent reinforcement learning approach to simultaneously execute multiple financial orders while maximizing overall profit. The method treats each order as an individual agent that communicates its intended actions to others through a learnable multi-round protocol, allowing agents to refine decisions and avoid cash shortages. A novel action value attribution method is used to optimize intended actions directly, improving sample efficiency. Experiments on two real-world markets show the approach outperforms baselines on execution gain, collaboration effectiveness, and cash management.

## Method Summary
The method uses a multi-agent reinforcement learning framework where each order is executed by a dedicated agent. Agents communicate their intended actions through a learnable multi-round protocol, refining decisions based on others' intentions. The action value attribution method optimizes these intended actions by attributing the final cumulative reward across communication rounds. Proximal policy optimization (PPO) is used for training, with shared policy and value networks across agents to ensure scalability. The approach is evaluated on historical transaction data from China A-share and US stock markets.

## Key Results
- Intention-aware communication method outperforms baselines (TWAP, VWAP, AC, PPO, DDQN, CommNet, TarMAC, IS) on execution gain (EG) and collaboration effectiveness (TOC)
- Method achieves higher annualized rate of return (ARR) and better gain-loss ratio (GLR) compared to baselines
- Performance improves with 3 communication rounds (K=3), then plateaus, indicating optimal balance between coordination and computational cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-round intention-aware communication allows agents to progressively refine intended actions by sharing them directly, rather than just sharing partial observations.
- Mechanism: Each round, agents exchange intended actions (what they plan to do now), then update hidden states based on both their own observation and others' intentions. This feedback loop enables coordination around cash balance and order timing.
- Core assumption: Agents can learn to interpret and respond to others' intended actions in a way that improves joint reward.
- Evidence anchors:
  - [abstract] "...propose a learnable multi-round communication protocol, for the agents communicating the intended actions with each other and refining accordingly."
  - [section 4.2.1] "Our central improvement lands in the multi-round communication process...we focus on 'what to communicate' and share the intended actions of agents during each round of communication."
  - [corpus] No direct evidence; this is novel to the paper.
- Break condition: If the communication channel fails to pass intended actions clearly, or if agents' reward structures don't encourage coordination, refinement will not improve performance.

### Mechanism 2
- Claim: The action value attribution method makes intended actions directly optimize the final cumulative reward without introducing bias.
- Mechanism: For each round, an auxiliary objective maximizes the expected return of the intended action, with the baseline being the return of the previous round's intended action. This attribution distributes the credit of the final decision across rounds.
- Core assumption: The Q-value estimator trained on final trajectories can reasonably approximate the value of intermediate intended actions.
- Evidence anchors:
  - [section 4.2.2] "We optimize ð…ð‘˜ by defining an auxiliary objective function to maximize the expected cumulative reward ð‘„ (ð’”, ð’‚ð‘˜)..."
  - [section 4.2.2] "Taking derivatives... we can easily find that âˆ‡ðœ½ ð½ð‘˜ (ðœ½) = âˆ‡ðœ½ ð½ â€²ð‘˜ (ðœ½)."
  - [corpus] No external evidence; derivation is internal to the paper.
- Break condition: If the action value estimator is inaccurate or the policy gradient becomes unstable, the auxiliary objectives will not improve intended actions.

### Mechanism 3
- Claim: Factorizing the joint action space by treating each order as an agent allows scalability to many orders and varying numbers of orders.
- Mechanism: Each agent controls one order, so the joint action space grows linearly rather than exponentially. Shared reward encourages collaboration to maximize overall profit.
- Core assumption: The environment's transition function and reward can be computed from individual agent actions without loss of information.
- Evidence anchors:
  - [abstract] "...treat every agent as an individual operator to trade one specific order, while keeping communicating with each other and collaborating for maximizing the overall profits."
  - [section 3.2] "We define the agent to be the operator for a single asset, and each agent would be responsible for the execution of the corresponding order."
  - [corpus] Related literature [13â€“15] supports MARL for scalability, but this specific multi-order execution formulation is novel.
- Break condition: If agent interactions are too complex to be captured by the communication protocol, or if the shared reward doesn't incentivize coordination, scalability gains will be lost.

## Foundational Learning

- Concept: Multi-agent reinforcement learning (MARL) and its communication mechanisms.
  - Why needed here: The problem requires multiple agents to act simultaneously on different orders while sharing a limited resource (cash), necessitating coordination.
  - Quick check question: Can you explain the difference between independent Q-learning and a centralized training with decentralized execution approach in MARL?

- Concept: Reinforcement learning policy gradient methods (e.g., PPO) and value function estimation.
  - Why needed here: Agents must learn policies that maximize cumulative reward, and the action value attribution method requires estimating Q-values for intended actions.
  - Quick check question: How does PPO's clipped objective help with stable training compared to vanilla policy gradient?

- Concept: Order execution in quantitative finance and market impact modeling.
  - Why needed here: The reward function balances execution profit against market impact and cash shortage penalties; understanding this domain is critical for interpreting results.
  - Quick check question: Why does the quadratic penalty on trading volume help reduce market impact in order execution?

## Architecture Onboarding

- Component map:
  - Extractor network (E) -> Communication channel (C) -> Decision module (D) -> Action value estimator (QÌ‚)

- Critical path:
  1. Extract hidden state from observation.
  2. Exchange intended actions and update hidden states for K rounds.
  3. Execute final intended action (round K) in environment.
  4. Update networks using multi-objective loss combining all rounds' auxiliary objectives.

- Design tradeoffs:
  - More communication rounds (K) improve coordination but increase computational cost and risk of over-communication.
  - Smaller action spaces simplify learning but reduce execution flexibility.
  - Shared networks reduce parameters but may limit agent specialization.

- Failure signatures:
  - High TOC (time of conflict) indicates poor cash coordination despite communication.
  - Stagnant or decreasing EG over training rounds suggests the action value estimator is not guiding improvement.
  - High variance in EG across seeds indicates unstable learning, possibly from inaccurate Q-value estimates.

- First 3 experiments:
  1. Vary K (number of communication rounds) and measure EG and TOC; expect EG to improve up to K=3 then plateau.
  2. Remove the action value attribution baseline and compare EG; expect reduced performance due to less stable gradients.
  3. Replace intended actions with partial observation messages (like TarMAC) and measure EG and TOC; expect higher TOC and lower EG due to less explicit coordination.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed intention-aware communication method generalize to environments with more complex market dynamics, such as high-frequency trading or markets with significant latency?
- Basis in paper: [inferred] The paper mentions the financial market is "noisy and complicated" but does not extensively test the method under varying market complexities.
- Why unresolved: The experiments focus on intraday order execution in two real-world markets but do not explore high-frequency scenarios or environments with significant latency.
- What evidence would resolve it: Experiments demonstrating the method's performance in high-frequency trading environments or under varying latency conditions.

### Open Question 2
- Question: What is the impact of varying the number of communication rounds (K) on the method's performance, and is there an optimal number of rounds for different market conditions?
- Basis in paper: [explicit] The paper mentions that "the performances first improve sharply and then remain stable when K â‰¥ 3" but does not explore the impact of different K values across various market conditions.
- Why unresolved: The analysis is limited to a fixed number of communication rounds (K = 3) without exploring the sensitivity of performance to different values of K.
- What evidence would resolve it: Systematic experiments varying K across different market conditions and analyzing the trade-off between communication rounds and performance.

### Open Question 3
- Question: How does the method perform when integrated with portfolio management strategies that dynamically adjust the number and size of orders throughout the trading day?
- Basis in paper: [inferred] The paper focuses on order execution with a fixed set of orders for each trading day but does not explore integration with dynamic portfolio management strategies.
- Why unresolved: The experiments assume a static set of orders, which may not reflect real-world scenarios where portfolio management strategies dynamically adjust orders.
- What evidence would resolve it: Experiments integrating the method with dynamic portfolio management strategies and analyzing performance under varying order sizes and frequencies.

## Limitations
- The effectiveness of the action value attribution method depends critically on the accuracy of the Q-value estimator; without validation of this estimator's performance, the claimed improvements in intended action optimization remain uncertain.
- The communication protocol's scalability to significantly larger numbers of orders (>10) is untested; the linear growth assumption may break down with more complex interactions.
- The "Buying-Winners-and-Selling-Losers" order generation strategy is not fully specified, making it difficult to reproduce or assess the generalizability of results to other order types.

## Confidence

- **High confidence**: The multi-round communication mechanism is clearly described and grounded in the architecture; the communication protocol is implementable as specified.
- **Medium confidence**: The action value attribution method's derivation is sound, but empirical validation of the Q-value estimator's accuracy is lacking.
- **Medium confidence**: The claim that factorizing joint actions enables scalability is theoretically justified but lacks empirical stress-testing beyond the reported experiments.

## Next Checks

1. **Validate Q-value estimator accuracy**: Run ablation studies removing the action value attribution baseline and measure changes in EG and training stability; compare estimated Q-values against Monte Carlo returns on held-out trajectories.

2. **Stress-test communication scalability**: Replicate experiments with 5, 10, and 15 concurrent orders (if feasible with available data) and measure how EG, TOC, and computational cost scale; identify the point where communication overhead outweighs coordination benefits.

3. **Generalize order generation**: Replace the "Buying-Winners-and-Selling-Losers" strategy with a different order generation method (e.g., uniform random orders or market-neutral strategies) and assess whether EG and TOC remain robust across different order distributions.