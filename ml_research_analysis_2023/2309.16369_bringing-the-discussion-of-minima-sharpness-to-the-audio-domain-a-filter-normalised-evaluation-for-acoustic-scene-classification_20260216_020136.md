---
ver: rpa2
title: 'Bringing the Discussion of Minima Sharpness to the Audio Domain: a Filter-Normalised
  Evaluation for Acoustic Scene Classification'
arxiv_id: '2309.16369'
source_url: https://arxiv.org/abs/2309.16369
tags:
- sharpness
- minima
- generalisation
- data
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work explores the relationship between loss landscape sharpness\
  \ and generalisation in acoustic scene classification (ASC), extending the well-studied\
  \ computer vision discussion to the audio domain using the DCASE2020 dataset. The\
  \ authors use filter-normalised 2D visualisations and a derived sharpness measure\
  \ based on the \u03F5-sharpness concept to analyse model minima."
---

# Bringing the Discussion of Minima Sharpness to the Audio Domain: a Filter-Normalised Evaluation for Acoustic Scene Classification

## Quick Facts
- **arXiv ID**: 2309.16369
- **Source URL**: https://arxiv.org/abs/2309.16369
- **Reference count**: 0
- **Primary result**: Sharper minima tend to generalise better in acoustic scene classification, especially for out-of-domain data from unseen devices.

## Executive Summary
This work investigates the relationship between loss landscape sharpness and generalisation in acoustic scene classification (ASC), extending computer vision insights to the audio domain. Using the DCASE2020 dataset and filter-normalised 2D visualisations, the authors analyse model minima and derive a sharpness measure. Contrary to typical findings in image classification, their experiments show that sharper minima actually generalise better, particularly for out-of-domain data recorded from unseen devices. The choice of optimiser emerges as the primary driver of sharpness, while hyperparameters like learning rate and batch size have more aligned effects on both sharpness and accuracy. This domain-specific finding challenges conventional wisdom and highlights the need for further investigation into audio-specific factors affecting sharpness-generalisation relationships.

## Method Summary
The study uses the DCASE2020 ASC dataset, training CNN10 and CNN14 PANNs with various optimisers (SGD, Adam, KFAC, GDTUO) and grid-searched hyperparameters including learning rates (1e-3, 1e-4, 1e-5) and batch sizes (16, 32). Models are trained for 50 epochs, and filter-normalised 2D visualisations of loss landscapes are generated to compute a custom sharpness measure using radius 0.25. The analysis focuses on correlating sharpness values with both in-domain (ID) and out-of-domain (OOD) test accuracy, where OOD represents data from previously unseen recording devices. Filter-normalisation addresses scale bias between different model layers, enabling fair comparison of sharpness across architectures and training conditions.

## Key Results
- Sharper minima generalise better than flatter minima in ASC tasks, contrary to typical CV findings
- This effect is particularly pronounced for out-of-domain data from unseen recording devices
- Optimiser choice is the main driver of sharpness, while learning rate and batch size effects align with both sharpness and accuracy
- Filter-normalised 2D visualisations enable reliable sharpness comparisons across different architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filter-normalised 2D loss landscape visualisations enable more reliable sharpness measurements by adjusting perturbations relative to filter magnitudes
- Mechanism: The normalisation removes scale bias between different model layers, allowing fair comparison of sharpness across architectures and training conditions
- Core assumption: Filter-normalisation produces stable sharpness values when measuring with different random directions
- Evidence anchors:
  - [abstract]: "Our analysis is based on two-dimensional filter-normalised visualisations and a derived sharpness measure."
  - [section]: "In order to overcome this limitation, they suggest to use filter-normalisation for the visualisation of loss landscapes and argue that flatter minima in low-dimensional visualisations with filter-normalised directions go hand-in-hand with better generalisation capabilities."
  - [corpus]: Found related work on filter-normalised sharpness but no direct evidence of stability across random directions
- Break condition: If different random directions produce significantly different sharpness values (as noted in the paper with some high standard deviation cases)

### Mechanism 2
- Claim: Sharper minima can generalise better in acoustic scene classification tasks, especially for out-of-domain data
- Mechanism: Sharper minima may capture device-specific features more precisely, leading to better performance when encountering unseen recording devices in OOD scenarios
- Core assumption: The relationship between sharpness and generalisation is domain-dependent and differs from computer vision findings
- Evidence anchors:
  - [abstract]: "Our exploratory analysis shows that sharper minima tend to show better generalisation than flat minima -even more so for out-of-domain data, recorded from previously unseen devices-"
  - [section]: "We found that for our trained models, sharper minima generalised better to unseen (in particular to OOD) data, which has rarely been observed in the computer vision domain."
  - [corpus]: Weak evidence in corpus; related papers discuss flatness-generalisation but not domain-specific sharpness effects
- Break condition: If subsequent experiments show the opposite trend (flatter minima generalising better) or if the effect disappears when controlling for other hyperparameters

### Mechanism 3
- Claim: The choice of optimiser is the primary driver of sharpness, while hyperparameters like learning rate and batch size have more aligned effects on both sharpness and accuracy
- Mechanism: Different optimisation algorithms inherently produce minima with distinct sharpness characteristics due to their update dynamics and convergence properties
- Core assumption: Optimiser choice creates more variance in sharpness than other training parameters
- Evidence anchors:
  - [abstract]: "We further find that, in particular, the choice of optimisers is a main driver of the sharpness of minima"
  - [section]: "Figure 4 suggests that both sharpness and accuracy are similarly affected by the training parameters. Certain hyperparameters lead to a higher value in both subplots compared to the other hyperparameters in the group, except for the batch size."
  - [corpus]: No direct corpus evidence supporting optimiser-driven sharpness differences
- Break condition: If experiments show learning rate or batch size variations create larger sharpness differences than optimiser choice

## Foundational Learning

- Concept: Loss landscape geometry and curvature
  - Why needed here: Understanding sharpness requires grasping how loss surfaces behave around minima and how curvature relates to generalisation
  - Quick check question: How does the Hessian matrix relate to the concept of sharpness in loss landscapes?

- Concept: Filter-normalisation technique
  - Why needed here: The method's effectiveness depends on understanding how normalising perturbations by filter magnitudes removes scale bias
  - Quick check question: Why might unnormalised perturbations lead to misleading sharpness comparisons between different model architectures?

- Concept: Out-of-domain generalisation in audio tasks
  - Why needed here: The study's key finding about sharper minima generalising better specifically applies to OOD scenarios with unseen recording devices
  - Quick check question: What makes acoustic scene classification particularly susceptible to OOD generalisation challenges compared to image classification?

## Architecture Onboarding

- Component map: Data pipeline -> Model training -> Filter-normalised visualisation -> Sharpness calculation -> Correlation analysis
- Critical path: Train model → Generate filter-normalised loss landscape → Calculate sharpness → Correlate with test accuracy
- Design tradeoffs:
  - Filter-normalisation adds computational overhead but enables fair cross-architecture comparison
  - 2D visualisations provide interpretability but may miss important high-dimensional features
  - Sharpness measurement requires multiple random directions for stability, increasing computation time
- Failure signatures:
  - High variance in sharpness measurements across random directions
  - No correlation between sharpness and accuracy (suggests domain-specific effects not captured)
  - Sharpness values dominated by single hyperparameter choice (indicates measurement instability)
- First 3 experiments:
  1. Reproduce filter-normalised loss landscape visualisation for a trained CNN10 model
  2. Calculate sharpness using three different random directions and verify stability
  3. Train identical architectures with different optimisers and compare resulting sharpness values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the positive correlation between sharpness and generalization in acoustic scene classification (ASC) a general property of audio tasks, or specific to the DCASE datasets and CNN architectures studied?
- Basis in paper: [explicit] The paper reports that sharper minima tend to generalize better in ASC experiments, which contrasts with common findings in image classification. The authors suggest this might indicate a general disparity but call for further investigation.
- Why unresolved: The experiments were limited to the DCASE2020 dataset and a few CNN architectures. It's unclear if the same pattern holds for other audio datasets, tasks, or model types.
- What evidence would resolve it: Testing sharpness-generalization relationships across diverse audio tasks (e.g., speech recognition, music classification) and architectures (e.g., transformers, RNNs) would clarify whether the effect is domain-specific or more general.

### Open Question 2
- Question: How do different optimizers affect sharpness-generalization trade-offs in ASC, and can we predict which optimizer-sharpness combinations yield better OOD performance?
- Basis in paper: [explicit] The paper found that optimizer choice has a large impact on sharpness but not always on accuracy. This mismatch suggests that sharpness alone may not fully explain generalization differences.
- Why unresolved: The paper observed the effect but did not systematically analyze the interaction between optimizers, sharpness, and OOD performance, nor did it propose a predictive framework.
- What evidence would resolve it: A detailed ablation study comparing optimizers across sharpness levels and OOD settings, coupled with a theoretical or empirical model linking optimizer dynamics to sharpness and generalization, would help.

### Open Question 3
- Question: Why do flatter minima in ASC appear to overfit to the dominant training device (Device A), leading to worse OOD performance, and how can this be mitigated?
- Basis in paper: [explicit] The authors hypothesize that flatter minima are more over-optimized for Device A, which dominates the training set, reducing their ability to generalize to unseen devices.
- Why unresolved: The hypothesis is based on observational correlation; the underlying mechanisms by which flatness interacts with device-specific biases are not explored.
- What evidence would resolve it: Experiments isolating device-specific contributions to flatness (e.g., device-balanced training, targeted regularization) and analyzing their effect on OOD generalization would clarify the causal relationship.

## Limitations
- Limited to two specific CNN architectures (CNN10 and CNN14 PANNs)
- High variance in sharpness measurements across random directions
- Weak corpus evidence supporting domain-specific sharpness-generalisation relationships

## Confidence

**High confidence**: Filter-normalisation technique implementation and its role in enabling fair sharpness comparisons across architectures

**Medium confidence**: The observed inverse relationship between sharpness and generalisation in ASC tasks, particularly for OOD data

**Low confidence**: The domain-specific nature of sharpness-generalisation relationships and the claim that this differs fundamentally from CV findings

## Next Checks

1. **Stability verification**: Compute sharpness using 10+ random directions for the same model to quantify measurement variance and establish reliability thresholds
2. **Architecture generalisation**: Test the sharpness-generalisation relationship on a third ASC architecture (e.g., ResNet-based) to validate domain-specific findings beyond PANNs
3. **Controlled ablation**: Systematically vary only learning rate while holding other hyperparameters constant to isolate the individual contribution of each training parameter to sharpness development