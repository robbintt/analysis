---
ver: rpa2
title: Theory of Mind for Multi-Agent Collaboration via Large Language Models
arxiv_id: '2310.10701'
source_url: https://arxiv.org/abs/2310.10701
tags:
- room
- agents
- bomb
- task
- team
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates LLM-based agents in multi-agent cooperative
  tasks, comparing their performance with MARL and planning-based baselines. LLM-based
  agents demonstrated emergent collaborative behaviors and high-order Theory of Mind
  capabilities, though systematic failures in managing long-horizon contexts and hallucination
  about task state limited their efficiency.
---

# Theory of Mind for Multi-Agent Collaboration via Large Language Models

## Quick Facts
- arXiv ID: 2310.10701
- Source URL: https://arxiv.org/abs/2310.10701
- Reference count: 13
- Key outcome: GPT-4+Belief teams achieved full scores and completed tasks in an average of 12.3 rounds, outperforming other LLM-based agents and approaching the efficiency of optimal planning baselines.

## Executive Summary
This study evaluates LLM-based agents in multi-agent cooperative tasks, comparing their performance with MARL and planning-based baselines. The research demonstrates that LLM-based agents can exhibit emergent collaborative behaviors and high-order Theory of Mind capabilities in a text-based game environment. The introduction of explicit belief state representations significantly improves both task performance and ToM inference accuracy, with GPT-4+Belief teams achieving optimal performance. However, systematic failures in managing long-horizon contexts and hallucination about task state remain limitations that affect efficiency.

## Method Summary
The study implements a text-based cooperative game environment where three agents (Alpha, Bravo, Charlie) must collaborate to defuse bombs with different phase sequences across five rooms. LLM-based agents (GPT-4, ChatGPT) are prompted to play the game with and without explicit belief state representations, while their performance is compared against MARL (MAPPO) and planning (CBS Planner) baselines. The evaluation measures team score, rounds to completion, valid action percentage, and Theory of Mind inference accuracy. The belief state representation is integrated into the prompting system to help agents maintain and update their beliefs about the environment and other agents' knowledge states.

## Key Results
- GPT-4+Belief teams achieved full scores and completed tasks in an average of 12.3 rounds
- LLM-based agents demonstrated emergent collaborative behaviors and high-order Theory of Mind capabilities
- Explicit belief state representations improved task performance and ToM inference accuracy for LLM-based agents

## Why This Works (Mechanism)
LLM-based agents succeed in multi-agent collaboration by leveraging their natural language understanding capabilities to interpret game observations, maintain mental models of other agents' beliefs, and generate appropriate communication and actions. The explicit belief state representation provides a structured framework that helps overcome LLM limitations in managing long-horizon contexts and reduces hallucination about task state. This structured approach enables more accurate Theory of Mind reasoning by giving agents a concrete way to represent and update their beliefs about what other agents know, reducing the cognitive load on the LLM's reasoning capabilities.

## Foundational Learning

1. **Theory of Mind in Multi-Agent Systems** (why needed: understanding agent mental modeling; quick check: agents can infer other agents' beliefs)
2. **Belief State Representation** (why needed: provides structured framework for belief management; quick check: agents maintain consistent beliefs over time)
3. **Emergent Collaborative Behaviors** (why needed: LLM agents develop coordination strategies without explicit training; quick check: teams achieve higher scores than individuals)
4. **Prompt Engineering for Multi-Agent Tasks** (why needed: guides LLM behavior in structured ways; quick check: agents follow task rules and communication protocols)
5. **Natural Language Game Interfaces** (why needed: enables rich agent interactions; quick check: agents can communicate effectively about game state)
6. **Baseline Comparison Methods** (why needed: contextualizes LLM performance; quick check: performance metrics compared across methods)

## Architecture Onboarding

**Component Map:** Game Environment -> Observation Translator -> LLM Agent (with Belief State) -> Action Generator -> Communication Interface -> Belief Updater

**Critical Path:** Game observation → belief state update → action selection → communication → belief state propagation → next observation

**Design Tradeoffs:** Structured belief representations improve accuracy but add complexity; natural language interfaces enable rich communication but increase computational overhead; baseline comparisons provide context but may not reflect real-world applicability.

**Failure Signatures:** Invalid actions violating task rules, hallucination of task state, propagation of false beliefs through communication, inefficient long-horizon planning.

**First Experiments:**
1. Test belief state updates with synthetic observation sequences to verify correct belief maintenance
2. Evaluate single-agent performance with and without belief state to isolate its impact
3. Compare communication patterns between LLM agents and baseline methods in simple coordination tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the explicit belief state representation impact the performance of LLM-based agents in more complex multi-agent environments with larger state spaces?
- Basis in paper: [inferred]
- Why unresolved: The paper demonstrates effectiveness in a relatively simple environment with 5 rooms and 5 bombs. It is unclear whether this approach would scale to more complex environments with larger state spaces, more agents, or more intricate dynamics.
- What evidence would resolve it: Conducting experiments with larger and more complex environments, varying the number of rooms, bombs, and agents, and evaluating the performance of LLM-based agents with and without explicit belief state representation.

### Open Question 2
- Question: How do LLM-based agents handle false beliefs and misinformation in multi-agent collaboration tasks?
- Basis in paper: [explicit]
- Why unresolved: The paper mentions that LLM-based agents can propagate false beliefs through communication, leading to widespread misinformation. However, it does not provide a detailed analysis of how agents handle such situations or the strategies they employ to correct false beliefs.
- What evidence would resolve it: Investigating the agents' behavior in scenarios where false beliefs are introduced, either intentionally or due to errors, and analyzing their strategies for detecting, correcting, and communicating accurate information.

### Open Question 3
- Question: How does the performance of LLM-based agents in multi-agent collaboration tasks compare to human teams?
- Basis in paper: [inferred]
- Why unresolved: The paper demonstrates that LLM-based agents can perform comparably to state-of-the-art MARL algorithms in a specific task environment. However, it is unclear how their performance would compare to human teams in terms of efficiency, adaptability, and overall task completion.
- What evidence would resolve it: Conducting experiments where human teams and LLM-based agent teams perform the same multi-agent collaboration tasks, and comparing their performance metrics, such as task completion time, efficiency, and accuracy.

## Limitations
- Systematic failures in managing long-horizon contexts limit LLM agent efficiency
- Hallucination about task state leads to valid but infeasible actions based on false beliefs
- Limited characterization of which belief state representation components are most critical for success

## Confidence

**High confidence:** LLM agents demonstrate emergent collaborative behaviors and ToM capabilities; explicit belief state representations improve performance

**Medium confidence:** GPT-4+Belief achieves near-optimal efficiency compared to planning baselines; systematic failure modes are identified

**Low confidence:** Exact mechanisms by which belief state representations improve performance; generalizability of findings to other cooperative tasks

## Next Checks

1. Systematically ablate components of the belief state representation to identify which elements are most critical for performance improvements
2. Conduct deeper analysis of LLM failure modes by tracking specific instances of long-horizon context management failures and hallucination events
3. Implement and evaluate additional baseline methods, including varying the MARL training parameters and testing alternative planning algorithms