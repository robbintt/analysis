---
ver: rpa2
title: Interpreting User Requests in the Context of Natural Language Standing Instructions
arxiv_id: '2311.09796'
source_url: https://arxiv.org/abs/2311.09796
tags:
- instructions
- user
- standing
- examples
- calls
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new language-to-program dataset called NLSI,
  which aims to incorporate user-specific standing instructions (preferences) as additional
  context for interpreting natural language requests. The key challenge is to identify
  which subset of standing instructions is relevant to a given dialogue and use them
  to generate structured API calls.
---

# Interpreting User Requests in the Context of Natural Language Standing Instructions

## Quick Facts
- arXiv ID: 2311.09796
- Source URL: https://arxiv.org/abs/2311.09796
- Reference count: 13
- The paper introduces the NLSI dataset and explores methods for interpreting user requests with standing instructions as context.

## Executive Summary
This paper introduces the NLSI dataset, which contains 2,441 dialogues across 17 domains designed to test the integration of user-specific standing instructions with natural language understanding. The key challenge addressed is identifying which subset of standing instructions is relevant to a given dialogue and using them to generate structured API calls. The authors experiment with different methods using large language models (LLMs) for selection and interpretation tasks, finding that decoupled selection and interpretation approaches outperform joint generation methods. The best-performing method achieves 44.7% exact match on API prediction, highlighting the challenges in effectively incorporating standing instructions for structured prediction tasks.

## Method Summary
The authors introduce the NLSI dataset and propose methods for interpreting user utterances into API calls using user-specific standing instructions as additional context. The approach involves two main components: selection of relevant standing instructions from a user profile and interpretation of these instructions along with the dialogue context to generate structured API calls. Six reasoning types are defined (PLAIN, MULTIHOP, MULTI DOMAIN, MULTIPREFERENCE, CONFLICT, NONE APPLICABLE) to capture various complexities. The authors experiment with LLM-based methods using in-context learning, BM25, and Contriever for selection and interpretation, comparing DIRECT interpretation, SELECT-AND-INTERPRET, and SELECT-THEN-INTERPRET approaches.

## Key Results
- The best-performing method achieves 44.7% exact match on API prediction, demonstrating the difficulty of incorporating standing instructions effectively
- LLM-based selection methods outperform traditional lexical and embedding similarity approaches
- Multi-step selection improves performance for multi-hop and multi-domain reasoning types
- Direct interpretation of context with instructions gives better results than joint generation of selection and interpretation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM performance improves when relevant standing instructions are explicitly selected before interpretation.
- Mechanism: Selection models identify user profile entries most similar to the dialogue context, reducing irrelevant context in the prompt and improving API call accuracy.
- Core assumption: There is a measurable semantic similarity between user utterances and relevant standing instructions that can be captured by retrieval methods.
- Evidence anchors:
  - [abstract] "A key challenge in NLSI is to identify which subset of the standing instructions is applicable to a given dialogue."
  - [section 4.3] "We find that LLM-based selection methods surpass traditional methods based on lexical statistics and embedding similarity."
  - [corpus] Weak - related papers focus on retrieval but not on instruction selection for structured prediction.

### Mechanism 2
- Claim: Prompting LLMs to generate both instruction selection and API calls in a single step leads to lower performance than decoupled selection and interpretation.
- Mechanism: Joint generation overloads the model's context window, causing hallucination and skipped arguments.
- Core assumption: LLMs have limited capacity to reason about multiple interdependent tasks in a single prompt.
- Evidence anchors:
  - [section 5.4] "DIRECT interpretation gives the best result, closely followed by the SELECT -AND-INTERPRET"
  - [section 5.4] "generating selection step and generating API call within the same prompt may not be suitable"
  - [corpus] Weak - related work focuses on prompt engineering but not specifically on instruction selection for structured outputs.

### Mechanism 3
- Claim: Multi-step selection improves performance for multi-hop and multi-domain reasoning types.
- Mechanism: A second pass over selected instructions allows the model to identify dependencies between instructions that were missed in the first pass.
- Core assumption: Complex reasoning types require iterative refinement of the relevant instruction set.
- Evidence anchors:
  - [section 5.3] "MULTI-PASS setup has an overall exact match lower than ICL. However, the improvement in MULTI-HOP, MULTIPREFERENCE, and MULTIDOMAIN reasoning type types over the ICL setup"
  - [section 4.3] "we observed that the LLMs consistently missed a subset of relevant instructions in the MULTI HOP and MULTI DOMAIN reasoning types"
  - [corpus] Weak - related papers do not discuss iterative selection for structured prediction tasks.

## Foundational Learning

- Concept: Semantic parsing into structured API calls
  - Why needed here: The core task is converting natural language requests into executable API calls with correct arguments.
  - Quick check question: Given "I want to watch a movie in Santa Rosa," what API call structure would you generate?

- Concept: Information retrieval and semantic similarity
  - Why needed here: Selection of relevant standing instructions relies on retrieving the most semantically similar instructions from the user profile.
  - Quick check question: How would you measure similarity between "I prefer Italian food" and "Find me a restaurant"?

- Concept: Chain-of-thought reasoning and multi-hop inference
  - Why needed here: Some standing instructions require reasoning across multiple steps (e.g., if airline is American Airlines, then seating class is Economy).
  - Quick check question: If a user says "I'm flying American Airlines," which standing instructions would you need to apply?

## Architecture Onboarding

- Component map: User Profile -> Selection Module -> Interpretation Module -> API call generation
- Critical path: Dialogue context → Selection → Interpretation → API call generation
- Design tradeoffs:
  - Selection accuracy vs. computational cost (BM25 is fast but less accurate than ICL)
  - Prompt length vs. performance (longer prompts may improve accuracy but hit model limits)
  - Single-step vs. multi-step selection (simpler but less accurate for complex reasoning)
- Failure signatures:
  - Hallucinated arguments in API calls
  - Missing API calls for secondary domains
  - Skipping conditional arguments based on other attributes
  - Over-generation of unrelated API calls
- First 3 experiments:
  1. Implement BM25-based selection and ICL-based interpretation to establish baseline performance
  2. Add multi-step selection (MULTI-PASS) to improve multi-hop reasoning accuracy
  3. Experiment with different demonstration counts in ICL to optimize in-context learning effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the size of user profiles be effectively managed when the number of standing instructions exceeds the prompt's capacity?
- Basis in paper: [explicit] "As the user profile size increases, and the instructions no longer fit into the prompt, a separate selection step can be convenient."
- Why unresolved: The paper acknowledges the potential issue of user profile size exceeding the prompt's capacity but does not provide a concrete solution for managing large user profiles.
- What evidence would resolve it: Developing and testing methods for efficiently handling large user profiles, such as hierarchical or distributed representations, would provide evidence for effective management of user profile size.

### Open Question 2
- Question: What is the impact of different instruction selection methods on the performance of the interpretation task across various reasoning types?
- Basis in paper: [explicit] "We compared and contrasted several baseline methods on the NLSI dataset."
- Why unresolved: While the paper presents results for different selection methods, it does not provide a comprehensive analysis of the impact of these methods on the interpretation task across various reasoning types.
- What evidence would resolve it: Conducting a detailed analysis of the performance of different instruction selection methods on the interpretation task across various reasoning types would provide insights into their impact.

### Open Question 3
- Question: How can the selection of relevant standing instructions be improved to enhance the overall performance of the interpretation task?
- Basis in paper: [explicit] "The generation of the correct API call requires understanding of the user's context, the schema, the set of relevant standing instructions, as well as the dependence between standing instructions."
- Why unresolved: The paper identifies the challenges in selecting relevant standing instructions but does not provide a clear solution for improving the selection process.
- What evidence would resolve it: Developing and testing advanced instruction selection methods, such as attention mechanisms or graph-based approaches, would provide evidence for improving the selection of relevant standing instructions.

## Limitations

- Dataset size may be insufficient to draw robust conclusions across all 17 domains and six reasoning types
- Exact match metrics may not capture partial successes or practical utility of near-correct API calls
- Comparison limited to a few baseline methods without exploring other retrieval or semantic parsing techniques

## Confidence

- **High Confidence**: The observation that instruction selection is a key challenge, supported by clear performance gaps between different selection methods and the consistent failure of direct interpretation approaches.
- **Medium Confidence**: The claim that multi-step selection improves performance for complex reasoning types, based on limited empirical evidence showing improvements in specific reasoning types but not overall.
- **Low Confidence**: The assertion that joint generation of selection and interpretation is inherently unsuitable, as this conclusion is drawn from a single comparison and may depend on specific prompt engineering choices.

## Next Checks

1. **Dataset Expansion Validation**: Test the selection and interpretation methods on a larger, more diverse dataset to verify if performance improvements scale with data volume and domain coverage.
2. **Alternative Baseline Comparison**: Implement and evaluate additional selection methods (e.g., dense passage retrieval, learned sparse retrieval) to establish stronger baselines and validate the relative performance of current approaches.
3. **Robustness Testing**: Conduct ablation studies on prompt templates, demonstration counts, and model parameters to identify the key factors driving performance differences and validate the robustness of the claimed mechanisms.