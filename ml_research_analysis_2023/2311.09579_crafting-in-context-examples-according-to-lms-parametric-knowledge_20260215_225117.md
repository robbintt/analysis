---
ver: rpa2
title: Crafting In-context Examples according to LMs' Parametric Knowledge
arxiv_id: '2311.09579'
source_url: https://arxiv.org/abs/2311.09579
tags:
- in-context
- answer
- examples
- answers
- ordering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how a language model's parametric knowledge
  of in-context examples impacts their effectiveness for knowledge-rich tasks like
  question answering. The authors find that using in-context examples that contain
  both known and unknown information leads to better performance compared to solely
  known or unknown examples.
---

# Crafting In-context Examples according to LMs' Parametric Knowledge

## Quick Facts
- arXiv ID: 2311.09579
- Source URL: https://arxiv.org/abs/2311.09579
- Authors: 
- Reference count: 19
- Primary result: Knowledge-aware in-context example construction improves performance on multi-answer QA tasks

## Executive Summary
This paper investigates how a language model's parametric knowledge of in-context examples impacts their effectiveness for knowledge-rich tasks like question answering. The authors find that using in-context examples containing both known and unknown information leads to better performance compared to solely known or unknown examples. They also explore strategies for ordering multi-answer sets based on the model's knowledge, finding that ordering answers in descending order of model knowledge often improves performance. The study sheds light on best practices for constructing in-context example sets for knowledge-intensive tasks.

## Method Summary
The study uses Llama-2 (13B) and OPT (13B) language models with in-context examples retrieved based on semantic similarity using SimCSE embeddings. The method involves constructing in-context example sets with varying levels of model knowledge (unknown, random, half-known, known) and ordering answers within examples based on knowledge assessment (using perplexity). Performance is evaluated on three multi-answer QA datasets (AmbigQA, QAMPARI, QUEST) and one math QA dataset (GSM8K) using exact match, F1-score, and F1EM metrics.

## Key Results
- Mixed known/unknown in-context examples consistently outperform purely known or unknown sets across diverse settings
- Ordering answers in descending order of model knowledge (from most to least known) generally improves performance
- Models learn to follow demonstrated answer ordering patterns, generating more answers when presented with consistent ordering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Presenting in-context examples with mixed known and unknown information leads to better model performance than presenting only known or only unknown examples.
- Mechanism: The model learns to leverage its parametric knowledge while also developing strategies for handling uncertainty when encountering unknown information.
- Core assumption: The model's performance improves when exposed to a mix of familiar and unfamiliar in-context examples because this encourages both knowledge retrieval and adaptive reasoning.
- Evidence anchors:
  - [abstract] "Constructing an in-context example set that presents both known and unknown information performs the best across diverse settings."
  - [section 3.2] "Throughout all datasets, however, providing in-context examples that have a mixture of known and unknown information leads to superior performance compared to solely known or unknown in-context examples."
  - [corpus] "Average neighbor FMR=0.35" (suggests moderate similarity between retrieved examples and queries)
- Break condition: If the model's parametric knowledge is insufficient for the known examples, or if the unknown examples are too far from the model's knowledge distribution, performance may degrade.

### Mechanism 2
- Claim: Ordering answers in descending order of model knowledge improves performance compared to random ordering.
- Mechanism: When answers are ordered from most to least known, the model is more likely to generate the correct answers first, establishing a knowledge-based pattern that guides subsequent predictions.
- Core assumption: The model's generation process is influenced by the ordering of answers in the in-context examples, and it tends to follow the pattern of presenting more confident answers first.
- Evidence anchors:
  - [section 4.1] "We compute the length normalized perplexity of each answer... Then, we sort the answers in ascending order of these perplexities, resulting in 'known' answers placed earlier."
  - [section 5.3] "Overall, we see that answer ordering does not bring large impact in final end task performance, but notice consistent patterns... Overall, presenting more confident answers first (GREEDY and PERPLEXITY) yielded better results than their REVERSE counterparts."
  - [corpus] "Max neighbor FMR=0.6383" (indicates some examples are highly relevant to queries)
- Break condition: If the model's knowledge assessment is inaccurate (e.g., perplexity doesn't reflect true knowledge), or if the task requires generating all answers regardless of confidence, this ordering may not help.

### Mechanism 3
- Claim: The model learns to follow the answer ordering strategy shown in in-context examples, generating more answers when presented with consistent ordering patterns.
- Mechanism: The model internalizes the ordering pattern from in-context examples and applies it to its own generation, producing more answers when the ordering is semantically meaningful.
- Core assumption: The model treats the in-context examples as demonstrations of desired behavior, including the order in which answers should be generated.
- Evidence anchors:
  - [section 5.1] "We find that in every cell, the first number is higher than the second number, suggesting that the model follows to the answer ordering pattern presented in the in-context examples."
  - [section 5.2] "We find that generation order impacts the number of generated answer, with ALPHABET ordering substantially increasing the number of generated answers the most."
  - [corpus] "Average citations=0.0" (suggests this is novel research with limited prior work to reference)
- Break condition: If the in-context examples are too dissimilar from the evaluation examples, or if the model's capacity to learn ordering patterns is exceeded, it may not follow the demonstrated ordering.

## Foundational Learning

- Concept: Parametric Knowledge
  - Why needed here: The study is fundamentally about how a model's internal knowledge affects its performance with in-context learning, so understanding what parametric knowledge is and how it's measured is crucial.
  - Quick check question: What is the difference between parametric knowledge and knowledge retrieved from external sources in the context of language models?

- Concept: Perplexity as Knowledge Proxy
  - Why needed here: The paper uses perplexity to operationalize how well the model "knows" an answer, so understanding this relationship is essential for interpreting the results.
  - Quick check question: How does lower perplexity for a given answer indicate that the model has more knowledge about that answer?

- Concept: In-Context Learning
  - Why needed here: The entire study is about optimizing in-context examples, so a solid understanding of how in-context learning works is necessary.
  - Quick check question: How does providing in-context examples help a language model perform a task it hasn't been explicitly trained on?

## Architecture Onboarding

- Component map: Language model (Llama-2 13B or OPT 13B) -> Retriever (SimCSE embeddings) -> In-context example constructor -> Answer ordering module -> Generator -> Evaluator
- Critical path: Retrieve similar in-context examples → Construct prompt with examples ordered by similarity → Order answers within examples based on model knowledge → Generate answers → Evaluate using token match metrics
- Design tradeoffs: Using more similar in-context examples improves performance but requires more computation; ordering answers by knowledge may help but could be misleading if knowledge assessment is inaccurate; mixing known and unknown examples helps but requires careful calibration
- Failure signatures: Performance doesn't improve with knowledge-aware ordering (suggests knowledge assessment is inaccurate); random ordering performs as well as semantic ordering (suggests task doesn't benefit from ordering); mixing known and unknown examples hurts performance (suggests examples are too dissimilar)
- First 3 experiments:
  1. Test the effect of answer ordering on a simple dataset where the model's knowledge is clearly bimodal (very confident or very uncertain)
  2. Compare performance using only known vs. only unknown in-context examples on a dataset where the model has moderate knowledge of most answers
  3. Test the impact of using random vs. similar in-context examples on a dataset with high answer variability

## Open Questions the Paper Calls Out
None explicitly called out in the provided text.

## Limitations
- The study focuses on four datasets (three multi-answer QA and one math QA), which may not capture the full diversity of knowledge-intensive tasks
- Using perplexity to measure model knowledge assumes a strong correlation between low perplexity and actual knowledge, which may not always hold true
- The study evaluates knowledge at a single point in time, without considering how model knowledge might evolve with continued pretraining or fine-tuning

## Confidence

- **High Confidence**: The finding that mixed known/unknown in-context examples outperform purely known or unknown sets is well-supported across multiple datasets and models
- **Medium Confidence**: The ordering strategies show consistent patterns but with smaller effect sizes, with impact appearing to be task-dependent
- **Medium Confidence**: The claim that models learn to follow demonstrated ordering patterns is supported by evidence showing increased answer generation when semantic ordering is used

## Next Checks

1. Test the knowledge-aware in-context example construction on datasets from different domains (e.g., scientific reasoning, commonsense reasoning) to assess generalizability beyond QA tasks
2. Compare perplexity-based knowledge assessment with alternative methods such as calibration scores or human judgment to verify that perplexity accurately reflects model knowledge
3. Evaluate how the effectiveness of knowledge-aware in-context examples changes as models are further pretrained or fine-tuned on additional data