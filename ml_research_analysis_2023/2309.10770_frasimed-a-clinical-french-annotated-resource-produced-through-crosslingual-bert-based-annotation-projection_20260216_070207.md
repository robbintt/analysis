---
ver: rpa2
title: 'FRASIMED: a Clinical French Annotated Resource Produced through Crosslingual
  BERT-Based Annotation Projection'
arxiv_id: '2309.10770'
source_url: https://arxiv.org/abs/2309.10770
tags:
- annotation
- projection
- french
- annotations
- annotated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel crosslingual annotation projection
  approach using language-agnostic BERT embeddings to generate French annotated datasets
  from Spanish medical corpora. The method projects annotations from Spanish to French
  through sentence and word alignment, followed by manual correction.
---

# FRASIMED: a Clinical French Annotated Resource Produced through Crosslingual BERT-Based Annotation Projection

## Quick Facts
- arXiv ID: 2309.10770
- Source URL: https://arxiv.org/abs/2309.10770
- Reference count: 0
- Key outcome: Crosslingual BERT-based annotation projection achieved 96.4% F1-score (relaxed) in projecting Spanish medical annotations to French

## Executive Summary
This paper introduces FRASIMED, a clinical French annotated resource created through a crosslingual annotation projection approach using language-agnostic BERT embeddings. The methodology projects annotations from Spanish medical corpora (CANTEMIST and DISTEMIST) to French through machine translation, sentence alignment, word alignment, and greedy projection strategies. The resulting corpus contains 2,051 clinical cases with annotations linked to ICD-O and SNOMED-CT medical terminologies, achieving 96.4% F1-score (relaxed) compared to manually corrected gold standard.

## Method Summary
The method uses language-agnostic BERT embeddings (LABSE) to align sentences between Spanish and French, then employs multilingual BERT with cosine similarity for bidirectional word alignment. Annotations are projected from Spanish to French using a greedy strategy that handles discontinuous annotations by filling gaps. The pipeline begins with document-level machine translation of Spanish medical corpora, followed by sentence and word alignment, annotation projection, error detection through TSV reports, and manual correction using Brat annotation tool.

## Key Results
- FRASIMED corpus contains 2,051 clinical cases with French annotations linked to ICD-O and SNOMED-CT
- Automatic projection achieved 96.4% F1-score (relaxed) for CANTEMIST and 96.8% for DISTEMIST
- Over 97% precision in automatic projection before manual correction
- Largest open French medical annotated dataset available for NLP applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-agnostic BERT embeddings enable effective crosslingual annotation projection by capturing semantic similarity across languages.
- Mechanism: The LABSE model uses both MLM and TLM objectives to learn representations that are invariant to language, allowing alignment of sentences and words across Spanish and French despite no direct lexical overlap.
- Core assumption: Semantic meaning is preserved sufficiently in the BERT embeddings across languages to enable meaningful alignment.
- Evidence anchors: [abstract]: "Leveraging a language agnostic BERT-based approach"; [section]: "LABSE surpasses multilingual BERT in generating superior crosslingual sentence embeddings, primarily due to TLM's focus on sentence-level objective"; [corpus]: Weak - the paper doesn't explicitly evaluate the quality of LABSE embeddings independent of the projection pipeline

### Mechanism 2
- Claim: Bidirectional word alignment using cosine similarity on multilingual BERT embeddings improves projection recall by handling many-to-many mappings.
- Mechanism: By computing similarity scores in both directions (Spanish→French and French→Spanish), the system can capture cases where one word maps to multiple words or vice versa, reducing missed annotations.
- Core assumption: Word-level semantic similarity in multilingual BERT space is sufficient to establish reliable alignments.
- Evidence anchors: [abstract]: "word alignment is computed by encoding the words using multilingual BERT and measuring the cosine similarity between them"; [section]: "To account for the one-to-many relationships between words, the word alignment is applied bidirectionally"; [corpus]: Weak - no explicit evaluation of word alignment quality independent of the full pipeline

### Mechanism 3
- Claim: Greedy annotation projection with discontinuity handling improves recall by filling gaps between fragmented projections.
- Mechanism: When word alignment produces discontinuous annotations, the system annotates all intervening words to create a cohesive span, favoring recall over precision.
- Core assumption: Missing words between aligned spans are semantically related enough to warrant inclusion.
- Evidence anchors: [section]: "In cases where the resulting annotation projection exhibits discontinuity, a greedy strategy is employed: this approach involves annotating all the words intervening between the fragmented annotations"; [section]: "This approach aims to prevent fragmented annotations while favoring the recall"; [corpus]: Weak - the paper doesn't evaluate how often greedy filling introduces noise

## Foundational Learning

- Concept: Crosslingual annotation projection
  - Why needed here: To create French medical annotations without manually annotating French text from scratch
  - Quick check question: What is the main advantage of using annotation projection over manual annotation for low-resource languages?

- Concept: BERT embeddings and semantic similarity
  - Why needed here: To align words and sentences across languages based on meaning rather than surface forms
  - Quick check question: How do MLM and TLM objectives in LABSE help create language-agnostic embeddings?

- Concept: Entity linking to medical terminologies
  - Why needed here: To connect recognized medical entities to standardized codes (ICD-O, SNOMED-CT) for interoperability
  - Quick check question: Why is it important to link medical entities to standardized terminologies like SNOMED-CT?

## Architecture Onboarding

- Component map: Spanish annotated corpus (CANTEMIST, DISTEMIST) → Document-level MT (DeepL Pro) → Sentence alignment using LABSE embeddings → Word alignment using multilingual BERT + cosine similarity (bidirectional) → Annotation projection with greedy discontinuity handling → Error identification via TSV report → Manual correction using Brat → FRASIMED corpus with French annotations linked to ICD-O and SNOMED-CT

- Critical path: MT → Sentence alignment → Word alignment → Projection → Error detection → Manual correction

- Design tradeoffs:
  - Recall vs. precision: Greedy filling and bidirectional alignment favor recall
  - MT quality dependence: Final annotation quality depends on MT accuracy
  - Manual effort: Semi-automatic pipeline reduces but doesn't eliminate manual work

- Failure signatures:
  - Low precision in automatic projection (many spurious annotations)
  - High number of badly translated annotations due to MT limitations
  - Missing entities that weren't projected correctly

- First 3 experiments:
  1. Evaluate sentence alignment accuracy using a small manually aligned subset
  2. Test word alignment quality on sentence pairs with known alignments
  3. Measure impact of greedy filling by comparing with a conservative projection baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using document-level versus sentence-level machine translation on annotation projection quality, and how does this vary across different language pairs?
- Basis in paper: [explicit] The paper mentions using document-level MT for better context capture but notes this loses sentence alignment information, requiring additional steps for alignment.
- Why unresolved: The paper chose document-level MT but doesn't directly compare it to sentence-level MT in their results or provide evidence of the trade-offs.
- What evidence would resolve it: A controlled experiment comparing annotation projection quality using document-level vs sentence-level MT across multiple language pairs would provide clear evidence.

### Open Question 2
- Question: How does the annotation projection methodology perform when applied to low-resource languages with limited parallel corpora compared to high-resource languages?
- Basis in paper: [inferred] The paper claims the method is applicable to low-resource languages but doesn't test this claim or provide comparative results.
- Why unresolved: The experiments only demonstrate the method on Spanish-French translation, which are both relatively high-resource languages.
- What evidence would resolve it: Applying the methodology to low-resource language pairs and comparing projection quality metrics to those achieved with high-resource pairs would provide evidence.

### Open Question 3
- Question: What is the long-term maintenance cost and quality decay rate for semi-automatically generated annotated datasets that require periodic updates?
- Basis in paper: [inferred] The paper demonstrates successful creation of annotated datasets but doesn't address how annotation quality might change over time or with updates.
- Why unresolved: The paper presents a one-time creation process without discussing ongoing maintenance requirements or quality monitoring.
- What evidence would resolve it: Longitudinal studies tracking annotation quality metrics over time for semi-automatically generated datasets would provide evidence of maintenance needs.

## Limitations

- Evaluation relies entirely on manual correction rather than direct comparison with existing gold standards
- Dependency on DeepL Pro for document-level translation introduces a black box that could significantly impact annotation quality
- The evaluation metrics (relaxed F1-score) are lenient and may overestimate the true precision of the projection approach

## Confidence

- High Confidence: The overall methodology of using crosslingual annotation projection to create French medical corpora from Spanish sources is sound and well-executed
- Medium Confidence: The reported F1-scores of 96.4% and 96.8% are internally validated but lack external benchmarking
- Low Confidence: The specific contribution of the greedy discontinuity handling strategy and bidirectional word alignment to the final performance cannot be isolated without component-level evaluation

## Next Checks

1. **Component Evaluation**: Measure sentence alignment accuracy using a small manually aligned subset (e.g., 100 sentence pairs) to determine if alignment errors are the primary source of projection failures.

2. **Translation Quality Impact**: Compare projection results using different translation methods (DeepL Pro vs. Google Translate vs. no translation) to quantify the impact of MT quality on final annotation accuracy.

3. **Baseline Comparison**: Implement a conservative annotation projection baseline (without greedy filling, with stricter similarity thresholds) and compare F1-scores to assess whether the current approach's performance gains are statistically significant.