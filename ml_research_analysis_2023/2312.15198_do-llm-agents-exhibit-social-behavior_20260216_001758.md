---
ver: rpa2
title: Do LLM Agents Exhibit Social Behavior?
arxiv_id: '2312.15198'
source_url: https://arxiv.org/abs/2312.15198
tags:
- agents
- social
- llms
- reciprocity
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether Large Language Models (LLMs) exhibit
  human-like social behaviors in various contexts. It introduces a novel framework
  called State-Understanding-Value-Action (SUVA) to systematically analyze LLM responses
  based on their textual outputs (utterances) in social interactions.
---

# Do LLM Agents Exhibit Social Behavior?

## Quick Facts
- arXiv ID: 2312.15198
- Source URL: https://arxiv.org/abs/2312.15198
- Reference count: 7
- The paper finds that most LLMs do not generate decisions aligned solely with self-interest but instead reflect social welfare considerations and patterns consistent with direct and indirect reciprocity.

## Executive Summary
This study investigates whether Large Language Models (LLMs) exhibit human-like social behaviors in various contexts. Using a novel framework called State-Understanding-Value-Action (SUVA), the research systematically analyzes LLM responses based on their textual outputs in social interactions. The framework assesses both final decisions and reasoning processes across canonical behavioral economics games and social preference concepts. The analysis of eight LLMs, including GPT-4, LLaMA, and Mistral models, reveals that most models demonstrate social welfare considerations rather than pure self-interest, with higher-capacity models more frequently displaying group identity effects.

## Method Summary
The study introduces the State-Understanding-Value-Action (SUVA) framework to analyze LLM responses in social contexts based on their textual outputs. SUVA assesses LLMs' social behavior through both their final decisions and the reasoning processes leading to those decisions, using canonical behavioral economics games and social preference concepts. The method involves implementing SUVA to parse LLM responses, applying behavioral economics games to assess social behavior through decisions and reasoning, and analyzing results using metrics such as accuracy in social learning games, charity, envy, and reciprocity. The study employs zero-shot learning to preserve natural LLM behavior and uses probabilistic reasoning to account for the inherent uncertainty in LLM outputs.

## Key Results
- Most LLMs do not generate decisions aligned solely with self-interest but reflect social welfare considerations
- Higher-capacity models more frequently display group identity effects
- LLMs exhibit patterns consistent with direct and indirect reciprocity
- SUVA provides explainable tools including tree-based visualizations and probabilistic dependency analysis

## Why This Works (Mechanism)

### Mechanism 1: Social Learning
LLM agents display social learning by integrating their own signals with observed decisions of other agents. The agents observe others' choices, then probabilistically adjust their own predictions based on both their private information and the collective information. This works because the LLM's probabilistic output layer allows it to weigh multiple sources of evidence without explicit instructions to do so. The mechanism is evidenced by models producing responses reflecting social welfare considerations rather than pure self-interest. The mechanism fails if the LLM's probability distribution collapses to self-interest in repeated trials.

### Mechanism 2: Distributional Preferences
LLM agents exhibit distributional preferences that depend on group identity and payoff comparisons. The LLM's response is shaped by internal representations of fairness, charity, and envy, modulated by whether the other party is perceived as in-group or out-group. This mechanism relies on the LLM having been exposed to enough training data with social group dynamics to internalize group identity effects. Evidence shows higher-capacity models display group identity effects, with higher levels of charity towards in-group members receiving lower payoffs and less envy towards them when receiving higher payoffs. The mechanism fails if the LLM's preferences do not shift when group identity is explicitly manipulated.

### Mechanism 3: Indirect Reciprocity
LLM agents exhibit indirect reciprocity through image scoring and upstream reciprocity. The LLM tracks past actions of other agents and uses this "reputation" to inform future decisions, either by rewarding good reputation or by paying it forward. This depends on the LLM's training data including sufficient examples of reputation-based interactions to form internal representations of indirect reciprocity. The mechanism is supported by evidence of upstream indirect reciprocity with magnitude similar to direct reciprocity, primarily driven by fairness concerns. The mechanism fails if the LLM fails to adjust behavior based on past actions of unseen agents.

## Foundational Learning

- **Zero-shot learning**: Ensures the LLM's responses are based on its pre-trained knowledge rather than guided examples. *Quick check*: Does the prompt include any explicit examples of desired behavior?
- **Probabilistic reasoning**: LLM outputs are generated from probability distributions over possible continuations. *Quick check*: How does the temperature setting affect the randomness of the LLM's responses?
- **Step-by-step reasoning**: Forces the LLM to articulate its reasoning process, making it analyzable. *Quick check*: Does the LLM's reasoning include logical steps or just a summary of its decision?

## Architecture Onboarding

- **Component map**: Input (Game instructions + step-by-step reasoning prompt) → LLM (GPT-4 model generating responses) → Output (Structured response with reasoning and decision) → Analysis (Template parsing + statistical modeling of preferences)
- **Critical path**: Input → LLM → Output → Analysis → Interpretation
- **Design tradeoffs**: Zero-shot vs. few-shot (Zero-shot preserves natural behavior but may be less controlled); Temperature=0 vs. >0 (Zero gives reproducibility but may reduce diversity)
- **Failure signatures**: Non-responsive LLM (Check prompt formatting and API connection); Inconsistent decisions (Check temperature and randomness); Misaligned analysis (Verify template parsing and statistical models)
- **First 3 experiments**: 1) Dictator game with group identity manipulation, 2) Social learning game with varying network costs, 3) Upstream indirect reciprocity game with payoff amplification

## Open Questions the Paper Calls Out

### Open Question 1
How do the social behaviors of LLM agents change as models are updated or new versions are released? The study primarily focuses on GPT-4 and does not explore how social behaviors might change with different versions or models of LLMs. Comparative studies using different versions or models of LLMs would resolve this question.

### Open Question 2
How do the social behaviors of LLM agents differ across languages? The study does not investigate heterogeneity of results across languages or use agents from diverse linguistic backgrounds. Studies using LLM agents from different linguistic backgrounds would assess variations in social behaviors across languages.

### Open Question 3
How robust are the conclusions about LLM social behaviors when transitioning from laboratory experiments to more realistic field settings? The study primarily centers on laboratory experiments and does not explore application of these findings in real-world scenarios. Studies applying the experimental framework in field settings would compare results with those from laboratory experiments.

## Limitations
- Findings are based on zero-shot interactions which may not capture extended social contexts
- SUVA framework relies on parsing natural language outputs which could miss nuanced reasoning
- Analysis focuses on decision outcomes and reasoning traces but does not account for potential hidden states or intermediate representations within the LLM

## Confidence

- **High confidence**: LLMs generally do not exhibit purely self-interested behavior and show evidence of social welfare considerations
- **Medium confidence**: Group identity effects are more prominent in higher-capacity models, though the strength and consistency require further validation
- **Low confidence**: The specific mechanisms of indirect reciprocity are inferred from decision patterns rather than directly observed

## Next Checks
1. Replicate the analysis using few-shot learning to determine if explicit examples of desired behavior significantly alter LLM social preferences
2. Conduct extended interaction sequences to test whether LLMs maintain consistent social preferences over time or adapt based on repeated interactions
3. Compare LLM responses to human behavioral data from the same games to quantify the degree of alignment between artificial and natural social reasoning