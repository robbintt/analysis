---
ver: rpa2
title: An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in
  RCC-8
arxiv_id: '2309.15577'
source_url: https://arxiv.org/abs/2309.15577
tags:
- reasoning
- relation
- spatial
- relations
- ntpp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates ChatGPT-4's ability to perform qualitative
  spatial reasoning using the RCC-8 calculus. It tests compositional reasoning, preferred
  compositions, and spatial continuity reasoning.
---

# An Evaluation of ChatGPT-4's Qualitative Spatial Reasoning Capabilities in RCC-8

## Quick Facts
- arXiv ID: 2309.15577
- Source URL: https://arxiv.org/abs/2309.15577
- Reference count: 8
- ChatGPT-4 correctly predicts around 70% of composition relations and 89% of continuity relations, but struggles with preferred compositions at only 50% agreement with human preferences.

## Executive Summary
This paper evaluates ChatGPT-4's ability to perform qualitative spatial reasoning using the RCC-8 calculus, testing compositional reasoning, preferred compositions, and spatial continuity reasoning. The results show that while ChatGPT-4 demonstrates some spatial reasoning capabilities with ~70% accuracy on composition relations and ~89% on continuity relations, it falls significantly short of perfect logical reasoning. The model struggles particularly with preferred compositions, agreeing with human preferences in only about 50% of cases, and frequently makes elementary reasoning errors such as confusing relations with their inverses.

## Method Summary
The study employs an interactive dialogue approach with ChatGPT-4 using carefully constructed prompts for three distinct experiments. The first experiment tests compositional reasoning by asking the model to predict RCC-8 composition table entries. The second experiment evaluates preferred compositions by comparing ChatGPT-4's choices to human cognitive preferences for simpler spatial models. The third experiment tests spatial continuity reasoning by asking the model to predict immediate neighboring relations under continuous motion. All conversations are documented and the model's responses are compared against established RCC-8 tables and human preference data.

## Key Results
- ChatGPT-4 achieves approximately 70% accuracy on predicting RCC-8 composition relations
- The model demonstrates strong performance on spatial continuity reasoning with ~89% accuracy
- ChatGPT-4 agrees with human preferences for preferred compositions in only about 50% of cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT-4 demonstrates partial spatial reasoning capability through composition table predictions
- Mechanism: The model leverages patterns from its training corpus where RCC-8 composition tables appear frequently, enabling approximate reasoning
- Core assumption: Training data contains sufficient RCC-8 composition examples for pattern recognition
- Evidence anchors:
  - [abstract] "around 70% of composition relations" and "89% of continuity relations"
  - [section] "sometimes ChatGPT-4 does appear able to do some interesting (qualitative) spatial reasoning, but often fails, sometimes making elementary mistakes"
  - [corpus] Weak evidence - corpus contains related papers but no direct composition table examples
- Break condition: When composition requires novel spatial arrangements not covered in training data, or when reasoning requires chaining multiple compositional steps

### Mechanism 2
- Claim: ChatGPT-4 struggles with preferred compositions due to lack of computational reasoning models
- Mechanism: The model attempts to predict preferred relations but lacks the underlying cognitive simplicity model that humans use
- Core assumption: Humans prefer simpler models based on computational cost (Knauff, Rauh, and Schlieder 1995)
- Evidence anchors:
  - [section] "ChatGPT-4 only agreed with the average human on 20/49 (40.82%) though in a further three cases it agreed with one of the language groups"
  - [section] "ChatGPT-4 justified its choice by saying it was the 'cautious' choice, or the 'safest choice'"
  - [corpus] No direct evidence of preferred composition models in training data
- Break condition: When asked to justify preferences beyond simple spatial descriptions, or when multiple equally valid preferences exist

### Mechanism 3
- Claim: ChatGPT-4 can perform continuity reasoning through pattern matching on spatial transformations
- Mechanism: The model recognizes spatial continuity patterns from training examples and applies them to predict immediate neighbors
- Core assumption: Training data contains sufficient examples of spatial continuity and transformation descriptions
- Evidence anchors:
  - [abstract] "89% of continuity relations" - much higher accuracy than composition
  - [section] "The two missing links are both from EQ (to NTPP and to NTPPi)" - shows systematic pattern recognition
  - [corpus] Weak evidence - corpus mentions continuity but no specific RCC-8 continuity tables
- Break condition: When spatial transformations involve non-continuous changes or when reasoning requires bidirectional consistency checking

## Foundational Learning

- Concept: Qualitative Spatial Relations (RCC-8 calculus)
  - Why needed here: Understanding the eight JEPD base relations and their compositional properties is fundamental to interpreting ChatGPT-4's performance
  - Quick check question: Can you name all eight RCC-8 relations and explain what DC(x,y) means?

- Concept: Composition Tables and Qualitative Reasoning
  - Why needed here: The evaluation focuses on ChatGPT-4's ability to predict entries in the RCC-8 composition table, which requires understanding compositional reasoning
  - Quick check question: What would be the composition of DC(x,y) and PO(y,z) in RCC-8?

- Concept: Preferred Mental Models and Cognitive Simplicity
  - Why needed here: The paper compares ChatGPT-4's preferred compositions to human preferences based on computational simplicity
  - Quick check question: According to Knauff, Rauh, and Schlieder's theory, what type of spatial model do humans typically prefer?

## Architecture Onboarding

- Component map: Input parser -> Relation extraction -> Reasoning engine -> Pattern matching -> Response generator
- Critical path: Prompt → Relation parsing → Reasoning attempt → Pattern matching → Response generation
- Design tradeoffs: The system prioritizes pattern matching over explicit logical reasoning, trading accuracy for broader coverage
- Failure signatures: Inconsistent predictions for inverse relations, elementary reasoning errors, failure to recognize unique compositions
- First 3 experiments:
  1. Test composition of DC with all other relations to verify baseline accuracy
  2. Test continuity predictions for EC relation to validate spatial transformation understanding
  3. Test preferred composition for TPP/NTPPi to evaluate cognitive model alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would performance vary across different LLM architectures and training paradigms for qualitative spatial reasoning tasks?
- Basis in paper: [explicit] The paper notes that "Other LLMs could be evaluated" and that "different LLMs have different strengths" from previous work
- Why unresolved: The paper only tested ChatGPT-4, leaving performance comparisons across model architectures unexplored
- What evidence would resolve it: Systematic testing of multiple LLMs (GPT, Claude, LLaMA variants, etc.) on the same RCC-8 reasoning tasks with controlled prompts

### Open Question 2
- Question: What is the impact of explicit chain-of-thought prompting on LLM performance in qualitative spatial reasoning?
- Basis in paper: [explicit] The paper states "it has already been observed that different LLMs have different strengths" and suggests "investigating whether a multi-modal FM with such abilities would be of great interest"
- Why unresolved: The paper deliberately used "vanilla" LLM without specialized prompting strategies, leaving their effectiveness untested
- What evidence would resolve it: Head-to-head comparisons of standard vs. chain-of-thought prompting on all three RCC-8 tasks (composition, preferred relations, continuity)

### Open Question 3
- Question: Can multimodal FMs with drawing capabilities significantly improve qualitative spatial reasoning performance?
- Basis in paper: [inferred] The paper suggests "it is natural to use pencil and paper to sketch diagrams" and proposes investigating "whether a multi-modal FM with such abilities would be of great interest"
- Why unresolved: Current evaluation relies on text-only interactions, not leveraging potential visual reasoning capabilities
- What evidence would resolve it: Direct comparison of text-only vs. multimodal LLM performance on RCC-8 tasks, measuring both accuracy and reasoning quality

## Limitations
- ChatGPT-4 frequently confuses inverse relations and makes elementary reasoning errors
- The model's performance varies significantly across different types of spatial reasoning tasks
- Results may not be reproducible due to potential version differences in ChatGPT-4 and unknown API parameter settings

## Confidence
- High Confidence: The finding that ChatGPT-4 achieves ~89% accuracy on continuity relations is well-supported by systematic testing and clear patterns in the results.
- Medium Confidence: The ~70% accuracy on composition relations is reliable but shows higher variability, particularly with more complex compositions.
- Low Confidence: The ~50% agreement with human preferences for compositions is less reliable due to the subjective nature of these preferences and potential differences in how ChatGPT-4 interprets "simplicity" versus human cognitive models.

## Next Checks
1. **Inverse Relation Consistency Test**: Systematically test ChatGPT-4's ability to correctly identify and use inverse relations (e.g., TPP vs TPPi, NTPP vs NTPPi) across all composition scenarios to quantify the extent of confusion between related but distinct relations.

2. **Multi-Step Composition Chain Validation**: Evaluate ChatGPT-4's performance on chained compositions (e.g., DC(x,y) composed with PO(y,z) then composed with EC(z,w)) to determine whether errors compound in sequential reasoning tasks and identify at which step failures typically occur.

3. **Cross-Model Comparison Study**: Replicate the same RCC-8 reasoning tasks with other contemporary LLMs (e.g., Claude, Gemini) to establish whether ChatGPT-4's performance represents a general limitation of transformer-based models or is specific to this implementation.