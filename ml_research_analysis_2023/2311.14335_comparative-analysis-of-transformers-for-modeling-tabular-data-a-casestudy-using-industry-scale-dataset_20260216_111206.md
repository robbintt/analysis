---
ver: rpa2
title: 'Comparative Analysis of Transformers for Modeling Tabular Data: A Casestudy
  using Industry Scale Dataset'
arxiv_id: '2311.14335'
source_url: https://arxiv.org/abs/2311.14335
tags:
- data
- values
- transformer
- tower
- tabular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper compares different transformer architectures for modeling
  tabular data on both synthetic and industry-scale datasets. Key findings include:
  Twin Tower, a direct supervised approach, performs well on large datasets with lower
  computational requirements.'
---

# Comparative Analysis of Transformers for Modeling Tabular Data: A Casestudy using Industry Scale Dataset

## Quick Facts
- arXiv ID: 2311.14335
- Source URL: https://arxiv.org/abs/2311.14335
- Reference count: 33
- Primary result: Twin Tower transformer outperforms TabBERT and LUNA on industry-scale tabular datasets for fraud, credit default, and spend prediction tasks.

## Executive Summary
This paper presents a comparative analysis of different transformer architectures for modeling tabular data, focusing on their performance on industry-scale datasets. The study evaluates Twin Tower, TabBERT, and LUNA models across three financial tasks: fraud prediction, credit default prediction, and spend prediction. Key findings indicate that Twin Tower, a direct supervised approach, achieves superior performance on large datasets while requiring lower computational resources compared to pre-training-based models like TabBERT. The research highlights the importance of data preprocessing choices and model architecture selection when working with tabular data in industry settings.

## Method Summary
The study compares three transformer architectures (Twin Tower, TabBERT, and LUNA) for modeling tabular data using both synthetic and industry-scale datasets. The Twin Tower architecture uses two parallel transformer blocks with a gating mechanism, while TabBERT employs hierarchical transformers with pre-training. LUNA uses a hierarchical structure with separate loss terms for numerical and categorical features. The evaluation covers three tasks: fraud prediction on synthetic data, credit default prediction on the American Express 2022 Kaggle dataset, and spend prediction on an industry dataset. Models are evaluated using F1 score for classification tasks and specialized metrics for regression tasks.

## Key Results
- Twin Tower achieves F1 scores of 0.844, 0.795, and 0.792 on fraud prediction, credit default prediction, and spend prediction tasks respectively
- TabBERT struggles with high-dimensional industry data due to information loss from converting numerical values to categorical features
- Twin Tower outperforms other methods while consuming fewer computational resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Twin Tower outperforms pre-training-based models like TabBERT on large industry datasets because it avoids categorical quantization of numerical features, preserving information and discriminative power.
- Mechanism: By treating numerical values as continuous inputs to the embedding layer, Twin Tower retains fine-grained distinctions between close numerical values that would otherwise be lost in binning or quantization.
- Core assumption: Numerical features in financial datasets carry discriminative information at a granular level, and the Transformer's attention mechanism can effectively model these continuous inputs.
- Evidence anchors:
  - [abstract] "Twin Tower, a direct supervised approach, performs well on large datasets with lower computational requirements."
  - [section] "Converting numerical features into categorical ones results in information loss, diminishing discriminative power, and relying on arbitrary cutoff points due to data quantization mechanism."

### Mechanism 2
- Claim: Direct supervised training (Twin Tower) is more effective than decoupled pre-training and fine-tuning for industry-scale tabular data because it avoids the computational overhead and vocabulary explosion associated with large-scale pre-training.
- Mechanism: Direct supervision allows the model to learn task-specific representations without first learning a general-purpose embedding space, reducing training time and memory requirements.
- Core assumption: Industry-scale datasets have sufficient labeled data for direct supervised learning, and the computational cost of pre-training outweighs its benefits for these specific tasks.
- Evidence anchors:
  - [abstract] "Twin Tower, a direct supervised approach, performs well on large datasets with lower computational requirements."
  - [section] "Industry datasets often present large scale and high dimensionality. Therefore, simpler architectures for models are preferred, as they can offer comparable performance to pre-training-based models while consuming fewer resources."

### Mechanism 3
- Claim: Attention across time steps is more critical than attention across feature dimensions for modeling financial tabular data, as temporal patterns drive predictive performance.
- Mechanism: The Twin Tower architecture's left tower, which focuses on temporal attention, contributes more to overall performance than the right tower's feature-wise attention, indicating that sequential dependencies are more informative than feature interactions for these tasks.
- Core assumption: Financial tabular data exhibits strong temporal patterns and dependencies that are more predictive than cross-feature interactions for the tasks studied.
- Evidence anchors:
  - [section] "The findings reveal that the isolated performance of the left tower closely resembles that of the complete Twin Tower model. However, the isolated performance of the right tower is poor, suggesting that, in the dataset used, temporal information holds more importance compared to interactions across features."

## Foundational Learning

- Concept: Tabular data preprocessing techniques (binning, encoding, imputation)
  - Why needed here: The paper compares different preprocessing approaches and their impact on model performance, requiring understanding of how categorical and numerical features are handled.
  - Quick check question: What are the trade-offs between converting numerical values to categorical values versus treating them as continuous inputs in transformer models for tabular data?

- Concept: Transformer architecture components (attention mechanism, encoder/decoder blocks)
  - Why needed here: The paper analyzes different transformer architectures and their suitability for tabular data, requiring understanding of how transformers process sequential information.
  - Quick check question: How does the attention mechanism in transformers enable the modeling of long-range dependencies in sequential tabular data?

- Concept: Imbalanced classification and evaluation metrics (F1 score, precision, recall)
  - Why needed here: The paper deals with imbalanced datasets (fraud detection) and uses F1 score as the primary evaluation metric, requiring understanding of why accuracy is not suitable for such tasks.
  - Quick check question: Why is F1 score preferred over accuracy for evaluating models on imbalanced classification tasks, and what are the implications for model selection?

## Architecture Onboarding

- Component map: Twin Tower (temporal attention tower + feature-wise attention tower + gating mechanism) -> TabBERT (row-level transformer + attribute-level transformer + pre-training) -> LUNA (hierarchical transformer + numerical regression loss + categorical cross-entropy loss)

- Critical path: 1. Data preprocessing (handling categorical and numerical features) 2. Model selection (Twin Tower vs TabBERT vs LUNA based on dataset characteristics) 3. Training (direct supervised vs. pre-training and fine-tuning) 4. Evaluation (F1 score for imbalanced tasks, mean of Gini and capture rate for default prediction)

- Design tradeoffs:
  - Twin Tower vs TabBERT: Computational efficiency and information preservation vs hierarchical attention and pre-training benefits
  - Direct supervised vs pre-training: Task-specific learning vs transfer learning and data efficiency
  - Categorical vs continuous numerical features: Information loss vs computational simplicity

- Failure signatures:
  - TabBERT underperformance: High dimensionality and information loss from numerical feature quantization
  - Twin Tower failure: Weak temporal patterns in the dataset, strong feature interactions
  - LUNA issues: Complex loss function and hyperparameter tuning challenges

- First 3 experiments:
  1. Compare Twin Tower with different window sizes and strides to optimize temporal attention
  2. Ablation study: Train Twin Tower with only left tower (temporal attention) and only right tower (feature-wise attention) to quantify their individual contributions
  3. Evaluate Twin Tower's performance with different numerical feature preprocessing approaches (continuous vs categorical encoding) to validate the information preservation mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of transformer-based models for tabular data compare to traditional gradient boosted decision trees (GBDT) on industry-scale datasets, and what are the trade-offs in terms of computational resources and accuracy?
- Basis in paper: [explicit] The paper discusses the limitations of GBDT models and compares the performance of transformer-based models (Twin Tower, TabBERT, LUNA) with GBDT on industry-scale datasets.
- Why unresolved: The paper provides some comparative results, but a comprehensive analysis of the trade-offs between transformer-based models and GBDT on industry-scale datasets is still needed.
- What evidence would resolve it: Conducting extensive experiments on various industry-scale datasets and comparing the performance, computational resources, and accuracy of transformer-based models and GBDT would provide a clearer understanding of the trade-offs.

### Open Question 2
- Question: How does the choice of data preprocessing techniques (e.g., converting numerical values to categorical values or vice versa) affect the performance of transformer-based models on tabular data?
- Basis in paper: [explicit] The paper discusses different approaches to handle numerical and categorical values in tabular data, such as converting numerical values to categorical values or treating them separately.
- Why unresolved: The paper mentions the impact of data preprocessing on model performance, but a detailed investigation into the effects of different preprocessing techniques on transformer-based models is needed.
- What evidence would resolve it: Conducting experiments with various preprocessing techniques and evaluating their impact on the performance of transformer-based models would provide insights into the optimal preprocessing strategies for tabular data.

### Open Question 3
- Question: How does the attention mechanism in transformer-based models contribute to capturing temporal dependencies and feature interactions in tabular data, and how can this be leveraged to improve model performance?
- Basis in paper: [explicit] The paper discusses the importance of attention across time steps and attribute dimensions in transformer-based models for tabular data.
- Why unresolved: The paper mentions the significance of attention in capturing temporal dependencies and feature interactions, but a deeper understanding of how attention contributes to model performance and how it can be optimized is still needed.
- What evidence would resolve it: Conducting ablation studies and analyzing the attention weights in transformer-based models would provide insights into the role of attention in capturing temporal dependencies and feature interactions, leading to potential improvements in model performance.

## Limitations
- Findings are primarily based on experiments with a specific industry partner's data, limiting generalizability to other domains
- Limited exploration of hyperparameter tuning impact on model performance
- Computational resource comparison between models is not detailed enough for comprehensive trade-off understanding

## Confidence
- High Confidence: The comparative performance of Twin Tower, TabBERT, and LUNA on the specific industry datasets used in the study
- Medium Confidence: The generalizability of the findings to other domains or datasets with different characteristics
- Low Confidence: The impact of hyperparameter tuning on model performance and the detailed computational resource comparison between models

## Next Checks
1. Validate the findings on additional datasets from different domains to assess generalizability of comparative performance
2. Conduct comprehensive hyperparameter sensitivity analysis to understand impact on model performance and identify optimal configurations
3. Perform detailed benchmarking of computational resources (training time, GPU memory) for each model architecture across various dataset sizes and complexities