---
ver: rpa2
title: A Language Agent for Autonomous Driving
arxiv_id: '2311.10813'
source_url: https://arxiv.org/abs/2311.10813
tags:
- driving
- planning
- reasoning
- agent-driver
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Agent-Driver introduces a paradigm shift in autonomous driving
  by leveraging Large Language Models (LLMs) as a cognitive agent. Unlike conventional
  perception-prediction-planning frameworks, it employs a tool library for dynamic
  function calls to neural modules, a cognitive memory storing common sense and driving
  experiences, and a reasoning engine for chain-of-thought reasoning, task planning,
  motion planning, and self-reflection.
---

# A Language Agent for Autonomous Driving

## Quick Facts
- arXiv ID: 2311.10813
- Source URL: https://arxiv.org/abs/2311.10813
- Reference count: 40
- Agent-Driver achieves 0.74 L2 error and 0.21% collision rate on nuScenes, a 32% improvement over state-of-the-art

## Executive Summary
Agent-Driver introduces a paradigm shift in autonomous driving by leveraging Large Language Models (LLMs) as cognitive agents. Unlike conventional perception-prediction-planning frameworks, it employs a tool library for dynamic function calls to neural modules, a cognitive memory storing common sense and driving experiences, and a reasoning engine for chain-of-thought reasoning, task planning, motion planning, and self-reflection. Evaluated on the nuScenes benchmark, Agent-Driver significantly outperforms state-of-the-art methods, achieving an average L2 error of 0.74 and a collision rate of 0.21%, representing a 32% improvement in collision rate over the previous best. It also demonstrates strong few-shot learning ability and superior interpretability.

## Method Summary
Agent-Driver transforms the traditional autonomous driving pipeline by introducing a versatile tool library accessible via function calls to neural modules, a cognitive memory of common sense and experiential knowledge, and a reasoning engine capable of chain-of-thought reasoning, task planning, motion planning, and self-reflection. The system uses GPT-3.5-turbo-0613 as its core LLM agent, which dynamically orchestrates tool functions to extract environmental information, retrieves relevant context from cognitive memory, and performs multi-stage reasoning to generate safe, interpretable motion plans. The approach was evaluated on the nuScenes dataset using L2 error and collision rate metrics over a 3-second planning horizon.

## Key Results
- Achieves 0.74 average L2 error on nuScenes validation set
- Reduces collision rate to 0.21%, a 32% improvement over previous best
- Demonstrates strong few-shot learning with motion planner trained on only 0.1% of data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large Language Models (LLMs) can serve as a unified cognitive agent that replaces the traditional perception-prediction-planning pipeline by dynamically orchestrating tool functions and reasoning about driving scenarios.
- Mechanism: The system uses LLMs to select relevant neural modules via function calls from a tool library, thereby reducing redundant perception and focusing on critical environmental information. This mimics human selective attention.
- Core assumption: LLMs can effectively determine which environmental information is necessary for decision-making without explicit hard-coded rules.
- Evidence anchors:
  - [abstract]: "transforms the traditional autonomous driving pipeline by introducing a versatile tool library accessible via function calls... a reasoning engine capable of chain-of-thought reasoning, task planning, motion planning, and self-reflection."
  - [section 3.2]: "Our approach reduces the redundancy in current systems by leveraging the reasoning power of the LLM to determine what environmental information is of real importance to the decision-making process."
  - [corpus]: Weak evidence—no corpus entries discuss this exact dynamic function-call orchestration.
- Break condition: If LLMs fail to identify critical objects or misinterpret environmental context, safety-critical errors could occur.

### Mechanism 2
- Claim: A cognitive memory containing commonsense knowledge and past driving experiences enables the agent to make human-like decisions by retrieving relevant context for current scenarios.
- Mechanism: Two-stage memory search (vector + LLM-based fuzzy ranking) retrieves similar past scenarios and traffic rules, which are combined with real-time perception data to guide planning.
- Core assumption: Past experiences and commonsense rules encoded as text are sufficient for the LLM to reason about novel situations.
- Evidence anchors:
  - [abstract]: "a cognitive memory of common sense and experiential knowledge for decision-making"
  - [section 3.3]: "we propose a configurable cognitive memory that explicitly stores common sense and driving experiences... The cognitive memory contains two sub-memories: commonsense memory and experience memory."
  - [corpus]: Weak evidence—no corpus entries describe a similar two-stage text-based memory search for driving.
- Break condition: If memory retrieval fails to find relevant experiences or commonsense is incomplete, the agent may make suboptimal or unsafe decisions.

### Mechanism 3
- Claim: Chain-of-thought reasoning, hierarchical task planning, and self-reflection enable interpretable and safe motion planning that outperforms black-box neural planners.
- Mechanism: The reasoning engine performs multi-stage reasoning: identifying notable objects, planning high-level behavior, generating trajectories via fine-tuned LLM, and checking for collisions with trajectory refinement.
- Core assumption: LLMs can perform multi-step reasoning and trajectory generation that aligns with human driving patterns when fine-tuned on real driving data.
- Evidence anchors:
  - [abstract]: "a reasoning engine capable of chain-of-thought reasoning, task planning, motion planning, and self-reflection"
  - [section 3.4]: "Our reasoning engine models the human decision-making process in driving as a step-by-step procedure involving reasoning, hierarchical planning, and self-reflection."
  - [corpus]: Weak evidence—no corpus entries describe this exact chain-of-thought reasoning pipeline for autonomous driving.
- Break condition: If LLM reasoning is flawed or self-reflection fails to detect collisions, the system could generate unsafe trajectories.

## Foundational Learning

- Concept: Chain-of-thought reasoning in LLMs
  - Why needed here: Enables the agent to break down complex driving decisions into interpretable sub-steps (object identification, behavior planning, trajectory generation).
  - Quick check question: How does the LLM decide which objects are "notable" in a given scene?

- Concept: Few-shot learning with LLMs
  - Why needed here: Allows the motion planning LLM to achieve high performance with minimal training data, improving generalization.
  - Quick check question: What enables the motion planning LLM to perform well with only 0.1% of training data?

- Concept: Vector embeddings and K-NN search
  - Why needed here: Supports efficient retrieval of similar past driving experiences from the experience memory.
  - Quick check question: Why is an LLM-based fuzzy search used after vector K-NN retrieval?

## Architecture Onboarding

- Component map:
  Tool Library -> Cognitive Memory -> Reasoning Engine -> Neural Modules -> LLM Core

- Critical path:
  1. Sensory data → Neural modules → Tool library functions
  2. Environmental information + Memory queries → LLM reasoning engine
  3. Reasoning steps → Motion plan → Collision check/refinement → Output trajectory

- Design tradeoffs:
  - Flexibility vs. latency: Dynamic function calls allow adaptability but add inference overhead.
  - Interpretability vs. performance: Text-based reasoning is traceable but may be slower than end-to-end neural planners.
  - Memory size vs. retrieval accuracy: Larger experience memory improves recall but increases search time.

- Failure signatures:
  - High collision rate → Possible reasoning engine or self-reflection failure
  - Slow planning → Tool library function calls or memory retrieval bottleneck
  - Suboptimal trajectories → Inadequate commonsense rules or poor experience memory examples

- First 3 experiments:
  1. Ablation study: Remove tool library and measure change in collision rate and planning time.
  2. Memory retrieval test: Vary K in K-NN search and observe effect on planning quality.
  3. Few-shot scaling: Fine-tune motion planner with 0.1%, 1%, 10% data and compare L2 error/collision rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the reasoning engine's chain-of-thought reasoning component identify key objects and their potential effects in complex driving scenarios, and what factors influence its accuracy?
- Basis in paper: [explicit] The paper mentions that the reasoning engine performs chain-of-thought reasoning to recognize key objects and events, but it does not provide details on the specific mechanisms or factors influencing its accuracy.
- Why unresolved: The paper lacks a detailed explanation of the chain-of-thought reasoning process and the factors that affect its performance.
- What evidence would resolve it: A comprehensive analysis of the chain-of-thought reasoning process, including the factors that influence its accuracy and performance in various driving scenarios, would provide insights into its effectiveness.

### Open Question 2
- Question: How does the tool library's dynamic function calling mechanism adapt to different driving scenarios and ensure the collection of necessary environmental information while minimizing redundancy?
- Basis in paper: [explicit] The paper introduces the tool library as a mechanism for dynamic function calls to collect environmental information, but it does not elaborate on how it adapts to different scenarios or minimizes redundancy.
- Why unresolved: The paper does not provide a detailed explanation of the dynamic function calling mechanism and its adaptation to different driving scenarios.
- What evidence would resolve it: A detailed analysis of the tool library's dynamic function calling mechanism, including its adaptation to various driving scenarios and its effectiveness in minimizing redundancy, would provide insights into its performance.

### Open Question 3
- Question: How does the cognitive memory's two-stage search algorithm effectively retrieve the most similar past driving experiences, and what factors influence its performance?
- Basis in paper: [explicit] The paper describes the two-stage search algorithm for retrieving past driving experiences but does not provide details on its effectiveness or the factors influencing its performance.
- Why unresolved: The paper lacks a comprehensive analysis of the two-stage search algorithm's effectiveness and the factors that affect its performance.
- What evidence would resolve it: A thorough evaluation of the cognitive memory's two-stage search algorithm, including its effectiveness in retrieving similar past experiences and the factors that influence its performance, would provide insights into its reliability and utility.

## Limitations

- The paper lacks detailed implementation specifics for critical components, particularly the collision check function and in-context learning examples.
- Empirical evaluation relies heavily on synthetic metrics without real-world testing or comparison to diverse driving scenarios.
- System's reliance on text-based representations for complex spatial reasoning introduces potential failure modes that aren't fully characterized.

## Confidence

**High Confidence**: The architectural framework of using LLMs as cognitive agents with tool libraries, cognitive memory, and reasoning engines is technically sound and represents a legitimate paradigm shift from traditional autonomous driving pipelines. The reported performance improvements on nuScenes (32% reduction in collision rate, 0.74 average L2 error) are specific and measurable.

**Medium Confidence**: The claimed superiority over state-of-the-art methods depends on the assumption that the nuScenes benchmark fully captures real-world driving complexity. The few-shot learning claims (0.1% training data) are plausible given LLM capabilities but lack detailed validation across diverse driving conditions.

**Low Confidence**: The interpretability benefits are asserted but not rigorously demonstrated. The paper doesn't provide systematic analysis of when and why the reasoning engine might fail, nor does it address potential safety-critical edge cases where text-based reasoning could be inadequate.

## Next Checks

1. **Collision Check Function Implementation**: Reconstruct and test the Fcol(τ) collision detection algorithm independently to verify it correctly identifies trajectory collisions with obstacles. Validate using both synthetic scenarios and real nuScenes scenes with known collision risks.

2. **Memory Retrieval Effectiveness**: Conduct controlled experiments varying the K-NN parameter and experience memory content to quantify how memory retrieval quality impacts planning decisions. Test whether the two-stage search (vector + LLM) provides measurable benefits over simpler retrieval methods.

3. **Few-Shot Generalization Test**: Systematically evaluate the motion planning LLM across multiple data subsets (0.1%, 1%, 10%, 100%) on both nuScenes and external datasets to verify consistent few-shot performance and identify failure modes that emerge with limited training data.