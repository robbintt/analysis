---
ver: rpa2
title: 'Two Heads are Actually Better than One: Towards Better Adversarial Robustness
  via Transduction and Rejection'
arxiv_id: '2305.17528'
source_url: https://arxiv.org/abs/2305.17528
tags:
- rejection
- robust
- adversarial
- transduction
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a defense that combines transduction and rejection
  for adversarial robustness, demonstrating that rejection allows transduction to
  handle larger perturbations. It proves that in the transductive-rejection setting,
  robustness can be achieved with weaker data assumptions and lower sample complexity
  than using either technique alone.
---

# Two Heads are Actually Better than One: Towards Better Adversarial Robustness via Transduction and Rejection

## Quick Facts
- arXiv ID: 2305.17528
- Source URL: https://arxiv.org/abs/2305.17528
- Reference count: 40
- Primary result: TLDR achieves 81.6% robust accuracy on CIFAR-10 and 57.9% on CIFAR-100 under ℓ∞ perturbations with budget 8/255

## Executive Summary
This paper proposes TLDR, a defense mechanism that combines transduction and rejection to improve adversarial robustness. The key insight is that rejection allows transduction to handle larger perturbations by removing points near decision boundaries, enabling the model to focus on reliable regions. The authors prove that this combination achieves robustness with weaker data assumptions and lower sample complexity than either technique alone. TLDR adversarially trains on both labeled and unlabeled data, then transforms the model to reject points near decision boundaries. Experiments on MNIST and CIFAR datasets demonstrate that TLDR outperforms baselines that use only transduction or rejection, achieving state-of-the-art robust accuracy on CIFAR-10 and CIFAR-100.

## Method Summary
TLDR combines transduction (using test data during training) with rejection (abstaining from uncertain predictions) to improve adversarial robustness. The method adversarially trains on both labeled training data and unlabeled test inputs using a combined loss function. It then transforms the classifier into a selective one that rejects inputs within a rejection radius of the decision boundary. The approach is theoretically motivated by a reduction showing that transduction plus rejection requires only that a classifier exist with zero robust error under smaller perturbations (2ϵ/3 vs 2ϵ), leading to improved sample complexity. The defense is evaluated against an adaptive attack specifically designed to handle the rejection mechanism.

## Key Results
- TLDR achieves 81.6% robust accuracy on CIFAR-10 and 57.9% on CIFAR-100 under ℓ∞ perturbations with budget 8/255
- Outperforms baselines including AT, AT with rejection, RMC, DANN, TADV, and Rejectron
- Shows stable performance across different rejection radii (ϵdefense)
- Rejection mechanism effectively removes adversarial examples near decision boundaries

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Transduction + rejection enables tolerating larger adversarial perturbations than either technique alone
- **Mechanism**: Transduction allows leveraging unlabeled test inputs for learning, shifting decision boundaries closer to perturbed test points. Rejection removes points near decision boundaries, focusing the model on reliable regions and reducing effective adversarial budget needed.
- **Core assumption**: A classifier exists with zero robust error under U2/3 perturbations (OPTU2/3 = 0), weaker than OPTU2 = 0 required for transduction alone
- **Evidence anchors**:
  - [abstract]: "rejection allows transduction to handle larger perturbations"
  - [section]: "Using transduction alone requires that there exists a classifier with 0 robust error for perturbations U2(x) which are ℓp norm perturbations of adversarial budget 2ϵ. In contrast, our result shows that using both transduction and rejection only requires there exists a classifier with 0 robust error for perturbations U(x) which are ℓp norm perturbations of adversarial budget 2ϵ/3."
- **Break condition**: If OPTU2/3 ≠ 0, the theoretical guarantee fails

### Mechanism 2
- **Claim**: Classifier-to-detector reduction from Tramèr's work improves sample complexity in transductive setting
- **Mechanism**: The reduction transforms a classifier without rejection into a selective classifier that rejects points near decision boundaries. In transductive setting, this allows dynamic adjustment to test data, improving robustness with fewer samples.
- **Core assumption**: Perturbation set U satisfies U = U−1 and U1/3 exists
- **Evidence anchors**:
  - [abstract]: "a novel application of Tramèr's classifier-to-detector technique in the transductive setting can give significantly improved sample-complexity for robust generalization"
  - [section]: "Following Tramèr [Tra22], we can define a transformation FU1/3 that maps a classifier without rejection, h, to the selective classifier ˆh = FU1/3(h)"
- **Break condition**: If U doesn't satisfy required properties (e.g., U ≠ U−1), transformation cannot be applied

### Mechanism 3
- **Claim**: Joint training of labeled and unlabeled data improves adversarial handling
- **Mechanism**: Incorporating unlabeled test data into training objective allows model to learn robustness not just on training set but also on specific test inputs it will encounter, leading to better generalization under adversarial attacks.
- **Core assumption**: Unlabeled test data is representative of distribution model will encounter during evaluation
- **Evidence anchors**:
  - [abstract]: "adversarially trains on both labeled and unlabeled data"
  - [section]: "we perform adversarial training on both the training set and the test set, using a robust cross-entropy objective"
- **Break condition**: If test data distribution differs significantly from training data, transduction-based training may not generalize well

## Foundational Learning

- **Concept: VC dimension and its role in generalization bounds**
  - Why needed here: Paper relies on VC dimension to establish sample complexity bounds for robust learning
  - Quick check question: What is the relationship between VC dimension and the sample complexity required for PAC learning?

- **Concept: Transduction vs. induction in machine learning**
  - Why needed here: Paper contrasts transduction (using test data during training) with induction (training on labeled data only)
  - Quick check question: How does transduction differ from induction in terms of the information available to the learning algorithm?

- **Concept: Rejection in classification tasks**
  - Why needed here: Rejection allows model to abstain from making predictions on uncertain inputs, central to proposed defense mechanism
  - Quick check question: What are the trade-offs involved in using rejection in a classification system?

## Architecture Onboarding

- **Component map**: Input data (labeled training + unlabeled test) -> Adversarial training with combined loss -> Transformation to selective classifier -> Rejection of near-boundary points -> Robust predictions

- **Critical path**:
  1. Train base classifier on labeled data with adversarial examples
  2. Incorporate unlabeled test data into training objective
  3. Transform classifier into selective one using rejection mechanism
  4. Evaluate robustness using adaptive attacks

- **Design tradeoffs**:
  - Transduction improves robustness but increases computational cost due to processing test data during training
  - Rejection reduces misclassifications but may also reject some clean inputs, affecting overall accuracy
  - Choice of rejection radius (ϵdefense) balances robustness and accuracy

- **Failure signatures**:
  - High rejection rate on clean data indicates overly aggressive rejection
  - Low robust accuracy suggests model is not effectively leveraging transduction or rejection
  - Poor performance against adaptive attacks implies defense is not sufficiently robust

- **First 3 experiments**:
  1. Compare TLDR with baselines (AT, AT with rejection, RMC, DANN, TADV, Rejectron) on CIFAR-10 under AutoAttack and GMSA
  2. Ablation study: Evaluate impact of removing transduction or rejection from TLDR
  3. Sensitivity analysis: Test effect of varying rejection radius (ϵdefense) on robust accuracy and rejection rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what conditions does the sample complexity of transduction+rejection improve beyond the linear dependence on VC dimension observed in this paper?
- Basis in paper: [explicit] Paper shows transduction+rejection achieves linear sample complexity (O(VC(H)) vs exponential) but suggests conditions for further improvement are an "interesting direction for future research."
- Why unresolved: Paper proves linear complexity is possible but doesn't characterize when better than linear might be achievable, leaving theoretical limits unclear
- What evidence would resolve it: Proof of either (1) lower bounds showing linear is optimal under reasonable conditions, or (2) algorithms achieving sublinear sample complexity for transduction+rejection under specific data distributions or hypothesis classes

### Open Question 2
- Question: How does the proposed adaptive attack (LREJ + GMSA) perform against defenses that combine transduction with rejection but use different objective functions or model architectures?
- Basis in paper: [explicit] Paper designs LREJ specifically for TLDR algorithm and shows it outperforms standard attacks, but notes this was designed for specific defense architecture
- Why unresolved: Attack was optimized for TLDR's particular loss function and transformation; its effectiveness against other transduction+rejection combinations with different designs is unknown
- What evidence would resolve it: Empirical evaluation of LREJ + GMSA against alternative transduction+rejection defenses using different training objectives, rejection mechanisms, or model architectures

### Open Question 3
- Question: What is the theoretical relationship between the rejection radius parameter (ϵdefense) and the fundamental limits of robustness in the transduction+rejection setting?
- Basis in paper: [explicit] Paper shows performance is relatively stable across rejection radii but doesn't establish theoretical connections between rejection radius and achievable robustness guarantees
- Why unresolved: While empirical results show some stability, there's no theoretical analysis explaining why certain rejection radii work better or how they relate to data distribution's margin or perturbation structure
- What evidence would resolve it: Theoretical analysis deriving optimal rejection radii as functions of data distribution properties, perturbation structure, or fundamental robustness bounds in transduction+rejection setting

## Limitations
- Relies on unlabeled test data being available during training, which may not be practical in all deployment scenarios
- Computational overhead from transductive training increases training time significantly
- Theoretical guarantees depend on idealized assumptions about classifier existence that may not hold in practice
- Potential for overfitting to specific attack strategies used in evaluation

## Confidence
- Theoretical framework: Medium - builds on prior work but practical tightness of bounds remains unverified
- Empirical results: Medium - TLDR outperforms baselines but improvements are modest and rejection mechanism's impact not fully isolated
- Adaptive attack evaluation: Medium - thorough but doesn't cover all possible adaptive adversaries

## Next Checks
1. Test TLDR's performance on a dataset where the test distribution differs from training to assess transductive generalization
2. Perform an ablation study isolating the impact of the rejection mechanism on robust accuracy vs rejection rate
3. Evaluate robustness against a broader range of adaptive attacks, including those that exploit the rejection threshold