---
ver: rpa2
title: On the Learning Dynamics of Attention Networks
arxiv_id: '2307.13421'
source_url: https://arxiv.org/abs/2307.13421
tags:
- attention
- focus
- hard
- soft
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies the learning dynamics of three attention paradigms
  - soft, hard, and latent variable marginal likelihood (LVML) - for selective dependence
  classification tasks. The key insight is that each paradigm behaves differently
  during training: soft attention improves quickly at initialization but splutters
  later, hard attention is slow at initialization but improves rapidly once a non-trivial
  focus model is attained, and LVML combines the advantages of both.'
---

# On the Learning Dynamics of Attention Networks

## Quick Facts
- arXiv ID: 2307.13421
- Source URL: https://arxiv.org/abs/2307.13421
- Reference count: 40
- This paper studies the learning dynamics of soft, hard, and latent variable marginal likelihood (LVML) attention paradigms for selective dependence classification tasks.

## Executive Summary
This paper investigates the learning dynamics of three attention paradigms - soft, hard, and LVML - in the context of selective dependence classification tasks. The authors identify that each paradigm exhibits distinct training behavior: soft attention provides strong initial incentives for focus model improvement that decay over time, hard attention provides weak initial incentives that strengthen as the focus model becomes more accurate, and LVML combines advantages of both. Based on these observations, the authors propose a hybrid approach that initializes with soft attention then switches to hard attention, demonstrating improved performance in both accuracy and interpretability across multiple datasets.

## Method Summary
The authors study Focus-Classify Attention Models (FCAM) that use a focus function to score input segments and a classification function to make predictions based on weighted segment aggregation. Three loss functions are analyzed: soft attention (using softmax), hard attention (using one-hot selection), and latent variable marginal likelihood (LVML). The hybrid approach sequentially applies soft attention initially, then switches to hard attention. Experiments are conducted on semi-synthetic CIFAR10/CIFAR100 mosaic datasets, HateXplain text data, and MSCOCO image captioning data, with evaluation using accuracy and Strongly Accurate Interpretable Fraction (SAIF).

## Key Results
- Soft attention improves focus model quickly at initialization but splutters later in training
- Hard attention is slow to improve initially but shows rapid improvement once a non-trivial focus model is attained
- LVML combines the advantages of both soft and hard attention paradigms
- The proposed hybrid approach achieves improved performance in terms of both accuracy and interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The focus model's incentive to improve depends critically on the choice of attention loss function, with soft attention providing strong initial incentive that decays, and hard attention providing weak initial incentive that strengthens as the focus model becomes more accurate.
- Mechanism: During training, the classification model's ability to minimize loss depends on the quality of the focus model's attention weights. Different loss functions create different gradients with respect to the focus scores, leading to varying incentives for the focus model to improve. Soft attention allows the classification model to partially succeed even with poor focus, reducing the need for focus improvement. Hard attention makes classification extremely difficult with poor focus, creating strong incentive for focus improvement once any reasonable focus model emerges.
- Core assumption: The loss function gradient with respect to focus scores creates the incentive structure for focus model improvement.
- Evidence anchors:
  - [abstract] "With the soft attention loss, the focus model improves quickly at initialization and splutters later on. On the other hand, hard attention loss behaves in the opposite fashion."
  - [section 3.2] "The focus improvement incentive for the soft attention paradigm diminishes with increase in α... The focus improvement incentive for the hard attention paradigm increases with increase in α"
- Break condition: If the classification model can succeed without accurate focus (e.g., through redundancy in features), the incentive structure may break down.

### Mechanism 2
- Claim: The learning dynamics create distinct patterns in the final model's interpretability, with soft attention models having non-confident focus but confident classification, hard attention having confident but potentially incorrect focus, and LVML having both confident focus and classification.
- Mechanism: The incentive structure affects the final converged state of the model. Soft attention's early strong focus improvement followed by decay leads to focus models that don't strongly distinguish foreground from background. Hard attention's delayed strong incentive leads to focus models that are confident but may overfit to specific patterns. LVML's balanced incentive structure allows both components to develop confidence simultaneously.
- Core assumption: The final model state reflects the cumulative effect of the incentive structure throughout training.
- Evidence anchors:
  - [abstract] "The models trained with soft attention gives confident class label predictions... The focus model in the FCAM trained with soft attention loss is often not very confident"
  - [section 2.4] "The focus model in the FCAM trained with hard attention loss is often confidently wrong or right"
  - [section 2.4] "The FCAM models trained with latent variable marginal likelihood have both confident class label predictions and focus scores"
- Break condition: If regularization or architectural constraints prevent the focus model from becoming too confident, the interpretability patterns may change.

### Mechanism 3
- Claim: The hybrid approach works by initializing with soft attention (leveraging its strong early focus improvement incentive) then switching to hard attention (leveraging its strong late-stage incentive), combining the advantages of both paradigms.
- Mechanism: Soft attention rapidly improves the focus model at initialization, creating a non-trivial focus model. The switch to hard attention then provides strong incentive for further focus improvement while the classification model is already reasonably capable, leading to both confident focus and confident classification in the final model.
- Core assumption: The benefits of each paradigm's incentive structure can be sequentially exploited.
- Evidence anchors:
  - [abstract] "Based on our observations, we propose a simple hybrid approach that combines the advantages of the different loss functions"
  - [section 5] "Soft attention provides strong incentives for the focus model to improve at initialization but stagnates later on. In the Hard attention paradigm the incentive for focus model to improve is small at initialization but becomes larger once a non-trivial focus model is attained."
- Break condition: If the transition between paradigms causes instability or if the soft attention initialization is poor, the hybrid approach may not work.

## Foundational Learning

- Concept: Gradient flow analysis in simplified settings
  - Why needed here: Understanding the learning dynamics requires analyzing how parameters evolve under gradient descent, particularly when one component is fixed
  - Quick check question: Can you derive the gradient flow trajectory for a simple classification problem when the focus model is fixed to give specific attention weights?

- Concept: Fixed focus loss analysis
  - Why needed here: The key insight comes from analyzing how the classification model performs under fixed focus conditions, revealing the incentive structure for focus improvement
  - Quick check question: For a fixed focus model giving attention weight α to the foreground, what is the population loss for soft vs hard attention, and how does it vary with α?

- Concept: Interpretability metrics for attention models
  - Why needed here: Evaluating the quality of attention models requires metrics that capture both accuracy and interpretability (whether the model attends to the correct input segments)
  - Quick check question: How would you compute the "Strongly Accurate Interpretable Fraction" (SAIF) for a model on a dataset where you know the true foreground segments?

## Architecture Onboarding

- Component map: Focus model (CNN/linear layer) -> Attention mechanism (softmax) -> Classification model (CNN/linear layer) -> Loss function (soft/hard/LVML)

- Critical path: Forward pass → Attention weight computation → Segment aggregation → Classification → Loss computation → Backward pass → Parameter updates

- Design tradeoffs:
  - Soft attention: Computationally efficient but may produce uninterpretable focus models
  - Hard attention: More interpretable but computationally expensive and slow to train initially
  - LVML: Best of both worlds but computationally intractable for large segment counts
  - Hybrid approach: Combines soft and hard attention sequentially

- Failure signatures:
  - Soft attention failure: High accuracy but poor interpretability (focus model not confident)
  - Hard attention failure: Poor initial convergence, potentially overconfident but incorrect focus
  - LVML failure: Intractable computation for large segment counts
  - Hybrid failure: Poor soft attention initialization leading to bad hard attention convergence

- First 3 experiments:
  1. Implement fixed focus loss analysis on a simple linear orthogonal dataset to verify the incentive structure differences
  2. Train FCAM with soft attention on CIFAR10 subset and analyze focus-prediction heatmaps
  3. Implement hybrid approach and compare SAIF and accuracy against pure soft/hard attention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the exact computational complexity differences between soft attention, hard attention, and LVML attention when scaling to very large input sequences?
- Basis in paper: [explicit] The paper discusses computational issues with different attention paradigms, noting that soft attention is efficient in the number of calls to the classification function, while hard attention and LVML can be computationally expensive when the alignment variable takes exponentially many values.
- Why unresolved: The paper mentions computational complexity as a disadvantage of LVML but doesn't provide specific complexity analysis or comparisons for large-scale settings.
- What evidence would resolve it: Detailed complexity analysis showing time/space complexity as a function of input sequence length for all three paradigms, including empirical benchmarks on large datasets.

### Open Question 2
- Question: How does the hybrid approach perform when the number of segments m becomes very large (e.g., 1000+)?
- Basis in paper: [inferred] The paper proposes a hybrid approach that combines soft and hard attention to address limitations of both, but only tests it on datasets with relatively small m values (5-20 for CIFAR, 196 for MSCOCO).
- Why unresolved: The hybrid approach is only evaluated on datasets with moderate segment counts, leaving its scalability to high-dimensional attention problems untested.
- What evidence would resolve it: Experiments showing the hybrid approach's performance and computational efficiency on tasks with thousands of segments, comparing it to pure soft/hard attention baselines.

### Open Question 3
- Question: What causes the convergence rate differences observed in the linear orthogonal setting between the three attention paradigms?
- Basis in paper: [explicit] The paper derives closed-form expressions for parameter trajectories under gradient flow in the linear orthogonal setting and shows that LVML converges faster than both soft and hard attention.
- Why unresolved: While the paper shows different convergence rates, it doesn't provide a theoretical explanation for why the LVML paradigm achieves faster convergence.
- What evidence would resolve it: A theoretical analysis connecting the convergence rates to the structure of the loss functions, possibly through analysis of the Hessian spectrum or other optimization landscape properties.

## Limitations

- The theoretical justification for the proposed mechanisms lacks rigorous mathematical proof and is primarily supported by empirical observations
- The findings are based on semi-synthetic datasets and specific real-world datasets, limiting generalizability to other domains and task types
- The interpretability metrics depend on knowing the true foreground segments, which may not be available in many real-world applications

## Confidence

- **High Confidence**: The empirical observations about the learning dynamics of soft, hard, and LVML attention paradigms are well-supported by experiments. The hybrid approach showing improved performance over pure paradigms is convincingly demonstrated.
- **Medium Confidence**: The proposed mechanisms explaining why different attention paradigms behave differently during training are plausible but lack rigorous theoretical justification. The connection between incentive structures and final interpretability patterns is supported by evidence but not proven.
- **Low Confidence**: The generalizability of findings to other domains, architectures, and task types is uncertain. The long-term stability of the hybrid approach and its behavior in settings with multiple foreground segments are not well-established.

## Next Checks

1. **Theoretical Gradient Analysis**: Derive the exact gradient flow trajectories for a simple binary classification problem with a fixed focus model giving attention weight α to the foreground. Compare the population loss for soft vs hard attention as a function of α, and verify the predicted incentive structures.

2. **Architecture Ablation Study**: Test the hybrid approach on different CNN architectures (varying depth, width, and normalization) to assess the robustness of the findings. Evaluate whether the sequential exploitation of incentive structures still works when the focus and classification models have different capacities.

3. **Cross-Domain Validation**: Apply the proposed methods to a non-image domain, such as natural language processing or time series analysis, where the attention mechanism operates on segments of text or temporal windows. Assess whether the learning dynamics and interpretability patterns observed in image data hold in this new domain.