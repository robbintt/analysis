---
ver: rpa2
title: A Benchmark for Learning to Translate a New Language from One Grammar Book
arxiv_id: '2309.16575'
source_url: https://arxiv.org/abs/2309.16575
tags:
- translation
- language
- kalamang
- https
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MTOB is a novel benchmark for learning to translate between English\
  \ and Kalamang\u2014a language with fewer than 200 speakers and virtually no web\
  \ presence\u2014using only several hundred pages of field linguistics reference\
  \ materials. Unlike traditional machine translation that relies on large parallel\
  \ corpora, this task asks models to learn a new language from human-readable grammar\
  \ explanations, more akin to L2 learning than L1 acquisition."
---

# A Benchmark for Learning to Translate a New Language from One Grammar Book

## Quick Facts
- **arXiv ID:** 2309.16575
- **Source URL:** https://arxiv.org/abs/2309.16575
- **Reference count:** 38
- **Key outcome:** MTOB evaluates LLM translation of Kalamang (a language with <200 speakers, virtually no web presence) using only grammar books, word lists, and parallel sentences; human baselines exceed best LLM performance (51.6 vs 44.7 chrF kgv→eng, 57.0 vs 45.8 chrF eng→kgv).

## Executive Summary
MTOB introduces a novel benchmark for machine translation where models must learn to translate a genuinely new language—Kalamang—using only several hundred pages of field linguistics reference materials rather than large parallel corpora. This task mimics L2 language learning, requiring models to extract linguistic rules from grammar explanations, word lists, and limited sentence pairs. The benchmark evaluates current LLMs using zero-shot and context-augmented baselines, revealing that while retrieval-augmented approaches improve performance, results remain below human levels achieved with the same materials. This highlights both the promise and limitations of adapting LLMs to genuinely new languages from minimal data.

## Method Summary
The benchmark uses Kalamang—a language with fewer than 200 speakers and virtually no web presence—as the test case. Models are provided with a grammar book (217,388 tokens), a bilingual word list (2,531 pairs), and a small parallel corpus (400 train + 100 test sentence pairs). Translation quality is measured using chrF for both directions (kgv→eng and eng→kgv). Experiments test zero-shot baselines and context-augmented approaches using retrieval of wordlist entries, sentence pairs, and grammar passages, with combinations of these contexts. Twelve models across various sizes and architectures are evaluated, with human baselines established by a linguist learning from the same materials.

## Key Results
- Human baselines substantially exceed LLM performance: 51.6 chrF kgv→eng and 57.0 chrF eng→kgv
- Best LLM results: 44.7 chrF kgv→eng and 45.8 chrF eng→kgv
- Context retrieval improves translation quality, with grammar book passages particularly helpful for English→Kalamang translation
- Larger models and more diverse context types generally yield better performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MTOB isolates LLM adaptation to a genuinely unseen language domain, free from pretraining contamination
- Mechanism: Using Kalamang—a language with virtually no web presence—ensures translation success must result from in-context learning rather than memorization
- Core assumption: Kalamang was truly absent from LLM pretraining data
- Evidence anchors: "Kalamang—a language with fewer than 200 speakers and virtually no web presence"; "virtually no presence on the web"
- Break condition: Discovery of Kalamang content in pretraining datasets or large-scale web scraping contamination

### Mechanism 2
- Claim: Long context retrieval of grammar explanations improves translation accuracy by providing structured linguistic knowledge
- Mechanism: Models retrieve and attend to grammar book passages to apply morphological and syntactic rules, especially for English→Kalamang translation
- Core assumption: Grammar explanations are structured for LLMs to parse and apply
- Evidence anchors: "in contrast to the ungrammatical gibberish produced in other settings, and the results are remarkable given the miniscule number of parallel sentences"
- Break condition: Grammar explanations are too implicit or complex for the model to extract actionable rules

### Mechanism 3
- Claim: Combining multiple types of context yields better translation performance than any single source alone
- Mechanism: Wordlists supply vocabulary, sentence pairs give usage patterns, and grammar passages explain rules—together enabling more accurate translations
- Core assumption: LLMs can effectively integrate heterogeneous context types in a single prompt
- Evidence anchors: "Combining these kinds of context tends to improve results further, though this is less clear for W + S vs. W + S + Gs"
- Break condition: Context overload causes confusion or the model cannot prioritize relevant information

## Foundational Learning

- **Concept: In-context learning**
  - Why needed here: The task relies on LLMs learning to translate without parameter updates, purely from prompt context
  - Quick check question: What happens to translation quality if you remove all context and provide only the source sentence?

- **Concept: Retrieval-augmented generation**
  - Why needed here: Context is dynamically retrieved from grammar books, word lists, and parallel sentences to aid translation
  - Quick check question: How does the choice of retrieval method (e.g., LCS vs. embeddings) affect the relevance of retrieved passages?

- **Concept: Zero-shot translation evaluation**
  - Why needed here: The benchmark tests whether models can translate a language they were not explicitly trained on
  - Quick check question: How can you verify that the model is not relying on memorized translations?

## Architecture Onboarding

- **Component map:** Input sentence → context retrieval (wordlist, sentence pairs, grammar passages) → prompt assembly → LLM inference → output translation
- **Critical path:** Sentence retrieval → grammar book chunk retrieval → prompt formatting → model inference → evaluation
- **Design tradeoffs:** Long context vs. inference speed and cost; retrieval accuracy vs. computational overhead; model size vs. ability to process and utilize complex grammar explanations
- **Failure signatures:** Hallucinated translations with no grounding in provided context; over-reliance on irrelevant retrieved words or passages; inability to generate grammatical sentences in target language
- **First 3 experiments:** 1) Run baseline translation with no context to confirm zero Kalamang knowledge; 2) Test retrieval of wordlist entries only to measure vocabulary impact; 3) Combine sentence pairs and grammar passages to evaluate rule application

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MTOB vary across typologically diverse languages beyond Kalamang?
- Basis in paper: "Linguistic diversity. Kalamang is just one language, and one that we specifically choose to minimize the impact of orthogonal problems like tokenization. Once MTOB is solved for Kalamang, it will be important to ensure that the methods involved work across typologically diverse languages."
- Why unresolved: The current benchmark only tests one language, so generalization to other language families and typologies remains unknown
- What evidence would resolve it: Replicating the MTOB benchmark with grammars and parallel corpora from languages from different families (e.g., Sino-Tibetan, Uralic, Afro-Asiatic) and measuring translation performance across the same model families

### Open Question 2
- Question: Would finetuning on synthetic data generated from grammar explanations improve performance compared to retrieval-only approaches?
- Basis in paper: "It is possible that finetuning with different parameters or different kinds of data preparation—or with more novel approaches like in-context generation of synthetic data for self-distillation—could improve results."
- Why unresolved: The paper only experimented with direct finetuning on raw grammar text, not with synthetic data generation approaches
- What evidence would resolve it: Training models using synthetic parallel data generated by prompting LLMs to translate grammar examples, then evaluating on the MTOB test set

### Open Question 3
- Question: How does model performance change when using multimodal inputs (text + audio/video) instead of text-only grammar books?
- Basis in paper: "Mismatched modality. Even if there were a communication barrier to overcome, text-to-text translation is insufficient for primarily oral languages like Kalamang."
- Why unresolved: The benchmark uses only text-based grammar books, while Kalamang is primarily oral with no formal written tradition
- What evidence would resolve it: Creating parallel audio recordings of grammar explanations paired with corresponding text, then evaluating whether multimodal models outperform text-only models on MTOB tasks

### Open Question 4
- Question: What is the minimum amount of parallel data needed to achieve human-level performance when combined with grammar explanations?
- Basis in paper: "It is not possible to learn a language in generality just by reading a book" and suggests "more complete solutions would require a hybrid approach, combining synthetic data generated using reference materials with a modest amount of real in-domain data."
- Why unresolved: The paper only tests with ~2,000 parallel sentences and does not explore the trade-off between grammar explanations and parallel data quantity
- What evidence would resolve it: Systematically varying the amount of parallel training data (e.g., 100, 500, 1000, 2000 sentences) while keeping grammar explanations constant, then measuring performance relative to human baselines

### Open Question 5
- Question: How does retrieval quality affect translation performance, and can better retrieval methods bridge the gap to human performance?
- Basis in paper: "the retrieved excerpts are lexically similar to the input sentence but do not necessarily contain useful grammatical explanations; they function more like extra retrieved sentences with paired glosses"
- Why unresolved: The paper uses simple retrieval methods (LCS and basic embeddings) without exploring more sophisticated approaches like dense retrieval, cross-encoder reranking, or retrieval of grammatical explanations rather than similar sentences
- What evidence would resolve it: Implementing and comparing advanced retrieval methods (e.g., dense passage retrieval with cross-encoders, retrieval of grammatical explanations by semantic similarity) and measuring their impact on translation quality

## Limitations

- The benchmark's claim that Kalamang is genuinely unseen by LLMs rests on weak empirical verification—no direct audit of pretraining data confirms the language's absence
- chrF scores may not fully capture grammatical correctness in Kalamang, and absolute interpretation of scores remains unclear without qualitative assessment
- The retrieval-augmented approach lacks rigorous ablation studies to isolate the contribution of each context type

## Confidence

- **High Confidence:** Benchmark construction methodology is sound with clearly specified task design, data collection, and evaluation framework; human performance exceeding LLM performance is well-supported
- **Medium Confidence:** Combining multiple context types improves translation quality is supported by results though evidence is correlational; larger models generally outperform smaller variants
- **Low Confidence:** Assertion that LLMs are genuinely learning from scratch rather than leveraging pretraining data is weakest given lack of direct verification of Kalamang's absence from training corpora

## Next Checks

1. **Pretraining Data Audit:** Conduct systematic search of major LLM pretraining datasets and web archives to verify absence of Kalamang content in Common Crawl snapshots, linguistic datasets, and academic repositories

2. **Ablation Study on Context Types:** Design controlled experiments isolating contribution of each context type (wordlist, sentence pairs, grammar passages) with sufficient statistical power to detect meaningful differences

3. **Human Evaluation Study:** Complement chrF scores with qualitative human assessment of translation quality focusing on grammaticality, meaning preservation, and naturalness in both translation directions