---
ver: rpa2
title: Reinforcement Learning with Fast and Forgetful Memory
arxiv_id: '2310.04128'
source_url: https://arxiv.org/abs/2310.04128
tags:
- memory
- recurrent
- reward
- time
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Fast and Forgetful Memory (FFM) is a new recurrent memory model
  for partially observable reinforcement learning. It uses strong inductive biases
  inspired by computational psychology to enable faster training (nearly two orders
  of magnitude) and competitive performance compared to RNNs, while requiring only
  a single line of code change to existing recurrent RL algorithms.
---

# Reinforcement Learning with Fast and Forgetful Memory

## Quick Facts
- arXiv ID: 2310.04128
- Source URL: https://arxiv.org/abs/2310.04128
- Reference count: 40
- Key outcome: Fast and Forgetful Memory (FFM) achieves training speeds two orders of magnitude faster than RNNs while maintaining competitive performance in partially observable reinforcement learning tasks.

## Executive Summary
Fast and Forgetful Memory (FFM) is a novel recurrent memory model designed to address the computational inefficiencies of RNNs in partially observable reinforcement learning (POMDPs). By leveraging strong inductive biases inspired by computational psychology, FFM achieves logarithmic time and linear space complexity through parallel computation across the time dimension. The model's explicit forgetting mechanism, implemented via exponential decay, enables faster training and improved stability compared to traditional RNNs. FFM demonstrates competitive performance across multiple RL benchmarks while requiring only a single line of code change to existing recurrent RL algorithms.

## Method Summary
FFM is implemented as a hybrid memory model that replaces RNNs in recurrent RL algorithms. The method uses complex-valued states with exponential decay to encode temporal context, allowing for efficient parallel computation. The core components include an aggregator that computes a summary of past observations using decay and context parameters, and a cell that applies input gating, updates recurrent states, and extracts a real-valued Markov state. FFM requires double precision floats to maintain numerical stability and can be initialized using prior knowledge about task characteristics.

## Key Results
- FFM achieves training speeds two orders of magnitude faster than RNNs
- FFM maintains competitive episodic rewards compared to RNNs across multiple RL benchmarks
- The explicit forgetting mechanism is identified as key to FFM's success, addressing a limitation of traditional RNNs

## Why This Works (Mechanism)

### Mechanism 1
FFM achieves logarithmic time and linear space complexity by leveraging parallel computation across the time dimension. The model uses a closed-form solution that computes all states in parallel using matrix operations, reducing the need for sequential computation seen in traditional RNNs.

### Mechanism 2
FFM's explicit forgetting mechanism improves training stability and sample efficiency compared to RNNs. The model uses exponential decay (exp(-tα)) to forget old traces, preventing the retention of irrelevant information and focusing on current context.

### Mechanism 3
FFM's use of complex-valued states allows for efficient temporal context encoding. The model uses complex exponentials (exp(-iωt)) to encode temporal relationships in a compact form, enabling reasoning about relative time without explicitly storing all past inputs.

## Foundational Learning

- **Partially Observable Markov Decision Processes (POMDPs)**: Understanding why memory is necessary in POMDPs, as they differ from MDPs by requiring summarization of past observations to make optimal decisions. Quick check: Why is memory necessary in POMDPs, and how does it differ from MDPs?

- **Recurrent Neural Networks (RNNs) and their limitations**: Knowledge of RNNs' training and inference characteristics, including their computational bottlenecks. Quick check: What are the main computational bottlenecks of RNNs during training and inference?

- **Transformer architectures and their efficiency trade-offs**: Familiarity with transformers' computational complexity and why they might be less suitable for RL compared to RNNs. Quick check: How do transformers' space and time complexity compare to RNNs, and why might they be less suitable for RL?

## Architecture Onboarding

- **Component map**: Aggregator -> Cell -> Markov State Extraction
- **Critical path**: 
  1. Receive input and recurrent state
  2. Apply input gating to preprocess the input
  3. Use the aggregator to update the recurrent state with temporal context and decay
  4. Extract the real-valued Markov state from the complex recurrent state
  5. Apply output gating to produce the final output
- **Design tradeoffs**: Complex-valued states enable efficient temporal context encoding but may introduce numerical precision issues; explicit forgetting improves training stability but may lead to loss of important information if decay rate is too high; parallel computation improves training speed but requires careful handling of numerical stability
- **Failure signatures**: Poor performance on tasks requiring long-term memory (decay rate too high or inappropriate context parameters); numerical instability during training (issues with complex-valued states or exponential decay terms); slow convergence (ineffective utilization of parallel computation or malfunctioning gating mechanisms)
- **First 3 experiments**: 
  1. Compare FFM's performance on a simple POMDP task with varying decay rates to determine optimal forgetting rate
  2. Test FFM's ability to handle long sequences by evaluating performance on tasks with varying episode lengths
  3. Analyze learned decay and context parameters for interpretability and to ensure appropriate temporal relationships are captured

## Open Questions the Paper Calls Out

1. **Optimal balance between forgetting rate and temporal context length**: What is the optimal balance for different types of POMDP tasks? The paper shows FFM learns interpretable decay rates and context lengths but uses a single configuration across all experiments without task-specific tuning.

2. **FFM in offline RL settings**: How does FFM compare to other memory models in offline RL where no environment interaction occurs? The paper states it evaluated on-policy and off-policy algorithms but did not experiment with offline or model-based RL algorithms.

3. **Impact of floating point precision**: What is the impact of using single vs double precision floating point arithmetic on FFM's training stability and final performance? The paper notes numerical precision issues but doesn't explore trade-offs between precision levels.

## Limitations
- Narrow experimental scope limited to specific RL algorithms and environments
- Computational complexity claims rely on idealized parallel computation assumptions
- Reliance on complex-valued states introduces potential numerical precision issues
- Forgetting mechanism effectiveness lacks comprehensive ablation studies across diverse task types

## Confidence
- **High confidence**: FFM achieves logarithmic time and linear space complexity
- **Medium confidence**: FFM provides competitive performance with RNNs across diverse tasks
- **Medium confidence**: Explicit forgetting mechanism is crucial for FFM's success

## Next Checks
1. Conduct ablation studies across different decay rates (α) and context parameters (ω) to identify optimal settings for various task categories
2. Evaluate FFM on additional RL algorithms (e.g., DQN, A3C) and benchmark suites to test generalizability
3. Perform extended training runs (10K+ episodes) to assess long-term stability and potential numerical precision degradation in complex-valued state computations