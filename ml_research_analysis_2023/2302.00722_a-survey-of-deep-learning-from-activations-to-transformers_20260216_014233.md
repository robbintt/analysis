---
ver: rpa2
title: 'A Survey of Deep Learning: From Activations to Transformers'
arxiv_id: '2302.00722'
source_url: https://arxiv.org/abs/2302.00722
tags:
- learning
- deep
- attention
- have
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper provides a comprehensive overview of deep learning from
  a holistic perspective, covering influential works in areas such as loss functions,
  regularization, optimization, self-supervised learning, attention mechanisms, transformers,
  and graph neural networks. It highlights key design patterns like using multiple
  instances in parallel, higher-order layers, moving averages, and decomposition.
---

# A Survey of Deep Learning: From Activations to Transformers

## Quick Facts
- **arXiv ID**: 2302.00722
- **Source URL**: https://arxiv.org/abs/2302.00722
- **Reference count**: 8
- **Primary result**: Comprehensive overview of deep learning covering influential works from activations to transformers, identifying key design patterns like multi-X approaches and decomposition

## Executive Summary
This survey provides a holistic treatment of influential deep learning works, aiming to reveal connections between diverse areas of the field. The authors systematically cover architectures, layers, objectives, and optimization techniques, with special attention to works established through platform usage statistics. They identify recurring design patterns such as using multiple instances in parallel, higher-order layers, moving averages, and decomposition techniques. While acknowledging the creativity of recent innovations, the paper suggests that deep learning progress has been characterized by incremental improvements rather than breakthrough innovations, calling for more daring approaches to advance the field.

## Method Summary
The survey methodology involves gathering influential deep learning papers from 2016 onwards using usage statistics from Paperswithcode.com and specialized surveys. Each paper is read and summarized, focusing on key contributions and design patterns. The summaries are organized into a coherent survey that highlights connections between diverse areas of deep learning. The authors acknowledge that their selection criteria may introduce bias but argue that platform metrics provide a valuable new perspective on research impact.

## Key Results
- Identified recurring design patterns across deep learning domains: Multi-X approaches, higher-order layers, moving averages, and decomposition
- Found that recent progress has been characterized by incremental improvements rather than breakthrough innovations
- Highlighted the importance of self-supervised learning combined with transformers, while noting transformers' dependency on large-scale data
- Established a comprehensive mapping between different deep learning components and their applications across domains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Holistic surveys accelerate innovation by revealing cross-domain design patterns
- Mechanism: When researchers see how concepts like skip connections, multi-head attention, and normalization transfer across domains, they can recombine them in novel ways rather than working in isolation
- Core assumption: Deep learning innovations follow reusable architectural patterns that transcend specific applications
- Evidence anchors:
  - [abstract] "We hope that a holistic and unified treatment of influential, recent works helps researchers to form new connections between diverse areas of deep learning."
  - [section] "Our findings suggest that despite many small and creative innovations since the original transformer architecture, there have not been any significant 'breakthrough' innovations that have led to better leaderboard results."
  - [corpus] Weak evidence - no citations yet for this paper, but the claim aligns with established innovation theory about cross-pollination
- Break condition: If pattern transfer doesn't lead to measurable performance improvements across different domains, the mechanism fails

### Mechanism 2
- Claim: Platform-based relevance metrics (usage stats, leaderboards) can identify emerging research trends earlier than traditional citation analysis
- Mechanism: Platforms like Paperswithcode track real-world usage and performance, capturing practical impact before academic recognition catches up
- Core assumption: Usage statistics correlate with future citation impact and research significance
- Evidence anchors:
  - [section] "We have given special attention to works that have been somewhat established based on the usage statistics from the platform 'Paperswithcode.com.'"
  - [section] "Although there are drawbacks when utilizing data from these platforms, we believe that it gives us a new perspective."
  - [corpus] Weak evidence - no neighbor papers have citations yet, making it impossible to validate the correlation assumption
- Break condition: If platform metrics consistently point to low-impact research or miss important foundational work

### Mechanism 3
- Claim: Multi-component approaches (multi-X, higher-order layers, moving averages, decomposition) are more effective than single-component innovations
- Mechanism: Complex problems require multiple complementary techniques rather than silver bullets, as evidenced by the success patterns identified in the survey
- Core assumption: Deep learning challenges are multifaceted enough that single innovations rarely suffice
- Evidence anchors:
  - [section] "Our findings suggest that despite many small and creative innovations since the original transformer architecture... the last few years have been characterized by the enlargement of existing networks... and a shift towards self-supervised learning."
  - [section] "We noted a few general patterns that have been proven effective in many areas: 'Multi-X', i.e., using the same element multiple times in parallel, such as using multiple residual blocks (ResNeXt) or multi-head attention"
  - [corpus] No direct evidence in neighbors, but the pattern aligns with observed trends in deep learning literature
- Break condition: If single-component innovations suddenly dominate leaderboard performance across multiple domains

## Foundational Learning

- Concept: Activation functions and their impact on gradient flow
  - Why needed here: The survey covers activation functions from ReLU to GELU and Mish, showing how they evolved to solve vanishing gradient problems
  - Quick check question: Why did ReLU solve training problems that sigmoid and tanh couldn't, and what limitations does it still have?

- Concept: Attention mechanisms and their computational trade-offs
  - Why needed here: The survey extensively covers attention variants from scaled dot-product to factorized attention, showing the evolution of efficiency techniques
  - Quick check question: How does factorized attention reduce computational complexity compared to full self-attention, and what architectural constraints does this impose?

- Concept: Normalization techniques and their domain-specific applications
  - Why needed here: The survey discusses batch normalization, layer normalization, instance normalization, and layer scale, each suited to different scenarios
  - Quick check question: What are the key differences between batch normalization and layer normalization, and when would you choose one over the other?

## Architecture Onboarding

- Component map: The survey reveals a modular architecture ecosystem where components like attention, normalization, skip connections, and loss functions can be mixed and matched. Each component has multiple variants with different trade-offs in terms of computational cost, generalization, and ease of training.

- Critical path: For a new engineer, the critical path is understanding how components interact: how attention affects gradient flow, how normalization stabilizes training, how skip connections enable deeper networks, and how loss functions shape the learning objective. Start with understanding the original transformer block, then explore variations.

- Design tradeoffs: The survey highlights recurring tradeoffs between computational efficiency and performance. For example, factorized attention reduces complexity but limits the model's ability to capture long-range dependencies. Similarly, layer normalization works better for small batch sizes but may generalize differently than batch normalization.

- Failure signatures: Common failure modes include poor gradient flow (mitigated by proper activation functions and normalization), overfitting (addressed by regularization techniques), and training instability (solved by techniques like TTUR for GANs or sharpness-aware minimization). The survey shows how different components address specific failure modes.

- First 3 experiments:
  1. Implement a simple transformer block with multi-head attention, layer normalization, and skip connections, then test how removing each component affects training stability and performance
  2. Compare different activation functions (ReLU, GELU, Mish) in a small network on a standard benchmark, measuring both training speed and final accuracy
  3. Implement factorized attention and compare its performance and memory usage against full self-attention on a sequence modeling task, varying the window size parameter

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most promising directions for developing fundamentally new deep learning architectures beyond incremental improvements to transformers and existing models?
- Basis in paper: [explicit] The authors state "we need to generate fundamentally new and successful approaches, as the improvements made in the past few years were numerous and often very creative but mainly incremental."
- Why unresolved: While the paper identifies this need, it does not propose specific directions or methodologies for creating breakthrough innovations in deep learning architectures.
- What evidence would resolve it: Demonstration of novel architectures that achieve significant improvements over state-of-the-art models on multiple benchmarks, along with theoretical justifications for their effectiveness.

### Open Question 2
- Question: How can self-supervised learning be further improved to reduce its dependency on large-scale data and computational resources while maintaining or improving performance?
- Basis in paper: [explicit] The paper discusses the importance of self-supervised learning and its combination with transformers, but also mentions that transformers "require more training data to compensate for the lack of inductive bias."
- Why unresolved: Current self-supervised learning methods still require massive amounts of data and compute. The paper identifies this as a challenge but does not propose specific solutions to address this limitation.
- What evidence would resolve it: Development of self-supervised learning techniques that achieve state-of-the-art results with significantly less data and computational resources, along with theoretical analysis of their efficiency.

### Open Question 3
- Question: What are the fundamental limitations of current attention mechanisms, and how can they be overcome to improve efficiency and performance?
- Basis in paper: [inferred] The paper extensively discusses various attention mechanisms and their applications, but also mentions the computational overhead and memory footprint of attention-based models like transformers.
- Why unresolved: While the paper describes many attention variants and their applications, it does not provide a comprehensive analysis of the fundamental limitations of attention mechanisms or propose concrete solutions to overcome these limitations.
- What evidence would resolve it: Theoretical analysis of the limitations of current attention mechanisms, along with empirical evidence demonstrating improved attention-based models that address these limitations in terms of efficiency and performance.

## Limitations
- Selection bias from platform metrics: Using Paperswithcode statistics may overlook foundational papers that haven't yet gained platform traction
- Coverage challenges: The survey inevitably misses some niche topics while attempting comprehensive coverage
- Incremental focus: Heavy emphasis on recent works may underrepresent earlier foundational contributions that shaped current approaches

## Confidence
- Mechanism validity (cross-domain patterns): Medium
- Platform metrics correlation: Low
- Design pattern generalizability: High
- Innovation breakthrough claim: Medium

## Next Checks
1. Cross-reference the survey's coverage with other comprehensive deep learning surveys to identify missing areas
2. Track whether papers identified through platform metrics show higher citation growth compared to traditionally selected papers over a 2-year period
3. Implement a few identified design patterns (like factorized attention or layer scale) in new contexts to test their generalizability