---
ver: rpa2
title: Recasting Continual Learning as Sequence Modeling
arxiv_id: '2310.11952'
source_url: https://arxiv.org/abs/2310.11952
tags:
- uni00000013
- uni00000044
- uni00000056
- uni00000048
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reformulates continual learning as a sequence modeling
  problem, casting the learning process as the forward pass of a sequence model. The
  authors leverage meta-continual learning to train Transformers and their efficient
  variants on multiple continual learning episodes.
---

# Recasting Continual Learning as Sequence Modeling

## Quick Facts
- arXiv ID: 2310.11952
- Source URL: https://arxiv.org/abs/2310.11952
- Authors: 
- Reference count: 40
- This paper reformulates continual learning as a sequence modeling problem, casting the learning process as the forward pass of a sequence model. The authors leverage meta-continual learning to train Transformers and their efficient variants on multiple continual learning episodes. Experiments on seven benchmarks (CIFAR-100, Omniglot, CASIA, MS-Celeb-1M, Sine, Rotation, Completion) show that sequence models, especially Transformers, achieve strong performance in continual learning, particularly in large data regimes. Transformers also offer advantages in terms of parallel training and computational efficiency.

## Executive Summary
This paper proposes a novel reformulation of continual learning (CL) as a sequence modeling problem. Instead of using SGD-based parameter updates, the framework treats the CL process as the forward pass of a sequence model, with meta-continual learning (MCL) used to train the model on multiple CL episodes. The authors demonstrate that this approach, particularly when using Transformers, can effectively address catastrophic forgetting and achieve strong performance across various CL benchmarks.

## Method Summary
The method reformulates continual learning as sequence modeling, where the learning process is cast as the forward pass of a sequence model. In this framework, the inner loop and outer loop of meta-continual learning are equivalent to the forward pass and SGD update of the sequence model, respectively. The class label y is taken as an input to the sequence model, allowing the model to incorporate new information without explicit parameter updates. The authors train Transformers and their efficient variants (Linear Transformer, Performer) on multiple CL episodes using meta-continual learning. The sequence model processes encoded inputs and class labels as a sequence, and predictions are generated for test inputs. The model parameters are updated via meta-SGD based on the meta-loss computed across the meta-test set.

## Key Results
- Sequence models, especially Transformers, achieve strong performance in continual learning, particularly in large data regimes
- The approach effectively addresses catastrophic forgetting compared to traditional CL methods
- Transformers offer advantages in terms of parallel training and computational efficiency over recurrent models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transformers with causal attention can be interpreted as recurrent models where the internal state consists of attention keys and values.
- Mechanism: Each token's output depends only on preceding tokens, allowing new tokens to be added without recomputing existing embeddings. The keys and values in each attention head serve as the recurrent state that is updated with each new token.
- Core assumption: Causal attention creates a Markov property where the current state contains all necessary information from previous tokens.
- Evidence anchors:
  - [section] "Transformers with causal attention can be regarded as recurrent models. A decoder-only Transformer layer consists of two sublayers: a causal attention layer and a feed-forward layer. Since the feed-forward layers work on each token independently, information transfer across time steps occurs exclusively through the causal attention sublayer, and only in the temporal direction (→). Therefore, at any time t, all the past information is represented as keys and values in each attention head l, which are accessed by the query ql t. We can think of the keys and values in each attention layer as the internal state of a recurrent sequence model."
  - [corpus] "Found 25 related papers... Learning Mamba as a Continual Learner: Meta-learning Selective State Space Models for Efficient Continual Learning" - This suggests state-space models are being explored for similar CL applications.
- Break condition: If attention is non-causal or bidirectional, the recurrent interpretation breaks down and requires access to all data simultaneously.

### Mechanism 2
- Claim: The sequence modeling formulation replaces SGD-based parameter updates with in-context learning through forward passes.
- Mechanism: Instead of using gradients to update parameters, the model learns to incorporate new information by processing it as part of the sequence. The class label y is fed as input to the sequence model, allowing flexible update rules learned through meta-training.
- Core assumption: Forward passes can learn more flexible update rules than fixed SGD procedures.
- Evidence anchors:
  - [section] "In contrast, we propose to formulate CL as a sequence modeling problem and let a sequence model play both roles of the continual learner and the model... In our framework, the inner loop and outer loop are equivalent to the forward pass and SGD update of the sequence model, respectively."
  - [section] "An important characteristic of our approach is that the target y is also taken as an input of the sequence model. Since conventional MCL algorithms maintain an explicit model fθ : X → Y, the only way for them to inject the knowledge from y into the model is the SGD update with the inner loss ∇θL(fθt(x), y). On the other hand, our sequence modeling approach replaces the SGD update with the forward pass, which incorporates y's information by taking it as input."
- Break condition: If the sequence model cannot effectively encode both inputs and targets, the in-context learning mechanism fails.

### Mechanism 3
- Claim: Meta-training the sequence model on multiple CL episodes enables zero-shot continual learning without catastrophic forgetting.
- Mechanism: By training on diverse CL episodes, the model learns to handle non-stationary data streams and generalize to new task combinations in meta-test.
- Core assumption: Meta-training on varied CL episodes provides sufficient diversity to prevent overfitting to specific task orders.
- Evidence anchors:
  - [abstract] "By adopting the meta-continual learning (MCL) framework, we can train the sequence model at the meta-level, on multiple continual learning episodes."
  - [section] "Since there are no overlapping tasks between the meta-splits, one cannot achieve a high meta-test score by simply memorizing all tasks in meta-training."
  - [section] "In CIFAR-100 and Omniglot, all methods show a severe degree of meta-overfitting, the gap between the meta-training and meta-test scores. Meta-overfitting is more serious in CIFAR-100 where the task (class) diversity is lower."
- Break condition: If meta-training episodes lack sufficient diversity or are too similar, meta-overfitting occurs and generalization fails.

## Foundational Learning

- Concept: Sequence modeling and recurrent architectures
  - Why needed here: Understanding how Transformers can be interpreted as recurrent models is fundamental to grasping the core mechanism
  - Quick check question: Can you explain how causal attention creates a sequential dependency chain similar to RNNs?

- Concept: Meta-learning and few-shot learning
  - Why needed here: The framework relies on meta-training across multiple CL episodes to learn how to continually learn
  - Quick check question: What's the difference between standard training and meta-training in the context of learning to learn?

- Concept: Catastrophic forgetting and continual learning
  - Why needed here: The work aims to solve the forgetting problem by using sequence modeling instead of traditional parameter updates
  - Quick check question: Why does standard SGD typically cause catastrophic forgetting in CL scenarios?

## Architecture Onboarding

- Component map: Input encoder (CNN/MLP) -> Sequence model (Transformer/Linear Transformer/Performer) -> Class representation system -> Meta-loss computation

- Critical path:
  1. Encode input x and target y into sequence tokens
  2. Process sequence through causal attention layers
  3. Generate predictions for test inputs
  4. Compute meta-loss across meta-test set
  5. Update sequence model parameters via meta-SGD

- Design tradeoffs:
  - Standard vs. efficient Transformers: Accuracy vs. computational efficiency
  - Model size: Larger models show better scaling but require more resources
  - Class representation length: Longer representations increase sequence length and computation

- Failure signatures:
  - Uniform attention distribution across all tokens (attention loss not converging)
  - Large gap between meta-training and meta-test performance (meta-overfitting)
  - Degraded performance on longer episodes (efficient Transformer limitations)

- First 3 experiments:
  1. CIFAR-100 20-task 5-shot classification with standard Transformer
  2. CASIA 20-task 5-shot classification with Linear Transformer
  3. Sine wave regression with standard Transformer

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different sequence modeling architectures beyond Transformers and their efficient variants perform in the meta-continual learning framework?
- Basis in paper: [explicit] The authors state that their approach is not limited to Transformers and opens up possibilities for other sequence models to be applied to meta-continual learning.
- Why unresolved: The paper only tests Transformers and their efficient variants (Linear Transformer and Performer) as examples of their framework. Other sequence models like RNNs, CNNs, or newer architectures like Mamba are not explored.
- What evidence would resolve it: Experiments comparing the performance of various sequence modeling architectures (RNNs, CNNs, Transformers, efficient Transformers, and potentially newer models) on the same meta-continual learning benchmarks would provide insights into which architectures are most effective.

### Open Question 2
- Question: What is the impact of different tokenization strategies for class labels in the sequence modeling approach to meta-continual learning?
- Basis in paper: [explicit] The authors discuss using a fixed vocabulary to tokenize class labels and explore different class representation lengths (C=1, 2, 3) in their experiments.
- Why unresolved: The paper only tests a few tokenization strategies and does not explore the full space of possibilities. For example, they do not investigate subword tokenization methods commonly used in language modeling or the impact of vocabulary size on performance.
- What evidence would resolve it: Experiments comparing the performance of different tokenization strategies (e.g., different vocabulary sizes, subword tokenization methods) on the same meta-continual learning benchmarks would provide insights into the optimal tokenization approach.

### Open Question 3
- Question: How does the scaling behavior of sequence models in meta-continual learning compare to their scaling behavior in other domains like language modeling?
- Basis in paper: [explicit] The authors observe that the performance of Transformers scales with the number of parameters in their meta-continual learning experiments, similar to the scaling behavior observed in large language models.
- Why unresolved: The paper only tests a limited range of model sizes and does not explore the full scaling behavior. Additionally, they do not compare the scaling behavior in meta-continual learning to other domains.
- What evidence would resolve it: Experiments scaling sequence models to a wider range of sizes in meta-continual learning and comparing the scaling behavior to other domains (e.g., language modeling) would provide insights into the generalizability of scaling laws.

## Limitations
- Meta-overfitting is a significant issue, particularly on datasets with lower task diversity like CIFAR-100
- The computational overhead of sequence modeling for CL remains substantial, despite using efficient Transformer variants
- Scalability to very long task sequences may be limited by attention convergence issues

## Confidence
**High Confidence**: The core mechanism of interpreting Transformers with causal attention as recurrent models is well-established in the literature and the paper provides clear mathematical grounding. The sequence modeling reformulation itself is logically sound.

**Medium Confidence**: The experimental results showing performance gains, particularly on Omniglot and MS-Celeb-1M, are promising. However, the meta-overfitting issue and the significant gap between meta-training and meta-test performance on some benchmarks suggest the approach may not generalize as robustly as claimed across all CL scenarios.

**Low Confidence**: The scalability claims to large data regimes are based on limited empirical evidence. The paper's assertion that Transformers offer computational efficiency advantages in continual learning is questionable given the substantial computational requirements of attention mechanisms, even with efficient variants.

## Next Checks
1. **Meta-overfitting Analysis**: Conduct ablation studies varying the diversity of meta-training episodes to quantify the relationship between task diversity and meta-overfitting severity. This would validate whether the meta-overfitting problem is fundamental to the approach or can be mitigated through better episode design.

2. **Scalability Testing**: Evaluate the approach on significantly larger task sequences (50+ tasks) to test the convergence and performance claims under more challenging conditions. Monitor attention patterns and computational requirements as sequence length increases.

3. **Efficiency Comparison**: Perform head-to-head runtime and memory usage comparisons between the sequence modeling approach and state-of-the-art CL methods (both traditional and transformer-based) on identical hardware, measuring both training and inference costs across different task sequence lengths.