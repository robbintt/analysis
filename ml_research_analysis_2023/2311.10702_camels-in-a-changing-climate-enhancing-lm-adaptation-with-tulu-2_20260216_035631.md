---
ver: rpa2
title: 'Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2'
arxiv_id: '2311.10702'
source_url: https://arxiv.org/abs/2311.10702
tags:
- training
- code
- arxiv
- llama
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents T\xDCLU 2, a suite of improved language models\
  \ (LMs) built upon recent advancements in LM adaptation. The authors address the\
  \ challenge of enhancing the performance of open-weight LMs by incorporating new\
  \ techniques such as improved data mixtures, scaling DPO training to 70 billion\
  \ parameters, and experimenting with QLoRA for parameter-efficient finetuning."
---

# Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2

## Quick Facts
- arXiv ID: 2311.10702
- Source URL: https://arxiv.org/abs/2311.10702
- Reference count: 18
- TÜLU 2 achieves competitive results with GPT-3.5-turbo-0301 and matches or exceeds its performance on several benchmarks

## Executive Summary
TÜLU 2 is a suite of improved language models built upon recent advancements in LM adaptation, addressing the challenge of enhancing open-weight LM performance through improved data mixtures, scaling DPO training to 70 billion parameters, and experimenting with QLoRA for parameter-efficient finetuning. The authors present models finetuned on an enhanced data mixture (TÜLU-V2-mix) and further trained using DPO, resulting in state-of-the-art performance among open models on various benchmarks including MT-Bench and AlpacaEval. TÜLU 2 includes both general-purpose models and CODE TÜLU 2 variants that specialize in coding tasks while maintaining competitive general performance.

## Method Summary
TÜLU 2 models are built by fine-tuning Llama-2 and Code Llama base models on an enhanced data mixture (V2 mix) containing 326,154 samples from diverse sources including FLAN, CoT, Open Assistant, and others. The fine-tuning process uses full fine-tuning and QLoRA approaches with extended context length (8,192 tokens) and BFloat16 precision. Some models undergo additional DPO training on UltraFeedback dataset to optimize preference alignment. The suite includes multiple model sizes (7B, 13B, 70B parameters) and both general-purpose and code-specialized variants, evaluated across diverse benchmarks including MMLU, GSM8k, CodexEval, and MT-Bench.

## Key Results
- TÜLU 2 models achieve state-of-the-art performance among open models on MT-Bench and AlpacaEval benchmarks
- DPO training improves open-ended generation metrics by an average of 13% across model sizes without degrading capabilities
- CODE TÜLU 2 models significantly outperform TÜLU 2 models at coding tasks while maintaining competitive general performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DPO training improves open-ended generation metrics without degrading model capabilities
- Mechanism: DPO directly optimizes for preference alignment by minimizing the KL divergence between the policy and the reference model, using binary preference data to update model weights without requiring reward model training
- Core assumption: The preference data used for DPO training (UltraFeedback) is high quality and represents user preferences well
- Evidence anchors:
  - [abstract] "DPO training scales to 70 billion parameter models, and significantly improves open-ended generation metrics without degrading model capabilities, improving AlpacaEval performance by an average of 13% across model sizes"
  - [section 3.3] "DPO training significantly improves AlpacaEval and MT-Bench performance. At all sizes, DPO training provides significant improvements in AlpacaEval"
- Break condition: If the preference data contains noise or systematic bias, the DPO optimization will amplify these issues rather than improve alignment

### Mechanism 2
- Claim: Scaling model size improves performance across almost all tasks
- Mechanism: Larger models have more parameters and capacity to learn complex patterns, allowing them to better capture task-specific knowledge and reasoning abilities
- Core assumption: The training data and optimization methods scale effectively with model size
- Evidence anchors:
  - [section 3.1] "Scaling trends remain strong with TÜLU 2. Increasing model size improves almost every metric when the finetuning setup is held consistent across our model suite"
  - [section 3.2] "Improvements from the V2 mix shrink with model size. While the V2 mix provides a 13% average improvement at the 7B scale, it only provides a 1% improvement at the 70B scale"
- Break condition: If the optimization becomes unstable at larger scales or if the data is insufficient to train the increased parameter count, performance gains may plateau or degrade

### Mechanism 3
- Claim: Domain-specific pretraining (CODE LLAMA) improves coding performance while maintaining general capabilities
- Mechanism: Additional pretraining on code data creates specialized representations and knowledge that transfer to coding tasks, while the underlying LLM architecture preserves general reasoning abilities
- Core assumption: The code pretraining data is diverse and representative enough to improve coding without overfitting
- Evidence anchors:
  - [section 3.5] "CODE TÜLU 2 models significantly outperform TÜLU 2 models at coding tasks. As expected, CODE TÜLU 2 models report drastically improved Codex-Eval performance compared to TÜLU 2"
  - [section 3.5] "CODE TÜLU 2 and TÜLU 2 display drastically different results across non-code evaluations"
- Break condition: If the code pretraining data is too narrow or contains artifacts, the model may develop biases that harm general reasoning or produce code that works in training contexts but fails in real-world scenarios

## Foundational Learning

- Concept: Preference optimization vs traditional RLHF
  - Why needed here: Understanding the difference between DPO and PPO/RLHF helps explain why DPO scales better and achieves stable training at 70B parameters
  - Quick check question: What are the key computational and implementation differences between DPO and traditional RLHF methods?

- Concept: Data mixture design and curriculum learning
  - Why needed here: The V2 data mixture combines multiple datasets with different characteristics, and understanding how this affects learning is crucial for replicating or improving results
  - Quick check question: How does the diversity of datasets in the V2 mixture contribute to the observed performance improvements?

- Concept: Parameter-efficient finetuning techniques
  - Why needed here: QLoRA represents a compute-efficient alternative to full finetuning, and understanding its limitations is important for deployment decisions
  - Quick check question: Under what circumstances would QLoRA be preferred over full fine-tuning despite the performance gap in open-ended generation?

## Architecture Onboarding

- Component map: Base model (LLAMA-2 or CODE LLAMA) -> Instruction tuning (V2 mix) -> Optional DPO finetuning (UltraFeedback) -> Evaluation
- Critical path: Data preparation -> Base model selection -> Instruction tuning -> (Optional) DPO training -> Evaluation and deployment
- Design tradeoffs: Full finetuning vs QLoRA (performance vs compute), DPO vs PPO (simplicity vs flexibility), model size vs inference cost
- Failure signatures: DPO training instability at scale, QLoRA underperformance on open-ended tasks, degradation in multilingual capabilities after DPO
- First 3 experiments:
  1. Replicate the instruction tuning results with the V2 mix on a smaller model to verify data quality impact
  2. Test DPO training stability by gradually increasing model size from 7B to 13B
  3. Compare full finetuning vs QLoRA performance on a subset of tasks to quantify the trade-off

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: What is the optimal data mixture composition for instruction tuning large language models to maximize performance across diverse tasks?
- Basis in paper: [inferred] The authors compare different data mixtures (V1 vs V2) and find varying performance across tasks, suggesting no single optimal mixture exists.
- Why unresolved: The paper shows performance differences but doesn't explore the full space of possible data mixtures or provide a definitive optimal composition.
- What evidence would resolve it: Systematic ablation studies testing all combinations of datasets and their relative weighting to determine the optimal mixture for each task type.

**Open Question 2**
- Question: How does the choice of RLHF method (DPO vs PPO) affect model capabilities and safety considerations at different model scales?
- Basis in paper: [explicit] The authors implement DPO and compare it to previous RLHF methods, noting differences in performance and training stability.
- Why unresolved: The paper only tests DPO and doesn't compare it directly to PPO at scale, nor does it thoroughly investigate safety implications.
- What evidence would resolve it: Direct comparison of DPO and PPO training across multiple model sizes, including comprehensive safety evaluations.

**Open Question 3**
- Question: At what model scale does parameter-efficient finetuning (QLoRA) match full fine-tuning performance?
- Basis in paper: [explicit] The authors find QLoRA underperforms full fine-tuning, with the gap shrinking at larger model sizes.
- Why unresolved: The paper doesn't test QLoRA on models larger than 70B or provide a clear threshold where performance parity is achieved.
- What evidence would resolve it: Systematic testing of QLoRA across a wide range of model sizes up to the largest available, identifying the exact scale where performance matches full fine-tuning.

## Limitations

- DPO training stability at 70B parameters lacks detailed ablation studies to fully validate the claimed improvements
- QLoRA underperforms on open-ended tasks but underlying causes and potential mitigations are not explored in depth
- Environmental and computational costs of scaling to 70B parameters for DPO training are not explicitly discussed

## Confidence

- Evidence supporting TÜLU 2's instruction tuning improvements: High
- Evidence supporting DPO scaling claims to 70B parameters: Medium
- Evidence supporting QLoRA performance characterization: Low

## Next Checks

1. Conduct systematic ablation studies varying batch sizes and learning rates across 7B, 13B, and 34B models to identify precise factors enabling stable 70B DPO training, and test whether these findings generalize to other base models.

2. Evaluate TÜLU 2 and CODE TÜLU 2 on comprehensive multilingual benchmarks to quantify potential degradation in non-English capabilities after DPO training, particularly focusing on code-switching and cross-lingual reasoning tasks.

3. Perform detailed audit of UltraFeedback dataset to identify potential biases, annotation inconsistencies, or systematic patterns that could be amplified through DPO training, and test model robustness to adversarial or edge-case preferences.