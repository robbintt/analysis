---
ver: rpa2
title: 'MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental Learning'
arxiv_id: '2307.05707'
source_url: https://arxiv.org/abs/2307.05707
tags:
- domain
- learning
- domains
- performance
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoP-CLIP addresses catastrophic forgetting under domain distributional
  drift in continual learning by introducing a prompt-based method that generalizes
  S-Prompts to handle both in-distribution and out-of-distribution data. The method
  learns class-wise feature distributions for each domain during training, using individual
  text and visual prompts, and employs a mixture of prompt-tuned CLIP models at inference.
---

# MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental Learning

## Quick Facts
- arXiv ID: 2307.05707
- Source URL: https://arxiv.org/abs/2307.05707
- Authors: 
- Reference count: 40
- Key outcome: MoP-CLIP achieves 11-40% accuracy improvements on unseen domains compared to exemplar-free methods while maintaining competitive performance in standard settings.

## Executive Summary
MoP-CLIP addresses catastrophic forgetting in continual learning by introducing a prompt-based method that generalizes S-Prompts to handle both in-distribution and out-of-distribution data. The method learns class-wise feature distributions for each domain during training using individual text and visual prompts, and employs a mixture of prompt-tuned CLIP models at inference. It detects whether test samples come from known or unseen domains, selecting the correct prompt for known domains or using an ensemble of prompts for unseen domains. Experiments show MoP-CLIP outperforms state-of-the-art DIL methods in out-of-distribution scenarios.

## Method Summary
MoP-CLIP is a domain incremental learning method that uses prompt tuning to prevent catastrophic forgetting while generalizing to unseen domains. During training, it learns class-wise prototypes per domain along with domain-specific text and visual prompts. At inference, it detects whether samples are from known or unseen domains using Gaussian distribution of distances to prototypes, selecting the appropriate prompt or using an ensemble for OOD samples. The method leverages CLIP's vision and text encoders with learnable prompt tokens prepended to image and text embeddings.

## Key Results
- Outperforms state-of-the-art DIL methods in out-of-distribution scenarios
- Achieves accuracy improvements of 11-40% on unseen domains compared to exemplar-free methods
- Maintains competitive performance in standard in-distribution settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method detects whether a test sample comes from a known domain by comparing the distance to the nearest prototype against a Gaussian distribution of distances computed during training.
- Mechanism: For each domain and class, the method computes a Gaussian distribution (mean μ and standard deviation σ) of distances between features and their class prototypes. At inference, if the distance of a test sample to its nearest prototype falls within a threshold of this distribution (using cumulative distribution function F and threshold q), the sample is classified as in-distribution; otherwise, it's treated as out-of-distribution.
- Core assumption: The distances from samples to their class prototypes follow a Gaussian distribution within each domain, allowing reliable statistical detection of out-of-distribution samples.
- Evidence anchors:
  - [abstract] "At inference, the learned distributions allow us to identify whether a given test sample belongs to a known domain, selecting the correct prompt for the classification task, or from an unseen domain, leveraging a mixture of the prompt-tuned CLIP models."
  - [section 3.3] "During training, the distribution of distances for each domain Ds and class k is estimated from Ψk s with a Gaussian of mean μk s and standard deviation σk s."
  - [corpus] Weak - no direct evidence in corpus about this specific mechanism; likely novel approach.
- Break condition: If the Gaussian assumption fails (e.g., multimodal distributions or heavy tails), the detection threshold becomes unreliable, causing false positives/negatives in domain classification.

### Mechanism 2
- Claim: For out-of-distribution samples, the method uses an ensemble of prompts weighted by a Gaussian mixture model over distances to prototypes.
- Mechanism: When a sample is detected as OOD, the method computes weights ws for each seen domain s using a Gaussian mixture model on the distances to class prototypes. These weights represent the likelihood that the sample belongs to each domain. The final prediction is a weighted combination of predictions from all domain-specific prompts.
- Core assumption: The distance to the closest prototype for a sample from an unseen domain can be modeled as a mixture of the known domain distributions, allowing the method to infer which seen domains are most relevant.
- Evidence anchors:
  - [abstract] "At inference, the learned distributions allow us to identify whether a given test sample belongs to a known domain, selecting the correct prompt for the classification task, or from an unseen domain, leveraging a mixture of the prompt-tuned CLIP models."
  - [section 3.3] "We model zv as being part of a mixture of the known domains. In particular, we resort to a Gaussian mixture model to estimate the mixture weights (ws = p(s|x))."
  - [corpus] Weak - no direct evidence in corpus about this specific ensembling mechanism; appears to be novel.
- Break condition: If the unseen domain's distribution is too different from all seen domains, the mixture weights may be poorly distributed, leading to diluted or incorrect predictions.

### Mechanism 3
- Claim: Learning class-wise prototypes per domain (rather than domain-wise k-means centroids) provides more discriminative features for domain selection.
- Mechanism: Instead of using k-means to find domain centroids from all features, the method computes a prototype for each class in each domain by averaging the features of all samples of that class in that domain. This creates a set of class-specific prototypes per domain, which are more representative of the actual data distribution.
- Core assumption: Class-wise prototypes capture more specific and discriminative information about each domain than domain-wise centroids, improving the accuracy of domain selection.
- Evidence anchors:
  - [section 3.3] "we propose a strategy based on a set of class-specific prototypes for each domain, Es = {mk s }K k=1, instead of prototypes obtained with K-Means as in [38]."
  - [section 4.2] "To alleviate this issue, we instead use class-wise prototypes as a hyperparameter-free alternative to compute representative prototypes."
  - [corpus] Weak - no direct evidence in corpus about this specific choice; appears to be a novel design decision.
- Break condition: If class distributions within a domain are highly overlapping or noisy, class-wise prototypes may become less discriminative than domain centroids, potentially degrading domain selection performance.

## Foundational Learning

- Concept: Gaussian Mixture Models for density estimation
  - Why needed here: The method uses GMMs to estimate the distribution of distances between samples and prototypes, which is crucial for both domain detection and weight computation for OOD samples.
  - Quick check question: Can you explain how a GMM with multiple components can model complex distributions better than a single Gaussian?

- Concept: Cosine similarity in high-dimensional spaces
  - Why needed here: The method uses cosine similarity between visual and text embeddings to compute classification probabilities, which is fundamental to how CLIP-based models work.
  - Quick check question: Why is cosine similarity often preferred over Euclidean distance for comparing embeddings in CLIP models?

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The method addresses catastrophic forgetting by using prompt tuning instead of fine-tuning model parameters, preserving knowledge from previous domains.
  - Quick check question: What is catastrophic forgetting, and why does it occur in neural networks trained on sequential tasks?

## Architecture Onboarding

- Component map: CLIP Image Encoder (fθ) -> Visual Prompts (Pv) -> Visual Embeddings -> Distance Computation -> Domain Selection -> Text Prompts (Pt) -> Text Embeddings -> Cosine Similarity -> Classification
- Critical path: Sample → Feature Extraction → Distance Computation → Domain Classification → Prompt Selection → Prediction
- Design tradeoffs: Using class-wise prototypes instead of k-means centroids provides more discriminative features but increases storage requirements. Using Gaussian distributions for distance modeling simplifies computation but assumes normality.
- Failure signatures: Poor domain detection (high false positive/negative rate), degraded performance on OOD samples, sensitivity to threshold q.
- First 3 experiments:
  1. Test domain detection accuracy on ID samples with varying threshold q values
  2. Compare performance of class-wise prototypes vs k-means centroids for domain selection
  3. Evaluate OOD sample classification with and without the ensembling mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of MoP-CLIP's performance in highly diverse domain shifts, and how does it scale with the number of unseen domains?
- Basis in paper: [inferred] The paper demonstrates significant performance improvements on unseen domains compared to baseline methods, but does not explore the theoretical limits or scalability with increasing domain diversity.
- Why unresolved: The experiments focus on a limited number of unseen domains and do not provide a theoretical analysis of performance scaling with domain diversity.
- What evidence would resolve it: Empirical studies evaluating MoP-CLIP's performance across a wide range of domain shifts and increasing numbers of unseen domains, alongside theoretical bounds on its generalization capabilities.

### Open Question 2
- Question: How does the choice of the threshold q in the out-of-distribution detection mechanism impact the trade-off between in-distribution and out-of-distribution performance in different datasets?
- Basis in paper: [explicit] The paper mentions that q = 0.94 is empirically fixed for the three datasets based on an ablation study, but does not explore the sensitivity of this choice across different datasets or provide guidelines for selecting q.
- Why unresolved: The ablation study focuses on a single fixed value of q, and the paper does not discuss how to adapt this parameter for different datasets or domain characteristics.
- What evidence would resolve it: A comprehensive study of q's impact on performance across various datasets with different domain characteristics, providing insights into how to select q for optimal performance in diverse scenarios.

### Open Question 3
- Question: Can the Gaussian Mixture Model (GMM) approximation used in MoP-CLIP be further improved by exploring alternative distributions for modeling the distance to prototypes?
- Basis in paper: [explicit] The paper acknowledges that the GMM approximation assumes an isotropic Gaussian distribution and suggests that exploring other distributions, such as the Weibull Distribution or the Generalized Pareto Distribution, could be investigated in the future.
- Why unresolved: The paper does not explore alternative distributions for modeling the distance to prototypes, leaving the potential for improvement through more sophisticated modeling techniques.
- What evidence would resolve it: Empirical comparisons of MoP-CLIP's performance using different distributions for modeling the distance to prototypes, demonstrating the impact of distribution choice on out-of-distribution detection and classification accuracy.

### Open Question 4
- Question: How does MoP-CLIP's performance compare to methods that leverage self-supervised learning or unsupervised domain adaptation techniques for handling domain shifts in continual learning scenarios?
- Basis in paper: [inferred] The paper focuses on comparing MoP-CLIP to existing DIL methods, but does not explore the potential benefits of incorporating self-supervised learning or unsupervised domain adaptation techniques for handling domain shifts.
- Why unresolved: The experiments are limited to comparing MoP-CLIP with traditional DIL methods, and the paper does not investigate the potential synergies between MoP-CLIP and self-supervised or unsupervised domain adaptation approaches.
- What evidence would resolve it: Empirical studies comparing MoP-CLIP's performance with methods that incorporate self-supervised learning or unsupervised domain adaptation techniques, highlighting the potential benefits and limitations of combining these approaches for handling domain shifts in continual learning.

## Limitations

- Gaussian distribution assumption for prototype distances may not hold for complex or multimodal data distributions
- Performance depends critically on the similarity between unseen and seen domains, with potential degradation for extreme distribution shifts
- Limited exploration of threshold q sensitivity and alternative distributions for distance modeling

## Confidence

- Mechanism 1 (Domain Detection): **Medium** - The Gaussian modeling approach is well-established, but validation of the normality assumption on real data is limited
- Mechanism 2 (OOD Ensembling): **Medium** - The mathematical formulation is sound, but performance sensitivity to domain similarity is not thoroughly characterized
- Mechanism 3 (Class-wise Prototypes): **Medium** - The design choice is logical but lacks ablation studies comparing against domain-wise alternatives
- Overall Claims: **Medium** - Strong empirical results on standard benchmarks, but limited OOD evaluation diversity

## Next Checks

1. **Distribution Validation**: Perform Kolmogorov-Smirnov tests on prototype distances across all domains to quantify how well the Gaussian assumption holds in practice

2. **Threshold Sensitivity Analysis**: Systematically vary the domain selection threshold q and measure its impact on both in-distribution accuracy and false-positive rates for domain classification

3. **Extreme OOD Testing**: Evaluate performance on synthetic OOD datasets with progressively increasing distributional distance from seen domains to identify breaking points in the ensemble weighting mechanism