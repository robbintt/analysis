---
ver: rpa2
title: 'Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method'
arxiv_id: '2310.17918'
source_url: https://arxiv.org/abs/2310.17918
tags:
- arxiv
- question
- llms
- questions
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel self-detection method to identify questions
  that large language models (LLMs) do not know the answers to. The core idea is to
  leverage the inconsistency of LLM responses to paraphrased versions of the same
  question as an indicator of unknown knowledge.
---

# Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method

## Quick Facts
- arXiv ID: 2310.17918
- Source URL: https://arxiv.org/abs/2310.17918
- Reference count: 11
- The paper proposes a self-detection method to identify questions LLMs do not know by examining response inconsistency across paraphrased versions

## Executive Summary
This paper introduces a novel approach to detecting unknown knowledge in large language models by leveraging the inconsistency of responses to paraphrased versions of the same question. The core insight is that when LLMs lack knowledge about a topic, their responses to semantically equivalent questions will diverge. The method generates multiple paraphrased questions, collects LLM responses, and computes entropy across answer clusters to identify unknown questions. Experiments across multiple LLM architectures (ChatGPT, GPT-3.5, GPT-4, Vicuna) and task types demonstrate the effectiveness of this approach with PR-AUC scores ranging from 0.744 to 0.855.

## Method Summary
The self-detection method operates by first generating multiple paraphrased versions of a given question using the LLM itself with high-temperature sampling. Each paraphrased question is then submitted to the LLM with greedy decoding to obtain responses. The LLM is prompted to evaluate pairwise consistency between all generated answers, determining whether they are "Same" or "Contradicted". Consistent answers are clustered, and the entropy of the resulting cluster distribution serves as an uncertainty score. Questions with higher entropy scores are classified as unknown to the LLM. The method requires no external resources or training, relying entirely on the LLM's own capabilities for paraphrasing, response generation, and consistency detection.

## Key Results
- PR-AUC scores of 0.855, 0.744, and 0.762 across factoid QA, arithmetic reasoning, and commonsense reasoning tasks respectively
- Method demonstrates effectiveness across multiple LLM architectures including ChatGPT, GPT-3.5, GPT-4, and Vicuna
- Performance correlates with knowledge popularity, with unknown questions showing significantly fewer search results
- Method successfully identifies knowledge gaps without requiring external resources or training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Inconsistent responses to paraphrased versions of the same question reveal LLM knowledge gaps.
- Mechanism: When an LLM lacks knowledge about a topic, its responses to semantically equivalent questions in different verbalizations will diverge. The entropy of the answer distribution across paraphrases serves as an uncertainty score.
- Core assumption: LLMs are sensitive to question verbalization when they lack factual knowledge about the topic.
- Evidence anchors:
  - [abstract] "we examine the divergencies between the generated answers to identify the questions that the model may generate falsehoods"
  - [section 2] "Based on the observation, this paper proposes to leverage the inconsistency between the responses to different verbalizations of the same questions"
- Break condition: If LLM responses are consistently wrong but internally consistent across paraphrases, this method will miss knowledge gaps.

### Mechanism 2
- Claim: Self-consistency of generated answers can be computed without external resources.
- Mechanism: The LLM itself is prompted to determine whether pairs of generated answers are "Same" or "Contradicted", then clustering algorithms group consistent answers to compute entropy.
- Core assumption: LLMs have sufficient logical reasoning ability to judge consistency between their own outputs.
- Evidence anchors:
  - [section 4.3] "For question answering and arithmetic reasoning, we use the LLM itself to handle the inconsistency detection by asking whether the two answers are the same or contradicted"
  - [section 4.3] "This task is a strength of the latest LLMs even in a zero-shot measure as it demands basic logical reasoning abilities"
- Break condition: If LLM cannot accurately judge consistency between answers, entropy scores will be unreliable.

### Mechanism 3
- Claim: Knowledge popularity correlates with detection performance.
- Mechanism: Unknown questions tend to be about less popular knowledge, which shows up as lower search result counts and higher response divergence.
- Core assumption: LLMs have stronger memorization for frequently discussed topics.
- Evidence anchors:
  - [section 5.3] "we consult search engines... We use the number of returned search results as an indicator of the popularity of the knowledge"
  - [section 5.3] "the number of search results for unknown questions is significantly lower than for known questions"
- Break condition: If popularity doesn't correlate with knowledge retention in the specific LLM, this pattern won't hold.

## Foundational Learning

- Concept: Entropy as a measure of uncertainty
  - Why needed here: Entropy quantifies the divergence between generated responses, serving as the core uncertainty score
  - Quick check question: If an LLM gives 4 identical answers to paraphrased questions, what is the entropy of the answer distribution?

- Concept: Clustering algorithms for grouping similar responses
  - Why needed here: Clustering identifies consistent answer groups to calculate entropy across the response distribution
  - Quick check question: If you have 10 answers forming 3 clusters with sizes 4, 4, and 2, what is the cluster distribution?

- Concept: Paraphrasing and semantic equivalence
  - Why needed here: The method relies on generating semantically equivalent but verbally distinct questions
  - Quick check question: If a question is "What is the capital of France?" what would be a valid paraphrased version that maintains semantic equivalence?

## Architecture Onboarding

- Component map:
  Input question → Paraphrasing module → Response generation → Consistency detection → Clustering → Entropy calculation → Detection output
- Critical path:
  Paraphrasing → Response generation → Consistency detection → Clustering → Entropy calculation
  This path must complete within acceptable latency for practical use
- Design tradeoffs:
  - Number of paraphrases vs. detection accuracy (more paraphrases improve detection but increase cost)
  - Temperature settings for paraphrasing (higher temperature increases diversity but may introduce semantic drift)
  - Clustering algorithm choice (impacts precision and computational cost)
- Failure signatures:
  - High false negatives: Method misses knowledge gaps when LLM gives consistently wrong answers
  - High false positives: Method flags known questions due to generation randomness
  - Performance degradation: Inconsistent consistency detection or clustering failures
- First 3 experiments:
  1. Baseline accuracy: Test on FaVIQ dataset with known/unknown ground truth to establish PR-AUC baseline
  2. Paraphrase count sweep: Measure detection performance with 5, 10, 20, and 30 paraphrases to find optimal tradeoff
  3. Cross-model validation: Run same questions through ChatGPT, GPT-3.5, and GPT-4 to compare detection patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed self-detection method perform on other types of tasks beyond factoid question answering, commonsense reasoning, and arithmetic reasoning?
- Basis in paper: [explicit] The authors mention evaluating the method on these three types of tasks, but do not explore other potential applications.
- Why unresolved: The paper focuses on a limited set of tasks, leaving the generalizability of the method to other domains unexplored.
- What evidence would resolve it: Conducting experiments on a wider range of tasks, such as text summarization, machine translation, or code generation, would provide insights into the method's broader applicability.

### Open Question 2
- Question: How does the performance of the self-detection method vary with different levels of randomness in the LLM's responses?
- Basis in paper: [inferred] The authors mention using a greedy decoding strategy to minimize randomness, but do not investigate the impact of varying randomness levels.
- Why unresolved: The effect of randomness on the self-detection method's accuracy is not explored, which could be crucial for understanding its limitations and potential improvements.
- What evidence would resolve it: Experimenting with different randomness levels in the LLM's responses and evaluating the self-detection method's performance under each condition would shed light on this relationship.

### Open Question 3
- Question: How does the self-detection method handle cases where the LLM generates consistently incorrect but confident responses?
- Basis in paper: [explicit] The authors acknowledge that the method cannot detect cases where the model generates consistently but incorrectly, leading to false negatives.
- Why unresolved: The paper does not provide a solution or mitigation strategy for this limitation, which is crucial for improving the method's overall effectiveness.
- What evidence would resolve it: Developing and testing additional techniques to identify consistently incorrect but confident responses would address this limitation and enhance the self-detection method's reliability.

## Limitations

- The method cannot detect cases where the LLM generates consistently incorrect but internally coherent answers across paraphrased questions
- Performance depends critically on the LLM's ability to generate semantically equivalent paraphrases without introducing semantic drift
- The approach assumes that binary classification of questions as "known" or "unknown" based on search results accurately reflects LLM knowledge boundaries

## Confidence

**High confidence**: The observation that LLM responses to paraphrased questions show divergence patterns for unknown knowledge is well-supported by the experimental results showing PR-AUC scores of 0.855, 0.744, and 0.762 across different tasks.

**Medium confidence**: The claim that entropy of answer distributions serves as an effective uncertainty score. While results are promising, the method's performance relative to alternative uncertainty quantification approaches is not explored.

**Medium confidence**: The claim that the method requires no external resources or training. While technically true, the method depends on the LLM's inherent capabilities for paraphrasing and consistency detection, which may vary across models and domains.

## Next Checks

1. **Robustness to paraphrase quality**: Systematically vary the temperature and quality of paraphrased questions to determine the method's sensitivity to semantic drift. Measure detection performance when paraphrases maintain vs. break semantic equivalence.

2. **Cross-task generalization**: Apply the method to LLM tasks beyond question answering and reasoning (e.g., code generation, text summarization) to validate whether response inconsistency remains a reliable indicator of unknown knowledge across different domains.

3. **Alternative uncertainty baselines**: Compare the entropy-based detection method against established uncertainty quantification approaches (Monte Carlo dropout, ensemble methods, temperature scaling) to establish relative effectiveness and identify complementary strategies.