---
ver: rpa2
title: 'NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit
  Neural Networks'
arxiv_id: '2311.00428'
source_url: https://arxiv.org/abs/2311.00428
tags:
- adversarial
- exit
- attack
- exits
- multi-exit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of improving adversarial robustness
  in multi-exit neural networks, where high correlation among submodels leads to strong
  adversarial transferability. The proposed NEO-KD method combines neighbor knowledge
  distillation (NKD) and exit-wise orthogonal knowledge distillation (EOKD) to address
  this issue.
---

# NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit Neural Networks

## Quick Facts
- arXiv ID: 2311.00428
- Source URL: https://arxiv.org/abs/2311.00428
- Reference count: 40
- Primary result: NEO-KD achieves best adversarial accuracy with reduced computation budgets compared to baselines in multi-exit networks

## Executive Summary
This paper addresses the challenge of improving adversarial robustness in multi-exit neural networks, where high correlation among submodels leads to strong adversarial transferability. The proposed NEO-KD method combines neighbor knowledge distillation (NKD) and exit-wise orthogonal knowledge distillation (EOKD) to address this issue. NKD guides the output of adversarial examples to follow the ensemble outputs of neighbor exits of clean data, while EOKD reduces adversarial transferability by encouraging non-ground-truth predictions of individual exits to be mutually orthogonal. Experiments on various datasets show that NEO-KD achieves the best adversarial accuracy with reduced computation budgets compared to baselines.

## Method Summary
NEO-KD combines neighbor knowledge distillation (NKD) and exit-wise orthogonal knowledge distillation (EOKD) to improve adversarial robustness in multi-exit neural networks. For NKD, each exit distills the averaged predictions of its two neighbor exits computed on clean inputs to its corresponding output on adversarial inputs. For EOKD, non-ground-truth predictions are assigned disjoint subsets of classes to each exit, normalized as soft labels, and distilled back to that exit's adversarial output. The method is evaluated on MNIST, CIFAR-10/100, Tiny-ImageNet, and ImageNet using SmallCNN and MSDNet architectures with adversarial examples generated via PGD attack.

## Key Results
- NEO-KD achieves highest adversarial accuracy across multiple datasets and attack types
- Combines NKD and EOKD for complementary robustness gains beyond individual components
- Reduces computation budgets while maintaining or improving adversarial robustness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NKD improves robustness by guiding adversarial outputs toward high-quality ensemble predictions of clean data
- Mechanism: For each exit, NKD distills the averaged predictions of its two neighbor exits computed on clean inputs to the corresponding exit's output on adversarial inputs
- Core assumption: The ensemble of neighbor exits on clean data contains richer, more robust features than any single exit alone
- Evidence anchors:
  - [abstract]: "NEO-KD first resorts to neighbor knowledge distillation to guide the output of the adversarial examples to tend to the ensemble outputs of neighbor exits of clean data."
  - [section 3.2]: "Different from previous self-knowledge distillation methods, for each exit i, NKD generates a teacher prediction by ensembling (averaging) the neighbor predictions... before distillation, NKD provides a higher quality feature of the original data..."
- Break condition: If neighbor exits themselves are corrupted or misaligned, the ensemble may not be more robust than individual exits

### Mechanism 2
- Claim: EOKD reduces adversarial transferability by encouraging non-ground-truth predictions to be mutually orthogonal across exits
- Mechanism: During exit-wise distillation, EOKD randomly assigns disjoint subsets of non-ground-truth classes to each exit, normalizes these soft labels, and distills them back to that exit's adversarial output
- Core assumption: Forcing different exits to focus on disjoint class subsets reduces parameter correlation and weakens transferability
- Evidence anchors:
  - [abstract]: "EOKD reduces adversarial transferability by encouraging non-ground-truth predictions of individual exits to be mutually orthogonal."
  - [section 3.2]: "EOKD provides orthogonal soft labels to each exit for the non-ground-truth predictions... We randomly allocate the classes of non-ground-truth predictions to each exit for every epoch..."
- Break condition: If the class subsets overlap or if ground-truth class is not consistently preserved, orthogonality may degrade

### Mechanism 3
- Claim: Combining NKD and EOKD yields complementary robustness gains not achievable by either alone
- Mechanism: NKD supplies high-quality teacher signals; EOKD breaks correlation patterns. Together they improve both feature quality and inter-exit independence
- Core assumption: Robustness gains are additive when feature quality and inter-exit orthogonality are both improved
- Evidence anchors:
  - [section 4.3]: "It shows that combining NKD and EOKD boosts up the performance beyond the sum of their original gains."
  - [section 3.2]: "The NKD and EOKD components... work together to reduce adversarial transferability... while correctly guiding the predictions..."
- Break condition: If either component's effect is saturated or if hyperparameter tuning is poor, synergy may vanish

## Foundational Learning

- Concept: Knowledge distillation
  - Why needed here: To transfer robust representations from clean to adversarial samples within the same network
  - Quick check question: What loss function is used to measure distillation quality in NEO-KD?
- Concept: Adversarial transferability
  - Why needed here: Multi-exit networks share parameters, so attacks on one exit degrade others; reducing this is central to robustness
  - Quick check question: How does NEO-KD measure adversarial transferability across exits?
- Concept: Ensemble of neighbor predictions
  - Why needed here: NKD uses neighbor exits' clean predictions as teacher signals; understanding averaging and ensembling is key
  - Quick check question: In NKD, which exits are used to form the teacher signal for exit i?

## Architecture Onboarding

- Component map:
  - Input → Shared feature extractor → Multiple exits (blocks + classifiers)
  - Each exit i has submodel θᵢ = [ϕ₁,…,ϕᵢ, wᵢ]
  - NKD module: neighbor averaging + distillation loss
  - EOKD module: orthogonal class subset assignment + distillation loss
- Critical path:
  1. Generate adversarial examples (max-average or average attack)
  2. For each exit i, compute neighbor ensemble on clean data
  3. Distill neighbor ensemble to exit i's adversarial output (NKD)
  4. Assign orthogonal non-ground-truth class subsets to each exit
  5. Distill orthogonal labels to exit i's adversarial output (EOKD)
  6. Combine clean loss, adversarial loss, NKD loss, and EOKD loss
- Design tradeoffs:
  - Neighbor choice: neighbors provide richer signals but more correlation; too far reduces quality
  - Orthogonal class subsets: more classes per exit increases representational capacity but reduces orthogonality
  - Hyperparameter α, β: balance between NKD and EOKD strength
- Failure signatures:
  - High adversarial transferability map values → EOKD not effective
  - Stagnant clean accuracy → NKD loss too strong
  - Low adversarial accuracy → either NKD/EOKD weights too low or attack too strong
- First 3 experiments:
  1. Train with NKD only; measure adversarial accuracy vs. baseline
  2. Train with EOKD only; measure transferability reduction
  3. Combine NKD + EOKD; compare both metrics to steps 1 and 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does NEO-KD's performance scale with the number of exits in multi-exit neural networks?
- Basis in paper: [inferred] The paper only tested NEO-KD with up to 7 exits. It would be interesting to see how the method performs with a larger number of exits, which is common in some real-world applications
- Why unresolved: The paper does not provide experimental results with a large number of exits. Scaling up the number of exits could introduce new challenges for NEO-KD, such as increased computational complexity and potential degradation of performance
- What evidence would resolve it: Experiments evaluating NEO-KD's performance with a large number of exits (e.g., 10, 20, or more) would provide insights into the method's scalability and limitations

### Open Question 2
- Question: Can NEO-KD be extended to handle multi-task learning scenarios where different exits are responsible for different tasks?
- Basis in paper: [inferred] The paper focuses on multi-exit neural networks for single-task classification. It would be valuable to investigate how NEO-KD can be adapted to handle multi-task learning scenarios where different exits are specialized for different tasks
- Why unresolved: The paper does not explore the application of NEO-KD to multi-task learning. Extending NEO-KD to handle multiple tasks could require modifications to the knowledge distillation and orthogonalization mechanisms
- What evidence would resolve it: Experiments evaluating NEO-KD's performance on multi-task learning datasets and comparing it to existing multi-task learning methods would demonstrate the effectiveness of the adapted approach

### Open Question 3
- Question: How does NEO-KD's performance compare to other adversarial training methods that are specifically designed for multi-exit networks?
- Basis in paper: [explicit] The paper compares NEO-KD to several baselines, including Adv. w/o Distill, SKD, ARD, and LW. However, it does not compare to other adversarial training methods that are specifically designed for multi-exit networks, such as the methods proposed in [3] and [12]
- Why unresolved: The paper does not provide a comprehensive comparison with all relevant adversarial training methods for multi-exit networks. Including these methods in the comparison would provide a more complete picture of NEO-KD's performance relative to the state-of-the-art
- What evidence would resolve it: Experiments comparing NEO-KD to other adversarial training methods for multi-exit networks on various datasets would demonstrate the relative strengths and weaknesses of each approach

## Limitations

- NKD's effectiveness depends on the quality of neighbor ensemble predictions, which may fail if neighbor exits are compromised
- EOKD requires careful hyperparameter tuning of orthogonal class subsets and may interrupt adversarial training if β is too large
- The method introduces computational overhead during training due to multiple distillation losses

## Confidence

- **High confidence** in the theoretical framework and mechanism descriptions
- **Medium confidence** in experimental results due to limited ablation studies on hyperparameter sensitivity
- **Low confidence** in real-world deployment without extensive hyperparameter tuning

## Next Checks

1. Perform sensitivity analysis on β parameter in EOKD across different dataset sizes and exit configurations
2. Test NEO-KD's robustness against transfer attacks from models trained with different defense methods
3. Evaluate computational overhead during inference compared to standard multi-exit networks