---
ver: rpa2
title: A General Recipe for Automated Machine Learning in Practice
arxiv_id: '2308.15647'
source_url: https://arxiv.org/abs/2308.15647
tags:
- learning
- machine
- automl
- data
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a framework for building general Automated
  Machine Learning (AutoML) systems in practice. Through a narrative review of the
  main approaches in the area, the authors distill fundamental concepts to support
  a single design.
---

# A General Recipe for Automated Machine Learning in Practice

## Quick Facts
- arXiv ID: 2308.15647
- Source URL: https://arxiv.org/abs/2308.15647
- Reference count: 40
- Primary result: A three-loop framework (scheduling, meta, training) for building practical AutoML systems that optimize both predictive performance and resource efficiency

## Executive Summary
This paper proposes a framework for building general Automated Machine Learning (AutoML) systems by distilling fundamental concepts from a narrative review of main approaches in the area. The authors identify three main learning loops in AutoML systems: scheduling, meta, and training loops, each operating at different time scales and scopes. The framework addresses the combined algorithm selection and hyperparameter optimization problem from a multi-objective perspective, considering time and computational resources as objectives to minimize alongside generalization error.

## Method Summary
The paper synthesizes insights from 19 recent AutoML papers to propose a three-loop architecture: the scheduling loop explores search space and decides which configurations to evaluate; the meta loop enables learning from experience to improve exploration efficiency; and the training loop adapts configurations during single trial training. The authors formulate AutoML as a multi-objective optimization problem that minimizes generalization error while also minimizing time and computational resources, reflecting real-world constraints more accurately than single-objective formulations.

## Key Results
- Identifies three distinct learning loops in AutoML systems that operate at different time scales
- Proposes treating AutoML as a multi-objective optimization problem with error, time, and resource objectives
- Provides a conceptual framework for integrating latest AutoML research into practical systems

## Why This Works (Mechanism)

### Mechanism 1
The three-loop architecture (scheduling, meta, training) allows modular integration of AutoML methods by clearly separating exploration, learning from experience, and in-training adaptation. Each loop operates at a different time scale and scope, enabling independent optimization and clearer fault isolation.

### Mechanism 2
Modeling AutoML as a multi-objective optimization (minimize generalization error, time, and computational resources) better reflects real-world constraints than single-objective formulations. By explicitly treating time and resources as objectives, the scheduler can balance predictive performance against efficiency, enabling more practical deployments.

### Mechanism 3
The feedback loop structure enables learning from past evaluations to improve future search efficiency, embodying the essence of meta-learning in AutoML. Results from each configuration evaluation are stored in history and consumed by the meta loop to guide the scheduler's future decisions.

## Foundational Learning

- **Hyperparameter optimization and search spaces**: Why needed - AutoML fundamentally reduces to searching over model configurations; understanding search spaces is essential to grasp the scheduler's role. Quick check - What are the two main components of a search space in AutoML?
- **Multi-objective optimization**: Why needed - The paper's novel contribution is treating AutoML as a multi-objective problem; knowing how to balance conflicting objectives is key. Quick check - In the context of AutoML, what are the three objectives being minimized?
- **Meta-learning and meta-features**: Why needed - The meta loop relies on learning from past tasks; understanding how tasks are represented and how knowledge is transferred is critical. Quick check - What are meta-features and why are they important for the meta loop?

## Architecture Onboarding

- **Component map**: Scheduler (controls outer loop) -> Meta Loop (learns from history) -> Training Loop (adapts during training) -> Objective Function (multi-objective minimization) -> History Store (persists results)
- **Critical path**: 1. Scheduler selects configuration and evaluation plan 2. Configuration is evaluated (training + testing) 3. Results stored in history 4. Meta loop updates knowledge from history 5. Next scheduler iteration uses updated knowledge
- **Design tradeoffs**: Modularity vs. communication overhead between loops; granularity of configuration space vs. search efficiency; balancing exploration vs. exploitation in scheduler
- **Failure signatures**: Scheduler stuck in local optima (scheduler not exploring enough); Meta loop not improving (poor task representation or insufficient history); Training loop ineffective (adaptive configurations not beneficial)
- **First 3 experiments**: 1. Implement a simple scheduler with random search and single objective (error only) to validate basic loop 2. Add meta loop that learns from past runs to bias future searches (e.g., using meta-features) 3. Integrate a training loop that adapts learning rates during training and measures impact on overall performance

## Open Questions the Paper Calls Out

### Open Question 1
How can we effectively quantify and optimize the trade-off between maximizing predictive performance and minimizing time and computational resource consumption in AutoML systems? Basis in paper - The paper proposes modeling AutoML as a multi-objective optimization problem. Why unresolved - Determining the optimal balance between predictive performance and resource consumption is complex and may vary depending on the specific application and context.

### Open Question 2
How can we effectively integrate and automate the Task Formulation and Prediction Engineering phases in AutoML systems? Basis in paper - The paper identifies Task Formulation and Prediction Engineering as the least explored areas in AutoML. Why unresolved - These phases involve complex decision-making and domain expertise that are challenging to automate and require further research.

### Open Question 3
How can we develop more efficient and scalable methods for meta-learning in AutoML systems? Basis in paper - The paper discusses the importance of the Meta-Loop in AutoML. Why unresolved - Meta-learning is a complex problem that requires further research to develop effective methods for task representation, knowledge transfer, and efficient learning algorithms.

## Limitations

- Framework remains conceptual with limited empirical validation
- Specific implementation details of meta-learning mechanisms are not provided
- Multi-objective optimization approach lacks demonstration of practical trade-off balancing

## Confidence

- **High confidence**: The identification of three distinct learning loops as a structural framework for AutoML systems
- **Medium confidence**: The claim that separating loops into scheduling, meta, and training provides modularity benefits
- **Medium confidence**: The assertion that multi-objective optimization better reflects real-world constraints

## Next Checks

1. **Implement and benchmark**: Build a minimal working AutoML system following the three-loop architecture and compare its performance against established systems on standard tabular datasets, measuring both accuracy and resource usage.

2. **Test meta-learning generalization**: Evaluate the meta loop's ability to transfer knowledge across different problem types by measuring performance degradation when training on one domain and testing on another.

3. **Analyze loop coupling effects**: Systematically measure the communication overhead and potential interference between loops by implementing both tightly-coupled and loosely-coupled versions, then quantifying the impact on search efficiency and final model quality.