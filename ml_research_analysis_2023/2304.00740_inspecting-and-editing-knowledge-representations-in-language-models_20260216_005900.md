---
ver: rpa2
title: Inspecting and Editing Knowledge Representations in Language Models
arxiv_id: '2304.00740'
source_url: https://arxiv.org/abs/2304.00740
tags:
- remedi
- language
- entity
- context
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces REMEDI, a method for learning to map natural
  language statements to fact encodings in language models' internal representations.
  The method learns affine transformations that, when applied to entity representations,
  edit the model's factual knowledge and influence downstream generation.
---

# Inspecting and Editing Knowledge Representations in Language Models

## Quick Facts
- arXiv ID: 2304.00740
- Source URL: https://arxiv.org/abs/2304.00740
- Reference count: 40
- Primary result: REMEDI learns to map natural language statements to model-internal fact encodings, enabling both editing of LM outputs and probing of knowledge representations

## Executive Summary
This paper introduces REMEDI, a method for learning to map natural language statements to fact encodings in language models' internal representations. The method learns affine transformations that, when applied to entity representations, edit the model's factual knowledge and influence downstream generation. REMEDI encodings can be used both as knowledge editors—modifying model outputs to be consistent with new facts—and as probes—revealing which properties models attribute to entities and predicting when they will generate incorrect outputs. The approach is evaluated on tasks involving both context-specific knowledge (patching errors in entity descriptions) and general background knowledge (updating facts about famous entities and redefining concepts). Results show REMEDI successfully controls model behavior even when textual prompting fails, achieving over 98% efficacy at eliciting consistent generations in factual editing tasks, while preserving fluency and entity essence better than fine-tuning or prefixing baselines.

## Method Summary
REMEDI learns affine transformations F(hentity, hattr) = hentity + Whattr + b that map textual attribute descriptions to vector directions in model representation space. The method is trained to maximize the probability that a language model generates target tokens consistent with the attribute when the transformation is applied to entity representations. The approach uses cross-entropy loss on target generation combined with KL divergence regularization to preserve original entity features. REMEDI is evaluated across three datasets: COUNTER FACT (entity-relation edits), Bias in Bios (occupation attributes), and McRae Norms (concept features). The method operates by inserting edited representations at specified transformer layers and measuring generation quality through accuracy, fluency, and essence preservation metrics.

## Key Results
- REMEDI achieves over 98% efficacy at eliciting consistent generations when editing factual knowledge in language models
- The method outperforms prompting baselines (prefixing) and performs comparably to fine-tuning while being more efficient
- REMEDI can predict context-ignoring behavior with F1 scores up to 0.74 by comparing learned attribute directions to unmodified representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: REMEDI works by learning affine transformations that add directional edits to entity representations, causing the LM to generate text consistent with new attributes.
- Mechanism: The method learns a function F(hentity, hattr) = hentity + Whattr + b where Whattr + b encodes the target attribute direction. Adding this direction to an entity's hidden state modifies downstream generation probabilities.
- Core assumption: Entity representations encode factual attributes in a linearly decodable space, and adding learned directions in this space causally influences generation.
- Evidence anchors:
  - [abstract]: "REMEDI learns a map from textual queries to fact encodings in an LM's internal representation system. These encodings can be used as knowledge editors: by adding them to LM hidden representations, we can modify downstream generation to be consistent with new facts."
  - [section 2]: "Following Li et al. (2021), we obtain a vector representation hattr by averaging the LM's encoding of tattr. We then train the editor F to maximize the probability that pLM assigns to ttgt after modifying the hidden representation of xentity"
  - [corpus]: Weak - no direct citation of this specific affine transformation mechanism in related papers
- Break condition: If entity representations do not encode attributes in a linearly separable manner, or if the causal relationship between representations and generation is broken.

### Mechanism 2
- Claim: REMEDI can detect failures of context mediation by comparing learned attribute directions to unmodified entity representations.
- Mechanism: By computing the dot product h^T_entity * d_attr (where d_attr = Wh_attr + b), REMEDI measures how strongly an entity's representation aligns with the target attribute direction. Misalignment indicates the LM ignored context.
- Core assumption: The learned directions d_attr capture the LM's internal encoding of attributes, and dot product similarity reflects attribute presence in representations.
- Evidence anchors:
  - [abstract]: "REMEDI encodings may also be used as probes: by comparing them to LM representations, they reveal which properties LMs already attribute to mentioned entities"
  - [section 5]: "We may then quantify how strongly an LM 'believes' the attribute to be true of the entity by computing: h^T_entity * d_attr"
  - [corpus]: Weak - related probing work exists but not specifically this dot product comparison method
- Break condition: If the learned directions don't capture attribute encoding, or if dot product similarity doesn't correlate with attribute presence.

### Mechanism 3
- Claim: REMEDI preserves entity essence while editing by balancing target attribute insertion with original feature preservation.
- Mechanism: The LKL loss term prevents dramatic changes to intermediate token distributions, maintaining fluency and entity identity while promoting target attribute generation.
- Core assumption: The KL divergence between pre-edit and post-edit distributions captures changes to entity essence and generation quality.
- Evidence anchors:
  - [section 2]: "to prevent the degenerate solution in which the language model always (and only) predict ttgt, we penalize the language model for changing its distributions on all tokens between the entity mention and the time at which it predict ttgt"
  - [section 4.2]: "Essence... captures how much the edited entity is still 'itself' according to the model"
  - [corpus]: Weak - no direct evidence this specific KL-based preservation mechanism is what maintains essence
- Break condition: If the KL term doesn't adequately capture essence preservation, or if other factors dominate the edit process.

## Foundational Learning

- Concept: Linear probing and representation editing
  - Why needed here: REMEDI builds on prior work showing factual knowledge is linearly decodable from LM representations, and extends this to causal editing rather than just classification
  - Quick check question: How does linear probing differ from causal editing in neural network representations?

- Concept: Autoregressive language modeling
  - Why needed here: REMEDI operates within the autoregressive generation framework, modifying hidden states that influence next-token probabilities
  - Quick check question: What is the relationship between hidden state modifications and next-token probability distributions in autoregressive models?

- Concept: Representation space geometry
  - Why needed here: REMEDI assumes entity attributes exist as directions in representation space that can be added to modify generation
  - Quick check question: How do linear transformations in representation space translate to semantic changes in model outputs?

## Architecture Onboarding

- Component map: Entity mention extraction -> Attribute text encoding -> REMEDI transformation training -> Hidden state insertion -> Generation -> Evaluation
- Critical path: Entity mention → Attribute encoding → REMEDI transformation training → Hidden state insertion → Generation → Evaluation
- Design tradeoffs:
  - Linear vs. nonlinear transformations: Linear is simpler but may miss complex attribute encodings
  - Layer selection: Earlier layers for overriding prior knowledge vs. later layers for fine-tuning
  - Training objectives: Balancing target generation vs. preserving original features
- Failure signatures:
  - Low efficacy: REMEDI direction doesn't influence generation (wrong layer, poor training)
  - Low fluency: KL term insufficient or over-constrained
  - Low essence: Too aggressive editing, losing original entity features
  - High variance: Inconsistent performance across entities or attributes
- First 3 experiments:
  1. Baseline: Measure GPT-J accuracy on Bias in Bios without any edits to establish context mediation failure rate
  2. Layer ablation: Train REMEDI at each layer and evaluate generation metrics to find optimal insertion point
  3. Control comparison: Compare REMEDI to prefixing baseline on COUNTER FACT to measure editing advantage over prompting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the limitations of REMEDI in handling complex or multi-faceted entity attributes beyond simple factual statements?
- Basis in paper: [explicit] The paper mentions that REMEDI is primarily designed for handling factual attributes in the form of (entity, attribute) pairs, but it also discusses potential future research directions to generalize REMEDI beyond these limitations.
- Why unresolved: The paper does not provide detailed experiments or analysis on REMEDI's performance with complex attributes, such as those involving nuanced relationships or multi-step reasoning.
- What evidence would resolve it: Conducting experiments with diverse and complex attributes, such as those requiring multi-step reasoning or understanding of nuanced relationships, and analyzing REMEDI's performance in these scenarios.

### Open Question 2
- Question: How does REMEDI's performance scale with the size and complexity of the language model it is applied to?
- Basis in paper: [explicit] The paper mentions that REMEDI is tested on GPT-J, a 6B parameter model, but it does not discuss how its performance might change with larger or smaller models.
- Why unresolved: The paper does not provide a systematic study of REMEDI's performance across different model sizes or architectures, which could reveal important insights into its scalability and effectiveness.
- What evidence would resolve it: Conducting experiments with REMEDI on language models of varying sizes and architectures, and comparing its performance across these different settings.

### Open Question 3
- Question: Can REMEDI be extended to handle non-factual knowledge, such as opinions, preferences, or emotional states of entities?
- Basis in paper: [explicit] The paper discusses REMEDI's application to factual knowledge, but it also suggests future research directions to generalize it beyond factual knowledge.
- Why unresolved: The paper does not provide any experiments or analysis on REMEDI's ability to handle non-factual knowledge, which could be a significant extension of its capabilities.
- What evidence would resolve it: Developing and testing REMEDI's application to non-factual knowledge, such as opinions, preferences, or emotional states, and evaluating its effectiveness in these domains.

## Limitations
- The evaluation relies on three datasets that may not fully capture the diversity of factual knowledge in real-world scenarios
- All experiments are conducted exclusively on GPT-J (6B parameters), limiting generalizability to other architectures
- The linear transformation assumption may be too simplistic for capturing complex attribute encodings

## Confidence

**High Confidence**:
- REMEDI can successfully edit factual knowledge in language models with >98% efficacy on the tested datasets
- REMEDI outperforms prompting baselines (prefixing) and performs comparably to fine-tuning while being more efficient
- The learned directions capture some aspects of attribute encoding as evidenced by F1 scores up to 0.74 on probing tasks

**Medium Confidence**:
- REMEDI preserves entity essence better than fine-tuning or prefixing (the tf-idf similarity metric may not fully capture semantic preservation)
- The method can predict context-ignoring behavior through dot product similarity measures (correlation between alignment scores and actual generation behavior needs more validation)
- The approach generalizes across different types of knowledge (entity facts, occupations, concept features) despite using different datasets

**Low Confidence**:
- REMEDI directions encode rich semantic knowledge beyond simple attribute addition (the linear transformation assumption may be too simplistic)
- The method scales to arbitrary knowledge editing tasks without architectural modifications (no testing on larger or different model architectures)
- KL divergence regularization adequately prevents catastrophic forgetting of original entity features (the preservation mechanism's effectiveness across diverse knowledge types is untested)

## Next Checks

**Check 1: Cross-Model Generalization Test**
Train REMEDI on GPT-J using COUNTER FACT, then evaluate on GPT-3 (175B) and OPT (175B) without retraining. Measure efficacy, fluency, and essence metrics to determine if learned directions transfer across architectures and scales. This tests whether REMEDI captures universal knowledge representations or architecture-specific encodings.

**Check 2: Complex Knowledge Structure Evaluation**
Create a dataset with multi-hop reasoning facts (e.g., "X is a mammal, mammals give live birth, therefore X gives live birth") and evaluate REMEDI's ability to edit and probe these relationships. Compare against baseline methods on tasks requiring inference beyond direct attribute editing. This validates whether REMEDI handles compositional knowledge rather than just atomic facts.

**Check 3: Ablation of KL Regularization Impact**
Train REMEDI variants with different λ2 values (including λ2=0) on Bias in Bios and measure essence preservation, fluency, and efficacy. Additionally, conduct human evaluations of entity identity preservation across these variants. This directly tests whether the KL term actually preserves entity essence as claimed, or if other factors dominate the preservation effect.