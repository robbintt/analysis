---
ver: rpa2
title: Increasing Probability Mass on Answer Choices Does Not Always Improve Accuracy
arxiv_id: '2305.14596'
source_url: https://arxiv.org/abs/2305.14596
tags:
- answer
- accuracy
- choices
- mass
- string
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how attention to answer choices relates
  to accuracy in multiple-choice tasks using large language models (LMs). It introduces
  a mathematical formalism to quantify the "surface form competition" (SFC) hypothesis,
  where LMs distribute probability mass across semantically equivalent surface forms,
  potentially hurting performance.
---

# Increasing Probability Mass on Answer Choices Does Not Always Improve Accuracy

## Quick Facts
- arXiv ID: 2305.14596
- Source URL: https://arxiv.org/abs/2305.14596
- Reference count: 40
- Key outcome: Higher attentiveness to answer choices (measured by probability mass on valid answers) does not always improve accuracy; in fact, it can sometimes decrease it for certain models.

## Executive Summary
This paper investigates the relationship between a language model's attentiveness to answer choices and its accuracy on multiple-choice tasks. The authors introduce a mathematical formalism for "surface form competition" (SFC), where models distribute probability mass across semantically equivalent surface forms, potentially hurting performance. They propose a metric for measuring attentiveness and show that including answer choices in prompts and using in-context learning significantly increases this attentiveness. However, they find that higher attentiveness does not always improve accuracy; for some models, increased probability mass on answer choices can actually decrease accuracy. The study also reveals that probability normalization methods like PMI-based scoring are less effective for instruction-tuned LMs, providing practical recommendations for optimizing LM performance based on model type and prompt format.

## Method Summary
The paper introduces a mathematical framework to study surface form competition (SFC) in language models on multiple-choice tasks. The authors define a metric called "probability mass on valid answer choices" (PMV) to quantify model attentiveness. They conduct experiments across six language models (curie, davinci, OPT 30B, FLAN-T5-XXL, davinci-instruct-beta, text-davinci-003) and three datasets (MMLU, CommonsenseQA, OpenbookQA). The study tests different prompting strategies including various formats with and without answer choices, and different numbers of in-context examples. They also evaluate the effectiveness of PMI-based probability normalization methods. The experiments systematically measure both PMV and task accuracy to understand how attentiveness relates to performance across different model types and prompting approaches.

## Key Results
- Including answer choices in prompts and using in-context learning significantly increases probability mass on valid answer choices (PMV) for most models
- Higher PMV does not always correlate with higher accuracy; for some models, increased attentiveness can decrease accuracy
- PMI-based scoring is less effective for instruction-tuned models compared to vanilla models
- The relationship between attentiveness and accuracy varies by model type, with instruction-tuned models showing different behavior patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Including answer choices in the prompt increases the model's probability mass on valid answer choices (PMV), thereby reducing surface form competition (SFC).
- Mechanism: By explicitly showing the valid answer choices in the prompt, the model is conditioned to focus its probability distribution on those specific surface forms, rather than spreading probability mass across semantically equivalent but invalid alternatives.
- Core assumption: The model's probability distribution over vocabulary tokens is influenced by the prompt content, and explicitly mentioning valid answer choices will shift this distribution toward those choices.
- Evidence anchors:
  - [abstract] "We identify a simple method for reducing it—namely, in-context learning with even just one example containing answer choices."
  - [section] "We find that this strategy is effective only for certain LMs."
  - [corpus] Weak evidence; no direct citation in corpus neighbors.
- Break condition: If the model is not sensitive to prompt formatting or if the answer choices are not clearly distinguished in the prompt.

### Mechanism 2
- Claim: In-context learning with even one example can significantly increase a model's attentiveness to valid answer choices.
- Mechanism: Providing in-context examples demonstrates the expected task format and answer structure, guiding the model to generate predictions that align with the valid answer choices.
- Core assumption: The model learns from in-context examples and adjusts its probability distribution accordingly, even with minimal examples.
- Evidence anchors:
  - [abstract] "We identify a simple method for reducing it—namely, in-context learning with even just one example containing answer choices."
  - [section] "We observe that all models place significantly more probability mass on valid answer choices after seeing only one in-context example that includes the answer choices."
  - [corpus] Weak evidence; no direct citation in corpus neighbors.
- Break condition: If the in-context examples are ambiguous or if the model does not effectively learn from few-shot demonstrations.

### Mechanism 3
- Claim: Probability normalization methods like PMI-based scoring can improve accuracy by addressing surface form competition.
- Mechanism: By normalizing the probability scores, the method reduces the impact of competing surface forms, allowing the model to better identify the correct answer choice.
- Core assumption: The model's raw probability scores are influenced by surface form competition, and normalization can mitigate this effect.
- Evidence anchors:
  - [abstract] "This has motivated the introduction of various probability normalization methods."
  - [section] "We find that PMI-based scoring with no answer choices in the prompt results in the highest accuracy for these datasets."
  - [corpus] Weak evidence; no direct citation in corpus neighbors.
- Break condition: If the model is already highly attentive to valid answer choices, normalization may not provide additional benefits and could even be detrimental.

## Foundational Learning

- Concept: Surface form competition (SFC)
  - Why needed here: Understanding SFC is crucial because it explains why models might distribute probability mass across semantically equivalent surface forms, leading to underestimation of performance.
  - Quick check question: What is surface form competition, and how does it affect a model's performance on multiple-choice tasks?

- Concept: Probability mass on valid answer choices (PMV)
  - Why needed here: PMV is a metric used to quantify a model's attentiveness to valid answer choices, which is essential for measuring and reducing SFC.
  - Quick check question: How is PMV calculated, and what does it indicate about a model's performance?

- Concept: In-context learning
  - Why needed here: In-context learning is a technique used to guide the model's predictions by providing examples within the prompt, which can increase attentiveness to valid answer choices.
  - Quick check question: How does in-context learning influence a model's probability distribution over vocabulary tokens?

## Architecture Onboarding

- Component map: Language Model -> Prompt Format -> Probability Distribution -> PMV Calculation -> Accuracy Evaluation
- Critical path:
  1. Design prompt format (with or without answer choices, in-context examples)
  2. Input prompt to LM and obtain probability distribution
  3. Apply probability normalization if needed
  4. Calculate PMV to measure attentiveness
  5. Evaluate accuracy to assess overall performance
- Design tradeoffs:
  - Including answer choices in the prompt can increase PMV but may not always improve accuracy for all models
  - Using in-context examples can guide the model but may introduce additional complexity in prompt design
  - Probability normalization can mitigate SFC but may be less effective for instruction-tuned models
- Failure signatures:
  - Low PMV indicates the model is not attentive to valid answer choices
  - Inconsistent accuracy across different prompt formats suggests sensitivity to prompt design
  - Detrimental effects of probability normalization on certain models indicate potential overfitting or misalignment
- First 3 experiments:
  1. Compare PMV and accuracy across different prompt formats (with vs. without answer choices)
  2. Evaluate the impact of varying numbers of in-context examples on PMV and accuracy
  3. Test the effectiveness of probability normalization methods on models with different levels of instruction tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the relationship between attentiveness and accuracy vary across different task domains and difficulty levels?
- Basis in paper: [explicit] The paper shows that attentiveness (probability mass on valid answer choices) does not always correlate with accuracy, and this relationship varies by model type.
- Why unresolved: The study only tested three datasets (MMLU, CommonsenseQA, OpenbookQA) which cover limited domains. The varying effectiveness across models suggests domain-specific factors may influence the attentiveness-accuracy relationship.
- What evidence would resolve it: Experiments testing the same models across a broader range of task domains (e.g., logic, mathematics, language understanding) with varying difficulty levels would clarify whether the attentiveness-accuracy relationship is domain-dependent.

### Open Question 2
- Question: What is the precise mechanism by which instruction tuning affects the relationship between attentiveness and accuracy?
- Basis in paper: [explicit] The paper observes that instruction-tuned models (FLAN-T5-XXL, davinci-instruct-beta, text-davinci-003) show different behavior compared to vanilla models when it comes to the effect of showing answer choices in prompts.
- Why unresolved: While the paper demonstrates that instruction tuning changes model behavior, it does not explain the underlying mechanisms causing this difference.
- What evidence would resolve it: Controlled experiments comparing the internal representations and attention patterns of instruction-tuned versus vanilla models when processing prompts with and without answer choices would reveal the mechanisms at play.

### Open Question 3
- Question: How does the length of answer choices affect the efficacy of different prompting strategies and probability normalization methods?
- Basis in paper: [inferred] The paper mentions that answer choices may be more than one token in length and that PMV is computed using first tokens, but does not investigate how answer choice length impacts results.
- Why unresolved: The study focuses on short answer choices and does not explore how longer, more complex answer choices might affect the surface form competition phenomenon.
- What evidence would resolve it: Experiments systematically varying answer choice length while keeping other factors constant would reveal whether longer answers exacerbate or mitigate surface form competition and how this affects different prompting strategies.

## Limitations
- The study focuses on six specific models and three multiple-choice datasets, which may not capture the full spectrum of LLM behaviors
- The mathematical formalism for SFC makes simplifying assumptions about token independence that may not hold in practice
- The paper doesn't extensively explore the impact of different tokenization schemes on SFC measurements

## Confidence
*High Confidence Claims:*
- The PMV metric reliably measures model attentiveness to answer choices across tested models
- Including answer choices in prompts increases PMV for most models
- In-context learning with answer choices improves attentiveness
- Instruction-tuned models show different responsiveness patterns compared to base models

*Medium Confidence Claims:*
- The relationship between PMV and accuracy varies by model type
- PMI-based normalization is less effective for instruction-tuned models
- SFC is a significant factor in multiple-choice task performance

*Low Confidence Claims:*
- Specific threshold values for PMV that indicate optimal performance
- Universal recommendations for prompt formatting across all model types
- Exact mathematical relationship between surface form competition and accuracy

## Next Checks
1. **Cross-Dataset Validation**: Test the PMV-attentiveness relationship on non-multiple-choice tasks (e.g., classification, generation) to verify if the findings generalize beyond multiple-choice formats.

2. **Tokenization Sensitivity Analysis**: Systematically vary tokenization schemes and measure how SFC metrics change, particularly for languages with different morphological structures.

3. **Model Architecture Comparison**: Test the PMV-attentiveness relationship across different model families (decoder-only, encoder-decoder, hybrid) to identify architectural factors influencing SFC susceptibility.