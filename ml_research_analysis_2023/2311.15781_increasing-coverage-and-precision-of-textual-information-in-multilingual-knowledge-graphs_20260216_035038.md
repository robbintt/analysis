---
ver: rpa2
title: Increasing Coverage and Precision of Textual Information in Multilingual Knowledge
  Graphs
arxiv_id: '2311.15781'
source_url: https://arxiv.org/abs/2311.15781
tags:
- entity
- knowledge
- language
- names
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the task of Knowledge Graph Enhancement (KGE)
  to increase multilingual coverage and precision of textual information (names and
  descriptions) in knowledge graphs like Wikidata. It identifies a large disparity
  between English and non-English languages, with non-English coverage as low as 15-30%
  for entity names and 20-50% for descriptions.
---

# Increasing Coverage and Precision of Multilingual Textual Information in Knowledge Graphs

## Quick Facts
- arXiv ID: 2311.15781
- Source URL: https://arxiv.org/abs/2311.15781
- Reference count: 40
- Primary result: Introduces M-NTA, achieving +12% coverage and +22% precision gains for multilingual KG textual information

## Executive Summary
This paper addresses the critical gap in multilingual coverage and precision of textual information (entity names and descriptions) in knowledge graphs, particularly Wikidata. The authors identify a stark disparity where non-English languages have only 15-30% coverage for entity names and 20-50% for descriptions compared to English. To address this, they propose M-NTA, an unsupervised approach that combines Machine Translation, Web Search, and Large Language Models to generate high-quality textual information. M-NTA achieves significant improvements over baseline methods and demonstrates practical value by improving downstream tasks like Entity Linking, Knowledge Graph Completion, and Question Answering.

## Method Summary
M-NTA (Multi-source Naturalization, Translation, and Alignment) is an unsupervised approach that enhances multilingual knowledge graphs by generating high-quality entity names and descriptions. The method combines outputs from three subsystems: Machine Translation (NLLB-200), Web Search (Google), and Large Language Models (GPT). Entity names are first contextualized using their descriptions, then translated through all three systems. The outputs are ranked using agreement scoring, where candidate names appearing across multiple sources receive higher scores. A threshold parameter (λ) controls the precision-recall tradeoff. The approach is evaluated on WikiKGE-10, a new human-curated benchmark spanning 10 languages.

## Key Results
- M-NTA achieves +12% coverage and +22% precision improvements over GPT-3.5 baseline
- Improves Entity Linking F1 score by +1.2 points
- Increases Knowledge Graph Completion MRR by +1.3 points
- Reduces unanswerable queries in Question Answering by 15-53%
- Identifies significant English-centric bias in Wikidata (15-30% name coverage, 20-50% description coverage for non-English languages)

## Why This Works (Mechanism)

### Mechanism 1
Claim: Combining multiple translation sources improves multilingual entity name coverage and precision.
Mechanism: M-NTA uses agreement scoring across MT, WS, and LLM outputs to filter out incorrect translations. When multiple sources agree on a name, it's more likely to be correct.
Core assumption: Different systems have complementary strengths and weaknesses; their agreement indicates higher accuracy.
Evidence anchors:
- [abstract]: "M-NTA, a novel unsupervised approach that combines MT, WS, and LLMs to generate high-quality textual information"
- [section 4.2]: "The intuition behind M-NTA is that obtaining a fact from multiple source systems may offer complementary pieces of information which provide varying views on our world knowledge"
- [corpus]: Weak evidence; related papers focus on KG completion but not specifically on multi-source agreement methods.
Break condition: If all source systems consistently make the same type of error, agreement scoring won't help.

### Mechanism 2
Claim: Contextualizing entity names before translation improves accuracy.
Mechanism: M-NTA retrieves entity descriptions and uses them to create natural language representations, providing context that disambiguates polysemous names.
Core assumption: Entity names without context are ambiguous and prone to mistranslation.
Evidence anchors:
- [section 4.2]: "entity names are not suitable for direct translation since they might not provide sufficient context"
- [section C.1]: "contextualizing entity names before converting them from one language to another"
- [corpus]: No direct evidence; assumption based on general translation challenges.
Break condition: If descriptions are unavailable or uninformative, contextualization won't help.

### Mechanism 3
Claim: M-NTA's performance gains come from leveraging complementary knowledge across languages.
Mechanism: By translating from multiple source languages rather than just English, M-NTA accesses different knowledge distributions.
Core assumption: Different languages encode different knowledge about entities.
Evidence anchors:
- [section 5.2]: "different languages hold different knowledge" and "generating entity names in non-English languages by translating English-only textual information does not provide the best results"
- [section C.3]: Discussion of using multiple source languages
- [corpus]: Weak evidence; related papers discuss multilingual KGs but not multi-source translation strategies.
Break condition: If knowledge distributions across languages are similar, multi-source approach won't provide benefits.

## Foundational Learning

- Concept: Knowledge Graph Enhancement (KGE) task definition
  - Why needed here: Understanding the specific problem of increasing multilingual coverage and precision is essential for implementing M-NTA correctly.
  - Quick check question: What are the two subtasks of KGE, and how do they differ in their objectives?

- Concept: Entity name contextualization and de-contextualization
  - Why needed here: M-NTA relies on creating natural language representations of entities and then extracting translated names from translated sentences.
  - Quick check question: How does M-NTA use special markers to extract translated entity names from contextualized sentences?

- Concept: Agreement scoring and threshold selection
  - Why needed here: M-NTA uses agreement scores to rank and filter candidate names, with λ controlling the precision-recall tradeoff.
  - Quick check question: How does changing the λ parameter affect the precision and recall of M-NTA's output?

## Architecture Onboarding

- Component map: MT -> WS -> LLM -> Agreement Scoring -> Filtered Output
- Critical path: For each entity in target language: 1) Retrieve descriptions, 2) Generate contextualized representations, 3) Run through MT, WS, and LLMs, 4) Apply agreement scoring, 5) Filter by threshold λ, 6) Output final names.
- Design tradeoffs: Using multiple systems increases computational cost but improves accuracy. The λ threshold trades precision for recall. Choosing which source languages to use affects coverage.
- Failure signatures: Low coverage suggests insufficient source languages or poor agreement scoring. Low precision suggests the agreement threshold is too low or source systems are unreliable.
- First 3 experiments:
  1. Run M-NTA with λ=1 on a small test set and measure coverage/precision to establish baseline performance.
  2. Vary λ from 1-6 on the same test set to understand precision-recall tradeoff.
  3. Run ablation study removing each subsystem (MT, WS, LLM) to measure individual contributions.

## Open Questions the Paper Calls Out

### Open Question 1
Question: How does the performance of M-NTA change when applied to lower-resource languages compared to higher-resource languages?
Basis in paper: [inferred] The paper mentions that the quality of translations from automatic systems lags behind when the source language is a lower-resource language, suggesting that M-NTA's performance might be affected.
Why unresolved: The paper focuses on higher-resource languages for MT experiments, hypothesizing that translating from lower-resource languages does not result in significantly better performance, but does not provide empirical evidence for this claim.
What evidence would resolve it: Conducting experiments with M-NTA using lower-resource languages as source languages and comparing the results with those from higher-resource languages would provide empirical evidence to support or refute the hypothesis.

### Open Question 2
Question: How can the computational cost of M-NTA be reduced while maintaining or improving its performance?
Basis in paper: [explicit] The paper mentions that M-NTA requires the output from MT, WS, and LLMs, resulting in a high computational cost if run sequentially.
Why unresolved: The paper acknowledges the high computational cost of M-NTA but does not propose any methods to reduce it while maintaining or improving performance.
What evidence would resolve it: Developing and testing new methods to reduce the computational cost of M-NTA, such as parallel processing or more efficient algorithms, and comparing their performance with the original M-NTA would provide evidence for potential improvements.

### Open Question 3
Question: How does the performance of M-NTA vary across different types of textual information in knowledge graphs, such as longer descriptions or coreferential information?
Basis in paper: [explicit] The paper focuses on entity names and descriptions but mentions that the observations might generalize to other types of textual information, leaving deeper investigations to future work.
Why unresolved: The paper does not provide empirical evidence on how M-NTA performs with other types of textual information in knowledge graphs, such as longer descriptions or coreferential information.
What evidence would resolve it: Conducting experiments with M-NTA using different types of textual information in knowledge graphs and comparing the results with those obtained for entity names and descriptions would provide evidence for its performance across various types of textual information.

## Limitations

- The approach assumes that agreement across multiple translation sources reliably indicates higher accuracy, which may not hold if all systems share common biases or errors.
- The contextualization approach depends on the availability of informative entity descriptions, which may be limited for some entities.
- The method's effectiveness relies on different languages encoding meaningfully different knowledge, which may vary across language families or domains.

## Confidence

**High Confidence**: The empirical results showing M-NTA's performance improvements over baselines (12% coverage and 22% precision gains) are well-supported by the WikiKGE-10 benchmark. The downstream task improvements (EL F1 +1.2, KG Completion MRR +1.3, QA unanswerable reduction 15-53%) provide strong validation of the approach's practical utility.

**Medium Confidence**: The mechanism explanations for why agreement scoring works and why multi-source translation helps are reasonable but not exhaustively validated. The paper presents intuitive arguments and some supporting evidence, but doesn't conduct thorough ablation studies to isolate the contribution of each component.

**Low Confidence**: The assumption that different languages encode substantially different knowledge is supported by empirical results but lacks theoretical grounding. The paper doesn't explore the linguistic or cultural factors that might drive these differences, nor does it test the approach's effectiveness on language pairs with more similar knowledge distributions.

## Next Checks

1. **Ablation Study on Agreement Scoring**: Run experiments with only pairwise agreements (MT-WS, MT-LLM, WS-LLM) versus full three-way agreement to quantify how much each agreement combination contributes to precision gains.

2. **Knowledge Distribution Analysis**: Analyze the overlap and uniqueness of entity names across the seven source languages to quantify how much genuinely different knowledge each language contributes versus redundant information.

3. **Edge Case Investigation**: Systematically analyze entities where M-NTA fails or produces incorrect translations, categorizing failure modes (missing descriptions, polysemy, cultural differences) to understand the approach's limitations.