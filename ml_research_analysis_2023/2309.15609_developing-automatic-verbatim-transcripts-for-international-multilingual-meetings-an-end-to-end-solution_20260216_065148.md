---
ver: rpa2
title: 'Developing automatic verbatim transcripts for international multilingual meetings:
  an end-to-end solution'
arxiv_id: '2309.15609'
source_url: https://arxiv.org/abs/2309.15609
tags:
- wipo
- data
- speech
- meeting
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an end-to-end solution for automatic conference
  meeting transcripts and their machine translations into various languages, developed
  at the World Intellectual Property Organization (WIPO) using in-house developed
  speech-to-text (S2T) and machine translation (MT) components. The system has replaced
  manually prepared verbatim reports, creating a new type of product and achieving
  considerable cost reduction.
---

# Developing automatic verbatim transcripts for international multilingual meetings: an end-to-end solution

## Quick Facts
- arXiv ID: 2309.15609
- Source URL: https://arxiv.org/abs/2309.15609
- Reference count: 1
- Primary result: An end-to-end solution using in-house S2T and MT components that replaced manual reporting at WIPO, achieving cost reduction and positive user feedback.

## Executive Summary
This paper presents an end-to-end solution for automatic conference meeting transcripts and their machine translations into various languages, developed at the World Intellectual Property Organization (WIPO) using in-house developed speech-to-text (S2T) and machine translation (MT) components. The system has replaced manually prepared verbatim reports, creating a new type of product and achieving considerable cost reduction. The solution leverages open-source tools, is installed on-premises to meet data security and privacy policies, and is integrated with other WIPO conference room systems. The system has been well received by users, with positive feedback on speed, access, and quality, despite some inaccuracies in certain languages.

## Method Summary
The solution uses a cascaded approach, first applying in-house developed S2T models (RETURNN and ESPnet) to meeting audio, then applying machine translation using WIPO Translate models. The system processes 7 audio channels (6 monolingual interpretation channels plus 1 multilingual floor channel) separately, integrates metadata from connected systems like Connectedviews and Televic, and publishes results through a unified web portal. The S2T models were trained on a combination of WIPO meeting data, collaborative data from other international organizations, commercial corpora, and public speech datasets, with fine-tuning for domain terminology.

## Key Results
- Successfully replaced manual verbatim reporting with automated transcripts and translations for all six UN official languages plus Portuguese
- Achieved considerable cost reduction while maintaining acceptable quality levels across all language pairs
- Integrated tightly with existing WIPO systems, enabling rapid deployment and rich metadata enhancement
- Received positive user feedback on speed, access, and quality despite some inaccuracies in certain languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cascading S2T with MT delivers acceptable accuracy across all UN languages.
- Mechanism: By using separate, specialized models for speech recognition and translation, the system leverages existing high-performance MT models trained on WIPO's meeting domain data, which compensates for potential S2T errors.
- Core assumption: The MT models are robust enough to handle the noisy, domain-specific input from S2T and still produce understandable translations.
- Evidence anchors:
  - [abstract] "We took a conscious decision to cascade S2T with MT instead of end-to-end speech-to-translated-text approach, because it did not meet minimum quality requirements across all language pairs."
  - [section 6] "These WIPO Translate MT (Pouliquen, 2017) models are custom-trained using text from WIPO meetings and other documents."
- Break condition: If MT models degrade significantly when processing S2T errors, or if end-to-end S2T+MT models surpass cascaded quality for all languages.

### Mechanism 2
- Claim: Integration with existing WIPO systems enables rapid deployment and metadata enrichment.
- Mechanism: Tight coupling with Connectedviews and other internal systems allows automatic download of media files, metadata (speaker timestamps, agenda items), and seamless indexing in Elasticsearch, reducing manual effort and turnaround time.
- Core assumption: Metadata is consistently structured and available in real time from connected systems.
- Evidence anchors:
  - [section 8] "A tight integration with other WIPO systems allows us access to rich meta data to enhance the user experience."
  - [section 8] "Connectedviews itself is integrated with other videoconferencing systems like Televic that provide meta data information like 'who spoke when.'"
- Break condition: If metadata schemas change, data access is interrupted, or manual intervention becomes necessary for a large portion of meetings.

### Mechanism 3
- Claim: In-domain fine-tuning with collaborative data improves S2T accuracy for domain terminology.
- Mechanism: WIPO supplements limited internal data by collaborating with other international organizations, using commercial corpora, and contracting external providers, then fine-tunes RETURNN and ESPnet models on this aggregated dataset.
- Core assumption: The collaborative and external data is representative of WIPO's meeting domain and terminology.
- Evidence anchors:
  - [section 4] "To overcome this challenge, we did the following: 1) we collaborated with other international organizations to leverage their historical meetings data, 2) contracted external providers to prepare transcriptions of WIPO in-domain audio, 3) and bought out-of-domain proprietary corpora"
  - [section 5] "The training corpora thus prepared are combined, harmonized, and filtered before proceeding for training different RNN, and Transformer models using RETURNN and ESPnet training scripts."
- Break condition: If collaborative data introduces significant noise or legal/administrative barriers prevent effective use.

## Foundational Learning

- Concept: Speech-to-Text (S2T) pipeline architecture
  - Why needed here: To understand how audio is processed from raw meeting recordings into text, including segmentation, alignment, and model selection.
  - Quick check question: What are the two main types of segmentation used before feeding audio to S2T models?

- Concept: Machine Translation (MT) customization for domain terminology
  - Why needed here: To grasp how WIPO Translate models are adapted for the specialized vocabulary of international meetings.
  - Quick check question: Why did WIPO choose cascading S2T+MT over end-to-end speech-to-translated-text?

- Concept: Metadata integration and indexing
  - Why needed here: To see how external system data (speaker info, agenda) is combined with transcripts for user-facing search and navigation.
  - Quick check question: Which system provides speaker timestamps that are more accurate than automatic speaker diarization?

## Architecture Onboarding

- Component map: Meeting audio (7 channels) -> WebRTC/VAD + Kaldi segmentation -> RETURNN/ESPnet S2T models -> WIPO Translate MT models -> Gentle/Kaldi alignment -> Elasticsearch indexing -> webcast.wipo.int frontend
- Critical path: Meeting ends → fetch media + metadata → split audio → S2T decode → MT translate → align → index → publish
- Design tradeoffs: S2T+MT cascade vs. end-to-end (cascade chosen for quality across all languages); modular architecture allows swapping S2T providers (RETURNN/ESPnet/Whisper) but increases complexity.
- Failure signatures: Duplicate meeting entries (metadata naming inconsistency), missing metadata (polling dependency), inconsistent transcripts for same audio (monolingual vs multilingual channel issue).
- First 3 experiments:
  1. Test segmentation pipeline on a sample multilingual meeting audio file; verify segment length and silence/music detection.
  2. Run both RETURNN and ESPnet S2T models on a known test utterance; compare WER and adequacy (deletion errors).
  3. Simulate metadata polling failure; observe impact on indexing and frontend display; implement fallback handling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can manual edition of pieces of transcripts be best incorporated into the automated system to improve accuracy while maintaining efficiency?
- Basis in paper: [explicit] The paper mentions "There are also many open questions: such as the best way to allow for and incorporate the manual edition of pieces of transcripts" in the conclusions section.
- Why unresolved: This question remains unresolved because the paper does not provide a clear strategy for integrating manual editing into the automated system. It is likely a complex issue involving user experience, system architecture, and resource allocation.
- What evidence would resolve it: Evidence that would resolve this question includes user studies comparing different manual editing workflows, system architecture designs that facilitate easy integration of manual edits, and cost-benefit analyses of manual editing versus purely automated approaches.

### Open Question 2
- Question: How can the system best utilize the various outputs generated from different audio streams (e.g., direct S2T on interpretation channel vs MT cascade)?
- Basis in paper: [explicit] The paper states "There are also many open questions: such as the best way to allow for and incorporate the manual edition of pieces of transcripts; and how to best use the multitude of various outputs generated from different audio streams: for example, direct S2T on interpretation channel vs MT cascade" in the conclusions section.
- Why unresolved: This question is unresolved because the paper does not provide a definitive answer on which approach yields better results for different scenarios. The effectiveness of each method likely depends on factors such as language pair, audio quality, and user preferences.
- What evidence would resolve it: Evidence that would resolve this question includes comparative studies of WER, user satisfaction surveys, and analysis of error types for different output methods across various language pairs and meeting contexts.

### Open Question 3
- Question: What is the optimal balance between in-house developed components and pre-trained commercial components for different languages to achieve the best quality and user experience?
- Basis in paper: [explicit] The paper mentions "While most models and components are trained in-house, it remains to be seen which mix of such components will deliver the best possible quality and experience for users in the long term. Especially for high resource languages such as English and French, pre-trained components may be more than competitive to the existing in-house produced" in the conclusions section.
- Why unresolved: This question is unresolved because the paper does not provide a clear recommendation on the optimal balance between in-house and commercial components. The effectiveness of each approach likely varies by language, domain, and specific use case.
- What evidence would resolve it: Evidence that would resolve this question includes head-to-head comparisons of WER, user satisfaction surveys, and cost analyses for different language pairs using various combinations of in-house and commercial components.

## Limitations
- The cascaded S2T+MT approach introduces cumulative error propagation that may impact less resourced language pairs
- The paper lacks comprehensive quantitative performance metrics across all six UN languages
- The system relies on proprietary commercial data sources and collaborations with other international organizations, creating potential reproducibility barriers

## Confidence
- **High Confidence**: The technical architecture and integration approach are well-documented and technically sound
- **Medium Confidence**: The claimed cost reduction and business impact are supported by qualitative user feedback but lack specific quantitative metrics
- **Low Confidence**: The scalability of the collaborative data approach is uncertain due to dependencies on partnerships and data source availability

## Next Checks
1. **Quantitative Performance Validation**: Conduct comprehensive WER/CER evaluation across all six UN languages using standardized test sets, comparing cascaded vs. end-to-end approaches under identical conditions to validate the quality claims.

2. **User Satisfaction Quantification**: Implement systematic user satisfaction surveys with statistical analysis to measure actual quality perceptions, usability metrics, and business impact quantification (cost savings, turnaround time improvements).

3. **Scalability and Dependency Analysis**: Perform a stress test of the data pipeline under increased meeting volume, and conduct a dependency risk assessment for commercial data sources and international organization collaborations, with mitigation strategies for potential disruptions.