---
ver: rpa2
title: Impact of Tokenization on LLaMa Russian Adaptation
arxiv_id: '2312.02598'
source_url: https://arxiv.org/abs/2312.02598
tags:
- language
- russian
- tokenization
- vocabulary
- unigram
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study investigates tokenization efficiency in adapting LLaMa\
  \ models to Russian, revealing that subword tokenization quality directly impacts\
  \ model performance and resource usage. Three vocabulary adaptation variants\u2014\
  BPE, Unigram, and the original\u2014were evaluated using continued pre-training\
  \ on a large Russian corpus, with Unigram demonstrating superior morphological accuracy\
  \ and better downstream task results."
---

# Impact of Tokenization on LLaMa Russian Adaptation

## Quick Facts
- arXiv ID: 2312.02598
- Source URL: https://arxiv.org/abs/2312.02598
- Reference count: 18
- Primary result: Unigram tokenization improves Russian LLaMa adaptation with 60% faster inference and 35% faster fine-tuning

## Executive Summary
This study investigates how tokenization efficiency affects the adaptation of LLaMa models to Russian, a morphologically rich language. The research compares three vocabulary adaptation variants - BPE, Unigram, and the original tokenizer - through continued pre-training on a large Russian corpus. The results demonstrate that Unigram tokenization provides superior morphological accuracy and better downstream task performance compared to BPE, while also delivering significant resource efficiency gains. The findings highlight tokenization optimization as a critical factor in effective multilingual language model adaptation.

## Method Summary
The researchers adapted LLaMa models to Russian using a generalized vocabulary substitution method. They built three different tokenizers (BPE, Unigram, and original) using SentencePiece on a 5M document subset, then replaced the vocabulary while rebuilding embedding and LM head layers. The models were initialized based on token overlap with original embeddings and pre-trained only on embedding layers for one epoch using Adam optimizer (lr=3e-4, 2000-step warmup, batch size=240). The training corpus consisted of 9 million deduplicated documents (3.5 billion words) from diverse Russian sources including Wikipedia, Pikabu, news sites, and books.

## Key Results
- Unigram tokenization achieved higher morphological accuracy with better root preservation rates than BPE for Russian
- Unigram-based models outperformed both baseline and BPE variants on Russian Super Glue (RSG) benchmark tasks
- Resource efficiency improvements: 35% faster fine-tuning and up to 60% faster inference with reduced memory consumption

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Subword tokenization quality directly impacts model performance and resource usage for non-English languages
- Mechanism: Low representation of non-English languages in pre-training data leads to inefficient tokenization, degrading morphological accuracy and increasing token count per word
- Core assumption: Tokenization algorithm choice affects morphological preservation and token efficiency
- Evidence anchors:
  - "inefficient tokenization caused by low language representation in pre-training data which hinders the comprehension of non-English instructions"
  - "Figure 2 conventional BPE algorithm have lower root preservation rate than Unigram regardless of selected vocabulary"
- Break condition: If morphological accuracy gains don't translate to downstream task improvements, the mechanism fails

### Mechanism 2
- Claim: Vocabulary substitution with morphologically accurate tokenization improves downstream task performance
- Mechanism: Replacing original vocabulary with target-language optimized one while keeping model architecture allows better representation of language-specific morphology
- Core assumption: Continued pre-training on target language corpus with adapted vocabulary is sufficient for quality improvement
- Evidence anchors:
  - "vocabulary substitution not only improves the model's quality in Russian but also accelerates fine-tuning (35%) and inference (up to 60%)"
  - "Russian large language models highly benefit from morphologically accurate word tokenization, achieving a considerable quality boost with Unigram vocabulary"
- Break condition: If fine-tuning on downstream tasks shows no improvement over original model, the mechanism is invalid

### Mechanism 3
- Claim: Tokenization optimization leads to significant resource efficiency gains
- Mechanism: More accurate tokenization reduces average token count per word, directly decreasing memory usage and inference time
- Core assumption: Generation time and memory consumption scale with token count
- Evidence anchors:
  - "accelerates fine-tuning (35%) and inference (up to 60%) while reducing memory consumption"
  - "Figure 5 shows the generation time and additional memory consumed for text length breakpoints. As can be seen the improvement in average token count substantially boosts the resource efficiency."
- Break condition: If resource savings don't materialize in practice, the mechanism is broken

## Foundational Learning

- Concept: Morphological accuracy in tokenization
  - Why needed here: Russian is highly inflected where morphological preservation is crucial for model understanding
  - Quick check question: How does root preservation rate differ between BPE and Unigram tokenization for Russian words?

- Concept: Subword tokenization algorithms (BPE vs Unigram)
  - Why needed here: Different algorithms have different trade-offs between morphological accuracy and token efficiency
  - Quick check question: What is the median token count per word for BPE vs Unigram on Russian text?

- Concept: Vocabulary substitution methodology
  - Why needed here: Understanding how to replace vocabulary while maintaining model coherence is key to adaptation process
  - Quick check question: What are the two options for LM head initialization during vocabulary substitution?

## Architecture Onboarding

- Component map: Tokenizer (SentencePiece BPE/Unigram) -> Embedding layer (initialized with averaged vectors) -> LM head (copied from embedding or initialized by analogy) -> Pre-trained backbone (frozen except embedding/LM head) -> Training pipeline (continued pre-training)

- Critical path:
  1. Build target language tokenizer
  2. Replace vocabulary and reinitialize embedding/LM head
  3. Continue pre-training on target language corpus
  4. Fine-tune on downstream tasks or instruction-tune
  5. Evaluate on benchmarks and human evaluation

- Design tradeoffs:
  - BPE vs Unigram: Morphological accuracy vs token efficiency
  - Embedding vs LM head initialization: Simpler vs potentially more accurate
  - Pre-training duration: Quality vs resource constraints
  - Fine-tuning vs instruction-tuning: Task-specific vs general capability

- Failure signatures:
  - No improvement on downstream tasks
  - Increased perplexity on target language text
  - No resource efficiency gains
  - Human evaluation shows no preference for adapted model

- First 3 experiments:
  1. Compare root preservation rate of BPE vs Unigram on Russian morphological dataset
  2. Measure token count per word for both tokenizers on Russian text
  3. Run continued pre-training with both tokenizers and compare perplexity on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the morphological accuracy of Unigram tokenization compare to BPE across different inflected languages beyond Russian?
- Basis in paper: The study demonstrates Unigram has higher morphological accuracy than BPE for Russian but doesn't explore other languages
- Why unresolved: Study focuses solely on Russian with no comparative analysis with other inflected languages
- What evidence would resolve it: Conducting similar experiments across diverse inflected languages like Polish, Czech, or Turkish

### Open Question 2
- Question: What are the long-term effects of vocabulary substitution on model performance and resource efficiency when applied to larger and more diverse datasets?
- Basis in paper: Experiments are limited to specific dataset size and scope
- Why unresolved: Limited to particular dataset size and scope, leaving questions about scalability
- What evidence would resolve it: Extending experiments to larger, more diverse datasets with extended training periods

### Open Question 3
- Question: How does the choice of LM head initialization method impact overall model performance across different tasks and languages?
- Basis in paper: Paper investigates two initialization methods and finds differences in instruction-tuning efficiency
- Why unresolved: Provides initial insights but doesn't explore broader implications across various tasks and languages
- What evidence would resolve it: Comprehensive experiments with different initialization methods across multiple tasks and languages

## Limitations
- Data distribution constraints: Corpus composition may not fully represent Russian language diversity despite 9M documents
- Evaluation scope boundaries: Human evaluation limited to side-by-side comparisons without systematic diversity in evaluation prompts
- Generalization uncertainties: Findings specific to Russian may not directly transfer to languages with different morphological structures

## Confidence

**High Confidence Claims**:
- Tokenization quality directly impacts model performance metrics
- Unigram tokenization demonstrates superior morphological accuracy compared to BPE for Russian
- Resource efficiency gains (35% fine-tuning, 60% inference speedup) are measurable

**Medium Confidence Claims**:
- Downstream task performance improvements translate to practical utility across all RSG tasks
- Human preference consistently favors Unigram-based models
- Generalized vocabulary substitution methodology applies to other non-English languages

**Low Confidence Claims**:
- Efficiency gains magnitude remains consistent across different hardware configurations
- Exclusive LM head initialization variant provides meaningful improvements
- 1-epoch continued pre-training represents optimal tradeoff

## Next Checks
1. **Cross-linguistic Generalization Test**: Apply vocabulary substitution to morphologically distinct language (e.g., Turkish or Finnish) to verify Unigram consistently outperforms BPE across different morphological typologies
2. **Resource Efficiency Scaling Analysis**: Evaluate efficiency gains across different hardware configurations and model scales to confirm resource efficiency claims hold under diverse deployment conditions
3. **Longitudinal Quality Stability Assessment**: Conduct time-series evaluation of model quality over extended inference sessions to verify tokenization improvements maintain stability over prolonged usage