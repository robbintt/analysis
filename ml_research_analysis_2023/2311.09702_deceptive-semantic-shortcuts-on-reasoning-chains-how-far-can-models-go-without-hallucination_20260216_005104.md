---
ver: rpa2
title: 'Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without
  Hallucination?'
arxiv_id: '2311.09702'
source_url: https://arxiv.org/abs/2311.09702
tags:
- reasoning
- llms
- chain
- layer
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel QA benchmark for probing LLM hallucinations
  induced by semantic associations. We introduced a systematic method for generating
  question answering data with extended reasoning chains.
---

# Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?

## Quick Facts
- arXiv ID: 2311.09702
- Source URL: https://arxiv.org/abs/2311.09702
- Reference count: 4
- Primary result: LLMs predominantly depend on semantic shortcuts for reasoning, leading to hallucination and failure on masked reasoning chains

## Executive Summary
This paper introduces a novel QA benchmark to probe LLM hallucinations induced by semantic associations. The research presents a systematic method for generating question-answering data with extended reasoning chains, where entities are masked with hypernyms to force models to engage in genuine reasoning rather than relying on semantic shortcuts. The experimental results demonstrate that current LLMs struggle significantly when semantic shortcuts are eliminated, with hallucination during reasoning being the primary failure mode in 80% of incorrect cases.

## Method Summary
The researchers generated a dataset called EURE QA by extracting deterministic reasoning chains from DBpedia using a random-walk algorithm, then recursively masking entities with hypernyms while maintaining human-readable natural language questions. Two prompting strategies were evaluated: direct zero-shot prompting and in-context learning (ICL) with human-written exemplars. The study tested multiple LLM variants (Llama-2-7b/13b/70b-Chat, ChatGPT, and GPT-4) across reasoning chains of varying depths (1-5 layers), measuring accuracy when entities were masked with hypernyms to eliminate semantic shortcuts.

## Key Results
- LLMs show significant performance degradation as reasoning chain depth increases when semantic shortcuts are masked
- Hallucination during intermediate reasoning steps accounts for 80% of incorrect answers
- Larger models (GPT-4) benefit from exemplar-based ICL prompts, while smaller models (Llama-2-7b/13b) struggle with long exemplars
- Semantic shortcut elimination forces models to fail rather than engage in genuine reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs solve multi-hop QA by matching semantic cues rather than genuine reasoning
- Mechanism: Masking entities with hypernyms eliminates direct semantic shortcuts, forcing models to rely on intermediate reasoning
- Core assumption: The reasoning chain is deterministic and solvable by correct inference alone
- Evidence anchors:
  - [abstract] "we investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path"
  - [section 4] "The most frequent error pattern lies in hallucination during reasoning process, encompassing 80% of the cases"
- Break condition: If the reasoning chain is not deterministic or contains hidden semantic links

### Mechanism 2
- Claim: ICL exemplars improve reasoning performance by encouraging correct inference paths
- Mechanism: Human-written reasoning chains guide models to replicate structured reasoning instead of guessing based on surface cues
- Core assumption: Models can learn to follow structured reasoning from exemplars without overfitting
- Evidence anchors:
  - [section 3.2] "Each exemplar is characterized by a sample problem, succeeded by the statement 'Let's solve this question step by step' and a step-by-step solution written by humans"
  - [section 4] "These models can benefit from examples that provide explicit and well-crafted human-written reasoning processes"
- Break condition: Exemplars may distract smaller models due to long text processing limits

### Mechanism 3
- Claim: Hallucination indicates shortcut-taking rather than correct reasoning
- Mechanism: When models hallucinate intermediate steps, errors accumulate and lead to wrong answers, revealing reliance on semantic shortcuts
- Core assumption: Hallucination is distinct from reasoning errors and indicates shortcut usage
- Evidence anchors:
  - [section 4] "The most frequent error pattern lies in hallucination during reasoning process, encompassing 80% of the cases"
  - [abstract] "Distractor semantic associations often lead to model hallucination, which is strong evidence that questions the validity of current LLM reasoning"
- Break condition: If hallucination detection is ambiguous or conflated with other error types

## Foundational Learning

- Concept: Deterministic reasoning chains
  - Why needed here: Ensures correct answers can be reached purely through logical inference without relying on external knowledge or shortcuts
  - Quick check question: If one layer of the chain is removed, can the model still deduce the correct answer from the remaining layers?

- Concept: Semantic masking and hypernym substitution
  - Why needed here: Reduces model's ability to use direct entity matching, forcing engagement with reasoning process
  - Quick check question: After masking an entity with its hypernym, can a human still follow the reasoning chain to the correct answer?

- Concept: Hallucination detection in reasoning
  - Why needed here: Differentiates between correct reasoning errors and shortcut-based hallucinations
  - Quick check question: Does the model's intermediate step introduce an entity or fact not present in the provided chain?

## Architecture Onboarding

- Component map: Knowledge base (DBpedia) → Chain generator → Masking engine → LLM prompt interface → Evaluation module
- Critical path: 1) Extract deterministic reasoning chain from DBpedia 2) Mask entities with hypernyms recursively 3) Generate question with masked chain 4) Apply prompt strategy and run LLM 5) Evaluate answer accuracy and hallucination
- Design tradeoffs: Deterministic chains limit dataset size but ensure validity; masking reduces semantic shortcuts but may over-constrain reasoning; exemplar-based prompts help larger models but may confuse smaller ones
- Failure signatures: Consistent accuracy drop with chain depth indicates shortcut reliance; hallucination in intermediate steps signals shortcut usage; ICL prompts degrading performance in smaller models suggests long-context overload
- First 3 experiments: 1) Run Llama-2-7b with direct and ICL prompts on 1-layer masked questions to confirm baseline behavior 2) Compare GPT-4 performance on 3-layer vs 5-layer chains to quantify depth sensitivity 3) Analyze hallucination frequency by sampling incorrect answers and checking intermediate reasoning steps for introduced facts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does elimination of semantic shortcuts affect LLM performance on multi-hop reasoning tasks?
- Basis in paper: [explicit] This paper proposes a novel probing method and benchmark called EURE QA to investigate the extent to which LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path
- Why unresolved: Experimental results suggest LLMs depend on semantic associations, but the extent of this dependency and its impact on multi-hop reasoning remains unclear
- What evidence would resolve it: Further experiments with varying degrees of semantic shortcut elimination and their effects on LLM performance in multi-hop reasoning tasks

### Open Question 2
- Question: How does depth of reasoning chains influence accuracy of LLM responses?
- Basis in paper: [explicit] The paper finds that as reasoning depth grows, accuracy of LLM responses decreases, indicating models struggle with deeper chains
- Why unresolved: Paper provides initial evidence of this trend, but comprehensive understanding of relationship between chain depth and LLM accuracy is lacking
- What evidence would resolve it: Systematic experiments varying depth of reasoning chains and analyzing corresponding accuracy of LLM responses

### Open Question 3
- Question: Can LLMs be trained to resist greedy shortcuts and follow correct reasoning paths?
- Basis in paper: [inferred] Findings suggest LLMs lack capabilities to follow correct reasoning paths and resist greedy shortcuts, leading to hallucination
- Why unresolved: Paper highlights issue of LLM reliance on semantic shortcuts but doesn't provide solution for training models to overcome this limitation
- What evidence would resolve it: Development and evaluation of training methods aimed at improving LLM reasoning capabilities and reducing reliance on semantic shortcuts

## Limitations
- Small sample size (20 cases) used to draw conclusions about hallucination patterns
- Deterministic reasoning chains may not capture full complexity of real-world reasoning tasks
- Masking approach may create artificial constraints that don't reflect natural reasoning contexts

## Confidence
- High Confidence: Observation that LLM performance degrades with increased reasoning depth is well-supported
- Medium Confidence: Claim that semantic shortcuts are primary failure mode is supported but limited by small sample size
- Low Confidence: Assertion that hallucinations specifically indicate shortcut-taking versus other reasoning failures requires more systematic analysis

## Next Checks
1. Expand error analysis to include all incorrect responses (not just 20 sampled cases) to verify hallucination patterns across full dataset
2. Test model performance on reasoning chains with controlled semantic variations to distinguish between shortcut-taking and genuine reasoning failures
3. Evaluate whether exemplar-based prompting effects persist across different chain structures and reasoning depths to validate proposed mechanism