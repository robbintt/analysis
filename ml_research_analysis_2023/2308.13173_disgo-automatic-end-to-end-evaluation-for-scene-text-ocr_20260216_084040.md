---
ver: rpa2
title: 'DISGO: Automatic End-to-End Evaluation for Scene Text OCR'
arxiv_id: '2308.13173'
source_url: https://arxiv.org/abs/2308.13173
tags:
- word
- block
- errors
- error
- location
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DISGO, a unified word error rate (WER)-based
  metric for evaluating scene text OCR systems end-to-end. DISGO considers deletion,
  insertion, substitution, and grouping/ordering errors.
---

# DISGO: Automatic End-to-End Evaluation for Scene Text OCR

## Quick Facts
- arXiv ID: 2308.13173
- Source URL: https://arxiv.org/abs/2308.13173
- Reference count: 23
- Key outcome: Introduces DISGO metric for end-to-end OCR evaluation using WER-based approach that considers deletion, insertion, substitution, and grouping/ordering errors

## Executive Summary
This paper presents DISGO, a unified word error rate (WER)-based metric for evaluating scene text OCR systems end-to-end. The metric addresses the challenge of automatically measuring end-to-end OCR performance by introducing a location map for word alignment, measuring grouping/ordering errors through block definitions and reading orders, and extending the approach to automatic machine translation evaluation using superblocks and BLEU scores. The authors demonstrate the metric on a modularized OCR system achieving WER(e2e) of 64.3% on a public test set.

## Method Summary
DISGO evaluates scene text OCR systems by calculating word error rates that combine deletion, insertion, substitution, and grouping/ordering errors. The method uses a location map to assign unique IDs to ground truth and predicted words based on bounding box coordinates, then aligns them using intersection-over-union. Grouping and ordering errors are measured by comparing block definitions and reading orders between ground truth and predictions. The metric is validated on a modularized OCR system with word detection (Faster R-CNN), word recognition (CTC-based CNN), and heuristic-based grouping/ordering.

## Key Results
- DISGO WER(e2e) = 64.3% on SCUT-CTW1500 test set for end-to-end OCR evaluation
- Component-level WER: detection WER = 16.5%, recognition WER = 19.6%, grouping/ordering WER = 47.2%
- Superblock-based BLEU enables automatic evaluation from OCR to machine translation
- Location map with IoU threshold successfully aligns GT and predicted words for error calculation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The location map assigns unique IDs to ground truth and predicted words for alignment.
- Mechanism: Uses bounding box coordinates to assign unique IDs and IoU for precise word alignment between GT and predictions.
- Core assumption: Each word can be uniquely identified by its bounding box coordinates.
- Evidence anchors: [abstract], [section 3.1]
- Break condition: Overlapping words may cause IoU-based assignment to fail.

### Mechanism 2
- Claim: Grouping and ordering errors are measured by comparing block definitions and reading orders.
- Mechanism: Compares GT block definitions vs. predicted blocks and reading order within/across blocks.
- Core assumption: Semantic structure can be represented as blocks with defined reading orders.
- Evidence anchors: [abstract], [section 3.2]
- Break condition: Human annotator disagreement on block definitions may cause inconsistent results.

### Mechanism 3
- Claim: Superblock-based sacreBLEU enables automatic evaluation from OCR to machine translation.
- Mechanism: Maps OCR blocks to ground truth translations using equivalence classes for automatic BLEU score computation.
- Core assumption: OCR-defined blocks can be mapped to ground truth translations.
- Evidence anchors: [abstract], [section 3.4]
- Break condition: Misalignment between OCR blocks and translation units may not reflect true MT quality.

## Foundational Learning

- Concept: Intersection-over-Union (IoU) for bounding box overlap
  - Why needed here: Determines if predicted word box corresponds to ground truth word box
  - Quick check question: What is the formula for IoU between two bounding boxes?

- Concept: Word Error Rate (WER) and its components
  - Why needed here: Core metric for evaluating OCR performance including deletion, insertion, substitution, and grouping/ordering errors
  - Quick check question: How is WER calculated from the number of errors and ground truth words?

- Concept: Equivalence classes for block alignment
  - Why needed here: Finds best alignment between ground truth and predicted blocks when multiple annotations exist
  - Quick check question: How do you determine if two locations belong to the same equivalence class?

## Architecture Onboarding

- Component map: Word detection (Faster R-CNN) → Word recognition (CNN + CTC or Transformer) → Grouping/ordering (heuristic-based)
- Critical path: image → word detection → word recognition → grouping/ordering → location map → WER calculation
- Design tradeoffs: Modular approach enables rapid prototyping but may be less efficient than end-to-end models; heuristic-based grouping/ordering may struggle with curved text
- Failure signatures: High deletion errors indicate poor detection recall; high insertion errors indicate poor detection precision; high substitution errors indicate poor recognition accuracy; high GO errors indicate poor grouping/ordering logic
- First 3 experiments:
  1. Evaluate word detection alone using WER(detection) = (D+I)/|G| with IoU threshold 0.5
  2. Evaluate word recognition alone using WER(recognition) = (S+D)/|G| with ground truth bounding boxes
  3. Evaluate grouping/ordering alone using WER(grouping&ordering) = GO/|G| with ground truth words and boxes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the DISGO metric perform when applied to languages with complex character sets, such as Chinese or Arabic, compared to Latin-based scripts?
- Basis in paper: [inferred] Paper focuses on English and Spanish but mentions 110 alphabets without exploring complex scripts
- Why unresolved: No empirical data or theoretical analysis on how DISGO handles languages with non-Latin scripts
- What evidence would resolve it: Experiments with DISGO on diverse languages including complex scripts, compared with existing metrics

### Open Question 2
- Question: Can the DISGO metric be extended to evaluate OCR systems that use transformer-based models with "soft" bounding boxes instead of traditional rectangular ones?
- Basis in paper: [explicit] Mentions transformer-based models use "soft" boundaries based on attentions
- Why unresolved: Paper doesn't address adaptation for transformer models without traditional bounding boxes
- What evidence would resolve it: Modified DISGO version handling "soft" bounding boxes tested on transformer-based OCR systems

### Open Question 3
- Question: How does the inclusion of linguistic knowledge through language models impact the performance of the DISGO metric in terms of grouping and ordering errors?
- Basis in paper: [inferred] Discusses incorporating linguistic knowledge via language models but doesn't analyze impact on grouping/ordering errors
- Why unresolved: No detailed analysis of language models' impact on grouping/ordering components of DISGO
- What evidence would resolve it: Experiments measuring effect of language models on grouping/ordering errors within DISGO framework

## Limitations
- Metric effectiveness unproven beyond SCUT-CTW1500 dataset without validation on diverse datasets
- Heuristic-based grouping/ordering may not generalize to complex layouts with curved text
- Claims about metric superiority lack comparative experiments with existing evaluation methods

## Confidence

**High Confidence**: Basic WER calculation methodology and IoU-based word alignment are well-established techniques with sound mathematical framework.

**Medium Confidence**: Extension to grouping/ordering error measurement and superblock-based BLEU computation are novel but require more extensive validation with limited empirical evidence.

**Low Confidence**: Claims about metric's superiority over existing methods are not substantiated with comparative experiments or demonstrated correlation with human judgment.

## Next Checks

1. Evaluate DISGO on at least three additional public OCR datasets (e.g., ICDAR, COCO-Text) to assess metric consistency across different text layouts and languages.

2. Compare DISGO scores with human evaluation rankings of OCR system outputs to validate correlation with human judgment.

3. Test the superblock-based BLEU computation on end-to-end OCR+MT systems with varying translation quality to verify its effectiveness as an automatic evaluation metric.