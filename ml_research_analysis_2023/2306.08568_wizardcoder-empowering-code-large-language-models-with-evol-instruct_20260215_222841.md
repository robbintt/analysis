---
ver: rpa2
title: 'WizardCoder: Empowering Code Large Language Models with Evol-Instruct'
arxiv_id: '2306.08568'
source_url: https://arxiv.org/abs/2306.08568
tags:
- code
- wizardcoder
- llms
- humaneval
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces WizardCoder, a code-focused Large Language
  Model (LLM) that achieves state-of-the-art (SOTA) performance on multiple code generation
  benchmarks. The authors adapt the Evol-Instruct method to the code domain, evolving
  the Code Alpaca dataset with code-specific evolutionary prompts and fine-tuning
  StarCoder with the evolved data.
---

# WizardCoder: Empowering Code Large Language Models with Evol-Instruct

## Quick Facts
- **arXiv ID**: 2306.08568
- **Source URL**: https://arxiv.org/abs/2306.08568
- **Reference count**: 40
- **Primary result**: WizardCoder achieves state-of-the-art performance on multiple code generation benchmarks, outperforming both open-source and larger closed-source models.

## Executive Summary
WizardCoder is a code-focused Large Language Model that achieves state-of-the-art performance on code generation benchmarks by adapting the Evol-Instruct method to the code domain. The authors evolved the Code Alpaca dataset using code-specific evolutionary prompts and fine-tuned StarCoder with the evolved data, resulting in a model that surpasses all other open-source Code LLMs and even outperforms larger closed models like Claude and Bard on key benchmarks.

## Method Summary
The authors adapt the Evol-Instruct method to create code-specific instruction-following data by refining evolutionary instructions, simplifying prompt forms, and adding code-specific evolutions like debugging and complexity constraints. They then fine-tune StarCoder 15B with approximately 78k samples of this evolved data. The fine-tuning process uses 200 steps with batch size 512, learning rate 2e-5, and a Cosine scheduler, resulting in a model that demonstrates superior performance on code generation tasks.

## Key Results
- Achieves +22.3 pass@1 improvement on HumanEval compared to other open-source models
- Surpasses larger closed models like Claude-Plus and Bard on HumanEval and HumanEval+
- Despite being smaller, outperforms models like GPT-3.5-turbo and GPT-4 on certain benchmarks
- Demonstrates consistent performance improvements across multiple benchmarks including MBPP and DS-1000

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evolving code instructions using Evol-Instruct creates more complex and diverse prompts that improve code model performance.
- Mechanism: The paper adapts Evol-Instruct for code by refining evolutionary instructions, simplifying prompt forms, and adding code-specific evolutions like debugging and complexity constraints.
- Core assumption: More complex instructions during fine-tuning will improve the model's ability to handle real-world code generation tasks.
- Evidence anchors: [abstract] "by adapting the Evol-Instruct method to the domain of code" and [section] "enhance the capabilities of the SOTA open-source Code LLM, StarCoder"

### Mechanism 2
- Claim: Fine-tuning StarCoder with evolved code instruction data significantly improves performance on code generation benchmarks.
- Mechanism: The paper fine-tunes StarCoder 15B with approximately 78k samples of evolved instruction-following data created through the code-specific Evol-Instruct process.
- Core assumption: Fine-tuning a pre-trained code model with instruction-specific data will improve its ability to follow complex coding instructions.
- Evidence anchors: [abstract] "conduct fine-tuning of StarCoder using our newly created code instruction-following training set" and experimental results showing performance improvements

### Mechanism 3
- Claim: WizardCoder achieves state-of-the-art performance by combining code-specific instruction evolution with fine-tuning of a strong base model.
- Mechanism: The combination of adapting Evol-Instruct specifically for code with fine-tuning StarCoder creates a model that outperforms both other open-source models and even larger closed-source models.
- Core assumption: The synergy between evolved instruction complexity and strong base model architecture produces superior performance.
- Evidence anchors: [abstract] "our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard" and [section] "Our WizardCoder attains the third position in this benchmark"

## Foundational Learning

- **Concept**: Code Large Language Models and their training paradigms
  - Why needed here: Understanding the difference between pre-training on raw code data versus instruction fine-tuning is crucial to grasp why WizardCoder's approach is novel
  - Quick check question: What is the key difference between pre-training and instruction fine-tuning in the context of code models?

- **Concept**: Evaluation metrics for code generation (pass@1, HumanEval, MBPP)
  - Why needed here: The paper's performance claims are based on specific benchmarks and metrics that need to be understood to evaluate the results
  - Quick check question: What does pass@1 measure in code generation benchmarks?

- **Concept**: Evolutionary algorithms and their application to prompt generation
  - Why needed here: The Evol-Instruct method is central to the paper's approach, and understanding how evolutionary algorithms work is key to grasping the technique
  - Quick check question: How does the evolutionary process in Evol-Instruct create more complex instructions?

## Architecture Onboarding

- **Component map**: Code Alpaca dataset -> Evol-Instruct evolution -> fine-tuning dataset -> StarCoder 15B base model -> WizardCoder

- **Critical path**: Start with Code Alpaca dataset (20K samples) → Apply code-specific Evol-Instruct evolution iteratively → Monitor pass@1 on HumanEval during evolution → Stop when pass@1 declines (typically after 3 rounds) → Fine-tune StarCoder with evolved data → Evaluate on all four benchmarks

- **Design tradeoffs**: Using StarCoder as base vs. larger models (smaller model size but better performance through instruction evolution); Number of evolution rounds (more rounds create more complex data but risk overfitting); Instruction complexity vs. practical applicability (very complex instructions may not reflect real-world usage)

- **Failure signatures**: Performance degradation on HumanEval during evolution indicates overfitting to the evolved data; Poor results on MBPP or DS-1000 despite good HumanEval scores suggest lack of generalization; Failure to outperform base StarCoder indicates the evolution process isn't effective

- **First 3 experiments**: 1) Replicate the ablation study showing pass@1 changes with different numbers of evolution rounds; 2) Test the model on a held-out validation set of code generation problems not in the training data; 3) Compare performance when using different base models (e.g., CodeT5+ vs StarCoder) with the same evolved data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of WizardCoder compare to GPT-4 on code generation benchmarks when using the same evaluation methodology?
- Basis in paper: [inferred] The paper states WizardCoder outperforms other open-source models and even surpasses some closed-source models like Claude and Bard, but explicitly notes it still falls significantly behind GPT-4
- Why unresolved: The paper retrieves GPT-4 scores from external benchmarks but doesn't directly compare them under identical experimental conditions
- What evidence would resolve it: Direct head-to-head evaluation of WizardCoder and GPT-4 on the same benchmarks using identical sampling strategies and evaluation code

### Open Question 2
- Question: What is the optimal number of evolutionary rounds for the Evol-Instruct method when applied to code generation tasks?
- Basis in paper: [explicit] The ablation study shows performance peaks after 3 rounds of evolution and declines thereafter
- Why unresolved: The paper only explores up to 4 rounds and uses a fixed fine-tuning schedule; the interaction between evolution rounds and training steps is unclear
- What evidence would resolve it: Systematic exploration of different fine-tuning step counts across multiple evolution round configurations

### Open Question 3
- Question: How does WizardCoder perform on code generation tasks in programming languages other than Python?
- Basis in paper: [inferred] All benchmarks mentioned (HumanEval, HumanEval+, MBPP, DS-1000) focus on Python code generation
- Why unresolved: The paper doesn't evaluate performance on other programming languages despite StarCoder being trained on multilingual code data
- What evidence would resolve it: Evaluation on benchmarks like MBPP-Java, CodeNet, or other language-specific code generation tasks

## Limitations

- The exact evolutionary prompts and quality control mechanisms are not fully specified, making it difficult to determine whether performance gains are primarily due to the evolution process or simply increased training data volume
- Performance improvements are most pronounced on HumanEval, with more moderate gains on MBPP and DS-1000, raising questions about generalizability across different types of code generation tasks
- Comparisons with closed models like Claude and Bard should be interpreted cautiously due to potential differences in evaluation settings, model versions, or access limitations

## Confidence

- **High Confidence**: The fundamental methodology of adapting Evol-Instruct for code and fine-tuning StarCoder is sound and well-explained
- **Medium Confidence**: The claim that instruction evolution is the primary driver of performance gains has moderate support
- **Low Confidence**: The assertion that WizardCoder consistently outperforms all major closed models across all evaluation settings has low confidence

## Next Checks

1. **Ablation on Evolution Rounds**: Conduct a systematic ablation study testing WizardCoder performance with 0, 1, 2, and 4+ evolution rounds to determine the optimal point where instruction complexity provides maximum benefit without overfitting

2. **Cross-Domain Generalization Test**: Evaluate WizardCoder on code generation tasks from completely different domains or programming languages than those used in the evolved training data to assess whether the instruction evolution process creates genuinely generalizable coding capabilities

3. **Human Evaluation of Generated Code**: Perform a human evaluation study where developers assess the quality, correctness, and practicality of code generated by WizardCoder versus baseline models on real-world coding tasks, moving beyond automated pass@1 metrics to validate practical utility claims