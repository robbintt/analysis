---
ver: rpa2
title: Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning
arxiv_id: '2309.06086'
source_url: https://arxiv.org/abs/2309.06086
tags:
- learning
- tasks
- data
- network
- pocon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of catastrophic forgetting in
  continual unsupervised representation learning, where methods struggle to adapt
  to new tasks while maintaining previous knowledge. The authors propose Plasticity-Optimized
  Complementary Networks (POCON), which separates the learning of new knowledge from
  knowledge integration.
---

# Plasticity-Optimized Complementary Networks for Unsupervised Continual Learning

## Quick Facts
- arXiv ID: 2309.06086
- Source URL: https://arxiv.org/abs/2309.06086
- Reference count: 40
- Key outcome: POCON outperforms exemplar-free CURL methods (PFR, CaSSLe) by 5-9% on CIFAR-100, TinyImageNet, and ImageNet100 across many-task settings

## Executive Summary
This paper addresses catastrophic forgetting in continual unsupervised representation learning by proposing Plasticity-Optimized Complementary Networks (POCON). The method separates learning into three stages: training an expert network without regularization on current task data, integrating knowledge into a main network through adaptation and retrospection projectors, and initializing new experts from the updated main network. POCON achieves superior stability-plasticity trade-off compared to existing exemplar-free approaches, showing 5-9% performance gains in many-task scenarios (50-100 tasks) across multiple datasets.

## Method Summary
POCON implements a three-stage continual learning process for unsupervised representation learning. First, an expert network trains on current task data using BarlowTwins self-supervised learning without any regularization. Second, knowledge integrates into the main network through adaptation and retrospection projectors in a knowledge distillation phase. Third, a new expert initializes from the updated main network using either weight copying (homogeneous) or projected distillation (heterogeneous). This separation of plasticity (expert training) and stability (knowledge integration) phases eliminates the trade-off bottleneck in traditional regularization-based CURL methods.

## Key Results
- POCON achieves 5-9% performance gains over PFR and CaSSLe on CIFAR-100 in many-task settings (50-100 tasks)
- Outperforms existing methods on TinyImageNet and ImageNet100 across different task splits
- Demonstrates superior stability-plasticity trade-off with consistent performance improvements
- Shows adaptability to semi-supervised continual learning scenarios

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Separating plasticity and stability phases eliminates the plasticity-stability trade-off bottleneck in traditional CURL methods.
- **Mechanism**: The expert network trains without regularization, achieving maximum plasticity for the current task. The main network then integrates this knowledge while preserving previous tasks through distillation, ensuring stability.
- **Core assumption**: Knowledge integration through distillation is sufficient to prevent catastrophic forgetting without regularizing the expert during training.
- **Evidence anchors**:
  - [abstract]: "We propose to train an expert network that is relieved of the duty of keeping the previous knowledge and can focus on performing optimally on the new tasks (optimizing plasticity). In the second phase, we combine this new knowledge with the previous network in an adaptation-retrospection phase to avoid forgetting."
  - [section 3.3]: "We allow the expert network to be fully plastic and optimal for learning representation for the current task."
- **Break condition**: If the distillation process fails to adequately transfer knowledge, catastrophic forgetting will occur despite the separation of phases.

### Mechanism 2
- **Claim**: The complementary learning system framework enables better knowledge consolidation than single-network approaches.
- **Mechanism**: The fast-expert learner rapidly adapts to new data while the slow-main network gradually integrates new knowledge, mimicking biological memory systems.
- **Core assumption**: The dual-network architecture with staged training provides a better stability-plasticity balance than regularization-based methods.
- **Evidence anchors**:
  - [abstract]: "Complementary learning systems (CLS) theory [40, 31] proposes a computational framework in which the interplay between a fast (episodic memory/specific experience) and a slow (semantic/general structured) memory system is the core of the mechanism of knowledge consolidation."
  - [section 3.3]: "POCON is composed of three stages training... Each stage is explained in the details in the next sections."
- **Break condition**: If the main network cannot effectively absorb knowledge from the expert, the system's performance will degrade over time.

### Mechanism 3
- **Claim**: Heterogeneous network architectures can improve performance in low-data regimes.
- **Mechanism**: Using smaller networks for the expert (or both networks) when data is limited allows more efficient learning without sacrificing representation quality.
- **Core assumption**: Smaller networks can achieve comparable performance to larger ones when data is scarce, and the distillation process can effectively transfer knowledge between different architectures.
- **Evidence anchors**:
  - [section 4.2]: "An expert in POCON can use a different network architecture than the main network. That opens the possibility of using a smaller network for the expert whenever this can be beneficial, e.g., tasks are small with not enough data to train a large ResNet-18 network, or the device where the expert network is being trained is not powerful enough (robot, edge)."
  - [section 4.2]: "With increasing number of tasks it is more beneficial to use smaller expert (20 tasks). And having less data per task (50 and 100 tasks) we see improved results when as well the main network is smaller, we gain 4.6% changing from ResNet-18 to ResNet-9 for one hundred tasks."
- **Break condition**: If the architecture difference is too large, the distillation process may fail to effectively transfer knowledge.

## Foundational Learning

- **Concept**: Self-supervised learning (SSL) and contrastive learning
  - Why needed here: POCON builds upon SSL frameworks like BarlowTwins to learn representations without labels, which is fundamental to the method's operation
  - Quick check question: Can you explain how BarlowTwins uses cross-correlation to learn invariant representations without explicit negative samples?

- **Concept**: Catastrophic forgetting and continual learning
  - Why needed here: Understanding catastrophic forgetting is crucial to grasp why POCON's separation of plasticity and stability phases is necessary
  - Quick check question: What are the main differences between regularization-based, replay-based, and architectural approaches to mitigating catastrophic forgetting?

- **Concept**: Knowledge distillation and representation transfer
  - Why needed here: The core mechanism of POCON relies on distilling knowledge from the expert to the main network, making understanding distillation techniques essential
  - Quick check question: How does knowledge distillation differ from traditional supervised learning, and what are its key challenges in continual learning scenarios?

## Architecture Onboarding

- **Component map**:
  - Expert network (g_ϕ) -> BarlowTwins self-supervised learning -> Current task representations
  - Main network (f_θ) <- Adaptation projector (n) <- Expert embeddings
  - Main network (f_θ) <- Retrospection projector (m) <- Current task embeddings
  - Expert network (g_ϕ) <- Initialization from main network

- **Critical path**:
  1. Train expert network on current task data using BarlowTwins self-supervised learning (Stage 1)
  2. Integrate expert knowledge into main network using adaptation and retrospection projectors (Stage 2)
  3. Initialize new expert from updated main network (Stage 3)
  4. Repeat for each new task

- **Design tradeoffs**:
  - Homogeneous vs. heterogeneous architectures: Homogeneous provides simpler knowledge transfer but less flexibility; heterogeneous allows adaptation to different computational constraints but requires more complex initialization
  - Number of distillation epochs: More epochs improve knowledge transfer but increase training time
  - Projector architecture: More complex projectors may improve adaptation but increase computational overhead

- **Failure signatures**:
  - Performance degradation on previous tasks: Indicates insufficient knowledge retention during integration
  - Slow learning on new tasks: Suggests poor initialization of the expert network
  - High computational cost: May indicate inefficient architecture choices or excessive distillation epochs

- **First 3 experiments**:
  1. Compare POCON with baseline CURL methods (PFR, CaSSLe) on CIFAR-100 with 10 tasks to verify performance improvements
  2. Test homogeneous vs. heterogeneous setups on CIFAR-100 with increasing task numbers to evaluate architectural flexibility
  3. Evaluate task-free setting performance on CIFAR-100 with beta=4 to assess adaptation to continuous data streams

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the heterogeneous architecture setup (D2eOP) compare to the homogeneous setup (CopyOP) in terms of performance and computational efficiency across different datasets and task splits?
- Basis in paper: [explicit] The paper discusses the heterogeneous setup (D2eOP) as an alternative to the homogeneous setup (CopyOP), mentioning its potential benefits for low-data regimes and edge devices.
- Why unresolved: The paper provides results comparing the two setups only for CIFAR-100. It would be valuable to know how these setups perform on other datasets and in different task split scenarios.
- What evidence would resolve it: Additional experimental results comparing D2eOP and CopyOP across various datasets (e.g., TinyImageNet, ImageNet100) and different task splits (e.g., 4 tasks, 50 tasks, 100 tasks).

### Open Question 2
- Question: What is the optimal balance between the adaptation projector and retrospection projector in the knowledge integration stage (Stage 2) for maximizing stability and plasticity?
- Basis in paper: [inferred] The paper introduces both adaptation and retrospection projectors in Stage 2, but does not provide a detailed analysis of their individual contributions to the overall performance.
- Why unresolved: Understanding the optimal balance between these projectors could lead to further improvements in the stability-plasticity trade-off.
- What evidence would resolve it: Ablation studies varying the relative importance of the adaptation and retrospection projectors in the loss function, and analyzing their impact on forgetting and adaptation to new tasks.

### Open Question 3
- Question: How does the proposed POCON method perform in scenarios with more severe data distribution shifts between tasks, such as domain-incremental learning?
- Basis in paper: [inferred] The paper focuses on class-incremental learning, where the data distribution shifts are relatively mild. However, it does not explore more challenging scenarios with significant domain shifts.
- Why unresolved: Evaluating POCON's performance in more challenging settings would provide insights into its robustness and generalizability.
- What evidence would resolve it: Experiments applying POCON to domain-incremental learning tasks, where the data distribution shifts significantly between tasks, and comparing its performance to existing methods.

## Limitations
- High computational overhead due to three-stage training process and knowledge distillation phases
- Mixed results with heterogeneous architectures depending on task complexity and data availability
- Performance in highly dynamic, real-world scenarios with concept drift and non-stationary distributions not evaluated

## Confidence
- High confidence: The separation of plasticity and stability phases effectively addresses catastrophic forgetting in controlled benchmark settings
- Medium confidence: The superiority of POCON over existing methods in many-task scenarios (50-100 tasks)
- Low confidence: Generalization to real-world scenarios with concept drift and non-stationary distributions

## Next Checks
1. Evaluate POCON on streaming data with concept drift to assess robustness in dynamic environments
2. Compare computational efficiency against baseline methods in terms of training time and memory usage
3. Test the method's performance when expert and main networks have significantly different architectures (e.g., ResNet-18 to MobileNet)