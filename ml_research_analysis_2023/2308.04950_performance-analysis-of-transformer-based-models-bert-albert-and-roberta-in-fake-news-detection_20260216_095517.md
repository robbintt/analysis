---
ver: rpa2
title: Performance Analysis of Transformer Based Models (BERT, ALBERT and RoBERTa)
  in Fake News Detection
arxiv_id: '2308.04950'
source_url: https://arxiv.org/abs/2308.04950
tags:
- bert
- news
- roberta
- fake
- albert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research evaluates three transformer-based models (BERT, ALBERT,
  RoBERTa) for fake news detection in Bahasa Indonesia. Models were trained and tested
  on a combined dataset of 1,583 Indonesian fake/real news articles using preprocessing
  steps like stopword removal and tokenization.
---

# Performance Analysis of Transformer Based Models (BERT, ALBERT and RoBERTa) in Fake News Detection

## Quick Facts
- arXiv ID: 2308.04950
- Source URL: https://arxiv.org/abs/2308.04950
- Reference count: 18
- Primary result: ALBERT achieves highest accuracy (87.6%), precision (86.9%), and F1-score (86.9%) for fake news detection in Bahasa Indonesia

## Executive Summary
This research evaluates three transformer-based models (BERT, ALBERT, RoBERTa) for fake news detection in Bahasa Indonesia. The study trains and tests models on a combined dataset of 1,583 Indonesian fake/real news articles using preprocessing steps like stopword removal and tokenization. ALBERT demonstrates superior performance with 87.6% accuracy and 86.9% F1-score, outperforming BERT (78% accuracy) and RoBERTa (83.3% accuracy). The study concludes that ALBERT is the most effective transformer model for detecting fake news in Indonesian text, though results were influenced by computational environment differences. Future work may explore data augmentation and tokenizer optimization for further performance improvements.

## Method Summary
The study trains and evaluates BERT, ALBERT, and RoBERTa models on a combined dataset of 1,583 Indonesian fake/real news articles from three sources. Models are preprocessed with stopword removal and tokenization using model-specific tokenizers (BertTokenizer, BertTokenizerFast, RobertaTokenizer). Training uses PyTorch and HuggingFace libraries with hyperparameters including batch size 16, epoch 50, Adam optimizer, categorical crossentropy loss, dropout 0.5, and learning rate 2e-5. Performance is measured via accuracy, precision, F1-score, and runtime per epoch.

## Key Results
- ALBERT achieves highest accuracy (87.6%), precision (86.9%), and F1-score (86.9%) with runtime of 174.5 seconds per epoch
- BERT achieves 78% accuracy, lowest among the three models
- RoBERTa achieves 83.3% accuracy with runtime of 176.8 seconds per epoch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ALBERT's parameter reduction techniques enable faster training and better performance with fewer parameters
- Mechanism: ALBERT uses factorized embedding parameterization and cross-layer parameter sharing to reduce parameters, which improves training efficiency and acts as regularization to stabilize training
- Core assumption: Parameter reduction maintains model expressiveness while improving efficiency
- Evidence anchors:
  - [abstract] ALBERT achieved highest accuracy (87.6%), precision (86.9%), and F1-score (86.9%) with runtime of 174.5 seconds per epoch
  - [section] ALBERT configuration comparable to BERT-large has 18x fewer parameters and is 1.7x faster
  - [corpus] Weak evidence - no corpus neighbors directly discuss ALBERT's parameter reduction
- Break condition: If reduced parameters significantly degrade model capacity for complex language tasks, performance would drop despite efficiency gains

### Mechanism 2
- Claim: ALBERT's sentence-order prediction (SOP) loss improves inter-sentence coherence understanding compared to BERT's next sentence prediction (NSP) loss
- Mechanism: SOP focuses on predicting the correct order of sentences, addressing NSP's inefficiency in capturing sentence relationships
- Core assumption: Sentence order prediction is more effective than next sentence prediction for understanding text coherence
- Evidence anchors:
  - [abstract] SOP is added to boost ALBERT's performance by focusing on inter-sentence coherence
  - [section] SOP overcomes NSP's inefficiency in capturing sentence coherence
  - [corpus] Weak evidence - corpus neighbors don't discuss SOP vs NSP differences
- Break condition: If datasets don't contain meaningful sentence order relationships, SOP would provide no advantage over NSP

### Mechanism 3
- Claim: RoBERTa's improved training techniques (no NSP, larger datasets, longer training) enhance performance over BERT
- Mechanism: Removing NSP, using larger datasets, longer training duration, and dynamic masking improve contextual understanding and model robustness
- Core assumption: These training modifications address BERT's limitations without introducing new problems
- Evidence anchors:
  - [abstract] RoBERTa uses more sophisticated training techniques and larger data for better performance
  - [section] RoBERTa removes NSP, uses optimal MLM, longer training, and data augmentation
  - [corpus] Weak evidence - corpus neighbors don't specifically validate RoBERTa's training modifications
- Break condition: If training modifications lead to overfitting or don't generalize across different language tasks, performance gains would be limited

## Foundational Learning

- Concept: Transformer architecture with self-attention mechanism
  - Why needed here: All models (BERT, ALBERT, RoBERTa) are transformer-based, and understanding self-attention is crucial for understanding their differences
  - Quick check question: How does the self-attention mechanism allow transformers to process text in parallel rather than sequentially?

- Concept: Pre-training vs fine-tuning distinction
  - Why needed here: Models are pre-trained on large corpora then fine-tuned for specific tasks like fake news detection
  - Quick check question: What is the key difference between pre-training and fine-tuning phases in transformer models?

- Concept: Tokenization and vocabulary handling
  - Why needed here: Different models use different tokenizers (BertTokenizer, BertTokenizerFast, RobertaTokenizer), affecting performance
  - Quick check question: How does tokenization impact the input representation and subsequent model performance?

## Architecture Onboarding

- Component map: Indonesian text articles (1,583 samples) -> Preprocessing (stopword removal, tokenization) -> Models (BERT, ALBERT, RoBERTa) -> Training (50 epochs, batch size 16) -> Evaluation (accuracy, precision, F1-score, runtime)

- Critical path: Data preprocessing → Model training → Performance evaluation → Comparison

- Design tradeoffs:
  - Model size vs performance: ALBERT offers better performance with fewer parameters
  - Training time vs accuracy: RoBERTa trains faster but with lower accuracy than ALBERT
  - Tokenizer choice: Different tokenizers may significantly impact model performance

- Failure signatures:
  - Poor accuracy despite proper implementation may indicate tokenizer incompatibility with Indonesian language
  - Inconsistent results across runs may suggest random seed or data split issues
  - Runtime significantly different from reported values may indicate hardware or implementation differences

- First 3 experiments:
  1. Compare tokenizer performance: Train ALBERT with different tokenizers (BertTokenizer vs BertTokenizerFast) on the same dataset
  2. Hyperparameter sensitivity: Vary learning rate (1e-5, 2e-5, 3e-5) and observe impact on ALBERT performance
  3. Data augmentation impact: Apply simple data augmentation techniques and measure effect on RoBERTa performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of transformer models change with larger and more diverse Indonesian fake news datasets?
- Basis in paper: [explicit] The paper notes they used a combined dataset of 1,583 articles and suggests future work may explore data augmentation
- Why unresolved: The current study used a relatively small dataset for Indonesian fake news, which may limit the generalizability of results
- What evidence would resolve it: Performance comparisons of BERT, ALBERT, and RoBERTa models trained and tested on significantly larger Indonesian fake news datasets

### Open Question 2
- Question: How does tokenizer selection specifically impact the performance of RoBERTa and ALBERT models for Indonesian fake news detection?
- Basis in paper: [explicit] The paper states that tokenizer inclusion is crucial for BERT models but has not been examined for RoBERTa and ALBERT
- Why unresolved: The study used default tokenizers for RoBERTa and ALBERT without exploring alternative tokenizer options
- What evidence would resolve it: Systematic comparison of different tokenizer approaches for RoBERTa and ALBERT on Indonesian fake news datasets

### Open Question 3
- Question: What specific hyperparameter tuning strategies could improve transformer model performance beyond the current standardized settings?
- Basis in paper: [explicit] The paper suggests investigating other hyperparameter tunings could enhance performance and used standardized hyperparameters across all models
- Why unresolved: The study used uniform hyperparameters (batch size 16, epoch 50, etc.) without exploring model-specific optimization
- What evidence would resolve it: Comparative analysis of model-specific hyperparameter optimization versus standardized settings for Indonesian fake news detection

## Limitations

- Computational environment differences may affect runtime measurements and model convergence
- Limited dataset size (1,583 samples) may not fully represent the complexity of fake news detection in real-world scenarios
- The study does not account for Indonesian language-specific nuances that may impact tokenizer performance across different models

## Confidence

- **High confidence**: ALBERT achieving superior accuracy (87.6%) and F1-score (86.9%) compared to BERT and RoBERTa in this specific Indonesian dataset
- **Medium confidence**: The runtime performance differences (ALBERT at 174.5 seconds per epoch) are influenced by computational environment variables not fully controlled
- **Low confidence**: Claims about ALBERT's superiority in handling Indonesian language specifically require validation on larger, more diverse datasets

## Next Checks

1. Replicate the experiments across multiple computational environments with identical hardware specifications to isolate model performance from system-level effects
2. Test tokenizer performance variations by training ALBERT with different Indonesian-specific tokenizers and measuring impact on accuracy and F1-score
3. Validate findings on a larger Indonesian fake news dataset (minimum 5,000 samples) to assess whether performance trends hold with increased data complexity