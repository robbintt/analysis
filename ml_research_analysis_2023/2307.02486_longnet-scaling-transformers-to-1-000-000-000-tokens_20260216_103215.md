---
ver: rpa2
title: 'LongNet: Scaling Transformers to 1,000,000,000 Tokens'
arxiv_id: '2307.02486'
source_url: https://arxiv.org/abs/2307.02486
tags:
- attention
- sequence
- length
- long
- dilated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LongNet introduces dilated attention to scale Transformers to sequences
  of over 1 billion tokens while maintaining performance on shorter sequences. The
  key innovation is attention allocation that decreases exponentially with distance,
  achieving linear computation complexity and logarithmic token dependency.
---

# LongNet: Scaling Transformers to 1,000,000,000 Tokens

## Quick Facts
- arXiv ID: 2307.02486
- Source URL: https://arxiv.org/abs/2307.02486
- Reference count: 26
- LongNet scales Transformers to sequences over 1 billion tokens with linear computation complexity

## Executive Summary
LongNet introduces dilated attention to scale Transformers to sequences exceeding 1 billion tokens while maintaining performance on shorter sequences. The key innovation is attention allocation that decreases exponentially with distance, achieving linear computation complexity and logarithmic token dependency. This enables efficient distributed training across GPU devices by partitioning the sequence dimension, overcoming memory and computation constraints. Experiments show LongNet outperforms sparse Transformers and vanilla Transformers on language modeling tasks.

## Method Summary
LongNet replaces standard self-attention with dilated attention, where the attention allocation decreases exponentially with distance. The model uses a mixture of dilated attentions with different segment sizes and dilation rates to capture both short-range and long-range dependencies. For distributed training, LongNet partitions the sequence dimension across GPU devices, using all-gather and reduce-scatter operations for key-value pairs when needed. The approach maintains constant communication cost regardless of sequence length.

## Key Results
- LongNet achieves linear computation complexity (O(Nd)) compared to quadratic for standard attention
- The model successfully scales to 1 billion tokens while maintaining performance on shorter sequences
- LongNet outperforms sparse Transformers and vanilla Transformers on language modeling tasks with better perplexity scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dilated attention achieves linear computation complexity while maintaining logarithmic token dependency.
- Mechanism: The attention allocation decreases exponentially with distance, allowing each query to attend to a subset of keys spaced at intervals that grow exponentially. This reduces the number of attention operations from quadratic to linear while preserving the ability to access distant tokens through the exponential spacing.
- Core assumption: The exponential spacing of attended tokens provides sufficient coverage of the sequence to capture dependencies while dramatically reducing computation.
- Evidence anchors:
  - [abstract] "Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between tokens"
  - [section 2.4] "the computation complexity of dilated attention is approximate to O(N d)" and "the token dependency is approximate to O(log N)"
  - [corpus] Weak - no direct citations to similar exponential spacing mechanisms

### Mechanism 2
- Claim: The distributed training algorithm enables scaling to 1 billion tokens by partitioning the sequence dimension.
- Mechanism: By splitting the input sequence along the sequence dimension and using all-gather/reduce-scatter operations for key-value pairs when needed, the algorithm distributes computation and communication across multiple GPU devices. The communication cost remains constant because the size of gathered key-value pairs is independent of the total sequence length.
- Core assumption: Modern distributed systems can handle the all-gather and reduce-scatter operations efficiently enough that communication overhead doesn't become prohibitive.
- Evidence anchors:
  - [section 3.1] "We take advantage of the linear computation complexity of LONG NET for the distributed training of sequence dimension" and "the communication cost constant"
  - [section 3.2] "We verify the feasibility of scaling to 1B tokens with the modern distributed systems" and "LONG NET can successfully scale up the sequence length with almost constant latency"
  - [corpus] Weak - no direct citations to similar distributed sequence partitioning approaches

### Mechanism 3
- Claim: The mixture of dilated attentions with different segment sizes and dilation rates captures both short-range and long-range dependencies efficiently.
- Mechanism: By using multiple attention patterns with different (segment size, dilation rate) pairs, the model can attend precisely to local context while also approximating global context. The dynamic weights calculated from the attention softmax denominator provide better mixing than fixed weights.
- Core assumption: The combination of multiple dilated attention patterns can approximate the full attention matrix well enough for practical performance.
- Evidence anchors:
  - [section 2.2] "To capture both long-range and short-range information efficiently, we implement a mixture of dilated attentions with different segment sizes and dilation rates"
  - [section 2.2] "Experiments show that dynamic weights calculated by the denominator of the attention softmax are better than learnable fixed weights"
  - [corpus] Weak - no direct citations to similar multi-scale dilated attention approaches

## Foundational Learning

- Concept: Self-attention mechanism and its quadratic complexity
  - Why needed here: Understanding why standard transformers struggle with long sequences is fundamental to appreciating the problem LongNet solves
  - Quick check question: What is the computational complexity of standard self-attention in terms of sequence length N and hidden dimension d?

- Concept: Sparse attention patterns and their trade-offs
  - Why needed here: LongNet builds on sparse attention concepts but introduces a novel approach, so understanding sparse attention is crucial
  - Quick check question: How do sparse attention patterns like those in Sparse Transformer reduce computation complexity, and what is their limitation?

- Concept: Distributed training paradigms (data parallelism, model parallelism, pipeline parallelism)
  - Why needed here: LongNet introduces a novel distributed training approach for the sequence dimension, which requires understanding existing paradigms
  - Quick check question: What are the key differences between data parallelism, model parallelism, and pipeline parallelism in distributed training?

## Architecture Onboarding

- Component map:
  - Input projection: Q, K, V matrices are projected from input embeddings
  - Dilated attention layers: Multiple attention heads with different (segment size, dilation rate) pairs
  - Mixing layer: Combines outputs from different dilated attention patterns using dynamic weights
  - Output projection: Projects combined attention output back to model dimension
  - Distributed training module: Handles sequence dimension partitioning and cross-device communication

- Critical path:
  1. Input embedding projection to Q, K, V
  2. Segment and sparsify Q, K, V for each dilated attention pattern
  3. Compute attention scores and apply softmax
  4. Mix outputs from different patterns using dynamic weights
  5. Project output and feed to next layer
  6. During distributed training: handle all-gather/reduce-scatter operations as needed

- Design tradeoffs:
  - Segment size vs. dilation rate: Larger segments with larger dilation rates reduce computation but may miss important short-range dependencies
  - Number of attention patterns: More patterns improve approximation quality but increase computation and memory usage
  - Communication vs. computation in distributed training: Larger gather operations reduce computation per device but increase communication overhead

- Failure signatures:
  - Performance degradation on short sequences: Indicates the dilated attention approximation is too coarse
  - Poor scaling to very long sequences: Suggests the distributed training algorithm has communication bottlenecks
  - Memory issues: May indicate the segment size or number of attention patterns is too large for available memory

- First 3 experiments:
  1. Compare LongNet with standard transformer on a language modeling task with sequences of varying lengths (2K, 8K, 32K) to verify performance on short sequences
  2. Measure computation time and memory usage as sequence length scales from 2K to 1M tokens to verify linear complexity
  3. Implement distributed training with 2, 4, and 8 GPU devices on a 1M token sequence to measure scaling efficiency and identify communication bottlenecks

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- The approximation quality of dilated attention for capturing long-range dependencies needs more rigorous validation across diverse domains
- Distributed training implementation details are sparse, making practical scalability limits difficult to assess
- Experimental validation focuses heavily on language modeling with limited exploration of tasks requiring precise long-range reasoning

## Confidence

**High Confidence Claims:**
- Dilated attention achieves linear computation complexity and logarithmic token dependency - mathematically proven
- LongNet can scale to 1B tokens using distributed training - empirical evidence provided

**Medium Confidence Claims:**
- LongNet outperforms sparse Transformers and vanilla Transformers on language modeling - supported by experimental results but limited comparison
- The mixture of dilated attentions with dynamic weights provides better approximation than fixed weights - supported by ablation studies but task-dependent

**Low Confidence Claims:**
- LongNet seamlessly integrates with existing Transformer optimizations - mentioned as benefit but no concrete examples
- The model follows power-law scaling with compute - claimed based on scaling curves but needs broader validation

## Next Checks

1. **Ablation Study on Dilation Parameters**: Systematically vary segment sizes and dilation ratios to identify optimal configuration for different sequence length ranges and determine if current configuration is Pareto-optimal.

2. **Communication Overhead Benchmarking**: Implement distributed training algorithm and measure actual communication costs across different cluster sizes (2, 4, 8, 16 devices) and network bandwidths to identify when communication overhead dominates computation.

3. **Cross-Task Generalization Test**: Evaluate LongNet on diverse tasks including document-level QA, cross-document summarization, and multi-modal reasoning to understand practical limitations of the approximation and compare performance degradation with sparse Transformers.