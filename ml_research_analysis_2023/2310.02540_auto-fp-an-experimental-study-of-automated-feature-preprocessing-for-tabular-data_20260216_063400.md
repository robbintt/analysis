---
ver: rpa2
title: 'Auto-FP: An Experimental Study of Automated Feature Preprocessing for Tabular
  Data'
arxiv_id: '2310.02540'
source_url: https://arxiv.org/abs/2310.02540
tags:
- search
- auto-fp
- algorithms
- feature
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the automated feature preprocessing (Auto-FP)
  problem for classical machine learning models on tabular data. The key idea is to
  model Auto-FP as either a hyperparameter optimization (HPO) or a neural architecture
  search (NAS) problem, which enables extending existing HPO and NAS algorithms to
  solve it.
---

# Auto-FP: An Experimental Study of Automated Feature Preprocessing for Tabular Data

## Quick Facts
- arXiv ID: 2310.02540
- Source URL: https://arxiv.org/abs/2310.02540
- Reference count: 40
- Key outcome: Evolution-based algorithms, especially PBT, achieve best overall performance in automated feature preprocessing, with random search as a strong baseline

## Executive Summary
This paper presents a comprehensive experimental study of Automated Feature Preprocessing (Auto-FP) for classical machine learning models on tabular data. The authors model Auto-FP as either a hyperparameter optimization (HPO) or neural architecture search (NAS) problem, enabling the adaptation of existing search algorithms. Experiments across 45 public ML datasets and three downstream models (LR, XGB, MLP) reveal that evolution-based algorithms, particularly Population-Based Training (PBT), outperform other approaches. Surprisingly, random search serves as a strong baseline, with many sophisticated algorithms underperforming. The study also proposes two approaches (One-step and Two-step) for extending Auto-FP to support parameter search and demonstrates that Auto-FP outperforms the feature preprocessing modules of popular AutoML tools.

## Method Summary
The study extends 15 search algorithms from HPO and NAS domains to solve the Auto-FP problem, testing them across 45 public ML datasets with numerical features. Each algorithm is evaluated under time constraints (60-3600 seconds) on three downstream models (LR, XGB, MLP). The Auto-FP problem is modeled as either HPO (treating preprocessing pipelines as hyperparameters) or NAS (treating them as chain-structured architectures). The search space consists of 7 preprocessors (StandardScaler, MaxAbsScaler, MinMaxScaler, Normalizer, PowerTransformer, QuantileTransformer, Binarizer) with pipeline length constraints. Performance is measured by downstream model accuracy on validation data, with experiments designed to identify the most effective algorithms, analyze performance bottlenecks, and explore parameter search integration.

## Key Results
- Evolution-based algorithms, especially PBT, achieve the highest overall average ranking across all datasets and models
- Random search is a surprisingly strong baseline, with many sophisticated algorithms (RL-based, bandit-based, most surrogate-model-based) underperforming it
- No frequent patterns emerge in optimal preprocessing pipelines, indicating high context-dependency
- Data size reduction is identified as a promising direction for improving performance bottlenecks
- The One-step approach is preferred for low-cardinality search spaces while Two-step is better for high-cardinality spaces when supporting parameter search

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Modeling Auto-FP as either HPO or NAS enables the reuse of existing search algorithms, significantly reducing development effort.
- **Mechanism:** The Auto-FP problem is transformed into a search over preprocessor sequences, which mirrors the structure of hyperparameter tuning or neural architecture search. This allows direct application of algorithms like Random Search, TPE, PBT, etc.
- **Core assumption:** The search space structure of Auto-FP is sufficiently similar to HPO/NAS that existing algorithms can be adapted without fundamental redesign.
- **Evidence anchors:**
  - [abstract]: "Interestingly, we observe that Auto-FP can be modelled as either a hyperparameter optimization (HPO) or a neural architecture search (NAS) problem."
  - [section 3.2]: "Auto-FP as HPO. HPO aims to find the best combination of classifier and its related hyperparameters... Auto-FP as NAS. NAS aims to find the best neural architecture... Auto-FP can be modelled as the chain-structure NAS problem."
- **Break condition:** If the preprocessing pipeline search space has fundamentally different properties (e.g., strong sequential dependencies, non-composable preprocessors), existing HPO/NAS algorithms may underperform or fail to converge.

### Mechanism 2
- **Claim:** Evolution-based algorithms outperform others because they balance exploration and exploitation while maintaining low per-iteration overhead.
- **Mechanism:** Algorithms like PBT maintain a population of pipelines, iteratively mutating and evaluating the best performers. This allows directed search without the high overhead of surrogate model fitting.
- **Core assumption:** The evaluation cost of a preprocessing pipeline is dominated by downstream model training, not by the search algorithm's internal operations.
- **Evidence anchors:**
  - [abstract]: "Overall, evolution-based algorithms show the leading average ranking."
  - [section 5.2]: "Evolution-based algorithms, especially PBT, give the highest overall average ranking... They have more exploitation with a similar small overhead for picking up the next pipeline compared to RS."
  - [section 5.3]: "‘Train’ is the bottleneck in most cases... reducing data size (e.g. by sampling) is meaningful for improving the performance."
- **Break condition:** If pipeline evaluation becomes cheap (e.g., via approximate validation), algorithms with higher per-iteration overhead but better exploration (like surrogate-based methods) may surpass evolution-based methods.

### Mechanism 3
- **Claim:** The absence of frequent preprocessor patterns indicates the Auto-FP search space is complex and context-dependent, necessitating search rather than rule-based selection.
- **Mechanism:** When preprocessing pipeline performance is highly dataset-specific, no single pattern dominates, making automated search essential.
- **Core assumption:** If preprocessing effectiveness were governed by simple, universal rules (e.g., "always scale numeric features"), frequent patterns would emerge across datasets.
- **Evidence anchors:**
  - [section 5.2]: "We tried to dig out if there are frequent patterns in the best pipelines... However, the support of discovered patterns is very low, i.e. there are no obvious frequent patterns."
- **Break condition:** If future analysis reveals strong correlations between dataset characteristics and optimal preprocessing sequences, rule-based or meta-learning approaches could replace or augment search.

## Foundational Learning

- **Concept:** Hyperparameter Optimization (HPO) fundamentals
  - Why needed here: Understanding how HPO algorithms explore hyperparameter spaces is essential to adapt them for preprocessor sequences.
  - Quick check question: What is the difference between a grid search and a random search in HPO, and why might random search sometimes outperform grid search?

- **Concept:** Neural Architecture Search (NAS) chain-structure modeling
  - Why needed here: Auto-FP is modeled as chain-structure NAS, so understanding how NAS algorithms handle sequential operator selection is critical.
  - Quick check question: In chain-structure NAS, what are the two key decisions the algorithm must make at each step?

- **Concept:** Surrogate model usage in optimization
  - Why needed here: Many HPO/NAS algorithms use surrogate models (e.g., random forests, KDEs) to predict performance and guide search; understanding their limitations is key to interpreting results.
  - Quick check question: What are the main trade-offs between using a random forest vs. a Gaussian process as a surrogate model in HPO?

## Architecture Onboarding

- **Component map:** Dataset → Preprocessor sequence selection → Preprocess training data → Train downstream model → Evaluate on validation set → Update search algorithm → Repeat

- **Critical path:** Dataset → Preprocessor sequence selection → Preprocess training data → Train downstream model → Evaluate on validation set → Update search algorithm → Repeat until time budget exhausted

- **Design tradeoffs:**
  - Search space size vs. evaluation cost: Larger search spaces increase potential for better pipelines but also increase evaluation time
  - Algorithm sophistication vs. overhead: More complex algorithms (e.g., surrogate-based) may provide better direction but incur higher computational overhead per iteration
  - Pipeline length constraint: Limiting maximum pipeline length reduces search space but may exclude optimal longer pipelines

- **Failure signatures:**
  - No improvement over baseline (no preprocessing): Indicates search algorithm is not effectively exploring the space or the search space is too constrained
  - High variance in results across runs: Suggests insufficient exploration or sensitivity to initialization
  - Long evaluation times with minimal progress: Points to bottlenecks in pipeline evaluation (e.g., large dataset size, complex downstream models)

- **First 3 experiments:**
  1. **Baseline comparison:** Run Random Search for 60 seconds on a small dataset (e.g., Heart) with all three downstream models to establish baseline performance
  2. **Algorithm comparison:** Run PBT and TPE for 60 seconds on the same dataset to compare evolution-based vs. surrogate-based approaches
  3. **Data size impact:** Run the same algorithms on the Heart dataset and a larger dataset (e.g., Covtype) to observe how data size affects performance and bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can warm-starting evolution-based algorithms with meta-learning significantly improve Auto-FP performance compared to random initialization?
- Basis in paper: [inferred] The paper identifies evolution-based algorithms, especially PBT, as top performers but notes random initialization as a limitation. It mentions meta-learning has been used for warm-starting HPO and database tuning.
- Why unresolved: No experiments were conducted to compare meta-learning-based warm-starting against random initialization for Auto-FP. The potential impact on convergence speed and final accuracy remains unknown.
- What evidence would resolve it: Experiments comparing PBT with and without meta-learning-based warm-starting on the same datasets, measuring both convergence speed and final validation accuracy.

### Open Question 2
- Question: How can data size be reduced intelligently for Auto-FP without compromising downstream model performance?
- Basis in paper: [explicit] The bottleneck analysis identifies "Train" and "Prep" as bottlenecks, with smaller data size leading to shorter training and preprocessing times. The paper suggests simple random sampling but acknowledges each example and feature's unique impact on accuracy.
- Why unresolved: The paper only suggests random sampling as a simple approximation. No experiments were conducted to explore more sophisticated data reduction techniques that preserve important information.
- What evidence would resolve it: Comparative experiments using different data reduction strategies (e.g., influence-based sampling, SHAP-based feature selection) showing their impact on Auto-FP runtime and downstream model accuracy.

### Open Question 3
- Question: What is the optimal allocation of time budget between pipeline search and parameter search in Auto-FP?
- Basis in paper: [explicit] The paper explores two approaches (One-step and Two-step) for supporting parameter search and finds different approaches work better for different search space cardinalities, but notes a trade-off between pipeline and parameter search time allocation.
- Why unresolved: No systematic experiments were conducted to determine optimal time allocation strategies across different dataset characteristics and downstream models.
- What evidence would resolve it: Experiments varying the proportion of time budget allocated to pipeline vs. parameter search, measuring the resulting validation accuracy across diverse datasets and models to identify optimal allocation strategies.

### Open Question 4
- Question: Can Auto-FP be effectively benchmarked and extended to deep learning models for specific domains like recommendation systems?
- Basis in paper: [explicit] The paper mentions that deep models are mainstream in specific domains (e.g., recommendation systems) and provides preliminary results showing Auto-FP applicability to DeepFM on recommendation datasets, but notes that domain-specific deep models may require specialized search algorithms.
- Why unresolved: Only preliminary results were shown on two recommendation datasets. No comprehensive benchmarking or algorithm development was performed for domain-specific deep models.
- What evidence would resolve it: Comprehensive benchmarking of Auto-FP on multiple deep learning models across different domains (recommendation, computer vision, NLP) using appropriate feature preprocessors for each domain, comparing against domain-specific baselines.

## Limitations

- Performance of sophisticated algorithms (RL-based, bandit-based, most surrogate-model-based) was surprisingly poor compared to random search, suggesting potential implementation issues or fundamental limitations
- Analysis of frequent patterns was limited to top-3 preprocessors, potentially missing longer optimal sequences
- Study focused exclusively on numerical features, excluding common categorical and textual data in real-world tabular datasets

## Confidence

- **High Confidence:** The core finding that evolution-based algorithms (particularly PBT) outperform other approaches is well-supported by experimental results across 45 datasets and three downstream models
- **Medium Confidence:** The observation that random search is a strong baseline and many sophisticated algorithms underperform is supported, but may be influenced by implementation specifics of the adapted algorithms
- **Medium Confidence:** The identification of data size reduction as a promising direction for improving performance is based on bottleneck analysis but requires further empirical validation

## Next Checks

1. **Implementation Verification:** Re-implement the RL-based and bandit-based algorithms to verify if the poor performance is due to adaptation challenges or fundamental algorithmic limitations
2. **Extended Pattern Analysis:** Analyze frequent patterns in optimal pipelines of length 4+ preprocessors to determine if longer sequences exhibit more consistent patterns
3. **Categorical Feature Extension:** Extend the Auto-FP framework to handle categorical and textual features, evaluating whether the same algorithm performance trends hold for mixed-feature datasets