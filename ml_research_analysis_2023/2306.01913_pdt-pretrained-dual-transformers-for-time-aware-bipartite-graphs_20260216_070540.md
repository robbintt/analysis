---
ver: rpa2
title: 'PDT: Pretrained Dual Transformers for Time-aware Bipartite Graphs'
arxiv_id: '2306.01913'
source_url: https://arxiv.org/abs/2306.01913
tags:
- user
- learning
- content
- movies
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses learning contextual knowledge from user-content
  interaction data represented as bipartite graphs. The authors propose a pretraining
  method that learns bi-directional mapping between user-side and content-side contexts
  using contrastive learning with a dual Transformer architecture.
---

# PDT: Pretrained Dual Transformers for Time-aware Bipartite Graphs

## Quick Facts
- arXiv ID: 2306.01913
- Source URL: https://arxiv.org/abs/2306.01913
- Reference count: 5
- Primary result: Achieves up to 30% improvement over general recommenders and 11% over sequential recommenders on MovieLens 25M

## Executive Summary
This paper introduces PDT, a pretraining method that learns contextual knowledge from user-content interaction data represented as bipartite graphs. The method uses a dual Transformer architecture with contrastive learning to establish bidirectional mapping between user-side and content-side contexts. Evaluated on MovieLens 25M for sequential recommendation, PDT demonstrates significant improvements over baseline methods while producing interpretable embeddings that capture movie metadata like release year, ratings, and genres.

## Method Summary
PDT employs a dual Transformer architecture where user behavior history and content history are processed by separate encoders to learn representations in the same latent space. The model uses contrastive learning with InfoNCE loss to maximize mutual information between user embeddings and user-side contexts, as well as between content embeddings and content-side contexts. During fine-tuning for sequential recommendation, the pre-trained user embeddings are combined with recent behavior history to capture both long-term and short-term patterns, optimizing with a combined BPR and contrastive loss objective.

## Key Results
- Outperforms all baselines on MovieLens 25M with up to 30% improvement over general recommenders
- Achieves up to 11% better performance than sequential recommenders across 6 metrics (Recall@10/20/50, NDCG@10/20/50)
- Ablation studies confirm contributions from both user-side and content-side contextual knowledge
- Produces interpretable embeddings that capture release year, ratings, and genres

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual Transformer architecture enables mutual information maximization between user and content contexts
- Mechanism: Two separate Transformer encoders map user behavior history and content history into the same latent space, with contrastive loss aligning embeddings with their respective contexts
- Core assumption: User behavior history and content history are meaningful heterogeneous representations that can be aligned in latent space
- Evidence anchors: Abstract and section 3.1 describe the bidirectional mapping objective between embeddings and contexts
- Break condition: If user behavior history or content history fails to capture meaningful context, contrastive learning fails to learn useful representations

### Mechanism 2
- Claim: Pre-training captures both long-term and short-term patterns for recommendation
- Mechanism: Pre-training learns long-term patterns through user embeddings, while fine-tuning combines these with recent behavior history to capture short-term patterns
- Core assumption: Pre-trained user embeddings contain meaningful long-term behavioral patterns that complement short-term recent history patterns
- Evidence anchors: Section 3.2 explains the dual pattern learning insight, section 4.4 shows quantitative improvements
- Break condition: If pre-trained user embeddings fail to capture long-term patterns, the combined representation loses its advantage over sequential recommenders

### Mechanism 3
- Claim: Joint training with Lu and Lc losses enables better representation learning than single-side context modeling
- Mechanism: Training with both user-side and content-side contrastive losses forces the model to learn representations meaningful from both perspectives
- Core assumption: User-side and content-side contexts provide complementary information that cannot be fully captured by modeling only one perspective
- Evidence anchors: Section 4.5 shows ablation results comparing single-side vs joint training, section 4.6 demonstrates embedding interpretability
- Break condition: If either context contains redundant or noisy information that doesn't complement the other, joint training may degrade performance

## Foundational Learning

- Concept: Contrastive learning and InfoNCE loss
  - Why needed here: The paper formulates pre-training as maximizing mutual information between embeddings and contexts, implemented via contrastive learning with InfoNCE loss
  - Quick check question: How does minimizing InfoNCE loss relate to maximizing mutual information between two variables?

- Concept: Transformer encoder architecture and CLS token usage
  - Why needed here: The dual Transformer encoders use CLS tokens as aggregated representations of user behavior history and content history for contrastive learning
  - Quick check question: Why is the CLS token output from a Transformer encoder used as the representation for the entire sequence rather than other positions?

- Concept: Bipartite graph representation and heterogeneous representations
  - Why needed here: User-content interaction data is modeled as a bipartite graph where user behavior history and content history are defined as heterogeneous representations of the same entity
  - Quick check question: How does representing user behavior history and content history as heterogeneous representations enable the bidirectional mapping objective?

## Architecture Onboarding

- Component map: User ID → embedding layer → history encoder → CLS token → contrastive loss (pre-training) or recommendation head (fine-tuning)
- Critical path: User/content ID → embedding layer → history encoder → CLS token → contrastive loss (pre-training) or recommendation head (fine-tuning)
- Design tradeoffs:
  - Separate vs shared Transformer encoders: Separate encoders allow different context modeling but increase parameters
  - Fixed vs learned history length: Fixed length simplifies batching but may truncate important information
  - Pre-training task vs direct fine-tuning: Pre-training provides initialization but adds computational overhead
- Failure signatures:
  - Poor performance on sequential baselines suggests pre-trained embeddings fail to capture long-term patterns
  - High variance in results across seeds indicates instability in contrastive learning objective
  - Embeddings show no meaningful clustering in visualization suggests contrastive learning fails to learn structure
- First 3 experiments:
  1. Train with only Lu loss (user-side context only) and compare to full model to validate contribution of content-side context
  2. Train with only Lc loss (content-side context only) and compare to full model to validate contribution of user-side context
  3. Train without pre-training (direct fine-tuning on recommendation task) to measure value of pre-training initialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would PDT perform on datasets with significantly longer user histories compared to MovieLens 25M?
- Basis in paper: The paper uses a fixed history length of 9 for pre-training and 8 for fine-tuning but does not explore how performance scales with longer sequences
- Why unresolved: The authors did not conduct experiments with varying history lengths to identify optimal sequence length for different dataset characteristics
- What evidence would resolve it: Systematic experiments varying history length on datasets with different interaction densities and user behavior patterns would show how sequence length affects PDT performance

### Open Question 2
- Question: How does PDT handle cold-start scenarios where users or items have very few interactions?
- Basis in paper: The authors explicitly note that "cold start problem is beyond the paper's discussion" and removed movies not in training from validation and test sets
- Why unresolved: The paper does not address scenarios with sparse interactions for new users or items, which is a critical real-world challenge
- What evidence would resolve it: Experiments on datasets with explicit cold-start scenarios or synthetic cold-start conditions would demonstrate PDT's effectiveness in these situations

### Open Question 3
- Question: Would incorporating additional content features (beyond release year, ratings, and genres) improve PDT's performance?
- Basis in paper: The visualization shows PDT captures release year, ratings, and genres, but the model architecture could theoretically incorporate additional content attributes
- Why unresolved: The authors only used basic movie metadata and did not experiment with richer content representations
- What evidence would resolve it: Experiments incorporating additional features like movie descriptions, director information, or cast details would show whether richer content representations enhance performance

## Limitations

- Evaluation is limited to a single benchmark dataset (MovieLens 25M), raising questions about generalizability to other domains
- Dual Transformer architecture introduces significant computational overhead compared to standard sequential recommenders
- Claims about interpretability of learned embeddings are based on qualitative visualization without quantitative metrics or statistical analysis

## Confidence

- **High confidence**: The core mechanism of contrastive learning for mutual information maximization between embeddings and contexts is well-established and technically sound
- **Medium confidence**: Ablation studies showing contributions of user-side and content-side contexts are convincing, but the interpretation that both contexts are truly complementary rather than redundant is not definitively proven
- **Low confidence**: Claims about the interpretability of learned embeddings and their ability to capture release year, ratings, and genres are based on qualitative visualization without quantitative metrics or statistical analysis

## Next Checks

1. **Dataset generalization**: Evaluate on at least two additional bipartite graph datasets (e.g., Amazon product ratings, Last.fm music listening) to assess whether the 30% improvement over general recommenders and 11% over sequential recommenders holds across domains

2. **Ablation on history length**: Systematically vary the pre-training history length (9) and fine-tuning history length (8) to determine optimal settings and validate the claimed efficiency benefits of the dual pattern learning approach

3. **Statistical significance testing**: Conduct paired t-tests or bootstrap confidence intervals on the 6 metrics across multiple random seeds to determine whether observed improvements are statistically significant rather than due to variance in training