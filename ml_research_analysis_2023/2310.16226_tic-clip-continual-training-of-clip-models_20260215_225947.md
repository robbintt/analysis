---
ver: rpa2
title: 'TiC-CLIP: Continual Training of CLIP Models'
arxiv_id: '2310.16226'
source_url: https://arxiv.org/abs/2310.16226
tags:
- data
- training
- learning
- time
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently updating large
  vision-language models like CLIP as new data arrives over time, without the prohibitive
  cost of retraining from scratch. The authors introduce the first large-scale Time-Continual
  (TiC) benchmarks for CLIP training, including TiC-DataComp (12.7B image-text pairs),
  TiC-YFCC, and TiC-RedCaps.
---

# TiC-CLIP: Continual Training of CLIP Models

## Quick Facts
- arXiv ID: 2310.16226
- Source URL: https://arxiv.org/abs/2310.16226
- Reference count: 40
- Key outcome: Continual training with replay reduces compute by 2.5× compared to retraining from scratch while maintaining CLIP performance

## Executive Summary
This paper addresses the challenge of efficiently updating large vision-language models like CLIP as new data arrives over time, without the prohibitive cost of retraining from scratch. The authors introduce the first large-scale Time-Continual (TiC) benchmarks for CLIP training, including TiC-DataComp (12.7B image-text pairs), TiC-YFCC, and TiC-RedCaps. They show that models trained on older data (e.g., OpenAI CLIP) suffer significant performance drops on newer data (up to 8% in retrieval tasks), highlighting the need for continual adaptation. The key contribution is demonstrating that a simple rehearsal-based approach—continuing training from the last checkpoint and replaying old data—reduces computational cost by 2.5× compared to retraining from scratch, while maintaining competitive performance.

## Method Summary
The method involves warm starting from the last checkpoint and replaying old data during continual training. The approach is compared against baselines including Sequential (no replay), Patching, LwF (Learning without Forgetting), and an Oracle baseline that has access to all data. The training uses a cosine learning rate schedule with warmup only on the first step. Experiments are conducted across three TiC-CLIP benchmarks with varying scales, and models are evaluated on both static tasks (ImageNet, etc.) and dynamic time-evolving tasks (retrieval and classification over time).

## Key Results
- Models trained on older data (pre-2020) show up to 8% accuracy drop on newer data (2021-2022) compared to more recently trained models
- Rehearsal-based approach reduces compute by 2.5× compared to retraining from scratch while maintaining competitive performance
- Cumulative-All method (replay all old data) achieves performance competitive to Oracle while being 2.7× more computationally efficient
- Reducing replay buffer size with -Exp and -Equal strategies maintains performance close to -All while improving computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual training from the last checkpoint with replay of old data maintains performance close to retraining from scratch while reducing compute by ~2.5×.
- Mechanism: The model leverages previously learned representations (warm start) and periodically revisits old data to prevent catastrophic forgetting. This allows the model to adapt to new data distributions without losing performance on older data.
- Core assumption: The old data distribution is sufficiently similar to the new data that replaying it helps maintain generalization.
- Evidence anchors:
  - [abstract]: "a simple rehearsal-based approach that continues training from the last checkpoint and replays old data reduces compute by 2.5× when compared to the standard practice of retraining from scratch"
  - [section]: "Cumulative method that warm starts training with the latest checkpoint and replays all old data, achieves performance competitive to an Oracle while being 2.7× computationally more efficient"
- Break condition: If the old data distribution becomes irrelevant or highly dissimilar to new data, replay may not prevent forgetting or may even hinder adaptation.

### Mechanism 2
- Claim: The temporal robustness gap between OpenAI CLIP and OpenCLIP models highlights the need for continual adaptation to evolving data distributions.
- Mechanism: Models trained on older data (pre-2020) perform worse on newer data (2021-2022) compared to models trained on more recent data, indicating distribution shift over time.
- Core assumption: The data distribution changes significantly over time, and models must adapt to maintain performance.
- Evidence anchors:
  - [abstract]: "OpenAI's CLIP (trained on data up to 2020) loses ≈8% zero-shot accuracy on our curated retrieval task from 2021–2022 compared with more recently trained models"
  - [section]: "OpenAI models show less zero-shot robustness on retrieval task from 2021–2022... highlighting susceptibility to a time-evolving data distribution"
- Break condition: If data distributions remain stable over time, the need for continual adaptation diminishes.

### Mechanism 3
- Claim: Dynamic evaluation tasks (e.g., retrieval and classification over time) provide complementary information to static benchmarks for assessing model robustness.
- Mechanism: Static benchmarks like ImageNet do not capture temporal distribution shifts, while dynamic tasks reveal performance degradation on newer data.
- Core assumption: Temporal distribution shifts are a significant factor in real-world deployment, and static benchmarks are insufficient to measure this.
- Evidence anchors:
  - [section]: "Our findings not only demonstrate the critical need for models to adapt and evolve alongside dynamic data distributions, but also underscores the limitations of relying solely on static benchmarks"
  - [section]: "Dynamic tasks provide complimentary information for model selection compared to static tasks"
- Break condition: If temporal shifts are negligible or models are only deployed in static environments, dynamic evaluation may not be necessary.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: The paper addresses how to update CLIP models without losing performance on previously seen data, which is a core challenge in continual learning.
  - Quick check question: What is catastrophic forgetting, and why does it occur in neural networks trained on sequential tasks?

- Concept: Contrastive learning in vision-language models
  - Why needed here: CLIP is trained using a contrastive objective that aligns image and text representations, which is the foundation of the model being continually trained.
  - Quick check question: How does the contrastive loss function work in CLIP, and why is it effective for vision-language tasks?

- Concept: Distribution shift and its impact on model performance
  - Why needed here: The paper demonstrates that data distributions evolve over time, and models trained on older data perform worse on newer data, highlighting the importance of addressing distribution shift.
  - Quick check question: What is distribution shift, and how can it affect the performance of machine learning models in real-world scenarios?

## Architecture Onboarding

- Component map: CLIP model (ViT-B/16 image encoder + text encoder) -> replay buffer -> training loop -> static/dynamic evaluation tasks
- Critical path: Continual training loop: load checkpoint → sample replay data → combine with new data → train with fixed compute budget → evaluate on static and dynamic tasks
- Design tradeoffs: Replay buffer size vs. compute efficiency; warmup usage vs. training stability; maximum learning rate for sequential runs vs. convergence speed
- Failure signatures: Performance degradation on older data (backward transfer failure); poor performance on new data (forward transfer failure); increased training loss when resetting learning rate
- First 3 experiments:
  1. Compare Cumulative-All (replay all old data) vs. Sequential (no replay) on TIC-DataComp to quantify the impact of replay.
  2. Test different replay buffer strategies (-Exp vs. -Equal) to find the optimal balance between buffer size and performance.
  3. Ablate learning rate warmup for subsequent training runs to determine its effect on training stability and final performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Continual CLIP models scale with larger datasets (e.g., 100B+ image-text pairs) compared to current benchmarks?
- Basis in paper: [inferred] The paper mentions that their largest dataset is TiC-DataComp with 12.7B image-text pairs and suggests future work could expand to larger datasets.
- Why unresolved: The paper only experiments with datasets up to 12.7B image-text pairs, leaving the scalability to much larger datasets unexplored.
- What evidence would resolve it: Experiments showing the performance of Continual CLIP models on datasets with 100B+ image-text pairs, comparing them to current benchmarks and analyzing the trade-offs in terms of computational cost and model performance.

### Open Question 2
- Question: How effective are alternative learning rate schedules, such as the Const-Cosine schedule, in improving the performance of Continual CLIP models compared to the standard cosine schedule with warm-up?
- Basis in paper: [explicit] The paper introduces the Const-Cosine learning rate schedule and shows preliminary experiments that it improves performance compared to the cyclic cosine schedule.
- Why unresolved: The paper only presents preliminary experiments on Const-Cosine, and a comprehensive evaluation across different scales and datasets is needed to fully assess its effectiveness.
- What evidence would resolve it: Extensive experiments comparing the performance of Continual CLIP models using Const-Cosine and standard cosine schedules with warm-up across various scales and datasets, including dynamic and static evaluation tasks.

### Open Question 3
- Question: What is the optimal replay buffer size for Continual CLIP models to balance computational efficiency and model performance, especially when dealing with time-evolving data distributions?
- Basis in paper: [inferred] The paper explores different replay buffer strategies (-All, -Exp, -Equal) and finds that reducing the buffer size with -Exp and -Equal maintains performance close to -All while being more computationally efficient.
- Why unresolved: The paper does not determine the exact optimal replay buffer size for different scales and data distributions, and the trade-offs between buffer size, computational efficiency, and model performance are not fully explored.
- What evidence would resolve it: Systematic experiments varying the replay buffer size for Continual CLIP models across different scales and data distributions, analyzing the impact on computational efficiency and model performance on both static and dynamic tasks.

## Limitations

- Dataset construction transparency is limited, with unclear filtering criteria for Basic vs. Bestpool splits in TiC-DataComp
- Reproducibility constraints due to missing implementation details like exact replay buffer management and learning rate schedule parameters
- Generalizability to other domains beyond CLIP and image-text data is not established

## Confidence

- High Confidence: Computational efficiency claim (2.5× reduction compared to retraining) is well-supported by ablation studies
- Medium Confidence: Temporal robustness gaps demonstrate need for continual adaptation, but based on limited retrieval tasks
- Medium Confidence: Dynamic evaluation tasks provide complementary information, but systematic framework for quantifying this is lacking

## Next Checks

1. **Distribution Shift Validation**: Conduct experiments to quantify the actual distribution shift between TiC-DataComp time periods using KL divergence or other statistical measures between consecutive time bins, to verify that the observed performance gaps are indeed due to temporal distribution changes rather than other factors.

2. **Ablation of Learning Rate Strategy**: Systematically test the impact of learning rate warmup across multiple runs with varying warmup durations and maximum learning rates to establish whether the "no warmup after first step" finding holds consistently across different dataset scales and model sizes.

3. **Replay Buffer Strategy Comparison**: Implement and evaluate additional replay buffer management strategies (e.g., reservoir sampling, weighted sampling by timestamp) beyond the -Exp and -Equal approaches to determine if more sophisticated replay strategies could achieve better trade-offs between computational efficiency and performance retention.