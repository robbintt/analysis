---
ver: rpa2
title: Assessment of Differentially Private Synthetic Data for Utility and Fairness
  in End-to-End Machine Learning Pipelines for Tabular Data
arxiv_id: '2310.19250'
source_url: https://arxiv.org/abs/2310.19250
tags:
- data
- synthetic
- real
- trained
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines the use of differentially private synthetic
  tabular data for training and evaluating machine learning models, focusing on both
  utility and fairness. The study compares marginal-based and GAN-based synthetic
  data generation algorithms across multiple privacy budgets, analyzing their performance
  on real-world tabular datasets.
---

# Assessment of Differentially Private Synthetic Data for Utility and Fairness in End-to-End Machine Learning Pipelines for Tabular Data

## Quick Facts
- arXiv ID: 2310.19250
- Source URL: https://arxiv.org/abs/2310.19250
- Reference count: 40
- Primary result: Marginal-based methods (especially MWEM PGM) produce synthetic tabular data that train models with utility and fairness matching real data; GAN-based methods consistently underperform.

## Executive Summary
This paper evaluates differentially private synthetic tabular data for use in complete ML pipelines, focusing on both utility and fairness. The authors compare marginal-based methods (MWEM PGM, MST, PrivBayes) with GAN-based approaches (DP-GAN, DP-CTGAN, PATE-GAN variants) across multiple privacy budgets. Their framework enables model evaluation using only synthetic data, preserving privacy. The primary finding is that marginal-based synthesizers, particularly MWEM PGM, consistently produce synthetic data that train models achieving utility and fairness metrics closely matching those trained on real data, while GAN-based methods fail to produce useful synthetic samples.

## Method Summary
The study uses three real-world tabular datasets (Adult, COMPAS, fair COMPAS) and generates synthetic data using six algorithms across four privacy budgets (ε = {0.5, 1.0, 5.0, 10.0}). Logistic regression models are trained on synthetic training data and evaluated on both synthetic and real test sets. Utility is measured by AUC-ROC and fairness by difference in Equal Opportunity (DEO) and difference in Statistical Parity (DSP). The framework allows complete pipelines using only synthetic data, avoiding real data exposure during evaluation.

## Key Results
- MWEM PGM consistently outperforms all other synthesizers in both utility and fairness across all privacy budgets
- GAN-based methods produce near-random performance (AUC ≈ 0.5) regardless of privacy budget
- Synthetic test data generated by MWEM PGM provides reliable evaluation metrics matching real data evaluation
- Marginal-based methods show increasing utility with higher privacy budgets (ε > 5.0), while GAN-based methods show no such improvement

## Why This Works (Mechanism)

### Mechanism 1
Marginal-based synthesizers preserve utility and fairness by accurately estimating data marginals with DP noise injection, capturing sufficient joint distribution structure for model training. This works when tabular data marginals contain enough information to train effective models.

### Mechanism 2
GAN-based methods fail because DP-SGD injects excessive noise during adversarial training, destabilizing the discriminator-generator balance and producing poor-quality samples that yield random-guessing performance.

### Mechanism 3
MWEM PGM preserves subgroup distributions sufficiently well that synthetic test sets mimic real test set evaluation, enabling reliable model assessment without real data exposure.

## Foundational Learning

- **Differential Privacy (DP) and epsilon-delta guarantees**: Essential for understanding how privacy budgets affect synthetic data quality and downstream model performance.
  - Quick check: If epsilon = 1.0 and delta = 1e-5, how much more private is this than epsilon = 10.0 with delta = 1e-5?

- **Marginal distributions in tabular data**: Critical for understanding how marginal-based synthesizers capture data structure under DP constraints.
  - Quick check: Why are 2-way and 3-way marginals often sufficient for capturing most dependencies in tabular data?

- **Fairness metrics (DEO, DSP, subgroup accuracy)**: Required for precise computation and interpretation of fairness evaluation results.
  - Quick check: How does DEO differ from DSP in terms of what bias it detects?

## Architecture Onboarding

- **Component map**: Real data → DP synthetic data generation (MWEM PGM/MST/PrivBayes/DP-GAN variants) → Logistic regression training → Evaluation on real and synthetic test sets → Metric aggregation
- **Critical path**: Data generation → Model training → Fairness/utility evaluation → Comparison with baseline
- **Design tradeoffs**: Logistic regression simplifies analysis but may not represent deep learning impacts; synthetic test data evaluation saves privacy cost but may introduce bias
- **Failure signatures**: AUC ≈ 0.5 for GAN-based models; large gaps between real and synthetic evaluation metrics; high variance across runs
- **First 3 experiments**:
  1. Generate MWEM PGM synthetic data at epsilon=5.0 and train logistic regression; compare AUC to real-data baseline
  2. Generate DP-GAN synthetic data at same epsilon; observe performance drop and variance
  3. Use MWEM PGM synthetic test set to evaluate a model trained on real data; compare results to real-data evaluation

## Open Questions the Paper Calls Out

### Open Question 1
How do marginal-based synthetic data generation algorithms perform on larger-scale tabular datasets compared to their performance on the Adult and COMPAS datasets? The study's findings are based on relatively small-scale datasets, and it is unclear if the observed performance advantages will persist or change with larger datasets.

### Open Question 2
How do differentially private synthetic data generation algorithms perform on non-tabular data, such as images or time-series data? The study focuses exclusively on tabular data, leaving the performance on other data types unexplored.

### Open Question 3
What are the implications of using differentially private synthetic data for model training and evaluation in real-world applications, such as healthcare or humanitarian action, where data is scarce and regulated by restrictive privacy laws? While the study provides insights into controlled experiments, real-world implications remain unexplored.

## Limitations

- Results may not generalize to deep learning architectures since only logistic regression was evaluated
- DP-GAN variants show consistently poor performance but lack detailed architectural specifications for diagnosis
- Synthetic test data evaluation framework lacks direct validation against real test data across all conditions
- Study limited to three specific tabular datasets, raising questions about generalizability

## Confidence

- **High Confidence**: MWEM PGM's superior performance in utility and fairness preservation; marginal-based methods outperforming GAN-based methods for tabular DP synthesis
- **Medium Confidence**: Synthetic test data reliably substitutes for real test data in evaluation; specific performance thresholds where GAN-based methods fail
- **Low Confidence**: Generalization of results to deep learning models; exact mechanisms causing DP-GAN variants' consistent failure

## Next Checks

1. Replicate experiments using a neural network classifier to verify if marginal-based advantages persist in deep learning contexts
2. Conduct ablation studies on DP-GAN architectures to isolate whether noise injection or architectural choices drive poor performance
3. Compare synthetic test evaluation metrics against real test data evaluation across all privacy budgets to quantify any systematic bias in the evaluation framework