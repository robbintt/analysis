---
ver: rpa2
title: Sparse Transformer with Local and Seasonal Adaptation for Multivariate Time
  Series Forecasting
arxiv_id: '2312.06874'
source_url: https://arxiv.org/abs/2312.06874
tags:
- time
- attention
- forecasting
- steps
- dozerformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the limitations of transformers in multivariate
  time series forecasting, specifically their quadratic time complexity and reliance
  on the entire historical sequence. The proposed Dozer Attention mechanism addresses
  these issues by introducing three sparse components: Local (attends to neighboring
  time steps), Stride (attends to time steps at fixed intervals), and Vary (adaptively
  expands the historical window based on forecasting horizon).'
---

# Sparse Transformer with Local and Seasonal Adaptation for Multivariate Time Series Forecasting

## Quick Facts
- arXiv ID: 2312.06874
- Source URL: https://arxiv.org/abs/2312.06874
- Reference count: 40
- Achieves up to 40.6% lower MSE than Crossformer

## Executive Summary
This paper introduces Dozer Attention, a sparse attention mechanism for multivariate time series forecasting that addresses the quadratic time complexity of traditional transformers. The mechanism consists of three components: Local (attends to neighboring time steps), Stride (attends to time steps at fixed intervals for seasonality), and Vary (adaptively expands historical window based on forecasting horizon). The Dozerformer framework incorporates this attention mechanism and demonstrates superior performance on nine benchmark datasets compared to state-of-the-art methods.

## Method Summary
The method addresses transformer limitations in MTS forecasting by introducing Dozer Attention with three sparse components. Local attention attends to neighboring time steps based on locality assumptions, Stride captures seasonality through fixed-interval attention, and Vary dynamically adjusts historical window size based on forecasting horizon. The Dozerformer framework combines these components with a transformer encoder-decoder architecture, processing seasonal and trend components separately before combining predictions.

## Key Results
- Achieves up to 40.6% lower MSE than Crossformer on benchmark datasets
- Demonstrates linear computational complexity O((w+s)I/p) vs O(I²) for full attention
- Superior performance across nine benchmark datasets including ETTh1, ETTh2, Traffic, and Electricity

## Why This Works (Mechanism)

### Mechanism 1: Local Attention
- **Claim:** Each query attends only to keys within a time window of size w centered around the query position
- **Core assumption:** Nearby time steps have higher correlation than distant ones in MTS data
- **Evidence anchors:** Abstract mentions "Local component exclusively attends to keys within a localized window", section notes "nearby values in the time dimension displaying higher correlations"

### Mechanism 2: Stride Attention
- **Claim:** Each query attends to keys positioned at multiples of a fixed interval to capture seasonality
- **Core assumption:** Time series exhibit periodic patterns at known intervals
- **Evidence anchors:** Abstract states "Stride enables each query to attend to keys at predefined intervals", section identifies "Seasonality is a key characteristic of time series data"

### Mechanism 3: Vary Attention
- **Claim:** Dynamically adjusts historical window size based on forecasting horizon
- **Core assumption:** Different forecasting horizons require different amounts of historical context
- **Evidence anchors:** Abstract notes "Vary allows queries to selectively attend to keys from a subset of the historical sequence", section explains "the size of this subset dynamically expands as forecasting horizons extend"

## Foundational Learning

- **Concept:** Sparse attention mechanisms
  - Why needed here: Reduces quadratic complexity to linear, enabling longer sequence processing
  - Quick check question: How does Dozer Attention achieve O((w+s)I/p) complexity vs O(I²) for full attention?

- **Concept:** Seasonal decomposition
  - Why needed here: Separates trend and seasonal components for specialized modeling
  - Quick check question: What mathematical operation decomposes MTS into seasonal and trend components?

- **Concept:** Channel-independent transformer design
  - Why needed here: Processes each variable independently before combining results
  - Quick check question: Why does Dozerformer process variables independently despite MTS data being multivariate?

## Architecture Onboarding

- **Component map:** Input → Decomposition → Encoder self-attention → Decoder cross-attention → Output combination
- **Critical path:** Decomposition → Encoder self-attention → Decoder cross-attention → Output combination
- **Design tradeoffs:** Sparsity vs accuracy (more sparse = faster but potentially less accurate), window size selection, component balance allocation
- **Failure signatures:** Performance degradation with longer sequences (local window too small), poor seasonality capture (stride interval misaligned), horizon-specific failures (vary window size inappropriate)
- **First 3 experiments:**
  1. Ablation study: Test each component (Local, Stride, Vary) individually to verify contribution
  2. Parameter sensitivity: Vary w, s, v values to find optimal sparsity-accuracy tradeoff
  3. Horizon scaling: Test different look-back window sizes to validate horizon-adaptive design

## Open Questions the Paper Calls Out
- How does Dozerformer perform on multivariate time series datasets with very long seasonal periods (e.g., yearly seasonality)?
- What is the impact of the Vary component's starting size (v) on forecasting accuracy for different types of time series data?
- How does the Dozerformer's performance scale with increasing dimensionality of multivariate time series data?

## Limitations
- Performance may degrade when locality assumptions don't hold for the data
- Fixed hyperparameter selection may not generalize to datasets with irregular seasonal patterns
- Decomposition-based approach assumes MTS data can be cleanly separated into seasonal and trend components

## Confidence
- **High:** Sparse attention mechanism effectively reduces computational complexity
- **Medium:** Vary component's adaptive windowing mechanism needs more extensive validation
- **Medium:** Component balance selection may not scale well to diverse MTS datasets

## Next Checks
1. Conduct systematic ablation studies varying each sparse component independently across all nine datasets to quantify individual contributions and identify failure cases
2. Test Dozerformer on datasets with known weak locality or irregular seasonality to validate the robustness of component selection
3. Implement a learned variant of the Vary component that adapts window size based on data characteristics rather than fixed horizon rules