---
ver: rpa2
title: 'CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study'
arxiv_id: '2307.11346'
source_url: https://arxiv.org/abs/2307.11346
tags:
- medical
- samples
- disease
- report
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CohortGPT, an enhanced GPT model for participant
  recruitment in clinical studies using unstructured medical texts like clinical notes
  and radiology reports. The method addresses the challenge of classifying medical
  reports into disease labels by leveraging large language models (LLMs) and incorporating
  domain-specific knowledge.
---

# CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study

## Quick Facts
- arXiv ID: 2307.11346
- Source URL: https://arxiv.org/abs/2307.11346
- Reference count: 40
- Primary result: Few-shot learning with LLM and KG integration achieves F1-scores of 0.69 on IU-RR and 0.81 on MIMIC-CXR

## Executive Summary
This paper introduces CohortGPT, a framework that enhances large language models (LLMs) for classifying medical reports into disease labels for clinical study participant recruitment. The method addresses the challenge of limited labeled medical data by incorporating domain-specific knowledge graphs and a reinforcement learning-enhanced chain-of-thought sample selection strategy. Experimental results on two medical datasets demonstrate that CohortGPT outperforms traditional fine-tuning approaches in few-shot learning settings, achieving high classification accuracy with minimal labeled examples.

## Method Summary
CohortGPT leverages few-shot learning with LLMs (ChatGPT/GPT-4) for medical report classification, integrating a knowledge graph to provide hierarchical disease relationships and a dynamic chain-of-thought sample selection strategy guided by reinforcement learning. The knowledge graph is transformed into prompt-based rules that guide the LLM's reasoning, while a policy model (BioGPT-based) selects relevant CoT samples from a candidate pool to improve classification accuracy. The framework is evaluated on two datasets (IU-RR and MIMIC-CXR) using multi-label classification metrics including F1-score, precision, recall, and exact match ratio.

## Key Results
- F1-scores of 0.69 on IU-RR and 0.81 on MIMIC-CXR with only 185 labeled training samples
- Outperforms traditional fine-tuning strategies (BioBERT F1: 0.44, BioGPT F1: 0.25) in few-shot settings
- Ablation studies confirm effectiveness of knowledge graph integration and dynamic CoT sample selection
- Superior performance when labeled data is limited compared to conventional fine-tuning approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge graphs provide hierarchical disease relationships that help LLMs understand medical text classification.
- Mechanism: The KG encodes domain-specific relationships between diseases, organs, and symptoms. When embedded into prompts via rule-based transformations, these relationships guide the LLM to make more accurate classifications by constraining possible label combinations.
- Core assumption: LLMs can effectively leverage structured knowledge when explicitly provided in prompt format.
- Evidence anchors:
  - [abstract] "we propose to use a knowledge graph as auxiliary information to guide the LLMs in making predictions"
  - [section 3.1] "To embed the tree-structured knowledge into the LLMs, we propose several simple but effective prompt-based methods"
  - [corpus] Weak - corpus contains related work on KG-LLM integration but no direct evidence for this specific approach
- Break condition: If the KG contains incorrect or incomplete relationships, the LLM may learn erroneous classification rules.

### Mechanism 2
- Claim: Chain-of-thought sample selection using reinforcement learning improves LLM reasoning for medical classification.
- Mechanism: A policy model dynamically selects relevant CoT samples from a candidate pool based on the input report. The selection is optimized via policy gradient to maximize classification reward, ensuring samples provide relevant reasoning steps.
- Core assumption: The similarity between input reports and CoT samples, combined with classification performance, can be captured by a learnable policy.
- Evidence anchors:
  - [section 3.2] "we employ Chain-of-Thought (CoT) prompting to guide the model to think step by step, thereby further bridging the domain knowledge gap"
  - [section 4.3] "Experimental results show that the proposed framework could guide the LLMs to achieve satisfactory performance in a few-shot-learning setting"
  - [corpus] Weak - corpus contains related work on CoT but no direct evidence for RL-enhanced selection in medical domain
- Break condition: If the candidate pool lacks diverse or relevant samples, the policy cannot learn effective selection strategies.

### Mechanism 3
- Claim: Few-shot learning with large language models outperforms traditional fine-tuning when labeled data is limited.
- Mechanism: LLMs leverage their pre-trained language understanding to perform classification tasks with minimal labeled examples by using in-context learning through carefully constructed prompts.
- Core assumption: LLMs retain sufficient medical knowledge from pre-training to make reasonable classifications with minimal fine-tuning.
- Evidence anchors:
  - [abstract] "our few-shot learning method achieves satisfactory performance compared with fine-tuning strategies and gains superb advantages when the available data is limited"
  - [section 4.2] "when only a limited number of training samples (e.g., 185) are accessible, the F1-Score performance of the proposed method integrated with ChatGPT (0.69) or GPT-4 (0.81) could outperform the traditional fine-tuning strategy (0.44 for BioBERT and 0.25 for BioGPT)"
  - [corpus] Moderate - contains related work on few-shot learning but limited evidence for medical text classification specifically
- Break condition: When sufficient labeled data becomes available, traditional fine-tuning methods eventually outperform few-shot approaches.

## Foundational Learning

- Concept: Multi-label text classification
  - Why needed here: Medical reports can indicate multiple diseases simultaneously, requiring models to predict multiple labels per instance
  - Quick check question: How would you modify a binary classifier to handle multiple diseases in a single radiology report?

- Concept: Reinforcement learning for discrete action spaces
  - Why needed here: The CoT sample selection problem involves choosing discrete samples from a candidate pool, which is naturally formulated as a reinforcement learning problem
  - Quick check question: What is the difference between policy gradient methods and value-based methods for discrete action selection?

- Concept: Knowledge graph embedding and traversal
  - Why needed here: The hierarchical disease relationships must be converted into prompt-friendly formats while preserving semantic relationships
  - Quick check question: How would you represent a tree-structured knowledge graph as a set of human-readable rules?

## Architecture Onboarding

- Component map: Medical report → KG prompt generation → CoT selection → LLM query → classification output
- Critical path: Medical report → Knowledge Graph module → CoT Candidate Pool → Policy Model → LLM → Output classification
- Design tradeoffs:
  - KG complexity vs prompt readability
  - Candidate pool size vs selection efficiency
  - Number of CoT samples vs LLM context window limits
  - RL training time vs classification performance
- Failure signatures:
  - Low F1-score despite high training accuracy (overfitting to CoT samples)
  - Inconsistent predictions across similar reports (poor CoT selection)
  - Degradation with larger candidate pools (policy model capacity limits)
  - Poor performance on rare diseases (KG coverage gaps)
- First 3 experiments:
  1. Compare KG-as-Rule vs KG-as-Tree vs no KG baseline on IU-RR dataset with fixed CoT samples
  2. Evaluate policy model convergence with varying candidate pool sizes (10, 25, 50 samples)
  3. Test different reward function coefficient combinations (λ1, λ2) on classification stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of CohortGPT compare to other state-of-the-art few-shot learning methods specifically designed for medical text classification tasks?
- Basis in paper: [inferred] The paper mentions that CohortGPT outperforms traditional fine-tuning strategies in few-shot learning settings, but does not provide a direct comparison to other few-shot learning methods specifically designed for medical text classification.
- Why unresolved: The paper focuses on comparing CohortGPT to fine-tuning strategies and other large language models (LLMs) like Alpaca and BloomZ, but does not directly compare it to other few-shot learning methods tailored for medical text classification.
- What evidence would resolve it: Conducting experiments to compare CohortGPT's performance against other state-of-the-art few-shot learning methods specifically designed for medical text classification tasks would provide insights into its relative effectiveness.

### Open Question 2
- Question: How does the dynamic CoT sample selection strategy in CohortGPT affect the model's performance when applied to other medical NLP tasks beyond participant recruitment?
- Basis in paper: [explicit] The paper discusses the use of a dynamic CoT sample selection strategy enhanced by reinforcement learning to guide LLMs in making accurate predictions for participant recruitment.
- Why unresolved: While the paper demonstrates the effectiveness of the dynamic CoT sample selection strategy for participant recruitment, it does not explore its applicability to other medical NLP tasks.
- What evidence would resolve it: Conducting experiments to apply CohortGPT's dynamic CoT sample selection strategy to other medical NLP tasks, such as diagnosis or prognosis, and comparing its performance to baseline methods would provide insights into its generalizability.

### Open Question 3
- Question: How does the integration of a knowledge graph as auxiliary information impact the performance of CohortGPT when applied to different medical datasets or domains?
- Basis in paper: [explicit] The paper proposes using a knowledge graph as auxiliary information to guide LLMs in making predictions for participant recruitment.
- Why unresolved: The paper evaluates CohortGPT's performance on two medical datasets (IU-RR and MIMIC-CXR), but does not explore its effectiveness on different medical datasets or domains.
- What evidence would resolve it: Conducting experiments to apply CohortGPT with knowledge graph integration to different medical datasets or domains and comparing its performance to baseline methods would provide insights into its versatility and generalizability.

## Limitations

- Knowledge graph construction methodology is underspecified, making reproduction difficult
- Reinforcement learning policy lacks detailed hyperparameter documentation
- Results limited to radiology reports from specific institutions, raising generalizability concerns
- Few-shot setting (185 samples) may not reflect real-world clinical recruitment scenarios with even fewer labeled examples

## Confidence

- High confidence: Overall framework design and experimental methodology are sound
- Medium confidence: Superiority of few-shot learning over fine-tuning for limited data scenarios
- Low confidence: Specific implementation details of KG integration and RL-based CoT selection

## Next Checks

1. **Knowledge Graph Ablation**: Test the framework performance with three knowledge graph variants: (a) hierarchical tree structure, (b) flat rule-based representation, and (c) no KG baseline. This would validate whether the KG integration is providing meaningful guidance beyond the LLM's inherent capabilities.

2. **Policy Model Capacity Analysis**: Evaluate the CoT selection policy with varying candidate pool sizes (10, 25, 50 samples) and different BioGPT model sizes to determine the scalability limits and optimal configuration for medical report classification.

3. **Cross-Domain Transfer**: Apply the trained CohortGPT model to a different medical domain (e.g., pathology reports or discharge summaries) without retraining to assess the robustness and generalizability of the KG and CoT mechanisms across medical specialties.