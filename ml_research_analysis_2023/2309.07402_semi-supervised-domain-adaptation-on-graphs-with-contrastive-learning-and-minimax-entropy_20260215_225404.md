---
ver: rpa2
title: Semi-supervised Domain Adaptation on Graphs with Contrastive Learning and Minimax
  Entropy
arxiv_id: '2309.07402'
source_url: https://arxiv.org/abs/2309.07402
tags:
- node
- graph
- target
- nodes
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SemiGCL addresses the semi-supervised domain adaptation (SSDA)
  problem on graphs, where the goal is to leverage knowledge from a labeled source
  graph to aid in node classification on a target graph with limited labels. The method
  combines graph contrastive learning and minimax entropy training to generate informative
  node representations that are both transferrable and discriminative.
---

# Semi-supervised Domain Adaptation on Graphs with Contrastive Learning and Minimax Entropy

## Quick Facts
- arXiv ID: 2309.07402
- Source URL: https://arxiv.org/abs/2309.07402
- Reference count: 40
- One-line primary result: SemiGCL achieves 80.32% average accuracy on citation graphs and 92.94% on social graphs for semi-supervised domain adaptation.

## Executive Summary
SemiGCL addresses the challenge of semi-supervised domain adaptation on graphs, where knowledge from a labeled source graph is leveraged to aid node classification on a target graph with limited labels. The method combines graph contrastive learning with minimax entropy training to generate node representations that are both transferable across domains and discriminative for classification. By constructing two graph neural network encoders to extract local and global views of the graph structure, SemiGCL maximizes mutual information between these views while adversarially optimizing to reduce domain divergence.

## Method Summary
SemiGCL tackles semi-supervised domain adaptation on graphs by using two GNN encoders to extract node representations from local (original graph) and global (diffusion-augmented graph) views. Contrastive learning maximizes mutual information between these views to encode rich structural information. A cosine similarity-based classifier is trained using minimax entropy, where the classifier maximizes entropy of unlabeled target nodes to explore the target domain, while GNN encoders minimize this entropy to cluster embeddings around learned prototypes. The method shares parameters across source and target graphs using a union attribute set, enabling adaptation even with attribute distribution shifts.

## Key Results
- Achieves 80.32% average accuracy on three citation graph datasets (Cora, Citeseer, Pubmed)
- Achieves 92.94% average accuracy on three social graph datasets (Amazon, Coauthor-CS, Coauthor-Phy)
- Outperforms state-of-the-art baselines on semi-supervised domain adaptation tasks across both graph types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph contrastive learning maximizes mutual information between local and global views, leading to richer node representations.
- Mechanism: Two GNN encoders extract representations from original (local) and diffusion-augmented (global) graphs. A bilinear scoring function contrasts these representations, pulling same-node pairs closer and pushing apart corrupted pairs.
- Core assumption: Mutual information maximization between views yields more informative embeddings.
- Evidence anchors:
  - [abstract] "Contrastive learning is employed to maximize the mutual information between representations learned from the two views, encouraging the GNN encoders to encode rich local and global information of a graph."
  - [section] "Graph contrastive learning is applied to maximize the agreement between representations learned from the local and global views."
- Break condition: If diffusion augmentation doesn't add meaningful global context (e.g., small, dense graphs), contrastive learning benefit diminishes.

### Mechanism 2
- Claim: Minimax entropy training reduces domain divergence by aligning source and target embedding distributions.
- Mechanism: Classifier maximizes entropy of unlabeled target nodes to explore target domain space, while GNN encoders minimize this entropy to cluster embeddings around classifier weight vectors.
- Core assumption: Maximizing entropy forces classifier to explore target domain, minimizing entropy pulls embeddings toward prototypes.
- Evidence anchors:
  - [abstract] "SemiGCL is adversarially optimized with the entropy loss of unlabeled target nodes to reduce domain divergence."
  - [section] "The node classifier is trained to maximize the entropy of unlabeled nodes in the target graph. By doing so, the classifier's weight vectors can be moved to the target graph, alleviating the model bias towards the source graph."
- Break condition: If labeled target nodes are too few, entropy-based alignment may not capture class structure.

### Mechanism 3
- Claim: Parameter sharing between source and target graphs enables transferable representations without aligned attributes.
- Mechanism: Union attribute set allows same GNN encoders to process both graphs, enforcing consistent feature extraction across domains.
- Core assumption: Shared encoders can produce comparable embeddings despite domain shifts in node attributes.
- Evidence anchors:
  - [section] "A union attribute set is created and denoted as X = X s ∪ X t. The attribute matrices can then be expressed as X s ∈ RN s×U and X t ∈ RN t×U, where U = |X | represents the total number of attributes across both graphs. This union attribute set enables parameter sharing, allowing the same model to be applied to both graphs."
- Break condition: If attribute overlap is minimal (common attribute rate near zero), shared encoders struggle to learn meaningful mappings.

## Foundational Learning

- Concept: Graph neural networks and neighborhood aggregation
  - Why needed here: Model relies on GNNs to extract node representations from local and global structural views.
  - Quick check question: What does the neighborhood aggregation operation in Eq. 3 and Eq. 4 accomplish?

- Concept: Mutual information maximization in contrastive learning
  - Why needed here: Core mechanism for aligning local and global views.
  - Quick check question: How does the bilinear scoring function in Eq. 10 differentiate positive and negative samples?

- Concept: Entropy maximization as a domain alignment tool
  - Why needed here: Main domain adaptation technique.
  - Quick check question: What is the effect of maximizing entropy for unlabeled target nodes on the classifier's weight vectors?

## Architecture Onboarding

- Component map: Input -> Two GNN encoders (local and global views) -> Concatenated embeddings -> Cosine similarity-based classifier -> Contrastive loss + Cross-entropy loss + Entropy loss -> Backpropagation with gradient reversal
- Critical path: Input → GNN encoders → Concatenated embeddings → Classifier → Losses → Backpropagation with gradient reversal
- Design tradeoffs: Two GNN encoders increase parameter count but enable separate local/global modeling; parameter sharing reduces memory but may limit flexibility.
- Failure signatures: High contrastive loss indicates poor view alignment; high entropy loss means classifier fails to align with target domain; low cross-entropy accuracy suggests embeddings lack discriminative power.
- First 3 experiments:
  1. Train with only local view (remove global view) and measure accuracy drop.
  2. Train without contrastive loss (remove mutual information maximization) and compare embedding quality.
  3. Train without entropy loss (remove minimax adaptation) and observe domain bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SemiGCL performance scale with the number of labeled nodes per class in the target graph? Is there a point of diminishing returns?
- Basis in paper: [explicit] Section 4.4, "Effect of Target Labeled Number (RQ3)", investigates performance with n ∈ {0, 5, 10} labeled nodes per class.
- Why unresolved: Paper only tests three specific values of n without exploring the full range or identifying diminishing returns point.
- What evidence would resolve it: Additional experiments testing wider range of n values (e.g., 1, 2, 3, 4, 6, 7, 8, 9, 10, 15, 20) and analyzing performance curve.

### Open Question 2
- Question: How robust is SemiGCL to noise in node labels, especially in the target graph with limited labels?
- Basis in paper: [inferred] Paper assumes correctly labeled nodes but real-world scenarios may have label noise, particularly problematic with limited target labels.
- Why unresolved: Paper does not discuss or test impact of label noise on model performance.
- What evidence would resolve it: Experiments introducing varying levels of label noise to labeled nodes in target graph and measuring classification accuracy impact.

### Open Question 3
- Question: How does choice of graph diffusion method affect SemiGCL performance? Are there alternative methods that could be more effective?
- Basis in paper: [explicit] Section 3.3, "Node Representation Learning", describes use of graph diffusion but does not explore alternative methods.
- Why unresolved: Paper uses specific diffusion method without comparing to others or discussing performance impact.
- What evidence would resolve it: Experiments replacing current diffusion method with alternatives (e.g., heat kernel, personalized PageRank) and comparing impact on classification accuracy.

## Limitations
- Method lacks detailed hyperparameter configurations, making exact reproduction challenging.
- Empirical validation limited to three citation graphs and three social graphs, may not generalize to other graph types or domains.
- Diffusion augmentation mechanism's sensitivity to graph structure and size not thoroughly explored.

## Confidence
- **High confidence**: Mechanism of contrastive learning for mutual information maximization between local and global views.
- **Medium confidence**: Minimax entropy training's ability to reduce domain divergence, depends heavily on labeled target nodes and attribute overlap.
- **Low confidence**: Robustness to extreme domain shifts, especially when common attributes are sparse.

## Next Checks
1. **Hyperparameter Sensitivity**: Systematically vary number of GNN encoder layers, learning rate, and batch size to identify optimal configurations and assess stability.
2. **Domain Shift Robustness**: Test SemiGCL on datasets with artificially reduced common attribute rates (e.g., 0%, 25%, 50%) to evaluate performance under extreme domain divergence.
3. **Ablation on Diffusion Augmentation**: Compare performance with and without diffusion-augmented global view on graphs of varying sizes and densities to assess contribution of global context.