---
ver: rpa2
title: 'NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports'
arxiv_id: '2305.03598'
source_url: https://arxiv.org/abs/2305.03598
tags:
- task
- inference
- test
- arxiv
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NLI4CT, a novel dataset for Natural Language
  Inference (NLI) on Clinical Trial Reports (CTRs). NLI4CT contains 2,400 expert-annotated
  statements and CTRs, requiring both biomedical and numerical reasoning to determine
  entailment or contradiction.
---

# NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports

## Quick Facts
- **arXiv ID**: 2305.03598
- **Source URL**: https://arxiv.org/abs/2305.03598
- **Reference count**: 17
- **Key outcome**: Introduces NLI4CT dataset with 2,400 expert-annotated statements and CTRs requiring biomedical and numerical reasoning, achieving max F1 score of 0.627 on state-of-the-art NLI models

## Executive Summary
This paper introduces NLI4CT, a novel dataset for Natural Language Inference (NLI) on Clinical Trial Reports (CTRs) that requires both biomedical and numerical reasoning to determine entailment or contradiction. The dataset contains 2,400 expert-annotated statements paired with CTRs, divided into two tasks: predicting inference relations and retrieving supporting evidence. The authors evaluate six state-of-the-art NLI models on this challenging dataset, revealing significant limitations in handling biomedical terminology and numerical reasoning. The dataset, code, and competition leaderboard are publicly released to encourage further research in this domain.

## Method Summary
NLI4CT provides a benchmark for NLI on clinical trial reports, consisting of 2,400 expert-annotated statement-CTR pairs. The dataset is split into training (1,700), test (500), and development (200) sets. CTRs are structured into four sections: Eligibility criteria, Intervention, Results, and Adverse Events. Two tasks are defined: Task 1 predicts entailment or contradiction relations, while Task 2 retrieves supporting facts from CTRs. Six state-of-the-art NLI models (BERT-base, RoBERTa-base, GPT-2, T5-base, Megatron-lm, and BM25) are evaluated, with additional biomedical pretraining using BioBERT and BioMegatron to assess domain adaptation effects.

## Key Results
- State-of-the-art NLI models achieve maximum F1 score of 0.627 on NLI4CT, highlighting significant challenges in biomedical and numerical reasoning
- BioBERT and BioMegatron show overfitting to training data, with performance drops on test sets
- Evidence retrieval task yields lower performance than entailment prediction, suggesting models struggle to identify relevant supporting facts
- Models perform particularly poorly on numerical reasoning and biomedical terminology comprehension tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The task design forces models to engage in multi-hop biomedical and numerical reasoning rather than relying on lexical shortcuts.
- Mechanism: By requiring statements to be non-trivial and evidence to be aggregated from multiple CTR facts, the task architecture inherently demands multi-step inference chains that cannot be solved through pattern matching alone.
- Core assumption: Non-trivial statements cannot be verified by simple lexical or syntactic cues; they require genuine understanding of the CTR content and reasoning capabilities.
- Evidence anchors:
  - [abstract] "This task differentiates itself from other biomedical NLI tasks by considering the full scope of CTRs, not limited to results and interventions, instead including eligibility and Adverse events (AE) as well"
  - [section] "Non-trivial statements typically included summarisation, comparisons, negation, relations, inclusion, superlatives, aggregations, or rephrasing"
  - [corpus] Strong - The corpus explicitly contains non-trivial statements requiring multi-hop reasoning as documented in the annotation process

### Mechanism 2
- Claim: Biomedical pretraining significantly improves model performance on CTR-specific terminology and concepts.
- Mechanism: Models pre-trained on biomedical corpora (BioBERT, BioMegatron) develop better representations for domain-specific terminology, acronyms, and taxonomic relationships, enabling more accurate inference over CTRs.
- Core assumption: The distribution shift from general domain corpora to biomedical corpora can be effectively mitigated through targeted pretraining on relevant biomedical text.
- Evidence anchors:
  - [abstract] "many state-of-the-art (SOTA) NLI models fail to effectively surmount the word distribution shift from general domain corpora to biomedical corpora through transfer learning"
  - [section] "Therefore, we also include BioBERT and BioMegatron (versions of BERT-base and Megatron-lm that are pre-trained on biomedical corpora) in our baselines to assess the effects of pre-training on performance on our dataset"
  - [corpus] Moderate - While the corpus contains biomedical terminology, the pretraining effectiveness is demonstrated through experimental results rather than corpus analysis

### Mechanism 3
- Claim: The dual-task structure (entailment prediction + evidence retrieval) creates a more robust evaluation framework that exposes model limitations.
- Mechanism: By separating the label prediction task from the evidence selection task, the evaluation can identify whether models are genuinely understanding CTRs or merely exploiting spurious correlations in the label space.
- Core assumption: Evidence selection is a simpler task than entailment prediction, so poor performance on evidence selection indicates fundamental comprehension issues rather than just label prediction difficulties.
- Evidence anchors:
  - [abstract] "Secondly, to retrieve supporting facts from the CTR(s) to justify the predicted relation"
  - [section] "For Task 2 the output is a subset of the facts within the CTR premise. We can frame the evidence selection problem as a ranking problem"
  - [corpus] Strong - The corpus explicitly provides gold evidence annotations that enable this dual-task evaluation

## Foundational Learning

- Concept: Multi-hop reasoning
  - Why needed here: NLI4CT statements often require aggregating information from multiple CTR sections or combining different types of evidence to be verified
  - Quick check question: Can you trace through an example where a statement requires evidence from both eligibility criteria and intervention sections to be verified?

- Concept: Biomedical terminology and taxonomic relationships
  - Why needed here: CTRs contain domain-specific terms, acronyms, and hierarchical relationships (e.g., chemotherapy is a superset of specific drug treatments) that must be understood for accurate inference
  - Quick check question: Given a statement about "chemotherapy," can you identify which specific drug names in the CTR would serve as evidence?

- Concept: Numerical reasoning and unit conversions
  - Why needed here: Many NLI4CT instances involve comparing dosages, frequencies, or statistical measures that require mathematical operations and unit awareness
  - Quick check question: If one trial reports dosage as "1500 mg/m2" and another as "75 mg daily," what additional information would you need to compare these dosages?

## Architecture Onboarding

- Component map: CTR preprocessing (section extraction and tokenization) -> statement encoding -> evidence ranking (Task 2) -> entailment classification (Task 1)
- Critical path: For Task 1: CTR section(s) → model encoding → entailment classification. For Task 2: CTR facts → model encoding → evidence ranking
- Design tradeoffs: Using full CTR sections enables comprehensive reasoning but exceeds transformer context windows, requiring truncation or chunking strategies
- Failure signatures: Poor performance on numerical reasoning tasks, inability to handle acronyms and synonyms, overfitting to training set patterns, reliance on superficial lexical cues
- First 3 experiments:
  1. Compare baseline model performance on numerical vs non-numerical instances to quantify quantitative reasoning difficulty
  2. Evaluate evidence-only baselines to determine whether models leverage relevant evidence or rely on spurious correlations
  3. Test different truncation strategies for handling long CTR sections to optimize context completeness vs model efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do NLI models perform on NLI4CT when provided with additional biomedical context or external knowledge sources beyond the CTR premise?
- Basis in paper: [explicit] The paper mentions that future systems must overcome the challenge of acronyms and aliases to perform biomedical NLI, suggesting that additional biomedical context could improve performance.
- Why unresolved: The experiments only evaluated the performance of NLI models using the CTR premise as the sole source of information. The potential impact of incorporating external biomedical knowledge or context was not explored.
- What evidence would resolve it: Experiments comparing the performance of NLI models on NLI4CT with and without access to additional biomedical context or external knowledge sources.

### Open Question 2
- Question: Can NLI models be developed that effectively handle both biomedical terminology and numerical reasoning in clinical trial reports?
- Basis in paper: [explicit] The paper highlights the limitations of existing NLI models in handling biomedical terminology and numerical reasoning, with a maximum F1 score of 0.627 achieved on the NLI4CT dataset.
- Why unresolved: The experiments demonstrate the current challenges faced by NLI models in simultaneously addressing biomedical and numerical reasoning, but do not provide a solution to this problem.
- What evidence would resolve it: Development and evaluation of NLI models that successfully integrate biomedical knowledge and numerical reasoning capabilities, achieving high performance on the NLI4CT dataset.

### Open Question 3
- Question: What is the impact of the size of the training set on the performance of NLI models on the NLI4CT dataset?
- Basis in paper: [inferred] The paper mentions that the NLI4CT dataset contains 2,400 expert-annotated instances, which is smaller than other published biomedical NLI datasets. The authors note that the size of the training set may be problematic for large models.
- Why unresolved: The experiments did not investigate the effect of varying the size of the training set on the performance of NLI models.
- What evidence would resolve it: Experiments evaluating the performance of NLI models on NLI4CT with different sizes of training sets, to determine the minimum required size for effective learning.

## Limitations

- Significant performance gap remains between current models and ideal performance, with maximum F1 score of only 0.627
- Limited analysis of specific failure modes, particularly regarding how models handle numerical comparisons and unit conversions
- Biomedical pretraining effectiveness not fully analyzed in terms of which concepts benefit most or whether pretraining corpus adequately represents CTR language patterns

## Confidence

**High Confidence**: The dataset construction methodology and annotation process are well-documented and follow established NLI practices. The dual-task evaluation framework (entailment prediction + evidence retrieval) is clearly defined and implemented.

**Medium Confidence**: The reported performance limitations of state-of-the-art models are consistent with the expected difficulty of the task, given the complexity of biomedical terminology and numerical reasoning requirements. However, the specific contribution of each challenge factor to overall performance is not fully isolated.

**Low Confidence**: The effectiveness of biomedical pretraining on CTR-specific terminology is demonstrated through experimental results but lacks deeper analysis of which types of biomedical concepts benefit most from pretraining, or whether the pretraining corpus adequately represents CTR language patterns.

## Next Checks

1. **Controlled ablation study**: Create a subset of NLI4CT instances with varying levels of biomedical complexity (basic medical terms, domain-specific acronyms, complex taxonomic relationships) and evaluate model performance across these subsets to identify which aspects of biomedical reasoning are most challenging.

2. **Numerical reasoning isolation**: Extract and separately evaluate all instances requiring numerical comparisons, unit conversions, or statistical reasoning to quantify the exact performance penalty imposed by quantitative reasoning requirements.

3. **Evidence relevance analysis**: Conduct human evaluation of top-k evidence retrieval results to determine whether poor mAP scores reflect models selecting irrelevant evidence or simply ranking relevant evidence poorly, providing clearer insight into the nature of evidence selection failures.