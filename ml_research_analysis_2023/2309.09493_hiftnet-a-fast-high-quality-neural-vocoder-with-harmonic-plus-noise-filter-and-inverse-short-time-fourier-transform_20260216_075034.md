---
ver: rpa2
title: 'HiFTNet: A Fast High-Quality Neural Vocoder with Harmonic-plus-Noise Filter
  and Inverse Short Time Fourier Transform'
arxiv_id: '2309.09493'
source_url: https://arxiv.org/abs/2309.09493
tags:
- neural
- source
- speech
- hiftnet
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HiFTNet introduces a harmonic-plus-noise source filter combined
  with inverse short-time Fourier transform to improve neural vocoder quality and
  efficiency. It generates sinusoidal excitation from F0 estimated by a pre-trained
  network and processes it in the time-frequency domain.
---

# HiFTNet: A Fast High-Quality Neural Vocoder with Harmonic-plus-Noise Filter and Inverse Short Time Fourier Transform

## Quick Facts
- **arXiv ID**: 2309.09493
- **Source URL**: https://arxiv.org/abs/2309.09493
- **Reference count**: 0
- **Primary result**: 4× faster than BigVGAN with 1/6 the parameters while matching quality on LibriTTS

## Executive Summary
HiFTNet introduces a harmonic-plus-noise source filter combined with inverse short-time Fourier transform to improve neural vocoder quality and efficiency. It generates sinusoidal excitation from F0 estimated by a pre-trained network and processes it in the time-frequency domain. This approach outperforms iSTFTNet and HiFi-GAN on LJSpeech with a CMOS of -0.06 (p ≫ 0.05) and matches BigVGAN on LibriTTS while being 4× faster and using only 1/6 the parameters. The design enables high-fidelity, real-time speech synthesis suitable for modern text-to-speech and voice conversion applications.

## Method Summary
HiFTNet extends iSTFTNet by incorporating a harmonic-plus-noise source filter that operates in the time-frequency domain. The system uses a pre-trained neural F0 estimation network (JDC) to extract pitch information from mel-spectrograms, which is then used to generate a sinusoidal source waveform. This source is transformed via STFT, processed by neural source filter blocks, combined with Gaussian noise for unvoiced segments, and finally reconstructed using inverse STFT. The architecture employs a multi-resolution discriminator with truncated pointwise relativistic loss and replaces leaky ReLU with Snake activation functions to better capture periodic signal structure.

## Key Results
- Outperforms iSTFTNet and HiFi-GAN on LJSpeech with CMOS of -0.06 (p ≫ 0.05)
- Matches BigVGAN on LibriTTS while being 4× faster and using only 1/6 the parameters
- Achieves real-time factor of 0.05 on RTX 3090 Ti GPU
- Reduces parameters from BigVGAN's ~24M to ~4M while maintaining quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Time-frequency harmonic-plus-noise source filter with STFT pre-processing generates higher-quality phase information than direct time-domain waveform generation
- Mechanism: The sinusoidal source is first transformed into the time-frequency domain via STFT, processed by the neural source filter, then combined with Gaussian noise for unvoiced segments before iSTFT reconstruction
- Core assumption: Processing excitation in the time-frequency domain rather than time-domain preserves phase coherence better and enables more efficient modeling
- Evidence anchors:
  - [abstract]: "incorporates a harmonic-plus-noise source filter in the time-frequency domain that uses a sinusoidal source from the fundamental frequency (F0) inferred via a pre-trained F0 estimation network"
  - [section 2.1.3]: "we initially perform an STFT transformation using the same parameters... thereby converting the source waveform to the time-frequency domain"
  - [corpus]: Weak - no direct evidence in neighbors about STFT vs time-domain processing
- Break condition: If the STFT module is replaced with a learnable CNN (as in ablation), quality drops significantly (CMOS -0.358)

### Mechanism 2
- Claim: Pre-trained neural F0 estimation provides more accurate and efficient pitch information than traditional acoustic algorithms
- Mechanism: JDC network pre-trained with DIO and Harvest labels extracts F0 from mel-spectrograms, avoiding the need for time-domain waveform input
- Core assumption: Neural pitch extraction can match or exceed traditional methods while being faster and not requiring the target waveform
- Evidence anchors:
  - [abstract]: "uses a sinusoidal source from the fundamental frequency (F0) inferred via a pre-trained F0 estimation network"
  - [section 2.1.2]: "we employ a neural network for F0 estimation... pre-trains a JDC network using pitch labels extracted with DIO and Harvest"
  - [corpus]: Weak - neighbors don't discuss F0 estimation methods
- Break condition: Removing LSTM from F0 network causes significant performance degradation (CMOS -0.475), indicating global context matters

### Mechanism 3
- Claim: Snake activation function better captures periodic structure in spectrogram phase than leaky ReLU
- Mechanism: Snake function's periodic nature aligns with the periodic structure of speech signals, improving phase prediction
- Core assumption: Phase information has inherent periodicity that benefits from periodic activation functions
- Evidence anchors:
  - [abstract]: "we substitute the leaky ReLU activation function in the generator with the Snake activation function"
  - [section 2.2]: "we employ the Snake activation function aids in the model's capacity to learn the periodic structure of the speech signal"
  - [corpus]: Weak - no neighbor papers discuss Snake activation specifically
- Break condition: Switching back to leaky ReLU causes minor performance dip (CMOS -0.108), suggesting benefit is modest but real

## Foundational Learning

- Concept: Inverse Short-Time Fourier Transform (iSTFT)
  - Why needed here: HiFTNet outputs magnitude and phase spectrograms rather than waveforms, requiring iSTFT for final waveform reconstruction
  - Quick check question: What are the two outputs of HiFTNet's generator before iSTFT, and why are both needed?

- Concept: Harmonic-plus-noise modeling
  - Why needed here: Separates voiced (harmonic) and unvoiced (noise) components for more natural synthesis, addressing phase distortion issues
  - Quick check question: How does HiFTNet determine whether a frame is voiced or unvoiced, and what source is used for each?

- Concept: Adversarial training with multi-resolution discriminators
  - Why needed here: MRD discriminator with TPR loss improves synthesis quality by encouraging better local and global consistency
  - Quick check question: What is the difference between MRD and MSD discriminators, and why might MRD be preferred?

## Architecture Onboarding

- Component map: Input mel-spectrogram → F0 estimation network → Sinusoidal source generation → STFT → NSF blocks → Magnitude/Phase prediction → iSTFT → Output waveform
- Critical path: F0 estimation → Source generation → Time-frequency processing → NSF filtering → iSTFT reconstruction
- Design tradeoffs: STFT processing vs direct time-domain generation (accuracy vs complexity), Snake activation vs ReLU (quality vs speed), LSTM in F0 vs no LSTM (quality vs inference speed)
- Failure signatures: Poor F0 estimation causes harmonic misalignment; incorrect STFT parameters cause reconstruction artifacts; inadequate NSF blocks cause phase distortion
- First 3 experiments:
  1. Replace the STFT modules with learnable 1D convolutions (should cause quality drop per ablation results)
  2. Switch Snake activation back to leaky ReLU (should cause minor quality degradation)
  3. Remove LSTM from F0 estimation network (should cause significant quality degradation but improve inference speed)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the quality of HiFTNet change if the pre-trained F0 network is replaced with a learned F0 estimation module within the HiFTNet architecture?
- Basis in paper: [explicit] The paper mentions that they used a pre-trained F0 network and explores an alternative architecture without LSTM in the F0 network.
- Why unresolved: The paper does not compare the performance of a pre-trained F0 network versus a learned F0 estimation module integrated into HiFTNet.
- What evidence would resolve it: Training and evaluating HiFTNet with both a pre-trained F0 network and a learned F0 estimation module, then comparing their performance in terms of CMOS and MCD.

### Open Question 2
- Question: Would the performance of HiFTNet improve if the sinusoidal source generation method is modified to account for the discrete nature of the integration process?
- Basis in paper: [explicit] The paper discusses the difference between the continuous and discrete versions of the integration process and its impact on the sinusoidal source generation.
- Why unresolved: The paper does not explore whether adjusting the sinusoidal source generation to account for the discrete integration would lead to better performance.
- What evidence would resolve it: Implementing and evaluating HiFTNet with an adjusted sinusoidal source generation method that considers the discrete integration, then comparing the results with the current method.

### Open Question 3
- Question: How does the choice of the truncation factor τ in the TPR loss function affect the performance of HiFTNet?
- Basis in paper: [explicit] The paper mentions that they set τ to 0.04 per [24], but does not explore the impact of different τ values on performance.
- Why unresolved: The paper does not provide an ablation study on the effect of varying the truncation factor τ in the TPR loss function.
- What evidence would resolve it: Conducting an ablation study by training and evaluating HiFTNet with different τ values in the TPR loss function, then analyzing the impact on CMOS, MCD, and other performance metrics.

## Limitations

- Limited dataset scope: Results validated only on LJSpeech and LibriTTS English datasets, lacking cross-lingual generalization testing
- Small CMOS differences: Performance gains over baselines are modest (CMOS -0.06) with p-values > 0.05 indicating no statistical significance
- Evidence gaps in core mechanisms: Limited ablation studies for key architectural decisions like STFT vs time-domain processing

## Confidence

- **High confidence** (★★★★☆): Parameter efficiency claims (1/6 parameters of BigVGAN), basic architecture description, and training procedure details
- **Medium confidence** (★★★☆☆): Subjective quality comparisons (CMOS scores) and inference speed measurements
- **Low confidence** (★★☆☆☆): Claims about why specific design choices work (STFT vs time-domain, Snake activation benefits, harmonic-plus-noise advantages)

## Next Checks

**Next check 1**: Implement and evaluate a direct time-domain baseline (e.g., modified iSTFTNet without STFT/iSTFT) to directly test whether time-frequency processing provides measurable quality improvements over pure time-domain approaches.

**Next check 2**: Conduct cross-lingual evaluation by testing HiFTNet on non-English datasets (e.g., Japanese or Mandarin speech corpora) to assess generalization beyond the English-language training data.

**Next check 3**: Perform ablation studies specifically isolating the contributions of F0 estimation accuracy versus the neural source filter processing, to determine whether the quality gains primarily come from better pitch tracking or from the time-frequency domain processing itself.