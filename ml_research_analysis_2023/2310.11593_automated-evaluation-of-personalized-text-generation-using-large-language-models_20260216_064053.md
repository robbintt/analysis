---
ver: rpa2
title: Automated Evaluation of Personalized Text Generation using Large Language Models
arxiv_id: '2310.11593'
source_url: https://arxiv.org/abs/2310.11593
tags:
- text
- quality
- evaluation
- relevance
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces AuPEL, an automated evaluation framework
  for personalized text generation that uses large language models (LLMs) to assess
  three key dimensions: personalization, quality, and relevance. The method employs
  pairwise comparisons and Elo ratings to evaluate generated text without requiring
  human references or annotations.'
---

# Automated Evaluation of Personalized Text Generation using Large Language Models

## Quick Facts
- **arXiv ID**: 2310.11593
- **Source URL**: https://arxiv.org/abs/2310.11593
- **Reference count**: 37
- **Primary result**: AuPEL achieves 13-22% higher accuracy than human raters and traditional metrics in evaluating personalized text generation

## Executive Summary
This paper introduces AuPEL, an automated framework for evaluating personalized text generation that uses large language models (LLMs) to assess personalization, quality, and relevance without requiring human references. The method employs pairwise comparisons and Elo ratings to evaluate generated text, achieving over 90% consistency with fewer than 100 test cases. Experiments show AuPEL outperforms human raters and traditional reference-based metrics (BLEU, ROUGE) by 13-22% in accuracy while demonstrating high sensitivity to subtle differences in personalization. The results validate that LLM-based evaluation is more reliable and efficient for assessing personalized text generation compared to existing methods.

## Method Summary
AuPEL uses LLMs to perform pairwise comparisons between generated texts across three dimensions: quality, relevance, and personalization. For each pair, the evaluator is prompted to choose which text is better on each dimension or declare a tie. To ensure consistency and mitigate order bias, each comparison is repeated 40 times with randomized ordering. The outcomes are aggregated into win/loss/tie decisions, which are then converted into Elo ratings for each generator model. The framework uses public datasets (Amazon reviews, Reddit comments, Avocado emails) and finetunes T5 models as generators, with PaLM 2-IT models serving as evaluators. User profiles and immediate context serve as personalization references for evaluation.

## Key Results
- AuPEL achieves 13-22% higher accuracy than human raters and traditional metrics (BLEU, ROUGE) in evaluating personalized text generation
- The framework demonstrates over 90% consistency with fewer than 100 test cases
- Elo ratings derived from AuPEL effectively distinguish between T5 model sizes (Base < Large < XL < XXL) in personalization capability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLM evaluators can distinguish personalization from general text quality by reframing personalization as an author attribution task.
- **Mechanism**: The LLM is prompted to compare two generated texts and select which is more likely written by the same author who wrote the user's personal context examples. This converts a subjective personalization judgment into an objective binary classification.
- **Core assumption**: LLMs possess sufficient world knowledge and reasoning capability to identify subtle stylistic and thematic cues that signal authorship, even across different topics.
- **Evidence anchors**:
  - [abstract]: "Solving this problem requires the evaluator to consider the full spectrum of features related to personalization and discern nuanced differences across these facets, a task that LLMs are remarkably capable of even compared with humans."
  - [section]: "Indeed, when tasked to distinguish between a user-produced text and a text generated by T5 XXL for the same user, an LLM evaluator could correctly attribute the author with a 90% accuracy while trained human annotators only achieved around 70%."
  - [corpus]: Weak - the corpus neighbors discuss similar themes but do not provide direct empirical evidence for the author attribution framing.
- **Break condition**: The mechanism fails if the LLM lacks sufficient training examples of the user's writing style, or if the user's personal context is too sparse or ambiguous to serve as a reliable attribution reference.

### Mechanism 2
- **Claim**: Pairwise comparisons with repeated evaluation and aggregation yield more consistent and reliable judgments than single-score evaluations.
- **Mechanism**: Each pair of generated texts is evaluated 40 times with random ordering, and outcomes are aggregated to decide win/loss/tie. This mitigates order bias and LLM stochasticity.
- **Core assumption**: LLM responses are sufficiently stable that repeating evaluations and averaging outcomes will converge on a consistent judgment.
- **Evidence anchors**:
  - [abstract]: "AuPEL not only distinguishes and ranks models based on their personalization abilities more accurately, but also presents commendable consistency and efficiency for this task."
  - [section]: "To ensure the consistency of the judgment and mitigate potential order biases, we repeat every evaluation for an even number of times."
  - [corpus]: Missing - no corpus evidence directly supports the efficacy of this aggregation strategy.
- **Break condition**: If the LLM's stochasticity is too high relative to the signal in the texts, repeated evaluations may not converge, leading to unreliable aggregations.

### Mechanism 3
- **Claim**: Elo ratings derived from pairwise judgments provide a global, interpretable ranking of generators across multiple dimensions.
- **Mechanism**: Pairwise outcomes are converted into Elo scores for each generator model, allowing comparison beyond simple win/loss tallies and capturing relative strengths across quality, relevance, and personalization.
- **Core assumption**: The pairwise judgments are transitive and consistent enough that Elo ratings will meaningfully reflect true model capabilities.
- **Evidence anchors**:
  - [abstract]: "The Elo ratings based on AuPEL provide an objective and robust metric to benchmark the progress of personalized text generation."
  - [section]: "To address this limitation, we leverage the Elo rating system... and translate the outcomes of pairwise comparisons into Elo scores."
  - [corpus]: Weak - the corpus discusses evaluation benchmarks but does not provide empirical support for Elo ratings in this context.
- **Break condition**: If pairwise judgments are highly inconsistent or if there are cycles in preferences (A > B, B > C, but C > A), Elo ratings may not converge or may misrepresent true model capabilities.

## Foundational Learning

- **Concept**: Author attribution as a proxy for personalization evaluation
  - Why needed here: Personalization is inherently subjective and difficult to quantify; reframing it as author attribution provides an objective measure.
  - Quick check question: Why is author attribution a more reliable proxy for personalization than direct similarity metrics like BLEU or ROUGE?

- **Concept**: Pairwise comparison methodology in preference elicitation
  - Why needed here: Direct scoring of text quality is difficult for both humans and LLMs; pairwise judgments simplify decision-making and reduce bias.
  - Quick check question: How does pairwise comparison reduce cognitive load compared to assigning absolute quality scores?

- **Concept**: Elo rating system for aggregating pairwise outcomes
  - Why needed here: Pairwise comparisons alone don't provide a global ranking; Elo ratings synthesize multiple comparisons into a single interpretable metric.
  - Quick check question: What is the key advantage of Elo ratings over simple win/loss tallies when comparing multiple models?

## Architecture Onboarding

- **Component map**: Generators (T5 family models) -> Finetuning pipeline -> Test case generation -> LLM evaluator (PaLM 2-IT models) -> Pairwise comparison (40 repeats) -> Aggregation -> Elo rating computation

- **Critical path**:
  1. Sample test cases (Q, t, U_t)
  2. Generate text pairs using two generators
  3. Evaluate each pair using LLM with 40 repeats (randomized order)
  4. Aggregate results to win/loss/tie
  5. Compute Elo ratings across all test cases

- **Design tradeoffs**:
  - **Evaluation cost vs. accuracy**: More repeats per pair increases consistency but also computational cost
  - **Model size vs. capability**: Larger evaluators (PaLM 2-IT-L vs. S) show better personalization assessment but at higher inference cost
  - **Test case diversity vs. sample size**: Broader test case coverage improves generalizability but requires more samples for statistical power

- **Failure signatures**:
  - Low consistency (<90%) across repeated evaluations suggests LLM instability or ambiguous text pairs
  - Elo rating inversions (smaller model rated higher than larger) indicate evaluation artifacts or dataset-specific effects
  - High tie rates (>30%) suggest evaluators cannot distinguish between models on that dimension

- **First 3 experiments**:
  1. **Consistency validation**: Run 100 test cases with 40 repeats each, measure agreement between evaluator and assumed truth (GOLD > XXL > XL), verify >90% consistency threshold
  2. **Ablation study**: Swap user context between test cases, verify personalization evaluation drops significantly while quality/relevance remain stable
  3. **Elo ranking sanity check**: Compare Elo ratings across T5 checkpoints, verify monotonic improvement with model size and reasonable confidence intervals

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AuPEL's performance scale with the size of the LLM evaluator across different personalization tasks?
- Basis in paper: [explicit] The paper shows that PaLM 2-IT-L (larger) outperforms PaLM 2-IT-S (smaller) on personalization evaluation, with PaLM 2-IT-S showing significantly more ties.
- Why unresolved: The paper only compares two sizes of one model family. Performance may vary across different model families or tasks.
- What evidence would resolve it: Systematic evaluation of multiple LLM sizes and families across diverse personalization tasks.

### Open Question 2
- Question: What specific aspects of personalization (vocabulary, tone, structure, etc.) does AuPEL capture most effectively?
- Basis in paper: [explicit] The paper mentions that personalization encompasses vocabulary, tone, writing style, and ideology, but doesn't detail which aspects AuPEL measures best.
- Why unresolved: The paper doesn't provide granular analysis of which personalization facets AuPEL excels at versus struggles with.
- What evidence would resolve it: Detailed ablation studies isolating different personalization aspects and measuring AuPEL's accuracy on each.

### Open Question 3
- Question: How does AuPEL handle personalization in languages other than English?
- Basis in paper: [inferred] The paper only evaluates English datasets, suggesting potential language limitations.
- Why unresolved: No multilingual evaluation is presented, and language-specific personalization patterns may affect performance.
- What evidence would resolve it: Systematic evaluation of AuPEL across multiple languages with comparable datasets.

### Open Question 4
- Question: What is the impact of user context length and quality on AuPEL's personalization evaluation accuracy?
- Basis in paper: [explicit] The paper uses user context in evaluations but doesn't analyze how context variations affect results.
- Why unresolved: The ablation study only swaps user contexts but doesn't analyze the effect of context length or quality.
- What evidence would resolve it: Controlled experiments varying user context length and quality while measuring AuPEL's accuracy.

## Limitations

- **Prompt engineering sensitivity**: Framework performance depends heavily on specific prompt templates used for LLM evaluation, with no exploration of sensitivity to prompt variations.
- **Dataset and domain constraints**: Evaluation is limited to English text and Western user contexts, with unknown effectiveness for other languages, cultural contexts, or specialized domains.
- **Computational resource requirements**: The 40-repeat evaluation strategy per pair provides robustness but at significant computational cost that may become prohibitive for large-scale evaluation.

## Confidence

**High confidence**: The core mechanism of using LLM pairwise comparisons for automated evaluation is well-supported by experimental results, with demonstrated superiority over human raters and traditional metrics.

**Medium confidence**: The claimed superiority over human raters (13-22% accuracy improvement) is based on comparisons with human performance, but the exact conditions and expertise level of human raters could affect this comparison.

**Low confidence**: The framework's sensitivity to subtle personalization differences relies heavily on the author attribution framing, which is supported by single case evidence but needs broader validation across diverse personalization styles.

## Next Checks

1. **Prompt robustness test**: Systematically vary the evaluation prompts (wordings, instruction details, temperature settings) across the three dimensions and measure consistency of Elo ratings and pairwise outcomes to quantify framework sensitivity to prompt engineering.

2. **Cross-domain generalization**: Apply AuPEL to evaluate personalized text generation in non-review domains (e.g., medical advice, legal document generation) with different user profile structures and context types to assess domain transfer capability.

3. **Resource efficiency analysis**: Conduct ablation studies varying the number of repeats per pair (5, 10, 20, 40) and measure the trade-off between evaluation consistency and computational cost to identify the minimum repeats needed for acceptable performance.