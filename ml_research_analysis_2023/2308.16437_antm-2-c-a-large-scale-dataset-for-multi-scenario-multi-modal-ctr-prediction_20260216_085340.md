---
ver: rpa2
title: 'AntM$^{2}$C: A Large Scale Dataset For Multi-Scenario Multi-Modal CTR Prediction'
arxiv_id: '2308.16437'
source_url: https://arxiv.org/abs/2308.16437
tags:
- scenarios
- items
- data
- features
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AntM2C, a large-scale dataset for multi-scenario
  multi-modal CTR prediction. AntM2C contains 1 billion samples from 5 scenarios on
  the Alipay platform, including advertisements, vouchers, mini-programs, contents,
  and videos.
---

# AntM$^{2}$C: A Large Scale Dataset For Multi-Scenario Multi-Modal CTR Prediction

## Quick Facts
- arXiv ID: 2308.16437
- Source URL: https://arxiv.org/abs/2308.16437
- Reference count: 15
- Key outcome: Introduces AntM2C, a 1 billion sample dataset for multi-scenario multi-modal CTR prediction across 5 Alipay business scenarios

## Executive Summary
This paper introduces AntM2C, a large-scale dataset for multi-scenario multi-modal CTR prediction. AntM2C contains 1 billion samples from 5 scenarios on the Alipay platform, including advertisements, vouchers, mini-programs, contents, and videos. The dataset provides not only ID features but also multi-modal features such as raw text and image. Based on AntM2C, the paper constructs several typical CTR tasks, including multi-scenario modeling, cold-start modeling, and multi-modal modeling. For each task, baseline methods are evaluated, demonstrating the effectiveness of the dataset in addressing the limitations of existing CTR datasets and providing a more comprehensive and reliable evaluation for CTR models.

## Method Summary
The paper introduces AntM2C, a large-scale dataset for multi-scenario multi-modal CTR prediction. The dataset contains 1 billion samples from 5 scenarios on the Alipay platform, with 200 features including ID features and multi-modal features (text and images). The authors construct several typical CTR tasks using this dataset: multi-scenario modeling, cold-start modeling, and multi-modal modeling. They evaluate baseline methods including DNN, Shared Bottom, MMoE, and PLE for multi-scenario modeling, and meta-learning approaches (DropoutNet, MAML, MeLU, MetaEmb) for cold-start scenarios. The dataset provides a more comprehensive and reliable evaluation platform for CTR models compared to existing datasets.

## Key Results
- AntM2C contains 1 billion samples, 200 million users, and 6 million items from 5 Alipay business scenarios
- Multi-scenario modeling with MMoE achieves 0.0035-0.0057 improvement in AUC over single-scenario DNN
- Meta-learning approaches (MAML, MeLU) outperform DNN by 0.0022-0.0044 in AUC for cold-start scenarios
- Text modality integration improves CTR performance by 0.0005-0.0016 in AUC for data-sparse scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-scenario modeling captures cross-scenario user preferences and enables knowledge transfer.
- Mechanism: By jointly modeling CTR data from five distinct scenarios (advertisements, vouchers, mini-programs, contents, videos), the model learns shared user representations that capture preferences across item types, improving prediction accuracy in each individual scenario.
- Core assumption: Users exhibit consistent preference patterns across different business scenarios, and these patterns can be effectively learned from joint modeling.
- Evidence anchors:
  - [abstract]: "modeling from multiple scenarios can provide a more comprehensive understanding of users"
  - [section]: "Jointly modeling this multi-scenario CTR data can provide a more comprehensive understanding of user preferences, and the knowledge across scenarios can be shared to improve the CTR performance in each scenario."
  - [corpus]: Weak - no direct evidence in neighbors about multi-scenario knowledge transfer
- Break condition: If user behavior patterns are entirely scenario-specific with no transferable knowledge, multi-scenario modeling would provide no benefit over single-scenario models.

### Mechanism 2
- Claim: Multi-modal features (text and images) bridge the gap between items with different IDs across scenarios.
- Mechanism: Raw text features like item titles and user search queries carry semantic meaning that allows the model to recognize relationships between items with different IDs but similar content across scenarios, addressing inconsistent ID encoding.
- Core assumption: Semantic similarity in text features can effectively connect items across scenarios despite different ID encodings.
- Evidence anchors:
  - [abstract]: "multi-modal features are essential in multi-scenario prediction as they address the issue of inconsistent ID encoding between different scenarios"
  - [section]: "Multi-modal features inherently carry semantic meaning and can better compensate for the inconsistency of ID features across different domains."
  - [corpus]: Weak - no direct evidence in neighbors about multi-modal bridging across scenarios
- Break condition: If text features are too sparse, noisy, or the semantic relationships are too complex for current models to capture effectively.

### Mechanism 3
- Claim: Large scale (1 billion samples) provides more reliable evaluation and better model performance differentiation.
- Mechanism: The large sample size with 200 million users and 6 million items creates sufficient statistical power to reliably distinguish between model performances and capture long-tail user-item interactions that smaller datasets miss.
- Core assumption: Model performance differences become more apparent and statistically significant with larger datasets.
- Evidence anchors:
  - [abstract]: "a large-scale dataset can provide a more reliable evaluation of complex models, fully reflecting the performance differences between models"
  - [section]: "The scale of existing datasets is around 100 million, which is relatively small compared to the real-world industrial CTR prediction."
  - [corpus]: Weak - no direct evidence in neighbors about scale benefits for model differentiation
- Break condition: If the dataset contains significant noise or bias that scales with size, larger datasets may not provide better evaluation.

## Foundational Learning

- Concept: Multi-task learning and knowledge sharing across scenarios
  - Why needed here: The dataset requires models that can learn from multiple related CTR prediction tasks simultaneously, sharing representations while maintaining scenario-specific capabilities.
  - Quick check question: Can you explain how MMoE (Multi-gate Mixture-of-Experts) differs from simple shared-bottom architectures in handling scenario-specific knowledge?

- Concept: Meta-learning for cold-start scenarios
  - Why needed here: The dataset includes significant cold-start situations (both user cold-start and item cold-start) that require models capable of quickly adapting to new users/items with limited interaction data.
  - Quick check question: What is the key difference between MAML and MeLU approaches for cold-start recommendation, and when would each be more appropriate?

- Concept: Multi-modal feature integration
  - Why needed here: The dataset provides text features that need to be effectively combined with traditional ID-based features for improved CTR prediction.
  - Quick check question: How would you design a model architecture to incorporate both text embeddings from a language model and ID embeddings for CTR prediction?

## Architecture Onboarding

- Component map:
  Data ingestion pipeline -> Feature engineering module -> Multi-task learning framework -> Cold-start handling module -> Multi-modal integration component -> Evaluation framework

- Critical path:
  1. Data loading and preprocessing (handle 1B samples efficiently)
  2. Feature extraction and normalization
  3. Model training with appropriate multi-task architecture
  4. Cold-start scenario evaluation
  5. Multi-modal feature integration testing
  6. Comprehensive performance evaluation across all scenarios

- Design tradeoffs:
  - Memory vs. computation: Full dataset processing vs. sampling strategies
  - Model complexity vs. generalization: More complex multi-task architectures vs. simpler baselines
  - Feature richness vs. sparsity: Including all 200 features vs. feature selection

- Failure signatures:
  - Poor cross-scenario generalization indicating insufficient knowledge sharing
  - Cold-start performance degradation suggesting meta-learning implementation issues
  - Text feature underutilization indicating ineffective multi-modal integration

- First 3 experiments:
  1. Baseline single-scenario DNN vs. multi-scenario shared bottom to verify knowledge transfer benefits
  2. Cold-start evaluation on few-shot vs. zero-shot samples to establish baseline cold-start performance
  3. Multi-modal integration test: Compare MMoE with and without text features to measure text modality impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multi-modal features (text and images) contribute to improved CTR prediction performance in scenarios with data sparsity?
- Basis in paper: [explicit] The paper mentions that multi-modal features can establish connections between similar items across scenarios and provide better evaluation for multi-modal CTR models. It also states that after adding text modality, CTR performance improved in data-sparse scenarios C, D, and E.
- Why unresolved: The paper only provides a simple baseline for using text modality (Bert-base) and mentions that the improvement in performance is not significant. More powerful language models and the application of text features will be supplemented in future work.
- What evidence would resolve it: Comparative experiments using various advanced multi-modal models (e.g., transformers, contrastive learning) on the AntM2C dataset, showing significant improvements in CTR prediction performance in data-sparse scenarios.

### Open Question 2
- Question: What are the most effective strategies for addressing the cold-start problem in multi-scenario CTR prediction?
- Basis in paper: [explicit] The paper evaluates several meta-learning-based cold-start methods (DropoutNet, MAML, MeLU, MetaEmb) and shows that they outperform the DNN model without cold-start optimization. However, it mentions that the lower performance of zero-shot compared to few-shot indicates that zero-shot CTR prediction is more challenging.
- Why unresolved: The paper only evaluates a limited set of cold-start methods and does not explore other potential strategies such as transfer learning, hybrid models, or domain adaptation techniques.
- What evidence would resolve it: Extensive experiments comparing various cold-start strategies, including transfer learning, hybrid models, and domain adaptation, on the AntM2C dataset, demonstrating their effectiveness in improving CTR prediction for cold-start users and items.

### Open Question 3
- Question: How does the scale of the dataset (1 billion samples) impact the evaluation and performance of CTR models compared to smaller datasets?
- Basis in paper: [explicit] The paper states that AntM2C is currently the largest-scale CTR dataset available, providing a reliable and comprehensive evaluation for CTR models. It also mentions that large-scale datasets can reliably and comprehensively reflect the performance of CTR models, while also highlighting the differences between CTR models.
- Why unresolved: The paper does not provide a direct comparison between the performance of CTR models on AntM2C and smaller datasets, nor does it explore the impact of dataset scale on model training and evaluation.
- What evidence would resolve it: Comparative studies evaluating the performance of CTR models on AntM2C and smaller datasets, analyzing the impact of dataset scale on model training time, convergence, and generalization ability.

## Limitations
- Limited experimental validation: Only baseline methods are evaluated, with no comparison to state-of-the-art models
- Feature definition ambiguity: While 200 features are mentioned, detailed feature descriptions are not provided
- Platform-specific data: The dataset comes from a single platform (Alipay), raising generalization concerns

## Confidence
- High confidence: Dataset construction methodology and scale are well-documented
- Medium confidence: Multi-scenario modeling effectiveness supported by theoretical arguments
- Low confidence: Cold-start and multi-modal performance claims based on baseline results only

## Next Checks
1. Implement specialized cold-start methods (meta-learning approaches) and compare their performance against baseline methods on the cold-start subset
2. Conduct ablation studies removing text features to quantify their exact contribution to CTR prediction performance
3. Test cross-platform generalization by evaluating models trained on AntM2C on CTR data from other e-commerce platforms