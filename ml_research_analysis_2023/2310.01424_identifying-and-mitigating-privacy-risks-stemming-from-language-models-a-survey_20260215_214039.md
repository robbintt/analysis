---
ver: rpa2
title: 'Identifying and Mitigating Privacy Risks Stemming from Language Models: A
  Survey'
arxiv_id: '2310.01424'
source_url: https://arxiv.org/abs/2310.01424
tags:
- data
- training
- privacy
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic literature review of privacy
  attacks and mitigations for language models. It introduces a taxonomy of attacks
  based on attacker goals (membership inference, model inversion, data extraction,
  model extraction), knowledge (black-box vs white-box), training phase (pre-trained,
  fine-tuned, compressed), and model type.
---

# Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey

## Quick Facts
- arXiv ID: 2310.01424
- Source URL: https://arxiv.org/abs/2310.01424
- Reference count: 40
- Key outcome: First systematic literature review of privacy attacks and mitigations for language models, introducing a comprehensive taxonomy based on attacker goals, knowledge levels, training phases, and model types

## Executive Summary
This survey provides the first systematic literature review of privacy attacks and mitigations for language models. The authors develop a comprehensive taxonomy that categorizes privacy threats based on attacker goals (membership inference, model inversion, data extraction, model extraction), knowledge levels (black-box vs white-box), training phases (pre-trained, fine-tuned, compressed), and model types. The survey identifies key trends including the vulnerability of fine-tuned models, the effectiveness of data deduplication in reducing memorization, and the challenges of applying differential privacy to large models. The authors also highlight open problems such as the need for systematic comparisons of privacy-preserving methods and broader privacy issues beyond data and model leakage.

## Method Summary
The paper conducts a systematic literature review by surveying existing research on privacy attacks and mitigations for language models. The methodology involves developing a taxonomy of attacks based on attacker goals, knowledge levels, training phases, and model types. The authors review relevant literature on membership inference, model inversion, data extraction, and model extraction attacks, along with mitigation strategies including data sanitization, deduplication, differential privacy, and knowledge unlearning. The survey synthesizes findings across different model architectures (static word embeddings, supervised neural networks, large language models) and training phases to provide a comprehensive overview of the privacy landscape.

## Key Results
- Fine-tuned language models show high vulnerability to membership inference and model inversion attacks, particularly when only the final layer is tuned
- Data deduplication can significantly reduce training data memorization and extraction attack success rates
- Differential privacy provides provable privacy guarantees but faces significant trade-offs in accuracy and computational efficiency for large language models
- Knowledge unlearning shows promise as a post-training mitigation strategy that can remove specific sensitive data with minimal performance degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models memorize training data, and the amount of memorization scales with model size and duplicated sequences
- Mechanism: As language models increase in parameter count and are trained on larger datasets with more repeated sequences, they have greater capacity to store verbatim training examples in their weights, making them more vulnerable to privacy attacks
- Core assumption: Memorization is primarily driven by model capacity and data repetition, not necessarily by overfitting
- Evidence anchors:
  - [abstract] "training data memorization in Machine Learning models scales with model size, particularly concerning for LLMs"
  - [section] "Memorization has been shown to scale with (1) model size (number of parameters) [12] and (2) duplicated sequences in the training data [48, 49, 12]"
  - [corpus] Found related work on privacy in fine-tuning LLMs, showing this is a recognized area of study
- Break Condition: If models are trained on deduplicated data or if specific unlearning techniques are applied to remove memorized content

### Mechanism 2
- Claim: Different phases of language model development (pre-training, fine-tuning, compression) have varying levels of vulnerability to privacy attacks
- Mechanism: Pre-trained models are vulnerable to data extraction and model extraction attacks, fine-tuned models are highly susceptible to membership inference and model inversion attacks, while compressed models may have reduced vulnerability but are still at risk
- Core assumption: The training phase determines the type and extent of private data exposure
- Evidence anchors:
  - [section] "Pre-training. Transformers are generally pre-trained using a general language modeling task... Fine-Tuning. Usually, the pre-trained model is adapted to specific downstream tasks by fine-tuning the model parameters on smaller task-specific datasets. Compression. Increasingly commonly, after pre-training and fine-tuning, LLMs are compressed..."
  - [section] "Supervised LMs, Static word-embedding models, and pre-trained, fine-tuned and compressed LLMs are vulnerable to membership inference attacks... fine-tuned LMs appear very vulnerable to both MIA and model inversion attacks, especially where only the final layer of the model has been tuned."
  - [corpus] Found surveys on security and privacy challenges of LLMs, indicating this is a recognized research area
- Break Condition: If privacy-preserving techniques are applied during each phase or if models are not fine-tuned on sensitive data

### Mechanism 3
- Claim: Differential privacy provides provable privacy guarantees but comes with trade-offs in accuracy and computational efficiency for large language models
- Mechanism: Training language models with differential privacy involves adding noise to gradients and clipping their norms, which provides formal privacy guarantees but can reduce model accuracy and increase computational overhead
- Core assumption: The privacy-utility trade-off can be managed through careful hyperparameter selection and model-specific adaptations
- Evidence anchors:
  - [section] "DP stochastic gradient descent (DP-SGD) is the canonical approach to training ML models with DP... DP-SGD can cause a significant reduction in accuracy due to applying gradient clipping and noising at the granularity of individual training examples."
  - [section] "Training LMs under the framework of DP provides a provable guarantee of privacy protection of a user's data. LMs need to be tailored to the specifics of DP training to achieve comparable results to non-private LM baselines..."
  - [corpus] Found work on large-scale differentially private BERT, showing practical applications of DP in LLMs
- Break Condition: If the privacy budget (ε) is set too low, resulting in unacceptable accuracy degradation, or if computational resources are insufficient for DP training

## Foundational Learning

- Concept: Language model architectures (encoder-only, decoder-only, encoder-decoder)
  - Why needed here: Understanding these architectures is crucial for identifying which types of models are vulnerable to which privacy attacks
  - Quick check question: Which architecture would be most suitable for a text summarization task and why?

- Concept: Differential privacy and its application to machine learning
  - Why needed here: DP is presented as a key mitigation strategy, so understanding its mechanics and guarantees is essential
  - Quick check question: What is the difference between (ε, δ)-differential privacy and pure ε-differential privacy?

- Concept: Membership inference attacks and their relationship to model memorization
  - Why needed here: MIAs are one of the primary privacy threats discussed, and understanding their mechanism helps in assessing model vulnerability
  - Quick check question: Why are out-of-distribution data points more vulnerable to membership inference attacks?

## Architecture Onboarding

- Component map: Privacy attack taxonomy (goals: membership inference, model inversion, data extraction, model extraction; knowledge: black-box vs white-box; phases: pre-trained, fine-tuned, compressed; models: supervised NN, word embedding, LLMs) -> Mitigation strategies (pre-processing: sanitization, deduplication; training: differential privacy; post-training: prompt restriction, knowledge unlearning) -> Evaluation metrics (AUC for attacks, accuracy for defenses, privacy budget ε)

- Critical path: Identify privacy risks → Select appropriate attacks based on model phase and access level → Apply relevant defenses → Evaluate privacy-utility trade-off

- Design tradeoffs: Strong privacy guarantees (low ε in DP) vs. model utility, comprehensive data sanitization vs. computational cost, restrictive prompt lengths vs. user experience

- Failure signatures: High success rates of membership inference attacks indicate memorization, significant accuracy drop with DP training suggests overly strict privacy parameters, model extraction success indicates API vulnerabilities

- First 3 experiments:
  1. Reproduce membership inference attack on a fine-tuned BERT model to verify vulnerability claims
  2. Apply data deduplication to a GPT-2 model and measure reduction in training data leakage
  3. Implement DP-SGD on a smaller language model to understand the privacy-utility trade-off before scaling to larger models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is differential privacy training for fine-tuned language models in real-world settings, and what are the practical trade-offs in terms of accuracy and privacy?
- Basis in paper: [explicit] The paper discusses the challenges of applying differential privacy to transformer-based language models and highlights the need for further research in this area, particularly for fine-tuned models
- Why unresolved: The paper notes that while differential privacy can provide strong privacy guarantees, achieving comparable accuracy to non-private models remains challenging. The effectiveness of differential privacy for fine-tuned models in real-world scenarios is not fully explored
- What evidence would resolve it: Empirical studies comparing the performance of differentially private fine-tuned language models against their non-private counterparts in various real-world applications, with a focus on accuracy, privacy, and computational efficiency

### Open Question 2
- Question: What are the most effective strategies for combining data deduplication and differential privacy to mitigate privacy risks in large language models?
- Basis in paper: [explicit] The paper mentions that data deduplication can reduce memorization and protect against extraction attacks, but it does not guarantee preventing memorization of specific examples. It also highlights the potential of combining data deduplication with other privacy methods
- Why unresolved: While data deduplication and differential privacy are both effective individually, their combined effectiveness in mitigating privacy risks in large language models is not well-studied. The optimal balance between these methods and their impact on model performance is unclear
- What evidence would resolve it: Comparative studies evaluating the effectiveness of combined data deduplication and differential privacy approaches in reducing privacy risks, with a focus on model accuracy, privacy guarantees, and computational overhead

### Open Question 3
- Question: How can knowledge unlearning be effectively implemented to remove specific sensitive data from large language models without significant degradation in model performance?
- Basis in paper: [explicit] The paper introduces knowledge unlearning as a post-training approach to remove specific sensitive data from models. It mentions that knowledge unlearning can be effective in protecting target sequences from extraction attacks with minimal performance degradation
- Why unresolved: While knowledge unlearning shows promise, its practical implementation and effectiveness in large language models are not fully explored. The impact of knowledge unlearning on model performance and the best practices for its implementation are not well-established
- What evidence would resolve it: Empirical studies demonstrating the effectiveness of knowledge unlearning in removing specific sensitive data from large language models, with a focus on the trade-offs between privacy and model performance, and the practical challenges of implementation

## Limitations
- Lack of systematic benchmarking across different privacy-preserving methods - direct comparative studies showing which approaches work best under different conditions are largely absent
- Limited empirical validation of differential privacy effectiveness specifically for large language models, with most studies focusing on smaller models
- Incomplete exploration of practical implementation challenges for knowledge unlearning in real-world settings

## Confidence

- Privacy attack taxonomy structure: High
- Memorization scaling with model size: Medium-High
- Differential privacy effectiveness: Medium
- Fine-tuning vulnerability claims: Medium-High

## Next Checks

1. Reproduce membership inference attack success rates on fine-tuned models using publicly available checkpoints to verify vulnerability claims
2. Implement data deduplication on a pre-trained GPT-2 model and measure reduction in training data extraction success rates
3. Conduct controlled experiments comparing DP-SGD performance across different LLM architectures to quantify the privacy-utility trade-off in practice