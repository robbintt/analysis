---
ver: rpa2
title: Predicting Outcomes in Long COVID Patients with Spatiotemporal Attention
arxiv_id: '2307.04770'
source_url: https://arxiv.org/abs/2307.04770
tags:
- attention
- data
- patients
- spatiotemporal
- lstm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a joint spatiotemporal attention mechanism to
  weigh feature importance jointly from the temporal dimension and feature space for
  predicting outcomes in long COVID patients. The proposed method integrates the joint
  spatiotemporal attention into a Local-LSTM model to enable separate learning of
  short-term and long-term dependencies from longitudinal medical data.
---

# Predicting Outcomes in Long COVID Patients with Spatiotemporal Attention

## Quick Facts
- arXiv ID: 2307.04770
- Source URL: https://arxiv.org/abs/2307.04770
- Reference count: 32
- Primary result: Joint spatiotemporal attention with Local-LSTM achieves 0.87 AUC for long COVID mortality prediction

## Executive Summary
This paper introduces a joint spatiotemporal attention mechanism integrated with a Local-LSTM architecture to predict outcomes in long COVID patients. The method addresses the challenge of learning both short-term and long-term dependencies from longitudinal medical data while weighing feature importance across both temporal and feature dimensions. Evaluated on a clinical dataset of 365 hospitalized COVID-19 patients, the approach achieves superior performance compared to state-of-the-art methods and clinical baselines, providing a tool for severity assessment in long COVID patients.

## Method Summary
The method combines Local-LSTM for short-term dependency learning within a sliding window (size 6) with joint spatiotemporal attention for long-term dependency capture. The attention mechanism uses 1x1 convolutional layers to compute keys, queries, and values, enabling joint weighing of feature importance across both time and feature space. The architecture processes multimodal longitudinal data including lab tests, vital signs, demographics, medical history, and chest X-ray RALE scores, with forward-filling imputation and min-max normalization. Training uses stochastic gradient descent with learning rate scheduling (1e-3 to 1e-5) over 50 epochs with five-fold cross-validation.

## Key Results
- Joint spatiotemporal attention with Local-LSTM achieves 0.87 AUC on test set for 60-day mortality prediction
- Outperforms LSTM with temporal attention and clinical Apache II scoring system
- Demonstrates superior handling of both short-term randomness and long-term patterns in longitudinal medical data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint spatiotemporal attention enables dynamic feature importance by considering both temporal evolution and inter-feature relationships simultaneously.
- Mechanism: Uses 2D convolutions (1x1 filters) to compute keys, queries, and values, allowing attention to weigh features jointly across time axis and feature space.
- Core assumption: Feature importance varies both over time and across different features, requiring joint modeling rather than separate attention mechanisms.
- Evidence anchors: Abstract states attention "jointly considers the time and features from a global perspective"; section notes previous works either ignore feature diversity or provide static spatial importance.
- Break condition: If 2D convolutional attention fails to capture meaningful feature interactions or computational cost outweighs performance gains.

### Mechanism 2
- Claim: Separating short-term and long-term dependency learning improves model performance for longitudinal medical data.
- Mechanism: Local-LSTM captures short-term/ordering dependencies within sliding window while joint spatiotemporal attention captures long-term dependencies across entire sequence.
- Core assumption: Medical data exhibits local randomness in short-term sequences while maintaining meaningful long-term patterns.
- Evidence anchors: Abstract mentions restricting short-term dependency learning with Local-LSTM; section explains electronic health records don't follow strict short-term orders.
- Break condition: If window size is poorly chosen or short-term randomness assumption doesn't hold for certain medical data types.

### Mechanism 3
- Claim: LSTM with attention mechanisms outperforms both standalone LSTM and clinical scoring systems for long COVID mortality prediction.
- Mechanism: LSTM extracts temporal patterns while attention mechanisms highlight important time points and feature combinations.
- Core assumption: Long COVID progression contains both sequential patterns (LSTM) and non-sequential important feature combinations (attention).
- Evidence anchors: Section explains RNN learns short-term dependencies while attention enables long-term dependency learning; experimental results show 0.87 AUC outperforming related methods.
- Break condition: If dataset is too small or temporal patterns too weak, complexity may not provide significant advantages over simpler models.

## Foundational Learning

- Concept: Longitudinal data modeling
  - Why needed here: Study deals with patient data collected over multiple time points, requiring models that capture temporal dependencies and patterns across visits.
  - Quick check question: What's the difference between modeling a single time point vs. modeling a sequence of time points in medical data?

- Concept: Attention mechanisms in deep learning
  - Why needed here: Standard RNNs struggle with long-term dependencies; attention mechanisms allow model to focus on most relevant features and time points when making predictions.
  - Quick check question: How does self-attention differ from temporal attention in context of sequential data?

- Concept: Trade-offs between model complexity and interpretability
  - Why needed here: Deep learning models may sacrifice interpretability compared to clinical scoring systems like Apache II, important for clinical adoption.
  - Quick check question: What are key considerations when choosing between highly accurate but complex model versus simpler but more interpretable model for clinical use?

## Architecture Onboarding

- Component map: Input → Local-LSTM (short-term) → Joint Spatiotemporal Attention (long-term) → MLP (prediction)
- Critical path: Local-LSTM extracts short-term patterns → joint spatiotemporal attention refines these features for long-term dependencies → MLP makes final prediction
- Design tradeoffs: Local-LSTM vs. standard LSTM trades some long-term pattern capture for better short-term randomness handling; joint spatiotemporal attention vs. separate attention mechanisms trades computational efficiency for more nuanced feature importance modeling
- Failure signatures: Poor performance could indicate incorrect window size, inadequate attention mechanism design, or insufficient data diversity; overfitting is also a risk with complex models on small clinical datasets
- First 3 experiments:
  1. Baseline: Train LSTM with all data modalities to establish performance benchmark (should achieve ~0.76 AUC based on paper results)
  2. Attention comparison: Compare LSTM with temporal attention vs. LSTM with joint spatiotemporal attention to validate proposed attention mechanism benefit
  3. Architecture ablation: Compare Local-LSTM with joint spatiotemporal attention against standard LSTM with same attention to demonstrate benefit of separating short-term and long-term dependency learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the joint spatiotemporal attention mechanism perform compared to other attention mechanisms (such as self-attention) in different medical prediction tasks beyond long COVID?
- Basis in paper: [explicit] Paper proposes joint spatiotemporal attention mechanism and compares it with LSTM with temporal attention, but does not explore other attention mechanisms
- Why unresolved: Paper focuses on specific task of long COVID mortality prediction without exploring generalizability to other tasks or attention mechanisms
- What evidence would resolve it: Experiments comparing joint spatiotemporal attention with other mechanisms (such as self-attention) on various medical prediction tasks would provide insights into generalizability and effectiveness

### Open Question 2
- Question: What is the optimal window size for the Local-LSTM model in different medical prediction tasks?
- Basis in paper: [explicit] Paper sets window size to six but does not explore impact of different window sizes on model's performance
- Why unresolved: Paper does not provide systematic analysis of different window sizes' impact, and optimal size may vary depending on specific task and data characteristics
- What evidence would resolve it: Experiments with different window sizes for Local-LSTM on various medical prediction tasks would help determine optimal window size for different scenarios

### Open Question 3
- Question: How does the joint spatiotemporal attention mechanism handle missing data in longitudinal medical records?
- Basis in paper: [inferred] Paper uses forward-filling to impute missing values but does not explore how joint spatiotemporal attention handles missing data
- Why unresolved: Paper does not provide detailed analysis of how joint spatiotemporal attention handles missing data, which is common issue in longitudinal medical records
- What evidence would resolve it: Experiments with different imputation methods and missing data patterns would help understand how joint spatiotemporal attention handles missing data and its impact on performance

## Limitations

- Performance gains over clinical baselines are demonstrated but exact contribution of joint spatiotemporal attention versus Local-LSTM architecture remains unclear due to lack of detailed ablation studies
- Dataset size (365 patients) raises concerns about potential overfitting and generalizability for complex model with attention mechanisms
- Paper does not specify hyperparameter values beyond basic architecture choices, making exact reproduction challenging

## Confidence

- High confidence: Core mechanism of using 2D convolutional attention for joint spatiotemporal feature weighting is technically sound and well-justified by need to capture both temporal and feature space relationships simultaneously
- Medium confidence: Separation of short-term and long-term dependencies through Local-LSTM provides meaningful improvements, though relies on assumptions about medical data structure that may not generalize to all clinical contexts
- Medium confidence: Claim that LSTM+attention outperforms clinical scoring systems is supported by experimental results, but clinical relevance and practical utility for different long COVID phenotypes requires further validation

## Next Checks

1. **Ablation study validation**: Compare Local-LSTM with joint spatiotemporal attention against (a) standard LSTM with same attention mechanism and (b) LSTM with separate temporal and spatial attention mechanisms to isolate specific benefits of each architectural choice

2. **Generalizability testing**: Evaluate model on separate, larger dataset of long COVID patients from different healthcare systems to assess whether performance gains hold across diverse populations and clinical settings

3. **Interpretability analysis**: Conduct feature importance analysis to verify attention mechanism correctly identifies clinically meaningful patterns (e.g., RALE scores and inflammatory markers) and assess whether these align with known long COVID pathophysiology