---
ver: rpa2
title: Temporal Graph Benchmark for Machine Learning on Temporal Graphs
arxiv_id: '2307.01026'
source_url: https://arxiv.org/abs/2307.01026
tags:
- datasets
- dataset
- temporal
- node
- dynamic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Temporal Graph Benchmark (TGB), a collection
  of large-scale, diverse benchmark datasets for evaluating machine learning models
  on temporal graphs. The authors propose improved evaluation protocols for both dynamic
  link property prediction and dynamic node property prediction tasks.
---

# Temporal Graph Benchmark for Machine Learning on Temporal Graphs

## Quick Facts
- arXiv ID: 2307.01026
- Source URL: https://arxiv.org/abs/2307.01026
- Reference count: 40
- Key outcome: Introduces TGB, a collection of large-scale, diverse benchmark datasets for ML on temporal graphs with improved evaluation protocols using MRR and NDCG metrics

## Executive Summary
This paper introduces the Temporal Graph Benchmark (TGB), a collection of large-scale, diverse benchmark datasets for evaluating machine learning models on temporal graphs. The authors propose improved evaluation protocols for both dynamic link property prediction and dynamic node property prediction tasks, addressing limitations of existing benchmarks. For link prediction, they sample multiple negative edges per positive edge and use Mean Reciprocal Rank (MRR) as the metric, sampling both historical and random negatives. For node property prediction, they use Normalized Discounted Cumulative Gain (NDCG) to evaluate the relative ordering of predicted labels. The TGB datasets span various domains and are orders of magnitude larger than existing benchmarks, with both node and edge-level tasks.

## Method Summary
The paper introduces the Temporal Graph Benchmark (TGB) with improved evaluation protocols for temporal graph learning. For dynamic link property prediction, they treat the task as a ranking problem by contrasting each positive sample against 20 negative samples (10 historical and 10 random) and using Mean Reciprocal Rank (MRR) as the metric. For dynamic node property prediction, they use NDCG@10 to evaluate the relative ordering of predicted labels. The benchmark includes seven datasets spanning various domains (Wikipedia, Amazon, Bitcoin forums, cryptocurrency, flights, trade, music genres, Reddit) that are orders of magnitude larger than existing benchmarks. The authors provide an automated machine learning pipeline for reproducible temporal graph research, including data loading, experiment setup, and performance evaluation.

## Key Results
- Simple heuristics like EdgeBank often outperform state-of-the-art temporal graph models on node property prediction tasks
- Model performance varies significantly across datasets, indicating the need for diverse benchmarks
- The surprise index (ratio of test edges unseen during training) inversely correlates with the performance of memorization-based methods
- The TGB datasets are orders of magnitude larger than existing benchmarks, spanning years in duration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using multiple negative samples per positive edge in dynamic link prediction improves evaluation realism by better simulating the candidate ranking problem faced in real-world applications.
- Mechanism: By sampling 20 negative edges per positive edge (10 historical and 10 random), the evaluation protocol shifts from binary classification to ranking, where models must distinguish the true edge from many plausible alternatives. This increases difficulty and makes evaluation more aligned with practical use cases where top-K predictions matter.
- Core assumption: Historical negatives are more challenging to predict than random negatives because they represent edges that existed in the past but are absent at the current timestamp, making them more similar to the positive edge.
- Evidence anchors: [abstract] "we propose to treat the task as a ranking problem, contrasting each positive sample against multiple negatives and using Mean Reciprocal Rank (MRR) as the metric"; [section] "historical negatives – past edges absent in the current step – are more difficult to predict correctly than randomly sampled negatives"

### Mechanism 2
- Claim: Using NDCG@10 as the metric for dynamic node property prediction evaluates the relative ordering of predicted labels, which is more suitable than exact match metrics for recommendation-like tasks.
- Mechanism: NDCG discounts the gain of items based on their position in the ranked list, so it rewards models that put the most relevant labels at the top, even if they don't get the exact top-10 list correct. This aligns with user-facing applications where top recommendations matter more than exhaustive lists.
- Core assumption: In recommendation and ranking tasks, the relative order of the top items is more important than the exact set of items, especially when the label space is large.
- Evidence anchors: [abstract] "we use the NDCG metric to evaluate the relative ordering of node labels within the top ranked classes"; [section] "NDCG is commonly used in information retrieval and recommendation systems as a measure of ranking quality"

### Mechanism 3
- Claim: Simple heuristics like EdgeBank can outperform complex temporal graph models on certain datasets because they exploit memorization of frequent or recent edges.
- Mechanism: EdgeBank stores observed edges in a memory (hash table) and predicts based on whether an edge was seen before. On datasets with low surprise index (many test edges were seen during training), this memorization is highly effective, outperforming models that try to learn temporal patterns.
- Core assumption: The surprise index (ratio of test edges unseen during training) is inversely correlated with the performance of memorization-based methods like EdgeBank.
- Evidence anchors: [abstract] "simple methods often achieve superior performance compared to existing temporal graph models"; [section] "As a heuristic that memorizes past edges, EdgeBank performance is inversely correlated with the surprise index"

## Foundational Learning

- Concept: Temporal graphs and their distinction from static graphs
  - Why needed here: The entire paper is about learning on temporal graphs, so understanding the basic structure (nodes, edges, timestamps) and how they differ from static graphs is essential.
  - Quick check question: What additional information does a temporal graph have compared to a static graph, and how is it typically represented?

- Concept: Evaluation metrics for ranking tasks (MRR, NDCG)
  - Why needed here: The paper introduces new evaluation protocols using MRR for link prediction and NDCG for node prediction, replacing traditional binary classification metrics. Understanding these metrics is crucial for interpreting results.
  - Quick check question: How does Mean Reciprocal Rank differ from Average Precision in evaluating ranked lists, and why might MRR be more appropriate for link prediction?

- Concept: Negative sampling in link prediction
  - Why needed here: The paper's improved evaluation relies on sampling multiple negative edges per positive edge. Understanding why this is necessary and how it differs from standard single-negative sampling is key.
  - Quick check question: Why does sampling only one negative edge per positive edge lead to inflated performance estimates in link prediction?

## Architecture Onboarding

- Component map: Dataset download and processing -> Experiment setup with configurable parameters -> Training on training split -> Evaluation on validation split -> Final evaluation on test split -> Leaderboard integration
- Critical path: For a new model evaluation, the critical path is: load dataset → configure model → run training on training split → evaluate on validation split (for hyperparameter tuning) → final evaluation on test split → submit results to leaderboard. The evaluation phase is most critical because it uses the improved protocols.
- Design tradeoffs: The choice of MRR over AP balances completeness (more negatives = better ranking signal) against computational cost (20x more negatives = longer inference). The fixed split (70/15/15) ensures consistency but may not reflect all real-world streaming scenarios. Including simple baselines like EdgeBank sets a strong floor but may discourage exploration of complex models on low-surprise datasets.
- Failure signatures: If a model performs well on validation but poorly on test, it likely overfits to the validation distribution or fails to handle temporal distribution shifts. If all models perform similarly, the dataset may have low complexity or the evaluation protocol may not be challenging enough. If a model runs out of memory on medium/large datasets, it may not scale well.
- First 3 experiments:
  1. Run EdgeBank on tgbl-wiki and tgbl-review to establish baseline MRR scores and observe the impact of surprise index on memorization-based methods.
  2. Run TGN on the same datasets to compare complex model performance against the heuristic and identify if temporal patterns help.
  3. Run DyRep on tgbl-review to test if interaction-based models can handle the high surprise index better than memory-based ones.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different negative sampling strategies (historical vs. random) affect model performance across various temporal graph domains?
- Basis in paper: [explicit] The paper discusses sampling both historical and random negatives for link prediction, noting that historical negatives are more difficult to predict correctly than randomly sampled negatives.
- Why unresolved: The paper mentions using a mix of both historical and random negatives but doesn't provide detailed ablation studies on their individual effects across different datasets.
- What evidence would resolve it: Detailed performance comparisons of models trained with only historical negatives, only random negatives, and various mixtures across multiple TGB datasets.

### Open Question 2
- Question: What is the impact of temporal graph size and duration on model generalization capabilities?
- Basis in paper: [explicit] The paper notes that TGB datasets span years in duration and are orders of magnitude larger than existing benchmarks, but finds that model performance can vary drastically across datasets.
- Why unresolved: While the paper shows performance variation across datasets, it doesn't systematically analyze how dataset size, duration, or temporal granularity specifically affect model generalization.
- What evidence would resolve it: Controlled experiments varying temporal granularity and dataset duration while keeping other factors constant, measuring performance trends.

### Open Question 3
- Question: Why do simple heuristics like EdgeBank and persistent forecast often outperform complex temporal graph models on node property prediction tasks?
- Basis in paper: [explicit] The paper demonstrates that simple heuristics can achieve superior performance compared to existing temporal graph models on dynamic node property prediction tasks.
- Why unresolved: The paper shows this empirical finding but doesn't provide theoretical insights into why memorization-based approaches might be more effective for node-centric tasks than sophisticated GNN-based methods.
- What evidence would resolve it: Theoretical analysis of the characteristics of node property prediction tasks that make them amenable to heuristic approaches, and identification of when complex models fail to outperform simple baselines.

## Limitations
- The optimal number of negative samples (20) was not systematically studied and may vary by dataset characteristics
- The TGB datasets, while larger than previous benchmarks, may still not capture all real-world temporal graph scenarios
- The fixed 70/15/15 data split may not reflect all real-world streaming scenarios

## Confidence
- **High confidence**: The core contribution of creating large-scale, diverse temporal graph datasets is well-supported by the provided statistics and domain examples.
- **Medium confidence**: The claim that simple heuristics outperform complex models on certain datasets is supported by empirical results, but generalizability remains to be seen.
- **Medium confidence**: The proposed evaluation metrics (MRR and NDCG) are appropriate for ranking tasks, but their superiority over traditional metrics has not been rigorously validated across different application domains.

## Next Checks
1. **Negative sampling sensitivity**: Systematically vary the number of negative samples (5, 10, 20, 50) and analyze the impact on MRR scores across different datasets to determine if 20 is optimal or if it varies by dataset characteristics.

2. **Cross-domain generalization**: Apply the TGB evaluation protocols to a temporal graph dataset from a completely different domain (e.g., biological interaction networks) to test whether the findings about model performance and heuristic effectiveness generalize beyond the current domains.

3. **Real-world deployment testing**: Deploy top-performing models from TGB on a live temporal graph system (e.g., recommendation engine or fraud detection) to validate whether benchmark performance correlates with real-world effectiveness, particularly for models that perform differently on high vs. low surprise index datasets.