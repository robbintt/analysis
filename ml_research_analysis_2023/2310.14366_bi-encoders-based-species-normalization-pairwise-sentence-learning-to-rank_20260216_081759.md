---
ver: rpa2
title: Bi-Encoders based Species Normalization -- Pairwise Sentence Learning to Rank
arxiv_id: '2310.14366'
source_url: https://arxiv.org/abs/2310.14366
tags:
- entity
- normalization
- species
- entities
- named
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of linking biomedical named entities
  to unique database identifiers, focusing on species normalization to NCBI taxonomy.
  The proposed method treats this as a pair-wise learning to rank problem, using BM25
  for candidate generation and BERT for re-ranking.
---

# Bi-Encoders based Species Normalization -- Pairwise Sentence Learning to Rank

## Quick Facts
- arXiv ID: 2310.14366
- Source URL: https://arxiv.org/abs/2310.14366
- Reference count: 34
- Primary result: BERT-based re-ranking improves species normalization accuracy to 88.56% (LINNAEUS) and 86.86% (S800)

## Executive Summary
This paper addresses biomedical species normalization by treating it as a pairwise learning-to-rank problem. The method uses BM25 for candidate generation and BERT for re-ranking to link species mentions to NCBI taxonomy identifiers. By framing the task as pairwise ranking rather than classification, the approach eliminates the need for manual feature engineering while achieving state-of-the-art accuracy on LINNAEUS and S800 corpora.

## Method Summary
The proposed method generates candidate NCBI taxonomy concepts using BM25 information retrieval, then re-ranks them using BERT fine-tuned as a pairwise sentence classifier. Each entity-candidate pair is encoded as a sentence pair and classified as matching or not, with the probability of a match used as the ranking score. The approach treats normalization as a pairwise ranking problem where named entities are queries and candidate concepts form pairs, allowing the model to learn from both positive and negative examples during training.

## Key Results
- Achieves 88.56% accuracy on LINNAEUS corpus
- Achieves 86.86% accuracy on S800 corpus
- Outperforms existing baselines OrganismTagger and ORGANISMS
- BERT re-ranking significantly improves accuracy over BM25 alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BM25 generates high-quality candidate lists that include the correct identifier for most named entities
- Mechanism: BM25 ranks candidate concepts based on term frequency and inverse document frequency, capturing lexical overlap between the entity and the candidate
- Core assumption: The correct identifier appears in the top-k BM25 candidates for a sufficient proportion of entities
- Evidence anchors: "Our method utilizes the widely-used information retrieval algorithm Best Matching 25 to generate candidate concepts"; "We choose the default values of parameters as k1 = 1.2, k2 = 100 and b = 0.75"
- Break condition: If the correct identifier is not in the top-k candidates for a significant number of entities, re-ranking cannot recover it

### Mechanism 2
- Claim: BERT-based re-ranking improves accuracy by capturing semantic similarity between entities and candidate concepts
- Mechanism: BERT encodes the entity-candidate pair as a sentence pair and learns to classify whether they match, using the probability of label=1 as a similarity score
- Core assumption: Semantic similarity correlates with correct identifier matching better than lexical overlap alone
- Evidence anchors: "followed by the application of bi-directional encoder representation from the encoder (BERT) to re-rank the candidate list"; "The probability of label = 1 is used as the score for reranking the list"
- Break condition: If BERT cannot distinguish between semantically similar but incorrect candidates, re-ranking won't improve accuracy

### Mechanism 3
- Claim: Pairwise learning to rank formulation enables effective training with limited labeled data
- Mechanism: Each training example is a (entity, candidate) pair with a binary label, allowing the model to learn from all candidates rather than just the correct one
- Core assumption: Learning from negative examples (incorrect candidates) improves model discrimination ability
- Evidence anchors: "treating it as a pair-wise learning to rank problem"; "In pLTR, named entities are considered queries and candidate concepts from the knowledge base form pairs with queries"
- Break condition: If the number of negative examples is insufficient to learn discrimination, pairwise learning won't provide benefits over pointwise approaches

## Foundational Learning

- Concept: Information retrieval fundamentals (BM25 algorithm)
  - Why needed here: BM25 generates candidate lists that BERT re-ranks
  - Quick check question: How does BM25 balance term frequency against document frequency when ranking candidates?

- Concept: Transformer architecture and pre-training objectives
  - Why needed here: BERT's pre-training enables effective fine-tuning for the re-ranking task
  - Quick check question: What is the purpose of the [CLS] token in BERT, and how is it used in classification tasks?

- Concept: Pairwise learning to rank vs. pointwise learning
  - Why needed here: The method treats normalization as a pairwise ranking problem rather than a classification problem
  - Quick check question: How does pairwise learning to rank differ from pointwise learning in terms of training objective and data requirements?

## Architecture Onboarding

- Component map: NCBI taxonomy dictionary → BM25 candidate generator → BERT re-ranker → Final identifier selection
- Critical path: Entity → BM25 → BERT scoring → Top-ranked candidate
- Design tradeoffs: Using pre-trained BERT vs. training from scratch (better generalization vs. task-specific optimization)
- Failure signatures: Low BM25 recall (correct identifiers not in top-k) or BERT failing to distinguish between semantically similar candidates
- First 3 experiments:
  1. Measure BM25 recall by checking if correct identifiers appear in top-10 candidates for each entity
  2. Evaluate BERT re-ranking effectiveness by comparing accuracy before and after re-ranking on a validation set
  3. Test the impact of different BERT models (BioBERT vs. BERT-base-uncased) on final accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed BERT-based re-ranking method compare to other deep learning approaches for biomedical entity normalization beyond species, such as for genes, diseases, or chemicals?
- Basis in paper: The paper compares the proposed method only to existing species normalization baselines and does not explore its applicability or performance on other biomedical entity types.
- Why unresolved: The paper focuses exclusively on species normalization and does not investigate the method's generalizability to other entity types or knowledge bases.
- What evidence would resolve it: Experiments applying the proposed BERT-based re-ranking approach to other biomedical entity normalization tasks and comparing its performance to state-of-the-art methods for those tasks.

### Open Question 2
- Question: What is the impact of using different candidate generation methods (beyond BM25) on the overall performance of the BERT-based re-ranking approach for species normalization?
- Basis in paper: The paper mentions that "rule-based, probabilistic, and semantic search-based candidate generators can be employed" but does not explore or compare the impact of using alternative candidate generation methods on the final normalization accuracy.
- Why unresolved: The paper uses BM25 as the candidate generation method but does not investigate how other methods might affect the quality of candidates and, consequently, the performance of the BERT-based re-ranking.
- What evidence would resolve it: Experiments comparing the performance of the BERT-based re-ranking approach when using different candidate generation methods and analyzing the relationship between candidate quality and final normalization accuracy.

### Open Question 3
- Question: How does the proposed method handle entities with high ambiguity or those that require context understanding beyond the sentence level for accurate normalization?
- Basis in paper: The paper discusses the challenges of ambiguity and term variation in entity normalization but does not explicitly address how the proposed method handles highly ambiguous entities or those requiring broader context for accurate normalization.
- Why unresolved: The paper focuses on the overall performance of the method but does not provide insights into its behavior with challenging cases that require deeper context understanding or disambiguation.
- What evidence would resolve it: Analysis of the method's performance on highly ambiguous entities or cases requiring broader context, along with ablation studies or modifications to the approach to improve its handling of such cases.

## Limitations

- Evaluation limited to two specific corpora (LINNAEUS and S800) that may not represent full biomedical text diversity
- Performance on highly ambiguous entities or cases requiring broader context is not explored
- Computational cost of BERT-based re-ranking for large candidate sets is not discussed

## Confidence

**High Confidence Claims:**
- BM25 effectively generates candidate lists containing correct identifiers
- BERT-based re-ranking improves accuracy over BM25 alone
- The method outperforms traditional rule-based systems like OrganismTagger and ORGANISMS

**Medium Confidence Claims:**
- Pairwise learning to rank formulation is optimal for this task
- The approach generalizes to other biomedical entity normalization tasks
- No manual feature engineering or rule creation is needed

**Low Confidence Claims:**
- The method achieves state-of-the-art performance across all biomedical entity normalization tasks
- The computational efficiency is sufficient for production deployment

## Next Checks

1. Measure BM25 recall by checking if correct identifiers appear in the top-10 candidates for each entity in the test sets
2. Compare accuracy when using only BM25, only BERT re-ranking, and the full pipeline to isolate component contributions
3. Apply the same pipeline to a different biomedical entity normalization task (e.g., gene/protein normalization) to assess cross-domain applicability