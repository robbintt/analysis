---
ver: rpa2
title: Are aligned neural networks adversarially aligned?
arxiv_id: '2306.15447'
source_url: https://arxiv.org/abs/2306.15447
tags:
- adversarial
- language
- attacks
- attack
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Current NLP-based attacks are not powerful enough to reliably cause
  aligned language models to emit harmful content. Even when adversarial examples
  are known to exist, state-of-the-art attacks fail to find them.
---

# Are aligned neural networks adversarially aligned?

## Quick Facts
- arXiv ID: 2306.15447
- Source URL: https://arxiv.org/abs/2306.15447
- Reference count: 26
- Current NLP-based attacks are not powerful enough to reliably cause aligned language models to emit harmful content

## Executive Summary
This paper investigates whether current alignment techniques can make language models robust against adversarial attacks that induce harmful outputs. The authors show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models, even when adversarial examples are known to exist. However, multimodal models that accept images as input can be easily attacked by perturbing the input image to induce arbitrary harmful behavior with only small distortions. This suggests that improved NLP attacks may be able to similarly control text-only aligned models.

## Method Summary
The paper evaluates the adversarial robustness of aligned language models using two attack approaches: NLP-based attacks (ARCA and GBDA) against aligned text models, and multimodal attacks that perturb image inputs to induce harmful text generation. The authors create test cases using GPT-2 to verify when adversarial examples exist, then test whether existing attacks can solve these cases. For multimodal attacks, they construct differentiable pipelines from image pixels through vision encoders to language model logits, optimizing for harmful outputs with small ℓ2 distortions.

## Key Results
- Current NLP attacks fail to find adversarial sequences even when brute-force methods verify their existence
- Multimodal attacks achieve 100% success rate at causing arbitrary toxic output with small image distortions
- LLaVA is 10× more vulnerable than other multimodal models, suggesting implementation details affect vulnerability
- Aligned models are not inherently robust - they appear robust only because current attacks are insufficiently powerful

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Current NLP attacks fail because they must operate in discrete token space while optimization requires gradients over continuous values.
- Mechanism: NLP attacks struggle to find adversarial sequences because text tokens are discrete while optimization methods require continuous gradients. The paper shows that even when adversarial examples exist (verified via brute force), existing attacks like ARCA and GBDA cannot find them.
- Core assumption: The discrete nature of text tokens creates a fundamental barrier that existing optimization methods cannot overcome.
- Evidence anchors:
  - [abstract]: "we show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models"
  - [section 4.3]: "Because ARCA and GBDA are largely ineffective at causing the model to emit toxic content in our setting even when allowed to inject thirty malicious tokens"
  - [corpus]: "Adversarial Illusions in Multi-Modal Embeddings" suggests continuous embedding space attacks are more effective
- Break condition: If new NLP attacks develop methods to effectively handle discrete token optimization, this mechanism would no longer explain the failure.

### Mechanism 2
- Claim: Multimodal models are vulnerable because adversarial image perturbations can be optimized in continuous pixel space.
- Mechanism: By perturbing image pixels continuously, attacks can easily find inputs that cause the language model to emit harmful content, bypassing the discrete token constraint that limits NLP attacks.
- Core assumption: The continuous nature of image pixels allows standard optimization techniques to find adversarial examples that would be difficult to find in discrete text space.
- Evidence anchors:
  - [abstract]: "multimodal models that accept images as input can be easily attacked by perturbing the input image to induce arbitrary harmful behavior, requiring only small distortions"
  - [section 6.1]: "Our attack approach directly follows the standard methodology for generating adversarial examples on image models"
  - [section 6.3]: "our attack has a 100% success rate at causing the model to emit arbitrary toxic output"
- Break condition: If multimodal models implement robust defenses against continuous perturbation attacks, this vulnerability would be mitigated.

### Mechanism 3
- Claim: Aligned models are not inherently robust - they appear robust only because current attacks are insufficiently powerful.
- Mechanism: The paper demonstrates that when adversarial examples are guaranteed to exist (via brute force search), current attacks still fail to find them, suggesting the apparent robustness is due to attack limitations rather than model resilience.
- Core assumption: The existence of brute-force-found adversarial examples proves that aligned models are not fundamentally robust, just hard to attack with current methods.
- Evidence anchors:
  - [abstract]: "even when adversarial examples are known to exist, state-of-the-art attacks fail to find them"
  - [section 5.2]: "we find that the existing state-of-the-art NLP attacks fail to successfully solve our test cases" even with 10× extra tokens
  - [section 5]: "This approach is effective when there exist effective brute force methods and the set of possible adversarial examples is effectively enumerable"
- Break condition: If future attacks consistently fail even when stronger methods are developed, this would suggest models may have genuine robustness.

## Foundational Learning

- Concept: Adversarial examples in machine learning
  - Why needed here: Understanding the fundamental concept of adversarial examples is crucial for grasping why current attacks fail and why multimodal attacks succeed
  - Quick check question: What makes an input an "adversarial example" versus just a normal input that happens to trigger unexpected behavior?

- Concept: Discrete vs continuous optimization spaces
  - Why needed here: The paper's core insight is that the discrete nature of text tokens creates a fundamental barrier that continuous image pixels do not have
  - Quick check question: Why can standard optimization techniques like gradient descent work on image pixels but not directly on text tokens?

- Concept: Alignment techniques (RLHF, instruction tuning)
  - Why needed here: Understanding how models are aligned helps explain why the paper focuses on "harmlessness" as a property that can be attacked
  - Quick check question: How do reinforcement learning from human feedback (RLHF) and instruction tuning differ in their approach to making models "helpful and harmless"?

## Architecture Onboarding

- Component map:
  - Multimodal model architecture: Vision encoder (CLIP) → Projection layer → Language model (Vicuna/LLaMA)
  - Attack pipeline: Random image → Continuous optimization → Adversarial image → Language model output
  - Evaluation: Toxicity classifier (substring matching) → Success/failure measurement

- Critical path: Image perturbation → Language model generation → Toxicity check
  - The optimization loop must be differentiable from pixels through the entire model to the output logits
  - Temperature is set to 0 to make models deterministic for evaluation

- Design tradeoffs:
  - Using substring matching for toxicity is simple but may miss nuanced cases
  - Open-source models are used instead of proprietary ones like GPT-4 due to accessibility
  - The attack targets small distortions to demonstrate ease of attack, though larger distortions might be more effective

- Failure signatures:
  - Attack fails to find adversarial images even with many optimization steps
  - Model outputs benign content despite adversarial input
  - Toxicity classifier misses harmful content due to simple matching

- First 3 experiments:
  1. Test standard attack on clean random images to establish baseline success rate
  2. Apply attack with ℓ2 distortion constraint to measure effectiveness under realistic conditions
  3. Compare success rates across different multimodal model implementations (Mini GPT-4 vs LLaVA vs LLaMA Adapter)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective would improved NLP attacks be at inducing harmful outputs in aligned text-only models?
- Basis in paper: Inferred from the authors' conjecture that improved NLP attacks may demonstrate adversarial control over text-only models, similar to what they achieved with multimodal models.
- Why unresolved: The authors show that current NLP attacks are not powerful enough to reliably attack aligned text models, but they have not tested improved attacks that may be developed in the future.
- What evidence would resolve it: Testing new, more powerful NLP attacks on aligned text models to see if they can successfully induce harmful outputs.

### Open Question 2
- Question: What specific design decisions in multimodal models make them more vulnerable to adversarial attacks?
- Basis in paper: Inferred from the observation that LLaVA is 10x more vulnerable than other multimodal models, indicating that implementation details affect vulnerability.
- Why unresolved: The paper does not analyze the specific architectural or training differences between multimodal models that lead to varying levels of vulnerability.
- What evidence would resolve it: Detailed analysis and comparison of different multimodal model architectures and training methods to identify the factors that increase vulnerability to adversarial attacks.

### Open Question 3
- Question: Can alignment techniques be improved to make models more robust to adversarial inputs?
- Basis in paper: Explicit, as the authors state that eliminating risks via current alignment techniques is unlikely to succeed, and call for improved evaluation of defenses.
- Why unresolved: The paper does not propose or test new alignment techniques that specifically account for adversarially optimized inputs.
- What evidence would resolve it: Developing and testing new alignment techniques that are designed to be robust against adversarial attacks, and evaluating their effectiveness.

## Limitations

- The paper focuses on specific alignment techniques and model architectures, which may not generalize to all aligned language models
- Simple substring matching for toxicity detection may miss nuanced harmful content that more sophisticated classifiers would catch
- The study does not explore potential hybrid approaches that could bridge the gap between discrete token optimization and continuous optimization techniques

## Confidence

- High Confidence: The empirical finding that current NLP attacks (ARCA, GBDA) fail to find adversarial examples even when they are known to exist via brute force. The multimodal attack results showing 100% success rate with small distortions are also highly reliable based on the experimental setup.
- Medium Confidence: The theoretical claim that discrete token spaces create a fundamental barrier to NLP attacks. While the evidence strongly supports this, future attack innovations could potentially overcome this limitation.
- Low Confidence: The broader implication that aligned models are not inherently robust and only appear robust due to weak attacks. This conclusion assumes current attack methods represent the limits of what's possible, which may not hold as attack techniques advance.

## Next Checks

1. **Advanced NLP Attack Implementation**: Implement and evaluate newer NLP attack methods such as differentiable search over token embeddings with projection to discrete tokens, or language model-guided search strategies that could potentially overcome the discrete optimization barrier demonstrated in the current study.

2. **Enhanced Toxicity Detection**: Replace the simple substring matching approach with a state-of-the-art toxicity classifier to evaluate whether the multimodal attacks produce more nuanced harmful content that current evaluation misses, and whether aligned models show robustness against sophisticated detection methods.

3. **Cross-Alignment Technique Evaluation**: Test the same attack methodology across multiple alignment techniques (RLHF, Constitutional AI, Debate) and model families to determine whether the observed vulnerabilities are specific to the alignment approaches used in the study or represent broader weaknesses in aligned language models.