---
ver: rpa2
title: Variational Density Propagation Continual Learning
arxiv_id: '2308.11801'
source_url: https://arxiv.org/abs/2308.11801
tags:
- learning
- network
- variational
- task
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a continual learning framework based on variational
  density propagation (VDP) to mitigate catastrophic forgetting. The method propagates
  the first two moments (mean and covariance) of the variational distribution through
  network layers using a first-order Taylor series approximation, removing the need
  for Monte Carlo sampling.
---

# Variational Density Propagation Continual Learning

## Quick Facts
- arXiv ID: 2308.11801
- Source URL: https://arxiv.org/abs/2308.11801
- Reference count: 24
- Key outcome: VDP framework propagates first two moments of variational distribution through layers using Taylor approximation, mitigating catastrophic forgetting by penalizing model likelihood changes via KL divergence regularization.

## Executive Summary
This paper introduces Variational Density Propagation (VDP), a continual learning framework that propagates the mean and covariance of the variational distribution through network layers using first-order Taylor series approximation. This approach eliminates the need for computationally expensive Monte Carlo sampling while maintaining uncertainty quantification. Catastrophic forgetting is addressed by minimizing KL divergence between variational posteriors of consecutive tasks, effectively approximating the Minimum Description Length (MDL) principle. The framework is evaluated on task incremental learning using fully-connected and convolutional networks across benchmark datasets (MNIST, CIFAR10) with varying task sequence lengths, demonstrating improved average test classification accuracy and backward transfer compared to baselines.

## Method Summary
The VDP framework optimizes a closed-form Evidence Lower Bound (ELBO) objective that approximates the predictive distribution by propagating the first two moments (mean and covariance) of the variational distribution through all network layers. A first-order Taylor series approximation enables efficient computation of these moments without Monte Carlo sampling. Catastrophic forgetting is mitigated by minimizing the KL divergence between the variational posterior for the current task and the previous task's variational posterior, which acts as the prior. This process effectively approximates the Minimum Description Length (MDL) principle, inherently penalizing changes in model likelihood.

## Key Results
- Improved average test classification accuracy compared to Monte Carlo sampling-based approaches and deterministic baselines
- Better backward transfer performance, indicating reduced catastrophic forgetting
- Effective mitigation of catastrophic forgetting across task sequences on MNIST, CIFAR10, and Permuted MNIST datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Propagating first two moments (mean and covariance) of variational distribution through network layers removes need for Monte Carlo sampling.
- Mechanism: Uses first-order Taylor series approximation to propagate mean and diagonal covariance elements through linear and non-linear layers, approximating predictive uncertainty without sampling.
- Core assumption: First-order Taylor series approximation sufficiently captures distributional changes through network transformations.
- Evidence anchors:
  - [abstract] "We optimize a closed-form Evidence Lower Bound (ELBO) objective approximating the predictive distribution by propagating the first two moments of a distribution, i.e. mean and covariance, through all network layers."
  - [section] "The goal for moments propagation is to produce the mean and the covariance of the predictive distribution... each algebraic operation in a deterministic network is replaced by the multiplication of a random variable with a constant, multiplication of two random variables, or approximating the non-linear transformation over random variables using a first-order Taylor-series approximation."
  - [corpus] Weak - corpus neighbors focus on sequential Bayesian inference and weight consolidation rather than density propagation.
- Break condition: Higher-order interactions between weights cause significant approximation error; non-linearities exhibit strong non-Gaussian behavior.

### Mechanism 2
- Claim: Minimizing KL divergence between variational posteriors approximates Minimum Description Length (MDL) principle.
- Mechanism: Sets prior for task t as posterior from task t-1, so KL term penalizes changes in model complexity, encouraging sparsity and preventing catastrophic forgetting.
- Core assumption: Changes in variational posterior parameters directly correlate with changes in model complexity.
- Evidence anchors:
  - [abstract] "Catastrophic forgetting is mitigated by using the closed-form ELBO to approximate the Minimum Description Length (MDL) Principle, inherently penalizing changes in the model likelihood by minimizing the KL Divergence between the variational posterior for the current task and the previous task's variational posterior acting as the prior."
  - [section] "This process aligns with the idea of the Minimum Description Length principle... By converging to the minimal model complexity on the first task, all subsequent tasks will retain the goal of minimal model complexity via updating the prior distribution to the variational posterior."
  - [corpus] Weak - corpus focuses on Bayesian continual learning and Fisher regularization rather than MDL approximation.
- Break condition: Model complexity changes are not solely captured by parameter distribution shifts; some forgetting may be necessary for learning new tasks.

### Mechanism 3
- Claim: Factorized variational distribution with independent parameters enables efficient gradient updates.
- Mechanism: Assumes all model parameters are independent random variables, simplifying KL divergence computation and allowing each parameter to receive independent gradient updates from KL term.
- Core assumption: Parameter independence assumption is reasonable approximation for deep networks.
- Evidence anchors:
  - [section] "The model parameters are assumed to be independent from each other... This formulation allows each parameter to receive independent gradient updates from the KL term so the new posterior remains close to the previous posterior while receiving updates from the model likelihood."
  - [corpus] Weak - corpus neighbors discuss weight consolidation and Fisher regularization rather than parameter independence.
- Break condition: Parameter dependencies become too strong; independent assumption breaks down leading to poor uncertainty estimation.

## Foundational Learning

- Concept: Bayesian inference and variational approximation
  - Why needed here: Framework uses variational inference to approximate intractable posterior over network weights, enabling uncertainty quantification and continual learning
  - Quick check question: Why can't we compute the true posterior over network weights directly?

- Concept: Taylor series approximation for random variable transformations
  - Why needed here: Enables propagation of first two moments through non-linear network layers without sampling
  - Quick check question: What happens to the approximation quality when non-linearities have high curvature?

- Concept: KL divergence and its role in regularization
  - Why needed here: KL term in ELBO objective penalizes changes in model parameters, approximating MDL principle to prevent catastrophic forgetting
  - Quick check question: How does the KL divergence term balance between fitting new data and preserving old knowledge?

## Architecture Onboarding

- Component map: Input -> Dense/CNN layers (shared) -> Task-specific output layer(s) -> Loss (ELBO) -> Optimizer (Adam)
- Critical path: Forward pass through network computing mean/variance -> ELBO computation with KL term -> Backpropagation -> Parameter update. Moment propagation and KL computation are bottlenecks.
- Design tradeoffs: Full covariance propagation (accurate but expensive) vs diagonal approximation (efficient but loses correlations); strong KL regularization (better forgetting prevention but slower learning) vs weak regularization (faster learning but more forgetting)
- Failure signatures: Poor performance on new tasks (underfitting due to strong regularization); catastrophic forgetting (under-regularization or poor initialization); unstable training (learning rate too high or variance initialization problematic)
- First 3 experiments:
  1. Train VDP PC on 2-Split MNIST with varying KL weights (τ) to find sweet spot between forgetting and learning
  2. Compare VDP PC with VDP-FT and VDP-JT on 5-Split CIFAR10 to validate forgetting mitigation
  3. Test VDP PC with different network depths on Permuted MNIST to understand approximation limits

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but based on the methodology and results presented, several questions emerge regarding the limitations and potential extensions of the approach.

## Limitations

- Taylor series approximation may fail to capture higher-order interactions between weights, particularly in networks with strong non-linearities
- Factorized variational distribution assumption may not hold for complex deep networks where parameter dependencies are significant
- Requires careful tuning of KL divergence weighting term τ, with poor choices leading to either excessive forgetting or inability to learn new tasks

## Confidence

- Mechanism 1: Medium - Theoretically sound but approximation quality not thoroughly validated
- Mechanism 2: Medium - Conceptually justified but effectiveness in preventing forgetting needs broader validation
- Mechanism 3: Medium - Practical benefit demonstrated but independence assumption may not hold for all architectures

## Next Checks

1. **Approximation Error Analysis**: Systematically evaluate the impact of Taylor series approximation order on performance across different non-linear activation functions and network depths.

2. **Parameter Dependence Testing**: Compare VDP performance with and without the factorized variational assumption on datasets where parameter correlations are known to be significant.

3. **Scalability Validation**: Test the framework on larger-scale continual learning benchmarks (e.g., CORe50, TinyImageNet) to verify computational tractability and performance retention.