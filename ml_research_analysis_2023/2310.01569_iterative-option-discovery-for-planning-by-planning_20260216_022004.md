---
ver: rpa2
title: Iterative Option Discovery for Planning, by Planning
arxiv_id: '2310.01569'
source_url: https://arxiv.org/abs/2310.01569
tags:
- options
- policy
- option
- learned
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Option Iteration (OptIt), a novel approach
  to option discovery for planning by amortizing the results of computationally expensive
  search. OptIt learns a set of option policies such that in each encountered state,
  at least one option is good for a fixed horizon into the future.
---

# Iterative Option Discovery for Planning, by Planning

## Quick Facts
- arXiv ID: 2310.01569
- Source URL: https://arxiv.org/abs/2310.01569
- Reference count: 20
- Key outcome: Option Iteration (OptIt) significantly outperforms Expert Iteration (ExIt) in gridworld planning tasks by learning a set of option policies through agreement optimization with search results

## Executive Summary
Option Iteration (OptIt) introduces a novel approach to option discovery for planning by learning a set of option policies through iterative agreement optimization with search results. The method trains options such that for each encountered state, at least one policy matches the search results for a fixed horizon into the future. This creates a virtuous cycle where learned options improve search, and improved search results lead to better options. The approach demonstrates significant performance gains compared to Expert Iteration, which learns a single rollout policy, across multiple gridworld planning domains.

## Method Summary
OptIt learns a set of option policies using Monte Carlo Search (MCS) operating in the joint space of primitive actions and options. The algorithm optimizes agreement between the learned option policies and search results over K-step trajectory segments, using a cross-entropy loss between the search policy and a weighted mixture of option policies. The learned options are then used to guide future search iterations, creating a self-improving cycle. The method employs a neural network architecture with shared trunk and separate output heads for each option policy and policy over options, trained with entropy regularization to encourage diversity.

## Key Results
- OptIt achieves significantly higher undiscounted returns than ExIt baseline in ElectricProcMaze environment
- The learned options demonstrate interpretable behaviors (e.g., directional movement policies in gridworlds)
- Performance improvements are most pronounced in environments requiring extended planning horizons

## Why This Works (Mechanism)

### Mechanism 1
Learning a set of options amortizes search results better than a single policy by representing uncertainty over optimal actions. The algorithm optimizes a weighted mixture of option policies to match search results across trajectory segments, capturing joint distributions over actions that a single policy cannot represent. This works when environments have local correlation between optimal actions in temporally contiguous states, making a small set of options sufficient to represent plausible future behaviors.

### Mechanism 2
Using learned options to guide Monte Carlo Search creates a virtuous cycle where better options lead to better search results. The algorithm searches in the joint space of primitive actions and options, evaluating each action under various possible behaviors rather than a single learned rollout policy, then uses these improved search results to update the option policies. This works when evaluating actions under a diverse set of plausible future behaviors provides more information than evaluating under a single policy.

### Mechanism 3
Optimizing agreement over trajectory segments rather than single steps encourages options to be useful for extended horizons. The loss function maximizes log-likelihood of search actions over K-step trajectory segments under a weighted mixture of options, creating pressure for at least one option to match the search policy across multiple steps. This temporal coherence requirement ensures options are useful beyond single-step decisions.

## Foundational Learning

- Concept: Expert Iteration (ExIt) as approximate policy iteration
  - Why needed here: OptIt builds directly on ExIt methodology, modifying it to learn a set of policies rather than a single policy
  - Quick check question: How does ExIt differ from traditional policy iteration in terms of computational efficiency?

- Concept: Options framework and temporal abstraction
  - Why needed here: The entire approach relies on the options framework for representing temporally extended behaviors
  - Quick check question: What are the three components of an option in the original Sutton et al. (1999) formulation?

- Concept: Entropy regularization in policy learning
  - Why needed here: Both the option policies and the policy over options use entropy regularization to encourage exploration and diversity
  - Quick check question: How does entropy regularization affect the learned policy distribution in reinforcement learning?

## Architecture Onboarding

- Component map: Search module -> Replay buffer -> Option learner (shared trunk, separate heads) -> Value function -> Updated options -> Search module
- Critical path: Search → Store results in buffer → Sample batch → Compute loss → Update option policies and value function → Use updated options in next search iteration
- Design tradeoffs:
  - Fixed horizon options vs. learned termination conditions: Fixed horizon simplifies implementation but reduces flexibility
  - Number of options: More options increase representational capacity but also computational cost during search
  - Batch size vs. learning speed: Larger batches provide more stable gradients but slower learning per environment step
- Failure signatures:
  - Options collapse to single-step actions: Indicates insufficient pressure for temporal abstraction in loss function
  - All options behave identically: Suggests lack of diversity encouragement or insufficient exploration
  - Search performance degrades over time: May indicate overfitting to specific search patterns or insufficient regularization
- First 3 experiments:
  1. Implement basic ExIt baseline in Compass environment to verify understanding of single-policy learning
  2. Add second option to OptIt and verify that the algorithm can learn distinct behaviors (e.g., one option for moving up, another for moving down)
  3. Test sensitivity to K (trajectory length) by running with K=1, K=5, and K=10 in ElectricProcMaze to observe impact on option quality

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Option Iteration compare to existing option discovery methods like the Option-Critic architecture or diversity-driven approaches when applied to single-task planning problems? The authors did not conduct experiments comparing OptIt to other established option discovery methods.

### Open Question 2
What is the impact of using more sophisticated bandit algorithms like PUCB or Sequential Halving instead of the simple uniform allocation strategy used in the Monte Carlo Search with Options? The paper uses a simple uniform allocation strategy and does not explore the potential benefits of more sophisticated bandit algorithms.

### Open Question 3
How does the performance of Option Iteration scale with the number of options, and is there an optimal number of options for a given problem? The paper uses 4-5 options in the experiments, but does not systematically explore the impact of varying the number of options.

### Open Question 4
How does Option Iteration perform in environments with sparse rewards, and can it be combined with intrinsic reward signals to improve exploration? The authors mention that OptIt relies on the reward signal and suggest using intrinsic rewards as a potential solution for sparse reward environments.

## Limitations

- Scalability to larger, more complex environments with higher-dimensional state spaces is not demonstrated
- Sensitivity to hyperparameters (number of options, trajectory length K, simulation budget) requires further exploration
- Computational overhead of searching in the joint space of actions and options may limit practical applicability

## Confidence

- High: The core mechanism of learning options through agreement optimization with search results
- Medium: The claimed performance improvements over ExIt in the tested environments
- Low: The generalizability to non-gridworld domains and larger state spaces

## Next Checks

1. Test sensitivity to number of options by running experiments with 2, 5, and 10 options in ElectricProcMaze to determine optimal setting
2. Evaluate computational overhead by measuring total training time per iteration compared to ExIt baseline
3. Apply OptIt to a non-gridworld domain (e.g., continuous control task) to assess scalability beyond discrete state spaces