---
ver: rpa2
title: Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000
  TPU Months
arxiv_id: '2310.18191'
source_url: https://arxiv.org/abs/2310.18191
tags:
- velo
- steps
- training
- optimizer
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically evaluates VeLO, a large-scale learned optimizer
  that was trained on thousands of machine learning tasks using over 4000 TPU months.
  The authors independently assess VeLO on the MLCommons optimizer benchmark suite
  to verify its claims of being hyperparameter-free, outperforming industry standards
  like Adam, and providing faster optimization.
---

# Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's 4000 TPU Months

## Quick Facts
- arXiv ID: 2310.18191
- Source URL: https://arxiv.org/abs/2310.18191
- Reference count: 40
- Authors: 
- Key outcome: VeLO fails to meet its claimed advantages of being hyperparameter-free, outperforming baselines, and providing faster optimization when evaluated on MLCommons benchmarks

## Executive Summary
This paper critically evaluates VeLO, a large-scale learned optimizer trained on thousands of machine learning tasks using over 4000 TPU months. The authors independently assess VeLO on the MLCommons optimizer benchmark suite to verify its claims of being hyperparameter-free, outperforming industry standards like Adam, and providing faster optimization. Their results challenge these claims, showing that VeLO has a critical hyperparameter that needs problem-specific tuning, does not consistently outperform baselines in quality of solution found, and is not faster than competing optimizers at reducing training loss. These findings question VeLO's generality and the value of the substantial investment in training it, suggesting that scaling up learned optimizer training may not be the silver bullet it was hoped to be.

## Method Summary
The authors evaluate VeLO using the MLCommons optimizer benchmark suite, which includes four diverse workloads: ResNet-50 on ImageNet, GNN on OGBG, DLRM on Criteo-1TB, and U-Net on FastMRI. They test three key claims about VeLO: that it is hyperparameter-free, outperforms baselines in quality of solution found, and provides faster optimization than competitors. The evaluation uses both time-to-performance target protocols and fixed-step budget comparisons, measuring both wall-clock time and steps to reach performance targets. VeLO is tested with different total steps prompts (100% vs 75% of maximum runtime) to assess its hyperparameter sensitivity, and results are compared against baseline optimizers including Adam, NAdam, NAdamW, Heavy Ball, and Nesterov.

## Key Results
- VeLO requires a critical hyperparameter (total steps prompt) that needs problem-specific tuning
- VeLO does not consistently outperform standard optimizers on training and validation loss minimization
- VeLO is not faster than competing optimizers at reducing training loss, with higher per-iteration computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VeLO requires a critical hyperparameter (total steps prompt) for initialization.
- Mechanism: VeLO's hierarchical hypernetwork architecture uses the total steps prompt to compute the fraction of training remaining, which directly influences the optimizer's internal state initialization and subsequent learning dynamics.
- Core assumption: The optimizer's performance is sensitive to the initialization of its internal states based on expected training duration.
- Evidence anchors:
  - [abstract] states "VeLO has a critical hyperparameter that needs problem-specific tuning"
  - [section] describes that "VeLO needs one special input: It must be prompted with the total training steps it is expected to run for in order to initialize its states"
  - [corpus] shows no direct evidence, indicating weak external validation
- Break condition: If the steps prompt is not representative of actual training duration, performance degrades significantly as shown by timeout failures and suboptimal convergence.

### Mechanism 2
- Claim: VeLO does not consistently outperform standard optimizers on training and validation loss minimization.
- Mechanism: Despite being trained on a large task distribution, VeLO's generalization to MLCommons benchmarks is limited, and different optimizer types excel on different workloads.
- Core assumption: Training on a diverse but specific task distribution leads to better generalization across similar tasks, but may underperform on out-of-distribution workloads.
- Evidence anchors:
  - [abstract] states "VeLO does not necessarily outperform competitors in quality of solution found"
  - [section] reports "VeLO is not a consistent winner in either train or validation loss achieved"
  - [corpus] neighbor papers focus on alternative learned optimizer approaches but don't directly validate or refute VeLO's performance claims
- Break condition: If the evaluation workload distribution significantly differs from the training distribution, the learned optimizer may underperform compared to well-tuned hand-crafted optimizers.

### Mechanism 3
- Claim: VeLO does not provide dramatically faster optimization than standard baselines.
- Mechanism: VeLO's per-iteration computational overhead combined with limited generalization to new tasks results in slower wall-clock time to target compared to optimized baselines.
- Core assumption: A learned optimizer's computational efficiency advantage can be negated by poor generalization and higher per-iteration cost.
- Evidence anchors:
  - [abstract] states "VeLO is not faster than competing optimizers at reducing the training loss"
  - [section] shows "VeLO is far from best in training speed" when evaluated on MLCommons benchmarks
  - [corpus] neighbor papers discuss computational aspects but don't directly address speed comparisons
- Break condition: If the learned optimizer's computational overhead per iteration exceeds the benefit gained from better optimization trajectories, it will be slower overall.

## Foundational Learning

- Concept: Bi-level optimization framework for learned optimizers
  - Why needed here: Understanding how the outer optimizer learns the inner optimization strategy is crucial for grasping VeLO's training methodology
  - Quick check question: How does the meta-training objective in Equation 3 relate to the learned optimizer's ability to generalize to new tasks?

- Concept: Performance profiling methodology
  - Why needed here: The MLCommons benchmark uses performance profiles to aggregate optimizer comparisons across multiple workloads, which is essential for interpreting the results
  - Quick check question: What does a performance ratio rˆa,w > 1 indicate about optimizer a's performance on workload w?

- Concept: Hyperparameter sensitivity analysis
  - Why needed here: VeLO's critical dependence on the steps prompt hyperparameter demonstrates the importance of understanding sensitivity in learned optimizer evaluation
  - Quick check question: How does changing the steps prompt from 100% to 75% of maximum runtime affect VeLO's ability to reach performance targets?

## Architecture Onboarding

- Component map: Iteration features and gradients → Per-tensor LSTM → Per-parameter MLP → Parameter updates
- Critical path: Steps prompt → state initialization → hypernetwork parameter generation → per-parameter MLP → optimization updates
- Design tradeoffs: Large scale meta-training provides broad generalization potential but requires significant computational resources; hierarchical architecture enables complex optimization strategies but increases per-iteration cost
- Failure signatures: Timeout failures when prompted with insufficient steps; poor performance on out-of-distribution workloads; higher per-iteration computational cost than baselines
- First 3 experiments:
  1. Run VeLO with different steps prompts (100% vs 75% of maximum runtime) on a single workload to observe sensitivity
  2. Compare VeLO against Adam on training loss minimization for fixed step budgets
  3. Measure wall-clock time to reach fixed performance targets across multiple workloads

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does VeLO's underperformance stem from insufficient generalization to MLCommons tasks or from inherent limitations in the learned optimizer architecture?
- Basis in paper: [explicit] The paper attributes VeLO's failure to meet claims to both weak generalization to MLCommons tasks and the effectiveness of Adam's default learning rate decay schedule.
- Why unresolved: The paper does not definitively determine which factor is more significant in causing VeLO's underperformance.
- What evidence would resolve it: A controlled experiment isolating the effects of task distribution mismatch versus architectural limitations, or a comprehensive hyperparameter optimization of VeLO on MLCommons tasks.

### Open Question 2
- Question: Is the significant upfront training cost of learned optimizers like VeLO justified by their potential for amortization across diverse machine learning workloads?
- Basis in paper: [inferred] The paper questions the value of VeLO's 4000 TPU months training investment due to its failure to meet claimed performance improvements and its hyperparameter sensitivity.
- Why unresolved: The paper provides empirical evidence against VeLO's claims but does not fully explore the broader economic implications of learned optimizer training costs versus potential benefits.
- What evidence would resolve it: A comprehensive cost-benefit analysis comparing the total computational cost of training learned optimizers against the cumulative computational savings they provide across diverse workloads over time.

### Open Question 3
- Question: How does the VeLOdrome benchmark suite compare to real-world machine learning tasks in terms of difficulty and task diversity?
- Basis in paper: [explicit] The paper suggests that VeLOdrome tasks may be "unrealistically easy" compared to MLCommons benchmarks, potentially explaining VeLO's strong performance on VeLOdrome but weak performance on MLCommons.
- Why unresolved: The paper does not provide a detailed analysis of the task distributions in VeLOdrome versus MLCommons, nor does it quantify the degree of task similarity between VeLO's training distribution and VeLOdrome.
- What evidence would resolve it: A systematic comparison of task characteristics (e.g., dataset size, model complexity, loss landscape properties) between VeLOdrome and a diverse set of real-world machine learning tasks, along with an analysis of the similarity between these tasks and VeLO's training distribution.

## Limitations
- Evaluation relies on MLCommons benchmarks which may not fully represent the diversity of tasks VeLO was trained on
- Unknown implementation details of VeLO's architecture and interface create barriers to faithful reproduction
- Uncertainty about whether VeLO's performance degradation is fundamental or can be mitigated through improved prompt engineering

## Confidence
- Hyperparameter claim: Medium confidence (VeLO requires problem-specific tuning of the steps prompt)
- Quality of solution claim: Medium confidence (VeLO does not consistently outperform baselines)
- Speed claim: Medium confidence (VeLO is not faster at reducing training loss)

## Next Checks
1. Evaluate VeLO across a broader distribution of tasks, including those more similar to its training set, to distinguish between fundamental limitations and evaluation bias.

2. Test alternative prompt engineering approaches for the steps hyperparameter to determine if performance can be improved without requiring 100% runtime estimation.

3. Profile VeLO's per-iteration computational overhead in detail to quantify the tradeoff between optimization quality and computational cost compared to hand-crafted optimizers.