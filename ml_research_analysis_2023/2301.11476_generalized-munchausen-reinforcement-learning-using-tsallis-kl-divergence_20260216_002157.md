---
ver: rpa2
title: Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence
arxiv_id: '2301.11476'
source_url: https://arxiv.org/abs/2301.11476
tags:
- tsallis
- learning
- policy
- regularization
- entropy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work generalizes KL regularization to Tsallis KL regularization
  in reinforcement learning. The Tsallis KL uses a q-logarithm instead of the standard
  logarithm, allowing for a broader family of regularization schemes.
---

# Generalized Munchausen Reinforcement Learning using Tsallis KL Divergence

## Quick Facts
- arXiv ID: 2301.11476
- Source URL: https://arxiv.org/abs/2301.11476
- Reference count: 40
- This work generalizes KL regularization to Tsallis KL regularization in reinforcement learning, achieving up to 400% improvement on Atari games with MVI(q=2) over standard MVI.

## Executive Summary
This paper extends Munchausen Value Iteration (MVI) by replacing standard KL regularization with Tsallis KL regularization, using q-logarithm instead of standard logarithm. This generalization allows for a broader family of regularization schemes, with q=1 recovering standard KL and q>1 offering new options. The authors prove convergence for Tsallis KL regularization and characterize the resulting policies as performing weighted averaging over past action values, with weighting controlled by q. Experiments on 35 Atari games show that MVI(q=2) significantly outperforms standard MVI (q=1), achieving up to 400% improvement over Tsallis-VI (which uses only Tsallis entropy regularization without KL regularization).

## Method Summary
The method implements MVI(q) by modifying the standard MVI algorithm to use q-logarithm and q-exponential functions for policy parameterization. The algorithm uses Q-network architecture with Conv4-Conv2-Conv1-FC512-FC layers, entropy coefficient τ=10, advantage coefficient α=0.9, and total training steps of 5×10^7. Policy evaluation updates Q-values using the Munchausen term Qk-Mq,τQk, which approximates implicit regularization. The method is tested on 35 Atari 2600 games with epsilon-greedy exploration (1.0→0.01 over first 10% of training) and Adam optimizer with learning rate 10^-4.

## Key Results
- MVI(q=2) achieves up to 400% improvement over Tsallis-VI on Atari games
- MVI(q=2) significantly outperforms standard MVI (q=1) across multiple benchmarks
- Convergence is proven for q=2 but remains open for other q values
- MVI(q=2) shows reduced performance on hard exploration games compared to MVI(q=1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tsallis KL regularization produces weighted averaging over past action-values, not uniform averaging.
- Mechanism: Replacing standard logarithm with q-logarithm introduces nonlinear interactions between past Q-values. For q=2, this creates terms like ∑ᵢ<ⱼ QᵢQⱼ in the policy, weighting later estimates differently than earlier ones.
- Core assumption: The q-logarithm pseudo-additivity property (ln_q(ab) ≠ ln_q(a) + ln_q(b)) fundamentally changes how past information is combined.
- Evidence anchors: [abstract]: "The Tsallis KL uses a q-logarithm instead of the standard logarithm, allowing for a broader family of regularization schemes." [section 3.3]: "Equation (10) states that any estimate Qⱼ is weighted by the summation of estimates before Qⱼ times (q-1)"
- Break condition: If q→1, the weighting disappears and we recover standard KL averaging. If q is very large, the weighting may become unstable or degenerate.

### Mechanism 2
- Claim: Tsallis KL regularization with q>1 improves learning speed and final performance compared to standard KL.
- Mechanism: The sparsemax policy induced by q=2 concentrates probability on higher-valued actions while maintaining stochasticity among the best options, reducing exploration of poor actions.
- Core assumption: Truncating low-value actions accelerates learning by focusing computation on promising regions of the action space.
- Evidence anchors: [abstract]: "Experiments on 35 Atari games show that MVI(q=2) significantly outperforms standard MVI (q=1), achieving up to 400% improvement" [section 5.1]: "The KL regularization term, however, likely slows this down. It is possible the Tsallis-VI is concentrating too quickly, resulting in insufficient exploration."
- Break condition: In environments requiring broad exploration, the truncation may harm performance by missing rare high-reward actions.

### Mechanism 3
- Claim: MVI(q) implicitly performs Tsallis KL regularization plus entropy regularization through its update rule.
- Mechanism: The Munchausen term Qk-Mq,τQk approximates the implicit regularization structure, with the advantage term capturing both KL and entropy effects.
- Core assumption: The approximation Qk-Mq,τQk ≈ -τ ln_q(1/πk+1) is sufficiently accurate to maintain regularization benefits.
- Evidence anchors: [section 4.2]: "It is not the case that -ατ lnq 1/πk+1 equals Qk-Mq,τQk" - acknowledges approximation limitations [section 4.2]: "Since ατ lnq 1/πk+1 is itself an approximation and one term is omitted, it is actually possible that Qk-Mq,τQk more faithfully approximates KL regularization plus entropy regularization."
- Break condition: If the approximation breaks down (e.g., for extreme q values), the regularization benefits may disappear.

## Foundational Learning

- Concept: q-logarithm and q-exponential functions
  - Why needed here: These functions define Tsallis entropy and Tsallis KL divergence, which are the core generalizations being studied
  - Quick check question: What happens to ln_q(x) and exp_q(x) as q approaches 1? (They should converge to standard logarithm and exponential)

- Concept: Strong convexity in policy optimization
  - Why needed here: Strong convexity of the regularizer is required for convergence guarantees of the value iteration algorithm
  - Quick check question: Why does the paper prove convergence only for q=2, not for all q values? (Because Dq_KL is strongly convex only for certain q)

- Concept: Sparsemax policy parameterization
  - Why needed here: The q=2 case produces sparsemax policies that truncate low-value actions, which is key to understanding the performance improvements
  - Quick check question: How does sparsemax differ from standard softmax in terms of action probability distribution? (Sparsemax can assign zero probability to some actions)

## Architecture Onboarding

- Component map: Q-values -> q-exponential -> Policy -> Policy evaluation -> Advantage term -> Q-update -> Q-values

- Critical path: 1. Compute Q-values from current policy 2. Apply q-exponential to get next policy 3. Compute advantage term Qk-Mq,τ Qk 4. Update Q-values with Munchausen term 5. Repeat

- Design tradeoffs:
  - q=1 (standard MVI): Proven convergence, but potentially slower learning
  - q=2: Better empirical performance, but requires sparsemax computation
  - q>2: May offer benefits in some environments, but risks numerical instability

- Failure signatures:
  - Divergence: Check if q-logarithm arguments become negative
  - Poor performance: Verify sparsemax computation and advantage term
  - Numerical instability: Monitor q-exponential and q-logarithm calculations

- First 3 experiments:
  1. Implement q=1 case and verify it matches standard MVI performance
  2. Test q=2 case on a simple environment to confirm sparsemax behavior
  3. Compare learning curves for different q values on a benchmark task to identify optimal q range

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the Munchausen term Qk-Mq,τ Qk and the Tsallis KL regularization term for q>1?
- Basis in paper: [explicit] The paper states that unlike q=1, the relationship between the Munchausen term and implicit KL regularization is lost for q>1, and that the additional term weighted by q-1 poses a problem as it involves both k and k+1.
- Why unresolved: The paper acknowledges that this is a big open question and empirically tested both approximations (adding ατ lnq 1/pik+1 vs adding Qk-Mq,τ Qk) without a clear theoretical understanding of the difference.
- What evidence would resolve it: A rigorous mathematical analysis of the relationship between the Munchausen term and implicit Tsallis KL regularization for q>1, potentially involving a different formulation or approximation that avoids the k and k+1 dependency issue.

### Open Question 2
- Question: How does the choice of q>1 impact the policy learned by MVI(q), and why do certain q values perform better in certain environments?
- Basis in paper: [explicit] The paper shows that q=2 performs well on some environments but q=3 and q=4 perform poorly, and mentions an oscillating pattern in performance across different q values.
- Why unresolved: The paper mentions that the oscillatory behavior is not yet fully understood and that it's important to understand the role of q on the policy and why certain q are more effective in certain environments.
- What evidence would resolve it: A comprehensive study of the impact of q on the learned policy, including an analysis of the weighting scheme and the nonlinear relationship to past action values, potentially using visualization techniques or specific metrics to quantify the differences in policy structure for different q values.

### Open Question 3
- Question: How can MVI(q) with q>1 be adapted to improve exploration in hard exploration games like PrivateEye and Seaquest?
- Basis in paper: [explicit] The paper notes that MVI(q=2) performs worse on hard exploration games compared to MVI(q=1), potentially due to the higher stochasticity in the softmax policy from MVI promoting more exploration.
- Why unresolved: The paper suggests that incorporating more directed exploration approaches into MVI(q=2) could be beneficial, but does not provide a concrete solution or empirical validation.
- What evidence would resolve it: Experiments comparing MVI(q=2) with different exploration strategies, such as intrinsic motivation or curiosity-driven exploration, on hard exploration games, and an analysis of how the Tsallis policy with q=2 interacts with these exploration methods.

## Limitations

- Convergence proof is limited to q=2, with open questions about stability for other q values
- Hyperparameter tuning procedure for different q values is not fully specified
- Performance gains may be sensitive to specific hyperparameter configurations
- MVI(q=2) shows reduced performance on hard exploration games compared to standard MVI

## Confidence

- **High confidence:** The mathematical formulation of Tsallis KL divergence and its properties (q-logarithm, pseudo-additivity) are well-established. The algorithm structure is clearly defined.
- **Medium confidence:** The convergence proof for q=2 is sound, but its generalizability to other q values is uncertain. The empirical improvements are significant but may be sensitive to hyperparameter choices.
- **Low confidence:** The mechanisms by which q>1 improves learning are not fully explained. The approximation quality of the implicit regularization structure for general q remains unclear.

## Next Checks

1. Implement a controlled ablation study comparing MVI(q) with different q values on a simple environment (e.g., CartPole) to isolate the effects of q-logarithm properties on policy averaging.

2. Conduct sensitivity analysis on the hyperparameters (τ, α) for different q values to determine if performance improvements persist across hyperparameter settings or are specific to fine-tuned configurations.

3. Test MVI(q) on environments requiring broad exploration (e.g., Montezuma's Revenge) to evaluate whether the sparsemax truncation for q=2 limits performance in exploration-heavy tasks.