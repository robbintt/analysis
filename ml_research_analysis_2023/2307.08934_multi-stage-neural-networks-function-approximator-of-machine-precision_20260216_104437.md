---
ver: rpa2
title: 'Multi-stage Neural Networks: Function Approximator of Machine Precision'
arxiv_id: '2307.08934'
source_url: https://arxiv.org/abs/2307.08934
tags:
- latexit
- training
- sha1
- base64
- equation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of achieving machine-precision
  accuracy in neural networks for scientific problems, particularly for physics-informed
  neural networks (PINNs) that solve differential equations. The core method introduces
  multi-stage neural networks, where training is divided into stages, with each stage
  using a new network optimized to fit the residue from the previous stage.
---

# Multi-stage Neural Networks: Function Approximator of Machine Precision

## Quick Facts
- arXiv ID: 2307.08934
- Source URL: https://arxiv.org/abs/2307.08934
- Reference count: 0
- Primary result: Prediction error can nearly reach machine precision (O(10^-16)) for both regression problems and PINNs through multi-stage training

## Executive Summary
This paper introduces multi-stage neural networks (MSNNs) to achieve machine-precision accuracy in scientific computing problems, particularly physics-informed neural networks (PINNs). The method overcomes the fundamental limitation of standard neural networks - their inability to capture high-frequency features due to spectral bias. By decomposing the approximation process into successive stages where each stage fits the residue from the previous stage, MSNNs can progressively capture increasingly high-frequency error components. The approach successfully solves 1D and 2D differential equations and combined forward-and-inverse problems, achieving prediction errors close to machine precision (O(10^-16)) within a finite number of iterations.

## Method Summary
The paper proposes a multi-stage training scheme where each stage uses a new neural network optimized to fit the residue from the previous stage. The method employs proper scaling of weights and activations to handle high-frequency residuals, with the first hidden layer using sine activation and modified scale factors. For PINNs, the approach balances data loss and equation loss through optimal γ settings. The process involves normalizing input/output data, training successive networks to fit normalized residues, and combining all networks for final predictions. The method is applied to both regression problems and physics-informed neural networks for solving differential equations.

## Key Results
- Prediction error can reach machine precision (O(10^-16)) for both regression problems and PINNs
- Multi-stage training effectively overcomes spectral bias, enabling capture of high-frequency features
- Successfully solves 1D and 2D differential equations and combined forward-and-inverse problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage training overcomes spectral bias by decomposing the approximation into successive refinements
- Mechanism: Each stage trains a new neural network to fit the normalized residue from the previous stage, progressively targeting higher-frequency errors
- Core assumption: Error between current approximation and true function becomes increasingly high-frequency and low-magnitude with each stage
- Evidence anchors:
  - [abstract]: "multi-stage neural networks that divides the training process into different stages, with each stage using a new network that is optimized to fit the residue from the previous stage."
  - [section 2.2.1]: "the error function e1(x) = ug(x) − u0(x) within the domain is indeed a high-frequency function."
  - [corpus]: Weak - neighbors discuss related concepts but no direct mechanistic match
- Break condition: If residue does not become higher frequency, further stages may not improve accuracy

### Mechanism 2
- Claim: Proper scaling of weights and activations enables accurate fitting of high-frequency residuals
- Mechanism: First hidden layer uses sine activation with modified scale factor κ to encode high-frequency components; magnitude normalization prevents training instability
- Core assumption: High-frequency functions require activation weights scaled to O(2πfd) to capture gradients effectively
- Evidence anchors:
  - [section 2.2.2]: "the weights w(0) within the activation function need to increase from their initialized value of O(√1/Var) to O(2πfd) during training."
  - [section 2.2.1]: "a neural network employing regular weight initialization methods... often struggles to capture training data whose magnitude is significantly larger or smaller than 1."
  - [corpus]: Weak - no direct mechanistic support found
- Break condition: If dominant frequency is too high relative to training points, overfitting or failure to converge occurs

### Mechanism 3
- Claim: Balancing data loss and equation loss via optimal γ enables PINNs to achieve machine precision
- Mechanism: For high-frequency solutions, equation loss dominates early training; setting γ ≈ Ld/(Le+Ld) ensures data loss minimized first
- Core assumption: Convergence rates of data loss and equation loss should be comparable for optimal training
- Evidence anchors:
  - [section 3.2.1]: "the optimal value of γ should yield a larger contribution from the data loss larger than from the equation loss."
  - [section 3.2.2]: Algorithm 3 estimates γ by matching convergence rates of data and equation loss
  - [corpus]: Weak - no direct mechanistic support found
- Break condition: If solution frequency is underestimated, γ may be set too high, causing boundary conditions to be ignored

## Foundational Learning

- Concept: Universal approximation theorem
  - Why needed here: MSNNs build on the fact that neural networks can approximate any continuous function, but need staged refinement to reach machine precision
  - Quick check question: If a single neural network can approximate any continuous function arbitrarily well, why does multi-stage training improve accuracy?

- Concept: Spectral bias in neural networks
  - Why needed here: Understanding that standard training struggles with high-frequency components explains why multi-stage refinement is necessary
  - Quick check question: What frequency characteristic of the training error motivates adding a new network stage?

- Concept: Root-mean-square normalization
  - Why needed here: Each stage normalizes its target residue to ensure stable training and prevent gradient explosion
  - Quick check question: Why is it important to normalize the error before training the next-stage network?

## Architecture Onboarding

- Component map:
  Input layer → first hidden layer (sine activation + scaled weights) → remaining hidden layers (tanh) → output layer
  For PINNs: additional collocation point sampling and equation residue computation modules
  gPINN variant: gradient of equation residue included in loss

- Critical path:
  1. Normalize input/output data
  2. Train first network to fit data or PDE
  3. Compute normalized residue
  4. Build next network with appropriate scale factor and activation
  5. Repeat until residue is machine precision
  6. Combine all networks for final prediction

- Design tradeoffs:
  - More stages → higher accuracy but increased computation and potential overfitting
  - Larger scale factor κ → better high-frequency capture but risk of overfitting if too large
  - Adam with SGD → better for high-frequency residuals vs L-BFGS

- Failure signatures:
  - Plateaued loss in early stages → spectral bias not addressed
  - Overshooting with large κ → overfitting to noise
  - γ too high in PINNs → boundary conditions ignored

- First 3 experiments:
  1. Fit sin(2x+1)+0.2e^(1.3x) with 3-stage MSNN; verify error ~10^-16
  2. Solve Poisson equation with high-frequency source using PINN; check error reduction per stage
  3. Apply gPINN to Burgers' equation; verify smoothness constraint improves λ inference accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal network size and configuration for each stage of the multi-stage training to minimize computational expense while maintaining high accuracy?
- Basis in paper: [inferred] The paper discusses the trade-off between network size and accuracy, suggesting that larger networks are not necessarily better and that appropriate training schemes are more crucial. It also mentions the need to determine the optimal strategy for selecting neural network size at different stages to minimize computational expense.
- Why unresolved: The paper does not provide a specific method or formula to determine the optimal network size for each stage. It only suggests that this is a future direction worth investigating.
- What evidence would resolve it: A systematic study comparing the performance of multi-stage neural networks with different network sizes and configurations for various problems, demonstrating the optimal network size for each stage that achieves the best balance between accuracy and computational efficiency.

### Open Question 2
- Question: How can the multi-stage training scheme be effectively extended to handle partial differential equations (PDEs) with high-dimensional input spaces?
- Basis in paper: [explicit] The paper discusses the application of the multi-stage training scheme to 2D PDEs and acknowledges that the convergence rate of multi-stage PINN method for solving 2D problems is slower than that for 1D problems. It also mentions that this challenge is expected to become more pronounced in higher-dimensional problems.
- Why unresolved: The paper does not provide a concrete solution or algorithm for handling high-dimensional PDEs. It only highlights the challenge and suggests that it needs to be addressed in future work.
- What evidence would resolve it: A novel algorithm or method that effectively extends the multi-stage training scheme to handle PDEs with high-dimensional input spaces, demonstrating improved convergence rates and accuracy compared to existing methods.

### Open Question 3
- Question: How can the multi-stage training scheme be adapted to handle functions or solutions with steep gradients or singularities?
- Basis in paper: [explicit] The paper mentions that functions with steep gradients are commonly encountered in differential equations, such as stiff equations, nonlinear equations, or singular perturbation equations. It also states that solving these types of equations via PINNs is beyond the scope of the paper and that more discussion of the challenge is given in the Discussion section.
- Why unresolved: The paper does not provide a specific solution or algorithm for handling functions or solutions with steep gradients or singularities. It only acknowledges the challenge and suggests that addressing it is beyond the scope of the paper.
- What evidence would resolve it: A novel approach or modification to the multi-stage training scheme that effectively handles functions or solutions with steep gradients or singularities, demonstrating improved accuracy and convergence rates for such problems.

## Limitations
- Precise hyperparameter settings (magnitude prefactor, scale factor, γ) are not fully specified and may vary across problems
- Method's performance on high-dimensional problems beyond 2D remains untested, raising scalability questions
- Assumption of progressively increasing error frequency may fail for functions with very narrow gradients or discontinuities

## Confidence

- High confidence in the core mechanism of spectral bias mitigation through staged refinement
- Medium confidence in the effectiveness of RMS normalization for training stability
- Medium confidence in the γ optimization algorithm for PINNs
- Low confidence in generalization to problems with discontinuous solutions or very high dimensionality

## Next Checks

1. Test multi-stage training on a function with discontinuous derivatives to verify if spectral bias assumptions hold
2. Implement the γ optimization algorithm on a problem where analytical solution is known, comparing estimated vs optimal values
3. Scale the method to 3D Poisson equation to assess computational scaling and accuracy retention