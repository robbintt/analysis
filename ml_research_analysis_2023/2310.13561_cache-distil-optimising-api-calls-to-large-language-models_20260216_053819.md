---
ver: rpa2
title: 'Cache & Distil: Optimising API Calls to Large Language Models'
arxiv_id: '2310.13561'
source_url: https://arxiv.org/abs/2310.13561
tags:
- student
- accuracy
- online
- learning
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies a scenario where a smaller student model is
  trained on predictions from a larger LLM, and a policy decides whether to use the
  student or the LLM for each incoming query. The goal is to maximize accuracy while
  minimizing LLM API calls.
---

# Cache & Distil: Optimising API Calls to Large Language Models

## Quick Facts
- arXiv ID: 2310.13561
- Source URL: https://arxiv.org/abs/2310.13561
- Reference count: 18
- Primary result: Active learning-based instance selection policies can significantly reduce LLM API calls while maintaining accuracy when training a smaller student model on LLM predictions.

## Executive Summary
This paper addresses the challenge of optimizing API calls to large language models (LLMs) in settings where a smaller student model can handle many queries independently. The authors propose an online distillation framework where a student model is periodically retrained on LLM-generated labels, with an instance selection policy deciding which queries to send to the LLM versus handling with the student. Through experiments on four classification datasets, they demonstrate that active learning criteria like Margin Sampling and Query by Committee consistently outperform simpler baselines across various budget constraints.

## Method Summary
The method involves a streaming scenario where incoming queries are first processed by a student model (T5-base with LoRA adapters). An instance selection policy evaluates each query and decides whether to use the student's prediction or call the LLM. The LLM's predictions are accumulated and used to periodically retrain the student model from scratch. Four active learning criteria are evaluated: Margin Sampling (selecting instances with high prediction uncertainty), Query by Committee (using ensemble disagreement), Coreset (selecting diverse examples), and Entropy (measuring prediction entropy). The student model is retrained every 1000 examples using the accumulated LLM labels.

## Key Results
- Margin Sampling and Query by Committee consistently outperform random selection and front-loading baselines across all datasets and budgets
- Student models demonstrate robustness to noise from incorrect LLM labels, maintaining performance even when LLM makes errors
- Periodic retraining enables the student to gradually handle more queries independently, reducing long-term API dependency
- Embedding-based strategies like Coreset perform poorly in this setting, suggesting they're better suited for highly repetitive query scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active learning-based instance selection improves the trade-off between online accuracy and API call budget.
- Mechanism: By prioritizing instances with high uncertainty (e.g., small margin between top two predictions), the system ensures that LLM calls are made on examples where the student model is most likely to be incorrect, thereby maximizing the value of each API call.
- Core assumption: The student model's uncertainty correlates with its likelihood of making a wrong prediction.
- Evidence anchors: Weak; no direct evidence found about correlation between uncertainty and error rate.

### Mechanism 2
- Claim: Periodic retraining of the student model on LLM-annotated data allows it to gradually handle more queries independently.
- Mechanism: The student model is periodically retrained on accumulated LLM predictions, improving its accuracy over time and reducing the need for future API calls.
- Core assumption: Retraining the student model from scratch on the accumulated data each time is effective for learning.
- Evidence anchors: Weak; no direct evidence found about the effectiveness of periodic retraining from scratch.

### Mechanism 3
- Claim: The student model is robust to noise from incorrect LLM labels, mitigating the impact of confirmation bias.
- Mechanism: Despite the LLM sometimes making incorrect predictions, especially on harder examples, the student model can still learn effectively without being overly influenced by these errors.
- Core assumption: The student model can distinguish between correct and incorrect labels to some extent during training.
- Evidence anchors: Weak; no direct evidence found about the student model's robustness to noisy labels.

## Foundational Learning

- **Active Learning (AL)**: Techniques for selecting the most informative instances to label in machine learning. Why needed: Provides the theoretical foundation for deciding which queries merit LLM API calls versus student handling. Quick check: How does Margin Sampling determine which instances to select for labeling?

- **Knowledge Distillation (KD)**: Training a smaller model to mimic a larger model's predictions. Why needed: The student model learns from LLM predictions, a form of distillation that reduces API dependency over time. Quick check: What is the primary goal of knowledge distillation in this context?

- **Online Learning**: Learning from data streams where decisions must be made in real-time without reprocessing historical data. Why needed: The system must make instant decisions about each query while maintaining performance over time. Quick check: What are the key differences between online learning and traditional batch learning?

## Architecture Onboarding

- **Component map**: Incoming query -> Student model prediction -> Policy algorithm decision -> LLM call (if needed) -> Data store -> Periodic retraining
- **Critical path**: 1) Incoming query is processed by the student model. 2) Policy algorithm evaluates the query and decides whether to use the student's prediction or call the LLM. 3) If LLM is called, the prediction is stored and used in the next student retraining cycle.
- **Design tradeoffs**: Retraining frequency vs. model freshness (more frequent retraining keeps student updated but increases computational overhead); Budget allocation vs. online accuracy (spending more budget early can improve student faster but may reduce accuracy in short term)
- **Failure signatures**: Student model performance plateaus or degrades despite retraining; LLM API costs exceed budget due to high uncertainty in student model; Policy algorithm fails to adapt to changing data distributions
- **First 3 experiments**: 1) Test impact of different retraining frequencies (e.g., f=100 vs. f=1000) on online accuracy. 2) Compare performance of different instance selection criteria (Margin Sampling vs. Query by Committee) under varying budgets. 3) Evaluate effect of using soft vs. hard labels from LLM on student model's final accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would neural caching perform with a non-stationary stream of requests where the data distribution changes over time?
- Basis in paper: Inferred - The authors mention that in practice the distribution of requests is likely to change over time, but they leave this investigation for future work.
- Why unresolved: The current study focuses on a stationary (i.i.d.) stream of requests. The authors acknowledge that in practice, the data distribution is likely to change over time, which could potentially affect the performance of neural caching strategies.
- What evidence would resolve it: Experiments comparing the performance of neural caching strategies on both stationary and non-stationary data streams would provide evidence. Specifically, showing improvements in both online and final accuracy on non-stationary streams compared to stationary ones would support the hypothesis that online accuracy gains are more significant in changing environments.

### Open Question 2
- Question: What is the optimal decision criterion for transitioning from front-loading to Margin Sampling when initial budgets vary?
- Basis in paper: Inferred - The authors note that Margin Sampling's performance is more sensitive to the initial budget spent to train the student model, suggesting that determining when to switch strategies is a relevant question.
- Why unresolved: While the paper shows that Margin Sampling outperforms front-loading in some cases, it also shows that its performance is sensitive to the initial budget. This suggests that there might be an optimal point to switch from front-loading to Margin Sampling, but this is not explored in the current study.
- What evidence would resolve it: Experiments that systematically vary the initial budget and measure the performance of different strategies at each point would help determine the optimal transition criterion. Specifically, identifying a budget threshold where switching strategies yields consistent improvements would resolve this question.

### Open Question 3
- Question: How does the performance of embedding-based strategies like Coreset change in scenarios with highly repetitive queries?
- Basis in paper: Explicit - The authors state that embedding-based strategies like Coreset performed poorly in their experiments but suggest these strategies might be useful in certain contexts, such as multiple near-identical calls to an LLM.
- Why unresolved: While the authors found that Coreset performed poorly in their experiments, they hypothesize that it might be useful in scenarios with highly repetitive queries. This hypothesis is not tested in the current study.
- What evidence would resolve it: Experiments that specifically test the performance of embedding-based strategies on datasets with high query repetition would provide evidence. If these strategies show significant improvements in such scenarios, it would support the authors' hypothesis.

## Limitations

- The paper lacks theoretical guarantees about the relationship between uncertainty-based selection and optimal budget utilization
- The framework assumes a single, consistent LLM source and doesn't address multiple teacher models with varying capabilities
- Periodic retraining from scratch may become computationally prohibitive as datasets grow, with no exploration of incremental learning approaches

## Confidence

- **High Confidence**: Empirical evaluation methodology is sound with appropriate baselines and multiple datasets
- **Medium Confidence**: Active learning policies outperform simpler heuristics, but magnitude of improvement varies significantly
- **Low Confidence**: Claims about student model robustness to noisy labels are based on aggregate results rather than detailed error analysis

## Next Checks

1. **Ablation on Student Architecture**: Test whether the observed robustness to noisy labels depends on the T5-base architecture with LoRA adapters, or if it generalizes to other student model configurations

2. **Dynamic Budget Adaptation**: Implement and evaluate policies that adapt the budget allocation threshold based on observed uncertainty distributions over time

3. **Teacher Model Uncertainty**: Extend the framework to handle cases where the LLM returns uncertainty signals (e.g., confidence scores, multiple plausible answers) and evaluate how this additional information impacts instance selection quality