---
ver: rpa2
title: Language Agent Tree Search Unifies Reasoning Acting and Planning in Language
  Models
arxiv_id: '2310.04406'
source_url: https://arxiv.org/abs/2310.04406
tags:
- search
- lats
- reasoning
- language
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LATS is the first framework to unify planning, acting, and reasoning
  for enhanced LLM problem solving. By deliberately constructing trajectories with
  search algorithms, incorporating external feedback, and enabling agents to learn
  from experience, LATS addresses key limitations of prior prompting techniques.
---

# Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models

## Quick Facts
- arXiv ID: 2310.04406
- Source URL: https://arxiv.org/abs/2310.04406
- Reference count: 24
- LATS achieves state-of-the-art pass@1 accuracy of 92.7% for programming on HumanEval with GPT-4

## Executive Summary
Language Agent Tree Search (LATS) introduces a unified framework that combines planning, acting, and reasoning for enhanced LLM problem solving. By using Monte Carlo Tree Search (MCTS) to deliberately construct trajectories with multiple action sampling, incorporating external feedback through environment observations and self-reflection, LATS addresses key limitations of prior prompting techniques. The framework demonstrates strong performance on programming tasks (HumanEval) and web navigation (WebShop) while providing a general approach applicable across diverse reasoning and decision-making domains.

## Method Summary
LATS implements a Monte Carlo Tree Search framework where a base language model generates actions and evaluates states through an LM-based value function. At each step, the agent samples multiple actions, expands the search tree with environment observations, and uses backpropagation to update node values. When trajectories fail, the agent generates self-reflections summarizing errors and proposing improvements, which are added as context for future iterations. The framework unifies reasoning and acting by defining a shared space of thoughts and actions, allowing the same search algorithm to work across different task types.

## Key Results
- Achieves state-of-the-art pass@1 accuracy of 92.7% for programming on HumanEval with GPT-4
- Demonstrates competitive performance for web navigation on WebShop with GPT-3.5
- Successfully unifies planning, acting, and reasoning in a single framework applicable to diverse domains

## Why This Works (Mechanism)

### Mechanism 1: Deliberate Planning Through MCTS Sampling
LATS uses MCTS to sample multiple trajectories rather than greedy decoding, enabling better exploration of the solution space. By sampling n actions at each step and expanding each into new nodes with environment observations, the framework prioritizes promising paths while exploring alternatives. This approach mitigates the stochastic nature of LM text generation and enables greater exploration of complex reasoning tasks.

### Mechanism 2: Adaptability Through External Feedback and Self-Reflection
The framework incorporates external feedback through environment observations after each action. When trajectories fail, the agent generates self-reflections summarizing errors and proposing improvements. These reflections are added as context for future iterations, allowing the agent to learn from trial and error rather than relying solely on scalar rewards.

### Mechanism 3: Generality Through Unified Reasoning and Acting Framework
LATS defines a shared space of thoughts and actions where both reasoning traces and API calls can coexist. This unified approach allows the same search algorithm to work for both reasoning tasks (like HotPotQA) and decision-making tasks (like WebShop) by simply modifying the state design and action space.

## Foundational Learning

- **Monte Carlo Tree Search (MCTS)**: Provides principled exploration-exploitation balance for finding good trajectories in complex tasks. Quick check: What are the four main operations of MCTS, and how do they work together to find good solutions?

- **Upper Confidence bounds applied to Trees (UCT)**: Used for node selection during search, balancing exploration of less-visited nodes with exploitation of high-value nodes. Quick check: What is the formula for UCT, and how do the exploration weight and visit counts affect node selection?

- **In-context learning**: LATS relies on the LM's ability to learn from examples provided in the prompt without additional training. Quick check: How does in-context learning differ from fine-tuning, and what are its limitations for complex reasoning tasks?

## Architecture Onboarding

- **Component map**: Agent -> Environment -> Search Algorithm (MCTS) -> Value Function -> Reflection Generator -> Memory
- **Critical path**: Input → Agent generates initial action → Environment provides observation → Search expands tree → Value function evaluates nodes → Backpropagation updates values → Reflection generated if failed → Memory updated → Next iteration
- **Design tradeoffs**: Sampling n actions vs. greedy decoding (exploration vs. computational cost); LM-based value function vs. programmed heuristic (flexibility vs. reliability); self-reflection vs. scalar rewards (semantic signals vs. simplicity); MCTS vs. simpler search (principled vs. fast)
- **Failure signatures**: Agent gets stuck in local minima with repeated similar failures; search tree grows too large without solution improvement; reflections are generic and unhelpful; value function values don't correlate with task success
- **First 3 experiments**: 1) Run LATS on simple HotPotQA question with n=1 vs. ReAct; 2) Enable reflections and measure impact on programming task success rate; 3) Vary n and measure trade-off between performance and computation time

## Open Questions the Paper Calls Out

- How does LATS performance scale with increasing depth of reasoning tasks?
- What is the computational overhead of LATS compared to baseline prompting methods?
- How does LATS perform on more complex multi-step reasoning tasks beyond programming and QA?
- Can LATS benefit from additional training or fine-tuning on specific task domains?
- How sensitive is LATS to the choice of base language model and its capabilities?

## Limitations

- Experimental validation primarily focused on two domains (HumanEval and WebShop) with limited ablation studies
- Computational overhead of MCTS with multiple action sampling is not fully characterized
- Framework's scalability to more complex reasoning tasks and open-ended domains remains unclear

## Confidence

**High confidence**: The core MCTS algorithm implementation and its integration with LLMs is well-established in prior literature. The general claim that tree search can improve LLM reasoning performance is supported by the experimental results.

**Medium confidence**: The specific design choices (sampling n actions, LM-based value function, self-reflection mechanism) are novel combinations, and while results are strong, the individual contribution of each component is not fully isolated.

**Low confidence**: The claim of generality across all reasoning and acting tasks is not fully validated. The paper demonstrates success on two specific domains but doesn't show how well the framework transfers to fundamentally different task types.

## Next Checks

1. Conduct ablation study systematically disabling each key component (sampling, value function, self-reflection) to measure their individual contribution to performance on HumanEval and WebShop tasks.

2. Measure computational cost (tree size, inference calls) as a function of task complexity and action space size to characterize practical limitations of the framework.

3. Apply LATS to a third, qualitatively different task domain (e.g., multi-hop reasoning on a new dataset or a creative writing task) to validate the claimed generality of the framework.