---
ver: rpa2
title: 'Observable Propagation: Uncovering Feature Vectors in Transformers'
arxiv_id: '2312.16291'
source_url: https://arxiv.org/abs/2312.16291
tags:
- token
- feature
- prompt
- score
- excerpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Observable Propagation (OBPROP) introduces a data-efficient method
  for finding feature vectors in transformer models by treating tasks as linear functionals
  (observables) on logits. The approach propagates these observables through the model
  using first-order approximations, bypassing the need for large labeled datasets.
---

# Observable Propagation: Uncovering Feature Vectors in Transformers

## Quick Facts
- arXiv ID: 2312.16291
- Source URL: https://arxiv.org/abs/2312.16291
- Reference count: 40
- Primary result: OBPROP achieves r² ≈ 0.945 for pronoun prediction vs. r² ≈ 0.945 for regression with 60 prompts in low-data regimes

## Executive Summary
Observable Propagation (OBPROP) introduces a data-efficient method for finding feature vectors in transformer models by treating tasks as linear functionals (observables) on logits. The approach propagates these observables through the model using first-order approximations, bypassing the need for large labeled datasets. OBPROP achieves superior performance to traditional probing methods in low-data regimes and reveals that models use identical features for gender pronoun and occupation prediction, explaining inherent bias challenges.

## Method Summary
OBPROP maps tasks to feature vectors by treating them as linear functionals (observables) on model logits, then propagates these observables backward through the transformer using first-order Taylor approximations of nonlinearities. The method computes feature vectors for each attention head and MLP layer, capturing how the model represents task-relevant information. By avoiding data-hungry regression methods, OBPROP identifies feature vectors that directly predict task outcomes through simple dot products, while also providing theoretical tools like coupling coefficients to measure task similarity.

## Key Results
- OBPROP achieves r² ≈ 0.945 for pronoun prediction versus r² ≈ 0.945 for regression with 60 prompts in low-data regimes
- Feature vectors computed with/without LayerNorm show cosine similarities near 0.99, validating approximation
- High cosine similarity (0.966) between gender pronoun and occupation prediction feature vectors reveals models use identical features for both tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OBPROP maps tasks to feature vectors without labeled data by treating tasks as linear functionals (observables) on logits.
- Mechanism: Each task is expressed as a logit difference, which is a linear functional on the logits. OBPROP propagates these observables backward through the model using first-order Taylor approximations of nonlinearities, yielding feature vectors that predict task-relevant outputs.
- Core assumption: The model's computation on a task is well-approximated by linear operations plus first-order nonlinearities.
- Evidence anchors:
  - [abstract] "Our paradigm centers on the concept of 'observables', linear functionals corresponding to given tasks."
  - [section 2.2] "if the logits are represented by the vector y, then each logit difference can be represented by nT y for some vector n."
  - [corpus] Weak evidence: no related papers cited by other work, average neighbor FMR=0.37 suggests moderate relatedness but limited direct citations.
- Break condition: If the model's task computation is dominated by highly nonlinear or non-linear-functional components, the linear approximation breaks down.

### Mechanism 2
- Claim: Feature vector directions are preserved across LayerNorm operations.
- Mechanism: LayerNorm gradients are inversely proportional to input norm but primarily scale rather than rotate feature vectors; empirical cosine similarities remain near 0.99 between vectors computed with and without LayerNorm.
- Core assumption: LayerNorm matrices are nearly diagonal with similar entries, acting like scalar multipliers on gradients.
- Evidence anchors:
  - [section 2.4] "Theorem 1...provides theoretical underpinning for this behavior."
  - [section E.2] "the average variance of scaling matrix entries across all LayerNorms...is 0.007827."
  - [corpus] No direct citations for LayerNorm gradient behavior; relies on internal analysis.
- Break condition: If LayerNorm scaling matrices have large variance or non-diagonal structure, direction preservation fails.

### Mechanism 3
- Claim: The coupling coefficient predicts how correlated outputs for two tasks are based on feature vectors.
- Mechanism: The coupling coefficient C(y1,y2) = y1·y2/∥y1∥² estimates the expected dot product of a vector with y2 given its dot product with y1, assuming uniform distribution on constrained hyperspheres.
- Core assumption: Input vectors are uniformly distributed on the hypersphere defined by fixed norms and fixed dot products with feature vectors.
- Evidence anchors:
  - [section 2.4] "we denote the value y1·y2/∥y1∥² by C(y1, y2), and call it the 'coupling coefficient from y1 to y2'."
  - [section 4.1] "The coupling coefficients are accurate estimators of the empirical dot products between feature vectors."
  - [corpus] No corpus evidence; theoretical derivation only.
- Break condition: If input distribution deviates strongly from uniformity on the constrained hypersphere, predictions become inaccurate.

## Foundational Learning

- Concept: Linear functionals and observables
  - Why needed here: OBPROP requires expressing tasks as linear maps from logits to scalars; observables formalize this mapping.
  - Quick check question: Can you represent the task "predict 'she' vs 'he'" as a vector n such that n·logits gives the logit difference?

- Concept: First-order Taylor approximation of nonlinearities
  - Why needed here: OBPROP linearizes MLP and LayerNorm layers to propagate observables backward; gradients serve as feature vectors.
  - Quick check question: What is the first-order Taylor expansion of f(x) around x0, and why is the gradient used as the feature vector?

- Concept: Hypersphere geometry and uniform distributions
  - Why needed here: Coupling coefficient theory relies on uniform distribution of inputs on hyperspheres defined by norm and dot product constraints.
  - Quick check question: Given ∥x∥=s and x·y1=k, what is the expected value of x·y2 in terms of y1·y2 and ∥y1∥²?

## Architecture Onboarding

- Component map: Observable definition → backward propagation through attention heads and MLPs → feature vector extraction
- Critical path: Observable definition → backward propagation through attention heads and MLPs → feature vector extraction; all intermediate computations must be cached to avoid recomputation.
- Design tradeoffs: (a) Accuracy vs speed: full LayerNorm gradients are accurate but expensive; scalar approximation is faster but slightly less precise. (b) Data efficiency vs completeness: OBPROP needs almost no data but only captures OV circuit behavior, missing QK circuit effects.
- Failure signatures: (a) Low cosine similarity between feature vectors computed with/without LayerNorm → LayerNorm approximation breaking. (b) High variance in LayerNorm scaling matrix entries → scalar approximation invalid. (c) Poor correlation between feature vector dot products and actual outputs → first-order linearization insufficient.
- First 3 experiments:
  1. Verify LayerNorm approximation: compute feature vectors for a simple observable with and without LayerNorm; check cosine similarity > 0.99.
  2. Test coupling coefficient: generate synthetic data satisfying constraints, compute empirical vs predicted correlations for y2 given y1.
  3. Validate on small task: apply OBPROP to a toy language model on a simple pronoun prediction task; compare predicted vs actual logit differences.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective is observable propagation at identifying bias in larger language models (e.g., GPT-4, LLaMA) beyond the 1.3B parameter GPT-Neo model studied?
- Basis in paper: [inferred] The paper demonstrates OBPROP's effectiveness on GPT-Neo-1.3B for gendered pronoun and occupation bias, but does not test on larger models where computational constraints or different architectural features might affect performance.
- Why unresolved: The paper only evaluates OBPROP on one relatively small model, limiting generalizability.
- What evidence would resolve it: Applying OBPROP to larger models and comparing feature vector predictions to ground-truth bias measures or alternative interpretability methods.

### Open Question 2
- Question: Can observable propagation distinguish between direct gender bias (e.g., "she" vs "he" pronouns) and indirect gender associations (e.g., occupational stereotypes) when these features overlap in activation space?
- Basis in paper: [explicit] The paper finds high cosine similarity (0.966) between gender pronoun and occupation prediction feature vectors, suggesting the model uses identical features for both tasks.
- Why unresolved: While the paper identifies feature overlap, it doesn't determine whether OBPROP can disentangle these overlapping features or measure their relative contributions.
- What evidence would resolve it: Experiments where OBPROP is used to selectively manipulate one feature while measuring effects on both tasks, or developing methods to decompose overlapping feature vectors.

### Open Question 3
- Question: What is the relationship between observable coupling coefficients and actual feature vector compositionality in transformers?
- Basis in paper: [explicit] Theorem 2 introduces coupling coefficients as a measure of feature vector similarity, but the paper only validates this empirically for simple cases.
- Why unresolved: The paper demonstrates coupling coefficients predict dot product relationships but doesn't explore whether high coupling implies actual compositional feature structures.
- What evidence would resolve it: Systematic experiments varying feature vector compositionality while measuring coupling coefficient accuracy, or theoretical work connecting coupling coefficients to circuit structures.

## Limitations
- OBPROP only captures OV circuit behavior, missing important QK circuit effects that may contain complementary task-relevant information
- The method relies on strong assumptions about first-order linearization that may break down in deeper or more nonlinear architectures
- Coupling coefficient theory assumes uniform distribution on constrained hyperspheres, which may not hold for real-world model activations

## Confidence
**High Confidence**: The empirical demonstration that OBPROP works well in low-data regimes (r² ≈ 0.945 for pronoun prediction vs. r² ≈ 0.945 for regression with 60 prompts) and the observation that models use identical features for gender pronoun and occupation prediction are well-supported by the experiments presented.

**Medium Confidence**: The theoretical justification for ignoring LayerNorm effects on feature vector directions (Theorem 1 and empirical cosine similarities near 0.99) is supported but relies on the assumption that LayerNorm scaling matrices are nearly diagonal with small variance (0.007827). This may not generalize to all architectures.

**Low Confidence**: The coupling coefficient theory's assumption of uniform distribution on constrained hyperspheres is elegant but untested against empirical distributions of model activations. The claim that this coefficient accurately predicts task correlations needs more validation across diverse tasks.

## Next Checks
1. **LayerNorm Generalization Test**: Apply OBPROP to transformers with non-standard LayerNorm implementations (e.g., RMSNorm, or LayerNorms with learned scale parameters) and measure feature vector cosine similarity degradation to quantify the approximation's limits.

2. **Distribution Validation**: Generate synthetic data matching the constrained hypersphere conditions (fixed norms and dot products) and empirically measure the distribution of input vectors to verify the uniform distribution assumption underlying coupling coefficient theory.

3. **QK Circuit Integration**: Extend OBPROP to incorporate QK circuit effects by modifying the propagation rules to handle attention weight matrices directly, then compare the resulting feature vectors and task predictions against the OV-only version to quantify what's missed.