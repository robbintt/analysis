---
ver: rpa2
title: Mixed Distillation Helps Smaller Language Model Better Reasoning
arxiv_id: '2312.10730'
source_url: https://arxiv.org/abs/2312.10730
tags:
- reasoning
- distillation
- arxiv
- mixed
- smaller
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixed Distillation (MD), a framework that
  distills both Chain of Thought (CoT) and Program of Thought (PoT) reasoning capabilities
  from large language models into smaller models. By combining multiple prompting
  techniques, MD enhances the reasoning ability of smaller models.
---

# Mixed Distillation Helps Smaller Language Model Better Reasoning

## Quick Facts
- arXiv ID: 2312.10730
- Source URL: https://arxiv.org/abs/2312.10730
- Authors: 
- Reference count: 16
- Key outcome: Mixed Distillation framework achieves 84.5% and 85.5% accuracy on SVAMP using Llama2-7B and CodeLlama-7B, outperforming GPT-3.5-Turbo by 2.5% and 3.5%

## Executive Summary
This paper introduces Mixed Distillation (MD), a framework that distills both Chain of Thought (CoT) and Program of Thought (PoT) reasoning capabilities from large language models into smaller models. By combining multiple prompting techniques, MD enhances the reasoning ability of smaller models. On the SVAMP benchmark, LLaMA2-7B and CodeLlama-7B using MD achieved accuracy of 84.5% and 85.5% respectively, outperforming GPT-3.5-Turbo by 2.5% and 3.5%. The method demonstrates significant improvements in both single-path and multi-path reasoning tasks, effectively bridging the performance gap between smaller models and LLMs.

## Method Summary
The Mixed Distillation framework extracts both Chain-of-Thought (CoT) and Program-of-Thought (PoT) reasoning paths from a teacher LLM (GPT-3.5-Turbo) using carefully crafted prompts. These reasoning paths are then used as multi-task supervision signals during fine-tuning of smaller student models (Llama2-7B, CodeLlama-7B, T5-Large). The approach combines both reasoning strategies through a weighted loss function and employs multi-path sampling during training to improve single-path reasoning capabilities at inference time.

## Key Results
- Llama2-7B and CodeLlama-7B with mixed distillation achieved 84.5% and 85.5% accuracy on SVAMP, outperforming GPT-3.5-Turbo by 2.5% and 3.5%
- PoT distillation showed 7.5-6.36% improvement over CoT distillation across three datasets
- Mixed distillation improved single-path reasoning performance, with CoT capability surpassing single-method training when sampling >13 paths
- Out-of-distribution evaluation showed consistent improvements when training on SVAMP and testing on GSM8K and ASDIV

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixed Distillation leverages complementary reasoning strengths of Chain-of-Thought (CoT) and Program-of-Thought (PoT) to enhance smaller model performance.
- Mechanism: The framework extracts both reasoning paths from LLMs and uses them as multi-task supervision signals during training, allowing the smaller model to learn distinct reasoning strategies.
- Core assumption: CoT and PoT solve different subsets of problems, and combining their strengths yields better overall performance than either alone.
- Evidence anchors:
  - [abstract] "Mixed Distillation (MD) framework, which capitalizes on the strengths of Program of Thought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining multiple prompting techniques and distilling these capabilities into smaller models."
  - [section 2] "We observe that LLMs' PoT and CoT reasoning capabilities uniquely solve specific problem subsets... Integrating both reasoning strategies is likely to yield the best outcomes."

### Mechanism 2
- Claim: Multi-path sampling during training improves single-path reasoning capabilities.
- Mechanism: By training on multiple reasoning paths generated through self-consistency sampling, the model learns to generate better reasoning traces even when asked for a single path at inference time.
- Core assumption: Exposure to diverse reasoning patterns during training enhances the model's ability to generate accurate reasoning paths.
- Evidence anchors:
  - [section 4.2] "With an increasing number of sampled reasoning paths, the PoT capability of the mixed distillation model consistently outperforms that of models trained using a single distillation method."
  - [section 4.2] "The CoT capability with mixed distillation surpasses the performance of smaller models trained using a single distillation method as the number of sampled paths > 13."

### Mechanism 3
- Claim: PoT distillation provides more effective supervision than CoT distillation for certain tasks.
- Mechanism: PoT generates executable code that can be precisely evaluated, providing cleaner feedback signals than natural language CoT explanations.
- Core assumption: The executable nature of PoT provides more reliable training signals for numerical reasoning tasks.
- Evidence anchors:
  - [section 4.1] "Employing the PoT distillation method yielded remarkable enhancements of 7.5%, 6.36%, and 3.45%, respectively, when compared to CoT distillation across the three datasets."
  - [section 4.4.1] "On Svamp dataset, the T5-large model... the experiments not only underscore the superiority of PoT over CoT distillation, but demonstrate the enhanced capabilities of single paths achieved with mixed distillation."

## Foundational Learning

- Concept: Chain-of-Thought prompting
  - Why needed here: Understanding CoT is essential for grasping how the framework extracts reasoning traces from LLMs and why CoT serves as a supervisory signal.
  - Quick check question: What distinguishes Chain-of-Thought from direct answer generation in LLMs?

- Concept: Program-of-Thought prompting
  - Why needed here: PoT is a key component of the mixed distillation framework, and understanding its executable nature is crucial for comprehending why it can provide effective supervision.
  - Quick check question: How does Program-of-Thought differ from Chain-of-Thought in terms of output format and evaluation?

- Concept: Knowledge distillation
  - Why needed here: The entire framework is built on transferring knowledge from larger to smaller models, so understanding distillation principles is fundamental.
  - Quick check question: What are the key differences between standard knowledge distillation and the mixed distillation approach proposed in this paper?

## Architecture Onboarding

- Component map:
  LLM Teacher (GPT-3.5-Turbo) -> Prompt Templates -> Multi-path Extraction Module -> Student Model (Llama2/CodeLlama/T5) -> Training Loop -> Evaluation Module

- Critical path: Unlabeled dataset -> Prompt LLM -> Extract CoT/PoT paths -> Train student model with multi-task loss -> Evaluate on downstream tasks

- Design tradeoffs:
  - Computational cost vs. performance: Using multiple sampling paths increases training time but improves reasoning capabilities
  - Model size vs. effectiveness: Smaller models may not benefit as much from distillation, requiring careful selection of student model architecture
  - Prompt engineering vs. generalization: Carefully crafted prompts improve extraction quality but may reduce model generalization

- Failure signatures:
  - Student model performance plateaus despite increased training steps
  - Generated reasoning paths become repetitive or incoherent
  - Model overfits to specific reasoning patterns in the training data
  - Evaluation performance degrades when tested on out-of-distribution problems

- First 3 experiments:
  1. Compare single-path (CoT-only) vs. single-path (PoT-only) distillation on SVAMP to validate individual reasoning path effectiveness
  2. Implement mixed distillation with 5 sampling paths and evaluate performance gains over single-path methods
  3. Test generalization by training on SVAMP and evaluating on GSM8K and ASDIV to assess transfer capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does mixed distillation provide consistent improvements across different model architectures and sizes beyond the Llama2-7B and CodeLlama-7B models tested?
- Basis in paper: [explicit] The authors mention their experiments cover "a wide range of student models" including Llama2-7B, CodeLlama-7B, and T5-Large, and they suggest that "harnessing smaller models pre-trained on code holds the potential to significantly enhance its efficacy"
- Why unresolved: The paper only provides detailed results for a limited set of model architectures and sizes. While they mention testing on "a wide range," specific results for other architectures and sizes are not shown.
- What evidence would resolve it: Comprehensive results showing performance improvements across a diverse set of model architectures (e.g., different transformer variants) and sizes (e.g., from 1B to 13B parameters), comparing mixed distillation to other methods.

### Open Question 2
- Question: What is the optimal balance between CoT and PoT reasoning paths in mixed distillation for different types of reasoning tasks?
- Basis in paper: [explicit] The authors mention using a weight parameter λ to balance CoT and PoT losses, and they discuss how different problems may be better suited to different reasoning approaches, but do not provide specific guidance on optimal weighting for different task types.
- Why unresolved: The paper uses a fixed weighting approach and does not explore how the optimal balance might vary depending on task characteristics or dataset properties.
- What evidence would resolve it: Experiments showing performance as a function of λ for different task categories (e.g., arithmetic, commonsense reasoning, logical deduction), and ideally a method to automatically determine optimal weighting based on task characteristics.

### Open Question 3
- Question: How does mixed distillation perform when the training data is not drawn from the same distribution as the test data?
- Basis in paper: [explicit] The authors conduct out-of-distribution evaluation, showing that mixed distillation improves performance compared to single-path methods when trained on SVAMP and tested on GSM8K and ASDIV, but the analysis is limited to this specific scenario.
- Why unresolved: The paper only examines one type of distribution shift (training on one dataset, testing on another). It does not explore how mixed distillation performs under different types of distribution shifts or how robust it is to various forms of data mismatch.
- What evidence would resolve it: Comprehensive experiments testing mixed distillation under various distribution shifts, including changes in problem difficulty, vocabulary, context length, and reasoning complexity, comparing performance to baseline methods across these different scenarios.

## Limitations
- Reliance on GPT-3.5-Turbo as teacher model raises cost and accessibility concerns for widespread adoption
- Evaluation focuses primarily on mathematical reasoning tasks, leaving questions about effectiveness on other reasoning domains
- Does not thoroughly explore scaling properties across different model sizes or discuss potential diminishing returns

## Confidence
- High confidence: The core experimental results showing accuracy improvements over baseline methods
- Medium confidence: The claims about complementary strengths of CoT and PoT reasoning paths, as the analysis is primarily empirical without deep theoretical justification
- Medium confidence: The assertion that mixed distillation provides consistent benefits across different student model architectures, as the evaluation covers a limited range of models

## Next Checks
1. **Cross-domain generalization test**: Evaluate the distilled models on non-mathematical reasoning tasks (e.g., commonsense reasoning datasets) to assess whether the mixed distillation approach generalizes beyond numerical problem-solving.

2. **Teacher-student gap analysis**: Systematically vary the capabilities of the teacher model (using different GPT versions or other LLMs) to determine the minimum teacher quality required for effective distillation and identify potential saturation points.

3. **Long-term reasoning capability assessment**: Conduct multi-step reasoning tasks requiring sustained logical chains to evaluate whether mixed distillation improves not just immediate reasoning accuracy but also the model's ability to maintain coherent reasoning over extended problem-solving sequences.