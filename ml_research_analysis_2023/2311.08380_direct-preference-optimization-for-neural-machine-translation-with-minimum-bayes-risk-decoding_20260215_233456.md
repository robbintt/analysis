---
ver: rpa2
title: Direct Preference Optimization for Neural Machine Translation with Minimum
  Bayes Risk Decoding
arxiv_id: '2311.08380'
source_url: https://arxiv.org/abs/2311.08380
tags:
- translation
- fine-tuning
- decoding
- preference
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose using direct preference optimization (DPO)
  to fine-tune multilingual language models so they achieve the same translation performance
  as MBR decoding without the computational expense. DPO is used to fine-tune models
  on preference pairs extracted from MBR-decoded translation sets, teaching the models
  to rank translations like MBR.
---

# Direct Preference Optimization for Neural Machine Translation with Minimum Bayes Risk Decoding

## Quick Facts
- arXiv ID: 2311.08380
- Source URL: https://arxiv.org/abs/2311.08380
- Reference count: 14
- Primary result: DPO fine-tuning achieves MBR-like translation performance without MBR's computational expense.

## Executive Summary
This paper introduces Direct Preference Optimization for Minimum Bayes Risk (DPO-MBR) decoding, a method to fine-tune multilingual language models to achieve MBR-like translation quality without the computational expense of MBR decoding. The approach uses DPO to train models on preference pairs extracted from MBR-decoded translation sets, enabling the model to internalize MBR's ranking function. Experiments on WMT21/22 Chinese-English and German-English test sets demonstrate that DPO-MBR achieves performance within 1 BLEURT point of MBR decoding while outperforming baseline beam search by at least 3 BLEURT points.

## Method Summary
The method combines MBR decoding with DPO fine-tuning to create models that can produce high-quality translations without expensive MBR decoding at inference time. First, MBR decoding generates a hypothesis set for each source sentence and scores each translation using BLEURT as the utility function. These scores create a preference ordering among hypotheses. DPO then fine-tunes the model using preference pairs extracted from this ranking, with KL-divergence regularization controlled by parameter β to prevent overfitting. The training uses various preference selection strategies (best-worst, best-middle, consecutive pairs) to balance efficiency and ranking coverage.

## Key Results
- DPO-MBR achieves BLEURT scores within 1 point of MBR decoding on WMT21/22 test sets
- Outperforms baseline beam search by at least 3 BLEURT points
- Optimal regularization parameter β found to be 0.3-0.6
- Best-worst-middle (BMW) selection strategy is more efficient than consecutive pairs (CP) while achieving similar performance

## Why This Works (Mechanism)

### Mechanism 1
- MBR decoding ranks translations by expected quality; DPO fine-tuning trains the model to prefer those rankings internally.
- The MBR scoring process creates a preference ordering that DPO learns to replicate through preference pairs, eliminating the need for explicit multi-step decoding.
- If BLEURT scores don't correlate well with human quality judgments, the learned ranking will be unreliable.

### Mechanism 2
- KL-divergence regularization in DPO prevents the model from diverging too far from its base distribution, preserving fluency while learning MBR preferences.
- Without proper regularization, the model would overfit to preference pairs and degrade fluency, as evidenced by BLEU scores dropping below 12 when β is too small.
- If β is too large, the model barely deviates from the base and loses the benefits of preference learning.

### Mechanism 3
- Selection strategy for preference pairs (BMW vs CP) affects training efficiency and model generalization but not ultimate translation quality.
- Different strategies sample different subsets of the MBR ranking, influencing convergence speed and diversity of preference patterns.
- If the MBR ranking contains subtle quality distinctions not captured by sparse strategies like BMW, the model may miss critical ranking information.

## Foundational Learning

- Concept: Minimum Bayes Risk (MBR) decoding
  - Why needed here: MBR provides the quality ranking that DPO fine-tuning aims to internalize; understanding its sampling and scoring is essential.
  - Quick check question: In MBR decoding, how is the score for a translation hypothesis computed and what role does the utility function (e.g., BLEURT) play?

- Concept: Direct Preference Optimization (DPO)
  - Why needed here: DPO is the training algorithm that converts MBR rankings into preference pairs and updates the model to prefer higher-ranked translations.
  - Quick check question: In DPO, what is the purpose of the KL-divergence regularization term and how does the β parameter control the trade-off between reward and regularization?

- Concept: Sampling strategies for hypothesis generation
  - Why needed here: The method uses sampled hypotheses to approximate the MBR expectation; understanding sampling impacts the diversity and representativeness of preference pairs.
  - Quick check question: How does the choice of sampling temperature and hypothesis set size affect the MBR score estimates and the quality of preference pairs?

## Architecture Onboarding

- Component map: BLOOMZ/BLOOMZ-mt -> MBR decoding module -> BLEURT scorer -> Preference pair extractor -> DPO fine-tuning loop -> Fine-tuned model
- Critical path: 1) Generate hypothesis set H(x) via sampling, 2) Compute MBR scores for each hypothesis, 3) Select preference pairs based on MBR ranking, 4) Apply DPO update using these pairs, 5) Evaluate on test sets with beam search
- Design tradeoffs:
  - Hypothesis set size vs. computational cost (larger |H| gives better MBR estimates but slower training)
  - Preference pair selection strategy vs. training efficiency (BMW is faster but may capture less ranking nuance than CP)
  - β regularization strength vs. model fluency vs. preference adherence (high β preserves fluency, low β risks overfitting)
- Failure signatures:
  - Low BLEU despite high BLEURT → over-regularization or preference pairs not capturing BLEU-relevant distinctions
  - Degraded translation quality vs. baseline → preference pairs poorly chosen or MBR ranking not representative
  - Training instability or divergence → insufficient regularization or sampling issues
- First 3 experiments:
  1. Run MBR decoding on a small fine-tuning set, inspect MBR scores and hypothesis diversity to validate the ranking reflects translation quality.
  2. Generate preference pairs using BMW strategy, train with varying β values, monitor reward margins and BLEU/BLEURT on a validation set to find the optimal β.
  3. Compare BMW vs. CP selection strategies on a held-out set, measure training efficiency and final translation performance to assess trade-offs.

## Open Questions the Paper Calls Out
- How does DPO-MBR perform on medium-resource and low-resource language pairs compared to high-resource ones?
- How does DPO-MBR perform across different domains beyond news translation?
- What is the impact of varying the hypothesis set size |H| during fine-tuning on different language pairs?

## Limitations
- Evaluation relies on BLEURT scores computed on small held-out validation sets, which may not generalize to full test sets.
- Exact implementation details of MBR decoding approximation and hypothesis sampling are not fully detailed.
- Preference pair selection strategies are compared only in terms of BLEURT, not BLEU or human evaluation.

## Confidence

- **High**: DPO-MBR achieves BLEURT scores close to MBR decoding and outperforms baseline beam search by >3 BLEURT points.
- **Medium**: The mechanism by which MBR rankings are internalized via DPO preference pairs is plausible but not directly verified.
- **Medium**: The claim that BMW selection strategy is more efficient while achieving similar performance is supported by internal experiments but lacks external validation.

## Next Checks
1. Run human evaluation on a subset of test translations to verify that BLEURT gains correspond to improved fluency and adequacy.
2. Perform ablation studies on sampling temperature and hypothesis set size to quantify their impact on MBR score accuracy and translation quality.
3. Test the trained models on a distinct, unseen test set (not used in any tuning) to assess true generalization and check for overfitting to the fine-tuning validation set.