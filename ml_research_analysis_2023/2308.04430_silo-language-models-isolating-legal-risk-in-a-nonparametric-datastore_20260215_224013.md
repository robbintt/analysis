---
ver: rpa2
title: 'SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore'
arxiv_id: '2308.04430'
source_url: https://arxiv.org/abs/2308.04430
tags:
- data
- datastore
- knn-lm
- text
- parametric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The legality of training large language models on copyrighted data
  is under intense debate. However, we show that training models only on low-risk
  data (e.g., public domain books) significantly degrades performance due to the limited
  size and domain coverage of such data.
---

# SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore

## Quick Facts
- arXiv ID: 2308.04430
- Source URL: https://arxiv.org/abs/2308.04430
- Reference count: 40
- Primary result: SILO trained on low-risk data closes 90% of performance gap with models trained on mostly high-risk data by using nonparametric datastore retrieval

## Executive Summary
SILO addresses the legal risk of training language models on copyrighted data by separating low-risk training data from high-risk retrieval data. The approach trains a parametric language model on permissively licensed text while using a nonparametric datastore containing high-risk content only at inference time. Experiments show that SILO recovers 90% of the performance gap compared to models trained on mostly high-risk data, demonstrating that high-quality language models can be built while mitigating legal exposure.

## Method Summary
SILO consists of two components: (1) a parametric language model trained on 228B tokens of public domain and permissively licensed text from the Open License Corpus, and (2) a nonparametric datastore containing high-risk data used for retrieval during inference. The approach uses k-nearest neighbors language modeling (kNN-LM) and retrieval-in-context (RIC-LM) methods to access high-risk content without training on it. The datastore supports sentence-level data attribution and enables data producers to opt out from the model.

## Key Results
- SILO trained on low-risk data struggles on out-of-domain data, but adding the nonparametric datastore greatly improves performance
- The nonparametric datastore scales well and the model is robust to domain shift
- SILO closes 90% of the performance gap with baseline models trained on mostly high-risk data
- kNN-LM generalizes better than RIC-LM due to less sensitivity to domain mismatch between parametric model and test data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating low-risk training data from high-risk retrieval data allows model to access broader knowledge without legal exposure during training.
- Mechanism: Parametric component learns stable representations from permissively licensed data, while nonparametric datastore provides dynamic access to restricted content at inference time.
- Core assumption: Legal risk primarily stems from training on restricted data, not from using it at inference time.
- Evidence anchors:
  - [abstract]: "SILO is built by (1) training a parametric LM on low-risk data... and (2) augmenting it with a nonparametric datastore that can include high-risk data."
  - [section]: "The datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model."
- Break condition: If legal risk extends to any use of restricted data (not just training), this mechanism fails.

### Mechanism 2
- Claim: kNN-LM generalizes better than RIC-LM because it directly interpolates with retrieved distributions rather than relying on parametric component's robustness to domain shift.
- Mechanism: kNN-LM's nonparametric distribution is added directly to output, making it less sensitive to domain mismatch between parametric model and test data.
- Core assumption: Parametric models are more sensitive to domain shift than nonparametric retrieval distributions.
- Evidence anchors:
  - [section]: "kNN-LM improves more rapidly than RIC-LM does, consistently over all datasets" and "kNN-LM generalizes better than RIC-LM, since RIC-LM is bottlenecked by the parametric component."
  - [section]: "the encoder, which provides the nonparametric distribution PkNN, is fairly robust to extreme domain shift."
- Break condition: If retrieved contexts are themselves out-of-domain or irrelevant, both methods fail.

### Mechanism 3
- Claim: Scaling the nonparametric datastore improves performance log-linearly because larger context pools provide better matches for diverse test inputs.
- Mechanism: More tokens in datastore increase probability of finding relevant retrieval context for any given input.
- Core assumption: Relevant retrieval context exists in datastore and can be found via similarity search.
- Evidence anchors:
  - [section]: "Figure 3 demonstrates that both kNN-LM and RIC-LM-based retrieval consistently improves performance as the datastore size increases, with a strong log-linear trend."
  - [section]: "Scaling the test-time datastore consistently improves performance over all domains."
- Break condition: If similarity search fails to find relevant contexts even in large datastore, scaling provides no benefit.

## Foundational Learning

- Concept: Nonparametric vs parametric models
  - Why needed here: SILO fundamentally separates these two approaches to balance legal risk and performance.
  - Quick check question: In kNN-LM, which component provides the final output distribution—the parametric model alone or a combination of parametric and nonparametric distributions?

- Concept: Domain generalization
  - Why needed here: OPEN LICENSE CORPUS has highly skewed domain distribution, creating extreme generalization challenge.
  - Quick check question: If a model is trained on only legal documents and code, which evaluation domain would likely show the largest performance gap compared to a model trained on diverse web data?

- Concept: Nearest neighbor search and vector quantization
  - Why needed here: Efficient retrieval from trillion-token datastores requires approximation techniques.
  - Quick check question: Why might using FAISS with IVFPQ quantization be preferable to exact nearest neighbor search for trillion-token datastores?

## Architecture Onboarding

- Component map: Parametric transformer (1.3B params) → Encoder (last layer output) → FAISS index (datastore) → kNN-LM interpolation (λ-weighted) → Final distribution
- Critical path: Input text → Parametric model → Retrieve k nearest neighbors → Compute nonparametric distribution → Interpolate with parametric distribution → Output
- Design tradeoffs: Larger datastores improve performance but slow inference; kNN-LM provides better domain generalization but is slower than RIC-LM
- Failure signatures: Performance plateaus despite larger datastore (similarity search failing); kNN-LM underperforms RIC-LM (encoder poorly aligned with parametric model); extreme domain gap persists (parametric model too specialized)
- First 3 experiments:
  1. Compare perplexity of parametric-only model vs. kNN-LM with small datastore on in-domain test data
  2. Scale datastore size exponentially and measure performance improvement curve
  3. Replace parametric component with Pythia while keeping kNN-LM encoder fixed to isolate component contributions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would SILO perform with an even larger datastore (e.g., trillions of tokens) using kNN-LM retrieval?
- Basis in paper: [explicit] The authors state that scaling the datastore beyond 1 billion tokens may enable SILO to match Pythia performance, and reference Borgeaud et al. (2022) which used trillions of tokens.
- Why unresolved: The experiments only scaled the datastore up to 1 billion tokens due to resource constraints.
- What evidence would resolve it: Running experiments with a much larger datastore (e.g., trillions of tokens) and comparing performance to Pythia on all domains.

### Open Question 2
- Question: Would training the parametric component of SILO on a more diverse dataset (beyond just permissive licenses) improve its out-of-domain generalization?
- Basis in paper: [inferred] The authors note that SILO struggles with extreme domain generalization because it is only trained on permissive data, which has a skewed domain distribution compared to typical pretraining corpora.
- Why unresolved: The experiments only explored training SILO on different subsets of permissive data.
- What evidence would resolve it: Training SILO on a more diverse dataset (e.g., including some non-permissive data) and evaluating its performance on out-of-domain data compared to Pythia.

### Open Question 3
- Question: How would different methods for inferring data licenses from documents in web crawl at scale impact the quality and diversity of permissive training data?
- Basis in paper: [explicit] The authors acknowledge that their study relies on explicit metadata to identify licenses, which may lead to underestimates of the amount and diversity of permissively licensed text available on the web. They suggest that future research may investigate inferring data licenses from documents in web crawl at scale.
- Why unresolved: The authors did not explore this approach in their experiments.
- What evidence would resolve it: Developing and evaluating a method for inferring data licenses from documents in web crawl at scale, and comparing the resulting dataset's quality and diversity to the authors' manually curated dataset.

## Limitations

- The legal risk assessment relies on assumptions about what constitutes legally risky training data that aren't fully validated
- The domain coverage analysis is limited to 14 test domains, all drawn from standardized benchmarks
- The scalability analysis shows log-linear improvements but doesn't establish practical limits at trillion-token scales

## Confidence

**High confidence**: The technical implementation of kNN-LM and RIC-LM retrieval methods works as described, and the log-linear scaling trend with datastore size is reproducible.

**Medium confidence**: The claim that SILO closes 90% of the performance gap between low-risk and high-risk training approaches, given the specific experimental setup and evaluation domains.

**Low confidence**: The broader claim that this approach meaningfully mitigates legal risk, as this depends on legal interpretations not addressed in the paper and requires domain-specific legal analysis.

## Next Checks

1. **Legal Risk Validation**: Conduct a legal analysis of training vs. inference-time use of restricted data across multiple jurisdictions, focusing on whether the parametric/nonparametric separation provides meaningful legal protection.

2. **Cross-Domain Robustness**: Evaluate SILO on a broader set of domains beyond standardized benchmarks, including domains with different linguistic patterns and knowledge requirements, to test the robustness of the nonparametric retrieval approach.

3. **Scalability Limits**: Test performance at trillion-token datastore scales with varying similarity search quality (FAISS vs exact search) to identify practical limits where approximation errors overwhelm retrieval benefits.