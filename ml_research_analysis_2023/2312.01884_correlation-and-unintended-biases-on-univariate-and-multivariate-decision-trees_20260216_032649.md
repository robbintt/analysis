---
ver: rpa2
title: Correlation and Unintended Biases on Univariate and Multivariate Decision Trees
arxiv_id: '2312.01884'
source_url: https://arxiv.org/abs/2312.01884
tags:
- decision
- datasets
- trees
- correlation
- split
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares univariate decision trees (UDTs) and multivariate
  decision trees (MDTs) to understand why UDTs often perform as well as MDTs despite
  being less expressive. The authors analyze the impact of feature correlation and
  label noise on DT performance, finding that MDTs excel when decision boundaries
  are oblique and feature correlations are high, while UDTs are more robust to noise.
---

# Correlation and Unintended Biases on Univariate and Multivariate Decision Trees

## Quick Facts
- arXiv ID: 2312.01884
- Source URL: https://arxiv.org/abs/2312.01884
- Reference count: 40
- Key outcome: Univariate decision trees consistently outperformed multivariate decision trees on 57 benchmark datasets, but this gap is explained by dataset bias toward low correlation and axis-parallel boundaries.

## Executive Summary
This paper investigates why univariate decision trees (UDTs) often perform as well as multivariate decision trees (MDTs) despite being less expressive. Through analysis of feature correlation, decision boundary characteristics, and label noise, the authors find that standard benchmark datasets are systematically biased toward conditions favoring UDTs (low correlation, axis-parallel boundaries). While MDTs excel when decision boundaries are oblique and features are highly correlated, benchmark datasets rarely exhibit these conditions. The study recommends testing dataset characteristics before choosing between UDT and MDT approaches.

## Method Summary
The authors compare UDTs (using CART) with MDTs (using an Omnivariate Tree implementation with various split functions including SVM, Ridge, Lasso, and CART splits) across 57 benchmark datasets. They employ stratified hold-out validation (90% train, 10% test) and analyze performance using accuracy, F1, AUC, and average precision. Synthetic datasets with controlled feature correlation and label noise are used to isolate the effects of these factors. Feature correlation is measured using Pearson, Spearman, and Kendall coefficients.

## Key Results
- UDTs consistently outperformed MDTs on benchmark datasets across all performance metrics
- MDTs showed smaller model sizes despite lower accuracy
- Standard benchmark datasets exhibit extremely low feature correlation (median Pearson correlation ~0.1)
- Decision boundaries in benchmark datasets are predominantly axis-parallel
- MDTs excel when decision boundaries are oblique (45° slope) and feature correlation is high
- UDTs demonstrate greater robustness to label noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature correlation and label noise directly affect the relative performance of univariate vs multivariate decision trees.
- Mechanism: When features are highly correlated and the decision boundary is oblique, multivariate splits can better capture the underlying structure, improving accuracy. However, label noise introduces complexity that can cause multivariate trees to overfit, favoring univariate trees that are more robust.
- Core assumption: The decision boundary shape and feature correlation can be estimated from the dataset characteristics.
- Evidence anchors:
  - [abstract] "The authors analyze the impact of feature correlation and label noise on DT performance, finding that MDTs excel when decision boundaries are oblique and feature correlations are high, while UDTs are more robust to noise."
  - [section] "Figure 1 shows the mean and standard deviation of the performances of DTs with a single univariate or multivariate split... For θ = 45°, the gap between MDTs and UDTs increases with correlation ρ."
  - [corpus] Weak evidence - corpus focuses on other decision tree algorithms rather than the correlation/noise effect.
- Break condition: If the dataset has low feature correlation and axis-parallel decision boundaries, the performance gap disappears.

### Mechanism 2
- Claim: Standard benchmark datasets are biased towards low feature correlation and axis-parallel decision boundaries, favoring univariate decision trees.
- Mechanism: Pre-processing practices often remove correlated features to avoid multicollinearity, and datasets naturally tend to have low correlation. This creates a bias in the evaluation, making univariate trees appear better than they would be on real-world data with higher correlation.
- Core assumption: Benchmark datasets are representative of real-world data distributions.
- Evidence anchors:
  - [abstract] "They also show that standard benchmark datasets are biased towards low correlation and axis-parallel decision boundaries, favoring UDTs."
  - [section] "Figure 4 shows the empirical cumulative distributions (CDFs) of Pearson, Spearman, and Kendal correlation coefficients over the benchmark datasets. The feature correlation is extremely low over all pairs of features."
  - [corpus] No direct evidence - corpus papers do not discuss dataset bias in this specific way.
- Break condition: If real-world datasets used for evaluation have higher correlation and oblique boundaries, the bias would be exposed.

### Mechanism 3
- Claim: The observed performance gap between univariate and multivariate decision trees can be explained by dataset bias and the impact of feature correlation and decision boundary characteristics.
- Mechanism: The combination of biased benchmark datasets (low correlation, axis-parallel boundaries) and the theoretical understanding that multivariate trees perform better under high correlation and oblique boundaries explains why univariate trees consistently outperform multivariate trees in experiments.
- Core assumption: The theoretical relationship between feature correlation, decision boundary shape, and tree performance holds in practice.
- Evidence anchors:
  - [abstract] "The authors conclude that the observed performance gap can be explained by dataset bias and recommend testing feature correlation and decision boundary characteristics before choosing between UDTs and MDTs."
  - [section] "In summary, the observed performances of UDTs and MDTs can be largely explained by the factors analyzed in this paper: feature correlation, slope of the decision boundary, and, we conjecture, label noise."
  - [corpus] Weak evidence - corpus papers do not discuss this specific explanatory mechanism.
- Break condition: If new experiments on diverse real-world datasets show a different pattern, the explanation would need revision.

## Foundational Learning

- Concept: Feature correlation and multicollinearity
  - Why needed here: Understanding how feature correlation affects decision tree performance is crucial for interpreting the experimental results and choosing the right tree type.
  - Quick check question: What is the difference between feature correlation and multicollinearity, and how do they affect linear models?

- Concept: Decision boundary shape (axis-parallel vs oblique)
  - Why needed here: The shape of the decision boundary determines whether univariate or multivariate splits are more appropriate, affecting tree performance.
  - Quick check question: How can you visually distinguish between axis-parallel and oblique decision boundaries in a 2D feature space?

- Concept: Label noise and overfitting
  - Why needed here: Label noise introduces complexity that can cause multivariate trees to overfit, favoring the more robust univariate trees.
  - Quick check question: How does label noise affect the bias-variance tradeoff in decision tree models?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Model training (UDT CART / MDT Omnivariate) -> Evaluation (Stratified hold-out) -> Analysis (Performance metrics + Complexity metrics)

- Critical path:
  1. Load and preprocess dataset (remove non-numeric features, normalize)
  2. Analyze feature correlation and decision boundary characteristics
  3. Train UDT and MDT models
  4. Evaluate performance on test set
  5. Compare results and analyze impact of correlation and noise

- Design tradeoffs:
  - Computational cost: MDTs are more expensive to train due to optimization of multivariate splits
  - Model interpretability: UDTs are easier to interpret due to axis-parallel splits, but MDTs can be more compact
  - Performance: UDTs may be better on low-correlation, axis-parallel datasets, while MDTs excel on high-correlation, oblique datasets

- Failure signatures:
  - UDTs underperform: Dataset likely has high feature correlation and oblique decision boundaries
  - MDTs underperform: Dataset likely has low feature correlation, axis-parallel boundaries, or high label noise
  - Both underperform: Dataset may be noisy, have complex decision boundaries, or require ensemble methods

- First 3 experiments:
  1. Train and evaluate UDT and MDT on a benchmark dataset with known low correlation and axis-parallel boundaries (e.g., Iris dataset)
  2. Train and evaluate UDT and MDT on a synthetic dataset with high correlation and oblique decision boundary (e.g., θ = 45°, ρ = 0.8)
  3. Analyze feature correlation and decision boundary characteristics of a real-world dataset, then train and evaluate UDT and MDT based on the analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of benchmark datasets lead to the observed performance gap between univariate and multivariate decision trees?
- Basis in paper: [explicit] The authors state that benchmark datasets exhibit distributions of feature correlation and decision boundary slope that are skewed towards low correlation values and approximately axis-parallel decision boundaries, favoring univariate DTs.
- Why unresolved: While the authors identify correlation and decision boundary slope as key factors, they do not provide a comprehensive analysis of all dataset characteristics that might contribute to the performance gap.
- What evidence would resolve it: A systematic study analyzing various dataset characteristics (e.g., noise levels, feature distributions, class balance) and their impact on UDT vs MDT performance across a wide range of datasets.

### Open Question 2
- Question: How does the choice of split function optimization algorithm impact the performance of multivariate decision trees in the presence of correlated features?
- Basis in paper: [explicit] The authors implement an Omnivariate Tree that tests several splits at each node, including SVM, gradient-SVM, Ridge, Least Squares, Elastic Net, Lasso, and CART splits. However, they do not provide a detailed comparison of the performance of these different optimization algorithms.
- Why unresolved: The paper focuses on comparing UDTs and MDTs in general, but does not delve into the specific impact of the choice of split function optimization algorithm on MDT performance in the presence of correlated features.
- What evidence would resolve it: An experimental study comparing the performance of MDTs using different split function optimization algorithms (e.g., SVM, gradient-SVM, Ridge, etc.) on datasets with varying levels of feature correlation.

### Open Question 3
- Question: What is the impact of label noise on the performance of univariate and multivariate decision trees, and how can this be mitigated?
- Basis in paper: [explicit] The authors mention that label noise is a factor affecting the performance of UDTs and MDTs, but they do not provide a detailed analysis of its impact or potential mitigation strategies.
- Why unresolved: While the authors acknowledge the role of label noise, they do not explore it in depth or propose methods to handle it in the context of UDT vs MDT performance.
- What evidence would resolve it: Experiments analyzing the impact of different levels of label noise on UDT and MDT performance, along with the evaluation of various noise-robust training techniques or preprocessing methods to mitigate its effects.

## Limitations

- The study relies on benchmark datasets that may not represent real-world data distributions, potentially limiting generalizability
- Analysis focuses on linear decision boundaries and Pearson correlation, potentially overlooking non-linear relationships
- Single decision trees are evaluated rather than ensembles, which may behave differently under various correlation and noise conditions

## Confidence

- High confidence: Feature correlation and decision boundary characteristics significantly impact UDT vs MDT performance
- Medium confidence: Benchmark dataset bias systematically favors UDTs in literature comparisons
- Medium confidence: MDT overfitting in high-noise conditions is a key performance limitation

## Next Checks

1. Evaluate UDT and MDT performance on real-world datasets with known high feature correlation and oblique decision boundaries
2. Test the robustness of conclusions using non-linear correlation measures and non-linear decision boundaries
3. Compare single tree performance with ensemble methods (Random Forests, Gradient Boosting) under varying correlation and noise conditions