---
ver: rpa2
title: Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning
arxiv_id: '2307.10274'
source_url: https://arxiv.org/abs/2307.10274
tags:
- domain
- whisper
- audio
- prompt
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes prompt-conditioning fine-tuning to achieve
  zero-shot domain-sensitive speech recognition. The method fine-tunes Whisper with
  text prompts containing domain tags, enabling the model to leverage domain information
  without retraining on unseen domains.
---

# Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning

## Quick Facts
- arXiv ID: 2307.10274
- Source URL: https://arxiv.org/abs/2307.10274
- Reference count: 0
- Authors: 
- Key outcome: Prompt-conditioning fine-tuning achieves up to 33% WER reduction on unseen domains

## Executive Summary
This paper proposes prompt-conditioning fine-tuning to achieve zero-shot domain-sensitive speech recognition. The method fine-tunes Whisper with text prompts containing domain tags, enabling the model to leverage domain information without retraining on unseen domains. The authors demonstrate that their approach, called Clairaudience, achieves significant word error rate (WER) reductions of up to 33% on unseen datasets across various domains such as medical conversation, air traffic control communication, and financial meetings. They also extend the method to text-only fine-tuning, showing that Clairaudience-text-only can still attend to prompt contexts and achieve WER reductions of up to 29% on the medical conversation dataset.

## Method Summary
The authors fine-tune Whisper with text prompts containing domain tags to achieve domain sensitivity without retraining on unseen domains. They use GPT3.5 to generate domain tags from audio transcripts, creating diverse prompts. The decoder is fine-tuned to attend to these domain tags during generation. They also propose a text-only fine-tuning approach by replacing cross-attention outputs with learnable bias terms, allowing domain adaptation without audio data. The model is evaluated on domain-specific datasets like medical conversations, air traffic control communications, and financial meetings to demonstrate zero-shot domain sensitivity.

## Key Results
- Up to 33% WER reduction on unseen datasets across various domains
- Clairaudience-text-only achieves up to 29% WER reduction on medical conversation dataset
- Performance comparable to domain adaptation methods on ATC and Medical-split datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-conditioning fine-tuning teaches the model to attend to domain tags during generation.
- Mechanism: By inserting domain tags into decoder input sequences during fine-tuning, the model learns to condition its predictions on the contextual information provided by the tags.
- Core assumption: The decoder's language modeling capability is strong enough to incorporate domain tags into its next-token prediction when fine-tuned appropriately.
- Evidence anchors:
  - [abstract] The authors show that fine-tuning Whisper with text prompts containing domain tags enables the model to leverage domain information without retraining on unseen domains.
  - [section] The decoder processes around 6B tokens over diverse web transcripts, making it a robust language model for the ASR system. We thus hypothesize that the Whisper model has enough language ability to be further fine-tuned for becoming domain-sensitive.
- Break condition: If the domain tags are too sparse or irrelevant to the audio content, the model may fail to effectively incorporate them into predictions.

### Mechanism 2
- Claim: Text-only fine-tuning with bias terms allows the model to learn domain sensitivity without access to audio data.
- Mechanism: Replacing cross-attention outputs with learnable bias terms allows the decoder to be fine-tuned on text-only data while still retaining the ability to attend to audio features during inference.
- Core assumption: The pre-trained encoder-decoder architecture can be separated such that the decoder can be fine-tuned independently while maintaining its ability to leverage audio features.
- Evidence anchors:
  - [section] We remove this attention mechanism by replacing the output of each cross-attention layer with trainable biases. The addition of bias term is inspired by classifier-free guidance [23]. The model is then tuned with conventional sequence-to-sequence training.
  - [abstract] The authors demonstrate that their text-only fine-tuned model can still attend to prompt contexts and achieve WER reductions.
- Break condition: If the bias terms are not properly regularized, the model may catastrophically forget how to process audio features.

### Mechanism 3
- Claim: GPT-generated domain tags provide diverse and relevant contextual information for fine-tuning.
- Mechanism: Using GPT3.5 to generate domain tags from audio transcripts creates a dataset that resembles real-world domain-specific scenarios without requiring manual annotation.
- Core assumption: The GPT-generated domain tags are sufficiently diverse and representative of the actual domain contexts present in the audio data.
- Evidence anchors:
  - [section] We generate concise and diverse domain tags based on the audio transcripts, resembling extensive work in large language models (LLM) for generating data for supervision tasks [20, 21, 22]. For each full audio (complete audio file with about 4k tokens on average), we generate ten domain tags with GPT3.5 using the audio transcript.
  - [corpus] The corpus neighbors show related work on domain-sensitive feature memory and zero-shot domain adaptation, suggesting this approach is novel.
- Break condition: If the GPT-generated tags are too generic or not well-aligned with the actual domain contexts, the fine-tuning may not effectively transfer to real domain-specific scenarios.

## Foundational Learning

- Concept: Encoder-decoder architecture
  - Why needed here: The Whisper model uses an encoder-decoder transformer architecture, which is crucial for understanding how the model processes audio features and generates text.
  - Quick check question: What are the main components of an encoder-decoder transformer architecture, and how do they interact during speech recognition?

- Concept: Prompt engineering
  - Why needed here: The paper leverages prompt engineering techniques from NLP to condition the speech recognition model on domain-specific information.
  - Quick check question: How does adding domain tags as prompts during fine-tuning affect the model's ability to generalize to unseen domains?

- Concept: Catastrophic forgetting
  - Why needed here: The text-only fine-tuning approach must avoid catastrophic forgetting to ensure the model retains its audio processing capabilities.
  - Quick check question: What techniques are used to prevent catastrophic forgetting when fine-tuning a pre-trained model on a new task?

## Architecture Onboarding

- Component map:
  - Encoder: Processes log-Mel spectrograms to produce audio features
  - Decoder: Takes audio features and prompt tokens to generate transcripts
  - Cross-attention layers: Connect encoder and decoder (modified in text-only fine-tuning)
  - Prompt tokens: Domain tags inserted into decoder input sequence

- Critical path:
  1. Audio input → Encoder → Audio features
  2. Audio features + Prompt tokens → Decoder → Transcript

- Design tradeoffs:
  - Audio-text fine-tuning vs. text-only fine-tuning: Audio-text provides more robust performance but requires paired data, while text-only is more resource-efficient but may have weaker domain sensitivity.
  - Prompt complexity: More detailed prompts may provide better context but could also introduce noise or confusion.

- Failure signatures:
  - Degradation on general domain datasets (e.g., Common Voice, LibriSpeech)
  - Performance drop on long-form audio due to segmented input handling
  - Catastrophic forgetting of audio processing capabilities in text-only fine-tuning

- First 3 experiments:
  1. Evaluate baseline Whisper performance with and without prompts on a domain-specific dataset.
  2. Fine-tune Whisper on GigaSpeech with domain prompts and assess zero-shot performance on unseen domains.
  3. Implement text-only fine-tuning by replacing cross-attention outputs with bias terms and evaluate domain sensitivity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance of prompt-conditioning fine-tuned models be further improved by incorporating a small amount of audio-text fine-tuning alongside text-only fine-tuning to address the issue of long-form audio transcription?
- Basis in paper: [explicit] The paper discusses the issue of long-form audio transcription in the text-only fine-tuned model, where the model tends to hallucinate or fail to transcribe audio clips starting with unfinished sentences. It suggests that this problem could be better solved by mixing text-only tuning with a small amount of audio-text tuning.
- Why unresolved: The paper mentions this as a potential solution but does not conduct experiments to test its effectiveness. The authors leave this study for future work.
- What evidence would resolve it: Conducting experiments to compare the performance of the text-only fine-tuned model with and without the addition of audio-text fine-tuning on long-form audio datasets would provide evidence for the effectiveness of this approach.

### Open Question 2
- Question: How does the performance of prompt-conditioning fine-tuned models compare to traditional domain adaptation methods when applied to unseen domains?
- Basis in paper: [explicit] The paper compares the performance of the prompt-conditioning fine-tuned model (Clairaudience) to the Whisper model fine-tuned directly on target domains (Whisper (DA)) in terms of Word Error Rate (WER). The results show that the prompt-conditioning fine-tuned model achieves similar performance to the domain adaptation method on the ATC and Medical-split datasets.
- Why unresolved: While the paper demonstrates the effectiveness of prompt-conditioning fine-tuning in achieving similar performance to traditional domain adaptation methods, further research is needed to explore its performance on a wider range of unseen domains and compare it to other domain adaptation techniques.
- What evidence would resolve it: Conducting experiments to evaluate the performance of prompt-conditioning fine-tuned models on a diverse set of unseen domains and comparing it to other domain adaptation methods would provide evidence for its effectiveness in domain adaptation.

### Open Question 3
- Question: Can prompt-conditioning fine-tuning be extended to other modalities of prompts, such as visual or multimodal prompts, to improve speech recognition performance in specific domains?
- Basis in paper: [explicit] The paper mentions that the approach of prompt-conditioning fine-tuning can be easily extended to accommodate other prompt formats, contexts, and even other modalities of prompts. However, the paper focuses on text prompts for speech recognition.
- Why unresolved: The paper does not explore the use of other modalities of prompts in prompt-conditioning fine-tuning. Further research is needed to investigate the effectiveness of visual or multimodal prompts in improving speech recognition performance in specific domains.
- What evidence would resolve it: Conducting experiments to compare the performance of prompt-conditioning fine-tuned models with different modalities of prompts, such as visual or multimodal prompts, on specific domains would provide evidence for the effectiveness of extending the approach to other modalities.

## Limitations

- Limited domain diversity: Only evaluated on four domains (medical, ATC, ATCO2, earnings meetings), which may not represent the full spectrum of potential application scenarios.
- Text-only fine-tuning effectiveness: Performance gap compared to audio-text fine-tuning suggests text-only approach may not be sufficient for all domain adaptation scenarios.
- Long-form audio handling: Performance degradation on long-form audio datasets due to segmented audio inputs starting with unfinished sentences.

## Confidence

**High Confidence**: The core claim that prompt-conditioning fine-tuning improves domain sensitivity is well-supported by consistent WER reductions across multiple datasets and evaluation metrics.

**Medium Confidence**: The claim about text-only fine-tuning maintaining domain sensitivity is supported by experimental results but has more limitations and potential for catastrophic forgetting.

**Medium Confidence**: The claim that GPT-generated domain tags are sufficient for effective domain adaptation is supported by results but relies on the assumption that the generated tags adequately represent domain contexts.

## Next Checks

1. **Cross-domain generalization test**: Evaluate the fine-tuned models on additional domains not covered in the current study (e.g., legal, educational, or technical domains) to assess the robustness of prompt-conditioning fine-tuning across a broader domain spectrum.

2. **Human evaluation of prompt quality**: Conduct human evaluation of the GPT-generated domain tags to assess their relevance, diversity, and alignment with actual domain contexts. Compare human-annotated tags with GPT-generated ones to identify potential discrepancies.

3. **Long-form audio adaptation**: Design experiments specifically targeting long-form audio scenarios by implementing better handling of segmented inputs (e.g., better sentence boundary detection or context preservation techniques) and evaluate whether the performance gap on long-form datasets can be reduced.