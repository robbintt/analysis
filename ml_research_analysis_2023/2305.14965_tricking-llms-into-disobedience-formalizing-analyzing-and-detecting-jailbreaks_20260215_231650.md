---
ver: rpa2
title: 'Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks'
arxiv_id: '2305.14965'
source_url: https://arxiv.org/abs/2305.14965
tags:
- prompt
- attack
- language
- user
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a formalism and taxonomy of jailbreaks, which
  are prompt injection attacks on large language models (LLMs). The authors analyze
  the effectiveness of different types of jailbreaks on various LLMs and tasks, using
  property tests and intent tests.
---

# Tricking LLMs into Disobedience: Formalizing, Analyzing, and Detecting Jailbreaks

## Quick Facts
- arXiv ID: 2305.14965
- Source URL: https://arxiv.org/abs/2305.14965
- Reference count: 39
- Key outcome: Proposes formalism and taxonomy of jailbreaks, analyzes effectiveness across LLMs and tasks, finds varying robustness, and proposes limited defense methods

## Executive Summary
This paper formalizes and analyzes jailbreak attacks on large language models (LLMs), where prompt injection techniques manipulate models into producing harmful or misaligned outputs. The authors conduct a comprehensive survey of existing jailbreak methods and evaluate their effectiveness across four different LLMs (GPT-3.5, OPT, BLOOM, FLAN-T5-XXL) and four task types (translation, sentiment/hate speech classification, code generation, summarization). They identify seven attack types organized into three categories: cognitive hacking, instruction repetition, and syntactical transformation, and propose property tests to detect successful jailbreaks.

## Method Summary
The researchers surveyed existing jailbreak methods and conducted experiments using 49 attack prompts across four attack types on four different LLMs performing four distinct tasks. They employed property tests to detect if task instructions were maintained and performed qualitative analysis on model outputs. The study examined how different attack categories (Direct Instruction, Instruction Repetition, Syntactical Transformation, Cognitive Hacking, Few-shot Hacking, Text Completion as Instruction, and Indirect Task Deflection) affected model performance across various architectures and tasks.

## Key Results
- LLMs show varying degrees of robustness to jailbreaks depending on their size, training data, and architecture
- Different attack types have different effectiveness levels across models and tasks
- Proposed defense mechanisms like batch prompt guards and checksum prompt guards show limited effectiveness
- Property tests can detect some jailbreak successes but may conflate task performance failures with attack success

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jailbreak prompts succeed by exploiting the instruction-following capability of LLMs through cognitive hacking techniques.
- Mechanism: Attackers craft prompts that place the model in a fictional scenario with an authority figure, tricking it into producing harmful outputs by framing the request as legitimate within the context.
- Core assumption: The LLM treats the fictional scenario as a valid instruction context and follows it without questioning the ethical implications.
- Evidence anchors:
  - [abstract] "users can jailbreak LLMs by simply manipulating their prompts; resulting in degenerate output behavior, privacy and security breaches, offensive outputs, and violations of content regulator policies."
  - [section 4.1.1] "Cognitive Hacking... where in the Language model is 'tricked' into performing an act of misalignment it otherwise would not perform, by providing a 'safe-space' or a situation that warrants such a response."
- Break condition: The LLM's safety training or alignment mechanisms recognize the manipulative context and refuse to generate the requested content.

### Mechanism 2
- Claim: Jailbreak prompts succeed by repeating or obfuscating instructions to bypass content filters.
- Mechanism: Attackers use instruction repetition or syntactical transformations (like LeetSpeak or Base64 encoding) to make the harmful request appear as a legitimate, repeated task or to evade pattern-based detection.
- Core assumption: The model processes the obfuscated or repeated text without recognizing the harmful intent behind it.
- Evidence anchors:
  - [section 4.1.1] "Instruction Repetition... feeding the same instruction multiple times so as to appear as though the attacker is 'begging' the language model."
  - [section 4.1.1] "Syntactical Transformation... involves an orthographic transformation to the attack text, such as by using LeetSpeak or Base64, to bypass content filters."
- Break condition: The model's content filtering system decodes or recognizes the obfuscated text and applies the same safety checks as with plain text.

### Mechanism 3
- Claim: Jailbreak prompts succeed by exploiting the model's text completion behavior to ignore previous instructions.
- Mechanism: Attackers provide incomplete sentences or use few-shot examples to force the model into text completion mode, where it continues the pattern without adhering to the original task instructions.
- Core assumption: The model prioritizes text completion patterns over explicit task instructions when presented with an incomplete context.
- Evidence anchors:
  - [section 4.1.2] "Text Completion as Instruction... attacks work by providing an incomplete sentence to the model, thereby forcing the model to complete the sentence, and in the process, ignore its previous instructions, causing misalignment."
  - [section 4.1.2] "Few-shot Hacking... involves the training paradigm of language models."
- Break condition: The model maintains awareness of the original task context even when presented with completion cues.

## Foundational Learning

- Concept: Instruction following and task alignment in LLMs
  - Why needed here: Understanding how LLMs interpret and follow instructions is crucial for designing effective jailbreaks and defenses.
  - Quick check question: What happens when an LLM receives conflicting instructions - one in the system prompt and another in the user input?

- Concept: Text completion and pattern continuation behavior
  - Why needed here: Jailbreak techniques often exploit the model's tendency to complete patterns rather than strictly follow instructions.
  - Quick check question: How does an LLM determine whether to continue a pattern or follow a new instruction?

- Concept: Content filtering and safety alignment mechanisms
  - Why needed here: Understanding how models detect and block harmful content is essential for both creating and defending against jailbreaks.
  - Quick check question: What are the common methods used to detect harmful content in LLM outputs?

## Architecture Onboarding

- Component map: Prompt parser -> Instruction interpreter -> Content filter -> Output generator -> Safety alignment -> Final Output
- Critical path: Prompt → Parser → Instruction Interpreter → Content Filter → Output Generator → Safety Alignment → Final Output
- Design tradeoffs:
  - Strict safety filtering vs. task completion accuracy
  - Response time vs. thorough content analysis
  - Generalization capability vs. specific task performance
- Failure signatures:
  - False positives: Legitimate requests blocked as harmful
  - False negatives: Harmful content generated despite safety measures
  - Incomplete responses: Task partially completed due to safety interventions
  - System errors: Crashes or freezes when processing complex jailbreak attempts
- First 3 experiments:
  1. Test the model's response to direct instruction jailbreaks (e.g., "Ignore previous instructions and...") across different tasks.
  2. Evaluate the effectiveness of syntactical transformations (Base64, LeetSpeak) in bypassing content filters.
  3. Assess the model's susceptibility to cognitive hacking techniques by presenting fictional authority scenarios.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How effective are prompt guards in mitigating different types of jailbreak attacks?
- Basis in paper: Explicit - The paper mentions that prompt guards can be effective against Direct Instruction (INSTR) attacks on text-davinci-003.
- Why unresolved: The paper only briefly discusses prompt guards and does not provide a comprehensive analysis of their effectiveness against various attack types and models.
- What evidence would resolve it: Conducting extensive experiments with prompt guards against different attack types and models to measure their effectiveness and limitations.

### Open Question 2
- Question: What are the most effective methods for detecting and mitigating jailbreak attacks that involve goal hijacking?
- Basis in paper: Inferred - The paper mentions goal hijacking as one of the attack intents and discusses its prevalence in real-world attacks.
- Why unresolved: The paper does not provide specific methods for detecting or mitigating goal hijacking attacks, which are a significant concern in the context of jailbreaks.
- What evidence would resolve it: Developing and testing detection and mitigation techniques specifically tailored to goal hijacking attacks, and evaluating their performance across different models and attack scenarios.

### Open Question 3
- Question: How can the trade-off between functionality and security be balanced when implementing jailbreak detection and mitigation measures?
- Basis in paper: Explicit - The paper discusses the limitations of property tests and the need for sanitizing and preprocessing outputs, indicating a trade-off between functionality and security.
- Why unresolved: The paper does not provide concrete solutions or guidelines for balancing this trade-off in practical applications.
- What evidence would resolve it: Conducting user studies and evaluating the impact of different detection and mitigation measures on model performance and user experience, while maintaining a high level of security against jailbreak attacks.

## Limitations
- Analysis limited by relatively small set of 49 attack prompts across four tasks
- Property tests may have false positives or negatives, conflating task performance failures with actual jailbreak success
- Study does not explore long-term effects of jailbreak attempts or potential for models to adapt to repeated attacks

## Confidence
**High Confidence**: The taxonomy of jailbreak types and their categorization (cognitive hacking, instruction repetition, syntactical transformation) is well-grounded in existing literature and the experimental results support these classifications.

**Medium Confidence**: The comparative analysis of jailbreak effectiveness across different models (GPT-3.5, OPT, BLOOM, FLAN-T5-XXL) shows consistent patterns but the sample size of attack prompts is relatively small, limiting generalizability.

**Low Confidence**: The proposed defense mechanisms (batch prompt guards and checksum prompt guards) show limited effectiveness and lack thorough evaluation against diverse attack types.

## Next Checks
1. Expand Attack Prompt Diversity: Generate and test a larger, more diverse set of jailbreak prompts (target: 200+ unique prompts) across additional task types to better understand the boundaries of jailbreak effectiveness and identify any new attack categories.

2. Cross-Model Generalization Study: Test the most effective jailbreaks from the current study against additional LLM architectures (including newer models like GPT-4, Claude, and open-source alternatives) to validate whether effectiveness patterns hold across a broader model spectrum.

3. Defense Mechanism Robustness Testing: Implement and rigorously evaluate the proposed batch and checksum prompt guards against adaptive adversaries who modify their attacks to evade these specific defenses, including testing against obfuscated and multi-turn jailbreak attempts.