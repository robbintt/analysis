---
ver: rpa2
title: Critical Learning Periods Emerge Even in Deep Linear Networks
arxiv_id: '2308.12221'
source_url: https://arxiv.org/abs/2308.12221
tags:
- learning
- singular
- matrix
- task
- deficit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper demonstrates that critical learning periods emerge
  even in deep linear networks, challenging the assumption that such phenomena are
  unique to biological systems. The authors investigate two settings: multi-pathway
  networks and matrix completion tasks.'
---

# Critical Learning Periods Emerge Even in Deep Linear Networks

## Quick Facts
- arXiv ID: 2308.12221
- Source URL: https://arxiv.org/abs/2308.12221
- Reference count: 40
- Primary result: Critical learning periods emerge in deep linear networks, dependent on depth and data structure, not architecture

## Executive Summary
This paper demonstrates that critical learning periods - windows during training where deficits permanently affect learning - emerge even in deep linear networks. The authors show that temporary deficits early in training can permanently impair feature learning, with deeper networks being more sensitive to such deficits. They also show that pre-training on certain tasks can damage transfer performance, especially when the pre-training task has higher rank than the target task. These phenomena depend on network depth and data distribution structure rather than specific architectural details.

## Method Summary
The authors use deep linear networks as analytically tractable models to study critical learning periods. In the multi-pathway setting, they train networks with two parallel pathways on a synthetic hierarchical task, applying temporary gating deficits to one pathway during specific training epochs. In the matrix completion setting, they study transfer learning by pre-training on a high-rank matrix completion task and then switching to a lower-rank task. Both settings use SGD with constant learning rates and track singular value learning trajectories. The authors provide analytical solutions for the learning dynamics in both settings.

## Key Results
- Early deficits in multi-pathway networks permanently prevent certain pathways from learning features, while late deficits have negligible impact
- Pre-training on high-rank tasks damages transfer performance on lower-rank tasks, with deeper networks being more susceptible
- Critical periods depend on network depth and data distribution structure, not specific architectural or optimization details
- Analytical solutions for learning dynamics match empirical observations in both settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Critical learning periods emerge from the interplay between network depth and data distribution structure, independent of architectural or optimization details.
- Mechanism: As depth increases, the competition between pathways for learning features intensifies, making early deficits more impactful. This is modeled analytically using deep linear networks, where the learning dynamics of singular values depend on the depth of the network and the structure of the data distribution.
- Core assumption: The deep linear network accurately captures the essential learning dynamics of more complex networks.
- Evidence anchors:
  - [abstract]: "critical periods depend on the depth of the model and structure of the data distribution"
  - [section]: "We show that critical periods depend on the depth of the model and structure of the data distribution."
  - [corpus]: Weak evidence; no direct support from corpus neighbors.
- Break condition: If the core assumption about deep linear networks is violated, or if the data distribution does not follow the assumed structure.

### Mechanism 2
- Claim: Temporary deficits early in training permanently affect feature learning, while late deficits have negligible impact.
- Mechanism: Early deficits prevent certain pathways from learning features, leading to a "winner-take-all" dynamic where the non-deprived pathway learns all features. This is modeled by gating deficits in the multi-pathway setting.
- Core assumption: The learning dynamics of the deep linear network accurately reflect the behavior of more complex networks.
- Evidence anchors:
  - [section]: "Early deficits affect learned representations in multi-pathway model, while late deficits do not."
  - [section]: "In contrast, deficits during the middle of training...has only affects singular values that were not previously learned"
  - [corpus]: Weak evidence; no direct support from corpus neighbors.
- Break condition: If the assumption about the accuracy of the deep linear network model is incorrect, or if the deficit timing is not properly controlled.

### Mechanism 3
- Claim: Pre-training on a different task can damage transfer performance, especially in deeper networks.
- Mechanism: When pre-training on a task with a higher rank than the final task, the network learns features that are brittle and do not generalize well. This is modeled using matrix completion, where the rank of the initial task affects the ability to learn the final task.
- Core assumption: The matrix completion setup accurately captures the generalization dynamics of more complex networks.
- Evidence anchors:
  - [section]: "We show that pre-training on certain tasks can damage the transfer performance on new tasks."
  - [section]: "In the matrix completion setup, sensitivity to an initial deficit occurs when the initial matrix completion task is higher rank than the final matrix completion task."
  - [corpus]: Weak evidence; no direct support from corpus neighbors.
- Break condition: If the assumption about the accuracy of the matrix completion model is incorrect, or if the relationship between tasks is not properly controlled.

## Foundational Learning

- Concept: Deep Linear Networks
  - Why needed here: They provide an analytically tractable model to study critical learning periods without the complexities of nonlinearities.
  - Quick check question: What is the primary advantage of using deep linear networks to study critical learning periods?

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: SVD is used to analyze the learning dynamics of the network, particularly how singular values evolve during training.
  - Quick check question: How does the evolution of singular values relate to the learning of features in the network?

- Concept: Matrix Completion
  - Why needed here: It provides a framework to study generalization and transfer learning, where the rank of the initial task affects the ability to learn the final task.
  - Quick check question: How does the rank of the initial task influence the generalization performance on the final task?

## Architecture Onboarding

- Component map: Multi-pathway linear network -> Synthetic hierarchical task; Matrix completion setup -> Deep linear factorization
- Critical path: Initialize network -> Apply deficits -> Train network -> Analyze learning dynamics
- Design tradeoffs: Simplicity of deep linear networks vs. realism of complex networks
- Failure signatures: Inability to learn features, poor generalization, sensitivity to initial conditions
- First 3 experiments:
  1. Train a multi-pathway network with a temporary deficit early in training and observe the effect on feature learning.
  2. Train a network on a high-rank matrix completion task and then transfer to a low-rank task, observing the effect on generalization.
  3. Vary the depth of the network and observe how it affects the sensitivity to deficits and the ability to learn features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the fundamental mathematical properties that determine the emergence of critical learning periods in deep networks?
- Basis in paper: [explicit] The paper states that critical periods depend on the depth of the model and the structure of the data distribution.
- Why unresolved: While the paper provides insights into how depth and data distribution affect critical periods, it does not fully explain the underlying mathematical principles governing their emergence.
- What evidence would resolve it: A comprehensive mathematical framework that predicts critical periods based on network architecture and data characteristics.

### Open Question 2
- Question: How do critical learning periods in artificial networks compare to those in biological systems in terms of mechanisms and effects?
- Basis in paper: [explicit] The paper draws parallels between critical periods in biological and artificial networks but acknowledges differences in underlying mechanisms.
- Why unresolved: The paper establishes that critical periods exist in both systems but does not fully explore the similarities and differences in their mechanisms and effects.
- What evidence would resolve it: Comparative studies that investigate the specific mechanisms and effects of critical periods in both biological and artificial systems.

### Open Question 3
- Question: How do critical learning periods impact transfer learning and multimodal learning in deep networks?
- Basis in paper: [explicit] The paper discusses the effects of pre-training on transfer performance but does not fully explore the implications for transfer learning and multimodal learning.
- Why unresolved: The paper provides insights into how pre-training affects transfer performance but does not fully investigate the broader implications for transfer learning and multimodal learning.
- What evidence would resolve it: Experiments that systematically explore the effects of critical learning periods on transfer learning and multimodal learning in deep networks.

### Open Question 4
- Question: What are the optimal strategies for mitigating the negative effects of critical learning periods in deep networks?
- Basis in paper: [inferred] The paper discusses the impact of critical periods but does not provide strategies for mitigating their negative effects.
- Why unresolved: The paper identifies the existence and effects of critical periods but does not explore potential strategies for mitigating their negative impact on learning.
- What evidence would resolve it: Studies that investigate and compare different strategies for mitigating the negative effects of critical learning periods in deep networks.

## Limitations
- Deep linear networks may not fully capture the complexity of nonlinear neural networks
- Synthetic tasks used in experiments may not generalize to real-world scenarios
- The paper focuses on theoretical analysis rather than empirical validation in practical applications

## Confidence
Medium confidence in the core findings about critical periods in deep linear networks, but uncertainty remains about how well these results extend to more complex architectures and real-world tasks.

## Next Checks
1. Test whether the critical period effects persist when introducing small nonlinearities to the deep linear network architecture
2. Apply the matrix completion framework to real-world transfer learning tasks (e.g., pre-training on CIFAR-10 then fine-tuning on CIFAR-100) to validate the rank-based transfer limitations
3. Conduct ablation studies varying the depth, initialization scale, and deficit timing to map the full parameter space of critical period emergence