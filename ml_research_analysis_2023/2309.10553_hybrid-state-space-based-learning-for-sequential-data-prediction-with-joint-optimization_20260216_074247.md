---
ver: rpa2
title: Hybrid State Space-based Learning for Sequential Data Prediction with Joint
  Optimization
arxiv_id: '2309.10553'
source_url: https://arxiv.org/abs/2309.10553
tags:
- state
- time
- data
- prediction
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of nonlinear time series prediction
  in an online setting, where the goal is to predict the next value in a sequence
  based on past values and side information. The core method introduces a hybrid architecture
  that combines a Long Short-Term Memory (LSTM) network with a Seasonal AutoRegressive
  Integrated Moving Average with eXogenous regressors (SARIMAX) model, both jointly
  optimized through a novel state space formulation and particle filtering.
---

# Hybrid State Space-based Learning for Sequential Data Prediction with Joint Optimization

## Quick Facts
- arXiv ID: 2309.10553
- Source URL: https://arxiv.org/abs/2309.10553
- Reference count: 40
- Primary result: Hybrid LSTM-SARIMAX model achieves superior performance across multiple time series datasets by jointly optimizing both components through state space formulation and particle filtering

## Executive Summary
This paper addresses the challenge of nonlinear time series prediction by introducing a hybrid model that combines LSTM and SARIMAX architectures within a unified state space framework. The key innovation is the joint optimization of both models using particle filtering, which enables simultaneous learning of parameters while leveraging the complementary strengths of each component. The LSTM handles nonlinear feature extraction from raw data, while SARIMAX captures linear time series characteristics like seasonality and trends. The approach demonstrates significant improvements over standalone models across diverse datasets including the M4 competition, real-world engineering datasets, and financial time series.

## Method Summary
The hybrid model jointly optimizes an LSTM for nonlinear feature extraction and a SARIMAX model for linear time series components through a unified state space formulation. Both models are embedded into a single state space representation where their parameters and states are concatenated into a joint state vector. Particle filtering is employed to train this hybrid model online, allowing for parameter updates even when some components are unobserved. The approach aims to overcome the limitations of disjoint training in ensemble models by enabling information flow between components during optimization, resulting in superior prediction performance.

## Key Results
- Hybrid model consistently outperforms standalone LSTM, SARIMAX, MLP, LightGBM, and Naive models across M4 competition datasets (yearly, daily, hourly)
- Significant performance gains demonstrated on real-world datasets including Bank, Elevators, Kinematics, and Pumadyn
- Hybrid approach shows superior results on financial time series (Alcoa stock prices) compared to all baseline models
- Model achieves better performance in both online and offline settings

## Why This Works (Mechanism)

### Mechanism 1
Joint state space formulation enables simultaneous learning of LSTM and SARIMAX parameters, avoiding suboptimal disjoint training of ensemble models. By embedding both models into a unified state space with shared state vector, particle filtering optimizes all parameters in a single pass, leveraging cross-model information flow during training.

### Mechanism 2
Particle filtering effectively handles nonlinear state transitions and parameter estimation in the hybrid model, which gradient-based methods cannot address due to unobserved components. The approach approximates posterior distribution using weighted particles, allowing online parameter updates even when some components are unobserved.

### Mechanism 3
Hybrid architecture leverages complementary strengths of LSTM (nonlinear feature extraction) and SARIMAX (linear trend/seasonality modeling), leading to superior performance. LSTM learns complex nonlinear patterns while SARIMAX captures linear time series characteristics, with joint optimization ensuring models benefit from each other's strengths.

## Foundational Learning

- **State Space Models**: Essential for expressing both LSTM and SARIMAX in compatible form to enable joint optimization. Quick check: Can you write state space equations for a simple AR(1) model and explain the state vector role?

- **Particle Filtering**: Used for online training of hybrid model, handling nonlinear state transitions and parameter estimation. Quick check: What is the role of importance distribution in particle filtering, and how does it relate to true posterior distribution?

- **LSTM and SARIMAX Architectures**: Understanding individual strengths and limitations is crucial for appreciating hybrid design. Quick check: What are key differences between LSTM and SARIMAX in modeling nonlinear patterns versus time series characteristics?

## Architecture Onboarding

- **Component map**: LSTM (nonlinear feature extraction) -> Joint State Space (unified representation) -> SARIMAX (linear time series modeling) -> Particle Filter (online optimization)

- **Critical path**:
  1. Initialize particles for joint state vector
  2. For each time step: propagate particles through state transitions
  3. Compute particle weights based on observation likelihood
  4. Resample particles if necessary
  5. Update state estimate as weighted average
  6. Generate prediction using estimated state

- **Design tradeoffs**: Joint vs. disjoint optimization (better performance vs. increased complexity), particle filter parameters (accuracy vs. efficiency), LSTM variant selection (impact on nonlinear pattern capture)

- **Failure signatures**: Particle filter degeneracy (frequent effective sample size drops), overfitting (LSTM memorizing training data), underfitting (SARIMAX missing linear patterns)

- **First 3 experiments**:
  1. Implement separate state space formulations for LSTM and SARIMAX, verify correctness
  2. Implement particle filter for joint state space, test on synthetic dataset
  3. Compare hybrid model with standalone LSTM and SARIMAX on real-world time series

## Open Questions the Paper Calls Out

- How does the hybrid model perform in highly non-stationary time series with sudden regime shifts compared to pure models? The paper doesn't extensively test performance under sudden regime shifts or analyze cumulative error over time.

- What is the impact of particle filter resampling frequency on performance and computational efficiency? The paper mentions resampling but doesn't explore the trade-off between resampling frequency and model accuracy or computational cost.

- How does performance scale with increasing dimensionality of exogenous variables? The paper uses varying feature dimensions but doesn't systematically analyze how model performance changes as exogenous variable dimensionality increases.

## Limitations

- Computational complexity of particle filtering may become prohibitive for high-dimensional state spaces or large datasets
- Limited ablation studies to isolate contribution of joint optimization versus individual components
- Reliance on assumption that datasets contain both nonlinear patterns and linear time series characteristics
- Experimental validation lacks rigorous statistical significance testing between model comparisons

## Confidence

- **High Confidence**: Conceptual framework of combining LSTM and SARIMAX through state space formulation is sound and well-motivated
- **Medium Confidence**: Experimental results showing performance improvements appear robust, though statistical significance is not rigorously established
- **Low Confidence**: Specific implementation details of joint state space formulation and particle filter algorithm remain unclear

## Next Checks

1. Implement ablation study comparing hybrid model with disjoint optimization variants and standalone models with identical architectures
2. Conduct computational benchmarking to measure runtime and memory requirements across different dataset sizes and state space dimensions
3. Perform statistical significance testing (e.g., paired t-tests or bootstrap confidence intervals) on experimental results to verify performance improvements are not due to random variation