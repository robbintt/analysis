---
ver: rpa2
title: Novelty Accommodating Multi-Agent Planning in High Fidelity Simulated Open
  World
arxiv_id: '2306.12654'
source_url: https://arxiv.org/abs/2306.12654
tags:
- novelty
- agent
- environment
- enemy
- range
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that the Hydra novelty reasoning framework
  can be adapted to operate within a high-fidelity military simulation environment.
  The agent successfully detects and characterizes a range-extension novelty in enemy
  SAM missiles and adjusts its internal planning model to avoid the newly exposed
  threat, completing the mission with a 95% post-novelty win rate compared to 79%
  for the baseline agent.
---

# Novelty Accommodating Multi-Agent Planning in High Fidelity Simulated Open World

## Quick Facts
- arXiv ID: 2306.12654
- Source URL: https://arxiv.org/abs/2306.12654
- Reference count: 28
- Primary result: Hydra framework achieves 95% post-novelty win rate vs 79% baseline in military SAM range extension scenario

## Executive Summary
This paper demonstrates the adaptation of the Hydra novelty reasoning framework to a high-fidelity military simulation environment. The agent successfully detects and characterizes a novelty in enemy SAM missile range extension, repairs its internal planning model using PDDL+ consistency checking and MMO-based repairs, and completes the mission with significantly improved performance over baseline. The work bridges theoretical novelty accommodation with practical military simulation, showing that model-based reasoning can maintain plan validity under changing conditions.

## Method Summary
The approach adapts Hydra to operate within AFSIM military simulator by discretizing continuous space into a grid and actions into five cardinal moves. A PDDL+ model is generated from initial observations, and consistency checking computes Euclidean distance between expected and observed trajectories. When this exceeds threshold T, Model Manipulation Operators (MMOs) adjust numeric model variables to repair the model. The execution engine translates discrete PDDL+ plans back to continuous simulator commands.

## Key Results
- 95% post-novelty win rate versus 79% baseline
- 95% novelty detection rate compared to 0% baseline
- Successful accommodation of SAM range extension from 40,000 to 90,000 meters
- Model repair via MMOs maintains plan validity under changing conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The PDDL+ model is automatically repaired via MMOs when novelty disrupts expected trajectories.
- Mechanism: Consistency checking computes Euclidean distance between expected and observed trajectories; when it exceeds threshold T, MMOs adjust numeric model variables (e.g., enemy SAM range) to realign model predictions with reality.
- Core assumption: The introduced novelty is within the set of variable types the MMOs can modify (e.g., numeric ranges).
- Evidence anchors:
  - [abstract] "The approach leverages PDDL+ model consistency checking and targeted MMO-based repairs to maintain plan validity under changing conditions."
  - [section 4] "The consistency checking ùê∂ ‚àà R‚â•0 is a calculated number representing how accurate the model M represents the realistic high fidelity simulator transition system ùê∏."
  - [corpus] Weak evidence: no corpus papers explicitly describe MMO-based repair, but they discuss similar novelty accommodation methods.
- Break condition: If novelty changes a model aspect outside MMO scope (e.g., qualitative rule changes), repair fails.

### Mechanism 2
- Claim: High-fidelity military simulation environment is abstracted into a discretized grid for PDDL+ planning while retaining essential dynamics.
- Mechanism: Continuous actions, states, and time are discretized into grid cells, five cardinal-move actions, and fixed time steps, then translated back via an execution engine to continuous simulator commands.
- Core assumption: Discretization preserves sufficient fidelity for plan success and novelty detection.
- Evidence anchors:
  - [section 5.2] "The continuous mission area space is discretized into a grid space... The infinitely large and continuous action space is discretized into five actions: move in four directions one cell and fire the JDAM..."
  - [section 4] "In order to utilize the Hydra novelty detection... it is required to define a PDDL+ model and search for a plan to execute in the pre-novelty environment E."
  - [corpus] Weak evidence: other novelty papers mention discretization but not specifically for military SAM missile environments.
- Break condition: If discretization introduces excessive path deviations or misses critical continuous-time constraints, the plan becomes infeasible.

### Mechanism 3
- Claim: Novelty is injected as a parameter change (SAM range increase) that breaks the agent's pre-existing model assumptions.
- Mechanism: At a predetermined battle index, the SAM weapon range variable in the environment is increased beyond the agent's internal model range; consistency checking flags the mismatch.
- Core assumption: Novelty is introduced in a controlled, predictable way and the agent's consistency threshold is tuned to detect it.
- Evidence anchors:
  - [section 5.3] "The introduced novelty are changes to the value of the enemy SAM range, from 40,000 meters to 90,000 meters."
  - [section 6.1] "During the mission, we consider the novelty which extends the effective range of enemy SAM missiles."
  - [corpus] Moderate evidence: papers describe similar parameter-change novelty injection but not in this specific military context.
- Break condition: If novelty is introduced too subtly to cross the consistency threshold, detection fails.

## Foundational Learning

- Concept: PDDL+ planning domain modeling
  - Why needed here: The Hydra framework relies on PDDL+ to represent mixed discrete-continuous systems and detect novelty via consistency checking.
  - Quick check question: What distinguishes PDDL+ from classic PDDL in handling continuous dynamics?

- Concept: Novelty detection via trajectory consistency
  - Why needed here: The agent must identify when environment changes invalidate its plan by comparing predicted vs. observed state trajectories.
  - Quick check question: How is the consistency threshold T chosen and what trade-offs affect false positives vs. false negatives?

- Concept: Model Manipulation Operators (MMOs) and meta-model repair
  - Why needed here: Repairs to the agent's internal model must be expressible as incremental changes to numeric variables for the search-based repair algorithm to succeed.
  - Quick check question: What limits the expressiveness of MMOs, and how does that constrain the types of novelty that can be accommodated?

## Architecture Onboarding

- Component map: PDDL+ Model Generator ‚Üí PDDL+ Planner ‚Üí Execution Engine ‚Üí AFSIM Simulator ‚Üí Consistency Checker ‚Üí Repair Module ‚Üí Updated Model ‚Üí (loop back to Planner)
- Critical path: Model generation ‚Üí planning ‚Üí execution ‚Üí consistency check ‚Üí repair ‚Üí replanning; each step must complete before the next to maintain mission continuity.
- Design tradeoffs: Discretizing continuous space simplifies planning but risks loss of tactical precision; higher resolution increases planning time and complexity.
- Failure signatures: Consistency score remains above threshold after repair attempts; plan length increases dramatically; execution engine fails to translate PDDL+ plan back to simulator actions.
- First 3 experiments:
  1. Run a pre-novelty baseline mission to verify baseline win rate and plan feasibility.
  2. Inject SAM range novelty and confirm detection threshold triggers repair after mission failure.
  3. Validate that repaired model yields successful post-novelty mission completion and measure improvement in win rate.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the Hydra framework perform in real-time applications where the novelty detection and accommodation process needs to happen within milliseconds rather than the relatively slower PDDL+ planning approach?
  - Basis in paper: [explicit] The paper mentions that PDDL+ planning involves discretization and that real-time applications would require translation between continuous and discrete variables.
  - Why unresolved: The paper only demonstrates Hydra's performance in a military simulation environment, not in real-time operational settings. The time complexity of novelty detection and accommodation in high-stakes, time-sensitive scenarios remains unknown.
  - What evidence would resolve it: Comparative studies of Hydra's performance in real-time military operations versus simulated environments, including response time measurements and success rates.

- **Open Question 2**: Can the model repair mechanism scale to handle multiple simultaneous novelties across different domains of the environment without becoming computationally intractable?
  - Basis in paper: [inferred] The paper shows successful handling of one novelty (SAM range extension) but doesn't address scenarios with multiple concurrent novelties or complex interdependencies between different types of novelties.
  - Why unresolved: The repair algorithm presented is domain-independent but was only tested on a single-novelty scenario. The scalability of the search-based repair mechanism when multiple novelties occur simultaneously is unknown.
  - What evidence would resolve it: Experiments with multi-novelty scenarios showing the success rate, computational time, and model accuracy of the repair mechanism when handling multiple simultaneous novelties.

- **Open Question 3**: How robust is the novelty detection threshold T across different types of environments and domains, and what is the optimal method for setting this threshold automatically?
  - Basis in paper: [explicit] The paper mentions that "The threshold is a hyperparameter that can be 'consistency shaped' based on domain knowledge" but doesn't provide guidance on how to set it.
  - Why unresolved: The paper treats the threshold as a domain-specific hyperparameter but doesn't explore how sensitive the system is to different threshold values or how to determine optimal thresholds across domains.
  - What evidence would resolve it: Systematic studies showing Hydra's performance across various threshold values and domains, including analysis of false positive/negative rates at different threshold settings.

- **Open Question 4**: What is the long-term learning capability of Hydra - can it retain knowledge about previously encountered novelties to improve future performance?
  - Basis in paper: [inferred] The paper demonstrates accommodation of a novelty within a single campaign but doesn't address whether the agent learns from past experiences or maintains a memory of encountered novelties.
  - Why unresolved: The current implementation appears to treat each novelty encounter independently without evidence of cumulative learning or knowledge retention across multiple campaigns or missions.
  - What evidence would resolve it: Experiments tracking agent performance across multiple campaigns where the same novelty type reappears, demonstrating whether accommodation becomes faster or more efficient with repeated exposure.

## Limitations

- Novelty injection is highly controlled and predictable, with single parameter change within MMO repair scope
- Discretization of continuous space may introduce fidelity loss not explicitly quantified
- Consistency threshold and MMO repair parameters are tuned for specific scenario, raising generalization concerns

## Confidence

- **High**: Basic framework integration and consistency checking implementation
- **Medium**: Post-novelty win rate and novelty detection metrics
- **Low**: Generalization to uncontrolled, diverse novelty types

## Next Checks

1. **Uncontrolled Novelty Testing**: Remove predetermined novelty injection timing and instead introduce random novelty types (qualitative changes, new obstacles, environmental effects) to evaluate robustness.
2. **Discretization Fidelity Analysis**: Systematically vary grid resolution and measure impact on plan feasibility and tactical effectiveness, particularly for time-critical maneuvers.
3. **Real-Time Performance Validation**: Measure computation time for consistency checking, MMO repair, and replanning during execution to ensure the approach scales to longer missions with multiple novelty events.