---
ver: rpa2
title: Using Global Land Cover Product as Prompt for Cropland Mapping via Visual Foundation
  Model
arxiv_id: '2310.10219'
source_url: https://arxiv.org/abs/2310.10219
tags:
- cropland
- prompt
- scenes
- foundation
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of cropland mapping in remote
  sensing, where traditional deep learning methods struggle with the complex domain
  gaps caused by diverse cropland attributes and imaging conditions. The authors propose
  a novel approach that leverages the "Pretrain+Prompting" paradigm via visual foundation
  models, specifically using the Segment Anything Model (SAM) and global land cover
  (GLC) products.
---

# Using Global Land Cover Product as Prompt for Cropland Mapping via Visual Foundation Model

## Quick Facts
- arXiv ID: 2310.10219
- Source URL: https://arxiv.org/abs/2310.10219
- Authors: 
- Reference count: 21
- Primary result: Auto-prompting method using GLC products achieves 5-20% improvement in OA, MIoU, and F1-score compared to supervised learning and fine-tuning approaches

## Executive Summary
This study addresses the challenge of cropland mapping in remote sensing, where traditional deep learning methods struggle with the complex domain gaps caused by diverse cropland attributes and imaging conditions. The authors propose a novel approach that leverages the "Pretrain+Prompting" paradigm via visual foundation models, specifically using the Segment Anything Model (SAM) and global land cover (GLC) products. Their method, called auto-prompting (APT), automatically generates prompts from GLC data to guide the model's reasoning process for each target sample, eliminating the need for additional labeled data. Experiments on two sub-meter-scale cropland datasets from southern and northern China demonstrate that APT outperforms traditional supervised learning and fine-tuning approaches, achieving significant improvements in overall accuracy (OA), mean Intersection over Union (MIoU), and F1-score. The proposed method simplifies domain adaptation and provides a cost-effective solution for large-scale cropland mapping.

## Method Summary
The study employs the Segment Anything Model (SAM) as a visual foundation model and introduces an auto-prompting (APT) method that uses global land cover (GLC) products to generate prompts for cropland mapping. The method involves three main steps: (1) obtaining cropland extent pre-labels from GLC products corresponding to target images, (2) generating positive and negative prompt points uniformly based on cropland and non-cropland proportions, and (3) inputting these prompts along with images into SAM's prompt encoder and image encoder to generate cropland probability maps. The approach is tested on two sub-meter-scale cropland datasets from southern (Hunan) and northern (Gansu) China, each containing 5,000 images (512×512) labeled as cropland and non-cropland.

## Key Results
- Auto-prompting (APT) method achieves 5-20% improvement in overall accuracy (OA) compared to supervised learning and fine-tuning approaches
- Mean Intersection over Union (MIoU) and F1-score also show significant improvements with APT
- The method successfully handles domain gaps between southern and northern China cropland datasets
- APT eliminates the need for additional labeled data by using GLC products as prompt sources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Auto-prompting transforms the domain adaptation problem from adjusting to entire datasets into adjusting to individual samples, simplifying complex feature distribution gaps.
- Mechanism: Instead of using sparse labeled samples to train the model for all cropland scenes, the method generates prompt points from global land cover (GLC) products for each individual image. These prompts guide the vision foundation model's reasoning process for that specific sample, bypassing the need to represent the entire target distribution.
- Core assumption: Prompt information, even if imperfect, is sufficient to guide the model's reasoning when combined with the strong feature extraction capabilities of the pretrained vision foundation model.
- Evidence anchors:
  - [abstract] "The 'Pretrain+Prompting' paradigm redesigns the optimization target by introducing individual prompts for each single sample."
  - [section] "Instead of using sparsely labeled samples that forces the model to adapt to complex cropland scenes, we utilize the prompts to guide the model's reasoning process for each specific target sample."
  - [corpus] Weak - no direct corpus evidence for this specific prompting mechanism in cropland mapping.
- Break condition: If prompt quality degrades significantly (e.g., incorrect land cover labels in GLC product), or if the pretrained model lacks sufficient generalization ability, the adaptation process will fail.

### Mechanism 2
- Claim: The vision foundation model's pretraining on massive data provides general feature representations that are robust to domain gaps caused by diverse cropland attributes and imaging conditions.
- Mechanism: The pretrained vision foundation model (SAM) has learned general visual representations from large-scale pretraining. These representations contain sufficient knowledge to handle diverse cropland scenes without requiring extensive fine-tuning on target data.
- Core assumption: The vision foundation model's pretraining captures sufficiently general features that are transferable to diverse cropland scenes.
- Evidence anchors:
  - [abstract] "the 'Pretrain+Prompting' paradigm redesigns the optimization target by introducing individual prompts for each single sample."
  - [section] "Although the prompt information we employed is vague and may contain errors, the vision foundation model benefits from the pretraining process with random noise and demonstrates certain robustness."
  - [corpus] Weak - no direct corpus evidence for this specific vision foundation model application in cropland mapping.
- Break condition: If the pretrained model's general representations are too generic and fail to capture cropland-specific features, or if the domain gap is too large for the model to bridge with prompts alone.

### Mechanism 3
- Claim: Using global land cover (GLC) products as prompt sources eliminates the need for additional labeled data, making large-scale cropland mapping cost-effective.
- Mechanism: The method automatically generates prompt points from the cropland layer in GLC products, which are freely available and geographically aligned with target images. This provides prior information for guiding the model without requiring manual labeling.
- Core assumption: GLC products provide accurate enough cropland extent information to serve as effective prompts for guiding the vision foundation model.
- Evidence anchors:
  - [abstract] "we introduce the 'Pretrain+Prompting' paradigm to interpreting cropland scenes and design the auto-prompting (APT) method based on freely available global land cover product."
  - [section] "we take the freely available global land cover (GLC) products as the source of prompt information, which can easily obtaioned the prompts for every cropland scenes."
  - [corpus] Weak - no direct corpus evidence for this specific GLC product application in cropland mapping.
- Break condition: If GLC products have poor spatial resolution, temporal mismatch, or significant classification errors, the generated prompts will mislead the model.

## Foundational Learning

- Concept: Domain Adaptation
  - Why needed here: Cropland mapping faces complex domain gaps due to diverse cropland attributes (topography, climate, crop type) and imaging conditions (viewing angle, illumination, scale). Models trained on specific scenes struggle to generalize to others.
  - Quick check question: What is the primary challenge in applying a cropland mapping model trained on southern China data to northern China data?

- Concept: Prompt Learning
  - Why needed here: Traditional fine-tuning requires labeled samples to adapt pretrained models to target domains. Prompt learning guides the model's reasoning process for each sample using additional information, simplifying domain adaptation.
  - Quick check question: How does prompt learning differ from traditional fine-tuning in terms of the adaptation target?

- Concept: Vision Foundation Models
  - Why needed here: These models are pretrained on massive datasets and capture general visual representations. They provide strong feature extraction capabilities that can be adapted to specific tasks like cropland mapping through prompting.
  - Quick check question: What is the key advantage of using a vision foundation model like SAM for cropland mapping compared to training from scratch?

## Architecture Onboarding

- Component map: Input image → Image encoder (from SAM) → Prompt encoder → Mask decoder → Cropland segmentation output. GLC product provides cropland extent pre-labels for prompt generation.
- Critical path: GLC product → Cropland extent extraction → Prompt point generation → Prompt encoding → Image encoding → Mask decoding → Segmentation output.
- Design tradeoffs: Using GLC products for prompts eliminates labeling costs but introduces potential errors from outdated or inaccurate land cover data. The approach trades perfect prompt accuracy for cost-effectiveness and scalability.
- Failure signatures: Poor segmentation results on cropland edges, over/under-segmentation of cropland extent, failure to distinguish cropland from similar land cover types.
- First 3 experiments:
  1. Validate prompt generation: Test automatic prompt point generation from GLC products on sample images and visually inspect cropland extent alignment.
  2. Compare with fine-tuning: Run fine-tuning experiments with varying numbers of labeled samples (5, 10, 20, 50) and compare performance against auto-prompting with GLC-derived prompts.
  3. Ablation study: Test the method with and without GLC-derived prompts to quantify the contribution of prompt information to segmentation accuracy.

## Open Questions the Paper Calls Out
No open questions were explicitly called out in the paper.

## Limitations
- The method's effectiveness depends heavily on the quality and spatial resolution of global land cover products, which may not capture fine-scale cropland boundaries in sub-meter imagery
- The paper lacks detailed implementation specifications for prompt generation and distribution, making exact replication challenging
- The evaluation is limited to two geographically distinct regions in China, raising questions about generalization to other global cropland systems

## Confidence
- High Confidence: The core mechanism of using prompts from GLC products to guide vision foundation models is technically sound and well-supported by the experimental results
- Medium Confidence: The claim that this approach simplifies domain adaptation is reasonable but requires more diverse geographic testing to validate fully
- Medium Confidence: The cost-effectiveness claim is valid but depends on the availability and quality of GLC products in different regions

## Next Checks
1. Test the method on cropland datasets from different continents and agro-ecological zones to assess geographic generalization
2. Conduct a systematic evaluation of how GLC product resolution and classification accuracy affect segmentation performance
3. Compare the auto-prompting approach against state-of-the-art domain adaptation techniques specifically designed for remote sensing applications