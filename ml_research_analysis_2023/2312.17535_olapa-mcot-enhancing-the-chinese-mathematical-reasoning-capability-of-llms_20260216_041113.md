---
ver: rpa2
title: 'Olapa-MCoT: Enhancing the Chinese Mathematical Reasoning Capability of LLMs'
arxiv_id: '2312.17535'
source_url: https://arxiv.org/abs/2312.17535
tags:
- reasoning
- arxiv
- chinese
- mathematical
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Olapa-MCoT, a specialized Chinese mathematical\
  \ reasoning large language model based on llama2-13B. To enhance the model\u2019\
  s reasoning capability, the authors proposed SimRRHF, a stable and efficient alignment\
  \ method that combines ranking loss, SFT loss, and similarity loss, and introduced\
  \ Incorrect Data Relearning (IDRL) to strengthen learning of difficult knowledge."
---

# Olapa-MCoT: Enhancing the Chinese Mathematical Reasoning Capability of LLMs

## Quick Facts
- arXiv ID: 2312.17535
- Source URL: https://arxiv.org/abs/2312.17535
- Reference count: 40
- Key outcome: Olapa-MCoT achieves 50% accuracy on Chinese mathematical reasoning tasks, a 36% improvement over the base llama2-13B model

## Executive Summary
This paper introduces Olapa-MCoT, a specialized Chinese mathematical reasoning large language model based on llama2-13B. To enhance the model's reasoning capability, the authors proposed SimRRHF, a stable and efficient alignment method that combines ranking loss, SFT loss, and similarity loss, and introduced Incorrect Data Relearning (IDRL) to strengthen learning of difficult knowledge. Olapa-MCoT was fine-tuned using 367K Chinese math samples constructed automatically and aligned with human feedback. Experimental results show that Olapa-MCoT achieves 50% accuracy on Chinese mathematical reasoning tasks, a 36% improvement over the base llama2-13B model, and also improves English reasoning accuracy by nearly 4%.

## Method Summary
Olapa-MCoT employs QLoRA for efficient fine-tuning of llama2-13B using a dataset of 367K Chinese mathematical reasoning samples. The model is first fine-tuned using supervised learning, then aligned using the SimRRHF method which combines ranking loss, SFT loss, and similarity loss to maintain stability during training. The Incorrect Data Relearning (IDRL) technique is applied to focus learning on previously misclassified samples. The model is evaluated on both Chinese and English mathematical reasoning tasks using custom test sets.

## Key Results
- Olapa-MCoT achieves 50% accuracy on Chinese mathematical reasoning tasks
- The model improves upon llama2-13B baseline by 36% in Chinese reasoning
- English reasoning accuracy improves by nearly 4% compared to baseline
- SimRRHF method demonstrates increased stability and convergence speed
- IDRL technique further boosts reasoning performance through focused learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SimRRHF stabilizes alignment training by combining ranking loss, SFT loss, and similarity loss, avoiding the instability of PPO-based RLHF
- Mechanism: The method scores responses with a reward model, uses ranking loss to align the model's output with the highest-rated response, applies SFT loss to match the top-rated response's conditional log probability, and adds similarity loss to maintain semantic distance from the reference model, preventing performance drift
- Core assumption: Maintaining semantic similarity to a reference model constrains the finetuned model from deviating too far from the base model's distribution
- Evidence anchors:
  - [abstract] "SimRRHF method increases stability and convergence speed"
  - [section 3.2.1] "we proposed the SimRRHF method in the alignment stage, and introduced similarity loss to constrain the performance of current finetuning model not to deivate from the best object"
  - [corpus] No direct corpus evidence found for this specific method; claims are based on internal comparisons
- Break condition: If the reward model is poorly calibrated or the similarity loss weight is set too high, the model may converge to a suboptimal solution or fail to improve reasoning capability

### Mechanism 2
- Claim: IDRL (Incorrect Data Relearning) improves learning of difficult knowledge by focusing on previously misclassified samples
- Mechanism: The method collects samples where the model previously generated incorrect inferences, combines them with new samples, and trains the model on this augmented dataset in subsequent rounds
- Core assumption: Human learning patterns of repeatedly practicing difficult knowledge apply to LLMs, and focusing training on error-prone samples improves overall performance
- Evidence anchors:
  - [abstract] "introduced Incorrect Data Relearning (IDRL) to strengthen learning of difficult knowledge"
  - [section 3.2.2] "Inspired by this method, during the alignment learning, we introduced Incorrect Data Relearning method. We collected data where the model made incorrect inferences in the training dataset"
  - [corpus] No direct corpus evidence found for this specific approach; claims are based on internal experimental results
- Break condition: If the incorrect data pool becomes too large or unrepresentative, or if new samples are not sufficiently diverse, the model may overfit to specific error patterns without generalizing

### Mechanism 3
- Claim: QLoRA enables efficient fine-tuning of a 13B parameter model on limited GPU resources
- Mechanism: QLoRA uses 4-bit quantization to reduce model memory footprint, allowing fine-tuning of large models on consumer GPUs while maintaining performance
- Core assumption: 4-bit quantization preserves sufficient model capacity for task-specific fine-tuning without significant accuracy loss
- Evidence anchors:
  - [section 3.1] "we adopt the QLoRA [10] method" and "Finetuning is a process with a high demand of GPUs"
  - [section 4.1.2] "All the experiments were conducted using QLoRA [10]"
  - [corpus] QLoRA is a well-established method; the paper applies it as expected without novel claims about its mechanism
- Break condition: If the quantization is too aggressive or the model requires fine-tuning on very large context windows, performance degradation may occur

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: The paper builds on CoT to improve mathematical reasoning by generating intermediate reasoning steps
  - Quick check question: What is the difference between Zero-Shot-CoT and Manual-CoT prompting?

- Concept: Reward modeling for alignment
  - Why needed here: The SimRRHF method uses a reward model to score responses, which is fundamental to understanding the alignment approach
  - Quick check question: How does a Process-supervised Reward Model (PRM) differ from an Outcome-supervised Reward Model (ORM)?

- Concept: Data construction for specialized tasks
  - Why needed here: The paper automatically constructs Chinese mathematical reasoning datasets using LLMs and filtering, which is critical to the approach
  - Quick check question: What are the key steps in the automatic construction of MCoT samples described in the paper?

## Architecture Onboarding

- Component map: llama2-13B base model -> QLoRA fine-tuning -> SimRRHF alignment -> IDRL iterations -> evaluation
- Critical path: Data construction → SFT fine-tuning → SimRRHF alignment → IDRL iterations → evaluation
- Design tradeoffs: The paper trades computational efficiency (QLoRA) for model size, and uses automatic data construction to avoid manual labeling costs, but this may introduce noise
- Failure signatures: Poor reward model calibration, insufficient diversity in constructed data, or incorrect hyperparameter settings in SimRRHF can all lead to suboptimal performance
- First 3 experiments:
  1. Run the data construction pipeline on a small subset of ape210K to verify the LLM generation and filtering steps work correctly
  2. Perform a single epoch of QLoRA fine-tuning on the constructed dataset to ensure the quantization and training pipeline function
  3. Execute a single step of SimRRHF training to verify the loss computation and gradient updates work as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can theoretical knowledge such as basic operation rules be integrated to improve the reasoning logic and stability of LLMs?
- Basis in paper: [explicit] The authors mention that "many basic operation rules are often calculated inaccurately" and plan to explore integrating theoretical knowledge in future work.
- Why unresolved: This requires developing new methodologies to incorporate domain-specific knowledge into the model's training and inference processes.
- What evidence would resolve it: Successful implementation and evaluation of a model that demonstrates improved accuracy in mathematical operations and reasoning compared to existing models.

### Open Question 2
- Question: What is the optimal balance between incorrect data relearning and new sample introduction in the IDRL method?
- Basis in paper: [inferred] The authors introduce IDRL but don't specify the optimal ratio of incorrect to new data for maximum effectiveness.
- Why unresolved: The effectiveness of IDRL likely depends on the specific task and data distribution, requiring further experimentation to determine optimal parameters.
- What evidence would resolve it: Systematic studies varying the ratio of incorrect to new data and measuring the resulting model performance across different tasks and data distributions.

### Open Question 3
- Question: How does the SimRRHF method perform compared to RLHF on other reasoning tasks beyond mathematics?
- Basis in paper: [explicit] The authors propose SimRRHF as an alternative to RLHF for mathematical reasoning but don't test it on other domains.
- Why unresolved: The effectiveness of the method may vary depending on the nature of the task and the type of human feedback required.
- What evidence would resolve it: Comparative studies applying SimRRHF and RLHF to various reasoning tasks (e.g., logical reasoning, commonsense reasoning) and measuring their relative performance.

## Limitations

- The evaluation framework uses only 100 test samples for Chinese reasoning, which is a small sample size that may not be representative of the broader task
- The paper doesn't report statistical significance tests for the claimed improvements, making it unclear whether the 36% improvement over baseline is robust
- The comparison only includes llama2-13B without baselines from other specialized mathematical models like WizardMath or MetaMath

## Confidence

- High confidence: The general approach of using QLoRA for efficient fine-tuning and the concept of combining multiple loss functions for alignment are well-established and technically sound
- Medium confidence: The claimed 50% accuracy on Chinese mathematical reasoning and 4% improvement on English tasks are plausible given the dataset size and approach, but the small evaluation set and lack of significance testing reduce confidence in these specific numbers
- Low confidence: The specific implementation details of SimRRHF and IDRL are underspecified, making it difficult to verify whether the reported results can be faithfully reproduced

## Next Checks

1. **Statistical validation check**: Re-run the evaluation on the 100 Chinese test samples 10 times with different random seeds and compute confidence intervals for the 50% accuracy claim to assess robustness

2. **Ablation study**: Train two additional models - one with only SimRRHF (no IDRL) and one with only IDRL (no SimRRHF) - to isolate the contribution of each method to the overall performance improvement

3. **Baselines comparison**: Evaluate Olapa-MCoT against at least two other specialized mathematical reasoning models (e.g., WizardMath, MetaMath) on the same 100-sample test set to establish relative performance in the field