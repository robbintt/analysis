---
ver: rpa2
title: Semi-supervised News Discourse Profiling with Contrastive Learning
arxiv_id: '2309.11692'
source_url: https://arxiv.org/abs/2309.11692
tags:
- learning
- news
- contrastive
- sentence
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Intra-document Contrastive Learning with
  Distillation (ICLD), a semi-supervised approach for news discourse profiling that
  addresses the challenge of limited human-annotated data. The method uses a teacher
  model to generate silver labels for unlabeled news articles, then applies intra-document
  contrastive learning and knowledge distillation to train student models.
---

# Semi-supervised News Discourse Profiling with Contrastive Learning

## Quick Facts
- arXiv ID: 2309.11692
- Source URL: https://arxiv.org/abs/2309.11692
- Reference count: 24
- Primary result: ICLD improves macro F1 by 4.9% and micro F1 by 2.3% for news discourse profiling

## Executive Summary
This paper introduces Intra-document Contrastive Learning with Distillation (ICLD), a semi-supervised approach for news discourse profiling that addresses the challenge of limited human-annotated data. The method uses a teacher model to generate silver labels for unlabeled news articles, then applies intra-document contrastive learning and knowledge distillation to train student models. The contrastive learning captures document-level event structures rather than just sentence-level semantics, while knowledge distillation prevents the model from collapsing into standard semantic similarity learning. Evaluation shows the ICLD method improves macro F1 score by 4.9% and micro F1 score by 2.3% compared to baseline models trained only on human-annotated data, demonstrating significant performance gains in this task.

## Method Summary
ICLD operates in two phases: first, a T5-large teacher model generates silver labels for unlabeled news articles by predicting discourse roles for each sentence. These silver labels are then used to train a student model (Longformer or RoBERTa-base) using intra-document contrastive learning, where sentence pairs within the same document are sampled based on their predicted discourse categories. Knowledge distillation ensures the model learns discourse role classifications rather than just semantic similarities. Finally, the model is fine-tuned using human-annotated golden data. The method includes dynamic filtering where sentences with low confidence scores are randomly filtered during training to prevent overfitting to potentially noisy silver labels.

## Key Results
- ICLD achieves 4.9% improvement in macro F1 score and 2.3% improvement in micro F1 score over baseline models
- The method demonstrates significant gains when using only 20% of available human-annotated data
- Random filtering with 50th percentile threshold provides optimal balance between noise reduction and data utilization
- Knowledge distillation and contrastive learning components are both essential, with ablation studies showing performance degradation when either is removed

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICLD prevents semantic collapse by combining contrastive learning with knowledge distillation
- Mechanism: The contrastive learning component captures document-level event structures while knowledge distillation ensures the model learns discourse role classifications rather than just semantic similarities
- Core assumption: Sentence embeddings that are semantically similar may have different discourse roles, and vice versa
- Evidence anchors:
  - [abstract] "knowledge distillation prevents the model from collapsing into standard semantic similarity learning"
  - [section 3.2] "To avoid the potential collapse of task-specific intra-document contrastive learning towards standard contrastive learning, which prioritizes the semantic meanings of individual sentences, the incorporation of explicit guidance and simultaneous distillation becomes imperative"
  - [corpus] Weak evidence - related papers focus on different aspects of discourse processing but don't directly address this mechanism

### Mechanism 2
- Claim: Intra-document contrastive learning leverages unlabeled data to capture event structure relationships
- Mechanism: By sampling positive and negative sentence pairs within the same document based on silver labels, the model learns to distinguish sentences that serve similar discourse functions even when they lack semantic similarity
- Core assumption: Sentences within the same document that share discourse roles will have similar positions in the learned embedding space, regardless of semantic similarity
- Evidence anchors:
  - [abstract] "intra-document contrastive learning and knowledge distillation to train student models"
  - [section 3.2] "Under the supervision of silver labels, unfiltered sentence pairs of the same category within each document are randomly sampled"
  - [corpus] Weak evidence - related papers don't address this specific intra-document contrastive approach for news discourse

### Mechanism 3
- Claim: Dynamic filtering prevents overfitting to potentially noisy silver labels
- Mechanism: Sentences with confidence scores below the k-th percentile threshold are randomly filtered during training, preventing the model from learning from potentially incorrect labels
- Core assumption: Low-confidence silver labels are more likely to be incorrect and should not be fully trusted during training
- Evidence anchors:
  - [section 3.2] "sentences with confidences lower than their respective thresholds are subjected to filtering with a probability of 0.5 during each epoch"
  - [section 4.4] "When the filtering probability is set to 0, no sentences are filtered out... results in the lowest performance"
  - [corpus] Weak evidence - filtering approaches are common but specific percentile-based dynamic filtering isn't addressed in related work

## Foundational Learning

- Concept: Contrastive learning fundamentals
  - Why needed here: The method relies on pulling similar samples together and pushing dissimilar samples apart in embedding space
  - Quick check question: How does contrastive learning differ from traditional supervised classification in terms of learning objectives?

- Concept: Knowledge distillation principles
  - Why needed here: The method uses a teacher model to generate soft labels and guide the student model's learning
  - Quick check question: What is the key difference between knowledge distillation and standard supervised learning with hard labels?

- Concept: Document-level vs sentence-level analysis
  - Why needed here: The task requires understanding how individual sentences contribute to the overall news event structure
  - Quick check question: Why might sentences that are semantically similar have different discourse roles in news articles?

## Architecture Onboarding

- Component map: Teacher model (T5-large) → Silver label generation → Student model (Longformer/RoBERTa) → Intra-document contrastive learning + Knowledge distillation → Fine-tuning with gold labels
- Critical path: Unlabeled news articles → Teacher model prediction → Silver label generation → Student model training with ICLD → Fine-tuning with human annotations
- Design tradeoffs: Using large teacher model vs computational cost, random filtering vs complete label trust, contrastive learning vs standard supervised learning
- Failure signatures: Performance degradation when unlabeled data distribution differs significantly from labeled data, model collapse when contrastive learning dominates, overfitting when filtering is too aggressive
- First 3 experiments:
  1. Train baseline model using only human-annotated data to establish performance floor
  2. Train with ICLD using small amount of unlabeled data to verify basic mechanism
  3. Vary filtering threshold (10th to 90th percentile) to find optimal balance between noise reduction and data utilization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of random filtering probability affect the performance of ICLD, and is there an optimal threshold that balances noise reduction with data retention?
- Basis in paper: [explicit] The paper discusses random filtering with different probabilities (0 to 1.0) and its impact on model performance, finding that a probability of 0.5 yields the best results.
- Why unresolved: While the paper identifies a favorable probability of 0.5, it does not explore whether this is truly optimal across different datasets or task configurations, nor does it examine the sensitivity of performance to small changes in this hyperparameter.
- What evidence would resolve it: Conducting a more granular analysis of filtering probabilities (e.g., 0.1 increments) across multiple datasets and tasks, and testing the stability of performance at the identified optimal point, would clarify the robustness of the chosen threshold.

### Open Question 2
- Question: Can ICLD be effectively extended to other discourse-level tasks beyond news discourse profiling, and what modifications might be necessary?
- Basis in paper: [inferred] The paper suggests that ICLD's success in news discourse profiling could indicate potential applicability to other tasks, but it does not test this hypothesis.
- Why unresolved: The paper focuses exclusively on news discourse profiling and does not provide evidence or analysis for ICLD's performance on other discourse-level tasks, leaving its generalizability uncertain.
- What evidence would resolve it: Applying ICLD to a diverse set of discourse-level tasks (e.g., dialogue analysis, argumentative structure parsing) and comparing its performance to task-specific baselines would demonstrate its versatility and identify necessary adaptations.

### Open Question 3
- Question: How does the performance of ICLD compare to other semi-supervised methods, such as self-training or co-training, in the context of news discourse profiling?
- Basis in paper: [inferred] The paper introduces ICLD as a novel semi-supervised approach but does not benchmark it against other established semi-supervised techniques.
- Why unresolved: While ICLD shows improvements over baseline models, the paper does not explore how it stacks up against other semi-supervised learning strategies, leaving questions about its relative effectiveness.
- What evidence would resolve it: Implementing and comparing ICLD with other semi-supervised methods (e.g., self-training, co-training) on the same task and dataset would provide a clearer picture of its advantages and limitations.

## Limitations
- Silver label quality depends heavily on teacher model performance, with no systematic evaluation of how silver label noise affects downstream performance
- Intra-document contrastive learning may miss broader discourse patterns that span multiple articles or domains
- Limited domain generalization as both datasets come from similar news domains

## Confidence

**High Confidence**
- The core claim that combining contrastive learning with knowledge distillation prevents semantic collapse is well-supported by ablation studies

**Medium Confidence**
- While the filtering mechanism shows clear benefits, the underlying assumption that teacher model confidence scores correlate with label accuracy is not independently validated

**Low Confidence**
- The paper provides no evidence about how the method performs outside the specific news domain or whether the learned discourse patterns transfer to other text types

## Next Checks
1. **Silver Label Quality Analysis**: Conduct an independent validation where a subset of silver labels are manually verified to establish the correlation between teacher model confidence scores and actual label accuracy.

2. **Cross-Domain Evaluation**: Test the trained model on discourse profiling tasks from different domains (e.g., academic papers, legal documents, social media) to assess whether the learned discourse patterns generalize beyond news articles.

3. **Contrastive Learning Ablation**: Perform a controlled experiment comparing intra-document contrastive learning against cross-document contrastive learning to determine whether the current design choice is optimal or if broader document comparisons would yield better discourse understanding.