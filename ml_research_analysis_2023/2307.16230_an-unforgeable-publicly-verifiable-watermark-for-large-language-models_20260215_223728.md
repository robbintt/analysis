---
ver: rpa2
title: An Unforgeable Publicly Verifiable Watermark for Large Language Models
arxiv_id: '2307.16230'
source_url: https://arxiv.org/abs/2307.16230
tags:
- watermark
- text
- detection
- network
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the first private watermark algorithm for LLMs
  that uses two separate neural networks for watermark generation and detection, rather
  than using the same key for both. By sharing the token embedding layers between
  the two networks, the detection network achieves high accuracy efficiently.
---

# An Unforgeable Publicly Verifiable Watermark for Large Language Models

## Quick Facts
- arXiv ID: 2307.16230
- Source URL: https://arxiv.org/abs/2307.16230
- Authors: 
- Reference count: 14
- Primary result: Achieves nearly 99% detection accuracy with unforgeable watermark using separate neural networks for generation and detection

## Executive Summary
This paper introduces the first private watermark algorithm for large language models that achieves both high detection accuracy and unforgeability through a novel architecture using separate neural networks for watermark generation and detection. The key innovation is sharing token embedding layers between these networks, enabling efficient training while maintaining security through the black-box nature of neural networks. Experiments demonstrate that the private watermark algorithm achieves detection accuracy nearly matching public watermark approaches while providing stronger security guarantees against forgery attempts.

## Method Summary
The method employs two distinct neural networks: a watermark generation network that determines green/red token classifications during text generation, and a watermark detection network that classifies whether text contains watermarks. The critical architectural choice is sharing the token embedding layer between both networks, which enables the detection network to achieve high accuracy without extensive fine-tuning. During generation, the system uses z-value tests and cyclic document labeling to apply watermarks, while detection processes entire documents through an LSTM classifier. The approach claims unforgeability because attackers cannot easily reverse-engineer watermark generation rules from the detection network's black-box parameters.

## Key Results
- Nearly 99% detection accuracy on test datasets, comparable to public watermark algorithms
- 0.2%-1.8% computational overhead during text generation
- High resistance to forgery attempts due to neural network black-box properties

## Why This Works (Mechanism)

### Mechanism 1: Shared Embedding Layer Efficiency
- Claim: Sharing token embedding layers between generation and detection networks enables high detection accuracy without fine-tuning detection parameters.
- Mechanism: The shared embedding layer provides prior knowledge about token patterns to the detection network, reducing the complexity of learning to detect watermarks from scratch.
- Core assumption: The token embedding contains sufficient information about watermark patterns to enable accurate detection.
- Evidence anchors:
  - [abstract]: "the token embedding parameters are shared between the generation and detection networks, which makes the detection network achieve a high accuracy very efficiently"
  - [section 4.4]: "The parameters of this token embedding network are identical to those of the watermark generation network, and will not be fine-tuned in the following training process"
- Break condition: If the embedding layer doesn't capture sufficient watermark-specific patterns, detection accuracy would drop significantly without shared parameters.

### Mechanism 2: Neural Network Black-Box Security
- Claim: Using separate neural networks for generation and detection creates unforgeability through black-box complexity.
- Mechanism: Attackers cannot easily reverse-engineer watermark generation rules from detection network parameters because neural networks are inherently opaque.
- Core assumption: Neural network parameters are sufficiently complex that reverse-engineering generation rules is computationally infeasible.
- Evidence anchors:
  - [abstract]: "the privacy of our algorithm derives from the black-box nature of neural networks, that is, it's nearly impossible to infer the watermark generation detail from the parameters of the detection network"
  - [section 4.5]: "Considering the black-box nature of neural networks, inferring watermark rules based on the parameters of the detection network is nearly impossible"
- Break condition: If attackers can query the detection network extensively, they might infer generation rules through statistical analysis of output changes.

### Mechanism 3: Cyclic Document Labeling
- Claim: Cyclic document labeling prevents attackers from deducing watermark rules through token manipulation.
- Mechanism: By treating text as cyclic and labeling the first w-1 tokens based on the cyclic structure, attackers cannot easily determine watermark rules by changing the last token and observing output changes.
- Core assumption: The cyclic labeling approach sufficiently obscures the watermark generation pattern.
- Evidence anchors:
  - [section 4.4]: "we also labeled the first w-1 tokens by treating the text as a cyclic document connected head-to-tail"
  - [section 4.5]: "we can determine the label for x0 through xn−w+1....xn"
- Break condition: If attackers can determine the window size w, they might still deduce rules through extensive experimentation.

## Foundational Learning

- Concept: Neural network black-box properties
  - Why needed here: The security of the algorithm relies on the computational infeasibility of reverse-engineering watermark generation rules from detection network parameters
  - Quick check question: Why can't an attacker simply examine the detection network parameters to learn the watermark generation rules?

- Concept: Statistical watermark detection using z-scores
  - Why needed here: The detection network is trained to classify based on z-score thresholds, and the watermark generation network aims to create predictable green token distributions
  - Quick check question: How does the cyclic document approach modify the standard z-score calculation for watermark detection?

- Concept: Parameter sharing in neural networks
  - Why needed here: The shared embedding layer is crucial for enabling efficient training of the detection network without requiring massive datasets
  - Quick check question: What specific advantage does sharing the embedding layer provide compared to training the detection network from scratch?

## Architecture Onboarding

- Component map:
  Language Model (M) -> Watermark Generation Network (W) -> Shared Embedding Layer (E) -> Watermark Detection Network (D) -> Classification Network (C)

- Critical path:
  1. Generate logits using language model
  2. Select top-K tokens
  3. Apply watermark generation network to determine green/red labels
  4. Modify logits based on green/red classification
  5. During detection, process entire text through detection network
  6. Output binary classification indicating watermark presence

- Design tradeoffs:
  - Using separate networks increases security but requires additional training
  - Shared embedding reduces training complexity but creates a potential attack vector if embeddings are leaked
  - Cyclic labeling prevents simple attacks but adds complexity to the generation process

- Failure signatures:
  - High false positive rate indicates detection network is too sensitive or training data is insufficient
  - High false negative rate suggests watermark generation network isn't creating strong enough patterns
  - Slow generation indicates watermark generation network is adding significant computational overhead

- First 3 experiments:
  1. Test detection accuracy with and without shared embedding layer to quantify its impact
  2. Measure false positive/false negative rates across different z-score thresholds
  3. Evaluate computational overhead by comparing generation times with and without watermark generation network

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the variance of the green label ratio generated by the watermark generation network affect the detection accuracy of the private watermark algorithm?
- Basis in paper: [explicit] The paper mentions that the watermark generation network cannot guarantee a fixed ratio γ and only obtains a ratio γ with an expectation γ and a standard deviation σ. It also states that the standard deviation σ is very small in practice and the increase in variance, σ²T, is also quite minimal.
- Why unresolved: The paper does not provide a detailed analysis of how the variance of the green label ratio affects the detection accuracy of the private watermark algorithm. It only mentions that the standard deviation is very small and the increase in variance is minimal.
- What evidence would resolve it: Experiments comparing the detection accuracy of the private watermark algorithm with different standard deviations of the green label ratio.

### Open Question 2
- Question: How does the shared token embedding layer between the watermark generation and detection networks impact the training efficiency and detection accuracy of the private watermark algorithm?
- Basis in paper: [explicit] The paper states that the shared token embedding layers between the watermark generation and detection networks make the training of the watermark detection network more efficient. It also mentions that without the shared layer, the proportion of false negatives and false positives dramatically decreases on an average of 15.1% and 32.0% respectively.
- Why unresolved: The paper does not provide a detailed analysis of how the shared token embedding layer impacts the training efficiency and detection accuracy of the private watermark algorithm. It only mentions that the shared layer makes the training more efficient and that without it, the detection accuracy decreases.
- What evidence would resolve it: Experiments comparing the training efficiency and detection accuracy of the private watermark algorithm with and without the shared token embedding layer.

### Open Question 3
- Question: How does the window size in the watermark generation network affect the difficulty of reverting the watermark generation rules from the watermark detection network?
- Basis in paper: [explicit] The paper mentions that a larger window size could make the watermark generation rules more difficult to decipher. It also states that the method which uses a global fixed red-green list as adopted by Zhao et al. [2023] is not suitable for the private watermark algorithm.
- Why unresolved: The paper does not provide a detailed analysis of how the window size in the watermark generation network affects the difficulty of reverting the watermark generation rules from the watermark detection network. It only mentions that a larger window size could make it more difficult and that the global fixed red-green list method is not suitable.
- What evidence would resolve it: Experiments comparing the difficulty of reverting the watermark generation rules from the watermark detection network with different window sizes in the watermark generation network.

## Limitations

- Limited empirical validation against sophisticated adversarial attacks beyond basic unified attack scenarios
- Security claims rely heavily on neural network black-box properties without rigorous mathematical proof of unforgeability
- No comprehensive analysis of parameter sensitivity or how parameter choices affect security guarantees

## Confidence

**High Confidence** (supported by direct experimental evidence):
- Detection accuracy metrics showing 99% performance on test sets
- Computational overhead measurements showing 0.2%-1.8% increase
- Comparison of shared vs. non-shared embedding performance

**Medium Confidence** (supported by theoretical arguments with limited empirical validation):
- Unforgeability claims based on neural network black-box properties
- Resistance to unified attacks on watermarks
- Effectiveness of cyclic labeling in preventing token manipulation attacks

**Low Confidence** (largely theoretical with minimal supporting evidence):
- Long-term security against evolving attack techniques
- Performance under real-world deployment conditions with diverse generation patterns
- Generalization across different language model architectures beyond the tested models

## Next Checks

1. **Adversarial attack benchmark**: Implement a comprehensive suite of watermark evasion attacks including token substitution, paraphrasing, and model extraction techniques. Measure detection accuracy under these attack conditions and quantify the trade-off between watermark strength and evasion resistance.

2. **Parameter sensitivity analysis**: Systematically vary z-score thresholds, δ parameters, and green token ratios across their plausible ranges. Document how these changes affect both detection accuracy and unforgeability claims, identifying optimal parameter settings that balance security and performance.

3. **Cross-model generalization study**: Test the watermark detection network's performance when the detection target uses a different language model architecture than the one used during training. Measure performance degradation and analyze whether the shared embedding approach generalizes across model families or requires architecture-specific training.