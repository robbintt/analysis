---
ver: rpa2
title: 'SpEL: Structured Prediction for Entity Linking'
arxiv_id: '2310.14684'
source_url: https://arxiv.org/abs/2310.14684
tags:
- entity
- linking
- candidate
- pages
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a structured prediction approach for entity
  linking, called SPEL, that classifies each input token as an entity and aggregates
  the token predictions. SPEL uses two refined fine-tuning steps, a context-sensitive
  prediction aggregation strategy, a reduced model output vocabulary, and addresses
  tokenization mismatch issues.
---

# SpEL: Structured Prediction for Entity Linking

## Quick Facts
- arXiv ID: 2310.14684
- Source URL: https://arxiv.org/abs/2310.14684
- Reference count: 24
- This paper introduces a structured prediction approach for entity linking that achieves state-of-the-art performance on the AIDA benchmark dataset for entity linking to Wikipedia.

## Executive Summary
This paper presents SPEL (Structured Prediction for Entity Linking), a novel approach that frames entity linking as a structured prediction problem where each token is classified as an entity. SPEL uses a context-sensitive aggregation strategy to combine subword-level predictions into coherent word-level and span-level entity predictions. The method addresses common challenges in entity linking including tokenization mismatch between training and inference, computational efficiency through reduced candidate sets, and the need for domain-specific fine-tuning. SPEL achieves state-of-the-art performance on the AIDA benchmark with F1 scores of 97.3 on testa and 96.8 on testb, while maintaining computational efficiency.

## Method Summary
SPEL uses RoBERTa as its underlying encoder and treats entity linking as a structured prediction problem where each token is classified as an entity. The method employs a three-step fine-tuning procedure: initial training on Wikipedia with mention-aware tokenization, re-training with mention-agnostic tokenization to address training/inference mismatch, and final domain-specific fine-tuning on the AIDA dataset. During inference, SPEL uses context-sensitive aggregation to combine subword-level predictions into coherent entity spans, and employs a reduced vocabulary of 500K entities to improve efficiency. The model also incorporates mention-specific candidate sets to filter predictions and improve precision.

## Key Results
- SPEL achieves state-of-the-art F1 scores of 97.3 on AIDA testa and 96.8 on testb
- Inference speed of 0.01 seconds per document represents a significant efficiency improvement
- The approach demonstrates superior accuracy compared to previous methods while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context-sensitive prediction aggregation improves entity linking by enforcing word-level and span-level prediction coherence.
- Mechanism: The model aggregates subword-level predictions by computing the weighted average probability for each entity identifier across subwords belonging to the same word, then joins consecutive word-level predictions referring to the same concept into a single mention span.
- Core assumption: The average probability across subwords provides a more reliable prediction than individual subword predictions, especially when subwords disagree.
- Evidence anchors:
  - [abstract]: "a context sensitive prediction aggregation strategy that enables subword-level token classification while enforcing word-level and span-level prediction coherence"
  - [section]: "For each word annotation, we generate an aggregated entity identifier prediction set by taking the union of the entity identifiers predicted for the subwords. We then compute the weighted average of the prediction probabilities for each entity identifier"
- Break condition: If subword predictions are consistently unreliable or the model cannot resolve conflicts between subwords, the aggregation strategy may not improve accuracy.

### Mechanism 2
- Claim: Two refined fine-tuning steps improve model performance by addressing tokenization mismatch and focusing on domain-specific knowledge.
- Mechanism: The first fine-tuning step uses mention-aware tokenization to teach the model entity boundary detection, while the second uses mention-agnostic tokenization to address training/inference mismatch. A final domain-specific fine-tuning step focuses on the target dataset.
- Core assumption: Fine-tuning on mention-aware and mention-agnostic versions of the same data helps the model learn robust representations that generalize better to inference scenarios.
- Evidence anchors:
  - [abstract]: "we address a common problem in entity-linking systems where there is a training vs. inference tokenization mismatch"
  - [section]: "To address this issue, as a subsequent fine-tuning step, we iterate again through the large entity-linked dataset which is re-tokenized without the knowledge of the mention spans"
- Break condition: If the model architecture or tokenization scheme changes significantly, the effectiveness of this multi-step fine-tuning approach may diminish.

### Mechanism 3
- Claim: Using mention-specific candidate sets improves precision by filtering out irrelevant high-probability predictions.
- Mechanism: When a mention's surface form matches a mention in a candidate set, predictions not in the set are filtered out regardless of their probability, reducing false positives.
- Core assumption: Mention-specific candidate sets contain relevant entities for the given mention, and filtering based on these sets reduces noise in predictions.
- Evidence anchors:
  - [abstract]: "We use the in-domain mention vocabulary to create a fixed candidate set. We use this to improve the efficiency and accuracy of entity linking"
  - [section]: "When a mention-specific candidate set is available, and the mention surface form matches one of the mentions in the candidate set, we filter out any predictions from the phrase annotation that are not present in the candidate set, regardless of their probability"
- Break condition: If candidate sets are incomplete or contain irrelevant entities, the filtering may reduce recall or introduce errors.

## Foundational Learning

- Concept: Structured prediction as sequence tagging
  - Why needed here: Entity linking is framed as classifying each token (subword) into one of many entity classes, requiring understanding of structured prediction techniques
  - Quick check question: How does structured prediction differ from independent classification for each token in the context of entity linking?

- Concept: Tokenization and subword modeling
  - Why needed here: The model operates on subword tokens, requiring understanding of tokenization schemes and how they affect entity boundary detection
  - Quick check question: What challenges arise when tokenizing entities that span multiple subwords, and how does the model address these challenges?

- Concept: Candidate set construction and usage
  - Why needed here: Efficient entity linking requires limiting the output vocabulary to a manageable set of candidates, requiring understanding of different candidate set types and their trade-offs
  - Quick check question: What are the differences between fixed candidate sets and mention-specific candidate sets, and when would each be appropriate?

## Architecture Onboarding

- Component map: Text input → RoBERTa encoding → Classification → Aggregation → Candidate filtering → Entity links
- Critical path: Text input → RoBERTa encoding → Classification → Aggregation → Candidate filtering → Entity links
- Design tradeoffs: Using a fixed candidate set reduces memory requirements but may limit recall; mention-specific sets improve precision but require additional computation; structured prediction simplifies the model but requires careful aggregation
- Failure signatures: Poor mention detection (O vs non-O confusion), incorrect span boundaries, entity predictions outside candidate sets, inconsistent subword predictions within words
- First 3 experiments:
  1. Train SPEL on Wikipedia without candidate sets and evaluate on AIDA to establish baseline performance
  2. Add context-sensitive aggregation and measure improvement in F1 score
  3. Introduce mention-specific candidate sets and evaluate impact on precision and recall

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can structured prediction for entity linking be extended to handle zero-shot entity linking scenarios?
- Basis in paper: [explicit] The paper acknowledges the limitation of SPEL relying on a predefined fixed candidate set and suggests this as an area for future exploration.
- Why unresolved: The paper does not provide a solution or detailed approach for zero-shot entity linking within the structured prediction framework.
- What evidence would resolve it: A proposed method or experimental results demonstrating structured prediction applied to zero-shot entity linking, along with performance comparisons to existing methods.

### Open Question 2
- Question: What is the impact of temporal changes in knowledge bases on the performance of entity linking models like SPEL?
- Basis in paper: [explicit] The paper mentions that temporal evolution of knowledge bases has introduced discrepancies, affecting out-of-domain datasets.
- Why unresolved: The paper does not provide a detailed analysis of how temporal changes impact model performance or strategies to mitigate these effects.
- What evidence would resolve it: A study analyzing model performance over time as knowledge bases evolve, or methods to dynamically update models to handle temporal changes.

### Open Question 3
- Question: How can SPEL be adapted to handle multilingual entity linking effectively?
- Basis in paper: [explicit] The paper acknowledges the limitation of focusing on English only and suggests extending work to cover multiple languages.
- Why unresolved: The paper does not explore multilingual applications or provide strategies for adapting SPEL to other languages.
- What evidence would resolve it: Experimental results showing SPEL's performance on multilingual datasets, or a proposed framework for adapting SPEL to handle multiple languages.

## Limitations
- The method does not address NIL (Not In List) predictions for mentions that don't correspond to any entity in the knowledge base
- Reliance on a fixed candidate set of 500K entities may limit recall for rare mentions and specialized domains
- The approach assumes the existence of mention-specific candidate sets but doesn't detail their construction or coverage across different domains

## Confidence

**High Confidence Claims:**
- SPEL achieves state-of-the-art performance on the AIDA benchmark with significant improvements over existing methods (F1 scores of 97.3 on testa and 96.8 on testb)
- The context-sensitive aggregation strategy effectively improves entity linking by enforcing word-level and span-level coherence
- The two-step fine-tuning approach addresses tokenization mismatch issues between training and inference

**Medium Confidence Claims:**
- The reduced vocabulary size (from ~6M to 500K entities) provides computational efficiency benefits without significant accuracy loss
- Mention-specific candidate sets improve precision by filtering irrelevant high-probability predictions
- The model's inference speed (0.01 seconds per document) represents a meaningful improvement over prior art

## Next Checks

1. **Reproduce core results on AIDA benchmark**: Implement the three-step fine-tuning procedure and context-sensitive aggregation strategy, then evaluate on AIDA testa and testb sets to verify claimed F1 scores of 97.3 and 96.8.

2. **Validate tokenization mismatch solution**: Compare entity linking performance when training with mention-aware tokenization versus mention-agnostic re-tokenization to empirically demonstrate the effectiveness of the two-step fine-tuning approach.

3. **Test candidate set impact**: Evaluate the model's performance with different candidate set sizes (e.g., 100K, 500K, 1M entities) to quantify the trade-off between computational efficiency and recall, verifying the claimed benefits of the reduced vocabulary approach.