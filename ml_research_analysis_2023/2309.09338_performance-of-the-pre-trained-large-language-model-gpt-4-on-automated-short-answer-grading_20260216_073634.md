---
ver: rpa2
title: Performance of the Pre-Trained Large Language Model GPT-4 on Automated Short
  Answer Grading
arxiv_id: '2309.09338'
source_url: https://arxiv.org/abs/2309.09338
tags:
- answer
- gpt-4
- grading
- student
- short
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluated the performance of the pre-trained Large Language
  Model (LLM) GPT-4 on the task of Automated Short Answer Grading (ASAG). The standard
  benchmark datasets SciEntsBank and Beetle were used, with GPT-4 prompted to grade
  student answers in both 2-way and 3-way scenarios, with and without reference answers.
---

# Performance of the Pre-Trained Large Language Model GPT-4 on Automated Short Answer Grading

## Quick Facts
- arXiv ID: 2309.09338
- Source URL: https://arxiv.org/abs/2309.09338
- Reference count: 0
- Primary result: GPT-4's performance on ASAG is comparable to hand-engineered models but worse than specialized pre-trained models

## Executive Summary
This study evaluates the performance of GPT-4, a pre-trained Large Language Model, on Automated Short Answer Grading (ASAG) tasks. Using standard benchmark datasets SciEntsBank and Beetle, the research investigates GPT-4's ability to grade student answers in both 2-way and 3-way scenarios, with and without reference answers. The findings reveal that while GPT-4 performs comparably to hand-engineered ASAG models, it falls short of specialized pre-trained models that have undergone additional task-specific training.

## Method Summary
The study utilized the SciEntsBank and Beetle benchmark datasets, which contain science questions, reference answers, and student responses with grades. GPT-4 was prompted to grade these student answers using the OpenAI API, both with and without reference answers provided. The evaluation was conducted in both 2-way (correct/incorrect) and 3-way (correct, contradictory, unrelated) grading scenarios. Precision, recall, and F1-scores were calculated to assess GPT-4's performance, which was then compared to existing hand-engineered and specialized pre-trained models.

## Key Results
- GPT-4 achieved an F1-score of 0.744 on the SciEntsBank dataset in the 2-way scenario and 0.731 without a reference answer.
- For the Beetle dataset, GPT-4's F1-scores were 0.611 and 0.651, respectively, for the 2-way and no reference answer scenarios.
- GPT-4's performance was comparable to hand-engineered ASAG models but inferior to specialized pre-trained models like BERT-family models with additional task-specific training.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can grade short answers without a reference answer by relying on its pre-trained knowledge of correct responses.
- Mechanism: The model leverages its general understanding of the subject matter acquired during pre-training to evaluate student responses based on correctness and comprehensiveness.
- Core assumption: GPT-4's pre-training corpus included sufficient coverage of the topics in the SciEntsBank and Beetle datasets to enable accurate grading without explicit reference answers.
- Evidence anchors:
  - [abstract] "even without additional training or reference answers"
  - [section] "in addition, it was also investigated if GPT-4 can adequately grade the student answers based on the question alone... the model would need to draw on its own pre-training from its text corpus to independently judge the correctness of the student answer."
- Break condition: If the pre-training corpus lacked coverage of the specific educational content, GPT-4's performance would degrade significantly when grading without reference answers.

### Mechanism 2
- Claim: GPT-4's performance on ASAG tasks is comparable to hand-engineered models but inferior to specialized pre-trained models like BERT-family models with additional task-specific training.
- Mechanism: While GPT-4 benefits from extensive general pre-training, specialized models achieve better performance through additional fine-tuning on domain-specific datasets, optimizing for the grading task.
- Core assumption: Task-specific fine-tuning provides significant performance gains over general-purpose pre-training for ASAG tasks.
- Evidence anchors:
  - [abstract] "the performance of the pre-trained general-purpose GPT-4 LLM is comparable to hand-engineered models, but worse than pre-trained LLMs that had specialized training."
  - [section] "The performance of the general-purpose Large Language Model GPT-4 on Automated Short Answer Grading does not reach that of specifically trained Deep-Learning models, but it is comparable to that of earlier hand-engineered ASAG models."
- Break condition: If the additional task-specific training does not significantly improve performance, the gap between GPT-4 and specialized models would narrow.

### Mechanism 3
- Claim: GPT-4 can provide more tailored feedback than simple correct/incorrect judgments, enhancing the learning experience.
- Mechanism: The model's ability to understand context and nuances in student responses allows it to generate detailed feedback beyond binary grading, potentially aiding student learning.
- Core assumption: GPT-4's language understanding capabilities extend to providing meaningful, educational feedback on student answers.
- Evidence anchors:
  - [abstract] "In addition to not needing additional training, GPT-4 can also perform ASAG without the need for providing reference answers, at least at the grade-school level covered by the datasets used in this study and likely at the introductory higher-education level."
  - [section] "A clear advantage of GPT-4 is that it does not need to be specifically trained for the task and can be used 'out-of-the-box,' which has the potential to turn it into a commodity for educators. In addition to not needing additional training, GPT-4 can also perform ASAG without the need for providing reference answers... and that automated grading of open-ended assessment content is possible beyond short answers."
- Break condition: If GPT-4's feedback is not pedagogically sound or consistent, its utility for educational purposes would be limited.

## Foundational Learning

- Concept: Understanding of Automated Short Answer Grading (ASAG) systems
  - Why needed here: To comprehend the task GPT-4 is being evaluated on and the metrics used for performance comparison.
  - Quick check question: What are the primary evaluation metrics used in ASAG, and how do they differ between 2-way and 3-way grading scenarios?

- Concept: Knowledge of pre-trained Large Language Models (LLMs) and their capabilities
  - Why needed here: To understand how GPT-4's general pre-training enables it to perform tasks without additional training and how it compares to specialized models.
  - Quick check question: How does the pre-training of LLMs like GPT-4 differ from the fine-tuning approach used by specialized models?

- Concept: Familiarity with educational assessment principles
  - Why needed here: To appreciate the significance of automated grading in educational contexts and the potential benefits and limitations of using AI for assessment.
  - Quick check question: What are the main challenges in automating the grading of open-ended student responses, and how might AI address these challenges?

## Architecture Onboarding

- Component map: Datasets -> Prompt Generation -> GPT-4 API -> Evaluation Metrics -> Performance Comparison

- Critical path:
  1. Load and parse the benchmark datasets.
  2. Generate prompts for each item, both with and without reference answers.
  3. Send prompts to GPT-4 API and collect responses.
  4. Calculate evaluation metrics (precision, recall, F1-scores).
  5. Compare results with existing models and analyze performance differences.

- Design tradeoffs:
  - Using a general-purpose LLM like GPT-4 offers ease of use and broad knowledge but may lack the specialized performance of fine-tuned models.
  - Excluding reference answers in prompts tests GPT-4's general knowledge but may introduce variability in grading consistency.
  - Relying on a single run due to computational constraints limits the ability to assess result variability.

- Failure signatures:
  - Poor performance on datasets outside the scope of GPT-4's pre-training.
  - Inconsistent grading when reference answers are omitted.
  - Inability to handle complex or nuanced student responses effectively.

- First 3 experiments:
  1. Evaluate GPT-4's performance on a small subset of the SciEntsBank dataset with and without reference answers to assess baseline capabilities.
  2. Compare GPT-4's grading results with a simple hand-engineered model on the same dataset to understand relative performance.
  3. Test GPT-4's ability to provide detailed feedback on student answers beyond binary grading to explore its potential for educational use.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPT-4 compare to specialized ASAG models when tasked with grading answers in higher education contexts, such as university-level physics?
- Basis in paper: [inferred] The paper suggests that GPT-4's performance on grade-school level datasets could extend to higher education, citing a study on university-level physics content.
- Why unresolved: The study did not directly test GPT-4 on higher education datasets or content.
- What evidence would resolve it: Testing GPT-4 on university-level physics or other higher education datasets to compare its performance with specialized models.

### Open Question 2
- Question: Can GPT-4's ability to provide tailored feedback beyond simple correct/incorrect judgments significantly enhance learning outcomes in ASAG tasks?
- Basis in paper: [explicit] The paper notes that GPT-4 can give more tailored feedback than simple correct/incorrect judgments, which has high potential for learning from short answer grading.
- Why unresolved: The study did not evaluate the impact of GPT-4's tailored feedback on learning outcomes.
- What evidence would resolve it: Conducting a study that measures the impact of GPT-4's tailored feedback on student learning outcomes in ASAG tasks.

### Open Question 3
- Question: How does the performance of locally installable models like Llama 2 compare to GPT-4 for ASAG tasks, especially considering data privacy concerns?
- Basis in paper: [explicit] The paper mentions Llama 2 as an alternative that might not need additional training but could be locally installed, and notes that preliminary studies indicate Llama 2 does not perform as well as GPT-4.
- Why unresolved: The study did not provide a detailed comparison of Llama 2's performance with GPT-4 on ASAG tasks.
- What evidence would resolve it: Conducting a detailed comparative study of Llama 2 and GPT-4 on ASAG tasks, considering both performance and data privacy aspects.

## Limitations
- The study relied on grade-school level datasets, limiting generalizability to higher education or complex assessment scenarios.
- Computational constraints prevented multiple runs, making it impossible to assess result variability and reliability.
- The pedagogical quality of GPT-4's feedback was not evaluated, focusing only on grading accuracy.

## Confidence

- **High Confidence**: GPT-4 can perform ASAG without reference answers and achieves comparable performance to hand-engineered models.
- **Medium Confidence**: GPT-4's performance is worse than specialized pre-trained models, and it can provide more nuanced feedback than simple correct/incorrect judgments.
- **Low Confidence**: The study's findings generalize to higher education levels or complex assessment scenarios beyond grade-school material.

## Next Checks
1. Run GPT-4 multiple times with the same prompts to quantify result variability and establish confidence intervals for the reported F1-scores.
2. Test GPT-4 on higher education datasets or more complex assessment tasks to evaluate whether the observed performance patterns hold beyond grade-school material.
3. Analyze the pedagogical quality of GPT-4's feedback by having human educators evaluate the usefulness and accuracy of the detailed feedback provided by the model, not just its grading accuracy.