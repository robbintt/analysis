---
ver: rpa2
title: 'Think Twice: Perspective-Taking Improves Large Language Models'' Theory-of-Mind
  Capabilities'
arxiv_id: '2311.10227'
source_url: https://arxiv.org/abs/2311.10227
tags:
- question
- story
- answer
- perspective-taking
- about
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of improving large language\
  \ models\u2019 (LLMs) theory-of-mind (ToM) capabilities, which are essential for\
  \ understanding and simulating human-like reasoning about others' mental states.\
  \ The authors introduce SIMTOM, a two-stage prompting framework inspired by cognitive\
  \ science's Simulation Theory, which emphasizes perspective-taking as a crucial\
  \ step in ToM reasoning."
---

# Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities

## Quick Facts
- arXiv ID: 2311.10227
- Source URL: https://arxiv.org/abs/2311.10227
- Reference count: 40
- Key outcome: SIMTOM achieves 29.5% and 14.2% absolute accuracy improvements on BigTOM's false belief subset over baselines.

## Executive Summary
This paper introduces SIMTOM, a two-stage prompting framework that improves large language models' theory-of-mind capabilities by separating perspective-taking from question-answering. The method requires no additional training and minimal prompt-tuning, making it easy to integrate with pre-trained LLMs. SIMTOM is evaluated on two ToM benchmarks, BigTOM and ToMI, showing substantial performance improvements over existing methods like zero-shot and chain-of-thought prompting.

## Method Summary
SIMTOM is a two-stage prompting framework that first filters context based on what a character knows (perspective-taking) and then answers ToM questions using this filtered context. The method requires no additional training and minimal prompt-tuning, making it easy to integrate with pre-trained LLMs. The approach is inspired by cognitive science's Simulation Theory, which emphasizes perspective-taking as a crucial step in ToM reasoning.

## Key Results
- SIMTOM achieved 29.5% and 14.2% absolute accuracy improvements over baselines for GPT-3.5-Turbo on BigTOM's challenging false belief subset.
- Domain-specific few-shot examples further improved perspective-taking accuracy.
- SIMTOM consistently outperformed baseline methods like zero-shot and chain-of-thought prompting on both BigTOM and ToMI benchmarks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating perspective-taking from question-answering reduces cognitive load and improves ToM performance.
- Mechanism: The two-stage prompting splits a complex reasoning task into simpler subtasks: first filtering context to what a character knows, then answering the question using only that filtered context.
- Core assumption: LLMs perform better when complex multi-step reasoning is broken into sequential, focused steps rather than attempted in a single pass.
- Evidence anchors:
  - [abstract] "SIMTOM first filters context based on what the character in question knows (perspective-taking) and then answers ToM questions using this filtered context."
  - [section] "we hypothesize that LLMs' may be having difficulty with ToM reasoning because they are attempting to perform two tasks in a single inference pass: perspective-taking and question-answering."
  - [corpus] Found 25 related papers; average neighbor FMR=0.367, average citations=0.0. (Weak corpus support for mechanism-level claims.)
- Break condition: If the perspective-taking step introduces significant noise or hallucinates events not grounded in the original story, the filtered context may mislead the question-answering step.

### Mechanism 2
- Claim: LLMs can effectively simulate a character's mental state by filtering out events the character did not observe.
- Mechanism: By hiding information the character lacks, the model is forced to reason from the character's limited perspective rather than from the full omniscient view.
- Core assumption: LLMs can correctly identify which events a character knows about when prompted explicitly.
- Evidence anchors:
  - [abstract] "SimToM first filters context based on what the character in question knows before answering a question about their mental state."
  - [section] "Perspective-Taking ST argues that perspective-taking... involves simulating the beliefs and goals of the other individual."
  - [corpus] Weak support; no direct corpus evidence for this filtering mechanism.
- Break condition: If the character's name is ambiguous or the story contains complex nested perspectives, the filtering may fail.

### Mechanism 3
- Claim: Domain-specific few-shot examples improve perspective-taking accuracy.
- Mechanism: Providing exemplars of correct perspective-taking in-domain helps the model learn the pattern of filtering context appropriately.
- Core assumption: LLMs benefit from in-domain few-shot learning for structured reasoning tasks.
- Evidence anchors:
  - [section] "we investigate to what extent current LLM's are capable of improving their perspective-taking capabilities... enhancing the SIMTOM with 3 examples of perspective-taking drawn from in-domain ToMI stories."
  - [section] "Domain-Specific perspective taking strategies lead to a substantial performance gain over SIMTOM."
  - [corpus] No direct corpus evidence; claim based on experimental results.
- Break condition: If the few-shot examples are not representative of the task distribution, performance may degrade.

## Foundational Learning

- Concept: Theory of Mind (ToM)
  - Why needed here: ToM is the target capability being improved; understanding its definition and benchmarks is essential.
  - Quick check question: What is the difference between false belief and true belief questions in ToM benchmarks?
- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: CoT is a baseline method; understanding its limitations on ToM tasks contextualizes SIMTOM's improvements.
  - Quick check question: Why does CoT prompting often fail on false belief questions?
- Concept: Sally-Anne false belief test structure
  - Why needed here: Both BigTOM and ToMI are based on this paradigm; knowing the structure is key to interpreting results.
  - Quick check question: In a Sally-Anne story, what does the agent who left the room believe about the object's location?

## Architecture Onboarding

- Component map: Raw story -> Character name extraction -> Perspective-taking prompt -> Filtered story -> Question-answering prompt -> Answer
- Critical path:
  1. Parse character name from question
  2. Generate perspective-taking response
  3. Construct filtered story
  4. Generate answer using filtered story
- Design tradeoffs:
  - Two prompts vs. one: Two prompts reduce cognitive load but add latency
  - Hard filtering vs. soft reasoning: Hard filtering enforces strict perspective but may lose useful context
  - Domain-specific few-shots: Improve accuracy but require in-domain data
- Failure signatures:
  - Model outputs "not enough information" -> perspective-taking failed
  - Model answers based on true world state -> filtering step insufficient
  - Model hallucinates events -> perspective-taking over-interprets
- First 3 experiments:
  1. Compare single-prompt vs. two-prompt SIMTOM on BigTOM
  2. Test SIMTOM with oracle perspective-taking on ToMI
  3. Evaluate domain-specific few-shot SIMTOM on both benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can large language models perform theory-of-mind reasoning in complex, real-world scenarios beyond the synthetic datasets used in the paper?
- Basis in paper: [inferred] The paper mentions extending the method to complex theory-of-mind scenarios, but the results are based on synthetic datasets.
- Why unresolved: The paper only provides qualitative analysis on a small subset of real-world questions from the CosmosQA dataset. It is unclear how well the method generalizes to more diverse and complex real-world scenarios.
- What evidence would resolve it: A comprehensive evaluation of the method on a large, diverse set of real-world theory-of-mind tasks, such as those involving social interactions, emotions, and intentions.

### Open Question 2
- Question: What is the relationship between perspective-taking and inference in theory-of-mind reasoning, especially in scenarios with limited information?
- Basis in paper: [explicit] The paper mentions that perspective-taking in real-world scenarios may involve more inferring than hiding information, but it does not explore this relationship in depth.
- Why unresolved: The paper does not provide a clear distinction between perspective-taking as inference and perspective-taking as hiding information. It is unclear how to handle scenarios where the agent's perspective is not fully known or where additional information needs to be inferred.
- What evidence would resolve it: A detailed analysis of perspective-taking in various real-world scenarios, comparing the performance of the method when perspective-taking involves inference versus when it involves hiding information.

### Open Question 3
- Question: How can perspective-taking capabilities be further enhanced in large language models to improve their theory-of-mind reasoning?
- Basis in paper: [explicit] The paper suggests that domain-specific perspective-taking strategies can lead to substantial performance gains, but it does not explore other potential methods for enhancing perspective-taking.
- Why unresolved: The paper only investigates one approach to enhancing perspective-taking (domain-specific examples). It is unclear whether other methods, such as fine-tuning on theory-of-mind tasks or incorporating additional cognitive science principles, could lead to further improvements.
- What evidence would resolve it: An exploration of various techniques for enhancing perspective-taking, such as fine-tuning, few-shot learning, or incorporating additional cognitive science principles, and their impact on theory-of-mind reasoning performance.

## Limitations

- The paper provides limited ablation studies to isolate whether the improvements stem specifically from perspective-taking versus other prompt engineering effects.
- Statistical significance tests for the performance improvements are not reported, making it difficult to assess reliability across different random seeds or model initializations.
- Evaluation focuses primarily on GPT-3.5-Turbo, with limited results for Llama2, leaving questions about generalizability to other model families.

## Confidence

- SIMTOM improves ToM performance (High): Multiple benchmark results consistently show substantial accuracy gains over baselines.
- Perspective-taking is the key differentiator (Medium): The mechanism is theoretically grounded, but ablation studies are limited.
- Two-stage prompting is superior to single-stage (Medium): The comparison is made, but confounding factors (prompt wording, context length) are not fully controlled.

## Next Checks

1. **Ablation study on perspective-taking necessity**: Run SIMTOM with and without the perspective-taking stage on a held-out subset of ToM questions to isolate the contribution of perspective-taking versus prompt structure.

2. **Statistical significance testing**: Conduct multiple runs with different random seeds and compute confidence intervals for the performance improvements to verify that gains are not due to chance.

3. **Generalization across model families**: Evaluate SIMTOM on additional LLM families (e.g., Claude, PaLM) to determine if the improvements are model-specific or generalize across architectures.