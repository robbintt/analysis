---
ver: rpa2
title: 'QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources'
arxiv_id: '2310.07147'
source_url: https://arxiv.org/abs/2310.07147
tags:
- memory
- training
- arxiv
- optimizer
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QFT is a quantized full-parameter tuning framework for large language
  models that reduces memory usage by 79% compared to standard floating-point training
  while maintaining comparable performance. The method employs a memory-efficient
  Lion optimizer and quantizes all model states (weights, gradients, and optimizer
  momentum) into INT8 format using a dense-and-sparse quantizer for weights and uniform
  quantizers for gradients and momentum.
---

# QFT: Quantized Full-parameter Tuning of LLMs with Affordable Resources

## Quick Facts
- arXiv ID: 2310.07147
- Source URL: https://arxiv.org/abs/2310.07147
- Reference count: 10
- QFT achieves 79% memory reduction while maintaining comparable performance to full-precision fine-tuning

## Executive Summary
QFT introduces a quantized full-parameter tuning framework for large language models that dramatically reduces memory usage while maintaining competitive performance. By employing a memory-efficient Lion optimizer and quantizing all model states to INT8 format, QFT enables fine-tuning of LLaMA-7B models using only 30GB of memory on a single GPU. The approach uses a dense-and-sparse quantizer for weights and uniform quantizers for gradients and momentum, achieving results comparable to full-precision tuning across multiple benchmarks.

## Method Summary
QFT implements full-parameter fine-tuning through INT8 quantization of all model states (weights, gradients, and optimizer momentum) using a Lion optimizer. The framework employs a dense-and-sparse quantizer for weights that decomposes them into densely quantized central values and sparsely stored outliers, while gradients and momentum use uniform quantization. A stack-based gradient flow scheme enables backpropagation through quantized weights without framework AutoGrad support. The method fine-tunes LLaMA-2 models using batch size 128, learning rate 2e-5, for 3 epochs on the ShareGPT dataset.

## Key Results
- Achieves 79% memory reduction compared to standard floating-point training
- Enables LLaMA-7B fine-tuning using only 30GB of memory on a single GPU
- Maintains comparable performance to full-precision tuning across ARC, HellaSwag, MMLU, TruthfulQA, and MT-Bench benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lion optimizer's sign-based update rule provides inherent robustness to quantization errors
- Mechanism: Lion's uniform update magnitude (determined by sign operation) eliminates parameter-specific scaling variations that amplify quantization noise
- Core assumption: Quantization noise affects all parameters uniformly when update magnitudes are identical
- Evidence anchors:
  - [abstract]: "its update has the same magnitude for each parameter, an inherent advantage for robust quantization"
  - [section 3.1]: "Lion ensures that updates have the same magnitude for each parameter, which is determined through the sign operation"
  - [corpus]: Weak - neighboring papers focus on LoRA/GaLore techniques rather than optimizer quantization robustness
- Break condition: If quantization noise overwhelms the consistent-magnitude property, performance degrades regardless of update uniformity

### Mechanism 2
- Claim: Dense-and-sparse quantization effectively handles heavy-tailed weight distribution
- Mechanism: Decomposes weights into densely quantized central values and sparsely stored outliers
- Core assumption: 99% of weight values cluster within 20% of total range
- Evidence anchors:
  - [section 3.2]: "99% of the values cluster within a mere 20% of the overall range"
  - [section 3.2]: "W = D + S s.t. D = W [Tmin ≤ w ≤ Tmax] and S = W [w < Tmin or w > Tmax]"
  - [corpus]: Weak - no direct neighbor work on heavy-tailed quantization for training
- Break condition: If outlier percentage exceeds practical thresholds, memory savings diminish and accuracy loss increases

### Mechanism 3
- Claim: Stack-based gradient flow enables backpropagation through quantized weights
- Mechanism: Gradients computed during backward pass are pushed to a global stack in layer-reversed order
- Core assumption: Stack LIFO property aligns naturally with backward-forward layer ordering
- Evidence anchors:
  - [section 3.3]: "the gradient of the current layer always occupies the last position in the stack, fully capitalizing on the first-in-last-out property"
  - [section 3.3]: "computational complexity consistently remains at O(1), independent of the stack length"
  - [section 3.3]: "Algorithm 1 Gradient Flow of Quantized Weights" and "Algorithm 2 Quantized Lion Optimizer" describe stack operations
- Break condition: If gradient computation order mismatches stack push/pop sequence, backpropagation fails

## Foundational Learning

- Concept: Quantization error propagation in neural network training
  - Why needed here: Understanding how quantization affects weight updates, gradient computation, and optimizer state tracking is critical for QFT design
  - Quick check question: What happens to gradient magnitudes when weights are quantized with uniform quantization versus outliers preserved?

- Concept: Optimizer state memory complexity trade-offs
  - Why needed here: QFT relies on Lion optimizer's reduced state (momentum only) vs Adam's three-state
  - Quick check question: How does memory usage scale with parameter count for Adam vs Lion vs SGD optimizers?

- Concept: Sparse matrix storage formats (CSR/CSC)
  - Why needed here: Dense-and-sparse quantizer stores outliers as sparse matrices to minimize memory overhead
  - Quick check question: What is the memory overhead of CSR format for a matrix with 1% non-zero entries?

## Architecture Onboarding

- Component map: Quantized weight storage → Dequantization on forward → Dense-and-sparse decomposition → Stack-based gradient collection → Quantized Lion optimizer → Parameter update loop
- Critical path: Forward (dequantize weights) → Backward (compute/store gradients on stack) → Optimizer (pop gradients, update quantized weights)
- Design tradeoffs: Memory vs accuracy in quantization (tighter outlier thresholds improve accuracy but reduce compression); optimizer choice (Lion reduces states but may converge slower than Adam)
- Failure signatures: Training divergence (quantization too aggressive); memory exhaustion (stack operations inefficient); accuracy collapse (outlier threshold too tight)
- First 3 experiments:
  1. Verify dense-and-sparse decomposition correctly isolates outliers by measuring L2 distance between dequantized and full-precision weights
  2. Test stack-based gradient flow with synthetic layer gradients to confirm O(1) pop complexity
  3. Benchmark Lion optimizer convergence vs Adam on small quantized model to validate sign-based robustness claim

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of quantization bit-width for gradients and momentum in the Lion optimizer before training performance degrades significantly?
- Basis in paper: [explicit] The paper mentions that Lion optimizer's consistent update magnitudes provide advantages for robust quantization, and discusses using INT8 format for all model states, but does not explore the minimum viable precision for gradients and momentum
- Why unresolved: The paper focuses on demonstrating that INT8 quantization works well but does not systematically explore lower precision levels (e.g., INT4, INT2) for gradients and momentum, nor does it provide theoretical analysis of precision requirements
- What evidence would resolve it: Empirical results showing training performance degradation at various quantization bit-widths (e.g., 8-bit, 4-bit, 2-bit) for gradients and momentum, along with theoretical analysis of quantization error propagation through the Lion update mechanism

### Open Question 2
- Question: How does the dense-and-sparse quantizer threshold percentage affect training convergence speed and final model quality across different LLM architectures?
- Basis in paper: [explicit] The paper discusses using a 1% threshold for the dense-and-sparse quantizer and evaluates its impact on memory and accuracy, but doesn't systematically explore how different threshold values affect training dynamics
- Why unresolved: The paper selects a threshold based on memory-accuracy trade-off without exploring how threshold choice affects training convergence rate, stability, or whether optimal thresholds vary by model architecture or dataset
- What evidence would resolve it: Systematic ablation studies varying the threshold percentage (e.g., 0.1%, 0.5%, 1%, 2%, 5%) across different LLM sizes and training datasets, measuring both convergence speed and final performance metrics

### Open Question 3
- Question: Does QFT's memory efficiency advantage translate to wall-clock time improvements when training on different GPU architectures?
- Basis in paper: [inferred] The paper emphasizes memory reduction benefits of QFT but doesn't report training throughput or wall-clock time comparisons across different GPU architectures
- Why unresolved: While QFT reduces memory usage by 79%, the computational overhead of quantization/dequantization operations and their impact on training speed is not analyzed, particularly across different GPU architectures with varying integer operation capabilities
- What evidence would resolve it: Direct wall-clock time comparisons of QFT versus full-precision training across different GPU models (e.g., A6000, H100, A100), measuring total training time for equivalent epochs and analyzing the trade-off between memory savings and computational overhead

## Limitations

- Memory efficiency claims rely on specific LLaMA-2 architecture and empirically chosen quantization parameters
- Benchmark evaluation focuses on academic datasets without testing real-world deployment performance
- Lion optimizer's quantization robustness advantages lack extensive ablation studies and cross-optimizer validation

## Confidence

**High Confidence**: Memory reduction mechanism is well-supported through quantitative comparisons and clear algorithmic descriptions
**Medium Confidence**: Performance claims are reasonably supported by benchmark results, though evaluation could be more comprehensive
**Low Confidence**: Claims about quantization error robustness being uniquely beneficial for Lion optimizer lack direct experimental validation

## Next Checks

1. **Outlier Threshold Sensitivity Analysis**: Systematically vary the dense-and-sparse quantizer's outlier threshold (0.1% to 5%) and measure impacts on both memory usage and task performance across all five benchmarks

2. **Cross-Model Generalization**: Apply QFT to transformer architectures beyond LLaMA-2 (e.g., OPT, BLOOM) with varying model sizes to test whether the 79% memory reduction and performance claims hold across different model families

3. **Distribution Shift Robustness**: Evaluate QFT-tuned models on out-of-distribution datasets and adversarial examples to assess whether quantization-induced robustness translates to improved generalization under data distribution shifts