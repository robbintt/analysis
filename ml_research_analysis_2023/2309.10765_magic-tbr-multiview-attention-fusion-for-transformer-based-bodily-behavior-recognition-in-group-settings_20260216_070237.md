---
ver: rpa2
title: 'MAGIC-TBR: Multiview Attention Fusion for Transformer-based Bodily Behavior
  Recognition in Group Settings'
arxiv_id: '2309.10765'
source_url: https://arxiv.org/abs/2309.10765
tags:
- fusion
- multiview
- attention
- features
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of recognizing fine-grained
  bodily behaviors like gesturing, grooming, and fumbling in group settings, where
  subtle non-verbal cues play a crucial role in social interaction. The proposed MAGIC-TBR
  method integrates multiview attention fusion with a transformer-based approach to
  combine spatial and motion information from RGB and Discrete Cosine Transform (DCT)
  features.
---

# MAGIC-TBR: Multiview Attention Fusion for Transformer-based Bodily Behavior Recognition in Group Settings

## Quick Facts
- arXiv ID: 2309.10765
- Source URL: https://arxiv.org/abs/2309.10765
- Reference count: 26
- Primary result: Achieves 0.49 validation MAP and 0.57 test MAP on BBSI dataset for multiview bodily behavior recognition

## Executive Summary
This paper addresses the challenge of recognizing fine-grained bodily behaviors in group settings using multiview attention fusion with transformer-based approaches. The MAGIC-TBR method integrates RGB and Discrete Cosine Transform (DCT) features from three perspectives per individual, employing Swin Transformer networks for robust feature extraction. The model demonstrates significant improvements over baseline methods, particularly through bimodal fusion of RGB and DCT features. The approach shows strong performance on the BBSI dataset, achieving state-of-the-art results for recognizing 14 behavior classes in complex social interactions.

## Method Summary
The MAGIC-TBR method employs multiview attention fusion to combine spatial and motion information from RGB and DCT features extracted using Swin Transformer networks. The model processes three perspectives (frontal, left, right) per individual, with attention mechanisms learning to prioritize informative views for each behavior class. Bimodal fusion combines RGB features encoding spatial appearance with DCT features capturing high-frequency motion patterns. The approach uses early fusion before classification, trained with stochastic gradient descent and evaluated using mean average precision on the BBSI dataset.

## Key Results
- Achieves 0.49 validation MAP and 0.57 test MAP on BBSI dataset
- Bimodal fusion of RGB and DCT features outperforms single-modality approaches
- DCT features excel at detecting high-frequency motion patterns like fumbling and stretching
- Multiview attention consistently prioritizes frontal view for most behavior classes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiview attention fusion allows the model to prioritize informative views for each behavior class.
- Mechanism: The model learns attention weights for each view (frontal, left, right) based on their relative usefulness for classifying specific behaviors. Higher attention scores are assigned to views with better illumination or less occlusion for that class.
- Core assumption: Different views provide complementary information, and the relative importance of each view varies by behavior class.
- Evidence anchors:
  - [abstract] "The proposed MAGIC-TBR method integrates multiview attention fusion with a transformer-based approach to combine spatial and motion information from RGB and Discrete Cosine Transform (DCT) features."
  - [section] "The attention scores highlight the frontal view as having the highest attention, offering focused and less occluded videos for both RGB and DCT."

### Mechanism 2
- Claim: DCT features capture high-frequency motion patterns that RGB features miss.
- Mechanism: DCT transforms video frames into the frequency domain, where high-frequency coefficients represent rapid intensity changes like fidgeting or stretching. These patterns are difficult to detect directly in pixel space.
- Core assumption: High-frequency DCT coefficients correlate with subtle, high-speed motion patterns in bodily behavior.
- Evidence anchors:
  - [section] "We observe Multiview DCT method surpasses the baseline performance in five specific classes: fumble, scratching, stretching, leg movements, and even settle. Except for leg movement and settle, all classes involve hand movements."
  - [section] "A previous study [22] indicate that hand trajectory signals result in the increase in the high-frequency components."

### Mechanism 3
- Claim: Bimodal fusion of RGB and DCT features captures both spatial appearance and motion dynamics.
- Mechanism: RGB features encode static spatial information (poses, postures), while DCT features encode temporal motion patterns. Early fusion combines these complementary modalities before classification.
- Core assumption: Bodily behaviors can be decomposed into spatial and temporal components that are best captured by different feature types.
- Evidence anchors:
  - [abstract] "The proposed MAGIC-TBR method integrates multiview attention fusion with a transformer-based approach to combine spatial and motion information from RGB and Discrete Cosine Transform (DCT) features."
  - [section] "Bimodal Fusion is outperforming all other methods, including trimodal, possibly due to the need for a more robust representation of LaViLa features for fusion."

## Foundational Learning

- Concept: Multiview attention mechanisms
  - Why needed here: Different camera angles provide complementary views of the same behavior, and the model needs to learn which view is most informative for each class.
  - Quick check question: If the frontal view is occluded but side views are clear, which view should receive the highest attention weight for a hand-gesture classification task?

- Concept: Discrete Cosine Transform for motion analysis
  - Why needed here: DCT converts spatial video data into frequency domain, making high-frequency motion patterns more accessible for analysis.
  - Quick check question: What type of bodily behavior would be best detected using DCT features - slow postural changes or rapid fidgeting?

- Concept: Transformer-based feature extraction
  - Why needed here: Transformers can capture long-range dependencies in video sequences and learn complex spatial-temporal patterns.
- Quick check question: Why might a Swin Transformer be more effective than a standard CNN for this task?

## Architecture Onboarding

- Component map: RGB video → Swin Transformer → 1024-dim features; DCT video → Swin Transformer → 1024-dim features; Each modality → Multiview attention fusion → Class scores; Bimodal fusion → Final class scores
- Critical path: Video input → Feature extraction (Swin) → Multiview attention → Bimodal fusion → Classification
- Design tradeoffs: Bimodal fusion outperforms trimodal, suggesting LaViLa features may not be robust enough for early fusion; DCT excels at high-frequency motion but may be redundant for low-frequency behaviors
- Failure signatures: Poor performance on classes with subtle movements (smearing hands, scratching) suggests the model struggles with limited training samples or high-frequency patterns
- First 3 experiments:
  1. Ablation study: Remove DCT modality and measure impact on high-frequency behavior classes (fumble, stretching)
  2. Attention visualization: Plot attention weights across views for each behavior class to verify the model prioritizes appropriate views
  3. Class imbalance analysis: Examine performance differences between well-represented and under-represented behavior classes

## Open Questions the Paper Calls Out
1. How can the model's performance be improved for classes with subtle movements and limited samples, such as smearing hands and scratching?
2. How can advanced LaViLa-based features be incorporated into the model to improve its overall performance?
3. How can a more robust fusion architecture be designed to further improve the model's performance?

## Limitations
- Model's reliance on specific multiview configurations (frontal, left, right) may limit generalizability to different camera setups
- Limited examples for certain subtle behaviors constrain the model's ability to learn these patterns effectively
- Bimodal fusion outperforming trimodal suggests potential limitations in LaViLa feature representation

## Confidence
- High Confidence: The core mechanism of multiview attention fusion and bimodal fusion of RGB and DCT features is well-supported by experimental results
- Medium Confidence: The claim that DCT features capture high-frequency motion patterns is supported by performance improvements but lacks direct quantitative evidence
- Medium Confidence: The overall framework architecture and training procedure are sufficiently detailed for reproduction, though specific implementation details remain unclear

## Next Checks
1. Generate attention weight heatmaps across all three views for each behavior class to empirically verify appropriate view prioritization
2. Systematically analyze performance differences between well-represented and under-represented classes using per-class precision-recall curves and apply class weighting techniques
3. Conduct controlled experiments removing DCT features and measure the specific performance drop on high-frequency behavior classes to quantify DCT's contribution