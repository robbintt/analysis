---
ver: rpa2
title: 'Unveiling Theory of Mind in Large Language Models: A Parallel to Single Neurons
  in the Human Brain'
arxiv_id: '2309.01660'
source_url: https://arxiv.org/abs/2309.01660
tags:
- embeddings
- trials
- were
- belief
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated whether large language models (LLMs) exhibit
  Theory of Mind (ToM) capabilities comparable to human cognitive processes. The researchers
  analyzed hidden embeddings from multiple open-source LLMs across various ToM tasks,
  drawing parallels with single-neuron recordings from human brain studies.
---

# Unveiling Theory of Mind in Large Language Models: A Parallel to Single Neurons in the Human Brain

## Quick Facts
- arXiv ID: 2309.01660
- Source URL: https://arxiv.org/abs/2309.01660
- Reference count: 0
- Large language models develop Theory of Mind-like embedding patterns that parallel human neural activity

## Executive Summary
This study investigates whether large language models (LLMs) exhibit Theory of Mind (ToM) capabilities comparable to human cognitive processes. The researchers analyzed hidden embeddings from multiple open-source LLMs across various ToM tasks, drawing parallels with single-neuron recordings from human brain studies. They found that a significant percentage of artificial embeddings (up to 6.3% in some layers) showed selective responses to true versus false-belief trials, with this percentage strongly correlating with model size and task performance. The embeddings' activities could accurately decode others' beliefs with up to 81% accuracy, and these effects were robust against control conditions. The findings reveal that ToM-related embeddings emerge in larger LLMs and exhibit patterns similar to human brain neurons, suggesting a fundamental parallel between artificial and biological systems in processing others' perspectives.

## Method Summary
The researchers extracted hidden embeddings from four open-source LLM families (Falcon, LLaMa, Pythia, GPT-2) with parameter sizes ranging from 1B to 40B. They fed concatenated statements and questions into each model and extracted embeddings from all layers. Mann-Whitney U tests were performed on embedding values to identify significant differences between true-belief and false-belief trials. Decoding analysis used logistic regression with L2 regularization to predict trial types from embedding populations. Control experiments included permuted words and question-only inputs to validate ToM-specific effects. The percentage of significant embeddings and decoding accuracy were correlated with model size and task performance metrics.

## Key Results
- Up to 6.3% of embeddings showed selective responses to true versus false-belief trials, with percentages strongly correlated with model size
- Decoding accuracy of others' beliefs from embedding populations reached up to 81% for larger models
- Embedding selectivity and task performance both increased non-linearly with model size, showing emergent ToM-like patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs develop emergent neural-like embedding patterns that mirror human ToM-related neuron selectivity
- Mechanism: As model size increases, specific hidden embeddings exhibit statistically significant differences in activity between true-belief and false-belief trials, analogous to single-neuron firing rate modulations in the dmPFC
- Core assumption: Hidden embeddings function as artificial neurons whose activity patterns encode higher-level cognitive features like belief attribution
- Evidence anchors:
  - [abstract] "hidden embeddings (artificial neurons) within LLMs started to exhibit significant responsiveness to either true- or false-belief trials, suggesting their ability to represent another's perspective"
  - [section] "49 (23%) displayed significant changes in activities for true- or false-belief trials when human participants performed ToM tasks" (Fig. 2A)
- Break condition: If selectivity is driven purely by word frequency or linguistic structure rather than belief representation

### Mechanism 2
- Claim: ToM capability in LLMs emerges non-linearly with model size, correlating with both embedding selectivity and task performance
- Mechanism: Larger models develop more ToM-responsive embeddings that directly contribute to improved false-belief reasoning accuracy through distributed representation learning
- Core assumption: Model size enables more complex internal representations that capture perspective-taking nuances beyond simple pattern matching
- Evidence anchors:
  - [abstract] "These artificial embedding responses were closely correlated with the LLMs' performance during the ToM tasks, a property that was dependent on the size of the models"
  - [section] "For large models ( 12b), there was an average of 3.9% of embeddings responding to ToM tasks, and this percentage dropped to 0.6% for smaller models"
- Break condition: If performance gains stem from better general language understanding rather than ToM-specific mechanisms

### Mechanism 3
- Claim: Population-level embedding activity can decode others' beliefs with high accuracy, indicating distributed ToM representation
- Mechanism: The entire population of embeddings across layers contains sufficient information to distinguish true from false beliefs, with higher accuracy in larger models
- Core assumption: Distributed representation across many embeddings provides robust encoding of perspective information
- Evidence anchors:
  - [abstract] "the other's beliefs could be accurately decoded using the entire embeddings, indicating the presence of the embeddings' ToM capability at the population level"
  - [section] "large models ( 12b) showing an average of 75% decoding accuracy" versus "smaller models ( 7b)...average accuracy of 67%"
- Break condition: If decoding accuracy relies on superficial linguistic cues rather than genuine belief representation

## Foundational Learning

- Concept: Theory of Mind (ToM)
  - Why needed here: Understanding what ToM is and how it's tested (false-belief tasks) is essential to grasp why this research matters and what the results indicate
  - Quick check question: What distinguishes a true-belief trial from a false-belief trial in the context of ToM testing?

- Concept: Transformer architecture and embeddings
  - Why needed here: The study analyzes hidden embeddings from transformer models, so understanding what embeddings are and how they function as "artificial neurons" is crucial
  - Quick check question: How do hidden embeddings in transformer models relate to the concept of artificial neurons in this study?

- Concept: Statistical significance testing
  - Why needed here: The research uses Mann-Whitney U tests to identify responsive embeddings, requiring understanding of non-parametric statistical tests
  - Quick check question: What does it mean when an embedding shows "significant differences" between true- and false-belief trials using Mann-Whitney U test?

## Architecture Onboarding

- Component map: Input layer → Transformer modules → Hidden embeddings (artificial neurons) → Output layer → Statistical analysis pipeline
- Critical path: Embedding extraction → Statistical testing → Decoding analysis → Performance correlation
- Design tradeoffs: Model size vs. computational resources (40b models require 8 GPUs), statistical power vs. multiple comparison corrections, control stringency vs. ecological validity
- Failure signatures: Low embedding selectivity percentages (<1%) across all models, decoding accuracy near chance levels, performance gains correlating with general language ability rather than ToM-specific metrics
- First 3 experiments:
  1. Extract embeddings from a medium-sized model (7-12b) on ToM tasks and perform Mann-Whitney U tests to identify responsive embeddings
  2. Train a logistic regression decoder on embedding populations to predict trial types, comparing accuracy across layers
  3. Implement control experiments with permuted words and question-only inputs to validate ToM-specific effects

## Open Questions the Paper Calls Out
- How do the emergent ToM-related embeddings in LLMs specifically contribute to the models' ability to generate contextually appropriate responses in social scenarios?
- What are the long-term effects of fine-tuning LLMs on datasets specifically designed to enhance ToM capabilities?
- How do the ToM-related embeddings in LLMs compare to the neural processes in humans across different developmental stages?

## Limitations
- The study cannot definitively distinguish whether embedding selectivity reflects genuine ToM reasoning or simpler linguistic patterns that correlate with perspective-taking
- Limited to four LLM families and a single human neural recording dataset, which may not generalize across different ToM paradigms
- The functional equivalence between artificial embeddings and biological neurons remains an open question despite suggestive parallels

## Confidence
- High Confidence: The correlation between model size and both embedding selectivity percentage and decoding accuracy is well-supported by the presented data
- Medium Confidence: The parallel between artificial embeddings and human neurons in dmPFC is suggestive but requires more direct validation
- Low Confidence: The claim that these embeddings "represent another's perspective" goes beyond what the statistical evidence directly demonstrates

## Next Checks
1. Test the same embedding analysis approach on alternative ToM paradigms to verify whether selectivity generalizes beyond the current task structure
2. Systematically remove or control for linguistic features that might drive embedding selectivity to isolate genuine ToM-related signals
3. Conduct a more direct comparison between embedding selectivity patterns and human neural recordings using the same task and analysis pipeline