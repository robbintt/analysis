---
ver: rpa2
title: Atomic Inference for NLI with Generated Facts as Atoms
arxiv_id: '2305.13214'
source_url: https://arxiv.org/abs/2305.13214
tags:
- facts
- fact
- premise
- each
- fglr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a logical reasoning framework for NLI that
  generates premise facts using a language model, then predicts entailment/contradiction
  for each fact-hypothesis pair. Logical rules aggregate fact-level predictions into
  an overall class.
---

# Atomic Inference for NLI with Generated Facts as Atoms

## Quick Facts
- arXiv ID: 2305.13214
- Source URL: https://arxiv.org/abs/2305.13214
- Reference count: 22
- Primary result: FGLR improves ANLI performance by +1.5-3.1% over BERT/DeBERTa baselines and sets new SOTA on ANLI R3 (+0.47%)

## Executive Summary
This paper introduces a logical reasoning framework for Natural Language Inference (NLI) that decomposes complex premises into atomic facts using a language model, then applies deterministic logical rules to aggregate fact-level predictions into overall entailment/contradiction/neutral classifications. The approach, called Fact Generation and Logical Reasoning (FGLR), generates premise facts using GPT-3, independently assesses each fact-hypothesis relationship using BERT/DeBERTa, and combines predictions through logical rules. FGLR demonstrates improved performance on the challenging ANLI dataset, particularly in low-data settings, while providing interpretable and faithful predictions by identifying specific premise facts responsible for classification decisions.

## Method Summary
FGLR generates atomic facts from premises using GPT-3, then independently evaluates each fact-hypothesis pair using BERT/DeBERTa-base models. The model architecture includes attention mechanisms to learn fact relevance during training, even though only observation-level labels are provided. Fact-level predictions for entailment and contradiction are aggregated using deterministic logical rules: if any fact contradicts the hypothesis, the overall prediction is contradiction; if any fact entails the hypothesis (and none contradict), the overall prediction is entailment; otherwise, the prediction is neutral. The system is trained end-to-end using observation-level supervision with additional fact-level loss components to encourage attention to relevant facts.

## Key Results
- FGLR improves ANLI performance by +1.5-3.1% over BERT and DeBERTa-base baselines
- Sets new state-of-the-art on ANLI R3 with +0.47% improvement
- Outperforms all baselines in reduced-data setting (1k examples) with +2.9-6.3% improvement
- Fact-level predictions align with human expectations despite training only on observation-level labels
- Provides faithful, interpretable predictions by identifying specific premise facts responsible for decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fact generation decomposes complex premises into atomic components that the model can reason about independently
- Mechanism: LLM generates facts representing individual pieces of information from the premise. NLI model makes predictions for each fact-hypothesis pair separately rather than reasoning about entire premise at once
- Core assumption: Complex NLI examples can be broken down into simpler atomic facts without losing essential information needed for inference
- Evidence anchors: [abstract] "predictions for different components (or atoms) of an instance"; [section 2.1] "generate a fact list that itemizes all of the information contained within the premise"
- Break condition: If LLM fails to generate complete or accurate facts, decomposition breaks down and model loses necessary information

### Mechanism 2
- Claim: Logical rules aggregate fact-level predictions into reliable observation-level predictions
- Mechanism: Deterministic logical rules (contradiction detection, entailment detection) combine individual fact-level predictions into final NLI classification
- Evidence anchors: [abstract] "Logical rules aggregate fact-level predictions into an overall class"; [section 2.2] "use a series of logical rules to determine the overall class for the observation"
- Break condition: If fact-level predictions are noisy or unreliable, logical aggregation may produce incorrect final predictions

### Mechanism 3
- Claim: Training regime with observation-level supervision enables fact-level reasoning without requiring fact-level labels
- Mechanism: Attention mechanisms learn which facts are most relevant for observation-level prediction. Loss function includes observation-level and fact-level components, encouraging model to attend to relevant facts
- Core assumption: Model can learn to identify relevant facts for each class through observation-level supervision alone
- Evidence anchors: [section 2.3] "model only uses observation-level labels during training"; [section 2.3] "additional fact-level loss LFact c uses unnormalized attention values"
- Break condition: If attention mechanism fails to learn meaningful fact relevance, model cannot make reliable fact-level predictions during inference

## Foundational Learning

- Concept: Natural Language Inference (NLI) task and its three classes (entailment, contradiction, neutral)
  - Why needed here: The entire system is designed for NLI, so understanding the task is fundamental
  - Quick check question: What are the three possible relationships between a premise and hypothesis in NLI?

- Concept: Logical reasoning and how atomic propositions can be used for inference
  - Why needed here: Core approach is based on decomposing NLI into atomic facts and applying logical rules
  - Quick check question: How do logical rules like "if any fact contradicts, then contradiction" help in making NLI predictions?

- Concept: Transformer-based language models and attention mechanisms
  - Why needed here: System uses BERT/DeBERTa as baselines and attention for fact selection
  - Quick check question: How does attention in transformers help identify which facts are most relevant for a prediction?

## Architecture Onboarding

- Component map: Fact Generator (LLM) → Fact List → NLI Model (BERT/DeBERTa) → Fact-level Predictions → Logical Rules → Final Prediction

- Critical path:
  1. Generate facts from premise using LLM
  2. Encode each fact-hypothesis pair with transformer
  3. Apply attention to identify relevant facts
  4. Make fact-level predictions
  5. Apply logical rules to aggregate predictions

- Design tradeoffs:
  - Fact generation quality vs. computational cost (generating multiple fact lists)
  - Granularity of facts (too fine-grained loses context, too coarse defeats purpose)
  - Number of facts to generate (more facts = better coverage but higher computational cost)

- Failure signatures:
  - Poor fact generation quality → irrelevant or missing information in facts
  - Attention mechanism not learning → fact-level predictions are random
  - Logical rules not working → aggregation produces incorrect final predictions

- First 3 experiments:
  1. Generate facts for a few ANLI examples and manually verify if they capture essential information
  2. Implement fact-level prediction without logical rules to see if model can learn meaningful fact-level relationships
  3. Test logical rule aggregation on synthetic data where ground truth fact-level labels are known

## Open Questions the Paper Calls Out

- Question: How does the performance of FGLR vary with different fact generation strategies (FactComb, FactExt, HypCond) across different NLI datasets?
  - Basis in paper: Paper mentions combining Fact list 1 & 2 (FactComb) performs better than single fact list, and including hypothesis-conditioned facts (HypCond) further improves performance
  - Why unresolved: Paper only evaluates these strategies on ANLI dataset
  - What evidence would resolve it: Evaluating FGLR with different fact generation strategies on variety of NLI datasets and comparing results

- Question: What is the impact of using observation-level labels versus fact-level labels during training on the performance of FGLR?
  - Basis in paper: Paper states FGLR is trained using observation-level labels, but would be interesting to see how performance changes if fact-level labels were used instead
  - Why unresolved: Paper does not explore use of fact-level labels during training
  - What evidence would resolve it: Training FGLR using fact-level labels and comparing its performance to observation-level approach on various NLI datasets

- Question: How does the performance of FGLR change when applied to NLI datasets with longer premises or hypotheses?
  - Basis in paper: Paper focuses on ANLI dataset which uses multi-sentence premises and single-sentence hypotheses
  - Why unresolved: Paper does not explore performance of FGLR on datasets with longer premises or hypotheses
  - What evidence would resolve it: Evaluating FGLR on NLI datasets with longer premises or hypotheses and comparing its performance to ANLI results

## Limitations

- Fact generation quality dependency: Entire system's performance hinges on quality of facts generated by GPT-3, with limited analysis of fact generation quality beyond surface-level examples
- Limited ablation studies: Lacks comprehensive ablation studies isolating contributions of each component, making it unclear how much improvement comes from logical reasoning framework versus model capacity
- Reproducibility constraints: Critical implementation details like exact GPT-3 prompts and specific training hyperparameters are not fully specified

## Confidence

- High confidence: Core logical framework is well-defined and theoretically sound; observation that fact-level predictions align with human expectations is supported by qualitative examples
- Medium confidence: Quantitative improvements over baselines are promising but based on single dataset (ANLI) and may not generalize to other NLI benchmarks
- Low confidence: Claims about interpretability and faithfulness are primarily supported by qualitative examples rather than systematic human evaluation studies

## Next Checks

1. **Fact generation quality audit**: Manually annotate a random sample of 100 generated facts to assess coverage of premise information, presence of hallucinations, and granularity appropriateness. Calculate precision and recall of fact generation.

2. **Component ablation study**: Implement ablations removing key components: (a) remove logical rule aggregation (use majority voting instead), (b) remove attention mechanisms (use equal weighting), and (c) use gold facts instead of generated facts. Measure performance degradation to isolate each component's contribution.

3. **Cross-dataset generalization test**: Evaluate the trained FGLR model on other NLI datasets (e.g., SNLI, MNLI) without fine-tuning to assess whether logical reasoning framework generalizes beyond ANLI's specific characteristics.