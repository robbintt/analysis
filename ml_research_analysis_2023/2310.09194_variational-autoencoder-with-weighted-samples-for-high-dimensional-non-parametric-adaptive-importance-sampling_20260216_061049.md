---
ver: rpa2
title: Variational autoencoder with weighted samples for high-dimensional non-parametric
  adaptive importance sampling
arxiv_id: '2310.09194'
source_url: https://arxiv.org/abs/2310.09194
tags:
- distribution
- algorithm
- variational
- sampling
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of estimating high-dimensional
  probability density functions using weighted samples, a fundamental problem in adaptive
  importance sampling. The authors propose a novel method that uses a variational
  autoencoder (VAE) to approximate the target distribution, extending the existing
  VAE framework to handle weighted samples.
---

# Variational autoencoder with weighted samples for high-dimensional non-parametric adaptive importance sampling

## Quick Facts
- arXiv ID: 2310.09194
- Source URL: https://arxiv.org/abs/2310.09194
- Reference count: 9
- Primary result: VAE-based approach outperforms classical methods like Gaussian and Gaussian mixture models in high-dimensional adaptive importance sampling

## Executive Summary
This paper addresses the challenge of estimating high-dimensional probability density functions using weighted samples, a fundamental problem in adaptive importance sampling. The authors propose a novel method that uses a variational autoencoder (VAE) to approximate the target distribution, extending the existing VAE framework to handle weighted samples. To improve the flexibility of the model and enable it to learn multimodal distributions, they introduce a learnable prior distribution for the VAE latent variables and a new pre-training procedure to prevent posterior collapse. The method is applied to two multimodal problems in high dimensions, demonstrating its effectiveness in generating points from the target distribution and estimating rare event probabilities.

## Method Summary
The proposed method extends variational autoencoders to handle weighted samples for high-dimensional non-parametric adaptive importance sampling. The approach uses a VAE with a VampPrior (variational mixture of posteriors prior) and a new pre-training procedure to prevent posterior collapse. The model is trained by maximizing a weighted Evidence Lower Bound (wELBO) objective function, which incorporates importance weights for target estimation. The method consists of three main steps: initializing pseudo-inputs via supervised learning, initializing encoder/decoder via autoencoding with weighted loss, and training the full VAE by maximizing the weighted ELBO.

## Key Results
- VAE-based approach outperforms classical methods like Gaussian and Gaussian mixture models in high-dimensional settings
- The method successfully generates points from the target distribution and estimates rare event probabilities
- The proposed pre-training procedure effectively prevents posterior collapse and improves training stability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VAE with weighted samples approximates high-dimensional target distributions more flexibly than classical parametric families
- Mechanism: By encoding weighted samples into a latent space and learning a parameterized decoder, the VAE forms a flexible non-parametric-like family that adapts to multimodal structures without explicit mixture components
- Core assumption: The latent space dimension is large enough to capture the data structure but small enough to avoid curse of dimensionality in density estimation
- Evidence anchors:
  - [abstract] "The flexibility of the obtained family of distributions makes it as expressive as a non-parametric model"
  - [section 2.2] "the estimated distribution gθ can be seen as an infinite mixture of Gaussian distributions"
- Break condition: If latent space dimension is too small or posterior collapse occurs, the model fails to represent complex multimodal distributions

### Mechanism 2
- Claim: VampPrior improves multimodal learning by providing a learnable mixture of posteriors as the prior
- Mechanism: Instead of using a fixed standard normal prior, VampPrior uses pseudo-inputs to form a mixture that adapts to the aggregated posterior, encouraging the model to capture multiple modes
- Core assumption: The number of pseudo-inputs K is sufficient to cover the modes of the target distribution
- Evidence anchors:
  - [section 2.3] "The VampPrior is flexible enough to be adapted to many kinds of problems"
  - [section 5.2.1] "The AIS-VAE algorithm finds both modes more than 70% of the time"
- Break condition: If K is too small relative to the number of modes, the model may miss some modes

### Mechanism 3
- Claim: Pre-initialization prevents posterior collapse and improves training stability
- Mechanism: By initializing pseudo-inputs via supervised learning and encoder/decoder via autoencoding, the model starts from a better local optimum that preserves information in the latent space
- Core assumption: The initial supervised/autoencoder phase produces meaningful latent representations before full VAE training
- Evidence anchors:
  - [section 4.2] "we propose a pre-initialisation procedure of the weights of the neural networks ϕ, θ and λ"
  - [section 5.2.1] "either both modes, and so the whole target distribution, are perfectly found and approximated"
- Break condition: If initialization is poor, posterior collapse may still occur despite the procedure

## Foundational Learning

- Concept: Variational inference and the evidence lower bound (ELBO)
  - Why needed here: The entire VAE training relies on maximizing the ELBO, which balances reconstruction accuracy and regularization
  - Quick check question: What happens to the ELBO if the variational posterior collapses to the prior?

- Concept: Importance sampling and weighted samples
  - Why needed here: The method extends VAEs to handle weighted samples, enabling adaptive importance sampling where the target is known only up to a constant
  - Quick check question: How does the weighted ELBO differ from the standard ELBO in terms of the target distribution?

- Concept: Curse of dimensionality and non-parametric methods
  - Why needed here: Explains why classical kernel methods fail in high dimensions and why VAEs offer a scalable alternative
  - Quick check question: Why does kernel density estimation suffer from the curse of dimensionality while VAEs do not?

## Architecture Onboarding

- Component map: Input x -> Encoder network (μx, Σx) -> Latent z -> Decoder network (μz, Σz) -> Output reconstruction

- Critical path:
  1. Initialize pseudo-inputs via supervised learning on weighted sample subset
  2. Initialize encoder/decoder via autoencoding with weighted loss
  3. Train full VAE by maximizing weighted ELBO
  4. Generate samples from learned distribution for importance sampling

- Design tradeoffs:
  - Latent dimension vs. expressiveness: higher dz captures more modes but increases training complexity
  - Number of pseudo-inputs K vs. flexibility: more K improves multimodal capture but risks overfitting
  - Weight initialization vs. posterior collapse: better initialization reduces collapse but adds training steps

- Failure signatures:
  - Posterior collapse: KL term in ELBO vanishes, decoder ignores latent space
  - Mode missing: generated samples miss some modes of target distribution
  - Over-regularization: model too constrained, fails to fit complex data

- First 3 experiments:
  1. Train VAE on 2D bimodal Gaussian with weighted samples, visualize latent space and generated samples
  2. Compare AIS-VAE vs AIS-GM on 10D bimodal target, measure success rate and KL divergence
  3. Test posterior collapse prevention by training with and without pre-initialization on multimodal data

## Open Questions the Paper Calls Out

- Question: How does the dimensionality reduction performed by the VAE contribute to its superior performance compared to Gaussian mixture models in high-dimensional adaptive importance sampling?
- Question: What is the optimal number of components for the VampPrior in a VAE when approximating multimodal distributions?
- Question: How does the proposed pre-training procedure for the VAE weights affect the prevention of posterior collapse compared to other existing methods?

## Limitations
- Performance in very high dimensions (>100D) remains untested
- Computational complexity of the VampPrior with many pseudo-inputs could become prohibitive
- Pre-training procedure adds complexity to the training pipeline

## Confidence
- High Confidence: The fundamental mechanism of using weighted ELBO for importance sampling (backed by variational inference theory)
- Medium Confidence: The effectiveness of VampPrior in preventing mode collapse (supported by experimental results but limited to 2-10D examples)
- Low Confidence: The scalability claims to truly high-dimensional problems (only tested up to 10D in experiments)

## Next Checks
1. Test the method on a 50-100 dimensional multimodal Gaussian mixture to verify scalability claims and measure computational complexity growth
2. Conduct ablation studies removing the pre-training procedure to quantify its impact on posterior collapse prevention across different problem difficulties
3. Compare against modern normalising flow approaches on the same benchmark problems to establish relative performance in high dimensions