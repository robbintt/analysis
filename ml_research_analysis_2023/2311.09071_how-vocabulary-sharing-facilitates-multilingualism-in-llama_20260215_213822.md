---
ver: rpa2
title: How Vocabulary Sharing Facilitates Multilingualism in LLaMA?
arxiv_id: '2311.09071'
source_url: https://arxiv.org/abs/2311.09071
tags:
- languages
- multilingual
- language
- performance
- bilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the multilingual capabilities of large
  language models (LLMs) when trained only on certain languages. The authors analyze
  101 languages using an embedding fine-tuning approach, categorizing languages into
  four quadrants based on performance gains: reciprocal, altruistic, selfish, and
  idle.'
---

# How Vocabulary Sharing Facilitates Multilingualism in LLaMA?

## Quick Facts
- arXiv ID: 2311.09071
- Source URL: https://arxiv.org/abs/2311.09071
- Reference count: 21
- Key outcome: Vocabulary sharing and language family similarities significantly influence multilingual performance in LLMs, with Indo-European languages showing enhanced cross-lingual transfer through shared subword representations.

## Executive Summary
This study investigates the multilingual capabilities of large language models (LLMs) when trained only on certain languages. Using an embedding fine-tuning approach on 101 languages, the authors categorize languages into four quadrants based on performance gains: reciprocal, altruistic, selfish, and idle. They find that existing LLMs possess stronger multilingual capabilities than expected and provide actionable guidelines for tuning different language groups. The study reveals that vocabulary sharing and language family similarities significantly influence multilingual performance, and proposes effective strategies including embedding fine-tuning, minimal dataset full fine-tuning, and subword sequence shortening to improve multilingual performance.

## Method Summary
The study fine-tunes LLaMA-7B using an embedding fine-tuning (Embed FT) strategy on bilingual instruction translation data covering 101 languages. The methodology involves evaluating bilingual and multilingual performance using spBLEU on Flores-101 devtest, then categorizing languages into four quadrants based on performance changes. The authors compare different fine-tuning strategies (Embed FT, Full FT, LoRA) and analyze tokenization patterns to understand how vocabulary sharing affects multilingual capabilities. The study also investigates over-tokenization issues and proposes solutions for improving performance of languages in the idle quadrant.

## Key Results
- Vocabulary sharing through BBPE tokenization enables significant cross-lingual transfer, particularly for Indo-European languages that cluster in the reciprocal quadrant
- Over-tokenization degrades multilingual performance by reducing information density, especially for languages with complex character sets
- Altruistic quadrant behavior is explained by error type transitions from "source copy" to "oscillatory hallucination" errors, rather than true performance decline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Vocabulary sharing enables cross-lingual transfer through shared subword representations
- Mechanism: LLaMA's BBPE tokenizer creates overlapping token vocabularies across languages, allowing parameters tuned on one language to benefit others with shared tokens
- Core assumption: Languages with similar token distributions (particularly Indo-European) will show enhanced multilingual performance when fine-tuned on one member
- Evidence anchors:
  - [abstract]: "The study reveals that vocabulary sharing and language family similarities significantly influence multilingual performance"
  - [section 3.2]: "The reciprocal quadrant is predominantly occupied by Indo-European languages...These languages are grouped together mainly due to their shared vocabulary and grammatical affixes"
  - [corpus]: Found 25 related papers, average neighbor FMR=0.351, indicating moderate relatedness to multilingual vocabulary sharing concepts
- Break condition: When languages use entirely disjoint character sets with minimal token overlap, vocabulary sharing benefits diminish significantly

### Mechanism 2
- Claim: Over-tokenization reduces information density for certain languages, degrading performance
- Mechanism: BBPE tokenization creates excessively long token sequences for languages with complex character sets, spreading information across too many tokens and reducing per-token information density
- Core assumption: Token sequences longer than a certain threshold (relative to sentence length) indicate over-tokenization that harms performance
- Evidence anchors:
  - [section 4.3]: "We find that the over-tokenization phenomenon is prevalent in LLaMA...a sentence in lo that contains 6 words expands to 352 tokens after tokenization"
  - [section 4.3]: "Over-tokenization leads to a decrease in information density for LLM"
  - [corpus]: Moderate relatedness to tokenization issues in multilingual models (FMR=0.579)
- Break condition: When token-to-character ratios fall below critical thresholds or when language-specific token patterns differ substantially from English

### Mechanism 3
- Claim: Error type transitions explain altruistic quadrant behavior - models shift from easily-scored errors to harder-to-score errors
- Mechanism: Fine-tuning changes the dominant error type from "source copy" (which receives moderate scores) to "oscillatory hallucination" (which receives very low scores), creating the appearance of performance decline while actually improving multilingual capability
- Core assumption: The scoring system's sensitivity to different error types drives the observed altruistic behavior in quadrant classification
- Evidence anchors:
  - [section 4.2]: "The primary error for LLaMA is 'source copy'...However, for the fine-tuned LLaMA, the main error shifts to 'oscillatory hallucination'"
  - [section 4.2]: "The improvement in multilingual performance, on the other hand, stems from vocabulary sharing"
  - [corpus]: Found related papers on multilingual model performance variations, suggesting broader interest in error type analysis
- Break condition: When evaluation metrics become more robust to error type variations or when models learn to avoid oscillatory behavior entirely

## Foundational Learning

- Concept: Quadrant-based performance analysis using two-dimensional Cartesian systems
  - Why needed here: The paper's core methodology relies on classifying languages into four quadrants based on bilingual vs multilingual performance changes, requiring understanding of this analytical framework
  - Quick check question: How would you calculate the quadrant classification for a language that shows +10% bilingual improvement but -5% multilingual improvement?

- Concept: BBPE tokenization and subword modeling
  - Why needed here: The entire analysis depends on understanding how BBPE creates vocabulary sharing across languages and how over-tokenization affects different language families
  - Quick check question: Why does BBPE use UTF-8 encoding and how does this enable vocabulary sharing across languages with non-overlapping character sets?

- Concept: Fine-tuning strategies and their parameter efficiency
  - Why needed here: The paper compares Embed FT, Full FT, and LoRA strategies, requiring understanding of their computational and performance tradeoffs
  - Quick check question: What is the key difference between Embed FT and Full FT in terms of which parameters are updated during training?

## Architecture Onboarding

- Component map: Tokenizer (BBPE-based) -> Embedding layer -> Lower transformer layers (0-14) -> Higher transformer layers (15-31) -> Evaluation pipeline (spBLEU on Flores-101)

- Critical path:
  1. Tokenize source text using BBPE tokenizer
  2. Pass through embedding layer (possibly fine-tuned)
  3. Process through lower transformer layers (possibly fine-tuned)
  4. Generate output and calculate spBLEU score
  5. Compare performance changes before/after fine-tuning to determine quadrant classification

- Design tradeoffs:
  - Embed FT vs Full FT: Embed FT preserves multilingual capabilities better but may sacrifice some bilingual performance gains
  - Vocabulary size: Larger vocabularies reduce over-tokenization but increase memory requirements and may reduce sharing benefits
  - Fine-tuning scope: Lower layers capture more multilingual patterns while higher layers specialize more for specific languages

- Failure signatures:
  - Over-tokenization: Token-to-character ratios exceeding 20-30x for certain languages
  - Quadrant misclassification: Languages showing unexpected performance patterns that don't align with their expected family characteristics
  - Diminishing returns: Performance improvements plateauing despite increasing training data

- First 3 experiments:
  1. Replicate quadrant classification on a subset of 10 languages using the Embed FT strategy to verify methodology
  2. Test the over-tokenization hypothesis by comparing performance of languages with high vs low token-to-character ratios
  3. Validate the altruistic quadrant mechanism by analyzing error types before and after fine-tuning using qualitative error analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do language family similarities quantitatively impact the effectiveness of multilingual models beyond vocabulary sharing?
- Basis in paper: [explicit] The paper discusses how reciprocal quadrant languages (mostly Indo-European) benefit from shared vocabulary and grammatical affixes, but doesn't provide quantitative metrics on family similarity impact
- Why unresolved: While the paper establishes correlation between language family and performance, it doesn't isolate or measure the specific contribution of family similarity separate from vocabulary overlap
- What evidence would resolve it: Controlled experiments comparing models trained on languages with varying degrees of family similarity but controlled vocabulary overlap, with performance metrics for each configuration

### Open Question 2
- Question: What is the optimal balance between dataset size and parameter tuning strategy for altruistic quadrant languages?
- Basis in paper: [explicit] The paper mentions that full fine-tuning with minimal datasets works best for altruistic languages, but doesn't determine the optimal dataset size or provide a framework for finding this balance
- Why unresolved: The paper shows that larger datasets lead to overfitting in altruistic languages, but doesn't establish clear thresholds or guidelines for dataset size selection
- What evidence would resolve it: Empirical studies mapping dataset size against performance metrics (bilingual and multilingual) for altruistic languages across multiple parameter tuning strategies

### Open Question 3
- Question: What are the precise linguistic characteristics that cause languages to fall into the idle quadrant?
- Basis in paper: [explicit] The paper identifies over-tokenization as a key factor for idle languages but doesn't comprehensively characterize all linguistic features contributing to this categorization
- Why unresolved: While over-tokenization is discussed, the paper doesn't systematically analyze other potential linguistic factors (morphological complexity, syntactic structure, etc.) that might contribute to idle quadrant placement
- What evidence would resolve it: Detailed linguistic analysis of idle quadrant languages comparing their structural features against non-idle languages, combined with controlled experiments modifying these features

## Limitations
- The quadrant-based methodology assumes linear relationships between bilingual and multilingual performance changes, which may not capture complex interactions in real-world multilingual scenarios
- The over-tokenization analysis shows severe effects for specific languages but lacks clear thresholds for identifying problematic tokenization across diverse language families
- The altruistic quadrant error type transition explanation relies heavily on qualitative analysis without sufficient quantitative validation across multiple evaluation metrics

## Confidence

**High Confidence**: The vocabulary sharing mechanism is well-supported by empirical evidence showing Indo-European languages clustering in the reciprocal quadrant due to shared grammatical affixes and token distributions.

**Medium Confidence**: The over-tokenization analysis shows clear evidence of excessive tokenization for specific languages, but proposed solutions lack detailed validation across diverse language families.

**Low Confidence**: The altruistic quadrant error type transition explanation relies heavily on qualitative error analysis, with the claim that this represents genuine multilingual capability improvement remaining insufficiently validated.

## Next Checks
1. Quantify tokenization thresholds: Systematically test performance across token-to-character ratios from 5:1 to 50:1 for 10 diverse languages to identify the precise threshold where over-tokenization begins degrading multilingual performance.

2. Evaluate scoring robustness: Replicate the altruistic quadrant analysis using multiple evaluation metrics (e.g., chrF, TER) to determine whether the error type transition pattern persists across different scoring systems.

3. Test vocabulary expansion strategies: Implement and compare three different vocabulary expansion approaches for idle languages across 20 representative languages to identify the most effective method for improving multilingual performance.