---
ver: rpa2
title: Scaling Sentence Embeddings with Large Language Models
arxiv_id: '2307.16645'
source_url: https://arxiv.org/abs/2307.16645
tags:
- sentence
- learning
- llms
- embeddings
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) can generate high-quality sentence
  embeddings without fine-tuning when combined with a simple prompt-based representation
  method and in-context learning demonstrations. Using 300 automatically generated
  demonstrations, OPT models with over 1 billion parameters outperform contrastive
  learning methods on semantic textual similarity tasks.
---

# Scaling Sentence Embeddings with Large Language Models

## Quick Facts
- arXiv ID: 2307.16645
- Source URL: https://arxiv.org/abs/2307.16645
- Reference count: 40
- One-line primary result: Large language models can generate high-quality sentence embeddings without fine-tuning when combined with a prompt-based representation method and in-context learning demonstrations

## Executive Summary
This paper demonstrates that large language models can generate high-quality sentence embeddings without any fine-tuning by using a simple prompt-based method combined with in-context learning demonstrations. The authors propose a new Prompt-based method with Explicit One word Limitation (PromptEOL) that leverages the autoregressive nature of LLMs to represent semantic information as a single token. By scaling model size from millions to billions of parameters, they observe significant improvements on semantic textual similarity tasks, though scaling beyond tens of billions of parameters may harm performance. Fine-tuning with contrastive learning and parameter-efficient methods further boosts performance, with a 2.7B OPT model surpassing a 4.8B ST5 model on STS tasks.

## Method Summary
The method involves using large language models (OPT and LLaMA) to generate sentence embeddings through a prompt-based approach called PromptEOL. The core idea is to design a prompt template that explicitly limits the semantic information to a single word, leveraging the autoregressive model's ability to generate contextually coherent tokens. In-context learning is then applied by providing 300 automatically generated demonstrations (100 from STS-B training set and 200 from Oxford dictionary) to guide the model in representing sentence semantics. The models are evaluated on STS tasks (STS12-16, STS-B, SICK-R) and transfer tasks (MR, CR, SUBJ, MPQA, SST-2, TREC, MRPC). For further improvements, the authors fine-tune the models using contrastive learning with QLoRA and LoRA parameter-efficient methods.

## Key Results
- LLMs with 1B+ parameters outperform contrastive learning methods on STS tasks without any fine-tuning when using 300 in-context learning demonstrations
- Scaling model size from millions to billions of parameters improves STS performance, but scaling beyond tens of billions harms results
- 2.7B OPT model with prompt-based method and fine-tuning surpasses 4.8B ST5 model on STS tasks
- LLMs with tens of billions of parameters achieve state-of-the-art performance on transfer tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models (LLMs) can generate high-quality sentence embeddings without fine-tuning when combined with a simple prompt-based representation method and in-context learning demonstrations.
- Mechanism: The LLM is prompted with a structured template that explicitly limits the semantic information to a single word, leveraging the autoregressive model's ability to generate contextually coherent tokens. In-context learning then enhances this by providing demonstrations that guide the model in representing the semantic content of sentences.
- Core assumption: The autoregressive nature of LLMs allows them to effectively encode sentence meaning into a single token when prompted with a template that includes an explicit "one word limitation."
- Evidence anchors:
  - [abstract] "Through extensive experiments, in-context learning enables LLMs to generate high-quality sentence embeddings without any fine-tuning."
  - [section 3.1] "We propose a new Prompt-based method with Explicit One word Limitation (PromptEOL) for LLMs... by directly adding some tokens in the template to instruct LLMs in predicting the meaning of sentence in one word."
  - [corpus] Weak - the corpus neighbors do not directly address this specific mechanism.
- Break condition: If the template fails to constrain the LLM to a single word or if the demonstrations are not relevant to the task, the quality of the sentence embeddings may degrade.

### Mechanism 2
- Claim: Scaling up the model parameters improves performance on semantic textual similarity (STS) tasks, but scaling beyond tens of billions of parameters may harm results on STS tasks.
- Mechanism: Larger models have more parameters to capture complex semantic relationships, which can lead to better performance on STS tasks. However, beyond a certain point, the increased complexity may not translate to better performance and could even lead to overfitting or other issues.
- Core assumption: The relationship between model size and performance on STS tasks follows a curve where initial increases in size lead to improvements, but further increases may lead to diminishing returns or even degradation.
- Evidence anchors:
  - [abstract] "By scaling model size, we find scaling to more than tens of billion parameters harms the performance on semantic textual similarity (STS) tasks."
  - [section 3.2] "By scaling up the parameters of LLMs, we find that transitioning from millions to billions of parameters results in improvements on STS tasks. However, continue scaling up may not yield further improvements."
  - [corpus] Weak - the corpus neighbors do not directly address this specific mechanism.
- Break condition: If the model size exceeds a certain threshold (tens of billions of parameters), the performance on STS tasks may start to degrade.

### Mechanism 3
- Claim: Fine-tuning LLMs with contrastive learning and parameter-efficient methods further boosts performance on STS tasks.
- Mechanism: Fine-tuning allows the LLM to adapt to the specific task of generating sentence embeddings by adjusting its parameters based on the contrastive learning objective. Parameter-efficient methods like QLoRA reduce the memory requirements, making fine-tuning feasible even with limited computational resources.
- Core assumption: The contrastive learning framework is effective for learning sentence embeddings, and parameter-efficient fine-tuning methods can achieve similar performance to full fine-tuning with reduced computational costs.
- Evidence anchors:
  - [abstract] "We also fine-tune LLMs with current contrastive learning approach, and the 2.7B OPT model, incorporating our prompt-based method, surpasses the performance of 4.8B ST5, achieving the new state-of-the-art results on STS tasks."
  - [section 3.3] "To solve this problem, we leverage current efficient fine-tuning method QLoRA [DPHZ23]. QLoRA combines two techniques to significantly reduces memory usage: 4-bit quantization and parameter efficient fine-tuning."
  - [corpus] Weak - the corpus neighbors do not directly address this specific mechanism.
- Break condition: If the contrastive learning framework is not effective for the specific task or if the parameter-efficient fine-tuning methods do not adequately adjust the model parameters, the performance boost may not be realized.

## Foundational Learning

- Concept: Autoregressive models
  - Why needed here: Understanding how autoregressive models generate text sequentially is crucial for designing prompts that effectively guide the model in generating sentence embeddings.
  - Quick check question: How does an autoregressive model generate text, and why is this relevant for prompt-based representation methods?

- Concept: Contrastive learning
  - Why needed here: Contrastive learning is the framework used to fine-tune the LLM for the specific task of generating sentence embeddings. Understanding its principles is essential for designing the fine-tuning process.
  - Quick check question: What is contrastive learning, and how does it help in learning sentence embeddings?

- Concept: In-context learning
  - Why needed here: In-context learning is the mechanism used to enhance the LLM's ability to generate sentence embeddings without fine-tuning. Understanding how it works is crucial for designing the demonstration sets and prompts.
  - Quick check question: How does in-context learning work, and why is it effective for enhancing the LLM's performance on sentence embeddings?

## Architecture Onboarding

- Component map: LLM -> Prompt template -> Demonstration set -> Contrastive learning framework -> Parameter-efficient fine-tuning method
- Critical path: 1. Design the prompt template with explicit "one word limitation" 2. Construct the demonstration set for in-context learning 3. Evaluate the LLM's performance on STS tasks without fine-tuning 4. Scale the model size and observe the impact on performance 5. Fine-tune the LLM with contrastive learning and parameter-efficient methods 6. Evaluate the final performance on STS tasks and transfer tasks
- Design tradeoffs: Prompt design vs. model performance (more complex prompts may lead to better performance but could also be harder to design and implement); Model size vs. computational resources (larger models may perform better but require more computational resources); Fine-tuning vs. in-context learning (fine-tuning may lead to better performance but requires more computational resources and data)
- Failure signatures: Poor performance on STS tasks (indicates issues with the prompt design, demonstration set, or model size); High computational costs (indicates issues with the parameter-efficient fine-tuning methods or model size); Overfitting (indicates issues with the model size or fine-tuning process)
- First 3 experiments: 1. Evaluate the performance of the LLM with the prompt template on STS tasks without any demonstrations 2. Construct a demonstration set and evaluate the impact of in-context learning on the LLM's performance 3. Scale the model size and observe the impact on performance, identifying the threshold where further scaling may harm results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal model size for LLMs to generate sentence embeddings without fine-tuning, considering the trade-off between performance and computational resources?
- Basis in paper: [explicit] The paper states that scaling up model size improves performance on STS tasks up to a certain point, but scaling beyond tens of billions of parameters may harm performance.
- Why unresolved: The paper does not provide a definitive answer on the optimal model size, and the trade-off between performance and computational resources is not fully explored.
- What evidence would resolve it: A systematic study comparing the performance and computational requirements of LLMs with different model sizes on various STS tasks would provide insights into the optimal model size.

### Open Question 2
- Question: How does the choice of demonstration set impact the performance of in-context learning for sentence embeddings?
- Basis in paper: [explicit] The paper uses 300 demonstrations (100 from STS-B and 200 from Oxford dictionary), but the quality and relevance of these demonstrations to the specific STS tasks are not thoroughly evaluated.
- Why unresolved: The paper does not explore the impact of different demonstration sets on the performance of in-context learning, and it is unclear whether the chosen demonstrations are optimal.
- What evidence would resolve it: A comparative study evaluating the performance of in-context learning using different demonstration sets on various STS tasks would shed light on the impact of demonstration choice.

### Open Question 3
- Question: What is the impact of fine-tuning on the performance of LLMs for sentence embeddings, and how does it compare to in-context learning?
- Basis in paper: [explicit] The paper demonstrates that fine-tuning LLMs with contrastive learning can achieve state-of-the-art performance on STS tasks, even with limited computational resources.
- Why unresolved: The paper does not provide a comprehensive comparison between fine-tuning and in-context learning, and it is unclear which approach is more effective for different tasks and model sizes.
- What evidence would resolve it: A comparative study evaluating the performance of fine-tuning and in-context learning on various STS tasks using different model sizes would provide insights into the relative effectiveness of these approaches.

## Limitations

- The exact prompt template used for generating demonstration words is not fully specified, creating uncertainty about reproducibility
- The quality and relevance of the demonstration sets to specific STS tasks are not thoroughly evaluated
- The scaling effects beyond tens of billions of parameters are based on observations from specific models (OPT and LLaMA) and may not generalize to other architectures or tasks

## Confidence

- High Confidence: The core claim that LLMs can generate high-quality sentence embeddings without fine-tuning using a simple prompt-based method and in-context learning demonstrations is well-supported by experimental results.
- Medium Confidence: The observation that scaling model size from millions to billions of parameters improves STS performance, but scaling beyond tens of billions may harm results, is supported by experiments on OPT and LLaMA models. However, the generalizability to other architectures and tasks is uncertain.
- Medium Confidence: The claim that fine-tuning with contrastive learning and parameter-efficient methods further boosts performance is supported by experiments. However, the specific impact of different fine-tuning methods and their interaction with the prompt-based approach is not fully explored.

## Next Checks

1. **Prompt Template Validation**: Conduct a systematic study to evaluate the impact of different prompt templates on the quality of sentence embeddings generated by LLMs. This includes testing variations in the explicit "one word limitation" and the structure of the prompt.

2. **Scaling Effects Across Architectures**: Test the scaling hypothesis on a wider range of LLM architectures (e.g., GPT, BERT) and tasks to determine whether the observed effects are generalizable or specific to OPT and LLaMA models.

3. **Demonstration Set Quality Assessment**: Evaluate the quality and relevance of the demonstration sets used for in-context learning. This could involve analyzing the correlation between demonstration quality and STS performance, and exploring methods to automatically generate or select high-quality demonstrations.