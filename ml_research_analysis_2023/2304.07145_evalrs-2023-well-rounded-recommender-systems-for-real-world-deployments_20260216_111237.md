---
ver: rpa2
title: EvalRS 2023. Well-Rounded Recommender Systems For Real-World Deployments
arxiv_id: '2304.07145'
source_url: https://arxiv.org/abs/2304.07145
tags:
- will
- evaluation
- workshop
- systems
- evalrs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EvalRS 2023 is a workshop focused on the multi-dimensional and
  multi-faceted evaluation of recommender systems, emphasizing real-world impact across
  diverse deployment scenarios. It aims to expand beyond traditional accuracy metrics
  to include aspects like fairness, bias, usefulness, and informativeness.
---

# EvalRS 2023. Well-Rounded Recommender Systems For Real-World Deployments

## Quick Facts
- arXiv ID: 2304.07145
- Source URL: https://arxiv.org/abs/2304.07145
- Reference count: 17
- Primary result: Workshop focused on multi-dimensional evaluation of recommender systems beyond accuracy, featuring hackathon and open artifacts

## Executive Summary
EvalRS 2023 is a workshop designed to expand recommender system evaluation beyond traditional accuracy metrics to include fairness, bias, usefulness, and informativeness. The event features a hackathon format to bridge theoretical concepts with practical implementation, bringing together diverse stakeholders from academia, industry, and other sectors. All workshop artifacts including video recordings and hackathon materials will be released openly to maximize community impact.

## Method Summary
The workshop employs a multi-faceted approach to recommender system evaluation through paper presentations, keynote speeches, and hands-on hackathon participation. Participants work with real-world datasets to apply evaluation techniques that go beyond accuracy, focusing on dimensions like fairness and social impact. The method emphasizes collaborative learning through shared datasets, open-source tools, and team-based problem solving in the hackathon format.

## Key Results
- Expands evaluation metrics beyond accuracy to include fairness, bias, usefulness, and informativeness
- Implements hackathon format for hands-on learning and cross-disciplinary collaboration
- Provides open access to all workshop artifacts for broader community benefit

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hackathon format bridges theory and practice
- Mechanism: Shared datasets, open-source tools, and collaborative teamwork enable direct application of evaluation techniques
- Core assumption: Hands-on problem solving accelerates learning compared to passive consumption
- Evidence anchors: Workshop features hackathon for hands-on experience; hackathon provides unique opportunity to test evaluation skills
- Break condition: Participants lack basic familiarity with evaluation metrics or programming

### Mechanism 2
- Claim: Multi-disciplinary participation leads to richer evaluation frameworks
- Mechanism: Diverse stakeholders ensure evaluation methods consider fairness, bias, and real-world constraints
- Core assumption: Diverse input surfaces blind spots single-domain researchers might miss
- Evidence anchors: Workshop fosters debate on rounded evaluation across deployment scenarios; expands beyond accuracy to include fairness and bias
- Break condition: Workshop fails to facilitate genuine dialogue between groups with conflicting priorities

### Mechanism 3
- Claim: Open-sourcing artifacts creates multiplier effect beyond physical event
- Mechanism: Released video recordings, datasets, and materials enable global participation and continuous learning
- Core assumption: Knowledge transfer is more effective when materials are accessible and reusable
- Evidence anchors: Workshop artifacts will be released openly; all materials in accessible format
- Break condition: Released materials are poorly organized or lack documentation

## Foundational Learning

- Concept: Multi-dimensional evaluation beyond accuracy
  - Why needed here: Traditional accuracy metrics fail to capture fairness, bias, usefulness, and social impact
  - Quick check question: What are three dimensions beyond accuracy that EvalRS 2023 explicitly emphasizes?

- Concept: Slice-based metrics and fairness assessment
  - Why needed here: Understanding performance across user segments reveals hidden biases and fairness issues
  - Quick check question: Why might a system performing well overall still fail fairness criteria when evaluated by subgroups?

- Concept: Behavioral testing and robustness
  - Why needed here: Evaluating responses to adversarial inputs ensures real-world reliability
  - Quick check question: What is one advantage of behavioral testing over traditional holdout set evaluation?

## Architecture Onboarding

- Component map: Paper submission and review system -> Hackathon infrastructure -> Video recording pipeline -> Sponsorship management -> Website platform
- Critical path: Paper submission → Review process → Camera-ready → Workshop execution (talks + hackathon) → Artifact release
- Design tradeoffs: Open submission (broader participation) vs. curated selection (higher quality), hackathon focus (hands-on learning) vs. traditional talks (research dissemination)
- Failure signatures: Low submission quality, hackathon disengagement, poor cross-domain collaboration, artifacts not being used post-workshop
- First 3 experiments: 1) Small-scale mock hackathon to test tooling, 2) Pilot paper submission workflow, 3) Test video recording and distribution pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the most effective multi-dimensional evaluation metrics for recommender systems that go beyond accuracy?
- Basis in paper: Workshop aims to expand evaluation beyond traditional accuracy metrics
- Why unresolved: No consensus on best multi-dimensional metrics for comprehensive evaluation
- What evidence would resolve it: Empirical studies comparing effectiveness of different multi-dimensional metrics

### Open Question 2
- Question: How can offline evaluation be made more trustworthy and unbiased?
- Basis in paper: Workshop encourages submissions on making offline evaluation more trustworthy
- Why unresolved: Current offline methods may not accurately reflect real-world performance
- What evidence would resolve it: Development of new techniques demonstrating improved correlation with online performance

### Open Question 3
- Question: What are the key factors that influence the social impact of recommender systems, and how can they be measured?
- Basis in paper: Workshop emphasizes understanding social impact including echo chambers and misinformation
- Why unresolved: Limited research on quantifying social impact of recommender systems
- What evidence would resolve it: Case studies measuring social impact in different domains

## Limitations
- Hackathon rules, dataset selection, and evaluation criteria were not finalized at time of writing
- Success depends on execution of multi-disciplinary collaboration between industry and academia
- Assumes released materials will be well-documented and accessible enough for independent use

## Confidence
- High Confidence: Goals of expanding beyond accuracy metrics align with current research trends
- Medium Confidence: Hackathon will effectively bridge theory and practice assuming adequate participant preparation
- Medium Confidence: Open-sourcing artifacts will create lasting impact assuming proper documentation and organization

## Next Checks
1. Review actual hackathon materials and datasets released after event to assess depth and clarity
2. Survey participants 3-6 months post-workshop about implementation of new evaluation practices
3. Analyze citation and usage patterns of released artifacts to measure actual reach and impact