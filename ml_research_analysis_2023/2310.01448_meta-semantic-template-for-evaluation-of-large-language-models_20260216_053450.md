---
ver: rpa2
title: Meta Semantic Template for Evaluation of Large Language Models
arxiv_id: '2310.01448'
source_url: https://arxiv.org/abs/2310.01448
tags:
- samples
- evaluation
- llms
- arxiv
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MSTemp, a method to evaluate the semantic
  understanding ability of large language models (LLMs) by generating out-of-distribution
  (OOD) samples. MSTemp leverages another language model to generate semantically-preserved
  sentences, which are then used as templates to create diverse evaluation samples
  through sentence parsing and random word replacement.
---

# Meta Semantic Template for Evaluation of Large Language Models

## Quick Facts
- arXiv ID: 2310.01448
- Source URL: https://arxiv.org/abs/2310.01448
- Reference count: 7
- Primary result: Introduces MSTemp method to evaluate LLMs' semantic understanding by generating out-of-distribution samples that significantly reduce LLM performance

## Executive Summary
This paper introduces MSTemp, a method to evaluate the semantic understanding ability of large language models (LLMs) by generating out-of-distribution (OOD) samples. MSTemp leverages another language model to generate semantically-preserved sentences, which are then used as templates to create diverse evaluation samples through sentence parsing and random word replacement. This approach aims to reduce the possibility of data contamination by generating new samples dynamically and flexibly. Initial experiments on sentiment analysis show that MSTemp-generated samples significantly reduce the performance of LLMs compared to existing datasets, indicating limitations in LLMs' ability to handle OOD samples.

## Method Summary
MSTemp generates OOD evaluation samples by first using an evaluator language model to create semantically-preserved sentences as templates, then performing sentence parsing and random word replacement to create diverse samples. The method includes a semantic filter using BERT embeddings to ensure semantic preservation, and can incorporate adversarial attacks to control sample difficulty. The generation process introduces randomness at multiple stages to ensure diverse outputs across runs.

## Key Results
- MSTemp-generated samples significantly reduce LLM performance compared to existing datasets
- Different evaluator LLMs (ChatGPT, Llama2-13b) produce varying levels of sample diversity
- Semantic preservation filter effectively maintains meaning while allowing structural variation
- Adversarial attacks increase sample complexity and challenge LLM capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MSTemp reduces data contamination by generating OOD samples using different evaluator LLMs
- Mechanism: Instead of evaluating on existing datasets directly, MSTemp leverages another language model (evaluator LM) to generate semantically-preserved sentences as templates, which are then used to create diverse evaluation samples through sentence parsing and random word replacement
- Core assumption: Different evaluator LLMs will generate sufficiently diverse templates that existing LLMs are unlikely to have seen during training
- Evidence anchors:
  - [abstract]: "MSTemp is highly flexible, dynamic, and cost-effective. Our initial experiments show that MSTemp-generated samples can significantly reduce the performance of LLMs using existing datasets as seeds"
  - [section]: "The generation introduces randomness: the filter, the selected templates, and the replacement of templates all involve randomness. In fact, MSTemp makes it possible to generate different evaluation samples in its running every time"
  - [corpus]: Weak evidence - corpus shows related template-based generation methods but no direct evidence for contamination reduction claim
- Break condition: If the evaluator LLM generates templates that are too similar to the original training data, or if the same evaluator LLM is used repeatedly, contamination reduction benefit diminishes

### Mechanism 2
- Claim: MSTemp preserves semantic meaning while generating diverse samples
- Mechanism: Uses a semantic filter (BERT-powered) to measure cosine similarity between original and generated sentence embeddings, ensuring semantic preservation during template generation
- Core assumption: High embedding similarity (cosine > threshold τ) guarantees semantic preservation at the input level
- Evidence anchors:
  - [section]: "We leverage an LLM C as the semantic preserving filter to measure the similarity between the original and the generated sentences... There is a threshold τ to control how much of the generated sentences we want to preserve"
  - [section]: "The design of the prompts to B is non-trivial. In order to generate sentences as different as possible, the prompts should be well-designed"
  - [corpus]: Weak evidence - corpus shows template-based generation but no direct evidence for semantic preservation mechanism
- Break condition: If the semantic filter fails to capture semantic nuances, or if prompt engineering is insufficient to generate truly diverse yet semantically equivalent sentences

### Mechanism 3
- Claim: MSTemp creates evaluation samples at different difficulty levels through adversarial attacks
- Mechanism: Incorporates adversarial attack module that can add word-level attacks (synonym replacement) or typo-based attacks to increase sample complexity
- Core assumption: LLMs are sensitive to adversarial perturbations, and word-level attacks are more successful than other attack types
- Evidence anchors:
  - [section]: "Zhu et al. [2023] and Wang et al. [2023] showed that LLMs are sensitive to adversarial attacks, among which the word-level attacks, i.e., replacing words in a sentence with their synonyms, remains the most successful threat"
  - [section]: "Therefore, the adversarial attack module in MSTemp can easily control the complexity of the generated samples by adding different types of adversarial attacks"
  - [corpus]: No direct evidence in corpus for this specific mechanism
- Break condition: If adversarial attacks are too weak to challenge LLMs or too strong to break semantic preservation

## Foundational Learning

- Concept: Semantic similarity measurement using embedding space
  - Why needed here: MSTemp uses BERT embeddings to filter generated samples based on semantic similarity to originals
  - Quick check question: What metric does MSTemp use to measure semantic similarity between original and generated sentences?

- Concept: Sentence parsing and template generation
  - Why needed here: MSTemp performs sentence parsing to identify replaceable modules in templates for creating diverse samples
  - Quick check question: Which NLP library does MSTemp use for sentence parsing operations?

- Concept: Adversarial attack generation
  - Why needed here: MSTemp incorporates adversarial attacks to control difficulty level of generated samples
  - Quick check question: According to the paper, which type of adversarial attack is most successful against LLMs?

## Architecture Onboarding

- Component map:
  Seed dataset (D) -> Evaluator LM (B) -> Semantic filter LM (C) -> Sentence parser -> Template generator -> Adversarial attack module -> Final samples

- Critical path: Seed dataset → Evaluator LM generation → Semantic filter → Sentence parsing → Template creation → Adversarial attack → Final samples

- Design tradeoffs:
  - Using multiple evaluator LLMs increases diversity but also computational cost
  - Higher semantic similarity thresholds preserve meaning better but reduce diversity
  - More complex adversarial attacks increase difficulty but may break semantic preservation

- Failure signatures:
  - Poor performance reduction indicates evaluator LLMs are generating similar samples to training data
  - Inconsistent results across runs suggest randomization issues
  - Low diversity in generated samples indicates prompt engineering problems

- First 3 experiments:
  1. Baseline: Run MSTemp on SST-2 with ChatGPT as evaluator LM, measure performance reduction
  2. Diversity test: Run MSTemp with multiple evaluator LLMs, compare sample diversity metrics
  3. Adversarial test: Run MSTemp with and without adversarial attacks, measure difficulty impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of evaluator LM affect the quality and diversity of generated OOD samples?
- Basis in paper: [explicit] The paper states "MSTemp remains general and flexible to many language tasks" and mentions using different evaluator LMs like ChatGPT and Llama2-13b, but does not deeply analyze the impact of different choices.
- Why unresolved: The paper only provides initial experimental results using two evaluator LMs (ChatGPT and Llama2-13b) and does not explore the effects of using different evaluator LMs on the quality, diversity, or difficulty of generated samples.
- What evidence would resolve it: Systematic experiments comparing the generated samples and their impact on LLM evaluation when using different evaluator LMs with varying capabilities and training data.

### Open Question 2
- Question: What is the optimal balance between semantic preservation and sample diversity in MSTemp?
- Basis in paper: [explicit] The paper mentions a "semantic preserving filter" with a threshold τ to control semantic difference, but does not explore the effects of different threshold values on sample quality and evaluation effectiveness.
- Why unresolved: The paper does not investigate how different values of τ affect the balance between maintaining semantic similarity to the original data and generating diverse, challenging OOD samples.
- What evidence would resolve it: Experiments varying the threshold τ and analyzing its impact on semantic similarity, sample diversity, and the ability to expose LLM weaknesses.

### Open Question 3
- Question: How does MSTemp compare to existing OOD evaluation methods in terms of effectiveness and efficiency?
- Basis in paper: [explicit] The paper positions MSTemp as a method for OOD evaluation and claims it can "reduce the possibility of data contamination," but does not provide a comprehensive comparison with other OOD evaluation approaches.
- Why unresolved: The paper focuses on introducing MSTemp and providing initial results, but does not benchmark it against other established OOD evaluation methods or analyze its computational efficiency.
- What evidence would resolve it: Comparative studies between MSTemp and other OOD evaluation methods, including quantitative analysis of effectiveness in exposing LLM weaknesses and computational resource requirements.

## Limitations
- The approach depends heavily on evaluator LM's ability to generate truly diverse templates
- Semantic preservation mechanism using cosine similarity thresholds may not capture all semantic nuances
- Adversarial attack module's effectiveness relies on external claims without direct validation
- Method's flexibility comes at the cost of increased computational overhead

## Confidence
- Mechanism 1 (contamination reduction): Medium - the approach is sound but depends on assumptions about evaluator LM diversity
- Mechanism 2 (semantic preservation): Medium - BERT-based similarity is a reasonable heuristic but not foolproof
- Mechanism 3 (adversarial difficulty): Low - relies on external claims without direct validation in this work

## Next Checks
1. Test MSTemp with multiple evaluator LLMs (e.g., ChatGPT, Llama2, Claude) to measure actual diversity in generated samples and compare against single-LLM baselines
2. Measure semantic drift by evaluating semantic similarity distributions between originals and generated samples across different threshold values (τ)
3. Conduct ablation studies removing the adversarial attack module to quantify its actual contribution to difficulty level changes in evaluation samples