---
ver: rpa2
title: Factuality Detection using Machine Translation -- a Use Case for German Clinical
  Text
arxiv_id: '2308.08827'
source_url: https://arxiv.org/abs/2308.08827
tags:
- german
- data
- factuality
- translation
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a machine learning-based approach for factuality
  detection in German clinical text, addressing the challenge of limited annotated
  data in non-English languages. The core method involves using a local machine translation
  system to translate English clinical data (i2b2) into German, enabling training
  of a German factuality detection model.
---

# Factuality Detection using Machine Translation -- a Use Case for German Clinical Text

## Quick Facts
- arXiv ID: 2308.08827
- Source URL: https://arxiv.org/abs/2308.08827
- Reference count: 15
- Key outcome: German BERT model trained on machine-translated English clinical data outperforms rule-based NegEx for factuality detection in German clinical text

## Executive Summary
This paper addresses the challenge of factuality detection in German clinical text by leveraging machine translation to overcome the limited availability of annotated German clinical data. The authors propose a method that translates English clinical data (i2b2) into German using a local machine translation system, then fine-tunes a German BERT model on this translated data. The resulting model demonstrates superior performance compared to the rule-based NegEx approach on both translated i2b2 data and German clinical datasets (Ex4CDS and BRONCO150), particularly excelling at detecting "possible" factuality labels. This approach offers a practical solution for processing non-English clinical text while addressing privacy concerns through local translation, though it requires more computational resources than rule-based alternatives.

## Method Summary
The method involves translating English i2b2 clinical text into German using a local TransIns machine translation system that preserves entity markup, then fine-tuning German-MedBERT on the translated data. The model is evaluated against the rule-based NegEx baseline on multiple German datasets including the translated i2b2 data, Ex4CDS, and BRONCO150. The approach addresses the data scarcity problem for German clinical NLP by leveraging abundant English clinical data while maintaining privacy through local processing rather than cloud-based translation services.

## Key Results
- German BERT model outperforms NegEx on translated i2b2 data across all evaluation metrics
- The model shows substantial performance gains for "possible" factuality detection compared to NegEx
- The approach achieves promising results on real German clinical datasets (Ex4CDS and BRONCO150), demonstrating generalizability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: German BERT model trained on machine-translated i2b2 data outperforms rule-based NegEx on factuality detection
- Mechanism: Transfer learning from English clinical text via machine translation enables German model to learn factuality patterns without native German training data
- Core assumption: Machine translation preserves factuality cues sufficiently for model training
- Evidence anchors:
  - [abstract] "German BERT-based model outperforms the rule-based NegEx approach on translated i2b2 data"
  - [section] "the BERT-based model outperforms NegEx, on all scores"
  - [corpus] Limited - corpus shows related papers on machine translation but not direct factuality evidence
- Break condition: Translation quality degrades significantly, losing critical factuality cues or introducing noise

### Mechanism 2
- Claim: Local machine translation enables training on sensitive clinical data without privacy risks
- Mechanism: Running TransIns locally avoids sharing patient data with external translation services
- Core assumption: Local MT system maintains sufficient translation quality for training purposes
- Evidence anchors:
  - [section] "we aim at a solution that could be applied to sensitive data. Therefore, the machine translation component must run locally"
  - [section] "a manual inspection revealed multiple problems with the translations"
  - [corpus] None - corpus doesn't address privacy aspects of MT in clinical settings
- Break condition: Local MT system becomes too resource-intensive or translation quality drops below acceptable threshold

### Mechanism 3
- Claim: Machine learning models better capture "possible" factuality than rule-based approaches
- Mechanism: Neural models learn nuanced linguistic patterns for uncertainty that rule-based systems miss
- Core assumption: Training data contains sufficient examples of "possible" factuality to learn patterns
- Evidence anchors:
  - [section] "BERT-based models show a substantial increase in performance for the possible label"
  - [section] "NegEx already struggles with negated (0.76) and performs low in the case of possible (0.26)"
  - [corpus] Weak - corpus contains related papers on translation but not specific evidence about "possible" factuality detection
- Break condition: Training data lacks sufficient "possible" examples for model to learn meaningful patterns

## Foundational Learning

- Concept: Clinical text characteristics (telegraphic style, abbreviations, misspellings)
  - Why needed here: Understanding why clinical text is challenging for both MT and factuality detection
  - Quick check question: Why can't general-purpose NLP models work well on clinical text without adaptation?

- Concept: Factuality vs negation vs uncertainty in medical language
  - Why needed here: Distinguishing between different factuality categories is core to the task
  - Quick check question: How does "possible" factuality differ linguistically from negation in clinical text?

- Concept: Machine translation limitations for domain-specific text
  - Why needed here: Understanding why local MT and quality inspection are necessary
  - Quick check question: What types of translation errors would most harm factuality detection?

## Architecture Onboarding

- Component map: English i2b2 clinical text -> Local TransIns MT system -> German-MedBERT fine-tuning -> Evaluation on German datasets
- Critical path: Translation → Training → Evaluation → Analysis
- Design tradeoffs:
  - Local MT vs cloud services: Privacy vs translation quality
  - Rule-based vs ML: Performance vs resource requirements
  - Training data size: More data improves performance but increases computational cost
- Failure signatures:
  - Poor translation quality → model learns wrong patterns
  - Insufficient "possible" examples → model fails on uncertainty detection
  - Overfitting to translated i2b2 → poor generalization to real German clinical text
- First 3 experiments:
  1. Evaluate English BERT on original i2b2 vs NegEx
  2. Evaluate German BERT on translated i2b2 vs German NegEx
  3. Test both German models on Ex4CDS and NegEx-Ger datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of machine translation impact the performance of factuality detection models when translating between languages with different linguistic structures?
- Basis in paper: [explicit] The paper discusses translation issues such as missing factuality cues, incorrect trigger translations, and differences in entity placement between English and German.
- Why unresolved: While the paper identifies these issues, it does not quantify their impact on model performance or provide solutions for mitigating translation-related errors.
- What evidence would resolve it: Detailed analysis of model performance on translated data with varying translation quality, or experiments comparing models trained on human-translated vs. machine-translated data.

### Open Question 2
- Question: Is a BERT-based approach always superior to rule-based methods like NegEx for factuality detection in clinical text, considering computational resource constraints?
- Basis in paper: [inferred] The paper shows that BERT models outperform NegEx on most datasets but acknowledges that BERT models require significantly more computational resources.
- Why unresolved: The paper does not provide a cost-benefit analysis comparing the performance gains of BERT models against their increased resource requirements.
- What evidence would resolve it: Comparative studies measuring both performance and resource usage (time, memory, energy) for BERT vs. NegEx across various clinical text datasets.

### Open Question 3
- Question: How does the distribution of factuality labels in training data affect model performance on different types of clinical text?
- Basis in paper: [explicit] The paper notes that BRONCO150 has an unusual label distribution with a high frequency of possible labels, which may influence model performance on other datasets.
- Why unresolved: The paper does not explore how different label distributions in training data impact model generalization to datasets with different label distributions.
- What evidence would resolve it: Experiments training models on datasets with varying label distributions and evaluating their performance on test sets with different label distributions.

## Limitations

- Translation quality uncertainty: The approach depends on machine translation quality, but the paper does not provide systematic evaluation of translation accuracy or its impact on model performance
- Resource-intensive: BERT-based models require significantly more computational resources than rule-based alternatives, limiting practical deployment
- Uneven performance gains: While the model shows substantial improvements for "possible" factuality, gains for negation detection are more modest

## Confidence

- **High Confidence**: The claim that German BERT outperforms NegEx on translated i2b2 data is well-supported by direct experimental results showing superior F1 scores across all metrics.
- **Medium Confidence**: The generalization claim to real German clinical datasets is supported but limited by the relatively small size of Ex4CDS (350 documents) and BRONCO150 (155 documents), which may not provide robust evidence of broader applicability.
- **Low Confidence**: The assertion that the approach provides a practical solution for non-English clinical text processing is supported by methodology but lacks systematic evaluation of resource requirements, scalability, or real-world deployment considerations.

## Next Checks

1. Conduct a systematic error analysis comparing translation errors to model prediction errors to quantify the contribution of translation quality to overall performance.
2. Perform ablation studies testing different amounts of translated training data to determine the minimum viable dataset size for acceptable performance.
3. Evaluate the approach on additional German clinical datasets with different characteristics (e.g., different specialties, document types) to assess generalizability beyond the current datasets.