---
ver: rpa2
title: 'Text2Reward: Reward Shaping with Language Models for Reinforcement Learning'
arxiv_id: '2309.11489'
source_url: https://arxiv.org/abs/2309.11489
tags:
- reward
- self
- robot
- cube
- chair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Text2Reward, a framework that automates the
  generation of dense reward functions for reinforcement learning tasks using large
  language models. The method takes natural language instructions and environment
  descriptions as input, and generates interpretable, executable reward code.
---

# Text2Reward: Reward Shaping with Language Models for Reinforcement Learning

## Quick Facts
- arXiv ID: 2309.11489
- Source URL: https://arxiv.org/abs/2309.11489
- Authors: 
- Reference count: 32
- Key outcome: Text2Reward automates dense reward function generation using LLMs, achieving 94%+ success on novel locomotion tasks and matching/exceeding expert rewards on 13/17 manipulation tasks.

## Executive Summary
This paper introduces Text2Reward, a framework that leverages large language models to automatically generate dense reward functions for reinforcement learning tasks. By grounding reward generation in Pythonic environment abstractions and utilizing few-shot examples from a skill library, Text2Reward produces interpretable, executable reward code from natural language instructions. The method demonstrates strong performance across manipulation and locomotion tasks, with the ability to incorporate human feedback for iterative refinement and real-world deployment.

## Method Summary
Text2Reward generates reward functions by combining natural language task instructions with Pythonic representations of environment state. The method constructs prompts including the instruction, environment abstraction as typed Python classes, background knowledge, and retrieved few-shot examples. GPT-4 generates reward code, which is then executed to identify and correct errors through an iterative feedback loop. The resulting rewards are used with standard RL algorithms like PPO or SAC, with optional human feedback integration for refinement.

## Key Results
- Achieved over 94% success rate on six novel locomotion tasks in MuJoCo
- Matched or exceeded expert-designed rewards on 13 of 17 manipulation tasks in ManiSkill2 and MetaWorld benchmarks
- Enabled real-world deployment of policies trained in simulation
- Demonstrated improvement with human feedback iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grounding reward generation in Pythonic environment abstractions reduces semantic ambiguity.
- Mechanism: Representing environment state as typed Python classes provides structured schema enabling precise mapping from instructions to reward code.
- Core assumption: LLM pre-training data contains sufficient Python class syntax and robotics domain code for accurate grounding.
- Evidence anchors: [abstract] "generates dense reward functions as an executable program grounded in a compact representation of the environment"; [section 2.2] "Pythonic representation has a higher level of abstraction and allows us to write general, reusable prompts across different environments"

### Mechanism 2
- Claim: Few-shot examples from a skill library improve reward function quality by providing task-relevant templates.
- Mechanism: Retrieving semantically similar instruction-code pairs from a library provides the LLM with proven reward structures that can be adapted to new tasks.
- Core assumption: The skill library contains diverse, verified reward functions covering a wide range of tasks.
- Evidence anchors: [section 2.2] "We utilize the sentence embedding model from Su et al. (2022) to encode each instruction. Given a new instruction, we use the embedding to retrieve the top-k similar instructions"

### Mechanism 3
- Claim: Code execution feedback enables iterative refinement and error correction in generated reward functions.
- Mechanism: Running generated code in an interpreter reveals syntax errors, shape mismatches, and attribute misuse, which are fed back to the LLM for correction in subsequent iterations.
- Core assumption: The LLM can effectively use execution feedback to correct errors when provided with sufficient context about the failure.
- Evidence anchors: [section 2.2] "Once the reward code is generated, we execute the code in the code interpreter... our experiments show that this step decreases error rates from 10% to near zero"

## Foundational Learning

- Concept: Python class abstraction and typing
  - Why needed here: The method relies on representing environment state as Python classes with typed attributes to provide structured input to the LLM
  - Quick check question: How would you represent a robot with 7 DoF joint positions and velocities as a Python class with proper typing?

- Concept: Natural language embeddings and similarity search
  - Why needed here: Few-shot examples are retrieved based on semantic similarity between task instructions using sentence embeddings
  - Quick check question: What properties should a sentence embedding model have to effectively retrieve similar robotics task instructions?

- Concept: Code execution and error handling
  - Why needed here: Generated reward functions must be syntactically correct and executable; execution feedback drives iterative improvement
  - Quick check question: What types of errors would you expect when executing reward code for a manipulation task, and how would you categorize them?

## Architecture Onboarding

- Component map:
  - Environment abstraction module: Generates Pythonic class representations of environment state
  - Prompt generation module: Constructs prompts with instruction, environment abstraction, background knowledge, and few-shot examples
  - LLM inference module: Generates reward function code using GPT-4
  - Code execution module: Runs generated code to identify errors and provide feedback
  - RL training module: Uses generated reward functions with PPO/SAC algorithms
  - Feedback collection module: Gathers human feedback on policy performance for iterative refinement

- Critical path: Instruction → Environment abstraction → Prompt generation → LLM inference → Code execution → RL training → Policy evaluation → (optional) Human feedback → Reward refinement → RL retraining

- Design tradeoffs:
  - Abstraction level vs. specificity: Higher-level abstractions enable reuse but may miss task-specific details
  - Few-shot vs. zero-shot: Few-shot provides better performance but requires maintaining a skill library
  - Code execution vs. static analysis: Execution provides concrete feedback but adds runtime overhead

- Failure signatures:
  - Compilation errors: Missing imports, undefined variables, syntax issues
  - Runtime errors: Shape mismatches, attribute access on wrong object types
  - Semantic errors: Reward functions that don't capture task objectives despite being executable
  - Performance degradation: Generated rewards lead to slower learning or suboptimal policies compared to expert rewards

- First 3 experiments:
  1. Test zero-shot generation on a simple Pick Cube task: Verify code compiles and produces reasonable rewards for reaching and grasping
  2. Test few-shot generation with 3 examples: Compare success rates and convergence speed against zero-shot baseline
  3. Test code execution feedback: Generate a reward function with known errors, verify the error detection and correction loop works

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Text2Reward scale with the complexity of the task, particularly for long-horizon tasks that require sequential subgoals?
- Basis in paper: [inferred] The paper mentions that instructions can be subgoals for long-horizon tasks planned by the LLM, but doesn't provide detailed analysis on how well Text2Reward handles complex, multi-step tasks.
- Why unresolved: The paper focuses on evaluating individual manipulation and locomotion tasks, but doesn't explore the scalability of the method to more complex, sequential tasks.
- What evidence would resolve it: Systematic evaluation of Text2Reward on a diverse set of long-horizon tasks, comparing performance and convergence speed to baseline methods, would provide insights into its scalability.

### Open Question 2
- Question: How sensitive is Text2Reward to the quality and specificity of the natural language instructions provided by users?
- Basis in paper: [explicit] The paper acknowledges that humans seldom specify precise intent in a single interaction and demonstrates the benefit of human feedback in refining rewards.
- Why unresolved: While the paper shows that feedback can improve performance, it doesn't quantify the impact of instruction quality on the initial reward generation or the number of feedback iterations required for different instruction qualities.
- What evidence would resolve it: A controlled study varying the specificity and quality of instructions while measuring initial reward quality and the number of feedback iterations needed to achieve optimal performance would address this question.

### Open Question 3
- Question: Can Text2Reward generate reward functions that generalize to novel environments or tasks beyond the training distribution of the LLM?
- Basis in paper: [inferred] The paper demonstrates generalization to novel locomotion tasks in MuJoCo, but doesn't explore whether the method can handle entirely new environment types or tasks outside the scope of the training data.
- Why unresolved: The evaluation focuses on tasks within known environments (ManiSkill2, MetaWorld, MuJoCo), leaving open the question of cross-environment generalization.
- What evidence would resolve it: Testing Text2Reward on tasks from completely different domains (e.g., from robotics to video games) or on entirely new environment types would reveal its generalization capabilities.

## Limitations
- Reliance on Pythonic environment abstractions may not capture all relevant state information for complex tasks
- Few-shot performance depends heavily on the quality and diversity of the skill library
- Code execution feedback loop may not detect semantic errors that compile and run correctly but produce incorrect rewards

## Confidence
**High Confidence**: The claim that Text2Reward achieves over 94% success rate on six novel locomotion tasks in MuJoCo is well-supported by the experimental results presented in the paper. The comparison against expert-designed rewards on 13 of 17 manipulation tasks in ManiSkill2 and MetaWorld benchmarks is also clearly demonstrated.

**Medium Confidence**: The mechanism by which Pythonic environment abstractions reduce semantic ambiguity relies on assumptions about the LLM's pre-training data and the completeness of the abstraction. While the paper provides evidence of successful implementation, the generalizability to environments with different APIs or state representations is uncertain.

**Low Confidence**: The effectiveness of few-shot examples depends on the quality and diversity of the skill library, which is not fully characterized. The paper shows improved performance with few-shot examples but does not provide detailed analysis of retrieval quality or failure modes when similar examples are unavailable.

## Next Checks
1. **Environment Abstraction Completeness**: Test the method on a manipulation task where the environment abstraction is deliberately missing critical state information (e.g., omitting gripper status). Measure the impact on reward quality and policy performance.

2. **Skill Library Diversity Analysis**: Conduct an ablation study where the skill library is systematically reduced in diversity. Evaluate how the performance degrades as the number of available examples decreases or as examples become less semantically similar to target tasks.

3. **Semantic Error Detection**: Create a set of reward functions that compile and execute without errors but produce incorrect rewards (e.g., inverted reward signals). Test whether the code execution feedback loop can detect and correct these semantic errors, or if additional validation mechanisms are needed.