---
ver: rpa2
title: Reusing Pretrained Models by Multi-linear Operators for Efficient Training
arxiv_id: '2310.10699'
source_url: https://arxiv.org/abs/2310.10699
tags:
- training
- mango
- tensor
- pretrained
- bert2bert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of training large Transformers
  efficiently by reusing pretrained small models. Previous approaches only mapped
  partial weights when growing models, potentially missing correlations across the
  entire model.
---

# Reusing Pretrained Models by Multi-linear Operators for Efficient Training

## Quick Facts
- arXiv ID: 2310.10699
- Source URL: https://arxiv.org/abs/2310.10699
- Authors: 
- Reference count: 40
- Primary result: Mango saves 76% computational costs when training DeiT-base from DeiT-small, outperforming bert2BERT by +12.0% and LiGO by +20.7%

## Executive Summary
This paper addresses the challenge of training large Transformers efficiently by reusing pretrained small models. Previous approaches only mapped partial weights when growing models, potentially missing important correlations across the entire model. The authors propose Mango, a multi-linear operator that linearly correlates each weight of the target model to all weights of the pretrained model. By decomposing the large mapping tensor using tensor ring decomposition, Mango enables efficient training while maintaining full model connectivity. Experiments demonstrate significant computational savings compared to existing methods.

## Method Summary
Mango uses multi-linear operators with tensor ring decomposition to create a growth operator that linearly correlates each target weight to all pretrained weights. The method trains this operator for a small number of steps (e.g., 100) on concatenated pretrained model weights, then applies it to initialize a larger target model. This function-preserving initialization is subsequently trained normally. The approach captures inter- and intra-model correlations that partial mapping methods miss, while reducing computational complexity through tensor decomposition.

## Key Results
- Mango achieves 76% FLOP savings when training DeiT-base from DeiT-small
- Outperforms bert2BERT by +12.0% and LiGO by +20.7% in training efficiency
- Demonstrates effectiveness across both vision (DeiT) and language (BERT/GPT) models
- Shows that rank 1 tensor decomposition is sufficient for high performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-linear operators enable efficient knowledge transfer by capturing inter- and intra-model correlations that partial mapping methods miss.
- Mechanism: The Mango operator uses tensor ring decomposition to linearly correlate each target weight to all pretrained weights, creating a low-rank approximation of the full mapping tensor. This allows capturing correlations across entire models rather than just partial weight mappings.
- Core assumption: The attention maps and weight similarities across different layers and models indicate meaningful correlations that can be leveraged for efficient training.
- Evidence anchors:
  - [abstract]: "there are inter- and intra-interactions among the weights of both the pretrained and the target models"
  - [section]: "The attention maps of BERT, as depicted in Figure 2, indicate that there are similarities not only in the weights of the same or different layers, but also across different models"
- Break condition: If the correlations between different model weights are not meaningful or consistent across different model architectures, the multi-linear operator would fail to provide efficient knowledge transfer.

### Mechanism 2
- Claim: Tensor ring decomposition reduces computational complexity while maintaining full model connectivity.
- Mechanism: The large mapping tensor S is decomposed into four smaller tensors (S_B, S_I, S_O, S_L) using tensor ring decomposition, where each tensor captures specific types of interactions (within-layer, input/output dimension transformations, and layer relationships). This decomposition exponentially reduces space requirements while preserving the full correlation structure.
- Core assumption: The tensor ring decomposition can adequately approximate the full mapping tensor while maintaining the essential correlations needed for effective model growth.
- Evidence anchors:
  - [abstract]: "We utilize multi-linear operators to reduce computational and spacial complexity, enabling acceptable resource requirements"
  - [section]: "The total size of S B, S I, S O, and S L is exponentially less than S, which makes Mango viable for practical implementation while maintaining the full correlation between the small and big models"
- Break condition: If the rank R needs to be too large to maintain performance, the complexity benefits would diminish and the approach would lose its efficiency advantage.

### Mechanism 3
- Claim: Training the growth operator on a small number of steps provides function-preserving initialization for large models.
- Mechanism: The Mango operators are trained for a limited number of steps (e.g., 100) using the small pretrained model to find a function-preserving mapping to the larger target model. This initialization then allows efficient full training of the larger model.
- Core assumption: A small number of training steps on the operator can find an effective function-preserving mapping that significantly accelerates subsequent full model training.
- Evidence anchors:
  - [abstract]: "We train these operators to obtain the function preserving M2"
  - [section]: "We train the growth operator ΦSB,SI,SO,SL items of Eq. (7) in a few steps (e.g., 100) to make transferred models maintaining function"
- Break condition: If the operator requires extensive training to find effective mappings, the efficiency benefits would be reduced or eliminated.

## Foundational Learning

- Concept: Tensor Ring Decomposition
  - Why needed here: Provides the mathematical foundation for decomposing the large mapping tensor into smaller, manageable components while preserving correlations
  - Quick check question: How does tensor ring decomposition differ from other tensor decomposition methods like CP or Tucker decomposition?

- Concept: Multi-linear Operators
  - Why needed here: Enable the creation of a growth operator that can linearly correlate each target weight to all pretrained weights
  - Quick check question: What is the key advantage of using multi-linear operators over simple linear transformations for model growth?

- Concept: Function-preserving Transformations
  - Why needed here: Ensures that the expanded model maintains the functionality of the smaller pretrained model while adding capacity
  - Quick check question: How does Mango ensure that the expanded model preserves the function of the original small model?

## Architecture Onboarding

- Component map:
  Pretrained model weights -> Mango operator training -> Growth operator -> Expanded model weights -> Full training

- Critical path:
  1. Concatenate weights of pretrained model into tensor M1
  2. Train Mango operator (tensor ring decomposition) for 100 steps with Adam/AdamW
  3. Apply operator to generate expanded model weights M2
  4. Use M2 as initialization for full training of target model

- Design tradeoffs:
  - Rank R vs. performance: Higher rank captures more correlations but increases complexity
  - Training steps for operator vs. quality of initialization: More steps improve quality but reduce efficiency gains
  - Choice of decomposition structure vs. expressiveness: Different tensor decompositions offer different tradeoffs

- Failure signatures:
  - Poor acceleration ratios despite high operator accuracy
  - Degraded performance on downstream tasks compared to scratch training
  - High memory usage during operator training phase

- First 3 experiments:
  1. Width expansion test: DeiT-T-A → DeiT-S on ImageNet, varying rank R
  2. Depth expansion test: DeiT-T-B → DeiT-S on ImageNet, fixed rank R=1
  3. Combined expansion test: DeiT-T-C → DeiT-S on ImageNet, comparing Mango vs. LiGO and bert2BERT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of rank in Mango affect the quality of the learned weight mapping and subsequent training efficiency?
- Basis in paper: [explicit] The paper explores the influence of rank setting in Mango, finding that while higher ranks can lead to better operator accuracy, they do not necessarily translate to faster training.
- Why unresolved: The paper shows that rank 1 is sufficient for achieving high training efficiency without performance loss, but it does not delve into the theoretical reasons behind this finding or explore the impact of higher ranks on different model architectures.
- What evidence would resolve it: Conducting experiments with a wider range of ranks and model architectures, and analyzing the resulting weight mappings to understand the relationship between rank and mapping quality.

### Open Question 2
- Question: Can Mango be extended to handle non-linear weight mappings for even greater training efficiency?
- Basis in paper: [inferred] The paper focuses on linear mappings using multi-linear operators, but the concept of utilizing the entire pretrained model for knowledge transfer could potentially be applied to non-linear mappings.
- Why unresolved: The paper does not explore the possibility of non-linear mappings, and the computational complexity of such mappings might be prohibitive.
- What evidence would resolve it: Developing and testing Mango variants that incorporate non-linear mappings, and comparing their performance to the linear version on various tasks and model architectures.

### Open Question 3
- Question: How does Mango's performance compare to other knowledge transfer methods that do not rely on pretrained models?
- Basis in paper: [explicit] The paper compares Mango to methods that reuse pretrained models, but it does not directly compare its performance to methods that learn from scratch or use other forms of knowledge transfer.
- Why unresolved: The paper focuses on the benefits of reusing pretrained models, but it does not provide a comprehensive comparison of different knowledge transfer approaches.
- What evidence would resolve it: Conducting experiments that compare Mango to a variety of knowledge transfer methods, including those that learn from scratch or use alternative forms of knowledge transfer, on a range of tasks and model architectures.

## Limitations

- The effectiveness of tensor ring decomposition at capturing complex weight correlations across different model architectures is not systematically explored
- The 100-step operator training appears somewhat arbitrary without sensitivity analysis
- Generalizability across substantially different architectures beyond tested vision-language pairs remains unproven

## Confidence

**High Confidence**: The FLOP savings measurements and comparative results against bert2BERT and LiGO are well-documented with clear methodology. The core premise that partial weight mapping misses correlations is supported by attention map visualizations.

**Medium Confidence**: The tensor ring decomposition implementation and its claimed exponential complexity reduction are theoretically sound but lack detailed validation. The operator training procedure (100 steps) appears somewhat arbitrary without sensitivity analysis.

**Low Confidence**: The generalizability of Mango across different model architectures beyond the tested vision and language models. The mechanism's reliance on attention map correlations as proxy for effective weight mapping is suggestive but not conclusively proven.

## Next Checks

1. **Rank Sensitivity Analysis**: Systematically vary the tensor ring rank R from 1 to 5 on the same model pairs to quantify the tradeoff between complexity reduction and performance preservation.

2. **Operator Training Duration Study**: Extend operator training beyond 100 steps to determine the minimum effective training duration and assess whether longer training yields diminishing returns or better initialization quality.

3. **Cross-Architecture Transfer**: Test Mango on transferring between substantially different architectures (e.g., CNN to Transformer or vice versa) to evaluate the limits of the correlation-based approach beyond the vision-language pairs studied.