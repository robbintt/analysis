---
ver: rpa2
title: 'Free-form Flows: Make Any Architecture a Normalizing Flow'
arxiv_id: '2310.16624'
source_url: https://arxiv.org/abs/2310.16624
tags:
- flows
- invertible
- loss
- normalizing
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a method to lift the architectural constraints
  of normalizing flows, generative models trained with maximum likelihood. Traditionally,
  the requirement of analytical invertibility limits the expressiveness of normalizing
  flows, which can be restrictive for specific applications.
---

# Free-form Flows: Make Any Architecture a Normalizing Flow

## Quick Facts
- arXiv ID: 2310.16624
- Source URL: https://arxiv.org/abs/2310.16624
- Reference count: 3
- Primary result: Free-form flows enable any dimension-preserving neural network to serve as a generative model through maximum likelihood training, achieving competitive performance with minimal fine-tuning requirements.

## Executive Summary
This work presents a method to lift the architectural constraints of normalizing flows by employing an efficient estimator for the gradient of the change of variables formula. This allows any dimension-preserving neural network to serve as a generative model through maximum likelihood training. The approach is demonstrated in molecule generation, where rotational equivariance is crucial, and inverse problems, where off-the-shelf ResNet architectures are employed. The method outperforms traditional normalizing flows and generates stable molecules significantly faster than previous approaches.

## Method Summary
Free-form flows (FFF) use a surrogate gradient estimator based on the Hutchinson trace estimator to compute the gradient of the log-determinant of the Jacobian during training. The method combines maximum likelihood with a reconstruction loss that ensures the decoder approximates the inverse of the encoder. This allows the use of any dimension-preserving neural network architecture, including those without analytical inverses. The reconstruction loss stabilizes training and ensures that the model converges to the same solutions as traditional normalizing flows when minimized.

## Key Results
- FFF outperforms traditional normalizing flows in molecule generation with E(n)-equivariant architectures
- Generates stable molecules more than an order of magnitude faster than previous approaches
- Achieves competitive performance on SBI benchmark inverse problems with minimal fine-tuning
- Enables use of off-the-shelf ResNet architectures for generative modeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The estimator in equation (6) provides an unbiased gradient estimate for the log-determinant term.
- Mechanism: The Hutchinson trace estimator is used to approximate the trace in the gradient formula. Specifically, it estimates tr(Jϕ∇θi Jθ) using random vectors v with unit covariance. This avoids computing the full Jacobian, reducing complexity from O(D) backprop steps to O(1) vector-Jacobian products.
- Core assumption: The random vector v must have unit covariance for the estimator to be unbiased.
- Evidence anchors:
  - [abstract]: "We overcome this constraint by a training procedure that uses an efficient estimator for the gradient of the change of variables formula."
  - [section 3.2]: The derivation shows that tr(Jϕ∇θi Jθ) is estimated via vT Jϕ(∇θi Jθ)v, where v is sampled from a distribution with unit covariance.
  - [corpus]: No direct evidence in the corpus neighbors.
- Break condition: If v does not have unit covariance, the estimator becomes biased and training may diverge.

### Mechanism 2
- Claim: The reconstruction loss LR ensures that gϕ approximates f⁻¹θ sufficiently well for stable training.
- Mechanism: The reconstruction loss ∥x - gϕ(fθ(x))∥² is minimized during training. This forces gϕ to be close to the inverse of fθ, which in turn makes JϕJθ close to the identity matrix. This reduces the error in the trace estimator and stabilizes training.
- Core assumption: Minimizing the reconstruction loss will drive gϕ toward f⁻¹θ.
- Evidence anchors:
  - [section 3.3]: "This requires a tractable inverse. Traditionally, this was achieved via invertible layers... We replace this constraint via a simple reconstruction loss."
  - [section 4.2]: Theorem 4.2 bounds the error of the estimator by ∥JϕJθ - I∥F, showing that reconstruction loss directly controls this term.
  - [corpus]: No direct evidence in the corpus neighbors.
- Break condition: If β (the weight on the reconstruction loss) is too small, the reconstruction loss may not be minimized and gϕ may not approximate f⁻¹θ well enough, leading to unstable gradients.

### Mechanism 3
- Claim: The combination of maximum likelihood and reconstruction loss yields the same critical points as exact normalizing flow training when the reconstruction loss is minimal.
- Mechanism: Theorem 4.3 shows that when the reconstruction loss is minimal, the critical points of the free-form flow loss Lg are the same as those of the exact loss Lf⁻¹. This means the model converges to the same solutions as traditional normalizing flows but with more architectural flexibility.
- Core assumption: The reconstruction loss being minimal implies gϕ = f⁻¹θ.
- Evidence anchors:
  - [section 4.3]: "Furthermore, every minimum of Lf⁻¹ is a minimum of Lg. If the reconstruction loss is minimal, Lg has no additional critical points."
  - [section 4.4]: The example in Figure 2 shows that Lg has the same minima as Lf⁻¹ when β is sufficiently large.
  - [corpus]: No direct evidence in the corpus neighbors.
- Break condition: If β is too small, additional non-invertible critical points may appear, and the model may not converge to the desired solution.

## Foundational Learning

- Concept: Change of variables formula in probability theory.
  - Why needed here: Normalizing flows rely on the change of variables formula to compute the likelihood of data under the model. Understanding this formula is crucial for grasping how the Jacobian determinant affects the density transformation.
  - Quick check question: What is the relationship between the Jacobian determinant of a transformation and the density of the transformed variable?

- Concept: Trace estimator and its application in gradient estimation.
  - Why needed here: The paper uses the Hutchinson trace estimator to efficiently compute the gradient of the log-determinant of the Jacobian. This is a key innovation that allows the use of arbitrary architectures.
  - Quick check question: How does the Hutchinson trace estimator approximate the trace of a matrix product?

- Concept: Equivalence of KL divergence minimization and maximum likelihood estimation.
  - Why needed here: The paper shows that minimizing the KL divergence between the data distribution and the model distribution is equivalent to maximizing the likelihood. This equivalence is fundamental to the training objective.
  - Quick check question: Why is minimizing the KL divergence equivalent to maximizing the likelihood in the context of generative modeling?

## Architecture Onboarding

- Component map:
  - Encoder (fθ) -> Decoder (gϕ) -> Latent distribution
  - Encoder: A dimension-preserving neural network that maps data to a latent space. Can be any architecture, including E(n)-equivariant networks for molecules.
  - Decoder: A dimension-preserving neural network that maps latent codes back to data space. Trained to approximate the inverse of the encoder.
  - Loss function: Combines maximum likelihood (via the surrogate gradient estimator) and reconstruction loss.
  - Latent distribution: Typically a standard normal distribution, but can be adapted for specific tasks (e.g., translation-invariant distributions for molecules).

- Critical path:
  1. Define the encoder and decoder architectures based on the task requirements.
  2. Initialize the model with skip connections to start close to the identity function.
  3. Set β large enough to ensure stable training and low reconstruction loss.
  4. Train using the FFF loss function, monitoring the negative log-likelihood and reconstruction loss.
  5. Evaluate the model by computing the likelihood of held-out data using the decoder Jacobian.

- Design tradeoffs:
  - Flexibility vs. Stability: Free-form flows allow any architecture but require careful tuning of β to ensure stability.
  - Speed vs. Accuracy: The surrogate gradient estimator is fast but may introduce some error if the reconstruction loss is not minimized.
  - Task-specific vs. General: Specialized architectures (e.g., E(n)-equivariant) can improve performance on specific tasks but may not generalize as well.

- Failure signatures:
  - Unstable gradients: Often due to β being too small, leading to high reconstruction loss and poor approximation of the inverse.
  - Poor likelihood estimates: Can occur if the encoder and decoder are not well-aligned, resulting in inaccurate Jacobian determinants.
  - Mode collapse: May happen if the latent distribution is not well-suited to the data, leading to the model focusing on a subset of the data distribution.

- First 3 experiments:
  1. Train a simple 1D free-form flow on a Gaussian mixture to verify that it can learn multi-modal distributions and compare the learned density to the true density.
  2. Apply an E(n)-free-form flow to a small molecule dataset (e.g., QM9) to test the equivariant architecture and evaluate the quality of generated molecules.
  3. Use a free-form flow for a conditional generation task (e.g., simulation-based inference) to assess its flexibility and performance compared to traditional normalizing flows.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of reconstruction weight β affect the trade-off between reconstruction quality and likelihood estimation in practice?
- Basis in paper: [explicit] The paper discusses the importance of β in ensuring global invertibility and mentions that there is a range of β values where optimization converges to the same quality, but does not provide a rigorous argument for choosing β.
- Why unresolved: The paper acknowledges the difficulty in computing the optimal β value for a given partition and suggests that β must be tuned as a hyperparameter until a suitable value is found.
- What evidence would resolve it: A theoretical analysis or empirical study demonstrating the relationship between β, reconstruction loss, and likelihood estimation accuracy across various tasks and architectures.

### Open Question 2
- Question: Can free-form flows be effectively extended to higher-dimensional data or tasks with more complex dependencies, such as images or videos?
- Basis in paper: [inferred] The paper demonstrates free-form flows on molecule generation and inverse problems, but these are relatively low-dimensional tasks. The scalability and effectiveness of free-form flows for higher-dimensional data are not explored.
- Why unresolved: The paper focuses on demonstrating the method on specific tasks and does not investigate its performance on higher-dimensional data or more complex dependencies.
- What evidence would resolve it: Experiments evaluating free-form flows on high-dimensional datasets (e.g., images, videos) and comparing their performance to other generative models.

### Open Question 3
- Question: What are the theoretical limitations of the gradient estimator used in free-form flows, and how do these limitations affect the model's performance in practice?
- Basis in paper: [explicit] The paper provides a bound on the difference between the true gradient of the log-determinant and the estimator used in free-form flows, but does not explore the practical implications of this bound.
- Why unresolved: The paper focuses on the theoretical development of the method and does not investigate the practical impact of the gradient estimator's limitations on model performance.
- What evidence would resolve it: An empirical study examining the effect of the gradient estimator's limitations on model performance across various tasks and architectures, as well as potential methods to mitigate these limitations.

## Limitations
- The choice of β is critical and must be tuned as a hyperparameter, which can be challenging in practice.
- The surrogate gradient estimator may introduce some error if the reconstruction loss is not minimized, potentially affecting likelihood estimation accuracy.
- The method has not been extensively tested on higher-dimensional data or more complex dependencies, such as images or videos.

## Confidence
- Mechanism 1 (Unbiased gradient estimation): High
- Mechanism 2 (Reconstruction loss ensures invertibility): Medium
- Mechanism 3 (Theoretical equivalence): High

## Next Checks
1. Perform systematic ablation studies on the β hyperparameter across different architectures and datasets to identify optimal values and failure modes.
2. Test FFF on additional SBI benchmark datasets to verify the generalizability of the performance improvements over traditional normalizing flows.
3. Conduct controlled experiments comparing FFF with exact normalizing flows on simple distributions where ground truth likelihoods are known, to quantify the impact of the estimator error.