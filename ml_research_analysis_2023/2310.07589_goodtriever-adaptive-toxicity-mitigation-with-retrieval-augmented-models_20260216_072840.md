---
ver: rpa2
title: 'Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models'
arxiv_id: '2310.07589'
source_url: https://arxiv.org/abs/2310.07589
tags:
- arxiv
- toxicity
- goodtriever
- language
- toxic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Goodtriever, a novel method for toxicity
  mitigation that utilizes retrieval-augmented models to adaptively address the changing
  nature of language and toxicity over time. Goodtriever combines a large language
  model with two external datastores containing toxic and non-toxic examples, enabling
  toxicity-controlled text generation without requiring model retraining.
---

# Goodtriever: Adaptive Toxicity Mitigation with Retrieval-augmented Models

## Quick Facts
- arXiv ID: 2310.07589
- Source URL: https://arxiv.org/abs/2310.07589
- Authors: 
- Reference count: 37
- Key outcome: Goodtriever achieves 43% latency reduction and competitive toxicity mitigation using retrieval-augmented models without requiring retraining.

## Executive Summary
Goodtriever is a novel toxicity mitigation method that combines a base language model with two external datastores containing toxic and non-toxic examples. By using k-nearest neighbors retrieval at inference time and Product of Experts ensembling, Goodtriever steers text generation toward non-toxic content while maintaining fluency. The method demonstrates consistent performance across different model families and sizes, showing particular strength in continual toxicity mitigation tasks where data drift is a concern.

## Method Summary
Goodtriever works by augmenting a base language model with two external datastores - one containing toxic examples and one containing non-toxic examples. During generation, the method retrieves k-nearest neighbors from each datastore based on context similarity, computes token probabilities from these neighbors, and combines them with the base model using a Product of Experts approach. The resulting logits are scaled by a tunable parameter alpha to control the strength of toxicity mitigation. This allows toxicity-controlled text generation without requiring model retraining.

## Key Results
- Achieves 43% relative latency reduction during inference compared to state-of-the-art techniques
- Reduces Expected Maximum Toxicity from 0.49 to 0.19 (optimal configuration)
- Demonstrates consistent performance across GPT2, Pythia, and OPT model families
- Shows promising results in continual toxicity mitigation, matching multitask finetuning baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining a base language model with separate toxic and non-toxic datastores through PoE interpolation effectively steers generation toward non-toxic content while preserving fluency.
- Mechanism: The toxic datastore reduces the probability of toxic tokens by subtracting its logits from the non-toxic datastore logits, which are then added to the base LM logits with scaling factor α. This creates a product-of-experts ensemble where tokens high in base LM and non-toxic datastore probabilities but low in toxic datastore probabilities dominate.
- Core assumption: Toxic and non-toxic datastores contain sufficiently representative examples to shift token distributions in the desired direction without catastrophic forgetting of general language capabilities.
- Evidence anchors:
  - [abstract]: "By incorporating a retrieval-based approach at decoding time, GOODTRIEVER enables toxicity-controlled text generation."
  - [section 2.1]: "Our method is based on product of experts which is first proposed by Hinton (2002)."
  - [corpus]: Weak; no direct citation of product-of-experts success in toxicity mitigation found in related papers.
- Break condition: If datastore examples are mislabeled or unrepresentative, the interpolation can amplify unwanted tokens or suppress benign ones, leading to either persistent toxicity or bland, overly safe outputs.

### Mechanism 2
- Claim: Using nearest-neighbor retrieval from fixed-size datastores at inference time avoids the computational cost of fine-tuning or auxiliary classifiers.
- Mechanism: For each generation step, GOODTRIEVER retrieves the k nearest neighbors from each datastore based on context embedding similarity, computes token probabilities over those neighbors, and interpolates them with the base LM logits.
- Core assumption: Fixed-size datastores can be indexed and queried efficiently enough to keep latency low, and the k-NN approximation remains stable across diverse contexts.
- Evidence anchors:
  - [abstract]: "achieving 43% relative latency reduction during inference and being more computationally efficient."
  - [section 2.1]: "By incorporating a retrieval-based approach at decoding time, GOODTRIEVER enables toxicity-controlled text generation."
  - [corpus]: Weak; latency claims not directly validated against related retrieval-based methods in the corpus.
- Break condition: If datastore size grows too large or k is set too high, retrieval time may dominate and erase latency gains; if embeddings poorly capture context, nearest neighbors may be irrelevant.

### Mechanism 3
- Claim: Separate toxic and non-toxic datastores allow fine-grained control over toxicity mitigation without retraining the base model.
- Mechanism: The datastore construction explicitly labels examples as toxic or non-toxic, enabling GOODTRIEVER to apply opposing probability adjustments; updates to either datastore can be made independently.
- Core assumption: Binary labeling is sufficient to capture the spectrum of toxicity and non-toxicity needed for effective mitigation.
- Evidence anchors:
  - [abstract]: "This property allows for convenient and immediate incorporation of new knowledge, as well as the ability to edit, correct and remove existing information without requiring any retraining of the LM."
  - [section 2.1]: "These datastores control text generation based on desirable (non-toxic) and undesirable (toxic) attributes."
  - [corpus]: Weak; no evidence in corpus of datastore update strategies or their effect on mitigation quality.
- Break condition: If labeling is noisy or the toxic/non-toxic boundary is ambiguous, the datastores may reinforce incorrect associations, causing either false positives or missed toxicity.

## Foundational Learning

- Concept: k-Nearest Neighbors (k-NN) retrieval for language modeling
  - Why needed here: GOODTRIEVER relies on retrieving nearest neighbors from toxic and non-toxic datastores to adjust token probabilities without retraining.
  - Quick check question: How does changing k affect the balance between latency and toxicity mitigation performance?
- Concept: Product of Experts (PoE) ensembling
  - Why needed here: PoE allows combining the base LM with two separate datastore distributions in a principled way, weighting toxic and non-toxic influences differently.
  - Quick check question: What happens to perplexity and toxicity when α is set to 0, 1, or 2?
- Concept: Embedding similarity for context matching
  - Why needed here: The retrieval mechanism depends on accurate embedding of context to find relevant toxic/non-toxic examples; poor embeddings break the mitigation.
  - Quick check question: If you use a different sentence embedding model, how does that change k-NN retrieval quality and toxicity scores?

## Architecture Onboarding

- Component map: Base LM (GPT2/Pythia/OPT) -> Context embedding -> Two datastore lookups (toxic/non-toxic) -> k-NN retrieval -> Probability aggregation (PoE) -> Adjusted logits -> Generation
- Critical path: Embedding generation -> k-NN search -> Probability combination -> Logit adjustment
- Design tradeoffs: Larger datastores improve mitigation but increase latency; higher k improves accuracy but increases compute; higher α increases toxicity control but may reduce fluency.
- Failure signatures: High perplexity with low toxicity -> datastore too restrictive; persistent toxicity -> datastore labels wrong or too small; extreme latency -> datastore too large or k too high.
- First 3 experiments:
  1. Run GOODTRIEVER with k=1, α=0 (base LM only) to establish baseline metrics.
  2. Increase k to 1024 with α=0.5, measure latency and EMT to confirm retrieval impact.
  3. Swap toxic and non-toxic datastore labels, verify that toxicity increases instead of decreases.

## Open Questions the Paper Calls Out

- What is the optimal strategy for selecting toxic and non-toxic samples to add to the datastores to observe a monotonic decline in toxicity?
- How does Goodtriever perform in a multilingual and multicultural setting for toxicity mitigation?
- What is the impact of Goodtriever on biases against marginalized groups?

## Limitations

- The method's effectiveness depends heavily on the quality and representativeness of the toxic/non-toxic datastores
- Limited exploration of multilingual and multicultural toxicity mitigation scenarios
- No evaluation of potential bias amplification against marginalized groups

## Confidence

- Latency reduction claims: Medium confidence - well-supported by quantitative results but lacks comparison to other retrieval-based methods
- Toxicity mitigation performance: Medium confidence - competitive results but limited ablation studies and hyperparameter sensitivity analysis
- Continual mitigation without retraining: Low confidence - claims not validated with long-term drift studies or demonstration of incorporating new toxic patterns

## Next Checks

1. Test GOODTRIEVER with corrupted datastore labels (randomly flip 10-30% of toxic/non-toxic labels) and measure EMT degradation to quantify robustness to labeling errors
2. Compare EMT scores across different embedding models (BERT, RoBERTa, Sentence-BERT) to determine if retrieval quality drives mitigation performance
3. Measure EMT and perplexity over multiple generations with the same prompt to check for consistency and detect if the method introduces stochastic toxicity patterns