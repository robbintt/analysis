---
ver: rpa2
title: 'DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method'
arxiv_id: '2305.16284'
source_url: https://arxiv.org/abs/2305.16284
tags:
- gradient
- dowg
- descent
- page
- stepsize
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DoWG (Distance over Weighted Gradients),
  a new parameter-free gradient-based optimizer that achieves optimal convergence
  rates for both smooth and nonsmooth convex optimization up to logarithmic factors.
  Unlike prior parameter-free methods, DoWG does not require line search or bisection
  subroutines.
---

# DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent Method

## Quick Facts
- arXiv ID: 2305.16284
- Source URL: https://arxiv.org/abs/2305.16284
- Reference count: 40
- Primary result: DoWG achieves optimal convergence rates for both smooth and nonsmooth convex optimization up to logarithmic factors without parameter tuning

## Executive Summary
DoWG (Distance over Weighted Gradients) is a new parameter-free gradient-based optimizer that achieves universal adaptivity across both smooth and nonsmooth convex optimization problems. Unlike prior parameter-free methods, DoWG eliminates the need for line search or bisection subroutines while maintaining optimal convergence rates. The key innovation is a distance-based weighted running average of gradients that enables automatic adaptation to both Lipschitz and smoothness constants. Empirically, DoWG demonstrates competitive performance comparable to hand-tuned AdaGrad while exhibiting edge-of-stability training dynamics.

## Method Summary
DoWG is a parameter-free gradient descent algorithm that maintains a distance-based weighted running average of gradients for normalization. The method uses the running distance from the initial point as weights for squared gradients, creating an adaptive stepsize that automatically adjusts to problem geometry. Unlike previous parameter-free methods, DoWG doesn't require line search or bisection, making it computationally efficient. The algorithm achieves optimal convergence rates for both smooth and nonsmooth convex functions up to logarithmic factors, effectively matching the performance of optimally tuned gradient descent across different problem regimes.

## Key Results
- DoWG achieves optimal convergence rates for both smooth and nonsmooth convex optimization up to logarithmic factors without parameter tuning
- The method eliminates the need for line search or bisection subroutines that previous parameter-free methods required
- Empirically competitive with hand-tuned AdaGrad on practical machine learning tasks while training at the edge of stability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DoWG automatically matches the convergence rate of optimally tuned gradient descent in both smooth and nonsmooth settings up to logarithmic factors without any parameter tuning.
- **Mechanism**: DoWG maintains a distance-based weighted running average of gradients for normalization, where the weights are the squared running distances from the initial point. This adaptive weighting scheme enables the algorithm to adjust its effective stepsize dynamically based on the geometry of the problem, achieving universal adaptivity.
- **Core assumption**: The function f is convex, and the domain X is a closed convex set. The distance-based weighting scheme can effectively estimate the Lipschitz constant in nonsmooth cases and the smoothness constant in smooth cases.
- **Evidence anchors**:
  - [abstract] "DoWG maintains a new distance-based weighted version of the running average, which is crucial to achieve the desired properties."
  - [section] "DoWG uses the same idea of estimating the distance from the optimum by using the distance from the initial point as a surrogate, but instead of using the square root of the running gradient sum... DoWG uses the square root of the weighted gradient sum vt =Pt
k=0 ¯r2
k∥∇f(xk)∥2."
  - [corpus] Weak evidence; corpus papers focus on related optimization methods but do not specifically validate DoWG's distance-based weighting mechanism.
- **Break condition**: If the function is non-convex or the domain is not closed and convex, the convergence guarantees may not hold.

### Mechanism 2
- **Claim**: DoWG adapts to smoothness without requiring knowledge of the smoothness constant L, achieving linear dependence on L similar to gradient descent with optimal stepsize.
- **Mechanism**: By maintaining a weighted sum of squared gradients where later gradients have larger impact due to increasing weights, DoWG implicitly estimates the smoothness constant. The key insight is that ∥∇f(x)∥ ≤
p
2L(f(x) − f∗) for smooth functions, and the weighted averaging amplifies gradients when the function is less smooth.
- **Core assumption**: The function f is L-smooth and convex. The weighting scheme with monotonically increasing distances effectively captures the local smoothness properties.
- **Evidence anchors**:
  - [abstract] "DoWG is the first parameter-free AdaGrad style algorithm that adapts to smooth optimization."
  - [section] "DoWG chooses the weights adaptively based on the running distance from the initial point. This use of distance-based weighted averaging is new..."
  - [corpus] No direct evidence in corpus; this mechanism is specific to DoWG's design.
- **Break condition**: If the function has regions of varying smoothness or is not globally smooth, the adaptive mechanism may not perform optimally.

### Mechanism 3
- **Claim**: DoWG trains at the edge of stability, similar to normalized gradient descent, by driving the effective stepsize to large values to preserve convergence rates.
- **Mechanism**: As DoWG approaches the optimum, the effective stepsize ηeff,t = ηt/∥∇f(xt)∥ increases. To maintain the convergence rate of gradient descent, DoWG must drive this effective stepsize to values proportional to 1/L. When the effective stepsize exceeds the stability threshold (2/L), gradient norms diverge, forcing the stepsize back down, creating a self-stabilizing oscillation.
- **Core assumption**: The function is smooth and convex, and the algorithm uses a sufficiently small initial distance estimate rϵ.
- **Evidence anchors**:
  - [abstract] "To complement our theory, we also show empirically that DoWG trains at the edge of stability..."
  - [section] "Like NGD, DoWG also tends to increase the stepsize and train at the edge of stability... Once it overshoots, the gradients quickly diverge, forcing the stepsize back down."
  - [corpus] No direct evidence; corpus papers do not discuss edge of stability phenomena for DoWG specifically.
- **Break condition**: If the function has discontinuous gradients or the domain is unbounded, the edge of stability behavior may not manifest or may lead to divergence.

## Foundational Learning

- **Concept**: Convex optimization and gradient descent
  - **Why needed here**: Understanding the convergence properties of gradient descent in convex settings is essential for appreciating DoWG's universal adaptivity claims.
  - **Quick check question**: What is the optimal convergence rate of gradient descent for a convex L-smooth function with stepsize η = 1/L?
- **Concept**: Lipschitz continuity and smoothness
  - **Why needed here**: DoWG's ability to adapt to both Lipschitz and smooth functions relies on understanding these regularity conditions and their implications for gradient-based optimization.
  - **Quick check question**: How does the Lipschitz constant G bound the gradient norm ∥∇f(x)∥ for all x in the domain?
- **Concept**: Parameter-free optimization and adaptive methods
  - **Why needed here**: DoWG is designed to eliminate the need for manual parameter tuning, which is a key advantage over standard gradient descent and other adaptive methods like AdaGrad.
  - **Quick check question**: What is the main challenge that parameter-free methods aim to solve in optimization?

## Architecture Onboarding

- **Component map**: Distance estimator -> Weighted gradient sum -> Stepsize calculator -> Gradient descent step
- **Critical path**: Distance estimator → Weighted gradient sum → Stepsize calculator → Gradient descent step
- **Design tradeoffs**:
  - Pros: Universal adaptivity to both smooth and nonsmooth functions, no parameter tuning required, competitive empirical performance
  - Cons: Extra logarithmic factor in convergence rate compared to optimally tuned gradient descent, requires tracking weighted gradient sum and running distance
- **Failure signatures**:
  - Divergence: If rϵ is chosen too large relative to the diameter of X, or if the function is non-convex
  - Slow convergence: If the function has highly varying smoothness or the domain is unbounded
  - Numerical instability: If gradients become extremely large, causing overflow in the weighted sum
- **First 3 experiments**:
  1. **Sanity check**: Run DoWG on a simple convex quadratic function (e.g., f(x) = x²) with different initializations to verify convergence to the optimum.
  2. **Adaptivity test**: Compare DoWG's performance on a smooth function (e.g., f(x) = x²) versus a nonsmooth function (e.g., f(x) = |x|) to observe universal adaptivity.
  3. **Edge of stability**: Monitor the effective stepsize ηeff,t over iterations to confirm the edge of stability behavior described in the paper.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Does DoWG's weighted gradient averaging scheme provide any advantage over standard AdaGrad-style methods when applied to non-convex optimization problems?
- **Basis in paper**: The paper mentions DoWG's weighted gradient averaging is a key innovation but only analyzes its performance for convex problems. It notes "we believe this adaptivity to smoothness might explain the empirical difference between DoWG and DoG, but leave a rigorous discussion of adaptivity to local smoothness to future work."
- **Why unresolved**: The paper focuses exclusively on convex optimization theory, leaving the non-convex case unexplored despite mentioning it in the experimental discussion.
- **What evidence would resolve it**: A rigorous convergence analysis of DoWG for non-convex problems, or empirical studies comparing DoWG to other adaptive methods on non-convex benchmarks showing clear advantages or disadvantages.

### Open Question 2
- **Question**: What is the precise relationship between DoWG's convergence rate and the choice of initial distance estimate r_ϵ?
- **Basis in paper**: The paper states "Suppose that r_ϵ < D" and mentions "This can be done by choosing r_ϵ ≤ ∥x − y∥ for any two x ≠ y ∈ X" but doesn't explore how different choices affect convergence speed.
- **Why unresolved**: While the paper establishes that r_ϵ must be less than D, it doesn't provide guidance on optimal selection or analyze sensitivity to this parameter.
- **What evidence would resolve it**: Empirical studies showing convergence speed as a function of r_ϵ, or theoretical bounds on convergence rate as a function of the ratio r_ϵ/D.

### Open Question 3
- **Question**: Does DoWG maintain its parameter-free property when extended to stochastic optimization settings?
- **Basis in paper**: The paper focuses on deterministic gradient descent with the notation "xt+1 = ΠX (xt − ηt∇f(xt))" and doesn't address stochastic gradients.
- **Why unresolved**: The paper only analyzes the deterministic case, leaving open whether DoWG can maintain its parameter-free nature with stochastic gradients.
- **What evidence would resolve it**: A convergence analysis of DoWG for stochastic optimization problems, or empirical results showing DoWG's performance with mini-batches on real datasets.

## Limitations
- Extra logarithmic factor in convergence rate compared to optimally tuned gradient descent
- Requires maintaining a growing weighted sum, potentially causing numerical stability issues for long training runs
- Edge-of-stability behavior is empirically observed but not rigorously proven to be necessary or beneficial

## Confidence
- High confidence in the core theoretical framework and convergence proofs
- Medium confidence in the empirical claims about edge-of-stability behavior
- Medium confidence in practical performance claims

## Next Checks
1. Implement a variant that caps the weighted sum growth to test numerical stability and whether this affects convergence rates
2. Quantify the actual constants in the logarithmic factors by running extensive experiments across different problem scales
3. Test DoWG on non-convex problems to empirically verify where the convergence guarantees break down, despite theoretical convex-only guarantees