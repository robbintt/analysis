---
ver: rpa2
title: Improved sampling via learned diffusions
arxiv_id: '2307.01198'
source_url: https://arxiv.org/abs/2307.01198
tags:
- sampling
- loss
- problem
- log-variance
- chen
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of sampling from unnormalized target
  densities by learning controlled diffusion processes. The authors propose a unified
  framework that generalizes existing diffusion-based sampling approaches by formulating
  the problem as minimizing divergences between path space measures of time-reversed
  diffusion processes.
---

# Improved sampling via learned diffusions

## Quick Facts
- arXiv ID: 2307.01198
- Source URL: https://arxiv.org/abs/2307.01198
- Reference count: 40
- Key outcome: Proposes log-variance divergence as an alternative to KL for diffusion-based sampling, demonstrating improved performance across multiple approaches

## Executive Summary
This paper addresses the problem of sampling from unnormalized target densities by learning controlled diffusion processes. The authors introduce a unified framework that generalizes existing diffusion-based sampling approaches by formulating the problem as minimizing divergences between path space measures of time-reversed diffusion processes. They propose the log-variance divergence as an alternative to the commonly used reverse KL divergence, which avoids mode collapse and offers favorable numerical properties such as lower variance, computational efficiency, and exploration-exploitation trade-offs.

## Method Summary
The authors present a unified framework for diffusion-based sampling that treats existing approaches (Schrödinger Bridge, Path Integral Sampler, Diffusion-based generative modeling) as special cases of minimizing divergences between path space measures. The key innovation is replacing the reverse KL divergence with a log-variance divergence, which exhibits zero gradient variance at the optimal solution and allows for exploration-exploitation trade-offs through choice of reference measure. The framework uses controlled SDEs for both generative and inference processes, with the log-variance loss computed using Monte Carlo estimation of the log-density ratio between path measures.

## Key Results
- The log-variance loss consistently improves performance across GMM, Funnel, and Double Well benchmark problems
- Achieves higher effective sample sizes and lower estimation errors compared to KL-based approaches
- Reduces training time by avoiding differentiation through the SDE solver
- Demonstrates robustness across different random seeds and problem dimensions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The log-variance divergence avoids mode collapse by balancing exploration and exploitation through choice of reference measure.
- Mechanism: By selecting the reference measure as the path space measure of a controlled process, the loss variance can be reduced compared to the reverse KL divergence, and the optimizer can explore multiple modes without collapsing to one.
- Core assumption: The variance of the log-density ratio estimator under the reference measure is lower than under the target measure.
- Evidence anchors: [abstract], [section 2.3]
- Break condition: If the reference measure is poorly chosen (e.g., far from both the generative and inference processes), the variance may not reduce and mode collapse can persist.

### Mechanism 2
- Claim: The log-variance loss has zero gradient variance at the optimal solution, improving convergence stability.
- Mechanism: At the optimal control, the Monte Carlo estimator of the log-variance loss has zero variance in its gradient with respect to both controls, preventing oscillations around the optimum that occur with KL-based losses.
- Core assumption: The optimal controls satisfy the time-reversal condition, making the log-density ratio zero almost surely along sampled paths.
- Evidence anchors: [section 2.3], [abstract]
- Break condition: If the optimal solution is not achieved (e.g., due to poor initialization or learning rate), the variance reduction benefit is not realized.

### Mechanism 3
- Claim: The unified path space measure framework allows using arbitrary divergences, enabling log-variance to be plugged in seamlessly.
- Mechanism: By formulating the problem as minimizing divergences between path space measures of time-reversed diffusion processes, the log-variance divergence can be used as a drop-in replacement for the reverse KL divergence without changing the overall optimization structure.
- Core assumption: The Radon-Nikodym derivative representation holds for any divergence, allowing the same gradient-based optimization pipeline to be used.
- Evidence anchors: [section 2.2], [section 2.3]
- Break condition: If the path space measure formulation is not correctly implemented (e.g., incorrect time-reversal), the divergence may not be valid and optimization can fail.

## Foundational Learning

- Concept: Time-reversal of diffusion processes and path space measures
  - Why needed here: The entire framework relies on viewing sampling as reversing a diffusion process, and the log-variance loss is defined as a divergence between path space measures.
  - Quick check question: Can you explain why the time-reversed process of a diffusion with drift μ and volatility σ has drift μ + σσ⊤∇log p - σv?

- Concept: Girsanov theorem and Radon-Nikodym derivatives
  - Why needed here: The likelihood of path measures under different controls is computed using Girsanov's theorem, which is essential for deriving both the KL and log-variance losses.
  - Quick check question: What is the Radon-Nikodym derivative between the path measures of two controlled diffusions with control difference u - r?

- Concept: Schrödinger bridge and entropy regularization
  - Why needed here: The paper connects existing diffusion-based sampling methods to the Schrödinger bridge problem, showing that the KL-based losses are special cases with entropy constraints.
  - Quick check question: How does adding an entropy constraint to the KL divergence objective ensure a unique solution to the Schrödinger bridge problem?

## Architecture Onboarding

- Component map: Generative SDE -> Inference SDE -> Path space measures P_X^u and P_Y^v -> Loss functions (KL and log-variance) -> Control updates
- Critical path:
  1. Sample paths from X_w using Euler-Maruyama with 200 steps
  2. Compute log-density ratio using Proposition 2.3 formula
  3. Estimate loss and its gradient via Monte Carlo (batch size 2048)
  4. Backpropagate through detached X_w (for log-variance) or X_u (for KL)
  5. Update controls u and v using Adam optimizer
- Design tradeoffs:
  - KL loss: Requires differentiating through SDE solver, higher computational cost, prone to mode collapse
  - Log-variance loss: No SDE differentiation, lower variance gradients, exploration-exploitation balance via reference measure choice
  - Reference measure choice: w = u (exploit current control) vs. w ≠ u (explore different modes)
- Failure signatures:
  - KL loss: Mode collapse (samples concentrate in one mode), high gradient variance, slow convergence
  - Log-variance loss: Poor performance if reference measure is poorly chosen, may require tuning of w
  - Both: Divergence of SDE solver if steps are too large or volatility is too high
- First 3 experiments:
  1. Implement Euler-Maruyama solver for X_w and verify marginal distributions match theory for known SDEs (e.g., Ornstein-Uhlenbeck).
  2. Implement log-variance loss with w = u and compare gradient variance to KL loss on a simple 2D Gaussian mixture.
  3. Implement full sampling pipeline (generative and inference SDEs, loss computation, control update) on the Funnel distribution and compare ESS and marginal std to ground truth.

## Open Questions the Paper Calls Out

- How does the log-variance divergence compare to other divergences like α-divergences or f-divergences in diffusion-based sampling?
- How sensitive is the log-variance loss to the choice of reference measure w in practice?
- What is the theoretical convergence rate of diffusion-based samplers using the log-variance loss compared to KL-based approaches?

## Limitations

- The theoretical framework relies heavily on the assumption that the path space measure formulation is correctly implemented, with limited empirical validation of the Radon-Nikodym derivative calculations beyond the specific examples provided.
- The choice of reference measure for the log-variance loss is presented as a key advantage, but the paper provides limited guidance on how to optimally select this measure for different problem classes.
- The experiments focus primarily on synthetic benchmark problems, leaving uncertainty about performance on real-world, high-dimensional datasets.

## Confidence

**High Confidence**: The mechanism by which the log-variance loss avoids mode collapse through reduced gradient variance at the optimal solution is theoretically well-founded and supported by Proposition 2.5. The computational efficiency gains from avoiding differentiation through the SDE solver are directly observable and measurable.

**Medium Confidence**: The claim that the unified path space measure framework enables seamless replacement of KL with log-variance loss across different diffusion-based sampling approaches is supported by experiments, but the robustness across diverse problem types remains to be thoroughly validated. The exploration-exploitation trade-off through reference measure selection is theoretically sound but lacks comprehensive empirical characterization.

**Low Confidence**: The assertion that log-variance loss consistently outperforms KL-based approaches across all settings without hyperparameter tuning requires more extensive validation, particularly for complex, real-world distributions where the optimal reference measure choice may be non-trivial.

## Next Checks

1. **Reference Measure Sensitivity Analysis**: Systematically vary the reference measure choice (w) for the log-variance loss across different benchmark problems to quantify the impact on performance metrics and identify guidelines for optimal selection.

2. **Real-World Dataset Validation**: Apply the log-variance loss framework to high-dimensional, real-world sampling problems (e.g., Bayesian neural networks, latent variable models) to assess scalability and robustness beyond synthetic benchmarks.

3. **Gradient Variance Profiling**: Empirically measure and compare the gradient variance of KL and log-variance losses throughout the optimization trajectory, not just at the optimal solution, to validate the claimed variance reduction benefits.