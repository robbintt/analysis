---
ver: rpa2
title: 'PersianLLaMA: Towards Building First Persian Large Language Model'
arxiv_id: '2312.15713'
source_url: https://arxiv.org/abs/2312.15713
tags:
- language
- persian
- text
- dataset
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PersianLLaMA introduces the first large language model for the
  Persian language, trained on a 94 GB Persian text corpus. Two versions (7B and 13B
  parameters) were developed using from-scratch training and LoRA adaptation methods.
---

# PersianLLaMA: Towards Building First Persian Large Language Model

## Quick Facts
- arXiv ID: 2312.15713
- Source URL: https://arxiv.org/abs/2312.15713
- Reference count: 40
- Key outcome: First large language model for Persian, trained on 94 GB Persian corpus, achieving 7.3/10 on Alpaca and 65% accuracy on sentiment analysis

## Executive Summary
PersianLLaMA introduces the first large language model specifically designed for the Persian language, addressing the significant gap in NLP resources for this widely spoken language. The model comes in two variants: a 7B parameter model trained from scratch on Persian data and a 13B parameter model adapted using LoRA fine-tuning on pre-trained LLaMA weights. Both models demonstrate superior performance compared to existing Persian language models on natural language generation and understanding tasks, marking a significant advancement in Persian NLP capabilities.

## Method Summary
PersianLLaMA was developed using two complementary approaches: training a 7B parameter LLaMA 2 model from scratch on a 94 GB Persian corpus (combining OSCAR and Persian Wikipedia data) using DeepSpeed and TencentPretrain frameworks, and adapting a 13B parameter LLaMA 2 model using LoRA fine-tuning on Persian Wikipedia. The models employed comprehensive preprocessing including text normalization, HTML tag removal, and SentencePiece BPE tokenization. Training used FP16 precision with cosine learning rate scheduling, while evaluation covered both natural language generation (using ChatGPT 3.5 scoring) and understanding tasks (using F1-score, accuracy, and semantic similarity metrics).

## Key Results
- Achieved 7.3/10 score on Alpaca dataset for natural language generation
- Reached 65% accuracy on sentiment analysis tasks
- Significantly outperformed existing Persian language models on both NLG and NLU tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large-scale pretraining on 94 GB Persian corpus provides robust language understanding and generation capabilities.
- Mechanism: Monolingual (7B) and multilingual (13B with LoRA) approaches leverage native Persian patterns and efficient transfer learning from English LLaMA models.
- Core assumption: Quality and diversity of Persian training data directly correlates with model performance.
- Evidence anchors: Abstract mentions "trained on a collection of Persian texts" and "evaluated for natural language generation tasks"; section discusses evaluation using "larger language models".
- Break condition: Insufficient diversity or quality in Persian training data would prevent performance gains.

### Mechanism 2
- Claim: LoRA fine-tuning enables efficient Persian adaptation with minimal computational resources.
- Mechanism: Low-rank adapter matrices freeze pre-trained weights while training only adapter parameters, reducing trainable parameters from 13B to 533M (4.1%) and enabling single A100 GPU training for 70 hours.
- Core assumption: Low-rank decomposition can capture Persian-specific linguistic patterns without full model retraining.
- Evidence anchors: Section explains "LoRA freezes the weights of the pre-trained model and injects trainable low-rank matrices into each layer".
- Break condition: Persian language patterns requiring full parameter updates would cause LoRA approach to underperform.

### Mechanism 3
- Claim: Comprehensive preprocessing and specialized tokenizers improve Persian linguistic understanding.
- Mechanism: Text normalization using Hazm library, HTML tag removal, punctuation handling, and SentencePiece BPE tokenization with 50K-64K tokens enable proper segmentation of Persian morphology.
- Core assumption: Proper tokenization and preprocessing are critical for handling Persian's rich morphology and orthographic variations.
- Evidence anchors: Section details "Text preprocessing meticulously cleans and normalizes the training dataset" and "SentencePiece supports word separation using BPE method".
- Break condition: Preprocessing pipeline introducing artifacts or tokenization failing to capture Persian patterns would degrade performance.

## Foundational Learning

- Concept: Tokenization and subword segmentation
  - Why needed here: Persian has complex morphology requiring proper segmentation for effective language modeling
  - Quick check question: What is the difference between word-level and subword tokenization, and why is BPE preferred for morphologically rich languages?

- Concept: Pretraining vs. fine-tuning paradigms
  - Why needed here: Understanding when to train from scratch vs. use transfer learning determines resource allocation and performance
  - Quick check question: What are the trade-offs between training a model from scratch versus using LoRA fine-tuning on a pre-trained model?

- Concept: Evaluation metrics for NLP models
  - Why needed here: Proper evaluation requires understanding F1-score, accuracy, ROUGE, and LLM-based assessment methods
  - Quick check question: How do traditional metrics like ROUGE differ from LLM-based evaluation approaches for text generation tasks?

## Architecture Onboarding

- Component map: Persian text corpus → preprocessing → tokenizer training → model initialization → training (scratch or LoRA) → evaluation on downstream tasks
- Critical path: Data pipeline → Tokenizer → Model architecture (LLaMA variants) → Training framework (DeepSpeed/TencentPretrain) → LoRA adapters → Evaluation framework
- Design tradeoffs: 7B scratch model vs. 13B LoRA model - computational cost vs. parameter efficiency; monolingual vs. multilingual tokenizer - language coverage vs. complexity
- Failure signatures: Training loss plateaus early (data quality issues), evaluation scores significantly below baseline (architecture/data problems), GPU memory errors (configuration issues)
- First 3 experiments:
  1. Verify tokenizer handles Persian compound words and morphological variations correctly
  2. Test training convergence on small subset of data before full training
  3. Validate evaluation pipeline using known good outputs vs. model outputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the long-term effects of training PersianLLaMA on a dataset lacking diversity in terms of topics and linguistic styles?
- Basis in paper: From limitations section stating "Given that our training dataset lacks diversity, our models' knowledge is limited."
- Why unresolved: Paper does not provide data on model performance over time with diverse dataset or explore how limited training data affects long-term capabilities.
- What evidence would resolve it: Longitudinal studies comparing PersianLLaMA's performance on various tasks before and after training on more diverse dataset, including metrics for topic coverage and linguistic style adaptability.

### Open Question 2
- Question: How can PersianLLaMA be effectively integrated with ethical safeguards to prevent generation of biased, toxic, or harmful content?
- Basis in paper: From limitations section mentioning "no measures were taken to cleanse or prevent the generation of unethical texts."
- Why unresolved: Paper does not discuss or implement any ethical guidelines or filtering mechanisms for content generation.
- What evidence would resolve it: Development and testing of ethical filters or guidelines, followed by empirical evaluation of PersianLLaMA's ability to adhere to these standards in generating content.

### Open Question 3
- Question: What are the specific challenges and potential solutions for improving PersianLLaMA's performance in logical reasoning and mathematical calculations?
- Basis in paper: From limitations section stating "The models may not perform well in logical reasoning and mathematical calculations."
- Why unresolved: Paper acknowledges this limitation but does not explore underlying causes or propose methods to enhance capabilities in these areas.
- What evidence would resolve it: Analysis of PersianLLaMA's performance on wide range of logical reasoning and mathematical tasks, identification of common failure patterns, and development of targeted training or architectural modifications.

## Limitations
- Limited diversity in training dataset may constrain model's knowledge and generalization capabilities
- No ethical safeguards implemented to prevent generation of biased or harmful content
- Performance limitations in logical reasoning and mathematical calculations tasks

## Confidence
- High Confidence: The fundamental approach of using large-scale pretraining and LoRA fine-tuning for Persian language modeling is theoretically sound and aligns with established NLP practices
- Medium Confidence: Specific performance metrics are based on described evaluation methodology, but limited peer validation and lack of comparison to established Persian models reduce confidence in absolute claims
- Low Confidence: Technical implementation details critical for reproduction have insufficient specification and external validation

## Next Checks
1. Cross-Validation with Independent Persian Corpora: Test PersianLLaMA's performance on independent Persian text datasets not included in original training corpus to assess generalization and potential overfitting
2. Human Evaluation of Generated Text: Conduct human evaluation studies with native Persian speakers to validate LLM-based scoring results and assess quality of generated text across different Persian dialects
3. Comparative Analysis with Emerging Persian Models: Benchmark PersianLLaMA against other recent Persian language models on identical evaluation datasets to establish relative performance and identify potential weaknesses