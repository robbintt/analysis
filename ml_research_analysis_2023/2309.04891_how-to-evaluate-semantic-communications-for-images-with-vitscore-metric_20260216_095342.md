---
ver: rpa2
title: How to Evaluate Semantic Communications for Images with ViTScore Metric?
arxiv_id: '2309.04891'
source_url: https://arxiv.org/abs/2309.04891
tags:
- vitscore
- image
- semantic
- lpips
- psnr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ViTScore, a novel semantic similarity evaluation
  metric for images based on pre-trained vision transformer (ViT) models. ViTScore
  addresses the limitation of existing image quality metrics (PSNR, MS-SSIM, LPIPS)
  which cannot effectively measure semantic similarity in semantic communications
  scenarios.
---

# How to Evaluate Semantic Communications for Images with ViTScore Metric?

## Quick Facts
- arXiv ID: 2309.04891
- Source URL: https://arxiv.org/abs/2309.04891
- Authors: 
- Reference count: 40
- Primary result: ViTScore outperforms PSNR, MS-SSIM, and LPIPS for measuring semantic similarity in semantic communications, particularly for image inversion tasks.

## Executive Summary
This paper introduces ViTScore, a novel semantic similarity evaluation metric for images based on pre-trained vision transformer (ViT) models. Unlike traditional image quality metrics that focus on pixel-level or structural similarity, ViTScore extracts semantic features using ViT and computes similarity through cosine similarity with max pooling. The metric is theoretically proven to have three key properties: symmetry, boundedness, and normalization. Extensive experiments across seven image datasets demonstrate ViTScore's superior performance in measuring semantic similarity, particularly for semantic transformations like image inversion, and its effectiveness in downstream computer vision tasks such as image captioning.

## Method Summary
ViTScore extracts semantic features from images using a pre-trained Vision Transformer (ViT) model, then computes similarity through cosine similarity of these features with max pooling. The method involves resizing input images to 224×224×3, extracting patch-level features using ViT, calculating pairwise cosine similarities between patches, and applying max pooling to aggregate these similarities into a final score. The metric is normalized to range between -1 and 1, with 1 indicating identical images. The approach addresses limitations of existing metrics like PSNR and MS-SSIM, which cannot effectively measure semantic similarity in semantic communications scenarios.

## Key Results
- ViTScore outperforms PSNR, MS-SSIM, and LPIPS in evaluating semantic changes from transformations like image inversion using GANs
- The metric demonstrates strong performance in downstream CV tasks, particularly image captioning, where it shows positive correlation with BERTScore
- ViTScore maintains effectiveness in image transmission scenarios, showing promise for semantic communications applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ViTScore measures global semantic similarity better than pixel-based metrics like PSNR.
- Mechanism: ViTScore extracts semantic features using a pre-trained Vision Transformer (ViT) and computes similarity via cosine similarity of these features with max pooling across all patches. This captures global semantic content rather than local pixel or structural details.
- Core assumption: ViT embeddings capture meaningful semantic features that correlate with human perception of image similarity.
- Evidence anchors:
  - [abstract] "The method extracts semantic features from images using ViT and computes similarity through cosine similarity of these features with max pooling."
  - [section] "ViTScore uses pre-trained ViT to extract the semantic features of given images, then calculated the semantic similarity using these features."
  - [corpus] Weak - corpus lacks direct evidence on ViTScore's superiority over other metrics.
- Break condition: If ViT embeddings fail to capture semantic features relevant to the task, or if max pooling discards critical positional information needed for similarity judgment.

### Mechanism 2
- Claim: ViTScore has desirable mathematical properties (symmetry, boundedness, normalization) making it convenient for implementation.
- Mechanism: The metric is defined such that ViTScore(A,B) = ViTScore(B,A) (symmetry), ranges between -1 and 1 (boundedness), and equals 1 when comparing an image to itself (normalization). These properties stem from using cosine similarity on normalized vectors and the max pooling operation.
- Core assumption: The mathematical properties hold for all possible image pairs under the ViTScore definition.
- Evidence anchors:
  - [abstract] "We prove theoretically that ViTScore has 3 important properties, including symmetry, boundedness, and normalization."
  - [section] "Theorem 1 (Symmetry). For any two images A and B, changing the order of the two images does not change the ViTScore... Theorem 2 (Boundedness)... Theorem 3 (Normalization)..."
  - [corpus] Weak - corpus doesn't verify these properties empirically.
- Break condition: If the underlying cosine similarity or max pooling operations behave unexpectedly for certain image types, the mathematical properties may not hold.

### Mechanism 3
- Claim: ViTScore outperforms LPIPS in evaluating semantic similarity, especially for semantic transformations like image inversion.
- Mechanism: While LPIPS uses mean pooling of feature distances from corresponding locations (losing global semantic measures), ViTScore uses max pooling which better captures global semantic information by finding the best matching patches across images.
- Core assumption: Global semantic information is more relevant than local perceptual similarity for semantic communications.
- Evidence anchors:
  - [abstract] "ViTScore outperforms the other 3 typical metrics in evaluating the image semantic changes by semantic attack, such as image inverse with Generative Adversarial Networks (GANs)."
  - [section] "LPIPS uses the mean pooling of feature distances from corresponding locations to evaluate the similarity of two images, leading to an absence of global semantic measures."
  - [corpus] Weak - corpus lacks direct evidence comparing ViTScore to LPIPS.
- Break condition: If local perceptual details are more important than global semantics for the specific application, LPIPS might perform better.

## Foundational Learning

- Concept: Cosine similarity and its properties
  - Why needed here: ViTScore fundamentally relies on cosine similarity between feature vectors extracted by ViT.
  - Quick check question: What is the range of cosine similarity values, and what do the extremes represent?

- Concept: Vision Transformer (ViT) architecture and self-attention mechanism
  - Why needed here: ViT is the core model used to extract semantic features from images.
  - Quick check question: How does ViT's self-attention mechanism enable it to capture global semantic information?

- Concept: Max pooling vs mean pooling in feature comparison
  - Why needed here: The choice between max and mean pooling significantly affects how local features are aggregated into a global similarity score.
  - Quick check question: What is the key difference between max pooling and mean pooling in terms of sensitivity to outliers and local feature matching?

## Architecture Onboarding

- Component map: Image → ViT feature extraction → Patch-level cosine similarities → Max pooling → Recall/Prevision scores → ViTScore
- Critical path: Image → ViT feature extraction → Patch-level cosine similarities → Max pooling → Recall/Prevision scores → ViTScore
- Design tradeoffs:
  - Pre-trained ViT vs training from scratch: Using pre-trained model saves computation but may not be optimal for all image domains
  - Max pooling vs mean pooling: Max pooling better captures global semantics but may be less stable
  - Fixed image size (224x224) vs variable sizes: Fixed size simplifies implementation but may lose information from very large/small images
- Failure signatures:
  - Low ViTScore for semantically similar images: May indicate ViT embeddings aren't capturing relevant semantics
  - High ViTScore for semantically dissimilar images: May indicate overfitting to specific image characteristics in pre-training
  - Unstable scores across different runs: May indicate issues with feature normalization or implementation
- First 3 experiments:
  1. Compare ViTScore vs PSNR/MS-SSIM/LPIPS on inverted images - expect ViTScore to perform better
  2. Test ViTScore on grayscale vs color versions of same image - expect high similarity scores
  3. Evaluate ViTScore correlation with BERTScore on image captioning dataset - expect positive correlation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical lower and upper bounds of ViTScore for semantically dissimilar images, and how do these compare to the practical bounds observed in experiments?
- Basis in paper: [explicit] The paper states ViTScore has a theoretical lower bound of -1 (Theorem 2) but notes that in practice, the minimum observed value is 0.19. The paper also mentions that the metric is normalized between -1 and 1.
- Why unresolved: The paper doesn't provide a theoretical derivation for the practical lower bound of 0.19, nor does it explain why this differs from the theoretical minimum. It also doesn't explore the upper bound behavior for extremely similar images.
- What evidence would resolve it: A theoretical analysis explaining why the practical minimum is 0.19, and empirical studies measuring ViTScore on images with varying degrees of semantic similarity to establish the actual practical bounds.

### Open Question 2
- Question: How does ViTScore perform in measuring semantic similarity for video content compared to its performance with static images?
- Basis in paper: [inferred] The paper focuses exclusively on image data and doesn't mention video applications. However, the authors note that video semantic communications is a future research direction, suggesting this is an unexplored area.
- Why unresolved: The paper's experiments are limited to image datasets, and the authors acknowledge that extending ViTScore to video would be future work. The temporal dimension in video presents unique challenges not present in static images.
- What evidence would resolve it: Experimental results comparing ViTScore performance on video datasets versus image datasets, and analysis of how temporal dynamics affect semantic similarity measurement.

### Open Question 3
- Question: How sensitive is ViTScore to different ViT model architectures and hyperparameters, and what is the optimal configuration for semantic similarity measurement?
- Basis in paper: [explicit] The paper mentions using a specific ViT model (vit base patch16 224) from the timm library but doesn't explore how different ViT architectures or hyperparameters affect ViTScore performance. An ablation study is mentioned but focuses on similarity metrics rather than ViT model choices.
- Why unresolved: The authors state they used pre-trained model parameters without exploring how different ViT configurations might impact semantic similarity measurement, leaving the optimal architecture unspecified.
- What evidence would resolve it: Comparative experiments using different ViT architectures (different patch sizes, depths, attention mechanisms) and hyperparameters to determine which configurations yield the best semantic similarity measurement performance.

## Limitations
- The paper demonstrates ViTScore's superiority on specific semantic transformation tasks but lacks comprehensive comparison across diverse image domains and quality degradation types
- While theoretical properties (symmetry, boundedness, normalization) are proven, empirical validation across edge cases is limited
- The choice of max pooling over mean pooling is justified through intuition rather than systematic ablation studies

## Confidence
- **High confidence**: Mathematical properties of ViTScore (symmetry, boundedness, normalization) - these follow directly from the cosine similarity formulation
- **Medium confidence**: ViTScore's superiority over existing metrics - supported by experiments but with limited dataset diversity
- **Low confidence**: Generalization to non-standard image transformations and quality degradations not explicitly tested

## Next Checks
1. **Edge case validation**: Test ViTScore on extreme cases including near-duplicate images with subtle differences, completely unrelated images, and images with severe compression artifacts to verify boundedness and sensitivity
2. **Cross-dataset generalization**: Evaluate ViTScore's correlation with human perceptual similarity ratings across datasets with different characteristics (medical images, satellite imagery, artistic content)
3. **Alternative pooling strategies**: Conduct systematic comparison of max pooling vs mean pooling vs attention-based pooling methods across the same experimental setup to quantify the contribution of the pooling choice