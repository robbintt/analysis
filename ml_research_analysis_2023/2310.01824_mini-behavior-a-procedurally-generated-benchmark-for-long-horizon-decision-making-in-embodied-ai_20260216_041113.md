---
ver: rpa2
title: 'Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making
  in Embodied AI'
arxiv_id: '2310.01824'
source_url: https://arxiv.org/abs/2310.01824
tags:
- vior
- objects
- agent
- tasks
- environment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mini-BEHAVIOR is a fast, procedurally generated benchmark for long-horizon
  decision-making in embodied AI. It provides a simplified 3D gridworld environment
  with realistic object states, vertical dimensions, and complex tasks derived from
  the original BEHAVIOR benchmark.
---

# Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI

## Quick Facts
- arXiv ID: 2310.01824
- Source URL: https://arxiv.org/abs/2310.01824
- Reference count: 40
- A fast, procedurally generated benchmark for long-horizon decision-making in embodied AI using a simplified 3D gridworld environment

## Executive Summary
Mini-BEHAVIOR is a procedurally generated benchmark designed for long-horizon decision-making in embodied AI. It provides a simplified 3D gridworld environment with realistic object states, vertical dimensions, and complex tasks derived from the original BEHAVIOR benchmark. The benchmark features procedural generation for infinite task variations, 96 objects with 23 symbolic states, 15 primitive actions, and support for both sparse and dense rewards. With 20 diverse household tasks ranging from simple object manipulation to complex multi-step activities, Mini-BEHAVIOR aims to serve as a user-friendly entry point for embodied AI research while preserving enough complexity for realistic task evaluation.

## Method Summary
Mini-BEHAVIOR uses a procedurally generated 3D gridworld environment with n × m × 3 cells to represent horizontal space and three discrete vertical levels. The environment supports 20 household tasks derived from the original BEHAVIOR benchmark, using BDDL (BEHAVIOR Domain Definition Language) to specify activities. Tasks are procedurally generated by recursively splitting a grid into rooms with walls and doors, then placing furniture and objects while ensuring reachability. The environment provides two action spaces: primitive actions (15 discrete) for transfer learning studies and Cartesian actions for efficient multi-object manipulation. Agents can be trained using reinforcement learning with either sparse rewards (task completion) or dense rewards (task progress).

## Key Results
- Mini-BEHAVIOR successfully generates infinite variations of household tasks through procedural generation
- Vanilla PPO struggles on tasks beyond the simplest ones even with dense rewards
- The 3D grid representation with three height levels enables realistic object state modeling while maintaining computational efficiency
- The benchmark provides a significant challenge for current decision-making algorithms despite its simplified representation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Procedural generation enables infinite task variations without manual scene design
- Mechanism: The environment takes an initial activity configuration and recursively generates walls with doors to split the grid into rooms, then places furniture and objects while ensuring no space overlap and reachability
- Core assumption: Random placement within defined ranges will always produce valid, reachable layouts
- Evidence anchors: [section]: "Procedural Floor Plan Generation... randomly generating a wall with a door to split the grid into two connected rooms, choosing a room, and then repeating the wall generation process to further split the grid until a desired number of rooms is reached." [corpus]: Weak evidence - corpus contains no papers specifically about procedural generation in gridworlds

### Mechanism 2
- Claim: 3D grid with vertical dimension enables realistic object state representations
- Mechanism: The environment uses an n × m × 3 cell structure where the z-axis represents vertical placement, allowing states like "OnTop" and "Under" to be physically meaningful
- Core assumption: Three discrete height levels provide sufficient realism for most household tasks while maintaining computational simplicity
- Evidence anchors: [section]: "Mini-BEHAVIOR is a Gridworld environment that is designed to support vertical relationships between objects and notions of height... We choose this limitation to balance simplicity and realism in our environment." [abstract]: "preserving a symbolic level of physical realism and complexity"

### Mechanism 3
- Claim: Multiple action spaces enable different research objectives
- Mechanism: Primitive actions (15 discrete) share the same space across tasks for transfer learning studies, while Cartesian actions (4 + sum of object-specific actions) enable efficient multi-object manipulation
- Core assumption: The trade-off between action space generality and task-specific efficiency is well-calibrated for target research questions
- Evidence anchors: [section]: "This results in an 15-dimensional discrete action space... Such an action space makes carrying and dropping multiple objects possible, and is often better suited for studying how to solve a single task." [abstract]: "open-ended generative system that can create potentially infinite variations of each task"

## Foundational Learning

- Concept: Symbolic state representation in embodied AI
  - Why needed here: Activities are defined using BDDL with predicates like "OnTop(printer, table)" and "ToggledOn(printer)" that must map to physical states
  - Quick check question: How does the environment represent the state "OnTop(book, table)" in its 3D grid structure?

- Concept: Procedural content generation
  - Why needed here: Manual scene design is infeasible for creating infinite task variations; procedural generation automates this while maintaining physical plausibility
  - Quick check question: What validation step ensures that procedurally generated environments are navigable by the agent?

- Concept: Action space design trade-offs
  - Why needed here: Different research goals (transfer learning vs single-task optimization) require different action space granularities and dimensions
  - Quick check question: What is the maximum action dimension in the Cartesian action space, and which task requires it?

## Architecture Onboarding

- Component map: Configuration input → Procedural generation → State initialization → Agent-environment interaction loop → Reward calculation → Episode termination
- Critical path: Configuration input → Procedural generation → State initialization → Agent-environment interaction loop → Reward calculation → Episode termination
- Design tradeoffs: Simplicity/speed vs realism (3 height levels vs continuous), discrete actions vs continuous control, partial vs full observability
- Failure signatures: Stuck agents (invalid generation), zero learning progress (sparse rewards), crashes (invalid object placement)
- First 3 experiments:
  1. Run "Installing a Printer" with sparse rewards and visualize agent behavior to verify basic mechanics
  2. Test procedural generation with varying room counts to ensure reachability check works
  3. Compare primitive vs Cartesian action space performance on "Putting away dishes" to understand action space impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the procedural generation in Mini-BEHAVIOR ensure the reachability and validity of generated task instances?
- Basis in paper: [explicit] The paper mentions a "reachability check" to ensure that all empty tiles are connected, but does not detail how this is implemented or its effectiveness
- Why unresolved: The paper provides a high-level overview of the procedural generation process but lacks detailed technical information on how the reachability check is performed
- What evidence would resolve it: A detailed description of the reachability check algorithm and empirical results demonstrating its effectiveness in generating valid task instances

### Open Question 2
- Question: What is the impact of the limited 3D environment (n × m × 3 cells) on the realism and complexity of tasks compared to the original BEHAVIOR benchmark?
- Basis in paper: [inferred] The paper states that the 3D environment is designed to support vertical relationships and notions of height, but it also acknowledges that the limitation of 3 z-axis coordinates is not entirely reflective of the real world
- Why unresolved: The paper does not provide a quantitative comparison of task complexity or realism between Mini-BEHAVIOR and BEHAVIOR
- What evidence would resolve it: A comparative analysis of task completion rates and agent performance between Mini-BEHAVIOR and BEHAVIOR, highlighting the limitations and advantages of the 3D environment

### Open Question 3
- Question: How do different reward structures (sparse vs. dense) affect the learning efficiency and performance of agents in Mini-BEHAVIOR?
- Basis in paper: [explicit] The paper mentions the use of both sparse and dense rewards but only provides preliminary results for a few tasks
- Why unresolved: The paper does not explore the impact of different reward structures across a wider range of tasks or provide a comprehensive analysis of their effects on learning efficiency
- What evidence would resolve it: A systematic study comparing the performance of agents trained with sparse and dense rewards across all 20 tasks, including learning curves and final task completion rates

## Limitations

- The 3D grid representation with only three discrete height levels may inadequately capture real-world physics and continuous spatial relationships
- The discrete action space restricts the types of behaviors that can be studied compared to continuous control approaches
- The benchmark's focus on household tasks limits its generalizability to other domains

## Confidence

- **High Confidence**: The procedural generation mechanism works as described and produces valid environments (supported by the reachability check implementation details and the successful demonstration of basic tasks)
- **Medium Confidence**: The claim that Mini-BEHAVIOR presents significant challenges for current decision-making algorithms is supported by the PPO results but requires more extensive evaluation across different algorithms and task complexities
- **Medium Confidence**: The benchmark serves as a user-friendly entry point for embodied AI research, based on its simplified interface and faster iteration times compared to the original BEHAVIOR, though user studies would strengthen this claim
- **Low Confidence**: The assertion that Mini-BEHAVIOR preserves "enough complexity for realistic task evaluation" lacks quantitative validation against real-world performance or comparison with alternative benchmarks

## Next Checks

1. **Reachability Verification**: Systematically test the procedural generation with varying room counts (2-10 rooms) and grid sizes to quantify the failure rate and ensure the reachability check reliably prevents invalid configurations

2. **Cross-algorithm Performance Analysis**: Evaluate multiple RL algorithms (PPO, SAC, A2C) and planning-based approaches on a representative subset of tasks to determine if the observed challenges are algorithm-specific or inherent to the benchmark design

3. **State Space Fidelity Assessment**: Compare the symbolic state representations in Mini-BEHAVIOR against the original BEHAVIOR benchmark for a subset of tasks to quantify the information loss and its impact on task solvability