---
ver: rpa2
title: Descriptive Knowledge Graph in Biomedical Domain
arxiv_id: '2310.11681'
source_url: https://arxiv.org/abs/2310.11681
tags:
- relation
- sentences
- entity
- entities
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a novel system for efficiently extracting and
  organizing relational knowledge from biomedical literature into a descriptive knowledge
  graph. The system uses entity extraction, sentence scoring, and graph construction
  to create a COVID-19-focused knowledge graph called CovidDEER.
---

# Descriptive Knowledge Graph in Biomedical Domain

## Quick Facts
- arXiv ID: 2310.11681
- Source URL: https://arxiv.org/abs/2310.11681
- Reference count: 12
- Primary result: Novel system for extracting and organizing biomedical relational knowledge into descriptive knowledge graphs

## Executive Summary
This paper presents CovidDEER, a system that constructs descriptive knowledge graphs from biomedical literature by treating sentences as edges rather than nodes. The system uses entity extraction, sentence scoring based on dependency path frequencies, and graph construction to create a COVID-19-focused knowledge graph. It provides interactive graph queries, multi-hop reasoning, and modifier filtering for exploration. The system employs both a fine-tuned relation synthesis model and ChatGPT to generate concise relation descriptions, reducing the need for extensive human reading. The system is evaluated through relation synthesis model faithfulness and demonstrated in drug repurposing and literature curation tasks.

## Method Summary
The system ingests biomedical literature (CORD-19 corpus), extracts and links entities using NCBI Pubtator API and SciSpacy, scores sentences using relation description score (RDS) based on dependency path frequencies, filters with threshold 0.7, and constructs a descriptive knowledge graph. It supports entity-entity, entity-type, and multi-hop queries with modifier filtering, and uses a fine-tuned relation synthesis model and ChatGPT to generate concise relation descriptions from multi-hop paths.

## Key Results
- Relation synthesis model achieves average faithfulness score of 4.10/5.0
- System successfully demonstrated in drug repurposing and literature curation tasks
- Graph query interface enables exploration of both direct and indirect entity relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using dependency path frequency from domain corpus to score relation descriptions enables accurate filtering of high-quality relational sentences.
- Mechanism: The relation description score (RDS) computes a score based on the frequency of dependency paths between head entity, tail entity, and relation words in the domain corpus. Higher frequency paths get higher scores.
- Core assumption: Dependency paths that frequently occur in the corpus reliably indicate meaningful relational sentences between entities.
- Evidence anchors:
  - [section] "We use the relation description score (RDS) introduced in Huang et al. (2022b) as the scoring function. This scoring function extracts the dependency path between the head entity, tail entity and other relation-related words in a sentence and generates a score between 0 and 1 to indicate how well this sentence expresses the relationship of the entities."
  - [corpus] Weak evidence - the paper uses CORD-19 corpus for path frequency but doesn't validate that this corpus has sufficient coverage of all possible biomedical dependency patterns.
- Break condition: If the corpus doesn't contain sufficient examples of the dependency patterns for a particular biomedical domain or relationship type, the scoring will be unreliable.

### Mechanism 2
- Claim: Organizing sentences as a descriptive knowledge graph enables efficient multi-hop reasoning while preserving contextual information.
- Mechanism: Instead of treating sentences as independent units, they're organized into a graph where edges represent relational sentences between entities. This allows traversal across multiple connected edges to infer indirect relationships.
- Core assumption: The majority of relevant relational sentences between entities will be captured as direct edges, making multi-hop inference meaningful.
- Evidence anchors:
  - [abstract] "enabling researchers to explore closely related biomedical entities (e.g., diseases treated by a chemical) or indirectly connected entities (e.g., potential drugs for treating a disease)"
  - [section] "With this tool, our system could beat traditional knowledge graphs by providing the contextualized knowledge, and beat the text-based search engines by allowing multi-hop retrieval with one query."
- Break condition: If critical relational sentences are missing from the graph (due to filtering or corpus limitations), multi-hop reasoning will produce incomplete or misleading results.

### Mechanism 3
- Claim: Using ChatGPT and a fine-tuned relation synthesis model to generate concise relation descriptions reduces reading effort while maintaining accuracy.
- Mechanism: The synthesis model takes multiple relational sentences from a path and generates a single summary sentence. ChatGPT provides an alternative with potentially richer context.
- Core assumption: The generated summaries accurately represent the information in the source sentences and don't introduce errors or hallucinations.
- Evidence anchors:
  - [section] "we trained a relation synthesis model (Huang et al., 2022b), which is based on a Fusion-in-Decoder model (Izacard and Grave, 2020) trained to take sequences of relation descriptions from the multi-hop paths between two entities in DEER and generate one single relation description for the entities."
  - [section] "the final average score for the 20 samples is 4.10, indicating that the generation is generally supported by the input. However, there is still a gap before we can fully trust it"
- Break condition: If the model generates summaries that omit critical information or hallucinate facts not present in the source sentences, users may be misled.

## Foundational Learning

- Concept: Biomedical entity extraction and linking
  - Why needed here: The system needs to identify and normalize biomedical entities (diseases, chemicals, genes, etc.) to build the knowledge graph
  - Quick check question: What libraries does the system use for entity extraction and linking in the biomedical domain?

- Concept: Dependency parsing and path extraction
  - Why needed here: The RDS scoring function relies on extracting dependency paths between entities to determine relation quality
  - Quick check question: How does the system extract dependency paths between head and tail entities in a sentence?

- Concept: Knowledge graph construction from text
  - Why needed here: The core innovation is organizing descriptive sentences as edges in a knowledge graph rather than as independent text units
  - Quick check question: What criteria determine whether a sentence becomes an edge in the descriptive knowledge graph?

## Architecture Onboarding

- Component map: Corpus Ingestion -> Entity Extraction & Linking -> Sentence Processing -> Graph Construction -> Graph Query Engine -> Modifier Filtering -> Relation Synthesis -> Web Interface
- Critical path: Corpus → Entity Extraction → Sentence Scoring → Graph Construction → Query Processing → User Interface
- Design tradeoffs: The system prioritizes comprehensiveness (including all relevant sentences) over conciseness (summarizing into fewer edges), trading storage for richer context
- Failure signatures: 
  - Empty query results indicate entity recognition failures or corpus coverage gaps
  - Poor-quality edges suggest RDS scoring threshold issues
  - Slow queries indicate graph traversal optimization needs
- First 3 experiments:
  1. Run entity extraction on a small sample of sentences to verify it correctly identifies biomedical entities and links them to ontologies
  2. Test RDS scoring on known good/bad relational sentences to verify the threshold of 0.7 effectively filters quality
  3. Perform a simple entity-entity query with a known relationship to verify the graph returns appropriate sentences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the relation synthesis model compare to other existing methods for generating concise relation descriptions in the biomedical domain?
- Basis in paper: [inferred] The paper mentions that the relation synthesis model is evaluated for faithfulness but does not provide a direct comparison to other methods.
- Why unresolved: The paper focuses on the faithfulness of the generated relation descriptions but does not benchmark the model against other existing methods in the field.
- What evidence would resolve it: Comparative evaluation results showing the performance of the relation synthesis model against other state-of-the-art methods for generating concise relation descriptions in the biomedical domain.

### Open Question 2
- Question: How does the system handle the extraction and organization of relational knowledge from biomedical literature when dealing with highly specialized or technical terms?
- Basis in paper: [inferred] The paper mentions the use of entity extraction and linking using the NCBI Pubtator API and SciSpacy library but does not provide specific details on handling specialized or technical terms.
- Why unresolved: The paper does not explicitly address how the system handles the extraction and organization of relational knowledge when dealing with highly specialized or technical terms in the biomedical domain.
- What evidence would resolve it: Detailed explanation or evaluation of the system's performance in handling specialized or technical terms during the extraction and organization of relational knowledge from biomedical literature.

### Open Question 3
- Question: How scalable is the system for handling larger and more diverse biomedical corpora beyond the COVID-19 domain?
- Basis in paper: [explicit] The paper mentions that the system is built upon the COVID-19 Open Research Dataset (CORD-19) and demonstrates its application in drug repurposing and literature curation tasks.
- Why unresolved: The paper does not provide information on the system's scalability for handling larger and more diverse biomedical corpora beyond the COVID-19 domain.
- What evidence would resolve it: Results from experiments or evaluations showing the system's performance and scalability when applied to larger and more diverse biomedical corpora beyond the COVID-19 domain.

## Limitations

- The relation synthesis model evaluation shows promising but incomplete results, with an acknowledged gap between current performance and full trustworthiness
- Reliance on ChatGPT introduces uncertainty due to lack of quantitative evaluation of its outputs compared to the fine-tuned model
- Limited empirical validation of multi-hop reasoning quality and reasoning accuracy

## Confidence

- Confidence in core graph construction mechanism: High
- Confidence in multi-hop reasoning capability: Medium
- Confidence in relation synthesis component: Low

## Next Checks

1. **Faithfulness Validation**: Conduct a systematic evaluation of both the fine-tuned relation synthesis model and ChatGPT outputs across a diverse set of biomedical relationships, measuring precision, recall, and hallucination rates compared to source sentences.

2. **Multi-hop Reasoning Accuracy**: Test the system's ability to correctly infer indirect relationships by comparing multi-hop query results against manually curated ground truth for known biomedical entity pairs.

3. **Coverage Assessment**: Evaluate the RDS scoring threshold of 0.7 by measuring recall of relevant sentences at various threshold levels and assessing whether the scoring function reliably captures diverse biomedical relationship types across different sub-domains.