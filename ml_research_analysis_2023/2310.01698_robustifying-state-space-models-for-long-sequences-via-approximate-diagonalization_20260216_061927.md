---
ver: rpa2
title: Robustifying State-space Models for Long Sequences via Approximate Diagonalization
arxiv_id: '2310.01698'
source_url: https://arxiv.org/abs/2310.01698
tags:
- have
- section
- matrix
- theorem
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a method for improving the robustness of
  state-space models (SSMs) for long sequences. SSMs, like the S4 layer, use the diagonal-plus-low-rank
  structure of the HiPPO initialization framework to efficiently capture long-range
  temporal dependencies.
---

# Robustifying State-space Models for Long Sequences via Approximate Diagonalization

## Quick Facts
- arXiv ID: 2310.01698
- Source URL: https://arxiv.org/abs/2310.01698
- Reference count: 40
- Primary result: Introduces PTD methodology that improves robustness of SSMs to Fourier-mode noise perturbations

## Executive Summary
This paper addresses the instability of diagonalized state-space models (SSMs) like S4D and S5, which arise from an ill-posed diagonalization problem in the HiPPO framework. The authors propose a "perturb-then-diagonalize" (PTD) methodology based on pseudospectral theory that adds small random perturbations before diagonalization, transforming an ill-posed problem into a well-posed one. This approach yields S4-PTD and S5-PTD models that demonstrate strong convergence to the HiPPO framework and improved robustness to Fourier-mode noise perturbations while maintaining competitive performance on long-sequence tasks.

## Method Summary
The authors introduce a PTD methodology that solves the ill-posed diagonalization problem by first perturbing the HiPPO matrix with a small random matrix before diagonalization. This involves solving an optimization problem to compute a perturbation matrix E that regularizes the ill-posed problem while preserving transfer function properties. The resulting models (S4-PTD and S5-PTD) maintain the diagonal structure of S4D/S5 but with improved backward stability and robustness. The methodology is applied to both S4 and S5 architectures, demonstrating improved resilience to Fourier-mode noise while maintaining competitive performance on the Long-Range Arena benchmark.

## Key Results
- S4-PTD model outperforms S4D on the Long-Range Arena benchmark
- S5-PTD matches S5 performance while demonstrating improved robustness
- S5-PTD achieves 87.6% average accuracy on Long-Range Arena
- Theoretical analysis shows S4-PTD/S5-PTD initialization strongly converges to HiPPO framework while S4D/S5 only weakly converges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perturbing the HiPPO matrix before diagonalization improves backward stability and robustness
- Mechanism: Small random perturbations regularize the ill-posed diagonalization problem, effectively transforming it into a well-posed problem while preserving the transfer function properties
- Core assumption: Backward stability (small backward error) is sufficient for good performance, even with large forward error
- Evidence anchors:
  - [abstract] "we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable 'perturb-then-diagonalize' (PTD) methodology"
  - [section] "using a backward stable eigensolver, one can recover the non-normal matrix accurately (i.e., we can have a small backward error) from the wrong eigenvalues and eigenvectors"
  - [corpus] Weak - no direct evidence in corpus about perturbation improving stability
- Break condition: If perturbation size becomes too large relative to the original matrix, the initialization scheme becomes random and performance degrades

### Mechanism 2
- Claim: Diagonal initialization (S4D/S5) only weakly converges to HiPPO framework while PTD initialization strongly converges
- Mechanism: Transfer function analysis shows that diagonal models diverge from HiPPO framework at high frequencies, while perturbed diagonal models maintain close transfer function behavior across all frequencies
- Core assumption: Transfer function similarity is sufficient for model equivalence in practical applications
- Evidence anchors:
  - [abstract] "we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences"
  - [section] "the outputs of the two systems diverge given the unit impulse (i.e., the Dirac delta function) as the input"
  - [corpus] Weak - no direct evidence in corpus about transfer function convergence properties
- Break condition: If input signals have frequency modes exactly at the spike locations, even PTD models may show instability

### Mechanism 3
- Claim: PTD models are robust to Fourier-mode noise perturbations while diagonal models are not
- Mechanism: Perturbation regularizes the eigenvector condition number, eliminating the exponential growth that causes sensitivity to specific frequency modes
- Core assumption: Sensitivity to Fourier-mode noise is primarily determined by eigenvector conditioning
- Evidence anchors:
  - [abstract] "our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models"
  - [section] "the S4D model is unstable near these modes" (referring to Fourier modes near spikes)
  - [corpus] Weak - no direct evidence in corpus about Fourier-mode noise robustness
- Break condition: If perturbation size is too small, eigenvector conditioning remains problematic; if too large, initialization loses HiPPO properties

## Foundational Learning

- Concept: Ill-posed problems and conditioning
  - Why needed here: Understanding why diagonalizing HiPPO matrices is ill-posed and how conditioning affects numerical stability
  - Quick check question: What is the relationship between condition number and backward/forward error in solving ill-posed problems?

- Concept: Transfer functions and frequency domain analysis
  - Why needed here: Comparing different initialization schemes requires analyzing how they transform inputs to outputs in frequency domain
  - Quick check question: How does the transfer function difference between two LTI systems relate to their output difference for a given input?

- Concept: State-space models and continuous-time systems
  - Why needed here: PTD methodology applies to any state-space model initialization, not just HiPPO
  - Quick check question: What is the relationship between continuous-time LTI systems and their discretized counterparts used in SSMs?

## Architecture Onboarding

- Component map: HiPPO matrix + Perturbation matrix -> Diagonalized matrix -> SSM initialization -> Model training
- Critical path:
  1. Compute perturbation matrix E by solving optimization problem (12)
  2. Add perturbation to HiPPO matrix: AH + E
  3. Diagonalize perturbed matrix: (AH + E) = VΛV^(-1)
  4. Initialize SSM with diagonal matrix Λ and transformed input matrix V^(-1)BH
  5. Train model normally
- Design tradeoffs:
  - Perturbation size vs. robustness: Larger perturbations improve robustness but may degrade initialization quality
  - Computational cost: Solving optimization problem adds overhead but enables stable diagonalization
  - Flexibility: PTD works with any initialization, not just HiPPO
- Failure signatures:
  - Eigenvector condition number κ(Ṽ) grows exponentially with state dimension
  - Transfer function deviation GPert(s) - GDPLR(s) becomes large at certain frequencies
  - Model performance degrades on Fourier-mode noise-perturbed inputs
- First 3 experiments:
  1. Verify perturbation improves eigenvector conditioning: Compare κ(Ṽ) for AH vs AH + E
  2. Test transfer function similarity: Compare |GPert(s) - GDPLR(s)| across frequency range
  3. Validate robustness: Train S4D and S4-PTD on noisy data, compare generalization performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical limit of the perturbation size in the PTD methodology, and how does it affect the performance and robustness of the model?
- Basis in paper: [explicit] The paper discusses the effect of the perturbation size on the performance of the S4-PTD and S5-PTD models, but does not provide a theoretical limit
- Why unresolved: The paper provides empirical evidence on the effect of perturbation size, but does not establish a theoretical limit
- What evidence would resolve it: A theoretical analysis of the maximum perturbation size that maintains model performance and robustness, supported by empirical validation

### Open Question 2
- Question: How does the PTD methodology perform on other non-normal matrix diagonalization problems in machine learning, beyond the HiPPO framework?
- Basis in paper: [explicit] The paper introduces the PTD methodology as a general solution for ill-posed diagonalization problems, but its performance on other non-normal matrices is not explored
- Why unresolved: The paper focuses on the application of PTD to the HiPPO framework and does not extend the methodology to other non-normal matrices
- What evidence would resolve it: Empirical studies applying the PTD methodology to other non-normal matrices in machine learning, demonstrating its effectiveness and limitations

### Open Question 3
- Question: What is the impact of the PTD methodology on the computational efficiency of state-space models, especially for very long sequences?
- Basis in paper: [explicit] The paper mentions that the PTD methodology simplifies the implementation and improves computational efficiency, but does not provide detailed analysis on its impact for very long sequences
- Why unresolved: The paper does not provide a comprehensive analysis of the computational efficiency of the PTD methodology, particularly for very long sequences
- What evidence would resolve it: A detailed computational analysis comparing the PTD methodology with other state-space models, focusing on efficiency for very long sequences

## Limitations
- The exact relationship between perturbation size and model performance remains empirical rather than theoretically characterized
- The optimization problem for computing the perturbation matrix uses gradient descent without thorough analysis of convergence properties
- Direct empirical validation of the backward error bounds is limited

## Confidence

- **High**: The core PTD methodology and its mathematical foundations (pseudospectral theory, backward stability)
- **Medium**: Claims about transfer function convergence properties and their practical implications
- **Low**: The specific choice of perturbation optimization formulation and its optimality

## Next Checks
1. **Eigenvector Condition Number Analysis**: Systematically measure κ(Ṽ) for AH vs AH + E across different state dimensions and perturbation sizes to verify the claimed improvement in conditioning
2. **Transfer Function Sensitivity**: Compute |GPert(s) - GDPLR(s)| across a dense frequency grid and identify specific frequency regions where the difference exceeds acceptable thresholds
3. **Perturbation Size Sweep**: Train S4-PTD models with varying perturbation constraints (∥E∥/∥AH∥ from 0.01 to 0.5) to characterize the trade-off between robustness and initialization quality