---
ver: rpa2
title: 'Length Extrapolation of Transformers: A Survey from the Perspective of Positional
  Encoding'
arxiv_id: '2312.17044'
source_url: https://arxiv.org/abs/2312.17044
tags:
- position
- length
- extrapolation
- which
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive and organized overview of
  research efforts on length extrapolation of Transformers, focusing on position encodings
  and relevant methods such as position interpolation and randomized position encodings.
  The paper highlights the importance of length extrapolation for large language models
  (LLMs) and their applications in long sequences.
---

# Length Extrapolation of Transformers: A Survey from the Perspective of Positional Encoding

## Quick Facts
- arXiv ID: 2312.17044
- Source URL: https://arxiv.org/abs/2312.17044
- Reference count: 12
- Primary result: This survey provides a comprehensive overview of research efforts on length extrapolation of Transformers, focusing on position encodings and relevant methods such as position interpolation and randomized position encodings.

## Executive Summary
This survey provides a comprehensive and organized overview of research efforts on length extrapolation of Transformers, focusing on position encodings and relevant methods such as position interpolation and randomized position encodings. The paper highlights the importance of length extrapolation for large language models (LLMs) and their applications in long sequences. Key methods discussed include absolute and relative position encodings, position interpolation techniques like NTK-aware interpolation, and randomized position encodings. The survey aims to enable researchers and practitioners to understand existing methods and stimulate future research in this vibrant area.

## Method Summary
The survey synthesizes various approaches to length extrapolation in Transformers, particularly focusing on position encoding methods. It examines absolute and relative position encodings, position interpolation techniques (such as NTK-aware Scaled RoPE), and randomized position encodings. The survey discusses how these methods address the challenge of enabling Transformers to handle sequences longer than those seen during training, with particular emphasis on their application to large language models.

## Key Results
- Relative position encodings (RPEs) inherently enable length extrapolation due to shift invariance
- Position interpolation methods align position indices during inference with those seen during training
- Randomized position encodings expose models to wider position representations during training
- The survey highlights the importance of length extrapolation for LLMs handling long sequences
- Existing evaluation metrics like perplexity are inadequate for measuring downstream task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Relative position encodings (RPEs) inherently enable length extrapolation due to shift invariance.
- Mechanism: RPEs encode relative distances between tokens rather than absolute positions, so the model's attention scores depend only on the distance between tokens, not on their absolute position in the sequence. This makes the model invariant to shifts in position indices, allowing it to generalize to longer sequences beyond those seen during training.
- Core assumption: The model's attention mechanism can learn to rely solely on relative positional information, ignoring absolute positions.
- Evidence anchors:
  - [abstract] "Albeit the stunning progress on various tasks made by neural networks, length extrapolation remains a significant challenge for them."
  - [section] "it is believed that RPEs are theoretically capable of running on unseen lengths and are more robust to input length change (Neishi and Yoshinaga, 2019; Likhomanenko et al., 2021; Chi et al., 2022), as RPEs only rely on relative positional information, which means they encode the idea of shift invariance naturally and are not subject to a maximum position value."
  - [corpus] Weak - no direct citations in corpus evidence.
- Break condition: If the model learns to rely on absolute position information or if the relative position representations do not capture sufficient information for the task.

### Mechanism 2
- Claim: Position interpolation methods enable length extrapolation by aligning position indices during inference with those seen during training.
- Mechanism: Position interpolation methods scale or modify position indices during inference so that they fall within the range of position indices seen during training. This prevents the model from encountering out-of-distribution position representations, which can lead to performance degradation.
- Core assumption: The model's performance is sensitive to the range of position indices it has seen during training.
- Evidence anchors:
  - [section] "Chen et al. (2023) firstly introduced position interpolation for RoPE based on a simple idea to extrapolate LLMs to longer sequences, which is applying linear scaling to down-scale this position indices so that the maximum position index matches the previous length limit during pretraining."
  - [section] "This method reduces absolute position indices from[0, L′) to [0, L) to match the original range, which also reduces maximum relative distance from L′ to L."
  - [corpus] Weak - no direct citations in corpus evidence.
- Break condition: If the model's attention mechanism is highly sensitive to the specific values of position indices, not just their relative ordering.

### Mechanism 3
- Claim: Randomized position encodings enable length extrapolation by exposing the model to a wider range of position representations during training.
- Mechanism: Randomized position encodings simulate longer sequences during training by randomly selecting subsets of positions from a larger range. This exposes the model to a wider range of position representations, preventing it from overfitting to a limited set of positions and enabling it to generalize to longer sequences during inference.
- Core assumption: The model's position representations are not tied to specific absolute positions, but rather to relative positions within the sequence.
- Evidence anchors:
  - [section] "As a materialization of this idea, Ruoss et al. (2023) proposed to simulate the positions of much longer sequences and randomly selects an ordered subset to fit the training context window."
  - [section] "Thus, through adequate training, we can ensure that the model encounters enough unique positions and all positions from 1 to M have been fully trained before inference, leading to consistent performance on any sequences within E tokens."
  - [corpus] Weak - no direct citations in corpus evidence.
- Break condition: If the model's position representations are tied to specific absolute positions, or if the randomization process introduces too much noise.

## Foundational Learning

- Concept: Transformer architecture and attention mechanism.
  - Why needed here: Understanding how Transformers work is crucial for understanding why length extrapolation is a challenge and how different position encoding methods address this challenge.
  - Quick check question: What is the key difference between the attention mechanism in a Transformer and a recurrent neural network (RNN)?

- Concept: Positional encoding methods (absolute and relative).
  - Why needed here: Different positional encoding methods have different properties that affect their ability to enable length extrapolation. Understanding these properties is crucial for selecting the appropriate method for a given task.
  - Quick check question: What is the key difference between absolute and relative positional encodings?

- Concept: Length extrapolation and its challenges.
  - Why needed here: Length extrapolation is the core problem being addressed in this survey. Understanding the challenges associated with length extrapolation is crucial for appreciating the importance of different position encoding methods.
  - Quick check question: Why is length extrapolation a challenge for Transformers?

## Architecture Onboarding

- Component map: Transformer architecture (encoder/decoder layers, self-attention, feed-forward networks) -> Positional encoding methods (absolute and relative) -> Length extrapolation methods (position interpolation, randomized position encodings)

- Critical path: Implementing a Transformer model with a specific positional encoding method and evaluating its ability to perform length extrapolation.

- Design tradeoffs:
  - Absolute vs. relative positional encodings: Absolute encodings are simpler but may not generalize as well to longer sequences. Relative encodings are more complex but can inherently handle longer sequences.
  - Position interpolation vs. randomized position encodings: Position interpolation is simpler but may require fine-tuning. Randomized position encodings are more complex but can enable extrapolation without fine-tuning.

- Failure signatures:
  - Poor performance on sequences longer than those seen during training.
  - Overfitting to a limited set of position representations.
  - Sensitivity to the specific values of position indices.

- First 3 experiments:
  1. Implement a Transformer model with sinusoidal absolute positional encodings and evaluate its ability to perform length extrapolation.
  2. Implement a Transformer model with rotary positional encodings (RoPE) and evaluate its ability to perform length extrapolation.
  3. Implement a Transformer model with randomized position encodings and evaluate its ability to perform length extrapolation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the true theoretical foundation for length extrapolation in Transformers, beyond empirical observations?
- Basis in paper: [explicit] The paper discusses the lack of a solid theoretical foundation and mentions efforts to quantify extrapolation capability through metrics like cumulative normalized gradient and attention resolution, but acknowledges that a definitive answer remains open.
- Why unresolved: Despite various methods and metrics proposed, the paper highlights that the underlying mechanisms enabling length extrapolation are not fully understood, with some studies even showing contradictory results (e.g., decoder-only models without PE performing better in certain tasks).
- What evidence would resolve it: A comprehensive theoretical framework that explains why certain positional encoding methods or architectural choices lead to better length extrapolation, validated across diverse tasks and model scales.

### Open Question 2
- Question: How can we design benchmarks and evaluation metrics that accurately reflect downstream task performance and not just perplexity for length extrapolation?
- Basis in paper: [explicit] The paper explicitly states that perplexity as the sole metric is inadequate for evaluating length extrapolation, as it does not shed light on downstream task performance.
- Why unresolved: Current evaluation practices primarily rely on perplexity and synthetic tasks, which may not capture the nuances of real-world applications that require handling long sequences.
- What evidence would resolve it: Development and validation of benchmark datasets and evaluation metrics that specifically target long-sequence understanding and generation tasks, showing correlation with improved downstream performance.

### Open Question 3
- Question: How can we achieve a balance between general performance and extrapolation capability in positional encodings for large language models?
- Basis in paper: [explicit] The paper discusses the trade-off between the general performance of widely adopted methods like RoPE and their poor extrapolation, suggesting that finding a balance remains a significant challenge.
- Why unresolved: While methods like NTK-aware interpolation and randomized PEs have shown promise in enhancing extrapolation, they may not always match the in-distribution performance of methods like RoPE, leaving the optimal balance unclear.
- What evidence would resolve it: Comparative studies demonstrating the effectiveness of different positional encoding methods across a range of sequence lengths and tasks, with clear metrics for both general performance and extrapolation capability.

## Limitations

- The survey lacks extensive empirical validation across diverse tasks for the mechanisms enabling length extrapolation
- Limited discussion of computational costs and practical deployment considerations for different extrapolation methods
- Insufficient analysis of failure modes when combining multiple extrapolation techniques

## Confidence

- Medium confidence in Mechanism 1 (RPEs enabling extrapolation) - supported by theoretical arguments but limited empirical validation
- Medium confidence in Mechanism 2 (position interpolation) - demonstrated in specific cases but lacks comprehensive analysis
- Low confidence in Mechanism 3 (randomized position encodings) - promising concept but insufficient empirical evidence

## Next Checks

1. Empirical comparison of RPE-based methods versus position interpolation across diverse model architectures and tasks to validate the claimed superiority of RPEs for length extrapolation
2. Systematic study of failure conditions for position interpolation, particularly when the scaling factor leads to position indices that fall outside the learned embedding space
3. Evaluation of randomized position encoding methods with varying randomization strategies to identify optimal approaches and failure conditions when randomization introduces excessive noise