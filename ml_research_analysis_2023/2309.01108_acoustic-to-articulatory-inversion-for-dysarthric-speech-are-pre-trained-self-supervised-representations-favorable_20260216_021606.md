---
ver: rpa2
title: 'Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained
  self-supervised representations favorable?'
arxiv_id: '2309.01108'
source_url: https://arxiv.org/abs/2309.01108
tags:
- speech
- features
- mfccs
- training
- dysarthric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates acoustic-to-articulatory inversion (AAI)
  for dysarthric speech using self-supervised learning (SSL) representations. The
  authors propose using pre-trained SSL models like wav2vec, APC, DeCoAR, and others,
  combined with x-vectors, to predict articulatory trajectories from acoustic features.
---

# Acoustic-to-articulatory inversion for dysarthric speech: Are pre-trained self-supervised representations favorable?

## Quick Facts
- arXiv ID: 2309.01108
- Source URL: https://arxiv.org/abs/2309.01108
- Reference count: 0
- Primary result: DeCoAR SSL features with fine-tuning achieve ~1.81% and ~4.56% relative improvement in Pearson Correlation Coefficient over MFCCs for healthy controls and dysarthric patients respectively

## Executive Summary
This paper investigates acoustic-to-articulatory inversion (AAI) for dysarthric speech using self-supervised learning (SSL) representations. The authors propose using pre-trained SSL models (wav2vec, APC, DeCoAR, etc.) combined with x-vectors to predict articulatory trajectories from acoustic features. Experiments on the TORGO dataset show that DeCoAR in the fine-tuned scheme achieves the best performance, with SSL features performing on par with or better than MFCCs in both seen and unseen subject conditions. The results demonstrate the potential of SSL features for improving AAI in low-resource settings with dysarthric speech.

## Method Summary
The method involves extracting SSL features from speech waveforms using pre-trained models like wav2vec, APC, and DeCoAR, along with x-vectors for speaker information. These features are concatenated and fed into a 3-layer BLSTM network with 256 units per layer to predict 24-dimensional articulatory trajectories (positions, velocities, accelerations of 6 articulators). The model is trained using MSE loss and Adam optimizer, evaluated in seen (subject-specific, pooled, fine-tuned) and unseen subject conditions using Pearson Correlation Coefficient between ground-truth and predicted articulatory trajectories.

## Key Results
- DeCoAR in fine-tuned scheme achieves ~1.81% and ~4.56% relative improvement in CC over MFCCs for healthy controls and dysarthric patients respectively
- SSL features (particularly wav2vec and DeCoAR) perform well or on par with MFCCs in both seen and unseen conditions
- SSL features show better generalization in low-resource settings with dysarthric speech

## Why This Works (Mechanism)

### Mechanism 1
Pre-trained SSL features capture richer acoustic-articulatory mappings than MFCCs alone for dysarthric speech. SSL models trained on large speech corpora learn general acoustic representations that include speaking styles, lexical patterns, and speaker identity. These features, when conditioned with x-vectors, provide more robust input for BLSTM articulatory prediction. Core assumption: SSL features learned from healthy speech can generalize to dysarthric speech patterns despite articulatory imprecision.

### Mechanism 2
Fine-tuning SSL features on dysarthric data yields better articulatory predictions than training from scratch. The pre-trained SSL model weights provide a strong initialization that captures general speech representations. Fine-tuning adapts these representations to the specific articulatory patterns of dysarthric speakers. Core assumption: The pre-trained SSL features contain transferable information relevant to dysarthric articulatory movements.

### Mechanism 3
X-vectors provide speaker-specific information that enhances SSL feature effectiveness for articulatory inversion. X-vectors encode speaker identity and vocal characteristics. When concatenated with SSL features, they help the model distinguish between speakers and adapt articulatory predictions accordingly. Core assumption: Speaker identity and vocal characteristics are important cues for predicting articulatory trajectories.

## Foundational Learning

- Concept: Acoustic-to-Articulatory Inversion (AAI)
  - Why needed here: AAI is the core task being addressed, converting speech acoustics to articulatory movements.
  - Quick check question: What is the primary challenge in AAI for dysarthric speech?

- Concept: Self-Supervised Learning (SSL) in speech processing
  - Why needed here: SSL models provide pre-trained features that capture rich acoustic representations without requiring labeled articulatory data.
  - Quick check question: How do SSL models like wav2vec and DeCoAR differ in their training objectives?

- Concept: Bidirectional LSTM (BLSTM) networks for sequence prediction
  - Why needed here: BLSTM is used to model the temporal dependencies in acoustic features and predict articulatory trajectories.
  - Quick check question: Why is a BLSTM chosen over a unidirectional LSTM for this task?

## Architecture Onboarding

- Component map: Speech waveforms -> SSL feature extraction -> x-vector extraction -> Concatenation -> 3-layer BLSTM (256 units) -> 24-dimensional articulatory trajectories
- Critical path: Speech → SSL features + x-vectors → BLSTM → Articulatory prediction
- Design tradeoffs:
  - Using pre-trained SSL vs training from scratch: Pre-trained SSL offers better generalization but requires careful fine-tuning
  - Number of BLSTM layers and units: More layers/units increase model capacity but also risk overfitting on small dysarthric datasets
  - Inclusion of x-vectors: Improves speaker-specific modeling but adds complexity and potential misalignment issues
- Failure signatures:
  - Poor performance on dysarthric speakers but good on healthy controls: SSL features may not generalize well to dysarthric speech patterns
  - Overfitting on training data: Model capacity too high for available data; consider reducing BLSTM units or adding regularization
  - No improvement over MFCCs: SSL features may not capture relevant articulatory information; try different SSL models or fine-tuning strategies
- First 3 experiments:
  1. Compare MFCCs + x-vectors vs. SSL features + x-vectors on a small subset of dysarthric data to verify SSL effectiveness
  2. Test different SSL models (wav2vec, APC, DeCoAR) on the same data to identify which captures articulatory information best
  3. Evaluate the impact of fine-tuning vs. freezing SSL feature extraction on dysarthric articulatory prediction performance

## Open Questions the Paper Calls Out
- Future work will involve understanding the effects of SSL networks on a language-mismatched dysarthric corpus

## Limitations
- Small sample size (4 speakers total) limits generalizability to broader dysarthric populations
- Potential domain mismatch between SSL pre-training data (healthy speech) and dysarthric speech patterns
- Lack of extensive ablation studies on SSL model choices and fine-tuning strategies

## Confidence
- High confidence: SSL features (wav2vec, DeCoAR) provide meaningful improvements over MFCCs for AAI in both seen and unseen conditions
- Medium confidence: Fine-tuning SSL features yields better results than training from scratch for dysarthric articulatory prediction
- Medium confidence: x-vector conditioning improves speaker-specific articulatory predictions when combined with SSL features

## Next Checks
1. Validate results on larger dysarthric speech corpora (e.g., Universal Access) to confirm SSL feature effectiveness scales with more speakers and diverse dysarthria types
2. Conduct ablation studies comparing different SSL model architectures and fine-tuning strategies to identify optimal configurations for dysarthric AAI
3. Test the approach on incremental speech data to evaluate real-time performance and robustness to streaming articulatory prediction scenarios