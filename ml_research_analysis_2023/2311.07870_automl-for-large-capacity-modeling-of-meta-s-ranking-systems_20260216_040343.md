---
ver: rpa2
title: AutoML for Large Capacity Modeling of Meta's Ranking Systems
arxiv_id: '2311.07870'
source_url: https://arxiv.org/abs/2311.07870
tags:
- gain
- data
- predicted
- ranking
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of efficiently scaling up large
  capacity ranking models at Meta for Click Through Rate (CTR) and Conversion Rate
  (CVR) prediction tasks. The authors propose a sampling-based AutoML method that
  combines a lightweight predictor-based searcher with reinforcement learning (RL)
  to jointly optimize neural architecture search and hyperparameter tuning.
---

# AutoML for Large Capacity Modeling of Meta's Ranking Systems

## Quick Facts
- arXiv ID: 2311.07870
- Source URL: https://arxiv.org/abs/2311.07870
- Reference count: 39
- Primary result: Achieves 0.09% NE loss reduction or 25% QPS increase by sampling only ~100 models

## Executive Summary
This paper presents an AutoML methodology for scaling large capacity ranking models at Meta, addressing the challenge of efficiently exploring vast search spaces for CTR and CVR prediction tasks. The approach combines a lightweight predictor-based searcher with reinforcement learning to jointly optimize neural architecture search and hyperparameter tuning. By leveraging pairwise ranking loss and ensemble modeling, the method achieves significant Return on Investment compared to human-tuned baselines while dramatically reducing the number of model evaluations needed. The discovered architectures were validated through large-scale online A/B tests, demonstrating statistically significant improvements in ranking performance.

## Method Summary
The proposed method uses a two-step approach: first, a lightweight predictor (multi-task MLP) is trained to estimate NE gain and FLOPs from model configurations using pairwise ranking loss and ensemble modeling; second, a reinforcement learning agent samples model architectures based on the predictor's outputs as rewards, optimizing for NE gain and cost tradeoffs. The method employs low-fidelity evaluation with training curve fitting to preserve ranking quality while reducing compute cost, and uses pairwise ranking loss to increase effective label count from O(N) to O(N²), focusing on relative ordering rather than absolute values.

## Key Results
- Achieved 0.09% Normalized Entropy loss reduction or 25% Queries Per Second increase
- Sampled only around 100 models to achieve significant ROI improvements
- Validated in large-scale online A/B tests showing statistically significant gains
- Demonstrated accelerated AutoML adoption in Meta's ranking systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The predictor-based RL searcher enables efficient exploration of vast search spaces by predicting NE gain and FLOPs without full model training.
- Mechanism: A lightweight MLP predictor maps model configurations to performance proxies (NE gain, FLOPs). RL uses these predictions as rewards to guide sampling, reducing the need for expensive full evaluations.
- Core assumption: Predictor generalization is sufficient despite limited training data.
- Evidence anchors:
  - [abstract] "It leverages a lightweight predictor-based searcher and reinforcement learning to explore vast search spaces, significantly reducing the number of model evaluations."
  - [section 3.1] "The basic idea of the predictor searcher is to build an efficient surrogate model (e.g., MLP), called a 'predictor', to map any model configuration to the metrics we are interested in."
  - [corpus] Weak - no direct corpus match for this specific predictor-RL integration claim.
- Break condition: Predictor predictions become uncorrelated with true model performance due to overfitting or distribution shift.

### Mechanism 2
- Claim: Low-fidelity evaluation with training curve fitting preserves ranking quality while reducing compute cost.
- Mechanism: Initial full training on pilot models establishes correlation between early and full learning curves. Subsequent models use reduced data but maintain relative ranking via curve extrapolation.
- Core assumption: Early learning curves are highly correlated with full convergence for ranking purposes.
- Evidence anchors:
  - [abstract] "It leverages a lightweight predictor-based searcher and reinforcement learning to explore vast search spaces, significantly reducing the number of model evaluations."
  - [section 3.1] "We then conduct a ranking correlation analysis to determine how much training data can be reduced (i.e., how early training can be stopped) while still preserving these pilot models' relative ranking order."
  - [corpus] Weak - corpus doesn't specifically address low-fidelity evaluation methodology.
- Break condition: Learning curves diverge significantly between early and late training stages for certain architectures.

### Mechanism 3
- Claim: Pairwise ranking loss and ensemble modeling improve predictor generalization under limited data.
- Mechanism: Pairwise ranking loss increases effective label count from O(N) to O(N²), focusing on relative ordering. Ensemble modeling averages multiple predictors to reduce variance.
- Core assumption: Relative ordering is more important than absolute value accuracy for model selection.
- Evidence anchors:
  - [section 3.1] "we use pairwise ranking loss to train the predictor. This increases the number of labels to O(N²), and is proven more accurate than mean squared error loss."
  - [section 4.2.2] "we found that using a pairwise ranking loss is more effective in identifying top models compared to losses such as MSE."
  - [section 4.2.3] "we employ ensemble modeling and multi-task learning to improve predictor's generalization capability."
- Break condition: Ensemble averaging fails to reduce variance when individual predictors are highly correlated in their errors.

## Foundational Learning

- Concept: Reinforcement Learning for Architecture Search
  - Why needed here: Enables efficient exploration of high-dimensional search spaces with reward signals based on predicted performance
  - Quick check question: What is the key difference between REINFORCE and gradient-based NAS methods in terms of search strategy?

- Concept: Low-fidelity Evaluation and Learning Curve Analysis
  - Why needed here: Reduces computational cost by stopping training early while maintaining ranking correlation for model selection
  - Quick check question: How does Kendall Tau correlation help determine the optimal early stopping point?

- Concept: Multi-task Learning for Joint Prediction
  - Why needed here: Simultaneously predicts multiple performance metrics (NE gain, FLOPs) sharing representations improves generalization
  - Quick check question: Why might joint prediction of NE gain and FLOPs be more efficient than separate predictors?

## Architecture Onboarding

- Component map: Predictor (MLP with pairwise ranking loss + ensemble) → RL Agent (REINFORCE) → Model Sampler → Performance Predictor → Reward Signal
- Critical path: Model configuration → Predictor → RL reward → Sampling distribution update → New model evaluation
- Design tradeoffs: Predictor accuracy vs. sampling efficiency; ensemble size vs. compute cost; reward weighting (NE vs. FLOPs) vs. search direction
- Failure signatures: Predictor overfitting (training correlation high, validation low); RL collapse to narrow regions; NE-FLOPs correlation breakdown
- First 3 experiments:
  1. Verify predictor ranking correlation on held-out validation set with varying ensemble sizes
  2. Test low-fidelity evaluation ranking preservation across different early stopping points
  3. Compare RL sampling efficiency against random search on a small search space

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of AutoML-discovered models degrade when the search space is expanded to include even more complex architectures or additional hyperparameters?
- Basis in paper: [explicit] The paper discusses the efficiency of the proposed AutoML method with a specific search space for DHEN models but does not explore the limits of search space expansion.
- Why unresolved: The paper focuses on a curated search space and demonstrates success within that scope. It does not provide evidence on how the method scales with significantly larger or more complex search spaces.
- What evidence would resolve it: Experiments comparing the performance and efficiency of AutoML across varying sizes and complexities of search spaces would provide insights into the scalability and robustness of the method.

### Open Question 2
- Question: What is the impact of using different types of ranking loss functions on the predictor's ability to identify top-performing models in various ranking tasks?
- Basis in paper: [explicit] The paper mentions the use of pairwise ranking loss for training the predictor and compares it with mean squared error loss, showing better performance with pairwise ranking loss.
- Why unresolved: The paper does not explore other types of ranking loss functions or their impact on the predictor's performance across different ranking tasks.
- What evidence would resolve it: A comparative study of different ranking loss functions and their effects on the predictor's accuracy in identifying top models across various ranking tasks would clarify the optimal loss function for different scenarios.

### Open Question 3
- Question: How does the proposed AutoML method perform when applied to ranking systems outside of Meta's ecosystem, such as in academic or industry settings with different data characteristics and constraints?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the AutoML method within Meta's ranking systems but does not discuss its applicability or performance in other contexts.
- Why unresolved: The method's success is shown in a specific setting with particular data and infrastructure, leaving questions about its generalizability and adaptability to other environments.
- What evidence would resolve it: Applying the AutoML method to ranking systems in different academic or industry settings and comparing its performance with Meta's results would provide evidence of its generalizability and adaptability.

## Limitations

- Predictor generalization concerns with limited training data (only 2000 examples)
- RL sampling strategy robustness to reward function tuning not thoroughly explored
- Pairwise ranking loss contribution versus ensemble modeling not rigorously ablated

## Confidence

- **High Confidence**: The core workflow combining predictor-based RL with low-fidelity evaluation is technically sound and well-grounded in existing AutoML literature.
- **Medium Confidence**: The claimed ROI improvements (0.09% NE reduction, 25% QPS increase) are supported by experimental results, but the sample size (100 models) and specific evaluation methodology could benefit from additional validation.
- **Medium Confidence**: The pairwise ranking loss and ensemble modeling approach shows promise, but the comparative advantage over simpler alternatives needs more rigorous quantification.

## Next Checks

1. **Predictor Robustness Test**: Evaluate the predictor's performance on an entirely disjoint model family not represented in the training data to assess generalization beyond interpolation.
2. **Ablation Study**: Systematically remove pairwise ranking loss and ensemble modeling components to quantify their individual contributions to predictor accuracy and RL sampling efficiency.
3. **Scaling Analysis**: Test the methodology with varying training data sizes (500, 1000, 2000, 5000 examples) to identify the minimum viable dataset size for maintaining performance gains.