---
ver: rpa2
title: Leveraging Multi-lingual Positive Instances in Contrastive Learning to Improve
  Sentence Embedding
arxiv_id: '2309.08929'
source_url: https://arxiv.org/abs/2309.08929
tags:
- multiple
- sentence
- positives
- languages
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MPCL (Multi-lingual Positives in Contrastive
  Learning) to improve multilingual sentence embedding learning by leveraging multiple
  positive examples instead of a single positive. The key idea is that in multilingual
  settings, translations of an anchor sentence can serve as multiple positives that
  share transitive similarity, providing richer cross-lingual information for learning.
---

# Leveraging Multi-lingual Positive Instances in Contrastive Learning to Improve Sentence Embedding

## Quick Facts
- arXiv ID: 2309.08929
- Source URL: https://arxiv.org/abs/2309.08929
- Reference count: 23
- Key outcome: MPCL improves multilingual sentence embedding learning by leveraging multiple positive translations instead of single positives, achieving up to 4.2 BUCC accuracy improvement and better cross-lingual transfer to unseen languages.

## Executive Summary
This paper proposes MPCL (Multi-lingual Positives in Contrastive Learning) to enhance multilingual sentence embedding learning by using multiple positive translations instead of a single positive instance. The approach leverages the transitive similarity among translations in different languages to capture richer cross-lingual information during training. MPCL constructs training data by grouping multilingual translations as positives and employs a multi-positive loss function to simultaneously learn contrastive correlations and structural information. Extensive experiments on various backbone models (LaBSE, mSimCSE, mBERT, XLM-RoBERTa) and downstream tasks demonstrate consistent improvements over conventional single-positive contrastive learning.

## Method Summary
MPCL constructs training data by grouping multilingual translations of an anchor sentence into positive sets, then applies a multi-positive loss function that captures normalized group-wise similarity distributions. The method calculates pairwise similarities among all positives in a group, normalizes these scores to maintain stable gradients, and optimizes the model to preserve both contrastive correlations and structural information across the positive set. Training uses standard PLMs with batch size 128, learning rate 1e-5, temperature 0.05, and max length 64, evaluated on BUCC, STS17, STS22, and MTOP tasks.

## Key Results
- MPCL achieves up to 4.2 improvement on BUCC retrieval accuracy
- Shows 2.8 improvement on STS17 semantic similarity task
- Demonstrates 1.5 improvement on MTOP classification task
- Exhibits better cross-lingual transfer performance on unseen languages compared to single-positive training

## Why This Works (Mechanism)

### Mechanism 1
Leveraging multiple positive translations provides richer cross-lingual information than single-positive training by capturing transitive similarity relationships among all translations in the positive set. This allows the model to learn from the full set of pairwise similarities rather than just anchor-positive pairs.

### Mechanism 2
Multiple positives improve structural information learning through group-wise similarity distributions. By calculating normalized group-wise similarity distributions, the model learns to optimize across all pairwise relationships within the positive set simultaneously, providing more stable gradient signals.

### Mechanism 3
Multiple positives enhance cross-lingual transfer to unseen languages by exposing the model to broader linguistic patterns and similarity structures during training, improving its ability to generalize to languages not seen during training.

## Foundational Learning

- Concept: Contrastive Learning Objective
  - Why needed here: Understanding the standard contrastive loss formulation is essential to grasp how MPCL modifies it with multiple positives.
  - Quick check question: What is the purpose of the temperature parameter τ in the contrastive loss formula?

- Concept: Cross-Lingual Semantic Similarity
  - Why needed here: The paper relies on the assumption that translations maintain semantic similarity across languages, which is fundamental to the MPCL approach.
  - Quick check question: How does the paper ensure that translations used as positives maintain consistent semantic meaning?

- Concept: Cross-Lingual Transfer Learning
  - Why needed here: Understanding how models transfer knowledge from seen to unseen languages is crucial for interpreting the transfer performance results.
  - Quick check question: What factors influence a model's ability to transfer knowledge to unseen languages according to the paper?

## Architecture Onboarding

- Component map: Data preprocessing -> Model training -> Loss computation -> Evaluation
- Critical path: Translation dataset → Group construction → Multiple positive loss computation → Model updates → Evaluation on downstream tasks
- Design tradeoffs: Number of positives vs. computational cost; Language diversity vs. translation consistency; Model capacity vs. cross-lingual relationship capture
- Failure signatures: Degraded performance on seen languages; Unstable training dynamics; Poor cross-lingual transfer despite multiple positives; Memory issues with large positive sets
- First 3 experiments: 1) Compare single-positive vs. multiple-positive training on small language pairs; 2) Test different numbers of positives (2-6) to find optimal balance; 3) Evaluate cross-lingual transfer to completely unseen language

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number and selection of languages to include in the training dataset for MPCL to maximize performance across all downstream tasks? The paper explores limited language combinations without comprehensive analysis of all possible selections and their impact.

### Open Question 2
How does MPCL perform when trained on high-quality human-translated data compared to machine-translated data? The paper uses machine-translated XNLI dataset without comparing to human-translated alternatives.

### Open Question 3
How does MPCL compare to other state-of-the-art multilingual sentence embedding models when trained on the same language coverage and with the same training data? The paper provides limited comparison under identical training conditions.

## Limitations
- Evaluation on unseen languages is limited to only one language (Italian), making generalization of transfer learning benefits difficult
- Limited analysis of when multiple positives might fail or optimal number selection for different scenarios
- Computational overhead of multiple positives isn't fully quantified beyond noting increased GPU memory requirements

## Confidence

- **High Confidence**: Experimental improvements on seen languages (BUCC, STS17, STS22, MTOP) are well-documented with clear baselines and consistent results across multiple backbone models.
- **Medium Confidence**: Cross-lingual transfer claims are moderately supported but limited by narrow evaluation scope (only one unseen language).
- **Low Confidence**: Limited analysis of failure conditions, optimal positive number selection, and computational trade-offs beyond basic memory considerations.

## Next Checks

1. Conduct systematic experiments on multiple unseen languages (3-5 additional languages not in the training set) across all downstream tasks to verify the generality of cross-lingual transfer improvements.

2. Systematically vary the number of positives (2, 3, 4, 5, 6) and measure the trade-off between performance gains and computational costs to identify optimal selection.

3. Design experiments to identify conditions where multiple positives fail or harm performance, including poor-quality translations and highly diverse language pairs.