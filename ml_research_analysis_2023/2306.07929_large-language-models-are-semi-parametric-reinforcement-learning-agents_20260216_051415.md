---
ver: rpa2
title: Large Language Models Are Semi-Parametric Reinforcement Learning Agents
arxiv_id: '2306.07929'
source_url: https://arxiv.org/abs/2306.07929
tags: []
core_contribution: This paper proposes REMEMBERER, a novel evolvable LLM-based agent
  framework that equips LLMs with a long-term experience memory to learn from past
  episodes. The key idea is to introduce Reinforcement Learning with Experience Memory
  (RLEM) to update the memory, allowing the system to learn from both successes and
  failures without fine-tuning the LLM parameters.
---

# Large Language Models Are Semi-Parametric Reinforcement Learning Agents

## Quick Facts
- arXiv ID: 2306.07929
- Source URL: https://arxiv.org/abs/2306.07929
- Authors: 
- Reference count: 27
- One-line primary result: REMEMBERER achieves average improvements of 2 points on WebShop and 4 points on WikiHow success rates compared to prior state-of-the-art models

## Executive Summary
This paper introduces REMEMBERER, a novel LLM-based agent framework that augments LLMs with a long-term experience memory to learn from past episodes without fine-tuning parameters. The key innovation is Reinforcement Learning with Experience Memory (RLEM), which updates the memory using off-policy learning, enabling the system to learn from both successes and failures. By storing task information, observations, actions, and Q-value estimations, REMEMBERER can retrieve relevant experiences for in-context learning, improving decision-making in sequential tasks.

## Method Summary
REMEMBERER implements a semi-parametric RL agent that combines LLM decision-making with external experience memory. The agent stores interaction experiences (state, action, reward, next state) in memory and updates Q-values using RLEM with Q-learning principles. During decision-making, it retrieves relevant experiences based on similarity matching between current observations and stored experiences. These retrieved experiences serve as exemplars for 2-shot in-context learning, providing the LLM with concrete examples of good and bad decisions. The system is evaluated on WebShop (simulated web store navigation) and WikiHow (navigation tasks) datasets using GPT-3.5 via OpenAI API.

## Key Results
- REMEMBERER achieves average improvements of 2 points on success rates compared to prior state-of-the-art on WebShop task set
- REMEMBERER achieves average improvements of 4 points on success rates compared to prior state-of-the-art on WikiHow task set
- The system demonstrates effective cross-goal experience transfer, leveraging past experiences even for different task goals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RLEM allows the agent to learn from both successes and failures without modifying LLM parameters
- Mechanism: Experience memory stores transitions and updates Q-values via off-policy learning, enabling improvement through experience replay rather than parameter updates
- Core assumption: The external memory can capture sufficient interaction experiences to provide meaningful guidance
- Evidence anchors:
  - [abstract]: "We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM."
  - [section]: "Such an agent turns to be a semi-parametric system that can evolve through RL process."

### Mechanism 2
- Claim: Cross-goal experience memory enables transfer learning across different tasks
- Mechanism: By storing experiences indexed by task information and observations, the agent can retrieve relevant experiences for new task goals through similarity matching
- Core assumption: Similar observation patterns across different tasks can provide useful guidance regardless of the specific task goal
- Evidence anchors:
  - [abstract]: "By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals"

### Mechanism 3
- Claim: Few-shot in-context learning with retrieved experiences improves decision quality
- Mechanism: Retrieved experiences are formatted as exemplars showing both encouraged (high Q-value) and discouraged (low Q-value) actions, providing concrete examples for the LLM
- Core assumption: LLMs can effectively learn from in-context exemplars that demonstrate decision patterns
- Evidence anchors:
  - [section]: "The exemplar is supposed to demonstrate the format of the input and the output to the LLM... This is motivated by the perspective that 'reasoning is remembering' to exploit both successful and failed experiences."

## Foundational Learning

- Concept: Q-learning and Bellman optimality equation
  - Why needed here: The experience memory updates Q-values using Q-learning principles to estimate the expected future rewards of actions
  - Quick check question: What is the difference between on-policy and off-policy learning in reinforcement learning?

- Concept: Similarity matching and retrieval systems
  - Why needed here: The agent retrieves relevant experiences from memory based on similarity between current state and stored experiences
  - Quick check question: How would you design a similarity function that balances task similarity and observation similarity?

- Concept: In-context learning and few-shot prompting
  - Why needed here: The LLM uses retrieved experiences as exemplars to guide its decision-making through in-context learning
  - Quick check question: What factors influence the effectiveness of in-context learning with exemplars?

## Architecture Onboarding

- Component map: Observation → Similarity retrieval → Exemplar generation → LLM decision → Action execution → Reward feedback → Memory update

- Critical path: Observation → Similarity retrieval → Exemplar generation → LLM decision → Action execution → Reward feedback → Memory update

- Design tradeoffs:
  - Memory size vs. retrieval speed: Larger memory provides more experiences but slows retrieval
  - Similarity function complexity vs. effectiveness: More sophisticated similarity matching may improve retrieval quality but increase computational cost
  - Flattening policy depth vs. accuracy: Longer flattening improves Q-value estimation but requires more interaction steps

- Failure signatures:
  - Agent consistently makes poor decisions: Likely issue with similarity function or exemplar formatting
  - Agent shows no improvement over time: Experience memory may not be updating correctly or similarity retrieval is failing
  - Agent is slow to respond: Memory retrieval or LLM processing may be inefficient

- First 3 experiments:
  1. Test similarity function: Feed known similar and dissimilar state pairs to verify the similarity function returns appropriate scores
  2. Test exemplar generation: Manually verify that encouraged/discouraged actions are correctly identified from stored experiences
  3. Test memory update: Simulate a few interaction steps and verify Q-values update according to the Q-learning formula

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of REMEMBERER scale with increasing experience memory size?
- Basis in paper: [inferred] The paper demonstrates REMEMBERER's effectiveness but does not explore the impact of varying memory sizes on performance
- Why unresolved: The experiments use a fixed memory size, leaving the relationship between memory capacity and performance unexplored
- What evidence would resolve it: Systematic experiments varying the number of stored experiences and measuring corresponding performance changes would clarify this relationship

### Open Question 2
- Question: Can REMEMBERER effectively transfer knowledge between tasks with different observation spaces?
- Basis in paper: [explicit] The paper mentions REMEMBERER can leverage experiences from different task goals, but doesn't test across significantly different observation spaces
- Why unresolved: All experiments use tasks with similar observation formats (web pages or UI elements), so transfer between fundamentally different observation types remains untested
- What evidence would resolve it: Experiments applying REMEMBERER trained on WebShop tasks to completely different domains (e.g., robotics or natural language tasks) would demonstrate transfer capabilities

### Open Question 3
- Question: What is the optimal similarity function design for balancing task and observation similarity?
- Basis in paper: [explicit] The paper uses a weighted combination of task and observation similarity but doesn't systematically explore the optimal weight parameter λ
- Why unresolved: The paper uses λ=0.5 without justification or exploration of other values, leaving the optimal balance between task and observation similarity unclear
- What evidence would resolve it: Systematic ablation studies varying λ and measuring performance would identify the optimal balance for different task domains

## Limitations

- The system's performance heavily depends on effective similarity functions for retrieving relevant experiences, but the paper lacks detailed specifications of how these functions are computed
- The memory update mechanism relies on Q-learning with a flattening policy, but the paper doesn't analyze how flattening depth affects performance or computational efficiency
- The approach assumes cross-goal transfer is beneficial, but this may not hold for highly dissimilar tasks where retrieved experiences could mislead the agent

## Confidence

**High Confidence Claims:**
- The semi-parametric RL framework with external memory is technically feasible and aligns with established RL principles
- The concept of using experience memory for in-context learning is supported by existing literature on LLM few-shot capabilities

**Medium Confidence Claims:**
- Cross-goal transfer learning effectiveness (the paper shows improvements but doesn't deeply analyze when and why transfer works)
- The specific performance improvements on WebShop and WikiHow (implementation details like similarity functions are underspecified)

**Low Confidence Claims:**
- Generalizability to other task domains beyond web navigation and WikiHow
- Scalability of the approach to more complex environments with larger state spaces

## Next Checks

1. **Similarity Function Validation**: Test the retrieval system with controlled experiments using known similar and dissimilar state pairs to verify the similarity function returns appropriate scores and retrieves relevant experiences

2. **Cross-Goal Transfer Analysis**: Systematically evaluate the agent's performance on both similar and dissimilar tasks to quantify when cross-goal transfer helps versus hurts performance, identifying the boundaries of effective transfer

3. **Memory Update Sensitivity**: Vary the flattening policy depth and measure its impact on both performance and computational efficiency to determine optimal configuration parameters for different task types