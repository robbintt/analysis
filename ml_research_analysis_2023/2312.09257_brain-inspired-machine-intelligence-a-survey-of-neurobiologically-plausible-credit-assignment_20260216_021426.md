---
ver: rpa2
title: 'Brain-Inspired Machine Intelligence: A Survey of Neurobiologically-Plausible
  Credit Assignment'
arxiv_id: '2312.09257'
source_url: https://arxiv.org/abs/2312.09257
tags:
- neural
- learning
- networks
- credit
- assignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper provides a survey of biologically-inspired credit assignment
  algorithms for artificial neural networks. It proposes a taxonomy based on the source
  and production of driving signals for synaptic plasticity, organizing approaches
  into six families: implicit, explicit global, non-synergistic explicit local, and
  synergistic explicit local (discrepancy reduction, energy-based, and forward-only).'
---

# Brain-Inspired Machine Intelligence: A Survey of Neurobiologically-Plausible Credit Assignment

## Quick Facts
- **arXiv ID:** 2312.09257
- **Source URL:** https://arxiv.org/abs/2312.09257
- **Reference count:** 40
- **Primary result:** Survey organizing biologically-inspired credit assignment algorithms into six families based on signal locality and production, with forward-only approaches showing promise in resolving backpropagation's biological implausibilities

## Executive Summary
This survey paper provides a comprehensive taxonomy of biologically-plausible credit assignment algorithms for artificial neural networks, organizing approaches into six families based on their driving signal characteristics. The paper examines how these families address fundamental limitations of backpropagation, including the global feedback pathway problem, weight transport problem, and inference-learning dependency issues. Forward-only algorithms, particularly recurrent forward-forward and predictive forward-forward learning, are highlighted as promising approaches that offer architecture agnosticism and suitability for temporal data while resolving multiple biological implausibilities simultaneously.

## Method Summary
This paper conducts a systematic survey and synthesis of existing literature on biologically-inspired credit assignment algorithms, organizing 40+ references into a novel taxonomy based on how algorithms produce driving signals for synaptic plasticity. The classification scheme distinguishes between implicit, explicit global, non-synergistic explicit local, and three types of synergistic explicit local algorithms (discrepancy reduction, energy-based, and forward-only). The survey examines each family's approach to addressing backpropagation's biological implausibilities and practical limitations, without conducting original experiments.

## Key Results
- Forward-only algorithms (particularly predictive forward-forward learning) show promise in resolving multiple backpropagation limitations simultaneously
- Architecture agnosticism emerges as a key property of forward-only schemes, enabling application to arbitrary neural architectures
- Temporal/sequential data processing is naturally supported by forward-only approaches due to their inherent handling of temporal dependencies
- Significant challenges remain in scaling biologically-plausible algorithms to large datasets and integrating them into reinforcement learning and control tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The taxonomy resolves backprop's global feedback pathway problem by categorizing credit assignment into local and synergistic schemes.
- **Mechanism:** By organizing algorithms based on signal locality (implicit, explicit global, non-synergistic local, synergistic local), the taxonomy highlights approaches that bypass the long global feedback pathway inherent to backprop.
- **Core assumption:** Local and synergistic local credit assignment schemes inherently avoid the long global feedback pathway problem by design.
- **Break condition:** If local or synergistic local schemes require long chains of operations or message passing that mimic the global feedback pathway.

### Mechanism 2
- **Claim:** The taxonomy addresses the weight transport problem by including schemes that use random or fixed feedback weights.
- **Mechanism:** By including families like explicit global and non-synergistic local, which encompass feedback alignment and direct feedback alignment, the taxonomy highlights approaches that use random or fixed feedback weights, resolving the weight transport problem.
- **Core assumption:** Random or fixed feedback weights can effectively transmit error signals without requiring symmetric connections.
- **Break condition:** If random or fixed feedback weights fail to provide sufficient learning signals or lead to instability.

### Mechanism 3
- **Claim:** The taxonomy enables addressing the inference-learning dependency problem by including forward-only schemes.
- **Mechanism:** By including forward-only schemes in the taxonomy, the paper highlights approaches that use the same neural machinery for both inference and learning, resolving the inference-learning dependency problem.
- **Core assumption:** Forward-only schemes can effectively learn without requiring separate backward passes.
- **Break condition:** If forward-only schemes require additional computations or structures that reintroduce dependency between inference and learning.

## Foundational Learning

- **Concept: Backpropagation of Errors (Backprop)**
  - Why needed here: Understanding backprop is crucial for appreciating the limitations that motivate the development of biologically-plausible alternatives.
  - Quick check question: What are the key criticisms of backprop in terms of biological plausibility and practical considerations?

- **Concept: Hebbian Learning**
  - Why needed here: Hebbian learning is a fundamental principle of synaptic plasticity in the brain and serves as a basis for many biologically-inspired credit assignment schemes.
  - Quick check question: How does Hebbian learning differ from backprop in terms of locality and the types of signals used for synaptic updates?

- **Concept: Spiking Neural Networks (SNNs)**
  - Why needed here: SNNs offer a more biologically realistic model of neuronal dynamics and are increasingly relevant for energy-efficient neuromorphic hardware.
  - Quick check question: How do credit assignment schemes for SNNs differ from those for traditional artificial neural networks?

## Architecture Onboarding

- **Component map:** Neural System Context -> Taxonomy -> Algorithm Families -> Problems and Solutions

- **Critical path:**
  1. Understand the neural system context and the role of credit assignment
  2. Grasp the taxonomy and its organizing principles
  3. Explore the algorithm families and their characteristics
  4. Analyze the problems with backprop and how different families address them
  5. Identify promising directions for future research

- **Design tradeoffs:**
  - Biological plausibility vs. computational efficiency: More biologically plausible schemes may be slower or require more resources
  - Locality vs. global coordination: Local schemes may be more efficient but may struggle with global credit assignment
  - Architecture specificity vs. generality: Some schemes may be tailored to specific architectures while others are more general

- **Failure signatures:**
  - Slow convergence: Some biologically-plausible schemes may converge more slowly than backprop
  - Instability: Random feedback weights or local updates may lead to instability in some cases
  - Limited scalability: Some schemes may not scale well to large datasets or complex architectures

- **First 3 experiments:**
  1. Implement a simple Hebbian learning rule on a small dataset and compare its performance to backprop
  2. Test a local credit assignment scheme like target propagation on a medium-sized dataset and analyze its scalability
  3. Implement a forward-only scheme like predictive forward-forward learning and evaluate its performance on a temporal sequence modeling task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance gap between biologically-inspired credit assignment algorithms and backpropagation be effectively measured and minimized, especially in complex, high-dimensional tasks?
- Basis in paper: The paper discusses the performance gap between biologically-inspired algorithms and backpropagation, particularly in challenging domains like reinforcement learning and control.
- Why unresolved: While some equivalence results exist, direct empirical comparisons are limited, and the impact of architectural complexity on algorithm performance remains unclear.
- What evidence would resolve it: Comprehensive benchmarking studies comparing multiple algorithms across diverse task families, coupled with theoretical analysis of their computational properties and convergence guarantees.

### Open Question 2
- Question: What is the optimal degree of entanglement between neural architecture and credit assignment mechanisms for achieving both biological plausibility and computational efficiency?
- Basis in paper: The paper discusses the trade-off between architectural agnosticism and entanglement, suggesting that some entanglement might facilitate parallelism and biological realism.
- Why unresolved: The relationship between architectural design choices and learning dynamics is complex, and the impact of different levels of entanglement on generalization and robustness is not fully understood.
- What evidence would resolve it: Systematic studies exploring the effects of varying architectural constraints and feedback mechanisms on learning performance and biological plausibility, potentially using hybrid models combining multiple credit assignment schemes.

### Open Question 3
- Question: How can biologically-inspired credit assignment algorithms be effectively extended to handle relational and graph-structured data, beyond the current focus on sequential and vision tasks?
- Basis in paper: The paper notes that very few efforts have engaged with relational/graph problems, while highlighting the importance of graph learning for cognitive architectures and knowledge representation.
- Why unresolved: Existing algorithms are primarily designed for sequential or grid-like data structures, and adapting them to handle arbitrary graph topologies and relational reasoning presents significant challenges.
- What evidence would resolve it: Development and evaluation of novel biologically-inspired credit assignment schemes specifically tailored for graph neural networks, demonstrating their effectiveness on relational reasoning tasks and knowledge graph completion.

## Limitations
- Limited empirical validation of the claimed advantages of forward-only algorithms
- Insufficient evidence for scalability claims beyond small-scale problems
- Potential oversimplification of the relationship between biological plausibility and algorithmic performance

## Confidence
- **Taxonomy organization:** Medium confidence - well-grounded in surveyed literature but remains theoretical without systematic testing
- **Forward-only algorithm advantages:** Medium confidence - promising results in limited settings but lack comprehensive validation
- **Scalability claims:** Low confidence - largely theoretical with minimal empirical evidence
- **Architecture agnosticism claims:** Medium confidence - demonstrated in principle but not extensively validated across diverse architectures

## Next Checks
1. Implement forward-only algorithms (predictive forward-forward) on standard benchmark datasets (CIFAR-10, ImageNet) to verify scalability claims
2. Test weight transport problem solutions (feedback alignment variants) on deep networks to assess stability across different architectures
3. Compare inference-learning dependency resolution in forward-only vs. traditional backprop on sequential/temporal tasks