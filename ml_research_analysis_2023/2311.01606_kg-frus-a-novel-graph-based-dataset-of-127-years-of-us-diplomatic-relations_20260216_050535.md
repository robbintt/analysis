---
ver: rpa2
title: 'KG-FRUS: a Novel Graph-based Dataset of 127 Years of US Diplomatic Relations'
arxiv_id: '2311.01606'
source_url: https://arxiv.org/abs/2311.01606
tags:
- person
- graph
- relations
- documents
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KG-FRUS, a knowledge graph (KG) derived from
  over 300,000 US diplomatic documents spanning 127 years. The authors convert the
  original XML-based corpus into a graph structure that captures not only document
  metadata but also relationships between entities like people, places, and events.
---

# KG-FRUS: a Novel Graph-based Dataset of 127 Years of US Diplomatic Relations

## Quick Facts
- arXiv ID: 2311.01606
- Source URL: https://arxiv.org/abs/2311.01606
- Reference count: 26
- 812,000+ nodes and 9 million+ relations capturing 127 years of US diplomatic relations

## Executive Summary
This paper introduces KG-FRUS, a knowledge graph derived from over 300,000 US diplomatic documents spanning 127 years. The authors convert the original XML-based corpus into a graph structure that captures document metadata and relationships between entities like people, places, and events. By enriching the base graph with entities and relations from Wikidata and applying NLP techniques, they create a comprehensive dataset containing over 812,000 nodes and 9 million relations. The resulting KG-FRUS enables historical and political research through graph-based analysis, including timeline visualization, redaction analysis, dynamic entity embeddings, and importance scoring via PageRank.

## Method Summary
The method involves parsing 300,000+ XML diplomatic documents to extract entities and metadata, creating a base knowledge graph schema, expanding it using Wikidata entities, and applying NLP techniques for entity recognition and redaction extraction. The KG is validated through person unification (achieving 91% accuracy) and wikification (achieving 90% accuracy). Applications demonstrated include timeline visualization of document origins, redaction pattern analysis, dynamic entity embeddings using Node2Vec and FastRP, and importance scoring via PageRank algorithms.

## Key Results
- Successfully created KG-FRUS with 812,000+ nodes and 9 million+ relations from 127 years of diplomatic documents
- Achieved 91% accuracy in person unification across name variations using multi-step similarity algorithms
- Obtained 90% accuracy in wikification by linking FRUS entities to Wikidata entries
- Demonstrated applications including timeline visualization, redaction analysis, dynamic embeddings, and PageRank importance scoring

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Person unification across volumes reduces name variations to unique individuals
- Mechanism: Algorithmic merging based on exact matches, word permutations, Damerau-Levenshtein/Jaro similarity, and near-miss misspellings progressively consolidates name variants
- Core assumption: Conservative distance thresholds prevent false merges while capturing legitimate variants
- Evidence anchors:
  - [abstract]: "Validation shows successful person unification (91% accuracy)"
  - [section 3.2.1]: Detailed 4-step merging algorithm with distance thresholds
  - [corpus]: Weak - no corpus-level validation data provided for false positive rates
- Break condition: Overly aggressive similarity thresholds cause incorrect merges, especially for common names

### Mechanism 2
- Claim: Wikidata wikification anchors FRUS entities to structured knowledge for enrichment
- Mechanism: SPARQL queries link person names to Wikidata entries; sentence-BERT embeddings disambiguate multiple matches by comparing FRUS descriptions with Wikidata headers
- Core assumption: Wikidata entries for frequently mentioned historical figures are accurate and complete
- Evidence anchors:
  - [abstract]: "successful...wikification (90% accuracy)"
  - [section 3.2.1]: BERT-based disambiguation using description embeddings
  - [corpus]: Weak - no corpus data on wikification coverage across name frequencies
- Break condition: Rare or ambiguous names lack accurate Wikidata matches, leading to incorrect links

### Mechanism 3
- Claim: Dynamic entity embeddings capture temporal shifts in entity relationships
- Mechanism: Co-occurrence graphs built from document-entity edges over time windows are embedded using Node2Vec/FastRP, reflecting changing proximities between entities
- Core assumption: Co-occurrence frequency reliably proxies real-world entity relationships
- Evidence anchors:
  - [abstract]: "dynamic entity embeddings using Node2Vec and FastRP"
  - [section 4.3]: Methodology for building time-windowed co-occurrence graphs
  - [corpus]: Weak - no external validation of embedding quality
- Break condition: Co-occurrence reflects document structure rather than true entity relationships

## Foundational Learning

- Named Entity Recognition (NER)
  - Why needed here: Extracts entities beyond annotated persons/terms for KG enrichment
  - Quick check question: What types of entities does spaCy NER extract in this context?

- Graph Database Operations
  - Why needed here: Efficient storage and querying of KG relationships
  - Quick check question: How does Neo4j handle billions of relationships compared to relational DBs?

- Embedding Similarity Metrics
  - Why needed here: Disambiguate person names via BERT embeddings and compute entity proximities
  - Quick check question: What similarity measure is used to compare BERT embeddings?

## Architecture Onboarding

- Component map: XML parser → Base KG schema → Neo4j graph + SQL metadata → Wikidata enrichment → NLP enrichment → Applications
- Critical path: XML parsing → Base KG construction → Person unification → Wikidata linking
- Design tradeoffs: Separate SQL DB for document text improves Neo4j query performance but adds sync complexity
- Failure signatures: High false positive rate in person unification, low wikification accuracy, embedding quality issues
- First 3 experiments:
  1. Parse a sample XML volume and validate extracted entities against ground truth
  2. Run person unification on a small dataset and manually verify merges
  3. Test wikification accuracy on a subset of known persons

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal configuration for dynamic entity embeddings in terms of window size and embedding algorithm for capturing geopolitical shifts?
- Basis in paper: [inferred] The paper tests Node2Vec and FastRP on a 4-year window from 1949-1985, but does not explore different window sizes or compare multiple embedding algorithms
- Why unresolved: The paper only uses a single time window (4 years) and two embedding algorithms, without exploring how different configurations affect the quality of captured dynamics
- What evidence would resolve it: Comparative studies testing various time window sizes (e.g., 2, 4, 8 years) and embedding algorithms (e.g., GraphSAGE, TransE) on KG-FRUS, measuring their ability to capture known historical events

### Open Question 2
- Question: How can person unification accuracy be improved beyond 91% while minimizing false positives?
- Basis in paper: [explicit] The paper achieves 91.2% accuracy using conservative Damerau-Levenshtein and Jaro similarity thresholds, acknowledging this may miss some valid merges
- Why unresolved: The authors deliberately chose conservative thresholds to avoid false positives, leaving room for improvement in recall without sacrificing precision
- What evidence would resolve it: Development and validation of more sophisticated entity resolution methods that can increase recall while maintaining or improving precision, tested on FRUS data with ground truth

### Open Question 3
- Question: Can redaction patterns in diplomatic documents predict future policy decisions or intelligence operations?
- Basis in paper: [explicit] The paper identifies monetary redactions clustered around specific topics (e.g., Chile, Middle East) but does not explore predictive capabilities
- Why unresolved: The analysis is descriptive rather than predictive, showing correlation between redactions and topics but not testing if these patterns forecast future actions
- What evidence would resolve it: Longitudinal studies linking historical redaction patterns to subsequent declassified policy documents or intelligence activities, using statistical or machine learning models

## Limitations

- Person unification accuracy relies on conservative similarity thresholds that may miss legitimate variants, particularly for historical figures with multiple name forms
- Wikidata coverage limitations may introduce systematic biases for rare or ambiguous historical names
- Temporal embedding validity assumes co-occurrence reflects real-world relationships without external validation

## Confidence

- High confidence: Basic KG construction from XML parsing, Neo4j integration, and core graph metrics (812K nodes, 9M relations)
- Medium confidence: Person unification and wikification accuracy claims (lacking detailed evaluation methodology)
- Low confidence: Dynamic embedding quality and their ability to capture meaningful temporal shifts in diplomatic relations

## Next Checks

1. Manual evaluation of person unification: Select 100 person names spanning common and rare variants, manually verify unification results, and calculate precision/recall beyond the reported accuracy
2. Wikification coverage analysis: For a stratified sample of person names by frequency, measure the proportion with Wikidata matches and assess match quality for ambiguous cases
3. Embedding validation: Compare entity embeddings from different time periods against external diplomatic event timelines to verify whether proximity shifts align with historical context changes