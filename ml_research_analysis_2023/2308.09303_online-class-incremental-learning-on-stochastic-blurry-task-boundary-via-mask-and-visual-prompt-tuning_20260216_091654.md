---
ver: rpa2
title: Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask
  and Visual Prompt Tuning
arxiv_id: '2308.09303'
source_url: https://arxiv.org/abs/2308.09303
tags:
- learning
- task
- class
- classes
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenges of catastrophic forgetting
  and class imbalance in continual learning scenarios where task boundaries are blurred
  and dynamically changing. The authors propose a new Stochastic incremental Blurry
  (Si-Blurry) scenario to simulate real-world data streams and introduce Mask and
  Visual Prompt tuning (MVP) to tackle these issues.
---

# Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning

## Quick Facts
- arXiv ID: 2308.09303
- Source URL: https://arxiv.org/abs/2308.09303
- Reference count: 40
- Key outcome: MVP significantly outperforms existing state-of-the-art methods in the Si-Blurry scenario, achieving up to 84.42% accuracy with a memory buffer of 2000 samples

## Executive Summary
This paper addresses the challenges of catastrophic forgetting and class imbalance in continual learning scenarios where task boundaries are blurred and dynamically changing. The authors propose a new Stochastic incremental Blurry (Si-Blurry) scenario to simulate real-world data streams and introduce Mask and Visual Prompt tuning (MVP) to tackle these issues. MVP uses instance-wise logit masking and contrastive visual prompt tuning loss to prevent intra- and inter-task forgetting, along with gradient similarity-based focal loss and adaptive feature scaling to address class imbalance. Experiments on CIFAR-100, Tiny-ImageNet, and ImageNet-R show that MVP significantly outperforms existing state-of-the-art methods in the Si-Blurry scenario.

## Method Summary
The paper introduces the Stochastic incremental Blurry (Si-Blurry) scenario, which simulates real-world data streams with dynamically changing task boundaries. In this scenario, tasks are generated with stochastic blurry boundaries, where a certain percentage of samples belong to classes not currently being learned. The authors propose Mask and Visual Prompt tuning (MVP) to address the challenges of catastrophic forgetting and class imbalance in this setting. MVP consists of four main components: instance-wise logit masking, contrastive visual prompt tuning loss, gradient similarity-based focal loss, and adaptive feature scaling. These components work together to prevent intra- and inter-task forgetting and mitigate the effects of class imbalance during continual learning.

## Key Results
- MVP achieves up to 84.42% accuracy on CIFAR-100 with a memory buffer of 2000 samples
- MVP significantly outperforms existing state-of-the-art methods (ER, EWC++, RM, CLIB, L2P, DualPrompt) in the Si-Blurry scenario
- MVP demonstrates robust performance across CIFAR-100, Tiny-ImageNet, and ImageNet-R datasets with varying class imbalance ratios and disjoint class ratios

## Why This Works (Mechanism)
### Mechanism 1
- Claim: Instance-wise logit masking reduces intra-task forgetting by filtering out irrelevant classes during classification.
- Mechanism: A learnable mask is applied element-wise to the logit vector, suppressing logits from classes not present in the current batch. This makes the classification task easier and directs gradient updates toward relevant knowledge.
- Core assumption: The mask can effectively identify and suppress logits from classes that are not currently relevant without losing important information.
- Evidence anchors:
  - [abstract] "we propose a novel instance-wise logit masking and contrastive visual prompt tuning loss. Both of them help our model discern the classes to be learned in the current batch."
  - [section 4.2] "we introduce a learnable mask paired with prompts that helps the model to learn more intra-relevant and easier learning goals."
  - [corpus] No direct evidence; assumed from proposed method description

## Foundational Learning
### Concept 1: Continual Learning
- Why needed: Continual learning is essential for enabling models to learn from sequential data streams without forgetting previously acquired knowledge.
- Quick check: Does the model maintain performance on previous tasks while learning new tasks?

### Concept 2: Catastrophic Forgetting
- Why needed: Catastrophic forgetting is a major challenge in continual learning, where models tend to forget previously learned knowledge when adapting to new tasks.
- Quick check: Does the model experience significant performance degradation on previous tasks after learning new tasks?

### Concept 3: Class Imbalance
- Why needed: Class imbalance can lead to biased models that favor majority classes, resulting in poor performance on minority classes.
- Quick check: Does the model maintain balanced performance across all classes, regardless of their frequency in the data?

### Concept 4: Task Boundaries
- Why needed: Clear task boundaries are crucial for effective continual learning, as they allow models to adapt their learning strategies based on the current task.
- Quick check: Can the model accurately identify and adapt to task boundaries in the data stream?

## Architecture Onboarding
### Component Map
- Data Stream -> Si-Blurry Scenario -> MVP (Instance-wise Logit Masking, Contrastive Visual Prompt Tuning, Gradient Similarity-based Focal Loss, Adaptive Feature Scaling) -> Continual Learner

### Critical Path
1. Generate data stream with stochastic blurry task boundaries
2. Apply MVP components to the data stream
3. Update the continual learner's knowledge
4. Evaluate the learner's performance on previous and current tasks

### Design Tradeoffs
- Tradeoff between model complexity and computational efficiency: MVP introduces additional components (e.g., learnable masks, prompts) that increase model complexity but may improve performance.
- Tradeoff between memory usage and performance: The use of a memory buffer (e.g., 2000 samples) helps mitigate forgetting but increases memory requirements.

### Failure Signatures
- Poor performance on previous tasks: Indicates that the model is experiencing catastrophic forgetting.
- Biased performance across classes: Suggests that the model is struggling with class imbalance.
- Inability to adapt to task boundaries: Implies that the model is not effectively identifying and responding to task changes in the data stream.

### First Experiments
1. Evaluate MVP's performance on CIFAR-100 with varying levels of task blurriness and class imbalance ratios.
2. Compare MVP's performance to baseline methods (ER, EWC++, RM, CLIB, L2P, DualPrompt) on Tiny-ImageNet and ImageNet-R datasets.
3. Conduct ablation studies to quantify the individual contributions of each MVP component and their interactions.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the proposed Si-Blurry scenario perform with varying degrees of class imbalance ratios and disjoint class ratios beyond the tested configurations?
- Basis in paper: [inferred] The paper tests the MVP method with different disjoint class ratios and blurry sample ratios, but does not explore the full range of possible class imbalance scenarios.
- Why unresolved: The experiments conducted in the paper focus on specific configurations of class imbalance, leaving open the question of how the proposed method would perform in scenarios with more extreme or different levels of class imbalance.
- What evidence would resolve it: Conducting experiments with a wider range of class imbalance ratios and disjoint class ratios would provide insights into the robustness of the MVP method in various challenging scenarios.

### Open Question 2
- Question: Can the proposed MVP method be extended to handle more complex task configurations, such as tasks with overlapping classes and varying task durations?
- Basis in paper: [inferred] The paper focuses on the Si-Blurry scenario with stochastic task boundaries, but does not explore more complex task configurations.
- Why unresolved: The experiments conducted in the paper are limited to the Si-Blurry scenario, leaving open the question of how the proposed method would perform in more complex task configurations.
- What evidence would resolve it: Conducting experiments with tasks that have overlapping classes and varying task durations would provide insights into the generalizability of the MVP method to more complex scenarios.

### Open Question 3
- Question: How does the proposed MVP method compare to other state-of-the-art continual learning methods in terms of computational efficiency and memory usage?
- Basis in paper: [explicit] The paper mentions that the MVP method has a lower computational cost compared to DualPrompt, but does not provide a comprehensive comparison with other methods.
- Why unresolved: The paper focuses on the performance of the MVP method but does not extensively compare its computational efficiency and memory usage with other state-of-the-art methods.
- What evidence would resolve it: Conducting experiments to compare the computational efficiency and memory usage of the MVP method with other state-of-the-art methods would provide insights into its practicality and scalability.

## Limitations
- The paper does not fully specify the implementation details for the stochastic task generation in the Si-Blurry scenario, which may affect reproducibility.
- The exact implementation of the gradient similarity-based focal loss and adaptive feature scaling components is not clearly described.
- The evaluation is limited to specific datasets (CIFAR-100, Tiny-ImageNet, ImageNet-R) with relatively small class numbers, raising questions about scalability to larger, more complex datasets.

## Confidence
- **High confidence**: The general effectiveness of MVP in addressing catastrophic forgetting and class imbalance in the Si-Blurry scenario, as evidenced by significant performance improvements over baseline methods across multiple datasets.
- **Medium confidence**: The specific mechanisms by which instance-wise logit masking and contrastive visual prompt tuning contribute to performance gains, due to limited detailed analysis of their individual effects.
- **Low confidence**: The scalability and robustness of the approach to larger, more complex datasets and real-world scenarios with highly dynamic task boundaries.

## Next Checks
1. Conduct a thorough ablation study to quantify the individual contributions of each MVP component and their interactions under varying levels of task blurriness and class imbalance.
2. Evaluate the scalability of MVP to larger datasets with more classes (e.g., ImageNet-1K) and assess its performance under more extreme task boundary conditions.
3. Investigate the impact of hyperparameter choices (e.g., learning rates, mask and prompt dimensions) on MVP's performance and provide guidelines for optimal hyperparameter selection in different scenarios.