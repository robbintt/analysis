---
ver: rpa2
title: Improving Translation Faithfulness of Large Language Models via Augmenting
  Instructions
arxiv_id: '2308.12674'
source_url: https://arxiv.org/abs/2308.12674
tags:
- translation
- instruction
- swie
- language
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of instruction forgetting in large
  language models (LLMs) during machine translation, where the attention mechanism's
  local focus causes models to overlook instructions, leading to unfaithful translations.
  To solve this, the authors propose SWIE (Segment-Weighted Instruction Embedding),
  which enhances global attention to instructions by using parameterized adapters
  and segmented weights, and OVERMISS, a contrastive dataset targeting over-translation
  and miss-translation errors.
---

# Improving Translation Faithfulness of Large Language Models via Augmenting Instructions

## Quick Facts
- arXiv ID: 2308.12674
- Source URL: https://arxiv.org/abs/2308.12674
- Authors: 
- Reference count: 9
- Key outcome: SWIE and OVERMISS methods significantly improve BLEU scores and faithfulness metrics in LLM translation, with combined approach yielding up to 0.56 BLEU improvement from English to German.

## Executive Summary
This paper addresses instruction forgetting in large language models (LLMs) during machine translation, where the attention mechanism's local focus causes models to overlook instructions, leading to unfaithful translations. The authors propose SWIE (Segment-Weighted Instruction Embedding) to enhance global attention to instructions by using parameterized adapters and segmented weights, and OVERMISS, a contrastive dataset targeting over-translation and miss-translation errors. Experiments on BLOOM and LLaMA models show significant improvements: SWIE boosts BLEU scores from 0.19 to 0.51 on WMT22 test sets and 0.67 on long sentences; OVERMISS increases BLEU scores from 0.69 to 3.12 on LLaMA-7b. Combining both methods yields further gains, such as up to 0.56 BLEU improvement from English to German. Both methods also enhance faithfulness metrics based on word alignment.

## Method Summary
The paper proposes two complementary methods to improve translation faithfulness in LLMs: SWIE and OVERMISS. SWIE addresses instruction forgetting by adding a global instruction representation that is fused into input and response segments using parameterized adapters and segmented weights. OVERMISS improves faithfulness by constructing a contrastive dataset of over-translation and miss-translation errors generated by GPT-3.5-turbo. The methods are evaluated on BLOOM and LLaMA models using BLEU and COMET scores, as well as faithfulness metrics based on cross-lingual word alignment. A two-stage curriculum learning approach is used, first fine-tuning with Parrot-hint dataset then with OVERMISS.

## Key Results
- SWIE improves BLEU scores from 0.19 to 0.51 on WMT22 test sets and 0.67 on long sentences
- OVERMISS increases BLEU scores from 0.69 to 3.12 on LLaMA-7b
- Combined approach yields up to 0.56 BLEU improvement from English to German
- Both methods enhance faithfulness metrics based on word alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The local focus of the attention mechanism in LLMs causes instructions to be forgotten during decoding, leading to unfaithful translations.
- Mechanism: SWIE addresses this by adding a global instruction representation that is fused into the input and response segments using parameterized adapters and segmented weights.
- Core assumption: The attention mechanism's bias towards nearby tokens is the primary cause of instruction forgetting.
- Evidence anchors:
  - [abstract]: "As the attention mechanism of LLMs has limitations on local focus, LLMs tend to focus more on the words or sentences nearby at each position."
  - [section 3.2]: "The sentence will be converted to a list of tokens after the tokenizer. We define a segment ID for each segment and then map the segment index to every token of the tokens list."
  - [corpus]: Weak evidence. Related papers focus on instruction tuning but don't explicitly validate the local focus hypothesis in translation.

### Mechanism 2
- Claim: OVERMISS improves translation faithfulness by contrasting over-translation and miss-translation errors with correct translations.
- Mechanism: The dataset is constructed by prompting GPT-3.5-turbo to generate negative samples that mimic over-translation and miss-translation errors, then using word alignment tools to evaluate the quality of these samples.
- Core assumption: Contrastive learning with high-quality negative samples can improve model faithfulness.
- Evidence anchors:
  - [abstract]: "OVERMISS improves model faithfulness by comparing over-translation and miss-translation results with the correct translation."
  - [section 3.3]: "We prompt gpt-3.5-turbo to mimic the two typical error types, and the prompts are appended in Table.1."
  - [corpus]: Weak evidence. Related papers discuss instruction tuning but don't provide concrete evidence for the effectiveness of contrastive learning in translation faithfulness.

### Mechanism 3
- Claim: Combining SWIE and OVERMISS yields further improvements in translation performance.
- Mechanism: SWIE enhances the model's instruction-following ability, while OVERMISS improves faithfulness. Together, they address both the cause of instruction forgetting and the resulting translation errors.
- Core assumption: The two methods are complementary and their combination leads to synergistic improvements.
- Evidence anchors:
  - [abstract]: "The combination of SWIE and OVERMISS achieves a further improvement up to 0.56 BLEU scores on three backbone models from English to German."
  - [section 4.4]: "By combining the OVERMISS and SWIE, a further improvement can be seen in all of the backbones in the En â‡’ De translation direction from 0.05 to 0.56 BLEU scores."
  - [corpus]: No direct evidence. Related papers focus on individual methods rather than their combination.

## Foundational Learning

- Concept: Attention Mechanism in Transformers
  - Why needed here: Understanding how the attention mechanism works is crucial to grasping why LLMs tend to forget instructions and how SWIE addresses this issue.
  - Quick check question: What is the role of the attention mechanism in transformer models, and how does it contribute to the local focus problem?

- Concept: Instruction Tuning
  - Why needed here: Instruction tuning is the process of adapting LLMs to follow instructions, which is the core task being improved by SWIE and OVERMISS.
  - Quick check question: How does instruction tuning differ from traditional fine-tuning, and why is it particularly important for LLMs?

- Concept: Contrastive Learning
  - Why needed here: OVERMISS uses contrastive learning to improve translation faithfulness by comparing correct translations with over-translation and miss-translation errors.
  - Quick check question: What is contrastive learning, and how does it help models distinguish between correct and incorrect outputs?

## Architecture Onboarding

- Component map: SWIE (instruction adapters + segmented weights) -> OVERMISS (contrastive dataset) -> Base models (BLOOM/LLaMA)

- Critical path:
  1. Fine-tune base models with SWIE to improve instruction-following ability
  2. Construct OVERMISS dataset using GPT-3.5-turbo
  3. Fine-tune models with OVERMISS to improve faithfulness
  4. Combine SWIE and OVERMISS for further improvements

- Design tradeoffs:
  - SWIE adds computational overhead due to the instruction adapters and segmented weights
  - OVERMISS relies on the quality of the generated negative samples, which may vary
  - Combining both methods may lead to longer training times

- Failure signatures:
  - SWIE: Poor performance on long text or zero-shot translation directions, indicating insufficient attention to instructions
  - OVERMISS: High rates of over-translation or miss-translation errors, suggesting low-quality negative samples or ineffective contrastive learning
  - Combination: No further improvement over individual methods, indicating potential interference or redundancy

- First 3 experiments:
  1. Fine-tune BLOOMZ-3b with SWIE and evaluate on WMT22 test sets to verify improved instruction-following ability
  2. Construct OVERMISS dataset using GPT-3.5-turbo and evaluate its quality using word alignment tools
  3. Fine-tune LLaMA-7b with OVERMISS and evaluate on Flores-200 dev-test to verify improved faithfulness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different architectures (causal vs encoder-decoder) impact instruction following ability in LLMs at large scales?
- Basis in paper: [explicit] The paper mentions that "a more comprehensive investigation of other architectures' performance at a large scale is still lacking" and references a study (Zhao et al., 2023) on this topic.
- Why unresolved: While the paper uses causal LMs, it acknowledges that encoder-decoder models remain the standard for machine translation. The relative strengths and weaknesses of different architectures for instruction-following tasks haven't been thoroughly compared at scale.
- What evidence would resolve it: Systematic experiments comparing instruction-following performance across different architectures (causal LMs, encoder-decoder models, and prefix LMs) on the same instruction-following datasets, with consistent scaling of model size and training data.

### Open Question 2
- Question: Can the segment-weighted instruction embedding method be generalized to other sequence-to-sequence tasks beyond machine translation?
- Basis in paper: [explicit] The authors suggest exploring "methods to reduce inference latency" and "extending the data construction method to other tasks" as future work.
- Why unresolved: While SWIE shows improvements specifically for translation, its applicability to other instruction-following tasks (summarization, question answering, code generation) remains untested. The method's effectiveness may depend on task-specific characteristics.
- What evidence would resolve it: Experiments applying SWIE to diverse instruction-following tasks, comparing performance against baseline methods, and analyzing how different task characteristics (input length, instruction complexity, output format) affect SWIE's effectiveness.

### Open Question 3
- Question: What is the optimal strategy for combining SWIE and OVERMISS to maximize translation quality while minimizing computational overhead?
- Basis in paper: [explicit] The paper notes that "by combining the OVERMISS and SWIE, a further improvement can be seen" but doesn't explore optimal combination strategies or trade-offs.
- Why unresolved: The paper combines both methods but doesn't investigate whether they can be applied selectively, whether one method is more beneficial for certain translation directions, or how to balance their computational costs.
- What evidence would resolve it: Ablation studies testing different combinations (SWIE only on certain layers, OVERMISS only for certain error types), efficiency analyses comparing computational costs against quality improvements, and task-specific optimization of the combination strategy.

## Limitations

- The local attention mechanism hypothesis is stated as the root cause but lacks direct experimental validation within this work
- The OVERMISS dataset quality depends entirely on GPT-3.5-turbo's ability to generate realistic translation errors, which is not independently verified
- No ablation studies showing whether SWIE's instruction adapters or OVERMISS's contrastive learning contributes more to the improvements

## Confidence

- **High confidence**: BLEU score improvements are well-documented with clear baselines and controlled experiments
- **Medium confidence**: The faithfulness improvements based on word alignment metrics, though the metrics themselves have known limitations
- **Low confidence**: The theoretical claim that local attention mechanisms are the primary cause of instruction forgetting in translation tasks

## Next Checks

1. Run an ablation study comparing SWIE alone vs OVERMISS alone vs the combination to quantify their individual contributions
2. Analyze attention weight distributions during decoding to empirically verify that SWIE increases attention to instruction segments
3. Perform human evaluation of OVERMISS-generated negative samples to validate their quality and realism compared to actual translation errors