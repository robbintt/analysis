---
ver: rpa2
title: 'Digital Socrates: Evaluating LLMs through Explanation Critiques'
arxiv_id: '2311.09613'
source_url: https://arxiv.org/abs/2311.09613
tags:
- explanation
- answer
- critique
- reasoning
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of evaluating the quality of
  explanations generated by large language models (LLMs). The core method involves
  defining a new task called "explanation critiquing," where the goal is to identify
  and categorize flaws in an explanation and provide suggestions for improvement.
---

# Digital Socrates: Evaluating LLMs through Explanation Critiques

## Quick Facts
- arXiv ID: 2311.09613
- Source URL: https://arxiv.org/abs/2311.09613
- Reference count: 40
- Primary result: Digital Socrates critique model generates high-quality, nuanced critiques of LLM explanations with strong correlation to human judgments

## Executive Summary
This paper addresses the challenge of evaluating the quality of explanations generated by large language models. The authors introduce a new task called "explanation critiquing" where the goal is to identify and categorize flaws in an explanation and provide improvement suggestions. They create a human-verified dataset of explanation critiques and train an open-source model called Digital Socrates to perform this task automatically. The approach provides a systematic way to reveal insights about student models by examining their reasoning chains, offering a valuable tool for understanding and improving the explanation behavior of LLMs.

## Method Summary
The authors define an explanation critiquing task where models identify the main flaw in an explanation, categorize it along 8 dimensions, and provide suggestions for improvement. They create the DS Critique Bank dataset containing 13,000+ human-verified critique entries by first eliciting seed data from GPT-4 and then performing expert and crowd-sourced annotations. The Digital Socrates models (Llama2-7B and Llama2-13B variants) are fine-tuned on this dataset using a curriculum learning approach that progresses from simple Q&A to expert-level critiques. The training follows a three-stage process using progressively higher-quality data partitions.

## Key Results
- Digital Socrates models achieve explanation score correlations of 0.84-0.89 with human judgments
- The critique quality ratings show that most DS-generated critiques are rated as "good" by human evaluators
- The models demonstrate strong performance across multiple datasets including ARC and RAINBOW, with explanations receiving higher scores when aligned with correct answers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Explanation critiquing identifies model weaknesses beyond answer accuracy
- **Mechanism**: The critique task explicitly isolates the most significant flaw in the reasoning chain and categorizes it along 8 dimensions, enabling targeted analysis of specific failure modes
- **Core assumption**: The most significant flaw can be localized to a specific part of the explanation and categorized into one of the predefined dimensions
- **Evidence anchors**:
  - [abstract] "identify and categorize any main flaw in an explanation and provide suggestions to address the flaw"
  - [section 3.1] "The flaw should be formulated as a standalone erroneous statement, that can be understood without reference to the full question or explanation"
  - [corpus] Weak - only 5 related papers, none directly address the same flaw-identification mechanism
- **Break condition**: If explanations contain multiple equally significant flaws that cannot be isolated, or if the flaw doesn't fit the predefined dimensions

### Mechanism 2
- **Claim**: Human-verified critique dataset enables training of effective critique models
- **Mechanism**: The dataset contains human-verified critiques with multiple quality levels (crowdsourced and expert annotations) that provide training signals for critique models to learn both flaw identification and quality assessment
- **Core assumption**: Human judgments of critique quality and flaw dimensions can be reliably collected and used as training targets
- **Evidence anchors**:
  - [abstract] "create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model"
  - [section 4.3] "We elicit seed explanation critique data from GPT-4 and perform expert and crowd-sourced annotations"
  - [corpus] Weak - related papers discuss evaluation of explanations but not critique model training
- **Break condition**: If human judgments are too inconsistent or subjective to serve as reliable training targets

### Mechanism 3
- **Claim**: Digital Socrates models provide nuanced, interpretable critiques comparable to GPT-4
- **Mechanism**: The DS models are trained on a curriculum of increasing critique quality, starting from simple Q&A to expert-level critiques, enabling them to generate high-quality critiques with good correlation to human judgments
- **Core assumption**: A training curriculum with increasing complexity and quality of examples can effectively teach critique generation
- **Evidence anchors**:
  - [abstract] "train an open-source, automatic critique model (called Digital Socrates) using this data"
  - [section 6.3] "Human judgments of critique quality (Table 5) show that the vast majority of the critiques generated by our DS models are rated good"
  - [corpus] Weak - no direct evidence about curriculum-based training for critique models
- **Break condition**: If the model overfits to training data patterns and cannot generalize to new explanation styles or domains

## Foundational Learning

- **Concept**: Socratic questioning methodology
  - Why needed here: Provides the systematic framework for identifying different types of reasoning flaws
  - Quick check question: Can you name all 6 types of Socratic questions that the 8 critique dimensions are based on?

- **Concept**: Explanation score calibration
  - Why needed here: Enables consistent quality assessment across different explanations and models
  - Quick check question: What's the difference between an explanation score of 2 vs 3 according to the scoring guidelines?

- **Concept**: Curriculum learning for critique generation
  - Why needed here: Allows models to gradually learn from simpler to more complex critique patterns
  - Quick check question: In what order are the training data partitions presented to the model during fine-tuning?

## Architecture Onboarding

- **Component map**: Critique generation pipeline with three main components - flaw identification, suggestion generation, and quality scoring
- **Critical path**: Question + Explanation → Flaw Identification → Suggestion Generation → Quality Scoring → Final Critique
- **Design tradeoffs**: 
  - Using standalone flaw statements vs. full-context critiques
  - Semi-structured format vs. free-form critique generation
  - 8 predefined dimensions vs. open-ended flaw categories
- **Failure signatures**: 
  - Model outputs "None" critique for explanations with subtle flaws
  - Generated suggestions don't address the identified flaw
  - Explanation scores don't correlate with human judgments
- **First 3 experiments**:
  1. Test flaw identification on a small set of known-good and known-bad explanations
  2. Evaluate suggestion quality by checking if they address the identified flaws
  3. Compare explanation scores between model-generated and human judgments on a validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of model-generated critiques from Digital Socrates compare to human-generated critiques when evaluated on a wider range of tasks beyond science and commonsense reasoning?
- Basis in paper: The paper focuses on science and commonsense reasoning tasks for evaluating the critique models. It mentions using ARC and RAINBOW datasets, but does not explore other types of tasks.
- Why unresolved: The evaluation is limited to science and commonsense reasoning tasks, so it's unclear how well the critique models would perform on other types of tasks.
- What evidence would resolve it: Evaluating the critique models on a diverse set of tasks, such as natural language inference, sentiment analysis, or summarization, and comparing their performance to human-generated critiques on those tasks.

### Open Question 2
- Question: How does the performance of Digital Socrates change when fine-tuned on a larger and more diverse dataset of human-verified critiques?
- Basis in paper: The paper uses a dataset of around 13,000 critique entries, but it's not clear if this is the maximum size or if there are plans to expand it further.
- Why unresolved: The current dataset may not be large or diverse enough to fully capture the range of possible critique dimensions and flaws in model-generated explanations.
- What evidence would resolve it: Fine-tuning the critique models on a significantly larger and more diverse dataset of human-verified critiques and comparing their performance to the current models.

### Open Question 3
- Question: How can the critique dimensions and scoring system be further refined to better capture the nuances of explanation quality and provide more actionable feedback?
- Basis in paper: The paper proposes 8 critique dimensions and a 5-point scoring system for explanation quality, but it's not clear if these are the optimal choices or if there is room for improvement.
- Why unresolved: The critique dimensions and scoring system may not fully capture all the relevant aspects of explanation quality, and there may be better ways to provide actionable feedback.
- What evidence would resolve it: Conducting user studies with human evaluators to gather feedback on the current critique dimensions and scoring system, and using that feedback to refine and improve them.

## Limitations
- The approach relies on predefined 8 critique dimensions which may not capture all possible explanation flaws, especially in novel domains
- The dataset construction depends heavily on GPT-4 for initial critique generation, potentially introducing systematic biases
- The methodology assumes flaws can be isolated and categorized, which may not hold for complex, interdependent reasoning errors

## Confidence

**High Confidence**: The methodology for creating semi-structured critique datasets is well-defined and reproducible. The correlation between model-generated critiques and human judgments (average 0.87) provides strong evidence for the approach's validity in identifying explanation flaws.

**Medium Confidence**: The effectiveness of the 8-dimension categorization scheme for capturing all relevant explanation flaws. While the dimensions cover common reasoning errors, the approach may miss domain-specific or novel failure modes not represented in the training data.

**Low Confidence**: The generalizability of Digital Socrates to domains outside of multiple-choice question answering and to explanation styles not represented in the training corpus. The models may overfit to the specific explanation patterns seen during training.

## Next Checks
1. **Cross-domain validation**: Test Digital Socrates on explanations from domains not represented in the original dataset (e.g., scientific papers, legal documents) to assess generalization capability.

2. **Ablation study on dimensions**: Remove each of the 8 critique dimensions from training to determine which are most critical for critique quality and which might be redundant.

3. **Longitudinal consistency test**: Evaluate whether critiques generated by Digital Socrates for the same explanation remain consistent across multiple generations, testing for reliability and avoiding hallucination patterns.