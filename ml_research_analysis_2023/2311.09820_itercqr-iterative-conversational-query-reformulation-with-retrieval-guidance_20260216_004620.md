---
ver: rpa2
title: 'IterCQR: Iterative Conversational Query Reformulation with Retrieval Guidance'
arxiv_id: '2311.09820'
source_url: https://arxiv.org/abs/2311.09820
tags:
- itercqr
- query
- conversational
- queries
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes IterCQR, an iterative approach for conversational
  query reformulation that does not rely on human-annotated rewrites. IterCQR leverages
  information retrieval signals as a reward to guide the reformulation model in generating
  queries that are optimized for retrieval performance.
---

# IterCQR: Iterative Conversational Query Reformulation with Retrieval Guidance

## Quick Facts
- arXiv ID: 2311.09820
- Source URL: https://arxiv.org/abs/2311.09820
- Reference count: 7
- Key outcome: Achieves state-of-the-art performance on TopiOCQA and QReCC datasets without human-annotated rewrites, demonstrating robustness in domain-shift and low-resource settings.

## Executive Summary
This paper introduces IterCQR, an iterative approach for conversational query reformulation that eliminates the need for human-annotated rewrites by leveraging information retrieval signals as rewards. The method uses dense embeddings of reformulated queries and gold passages to guide the reformulation model toward generating queries optimized for retrieval performance. IterCQR achieves state-of-the-art results on two widely-used datasets (TopiOCQA and QReCC) for both sparse and dense retrievers, showing particular strength in challenging scenarios such as domain-shift, low-resource, and topic-shift settings.

## Method Summary
IterCQR iteratively refines a query reformulation model by using dense embeddings as reward signals. The process starts with an initial model trained on LLM-generated rewrites, then generates candidate queries which are scored based on their cosine similarity to gold passage embeddings. The model employs Minimum Bayes Risk (MBR) training for exploration in the first iteration, then switches to top-1 candidate selection for exploitation in subsequent iterations. This approach optimizes query reformulation directly for retrieval performance without requiring human supervision.

## Key Results
- Achieves MRR of 0.263 on TopiOCQA with dense retriever ANCE, outperforming methods using human rewrites
- Demonstrates robust performance across domain-shift, low-resource, and topic-shift settings
- Works effectively with both sparse (BM25) and dense (ANCE) retrievers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IterCQR improves conversational query reformulation without human supervision by using IR signals as rewards.
- Mechanism: The model generates candidate queries using the previous iteration's model, then uses dense embeddings of the gold passage and candidate query as a reward signal to train the current iteration model.
- Core assumption: Dense embeddings of reformulated queries and gold passages can serve as a meaningful reward signal for optimizing query reformulation.
- Evidence anchors:
  - [abstract] "IterCQR leverages information retrieval signals as a reward to guide the reformulation model in generating queries that are optimized for retrieval performance."
  - [section 3.2.2] "We set the reward value as the cosine similarity between the reformulated query's dense embeddings and the gold passage's dense embeddings to optimize the IterCQR for the retriever."
  - [corpus] Weak corpus evidence - no direct mention of dense embeddings reward signals in neighbor papers.

### Mechanism 2
- Claim: MBR training enables effective exploration of the query space during the first iteration.
- Mechanism: MBR training considers all n candidate queries by re-normalizing their probabilities and maximizing the expected reward, allowing the model to explore diverse query options.
- Core assumption: Considering multiple candidate queries with re-normalized probabilities leads to better exploration than traditional single-target training.
- Evidence anchors:
  - [section 3.2.2] "Using all n candidates, it is possible to consider candidates with smaller probability values when training the model."
  - [section 3.2.2] "The MBR training algorithm seeks to minimize the expected value of a cost function C."
  - [corpus] Weak corpus evidence - MBR is mentioned but not directly tied to query reformulation exploration in neighbor papers.

### Mechanism 3
- Claim: Top-1 candidate selection in exploitation phase provides stable learning after exploration.
- Mechanism: After MBR exploration, the model selects the single highest-reward candidate as the training target for subsequent iterations, providing focused learning.
- Core assumption: The highest-reward candidate from exploration is likely to be the best query reformulation and can serve as a stable training target.
- Evidence anchors:
  - [section 3.2.3] "In this step, we select the top-1 candidate ctop among n candidates as the target for the training."
  - [section 3.2.3] "The loss in the exploitation step is defined as negative log likelihood between the current target query and top-1 candidate ctop i."
  - [corpus] No direct corpus evidence for top-1 selection in conversational query reformulation context.

## Foundational Learning

- Concept: Reinforcement Learning through reward signals
  - Why needed here: IterCQR uses retrieval performance (measured via dense embeddings) as a reward signal instead of human annotations
  - Quick check question: How does using IR signals as rewards differ from traditional supervised learning with human rewrites?

- Concept: Minimum Bayes Risk (MBR) training
  - Why needed here: MBR enables exploration of multiple candidate queries by considering the expected reward across all candidates
  - Quick check question: What advantage does MBR have over standard maximum likelihood training when dealing with multiple candidate outputs?

- Concept: Dense vs sparse retrieval embeddings
  - Why needed here: IterCQR uses dense embeddings for reward calculation but achieves good performance with both dense and sparse retrievers
  - Quick check question: Why might a model trained with dense embedding rewards still perform well with BM25 (sparse) retrieval?

## Architecture Onboarding

- Component map: LLM-rewritten queries (D0) → Model M0 → Iteration loop (generate candidates → MBR/Top-1 training → Update model) → Frozen dense retriever → T5-base model
- Critical path: 1. Generate candidates with previous model 2. Calculate dense embeddings for candidates and gold passages 3. Compute reward (cosine similarity) 4. Train with MBR (exploration) or Top-1 (exploitation) 5. Update model for next iteration
- Design tradeoffs: Using frozen retriever vs fine-tuning retriever for context, MBR exploration vs direct exploitation, dense embedding rewards vs other reward signals, number of candidates (n=10) vs computational cost
- Failure signatures: All candidates getting similar rewards (exploration fails), Top-1 selection consistently poor (exploitation fails), No improvement across iterations (training loop broken), Dense embeddings not capturing semantic similarity
- First 3 experiments: 1. Verify MBR training improves over random initialization by comparing iteration 0 vs iteration 1 performance 2. Test sensitivity to number of candidates (n=5, 10, 20) on retrieval performance 3. Validate dense embedding reward signal by comparing with alternative rewards (e.g., BM25 scores)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does IterCQR perform in zero-shot settings on unseen datasets compared to models fine-tuned on those datasets?
- Basis in paper: [inferred] The paper mentions robustness in challenging settings such as domain-shift and low-resource scenarios, but does not explicitly test zero-shot performance on entirely new datasets.
- Why unresolved: The paper only tests domain-shift by training on one dataset and evaluating on another, but does not explore truly zero-shot performance where the model has no prior exposure to the target domain.
- What evidence would resolve it: Empirical results comparing IterCQR's performance on completely new, unseen datasets without any fine-tuning or adaptation to models specifically trained on those datasets.

### Open Question 2
- Question: What is the optimal number of iterations for IterCQR, and does this vary based on dataset characteristics or retriever type?
- Basis in paper: [explicit] The paper mentions "as the iterations progress" but does not provide a systematic analysis of the optimal number of iterations or how this might vary across different scenarios.
- Why unresolved: While the paper shows performance improvements over iterations, it does not determine the point of diminishing returns or the optimal stopping criterion for different settings.
- What evidence would resolve it: A comprehensive study analyzing retrieval performance, computational cost, and query characteristics across varying numbers of iterations on multiple datasets and retriever types.

### Open Question 3
- Question: How does IterCQR's query reformulation affect interpretability and user satisfaction in real conversational search scenarios?
- Basis in paper: [inferred] The paper focuses on retrieval performance metrics but does not address user-facing aspects of the reformulated queries or their interpretability in conversational contexts.
- Why unresolved: The paper does not include any user studies or qualitative analysis of how the reformulated queries would be perceived in actual conversational search applications.
- What evidence would resolve it: User studies comparing the reformulated queries to human-written rewrites in terms of clarity, naturalness, and effectiveness in maintaining conversational context while improving retrieval.

## Limitations
- Evaluation primarily relies on TopiOCQA and QReCC datasets, which may not represent diverse real-world conversational search scenarios
- Approach depends heavily on quality of dense embeddings as reward signals, with limited exploration of alternative reward functions
- Iterative process assumes improvements in dense embedding similarity translate to retrieval performance gains without rigorous validation
- Computational cost of generating multiple candidates and computing dense embeddings may be prohibitive for production deployment at scale

## Confidence
**High Confidence:** The core mechanism of using dense embeddings as reward signals for iterative query reformulation is well-supported by experimental results showing consistent improvements across datasets and retrievers.

**Medium Confidence:** Robustness claims in domain-shift, low-resource, and topic-shift settings are supported by experiments but could benefit from more extensive validation across diverse domains.

**Low Confidence:** Long-term stability of iterative process beyond reported iterations is unknown, and the paper doesn't address potential catastrophic forgetting or convergence to local minima.

## Next Checks
1. **Reward Signal Ablation:** Systematically compare IterCQR's performance when using alternative reward signals (BM25 scores, cross-encoder relevance scores, or learned reward functions) against current dense embedding cosine similarity approach.

2. **Cross-Dataset Transfer:** Train IterCQR on one dataset (e.g., TopiOCQA) and evaluate zero-shot transfer to other conversational search datasets or domains not seen during training to validate robustness claims.

3. **Computational Efficiency Analysis:** Measure and analyze wall-clock time and computational resources required for each iteration, including candidate generation, dense embedding computation, and training updates, to establish practical deployment constraints.