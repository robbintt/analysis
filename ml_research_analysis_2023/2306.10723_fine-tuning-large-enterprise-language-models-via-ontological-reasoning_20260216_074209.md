---
ver: rpa2
title: Fine-tuning Large Enterprise Language Models via Ontological Reasoning
arxiv_id: '2306.10723'
source_url: https://arxiv.org/abs/2306.10723
tags:
- fine-tuning
- language
- reasoning
- knowledge
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a neurosymbolic approach to fine-tune Large
  Language Models (LLMs) for domain-specific tasks by leveraging ontological reasoning
  over Enterprise Knowledge Graphs (EKGs). The key innovation is generating fine-tuning
  corpora through verbalization of Datalog-based reasoning results, enabling models
  to learn both explicit data and inferred domain knowledge.
---

# Fine-tuning Large Enterprise Language Models via Ontological Reasoning

## Quick Facts
- arXiv ID: 2306.10723
- Source URL: https://arxiv.org/abs/2306.10723
- Reference count: 25
- One-line primary result: Neurosymbolic approach that generates fine-tuning corpora through verbalization of Datalog-based reasoning results, enabling LLMs to learn domain-specific knowledge beyond ground facts

## Executive Summary
This paper presents a neurosymbolic approach to fine-tune Large Language Models (LLMs) for domain-specific tasks by leveraging ontological reasoning over Enterprise Knowledge Graphs (EKGs). The key innovation is generating fine-tuning corpora through verbalization of Datalog-based reasoning results, enabling models to learn both explicit data and inferred domain knowledge. The method involves creating chase expansions of databases with ontological rules, verbalizing reasoning steps into natural language, and optimizing quality through filtering and paraphrasing. A proof-of-concept with a trading domain demonstrates that models fine-tuned on chase data outperform those trained only on ground facts for domain-specific question answering, capturing complex logical relationships not present in raw data.

## Method Summary
The approach uses ontological reasoning to generate fine-tuning corpora for LLMs by performing chase expansions on databases with Datalog rules, then verbalizing the reasoning steps into natural language narratives. A logic plan with placeholder tokens is created to minimize LLM calls while protecting sensitive data. The method includes quality filtering to ensure prompt-response pairs are specific, plausible, and unbiased, followed by paraphrasing to enhance generalization. The fine-tuned models can then perform domain-specific NLP tasks like question answering, explanation generation, and text-to-query translation.

## Key Results
- Models fine-tuned on chase-derived corpora outperform those trained only on ground facts for domain-specific question answering
- The approach captures complex logical relationships not present in raw data through ontological reasoning
- Chase-based fine-tuning enables LLMs to understand both explicit facts and inferred domain knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chase expansion with ontological rules generates additional facts beyond the ground database that capture implicit domain relationships
- Mechanism: The Vadalog chase procedure iteratively applies Datalog rules to derive new facts, effectively materializing the "reasoning space" that would otherwise remain implicit in the database schema and rules
- Core assumption: Domain rules (Î£) are sufficiently expressive to capture meaningful business-level definitions that can be verbalized into natural language
- Evidence anchors:
  - [abstract] "generating fine-tuning corpora through verbalization of Datalog-based reasoning results"
  - [section 2] "The set Î£(ð·) is computed via the chase [13]: starting from Î£(ð·) = ð·, the chase augments Î£(ð·) with facts derived from the application of the rules in Î£ to fixpoint"
- Break condition: If rules are too simple or domain-specific, the chase may not generate meaningful new facts beyond trivial inferences

### Mechanism 2
- Claim: Verbalizing reasoning steps into natural language creates a fine-tuning corpus that teaches the LLM both explicit data and inferred domain knowledge
- Mechanism: Each chase step is transformed into a "since-then" narrative format that captures the logical reasoning process in human-readable form, bridging the gap between formal reasoning and natural language understanding
- Core assumption: The deterministic transformation from Datalog rules to natural language preserves the logical structure while making it comprehensible to the LLM
- Evidence anchors:
  - [section 2] "Whenever a Vadalogrule is involved in the chase, it is translated into pure text with a deterministic transformation, based on the select-project-join semantics"
  - [section 2] "For instance, with respect to our running example, the chase step Open(EGTech, 0.3, 1), Â¬ MarketClose (1) â†’ Accepted(EGTech, 0.3, 1) (rule 1) is verbalized as: Since the trader EGTech at time 1 sends an order to open a position of size 0.3, and it is not true that 1 is a time when the market is closed, then the order of size 0.3 by EGTech is accepted at time 1"
- Break condition: If the verbalization loses critical logical details or becomes too verbose, the LLM may not learn the intended reasoning patterns

### Mechanism 3
- Claim: Lifting technique with logic plan instantiation minimizes LLM calls while protecting sensitive data
- Mechanism: A logic plan is verbalized once with placeholder tokens, then instantiated with actual values from chase steps, avoiding repeated corpus generation calls and preventing exposure of ground values to the LLM
- Core assumption: The regularity of logical languages allows for effective token-based instantiation that preserves semantic relationships
- Evidence anchors:
  - [section 2] "We leverage the regularity of logical languages and resort to a lifting technique. We build a logic plan out of Î£ (line 10). A plan is the equivalent in our context of a database execution plan and can be seen as the dependency graph of the rules of Î£, where nodes represent rules and edges stand for head-body dependencies"
  - [section 2] "Finally, a tokenized fine-tuning corpus is generated from the plan, after minor pre-processing (line 11). The form of the prompts depends on the task. Now, for each verbalized chase step, we look up the corresponding verbalized portion of the plan and instantiate its tokens (lines 13-15)"
- Break condition: If the logic plan abstraction is too coarse or loses important variable relationships, the instantiated prompts may be semantically incorrect

## Foundational Learning

- Concept: Datalog and chase procedure
  - Why needed here: The entire approach relies on ontological reasoning using Datalog rules and chase expansion to generate the fine-tuning corpus
  - Quick check question: What is the difference between ground facts and chase-derived facts in the context of this paper?

- Concept: Verbalization of formal reasoning into natural language
  - Why needed here: The core innovation involves transforming logical reasoning steps into human-readable narratives for LLM training
  - Quick check question: How does the "since-then" verbalization format preserve logical relationships while making them comprehensible?

- Concept: Logic plan as dependency graph for token instantiation
  - Why needed here: The lifting technique uses logic plans to minimize LLM calls and protect sensitive data
  - Quick check question: What role does the logic plan play in the token-based instantiation process?

## Architecture Onboarding

- Component map:
  Database (ð·) -> Vadalog engine -> Chase expansion -> Verbalizer -> Logic plan generator -> Corpus generator -> Quality filter -> Paraphraser -> Fine-tuning pipeline

- Critical path: Database + Rules â†’ Chase generation â†’ Verbalization â†’ Logic plan â†’ Token instantiation â†’ Corpus generation â†’ Quality filtering â†’ Paraphrasing â†’ Fine-tuning

- Design tradeoffs:
  - Chase completeness vs. computational cost: More extensive chase provides richer training data but increases processing time
  - Verbalization detail vs. readability: More detailed explanations preserve logical relationships but may be harder for LLM to process
  - Quality filtering strictness vs. corpus size: Stricter filtering improves quality but reduces available training data

- Failure signatures:
  - Poor chase expansion: If rules are too simple or domain-specific, the chase may not generate meaningful new facts
  - Lossy verbalization: If the transformation from logic to natural language loses critical details, the LLM won't learn proper reasoning
  - Template mismatch: If the logic plan abstraction doesn't match actual chase steps, instantiation will produce incorrect prompts

- First 3 experiments:
  1. Run chase expansion on a simple database with basic rules to verify that new facts are generated beyond the ground data
  2. Test the verbalization process on a single chase step to ensure the "since-then" format preserves logical relationships
  3. Verify the logic plan instantiation by creating one template and instantiating it with actual values from a chase step

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the chase-based fine-tuning approach compare to alternative knowledge injection methods like retrieval-augmented generation or explicit memory networks for enterprise NLP tasks?
- Basis in paper: [inferred] The paper compares chase-based fine-tuning to ground data fine-tuning but doesn't explore other knowledge injection approaches
- Why unresolved: The paper only compares against baseline ground data fine-tuning, leaving performance relative to other knowledge integration methods unknown
- What evidence would resolve it: Head-to-head experiments comparing chase-based fine-tuning against retrieval-augmented generation and explicit memory networks on the same enterprise NLP tasks and datasets

### Open Question 2
- Question: What is the optimal balance between the size of the fine-tuning corpus generated through ontological reasoning versus the pre-trained model size for maximizing domain-specific performance?
- Basis in paper: [explicit] The paper mentions cost and time efficiency concerns regarding LLM calls but doesn't explore scaling relationships between corpus size and model size
- Why unresolved: The paper focuses on the methodology but doesn't investigate how corpus size should scale with model capacity for optimal results
- What evidence would resolve it: Systematic experiments varying both corpus size (through reasoning depth/scope) and model size across multiple enterprise domains

### Open Question 3
- Question: How does the chase-based approach perform on complex multi-hop reasoning tasks that require temporal or spatial reasoning beyond the trading domain?
- Basis in paper: [inferred] The proof-of-concept focuses on a simple trading domain with limited temporal reasoning, suggesting broader evaluation is needed
- Why unresolved: The preliminary validation only demonstrates effectiveness in a constrained financial scenario without temporal or spatial complexity
- What evidence would resolve it: Comprehensive evaluation on benchmarks requiring temporal ordering, spatial relationships, or multi-domain knowledge integration

### Open Question 4
- Question: What is the computational overhead of the chase generation and verbalization steps compared to the actual fine-tuning process, and how does this scale with enterprise knowledge graph size?
- Basis in paper: [explicit] The paper describes the pipeline but doesn't provide computational complexity analysis or scaling behavior measurements
- Why unresolved: The paper presents the methodology without quantifying the computational cost of reasoning-based corpus generation
- What evidence would resolve it: Empirical measurements of chase generation time, verbalization overhead, and corpus generation scaling across knowledge graphs of varying sizes and complexity

## Limitations

- The approach assumes domain rules are sufficiently expressive to generate meaningful chase expansions, which may not hold for complex business domains
- The quality filtering mechanism relies on a scoring model that isn't fully specified, making it difficult to assess how well it prevents spurious or biased training data
- The proof-of-concept is limited to a simple trading domain without temporal or spatial reasoning complexity

## Confidence

- **High Confidence**: The basic mechanism of using chase expansion to generate additional facts beyond ground data is well-established in the Datalog literature. The verbalization process, while novel in this context, follows deterministic transformation patterns that should preserve logical structure.

- **Medium Confidence**: The proof-of-concept results showing improved performance for domain-specific QA are promising but limited in scope. The trading domain example demonstrates the approach works in at least one case, but generalization to other domains remains unproven.

- **Low Confidence**: The claim that this approach "bridges the gap between NLP flexibility and domain-oriented reasoning" is somewhat speculative. While the mechanism is plausible, the paper doesn't provide comprehensive evidence of how well the fine-tuned models actually capture complex logical relationships compared to baseline approaches.

## Next Checks

1. Test the chase expansion process on multiple domains (not just trading) to verify that meaningful reasoning facts are consistently generated across different business contexts.

2. Conduct ablation studies comparing model performance when trained on: (a) ground facts only, (b) chase-derived facts without verbalization, and (c) the full verbalized corpus, to isolate the contribution of each component.

3. Evaluate the fine-tuned models on out-of-distribution questions that require multi-step reasoning to assess whether they truly learned the logical patterns or just memorized surface-level patterns from the training data.