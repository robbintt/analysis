---
ver: rpa2
title: Investigating the Impact of Weight Sharing Decisions on Knowledge Transfer
  in Continual Learning
arxiv_id: '2311.09506'
source_url: https://arxiv.org/abs/2311.09506
tags:
- task
- tasks
- sharing
- subnetwork
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how weight sharing decisions in continual
  learning (CL) impact knowledge transfer between tasks. It focuses on structured
  pruning-based CL, where subnetworks are trained sequentially for each task and weights
  from past tasks can be shared to aid new tasks.
---

# Investigating the Impact of Weight Sharing Decisions on Knowledge Transfer in Continual Learning

## Quick Facts
- arXiv ID: 2311.09506
- Source URL: https://arxiv.org/abs/2311.09506
- Reference count: 33
- Key outcome: Sharing decisions based on subnetwork connectivity and activation scores can significantly improve task accuracy in continual learning, with oversharing leading to suboptimal results.

## Executive Summary
This paper investigates how weight sharing decisions impact knowledge transfer between tasks in continual learning (CL). The study focuses on structured pruning-based CL, where subnetworks are trained sequentially for each task, and weights from past tasks can be shared to aid new tasks. By systematically evaluating the influence of task complexity and similarity on optimal sharing decisions, the research provides insights into task relationships and informs decision-making in CL methods. The results demonstrate that sharing decisions based on measures like connectivity and activation scores can significantly improve task accuracy, while oversharing can be detrimental.

## Method Summary
The study implements structured pruning-based continual learning with constraints to ensure filter behavior consistency. Networks are trained sequentially on tasks, pruned, and evaluated based on sharing decisions informed by subnetwork usefulness measures. Three sequential datasets emphasizing variations in task complexity and similarity (Tiny Imagenet-CIFAR, Mixed PMNIST/CIFAR, Mixed KEF-MNIST/CIFAR) are used, along with VGG-16 and ResNet-18 architectures. The method involves training networks on tasks, pruning filters, and evaluating sharing decisions based on usefulness measures like activation scores and connectivities.

## Key Results
- Sharing decisions based on connectivity and activation scores can significantly improve task accuracy in continual learning.
- Sharing subnetworks from more complex or similar tasks often yields better performance.
- Oversharing weights from all past tasks can lead to suboptimal accuracy due to interference.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sharing subnetworks with lower connectivity scores leads to higher task accuracy in continual learning.
- Mechanism: Lower connectivity implies that a subnetwork's filters are less dependent on each other, making them more generalizable and less prone to interference when shared with new tasks.
- Core assumption: Connectivity is a reliable linear estimator of layer dependencies and reflects the information flow within a subnetwork.
- Evidence anchors:
  - [abstract] "Results show that sharing decisions based on these measures can significantly improve task accuracy."
  - [section] "By contrast we consistently observe lower mean connectivities in subnetworks which provide greater accuracy Acc α β."
  - [corpus] Weak or missing - no direct evidence in corpus.
- Break condition: If connectivity does not accurately reflect information flow or if sharing decisions based on it lead to worse performance in certain task sequences.

### Mechanism 2
- Claim: Task similarity and complexity influence the optimal weight sharing decisions in continual learning.
- Mechanism: Sharing subnetworks from tasks that are similar or more complex than the current task provides better forward knowledge transfer, as the features learned are more relevant and generalizable.
- Core assumption: Similar or complex tasks provide more relevant and generalizable features for new tasks.
- Evidence anchors:
  - [abstract] "Specifically, sharing subnetworks from more complex or similar tasks often yields better performance."
  - [section] "We observe that activations and connectivities tend to decrease in the subsequent layers, while the connectivity tends to be significantly lower for subnetworks which yield better accuracy Acc α β."
  - [corpus] Weak or missing - no direct evidence in corpus.
- Break condition: If task similarity or complexity does not correlate with better forward knowledge transfer or if sharing decisions based on them lead to worse performance in certain task sequences.

### Mechanism 3
- Claim: Oversharing weights from all past tasks can lead to suboptimal accuracy in continual learning.
- Mechanism: Including all past subnetworks in the current task can cause interference and reduce the effectiveness of knowledge transfer, as unrelated features may disrupt the learning process.
- Core assumption: Including unrelated features from past tasks can interfere with the learning process of new tasks.
- Evidence anchors:
  - [abstract] "Understanding which weights to share is important as sharing all weights can yield sub-optimal accuracy."
  - [section] "These results also support the idea that oversharing can be of significant detriment to CL."
  - [corpus] Weak or missing - no direct evidence in corpus.
- Break condition: If including all past subnetworks does not cause interference or if it leads to better performance in certain task sequences.

## Foundational Learning

- Concept: Catastrophic Forgetting (CF)
  - Why needed here: Understanding CF is crucial for grasping the motivation behind continual learning and the importance of weight sharing decisions.
  - Quick check question: What is catastrophic forgetting, and why does it occur in neural networks during sequential task training?

- Concept: Forward Knowledge Transfer (FKT)
  - Why needed here: FKT is the desired outcome of continual learning, where past knowledge is reused to benefit the handling of new tasks.
  - Quick check question: How does forward knowledge transfer differ from backward knowledge transfer in continual learning?

- Concept: Structured Pruning
  - Why needed here: Structured pruning is used in this study to create subnetworks for each task and control which weights are shared between tasks.
  - Quick check question: What is the difference between structured and unstructured pruning, and why is structured pruning preferred in this study?

## Architecture Onboarding

- Component map:
  Full network (F) composed of L layers (F1-FL) -> Filters (fil) within each layer -> Subnetworks (St) for each task (Tt) -> Binary masks (Mt) for pruned weights -> Separate classifier heads (Ht) for each task

- Critical path:
  1. Train network on first task (T1)
  2. Prune k% of filters per layer and freeze weights
  3. Store binary mask (M1) for pruned weights
  4. Train network on subsequent tasks (Tt)
  5. Share or omit past subnetworks (St'<t) based on sharing decisions
  6. Evaluate task accuracy and forward knowledge transfer

- Design tradeoffs:
  - Fixed network capacity vs. dynamic capacity expansion
  - Structured pruning vs. unstructured pruning
  - Sharing all past subnetworks vs. selective sharing based on connectivity scores

- Failure signatures:
  - Decreased task accuracy when sharing all past subnetworks
  - Inconsistent forward knowledge transfer between tasks
  - Difficulty in interpreting the usefulness of shared subnetworks

- First 3 experiments:
  1. Two-Task Knowledge Transfer: Evaluate the usefulness of sharing a single subnetwork (Sα) for a subsequent task (Tβ) based on connectivity scores and resulting accuracy.
  2. Three-Task Subnetwork Omission: Assess the impact of omitting certain subnetworks when training a third task (Tγ) based on predicted usefulness scores.
  3. Six-Task Subnetwork Sharing: Apply sharing strategies based on subnetwork connectivity scores to a sequence of six tasks and evaluate overall accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the structural constraints on weight sharing in continual learning impact the performance and generalizability of neural networks in practical, real-world applications?
- Basis in paper: [inferred] The paper discusses the constraints imposed on pruning and sharing for interpretability, noting that they may be overly restrictive for real-world problems and could lead to differences in filter behavior between tasks.
- Why unresolved: The paper acknowledges the constraints' necessity for interpretability but suggests they may not be suitable for practical applications, indicating a gap in understanding their real-world impact.
- What evidence would resolve it: Empirical studies comparing network performance with and without these constraints in diverse, real-world datasets would clarify their practical implications.

### Open Question 2
- Question: What is the relationship between connectivity scores and the generalizability of neural networks across different tasks in continual learning?
- Basis in paper: [explicit] The paper observes that subnetworks with lower connectivity scores yield better accuracy when shared, suggesting a potential link to generalizability, but this relationship is not fully explored.
- Why unresolved: While the paper suggests lower connectivity may be more generalizable, it does not provide a comprehensive analysis of how connectivity affects generalizability across various tasks.
- What evidence would resolve it: Experiments measuring the generalizability of networks with varying connectivity scores across diverse tasks would provide insights into this relationship.

### Open Question 3
- Question: How does sharing subnetworks from similar tasks impact the adversarial robustness of neural networks in continual learning?
- Basis in paper: [inferred] The paper suggests that sharing similar tasks is beneficial for forward knowledge transfer but does not explore its impact on adversarial robustness.
- Why unresolved: The paper does not investigate the potential trade-offs between task similarity and adversarial robustness, leaving this aspect unexplored.
- What evidence would resolve it: Comparative studies assessing adversarial robustness in networks sharing similar versus diverse tasks would clarify this impact.

### Open Question 4
- Question: How do task complexity and similarity influence the effectiveness of weight sharing strategies in continual learning?
- Basis in paper: [explicit] The paper demonstrates that task complexity and similarity affect forward knowledge transfer and optimal sharing decisions, but the underlying mechanisms are not fully elucidated.
- Why unresolved: While the paper shows correlations between task properties and sharing effectiveness, it does not provide a detailed explanation of the causal mechanisms.
- What evidence would resolve it: Detailed experiments isolating task complexity and similarity effects on weight sharing outcomes would elucidate these mechanisms.

## Limitations
- The findings rely heavily on specific architectural choices (ResNet-18 and VGG-16) and the three constructed datasets, raising questions about generalizability to other architectures or real-world scenarios.
- The paper does not provide a clear threshold or methodology for determining the optimal number of subnetworks to share, despite demonstrating that oversharing can be detrimental.

## Confidence
- **High Confidence:** The observation that including all past subnetworks can lead to decreased task accuracy (oversharing effect) is well-supported by the experimental results across multiple datasets and architectures.
- **Medium Confidence:** The claim that lower connectivity scores indicate more useful subnetworks for sharing is supported by the data but relies on the assumption that connectivity is a reliable proxy for filter independence and generalizability.
- **Low Confidence:** The specific impact of task complexity and similarity on optimal sharing decisions, while suggested by the results, requires further investigation to establish clear guidelines or thresholds for practical application.

## Next Checks
1. **Generalizability Test:** Replicate the experiments with a broader range of architectures (e.g., MobileNet, EfficientNet) and real-world datasets with naturally occurring task sequences to assess the robustness of the findings.
2. **Connectivity Threshold Analysis:** Conduct a systematic study to determine the optimal connectivity threshold for subnetwork selection, exploring whether a specific cutoff value or a relative ranking approach is more effective.
3. **Alternative Usefulness Measures:** Investigate the effectiveness of other subnetwork usefulness metrics (e.g., mutual information, Fisher information) compared to the proposed connectivity and activation scores to validate the choice of measures and potentially uncover additional insights.