---
ver: rpa2
title: Batching for Green AI -- An Exploratory Study on Inference
arxiv_id: '2307.11434'
source_url: https://arxiv.org/abs/2307.11434
tags:
- energy
- batch
- consumption
- image
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of energy consumption during inference
  in deep learning models, particularly focusing on the impact of batch size. The
  authors conduct an exploratory study on five state-of-the-art neural networks for
  computer vision, examining how different batch sizes affect energy consumption and
  response times.
---

# Batching for Green AI -- An Exploratory Study on Inference

## Quick Facts
- arXiv ID: 2307.11434
- Source URL: https://arxiv.org/abs/2307.11434
- Reference count: 21
- Primary result: Batching strategies affect energy consumption differently across neural network architectures, with some benefiting from larger batches while others perform better with smaller ones.

## Executive Summary
This paper investigates the energy consumption implications of batching strategies during neural network inference, addressing a critical gap in sustainable AI research. Through an empirical study of five state-of-the-art computer vision models, the authors demonstrate that the relationship between batch size and energy efficiency is highly architecture-dependent. The study reveals that while some networks like AlexNet and ConvNext achieve better energy efficiency with larger batches, others like DenseNet perform more efficiently with smaller batches. These findings challenge the conventional wisdom that larger batches universally improve computational efficiency and highlight the need for architecture-aware batching strategies in green AI deployment.

## Method Summary
The authors developed a testbed to simulate incoming image classification requests at various frequencies, measuring energy usage and maximum response time for different batching strategies across five pre-trained PyTorch models (AlexNet, DenseNet, ShuffleNetV2, VisionTransformer, ConvNext). The testbed used a GeForce GTX-1080 GPU and implemented greedy batching alongside fixed batch sizes of 16, 32, 64, and 128. Power consumption was monitored via NVIDIA System Management Interface (nvidia-smi) at 10ms intervals, with each configuration processing 213 requests. The study also presented a decade-long timeline comparing energy efficiency and accuracy evolution across different neural network architectures.

## Key Results
- Energy consumption per image varies significantly across models: AlexNet and ConvNext benefit from larger batches, DenseNet from smaller batches, while ShuffleNetV2 shows no significant difference
- Maximum response time increases with batch size, particularly at high request frequencies where GPU processing becomes a bottleneck
- Energy consumption per image has increased substantially over the past decade while accuracy gains have been more modest

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Batch size directly modulates GPU utilization efficiency, influencing energy per inference
- Mechanism: GPUs operate more efficiently when processing larger batches due to better utilization of parallel compute resources. Larger batches reduce idle periods and amortize fixed overhead costs per batch. However, oversized batches can cause queueing delays and increase maximum wait time
- Core assumption: GPU power draw during compute phases is relatively constant regardless of batch size, and energy savings come from reduced idle time and improved parallel efficiency
- Evidence anchors:
  - [abstract] "the effect of input batching on the energy consumption and response times"
  - [section] "the GPU always exerts some amount of power even when idle"
  - [corpus] "Parallel computing resources on the platforms, such as graphics processing units (GPUs), have higher computational and energy efficiency when operating with larger batch sizes"

### Mechanism 2
- Claim: Different neural network architectures exhibit distinct sensitivity to batch size changes
- Mechanism: Network architectures with dense connectivity (e.g., DenseNet) or high memory access overhead may perform less efficiently with large batches due to memory bandwidth bottlenecks. Conversely, simpler convolutional networks (e.g., AlexNet, ConvNext) benefit more from larger batch sizes due to better parallel compute utilization
- Core assumption: The internal architecture of each model (number of layers, connectivity patterns, memory access patterns) determines how well it scales with batch size
- Evidence anchors:
  - [section] "For another interesting observation we direct our attention to the scatterplot for ShuffleNetV2 in Figure 2c. We find that there is virtually no horizontal spread in the points, which suggests that the model's efficiency does not depend on the batch size."
  - [section] "ShuffleNetV2 and DenseNet are the anomalies in this experimentation. For ShuffleNet, we find that the energy consumption is completely invariant from the size of the batches."
  - [corpus] "each network responds to batching differently"

### Mechanism 3
- Claim: Incoming request frequency modulates the optimal batch size due to queuing dynamics
- Mechanism: At low request frequencies, small or greedy batching minimizes wait times without significant energy penalty. At high frequencies, larger batch sizes reduce energy per image by reducing idle time, even if wait times increase slightly. The optimal batch size balances energy efficiency gains against latency requirements
- Core assumption: The system can absorb variable batch sizes without causing memory overflow or significant reordering delays
- Evidence anchors:
  - [section] "For the high-frequency simulation, we observe a shift in this trend. Since the GPU is not quite able to process all the images as soon as they enter the queue, a bottleneck is formed."
  - [section] "the batch size of 64 is the most optimal with regard to the maximum wait time"
  - [corpus] "For servers incorporating parallel computing resources, batching is a pivotal technique for providing efficient and economical services at scale"

## Foundational Learning

- Concept: GPU utilization and power characteristics
  - Why needed here: Understanding how GPUs consume power during compute vs idle phases is essential to interpret energy efficiency results and design batching strategies
  - Quick check question: If a GPU consumes 100W during active compute and 30W when idle, what is the minimum batch size needed to achieve at least 80W average power consumption if each image takes 0.01s to process and requests arrive every 0.02s?

- Concept: Neural network architectural differences
  - Why needed here: Different architectures (CNNs vs transformers, dense vs sparse connections) respond differently to batching due to memory access patterns and computational parallelism
  - Quick check question: Why might a densely connected network like DenseNet be less efficient with large batches compared to a standard CNN like AlexNet?

- Concept: Queuing theory and latency trade-offs
  - Why needed here: The relationship between batch size, request frequency, and response time determines the practical feasibility of energy-efficient batching strategies
  - Quick check question: If requests arrive at 50 Hz and the GPU can process 100 images per second, what is the maximum batch size that keeps average wait time below 0.1 seconds?

## Architecture Onboarding

- Component map: Simulated request queue -> Batch assembly logic -> Neural network inference engine -> Power monitoring interface -> Result logging
- Critical path: Request generation → Queue management → Batch formation → Model inference → Power measurement → Result logging. The bottleneck typically occurs at batch formation when the queue is empty or at inference when the GPU cannot keep up with request frequency
- Design tradeoffs: Fixed batch sizes offer predictable energy consumption and latency but may underutilize resources at low frequencies. Greedy batching minimizes latency but can waste energy at high frequencies. The testbed abstracts away network training and focuses solely on inference, which limits applicability to training scenarios
- Failure signatures: Out-of-memory errors when batch size exceeds GPU capacity, excessive wait times when request frequency exceeds processing capability, and inconsistent power measurements due to thermal throttling or background processes
- First 3 experiments:
  1. Run the AlexNet model with frequencies 16, 32, 64, 128 Hz and batch sizes 16, 32, 64, 128, greedy. Verify that larger batches reduce energy per image at high frequencies while increasing wait times
  2. Repeat experiment 1 with DenseNet and observe that smaller batches are more energy-efficient, particularly at lower frequencies
  3. Test ShuffleNetV2 across all frequencies and batch sizes to confirm batch size invariance in energy consumption while monitoring wait time scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different neural network architectures respond to batching strategies in terms of energy efficiency and response time?
- Basis in paper: [explicit] The study found that each network behaves differently under varying batch sizes, with some benefiting from larger batches and others performing better with smaller ones
- Why unresolved: The study only evaluated five specific networks, and the results may not generalize to all neural network architectures
- What evidence would resolve it: Conduct similar experiments on a wider range of neural network architectures to determine if the observed trends hold true across different types of models

### Open Question 2
- Question: How does the energy efficiency of neural networks evolve over time, and what factors contribute to this trend?
- Basis in paper: [explicit] The study presented a timeline of energy efficiency and accuracy evolution over the past decade, revealing that energy consumption has increased substantially while accuracy gains have been more modest
- Why unresolved: The study only evaluated five specific networks and did not investigate the underlying factors driving the observed trends
- What evidence would resolve it: Conduct a comprehensive analysis of neural network architectures published over the past decade, examining their energy efficiency and the design principles that contribute to their performance

### Open Question 3
- Question: How can neural network design principles be optimized to improve energy efficiency without sacrificing accuracy?
- Basis in paper: [inferred] The study highlighted ShuffleNetV2 as an example of a network that achieved competitive performance while maintaining lower energy consumption, suggesting that there are design principles that can be optimized for energy efficiency
- Why unresolved: The study did not investigate the specific design principles that contribute to energy efficiency or how they can be applied to other neural network architectures
- What evidence would resolve it: Conduct research to identify the design principles that contribute to energy efficiency in neural networks and develop guidelines for incorporating these principles into the design of new architectures

## Limitations

- Narrow model scope: The study examines only five image classification architectures, limiting generalizability to other domains and model types
- Hardware specificity: Results are based on a single GPU model (GeForce GTX-1080), which may not represent behavior on other GPUs or AI accelerators
- Simplified workload: The use of only three repeated images does not reflect real-world inference scenarios with diverse inputs and data-dependent computational patterns

## Confidence

- High Confidence: The observation that batching effects vary significantly across different neural network architectures is well-supported by the data and consistent with known architectural differences
- Medium Confidence: The conclusion that energy efficiency and accuracy have diverged over the past decade is supported by the presented timeline, though the methodology for constructing this comparison introduces some uncertainty
- Low Confidence: The assertion that greedy batching always minimizes wait times may not hold under all conditions, particularly when considering the overhead of batch assembly and potential reordering delays

## Next Checks

1. Cross-Architecture Validation: Extend the study to include transformer-based models for language tasks (e.g., BERT, GPT variants) and recommendation systems (e.g., DLRM) to assess whether batching benefits generalize beyond computer vision

2. Hardware Diversity Testing: Replicate the experiments on at least two additional GPU architectures (e.g., RTX series and data center GPUs) to determine if the observed batching patterns are hardware-dependent or more universal

3. Real-World Workload Simulation: Replace the three-image cycle with a diverse dataset representative of real inference workloads, measuring energy consumption and latency under varying request patterns and data distributions