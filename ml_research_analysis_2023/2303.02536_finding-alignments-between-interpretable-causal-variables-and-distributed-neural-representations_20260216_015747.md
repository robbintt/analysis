---
ver: rpa2
title: Finding Alignments Between Interpretable Causal Variables and Distributed Neural
  Representations
arxiv_id: '2303.02536'
source_url: https://arxiv.org/abs/2303.02536
tags:
- causal
- high-level
- neural
- alignment
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of finding faithful and interpretable
  alignments between causal variables in symbolic models and distributed neural representations.
  The authors introduce distributed alignment search (DAS), which learns a rotation
  matrix to align high-level causal variables with subspaces of neural representations
  via gradient descent, allowing for soft interventions that reveal distributed structure.
---

# Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations

## Quick Facts
- **arXiv ID**: 2303.02536
- **Source URL**: https://arxiv.org/abs/2303.02536
- **Reference count**: 40
- **Primary result**: Gradient-based distributed alignment search (DAS) achieves near-perfect interchange intervention accuracy (100% for hierarchical equality, 99%+ for NLI) while revealing distributed neural representations of causal variables.

## Executive Summary
This paper introduces Distributed Alignment Search (DAS), a method for finding faithful and interpretable alignments between high-level causal variables in symbolic models and distributed neural representations. DAS learns an orthogonal rotation matrix via gradient descent to align subspaces of neural representations with interpretable causal variables, enabling soft interventions that reveal distributed structure. The method outperforms prior brute-force search approaches and achieves near-perfect interchange intervention accuracy on tasks including hierarchical equality and monotonicity NLI, while also testing whether relations are truly abstracted from their arguments.

## Method Summary
DAS addresses the challenge of aligning interpretable causal variables with distributed neural representations by learning orthogonal rotation matrices through gradient descent optimization. The method uses distributed (soft) interventions that rotate neural representations into alternative bases, allowing individual neurons to participate in multiple conceptual roles. The core training objective maximizes interchange intervention accuracy by minimizing the cross-entropy loss between counterfactual outputs under distributed interventions and the expected outputs under the high-level model. DAS is evaluated on synthetic hierarchical equality tasks and monotonicity NLI using fine-tuned BERT models.

## Key Results
- DAS achieves 100% interchange intervention accuracy on hierarchical equality tasks
- DAS achieves >99% interchange intervention accuracy on monotonicity NLI tasks
- DAS outperforms brute-force search and localist intervention baselines in both accuracy and efficiency
- DAS successfully tests whether learned representations are decomposable into entity representations or truly abstracted

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Gradient descent can replace brute-force search to find optimal alignments between high-level causal variables and distributed neural representations.
- **Mechanism**: DAS uses a differentiable training objective that quantifies the distance between counterfactual outputs under distributed interventions and expected outputs under the high-level model. Optimizing an orthogonal rotation matrix via SGD finds alignments without enumerating all possible discrete alignments.
- **Core assumption**: The alignment space is continuous enough for gradient descent to navigate, and the differentiable loss surface is not too noisy for SGD to converge.
- **Evidence anchors**: Definition 4 explicitly defines the distributed interchange intervention training objective used for SGD optimization; experimental results show DAS achieves near-perfect IIA.

### Mechanism 2
- **Claim**: Distributed (soft) interventions reveal hidden interpretable structure in neural representations that localist interventions miss.
- **Mechanism**: By rotating neural representations into a non-standard basis via an orthogonal matrix, DAS enables interchange interventions on orthogonal subspaces. This allows individual neurons to participate in multiple conceptual roles, uncovering distributed encodings invisible in the standard neuron-aligned basis.
- **Core assumption**: Neural representations contain meaningful structure along non-standard basis directions.
- **Evidence anchors**: Section 3.4 shows that viewing neural representations through alternative bases can reveal interpretable dimensions; example demonstrates rotation localizing information into interpretable dimensions.

### Mechanism 3
- **Claim**: Interchange intervention accuracy (IIA) provides a graded measure of causal abstraction quality between high-level models and neural networks.
- **Mechanism**: IIA measures the proportion of aligned interchange interventions that yield equivalent effects at high and low levels. DAS maximizes IIA, indicating that the learned distributed alignment yields faithful abstractions.
- **Core assumption**: IIA is a meaningful proxy for causal abstraction fidelity, grounded in approximate causal abstraction theory.
- **Evidence anchors**: Definition 5 formally defines distributed interchange intervention accuracy; experimental results show DAS achieves near-perfect IIA supporting its validity.

## Foundational Learning

- **Concept**: Causal abstraction theory
  - **Why needed here**: The paper relies on causal abstraction to formalize when a high-level causal model is a faithful simplification of a neural network. Understanding this framework is essential to grasp why DAS optimizes alignments and interventions.
  - **Quick check question**: What is the key condition for a high-level model H to be a constructive abstraction of a low-level model L under alignment (Π, τ)?
    - **Answer**: For every low-level input setting x and low-level intervention I←i, τ(LI←ipx)) = Hτ(I←i)(τ(x)).

- **Concept**: Interchange interventions
  - **Why needed here**: DAS uses distributed interchange interventions to test whether aligned high-level and low-level variables have equivalent causal effects. This is the operational test for causal abstraction.
  - **Quick check question**: How does a distributed interchange intervention differ from a localist one?
    - **Answer**: A distributed intervention rotates the representation into a new basis, intervenes on orthogonal subspaces, then rotates back, allowing soft interventions where the base input is partially preserved.

- **Concept**: Distributed representations
  - **Why needed here**: DAS exploits the idea that individual neurons can play multiple conceptual roles, and that interpretable structure may be visible only in non-standard bases. This motivates the use of rotation matrices and distributed interventions.
  - **Quick check question**: Why might a representation of a relation (e.g., "w=x") not be decomposable into representations of its arguments (w and x)?
    - **Answer**: If the relation is truly abstracted from its arguments, its distributed representation may not contain separable sub-representations of the individual entities.

## Architecture Onboarding

- **Component map**: Low-level model (neural network) -> Alignment (rotation matrix Rθ) -> High-level model (symbolic causal model)
- **Critical path**: 1) Train neural network to solve task, 2) Create interchange intervention training dataset, 3) Optimize rotation matrix via SGD on distributed interchange intervention training objective, 4) Evaluate IIA for learned alignment, 5) Compare with brute-force and localist baselines, 6) Test for decomposability of learned representations
- **Design tradeoffs**: Rotation matrix vs. localist alignment (rotation matrix allows distributed representations but adds optimization complexity); intervention size (larger sizes capture more structure but increase parameter count); layer selection (intervening at different layers yields different alignments)
- **Failure signatures**: IIA plateaus below 100% (suboptimal alignment or incorrect high-level model); rotation matrix degenerates (orthogonality constraints violated); decomposition test fails (representation may be truly abstract or decomposition method inadequate)
- **First 3 experiments**: 1) Train feedforward network on hierarchical equality and apply DAS to test learning of both equality relations as intermediate variables, 2) Fine-tune BERT on monotonicity NLI and use DAS to find distributed representations of negation and lexical entailment, 3) Compare DAS IIA with brute-force and localist baselines on both tasks

## Open Questions the Paper Calls Out

- **Open Question 1**: How do non-linear transformations compare to orthogonal rotations for distributed alignment search in capturing distributed neural representations?
  - **Basis in paper**: [inferred] The paper mentions that while orthogonal matrices are used for computational tractability, non-linear transformations like normalizing flows or invertible neural networks might be more suitable for capturing representations in non-linear sub-manifolds.
  - **Why unresolved**: The paper only speculates about the potential benefits of non-linear transformations without empirical comparison to orthogonal rotations.
  - **What evidence would resolve it**: Experiments comparing DAS with orthogonal rotations versus DAS with non-linear transformations (e.g., normalizing flows) on tasks where representations are known to be non-linear, measuring interchange intervention accuracy and interpretability.

- **Open Question 2**: What is the impact of initialization schemes on the stability and quality of learned distributed alignments?
  - **Basis in paper**: [explicit] The paper acknowledges that the initialization of the orthogonal matrix is important for finding local optima but only uses random seeds without exploring different initialization strategies.
  - **Why unresolved**: The paper uses a single default initialization from PyTorch without systematic comparison of alternative initialization methods.
  - **What evidence would resolve it**: Empirical studies comparing different orthogonal matrix initialization strategies (e.g., random, SVD-based, spectral initialization) across multiple tasks, measuring both stability (variance across seeds) and final interchange intervention accuracy.

- **Open Question 3**: Can DAS scale effectively to foundation models with very large hidden dimensions through low-rank approximations or subspace constraints?
  - **Basis in paper**: [explicit] The paper acknowledges that for large models like BERT with 768-dimensional representations, full rotation matrices become computationally intractable, and suggests low-rank approximations as a potential solution.
  - **Why unresolved**: The paper only discusses the theoretical possibility of low-rank approximations without implementing or testing them on large-scale models.
  - **What evidence would resolve it**: Implementation of DAS with low-rank rotation matrices (e.g., rank-32 or rank-64) on large language models, measuring whether sufficient interchange intervention accuracy can be achieved with dramatically fewer parameters than full rotations.

## Limitations

- **Generalization uncertainty**: The method's effectiveness on highly structured synthetic datasets may not generalize to more complex, naturalistic reasoning tasks.
- **Layer selection ambiguity**: The paper does not systematically justify which layers to intervene on, leaving open whether alignments found at one layer reflect deeper causal mechanisms.
- **IIA as proxy validity**: The theoretical grounding of interchange intervention accuracy as a reliable proxy for downstream behavioral fidelity remains underexplored.

## Confidence

- **High**: The claim that DAS can replace brute-force search with gradient descent for finding alignments. Supported by clear algorithmic descriptions and empirical results showing improved efficiency and accuracy.
- **Medium**: The claim that distributed interventions reveal hidden interpretable structure missed by localist interventions. While the mechanism is plausible and supported by toy examples, the broader applicability to diverse neural architectures is uncertain.
- **Low**: The claim that DAS definitively determines whether relations are abstracted or decomposable. The decomposition tests provide suggestive evidence but may not conclusively rule out alternative explanations.

## Next Checks

1. **Generalization to Non-Synthetic Tasks**: Apply DAS to a naturalistic reasoning task (e.g., commonsense QA or visual reasoning) to test whether distributed alignments and IIA remain meaningful outside structured datasets.
2. **Layer Sensitivity Analysis**: Systematically vary the intervention layer in DAS and measure how IIA and interpretability change, to identify the most causally relevant layers.
3. **Robustness to High-Level Model Misspecification**: Test whether DAS can detect when the proposed high-level causal model is incorrect or incomplete, by measuring IIA degradation or unexpected decomposition patterns.