---
ver: rpa2
title: Memorization Through the Lens of Curvature of Loss Function Around Samples
arxiv_id: '2307.05831'
source_url: https://arxiv.org/abs/2307.05831
tags:
- curvature
- samples
- training
- memorization
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using the curvature of the loss function around
  training samples as a measure of memorization. The curvature is estimated by averaging
  the squared Hessian-vector product over training epochs.
---

# Memorization Through the Lens of Curvature of Loss Function Around Samples

## Quick Facts
- arXiv ID: 2307.05831
- Source URL: https://arxiv.org/abs/2307.05831
- Authors: 
- Reference count: 20
- This paper proposes using the curvature of the loss function around training samples as a measure of memorization, achieving high AUROC scores (above 0.92) for identifying mislabeled examples.

## Executive Summary
This paper introduces a novel approach to measure memorization in neural networks by analyzing the curvature of the loss function around training samples. The method computes the average squared Hessian-vector product over training epochs to estimate curvature. Experiments across multiple datasets demonstrate that high curvature samples correspond to mislabeled, long-tailed, or conflicting examples. The approach successfully identifies a novel failure mode in CIFAR100: duplicated images with different labels. When synthetic label noise is introduced, the curvature metric achieves strong AUROC scores for detecting corrupted samples, while requiring only training a single network.

## Method Summary
The method estimates curvature by averaging the squared Hessian-vector product over training epochs. Using Hutchinson's trace estimator with Rademacher vectors, the approach computes the trace of the squared Hessian efficiently without explicitly forming the full matrix. The curvature is calculated as the average of these trace estimates across all training epochs. The technique identifies potential mislabeled or conflicting samples by sorting them based on their curvature scores, with higher curvature indicating stronger memorization. The method was evaluated on MNIST, FashionMNIST, CIFAR10, and CIFAR100 using a modified ResNet18 architecture.

## Key Results
- High curvature samples visually correspond to mislabeled, long-tailed, multiple class, or otherwise conflicting examples
- The method discovers duplicated images with different labels as a novel failure mode in CIFAR100
- When synthetic label noise is introduced, sorting by curvature yields high AUROC scores (above 0.92 on most datasets) for identifying mislabeled examples
- Curvature metric correlates well with memorization scores from an independent baseline method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curvature of the loss function around a training sample, averaged over training epochs, correlates with how much that sample is memorized by the model.
- Mechanism: Memorization leads to a complex decision boundary around a sample. The Hessian of the loss with respect to the input (H) captures the local curvature. High curvature means the model's loss is very sensitive to small perturbations of the input, which happens when the model is fitting very closely to that specific sample (i.e., memorizing it).
- Core assumption: A memorized sample will cause the model to create a highly curved, non-linear decision boundary around it, leading to high eigenvalues in the Hessian of the loss.
- Evidence anchors:
  - [abstract]: "We propose using the curvature of loss function around each training sample, averaged over training epochs, as a measure of memorization of the sample."
  - [section 3.1]: The derivation shows curvature is estimated by the trace of the squared Hessian, which equals the sum of squared eigenvalues, and is averaged over epochs.
  - [corpus]: The related paper "From Memorization to Reasoning in the Spectrum of Loss Curvature" explicitly connects memorization to loss curvature.
- Break condition: If the model uses aggressive regularization (e.g., high weight decay), the curvature around memorized samples may decrease, weakening the correlation.

### Mechanism 2
- Claim: High curvature samples are more likely to be mislabeled, rare, or ambiguous examples in the dataset.
- Mechanism: The model struggles to fit these ambiguous samples cleanly, so it creates a highly curved boundary around them to minimize loss, even if the label is incorrect or the sample is atypical.
- Core assumption: The loss surface becomes locally complex near samples that the model cannot easily categorize, which manifests as high curvature.
- Evidence anchors:
  - [section 4.1]: "We visualize the highest curvature samples... and note that they are made of mislabeled, long-tailed, multiple class, or otherwise conflicting samples that are not clearly representative of their labels."
  - [section 3.1]: The method explicitly targets the Hessian of the loss w.r.t. the input to detect these boundary conflicts.
  - [corpus]: "Unveiling Privacy, Memorization, and Input Curvature Links" connects input curvature to memorization and privacy leakage.
- Break condition: If the dataset is very clean and balanced, the distinction between high and low curvature samples may become less pronounced.

### Mechanism 3
- Claim: Curvature analysis can identify duplicated images with conflicting labels (same image, different class) that other memorization metrics miss.
- Mechanism: Duplicated samples with different labels create a severe boundary conflict. The model must curve the decision boundary sharply around each instance to satisfy both labels, leading to very high curvature for both copies.
- Core assumption: The model cannot resolve the conflict by averaging, so it memorizes both conflicting versions with high curvature.
- Evidence anchors:
  - [section 4.4]: "Using our method, we catch a failure mode on CIFAR100 that is to the best of our knowledge, unobserved until now, that of duplicated images with different labels."
  - [section 4.3]: The method found 36 out of the top 60 high-curvature samples were duplicated with different labels.
  - [corpus]: The FMR-based neighbor search found related work on memorization and curvature, supporting the plausibility of this mechanism.
- Break condition: If the model uses strong regularization, it may avoid memorizing the conflicting duplicates, reducing the curvature signal.

## Foundational Learning

- Concept: Hessian of a scalar function and its relation to curvature
  - Why needed here: The method relies on computing the Hessian of the loss with respect to the input to measure curvature.
  - Quick check question: What does the trace of the squared Hessian represent in terms of eigenvalues?
- Concept: Trace estimator (Hutchinson's method)
  - Why needed here: Computing the full Hessian is infeasible for high-dimensional inputs, so an efficient estimator is required.
  - Quick check question: How does Hutchinson's estimator approximate the trace of a matrix using random vectors?
- Concept: Memorization vs. generalization in over-parameterized models
  - Why needed here: The method exploits overfitting to identify memorized samples, so understanding the distinction is crucial.
  - Quick check question: What is the key difference between a model that memorizes and one that generalizes on a given sample?

## Architecture Onboarding

- Component map:
  - Data loader -> Model (ResNet18) -> Loss function (Cross-entropy) -> Curvature estimator (Hutchinson's trick) -> AUROC calculator
- Critical path:
  1. Load batch
  2. Forward pass
  3. Compute loss
  4. Apply Pearlmutter's trick to get Hv
  5. Accumulate curvature estimate
  6. Repeat over epochs
- Design tradeoffs:
  - n (number of Rademacher vectors): Higher n → more accurate curvature, more compute
  - h (perturbation magnitude): Too small → numerical instability; too large → bias in curvature estimate
  - Weight decay: On → smoother boundaries, less memorization; off → sharper memorization, possible discovery of conflicting duplicates
- Failure signatures:
  - AUROC near 0.5: Curvature not distinguishing corrupted from clean samples
  - Very low curvature for all samples: Possible numerical issue or insufficient overfitting
  - High curvature but no clear pattern in high-curvature samples: Possible dataset corruption or label noise
- First 3 experiments:
  1. Train ResNet18 on MNIST without weight decay, compute curvature, visualize highest curvature samples to check for mislabeled or ambiguous examples.
  2. Introduce synthetic label noise (e.g., 2% corruption) on CIFAR10, train, and measure AUROC of curvature ranking against corrupted samples.
  3. Train on CIFAR100 with and without weight decay, compare curvature of duplicated images with conflicting labels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the curvature metric compare to other memorization detection methods (e.g., influence functions, adversarial examples) in terms of accuracy and computational efficiency across different datasets?
- Basis in paper: [explicit] The paper compares curvature to confidence scores and Confident Learning (CL) on MNIST, FashionMNIST, CIFAR10, and CIFAR100 datasets, showing curvature achieves high AUROC scores.
- Why unresolved: While the paper demonstrates good performance, it only compares against a limited set of methods and datasets. A comprehensive comparison with other memorization detection techniques like influence functions or adversarial examples across diverse datasets is missing.
- What evidence would resolve it: A systematic study comparing curvature to other memorization detection methods on a wider range of datasets and using different network architectures, measuring both accuracy and computational cost.

### Open Question 2
- Question: What is the relationship between the choice of hyperparameters (h and n) in the curvature estimator and the accuracy of memorization detection?
- Basis in paper: [explicit] The paper mentions that h and n are hyperparameters in the curvature estimator and provides a range for h (10^-2 to 10^-4) and a chosen value for n (10).
- Why unresolved: The paper only explores a limited range of hyperparameters and does not provide a systematic analysis of how different choices affect the accuracy of memorization detection.
- What evidence would resolve it: An ablation study investigating the impact of different values of h and n on the accuracy of memorization detection across various datasets and network architectures.

### Open Question 3
- Question: How does the curvature metric generalize to other types of data beyond images, such as text or tabular data?
- Basis in paper: [inferred] The paper focuses on image datasets (MNIST, FashionMNIST, CIFAR10, CIFAR100) and does not explore the applicability of the curvature metric to other data types.
- Why unresolved: The paper does not provide any evidence or discussion on the generalization of the curvature metric to non-image data.
- What evidence would resolve it: Applying the curvature metric to different types of data (e.g., text, tabular data) and evaluating its effectiveness in detecting memorization compared to existing methods for those data types.

## Limitations
- The method's effectiveness depends on sufficient overfitting; with strong regularization, the curvature signal may diminish, potentially missing memorization events
- The computational cost scales linearly with the number of Rademacher vectors used in the trace estimator, making it expensive for large datasets
- The method assumes that high curvature consistently indicates memorization, but other phenomena (like adversarial examples) might also produce high curvature

## Confidence
- High confidence: The curvature method effectively identifies mislabeled and ambiguous samples, as evidenced by strong AUROC scores and qualitative visualizations
- Medium confidence: The discovery of duplicated images with conflicting labels represents a novel failure mode, though further investigation across more datasets is warranted
- Medium confidence: The correlation between curvature scores and independent memorization baselines, while strong, needs validation across more model architectures

## Next Checks
1. Test the curvature method on datasets with varying levels of label noise (0% to 20%) to establish the robustness threshold where the method begins to degrade
2. Apply the method to transformer-based models (like ViT) to verify if the curvature-memorization relationship holds across different architectures
3. Conduct ablation studies varying weight decay, learning rate, and training duration to identify the optimal conditions for the curvature method to detect memorization accurately