---
ver: rpa2
title: Local Environment Poisoning Attacks on Federated Reinforcement Learning
arxiv_id: '2303.02725'
source_url: https://arxiv.org/abs/2303.02725
tags:
- poisoning
- local
- learning
- policy
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies poisoning attacks on Federated Reinforcement
  Learning (FRL), where a small number of malicious agents can manipulate their local
  environments to mislead the trained global policy. A general optimization framework
  is proposed to characterize FRL poisoning under limited budget constraints.
---

# Local Environment Poisoning Attacks on Federated Reinforcement Learning

## Quick Facts
- arXiv ID: 2303.02725
- Source URL: https://arxiv.org/abs/2303.02725
- Reference count: 17
- Key outcome: A small number of malicious agents can significantly reduce the mean episode reward of the global policy in FRL systems by manipulating their local environments.

## Executive Summary
This paper investigates poisoning attacks on Federated Reinforcement Learning (FRL) systems, where malicious agents manipulate their local environments to mislead the trained global policy. The authors propose a general optimization framework for FRL poisoning under limited budget constraints and design two practical poisoning methods: one for policy-based FRL and another for actor-critic algorithms. Experiments on OpenAI Gym environments demonstrate that even a few malicious agents can significantly reduce the mean episode reward of the global policy. The paper also proposes a defense mechanism based on agent credit scoring, which shows improvement in some cases but remains vulnerable under complex environments.

## Method Summary
The paper proposes a poisoning framework for FRL where malicious agents manipulate their local observations (rewards, states, or actions) within a bounded budget. For policy-based FRL, attackers poison rewards using gradient-based optimization. For actor-critic algorithms, attackers train paired public and private critics—the public critic communicates poisoned updates to the server while the private critic maintains ground-truth values. The attack is formulated as a sequential bi-level optimization problem. A defense mechanism assigns credit scores to agents based on their policy performance on a test environment, weighting aggregation by these scores.

## Key Results
- Even a small number of malicious agents (e.g., 1 out of 10) can significantly reduce the mean episode reward of the global policy in FRL systems.
- The paired public/private critic attack effectively manipulates the global model in actor-critic FRL by maintaining plausible behavior while degrading overall performance.
- The credit-based defense mechanism improves performance in some cases but shows significant gaps compared to clean baselines under complex environments.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A small number of malicious agents can significantly reduce the mean episode reward of the global policy in FRL systems by manipulating their local environments.
- Mechanism: Attackers control a small subset of agents (M ⊂ [n]) and poison their local observations during training by perturbing rewards, states, or actions within a bounded budget. These poisoned updates are aggregated with clean updates, distorting the global model.
- Core assumption: The aggregation mechanism treats all agent updates equally (e.g., simple averaging), so poisoned updates from even a small proportion of agents can shift the global model toward suboptimal policies.
- Evidence anchors:
  - [abstract] states "even a few malicious agents can significantly reduce the mean episode reward of the global policy"
  - [section 3] describes the poisoning optimization problem with constraints showing how manipulated observations are incorporated into the federated aggregation
  - [corpus] shows related work on poisoning attacks in FL, establishing that aggregation-based poisoning is feasible
- Break condition: If the aggregation algorithm weights agent contributions by performance (as in the defense mechanism), or if the proportion of malicious agents is too small relative to the total number of agents, the poisoning effect diminishes.

### Mechanism 2
- Claim: Training paired public and private critics allows attackers to effectively manipulate the global model in actor-critic FRL.
- Mechanism: The attacker trains a public critic with poisoned rewards to communicate with the server, while maintaining a private critic with ground-truth rewards to make optimal poisoning decisions. This enables targeted reward manipulation that reduces the global policy's effectiveness.
- Core assumption: The attacker can maintain two separate critic models without detection, and the public critic's poisoned training still provides useful signal for the global aggregation.
- Evidence anchors:
  - [section 4.1] describes training public and private critics to manipulate the global model during training
  - [abstract] mentions extending the framework to actor-critic by "training a pair of private and public critics"
  - [corpus] lacks direct evidence for this specific dual-critic mechanism, indicating this is a novel contribution
- Break condition: If the server can detect discrepancies between public and private critic updates, or if the poisoned critic fails to provide meaningful signal, the attack becomes ineffective.

### Mechanism 3
- Claim: The defense mechanism based on agent credit scoring can mitigate poisoning attacks but remains vulnerable under complex environments.
- Mechanism: The server tests each received policy and assigns credit scores based on performance. Policies are then weighted in aggregation by these scores, reducing the influence of poorly performing (potentially poisoned) policies.
- Core assumption: Malicious agents' policies will perform poorly on the server's test environment, resulting in low credit scores that reduce their influence in aggregation.
- Evidence anchors:
  - [section 4.3] describes the defense mechanism using performance-based credit scoring
  - [abstract] states "A defense mechanism based on agent credit scoring is also proposed, which improves performance in some cases but remains vulnerable under complex environments"
  - [section 5] shows experimental results where defense improves rewards but "there is still a significant gap compared to the clean baseline"
- Break condition: If malicious agents can craft policies that perform well on the server's test environment while still being harmful in deployment, or if the test environment doesn't match the deployment environment, the defense fails.

## Foundational Learning

- Concept: Federated Learning (FL) aggregation mechanisms
  - Why needed here: Understanding how agent updates are combined is crucial for both attack effectiveness and defense design
  - Quick check question: What happens to the global model when a small number of agents submit poisoned updates under simple averaging aggregation?

- Concept: Actor-Critic algorithms and the role of critics in policy learning
  - Why needed here: The poisoning mechanism exploits the critic's role in estimating value functions to manipulate policy updates
  - Quick check question: How does corrupting the critic's value estimates affect the policy update in actor-critic methods?

- Concept: Optimization under constraints and bi-level optimization
  - Why needed here: The poisoning problem is formulated as a sequential bi-level optimization where the attacker optimizes over observation perturbations subject to budget constraints
  - Quick check question: In a bi-level optimization, what are the outer and inner problems in the context of FRL poisoning?

## Architecture Onboarding

- Component map:
  - Coordinator: Aggregates local models, broadcasts global model, runs defense tests
  - Clean Agents: Train local policies using observed states/actions/rewards, send updates to coordinator
  - Malicious Agents: Train paired public/private critics, poison observations within budget, send updates to coordinator
  - Environment: Provides state transitions and rewards (potentially manipulated by attackers)

- Critical path:
  1. Coordinator broadcasts global model to all agents
  2. Each agent (clean or malicious) performs L local training steps
  3. Agents send their updated models to coordinator
  4. Coordinator (with defense) aggregates models and broadcasts new global model
  5. Repeat for T rounds

- Design tradeoffs:
  - Simple averaging aggregation vs. performance-weighted aggregation (defense)
  - Single critic vs. paired public/private critics for attack sophistication
  - Fixed vs. adaptive attack budgets for poisoning effectiveness
  - Centralized testing vs. distributed validation for defense

- Failure signatures:
  - Rapid degradation in mean episode reward despite increasing training rounds
  - Policies that perform well on training environments but poorly on test environments
  - Discrepancy between public and private critic updates from same agent
  - Inconsistent credit scores across multiple test episodes

- First 3 experiments:
  1. Run clean FRL training on CartPole with VPG to establish baseline performance
  2. Introduce a single malicious agent with reward poisoning and observe degradation in mean episode reward
  3. Implement defense mechanism and measure improvement in poisoned system performance compared to un-defended poisoned system

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed defense mechanism perform against targeted poisoning attacks in FRL systems?
- Basis in paper: [inferred] The paper only discusses untargeted poisoning attacks and mentions that the defense mechanism can improve rewards in some cases but may not always be successful in more complex environments.
- Why unresolved: The paper does not provide any results or analysis on the performance of the defense mechanism against targeted poisoning attacks, which could have a different impact on the FRL system compared to untargeted attacks.
- What evidence would resolve it: Experimental results comparing the performance of the defense mechanism against both targeted and untargeted poisoning attacks in various FRL environments and system sizes.

### Open Question 2
- Question: How does the performance of the proposed poisoning method change when applied to multi-task FRL systems?
- Basis in paper: [inferred] The paper only considers single-task FRL systems and does not discuss the applicability or performance of the poisoning method in multi-task scenarios.
- Why unresolved: Multi-task FRL systems are more complex and may have different vulnerabilities compared to single-task systems, which could affect the effectiveness of the poisoning method.
- What evidence would resolve it: Experimental results evaluating the performance of the poisoning method in multi-task FRL systems with varying numbers of tasks and agent types.

### Open Question 3
- Question: What are the potential impacts of using different aggregation algorithms in the FRL system on the effectiveness of the proposed poisoning method?
- Basis in paper: [explicit] The paper mentions that the aggregation algorithm used in the experiments is a conventional federated framework where the central server aggregates the models submitted by local agents by taking the average at the end of each communication round.
- Why unresolved: Different aggregation algorithms may have different effects on the global model and could potentially influence the success of the poisoning attack.
- What evidence would resolve it: Experimental results comparing the performance of the poisoning method using various aggregation algorithms (e.g., weighted averaging, Krum, or coordinate-wise median) in FRL systems with different sizes and environments.

## Limitations
- Specific hyperparameters for VPG and PPO implementations are not provided, which could affect attack effectiveness and reproducibility
- The paper lacks detailed experimental results for individual environments, making it difficult to assess attack success rates across different task complexities
- The defense mechanism's vulnerability under complex environments is asserted but not quantified with specific metrics or failure conditions

## Confidence
- High confidence: General feasibility of FRL poisoning through local environment manipulation
- Medium confidence: Dual-critic attack mechanism effectiveness
- Medium confidence: Defense mechanism's practical effectiveness

## Next Checks
1. **Reproduce baseline results**: Implement clean FRL training on CartPole with VPG to establish baseline mean episode reward, then introduce a single malicious agent with reward poisoning (ϵ = 0.1) to verify the claimed degradation in performance
2. **Test dual-critic attack**: Implement the paired public/private critic mechanism on a simple environment (CartPole) and measure whether the public critic's poisoned updates successfully reduce the global policy's effectiveness while maintaining plausible behavior
3. **Evaluate defense robustness**: Run experiments comparing the credit-based defense against both simple reward poisoning and dual-critic attacks across multiple environments, measuring the gap between defended performance and clean baseline to quantify vulnerability levels