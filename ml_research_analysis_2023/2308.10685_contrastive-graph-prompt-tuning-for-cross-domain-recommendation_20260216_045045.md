---
ver: rpa2
title: Contrastive Graph Prompt-tuning for Cross-domain Recommendation
arxiv_id: '2308.10685'
source_url: https://arxiv.org/abs/2308.10685
tags:
- prompts
- pgprec
- graph
- recommendation
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses data sparsity in recommender systems through
  cross-domain knowledge transfer, using a prompt-tuning approach to improve efficiency
  and effectiveness. The proposed Personalised Graph Prompt-based Recommendation (PGPRec)
  framework introduces personalised graph prompts, including both hard (neighbouring-items)
  and soft (randomly initialised vectors) components, to enhance user/item representations.
---

# Contrastive Graph Prompt-tuning for Cross-domain Recommendation

## Quick Facts
- arXiv ID: 2308.10685
- Source URL: https://arxiv.org/abs/2308.10685
- Reference count: 40
- Key outcome: Reduces tuned parameters by up to 74% while achieving competitive performance and 11.41% cold-start improvement

## Executive Summary
This paper addresses data sparsity in recommender systems through cross-domain knowledge transfer using a prompt-tuning approach. The proposed Personalised Graph Prompt-based Recommendation (PGPRec) framework introduces personalised graph prompts to enhance user/item representations while tuning only a small subset of parameters. By combining hard prompts (neighbouring-items) and soft prompts (randomly initialised vectors) with contrastive learning, the framework achieves efficient transfer of pre-trained knowledge to target domains. Experiments on Amazon Review datasets demonstrate significant parameter reduction with competitive performance, particularly excelling in cold-start scenarios.

## Method Summary
PGPRec employs a two-phase approach: pre-training and fine-tuning. During pre-training, a graph encoder (LightGCN) learns domain-invariant representations using contrastive learning with edge dropout augmentation. In the fine-tuning phase, personalised graph prompts are generated for each user in the target domain - hard prompts derived from neighbouring-items metadata and soft prompts as random vectors. The model then fine-tunes using a joint loss combining BPR and contrastive objectives, updating only prompt parameters while freezing the pre-trained encoder. This approach achieves efficient cross-domain transfer while maintaining competitive recommendation performance.

## Key Results
- Reduces tuned parameters by up to 74% compared to full model fine-tuning
- Achieves competitive performance with Recall@10 and NDCG@10 metrics on cross-domain tasks
- Demonstrates 11.41% improvement in cold-start scenarios compared to strongest baseline

## Why This Works (Mechanism)

### Mechanism 1
Graph prompt-tuning efficiently transfers pre-trained knowledge by tuning only a small set of parameters instead of full model fine-tuning. Personalised graph prompts (hard and soft) are prepended to the output of a pre-trained graph encoder, allowing adaptation to the target domain without updating all parameters. Core assumption: The pre-trained graph encoder captures transferable structural knowledge that can be leveraged with minimal additional tuning. Evidence: 74% parameter reduction claim with competitive performance. Break condition: If pre-trained encoder doesn't capture transferable knowledge or prompt vectors fail to capture domain-specific patterns.

### Mechanism 2
Contrastive learning during pre-training creates more generalizable embeddings that improve cross-domain transfer. Edge dropout augmentation generates positive pairs of node representations, while contrastive loss maximizes agreement between pairs and minimizes agreement with negative pairs. Core assumption: Creating divergence in learned representations through contrastive learning makes them more adaptable to diverse target domains. Evidence: Greater generalizability through contrastive learning for pre-trained embeddings. Break condition: If contrastive learning objective doesn't create meaningful representation divergence or augmentation strategy fails to generate useful positive pairs.

### Mechanism 3
Hard prompts (neighbouring-items) are more effective than soft prompts for capturing domain-specific information. Hard prompts use actual item nodes from target domain's metadata to provide concrete contextual information, while soft prompts use randomly initialized vectors. Core assumption: Concrete item relationships captured by hard prompts provide more informative signals than randomly initialized soft prompts. Evidence: PGPRec-hard outperforms PGPRec-soft on all datasets with noticeable performance gap. Break condition: If neighboring-item relationships don't capture meaningful domain-specific patterns or metadata is insufficient.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message-passing mechanism
  - Why needed here: PGPRec builds on GNN architectures to model user-item interaction graphs and transfer knowledge between domains
  - Quick check question: What is the difference between GCN and LightGCN in terms of parameter learning?

- Concept: Contrastive Learning and InfoNCE loss
  - Why needed here: Contrastive learning is used both in pre-training to create generalizable embeddings and in fine-tuning to enhance recommendation performance
  - Quick check question: How does the InfoNCE loss encourage similar representations for positive pairs while pushing apart negative pairs?

- Concept: Cross-Domain Recommendation and knowledge transfer
  - Why needed here: PGPRec specifically addresses the challenge of transferring knowledge from source to target domains while maintaining efficiency
  - Quick check question: What are the main challenges in cross-domain recommendation that PGPRec aims to solve?

## Architecture Onboarding

- Component map: Pre-training phase (Graph encoder → Edge dropout augmentation → Contrastive loss) → Prompt generation (Hard prompts → Soft prompts) → Fine-tuning phase (Joint loss → Prompt-tuned graph encoder) → Evaluation (Recall@10, NDCG@10 metrics)

- Critical path: Pre-train graph encoder → Generate personalized prompts → Fine-tune with joint loss → Evaluate recommendation performance

- Design tradeoffs: Hard prompts provide concrete information but require domain-specific metadata; soft prompts are flexible but less informative and harder to train effectively; contrastive learning improves generalization but increases training complexity

- Failure signatures: Poor performance indicates ineffective prompt generation or insufficient transferable knowledge; slow convergence suggests joint loss weighting needs adjustment; high parameter count indicates suboptimal prompt configuration

- First 3 experiments:
  1. Compare PGPRec variants with only hard prompts vs only soft prompts to validate Mechanism 3
  2. Test different numbers of hard prompts to find optimal configuration for parameter efficiency
  3. Evaluate cold-start performance to verify Mechanism 1's effectiveness for sparse interaction scenarios

## Open Questions the Paper Calls Out

### Open Question 1
How do different graph neural network architectures (beyond LightGCN) impact the effectiveness of the PGPRec framework in cross-domain recommendation? The paper mentions PGPRec is "flexible in allowing to use other GNN-based techniques, such as GAT, NGCF or LightGCN" but only evaluates LightGCN. Empirical comparison of PGPRec performance using different GNN backbones on the same datasets would resolve this uncertainty.

### Open Question 2
What is the optimal balance between hard and soft prompt nodes across different domain pairs and dataset scales? While the paper investigates prompt combinations, it doesn't provide a systematic method for determining optimal ratios across different domain pairs or dataset characteristics. A comprehensive study mapping domain similarity, dataset size, and sparsity to optimal hard/soft prompt ratios would resolve this question.

### Open Question 3
How does PGPRec perform when applied to domains with different interaction patterns (e.g., implicit vs. explicit feedback, sequential vs. non-sequential)? The evaluation is limited to Amazon review datasets with implicit feedback, leaving uncertainty about generalizability to other interaction types and patterns. Empirical evaluation on datasets with explicit ratings, temporal dynamics, and different interaction modalities compared to state-of-the-art methods would resolve this question.

## Limitations
- Cross-domain metadata dependency: Hard prompt effectiveness relies heavily on availability and quality of item metadata across source and target domains
- Generalizability of contrastive learning gains: Specific augmentation strategy effectiveness across different domain pairs not thoroughly validated
- Parameter efficiency claims: 74% reduction depends on specific prompt configuration and baseline comparison not clearly specified

## Confidence

**High confidence**: The core mechanism of using graph prompt-tuning for parameter-efficient cross-domain transfer is well-supported by experimental results showing competitive performance with significantly fewer parameters.

**Medium confidence**: The effectiveness of hard prompts over soft prompts is demonstrated, but results are based on limited datasets with similar metadata structures that may not generalize to all recommendation scenarios.

**Medium confidence**: The contrastive learning approach for pre-training is theoretically sound, but specific gains in cross-domain generalizability need more extensive validation across diverse domain pairs.

## Next Checks

1. **Metadata robustness test**: Evaluate PGPRec performance when hard prompt metadata is partially missing or has different schemas across domains to assess real-world applicability limits.

2. **Cross-domain diversity analysis**: Test the framework across domain pairs with increasingly dissimilar item characteristics to determine where the contrastive learning pre-training stops providing meaningful transfer benefits.

3. **Ablation study on prompt configurations**: Systematically vary the number and types of hard vs soft prompts to identify optimal configurations that maximize parameter efficiency while maintaining recommendation performance.