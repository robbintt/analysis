---
ver: rpa2
title: Re-Reading Improves Reasoning in Large Language Models
arxiv_id: '2309.06275'
source_url: https://arxiv.org/abs/2309.06275
tags:
- reasoning
- language
- question
- pages
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a simple prompting strategy called "Re-reading"
  (RE2) to improve reasoning capabilities of large language models (LLMs). RE2 involves
  re-reading the input question twice within the prompt, mimicking human cognitive
  reinforcement.
---

# Re-Reading Improves Reasoning in Large Language Models

## Quick Facts
- arXiv ID: 2309.06275
- Source URL: https://arxiv.org/abs/2309.06275
- Reference count: 40
- This paper introduces a simple prompting strategy called "Re-reading" (RE2) to improve reasoning capabilities of large language models (LLMs).

## Executive Summary
This paper introduces "Re-reading" (RE2), a simple prompting strategy that improves reasoning capabilities of large language models by re-reading the input question twice within the prompt. RE2 addresses the limitations of unidirectional attention in decoder-only LLMs by creating a "bidirectional" encoding effect, where the first pass provides global information that the second pass can use to refine understanding. Experiments on 14 reasoning benchmarks show RE2 consistently enhances reasoning performance across various LLMs, prompting methods, and ensemble techniques, particularly when combined with Chain-of-Thought prompting.

## Method Summary
RE2 is a "plug & play" prompting module that wraps around existing prompting methods by repeating the input question twice before applying thought-eliciting prompts like Chain-of-Thought. The method leverages query augmentation techniques from information retrieval to enhance knowledge recall and addresses input context bias by increasing exposure to the original query. RE2 is designed to complement existing reasoning strategies by improving input understanding before the reasoning chain is generated, trading a small increase in prompt length for potentially significant improvements in reasoning accuracy.

## Key Results
- RE2 consistently enhances reasoning performance across 14 reasoning benchmarks (112 experiments)
- Achieves improvements ranging from 1.57% to 11% on various benchmarks
- Optimal performance with 2 question repetitions, with diminishing returns beyond this point
- Works effectively across different LLMs, prompting methods, and ensemble techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RE2 improves reasoning by creating "bidirectional" encoding in unidirectional decoder-only LLMs.
- Mechanism: The first pass of reading provides global information that the second pass can use to refine understanding and correct misconceptions.
- Core assumption: LLMs benefit from iterative context refinement rather than single-pass processing.
- Evidence anchors:
  - [abstract]: "Crucially, RE2 facilitates a 'bidirectional' encoding in unidirectional decoder-only LLMs because the first pass could provide global information for the second pass."
  - [section]: "This strategy aligns with the cognitive principle of reinforcement, allowing the models to iteratively build upon their initial understanding of the problem."
- Break condition: If the model already has sufficient bidirectional attention mechanisms or if the question is simple enough that a single pass suffices.

### Mechanism 2
- Claim: RE2 addresses input context bias by increasing exposure to the original query.
- Mechanism: By re-reading the question, the model counters its tendency to focus disproportionately on certain aspects of the input context.
- Core assumption: LLMs have inherent biases toward specific parts of input context that can be mitigated by repeated exposure.
- Evidence anchors:
  - [section]: "existing decoder-only causal language modeling (CLM) architecture often operates in a single forward pass, which may miss the richer, back-and-forth interactions that humans use when reasoning through a challenging problem."
  - [section]: "Often, due to the training data's nature or the inherent biases of the model, LLMs tend to focus disproportionately on certain aspects of input, possibly neglecting other crucial information."
- Break condition: If the model is already well-aligned to evenly distribute attention across input or if the input context is minimal.

### Mechanism 3
- Claim: RE2 enhances knowledge recall by increasing query exposure in parametric retrieval.
- Mechanism: Repeating the question in prompts acts as query augmentation, improving the model's ability to recall relevant patterns or knowledge from training.
- Core assumption: LLMs trained on CoT data can better recall reasoning patterns when the question is re-exposed.
- Evidence anchors:
  - [section]: "Besides, emphasizing the input through re-reading can enhance knowledge recall in a parametric retrieval manner."
  - [section]: "As such, it is natural to adapt the basic but prevalent query augmentation technique in the term-based retrieval domain [14], which repeats the original query multiple times over the augmented part [61, 50], into prompting LLMs."
- Break condition: If the model hasn't been exposed to similar CoT patterns during training or if query repetition causes interference.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: RE2 is designed to complement CoT by improving input understanding before the reasoning chain is generated.
  - Quick check question: What is the primary purpose of CoT prompting in LLM reasoning tasks?

- Concept: Bidirectional vs unidirectional attention
  - Why needed here: Understanding this distinction is crucial for grasping why RE2's "bidirectional" encoding is beneficial for decoder-only LLMs.
  - Quick check question: How does bidirectional attention differ from the unidirectional attention in decoder-only LLMs?

- Concept: Query augmentation in information retrieval
  - Why needed here: RE2's mechanism of repeating the question draws from established query augmentation techniques.
  - Quick check question: What is the purpose of query augmentation in traditional information retrieval systems?

## Architecture Onboarding

- Component map: Question → RE2 wrapper (re-read question) → Thought-eliciting prompt (e.g., CoT) → LLM reasoning → Answer

- Critical path: Question → RE2 wrapper (re-read question) → Thought-eliciting prompt (e.g., CoT) → LLM reasoning → Answer

- Design tradeoffs: RE2 trades a small increase in prompt length for potentially significant improvements in reasoning accuracy. The optimal number of re-reads appears to be 2, as more repetitions can lead to performance degradation.

- Failure signatures:
  - Performance degradation when question is repeated more than 2-3 times
  - Minimal improvement on very simple questions that don't require complex reasoning
  - Potential interference with models already optimized for query understanding

- First 3 experiments:
  1. Compare performance of RE2 vs no RE2 on a simple arithmetic benchmark (e.g., GSM8K) using CoT prompting
  2. Test different numbers of question repetitions (1x, 2x, 3x) to find the optimal point
  3. Evaluate RE2 compatibility with different thought-eliciting strategies (e.g., PS, PAL) beyond CoT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of times to re-read a question to maximize reasoning performance across different task types?
- Basis in paper: [explicit] The paper shows performance improves with 2-3 re-reads but declines with more, suggesting an optimal point.
- Why unresolved: The paper only tested up to 5 re-reads; the true optimum may vary by task complexity or model architecture.
- What evidence would resolve it: Systematic testing across a broader range of tasks and re-read counts to identify task-specific optimal re-reading frequencies.

### Open Question 2
- Question: How does re-reading interact with different model architectures beyond decoder-only LLMs?
- Basis in paper: [inferred] The paper focuses on decoder-only models but mentions encoder-decoder models have implicit re-reading; no direct comparison is provided.
- Why unresolved: The paper does not empirically test re-reading on encoder-decoder or hybrid architectures.
- What evidence would resolve it: Comparative experiments applying RE2 to encoder-decoder models and measuring performance gains relative to decoder-only models.

### Open Question 3
- Question: Does re-reading improve reasoning in multi-modal tasks involving images and text?
- Basis in paper: [inferred] The paper suggests future work on multi-modal tasks but provides no experimental results.
- Why unresolved: No experiments were conducted on tasks requiring reasoning across image and text modalities.
- What evidence would resolve it: Experiments applying RE2 to multi-modal reasoning tasks (e.g., visual question answering) and measuring performance improvements.

## Limitations

- Limited scope of effectiveness: Results may not generalize to all LLM reasoning tasks or non-reasoning tasks like generation and translation
- Limited ablation studies: The optimal number of repetitions and interaction with model scale is not fully explored
- Implementation sensitivity: The exact prompt format and placement of repeated question could significantly impact effectiveness

## Confidence

- **High Confidence**: RE2 consistently improves reasoning performance when combined with Chain-of-Thought prompting; the mechanism of bidirectional encoding in unidirectional decoder-only LLMs is theoretically sound; improvements are measurable across arithmetic, commonsense, and symbolic reasoning tasks
- **Medium Confidence**: RE2's effectiveness as a "plug & play" module across different LLMs and prompting methods; the claim that RE2 addresses input context bias in LLMs; the assertion that RE2 enhances knowledge recall through query augmentation
- **Low Confidence**: The cognitive analogy between human re-reading and LLM prompting; the claim that RE2 provides "global information" for the second pass without empirical validation; the assumption that observed improvements are solely due to the re-reading mechanism

## Next Checks

1. Cross-Domain Validation: Test RE2 on non-reasoning benchmarks (e.g., summarization, translation) to assess whether improvements are task-specific or generalize to other LLM capabilities. Compare performance gains against established prompting techniques in these domains.

2. Ablation Study on Repetition Count: Conduct a systematic study varying the number of question repetitions (1x, 2x, 3x, 4x, 5x) across different model sizes to determine if the 2-repetition optimum holds universally or varies with model scale and task complexity.

3. Prompt Engineering Sensitivity Analysis: Test RE2 with different prompt placements and formats (e.g., question at beginning vs. middle vs. end of prompt, with vs. without separators) to quantify how implementation details affect performance and identify robust configurations.