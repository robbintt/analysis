---
ver: rpa2
title: Parrot Captions Teach CLIP to Spot Text
arxiv_id: '2312.14232'
source_url: https://arxiv.org/abs/2312.14232
tags:
- text
- clip
- images
- captions
- embedded
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the visual text bias in CLIP models, which
  causes them to parrot the visual text embedded in images while disregarding authentic
  visual semantics. The authors find that in the LAION-2B dataset, around 50% of images
  contain visual text content, and around 30% of caption words overlap with this embedded
  text.
---

# Parrot Captions Teach CLIP to Spot Text

## Quick Facts
- **arXiv ID**: 2312.14232
- **Source URL**: https://arxiv.org/abs/2312.14232
- **Reference count**: 40
- **Primary result**: CLIP models parrot visual text in images due to parrot captions in training data, harming visual-language generalization.

## Executive Summary
This paper investigates the visual text bias in CLIP models, which causes them to rely on embedded visual text rather than authentic visual semantics. The authors find that in the LAION-2B dataset, around 50% of images contain visual text content, and around 30% of caption words overlap with this embedded text. They show that released CLIP models have strong text spotting bias and that training with parrot captions shapes this bias but harms visual-language representation learning. The authors suggest revisiting CLIP-like model designs or image-text dataset curation pipelines to address this issue.

## Method Summary
The paper analyzes the LAION-2B dataset to quantify the prevalence of visual text and its overlap with captions (Co-Emb. Text Rate). They use OCR to detect embedded text, perform text inpainting to remove text from images, and train CLIP models on subsets with varying levels of parrot captions. The models are evaluated on zero-shot classification and retrieval tasks to assess the impact of parrot captions on visual-language representation learning.

## Key Results
- Around 50% of images in LAION-2B contain embedded visual text, and around 30% of caption words overlap with this text.
- CLIP models trained on data with high CoTR develop strong text spotting bias but lose zero-shot generalization ability.
- Text inpainting significantly reduces CLIP similarity scores, confirming that embedded text is the dominant factor in CLIP's similarity judgment.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CLIP models inherit text spotting bias from training data where captions directly repeat text embedded in images (parrot captions).
- Mechanism: The CLIP training objective maximizes image-text similarity. When a caption contains text also visible in the image, the model learns to align these visual and textual tokens as a shortcut, bypassing genuine visual understanding.
- Core assumption: Image-text pairs in LAION-2B frequently contain concurrent text between captions and images, making parrot captions a dominant alignment signal.
- Evidence anchors:
  - [abstract] "around 50% of images are embedded with visual text content, and around 30% of caption words are in these embedded visual content."
  - [section 4.2] "In the subsets of images embedded with text, around 90% of captions at least parrot one concurrent word appearing in the image."
  - [corpus] Weak: no direct neighbor papers explicitly address this mechanism; related works focus on mitigating bias but not explaining its source.
- Break condition: If captions rarely overlap with embedded text, or if the model is trained with strict filtering to remove parrot captions, the bias diminishes.

### Mechanism 2
- Claim: Text removal via inpainting reduces CLIP similarity scores, confirming that embedded text is the dominant factor in CLIP's similarity judgment.
- Mechanism: When visual text is removed, the image's representation changes such that its similarity to the original caption drops significantly, indicating that CLIP relied on the visual text as a primary matching cue.
- Core assumption: CLIP similarity is sensitive to the presence or absence of embedded text in the image.
- Evidence anchors:
  - [section 5.1] "The CLIP scores significantly drop once we remove the text from the images compared to its random inpainting baseline."
  - [section 5.1] "The images embedded with text achieve higher CLIP scores in most clusters than those without embedded text."
  - [corpus] Weak: no direct neighbor evidence; the mechanism is demonstrated experimentally within the paper.
- Break condition: If CLIP relied more on holistic visual features rather than localized text, inpainting would not cause such large similarity drops.

### Mechanism 3
- Claim: Training CLIP on subsets dominated by parrot captions improves text spotting capacity but harms zero-shot generalization.
- Mechanism: Parrot captions provide abundant text-visual alignment signals, so the model overfits to spotting text and loses ability to generalize to images where text is absent or not parroted.
- Core assumption: CLIP can be shaped to prioritize text spotting when trained on data rich in parrot captions.
- Evidence anchors:
  - [section 6.2] "Using parrot captions data, the CLIP model can learn strong text spotting capacity but lose most of the zero-shot generalization ability on image-text downstream tasks."
  - [section 6.2] "With increasing CoTR, all the zero-shot benchmark performance drops significantly."
  - [corpus] Weak: related works discuss bias mitigation but do not explain this specific shaping mechanism.
- Break condition: If the model were regularized to balance text and non-text cues, or trained on diverse data, the trade-off would lessen.

## Foundational Learning

- Concept: Contrastive learning for vision-language models
  - Why needed here: CLIP's core training uses contrastive loss to align image and text embeddings; understanding this is key to grasping why parrot captions create bias.
  - Quick check question: What objective function does CLIP minimize to align images and text?
- Concept: Text spotting and OCR in computer vision
  - Why needed here: The paper relies on OCR to detect embedded text and quantify its overlap with captions; without this, the bias cannot be measured.
  - Quick check question: How does the paper define Co-Emb. Text Rate (CoTR)?
- Concept: Data curation pipelines and filtering criteria
  - Why needed here: The LAION-2B dataset is filtered using CLIP scores, creating a feedback loop that amplifies parrot captions; understanding this loop is crucial to the argument.
  - Quick check question: Why does using CLIP score as a filtering criterion introduce bias?

## Architecture Onboarding

- Component map: CLIP model = Vision Encoder (e.g., ViT-B/32) + Text Encoder (e.g., transformer) + Contrastive loss; Data pipeline = Web scrape → CLIP score filter → OCR → CoTR calculation → Training subsets.
- Critical path: Data collection → OCR detection → CoTR computation → Subset sampling → CLIP training → Evaluation on downstream tasks.
- Design tradeoffs: Strong text spotting from parrot captions vs. poor visual-language generalization; filtering for high CLIP scores risks amplifying bias; using synthetic images to probe text spotting preferences.
- Failure signatures: High similarity between images and captions when text is present but low when text is removed; poor performance on tasks requiring genuine visual understanding; models overfit to spotting text rather than understanding scenes.
- First 3 experiments:
  1. Run OCR on a sample of LAION-2B images and compute CoTR to confirm the prevalence of parrot captions.
  2. Perform text inpainting on a subset of images and measure change in CLIP similarity to test the dominance of embedded text.
  3. Train CLIP on a small subset with high CoTR and evaluate zero-shot performance to observe the trade-off.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The analysis relies on LAION-2B, a specific large-scale dataset with its own curation pipeline, so findings may not extend to other datasets or smaller-scale scenarios.
- The paper does not address potential confounding factors such as image domain differences, text language diversity, or the impact of multilingual captions on bias.
- The causal link between parrot captions and zero-shot performance degradation is demonstrated but not fully isolated from other dataset characteristics.

## Confidence

- **High Confidence**: The prevalence of parrot captions in LAION-2B (CoTR metrics, overlap statistics) and the experimental demonstration that text inpainting reduces CLIP similarity scores.
- **Medium Confidence**: The claim that training CLIP on parrot caption subsets shapes text spotting bias and harms zero-shot generalization, as this is demonstrated in controlled experiments but with limited ablation and potential confounding factors.
- **Low Confidence**: The broader implications for CLIP-like model design and dataset curation pipelines, as these require further validation across diverse datasets and model architectures.

## Next Checks

1. **Dataset Generalization**: Replicate the CoTR analysis and text inpainting experiments on a different large-scale image-text dataset (e.g., Conceptual Captions) to test if the visual text bias is a general phenomenon or specific to LAION-2B.
2. **Confounder Control**: Design an experiment that controls for image domain, text language, and caption length, comparing CLIP models trained on parrot caption subsets versus matched non-parrot subsets, to isolate the effect of text overlap on model bias and performance.
3. **Bias Mitigation**: Implement and evaluate a bias mitigation strategy (e.g., data filtering, contrastive loss modification) on the LAION-2B dataset and measure its impact on both text spotting accuracy and zero-shot generalization to assess the trade-off empirically.