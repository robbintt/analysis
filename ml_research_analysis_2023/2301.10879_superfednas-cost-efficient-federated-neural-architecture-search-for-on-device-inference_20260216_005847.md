---
ver: rpa2
title: 'SuperFedNAS: Cost-Efficient Federated Neural Architecture Search for On-Device
  Inference'
arxiv_id: '2301.10879'
source_url: https://arxiv.org/abs/2301.10879
tags:
- training
- superfed
- subnetwork
- subnetworks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SuperFedNAS enables cost-efficient federated neural architecture\
  \ search for on-device inference by co-training a large family of DNN architectures\
  \ in a weight-shared fashion under non-i.i.d data distributions. It introduces MaxNet,\
  \ a federated training algorithm that performs multi-objective optimization over\
  \ approximately 5x10\u2078 diverse architectures without requiring prior pre-trained\
  \ models."
---

# SuperFedNAS: Cost-Efficient Federated Neural Architecture Search for On-Device Inference

## Quick Facts
- arXiv ID: 2301.10879
- Source URL: https://arxiv.org/abs/2301.10879
- Reference count: 40
- Key outcome: SuperFedNAS achieves up to 37.7% higher accuracy for the same computational cost or up to 8.13x reduction in MACs for the same accuracy compared to existing federated NAS methods.

## Executive Summary
SuperFedNAS introduces a novel approach to federated neural architecture search that enables efficient training of diverse deep neural network architectures across heterogeneous edge devices. By co-training a supernet containing approximately 5x10⁸ different architectures using a weight-shared federated learning framework, SuperFedNAS decouples the computationally expensive training phase from the architecture search phase. This allows clients to perform local neural architecture search post-training without additional communication or computation costs, achieving significant improvements in both accuracy and efficiency compared to traditional federated NAS approaches.

## Method Summary
SuperFedNAS introduces MaxNet, a federated training algorithm that performs multi-objective optimization over a supernet containing diverse DNN architectures. The method trains all architectures simultaneously in a weight-shared fashion, with clients assigned different subnetworks each round using a spatio-temporal distribution heuristic (Tracking-Sandwich). A weighted averaging strategy with decaying β parameter mitigates interference between small and large subnetworks. After global training completes, clients perform local NAS on the trained supernet to extract architectures optimized for their specific deployment constraints, achieving up to 9.43x lower computational and 10.94x lower communication costs than independently training subnetworks with FedAvg.

## Key Results
- Achieves 37.7% higher accuracy for the same computational cost compared to existing federated NAS methods
- Reduces MACs by up to 8.13x for the same accuracy target
- Achieves 9.43x lower computational and 10.94x lower communication costs than FedAvg baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight-sharing in federated learning reduces communication and computation costs by an order of magnitude compared to training each architecture independently.
- Mechanism: A single supernet contains all candidate architectures, allowing clients to train different subnetworks without re-downloading full models. Shared parameters are updated and averaged across clients, reducing the number of model parameters transmitted per round.
- Core assumption: The largest subnetwork's parameters are always included in at least one client per round, ensuring full supernet coverage.
- Evidence anchors:
  - [abstract] "SuperFedNAS achieves 9.43x lower computational and 10.94x lower communication costs than independently training subnetworks with FedAvg."
  - [section] "SuperFed decouples training from neural architecture search which allows local clients to dynamically select subnetworks of their choice from the globally-trained supernetwork without any re-training."
- Break condition: If the largest subnetwork is never assigned to any client in a round, shared-parameter averaging will not cover the full parameter space, leading to incomplete training.

### Mechanism 2
- Claim: MaxNet's spatio-temporal subnetwork distribution mitigates interference between small and large subnetworks during training.
- Mechanism: MaxNet prioritizes the largest subnetwork in early rounds using a weighted averaging strategy (βt decays from 0.9 toward uniform), ensuring larger models converge before smaller ones can corrupt their gradients.
- Core assumption: Interference is primarily caused by smaller subnetworks dominating gradient updates in early training phases.
- Evidence anchors:
  - [section] "We demonstrate interference in weight shared federated learning... MaxNet's shared-parameter averaging reduces interference among subnetworks."
  - [section] "MaxNet weighs the importance of largest subnetwork's parameters and reduces its influence over time."
- Break condition: If βt decay is too aggressive or starts too low, smaller subnetworks may still interfere before the largest model stabilizes.

### Mechanism 3
- Claim: Tracking-sandwich subnetwork distribution ensures balanced exposure to all data partitions across rounds.
- Mechanism: Each round samples one smallest, one largest, and the rest randomly, while tracking which clients have trained each bound. This ensures all clients eventually contribute to both extremes, indirectly training all intermediate models.
- Core assumption: Optimizing lower and upper bounds of the model family implicitly optimizes all intermediate architectures.
- Evidence anchors:
  - [section] "With the sandwich rule, optimizing the lower and upper bound can implicitly optimize all the architectures in φ."
  - [section] "Tracking-sandwich performs slightly better than sandwich... tracking makes sure that every client contributes to the global supernet eventually."
- Break condition: If the tracking heuristic fails to rotate bounds evenly, some architectures may be underrepresented in the global updates.

## Foundational Learning

- Concept: Federated Averaging (FedAvg)
  - Why needed here: Provides the baseline for distributed averaging in weight-shared settings; SuperFed extends FedAvg to handle overlapping parameters.
  - Quick check question: How does FedAvg aggregate updates when clients train identical models versus when they train overlapping subnetworks?

- Concept: Neural Architecture Search (NAS)
  - Why needed here: SuperFed produces a supernet from which clients can extract specialized subnetworks post-training without additional training.
  - Quick check question: Why is decoupling NAS from training beneficial in a federated setting?

- Concept: Weight-sharing interference
  - Why needed here: Explains why naive weight-shared training fails in non-i.i.d federated settings and why MaxNet's heuristics are necessary.
  - Quick check question: What causes smaller subnetworks to interfere with the training of larger ones in a shared-weight supernet?

## Architecture Onboarding

- Component map: Server -> Supernet -> Client assignment -> Local training -> Gradient aggregation -> Updated supernet
- Critical path:
  1. Server initializes supernet weights
  2. For each round: sample clients → assign subnetworks → distribute → local training → return updates → weighted aggregation
  3. Repeat until convergence
  4. Clients perform NAS locally on trained supernet
- Design tradeoffs:
  - Training cost vs. interference: More aggressive βt decay reduces interference but may slow largest subnetwork convergence
  - Client load balancing vs. model coverage: Tracking ensures all clients see both bounds but may delay some architectures' updates
  - Subnetwork granularity vs. search flexibility: Finer subnetwork granularity increases post-training options but raises initial training complexity
- Failure signatures:
  - If largest subnetwork accuracy lags FedAvg baseline: Likely interference not mitigated
  - If smaller subnetworks underperform: Possible βt decay too aggressive or subnetwork sampling unbalanced
  - If training diverges: Check overlap cardinality handling or βt initialization
- First 3 experiments:
  1. Train supernet with MaxNet on CIFAR-10, C=0.4, α=100; verify largest subnetwork matches FedAvg accuracy
  2. Vary β0 (0.9 vs 0.5) and observe effect on largest vs smallest subnetwork accuracy
  3. Replace tracking-sandwich with random distribution; measure accuracy drop and interference

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the content and typical research gaps, several important questions arise:

### Open Question 1
- Question: How does SuperFed perform under extreme client dropout rates or highly intermittent client participation?
- Basis in paper: [inferred] The paper assumes that at least one client will always participate in each round to receive the largest subnetwork, but does not explore scenarios with very low participation rates or high dropout.
- Why unresolved: The experiments use fixed participation ratios (C = 0.2, 0.4, 0.8) but do not test highly variable or extremely low participation scenarios.
- What evidence would resolve it: Experiments varying C from near 0 to 1 and measuring accuracy and convergence rates would show robustness to participation variability.

### Open Question 2
- Question: What is the theoretical convergence guarantee of MaxNet under non-i.i.d data distributions?
- Basis in paper: [inferred] While the paper demonstrates empirical effectiveness, it does not provide theoretical analysis of convergence properties under heterogeneous data.
- Why unresolved: The paper focuses on empirical evaluation but lacks mathematical proofs or bounds on convergence rates.
- What evidence would resolve it: A formal convergence analysis showing bounds on how quickly MaxNet reaches optimal solutions under various degrees of data heterogeneity.

### Open Question 3
- Question: How does the computational overhead of NAS on the client side compare to the benefits gained from selecting optimal subnetworks?
- Basis in paper: [explicit] The paper mentions that clients perform local NAS post-training but does not analyze the computational cost of this search relative to accuracy gains.
- Why unresolved: The paper assumes NAS is performed but does not quantify its cost or demonstrate whether simpler selection methods could achieve similar results.
- What evidence would resolve it: A cost-benefit analysis comparing the computational overhead of NAS versus simpler selection heuristics, along with accuracy comparisons.

## Limitations
- Limited validation beyond ResNet-like architectures on image classification tasks
- No theoretical convergence guarantees provided for MaxNet under non-i.i.d data distributions
- Computational overhead of post-training NAS on client devices not quantified

## Confidence
- Weight-sharing cost reduction claim: High confidence (well-supported by empirical results)
- Interference mitigation mechanism: Medium confidence (validated on limited dataset/architecture combinations)
- Decoupling training from NAS: High confidence (clearly demonstrated through experiments)
- Scalability to 5×10⁸ architectures: Medium confidence (theoretically sound but not exhaustively tested)

## Next Checks
1. **Cross-architecture validation**: Test MaxNet on architectures beyond ResNet (e.g., MobileNet, EfficientNet) to verify interference mitigation generalizes across different design paradigms.

2. **Dynamic participation analysis**: Evaluate how varying client participation rates (C < 0.4) affect the largest subnetwork's convergence and whether the β-decay schedule needs adaptation.

3. **Memory efficiency quantification**: Measure actual memory usage during supernet training across different hardware configurations to validate the claimed computational efficiency improvements.