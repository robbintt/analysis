---
ver: rpa2
title: Functional trustworthiness of AI systems by statistically valid testing
arxiv_id: '2310.02727'
source_url: https://arxiv.org/abs/2310.02727
tags:
- systems
- system
- have
- trustworthiness
- functional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors are concerned about the safety, health, and rights
  of European citizens due to inadequate measures in the EU AI Act for conformity
  assessment of AI systems. They advocate for establishing functional trustworthiness
  through (1) precise definition of the application domain, (2) risk-based minimum
  performance requirements, and (3) statistically valid testing on independent random
  samples.
---

# Functional trustworthiness of AI systems by statistically valid testing

## Quick Facts
- arXiv ID: 2310.02727
- Source URL: https://arxiv.org/abs/2310.02727
- Reference count: 8
- Authors: Matthias Fejes, Roland H. C. Vogt
- Key outcome: Current EU AI Act approaches for conformity assessment are inadequate; functional trustworthiness requires precise domain definition, risk-based minimum performance requirements, and statistically valid testing on independent random samples

## Executive Summary
The paper critiques the EU AI Act's approach to AI system conformity assessment, arguing that it misses the fundamental requirement of functional trustworthiness. Instead of relying on fixed test sets or representativeness of training data, the authors propose a framework based on three pillars: precise definition of the application domain, risk-based minimum performance requirements, and statistically valid testing using independent random samples. This approach addresses the pseudo-chaotic nature of deep learning optimization and ensures that performance measurements are both valid and generalizable.

## Method Summary
The paper proposes a framework for assessing AI system trustworthiness through statistical testing of functional properties. The method requires (1) precise definition of the technical distribution representing the application domain, (2) derivation of risk-based minimum performance requirements through formal risk analysis, and (3) execution of statistically valid tests using independently sampled random test sets. The approach emphasizes that test samples must be drawn independently from the training process to ensure measurement validity, and that the technical distribution must comprehensively capture all relevant conditions of intended use.

## Key Results
- Fixed test sets and representativeness-based approaches are fundamentally flawed for AI system assessment
- Risk-based minimum performance requirements must emerge from application-specific risk analysis, not arbitrary thresholds
- Statistical testing on independently sampled random sets is the only reliable way to measure functional trustworthiness
- The EU AI Act's current approach misses the point of ensuring quality by functional trustworthiness

## Why This Works (Mechanism)

### Mechanism 1
Statistical testing on independently sampled random sets is the only reliable way to measure functional trustworthiness of AI systems. The high-dimensional optimization in deep learning is pseudo-chaotic and unpredictable. Unlike classical software, you cannot analytically prove correctness. Instead, you measure performance empirically by drawing independent random samples from a well-defined technical distribution and computing success/failure ratios. Core assumption: The technical distribution is precisely defined and covers all relevant conditions of intended use.

### Mechanism 2
Risk-based minimum performance requirements must emerge from a formal risk analysis of the specific application domain. The likelihood of adverse outcomes due to functional errors is not a fixed constant but a variable that must be defined based on the application's safety and ethical context. These target values become the minimum performance requirements that the AI model must meet in statistical tests. Core assumption: Risk analysis is integrated into the ML development process, not treated as a separate compliance step.

### Mechanism 3
Fixed test sets and representative training datasets are fallacious approaches for ensuring trustworthiness. Fixed test sets inevitably leak information into training if they are reused or known, invalidating the measurement. Training dataset "representativeness" does not guarantee fairness or non-discrimination because optimization is dominated by complex nonlinear correlations, not equal weighting of samples. Core assumption: Testing and training data must be strictly separated with no information flow.

## Foundational Learning

- Concept: Technical distribution definition
  - Why needed here: Enables independent random sampling across the full application domain including all relevant conditions (sensors, environment, user profiles)
  - Quick check question: Can an independent engineer generate test samples from your technical distribution description without ambiguity?

- Concept: Risk-based performance metrics
  - Why needed here: Links statistical measures to real-world safety and ethical consequences, making the test results meaningful for certification
  - Quick check question: Are your minimum performance requirements justified by a documented risk analysis that maps risks to statistical thresholds?

- Concept: Statistical validity and independence
  - Why needed here: Ensures that test results generalize to future use and are not contaminated by training data leakage
  - Quick check question: Is there a formal proof or documentation that your test samples are independent of your training process?

## Architecture Onboarding

- Component map: Technical distribution definition → Risk analysis → Minimum performance requirements → Independent random sampling → Statistical testing → Documentation
- Critical path: Definition of technical distribution → Risk analysis → Setting minimum requirements → Test sample generation → Statistical testing → Certification decision
- Design tradeoffs: Synthetic samples can reduce effort but require strong mathematical justification; stricter independence increases validity but may increase cost
- Failure signatures: False confidence from fixed test sets, vague domain definitions, or risk analysis disconnected from actual application use cases
- First 3 experiments:
  1. Take a simple ML model and define a minimal technical distribution; generate 10 random test samples and measure performance to see statistical variability
  2. Modify a training set slightly to simulate information leakage and observe how test performance changes if fixed test sets are reused
  3. Conduct a risk analysis on a toy application and derive minimum performance thresholds, then test if the model meets them under statistical testing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal statistical test sample size required to achieve reliable confidence levels for different AI system applications?
- Basis in paper: [inferred] The paper discusses statistically valid testing based on independent random samples and mentions that the practical applications of this basic testing procedure may induce an overly big effort due to the necessity of a large number of independent testing samples
- Why unresolved: The paper acknowledges the need for a large number of test samples but doesn't provide specific guidance on optimal sample sizes for different applications or discuss the trade-offs between sample size and confidence levels
- What evidence would resolve it: Empirical studies comparing different sample sizes and their impact on confidence levels for various AI applications, along with cost-benefit analyses of testing efforts

### Open Question 2
- Question: How can we effectively define and validate the technical distribution for complex, real-world AI applications where mathematical formal definitions are impossible?
- Basis in paper: [explicit] The paper states "It is understood that there is no mathematically formal way to define a probability distribution that reflects all possible samples and their probability of occurrence for any realistic application"
- Why unresolved: The paper acknowledges this limitation but doesn't provide concrete methods for creating valid technical distributions or validating their completeness and accuracy for complex applications
- What evidence would resolve it: Case studies demonstrating successful technical distribution definitions and validation methods for various real-world AI applications, along with metrics for assessing distribution quality

### Open Question 3
- Question: What are the most effective methods for detecting and mitigating domain shifts in AI systems deployed in real-world environments?
- Basis in paper: [explicit] The paper mentions domain gaps (also called distributional shifts) as an unsolved problem regarding certification, noting that domain gaps arise when the data distributions for learning and inference do not match, which can be quite common for real-world applications
- Why unresolved: While the paper acknowledges the problem, it doesn't provide specific solutions or methods for detecting and addressing domain shifts in deployed systems
- What evidence would resolve it: Empirical studies comparing different domain shift detection and mitigation techniques across various application domains, along with metrics for measuring their effectiveness

## Limitations

- The paper doesn't provide specific examples or case studies of AI systems successfully assessed using the proposed approach
- Practical challenges of implementing the framework, such as computational resources required for statistically valid testing, are not discussed
- The approach requires precise definition of technical distributions, which becomes increasingly challenging for complex real-world applications

## Confidence

**High Confidence**: The core principle that statistical testing on independent random samples is superior to fixed test sets or representativeness-based approaches. This is well-supported by the pseudo-chaotic nature of deep learning optimization and the fundamental impossibility of analytical correctness proofs.

**Medium Confidence**: The risk-based minimum performance requirements framework. While the theoretical justification is strong, practical implementation details and empirical validation are limited in the paper.

**Medium Confidence**: The critique of current EU AI Act provisions. The arguments are logically sound, but the paper doesn't provide concrete examples of how the current framework fails in practice or quantify the potential safety gaps.

## Next Checks

1. **Technical Distribution Definition Test**: Select three diverse AI applications (e.g., medical diagnosis, autonomous driving, content moderation) and attempt to define their technical distributions following the paper's principles. Document challenges encountered and assess whether the approach scales to real-world complexity.

2. **Risk-to-Statistical-Threshold Mapping**: Develop a small-scale case study where qualitative risks are systematically translated into quantitative performance requirements. Test whether different risk analysts produce consistent statistical thresholds for the same application.

3. **Independence Verification Protocol**: Design and implement a protocol to verify that test samples are truly independent of training data, particularly for cases where synthetic samples are used or when the application domain is learned from data. Evaluate the protocol's effectiveness and computational overhead.