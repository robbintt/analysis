---
ver: rpa2
title: Task Relation Distillation and Prototypical Pseudo Label for Incremental Named
  Entity Recognition
arxiv_id: '2308.08793'
source_url: https://arxiv.org/abs/2308.08793
tags:
- entity
- types
- learning
- type
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles catastrophic forgetting and background shift
  in incremental named entity recognition (INER). It introduces a task relation distillation
  scheme combining inter-task relation distillation loss and intra-task self-entropy
  loss to ensure semantic consistency across tasks and boost model confidence.
---

# Task Relation Distillation and Prototypical Pseudo Label for Incremental Named Entity Recognition

## Quick Facts
- arXiv ID: 2308.08793
- Source URL: https://arxiv.org/abs/2308.08793
- Reference count: 40
- Key outcome: Achieves 6.08% increase in Micro F1 and 7.71% increase in Macro F1 scores over previous SOTA methods on CoNLL2003, I2B2, and OntoNotes5 datasets under ten INER settings.

## Executive Summary
This paper addresses catastrophic forgetting and background shift in incremental named entity recognition (INER) through a novel approach called Task Relation Distillation and Prototypical Pseudo Label (RDP). The method introduces a task relation distillation scheme that combines inter-task relation distillation loss and intra-task self-entropy loss to ensure semantic consistency across tasks while boosting model confidence. Additionally, it employs a prototypical pseudo label strategy to generate high-quality pseudo labels by measuring distances between token embeddings and type-wise prototypes, effectively mitigating background shift.

## Method Summary
The RDP method tackles two primary challenges in INER: catastrophic forgetting and background shift. It introduces a task relation distillation scheme with inter-task relation distillation loss (stability) and intra-task self-entropy loss (plasticity) to achieve an improved stability-plasticity trade-off. Simultaneously, a prototypical pseudo label strategy generates high-quality pseudo labels by measuring distances between token embeddings and type-wise prototypes. The method is evaluated on three benchmark datasets (CoNLL2003, I2B2, OntoNotes5) under ten different INER settings, demonstrating significant improvements over previous SOTA methods.

## Key Results
- Average increase of 6.08% in Micro F1 scores over previous SOTA methods
- Average increase of 7.71% in Macro F1 scores over previous SOTA methods
- Consistent performance improvements across all ten INER settings on three benchmark datasets
- Significant gains in both old entity type preservation and new entity type learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task relation distillation balances stability and plasticity in incremental learning
- Mechanism: Combines inter-task relation distillation loss (stability) and intra-task self-entropy loss (plasticity)
- Core assumption: Semantic consistency across tasks can be maintained by transferring soft labels encoding task relationships
- Evidence anchors: Abstract mentions ensuring inter-task semantic consistency and enhancing prediction confidence
- Break condition: If semantic relationships between old and new tasks are too dissimilar or confidence improvement fails

### Mechanism 2
- Claim: Prototypical pseudo labels correct prediction errors and improve label quality for background shift mitigation
- Mechanism: Calculates type-wise prototypes and reweights old model's output probabilities based on distance to prototypes
- Core assumption: Token embeddings close to type-wise prototypes are more likely to belong to that entity type
- Evidence anchors: Abstract mentions generating high-quality pseudo labels by measuring distances to type-wise prototypes
- Break condition: If type-wise prototypes are not representative or embedding space doesn't capture semantic similarity

### Mechanism 3
- Claim: Combining both approaches addresses catastrophic forgetting and background shift simultaneously
- Mechanism: Task relation distillation mitigates forgetting while prototypical pseudo labels address background shift
- Core assumption: These are the primary challenges in incremental NER requiring both semantic consistency and error correction
- Evidence anchors: Abstract mentions proposing RDP method for INER challenges
- Break condition: If either challenge is not primary or mechanisms interfere with each other

## Foundational Learning

- Concept: Incremental Learning
  - Why needed here: INER involves sequential learning of new entity types without accessing previous training data
  - Quick check question: What are the two main challenges in INER mentioned in the paper?

- Concept: Knowledge Distillation
  - Why needed here: Transfers knowledge from old model to new model to prevent catastrophic forgetting
  - Quick check question: How does the paper's task relation distillation scheme differ from traditional knowledge distillation?

- Concept: Pseudo Labeling
  - Why needed here: Identifies old entity types in non-entity type to address background shift
  - Quick check question: How does the prototypical pseudo label strategy differ from naive pseudo label strategy?

## Architecture Onboarding

- Component map: Encoder (BERT-based) -> Classifier (fully-connected) -> Task Relation Distillation -> Prototypical Pseudo Label

- Critical path:
  1. Train base model on first task
  2. For each subsequent task: calculate type-wise prototypes, generate pseudo labels, train new model with combined losses
  3. Evaluate performance on validation set

- Design tradeoffs:
  - Stability vs. Plasticity: Balancing old knowledge retention with new knowledge acquisition
  - Pseudo Label Quality vs. Quantity: High-quality pseudo labels may result in fewer labels compared to naive approach

- Failure signatures:
  - Performance degradation on old entity types indicates catastrophic forgetting
  - Poor performance on new entity types indicates insufficient plasticity
  - High variance in pseudo label quality indicates prototype calculation issues

- First 3 experiments:
  1. Train base model on first task and evaluate on validation set
  2. Implement prototypical pseudo label strategy and test on small data subset
  3. Combine task relation distillation with prototypical pseudo labels and evaluate overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the task relation distillation scheme impact performance with many incremental tasks?
- Basis in paper: Paper mentions scheme is designed to strike balance between stability and plasticity
- Why unresolved: No empirical evidence on effectiveness with large number of incremental tasks
- What evidence would resolve it: Experimental results comparing method with varying numbers of incremental tasks

### Open Question 2
- Question: Impact of prototypical pseudo label strategy on highly imbalanced entity type distributions?
- Basis in paper: Strategy is designed to handle background shift by reweighting based on prototype distances
- Why unresolved: No empirical evidence on effectiveness with highly imbalanced distributions
- What evidence would resolve it: Experimental results comparing method with varying levels of entity type imbalance

### Open Question 3
- Question: Performance when entity types in different tasks are not non-overlapping?
- Basis in paper: Method assumes entity types in different tasks are non-overlapping
- Why unresolved: No empirical evidence on effectiveness with overlapping entity types
- What evidence would resolve it: Experimental results comparing method with overlapping entity types

## Limitations

- Lack of detailed ablation studies to isolate contributions of task relation distillation and prototypical pseudo labels
- No discussion of potential failure modes in scenarios with highly dissimilar entity types or limited training data
- Absence of comparisons with alternative approaches for incremental NER such as regularization-based or rehearsal-based methods

## Confidence

- High Confidence: Significant improvements in Micro F1 and Macro F1 scores over previous SOTA methods
- Medium Confidence: Task relation distillation effective in balancing stability and plasticity
- Low Confidence: Prototypical pseudo label strategy effectively corrects prediction errors and improves label quality

## Next Checks

1. Conduct detailed ablation study to isolate contributions of task relation distillation scheme and prototypical pseudo label strategy

2. Evaluate method's robustness to different incremental learning scenarios including highly dissimilar entity types and limited training data

3. Compare proposed method with alternative approaches for incremental NER such as regularization-based and rehearsal-based methods