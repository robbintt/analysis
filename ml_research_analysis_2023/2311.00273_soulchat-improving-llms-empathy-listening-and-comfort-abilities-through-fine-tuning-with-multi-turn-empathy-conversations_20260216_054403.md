---
ver: rpa2
title: 'SoulChat: Improving LLMs'' Empathy, Listening, and Comfort Abilities through
  Fine-tuning with Multi-turn Empathy Conversations'
arxiv_id: '2311.00273'
source_url: https://arxiv.org/abs/2311.00273
tags:
- your
- empathy
- have
- psychological
- multi-turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SoulChat improves large language models' empathy, listening, and
  comfort abilities through fine-tuning with a multi-turn empathy conversation dataset.
  The dataset contains over 2 million samples covering 12 counseling topics, with
  input being multi-turn conversation context and target being empathetic responses.
---

# SoulChat: Improving LLMs' Empathy, Listening, and Comfort Abilities through Fine-tuning with Multi-turn Empathy Conversations

## Quick Facts
- arXiv ID: 2311.00273
- Source URL: https://arxiv.org/abs/2311.00273
- Reference count: 40
- Key outcome: SoulChat outperforms baselines like ChatGPT and MeChat on both automatic metrics (BLEU-1,2,3,4, ROUGE-1,2,L) and manual evaluation metrics for empathy, helpfulness, and safety

## Executive Summary
SoulChat is a fine-tuned version of ChatGLM-6B designed to enhance large language models' abilities in empathy, listening, and comfort for psychological counseling scenarios. The model is trained on a novel multi-turn empathy conversation dataset (SoulChatCorpus) containing over 2 million samples across 12 counseling topics. Through fine-tuning with this specialized dataset and a Chinese empathy constraint prompt, SoulChat demonstrates superior performance compared to baseline models on both automatic and manual evaluation metrics for empathetic responses.

## Method Summary
The researchers collected a multi-turn empathy conversation dataset (SoulChatCorpus) with 2,300,248 samples by converting single-turn psychological counseling conversations to multi-turn formats using a Chinese empathy constraint prompt. They fine-tuned ChatGLM-6B on this dataset with batch size 80, 30,000 training steps, learning rate scheduler, and top-p sampling decoding. The model was evaluated using automatic metrics (BLEU-1,2,3,4, ROUGE-1,2,L) and a manual evaluation framework (CEHS) for empathy, helpfulness, and safety.

## Key Results
- SoulChat outperforms ChatGLM-6B, ChatGPT, and MeChat on automatic metrics (BLEU, ROUGE) and manual evaluation for empathy, helpfulness, and safety
- SoulChat demonstrates excellent zero-shot performance on SMILECHAT dataset
- The model shows improved performance across 12 counseling topics including emotional support, relationship issues, and career guidance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning LLMs on multi-turn empathy conversations improves their ability to generate empathetic responses in psychological counseling scenarios.
- Mechanism: The fine-tuning process exposes the LLM to large-scale examples of empathetic dialogue patterns, including questioning, comfort, recognition, listening, trust, and emotional support.
- Core assumption: The dataset contains high-quality, diverse examples of empathetic responses that cover the full range of psychological counseling techniques.
- Evidence anchors: [abstract] "Experiments have shown that the empathy ability of LLMs can be significantly enhanced when finetuning by using multi-turn dialogue history and responses that are closer to the expression of a psychological consultant." [section 2.1] "We obtained a multi-turn empathy conversation dataset, named SoulChatCorpus, with 2,300,248 samples."

### Mechanism 2
- Claim: Using multi-turn dialogue history during fine-tuning helps LLMs generate more contextually relevant and coherent empathetic responses.
- Mechanism: By training on multi-turn conversations, the LLM learns to maintain context and build upon previous exchanges, allowing it to generate responses that are more attuned to the user's evolving emotional state and needs.
- Core assumption: The multi-turn conversations in the dataset demonstrate coherent context building and empathetic responses across multiple turns.
- Evidence anchors: [section 2.1] "SoulChatCorpus, with 2,300,248 samples. The input of model is defined as: input = uu1 + '\n' + up1 + ... + uuN + '\n' + upN" [section 3.3] "SoulChat outperforms ChatGLM-6B, ChatGPT and MeChat in both automatic evaluation metrics and Emp. metric on test set of SoulChatCorpus and SMILECHAT."

### Mechanism 3
- Claim: The proposed Chinese empathy constraint prompt improves the quality of generated multi-turn empathy conversations compared to existing methods like SMILE.
- Mechanism: The prompt explicitly instructs the model to generate responses that combine the user's description with empathy expressions such as listening, comfort, understanding, trust, recognition, sincerity, and emotional support.
- Core assumption: The model is capable of understanding and following the instructions provided in the empathy constraint prompt.
- Evidence anchors: [section 2.1] "As shown in Figure 2 (English version: Appendix C), our empathy constraints are defined as '心理咨询师的回复需要结合用户的描述内容并提供共情，如：倾听、安慰、理解、信任、认可、真诚、情感支持等 (The response of the 'psychological counselor' needs to be combined with the user's description and provide empathy, such as listening, comfort, interpretation, trust, recognition, sincerity, emotional support, etc)'." [section 3.3] "Specifically, the results on SMILECHAT demonstrates SoulChat's excellent zero-shot performance in the field of mental health."

## Foundational Learning

- Concept: Empathy in psychological counseling
  - Why needed here: Understanding the role of empathy in psychological counseling is crucial for developing LLMs that can effectively support users seeking emotional help.
  - Quick check question: What are the key components of empathy in psychological counseling, and how do they differ from simply providing advice or solutions?

- Concept: Fine-tuning techniques for LLMs
  - Why needed here: Fine-tuning is a critical step in adapting pre-trained LLMs to specific tasks or domains, such as empathetic psychological counseling.
  - Quick check question: What are the main differences between supervised fine-tuning and reinforcement learning from human feedback (RLHF), and when might each be more appropriate for training empathetic LLMs?

- Concept: Evaluation metrics for empathetic responses
  - Why needed here: Developing effective evaluation metrics is crucial for assessing the quality and effectiveness of the empathetic responses generated by the LLM.
  - Quick check question: What are the key considerations when designing evaluation metrics for empathetic responses, and how can we balance the need for objective measures with the subjective nature of empathy?

## Architecture Onboarding

- Component map: Data collection and preprocessing -> Fine-tuning pipeline -> Inference and response generation -> Evaluation framework -> Deployment and monitoring
- Critical path: Data collection and preprocessing -> Fine-tuning pipeline -> Inference and response generation -> Evaluation framework -> Deployment and monitoring
- Design tradeoffs:
  - Dataset size vs. quality: Larger datasets may provide more diverse examples but could also include lower-quality responses that may negatively impact the model's performance.
  - Fine-tuning duration vs. computational resources: Longer fine-tuning may lead to better performance but requires more computational resources and time.
  - Model size vs. inference speed: Larger models may generate more nuanced responses but could be slower to respond, impacting user experience.
- Failure signatures:
  - Low empathy scores in manual evaluation
  - Repetitive or generic responses
  - Inconsistent context building across multiple turns
  - Failure to maintain coherence or relevance to the user's emotional state
- First 3 experiments:
  1. Fine-tune the model on a subset of the SoulChatCorpus dataset and evaluate its performance on a held-out test set.
  2. Compare the performance of the fine-tuned model with baseline models like ChatGPT and MeChat on the same test set.
  3. Conduct ablation studies to assess the impact of different components of the fine-tuning process, such as the empathy constraint prompt or the multi-turn dialogue history.

## Open Questions the Paper Calls Out
The paper identifies several open questions and limitations, including:
- The model's performance on English empathy datasets like EMPATHETICDIALOGUES or ESConv remains untested
- The long-term impact of using SoulChat for emotional support compared to human counselors is unknown
- The empathy performance variation across different user demographics (age, cultural background, psychological conditions) needs further investigation

## Limitations
- The dataset quality and representativeness may be limited due to heavy reliance on ChatGPT-generated multi-turn conversations
- The evaluation framework, while detailed, lacks external validation and shows only moderate inter-rater agreement
- The model's generalization capabilities to real-world psychological counseling scenarios remain untested

## Confidence

**High Confidence**
- The technical approach of fine-tuning on multi-turn empathy conversations is sound
- The automatic metrics results (BLEU, ROUGE scores) are directly measurable and reproducible
- The basic architecture and training pipeline can be validated

**Medium Confidence**
- The improvement claims relative to baselines are reasonable but depend on fair comparison conditions
- The effectiveness of the Chinese empathy constraint prompt is supported but could benefit from ablation studies
- The 12 counseling topic coverage appears comprehensive but may have gaps

**Low Confidence**
- Claims about real-world deployment readiness
- The model's ability to handle complex, nuanced psychological situations
- Long-term effectiveness and safety in actual counseling scenarios

## Next Checks
1. **Dataset Quality Audit**: Conduct an independent review of the SoulChatCorpus dataset samples and compare the distribution of empathetic expressions against established psychological counseling frameworks.
2. **Cross-Cultural Validation**: Evaluate the model's performance with diverse demographic groups and test its ability to adapt to different cultural contexts and counseling approaches.
3. **Longitudinal Safety Assessment**: Monitor the model's responses over extended conversations (10+ turns) and test edge cases to evaluate the model's ability to recognize when human intervention is needed.