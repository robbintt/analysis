---
ver: rpa2
title: 'Flacuna: Unleashing the Problem Solving Power of Vicuna using FLAN Fine-Tuning'
arxiv_id: '2307.02053'
source_url: https://arxiv.org/abs/2307.02053
tags:
- vicuna
- flacuna
- flan
- dataset
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper addresses the problem of improving the problem-solving
  abilities of large language models (LLMs) that utilize decoder-only architecture,
  such as VICUNA. While T5-based LLMs like FLAN-T5 outperform these models on tasks
  requiring general problem-solving skills, the performance discrepancy can be attributed
  to three key factors: pre-training data, backbone architecture, and instruction
  dataset.'
---

# Flacuna: Unleashing the Problem Solving Power of Vicuna using FLAN Fine-Tuning

## Quick Facts
- arXiv ID: 2307.02053
- Source URL: https://arxiv.org/abs/2307.02053
- Reference count: 3
- Fine-tuned VICUNA on FLAN-MINI dataset improves problem-solving on INSTRUCTEVAL benchmarks

## Executive Summary
This paper addresses the performance gap between VICUNA (a decoder-only LLM) and FLAN-T5 (an encoder-decoder model) on reasoning tasks. By fine-tuning VICUNA using LoRA on a customized dataset collection called FLAN-MINI, the authors create FLACUNA, which significantly outperforms VICUNA on most benchmark datasets in INSTRUCTEVAL. While FLACUNA shows marked improvements, particularly on reasoning-intensive tasks, it still falls short of FLAN-T5's performance, likely due to dataset size limitations and architectural differences.

## Method Summary
The authors fine-tuned VICUNA (13B parameters) using parameter-efficient LoRA on a customized instruction dataset collection called FLAN-MINI. FLAN-MINI includes a subset of the large-scale FLAN dataset, code-related datasets (CodeContests, APPS, CodeSearchNet), and conversational datasets (Alpaca, Code-Alpaca, ShareGPT). The model was trained for one epoch on 4×A6000 GPUs with 16 gradient accumulation steps, batch size 128, and warmup 3000 steps. LoRA adapters were inserted into all query and value projection layers, resulting in only 0.05% of VICUNA's parameters being trainable.

## Key Results
- FLACUNA outperforms VICUNA on most benchmark datasets in INSTRUCTEVAL
- Significant improvements on reasoning-intensive tasks
- FLACUNA's performance remains below FLAN-T5 on reasoning benchmarks
- FLACUNA retains VICUNA's conversational abilities while improving problem-solving

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA fine-tuning adapts VICUNA's parameters to task-specific reasoning patterns
- Mechanism: LoRA inserts low-rank adapters into transformer layers, allowing VICUNA to learn new reasoning behaviors from FLAN-MINI without retraining all parameters
- Core assumption: LoRA adapters can capture sufficient reasoning patterns from FLAN-MINI to improve performance
- Evidence: FLACUNA shows significant improvements on reasoning benchmarks after LoRA fine-tuning
- Break condition: If FLAN-MINI lacks diversity, LoRA cannot learn effective reasoning patterns

### Mechanism 2
- Claim: Code and conversational datasets in FLAN-MINI preserve VICUNA's chatting ability while improving reasoning
- Mechanism: Including code datasets and conversational data provides balanced training signals that maintain conversational fluency
- Core assumption: Dataset diversity in FLAN-MINI provides balanced training signal
- Evidence: FLACUNA retains VICUNA's conversational abilities as reported in the paper
- Break condition: If conversational datasets are too sparse, FLACUNA may lose conversational fluency

### Mechanism 3
- Claim: Task diversity and template augmentation in FLAN-MINI enable generalization across problem-solving domains
- Mechanism: FLAN-MINI includes diverse tasks with prompt templates and few-shot demonstrations, exposing FLACUNA to varied reasoning strategies
- Core assumption: Task diversity and prompt templates provide sufficient generalization signals
- Evidence: FLACUNA shows improved performance across multiple benchmark tasks
- Break condition: If task diversity is insufficient, FLACUNA may overfit to specific patterns

## Foundational Learning

- Concept: Parameter-efficient fine-tuning with LoRA
  - Why needed here: Full fine-tuning of 13B parameter model is computationally expensive; LoRA provides cost-effective alternative
  - Quick check question: How does LoRA reduce trainable parameters compared to full fine-tuning?

- Concept: Instruction tuning and few-shot learning
  - Why needed here: FLACUNA trained on FLAN-MINI using prompt templates and few-shot demonstrations
  - Quick check question: What is the difference between zero-shot and few-shot prompting in instruction tuning?

- Concept: Benchmark evaluation and held-in vs held-out datasets
  - Why needed here: Understanding INSTRUCTEVAL's held-in/held-out distinction is crucial for interpreting results
  - Quick check question: Why is it important to distinguish between held-in and held-out datasets when evaluating model performance?

## Architecture Onboarding

- Component map: VICUNA (13B) → LoRA adapters in query/value layers → FLAN-MINI training data → FLACUNA
- Critical path: Data preparation → LoRA adapter insertion → Parameter-efficient fine-tuning → INSTRUCTEVAL evaluation
- Design tradeoffs: LoRA reduces computational cost but may limit performance vs full fine-tuning; smaller FLAN-MINI may reduce diversity
- Failure signatures: Poor code task performance despite code datasets; generation of code snippets in non-coding contexts
- First 3 experiments:
  1. Evaluate FLACUNA on held-out datasets (MMLU, BBH, CRASS, HumanEval)
  2. Compare FLACUNA to VICUNA and FLAN-T5 on same benchmarks
  3. Test FLACUNA's conversational ability with IMPACT dataset prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does full fine-tuning of VICUNA on the FLAN collection improve FLACUNA's problem-solving more than LoRA fine-tuning?
- Basis: Authors suggest full fine-tuning could further improve performance
- Why unresolved: Current study only used LoRA on FLAN-MINI subset
- Evidence needed: Full fine-tuning on complete FLAN dataset and performance comparison

### Open Question 2
- Question: Does training FLACUNA on longer input sequences improve reasoning-intensive task performance?
- Basis: FLACUNA trained with 1280 sequence length, which limits comprehension of longer inputs
- Why unresolved: Impact of longer sequences on reasoning capabilities unexplored
- Evidence needed: Retraining with longer sequences and evaluating on reasoning benchmarks

### Open Question 3
- Question: Can prompt engineering improve FLACUNA's writing performance and reduce code snippet generation in non-coding contexts?
- Basis: Authors note prompt engineering can help with code generation issues and improve chatting performance
- Why unresolved: Specific techniques and their effectiveness not detailed or tested
- Evidence needed: Experimenting with various prompt engineering techniques and evaluating responses

## Limitations
- FLAN-MINI dataset (1.34M samples) is much smaller than original FLAN (1.8B+), potentially limiting diversity
- Evaluation focuses on INSTRUCTEVAL without clear separation of held-in vs held-out dataset performance
- Decoder-only VICUNA architecture may inherently limit problem-solving compared to encoder-decoder models

## Confidence

**High Confidence**:
- FLACUNA outperforms VICUNA on INSTRUCTEVAL benchmarks
- LoRA fine-tuning with FLAN-MINI improves problem-solving performance
- VICUNA's decoder-only architecture creates fundamental limitations

**Medium Confidence**:
- FLACUNA retains VICUNA's conversational abilities while improving reasoning
- Dataset size and diversity explain performance gap with FLAN-T5
- Parameter-efficient fine-tuning provides cost-effective improvements

**Low Confidence**:
- Specific contribution of each dataset component in FLAN-MINI
- Generalizability to completely unseen reasoning tasks
- Long-term retention of improved reasoning abilities

## Next Checks
1. Evaluate FLACUNA on completely held-out datasets (MMLU, BBH, CRASS, HumanEval) to measure true generalization
2. Systematically assess FLACUNA's conversational fluency using IMPACT dataset and held-out conversational prompts
3. Implement full fine-tuning of VICUNA on complete FLAN dataset to quantify performance gap with parameter-efficient approach