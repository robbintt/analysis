---
ver: rpa2
title: 'Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples
  for Supervised Learning Contribute the Least'
arxiv_id: '2302.09195'
source_url: https://arxiv.org/abs/2302.09195
tags:
- examples
- learning
- class
- subset
- contrastive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of finding the most important
  examples for contrastive self-supervised learning (SSL) to improve data efficiency.
  The authors prove that examples contributing most to contrastive SSL are those with
  the highest expected similarity between their augmented views and other examples
  in their latent class.
---

# Data-Efficient Contrastive Self-supervised Learning: Most Beneficial Examples for Supervised Learning Contribute the Least

## Quick Facts
- arXiv ID: 2302.09195
- Source URL: https://arxiv.org/abs/2302.09195
- Reference count: 33
- Primary result: CL-Core selects 20-40% of data that improves contrastive SSL performance by 3-4% over random subsets on CIFAR and STL datasets

## Executive Summary
This paper addresses data efficiency in contrastive self-supervised learning by identifying which examples contribute most to learning good representations. The authors prove that examples with highest expected similarity between their augmented views and other examples in their latent class are most beneficial for contrastive SSL. They provide theoretical guarantees showing that subsets preserving class center alignment and divergence can match full-data performance. Surprisingly, the subsets most beneficial for SSL are those least beneficial for supervised learning, allowing up to 20-40% of examples to be safely excluded without harming downstream task performance.

## Method Summary
The method identifies important examples for contrastive SSL by estimating expected augmentation distance between examples using a proxy model. It approximates latent classes using CLIP embeddings and a small labeled subset, then applies a greedy subset selection algorithm to maximize within-class similarity. The selected subsets are used to train SimCLR or BYOL models, with downstream performance evaluated using linear classification. The approach provides theoretical guarantees that subsets preserving alignment and divergence of class centers can achieve similar generalization performance to full data.

## Key Results
- CL-Core outperforms random subsets by over 3% on CIFAR100, CIFAR10, and STL10
- Subsets most beneficial for SSL are least beneficial for supervised learning
- Up to 20-40% of examples can be safely excluded without harming downstream performance
- The method works across different proxy models including partially trained and smaller networks

## Why This Works (Mechanism)

### Mechanism 1
Examples with highest expected similarity between their augmented views and other examples in their latent class contribute most to contrastive SSL. These examples pull different groups within a class together, enabling the contrastive loss to maximally push away representations of examples in different classes. Core assumption: expected augmentation distance captures semantic similarity. Break condition: if augmentations don't preserve semantic similarity, the metric fails.

### Mechanism 2
Subsets preserving alignment and divergence of class centers guarantee similar generalization performance to full data. Good alignment ensures augmented views of examples are similar, while good divergence ensures class centers are far apart, allowing the NN classifier to correctly classify examples. Core assumption: minimizing InfoNCE loss on the subset preserves alignment and divergence properties. Break condition: if the subset is too small to capture class diversity, properties may not be preserved.

### Mechanism 3
Easy examples (high confidence, low forgetting score) for supervised learning contribute least to supervised learning but most to contrastive SSL. These easy examples are representative of their class and help align augmentations, while difficult examples create noise in the contrastive loss. Core assumption: forgetting score and confidence are valid metrics for example difficulty. Break condition: if "easy" for supervised learning doesn't translate to being representative for contrastive SSL.

## Foundational Learning

- Concept: Contrastive learning basics (InfoNCE loss, positive/negative pairs)
  - Why needed here: Paper builds on contrastive learning theory and uses InfoNCE loss
  - Quick check question: What is the objective of contrastive learning in terms of positive and negative pairs?

- Concept: Submodular optimization
  - Why needed here: Subset selection problem is formulated as submodular maximization
  - Quick check question: What is the (1-1/e) approximation guarantee for the greedy algorithm on monotone submodular functions?

- Concept: Generalization bounds in machine learning
  - Why needed here: Paper provides generalization guarantees for downstream performance on subsets
  - Quick check question: What is the relationship between alignment, divergence, and generalization error in contrastive learning?

## Architecture Onboarding

- Component map: Data augmentation pipeline -> Proxy model predictions -> Latent class approximation -> Greedy subset selection -> SimCLR/BYOL training -> Linear evaluation

- Critical path:
  1. Preprocess data with augmentations
  2. Get proxy model predictions for all pairs
  3. Approximate latent classes
  4. Run greedy selection per class
  5. Train contrastive model on selected subset
  6. Evaluate with linear classifier

- Design tradeoffs:
  - Proxy model accuracy vs. computational cost
  - Subset size vs. downstream performance
  - Approximate vs. exact latent classes
  - Early stopping in contrastive training

- Failure signatures:
  - Poor proxy model leads to unrepresentative selected subsets
  - Poorly approximated latent classes group examples from different classes
  - Subset too small to capture class diversity

- First 3 experiments:
  1. Verify easy examples (low forgetting, high confidence) are selected by CL-Core
  2. Measure alignment loss and class center similarity on selected vs random subsets
  3. Test effect of different proxy models (R50, R18, R10) on downstream accuracy

## Open Questions the Paper Calls Out

### Open Question 1
How does CL-Core compare when using different proxy models of varying sizes and training stages? While the paper shows different proxy models yield similar performance, it doesn't explore the full spectrum of possible proxy models or provide theoretical justification for why smaller or partially trained models work as well as larger, fully trained ones.

### Open Question 2
Can CL-Core be extended to other self-supervised learning methods beyond contrastive learning, such as generative models or masked autoencoders? The theoretical framework relies heavily on properties specific to contrastive learning (alignment, divergence of class centers), which may not directly apply to other SSL approaches.

### Open Question 3
What is the impact of using different data augmentation strategies on the effectiveness of CL-Core? The paper uses standard augmentations but doesn't systematically investigate how different augmentation choices affect CL-Core's performance.

### Open Question 4
How does CL-Core perform in more challenging, real-world scenarios with class imbalance or domain shift? The paper evaluates on balanced datasets but doesn't address scenarios with class imbalance or distribution shift.

## Limitations

- Theoretical guarantees rely on strong assumptions about latent class structure and augmentation properties
- Latent classes must be approximated in practice, introducing potential approximation errors
- Claim about "safe exclusion" of 20-40% of examples may not hold for all dataset characteristics
- Framework may not generalize to non-contrastive SSL methods without significant modification

## Confidence

- High confidence: Empirical results showing CL-Core outperforms random subsets by 3-4% on CIFAR and STL datasets
- Medium confidence: Theoretical framework connecting expected augmentation distance to contrastive loss contribution
- Low confidence: Generalization to "safe exclusion" of 20-40% of examples without performance degradation

## Next Checks

1. Test the approach on ImageNet and more diverse datasets to verify scalability and robustness to different data distributions
2. Evaluate the effect of using different proxy models (varying architectures and pre-training tasks) on the quality of subset selection
3. Conduct ablation studies on augmentation parameters to determine their impact on the expected augmentation distance estimates and downstream performance