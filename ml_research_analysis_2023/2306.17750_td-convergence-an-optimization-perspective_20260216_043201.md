---
ver: rpa2
title: 'TD Convergence: An Optimization Perspective'
arxiv_id: '2306.17750'
source_url: https://arxiv.org/abs/2306.17750
tags:
- optimization
- function
- learning
- algorithm
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides an optimization perspective on the convergence\
  \ of temporal difference (TD) learning. It identifies two key forces\u2014the target\
  \ force and the optimization force\u2014whose interplay determines convergence."
---

# TD Convergence: An Optimization Perspective

## Quick Facts
- **arXiv ID**: 2306.17750
- **Source URL**: https://arxiv.org/abs/2306.17750
- **Reference count**: 40
- **Key outcome**: This paper proves TD convergence when the optimization force dominates the target force (Fw > Fθ), extending guarantees beyond linear approximation and squared loss to general function approximators and loss functions.

## Executive Summary
This paper provides a novel optimization perspective on temporal difference (TD) learning convergence by identifying two competing forces: a target force (from bootstrapping) and an optimization force (from minimizing the loss). The key insight is that TD converges when the optimization force dominates the target force, even in the presence of the "deadly triad" of reinforcement learning. This framework generalizes convergence guarantees to broader function approximators and loss functions beyond the traditional linear approximation and squared loss settings, providing theoretical explanation for TD's practical success across diverse reinforcement learning applications.

## Method Summary
The paper treats TD as an iterative optimization algorithm, formulating the problem as minimizing an objective function H(θ,w) that combines target parameter θ and optimization parameter w. Two algorithms are analyzed: one with exact minimization (Algorithm 1) and one with gradient updates (Algorithm 2). The convergence proofs rely on properties like Fw-strong convexity and Fθ-Lipschitz continuity, with the key condition being Fθ < Fw. The framework allows analysis of TD beyond linear approximation and squared loss by focusing on the interplay between the target and optimization forces rather than specific functional forms.

## Key Results
- TD converges when the optimization force dominates the target force (Fw > Fθ), even with bootstrapping, function approximation, and off-policy updates
- The convergence guarantee extends to general function approximators and loss functions beyond linear approximation and squared loss
- For approximate TD updates, convergence occurs with step size α = 1/L, with contraction factor improving as K increases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TD convergence depends on the relative strength of two forces: a target force (due to bootstrapping) and an optimization force (due to minimizing the loss).
- Mechanism: When the optimization force dominates the target force (Fw > Fθ), the algorithm converges even in the presence of the "deadly triad" (bootstrapping, function approximation, off-policy updates).
- Core assumption: The objective function H(θ,w) satisfies Fθ-Lipschitz continuity in θ and Fw-strong convexity in w.
- Break condition: If Fθ ≥ Fw, the algorithm diverges regardless of step size or iteration count.

### Mechanism 2
- Claim: Even approximate TD updates (with gradient steps) converge when the optimization force dominates, provided step size is chosen appropriately.
- Mechanism: Using gradient descent with step size α = 1/L (where L bounds the gradient in w), convergence occurs with contraction factor σK that improves as K increases.
- Core assumption: ∇wH is L-Lipschitz continuous in w.
- Break condition: If step size α > 1/L, convergence guarantees break down due to gradient descent instability.

### Mechanism 3
- Claim: The optimization perspective generalizes beyond linear approximation and squared loss, allowing convergence analysis for various function approximators and loss functions.
- Mechanism: By formulating TD as minimizing H(θ,w), convergence depends only on the interplay of forces, not on the specific form of approximation or loss.
- Core assumption: H(θ,w) can be constructed with any strongly convex loss and any function approximator satisfying the Lipschitz and strong convexity conditions.
- Break condition: If either the loss function or function approximator violates the strong convexity or Lipschitz assumptions, the convergence guarantee no longer holds.

## Foundational Learning

- Concept: Strongly convex functions and their optimization properties
  - Why needed here: The convergence proof relies on Fw-strong convexity of H in the optimization parameter w
  - Quick check question: If H(w) = ½w⊤Mw where M is positive definite, what is the strong convexity parameter?

- Concept: Lipschitz continuity of gradients
  - Why needed here: Both Fθ-Lipschitz in θ and L-Lipschitz in w are required for the convergence analysis
  - Quick check question: If ∇H(w₁) - ∇H(w₂) ≤ L||w₁ - w₂||, what does this tell us about the smoothness of H?

- Concept: Contraction mapping and spectral radius conditions
  - Why needed here: Corollary 2 states convergence occurs when ρ(Mw⁻¹Mθ) < 1, which requires understanding spectral radius
  - Quick check question: If A is a 2×2 matrix with eigenvalues 0.3 and 0.7, what is its spectral radius and does it satisfy the convergence condition?

## Architecture Onboarding

- Component map:
  - H(θ,w): Objective function combining target parameter θ and optimization parameter w
  - Fw: Strong convexity parameter for w (optimization force)
  - Fθ: Lipschitz constant for θ (target force)
  - L: Lipschitz constant for ∇wH in w
  - α: Step size parameter for gradient updates

- Critical path:
  1. Construct H(θ,w) using chosen function approximator and loss
  2. Compute or bound Fw, Fθ, and L
  3. Verify Fθ < Fw for convergence
  4. Choose α = 1/L for approximate updates
  5. Run algorithm and monitor convergence

- Design tradeoffs:
  - Exact vs. approximate optimization: Exact optimization (Algorithm 1) gives stronger convergence guarantees but is harder to implement
  - Step size selection: α = 1/L ensures convergence but may be slow; larger α risks divergence
  - Function approximator choice: Must balance representational power with maintaining strong convexity

- Failure signatures:
  - Divergence: Indicates Fθ ≥ Fw or inappropriate step size
  - Oscillation: Suggests step size too large relative to L
  - Slow convergence: May indicate κ = L⁻¹Fw is close to 1

- First 3 experiments:
  1. Implement Algorithm 2 with K=1, α=1/L on a simple linear MDP and verify convergence when Fθ < Fw
  2. Test convergence breakdown by increasing Fθ relative to Fw through bootstrapping weight γ
  3. Compare exact (K=∞) vs. approximate (K=1) optimization on a small problem to observe the effect of σK

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions on the target force F_theta and optimization force F_w does TD converge for general function approximation and loss functions?
- Basis in paper: [explicit] The paper states that convergence is guaranteed when F_theta < F_w, where F_theta and F_w are the magnitudes of the target and optimization forces, respectively.
- Why unresolved: The paper provides the general condition but does not specify how to compute or bound these forces for arbitrary function approximators and loss functions.
- What evidence would resolve it: Derivation of explicit formulas or bounds for F_theta and F_w for various function approximation methods (e.g., neural networks, kernel methods) and loss functions (e.g., Huber loss, logistic loss).

### Open Question 2
- Question: How does the convergence rate of TD with general function approximation and loss functions depend on the choice of state distribution d(s)?
- Basis in paper: [inferred] The paper shows that the stationary-state distribution is not the only distribution that guarantees convergence, and discusses how different distributions affect the relative strengths of the target and optimization forces.
- Why unresolved: The paper does not provide a detailed analysis of the convergence rate as a function of the state distribution.
- What evidence would resolve it: A theoretical analysis or empirical study demonstrating the convergence rate of TD for various state distributions and function approximation methods.

### Open Question 3
- Question: Can the optimization perspective of TD be extended to the control setting (MDPs with multiple actions) and what would be the implications for convergence guarantees?
- Basis in paper: [explicit] The paper mentions that results can be extended to the MDP setting with off-policy updates but focuses on the simpler MRP setting to study the root cause of TD's behavior.
- Why unresolved: The extension to MDPs introduces additional complexity, such as the need to consider multiple actions and the interplay between policy evaluation and improvement.
- What evidence would resolve it: A theoretical framework and convergence analysis for TD-like algorithms in the control setting, potentially building on existing work on Q-learning and actor-critic methods.

## Limitations
- The theoretical framework relies on abstract conditions (Fθ-Lipschitz, Fw-strong convexity) that may be difficult to verify for practical function approximators like neural networks
- The paper provides mathematical proofs for the abstract setting but lacks empirical validation across diverse real-world scenarios
- There is a significant gap between theory and practice for complex models, as the paper doesn't show how to construct H(θ,w) or verify conditions for arbitrary function approximators

## Confidence

- **High**: The core mathematical framework relating convergence to the relative strengths of target and optimization forces is well-established and rigorously proven
- **Medium**: The extension to general function approximators and loss functions follows logically from the framework but requires careful verification of assumptions in practice
- **Low**: Practical implementation details and empirical validation across diverse settings are not provided, making it unclear how to verify the conditions Fθ < Fw in real-world scenarios

## Next Checks

1. **Empirical verification of force conditions**: Implement TD with a neural network approximator and monitor the empirical values of Fθ and Fw during training to verify they satisfy the convergence condition Fθ < Fw

2. **Sensitivity analysis**: Systematically vary the bootstrapping parameter γ (which affects Fθ) and observe at what threshold divergence occurs, validating the theoretical prediction that Fθ ≥ Fw causes divergence

3. **Cross-distribution robustness test**: Compare TD convergence under different state distributions (on-policy vs. off-policy) while maintaining Fθ < Fw, to verify the claim that convergence can occur even with non-stationary distributions