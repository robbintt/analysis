---
ver: rpa2
title: Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based
  Visual Pre-training and Cross-Modal Fusion Encoder
arxiv_id: '2308.08488'
source_url: https://arxiv.org/abs/2308.08488
tags:
- visual
- fusion
- training
- speech
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving audio-visual speech
  recognition (AVSR) systems using low-quality videos, which often results in minimal
  performance gains over audio-only systems. The authors propose a two-stage training
  framework that decouples pre-training and fine-tuning, alleviating the issue of
  unmatched convergence rates and specialized input representations between audio
  and visual modalities.
---

# Improving Audio-Visual Speech Recognition by Lip-Subword Correlation Based Visual Pre-training and Cross-Modal Fusion Encoder

## Quick Facts
- arXiv ID: 2308.08488
- Source URL: https://arxiv.org/abs/2308.08488
- Reference count: 0
- One-line primary result: Achieves 24.58% CER on MISPoriginal dataset using correlation-based visual pre-training and audio-guided cross-modal fusion

## Executive Summary
This paper addresses the challenge of improving audio-visual speech recognition (AVSR) performance on low-quality videos where traditional systems show minimal gains over audio-only systems. The authors propose a two-stage training framework that decouples pre-training and fine-tuning, alleviating the issue of unmatched convergence rates and specialized input representations between audio and visual modalities. They introduce a novel visual pre-training method that correlates lip shapes with syllable-level subword units in Mandarin, enabling accurate alignment of video and audio streams. Additionally, they propose an audio-guided cross-modal fusion encoder (CMFE) neural network that leverages modality complementarity through multiple cross-modal attention layers. The proposed techniques are evaluated on the MISP2021-AVSR dataset, demonstrating superior performance compared to state-of-the-art systems.

## Method Summary
The authors propose a two-stage training framework for AVSR systems. In the first stage, they pre-train an audio-only model using hybrid CTC/Attention architecture and generate frame-level alignments using GMM-HMM forced alignment. The visual frontend is then pre-trained using lip shapes correlated with syllable-level subword units derived from the audio alignments. An up-sampling block handles the mismatch between video frame rate (25fps) and alignment rate (100fps). In the second stage, they fine-tune an audio-visual fusion model using the pre-trained components. The fusion model employs an audio-guided cross-modal fusion encoder (CMFE) with multiple cross-modal attention layers, prioritizing the audio modality based on its higher semantic information content. The system is trained with joint CTC/Attention loss and evaluated on the MISP2021-AVSR dataset.

## Key Results
- Achieves 24.58% CER on MISPoriginal dataset, outperforming existing systems with more complex front-ends and back-ends
- Demonstrates superior performance compared to state-of-the-art systems on MISP2021-AVSR dataset
- Shows effectiveness of two-stage training framework and correlation-based visual pre-training approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage training framework decouples the learning of audio and visual modalities, reducing interference and improving final performance.
- Mechanism: By pre-training audio and visual networks separately, each modality can learn optimal feature representations before being fused. This avoids the convergence issues that arise when trying to train a multi-modal system end-to-end from scratch.
- Core assumption: The visual frontend can be effectively pre-trained using audio-derived alignment labels without needing large amounts of labeled visual data.
- Evidence anchors:
  - [abstract] "Unmatching convergence rates and specialized input representations between audio and visual modalities are considered to cause the problem."
  - [section] "This divide-and-conquer strategy could effectively mitigate variations in learning dynamics between modalities and leverage their interactions."
- Break condition: If the alignment between audio and visual modalities is poor, the visual frontend pre-training will fail to produce useful features.

### Mechanism 2
- Claim: The lip-subword correlation method provides accurate frame-level syllable boundaries for visual pre-training without needing manually labeled data.
- Mechanism: Using a GMM-HMM trained on audio to generate frame-level state alignments, the system can map lip movements to specific syllables. This explicit alignment guides the visual network to focus on relevant visual features.
- Core assumption: The audio-derived syllable boundaries are accurate enough to be useful for visual pre-training.
- Evidence anchors:
  - [abstract] "First, we explore the correlation between lip shapes and syllable-level subword units in Mandarin to establish good frame-level syllable boundaries from lip shapes."
  - [section] "It offers explicit boundaries to establish a direct frame-level mapping from lip shapes to acoustic subwords and does not need extra data sets or manually-labeled word boundaries."
- Break condition: If the audio-syllable alignment is inaccurate, the visual features learned will be noisy and less useful.

### Mechanism 3
- Claim: The audio-guided cross-modal fusion encoder (CMFE) allows multiple cross-modal attention layers to make full use of modality complementarity.
- Mechanism: By prioritizing the audio modality in the fusion layers and inserting cross-attention blocks at different levels, the system can adaptively weigh the contributions of audio and visual information. This is inspired by the decoder architecture of the vanilla transformer.
- Core assumption: Audio contains more semantic information than visual, justifying its dominance in the fusion architecture.
- Evidence anchors:
  - [abstract] "we propose an audio-guided cross-modal fusion encoder (CMFE) neural network to utilize main training parameters for multiple cross-modal attention layers to make full use of modality complementarity."
  - [section] "Since speech contains more semantic information, we reduce the depth of the visual branch, and more parameters are used for multiple modal cross-attention in different layers."
- Break condition: If visual information becomes more important than assumed, the audio-dominated architecture may underutilize valuable visual cues.

## Foundational Learning

- Concept: GMM-HMM forced alignment for generating frame-level state boundaries
  - Why needed here: Provides the alignment labels needed to train the visual frontend without requiring manually labeled data.
  - Quick check question: How does forced alignment work in the context of GMM-HMMs?

- Concept: Conformer architecture and its components (self-attention, convolution, feed-forward)
  - Why needed here: The backbone of both the visual frontend and the cross-modal fusion encoder.
  - Quick check question: What are the advantages of using Conformer blocks in speech recognition systems?

- Concept: Cross-modal attention mechanisms and their role in multi-modal fusion
  - Why needed here: Enables the system to dynamically weigh the contributions of audio and visual modalities at different levels of the network.
  - Quick check question: How does cross-attention differ from self-attention in the context of multi-modal fusion?

## Architecture Onboarding

- Component map:
  - Audio branch: Pre-trained audio-only CTC/Attention model, fine-tuned with fusion model
  - Visual branch: Pre-trained video-only model with conv3d+resnet18, up-sampling block, and conformer blocks
  - Fusion encoder: Audio-dominated cross-modal fusion encoder (CMFE) with early and late fusion layers
  - Decoder: Transformer-based decoder with joint CTC/Attention training

- Critical path:
  - Pre-training: Train audio-only model → Generate frame-level alignments → Pre-train visual frontend
  - Fine-tuning: Initialize fusion model with pre-trained branches → Train with cross-modal attention layers

- Design tradeoffs:
  - Audio vs. visual modality dominance in the fusion architecture
  - Number of conformer blocks in the visual branch
  - Inner vs. outer insertion of cross-attention blocks

- Failure signatures:
  - Poor audio-visual alignment leading to noisy visual features
  - Underutilization of visual information due to audio dominance
  - Overfitting on the training data due to complex fusion architecture

- First 3 experiments:
  1. Train audio-only model from scratch and evaluate baseline performance
  2. Pre-train visual frontend using lip-subword correlation method and evaluate performance
  3. Implement and evaluate different cross-modal fusion strategies (e.g., TM-CTC, TM-Seq, CMFE)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different types of subword units, such as visemes and phonemes, affect the performance of correlation-based visual pre-training and cross-modal fusion encoders in AVSR systems?
- Basis in paper: [explicit] The authors mention exploring different subword units in the future but do not provide results for visemes or phonemes.
- Why unresolved: The paper focuses on syllable-level subword units in Mandarin, and the impact of other subword units on AVSR performance remains unexplored.
- What evidence would resolve it: Experimental results comparing the performance of AVSR systems using different subword units (e.g., visemes, phonemes) for visual pre-training and cross-modal fusion.

### Open Question 2
- Question: What is the impact of using a larger and more diverse training dataset on the performance of the proposed AVSR system with correlation-based visual pre-training and cross-modal fusion encoders?
- Basis in paper: [inferred] The authors mention using a relatively small amount of training data, implying that larger datasets might further improve performance.
- Why unresolved: The paper does not provide results for training the system on larger datasets, and the potential performance gains from increased data diversity are unknown.
- What evidence would resolve it: Experimental results showing the performance of the AVSR system trained on larger and more diverse datasets, compared to the current results.

### Open Question 3
- Question: How does the proposed AVSR system perform on languages other than Mandarin, especially those with different phonetic and orthographic structures?
- Basis in paper: [inferred] The paper focuses on Mandarin, and the effectiveness of the proposed methods on other languages is not discussed.
- Why unresolved: The paper does not provide results for languages other than Mandarin, and the generalizability of the proposed methods to different linguistic contexts is unclear.
- What evidence would resolve it: Experimental results demonstrating the performance of the AVSR system on languages other than Mandarin, with varying phonetic and orthographic structures.

## Limitations

- The paper's performance claims are built on a specific dataset (MISP2021-AVSR) that may have unique characteristics not generalizable to other AVSR scenarios.
- The reliance on GMM-HMM forced alignment for visual pre-training introduces potential error propagation - any inaccuracies in the audio-based alignments directly affect the quality of visual features learned.
- The architecture's heavy reliance on audio-dominance in the fusion encoder may limit performance in scenarios where visual information is more critical, such as in very noisy environments or when dealing with accents/dialects where lip movements provide crucial disambiguation cues.

## Confidence

- **High Confidence**: The two-stage training framework concept and its rationale for addressing convergence rate mismatches between modalities. The basic premise that decoupling pre-training from fine-tuning can improve performance is well-established in multi-modal learning literature.
- **Medium Confidence**: The lip-subword correlation method's effectiveness and the CMFE architecture's superiority over existing fusion strategies. While the methodology is sound, the specific implementation details and hyperparameters significantly impact performance.
- **Low Confidence**: The generalizability of results to other languages beyond Mandarin and to different types of low-quality video scenarios not represented in the MISP2021 dataset.

## Next Checks

1. **Ablation Study on Audio Dominance**: Systematically vary the depth and parameter allocation between audio and visual branches in the CMFE to quantify the impact of the audio-dominated design choice. This would reveal whether the current architecture is optimal or if visual information is being underutilized.

2. **Cross-Dataset Evaluation**: Test the pre-trained visual frontend on a different AVSR dataset (e.g., LRS3 or VOCA) without fine-tuning to assess the generalizability of the lip-subword correlation approach and identify potential overfitting to the MISP2021 dataset characteristics.

3. **Alignment Quality Analysis**: Conduct a detailed analysis of the GMM-HMM alignment accuracy by comparing automatically generated frame-level syllable boundaries against a small manually annotated subset of the data. This would quantify the potential error introduced during visual pre-training and help establish quality thresholds for the alignment process.