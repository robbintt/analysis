---
ver: rpa2
title: Diffusion Model with Clustering-based Conditioning for Food Image Generation
arxiv_id: '2309.00199'
source_url: https://arxiv.org/abs/2309.00199
tags:
- food
- images
- image
- diffusion
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a clustering-based conditional diffusion
  model, ClusDiff, for generating high-quality synthetic food images. The method addresses
  the challenge of high intra-class variance in food images by clustering images within
  each class into sub-classes and training the diffusion model conditionally on these
  sub-classes.
---

# Diffusion Model with Clustering-based Conditioning for Food Image Generation

## Quick Facts
- **arXiv ID:** 2309.00199
- **Source URL:** https://arxiv.org/abs/2309.00199
- **Reference count:** 40
- **Key outcome:** Introduces ClusDiff, a clustering-based conditional diffusion model that generates high-quality synthetic food images and improves long-tailed food classification through data augmentation.

## Executive Summary
This paper presents ClusDiff, a novel conditional diffusion model for generating high-quality synthetic food images. The key innovation addresses the challenge of high intra-class variance in food images by clustering images within each class into sub-classes and training the diffusion model conditionally on these sub-classes. Experiments on the Food-101 dataset demonstrate superior FID scores compared to fine-tuned stable diffusion and StyleGAN3 baselines, while synthetic images generated by ClusDiff effectively address class imbalance in long-tailed food classification on the VFN-LT dataset.

## Method Summary
ClusDiff operates through a three-stage pipeline: First, it extracts feature vectors from Food-101 images using a fine-tuned ResNet-18 model, then applies affinity propagation clustering to divide each food class into sub-classes based on visual similarity. Second, a pre-trained stable diffusion model is fine-tuned with conditional training using these sub-classes, incorporating cross-attention mechanisms with CLIP text embeddings to condition the denoising process. Finally, the model generates synthetic images conditioned on sub-class labels following the distribution of sub-classes within each food category, producing diverse and high-quality food images that capture cooking variations within classes.

## Key Results
- ClusDiff achieves FID score of 69.67 on Food-101, outperforming fine-tuned stable diffusion (74.38) and StyleGAN3 (107.9)
- On VFN-LT long-tailed classification, ClusDiff improves top-1 accuracy to 83.28% compared to 80.16% for baseline augmentation methods
- Generated synthetic images effectively address class imbalance while maintaining high visual quality and diversity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ClusDiff improves image diversity by conditioning on sub-classes derived from clustering.
- **Mechanism:** Intra-class variance in food images is high due to cooking variations. Clustering maps images to sub-classes using feature vectors from a fine-tuned ResNet-18. The diffusion model then learns conditional distributions per sub-class, allowing the generated images to better represent this variance.
- **Core assumption:** Feature vectors from ResNet-18 capture the relevant visual distinctions between cooking variations within a food class.
- **Evidence anchors:** [abstract]: "food images within each class are clustered as sub-classes, and then the model is trained with these sub-classes."; [section]: "we employ a clustering method to divide food images from a class label into several sub-classes based on their visual representations."; [corpus]: Weak correlation; corpus does not mention clustering-based conditioning for food images.
- **Break condition:** If the clustering step fails to separate meaningfully distinct cooking styles, the conditioning adds noise rather than structure.

### Mechanism 2
- **Claim:** Latent diffusion in latent space reduces computational cost and focuses on salient features.
- **Mechanism:** Instead of operating in pixel space, images are encoded to latent space via a perceptual compression model (encoder-decoder). Diffusion then runs on this lower-dimensional space, making training more efficient and focusing on critical features.
- **Core assumption:** The perceptual compression model (encoder-decoder) retains enough information for high-fidelity reconstruction while reducing dimensionality.
- **Evidence anchors:** [abstract]: "it utilizes a perceptual compression model consisting of an encoderE and a decoder ùê∑."; [section]: "The latent space representation reduces the dimension of the input data, therefore focusing on the critical features of the input data."; [corpus]: Moderate support; corpus neighbors mention latent diffusion models for other tasks.
- **Break condition:** If the encoder-decoder introduces significant artifacts, the reconstruction loss will degrade output quality.

### Mechanism 3
- **Claim:** Cross-attention with CLIP text embeddings conditions the denoising process on sub-class labels.
- **Mechanism:** Sub-class labels are projected into CLIP text embeddings, then added to intermediate layers of the U-Net via cross-attention. This steers the denoising process toward generating images consistent with the sub-class.
- **Core assumption:** CLIP text embeddings for sub-class labels are semantically meaningful and capture visual characteristics of the sub-class.
- **Evidence anchors:** [section]: "we use a pre-trained CLIP text encoder ùúèùúÉ to project ùë¶ùë†ùë¢ùëè into an embedding vector ùúèùúÉ (ùë¶ùë†ùë¢ùëè ), which is then added to the intermediate layers of the U-Net using a cross-attention mechanism."; [abstract]: "When generating images for a specific class of food, ClusDiff is conditioned on its sub-class labels."; [corpus]: No direct evidence in neighbors.
- **Break condition:** If the CLIP embeddings fail to encode visual differences, the conditioning becomes ineffective.

## Foundational Learning

- **Concept:** Diffusion models (forward and reverse processes)
  - **Why needed here:** Understanding the denoising mechanism is critical to modifying the model for conditional generation.
  - **Quick check question:** What is the role of the variance schedule {ùõΩ‚ÇÅ, ..., ùõΩùëá} in the forward diffusion process?

- **Concept:** Affinity propagation clustering
  - **Why needed here:** ClusDiff relies on clustering to form sub-classes; knowing how affinity propagation works is key to tuning it.
  - **Quick check question:** How does affinity propagation determine exemplars without a pre-specified number of clusters?

- **Concept:** CLIP text encoder embeddings
  - **Why needed here:** Conditioning uses CLIP embeddings; understanding their semantic mapping is important for designing sub-class labels.
  - **Quick check question:** What is the output dimensionality of CLIP text embeddings, and how are they typically used in vision-language tasks?

## Architecture Onboarding

- **Component map:** Food-101 images ‚Üí ResNet-18 feature extraction ‚Üí Affinity propagation clustering ‚Üí Diffusion model fine-tuning ‚Üí Cross-attention with CLIP embeddings ‚Üí Synthetic image generation
- **Critical path:** 1. Encode image ‚Üí Cluster into sub-classes 2. Fine-tune diffusion model with sub-class conditioning 3. Generate images using sub-class label + diffusion model
- **Design tradeoffs:** Clustering granularity vs. overfitting: Too many sub-classes may overfit; too few may not capture variance. Encoder choice: ResNet-18 may not capture fine-grained cooking differences; alternatives could be explored. CLIP embeddings: Semantic labels must align with visual features; ambiguous labels degrade conditioning.
- **Failure signatures:** Poor FID scores ‚Üí either diffusion model underfits or clustering poorly represents variance. Mode collapse ‚Üí sub-class conditioning not diverse enough or training data insufficient. Blurry or unrealistic images ‚Üí latent space compression lossy or denoising network underfit.
- **First 3 experiments:** 1. Cluster ablation: Run diffusion without sub-class conditioning; compare FID to baseline. 2. Encoder swap: Replace ResNet-18 with a deeper backbone (e.g., EfficientNet); measure clustering quality. 3. Label semantics: Test different sub-class label formats (e.g., "burger_cooked" vs. "burger_style1") and observe generation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can clustering information be integrated into the loss function or network structure of the diffusion model to better capture the variance of food images?
- **Basis in paper:** [explicit] The authors explicitly mention this as a future research direction in the conclusion section, stating "we plan to investigate integrating cluster information into the loss function or network structure of the diffusion model to better capture the variance of food images."
- **Why unresolved:** The paper does not provide any specific methodology or results for this integration, leaving it as an open research question for future work.
- **What evidence would resolve it:** A proposed method for integrating clustering information into the loss function or network structure, along with experimental results demonstrating improved performance compared to the current approach.

### Open Question 2
- **Question:** How does the performance of the proposed ClusDiff method compare to other state-of-the-art diffusion models specifically designed for food image generation?
- **Basis in paper:** [inferred] While the paper compares ClusDiff to a baseline diffusion model and a GAN-based method, it does not compare it to other state-of-the-art diffusion models specifically designed for food image generation.
- **Why unresolved:** The paper does not provide any comparisons with other food-specific diffusion models, which could provide valuable insights into the relative performance of ClusDiff.
- **What evidence would resolve it:** A comparison of ClusDiff's performance to other state-of-the-art diffusion models specifically designed for food image generation, using the same evaluation metrics and datasets.

### Open Question 3
- **Question:** How does the proposed ClusDiff method perform on other food image datasets with different characteristics (e.g., more diverse food classes, larger intra-class variance)?
- **Basis in paper:** [inferred] The paper only evaluates ClusDiff on the Food-101 dataset, which may not fully represent the diversity and variance present in real-world food images.
- **Why unresolved:** The paper does not provide any results on other food image datasets, which could demonstrate the generalizability and robustness of the proposed method.
- **What evidence would resolve it:** Experiments evaluating ClusDiff on other food image datasets with different characteristics, comparing its performance to the baseline diffusion model and other state-of-the-art methods.

## Limitations
- The clustering granularity is fixed at a maximum of 10 sub-classes per food category without ablation studies on the optimal number
- The paper does not provide a detailed analysis of how the sub-class labels were constructed or whether different semantic formulations impact generation quality
- The choice of clustering algorithm (affinity propagation) and its hyperparameters are not justified or explored

## Confidence

- **High Confidence:** The quantitative improvements in FID scores (69.67 vs. 74.38 for fine-tuned SD and 107.9 for StyleGAN3) and long-tailed classification accuracy gains (83.28% vs. 80.16% baseline) are directly measurable and reproducible
- **Medium Confidence:** The mechanism of clustering-based conditioning is theoretically sound, but the paper lacks ablation studies showing how clustering quality affects generation outcomes
- **Low Confidence:** The choice of clustering algorithm (affinity propagation) and its hyperparameters are not justified or explored, leaving uncertainty about whether this is optimal for the task

## Next Checks

1. **Clustering Sensitivity Analysis:** Systematically vary the number of sub-classes per food category (2, 5, 10, 20) and measure the impact on both FID scores and downstream classification accuracy to identify optimal granularity

2. **Encoder Backbone Comparison:** Replace ResNet-18 with deeper architectures like EfficientNet-B7 or Vision Transformers to evaluate whether feature extraction quality affects clustering effectiveness and generation diversity

3. **Label Semantic Variation:** Test different formulations of sub-class labels (e.g., "burger_grilled" vs. "burger_style1") to determine whether semantic clarity in labels correlates with generation quality and classification performance