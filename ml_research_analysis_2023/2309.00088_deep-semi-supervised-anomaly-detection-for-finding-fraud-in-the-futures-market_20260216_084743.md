---
ver: rpa2
title: Deep Semi-Supervised Anomaly Detection for Finding Fraud in the Futures Market
arxiv_id: '2309.00088'
source_url: https://arxiv.org/abs/2309.00088
tags:
- data
- deep
- anomaly
- labeled
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the effectiveness of a deep semi-supervised
  anomaly detection technique called Deep SAD for detecting fraud in high-frequency
  financial data. The approach leverages a small set of labeled fraud instances alongside
  a large amount of unlabeled data to improve upon purely unsupervised anomaly detection
  methods.
---

# Deep Semi-Supervised Anomaly Detection for Finding Fraud in the Futures Market

## Quick Facts
- **arXiv ID**: 2309.00088
- **Source URL**: https://arxiv.org/abs/2309.00088
- **Reference count**: 12
- **Primary result**: Deep SAD significantly outperforms Deep SVDD on proprietary TMX futures market data, with labeled anomaly scores 1.5-3x higher than unlabeled data scores on average.

## Executive Summary
This paper introduces Deep SAD, a semi-supervised anomaly detection technique that leverages a small set of labeled fraud instances alongside unlabeled data to improve upon purely unsupervised methods. The approach trains a neural network to minimize distances for normal data points from a hypersphere center while maximizing distances for known anomalies in the latent space. Evaluated on proprietary limit order book data from the TMX exchange, Deep SAD demonstrates significant performance improvements over its unsupervised predecessor Deep SVDD, with labeled anomalies receiving substantially higher anomaly scores than unlabeled data.

## Method Summary
The method uses proprietary limit order book data from TMX exchange (5,220,091 data points over one month) with 293 labeled fraud instances. Each data point contains 20 numerical fields representing the top 10 order book levels. A 3-layer feed-forward neural network (100 nodes per hidden layer, ReLU activation) is trained using 3-fold cross-validation. Both Deep SAD and the baseline Deep SVDD undergo autoencoder pre-training for 1,000 epochs, followed by 10,000 epochs of training with their respective objective functions. Performance is evaluated using ratio test (mean anomaly score of labeled data divided by mean anomaly score of unlabeled data) and rank test (average rank of true anomalies when sorted by anomaly score).

## Key Results
- Deep SAD outperforms Deep SVDD baseline with labeled anomaly scores 1.5-3x higher than unlabeled data scores
- The semi-supervised approach effectively incorporates labeled fraud instances into the anomaly detection framework
- The hypersphere distance metric provides an effective scoring mechanism for distinguishing anomalies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep SAD improves anomaly detection by explicitly pushing labeled anomalies away from the hypersphere center during training
- Mechanism: The loss function adds a term that maximizes the distance between the neural network representation of labeled anomalies and the hypersphere center, while minimizing distance for normal data
- Core assumption: Labeled anomalies are true outliers that should be maximally separated from the normal data distribution in the latent space
- Evidence anchors: Abstract states the method maximizes distance for known anomalies; section explains the new objective function

### Mechanism 2
- Claim: The hypersphere distance metric provides an effective anomaly scoring mechanism
- Mechanism: The anomaly score is defined as the distance from the center of the hypersphere in the latent space
- Core assumption: Data points that are farther from the center of the hypersphere in the latent space are more likely to be anomalies
- Evidence anchors: Section defines anomaly score as distance from hypersphere center; autoencoder reconstruction error correlates with anomalousness

### Mechanism 3
- Claim: Pre-training as an autoencoder provides a good initialization for the Deep SAD model
- Mechanism: Both SVDD and Deep SAD undergo unsupervised pre-training as an autoencoder before the main training routine
- Core assumption: Learning to reconstruct the input data helps the neural network learn useful features for anomaly detection
- Evidence anchors: Section states both models undergo 1,000 epochs of autoencoder pre-training; pre-training helps learn good latent representation

## Foundational Learning

- Concept: Hypersphere-based anomaly detection
  - Why needed here: Deep SAD is built on the Deep SVDD framework, which uses a hypersphere to define the boundary of normal data
  - Quick check question: What is the anomaly score in Deep SVDD and Deep SAD?

- Concept: Semi-supervised learning
  - Why needed here: Deep SAD leverages a small amount of labeled anomaly data alongside a large amount of unlabeled data
  - Quick check question: How does Deep SAD use labeled anomalies differently from normal data in its loss function?

- Concept: Autoencoder pre-training
  - Why needed here: Both SVDD and Deep SAD use an autoencoder pre-training phase to initialize the neural network
  - Quick check question: What is the purpose of the autoencoder pre-training phase in Deep SAD?

## Architecture Onboarding

- Component map: Input layer (20 nodes) -> Hidden layers (3 layers, 100 nodes each, ReLU activation) -> Output layer (20 nodes, linear activation) -> Hypersphere center calculation -> Loss function with two terms

- Critical path: 1. Pre-train autoencoder (1,000 epochs) 2. Initialize hypersphere center 3. Train with Deep SAD loss (10,000 epochs) 4. Calculate anomaly scores for test data

- Design tradeoffs: Fixed architecture vs. hyperparameter tuning; model complexity vs. overfitting risk; hypersphere distance vs. other distance metrics

- Failure signatures: Ratio test scores close to 1 indicates model not distinguishing anomalies from normal data; high rank test scores indicate poor anomaly ranking; overfitting to training data indicates poor generalization

- First 3 experiments: 1. Run SVDD and Deep SAD on small data subset to verify implementation and check if Deep SAD outperforms SVDD 2. Vary number of labeled anomalies and observe impact on performance 3. Try different neural network architectures and compare results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Deep SAD compare to other semi-supervised anomaly detection techniques on the same financial fraud dataset?
- Basis in paper: The paper compares Deep SAD to its unsupervised predecessor SVDD but does not compare it to other semi-supervised anomaly detection techniques
- Why unresolved: The paper does not evaluate Deep SAD against other semi-supervised methods, so its relative performance is unknown
- What evidence would resolve it: Running experiments comparing Deep SAD to other semi-supervised techniques like GPND, DSVDD, and Deep SAD with other loss functions on the same dataset would provide a direct comparison

### Open Question 2
- Question: How sensitive is Deep SAD to the choice of hyperparameters like the number of layers, nodes per layer, and the amount of labeled data?
- Basis in paper: The paper uses a fixed neural network architecture and amount of labeled data without exploring the sensitivity to these choices
- Why unresolved: The impact of varying these hyperparameters on the performance of Deep SAD is not investigated in the paper
- What evidence would resolve it: Conducting experiments with different neural network architectures and varying amounts of labeled data would reveal how sensitive Deep SAD is to these choices

### Open Question 3
- Question: Can Deep SAD effectively detect new, unseen types of financial fraud not present in the training data?
- Basis in paper: The authors mention that purely supervised techniques fail to generalize well to unseen anomalies but do not evaluate Deep SAD's ability to detect new types of fraud
- Why unresolved: The experiments use a fixed set of known fraud instances, so Deep SAD's ability to generalize to new types of fraud is unknown
- What evidence would resolve it: Evaluating Deep SAD on a dataset with multiple types of financial fraud, including some not present in the training data, would show how well it can detect new fraud patterns

## Limitations
- Proprietary data from a single exchange (TMX) restricts external validation
- Small number of labeled fraud instances (293 out of 5.2M data points) raises concerns about representativeness
- Fixed neural network architecture without hyperparameter tuning limits generalizability

## Confidence

- **High confidence**: The mechanism by which labeled anomalies are explicitly pushed away from the hypersphere center in Deep SAD's loss function is well-supported by the mathematical formulation and implementation details
- **Medium confidence**: The claim that Deep SAD significantly outperforms Deep SVDD is supported by the reported metrics but requires independent replication due to the proprietary nature of the data
- **Low confidence**: The generalizability of these results to other financial markets or fraud detection domains, given the single data source and specific market microstructure features used

## Next Checks
1. **Replication with public data**: Implement Deep SAD on publicly available financial datasets (e.g., Kaggle credit card fraud datasets) to verify the performance improvements over Deep SVDD in different contexts

2. **Ablation study on labeled data**: Systematically vary the proportion of labeled fraud instances (e.g., 1%, 5%, 10%) to determine the minimum effective labeled data requirement and test the robustness of the semi-supervised approach

3. **Alternative distance metrics**: Replace the hypersphere distance with other distance measures (e.g., Mahalanobis distance, local outlier factor) while keeping the same labeled anomaly incorporation mechanism to isolate the contribution of the distance metric versus the semi-supervised framework