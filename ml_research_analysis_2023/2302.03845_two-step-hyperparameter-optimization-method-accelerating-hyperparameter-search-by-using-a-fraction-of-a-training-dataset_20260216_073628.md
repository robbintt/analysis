---
ver: rpa2
title: 'Two-step hyperparameter optimization method: Accelerating hyperparameter search
  by using a fraction of a training dataset'
arxiv_id: '2302.03845'
source_url: https://arxiv.org/abs/2302.03845
tags:
- dataset
- two-step
- step
- training
- hyperparameter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a two-step hyperparameter optimization (HPO)
  method designed to reduce the computational burden of traditional HPO by using a
  small subset of a training dataset in the first step, followed by retraining the
  top candidates with the entire dataset. This approach is applied to optimize neural
  network emulators for aerosol activation, where only 5% of the training dataset
  was used in the first step to find optimal hyperparameter configurations from 10,000
  trials.
---

# Two-step hyperparameter optimization method: Accelerating hyperparameter search by using a fraction of a training dataset

## Quick Facts
- arXiv ID: 2302.03845
- Source URL: https://arxiv.org/abs/2302.03845
- Reference count: 40
- Key outcome: Two-step HPO method achieves up to 135× speedup using 5% training subset in initial search, finding optimal neural network configurations for aerosol activation modeling

## Executive Summary
This paper introduces a two-step hyperparameter optimization (HPO) method that significantly reduces computational costs by first searching optimal configurations on a small subset of training data, then retraining top candidates on the full dataset. The method is demonstrated for neural network emulators of aerosol activation in climate models, where it identifies optimal architectures with minimal complexity while maintaining high performance. The approach enables extensive HPO trials (10,000 configurations) that would be computationally prohibitive with full-data training, while providing architectural diversity for selecting models based on inference cost requirements.

## Method Summary
The method employs a two-stage random search approach: first, it evaluates 10,000 hyperparameter configurations using only 5% of the training dataset to identify promising architectures; second, it retrains the top 50 models from step one on the complete training dataset. The search space covers neural network architectures with 1-20 hidden layers and 8-2048 nodes per layer. Early stopping with patience=5 and maximum 100 epochs is used in both stages. The approach is designed for data-rich scenarios where subset performance correlates with full-data performance, enabling efficient pruning of the search space while maintaining architectural diversity.

## Key Results
- Achieved 135× speedup compared to full-data HPO while identifying optimal configurations
- 5% training subset size sufficient to find optimal hyperparameters from 10,000 trials
- Top-performing models require at least 2-3 hidden layers and 104 learnable parameters for optimal performance
- Model diversity from extensive search enables selection based on inference cost requirements for climate model integration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A small training subset can identify optimal hyperparameters that generalize to the full dataset
- Mechanism: Hyperparameter evaluations on a subset are indicative of performance on the entire dataset in data-rich limits, allowing efficient pruning of search space
- Core assumption: Model architectures that perform well on a subset will maintain performance when retrained on the full dataset
- Evidence anchors:
  - [abstract] "Using only 5% of a training dataset in the initial step is sufficient to find optimal hyperparameter configurations from much more extensive sampling."
  - [section] "The central idea behind our method is that HPO applied initially to a small subset of a training dataset can effectively identify optimal hyperparameter configurations, thus reducing the overall computational cost."
  - [corpus] Found related work on HPO with subsets (ARLBench, Grouped Sequential Optimization), but none explicitly validates subset-to-full generalization across data-rich regimes
- Break condition: If subset training reveals fundamentally different optimal architectures than full-data training, or if subset size is too small to capture relevant patterns

### Mechanism 2
- Claim: Random search with subset training reduces computational cost while maintaining architectural diversity
- Mechanism: Random search uniformly samples hyperparameters independent of dataset size, enabling efficient parallel exploration even with small subsets
- Core assumption: Random search's independence from preceding evaluations makes it robust to subset-based early filtering
- Evidence anchors:
  - [section] "We use a random search algorithm, which is easy to parallelize and fault-tolerant by design... Despite its simplicity, a random search algorithm is known to be more efficient than—or at least as good as—a grid search."
  - [section] "Unlike a random (or non-adaptive) algorithm that uniformly samples hyperparameters under a given distribution regardless of a training dataset size, an adaptive algorithm may converge towards different optima depending on the training dataset."
  - [corpus] Limited direct evidence; ARLBench benchmarks HPO algorithms but doesn't compare subset vs full-data random search specifically
- Break condition: If adaptive algorithms prove significantly more effective for subset-based HPO, or if subset filtering eliminates high-performing but rare architectures

### Mechanism 3
- Claim: Two-step HPO enables post-hoc analysis of hyperparameter-architecture relationships
- Mechanism: Extensive HPO trials across varying subset sizes reveal minimal model complexity needed for optimal performance
- Core assumption: Model performance vs complexity relationship is stable across subset sizes once top architectures are identified
- Evidence anchors:
  - [section] "The unusually extensive HPO project performed here provides an opportunity to examine the effect of hyperparameters on the model performance... at least two or three hidden layers and 104 learnable parameters are required for optimal performance."
  - [section] "The diversity of resulting top-performing models is another key merit of an extensive HPO... providing users with a range of options depending on their specific application needs."
  - [corpus] No direct corpus support for post-hoc complexity analysis from subset-based HPO; related papers focus on algorithm benchmarking rather than architectural insights
- Break condition: If subset-based filtering introduces bias toward simpler or more complex architectures than optimal for the full dataset

## Foundational Learning

- Concept: Random search vs grid search efficiency
  - Why needed here: Method relies on random search's uniform sampling and parallelization benefits
  - Quick check question: Why is random search often more efficient than grid search for HPO?

- Concept: Early stopping and model overfitting
  - Why needed here: Method uses early stopping with subset training, which may behave differently than full-data training
  - Quick check question: How might early stopping thresholds need adjustment when training on smaller subsets?

- Concept: Hyperparameter search space definition
  - Why needed here: Method searches over number of layers and nodes per layer independently
  - Quick check question: What are the implications of sampling nodes per layer independently vs using a total parameter budget?

## Architecture Onboarding

- Component map:
  Data pipeline: Subset selection → full dataset loading
  Search algorithm: Random search manager-worker distributed execution
  Model training: Early stopping with patience=5, max epochs=100
  Evaluation: Validation MSE tracking, top-k selection
  Retraining: Full dataset retraining of selected architectures

- Critical path: Data subset → Step 1 HPO → top model selection → Step 2 full-data retraining → final evaluation

- Design tradeoffs:
  - Subset size vs. search completeness: Smaller subsets save computation but risk missing optimal architectures
  - Number of trials vs. diversity: More trials increase chances of finding optimal architectures but require more subset computation
  - Early stopping patience: Too short risks premature termination; too long wastes subset computation

- Failure signatures:
  - Large performance gap between subset Step 1 and full-data Step 2 results
  - Step 1 identifies architectures that overfit to subset and generalize poorly
  - Random search fails to explore promising regions of hyperparameter space within computational budget

- First 3 experiments:
  1. Run Step 1 with 5% subset and 1000 trials to verify subset sufficiency and establish baseline performance
  2. Compare Step 1 results from 5%, 25%, and 100% subsets to validate subset-to-full generalization hypothesis
  3. Test different pretrain fractions (e.g., 0.01 vs 0.05) in Step 2 to optimize computational efficiency vs final model quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between psubset and pretrain values for different dataset complexities and model architectures?
- Basis in paper: [inferred] The paper mentions that psubset = 0.05 and pretrain = 0.005 worked well for their specific case study, but notes this may vary depending on dataset complexity
- Why unresolved: The paper only demonstrates one specific combination of values and acknowledges that optimal values may differ across different applications
- What evidence would resolve it: Systematic experiments testing various combinations of psubset and pretrain values across different dataset sizes, complexities, and model architectures to establish general guidelines

### Open Question 2
- Question: How does the two-step HPO method perform with adaptive search algorithms compared to random search?
- Basis in paper: [explicit] The paper states that their method has only been demonstrated using random search and notes that "its applicability with other search algorithms remains to be explored"
- Why unresolved: The paper explicitly limits its investigation to random search and acknowledges this as a limitation
- What evidence would resolve it: Comparative studies applying the two-step method to various adaptive search algorithms (Bayesian optimization, genetic algorithms, etc.) and measuring performance relative to random search

### Open Question 3
- Question: What is the relationship between dataset size and the effectiveness of the two-step HPO method?
- Basis in paper: [inferred] The paper demonstrates effectiveness at 5% subset size for a data-rich scenario but doesn't explore performance boundaries or how this scales to smaller datasets
- Why unresolved: The study focuses on a data-rich scenario with millions of samples and only briefly mentions testing down to 0.0025% without detailed analysis of effectiveness at smaller scales
- What evidence would resolve it: Experiments testing the method across varying dataset sizes, including smaller datasets, to determine minimum effective dataset size and how performance scales with dataset size

## Limitations
- Core assumption that subset-based HPO generalizes to full-data performance remains largely untested across diverse data-rich regimes
- Reliance on random search methodology may not be optimal for all problem domains or model architectures
- Early stopping with subset data introduces uncertainty about convergence behavior compared to full-data training

## Confidence

- **High Confidence**: The two-step methodology structure and computational efficiency gains (135× speedup) are well-supported by the empirical results presented
- **Medium Confidence**: The claim that 5% subset size is sufficient for identifying optimal hyperparameters, based on single-domain testing with specific data characteristics
- **Low Confidence**: Generalization of subset-to-full performance guarantees across different model types, datasets, and learning tasks beyond the aerosol activation case study

## Next Checks

1. **Cross-domain validation**: Apply the two-step HPO method to at least three additional domains (e.g., computer vision, NLP, and tabular data) to test the robustness of the 5% subset threshold

2. **Adaptive algorithm comparison**: Compare random search subset-based HPO against adaptive algorithms like Bayesian optimization or Hyperband to determine if the random search choice is optimal

3. **Early stopping behavior analysis**: Systematically vary early stopping patience parameters and measure their impact on subset training convergence and full-data retraining performance to identify optimal configuration rules