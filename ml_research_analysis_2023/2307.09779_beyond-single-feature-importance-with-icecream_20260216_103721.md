---
ver: rpa2
title: Beyond Single-Feature Importance with ICECREAM
arxiv_id: '2307.09779'
source_url: https://arxiv.org/abs/2307.09779
tags:
- features
- variables
- target
- coalition
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ICECREAM, a method for identifying coalitions
  of variables that explain a given outcome in machine learning models or complex
  systems. Unlike traditional approaches that rank individual feature importance,
  ICECREAM uses an information-theoretic score to quantify the contribution of variable
  coalitions to the target distribution.
---

# Beyond Single-Feature Importance with ICECREAM

## Quick Facts
- arXiv ID: 2307.09779
- Source URL: https://arxiv.org/abs/2307.09779
- Reference count: 21
- This paper introduces ICECREAM, a method for identifying coalitions of variables that explain a given outcome in machine learning models or complex systems.

## Executive Summary
This paper presents ICECREAM, a novel method for identifying coalitions of variables that explain observed outcomes in complex systems. Unlike traditional approaches that rank individual feature importance, ICECREAM uses causal interventions and information-theoretic scores to quantify how variable coalitions contribute to target distributions. The method identifies minimal coalitions that fully explain observed target values, providing more actionable insights about variable interactions rather than just feature rankings. Experiments demonstrate ICECREAM's effectiveness in both machine learning interpretability and root cause analysis tasks.

## Method Summary
ICECREAM identifies explanatory coalitions by measuring the information gain from intervening on variable sets. The method compares interventional distributions under different scenarios: when all variables are free, when only the coalition is fixed, and when all variables are fixed to their observed values. Using KL divergence, it quantifies how much fixing a coalition reduces uncertainty about the target. The algorithm iteratively tests increasing coalition sizes, returning the smallest coalitions that exceed a threshold score. This approach captures interactions between variables that only manifest when considered together, rather than treating features as independent contributors.

## Key Results
- ICECREAM achieves comparable or superior performance to state-of-the-art explainability methods like SHAP for ML model interpretation
- In root cause analysis tasks, ICECREAM significantly outperforms existing methods in identifying true sources of system failures
- The method provides more actionable insights about coalitions of variables rather than just individual feature rankings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICECREAM uses causal interventions to measure how much fixing a coalition of variables reduces the uncertainty in the target distribution.
- Mechanism: By comparing P[Y|do(VC)] to P[Y|do(V)] and P[Y], the method quantifies the relative information gain from intervening on the coalition. When the coalition fully explains the target, P[Y|do(VC)] equals the point mass distribution δy, yielding an explanation score of 1.
- Core assumption: The causal graph structure is known and the positivity assumption holds (interventions have positive observational probability).

### Mechanism 2
- Claim: ICECREAM satisfies the property that explanation scores of coalitions are not simply additive over individual variables.
- Mechanism: The score measures the joint effect of a coalition on the target distribution, not just the sum of individual effects. This captures interactions between variables that only manifest when considered together.
- Core assumption: Interactions between variables can be meaningfully captured through joint interventions on their coalition.

### Mechanism 3
- Claim: ICECREAM can identify minimal coalitions by iteratively testing increasing coalition sizes and stopping when a threshold is reached.
- Mechanism: The algorithm loops through all possible coalitions ordered by increasing size, calculating explanation scores and returning the smallest coalitions that exceed the threshold. This ensures concise explanations.
- Core assumption: There exists a "small" coalition with a "good" explanation score; otherwise, the search becomes computationally intractable.

## Foundational Learning

- Information theory and KL divergence
  - Why needed here: The explanation score is based on comparing distributions using KL divergence as the distance measure.
  - Quick check question: What does KL divergence measure between two probability distributions?

- Causal inference and do-calculus
  - Why needed here: ICECREAM requires knowledge of causal structures to perform interventions and distinguish causal from anti-causal explanations.
  - Quick check question: How does P[Y|do(X)] differ from P[Y|X] in causal inference?

- Feature selection and explainability
  - Why needed here: Understanding how ICECREAM relates to and differs from existing feature selection and explainability methods provides context for its contributions.
  - Quick check question: What is the key difference between ranking individual feature importance and identifying explanatory coalitions?

## Architecture Onboarding

- Component map:
  - Causal graph representation -> Intervention simulator for do-operations -> Explanation score calculator using KL divergence -> Coalition search algorithm (iterative size-based) -> Threshold comparison module

- Critical path:
  1. Load causal graph and observation data
  2. Implement do-calculus to compute interventional distributions
  3. Calculate explanation scores for candidate coalitions
  4. Iteratively search for minimal explanatory coalitions
  5. Return results meeting threshold criteria

- Design tradeoffs:
  - Computational cost vs. coalition size: Larger coalitions provide more complete explanations but are computationally expensive to evaluate.
  - Threshold sensitivity: Lower thresholds find more explanations but may include less relevant coalitions.
  - Graph completeness: Missing causal edges can lead to incorrect explanations or missed opportunities.

- Failure signatures:
  - If all explanation scores are negative, the observation is an outlier not explained by any coalition.
  - If the search doesn't terminate, the maximum coalition size may be set too high or no explanatory coalition exists.
  - If explanation scores are close to zero, the target value is not well-determined by the available variables.

- First 3 experiments:
  1. Verify the method correctly identifies explanatory coalitions in Example 1 (Bernoulli variables with AND target).
  2. Compare ICECREAM explanations with SHAP values on the South German Credit dataset for a few samples.
  3. Test root cause analysis on the synthetic cloud computing application with known ground truth.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ICECREAM be extended to handle continuous target variables, given that the current formulation relies on point mass distributions?
- Basis in paper: The paper explicitly states that the explanation score in its current form cannot be applied for continuous target variables since the point mass distribution δy is not a valid target distribution in this case.
- Why unresolved: The paper identifies this as a limitation but does not propose a solution or alternative formulation for continuous targets.
- What evidence would resolve it: A theoretical extension of the explanation score using probability density functions instead of point masses, or empirical validation on continuous target datasets showing improved explanation quality.

### Open Question 2
- Question: What algorithmic improvements could make coalition identification computationally feasible for large systems with many variables?
- Basis in paper: The paper mentions that iterating over all coalitions becomes computationally expensive as the number of variables increases, particularly when no small coalition with a good explanation score can be found and larger coalitions need to be tested.
- Why unresolved: The paper acknowledges this combinatorial explosion problem but does not propose specific algorithms to address it, only mentioning future research directions.
- What evidence would resolve it: Development and testing of a more efficient search algorithm that leverages the property that {C ∈ 2^I : E(C) = 1} is an upper set of 2^I, with demonstrated runtime improvements on benchmark datasets.

### Open Question 3
- Question: How sensitive is ICECREAM to errors or uncertainty in the known causal structure between variables?
- Basis in paper: The paper assumes knowledge of the causal structure between the target Y and all variables considered as possible explanations, but acknowledges this is a strong restriction and that discovering causal graphs from data is challenging.
- Why unresolved: The paper does not test ICECREAM's performance when the causal structure is partially or incorrectly known, or when there are hidden confounders.
- What evidence would resolve it: Experimental results comparing ICECREAM's explanation quality and accuracy when using correct vs. incorrect causal structures, or methods for robust coalition identification under causal uncertainty.

### Open Question 4
- Question: Can ICECREAM's explanation scores be used to directly identify optimal interventions for changing target values, beyond the current heuristic approach of looping through all possible values?
- Basis in paper: The paper mentions that for directive explanations (changing outcomes), a naive optimization by looping over all explanation scores becomes intractable with increasing numbers of variables and possible realizations, and further research is required to develop an efficient algorithm.
- Why unresolved: The paper only proposes the basic idea of finding values that maximize the explanation score for a desired target, without providing an efficient implementation.
- What evidence would resolve it: Development of an efficient algorithm for directive explanations that scales better than brute-force search, with demonstrated success on systems with many variables and outcomes.

## Limitations

- Computational Scalability: ICECREAM's iterative coalition search becomes computationally intractable for systems with many variables and no small explanatory coalitions.
- Causal Graph Dependency: The method requires complete knowledge of the causal graph structure and assumes the positivity condition holds.
- Threshold Sensitivity: The choice of explanation score threshold significantly impacts results, with suboptimal thresholds yielding either too many or too few explanations.

## Confidence

**High Confidence**: The information-theoretic foundation using KL divergence to measure distributional changes under interventions is mathematically sound and well-established in causal inference literature.

**Medium Confidence**: Claims about superior performance in root cause analysis tasks are supported by synthetic experiments but require validation on real-world failure scenarios with known ground truth.

**Low Confidence**: The assertion that ICECREAM provides more "actionable insights" than SHAP values is subjective and depends on the specific use case and user interpretation of coalition explanations versus individual feature rankings.

## Next Checks

1. **Real-World Root Cause Validation**: Apply ICECREAM to a real cloud computing system with documented failure cases where the true root causes are known, comparing performance against existing RCA methods.

2. **Scalability Benchmark**: Systematically evaluate ICECREAM's performance and runtime as a function of variable count and coalition size on synthetic datasets with controlled graph structures, identifying the practical limits of the approach.

3. **Threshold Sensitivity Analysis**: Conduct a comprehensive study varying the explanation score threshold across multiple datasets to determine how threshold selection affects explanation quality, quantity, and practical utility in different application domains.