---
ver: rpa2
title: Graph-based multimodal multi-lesion DLBCL treatment response prediction from
  PET images
arxiv_id: '2310.16863'
source_url: https://arxiv.org/abs/2310.16863
tags:
- data
- clinical
- lesions
- graph
- imaging
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a graph-based multimodal approach for predicting
  treatment response in diffuse large B-cell lymphoma (DLBCL) patients using PET images.
  The method constructs a lesion graph from imaging data and employs graph attention
  networks (GATv2) to capture inter-lesion relationships.
---

# Graph-based multimodal multi-lesion DLBCL treatment response prediction from PET images

## Quick Facts
- arXiv ID: 2310.16863
- Source URL: https://arxiv.org/abs/2310.16863
- Reference count: 19
- Primary result: Graph-based multimodal model achieves 0.72 ± 0.03 ROC AUC for 2-year PFS classification in DLBCL

## Executive Summary
This paper presents a graph-based multimodal approach for predicting treatment response in diffuse large B-cell lymphoma (DLBCL) patients using PET images. The method constructs a lesion graph from imaging data and employs graph attention networks (GATv2) to capture inter-lesion relationships, with a cross-attention module integrating clinical data. Evaluated on a private multicentric dataset of 583 patients, the model demonstrates superior performance compared to baseline methods, achieving a 2-year progression-free survival (PFS) classification accuracy with a ROC AUC of 0.72 ± 0.03.

## Method Summary
The method constructs a fully connected lesion graph where nodes represent individual lesions with both imaging features and spatial coordinates, and edges are weighted based on proximity and feature similarity. GATv2 layers process the graph to learn adaptive attention weights for message passing between lesions. A cross-attention module integrates clinical features by computing attention scores between lesion graph features and clinical data. The model uses max pooling to aggregate lesion representations and predicts 2-year PFS classification. The approach is trained with weighted binary cross-entropy loss using 10-fold cross-validation on a private multicentric DLBCL dataset.

## Key Results
- Graph-based multimodal model achieves 0.72 ± 0.03 ROC AUC for 2-year PFS classification
- Outperforms classical supervised methods using clinical, imaging, or both data types
- Ablation studies confirm effectiveness of both GATv2 layers and cross-attention fusion mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph representation captures spatial relationships between lesions, improving prediction over lesion averaging.
- Mechanism: Lesion Graph construction encodes both imaging features and spatial coordinates into a fully connected graph with edge weights based on proximity and feature similarity.
- Core assumption: Spatial distribution of lesions carries predictive signal beyond individual lesion features.
- Evidence anchors:
  - Edges eij are drawn between every pair of nodes vi and vj, including self-loops. Weights wij are assigned to each edge to favor message passing between closer and more similar lesions.
  - Experimental results show that our proposed method outperforms classical supervised methods based on either clinical, imaging or both clinical and imaging data.

### Mechanism 2
- Claim: Cross-attention fusion integrates clinical and imaging modalities more effectively than simple concatenation.
- Mechanism: Cross-attention module computes attention scores between lesion graph features and clinical data, allowing lesion-specific clinical weighting.
- Core assumption: Different clinical features have varying relevance to different lesions.
- Evidence anchors:
  - By defining Q = ZGAT and K = V = c(n), the signals assigned to each lesion are updated with the information procured by the clinical data.
  - Experimental results show that our proposed method outperforms classical supervised methods based on either clinical, imaging or both clinical and imaging data.

### Mechanism 3
- Claim: GATv2 layers with learned attention weights improve lesion representation learning compared to GraphConv.
- Mechanism: GATv2 convolution updates node features based on attention-weighted messages from neighboring lesions.
- Core assumption: Not all neighboring lesions are equally relevant for each lesion's representation.
- Evidence anchors:
  - We rely on the GATv2 convolution layer [5] for its capacity to adapt the neighbors' attention weights independently for each node.
  - The ablation studies confirm the effectiveness of both the GATv2 layers and the cross-attention fusion mechanism.

## Foundational Learning

- Graph Neural Networks
  - Why needed here: Handle variable number of lesions and capture inter-lesion relationships
  - Quick check question: What is the key difference between GraphConv and GATv2 layers in terms of message passing?

- Multimodal Fusion
  - Why needed here: Combine clinical biomarkers with imaging-derived features for comprehensive patient representation
  - Quick check question: How does cross-attention differ from late fusion in terms of feature integration?

- Graph Attention Mechanisms
  - Why needed here: Learn lesion-specific attention weights for adaptive information propagation
  - Quick check question: What role do edge weights play in the attention coefficient computation in GATv2?

## Architecture Onboarding

- Component map:
  - Lesion Graph Construction → GATv2 Layers → Cross-Attention Fusion → Max Pooling → Prediction
  - Input: Clinical data vector + PET image with lesion segmentations
  - Output: 2-year PFS classification probability

- Critical path:
  - Clinical features → Cross-attention weighting → GATv2 processing → Max pooling → Final prediction
  - Bottleneck: Lesion segmentation quality affects graph construction and downstream performance

- Design tradeoffs:
  - Fully connected graph vs. sparse graph: Computational cost vs. potential loss of relevant connections
  - Cross-attention vs. concatenation: Model complexity vs. potential overfitting
  - GATv2 vs. GraphConv: Adaptive attention vs. simpler message passing

- Failure signatures:
  - Performance degradation: Check lesion segmentation quality and clinical feature relevance
  - Overfitting: Monitor validation performance and consider regularization
  - Attention weights converging to uniform: May indicate limited inter-lesion relationships

- First 3 experiments:
  1. Replace cross-attention with simple concatenation and compare performance
  2. Vary the number of GATv2 layers and observe impact on validation ROC AUC
  3. Test different edge weight formulations (e.g., only spatial distance, only feature distance)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model perform when trained on larger datasets or data from multiple cancer types?
- Basis in paper: The paper uses a private multicentric dataset of 583 patients, and mentions that the quantity of available data is often limited for this disease.
- Why unresolved: The study is limited to a single dataset and cancer type, so it's unclear if the approach generalizes to larger datasets or other cancers.
- What evidence would resolve it: Experiments showing model performance on larger datasets or on data from multiple cancer types would help determine generalizability.

### Open Question 2
- Question: What is the impact of using graph representations based on lesion sub-regions rather than whole lesions?
- Basis in paper: The paper mentions studying graphs defined on lesions sub-regions as a future perspective to mitigate segmentation variability.
- Why unresolved: The current approach uses whole lesions for the graph representation, so the impact of using sub-regions is unknown.
- What evidence would resolve it: Experiments comparing model performance using graphs based on whole lesions vs. sub-regions would show the impact.

### Open Question 3
- Question: How well does the model handle patients with no lesions detected?
- Basis in paper: The paper constructs a lesion graph and uses GAT layers, which may not work well if no lesions are detected.
- Why unresolved: The paper doesn't discuss how the model handles cases with no lesions, which could be a limitation.
- What evidence would resolve it: Experiments evaluating model performance on patients with no lesions would show how well it handles this case.

## Limitations
- Model evaluated on private multicentric dataset of 583 patients, limiting external validation and generalizability
- Performance depends on manual lesion segmentation using majority vote of three methods, introducing potential inter-observer variability
- Class imbalance addressed through weighted loss, but impact on minority class performance not explicitly discussed

## Confidence

- High Confidence: The superiority of the graph-based multimodal approach over classical supervised methods (ROC AUC 0.72 ± 0.03 vs. baseline methods)
- Medium Confidence: The effectiveness of cross-attention fusion mechanism, as supported by ablation studies but limited by lack of comparison with alternative fusion strategies
- Medium Confidence: The superiority of GATv2 layers over GraphConv, based on ablation results but limited by lack of direct comparison in the paper

## Next Checks
1. External Validation: Evaluate the model on an independent DLBCL dataset from a different institution to assess generalizability
2. Ablation with Alternative Fusion: Replace cross-attention with late fusion and simple concatenation to quantify the specific contribution of the cross-attention mechanism
3. Interpretability Analysis: Conduct a clinical interpretation study to understand which lesions and clinical features contribute most to the predictions