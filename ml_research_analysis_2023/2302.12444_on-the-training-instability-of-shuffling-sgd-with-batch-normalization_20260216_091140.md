---
ver: rpa2
title: On the Training Instability of Shuffling SGD with Batch Normalization
arxiv_id: '2302.12444'
source_url: https://arxiv.org/abs/2302.12444
tags:
- batch
- have
- then
- which
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that using single shuffle SGD with batch normalization
  can lead to undesirable training dynamics, including divergence of the training
  risk. The key insight is that batch normalization is not permutation invariant across
  epochs, which interacts poorly with single shuffle SGD.
---

# On the Training Instability of Shuffling SGD with Batch Normalization

## Quick Facts
- arXiv ID: 2302.12444
- Source URL: https://arxiv.org/abs/2302.12444
- Reference count: 40
- Primary result: Single shuffle SGD with batch normalization can cause training risk divergence, while random reshuffle SGD is more robust.

## Executive Summary
This paper investigates the interaction between shuffling stochastic gradient descent (SGD) and batch normalization (BN) in deep learning training. The key finding is that batch normalization is not permutation invariant across epochs, causing single shuffle SGD (SS) to optimize a distorted risk compared to random reshuffle SGD (RR). For regression problems using linear networks with BN, SS converges to a different distorted optimum than gradient descent, while RR averages out this distortion. For classification, the authors prove that SS can cause training risk divergence in certain regimes, whereas RR provably avoids divergence by maintaining a strongly convex separability decomposition.

## Method Summary
The paper studies the theoretical properties of single shuffle and random reshuffle variants of SGD when used with batch normalization layers in neural networks. Synthetic and real-world datasets including CIFAR10, CIFAR100, and MNIST are used to evaluate convergence to distorted risks and training risk divergence. The method involves training neural networks with BN layers using both SS and RR, varying learning rates and batch sizes, and comparing the resulting training dynamics and risks.

## Key Results
- SS+BN converges to a distorted optimum different from gradient descent for linear regression problems
- SS+BN can cause training risk divergence in classification when the SS-distorted dataset becomes linearly separable but the GD dataset is not
- RR+BN provably avoids divergence by maintaining a strongly convex separability decomposition with high probability

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Batch normalization (BN) is not permutation invariant across epochs, causing SS+BN to optimize a different risk than GD+BN.
- **Mechanism:** When BN normalizes each mini-batch independently, the normalized dataset depends on the ordering of samples. SS fixes this ordering for the entire training, while RR reshuffles each epoch. This causes SS+BN to implicitly optimize the SS-distorted risk Lπ instead of the GD risk LGD.
- **Core assumption:** The distortion introduced by BN's batch-wise normalization interacts with the fixed permutation in SS but averages out in RR due to repeated reshuffling.
- **Evidence anchors:**
  - [abstract] "batch normalization is not permutation invariant across epochs"
  - [section 3.1] "SS data X π encodes the distortion due to the interaction between SS with permutation π and BN"
  - [corpus] Weak; no direct mention of permutation invariance, but neighboring papers focus on DP-SGD and shuffling, suggesting relevance.
- **Break condition:** If the batch size is large enough that batch statistics concentrate (e.g., B = Ω(log n)), monochromatic batches disappear and Z π becomes SC, preventing distortion.

### Mechanism 2
- **Claim:** SS+BN can cause training risk divergence in classification when the SS-distorted dataset becomes linearly separable but the GD dataset is not.
- **Mechanism:** When Z π is PLS or LS but ZGD is SC, the optimal direction v*π of Lπ misclassifies points in ZGD. Since SS+BN converges to the optimum of Lπ, the iterates v(t) inﬁmize Lπ and their projections onto Span(XSC π)⊥ converge to v*π, causing divergence of the GD risk.
- **Core assumption:** The separability decomposition of Z π can differ from that of ZGD due to the distortion from BN and the fixed permutation.
- **Evidence anchors:**
  - [section 4.1] "the SS dataset become LS, whereas the GD dataset stays SC. Hence, by Proposition 4.1.2, the GD risk diverges"
  - [section 4.2] "SS dataset is PLS with unique optimal direction v*π (green dotted line) distorted away from v*GD"
  - [corpus] Weak; no direct mention of separability decomposition or divergence, but neighboring papers discuss DP-SGD privacy, suggesting theoretical interest in SGD variants.
- **Break condition:** If Z π remains SC (e.g., small batch size B = o(log n)), divergence is prevented.

### Mechanism 3
- **Claim:** RR+BN provably avoids divergence by maintaining a strongly convex (SC) separability decomposition.
- **Mechanism:** RR resamples permutations each epoch, which with high probability creates monochromatic batches that make ZRR SC. Since SC datasets have optimal directions orthogonal to all of Rd, the GD risk does not diverge.
- **Core assumption:** With B > 2 and balanced classes, RR almost surely creates a full-rank batch-normalized dataset that is SC.
- **Evidence anchors:**
  - [section 4.1] "Theorem 4.1.4 establishes that even for small B, RR is almost surely robust to divergence as long as d ≤ (B − 1)(n/B)"
  - [section B.3] "ZRR is SC and XRR is full-rank almost surely, regardless of the separability decomposition of ZGD"
  - [corpus] Weak; no direct mention of RR preventing divergence, but neighboring papers discuss shuffling in DP-SGD, suggesting interest in RR variants.
- **Break condition:** If d > (B − 1)(n/B), ZRR becomes PLS deterministically, but this does not cause divergence in the GD risk.

## Foundational Learning

- **Concept:** Permutation invariance
  - **Why needed here:** Understanding that BN is not permutation invariant is key to grasping why SS+BN distorts the risk differently than RR+BN.
  - **Quick check question:** What happens to the batch-normalized dataset if you permute the samples? Does it stay the same for SS vs RR?

- **Concept:** Separability decomposition
  - **Why needed here:** The divergence of the GD risk depends on whether the dataset is LS, PLS, or SC, which is determined by the separability decomposition.
  - **Quick check question:** Given a dataset Z, how do you determine if it is LS, PLS, or SC? What role do monochromatic batches play?

- **Concept:** Monochromatic batches
  - **Why needed here:** Monochromatic batches (batches with all positive or all negative examples) are crucial for determining the separability decomposition of Z π and ZRR.
  - **Quick check question:** How does the presence of monochromatic batches affect the separability decomposition of the batch-normalized dataset?

## Architecture Onboarding

- **Component map:** Original dataset Z = (X, Y) -> Batch normalization (BN) with batch size B -> Single shuffle SGD (SS) or Random reshuffle SGD (RR) -> Linear network with BN layers (f(X; Θ) = W ΓBN(X)) -> Squared loss for regression, logistic loss for classification

- **Critical path:**
  1. Sample a permutation π (fixed for SS, resampled for RR)
  2. Partition data into mini-batches according to π
  3. Apply BN to each mini-batch, creating X π or XRR
  4. Compute gradients and update parameters using SGD
  5. Repeat for multiple epochs, monitoring training risk

- **Design tradeoffs:**
  - SS vs RR: SS is simpler but can cause divergence; RR is more stable but slightly more complex
  - Batch size: Small B prevents divergence but increases variance; large B reduces variance but can cause divergence
  - Model architecture: Deeper networks with BN layers inside can exacerbate divergence

- **Failure signatures:**
  - Training risk diverges or plateaus while validation risk decreases
  - SS+BN converges slower than RR+BN on the same dataset
  - Different separability decompositions for Z π and ZGD

- **First 3 experiments:**
  1. Train a linear+BN network on a synthetic dataset with SS and RR, compare training risks
  2. Vary the batch size and observe the effect on divergence for SS+BN
  3. Train a deeper linear+BN network and compare SS vs RR training dynamics

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does single shuffle SGD with batch normalization always converge to a different optimum than gradient descent for regression problems?
- **Basis in paper:** Explicit - The paper proves that for regression using a linear network with batch normalization, single shuffle SGD converges to a distorted optimum different from gradient descent.
- **Why unresolved:** The paper only provides a proof for a specific case of linear networks with batch normalization. It's unclear if this result generalizes to other architectures or loss functions.
- **What evidence would resolve it:** A theoretical proof or empirical study showing convergence behavior for a broader class of architectures and loss functions under single shuffle SGD with batch normalization.

### Open Question 2
- **Question:** Under what conditions can single shuffle SGD with batch normalization lead to divergence in classification problems?
- **Basis in paper:** Explicit - The paper characterizes conditions under which training divergence for single shuffle SGD can, and cannot occur in classification settings with batch normalization.
- **Why unresolved:** The paper provides theoretical conditions but doesn't offer a complete characterization or practical guidelines for when divergence is likely to occur.
- **What evidence would resolve it:** A comprehensive theoretical analysis or empirical study mapping out the exact conditions (e.g., dataset properties, architecture choices, hyperparameters) that lead to divergence.

### Open Question 3
- **Question:** How does the choice between single shuffle and random reshuffle SGD affect generalization performance when using batch normalization?
- **Basis in paper:** Inferred - The paper focuses on training dynamics and divergence, but mentions in the conclusion that similar phenomena may arise for generalization.
- **Why unresolved:** The paper does not provide any theoretical or empirical results on the impact of shuffling methods on generalization with batch normalization.
- **What evidence would resolve it:** A theoretical analysis or empirical study comparing the generalization performance of models trained with single shuffle vs. random reshuffle SGD, both with batch normalization, across various architectures and datasets.

## Limitations

- The theoretical claims are derived for linear models with BN, which may not fully extend to deeper nonlinear architectures
- The divergence mechanism relies on monochromatic batch creation, which becomes less likely as batch size increases
- The separability decomposition analysis assumes convex losses and may not capture all failure modes in deep networks

## Confidence

- **High** for the linear regression case where exact solutions are available
- **Medium** for the classification divergence results due to assumptions about separability preservation
- **Low** for the nonlinear experimental validations which are limited in scope and may not represent all deep learning scenarios

## Next Checks

1. Test SS+BN with batch sizes spanning the critical regime (B ≈ log n) to empirically verify the transition from divergence to stability.

2. Evaluate deeper MLP architectures (beyond 3 layers) to determine if divergence persists or if additional normalization layers mitigate the effect.

3. Implement the same experiments on more diverse datasets (ImageNet, natural language tasks) to assess generalization beyond the tested image classification benchmarks.