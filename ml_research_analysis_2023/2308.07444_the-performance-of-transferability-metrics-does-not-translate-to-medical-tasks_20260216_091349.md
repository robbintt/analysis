---
ver: rpa2
title: The Performance of Transferability Metrics does not Translate to Medical Tasks
arxiv_id: '2308.07444'
source_url: https://arxiv.org/abs/2308.07444
tags:
- transferability
- target
- medical
- dataset
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Transferability scoring methods are widely used to select pre-trained
  models for transfer learning in computer vision, but their effectiveness in medical
  image analysis remains unclear. This work comprehensively evaluates seven transferability
  scores on three medical classification tasks, including out-of-distribution scenarios.
---

# The Performance of Transferability Metrics does not Translate to Medical Tasks

## Quick Facts
- arXiv ID: 2308.07444
- Source URL: https://arxiv.org/abs/2308.07444
- Reference count: 25
- Primary result: No transferability score consistently predicts performance in medical image analysis due to domain shift

## Executive Summary
This paper investigates the effectiveness of transferability scoring methods for selecting pre-trained models in medical image classification tasks. The authors evaluate seven transferability scores across three medical datasets and 10 ImageNet-pretrained architectures. Results show that transferability scores fail to reliably predict model performance in medical contexts, with no single metric demonstrating consistent correlation across datasets. The instability is attributed to significant domain differences between general-purpose vision datasets and medical imaging data, where feature distributions diverge substantially.

## Method Summary
The study evaluates seven transferability scoring methods (both feature-based and label-based) on three medical classification tasks using pre-trained ImageNet models. For each model-dataset pair, the authors compute in-distribution and out-of-distribution transferability scores, perform hyperparameter optimization using Halton sequences (75 combinations of learning rate and weight decay), and evaluate fine-tuned models on test sets. Kendall's tau correlation is calculated between transferability scores and balanced accuracy to assess predictive power. The experimental setup includes binary and multi-class classification tasks across dermatology, histopathology, and brain tumor imaging datasets.

## Key Results
- No transferability score consistently correlates with actual performance across medical datasets
- Label-based methods excel in out-of-distribution binary tasks but fail in multi-class scenarios
- Domain shift between source and target domains significantly impacts transferability score reliability
- Instability of scores across different medical tasks suggests fundamental limitations for medical image analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transferability scores work well when source and target domains share similar feature spaces and label distributions.
- Mechanism: When feature distributions overlap significantly between source and target datasets, transferability scores can reliably estimate performance by measuring inter-class variance, conditional entropy, or likelihood between source predictions and target labels.
- Core assumption: Underlying data distributions and feature representations are sufficiently similar between source and target domains.
- Evidence anchors: [abstract] scores perform well when datasets share strong similarities in classes and image characteristics [1, 11].

### Mechanism 2
- Claim: Label-based transferability scores excel in out-of-distribution scenarios when the number of classes matches between source and target datasets.
- Mechanism: For binary tasks with matching class numbers, label-based methods can concentrate probability distributions on a single class, inflating transferability scores and providing better out-of-distribution performance prediction.
- Core assumption: The number of classes in source and target datasets is the same, and tasks are binary classification problems.
- Evidence anchors: [abstract] Label-based methods excel in out-of-distribution binary tasks.

### Mechanism 3
- Claim: The instability of transferability scores across different medical datasets is due to significant dissimilarity between source and target domains in medical image analysis.
- Mechanism: Medical image analysis involves substantial domain differences compared to general-purpose computer vision datasets, leading to unstable transferability scores across different medical tasks.
- Core assumption: Medical datasets have more significant domain differences compared to general-purpose computer vision datasets.
- Evidence anchors: [abstract] Our results show that no transferability score can reliably and consistently estimate target performance in medical contexts.

## Foundational Learning

- Concept: Domain shift and its impact on transferability scores
  - Why needed here: Understanding domain shift is crucial for interpreting why transferability scores fail in medical image analysis and how to design better evaluation methods.
  - Quick check question: What is domain shift, and how does it affect the performance of transferability scores in medical image analysis?

- Concept: Feature-based vs. label-based transferability scores
  - Why needed here: Distinguishing between feature-based and label-based transferability scores helps in understanding their different strengths and weaknesses in various scenarios, especially in medical image analysis.
  - Quick check question: What are the key differences between feature-based and label-based transferability scores, and in which scenarios do they perform better?

- Concept: Correlation metrics (e.g., Kendall's tau) for evaluating transferability scores
  - Why needed here: Understanding correlation metrics is essential for interpreting the results of transferability score evaluations and comparing their performance across different datasets and tasks.
  - Quick check question: What is Kendall's tau, and how is it used to evaluate the performance of transferability scores in medical image analysis?

## Architecture Onboarding

- Component map: Transferability scoring methods (feature-based and label-based) -> Medical datasets (ISIC2019, BreakHis, BrainTumor-Cheng, NINS, ICIAR2018, PAD-UFES-20) -> Evaluation metrics (balanced accuracy, Kendall's tau correlation)
- Critical path: 1. Select pre-trained models from ImageNet. 2. Compute in-distribution and out-of-distribution transferability scores. 3. Perform hyperparameter tuning and evaluate fine-tuned models. 4. Calculate correlation between transferability scores and model performance.
- Design tradeoffs: Balancing computational efficiency (using transferability scores instead of exhaustive fine-tuning) with the need for accurate model selection in medical image analysis, where domain differences are significant.
- Failure signatures: Unstable transferability scores across different medical datasets, poor correlation between scores and actual model performance, and domain shift between source and target datasets.
- First 3 experiments:
  1. Evaluate the performance of transferability scores on a small set of medical datasets with known domain similarities to the source dataset.
  2. Compare the performance of feature-based and label-based transferability scores on binary vs. multi-class medical classification tasks.
  3. Investigate the impact of domain shift on transferability scores by gradually increasing the difference between source and target dataset characteristics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do transferability scores perform when the number of classes in the target dataset is significantly different from the source dataset?
- Basis in paper: [explicit] The paper discusses how label-based methods excel in out-of-distribution scenarios, particularly for binary tasks where the number of classes matches the source dataset.
- Why unresolved: The paper suggests that the number of classes affects transferability scores but does not provide a comprehensive analysis of how scores perform when class numbers differ significantly between source and target datasets.
- What evidence would resolve it: Experiments comparing transferability scores across datasets with varying numbers of classes, particularly those with significant mismatches between source and target class counts.

### Open Question 2
- Question: What is the impact of domain shift on the stability of transferability scores across different medical imaging modalities?
- Basis in paper: [explicit] The paper highlights that domain differences between source and target datasets are significant in medical transfer learning.
- Why unresolved: While the paper identifies domain shift as a potential factor, it does not provide a detailed analysis of how different degrees of domain shift affect the stability of transferability scores across various medical imaging modalities.
- What evidence would resolve it: A systematic study measuring the correlation between domain shift magnitude and the stability of transferability scores across multiple medical imaging modalities.

### Open Question 3
- Question: How do transferability scores perform in scenarios with limited training data and class imbalance in medical datasets?
- Basis in paper: [explicit] The paper mentions that future work should assess the robustness of transferability scores regarding limited samples and unbalanced labels.
- Why unresolved: The paper does not evaluate transferability scores under conditions of limited training data and class imbalance, which are common in medical imaging tasks.
- What evidence would resolve it: Experiments evaluating transferability scores on medical datasets with varying levels of class imbalance and limited training samples to determine their robustness and predictive power.

## Limitations

- Evaluation limited to seven transferability metrics and three medical datasets, which may not capture full complexity of transferability challenges in medical imaging
- Does not investigate whether task-specific fine-tuning or domain adaptation techniques could improve transferability score performance
- Computational efficiency gains from using transferability scores versus exhaustive fine-tuning are not quantified

## Confidence

- **High confidence**: Transferability scores show poor correlation with actual performance in medical image analysis
- **Medium confidence**: Domain shift is the primary reason for transferability score failure
- **Medium confidence**: Label-based methods perform better for binary tasks in out-of-distribution scenarios

## Next Checks

1. **Dataset diversity validation**: Test transferability metrics across a broader range of medical imaging domains (radiology, pathology, ophthalmology) to assess generalizability of findings
2. **Domain adaptation baseline**: Compare transferability scores against simple domain adaptation techniques (batch normalization fine-tuning, feature normalization) to determine if these approaches outperform score-based model selection
3. **Task complexity gradient**: Systematically vary task complexity (binary to multi-class with increasing class numbers) while holding domain characteristics constant to isolate the impact of class count on transferability score performance