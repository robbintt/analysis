---
ver: rpa2
title: Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric
  Approach with Hybrid HMM and CNN-TDNN
arxiv_id: '2307.12759'
source_url: https://arxiv.org/abs/2307.12759
tags:
- which
- speech
- urdu
- data
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper describes an implementation of a code-switched Urdu
  ASR system for noisy telephonic environments using a data-centric approach with
  hybrid HMM and CNN-TDNN. The authors address the challenge of transcribing call
  center conversations in Urdu, which involves code-switching with English words and
  numbers, as well as dealing with noisy environments.
---

# Code-Switched Urdu ASR for Noisy Telephonic Environment using Data Centric Approach with Hybrid HMM and CNN-TDNN

## Quick Facts
- arXiv ID: 2307.12759
- Source URL: https://arxiv.org/abs/2307.12759
- Reference count: 40
- Primary result: Achieves 5.2% WER and 20.45% SER on code-switched Urdu ASR in noisy telephonic environments

## Executive Summary
This paper presents a code-switched Urdu automatic speech recognition (ASR) system designed for noisy telephonic environments using a data-centric approach with hybrid HMM and CNN-TDNN architecture. The system addresses the challenges of transcribing call center conversations that involve code-switching between Urdu and English, while handling background noise typical in telephonic settings. Through systematic data collection, preprocessing, and quality labeling, combined with a hybrid HMM-DNN approach that leverages CNN layers for improved frequency dimension capture, the system achieves competitive performance with only 10 hours of training data.

## Method Summary
The method employs a hybrid HMM-DNN approach combining traditional statistical modeling with deep neural networks, specifically adding CNN layers before TDNN to better capture frequency information in noisy speech. The system uses Chain Model training with LF-MMI objective function for efficient parameter estimation. Data preprocessing includes careful audio conversion to mono-channel 16kHz, MFCC feature extraction, and systematic quality labeling. The language modeling handles code-switching by using a unified Roman script for both Urdu and English words, while the acoustic modeling uses Kaldi's factored TDNN with CNN front-end blocks to improve performance in noisy environments.

## Key Results
- Achieves word error rate (WER) of 5.2% on the collected code-switched Urdu dataset
- Achieves sentence error rate (SER) of 20.45% demonstrating effective handling of code-switching
- Demonstrates effectiveness of data-centric approach with limited 10-hour training data
- CNN-TDNN architecture shows improved performance in noisy telephonic environments compared to TDNN alone

## Why This Works (Mechanism)

### Mechanism 1
Using CNN layers before TDNN improves recognition in noisy telephonic environments by capturing additional frequency dimension information from noisy speech. CNN's ability to perform convolutions in both frequency and space dimensions allows it to learn patterns that TDNN alone cannot effectively extract from the same data.

### Mechanism 2
Hybrid HMM-DNN approach enables effective ASR training with limited labeled data by using statistical HMM models to provide high-quality alignments for DNN training. This allows the DNN to learn from properly aligned data without requiring massive labeled datasets typically needed for end-to-end neural approaches.

### Mechanism 3
Data-centric approach with iterative preprocessing and quality labeling significantly improves ASR performance by ensuring high-quality training data. Systematic data collection, preprocessing, and labeling ensures that errors are identified early in the pipeline rather than in later stages where debugging becomes more complex.

## Foundational Learning

- Concept: Mel-frequency Cepstral Coefficients (MFCCs)
  - Why needed here: Standard feature extraction method for speech recognition that converts audio waveforms into format capturing relevant phonetic information while reducing noise and computational complexity
  - Quick check question: What is the primary advantage of using MFCCs over raw audio waveforms for speech recognition?

- Concept: Finite State Transducers (FSTs) in speech recognition
  - Why needed here: Used to compose decoding graphs that combine acoustic models, language models, and pronunciation dictionaries, enabling efficient search through hypothesis space
  - Quick check question: How do FSTs enable efficient decoding in large vocabulary speech recognition systems?

- Concept: Maximum Mutual Information (MMI) objective function
  - Why needed here: Trains models to maximize the gap between correct and incorrect answers rather than just assigning high weights to correct sequences, improving discrimination in noisy environments
  - Quick check question: How does the MMI objective function differ from traditional cross-entropy training in terms of what it optimizes?

## Architecture Onboarding

- Component map: Data preprocessing → Feature extraction (MFCCs) → Language modeling → Acoustic modeling (HMM alignment → CNN-TDNN with LF-MMI) → Decoding (HCLG/HCP graphs) → Scoring (WER/SER) → STT interface
- Critical path: Data preprocessing → Feature extraction → Acoustic modeling → Decoding → Scoring
- Design tradeoffs: HMM-DNN hybrid provides good performance with limited data but requires careful alignment quality; CNN-TDNN adds computational complexity but improves noisy environment performance; data-centric approach requires significant preprocessing effort but yields better results
- Failure signatures: Poor WER/SER despite good data quality suggests model architecture issues; good WER/SER on training data but poor on test data suggests overfitting or data distribution mismatch
- First 3 experiments:
  1. Verify data preprocessing pipeline correctly handles mono-channel 16kHz audio conversion and MFCC extraction
  2. Test basic HMM-GMM model training and decoding on small subset of data to verify feature extraction and alignment work correctly
  3. Train and evaluate CNN-TDNN model on same subset to verify the full pipeline from feature extraction through final scoring

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the code-switched Urdu ASR system degrade in scenarios with overlapping speech, improper pronunciations, and low volume compared to clean speech? The paper mentions performance degradation yielding 74% result but does not specify exact WER/SER metrics for these challenging scenarios.

### Open Question 2
What is the impact of using a unified Roman script for code-switched languages on the accuracy and complexity of the ASR system? The paper discusses using Roman script but does not provide comparative analysis of accuracy and model complexity versus using separate scripts.

### Open Question 3
How does the data-centric approach to ASR training improve the overall quality and reliability of the system compared to a model-centric approach? The paper emphasizes data-centric methodology but does not provide direct comparison of performance and reliability with model-centric approaches.

## Limitations

- Limited details about dataset quality, speaker demographics, and recording conditions that affect generalizability
- Missing critical implementation details for CNN-TDNN architecture configuration and training parameters
- Insufficient evaluation methodology details regarding test set size, composition, and statistical significance of reported metrics

## Confidence

**High Confidence Claims:**
- Hybrid HMM-DNN approach can effectively train ASR models with limited labeled data
- CNN layers can capture frequency-dimension information that benefits noisy speech recognition
- Data-centric preprocessing and quality labeling significantly impact model performance

**Medium Confidence Claims:**
- The specific CNN-TDNN architecture described will achieve 5.2% WER on code-switched Urdu
- The proposed system will generalize well to unseen noisy telephonic environments
- The Chain Model with LF-MMI objective function is optimal for this task

**Low Confidence Claims:**
- The reported WER and SER are achievable with the described methodology
- The system will maintain similar performance across different telephonic recording conditions
- The data-centric approach will scale effectively to larger datasets

## Next Checks

**Validation Check 1:** Reproduce the data preprocessing pipeline on a small subset of publicly available Urdu speech data to verify MFCC extraction, mono-channel 16kHz conversion, and basic feature processing work as described.

**Validation Check 2:** Train a baseline HMM-GMM model on the preprocessed subset to verify alignment quality and feature extraction functionality before attempting the full CNN-TDNN hybrid model.

**Validation Check 3:** Implement the CNN-TDNN architecture with configurable parameters and test on the small subset to verify the complete pipeline from feature extraction through scoring, ensuring each component functions correctly before scaling to the full dataset.