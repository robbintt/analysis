---
ver: rpa2
title: 'A critical look at the evaluation of GNNs under heterophily: Are we really
  making progress?'
arxiv_id: '2302.11640'
source_url: https://arxiv.org/abs/2302.11640
tags:
- datasets
- graph
- node
- nodes
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper critically evaluates existing graph neural networks
  (GNNs) on heterophilous graphs, revealing major issues with commonly used datasets.
  The authors identify significant data leakage in the squirrel and chameleon datasets
  due to duplicate nodes, which artificially inflates model performance.
---

# A critical look at the evaluation of GNNs under heterophily: Are we really making progress?

## Quick Facts
- arXiv ID: 2302.11640
- Source URL: https://arxiv.org/abs/2302.11640
- Reference count: 17
- Primary result: Standard GNNs outperform specialized heterophily models on new benchmark datasets

## Executive Summary
This paper critically examines the evaluation of graph neural networks (GNNs) on heterophilous graphs, revealing significant issues with commonly used benchmark datasets. The authors identify data leakage problems in the squirrel and chameleon datasets due to duplicate nodes, which artificially inflated model performance. They propose a new benchmark of diverse heterophilous graphs and demonstrate that standard GNN architectures (GraphSAGE, GAT, Graph Transformer) consistently outperform specialized heterophily-specific models when ego- and neighbor-embeddings are separated. This challenges the prevailing assumption that specialized methods are necessary for effective learning on heterophilous graphs.

## Method Summary
The authors collected 5 new heterophilous graph datasets from diverse domains and identified data leakage issues in existing benchmarks. They implemented various GNN architectures including standard models (GCN, GraphSAGE, GAT, Graph Transformer) and heterophily-specific models (H2GCN, CPGNN, GPR-GNN, etc.). Models were trained using PyTorch and DGL with specified hyperparameters (hidden dimension 512, dropout 0.2, learning rate 3e-5) for 1000 steps with early stopping. Performance was evaluated using accuracy for multi-class classification and ROC AUC for binary classification tasks.

## Key Results
- Data leakage in squirrel and chameleon datasets due to duplicate nodes artificially inflated model performance
- Standard GNNs (GraphSAGE, GAT-sep, GT-sep) consistently outperform specialized heterophily models on the new benchmark
- Separating ego- and neighbor-embeddings is crucial for strong performance on heterophilous graphs
- Label informativeness better explains GNN performance than traditional homophily measures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standard GNNs outperform specialized heterophily models on new benchmark datasets
- Mechanism: Standard GNNs achieve strong results because separating ego- and neighbor-embeddings helps capture important structural information even under heterophily
- Core assumption: Standard GNN architectures with ego-neighbor separation can learn effective representations on heterophilous graphs without needing specialized modifications
- Evidence anchors:
  - [abstract] "standard GNNs achieve strong results on these heterophilous graphs, almost always outperforming specialized models"
  - [section] "the best results are almost always achieved by models that separate ego- and neighbor-embeddings (GraphSAGE, GAT-sep, GT-sep)"
- Break condition: If ego-neighbor separation is removed from standard GNNs, their performance drops significantly on heterophilous graphs

### Mechanism 2
- Claim: Data leakage in squirrel and chameleon datasets inflated specialized model performance
- Mechanism: Duplicate nodes with identical neighborhoods across train/test splits allowed models to memorize and exploit this leakage
- Core assumption: The presence of duplicate nodes with identical neighborhoods creates artificial performance gains that don't reflect true model capabilities
- Evidence anchors:
  - [section] "The most significant of these drawbacks is the presence of a large number of duplicate nodes in the datasets squirrel and chameleon, which leads to train-test data leakage"
  - [section] "We see a significant performance drop for many models, particularly on the chameleon dataset"
- Break condition: If duplicate nodes are removed and performance remains high, the leakage explanation is incorrect

### Mechanism 3
- Claim: Label informativeness better explains GNN performance than homophily measures
- Mechanism: The mutual information between neighboring nodes' labels determines how much useful signal standard GNNs can extract
- Core assumption: Standard GNNs perform well when there's sufficient label information in the graph structure, regardless of edge homophily
- Evidence anchors:
  - [section] "label informativeness (LI) introduced in Platonov et al. (2022) and shown to better agree with GNN performance than homophily"
  - [section] "Interestingly, this dataset has a larger value of label informativeness compared to all the other heterophilous datasets"
- Break condition: If GNNs perform poorly on graphs with high label informativeness, the mechanism is incorrect

## Foundational Learning

- Concept: Graph Neural Networks and Message Passing
  - Why needed here: The entire paper is about evaluating GNN performance on heterophilous graphs
  - Quick check question: How does a standard GCN update node representations using its neighbors?

- Concept: Homophily vs Heterophily in graphs
  - Why needed here: The paper challenges the assumption that specialized models are needed for heterophilous graphs
  - Quick check question: What's the difference between edge homophily and adjusted homophily?

- Concept: Data leakage and duplicate detection
  - Why needed here: The paper identifies duplicate nodes causing train-test leakage in standard datasets
  - Quick check question: How would you detect duplicate nodes in a graph dataset?

## Architecture Onboarding

- Component map: Graph Data → Preprocessing → Model Training → Evaluation → Results Analysis
- Critical path: Data → Preprocessing → Model training → Evaluation → Results analysis
- Design tradeoffs: Standard GNNs vs specialized models, dataset size vs. diversity, homophily measures vs. label informativeness
- Failure signatures: Overfitting on duplicate nodes, poor generalization to new heterophilous graphs, reliance on data leakage
- First 3 experiments:
  1. Replicate the duplicate node detection on squirrel/chameleon datasets and measure performance drop
  2. Compare standard GNNs with ego-neighbor separation vs without on the new benchmark
  3. Evaluate the correlation between label informativeness and GNN performance across all datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would standard GNNs perform on other types of heterophilous graphs beyond those tested in this study?
- Basis in paper: [explicit] The paper demonstrates strong performance of standard GNNs on their proposed benchmark datasets but acknowledges these represent only a subset of possible heterophilous graphs.
- Why unresolved: The benchmark covers only 5 new datasets from specific domains, and the space of possible heterophilous graph structures is vast and diverse.
- What evidence would resolve it: Testing standard GNNs on a broader range of heterophilous graphs from different domains and with varying structural properties would help determine if the observed performance generalizes.

### Open Question 2
- Question: What is the fundamental reason that separating ego- and neighbor-embeddings consistently improves GNN performance on heterophilous graphs?
- Basis in paper: [explicit] The paper observes that models with ego-neighbor embedding separation (GraphSAGE, GAT-sep, GT-sep) consistently outperform their counterparts without this separation.
- Why unresolved: While the empirical observation is clear, the paper does not provide theoretical justification for why this architectural choice is particularly beneficial for heterophilous graphs.
- What evidence would resolve it: A theoretical analysis or ablation studies examining how information flows differently with and without embedding separation on heterophilous graphs would clarify the underlying mechanism.

### Open Question 3
- Question: How would the performance rankings of different models change if the squirrel and chameleon datasets were corrected using alternative methods beyond simply removing duplicates?
- Basis in paper: [explicit] The paper identifies duplicate nodes as a major source of data leakage and shows performance changes after removal, but notes that different models are affected differently.
- Why unresolved: The paper only explores one method of correction (removing duplicates) and doesn't investigate whether other correction strategies might lead to different conclusions about relative model performance.
- What evidence would resolve it: Evaluating models on squirrel and chameleon datasets corrected through alternative approaches (e.g., keeping one representative from each duplicate group, or using cross-validation that accounts for duplicates) would reveal if the observed performance rankings are robust to different correction methods.

## Limitations
- The findings are limited to node classification tasks and may not generalize to other GNN applications like link prediction or graph classification
- The proposed benchmark, while diverse, covers only a specific subset of heterophilous graph structures and may not represent all possible scenarios
- The analysis focuses on specific GNN architectures and may not capture the full space of potential solutions for heterophilous graphs

## Confidence
- **High Confidence:** The data leakage discovery in squirrel and chameleon datasets is well-supported with clear evidence of duplicate nodes and their impact on performance
- **Medium Confidence:** The benchmark results showing standard GNNs outperforming specialized models are compelling but require independent validation on additional datasets
- **Medium Confidence:** The claim about label informativeness being a better predictor than homophily measures needs further empirical verification across more diverse graph types

## Next Checks
1. **Independent Replication:** Replicate the duplicate node detection and performance analysis on the squirrel and chameleon datasets using a different codebase to verify the data leakage findings.

2. **Broader Benchmark Testing:** Evaluate the standard vs. specialized GNN performance on additional heterophilous datasets from different domains not included in the current benchmark.

3. **Ablation Study:** Conduct a systematic ablation study varying ego-neighbor separation in standard GNNs to quantify its exact contribution to performance on heterophilous graphs.