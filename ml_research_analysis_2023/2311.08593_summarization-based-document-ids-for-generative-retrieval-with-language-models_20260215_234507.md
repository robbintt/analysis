---
ver: rpa2
title: Summarization-Based Document IDs for Generative Retrieval with Language Models
arxiv_id: '2311.08593'
source_url: https://arxiv.org/abs/2311.08593
tags:
- document
- retrieval
- generative
- documents
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ACID (Abstractive, Content-based IDs), a
  novel method for creating document identifiers in generative retrieval. Instead
  of using integer sequences from hierarchical clustering, ACID uses abstractive keyphrases
  generated by a language model to serve as document IDs.
---

# Summarization-Based Document IDs for Generative Retrieval with Language Models

## Quick Facts
- **arXiv ID:** 2311.08593
- **Source URL:** https://arxiv.org/abs/2311.08593
- **Reference count:** 3
- **Primary result:** ACID improves top-10 and top-20 recall by 15.6% and 14.4% (relative) versus cluster-based integer ID baseline on MSMARCO 100k

## Executive Summary
This paper introduces ACID (Abstractive, Content-based IDs), a novel method for creating document identifiers in generative retrieval. Instead of using integer sequences from hierarchical clustering, ACID uses abstractive keyphrases generated by a language model to serve as document IDs. This approach leverages the LM's natural language generation capabilities and produces human-readable IDs. Experiments on MSMARCO and Natural Questions datasets show that ACID improves top-10 and top-20 recall by 15.6% and 14.4% (relative) respectively versus the cluster-based integer ID baseline on MSMARCO 100k, and 9.8% and 9.9% respectively on Natural Questions 100k. The results demonstrate the effectiveness of human-readable, natural-language IDs created through summarization for generative retrieval.

## Method Summary
ACID generates document IDs by using GPT-3.5 to create 5 abstractive keyphrases per document, which are then concatenated to form the ID. These natural language IDs are used in place of traditional integer sequences from hierarchical clustering. A pretrained language model (Pythia) is then fine-tuned on retrieval tasks using synthetic queries and user-generated queries from MSMARCO and Natural Questions datasets. During retrieval, constrained beam search decoding is used to generate the most relevant document ID for a given query. The method aims to leverage the language model's natural language generation capabilities rather than forcing it to generate arbitrary integer sequences.

## Key Results
- ACID improves top-10 recall by 15.6% (relative) versus cluster-based integer ID baseline on MSMARCO 100k
- ACID improves top-20 recall by 14.4% (relative) versus cluster-based integer ID baseline on MSMARCO 100k
- ACID improves top-10 recall by 9.8% (relative) versus cluster-based integer ID baseline on Natural Questions 100k

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models generate abstractive keyphrases more naturally than arbitrary integer sequences, leading to better retrieval performance.
- Mechanism: Large language models are pretrained to generate natural language text, not sequences of integers. By using abstractive keyphrases generated by an LM as document IDs, the model can leverage its natural language generation capabilities instead of learning to generate arbitrary integer sequences that don't align with its pretraining objective.
- Core assumption: The language model's pretraining objective of generating natural language text is transferable to the task of generating document IDs that are also natural language phrases.
- Evidence anchors:
  - [abstract]: "Instead of using integer sequences from hierarchical clustering, ACID uses abstractive keyphrases generated by a language model to serve as document IDs. This approach leverages the LM's natural language generation capabilities and produces human-readable IDs."
  - [section]: "Instead of retrieving documents based on cosine similarity, generative retrieval uses an LM to produce a sequence of tokens encoding the relevant document's ID, conditional on the query. Decoding constraints are applied to ensure that only document IDs that exist in the corpus are generated."
  - [corpus]: Weak evidence - the corpus does not directly address this mechanism.

### Mechanism 2
- Claim: Using content-based IDs that summarize document topics improves retrieval accuracy compared to cluster-based IDs.
- Mechanism: Content-based IDs (ACID) capture the main topics or keyphrases of a document, providing more semantic information than cluster-based integer IDs. This additional semantic information helps the language model better associate queries with relevant documents during retrieval.
- Core assumption: Keyphrases generated by the language model accurately capture the main topics of the document and provide more semantic information than cluster-based IDs.
- Evidence anchors:
  - [abstract]: "We find that abstractive, content-based IDs (ACID) and an ID based on the first 30 tokens are very effective in direct comparisons with previous approaches to ID creation."
  - [section]: "From the example, it is clear why we would expect ACID to outperform cluster-based IDs, since it is straightforward for LMs to generate the keyphrase sequence given an engineering-related query. The cluster ID, on the other hand, resembles an integer hash of the document."
  - [corpus]: Weak evidence - the corpus does not directly address this mechanism.

### Mechanism 3
- Claim: Wider beam widths improve retrieval performance more for ACID than for cluster-based IDs.
- Mechanism: ACID uses natural language IDs with a full vocabulary, allowing for more diverse and semantically meaningful document IDs. Wider beam widths can explore a larger space of possible document IDs, leading to better retrieval performance. Cluster-based IDs are limited to a small set of integers, so wider beams provide diminishing returns.
- Core assumption: The full vocabulary of the language model is used for ACID, allowing for more diverse and semantically meaningful document IDs compared to cluster-based IDs.
- Evidence anchors:
  - [abstract]: "Finally, we show that adjusting decoding hyperparameters like beam width can improve retrieval performance with ACID, whereas cluster-based IDs only experience a marginal benefit with wider beams."
  - [section]: "As discussed previously, the cluster ID is typically restricted to a small number of clusters per level (the digits 0 through 9, for example), and so a wide beam in excess of that number doesn't yield any improvements, whereas ACID does benefit from wider beams, since it is a natural-language ID with access to the full vocabulary of the LM."
  - [corpus]: Weak evidence - the corpus does not directly address this mechanism.

## Foundational Learning

- Concept: Hierarchical clustering for document indexing
  - Why needed here: Understanding how cluster-based IDs are created and their limitations compared to ACID.
  - Quick check question: How does hierarchical clustering work for creating document IDs in generative retrieval, and what are its limitations compared to ACID?

- Concept: Abstractive summarization
  - Why needed here: ACID relies on generating abstractive keyphrases to summarize document contents for use as document IDs.
  - Quick check question: What is abstractive summarization, and how does it differ from extractive summarization in the context of generating document IDs?

- Concept: Beam search decoding
  - Why needed here: Understanding how beam search decoding works and how it can be used to generate document IDs in generative retrieval.
  - Quick check question: How does beam search decoding work in the context of generative retrieval, and how does it affect the generation of document IDs?

## Architecture Onboarding

- Component map: Document preprocessing -> Generate abstractive keyphrases -> Document indexing -> Train LM to associate spans/queries with IDs -> Retrieval using constrained beam search

- Critical path:
  1. Preprocess documents to generate abstractive keyphrases
  2. Index documents by training the language model to associate document spans and synthetic queries with their IDs
  3. Retrieve documents by generating the most relevant document ID for a given user query

- Design tradeoffs:
  - Using abstractive keyphrases vs. extractive summaries or cluster-based IDs
  - The choice of language model for generating keyphrases (GPT-3.5 vs. other LMs)
  - The number of keyphrases to generate for each document ID (5 in this case)
  - The beam width for constrained decoding during retrieval

- Failure signatures:
  - Poor retrieval performance due to ineffective document IDs (e.g., keyphrases not accurately representing document contents)
  - High collision rate (multiple documents with the same ID)
  - Slow retrieval due to inefficient decoding or large document corpus

- First 3 experiments:
  1. Evaluate retrieval performance with ACID vs. cluster-based IDs on a small dataset (e.g., NQ 1k)
  2. Test the impact of different language models (e.g., GPT-3.5 vs. Pythia) for generating document IDs
  3. Assess the effect of varying the number of keyphrases per document ID on retrieval performance

## Open Questions the Paper Calls Out

- Question: How does the performance of ACID scale when applied to web-scale datasets like C4 (750GB) compared to smaller datasets like MSMARCO?
  - Basis in paper: [inferred] The authors hypothesize that natural-language IDs will scale better than cluster integer IDs as the number of documents increases, but they do not provide experimental evidence for web-scale datasets.
  - Why unresolved: The paper only tests ACID on relatively small datasets (MSMARCO and NQ), and does not explore its performance on larger, web-scale datasets.
  - What evidence would resolve it: Experimental results comparing ACID performance on web-scale datasets like C4 to its performance on smaller datasets, and comparisons with cluster-based approaches on the same web-scale data.

- Question: How does the choice of LM for generating abstractive keyphrases (e.g., GPT-3.5 vs. other LMs) impact the retrieval performance of ACID?
  - Basis in paper: [explicit] The authors mention that any reasonable pretrained LM can be used to generate keyphrases, but they only experiment with GPT-3.5.
  - Why unresolved: The paper does not compare the performance of ACID when using different LMs for keyphrase generation.
  - What evidence would resolve it: Retrieval performance results for ACID using different LMs (e.g., GPT-3.5, Pythia, etc.) on the same datasets.

- Question: What is the impact of document characteristics (e.g., length, structure) on the effectiveness of ACID compared to other content-based ID methods?
  - Basis in paper: [explicit] The authors observe that extractive IDs outperformed abstractive IDs on Wikipedia articles in NQ but not the snippets in MSMARCO, suggesting that document characteristics affect generative retrieval performance.
  - Why unresolved: The paper does not systematically investigate how different document characteristics influence the effectiveness of ACID versus other content-based ID methods.
  - What evidence would resolve it: Comparative analysis of ACID and other content-based ID methods across datasets with varying document characteristics (e.g., length, structure, domain).

- Question: How does the performance of ACID change with different document ID lengths, and what is the optimal length for various types of documents?
  - Basis in paper: [explicit] The authors experiment with different ID lengths (10, 20, 30, 40 tokens) and observe varying performance, but do not determine an optimal length.
  - Why unresolved: The paper provides results for different ID lengths but does not establish a clear relationship between ID length and retrieval performance, nor does it identify an optimal length for different document types.
  - What evidence would resolve it: Systematic experiments varying ID lengths across different document types and datasets, with analysis to determine optimal lengths for each scenario.

## Limitations
- Evaluation limited to two datasets (MSMARCO and Natural Questions), which may have characteristics that favor abstractive keyphrase representations
- Computational overhead of generating abstractive keyphrases using GPT-3.5 for document indexing is not quantified
- Performance advantage may not generalize across diverse document types and domains

## Confidence

- High confidence: Basic finding that ACID outperforms cluster-based IDs on tested datasets
- Medium confidence: Mechanism explanation (natural language generation capability transfer)
- Medium confidence: Beam width advantage claim
- Low confidence: Generalization to other document types and retrieval scenarios

## Next Checks

1. **Cross-domain validation**: Test ACID on datasets from different domains (medical literature, legal documents, news articles) to verify that the performance advantage generalizes beyond web documents and question-answer pairs.

2. **Computational overhead analysis**: Measure and compare the time and cost required to generate document IDs using GPT-3.5 versus the computational cost of the retrieval model itself, particularly for large-scale applications.

3. **Semantic accuracy assessment**: Conduct human evaluation of whether the generated keyphrases accurately represent document content and whether they capture the same semantic information that would be useful for retrieval, comparing this to the semantic information captured by cluster-based IDs.