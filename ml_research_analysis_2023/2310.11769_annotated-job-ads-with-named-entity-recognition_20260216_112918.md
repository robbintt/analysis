---
ver: rpa2
title: Annotated Job Ads with Named Entity Recognition
arxiv_id: '2310.11769'
source_url: https://arxiv.org/abs/2310.11769
tags:
- annotation
- class
- process
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We improved the manual annotation of job ads for named entity recognition
  by combining iterative annotation with bootstrapping, active learning, and strict
  cross-checking. The dataset was created in batches, where each sample was annotated
  by two people and conflicts were resolved in collective sessions.
---

# Annotated Job Ads with Named Entity Recognition

## Quick Facts
- arXiv ID: 2310.11769
- Source URL: https://arxiv.org/abs/2310.11769
- Reference count: 2
- Key outcome: Improved manual annotation of job ads for NER using iterative annotation with bootstrapping, active learning, and cross-checking, achieving entity-level F1 scores of 0.48-0.90 (micro-average 0.72)

## Executive Summary
This paper presents an iterative annotation framework for creating a Swedish NER dataset from job advertisements. The approach combines double annotation with collective conflict resolution, bootstrapping (using model predictions as pre-labels), active learning (uncertainty sampling), and dynamic class system simplification. The framework was applied to create a dataset of 260 job ads annotated for 10 entity types. The final KB-BERT model achieved reasonable performance (entity-level F1 0.48-0.90, micro-average 0.72) given the task complexity, demonstrating that combining multiple annotation optimization techniques can effectively bootstrap high-quality labeled data.

## Method Summary
The method involves iterative annotation of Swedish job ads in batches, where each sample is annotated by two annotators and conflicts are resolved in collective sessions. The process uses bootstrapping to accelerate annotation by having the model pre-label samples for correction, and active learning to select more informative samples based on uncertainty. The class system is dynamically simplified when performance does not improve. KB-BERT is fine-tuned on the progressively labeled dataset with early stopping, and entity-level F1 scores are used to evaluate performance per class and overall.

## Key Results
- KB-BERT model achieved entity-level F1 scores between 0.48 and 0.90 per class
- Micro-average F1 score of 0.72 on the test set
- Bootstrapping accelerated annotation by allowing model predictions to pre-label samples
- Dynamic class system simplification improved annotation efficiency
- Active learning helped select more informative samples for annotation

## Why This Works (Mechanism)

### Mechanism 1: Iterative Annotation with Cross-Checking
Annotating each sample twice and resolving conflicts in collective sessions improves label quality by reducing systematic errors and individual biases. The core assumption is that annotator disagreements are informative and resolvable through discussion, leading to higher-quality consensus labels.

### Mechanism 2: Bootstrapping with Early Stopping
Using model predictions as pre-labels accelerates annotation without sacrificing quality by allowing annotators to focus on corrections rather than starting from scratch. The core assumption is that model predictions improve sufficiently with each iteration to meaningfully assist human annotators.

### Mechanism 3: Dynamic Class System Simplification
Simplifying the class system based on performance feedback improves model learning and annotation efficiency by reducing task complexity. The core assumption is that class performance issues stem from task difficulty rather than fundamental data problems.

## Foundational Learning

- Concept: Transformer-based language models and fine-tuning
  - Why needed here: The work builds on BERT/KB-BERT as the base model, requiring understanding of pre-training and transfer learning
  - Quick check question: What is the difference between pre-training and fine-tuning in the context of BERT?

- Concept: Named Entity Recognition task structure
  - Why needed here: Understanding token-level vs entity-level evaluation metrics and BIO tagging schemes
  - Quick check question: How does entity-level F1 differ from token-level F1 in NER evaluation?

- Concept: Active learning acquisition functions
  - Why needed here: The paper uses uncertainty sampling to select informative samples
  - Quick check question: What is the intuition behind uncertainty sampling in active learning?

## Architecture Onboarding

- Component map: Raw job ads → annotation interface → merged labels → training data → KB-BERT fine-tuning → evaluation → class adjustment
- Critical path: Annotators complete individual annotations → system merges annotations and flags conflicts → annotators resolve conflicts in collective sessions → model is fine-tuned on updated dataset → performance is evaluated and class system is adjusted if needed
- Design tradeoffs:
  - Annotation quality vs. speed: Double annotation + resolution ensures quality but increases time per sample
  - Model assistance vs. human autonomy: Bootstrapping helps but risks annotator over-reliance
  - Class granularity vs. learnability: More classes capture more information but are harder to annotate consistently
- Failure signatures: Stagnant model performance despite more data, low inter-annotator agreement even after discussion, annotator fatigue or confusion about class boundaries, model overfitting to annotation artifacts
- First 3 experiments:
  1. Run the annotation pipeline on a small batch (10-20 samples) to validate the merge/resolution process works as intended
  2. Fine-tune KB-BERT on the initial annotated data and verify the early stopping mechanism functions correctly
  3. Implement active learning selection on a validation set to confirm it identifies genuinely challenging samples

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal number of annotators per batch to maximize efficiency without compromising data quality? The paper mentions using 5 annotators but does not explore the impact of varying the number of annotators. Experiments comparing annotation quality and time with different numbers of annotators (e.g., 2, 3, 4, 5) on the same dataset would resolve this.

### Open Question 2
How does the effectiveness of active learning compare to random sampling in this specific domain and class system? The authors acknowledge that there is no guarantee active learning helps in every use case, especially given the specific nature of their job ad dataset. A controlled experiment comparing model performance and annotation time when using active learning versus random sampling on the same job ad dataset would resolve this.

### Open Question 3
What is the long-term impact of class system simplification on downstream applications? The authors simplified their class system from 16 to 10 classes based on performance metrics, but do not discuss the potential impact on downstream applications that may have relied on the original, more granular classes. User studies or application tests showing how the simplified class system affects the performance of downstream tasks compared to using the original, more detailed class system would resolve this.

## Limitations

- Active learning acquisition function and batch selection strategy are not specified
- Dynamic class system simplification lacks concrete rules for when and how classes should be merged or dropped
- Evaluation focuses solely on entity-level F1 scores without token-level metrics or ablation studies on individual framework components

## Confidence

**High Confidence**: The core methodology of double annotation with collective conflict resolution is clearly described and represents the paper's main contribution. The final KB-BERT model architecture and evaluation setup are well-specified.

**Medium Confidence**: The bootstrapping and active learning components are described at a conceptual level but lack implementation details. The reported F1 scores (0.48-0.90, micro-average 0.72) are reasonable for this task complexity.

**Low Confidence**: The adaptive class system simplification mechanism is described but lacks specific criteria for class adjustments.

## Next Checks

1. **Annotator Agreement Validation**: Run the annotation pipeline on a small pilot batch (10-20 samples) and calculate inter-annotator agreement metrics (Cohen's kappa or similar) to verify that the double-annotation + collective resolution approach achieves meaningful consensus.

2. **Model Component Isolation**: Implement a simplified version of the pipeline without active learning and bootstrapping to determine whether the reported performance gains stem primarily from the annotation quality improvements or the ML optimization techniques.

3. **Class Definition Stability**: Conduct a stability analysis by having a subset of samples annotated by the same annotators after a time gap, measuring whether class boundary definitions remain consistent and identifying which entity types show the most annotation variability.