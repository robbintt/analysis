---
ver: rpa2
title: It's All Relative! -- A Synthetic Query Generation Approach for Improving Zero-Shot
  Relevance Prediction
arxiv_id: '2311.07930'
source_url: https://arxiv.org/abs/2311.07930
tags:
- query
- relevant
- queries
- generate
- irrelevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to synthetic query generation
  for improving zero-shot relevance prediction in information retrieval. The key idea
  is to generate queries simultaneously for different relevance labels, conditioning
  the generation of one query on another.
---

# It's All Relative! -- A Synthetic Query Generation Approach for Improving Zero-Shot Relevance Prediction

## Quick Facts
- arXiv ID: 2311.07930
- Source URL: https://arxiv.org/abs/2311.07930
- Reference count: 16
- Key outcome: Pairwise query generation improves zero-shot relevance prediction, achieving 8.6 NDCG@10 point average gain over existing methods

## Executive Summary
This paper proposes a novel approach to synthetic query generation for improving zero-shot relevance prediction in information retrieval. The key idea is to generate queries simultaneously for different relevance labels, conditioning the generation of one query on another. This pairwise query generation method simplifies the task for large language models compared to generating queries in isolation. Extensive experiments on seven IR datasets show that the proposed approach leads to improved downstream performance, with an average gain of 8.6 NDCG@10 points over existing methods. Notably, for three datasets, the model trained solely on synthetic data even outperforms the strong transfer learning baseline.

## Method Summary
The method uses a pairwise generation approach where an LLM generates both relevant and irrelevant queries simultaneously, conditioning the irrelevant query on the relevant one. The process involves constructing prompts using task-specific examples, generating synthetic queries using PaLM-2, filtering generated queries using LLM-based filtration, training a downstream model (mt5-XXL) on filtered synthetic data, and evaluating performance using NDCG@10 metric. The approach includes an iterative refinement step where relevant queries are first generated and filtered, then used to condition the generation of irrelevant queries.

## Key Results
- Average gain of 8.6 NDCG@10 points over existing synthetic query generation methods
- For three datasets, synthetic data training outperformed strong transfer learning baseline
- Effective for fine-grained relevance prediction across multiple labels
- Demonstrates effectiveness of pairwise query generation in producing high-quality synthetic data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pairwise query generation improves downstream relevance prediction by reducing the reasoning burden on LLMs.
- Mechanism: Instead of generating queries in isolation, the LLM is prompted to generate a relevant query and an irrelevant query simultaneously, conditioning the generation of one query on the other. This leverages the LLM's ability to compare items relative to each other rather than judging them in isolation.
- Core assumption: LLMs find relative comparisons easier than absolute judgments.
- Evidence anchors: Abstract states that generating an irrelevant query relative to a relevant query is simpler for the model to reason about.

### Mechanism 2
- Claim: Conditioning query generation on relevance labels improves the quality of generated queries.
- Mechanism: By including the desired relevance label in the prompt, the LLM is guided to generate queries that align with the specified label. This helps capture fine-grained nuances in relevance.
- Core assumption: Providing explicit labels helps the LLM understand the desired output better.
- Evidence anchors: Abstract mentions conditioning QGen model on relevance labels to generate queries across relevance buckets.

### Mechanism 3
- Claim: Iterative pairwise generation improves query quality by refining the irrelevant query generation.
- Mechanism: First, relevant queries are generated and filtered. Then, the LLM is prompted again with the document and the filtered relevant query to generate an irrelevant query. This iterative process ensures that the irrelevant query is conditioned on a high-quality relevant query.
- Core assumption: Iterative refinement leads to better-conditioned outputs.
- Evidence anchors: Section 2.2 describes prompting the LLM again with the filtered relevant query to generate an irrelevant query.

## Foundational Learning

- Concept: Large Language Models (LLMs) and their ability to generate synthetic data.
  - Why needed here: Understanding how LLMs can be used to generate high-quality synthetic query-document pairs is crucial for implementing the proposed approach.
  - Quick check question: What are the key differences between using LLMs for query generation versus traditional methods?

- Concept: Relevance prediction in information retrieval.
  - Why needed here: The ultimate goal of the proposed approach is to improve relevance prediction for tasks with no training data. Understanding the fundamentals of relevance prediction is essential for evaluating the effectiveness of the generated queries.
  - Quick check question: How does relevance prediction differ from other IR tasks like document retrieval or ranking?

- Concept: Prompt engineering and its impact on LLM outputs.
  - Why needed here: The quality of the generated queries heavily depends on the prompts used. Understanding how to craft effective prompts is crucial for achieving the desired results.
  - Quick check question: What are some best practices for prompt engineering when using LLMs for query generation?

## Architecture Onboarding

- Component map: Prompt Construction -> Query Generation (PaLM-2) -> Query Filtration (PaLM-2) -> Downstream Model Training (mt5-XXL)

- Critical path:
  1. Construct prompts using task-specific examples
  2. Generate synthetic queries using the LLM
  3. Filter generated queries using the LLM-based filtration
  4. Train downstream model on filtered synthetic data
  5. Evaluate downstream model performance

- Design tradeoffs:
  - Using fewer examples (10) from MS-MARCO vs. task-specific examples
  - Pairwise generation vs. label-conditioned generation
  - Iterative generation vs. single-pass generation

- Failure signatures:
  - Low quality or irrelevant queries after generation
  - High percentage of filtered queries
  - Poor downstream model performance despite high-quality synthetic data

- First 3 experiments:
  1. Compare the performance of GENERATE-PAIRWISE with GENERATE-RELEVANT-ONLY on a small dataset
  2. Evaluate the impact of using task-specific examples vs. generic examples from MS-MARCO
  3. Test the effectiveness of iterative pairwise generation on a subset of the data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed pairwise query generation approach work effectively for tasks without a document context, such as duplicate question retrieval?
- Basis in paper: The paper mentions that some BEIR tasks like duplicate question retrieval lack document context, making it unclear how to adapt the pairwise generation method to such tasks.
- Why unresolved: The paper does not explore or provide results for tasks without document context, focusing instead on tasks that have document passages to condition the query generation.
- What evidence would resolve it: Experimental results comparing the performance of the pairwise approach on tasks with and without document context would demonstrate its generalizability and limitations.

### Open Question 2
- Question: How can the quality of generated irrelevant queries be improved, especially for tasks like fact verification where the irrelevant queries need to be entity-centric?
- Basis in paper: The paper notes that for the FEVER fact verification task, the generated irrelevant queries were more general or concept-focused, while the test split required entity-centric irrelevant queries.
- Why unresolved: The paper does not propose or evaluate methods to control the LLM generation to produce more task-specific irrelevant queries.
- What evidence would resolve it: Results showing improved performance on fact verification or other tasks requiring specific types of irrelevant queries after applying methods to control LLM generation would demonstrate the effectiveness of such approaches.

### Open Question 3
- Question: Can evaluation techniques be developed to estimate the downstream performance of synthetic query generation models without having to run the full downstream training pipeline?
- Basis in paper: The paper mentions the high computational cost of running multiple experiments to evaluate the quality of generated queries by training downstream models, suggesting a need for more efficient evaluation methods.
- Why unresolved: The paper does not propose or test any alternative evaluation techniques that could provide a proxy for downstream performance without the full training process.
- What evidence would resolve it: If evaluation techniques are developed that can predict downstream performance with high accuracy, it would significantly reduce the computational burden and time required for researchers to explore synthetic data generation methods.

## Limitations
- Evaluation primarily focused on zero-shot settings, leaving few-shot scenarios unexplored
- Reliance on PaLM-2 for both generation and filtration introduces potential bottlenecks
- Filtration process reduces available synthetic data and may introduce bias if too strict

## Confidence
- High Confidence: The core mechanism of pairwise generation being easier for LLMs than absolute judgments is well-supported by experimental results
- Medium Confidence: The claim about iterative refinement improving query quality has moderate support from ablation studies
- Low Confidence: The generalizability to few-shot settings remains uncertain as only zero-shot performance is evaluated

## Next Checks
1. Test the pairwise generation approach on settings with limited training data (100-1000 examples) to assess effectiveness beyond zero-shot scenarios
2. Replicate key experiments using different LLM architectures (e.g., GPT-4, Claude) to verify that pairwise generation benefits are not specific to PaLM-2
3. Systematically vary the filtration criteria to understand the tradeoff between data quantity and quality, and identify optimal thresholds for different task types