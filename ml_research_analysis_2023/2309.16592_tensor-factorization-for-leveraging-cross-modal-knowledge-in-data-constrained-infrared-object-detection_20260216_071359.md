---
ver: rpa2
title: Tensor Factorization for Leveraging Cross-Modal Knowledge in Data-Constrained
  Infrared Object Detection
arxiv_id: '2309.16592'
source_url: https://arxiv.org/abs/2309.16592
tags:
- detection
- object
- modality
- images
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes TensorFact, a method to address the challenge
  of object detection in infrared (IR) images where labeled training data is scarce.
  The core idea is to leverage the availability of large-scale RGB training data by
  decomposing convolutional kernels into low-rank factor matrices.
---

# Tensor Factorization for Leveraging Cross-Modal Knowledge in Data-Constrained Infrared Object Detection

## Quick Facts
- arXiv ID: 2309.16592
- Source URL: https://arxiv.org/abs/2309.16592
- Reference count: 40
- Key outcome: TensorFact achieves 4% improvement in mAP 50 score on IR detection using only 1% of training data by decomposing convolution kernels into low-rank factors pre-trained on RGB data

## Executive Summary
This paper addresses the challenge of object detection in infrared images where labeled training data is scarce. The authors propose TensorFact, a method that leverages abundant RGB training data by decomposing convolutional kernels into low-rank factor matrices. These factors are pre-trained on RGB data and then augmented with a small number of parameters to adapt to the IR modality. Experiments on the FLIR ADAS v1 dataset show that TensorFact achieves a 4% improvement in mAP 50 score compared to a state-of-the-art detector when trained with only 1% of the IR training data. The method also offers compression benefits for RGB detection, maintaining or improving performance with fewer parameters.

## Method Summary
TensorFact decomposes convolutional kernels into low-rank factor matrices (A and B) that are pre-trained on RGB data, then augments with additional parameters for IR modality. The method uses YOLOv7 as backbone, ADAM optimizer (10^-5 for RGB, 10^-3 for IR), 200 epochs, batch size 40, and optional L1/L2 regularization to encourage complementary feature learning between RGB and IR branches. The approach splits each convolution kernel tensor into two smaller factor matrices, reducing parameters from T*S*D2*D1 to r*(T*S + D2*D1), where r is the inner dimension (rank). Capacity augmentation with a parallel branch enables the model to capture modality-specific features without forgetting cross-modal knowledge.

## Key Results
- 4% improvement in mAP 50 score on FLIR ADAS v1 validation set compared to state-of-the-art detector
- Achieves this performance using only 1% of the IR training data
- Offers compression benefits for RGB detection, maintaining or improving performance with fewer parameters
- Demonstrates effective cross-modal knowledge transfer from RGB to IR modalities

## Why This Works (Mechanism)

### Mechanism 1
Decomposing convolutional kernels into low-rank factor matrices reduces overfitting by limiting the number of trainable parameters. By splitting the original convolution kernel tensor into two smaller factor matrices (A and B), the total number of parameters is reduced from T*S*D2*D1 to r*(T*S + D2*D1), where r is the inner dimension (rank). This reduction in parameters allows the model to train more effectively on limited IR data without memorizing noise.

### Mechanism 2
Capacity augmentation with a parallel branch enables the model to capture modality-specific features without forgetting cross-modal knowledge. After pre-training on RGB data, the model increases the inner dimension r by ∆r, creating parallel trainable branches (∆A and ∆B). This allows the model to learn IR-specific features while the original factors retain cross-modal knowledge, similar to residual connections in ResNets.

### Mechanism 3
Explicit regularization that maximizes L1 or L2 distance between RGB and IR feature maps encourages the model to learn complementary rather than redundant features. The loss function includes an additional term L_c = -||K * X - ∆K * X||_p that penalizes similarity between the outputs of the RGB and IR branches, forcing them to capture different aspects of the input.

## Foundational Learning

- **Concept: Tensor decomposition and low-rank approximation**
  - Why needed here: Understanding how splitting a 4-way tensor (convolution kernel) into two factor matrices reduces parameters while maintaining functionality is fundamental to grasping TensorFact's approach.
  - Quick check question: Given a convolution kernel of size 64×3×3×3, what is the parameter count for a rank-16 decomposition versus the original?

- **Concept: Domain adaptation and cross-modal learning**
  - Why needed here: The method transfers knowledge from RGB to IR modalities, which represents a significant domain shift unlike typical domain adaptation scenarios.
  - Quick check question: How does cross-modal knowledge transfer differ from standard domain adaptation when the modalities have fundamentally different information (e.g., visible vs. thermal)?

- **Concept: Overfitting and model capacity**
  - Why needed here: The core motivation is addressing overfitting in data-constrained IR detection by reducing model capacity through factorization.
  - Quick check question: Why might a model with fewer parameters generalize better on a small dataset compared to an overparameterized model?

## Architecture Onboarding

- **Component map:** Input (RGB/IR images) -> Backbone (CNN with decomposed convolutional layers) -> Factor matrices (A and B) -> Capacity augmentation (∆A and ∆B) -> Output (Object detection predictions)

- **Critical path:** 1. Pre-train factor matrices on RGB data using standard detection loss 2. Augment with IR-specific capacity (∆r) 3. Fine-tune on IR data with frozen original factors 4. Apply optional complementarity regularization during IR training

- **Design tradeoffs:** Rank selection (r) balances capacity vs overfitting risk; augmentation ratio (∆r:rl) balances IR-specific learning vs cross-modal retention; regularization strength (ω_c) balances complementary vs integrated features

- **Failure signatures:** No performance improvement on IR (rank too low, augmentation insufficient, or RGB pre-training ineffective); degradation on RGB (too much capacity allocated to IR branch); overfitting on IR (augmentation ratio too high relative to available training data)

- **First 3 experiments:** 1. Implement decomposed convolution layer and verify parameter reduction compared to standard convolution 2. Train on RGB data only with different rank values and measure performance vs parameter count 3. Add capacity augmentation and test IR fine-tuning with frozen RGB factors, measuring performance with different augmentation ratios

## Open Questions the Paper Calls Out
- How does the performance of TensorFact scale with varying degrees of data scarcity in the IR modality, beyond the 1% training data scenario tested?
- What is the impact of different tensor decomposition ranks (r) on the trade-off between model compression and detection performance in both RGB and IR modalities?
- How does TensorFact perform on other cross-modal scenarios beyond RGB-to-IR, such as visible-to-thermal or different sensor modalities?

## Limitations
- Performance claims are limited to a single dataset (FLIR ADAS v1) and may not generalize to other IR detection benchmarks
- The method assumes RGB features are sufficiently generalizable to IR detection, which may not hold for all object types or environmental conditions
- Fixed rank decomposition may not optimally capture modality-specific patterns that vary across different layers or object classes

## Confidence
- **High Confidence:** The parameter reduction mechanism through low-rank factorization is mathematically sound and well-established. The 4% mAP improvement claim on FLIR ADAS v1 is specific and measurable.
- **Medium Confidence:** The complementarity regularization mechanism's effectiveness depends heavily on hyperparameter tuning (ω_c). The claim that cross-modal knowledge transfer improves IR detection performance is plausible but requires validation across diverse datasets.
- **Low Confidence:** The generalization of these results to other IR datasets or different backbone architectures (beyond YOLOv7) is not established. The interaction between factorization rank and augmentation ratio for optimal performance is not fully characterized.

## Next Checks
1. Test TensorFact on alternative IR detection benchmarks (e.g., KAIST, RGBT) to verify that the 4% improvement generalizes beyond FLIR ADAS v1.
2. Systematically vary the rank parameter α and augmentation ratio ∆r:rl across multiple layers to identify optimal configurations and quantify sensitivity to these hyperparameters.
3. Conduct controlled experiments comparing L1 vs L2 distance maximization, and test performance with and without complementarity regularization to isolate its contribution to the observed improvements.