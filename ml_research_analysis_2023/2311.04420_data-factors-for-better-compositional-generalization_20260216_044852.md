---
ver: rpa2
title: Data Factors for Better Compositional Generalization
arxiv_id: '2311.04420'
source_url: https://arxiv.org/abs/2311.04420
tags:
- generalization
- examples
- dataset
- datasets
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines how data factors influence compositional generalization.
  The authors show that increased dataset complexity facilitates compositional generalization,
  with dataset size and pattern complexity as two important factors behind the gain.
---

# Data Factors for Better Compositional Generalization

## Quick Facts
- arXiv ID: 2311.04420
- Source URL: https://arxiv.org/abs/2311.04420
- Authors: 
- Reference count: 40
- Primary result: Increased dataset complexity facilitates compositional generalization, with dataset size and pattern complexity as key factors

## Executive Summary
This paper investigates how data factors influence compositional generalization in semantic parsing tasks. The authors demonstrate that increasing dataset complexity through larger size and more diverse patterns improves generalization performance by making surface memorization harder and encouraging compositional understanding. They show that example difficulty affects compositional generalization differently in synthetic versus natural language datasets, with a balanced mixture of simple and hard examples being optimal for real-world data.

## Method Summary
The authors train 3-layer Transformer seq2seq models from scratch on semantic parsing datasets, systematically varying data factors like dataset size, pattern complexity, and example difficulty. They use Adam optimizer with Noam learning rate scheduling and evaluate exact-match accuracy on generalization splits. Data augmentation methods (AugZero and primitive augmentation) are employed to increase pattern complexity. Experiments are conducted on synthetic datasets (SCAN, SCAN*) and real-language datasets (GeoQuery, ATIS, SMCalFlow) with controlled complexity and difficulty levels.

## Key Results
- Increasing dataset size improves compositional generalization by reducing example repetition frequency
- Pattern complexity (more unique primitives and varied structures) prevents memorization and encourages compositional understanding
- On synthetic datasets, simple examples invoke stronger compositionality than hard examples; on natural language datasets, a balanced mixture of simple and hard examples yields best performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing dataset complexity improves compositional generalization by making surface memorization harder
- Mechanism: When datasets contain more diverse examples, the model cannot rely on simple one-to-one mappings between inputs and outputs
- Core assumption: Models have limited capacity and will prefer compositional understanding when surface memorization becomes computationally expensive
- Evidence anchors: [abstract], [section 4.2]
- Break condition: If model capacity increases substantially (e.g., large pretrained models), the memorization cost may no longer outweigh the benefit of compositional learning

### Mechanism 2
- Claim: Larger datasets prevent memorization by reducing example repetition frequency
- Mechanism: When similar examples appear less frequently during training, the model cannot establish reliable surface mappings
- Core assumption: Recurrent exposure to similar examples enables memorization; reducing this recurrence prevents it
- Evidence anchors: [abstract], [section 4.3]
- Break condition: If dataset size increases without increasing diversity, repetition frequency may not decrease enough to prevent memorization

### Mechanism 3
- Claim: Example difficulty affects compositional generalization differently for synthetic versus natural language datasets
- Mechanism: In synthetic datasets, simpler examples facilitate compositional learning because they contain fewer confounding factors
- Core assumption: Natural language contains phenomena that cannot be captured by simple examples alone, requiring a balanced mixture
- Evidence anchors: [abstract], [section 5.2]
- Break condition: If the dataset lacks sufficient diversity, even a balanced mixture may not achieve good generalization

## Foundational Learning

- Concept: Compositional generalization
  - Why needed here: The entire paper studies how data factors influence compositional generalization
  - Quick check question: What is the difference between compositional generalization and regular generalization?

- Concept: Surface memorization vs compositional understanding
  - Why needed here: The paper argues that dataset complexity influences which of these two behaviors the model adopts
  - Quick check question: Can you provide an example where a model achieves good training performance through memorization but fails on test data requiring composition?

- Concept: Data complexity metrics
  - Why needed here: The paper measures and manipulates pattern complexity and scale complexity to study their effects
  - Quick check question: How would you measure the pattern complexity of a semantic parsing dataset?

## Architecture Onboarding

- Component map: Data loading → preprocessing (vocabulary creation, augmentation) → model training → evaluation on generalization splits
- Critical path: Data loading → preprocessing → model training → evaluation
- Design tradeoffs: From-scratch training provides clean analysis but lacks pretraining benefits; data augmentation increases complexity but may not scale to all domains
- Failure signatures: Poor performance on generalization splits despite good training accuracy indicates memorization; good performance on both suggests compositional understanding
- First 3 experiments:
  1. Replicate SCAN Jump split results with 1x vs 20x augmentation to verify complexity benefits
  2. Implement AugZero augmentation method and test on GeoQuery to verify zero-knowledge augmentation works
  3. Subsample SMCalFlow dataset by difficulty and measure impact on compositional split performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mechanism by which example difficulty influences compositional generalization in real language datasets?
- Basis in paper: [explicit] The authors observe that on real language datasets like ATIS and SMCalFlow, a balanced mixture of simple and hard examples induces the strongest generalizability, but they note that the trend is more complicated than in synthetic datasets.
- Why unresolved: The authors hypothesize that simple examples alone do not provide enough coverage for all the diverse linguistic phenomena in the dataset, but they do not provide a detailed analysis of why this is the case.
- What evidence would resolve it: Experiments that systematically vary different aspects of example difficulty and measure their impact on compositional generalization performance.

### Open Question 2
- Question: How does the frequency of example repetition interact with other data factors to influence compositional generalization?
- Basis in paper: [explicit] The authors show that reduced example recurring frequency can make surface memorization harder and encourage compositional understanding.
- Why unresolved: The authors focus on the effect of example repetition in isolation, but they do not explore how it interacts with other data factors like scale and diversity.
- What evidence would resolve it: Experiments that manipulate both the frequency of example repetition and other data factors in a controlled manner and measure their combined effect.

### Open Question 3
- Question: How does the AugZero data augmentation method compare to more sophisticated augmentation techniques that leverage task-specific knowledge?
- Basis in paper: [explicit] The authors propose AugZero as a simple data augmentation method that requires zero prior knowledge about the task and satisfies their hypotheses on pattern complexity and scale complexity.
- Why unresolved: The authors do not directly compare AugZero to other augmentation methods that leverage task-specific knowledge.
- What evidence would resolve it: Experiments that compare AugZero to other augmentation methods on a range of compositional generalization tasks.

## Limitations
- Empirical rather than theoretical analysis limits understanding of causal mechanisms
- Focus on semantic parsing tasks may not generalize to other compositional domains
- Memorization-vs-composition tradeoff assumes fixed model capacity, which may not hold for modern large models

## Confidence
- High confidence: Dataset size effects on generalization performance (strong empirical evidence across multiple datasets)
- Medium confidence: Pattern complexity benefits (supported by experiments but mechanism assumptions about memorization costs are not directly validated)
- Low confidence: Example difficulty effects (findings show synthetic vs natural language differences, but underlying causes remain speculative)

## Next Checks
1. Test the memorization hypothesis by comparing model performance on datasets with controlled repetition frequency but identical overall complexity
2. Validate pattern complexity effects on non-semantic parsing tasks (e.g., visual question answering) to assess domain generality
3. Implement a capacity control experiment by training models with varying sizes on the same datasets to measure when memorization becomes dominant