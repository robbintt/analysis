---
ver: rpa2
title: "Can we Agree? On the Rash\u014Dmon Effect and the Reliability of Post-Hoc\
  \ Explainable AI"
arxiv_id: '2308.07247'
source_url: https://arxiv.org/abs/2308.07247
tags:
- sample
- size
- rash
- shap
- omon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study examined the impact of sample size on explainable AI
  (XAI) using SHAP explanations. Experiments on 5 datasets showed that explanations
  from models with <128 samples exhibited high variability, limiting reliable knowledge
  extraction.
---

# Can we Agree? On the Rashōmon Effect and the Reliability of Post-Hoc Explainable AI

## Quick Facts
- arXiv ID: 2308.07247
- Source URL: https://arxiv.org/abs/2308.07247
- Reference count: 40
- Key outcome: Experiments on 5 datasets showed that explanations from models with <128 samples exhibited high variability, limiting reliable knowledge extraction. As sample size increased, agreement between models improved, allowing for consensus. Bagging ensembles often had higher agreement than individual models.

## Executive Summary
This study investigates how sample size affects the reliability of post-hoc explainable AI using SHAP explanations. The authors examine the Rashōmon effect, where multiple models with similar performance may use different features, and test whether increasing sample size leads to consensus in explanations. Through experiments on five diverse datasets, they demonstrate that explanations become more stable and models converge on similar feature importance as sample size grows beyond 128 samples. The work provides practical guidance on when explanations can be trusted and suggests validation is necessary for reliable knowledge extraction from small datasets.

## Method Summary
The authors conduct experiments across five public datasets (Framingham, German Credit, Diabetes, COMPAS, Student) using PyCaret to train and select top-performing models based on Cohen's Kappa via 10-fold cross-validation. For each selected model, they perform 10-fold cross-validation on subsets of training data with increasing sample sizes, compute SHAP explanations for each fold, and analyze both intra-model (within the same model across folds) and inter-model (between different models) agreement using topj similarity and weighted cosine similarity metrics. They also test bagging ensembles to assess whether they achieve higher agreement than individual models.

## Key Results
- Explanations from models trained on <128 samples exhibited high variability and limited reliable knowledge extraction
- Agreement between models improved with larger sample sizes, allowing for consensus explanations
- Bagging ensembles often achieved higher agreement than individual models, particularly at larger sample sizes
- The German dataset showed persistent divergence even with larger samples due to high dimensionality and class imbalance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing sample size improves intra-model agreement (consistency across cross-validation folds)
- Mechanism: Larger datasets reduce sampling variance, leading to more stable model training and consequently more consistent SHAP attributions
- Core assumption: Model training stability scales with sample size, and SHAP values reflect underlying model feature importance rather than noise
- Evidence anchors: [abstract] "Explanations from <128 samples exhibited high variability, limiting reliable knowledge extraction"; [section] "Nearly each individual model converges towards the near-optimal consensus ¯φSHAP consensus (Table 5 and Table 6)"
- Break condition: If model architecture is highly sensitive to training set composition, or if the data distribution changes significantly with sample size

### Mechanism 2
- Claim: Increasing sample size improves inter-model agreement (consensus between different models)
- Mechanism: Larger datasets reduce model underspecification, leading different models to converge on similar underlying patterns and feature importance rankings
- Core assumption: The Rashomon effect diminishes with more data, allowing diverse models to agree on the most important features
- Evidence anchors: [abstract] "agreement between models improved with more data, allowing for consensus"; [section] "As the sample size increased, the variance in similarity diminished and models converged towards a unified explanation"
- Break condition: If the true data-generating process is highly complex or non-stationary, requiring very large sample sizes for convergence

### Mechanism 3
- Claim: Bagging ensembles achieve higher agreement than individual models, especially at larger sample sizes
- Mechanism: Ensembles average out individual model idiosyncrasies, leading to more stable and consistent explanations
- Core assumption: Averaging predictions reduces variance without introducing significant bias in feature importance attribution
- Evidence anchors: [abstract] "Bagging ensembles often had higher agreement"; [section] "Interestingly, it can be noted that none of the models successfully converged on the German dataset"
- Break condition: If ensemble members are highly correlated or if the base models have fundamentally different inductive biases that don't average well

## Foundational Learning

- Concept: SHAP (SHapley Additive exPlanations) values and their properties
  - Why needed here: The entire analysis relies on comparing SHAP values across models and sample sizes to assess agreement
  - Quick check question: What axioms must a feature attribution method satisfy to be considered a valid Shapley value?

- Concept: Rashomon Effect in machine learning
  - Why needed here: The paper's central hypothesis is that the Rashomon effect diminishes with larger sample sizes
  - Quick check question: How does the Rashomon parameter ε relate to the number of models in a Rashomon set?

- Concept: Spearman correlation and its interpretation
  - Why needed here: The paper uses Spearman correlations to assess monotonic relationships between sample size and agreement metrics
  - Quick check question: When would Spearman correlation be more appropriate than Pearson correlation for analyzing agreement trends?

## Architecture Onboarding

- Component map: Data preprocessing -> Model selection (10-fold CV) -> SHAP explanation generation -> Agreement metrics (topj similarity, weighted cosine similarity) -> Statistical analysis (Spearman correlation)
- Critical path: Model training and SHAP explanation generation are the most computationally intensive steps
- Design tradeoffs: Using SHAP (model-agnostic) vs. model-specific explanation methods; using multiple similarity metrics vs. a single comprehensive metric
- Failure signatures: Low agreement between models despite high predictive accuracy suggests potential Rashomon effect; inconsistent results across sample sizes may indicate sampling bias
- First 3 experiments:
  1. Reproduce the intra-model agreement analysis on a simple dataset to verify the monotonic relationship with sample size
  2. Test the bagging strategy on a dataset where individual models show low agreement to confirm the ensemble benefit
  3. Compare SHAP-based agreement metrics with an alternative explanation method (e.g., LIME) to validate the robustness of findings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Rashōmon effect manifest in neural networks compared to traditional machine learning models, and can transfer learning mitigate its impact?
- Basis in paper: [explicit] The authors state that "testing convergence in neural networks" is an impactful extension, as different initializations can learn distinct solutions, and transfer learning may provide more consistent representations
- Why unresolved: The paper focuses on traditional ML models and SHAP explanations, leaving the behavior of neural networks unexplored
- What evidence would resolve it: Experiments comparing explanation convergence across neural network architectures with and without transfer learning, analyzing the consistency of feature importances

### Open Question 2
- Question: What is the relationship between the curse of dimensionality and overconfidence in machine learning models, and how does this impact explanation reliability?
- Basis in paper: [inferred] The authors note that the German dataset's high dimensionality and class imbalance posed challenges, and reference work linking the curse of dimensionality to overconfidence issues
- Why unresolved: The paper does not directly investigate the curse of dimensionality's impact on explanation reliability
- What evidence would resolve it: Controlled experiments varying dataset dimensionality and measuring explanation consistency, particularly for high-dimensional datasets

### Open Question 3
- Question: How do model-specific explanation methods like attention maps or Grad-CAM behave in the context of the Rashōmon effect compared to model-agnostic methods like SHAP?
- Basis in paper: [explicit] The authors suggest that future work could explore how model-specific explanation methods behave in the context of the Rashōmon effect, as they may reveal divergences between architectures that model-agnostic methods cannot access
- Why unresolved: The paper uses only SHAP for explanation, leaving model-specific methods unexplored
- What evidence would resolve it: Comparative studies of explanation convergence patterns between model-agnostic and model-specific methods across various model architectures and datasets

## Limitations
- The analysis assumes SHAP values accurately capture feature importance without validation against ground truth
- Findings are limited to tabular datasets and may not generalize to image or text data
- The 128-sample threshold may be dataset-dependent and not universally applicable
- Convergence behavior in neural networks is explicitly noted as an open question

## Confidence

- **High Confidence**: The monotonic relationship between sample size and intra-model agreement is well-supported by experimental results across multiple datasets and metrics. The observation that bagging ensembles show higher agreement than individual models is consistently observed.
- **Medium Confidence**: The claim that 128 samples represents a threshold for reliable explanations is supported by the data but may be dataset-dependent. The generalization of findings to non-tabular data and neural networks remains uncertain.
- **Low Confidence**: The assumption that SHAP values represent "true" feature importance is not validated against ground truth. The paper does not address potential biases in the selection of top-performing models based on Cohen's Kappa.

## Next Checks

1. **Ground Truth Validation**: Test the agreement metrics against synthetic datasets where ground truth feature importance is known, to verify that the observed convergence reflects genuine feature discovery rather than coincidental agreement.

2. **Neural Network Extension**: Reproduce the analysis using a neural network architecture (e.g., MLP or CNN for tabular data) to assess whether the sample size thresholds and convergence patterns hold for deep learning models.

3. **Alternative Explanation Methods**: Compare SHAP-based agreement with LIME or permutation feature importance to determine if the observed patterns are method-specific or reflect general principles of model agreement.