---
ver: rpa2
title: 'ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large
  Language Models'
arxiv_id: '2310.05872'
source_url: https://arxiv.org/abs/2310.05872
tags:
- visual
- reasoning
- commonsense
- image
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies visual commonsense reasoning (VCR) by categorizing
  it into visual commonsense understanding (VCU) and visual commonsense inference
  (VCI). The authors propose ViCor, a framework that leverages large language models
  (LLMs) to classify VCR problems and direct vision-and-language models (VLMs) accordingly.
---

# ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models

## Quick Facts
- arXiv ID: 2310.05872
- Source URL: https://arxiv.org/abs/2310.05872
- Reference count: 4
- Key outcome: ViCor achieves state-of-the-art results on VCR (59.8%) and A-OKVQA (75.6%) datasets without supervised fine-tuning

## Executive Summary
This paper addresses visual commonsense reasoning (VCR) by categorizing problems into visual commonsense understanding (VCU) and visual commonsense inference (VCI). The proposed ViCor framework leverages large language models (LLMs) to classify VCR problems and direct vision-and-language models (VLMs) accordingly. For VCU problems, VLMs provide direct visual understanding, while for VCI problems, LLMs actively query VLMs for relevant visual clues before reasoning. The framework achieves state-of-the-art performance among methods without supervised fine-tuning, demonstrating the effectiveness of collaborative reasoning between LLMs and VLMs.

## Method Summary
The ViCor framework uses pre-trained LLMs (GPT-3.5-turbo-0613 and GPT-4-0613) for problem classification, VLM command, and visual commonsense reasoning. Pre-trained VLMs (BLIP2 and LLA V A-7B-v1.1) provide visual recognition and understanding capabilities. The framework implements a multi-step process where LLMs first evaluate potential answer candidates based on image captions, then, if uncertain, reason about visual factors and re-evaluate candidates with additional visual evidence from VLMs. The framework achieves state-of-the-art results on VCR (59.8%) and A-OKVQA (75.6%) datasets without requiring supervised fine-tuning.

## Key Results
- Achieves 59.8% accuracy on VCR dataset, state-of-the-art among methods without supervised fine-tuning
- Achieves 75.6% accuracy on A-OKVQA dataset, state-of-the-art among methods without supervised fine-tuning
- Demonstrates effectiveness of problem classification and collaborative reasoning between LLMs and VLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs excel at VCI when provided with sufficient visual clues, but struggle without them due to missing context.
- Mechanism: The framework classifies VCR problems into VCU and VCI, then uses LLMs to either directly evaluate visual content or actively query VLMs for specific visual clues before reasoning.
- Core assumption: Pre-trained VLMs can provide accurate visual understanding that LLMs can leverage for commonsense inference when properly queried.
- Evidence anchors: [abstract] "For problems where the goal is to infer conclusions beyond image content...VLMs face difficulties, while LLMs, given sufficient visual evidence, can use commonsense to infer the answer well."

### Mechanism 2
- Claim: The problem classification system effectively routes problems to the appropriate processing pipeline based on their nature.
- Mechanism: LLMs serve as problem classifiers that analyze the problem category and then either use VLMs to answer directly or actively instruct VLMs to gather relevant visual elements for inference.
- Core assumption: LLMs can accurately classify VCR problems into VCU and VCI categories using in-context examples and problem type descriptions.
- Evidence anchors: [abstract] "Our framework will leverage LLM to perform initial reasoning and confidence check. If the reasoning is not confident, it will reason about what visual factors should be perceived from the image to make a confident commonsense inference."

### Mechanism 3
- Claim: Active querying of VLMs for specific visual factors improves inference accuracy compared to passive caption-based reasoning.
- Mechanism: For VCI problems, LLMs reason about required visual factors, query VLMs for specific visual clues, and then re-evaluate answer choices with this additional context.
- Core assumption: The combination of LLM reasoning about visual factors and VLM perception of those factors provides better inference support than general image captions.
- Evidence anchors: [abstract] "To mitigate this issue, we suggest a collaborative approach where LLMs, when uncertain about their reasoning, actively direct VLMs to concentrate on and gather relevant visual elements to support potential commonsense inferences."

## Foundational Learning

- Concept: Visual Commonsense Understanding (VCU) vs Visual Commonsense Inference (VCI)
  - Why needed here: The framework's effectiveness depends on correctly distinguishing between these two problem types and applying appropriate processing pipelines.
  - Quick check question: Given a question about whether a person in an image is happy or sad, would this be VCU or VCI? (Answer: VCU, as it requires understanding the visual content directly)

- Concept: Image-Text Alignment (ITA) scores
  - Why needed here: The framework uses ITA scores to compare how well declarative sentences match image content, which is crucial for VCU problem processing.
  - Quick check question: If an image shows a cat sitting on a mat, which of these sentences would likely have a higher ITA score: "A cat is sitting on a mat" or "The cat is planning to jump"? (Answer: The first sentence)

- Concept: Chain-of-thought reasoning in LLMs
  - Why needed here: The framework relies on LLMs to perform multi-step reasoning, both for problem classification and for evaluating answer choices with visual clues.
  - Quick check question: Why might an LLM perform better at reasoning when given intermediate steps rather than just the final answer? (Answer: Chain-of-thought helps the model break down complex reasoning into manageable steps and reduces hallucination)

## Architecture Onboarding

- Component map: Image -> VLM (BLIP2/LLaVA) -> Image Caption -> LLM (GPT-3.5/4) -> Problem Classification -> (VCU: VLM Answer) or (VCI: Visual Clue Generation -> VLM Perception -> LLM Reasoning)

- Critical path:
  1. Input: Image, question, and answer choices
  2. LLM performs initial reasoning with image caption
  3. Confidence check determines if additional visual information is needed
  4. If needed, LLM classifies problem type and identifies required visual factors
  5. VLM provides specific visual clues or general understanding
  6. LLM performs final reasoning with all available information
  7. Output: Selected answer choice

- Design tradeoffs:
  - Using pre-trained models without fine-tuning vs. training on specific datasets (better generalization vs. potentially lower performance)
  - Active querying of VLMs vs. passive caption-based reasoning (more accurate but potentially slower)
  - LLM-based problem classification vs. rule-based classification (more flexible but potentially less consistent)

- Failure signatures:
  - Poor problem classification leading to inappropriate processing pipeline
  - VLM inability to perceive specified visual factors accurately
  - LLM reasoning that ignores or misinterprets visual clues
  - Confidence check that incorrectly determines additional information is or isn't needed

- First 3 experiments:
  1. Test the problem classification accuracy by running a set of VCR problems through the LLM classifier and comparing its categorizations to human annotations
  2. Evaluate the effectiveness of visual clue generation by comparing LLM reasoning performance with and without LLM-generated visual clues on VCI problems
  3. Benchmark the complete system against baselines on a subset of VCR and A-OKVQA datasets to validate overall performance improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between visual factor reasoning and VQA model usage in the ViCor framework for different types of VCR problems?
- Basis in paper: [inferred] The paper discusses using both visual factors reasoning and VQA models, but notes that VQA models sometimes lack context and understanding, while visual factor reasoning can provide more relevant information.
- Why unresolved: The paper does not provide a clear guideline on when to use VQA models versus visual factor reasoning, or how to optimally balance these two approaches for different types of VCR problems.
- What evidence would resolve it: Experimental results comparing the performance of ViCor with different balances of VQA and visual factor reasoning on various VCR datasets, particularly focusing on cases where one approach outperforms the other.

### Open Question 2
- Question: How does the performance of ViCor compare to state-of-the-art methods that use supervised fine-tuning on in-domain VCR datasets?
- Basis in paper: [explicit] The paper mentions that ViCor lags behind best performing methods which are based on supervised fine-tuning, and suggests future work could explore fine-tuning approaches.
- Why unresolved: The paper does not provide a direct comparison between ViCor and supervised fine-tuning methods, leaving the performance gap unclear.
- What evidence would resolve it: A comprehensive comparison of ViCor's performance against state-of-the-art supervised fine-tuning methods on multiple VCR datasets, including analysis of the specific areas where ViCor falls short.

### Open Question 3
- Question: What alternative communication mediums between LLMs and VLMs could improve the performance of the ViCor framework?
- Basis in paper: [explicit] The paper notes that text is the only communication medium between LLMs and VLMs, and suggests that the loss of visual details caused by captions may hinder performance in certain scenarios.
- Why unresolved: The paper does not explore or propose any specific alternative communication mediums between LLMs and VLMs, leaving the potential benefits of such alternatives untested.
- What evidence would resolve it: Experimental results comparing the performance of ViCor using different communication mediums (e.g., visual embeddings, multimodal tokens) against the current text-based approach on various VCR tasks.

## Limitations

- Prompt sensitivity: Framework's performance heavily depends on the quality of LLM prompts for problem classification and visual factor reasoning, but these prompts are not fully disclosed.
- Computational cost: Multi-step reasoning process involving multiple LLM and VLM interactions could be computationally expensive compared to end-to-end trained models.
- Domain specificity: Approach may not generalize well to domains requiring specialized visual knowledge or different types of reasoning beyond commonsense.

## Confidence

- High Confidence: Overall framework design and problem categorization approach are well-founded and supported by experimental results showing state-of-the-art performance without fine-tuning.
- Medium Confidence: Effectiveness of LLM-based problem classification, as the exact prompts and in-context examples are not fully specified, which could significantly impact performance.
- Low Confidence: Generalizability of the approach to domains beyond VCR and A-OKVQA, as the framework's performance on other visual reasoning tasks is not evaluated.

## Next Checks

1. **Prompt Ablation Study**: Systematically test the framework's performance using different LLM prompts for problem classification and visual factor reasoning to quantify the impact of prompt design on overall accuracy.

2. **Cross-Domain Evaluation**: Evaluate the framework on additional visual reasoning datasets (e.g., VQA, NLVR2) to assess its generalizability beyond VCR and A-OKVQA tasks.

3. **Computational Efficiency Analysis**: Measure and compare the computational cost (latency, token usage) of the ViCor framework against baseline methods to quantify the trade-off between accuracy and efficiency.