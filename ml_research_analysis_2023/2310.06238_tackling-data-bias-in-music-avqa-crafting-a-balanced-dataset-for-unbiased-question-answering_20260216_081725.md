---
ver: rpa2
title: 'Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased
  Question-Answering'
arxiv_id: '2310.06238'
source_url: https://arxiv.org/abs/2310.06238
tags:
- question
- audio-visual
- audio
- visual
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the data imbalance issue in the MUSIC-AVQA
  dataset, which hinders the progress of audio-visual question answering research.
  To tackle this, the authors systematically balance the dataset by identifying biased
  question templates and collecting complementary videos and questions.
---

# Tackling Data Bias in MUSIC-AVQA: Crafting a Balanced Dataset for Unbiased Question-Answering

## Quick Facts
- **arXiv ID**: 2310.06238
- **Source URL**: https://arxiv.org/abs/2310.06238
- **Reference count**: 40
- **Primary result**: Balanced MUSIC-AVQA v2.0 dataset with 1,204 additional videos and 8.1k QA pairs; model achieves 2% accuracy improvement on balanced test set

## Executive Summary
This paper addresses the critical issue of data imbalance in the MUSIC-AVQA dataset, which has hindered progress in audio-visual question answering research. The authors systematically identify and correct answer distribution biases across question templates by collecting complementary videos and questions. They construct MUSIC-AVQA v2.0, a balanced dataset that serves as a more reliable benchmark for AVQA. Additionally, they propose a novel model incorporating an Audio-Spectrogram-Transformer branch and cross-modal pixel-wise attention, demonstrating improved performance on the balanced dataset.

## Method Summary
The authors employ a two-stage approach to tackle data imbalance in MUSIC-AVQA. First, they systematically identify biased question templates where specific answers dominate the distribution (over 60% for binary questions, 50% for multi-class questions). They then collect complementary videos and questions to create balanced answer distributions. This results in MUSIC-AVQA v2.0 with 36.7k training, 5.25k validation, and 10.8k test QA pairs. The proposed model builds upon the LA VISH backbone by adding a pretrained Audio-Spectrogram-Transformer branch and implementing cross-modal pixel-wise attention between visual and audio feature maps, improving fine-grained audio-visual correspondence.

## Key Results
- Constructed MUSIC-AVQA v2.0 with 1,204 additional real videos and 8.1k QA pairs
- Model incorporating AST branch and cross-modal pixel-wise attention improves accuracy by 2% on balanced test set
- Systematic approach to dataset balancing successfully reduces answer distribution bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Balancing answer distributions across question templates improves model generalization.
- Mechanism: By collecting complementary videos and questions, the authors reduce the dominance of certain answers, preventing the model from learning spurious correlations between questions and biased answers.
- Core assumption: The model's performance bottleneck is caused by data imbalance rather than inherent model architecture limitations.
- Evidence anchors:
  - [abstract]: "We meticulously review each question type from the original dataset, selecting those with pronounced answer biases. To counter these biases, we gather complementary videos and questions, ensuring that no answers have outstanding skewed distribution."
  - [section 3]: "Our criteria for identifying bias is when a single answer represents over 60% of responses for binary questions or exceeds 50% for multi-class questions."
  - [corpus]: "FortisAVQA and MAVEN: a Benchmark Dataset and Debiasing Framework for Robust Multimodal Reasoning" (shows related work on dataset debiasing).
- Break condition: If the bias is not in the data but in the model's architecture or training process, balancing may have limited effect.

### Mechanism 2
- Claim: Adding a pretrained Audio-Spectrogram-Transformer (AST) branch enhances audio-visual fusion.
- Mechanism: The AST branch extracts richer semantic audio features than the vision transformer applied to audio spectrograms, providing complementary information for the audio-visual grounding module.
- Core assumption: Audio features extracted by AST are semantically richer and more discriminative than those extracted by the vision transformer on spectrograms.
- Evidence anchors:
  - [section 4]: "Incorporating the pretrained audio feature as an auxiliary branch enables our model to capture richer semantic audio information compared to the vision pretrained transformer applied to the audio spectrogram in LA VISH."
  - [section 4]: "Specifically, we extract the final hidden output of the AST model, which is a spatial feature map x^(1)_as."
  - [corpus]: "Revisit Modality Imbalance at the Decision Layer" (indicates modality imbalance is a known issue in multimodal learning).
- Break condition: If the AST pretrained weights are not well-aligned with the MUSIC-AVQA domain, the additional branch may not provide meaningful improvements.

### Mechanism 3
- Claim: Cross-modal pixel-wise attention improves fine-grained audio-visual correspondence.
- Mechanism: By computing mutual cross-attention between visual and audio feature maps at the pixel level, the model captures more detailed interactions than mean-pooled vector attention.
- Core assumption: The interaction between audio and visual features at the pixel level is more informative than at the vector level for audio-visual grounding.
- Evidence anchors:
  - [section 4]: "Existing spatial grounding module [25] uses a mean-pooled audio vector to compute attention with visual spatial maps. This attention approach: i) Losing the spatial details of spectrogram features. ii) is uni-directional, where only the audio vector serves as a query to the visual maps without any reciprocal interaction."
  - [section 4]: "To address these limitations, we propose a refined pixel-wise cross attention between the visual and audio maps, aiming to capture the correspondence between these two modalities more effectively."
  - [corpus]: "Multi-Modal Scene Graph with Kolmogorov-Arnold Experts for Audio-Visual Question Answering" (shows interest in fine-grained multimodal interactions).
- Break condition: If the added complexity of pixel-wise attention outweighs its benefits, especially for simpler audio-visual relationships.

## Foundational Learning

- Concept: Data imbalance and its impact on model generalization.
  - Why needed here: The paper's central contribution is addressing data imbalance in MUSIC-AVQA to create a more reliable benchmark.
  - Quick check question: What are the potential consequences of training a model on a dataset with significant answer imbalance?

- Concept: Multimodal learning and audio-visual fusion.
  - Why needed here: The paper introduces a new model that combines audio and visual features for audio-visual question answering.
  - Quick check question: What are the challenges in fusing information from different modalities like audio and visual?

- Concept: Attention mechanisms in deep learning.
  - Why needed here: The paper uses cross-modal pixel-wise attention to improve audio-visual correspondence.
  - Quick check question: How does cross-attention differ from self-attention, and what are its applications in multimodal learning?

## Architecture Onboarding

- Component map:
  LA VISH backbone (2-tower Swin-Transformer-V2 with adapters) -> AST branch (pretrained Audio-Spectrogram-Transformer) -> Cross-modal pixel-wise attention module -> Spatial and temporal grounding modules (from AVST) -> LSTM question encoder -> MLP classifier

- Critical path:
  1. Extract visual features from Swin-Transformer-V2
  2. Extract audio features from AST branch
  3. Apply cross-modal pixel-wise attention between visual and audio features
  4. Apply spatial grounding to fuse audio-visual features
  5. Apply temporal grounding with question context
  6. Concatenate outputs from all branches
  7. Pass through MLP for classification

- Design tradeoffs:
  - Using pretrained AST vs. using LA VISH's audio branch: AST provides richer semantic features but adds complexity.
  - Pixel-wise attention vs. vector-level attention: Pixel-wise attention captures finer details but is computationally more expensive.
  - Adding AST branch vs. only using cross-modal attention: AST branch provides complementary features, while cross-modal attention refines the fusion.

- Failure signatures:
  - Poor performance on balanced test set: Indicates the model is still overfitting to biases in the data.
  - No improvement with AST branch: Suggests the pretrained weights are not well-aligned with the domain.
  - Degraded performance with cross-modal attention: Indicates the added complexity is not beneficial or the module is not well-integrated.

- First 3 experiments:
  1. Evaluate the model on the balanced test set to verify it generalizes well.
  2. Remove the AST branch and evaluate to confirm its contribution.
  3. Remove the cross-modal pixel-wise attention and evaluate to confirm its contribution.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the analysis of the paper, two relevant open questions can be identified:

1. How does the effectiveness of the cross-modal pixel-wise attention mechanism compare to other fine-grained fusion techniques for audio-visual learning?
2. To what extent does the dataset balancing approach generalize to other audio-visual question answering datasets with similar bias issues?

## Limitations

- The performance improvement of 2% accuracy, while demonstrating progress, is relatively modest and suggests data bias may not have been the primary bottleneck.
- The added complexity of the AST branch and cross-modal attention mechanisms introduces computational overhead without dramatic performance gains.
- The generalizability of both the debiasing approach and the proposed model to other AVQA datasets and domains remains unclear.

## Confidence

- **High confidence**: The methodology for identifying and addressing dataset bias is well-documented and follows established practices in the AVQA literature.
- **Medium confidence**: The architectural contributions show promise but their individual contributions are not fully isolated through ablation studies.
- **Low confidence**: The scalability of the debiasing approach to other AVQA datasets and the generalizability of the proposed model to different domains remain unclear without further experimentation.

## Next Checks

1. **Ablation study**: Conduct a systematic ablation study to quantify the individual contributions of dataset balancing, AST branch, and cross-modal attention to the overall performance improvement.
2. **Generalization test**: Evaluate the model on out-of-distribution data or a different AVQA dataset to assess whether the improvements generalize beyond MUSIC-AVQA.
3. **Bias analysis**: Perform a detailed analysis of the model's predictions on the balanced test set to verify that it has learned to avoid the original biases rather than simply memorizing the new dataset distribution.