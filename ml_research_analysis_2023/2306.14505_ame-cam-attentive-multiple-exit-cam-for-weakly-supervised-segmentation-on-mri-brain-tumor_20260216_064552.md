---
ver: rpa2
title: 'AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on
  MRI Brain Tumor'
arxiv_id: '2306.14505'
source_url: https://arxiv.org/abs/2306.14505
tags:
- activation
- segmentation
- ame-cam
- image
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the problem of low-resolution class activation
  maps (CAMs) in weakly supervised brain tumor segmentation from MRI, which limits
  segmentation accuracy. The proposed AME-CAM method extracts activation maps from
  multiple exits of a classification network with varying resolutions and uses an
  attention model to learn pixel-wise weighted aggregation of these maps.
---

# AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor

## Quick Facts
- arXiv ID: 2306.14505
- Source URL: https://arxiv.org/abs/2306.14505
- Reference count: 32
- Key outcome: AME-CAM achieves Dice scores of 0.631-0.862 on BraTS 2021, outperforming existing weakly supervised CAM methods for brain tumor segmentation.

## Executive Summary
This study addresses the fundamental limitation of low-resolution class activation maps (CAMs) in weakly supervised brain tumor segmentation from MRI. The proposed AME-CAM method extracts activation maps from multiple exits of a classification network at varying resolutions and uses an attention model to learn pixel-wise weighted aggregation of these maps. Trained using a novel contrastive learning approach, AME-CAM achieves significant improvements over existing CAM-based methods on the BraTS 2021 dataset across four MRI modalities, demonstrating improved segmentation accuracy without requiring dense pixel-level labels.

## Method Summary
AME-CAM uses a multiple-exit ResNet-18 backbone with internal classifiers after each residual block to produce activation maps at different spatial resolutions (M1-M4). An attention network trained with contrastive learning learns pixel-wise weights to aggregate these maps, effectively disentangling foreground tumor from background tissue. The final segmentation combines averaged CAMs with attention-weighted aggregation and refines results using DenseCRF. The method is trained using only image-level labels on 2D slices from the BraTS 2021 dataset.

## Key Results
- Achieves Dice scores ranging from 0.631 to 0.862 across four MRI modalities (T1, T1-CE, T2, T2-FLAIR)
- Outperforms existing CAM-based weakly supervised methods on BraTS 2021
- Reduces under-estimation of tumor regions compared to standard CAM approaches
- Demonstrates effective segmentation without requiring dense pixel-level annotations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple-exit architecture improves segmentation resolution by aggregating CAMs at different spatial resolutions
- Mechanism: Network produces intermediate activation maps (M1-M4) at varying resolutions from exits after each residual block. Attention model learns pixel-wise weights for these maps, allowing high-resolution features from early exits to complement coarse but semantically rich features from later exits.
- Core assumption: Features at different network depths contain complementary information for segmentation, and attention can learn optimal weighting
- Evidence anchors: [abstract] "extracts activation maps from multiple exits of a classification network with varying resolutions and uses an attention model to learn pixel-wise weighted aggregation"

### Mechanism 2
- Claim: Contrastive learning on masked features disentangles foreground tumor from background tissue
- Mechanism: Attention network trained with contrastive loss that maximizes similarity between foreground features (masked by aggregated CAM) and minimizes similarity between foreground and background features
- Core assumption: Tumor-background contrast is sufficient for contrastive loss to effectively separate regions without explicit segmentation labels
- Evidence anchors: [section] "We train the attention network with unsupervised contrastive learning, which forces the network to disentangle the foreground and the background"

### Mechanism 3
- Claim: Averaging multiple CAM variants (M1-M4 and aggregated Mf) improves final segmentation stability
- Mechanism: Final segmentation uses average of all individual exit CAMs plus attention-learned aggregated map, creating consensus prediction that reduces variance from any single source
- Core assumption: Different CAM generation methods capture complementary aspects of tumor location, and averaging reduces false positives/negatives
- Evidence anchors: [section] "Finally, we average the activation maps M1 to M4 and the aggregated map Mf to obtain the final CAM results"

## Foundational Learning

- Concept: Class Activation Mapping (CAM)
  - Why needed here: CAM provides the weakly-supervised mechanism to localize tumors using only image-level labels
  - Quick check question: What is the fundamental limitation of standard CAM that AME-CAM addresses?

- Concept: Multiple-exit training
  - Why needed here: Enables extraction of activation maps at different spatial resolutions for hierarchical aggregation
  - Quick check question: How does adding classifiers after each residual block enable multiple-exit training?

- Concept: Attention-based feature aggregation
  - Why needed here: Learns optimal pixel-wise weights for combining multi-resolution features rather than simple averaging
  - Quick check question: What advantage does attention-based aggregation have over simple averaging of CAMs?

## Architecture Onboarding

- Component map: Input → Multiple-exit classification → Activation extraction → Attention aggregation → Weighted CAM combination → DenseCRF refinement
- Critical path: Input → Multiple-exit classification → Activation extraction → Attention aggregation → Weighted CAM combination → DenseCRF refinement
- Design tradeoffs: Multiple exits increase computational cost but provide multi-resolution features; attention aggregation adds parameters but learns optimal weighting vs. simple averaging; contrastive learning requires careful balancing of foreground/background sampling
- Failure signatures: Low Dice scores across all MRI modalities suggest attention model fails to distinguish tumor; high HD95 values indicate boundary estimation problems; inconsistent performance across modalities suggests modality-specific feature extraction issues
- First 3 experiments: 1) Test single-exit vs. multiple-exit performance to validate multi-resolution benefit; 2) Compare attention aggregation vs. simple averaging to quantify learned weighting benefit; 3) Evaluate contrastive loss ablations to confirm foreground/background disentanglement effectiveness

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but raises several implicit ones through its methodology and evaluation scope, particularly regarding generalization to other tumor types and 3D volume processing.

## Limitations

- The contrastive learning mechanism's effectiveness depends heavily on tumor-background intensity contrast in MRI data, which may vary across modalities and patient populations
- Evaluation uses 2D slices from 3D volumes, which may not capture volumetric tumor relationships and could affect Dice score comparability with fully supervised 3D methods
- Ablation studies lack sufficient detail to quantify individual contribution of each component (multi-exit, attention aggregation, contrastive learning)

## Confidence

- High confidence: The overall framework architecture (multi-exit + attention aggregation + DenseCRF) is technically sound and addresses the known CAM resolution limitation
- Medium confidence: The contrastive learning approach effectively disentangles foreground/background, based on reported Dice score improvements over baselines
- Medium confidence: The averaged CAM approach (M1-M4 + Mf) provides more stable predictions, though individual component contributions are not fully isolated in ablation studies

## Next Checks

1. Conduct ablation studies isolating each innovation (multi-exit architecture alone, attention aggregation alone, contrastive learning alone) to quantify individual contribution to performance gains
2. Test the method on multi-modal datasets with varying tumor-background contrast to evaluate robustness of the contrastive learning mechanism
3. Perform 3D volume-based evaluation using the same metrics to assess whether 2D slice-based evaluation introduces systematic bias in Dice score reporting