---
ver: rpa2
title: Defending Pre-trained Language Models as Few-shot Learners against Backdoor
  Attacks
arxiv_id: '2309.13256'
source_url: https://arxiv.org/abs/2309.13256
tags:
- few-shot
- samples
- latexit
- poisoned
- backdoor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel defense mechanism called MDP (masking-differential
  prompting) for pre-trained language models (PLMs) as few-shot learners against backdoor
  attacks. The authors address the challenge of defending PLMs under few-shot scenarios,
  where existing defenses are inadequate due to limited training data and computational
  capacity.
---

# Defending Pre-trained Language Models as Few-shot Learners against Backdoor Attacks

## Quick Facts
- arXiv ID: 2309.13256
- Source URL: https://arxiv.org/abs/2309.13256
- Reference count: 40
- Key outcome: MDP effectively defends PLMs against various backdoor attacks under few-shot setting, outperforming baselines while maintaining downstream task performance

## Executive Summary
This paper addresses the challenge of defending pre-trained language models (PLMs) as few-shot learners against textual backdoor attacks. The authors introduce MDP (masking-differential prompting), a defense mechanism that leverages the masking-sensitivity gap between clean and poisoned samples to detect backdoors. Unlike existing defenses that require substantial training data or computational resources, MDP operates effectively under few-shot scenarios by using limited data as distributional anchors to identify poisoned samples through their higher sensitivity to random masking.

## Method Summary
MDP detects poisoned samples by comparing representational changes under varying masking. For a given input, MDP constructs a prompt, queries the PLM to obtain a probability distribution, and measures the distance between this distribution and anchor distributions derived from few-shot clean data using KL divergence. The method then masks the input, recomputes the distribution, and calculates the representational change using Kendall rank coefficient. Poisoned samples exhibit larger variations in this representation under masking and are flagged as such. MDP optionally optimizes the prompt to improve masking-invariance of clean samples, enhancing detection accuracy. The method creates a theoretical trade-off for attackers between attack effectiveness and detection evasiveness.

## Key Results
- MDP achieves high clean accuracy (CA) while effectively detecting backdoor attacks, outperforming baseline defenses by large margins
- The method successfully defends against multiple attack types including BadNets, AddSent, LWP, EP, and SOS across benchmark datasets
- MDP's masking-invariance optimization further improves detection performance while maintaining classification accuracy on downstream tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MDP detects poisoned samples by comparing representational changes under varying masking
- Mechanism: For a given sample X test in, MDP constructs its prompt X test prompt, queries the PLM to obtain the distribution ktest, and measures the distance between ktest and each anchor a(i) using KL divergence. It then computes the coordinates d(X test in) = [DKL(ktest||a(i))] as the sample's representation with respect to the anchors. MDP masks X test in to get ˆX test in, computes its coordinates d( ˆX test in), and measures the representational change τ(X test in) = ∆(d( ˆX test in), d(X test in)) using Kendall rank coefficient. Poisoned samples show larger variations in τ under varying masking and are detected as such.
- Core assumption: Clean and poisoned samples exhibit discernible differences in their sensitivity to random masking, with poisoned samples showing higher sensitivity.
- Evidence anchors: [abstract]: "MDP leverages the gap between the masking-sensitivity of poisoned and clean samples"; [section 4.2]: "To quantify the representational change of a given sample under masking, we leverage the limited few-shot data... As the anchor set."; [corpus]: Weak evidence
- Break condition: If clean samples happen to show similar sensitivity to random masking as poisoned samples, or if the few-shot data fails to provide effective distributional anchors, MDP's detection mechanism breaks down.

### Mechanism 2
- Claim: MDP optionally optimizes the prompt to improve the masking-invariance of clean samples
- Mechanism: MDP defines a masking-invariant constraint LMI = E_Xin,mask(·)ℓ(fθ( ˆX prompt), fθ(X prompt)) where the expectation is taken over the few-shot data Xin and random masking mask(·). This constraint encourages the model to generate similar distributions for a clean sample under varying masking. LMI is pluggable into any prompt-based learning methods to complement other optimization objectives.
- Core assumption: Improving the masking-invariance of clean samples widens the gap between clean and poisoned samples' sensitivity to random masking, enhancing detection accuracy.
- Evidence anchors: [abstract]: "The method also optionally optimizes the prompt to improve the masking-invariance of clean samples."; [section 4.3]: "Recall that MDP distinguishes clean and poisoned samples based on the gap between their sensitivity to random masking. To further boost its distinguishing power, we (optionally) optimize the prompt to improve the masking invariance of clean samples."; [corpus]: Weak evidence
- Break condition: If the prompt optimization fails to improve the masking-invariance of clean samples, or if it negatively impacts the model's classification accuracy, the effectiveness of this mechanism is compromised.

### Mechanism 3
- Claim: MDP creates an interesting dilemma for the attacker to choose between attack effectiveness and detection evasiveness
- Mechanism: Theorem 4.1 reveals that to evade detection, the attacker must satisfy |h(κ+) - h(κ−)| ≤ n√(n - 1) γ, where κ+ and κ− are the model's output probabilities for positive class given a non-trigger token masked and trigger token masked, respectively. For the attack to be effective, κ+ should be large, but to evade detection, κ+ is upper-bounded by the inequality. Thus, MDP creates a dilemma for the attacker.
- Core assumption: The attacker cannot simultaneously achieve high attack effectiveness and evade MDP's detection mechanism.
- Evidence anchors: [abstract]: "We show analytically that MDP creates an interesting dilemma for the attacker to choose between attack effectiveness and detection evasiveness."; [section 4.4]: "Theorem 4.1 reveals that there exists a trade-off between attack effectiveness and detection evasiveness."; [corpus]: Weak evidence
- Break condition: If the attacker finds a way to circumvent the trade-off, such as by using more sophisticated trigger designs or attack strategies, MDP's theoretical justification breaks down.

## Foundational Learning

- Concept: Prompt-based learning
  - Why needed here: MDP is a defense mechanism specifically designed for pre-trained language models as few-shot learners under the prompt-based learning paradigm
  - Quick check question: What is the key difference between prompt-based learning and conventional fine-tuning?

- Concept: Backdoor attacks
  - Why needed here: MDP aims to defend against textual backdoor attacks, where misclassification rules are injected into language models and activated by poisoned samples containing triggers
  - Quick check question: How do backdoor attacks typically inject misclassification rules into language models?

- Concept: Masking sensitivity
  - Why needed here: MDP leverages the gap between the masking-sensitivity of poisoned and clean samples to detect poisoned samples
  - Quick check question: Why do poisoned samples tend to show higher sensitivity to random masking compared to clean samples?

## Architecture Onboarding

- Component map: Input sample X test in -> Prompt construction X test prompt -> PLM f(θ) to obtain distribution ktest -> Anchor set A = {a(i)} from few-shot data -> Representational change τ(X test in) using KL divergence and Kendall rank coefficient -> Detection threshold γ -> Output: poisoned or clean sample

- Critical path:
  1. Construct prompt X test prompt from input sample X test in
  2. Query PLM f(θ) to obtain distribution ktest
  3. Compute coordinates d(X test in) using KL divergence with anchor set A
  4. Mask X test in to get ˆX test in and compute coordinates d( ˆX test in)
  5. Calculate representational change τ(X test in) using Kendall rank coefficient
  6. Compare τ(X test in) with detection threshold γ
  7. Output poisoned or clean sample

- Design tradeoffs:
  - Masking rate: Higher masking rate may lead to more accurate detection but also higher computational cost
  - Number of trials: More trials may improve detection accuracy but increase runtime
  - Weight of LMI: Proper calibration of LMI weight is crucial for balancing masking invariance and classification accuracy

- Failure signatures:
  - High false positive rate: Clean samples are mistakenly detected as poisoned
  - High false negative rate: Poisoned samples are not detected
  - Poor runtime performance: MDP takes too long to process each sample

- First 3 experiments:
  1. Evaluate MDP's detection performance on a small dataset with synthetic poisoned samples
  2. Compare MDP's performance with baseline defenses on a benchmark dataset
  3. Investigate the impact of varying masking rates and number of trials on MDP's detection accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the masking-invariance constraint (LMI) interact with different prompt tuning methods beyond DART, such as P-Tuning or Prefix-Tuning?
- Basis in paper: [explicit] The paper mentions that LMI is pluggable into any prompt-based learning methods but only evaluates it with DART
- Why unresolved: The paper only tests LMI with one specific prompt tuning method, leaving the generalizability to other methods unexplored
- What evidence would resolve it: Empirical results comparing MDP with LMI across multiple prompt tuning methods on the same datasets

### Open Question 2
- Question: What is the performance impact of MDP when the backdoor trigger is inserted at different positions within the text (beginning, middle, end) rather than always at the first position?
- Basis in paper: [inferred] The paper assumes triggers are inserted at the first position for RAP baseline, but doesn't explore positional variations for other attacks or MDP's detection capability
- Why unresolved: The evaluation focuses on a single trigger position, potentially missing positional vulnerabilities or detection patterns
- What evidence would resolve it: Comparative analysis of MDP's FAR/FRR across different trigger positions for the same attack types

### Open Question 3
- Question: How does MDP's performance scale with extremely limited data scenarios, such as one-shot or zero-shot learning settings?
- Basis in paper: [explicit] The paper acknowledges evaluating MDP under limited few-shot data (K as low as 4) but notes that practical scenarios might involve even scarcer data
- Why unresolved: The paper doesn't evaluate MDP's effectiveness at the one-shot or zero-shot extremes despite acknowledging this as a limitation
- What evidence would resolve it: Empirical results showing MDP's detection accuracy and false rates at K=1 and K=0 across multiple datasets and attack types

## Limitations

- The theoretical trade-off between attack effectiveness and detection evasiveness lacks rigorous proof and empirical validation beyond the stated theorem
- MDP's effectiveness depends heavily on the representativeness of the few-shot anchor set, with no thorough investigation of scenarios where few-shot data might be biased or insufficient
- The evaluation focuses on five specific attack types that share certain characteristics, leaving unclear whether MDP would be effective against more sophisticated attacks that adaptively minimize masking sensitivity differences

## Confidence

- **High confidence**: The core mechanism of using masking-sensitivity differences for detection is empirically validated across multiple datasets and attacks, with clear performance improvements over baselines
- **Medium confidence**: The optional prompt optimization mechanism shows theoretical promise but lacks comprehensive ablation studies demonstrating its necessity versus the base MDP approach
- **Low confidence**: The theoretical trade-off claim between attack effectiveness and detection evasiveness lacks rigorous proof and empirical validation beyond the stated theorem

## Next Checks

1. **Cross-dataset anchor generalization**: Evaluate MDP's performance when the few-shot anchor set comes from a different but related dataset than the test samples. This would validate whether MDP's detection mechanism relies too heavily on dataset-specific characteristics or demonstrates true distributional learning.

2. **Adaptive attack evaluation**: Design and test a targeted attack that specifically optimizes for low masking sensitivity variation, such as an attack that uses gradient-based optimization to minimize the representational change under masking while maintaining attack effectiveness. Measure whether MDP's detection performance degrades against such adaptive adversaries.

3. **Sensitivity to anchor set size**: Systematically vary the number of few-shot samples (K) from 4 to 64 per class and measure the corresponding changes in detection accuracy (AUC), false positive rate, and false negative rate. This would establish the minimum data requirements for MDP to function effectively and identify potential failure modes when few-shot data is severely limited.