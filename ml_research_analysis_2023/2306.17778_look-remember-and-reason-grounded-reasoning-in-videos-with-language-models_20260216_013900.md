---
ver: rpa2
title: 'Look, Remember and Reason: Grounded reasoning in videos with language models'
arxiv_id: '2306.17778'
source_url: https://arxiv.org/abs/2306.17778
tags:
- objects
- visual
- reasoning
- rubber
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes "Look, Remember, Reason" (LRR), a framework
  that equips large language models with low-level visual capabilities for complex
  visual reasoning tasks. LRR uses rationales with surrogate tasks (e.g., object detection,
  re-identification, tracking) expressed in natural language, integrated with a two-stream
  video encoder and top-down attention.
---

# Look, Remember and Reason: Grounded reasoning in videos with language models

## Quick Facts
- arXiv ID: 2306.17778
- Source URL: https://arxiv.org/abs/2306.17778
- Reference count: 40
- The paper proposes a framework that equips LLMs with low-level visual capabilities for complex visual reasoning tasks using rationales with surrogate tasks.

## Executive Summary
The paper introduces the "Look, Remember, Reason" (LRR) framework that enables large language models to perform complex visual reasoning tasks by integrating low-level visual capabilities. LRR uses rationales with surrogate tasks (like object detection, re-identification, and tracking) expressed in natural language, combined with a two-stream video encoder and top-down attention mechanism. This allows the model to extract visual information step-by-step and integrate it to arrive at final answers. The approach achieves state-of-the-art performance on diverse visual reasoning tasks from ACRE, CATER, Something-Else, and STAR datasets.

## Method Summary
LRR integrates a CNN-based visual feature extractor with an auto-regressive language model using top-down cross-attention. The model processes interleaved visual and textual tokens, where the LLM generates rationales that describe visual tasks (surrogate tasks) in natural language. These rationales guide the extraction of low-level visual information through the attention mechanism, allowing the model to "look" at visual features, "remember" intermediate results, and "reason" to arrive at final answers. The framework is trained end-to-end on multiple visual reasoning datasets simultaneously.

## Key Results
- Achieves state-of-the-art performance on ACRE, CATER, Something-Else, and STAR datasets
- Outperforms task-specific methods by a large margin across diverse visual reasoning tasks
- Demonstrates effectiveness of rationales with surrogate tasks for grounding LLM reasoning in visual inputs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Top-down cross-attention layers allow the LLM to guide the extraction of low-level visual information from grid-level CNN features.
- Mechanism: The LLM's higher-level hidden states act as query vectors that modulate the attention over spatial grid features from the CNN, enabling object-level visual information extraction.
- Core assumption: The CNN's grid-level features preserve spatial information needed for visual reasoning, and the LLM's representations at higher layers encode task-relevant semantics.
- Evidence anchors:
  - [abstract] "We show that a two-stream video encoder with spatiotemporal attention is effective at capturing the required static and motion-based cues in the video."
  - [section 3.2] "Our LRR model, as shown in Figure 2, employs grid-level visual features obtained from ResNet(He et al., 2016) based CNN... In our approach, the CNN encodes the input image sequence I = (v1, . . . ,vtv) into ¯I = (¯v1, . . . ,¯vtv), where ¯vi = CNN(vi) and ¯vi ∈ Rg×q′."
  - [corpus] Weak - the corpus papers discuss grounded reasoning but not the specific mechanism of top-down cross-attention.

### Mechanism 2
- Claim: Rationales with surrogate tasks expressed in natural language enable the LLM to learn low-level visual capabilities.
- Mechanism: By generating rationales that explicitly describe visual tasks (e.g., object recognition, tracking), the LLM learns to solve these tasks and "remember" the results for subsequent reasoning steps.
- Core assumption: LLMs can learn to solve visual tasks when they are expressed in natural language and integrated into the reasoning process.
- Evidence anchors:
  - [abstract] "We propose training an LM end-to-end on low-level surrogate tasks, including object detection, re-identification, and tracking, to endow the model with the required low-level visual capabilities."
  - [section 3.3] "In our LRR model, we leverage the flexibility of LLMs to express diverse low-level visual tasks through language in a generalized setup... This enables the model to understand the spatial relation of the target object to other objects in the scene."
  - [corpus] Weak - the corpus papers discuss grounded reasoning but not the specific mechanism of using rationales with surrogate tasks.

### Mechanism 3
- Claim: The "Look, Remember, Reason" paradigm enables step-by-step reasoning by integrating visual information extracted through low-level tasks.
- Mechanism: The LLM generates rationales that describe the visual information extraction process (Look), remembers the results of intermediate steps, and uses this information to reason and arrive at a final answer (Reason).
- Core assumption: The LLM can effectively integrate information from multiple steps and use it for reasoning.
- Evidence anchors:
  - [abstract] "By leveraging the LM's ability to perform the low-level surrogate tasks, we can cast reasoning in videos as the three-step process of Look, Remember, Reason wherein visual information is extracted using low-level visual skills step-by-step and then integrated to arrive at a final answer."
  - [section 3.3] "This property can be exploited in pure text-based reasoning tasks by placing exemplary rationales ("chain-of-thought") as a prompt to the LLM along with the statement of the task to be solved... In our LRR model, we leverage the flexibility of LLMs to express diverse low-level visual tasks through language in a generalized setup."
  - [corpus] Weak - the corpus papers discuss chain-of-thought reasoning but not the specific "Look, Remember, Reason" paradigm for visual reasoning.

## Foundational Learning

- Concept: Understanding of auto-regressive language models and their ability to learn in-context.
  - Why needed here: The LRR model is based on an auto-regressive framework, and understanding how these models learn in-context is crucial for understanding the rationale generation process.
  - Quick check question: How does an auto-regressive language model learn to solve a new task when given a few examples in the prompt?

- Concept: Knowledge of multi-modal models and their ability to integrate visual and textual information.
  - Why needed here: The LRR model is a multi-modal model that integrates visual information from a CNN with textual information from the LLM. Understanding how these models work is crucial for understanding the top-down cross-attention mechanism.
  - Quick check question: How does a multi-modal model integrate visual and textual information, and what are the challenges in doing so?

- Concept: Familiarity with visual reasoning tasks and the challenges they present.
  - Why needed here: The LRR model is designed to solve visual reasoning tasks, and understanding the challenges these tasks present (e.g., spatial reasoning, object tracking) is crucial for understanding the need for rationales with surrogate tasks.
  - Quick check question: What are the key challenges in visual reasoning tasks, and how do current models address these challenges?

## Architecture Onboarding

- Component map: Visual input -> CNN feature extraction -> Grid-level features -> LLM with top-down cross-attention -> Rationale generation -> Final answer

- Critical path:
  1. Input: Interleaved sequence of visual and textual tokens
  2. Visual feature extraction: CNN encodes images into grid-level features
  3. Top-down attention: LLM's hidden states guide attention over visual features
  4. Rationale generation: LLM generates rationales with surrogate tasks
  5. Reasoning: LLM integrates information from rationales to arrive at final answer

- Design tradeoffs:
  - Using a CNN for visual features vs. a transformer-based vision model: CNNs preserve spatial information better, but transformers can capture more global context.
  - Including surrogate tasks in rationales vs. relying on the LLM's existing capabilities: Surrogate tasks ensure the LLM learns necessary low-level visual skills, but may increase the complexity of the rationale generation process.

- Failure signatures:
  - Poor performance on visual reasoning tasks: May indicate issues with the visual feature extraction, top-down attention, or rationale generation.
  - Incoherent or irrelevant rationales: May indicate issues with the LLM's ability to generate meaningful rationales or the surrogate tasks not being well-defined.

- First 3 experiments:
  1. Evaluate the LRR model on a simple visual reasoning task (e.g., object counting) to verify the basic functionality of the model.
  2. Ablate the top-down cross-attention layers to assess their importance for visual information extraction.
  3. Evaluate the impact of different types of surrogate tasks in the rationales on the model's performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the "Look, Remember, Reason" framework scale to more complex visual reasoning tasks beyond the evaluated datasets?
- Basis in paper: [explicit] The paper demonstrates effectiveness on ACRE, CLEVR, and CATER datasets, but mentions that "a promising direction of future research would be joint training on an even larger set of visual reasoning datasets."
- Why unresolved: The paper only evaluates the model on three specific datasets, and does not explore its performance on a broader range of visual reasoning tasks.
- What evidence would resolve it: Evaluating the LRR model on a diverse set of visual reasoning datasets, including those with more complex reasoning requirements, would provide evidence of its scalability and generalization capabilities.

### Open Question 2
- Question: How does the choice of backbone vision model (e.g., ResNet, ViT) affect the performance of the LRR framework?
- Basis in paper: [explicit] The paper mentions using ResNet-101 as the vision backbone across all tasks, but also discusses the use of CLIP and ViT in comparison baselines.
- Why unresolved: The paper does not provide a detailed analysis of how different backbone vision models impact the LRR framework's performance.
- What evidence would resolve it: Conducting experiments with various backbone vision models and comparing their performance on the evaluated datasets would provide insights into the optimal choice for different visual reasoning tasks.

### Open Question 3
- Question: How does the size of the language model (e.g., OPT-125M, OPT-1.3B) influence the effectiveness of the LRR framework?
- Basis in paper: [explicit] The paper uses OPT-125M and OPT-1.3B models, and mentions that "similar performance can be achieved using other pre-trained models."
- Why unresolved: The paper does not explore the impact of using larger language models on the LRR framework's performance.
- What evidence would resolve it: Evaluating the LRR model with different language model sizes, including larger models, would provide insights into the relationship between model size and performance.

## Limitations

- The specific construction process for rationales across different datasets remains unspecified, making it difficult to assess generalizability
- Hyper-parameters for training the LRR model are not provided, hindering faithful reproduction
- Most claims about the effectiveness of mechanisms rely on the paper's own experimental results rather than external validation

## Confidence

**High Confidence**: The basic architectural components (LLM backbone, CNN feature extraction, auto-regressive framework) are well-established and clearly specified.

**Medium Confidence**: The integration of top-down attention and rationale generation is described in sufficient detail to understand the approach, though the effectiveness of these mechanisms relies heavily on the paper's own experimental results.

**Low Confidence**: The specific mechanisms by which rationales with surrogate tasks enable learning of low-level visual capabilities are not independently verified. The paper's claims about this process are plausible but lack external validation.

## Next Checks

1. **Ablation Study on Rationale Quality**: Systematically vary the quality and completeness of rationales (e.g., remove object detection steps, remove tracking information) to quantify how much each component contributes to final performance. This would validate whether the "Look, Remember, Reason" paradigm actually requires all three steps or if some are redundant.

2. **Cross-Dataset Rationale Transfer**: Train rationales on one dataset (e.g., CLEVR) and test on another (e.g., ACRE) to assess whether the rationale generation process is truly generalizable or overfits to specific dataset characteristics. This would reveal whether the approach relies on dataset-specific engineering.

3. **Visual Feature Analysis**: Visualize the attention weights from the LLM to the CNN features during the "Look" phase to verify that the model is actually extracting relevant visual information rather than just using textual cues. This would confirm whether the top-down attention mechanism is functioning as claimed.