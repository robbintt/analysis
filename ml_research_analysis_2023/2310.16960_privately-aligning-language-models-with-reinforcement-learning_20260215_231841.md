---
ver: rpa2
title: Privately Aligning Language Models with Reinforcement Learning
arxiv_id: '2310.16960'
source_url: https://arxiv.org/abs/2310.16960
tags:
- reward
- privacy
- human
- alignment
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a differentially private framework for aligning
  large language models (LLMs) with reinforcement learning (RL). The authors propose
  a method that incorporates differential privacy (DP) guarantees throughout the alignment
  process, which consists of three main steps: supervised fine-tuning (SFT), learning
  a reward model, and fine-tuning with Proximal Policy Optimization (PPO).'
---

# Privately Aligning Language Models with Reinforcement Learning

## Quick Facts
- **arXiv ID:** 2310.16960
- **Source URL:** https://arxiv.org/abs/2310.16960
- **Reference count:** 40
- **Key outcome:** Introduces a differentially private framework for aligning large language models with reinforcement learning, achieving competitive performance with strong privacy guarantees.

## Executive Summary
This paper presents a differentially private framework for aligning language models with reinforcement learning (RL). The authors propose a three-stage pipeline that incorporates differential privacy (DP) guarantees throughout: supervised fine-tuning (SFT), learning a reward model, and fine-tuning with Proximal Policy Optimization (PPO). By applying differentially private stochastic gradient descent (DPSGD) at each stage and leveraging parallel composition, the framework ensures the final model parameters are (ε, δ)-differentially private with respect to the private dataset. The approach is evaluated on sentiment generation and summarization tasks, demonstrating that privately aligned models can achieve competitive performance compared to non-private models while providing strong privacy protections.

## Method Summary
The paper introduces a three-stage differentially private alignment framework for language models. First, a pre-trained language model undergoes DP-SFT on one subset of the private data. Second, a reward model is trained privately on human preference data using DPSGD. Third, the language model is fine-tuned with DP-PPO using the private reward model. LoRA adapters are used throughout to stabilize DP training. The framework ensures (ε, δ)-differential privacy through parallel composition, assuming each user contributes to at most one of the three disjoint datasets.

## Key Results
- Privately aligned models achieve competitive performance: On IMDb sentiment generation, DP GPT2-Large obtains average reward of 3.37 (ε=4) vs 3.47 for best non-private model
- Larger models show better privacy-utility tradeoffs
- DP framework successfully prevents reward model from leaking information through PPO gradients
- LoRA fine-tuning stabilizes DP training and reduces sensitivity to per-sample updates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The DP framework ensures final model parameters are (ε, δ)-DP with respect to the private dataset by applying DPSGD at each stage of the alignment pipeline.
- Mechanism: DPSGD adds Gaussian noise to gradients during each training stage (SFT, reward model learning, PPO), limiting the influence of any single data point. The parallel composition theorem then bounds the total privacy loss across stages.
- Core assumption: Each user contributes to at most one of the three disjoint datasets (D1, D2, D3).
- Evidence anchors:
  - [abstract] "The framework ensures that the final model parameters are (ε, δ)-differentially private with respect to the private dataset used in the alignment."
  - [section 4] "Due to the parallel composition theorem of DP (McSherry, 2009), our framework is (ε, δ)-DP."
  - [corpus] Weak: no direct DP pipeline studies found in neighbors; evidence comes from cited works (Abadi et al., 2016).
- Break condition: If a single user contributes to more than one dataset, parallel composition no longer applies and total ε would increase.

### Mechanism 2
- Claim: Making the reward model private prevents the PPO step from leaking information through reward gradients.
- Mechanism: If the reward model were non-private, PPO gradients would depend on the entire reward model training set D2, invalidating privacy amplification by subsampling. A private reward model allows the subsampling bound to hold.
- Core assumption: Privacy amplification by subsampling requires each mini-batch gradient to depend only on a random subset of the dataset.
- Evidence anchors:
  - [section 4] "For every random mini-batch in the third step, the gradients are a function of the reward model, which in turn implies that the gradients of the mini-batch are a function of the entire dataset D2."
  - [corpus] Assumption: No explicit neighbor citations on reward model privacy; relies on DP fine-tuning literature (Yu et al., 2022).
- Break condition: If reward model is non-private and gradients depend on all of D2, subsampling amplification fails and privacy budget explodes.

### Mechanism 3
- Claim: Using LoRA fine-tuning stabilizes DP training and prevents large drift from the base model.
- Mechanism: LoRA applies low-rank updates to attention layers, reducing the number of parameters changed and making gradient clipping more stable under noise addition.
- Core assumption: LoRA reduces the sensitivity of the model to per-sample updates compared to full fine-tuning.
- Evidence anchors:
  - [section 4] "We make this algorithmic choice due to 3 reasons: 1) DP training works better with LoRA as hyperparameters are more stable (Yu et al., 2022)."
  - [corpus] Weak: no LoRA-DP studies in neighbors; evidence from Yu et al. (2022) citation.
- Break condition: If LoRA rank is too high or updates are too aggressive, DP noise may not sufficiently mask individual contributions.

## Foundational Learning

- Concept: Differential Privacy (DP)
  - Why needed here: DP provides the formal privacy guarantee that the aligned model does not memorize or leak information about individual users in the private dataset.
  - Quick check question: What does it mean for an algorithm to be (ε, δ)-DP, and why is ε called the privacy budget?

- Concept: Privacy Amplification by Subsampling
  - Why needed here: DPSGD uses random mini-batches; amplification bounds the privacy loss by the sampling probability, crucial for controlling ε in large datasets.
  - Quick check question: If you sample a batch of size b from a dataset of size n, what is the amplification factor?

- Concept: Composition Theorems (Parallel vs. Advanced)
  - Why needed here: The paper uses parallel composition because datasets are disjoint; advanced composition would be needed if a user contributed to multiple stages.
  - Quick check question: When can you use parallel composition versus advanced composition for DP?

## Architecture Onboarding

- Component map: Pre-trained LLM -> DP-SFT (D1) -> DP Reward Model (D2) -> DP-PPO (D3) -> Final Policy
- Critical path:
  1. Load and partition private dataset into D1, D2, D3.
  2. Run DP-SFT on D1 → LM_sft.
  3. Initialize reward model from LM_sft; run DP reward training on D2.
  4. Initialize policy from LM_sft; run DPPPO on D3 with private reward model.
  5. Output final policy parameters.
- Design tradeoffs:
  - LoRA vs full fine-tuning: LoRA reduces sensitivity and stabilizes DP but may limit model capacity.
  - Batch size in DPPPO: Larger batches reduce noise scale but increase per-step privacy cost.
  - Dataset splitting: Disjoint splits enable parallel composition but reduce data per stage.
- Failure signatures:
  - ε grows too large: Likely due to dataset overlap or insufficient subsampling.
  - Model utility drops sharply: May indicate over-privacy (too much noise) or under-tuning of LoRA rank.
  - Reward model fails to train: Check if reward gradients depend on full D2.
- First 3 experiments:
  1. Run DP-SFT alone with varying ε, δ to confirm privacy-utility tradeoff matches prior work.
  2. Train reward model privately, then test reward accuracy on held-out data.
  3. Combine SFT and reward training privately, then run non-private PPO to isolate DPPPO effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal allocation of privacy budget across the three stages (SFT, reward modeling, PPO) in the DP alignment framework?
- Basis in paper: [inferred] The paper mentions that one can allocate the fixed privacy budget across the three steps, but does not explore this experimentally.
- Why unresolved: The paper takes the simpler approach of assuming a single user can contribute to at most one of the three datasets, avoiding the need to allocate privacy budget across steps. The optimal allocation strategy remains an open question.
- What evidence would resolve it: Systematic experiments varying the privacy budget allocation across the three stages, measuring the impact on final model utility and privacy guarantees.

### Open Question 2
- Question: How does the performance of privately aligned models scale with increasing model size and training data?
- Basis in paper: [explicit] The paper shows that larger models generally lead to better privacy-utility trade-offs, but does not explore scaling to much larger models like LLaMA.
- Why unresolved: The experiments were limited to GPT-2 family models due to compute constraints. The scaling behavior for state-of-the-art models remains unknown.
- What evidence would resolve it: Experiments training privately aligned models on much larger datasets and with models like LLaMA, measuring performance across different model sizes.

### Open Question 3
- Question: Can tighter privacy analysis techniques improve the privacy-utility trade-off in the DP alignment framework?
- Basis in paper: [inferred] The paper uses standard DP-SGD and privacy accounting techniques. There may be room for improvement with more advanced techniques.
- Why unresolved: The paper does not explore advanced privacy analysis techniques like Rényi DP or f-DP that could potentially yield tighter privacy bounds.
- What evidence would resolve it: Applying advanced privacy analysis techniques to the DP alignment framework and comparing the resulting privacy-utility trade-offs to the standard approach.

### Open Question 4
- Question: How does the DP alignment framework perform in the online setting where the reward model is continuously updated?
- Basis in paper: [inferred] The paper focuses on the offline setting with a fixed reward model. The online setting with dynamic reward models is not explored.
- Why unresolved: The paper does not consider the practical scenario where the reward model is continuously updated based on new human feedback. Adapting the DP framework to this setting is an open question.
- What evidence would resolve it: Extending the DP alignment framework to the online setting and evaluating its performance and privacy guarantees compared to the offline setting.

## Limitations

- The paper lacks detailed hyperparameter specifications for the DP components, making exact reproduction challenging
- The reward model architecture and training specifics for the summarization task are not provided
- No ablation studies are presented to isolate the contribution of each DP component to the final performance

## Confidence

- **High Confidence:** The core claim that DP can be incorporated throughout the alignment pipeline is well-supported by the mathematical framework and privacy analysis
- **Medium Confidence:** The empirical results showing competitive performance with privacy guarantees are convincing, though limited to two datasets and specific model sizes
- **Low Confidence:** The specific mechanism by which LoRA stabilizes DP training lacks direct empirical support within this paper, relying instead on citations to prior work

## Next Checks

1. **Verify DP Composition:** Conduct experiments to confirm that datasets D1, D2, and D3 are truly disjoint, and measure the actual privacy loss when a single user contributes to multiple stages
2. **Ablation Study:** Run non-private versions of each stage (SFT, reward model, PPO) to quantify the performance gap introduced by DP at each step
3. **Hyperparameter Sensitivity:** Perform a systematic sweep of key DP hyperparameters (noise multiplier, clipping norm, batch size) to identify the sensitivity of privacy-utility tradeoffs