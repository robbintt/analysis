---
ver: rpa2
title: 'ConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for Accelerating
  Language Models Inference'
arxiv_id: '2312.11882'
source_url: https://arxiv.org/abs/2312.11882
tags:
- layer
- instance
- early
- internal
- exiting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes ConsistentEE, an early exiting method for accelerating
  language models inference. It formulates the early exiting process as a reinforcement
  learning problem, where a policy network decides whether an instance should exit
  or continue.
---

# ConsistentEE: A Consistent and Hardness-Guided Early Exiting Method for Accelerating Language Models Inference

## Quick Facts
- **arXiv ID:** 2312.11882
- **Source URL:** https://arxiv.org/abs/2312.11882
- **Reference count:** 12
- **Primary result:** Achieves 34% saved layers without accuracy loss, and 51% saved layers with 1% accuracy drop compared to BERT-Base

## Executive Summary
ConsistentEE is an early exiting method for accelerating language model inference that addresses the inconsistency between training and inference phases in traditional early exiting approaches. The method formulates early exiting as a reinforcement learning problem where a policy network decides whether instances should exit or continue at each layer. By introducing the Memorized Layer concept to measure instance hardness and incorporating it into the reward function design, ConsistentEE allows easy instances to focus more on acceleration while hard instances focus more on accuracy. Experimental results on various natural language understanding and generation tasks demonstrate that ConsistentEE outperforms existing baselines.

## Method Summary
ConsistentEE proposes a reinforcement learning-based approach to early exiting that addresses the inconsistency between training and inference phases. The method introduces a policy network that decides whether to exit or continue at each layer, requiring only one internal classifier to correctly classify each instance rather than all classifiers. The Memorized Layer concept measures instance hardness by tracking when instances are consistently classified correctly, which is incorporated into a hardness-guided reward function. The training procedure employs iterative optimization that alternates between improving the policy network and internal classifiers/backbone until convergence is reached.

## Key Results
- Achieves 34% saved layers without accuracy loss on classification tasks
- Achieves 51% saved layers with only 1% accuracy drop compared to BERT-Base
- Outperforms baseline early exiting methods on both classification (RTE, MRPC, SST-2, QNLI, QQP, MNLI, M-CID, StackOverflow) and generation tasks (Alpaca, Dolly)

## Why This Works (Mechanism)

### Mechanism 1
The RL formulation reduces the strict requirement that all internal classifiers must correctly classify every instance. Traditional early exiting methods use weighted sum of cross-entropy losses during training, imposing that all classifiers predict all instances correctly. ConsistentEE only requires one classifier to correctly classify each instance, reducing the burden on each layer and allowing classifiers to focus on their strengths. Evidence shows that ConsistentEE enables each layer's classification accuracy to consistently remain at a high level.

### Mechanism 2
Incorporating instance hardness through the Memorized Layer concept into the reward function allows for differentiated treatment of easy and hard instances. The Memorized Layer measures when instances are consistently classified correctly, and the reward function is modified to give different weights to accuracy and acceleration based on this hardness measure. This encourages easy instances to exit early for acceleration while allowing hard instances to continue deeper for accuracy.

### Mechanism 3
The iterative training technique improves the capacity of both the policy network and internal classifiers through alternating optimization. The process involves three steps: calculating the memorized layer, optimizing the policy network, and optimizing internal classifiers and backbone. This iterative refinement gradually improves early exiting decisions and classification performance until convergence.

## Foundational Learning

- **Concept:** Reinforcement Learning (RL) - specifically Policy Gradient methods
  - *Why needed here:* To automatically learn the optimal layer for an instance to exit without ground truth exit layers
  - *Quick check question:* How does the policy gradient technique handle sparse rewards in the early exiting process?

- **Concept:** Cross-Entropy Loss and Classification Accuracy
  - *Why needed here:* To measure the performance of internal classifiers and optimize their classification capabilities
  - *Quick check question:* Why might requiring all internal classifiers to correctly classify every instance lead to poor performance in shallow layers?

- **Concept:** Instance Hardness and Curriculum Learning
  - *Why needed here:* To develop the Memorized Layer concept and incorporate instance hardness into the reward function design
  - *Quick check question:* How does the Memorized Layer concept differ from other measures of instance hardness like forgetting events?

## Architecture Onboarding

- **Component map:** Input -> Transformer layers -> Policy Network (MLP) -> Exit/Continue decision -> Internal Classifier (if exit) -> Output prediction
- **Critical path:** Input passes through Transformer layers; at each layer, policy network decides to exit or continue; if exit, internal classifier makes prediction; if continue, proceed to next layer; repeat until exit or final layer reached
- **Design tradeoffs:** Granularity of early exiting (layer-wise vs. token-wise), complexity of policy network architecture, balancing accuracy and acceleration in reward function, number of layers in backbone model
- **Failure signatures:** Policy network consistently makes poor exit decisions, internal classifiers fail to improve accuracy over iterations, Memorized Layer concept doesn't accurately reflect instance hardness, iterative training process doesn't converge
- **First 3 experiments:** 1) Implement basic RL formulation without memorized layer concept on small dataset to verify policy network's ability to learn exit decisions; 2) Add memorized layer concept and modify reward function to incorporate instance hardness, evaluate on larger dataset; 3) Implement full iterative training process and compare against baseline early exiting methods on multiple datasets

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unexplored:
1. How does the Memorized Layer concept improve the balance between accuracy and acceleration compared to traditional methods?
2. How does ConsistentEE's performance vary with different backbone models and tasks?
3. What is the decision-making process of the policy network and how does it impact overall performance?

## Limitations

- The paper lacks detailed architectural specifications for the policy network and hyperparameter values for training
- Performance comparisons with alternative instance hardness measures are not provided
- The iterative training process lacks convergence analysis and learning curve visualizations
- The method's effectiveness on a broader range of backbone models (beyond BERT) and tasks is not thoroughly evaluated

## Confidence

**Mechanism 1 (RL Formulation Reducing Strict Requirements): Medium**
- The RL formulation logically reduces the burden on internal classifiers
- Evidence is primarily comparative rather than mechanistic

**Mechanism 2 (Instance Hardness in Reward Function): Low-Medium**
- Memorized Layer concept is innovative but lacks direct validation
- Evidence is based on overall performance rather than isolating hardness-guided effects

**Mechanism 3 (Iterative Training Improvement): Medium**
- Iterative training is well-defined but effectiveness is supported mainly by comparative results
- Lacks convergence diagnostics and learning curve analysis

## Next Checks

1. **Ablation Study on Reward Function Components:** Remove the hardness-guided components from the reward function and retrain ConsistentEE. Compare performance specifically on easy vs. hard instances to validate whether the memorized layer concept improves the accuracy-acceleration balance.

2. **Convergence Analysis of Iterative Training:** Implement monitoring during iterative training to track changes in policy network decisions and internal classifier accuracy across iterations. Plot these metrics to verify convergence rather than oscillation or degradation.

3. **Comparison with Alternative Hardness Measures:** Implement ConsistentEE using alternative instance hardness measures (such as prediction entropy or forgetting events) instead of the memorized layer concept. Compare performance to determine whether the specific hardness measure matters or if any hardness indicator provides similar benefits.