---
ver: rpa2
title: LLM-powered Data Augmentation for Enhanced Cross-lingual Performance
arxiv_id: '2305.14288'
source_url: https://arxiv.org/abs/2305.14288
tags:
- data
- chatgpt
- gpt-4
- languages
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using Large Language Models (LLMs) for data
  augmentation to improve cross-lingual performance on commonsense reasoning tasks.
  The authors leverage four LLMs (Dolly-v2, StableVicuna, ChatGPT, and GPT-4) to generate
  synthetic data for three low-resource multilingual datasets (XCOPA, XWinograd, and
  XStoryCloze).
---

# LLM-powered Data Augmentation for Enhanced Cross-lingual Performance

## Quick Facts
- arXiv ID: 2305.14288
- Source URL: https://arxiv.org/abs/2305.14288
- Reference count: 21
- Key outcome: LLM-generated data improves cross-lingual performance on commonsense reasoning tasks, with GPT-4 showing most consistent gains.

## Executive Summary
This paper investigates using Large Language Models (LLMs) to generate synthetic training data for enhancing cross-lingual performance on low-resource multilingual commonsense reasoning tasks. The authors employ four LLMs (Dolly-v2, StableVicuna, ChatGPT, and GPT-4) to create synthetic examples for three multilingual datasets (XCOPA, XWinograd, and XStoryCloze). They then fine-tune smaller multilingual models (mBERT and XLMR) on this synthetic data and evaluate performance across multiple target languages. Results demonstrate consistent improvements when training on LLM-generated data compared to baselines, with GPT-4-generated data exhibiting the most robust performance across languages and tasks. Human evaluation reveals quality variations across languages, with Tamil showing particular challenges for ChatGPT and GPT-4 generation.

## Method Summary
The study uses four LLMs to generate synthetic training data for three low-resource multilingual commonsense reasoning tasks. The authors create datasets in both English and target languages, then fine-tune multilingual models (mBERT and XLMR-base/large) on this synthetic data. They evaluate cross-lingual transfer performance through zero-shot inference on validation sets across multiple languages. The experimental design includes comparisons between original data, LLM-generated data, and different combinations thereof, with particular attention to scaling effects and language-specific performance variations. Human evaluation assesses the naturalness and logical soundness of generated examples.

## Key Results
- GPT-4-generated data consistently improves performance across all three datasets compared to baselines
- Multilingual synthetic data outperforms zero-shot cross-lingual transfer from English data
- Larger models (XLMR-Large) show less improvement from synthetic data compared to smaller models
- Human evaluation reveals quality issues with Tamil and Swahili generation, despite performance improvements

## Why This Works (Mechanism)

### Mechanism 1
LLM-generated data supplements limited human training data by generating synthetic examples following the same format and reasoning patterns as human-created data, effectively expanding the training corpus. This works when generated examples maintain sufficient quality and relevance to the task, though it breaks down if generated examples diverge significantly from the task distribution or introduce noise.

### Mechanism 2
Training on multilingual synthetic data improves cross-lingual transfer more than zero-shot transfer from English data because LLMs generate examples directly in target languages, providing language-specific patterns that zero-shot transfer cannot capture. This mechanism relies on LLMs generating coherent text in target languages with sufficient quality, but breaks when LLM quality in target languages is insufficient to provide meaningful patterns.

### Mechanism 3
Larger models benefit less from synthetic data than smaller models because larger models have already captured more general patterns, leaving less room for synthetic data to provide additional benefit. This assumes model capacity relates inversely to synthetic data impact, but breaks if model architecture or training approach makes synthetic data more beneficial regardless of size.

## Foundational Learning

- Concept: Cross-lingual transfer learning
  - Why needed here: The paper relies on models trained in one language performing well in others
  - Quick check question: What factors influence how well knowledge transfers between languages?

- Concept: Data augmentation techniques
  - Why needed here: The paper's core approach is using LLMs to generate additional training data
  - Quick check question: What are the key differences between rule-based and model-based data augmentation?

- Concept: Evaluation of synthetic data quality
  - Why needed here: The paper includes human evaluation to assess generated data quality across languages
  - Quick check question: What metrics best capture the quality of synthetic data for NLP tasks?

## Architecture Onboarding

- Component map: LLM data generation -> Synthetic dataset creation -> Model fine-tuning -> Cross-lingual evaluation -> Human evaluation pipeline for quality assessment -> Translation API for synthetic multilingual data

- Critical path: 1. Generate synthetic data with LLMs 2. Fine-tune multilingual models on synthetic data 3. Evaluate cross-lingual performance

- Design tradeoffs: Open vs. closed LLMs (access, cost, performance) | Generate in English vs. target languages (quality vs. coverage) | Translation vs. direct generation (cost vs. quality)

- Failure signatures: Performance degradation when using synthetic data | Language-specific failures (e.g., Tamil in this paper) | Inconsistent improvements across different LLMs

- First 3 experiments: 1. Generate synthetic data in English and evaluate on XCOPA 2. Compare direct target language generation vs. translation for XCOPA 3. Scale up synthetic data volume and measure performance impact

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality of LLM-generated data vary across different target languages and what specific linguistic factors influence this variation? The paper identifies that certain languages like Tamil pose challenges for generation quality, but doesn't systematically analyze which linguistic features contribute to these difficulties or why some languages are particularly problematic.

### Open Question 2
What is the optimal balance between original human-created data and LLM-generated data for maximizing cross-lingual performance across different model sizes? While the paper finds that adding LLM-generated data generally improves performance, it doesn't systematically explore the optimal ratio or how this balance changes with model size and dataset characteristics.

### Open Question 3
How do different instruction formulations and prompting strategies affect the quality and consistency of LLM-generated data for multilingual commonsense reasoning tasks? The paper mentions experimenting with various instructions but doesn't provide a detailed analysis of how different instruction formulations impacted data quality or consistency across languages.

## Limitations

- Human evaluation reveals significant quality issues with generated data in certain languages (Tamil and Swahili), suggesting the approach may not generalize equally well across all language families
- The choice of LLM substantially impacts both data quality and performance outcomes, with GPT-4 showing the most consistent improvements but also being the most resource-intensive option
- The study focuses on commonsense reasoning tasks specifically, leaving open questions about whether these findings extend to other NLP domains

## Confidence

**High Confidence**: The finding that GPT-4-generated data consistently improves performance across multiple datasets and model sizes is well-supported by the experimental results. The comparison between zero-shot transfer and multilingual generation also shows clear advantages for the latter approach.

**Medium Confidence**: Claims about which languages benefit most from LLM-generated data and why certain languages (Tamil, Swahili) show degraded performance are less certain. The human evaluation provides qualitative insights but lacks statistical power to definitively explain these patterns.

**Low Confidence**: The assertion that larger models benefit less from synthetic data is based on limited observations and could be confounded by other factors like dataset size or task complexity.

## Next Checks

1. Conduct a more rigorous statistical comparison of generated vs. original data quality across all languages, particularly focusing on Tamil and Swahili to understand the sources of performance degradation

2. Replicate the experimental setup with non-commonsense reasoning tasks (e.g., named entity recognition, sentiment analysis) to test whether LLM-generated data augmentation generalizes beyond the current task scope

3. Quantify the computational and monetary costs of generating synthetic data across different LLMs and compare these to the performance gains, particularly for resource-constrained applications where smaller models like mBERT are preferred