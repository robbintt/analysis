---
ver: rpa2
title: Can training neural language models on a curriculum with developmentally plausible
  data improve alignment with human reading behavior?
arxiv_id: '2311.18761'
source_url: https://arxiv.org/abs/2311.18761
tags:
- training
- curriculum
- trained
- language
- babylm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigated whether training neural language models
  on developmentally plausible data (the BabyLM "strict-small" dataset) with a curriculum
  based on sentence difficulty would improve alignment with human reading behavior.
  The authors created a curriculum by training teacher LSTM models on subsets of the
  data and using their surprisal estimates to order sentences from easy to difficult.
---

# Can training neural language models on a curriculum with developmentally plausible data improve alignment with human reading behavior?

## Quick Facts
- arXiv ID: 2311.18761
- Source URL: https://arxiv.org/abs/2311.18761
- Reference count: 19
- Models trained on curriculum with developmentally plausible data showed poor alignment with human reading behavior on syntactic processing tasks

## Executive Summary
This paper investigates whether training neural language models on developmentally plausible data with a curriculum based on sentence difficulty can improve alignment with human reading behavior. The authors created a curriculum using cross-review surprisal estimates from LSTM teacher models trained on subsets of the BabyLM "strict-small" dataset. They then trained OPT 125M models with and without this curriculum and evaluated them on BabyLM challenge tasks and the SAP benchmark of syntactic processing. The results showed that curriculum training alone performed worse than random baselines, and all models showed poor alignment with human reading behavior on the SAP benchmark, suggesting that developmentally plausible data alone is insufficient to improve cognitive plausibility.

## Method Summary
The authors trained 5 LSTM teacher models on different metasets of the BabyLM "strict-small" dataset (~10M tokens), then used cross-review surprisal averaging to create difficulty scores for all sentences. These scores were used to order the training data from easy to difficult, which was then used to train OPT 125M models with a root-10 scheduler. Models were evaluated on BabyLM challenge tasks (BLiMP, SuperGLUE, MSGS) and the SAP benchmark for syntactic processing. The study compared curriculum-only models, curriculum + finetuning models, and random baseline models without curriculum.

## Key Results
- Curriculum-only models performed worse than random baselines on most BabyLM tasks
- Curriculum + finetuning models showed slightly better performance on grammatical tasks (BLiMP and MSGS) compared to random models
- All models demonstrated poor alignment with human reading behavior on the SAP benchmark, with no significant differences between experimental conditions
- Models trained on the curriculum acquired grammatical knowledge but struggled with lexical and factual information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Curriculum training with developmentally plausible data induces useful biases that help models acquire grammatical knowledge more efficiently.
- Mechanism: By training teacher models on subsets of the BabyLM data and using their surprisal estimates to order sentences from easy to difficult, the curriculum creates a learning trajectory that matches the gradual complexity of language acquisition.
- Core assumption: Language models can benefit from learning easier linguistic patterns before harder ones, similar to human language development.
- Evidence anchors:
  - [abstract] "models first trained on the BabyLM data curriculum and then on a few randomly ordered training epochs performed slightly better than models trained on randomly ordered epochs alone"
  - [section] "training on our curriculum by itself is insufficient to impart the necessary grammatical knowledge, but it might induce biases in the model that make it easier for the model to acquire this knowledge from training data"
- Break condition: If the correlation between our difficulty measure and simpler measures like sentence length is too high, simpler methods might work equally well.

### Mechanism 2
- Claim: Cross-review method for difficulty estimation reduces idiosyncratic noise in curriculum ordering.
- Mechanism: By averaging surprisal estimates from multiple teacher models trained on different subsets, the method creates a more robust measure of sentence difficulty that isn't tied to a single model's quirks.
- Core assumption: Individual teacher models will have varying perspectives on sentence difficulty due to training on different subsets, and averaging captures a more generalizable difficulty measure.
- Evidence anchors:
  - [section] "For any given sentence, there was a lot of variance in the surprisal estimates across the teachers: the average standard deviation was 113 bits of surprisal"
  - [section] "This highlights the importance of averaging the surprisal estimates across different teachers to avoid over-fitting to idiosyncrasies of any particular teacher model"
- Break condition: If teacher models become too similar (e.g., by training on larger, more overlapping subsets), the averaging benefit diminishes.

### Mechanism 3
- Claim: Developmentally plausible data can provide similar learning signals as larger, less curated datasets for certain linguistic tasks.
- Mechanism: The structured nature of child-directed speech and other developmentally plausible data contains concentrated linguistic information that can be extracted efficiently by properly trained models.
- Core assumption: The linguistic information in smaller, curated datasets is sufficient for models to learn core grammatical patterns if trained appropriately.
- Evidence anchors:
  - [abstract] "training on developmentally plausible datasets alone is likely insufficient to generate language models capable of accurately predicting human language processing"
  - [section] "This result, along with prior work on training models on child directed speech (Yedetore et al., 2023), suggests that merely training on developmentally plausible data is likely insufficient for bridging the gap between human behavior and language-model predicted behavior"
- Break condition: If models consistently underperform on grammatical tasks compared to models trained on larger datasets, the data efficiency claim breaks down.

## Foundational Learning

- Concept: Cross-review curriculum design
  - Why needed here: To create an ordering of training data that reflects relative difficulty and supports gradual learning
  - Quick check question: If we have 5 teacher models and 10,000 sentences, how many surprisal estimates does each sentence receive before averaging?

- Concept: Language model surprisal as difficulty measure
  - Why needed here: To quantify how predictable each sentence is, which serves as a proxy for processing difficulty
  - Quick check question: If a sentence has average surprisal of 5 bits per word and another has 2 bits per word, which is considered more difficult and why?

- Concept: Developmentally plausible datasets
  - Why needed here: To explore whether training on data similar to what humans encounter during language acquisition can improve model-human alignment
  - Quick check question: What are the key differences between the BabyLM "strict-small" dataset and typical web-scale training data in terms of token count and content sources?

## Architecture Onboarding

- Component map: Teacher LSTM models → Cross-review surprisal averaging → Difficulty ordering → OPT 125M target model training → Evaluation on BabyLM tasks and SAP benchmark
- Critical path: Data preparation → Teacher model training → Cross-review difficulty computation → Curriculum creation → Target model training → Evaluation
- Design tradeoffs: Sentence-level vs. token-level curriculum (sentence-level simpler but may underrepresent complex domains), LSTM vs. transformer teachers (LSTMs more efficient for small datasets), continuous vs. discrete scheduling (continuous allows gradual progression but requires careful hyperparameter tuning)
- Failure signatures: Poor curriculum performance might manifest as worse results than random baselines, high variance in teacher surprisal estimates suggesting difficulty measure instability, or curriculum that doesn't progress through domains as expected
- First 3 experiments:
  1. Train a single teacher LSTM on one metaset and use its surprisal estimates to order the full training data, then train a baseline model with this simple curriculum
  2. Create a curriculum using sentence length as the difficulty measure instead of cross-review surprisal, and compare performance to the surprisal-based curriculum
  3. Train a model with a discrete curriculum (fixed difficulty levels for fixed epoch ranges) instead of the continuous root-10 scheduler, and evaluate impact on BabyLM task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would training on a developmentally plausible curriculum significantly improve alignment with human reading behavior if the curriculum were optimized using a different difficulty measure than surprisal?
- Basis in paper: [inferred] The paper found that surprisal-based difficulty measures were highly correlated with simpler measures like sentence length, suggesting other measures might be equally or more effective.
- Why unresolved: The study only tested one type of curriculum based on surprisal estimates, leaving open whether other difficulty measures might yield better results.
- What evidence would resolve it: Comparing human reading time predictions from models trained on curricula using different difficulty measures (e.g., syntactic complexity, semantic plausibility) would show if alternative measures improve alignment.

### Open Question 2
- Question: Does the negative impact on acquiring lexical/factual knowledge from developmentally plausible data persist when using larger models or more diverse training data?
- Basis in paper: [explicit] The paper observed that models trained on the curriculum performed worse on tasks requiring lexical knowledge (irregular forms, hypernyms) and factual information (MNLI, BoolQ).
- Why unresolved: The study only tested the OPT 125M architecture on the BabyLM strict-small dataset, which may not be representative of all model sizes or data configurations.
- What evidence would resolve it: Testing larger models (e.g., OPT 1.3B) or models trained on more diverse subsets of BabyLM data on the same tasks would determine if the negative impact generalizes.

### Open Question 3
- Question: Are there specific developmental stages or types of linguistic knowledge acquisition that benefit more from curriculum learning than others?
- Basis in paper: [inferred] The paper found tentative evidence that the curriculum helped models acquire grammatical knowledge but not lexical or factual knowledge, suggesting stage-specific benefits.
- Why unresolved: The study did not systematically vary the curriculum or test knowledge acquisition across different linguistic domains.
- What evidence would resolve it: Designing curricula targeting specific linguistic phenomena (e.g., syntax vs. semantics) and testing models' acquisition of these skills would reveal stage-specific benefits.

## Limitations

- The BabyLM "strict-small" dataset (~10M tokens) may be too limited to properly evaluate curriculum effectiveness for complex syntactic phenomena
- The use of LSTM teacher models with different inductive biases than the transformer-based OPT 125M models may introduce architectural mismatches in difficulty ordering
- The study lacks comparison to alternative curriculum design methods, making it difficult to determine whether the cross-review approach or curriculum learning itself is ineffective

## Confidence

**High Confidence:** The finding that all models showed poor alignment with human reading behavior on the SAP benchmark is well-supported by the empirical results.

**Medium Confidence:** The conclusion that curriculum training alone does not improve grammatical knowledge acquisition is supported by the data, but the evidence for curriculum training inducing useful biases for finetuning is weaker.

**Low Confidence:** The assertion that the cross-review method is superior to simpler curriculum design approaches (like sentence length ordering) is not adequately tested in the paper.

## Next Checks

1. **Replicate with larger dataset and different teacher architecture:** Train curriculum models on a larger subset of the BabyLM data and use transformer-based teacher models instead of LSTMs to determine if poor curriculum performance is due to dataset size limitations or architectural mismatches.

2. **Compare cross-review to simple baselines:** Implement and evaluate a curriculum based solely on sentence length and another based on linguistic complexity measures to determine whether the cross-review method provides any advantage over simpler approaches.

3. **Evaluate on additional human reading behavior datasets:** Test the trained models on additional psycholinguistic datasets beyond SAP (such as Natural Stories or UCL corpus) to determine whether poor alignment with human processing is consistent across different types of syntactic phenomena.