---
ver: rpa2
title: Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning Strategies
  are not Better than Random Selection
arxiv_id: '2309.06131'
source_url: https://arxiv.org/abs/2309.06131
tags:
- training
- selection
- data
- effectiveness
- random
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of active learning strategies to
  reduce annotation costs when fine-tuning PLM-based rankers. The authors adapt three
  representative AL strategies (uncertainty-based, QBC, and diversity-based) to the
  task of selecting training data for fine-tuning PLM rankers.
---

# Annotating Data for Fine-Tuning a Neural Ranker? Current Active Learning Strategies are not Better than Random Selection

## Quick Facts
- arXiv ID: 2309.06131
- Source URL: https://arxiv.org/abs/2309.06131
- Reference count: 40
- Current mainstream active learning strategies fail to outperform random selection for fine-tuning PLM rankers

## Executive Summary
This paper investigates whether active learning (AL) strategies can reduce annotation costs when fine-tuning pre-trained language model (PLM) rankers. The authors adapt three representative AL strategies - uncertainty-based, Query-By-Committee (QBC), and diversity-based approaches - to select training data for PLM rankers. Surprisingly, their extensive experiments show that none of these AL strategies consistently outperform random selection in terms of effectiveness. While some AL strategies achieve higher effectiveness for specific training data sizes, these gains are not statistically significant and come at the expense of higher annotation costs. The budget-aware evaluation reveals that AL strategies generally require more assessments (and thus higher annotation costs) than random selection to achieve similar effectiveness.

## Method Summary
The study fine-tunes three PLM ranker architectures (MonoBERT, ColBERT, DPR) on MS Marco and TripClick datasets using incremental training on subsets of sizes [1K, 5K, 10K, 20K, 50K, 100K, 200K]. Random selection serves as the baseline, compared against uncertainty-based, QBC, and diversity-based active learning strategies. The evaluation tracks nDCG@10 effectiveness alongside annotation costs (number of query-passage assessments) and computational costs (GPU/CPU hours). Each experiment is repeated multiple times to measure variance in effectiveness between different randomly selected subsets of the same size.

## Key Results
- None of the three AL strategies (uncertainty-based, QBC, diversity-based) consistently outperform random selection across different training sizes and PLM architectures
- AL strategies generally require more assessments than random selection to achieve similar effectiveness, making them less cost-efficient
- Significant variability exists between different randomly selected training subsets of the same size, but current AL strategies cannot consistently identify high-yield subsets
- Computational costs are negligible compared to annotation costs, making annotation effort the primary cost factor

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active learning strategies do not consistently outperform random selection in fine-tuning PLM rankers
- Mechanism: Uncertainty-based, QBC, and diversity-based AL strategies fail to identify high-yield training subsets despite their theoretical advantages
- Core assumption: The effectiveness of a PLM ranker is primarily determined by the size of the training data, with variability between subsets of the same size
- Evidence anchors:
  - [abstract] "Our extensive analysis shows that AL strategies do not significantly outperform random selection of training subsets in terms of effectiveness"
  - [section] "For the task of fine-tuning PLM rankers, there is no single active learning selection strategy that consistently and significantly delivers higher effectiveness compared to a random selection of the training data"

### Mechanism 2
- Claim: The main cost factor in fine-tuning PLM rankers is annotation effort, not computational cost
- Mechanism: Annotation costs dominate the total cost of fine-tuning, making budget-aware evaluation crucial
- Core assumption: The number of assessments required to annotate training data is the primary cost driver
- Evidence anchors:
  - [abstract] "We further find that gains provided by AL strategies come at the expense of more assessments (thus higher annotation costs) and AL strategies underperform random selection when comparing effectiveness given a fixed annotation cost"
  - [section] "A first observation is that the main cost factor is the annotation cost, and hence the number of assessments needed to create the training data, which largely overrules the computational cost"

### Mechanism 3
- Claim: Variability in effectiveness exists between different randomly selected training subsets of the same size
- Mechanism: Some subsets of training data lead to significantly higher effectiveness than others, but AL strategies cannot consistently identify these high-yield subsets
- Core assumption: The effectiveness of a PLM ranker is not solely determined by the size of the training data but also by the specific samples selected
- Evidence anchors:
  - [abstract] "We observe a great variability in effectiveness when fine-tuning on different randomly selected subsets of training data"
  - [section] "We find that the best random selection run outperforms the worst one, and significantly"

## Foundational Learning

- Concept: Active Learning (AL) strategies
  - Why needed here: AL strategies are the primary focus of the paper, aiming to reduce annotation costs while maintaining effectiveness
  - Quick check question: What are the three main types of AL strategies adapted for fine-tuning PLM rankers in this paper?

- Concept: Budget-aware evaluation
  - Why needed here: To properly assess the effectiveness of AL strategies, considering both annotation and computational costs
  - Quick check question: What are the two main cost factors considered in the budget-aware evaluation?

- Concept: PLM rankers
  - Why needed here: The paper investigates the fine-tuning of PLM rankers under limited data and budget
  - Quick check question: What are the three PLM ranker architectures considered in this study?

## Architecture Onboarding

- Component map: Data collection -> PLM ranker fine-tuning -> AL strategy application -> Effectiveness evaluation
- Critical path: Data collection → PLM ranker fine-tuning → AL strategy application → Effectiveness evaluation
- Design tradeoffs:
  - Computational cost vs. annotation cost
  - Effectiveness vs. training data size
  - PLM ranker architecture choice
- Failure signatures:
  - AL strategies not outperforming random selection
  - High annotation costs outweighing computational costs
  - Inconsistent effectiveness across different training subsets
- First 3 experiments:
  1. Evaluate the effect of training data size on PLM ranker effectiveness
  2. Compare the effectiveness of different AL strategies to random selection
  3. Assess the budget-aware evaluation of AL strategies vs. random selection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific uncertainty estimation methods could be adapted from other domains (e.g., natural language processing) to improve active learning for PLM rankers?
- Basis in paper: [explicit] The paper notes that uncertainty estimation in Information Retrieval is largely unexplored for rankers based on PLMs, and existing approaches rely on assumptions and heuristics
- Why unresolved: These uncertainty estimation methods are not readily applicable to the PLM ranker architectures considered in the paper
- What evidence would resolve it: Empirical results showing improved effectiveness and cost savings when using adapted uncertainty estimation methods

### Open Question 2
- Question: How do more sophisticated active learning methods, such as alternating between selection types or using generative pseudo-labeling, perform in the context of fine-tuning PLM rankers?
- Basis in paper: [explicit] The authors mention that more sophisticated AL methods exist, including alternating between selection types like in AcTune and Augmented SBERT
- Why unresolved: The paper only considers common baseline active learning methods and does not explore these more advanced approaches
- What evidence would resolve it: Comparative studies showing the effectiveness and cost-efficiency of these advanced AL methods

### Open Question 3
- Question: What is the impact of annotation quality and reliability on the effectiveness of active learning strategies for PLM ranker fine-tuning?
- Basis in paper: [inferred] The paper models the annotation process by assuming that the first relevant passage found is annotated
- Why unresolved: The paper does not explicitly investigate the relationship between annotation quality and the performance of AL strategies
- What evidence would resolve it: Experimental results comparing the effectiveness of AL strategies under different annotation quality settings

## Limitations
- Only examines three specific AL strategies and three PLM ranker architectures, limiting generalizability
- Based on two specific datasets that may not capture all scenarios where PLM rankers are applied
- Annotation cost model assumes human assessors evaluate query-passage pairs, which may vary across applications

## Confidence

- High confidence: The observation that none of the tested AL strategies consistently outperform random selection across different training sizes and PLM architectures
- Medium confidence: The finding that AL strategies generally require more assessments (and thus higher annotation costs) than random selection to achieve similar effectiveness
- Medium confidence: The conclusion that variability exists between different randomly selected training subsets of the same size, but current AL strategies cannot consistently identify high-yield subsets

## Next Checks
1. Test additional AL strategies beyond uncertainty, QBC, and diversity-based approaches (e.g., meta-learning based, reinforcement learning based) to determine if other methods can overcome the limitations identified
2. Evaluate on additional datasets from different domains and with different query distributions to assess generalizability of the findings
3. Conduct ablation studies to isolate which components of the PLM rankers (pre-training data, architecture choices, fine-tuning procedures) contribute most to the observed variability in effectiveness between training subsets