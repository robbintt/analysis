---
ver: rpa2
title: Sentence-level Prompts Benefit Composed Image Retrieval
arxiv_id: '2310.05473'
source_url: https://arxiv.org/abs/2310.05473
tags:
- image
- prompt
- relative
- retrieval
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles composed image retrieval (CIR), where the goal
  is to retrieve an image that matches a reference image and a relative caption. Existing
  CIR methods often struggle with complex changes like object removal or attribute
  modification.
---

# Sentence-level Prompts Benefit Composed Image Retrieval

## Quick Facts
- arXiv ID: 2310.05473
- Source URL: https://arxiv.org/abs/2310.05473
- Authors: [Not specified in source]
- Reference count: 40
- Key outcome: Sentence-level prompts outperform pseudo-word embeddings in composed image retrieval, achieving SOTA results on Fashion-IQ and CIRR datasets

## Executive Summary
This paper addresses composed image retrieval (CIR) where the goal is to retrieve images matching both a reference image and a relative caption describing desired changes. The authors propose replacing pseudo-word embeddings with sentence-level prompts generated by a Q-Former network conditioned on both the reference image and relative caption. Their approach uses two loss terms—image-text contrastive loss and text prompt alignment loss—to guide the learning of appropriate sentence-level prompts. Experiments show significant improvements over state-of-the-art methods, with R@10 improvements of 4.3% on Fashion-IQ and 11.2% on CIRR.

## Method Summary
The method generates sentence-level prompts using a Q-Former network that takes both the reference image and relative caption as input. The generated prompt is concatenated with the relative caption to form a text query for image retrieval. Training uses two loss terms: image-text contrastive loss to align the augmented query with target images, and text prompt alignment loss to ensure the generated prompt matches an optimization-based auxiliary prompt. The approach is implemented using BLIP-2 with ViT-L visual encoder and is evaluated on Fashion-IQ and CIRR datasets using Recall@K metrics.

## Key Results
- Achieves SOTA performance on Fashion-IQ dataset with R@10 improvement of 4.3% and R@50 improvement of 4.4% over second-best method
- Shows R@10 improvement of 11.2% and R@50 improvement of 4.3% on CIRR dataset
- Demonstrates that sentence-level prompts outperform pseudo-word embeddings for complex CIR tasks involving object removal and attribute modification
- Ablation studies show γ=0.8 provides optimal balance between contrastive and alignment losses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sentence-level prompts outperform pseudo-word embeddings because they capture richer expressiveness and are conditioned on both reference image and relative caption
- Mechanism: Q-Former takes reference image feature and relative caption as input, generating a sentence-level prompt that explicitly describes the specific elements in the reference image relevant to the relative caption, removing irrelevant objects/attributes
- Core assumption: The relative caption describes changes to specific objects or attributes in the reference image; pseudo-word embeddings cannot disambiguate multiple objects
- Evidence anchors:
  - [abstract]: "the pseudo-word token is an overall textual representation of the objects in the reference image, and the length of the pseudo-word token is small... the above methods generally fail to decouple these objects in the pseudo-word tokens"
  - [section 1]: "Instead of relying on pseudo-word-based prompts, we propose to leverage pretrained V-L models, e.g., BLIP-2, to generate sentence-level prompts. By concatenating the learned sentence-level prompt with the relative caption, one can readily use existing text-based image retrieval models to enhance CIR performance."
  - [corpus]: Weak; no direct comparison in corpus papers, but many recent CIR works still use pseudo-word or late fusion approaches, suggesting sentence-level prompting is novel
- Break condition: If the relative caption is ambiguous or refers to abstract properties (e.g., view angle) not well captured by current V-L models

### Mechanism 2
- Claim: Dual loss terms (image-text contrastive + text prompt alignment) ensure both semantic correctness and prompt quality
- Mechanism: Image-text contrastive loss aligns augmented text query (sentence-level prompt + relative caption) with target image; text prompt alignment loss enforces generated prompt to match an optimization-based auxiliary prompt
- Core assumption: Both retrieval alignment and prompt generation quality are necessary; one loss alone is insufficient
- Evidence anchors:
  - [abstract]: "we introduce both image-text contrastive loss and text prompt alignment loss to enforce the learning of suitable sentence-level prompts"
  - [section 3]: "Two loss terms are introduced to generate proper sentence-level prompt as well as boost CIR performance. The first loss term is image-text contrastive loss... Second, a text prompt alignment loss is further introduced to directly guide the learning of sentence-level prompt generation."
  - [corpus]: No direct evidence in corpus; method is novel combination
- Break condition: If the auxiliary prompt optimization becomes unstable or the EMA of fζ causes gradient issues

### Mechanism 3
- Claim: Dynamic (sample-adaptive) sentence-level prompts outperform static prompt tokens
- Mechanism: Each query generates its own prompt conditioned on that reference image and relative caption, allowing fine-grained control over which objects/attributes to keep/remove
- Core assumption: The changes described in relative caption are specific to the reference image context; a fixed prompt cannot adapt
- Evidence anchors:
  - [abstract]: "In contrast to pseudo-word embedding... the sentence-level prompt has a richer expressivity and should depend on both the reference image and relative caption"
  - [section 1]: "different from pseudo-word embedding, the sentence-level prompt generation module takes both reference image and relative caption as the input"
  - [corpus]: BLIP4CIR+Bi (Liu et al., 2023a) uses static learnable tokens; our method explicitly contrasts this and claims improvements
- Break condition: If the prompt generation network overfits to training data and fails to generalize to new object types

## Foundational Learning

- Concept: Vision-Language Pre-training (V-L PT)
  - Why needed here: BLIP-2 backbone provides strong cross-modal embeddings; Q-Former bridges image and text modalities
  - Quick check question: What is the role of Q-Former in BLIP-2, and why is it suitable for prompt generation?

- Concept: Text Prompt Tuning
  - Why needed here: We learn a dynamic prompt conditioned on each query rather than static class tokens
  - Quick check question: How does dynamic prompt tuning differ from static prompt tuning in vision-language models?

- Concept: Contrastive Learning
  - Why needed here: Image-text contrastive loss aligns query representation with target image representation
  - Quick check question: What is the objective of image-text contrastive loss in this context?

## Architecture Onboarding

- Component map: Reference image (Ir) + Relative caption (t) -> Q-Former + MLP -> Sentence-level prompt (p) -> Concatenate(p, t) -> eQ -> fζ(eQ) vs fθ(It) via contrastive loss

- Critical path:
  1. Encode Ir → feature
  2. Q-Former + MLP → p
  3. Concatenate p + t → eQ
  4. Encode eQ → u
  5. Encode It → v
  6. Compute contrastive loss + alignment loss
  7. Backpropagate to Q-Former and MLP

- Design tradeoffs:
  - Prompt length 32 vs shorter: longer prompts give more expressivity but increase compute
  - γ = 0.8 vs higher: more alignment loss risks losing visual grounding
  - EMA branch vs no EMA: smoother training but slower adaptation

- Failure signatures:
  - Low recall on R@10/50: prompt generation not aligned with target images
  - Gradient explosion in Q-Former: too strong alignment loss or unstable optimization branch
  - Retrieval dominated by text only: prompt losing visual grounding

- First 3 experiments:
  1. Verify Q-Former can generate meaningful prompts by visualizing top-5 nearest captions in embedding space
  2. Ablation: remove alignment loss (γ=0) and measure drop in recall
  3. Test prompt length sensitivity: 4, 8, 16, 32 tokens and plot recall vs compute

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can vision-language models be improved to better handle changes in object view angles in composed image retrieval?
- Basis in paper: [explicit] The authors note that their method struggles with cases where the relative caption requires modifying the view angle of objects in the reference image, and attribute this limitation to the lack of angle-related knowledge in pre-trained models like BLIP-2.
- Why unresolved: The paper does not propose a solution for this limitation, and the authors acknowledge that injecting knowledge about angle transformations into the CIR model could be a potential direction for future work.
- What evidence would resolve it: Developing and evaluating a method that incorporates angle-related knowledge into the CIR model, and demonstrating improved performance on cases involving view angle changes compared to the current approach.

### Open Question 2
- Question: What is the optimal balance between the image-text contrastive loss and the text prompt alignment loss for training the sentence-level prompt generation module?
- Basis in paper: [explicit] The authors use both the image-text contrastive loss and the text prompt alignment loss to train their model, with a weighting factor γ for the alignment loss. They conduct experiments with different values of γ and find that γ = 0.8 yields the best performance on the Fashion-IQ dataset.
- Why unresolved: While the authors identify the best γ value for their specific experiments, it is unclear whether this value is optimal for all CIR tasks or datasets. The optimal balance between the two loss terms may vary depending on the specific characteristics of the dataset and the complexity of the CIR task.
- What evidence would resolve it: Conducting extensive experiments with different values of γ on various CIR datasets and tasks to determine the optimal balance between the image-text contrastive loss and the text prompt alignment loss for different scenarios.

### Open Question 3
- Question: How does the length of the sentence-level prompt affect the performance of the proposed method, and is there a point of diminishing returns?
- Basis in paper: [explicit] The authors investigate the effect of prompt length on their method's performance and find that increasing the prompt length generally improves performance, but also introduces higher computational complexity. They choose a prompt length of 32 as a trade-off between performance and efficiency.
- Why unresolved: While the authors identify the best prompt length for their specific experiments, it is unclear whether this length is optimal for all CIR tasks or datasets. Additionally, the relationship between prompt length and performance may not be linear, and there may be a point of diminishing returns where further increases in prompt length do not lead to significant performance gains.
- What evidence would resolve it: Conducting extensive experiments with different prompt lengths on various CIR datasets and tasks to determine the optimal prompt length for different scenarios and identify any points of diminishing returns.

## Limitations

- Limited to fashion domain datasets (Fashion-IQ and CIRR), leaving generalization to other CIR scenarios uncertain
- No direct quantitative comparison against specific pseudo-word methods like BLIP4CIR+Bi
- Struggles with view angle changes, highlighting limitations in current V-L model knowledge

## Confidence

- High confidence: The experimental results showing SOTA performance on both Fashion-IQ and CIRR datasets are well-documented and reproducible
- Medium confidence: The mechanism claims about sentence-level prompts capturing richer expressiveness than pseudo-word embeddings are logically sound but rely on indirect comparisons
- Medium confidence: The claim that dynamic sample-adaptive prompts outperform static tokens is supported by contrasting with existing methods but lacks extensive ablation studies

## Next Checks

1. **Direct pseudo-word comparison**: Implement and compare against Liu et al.'s BLIP4CIR+Bi method with static tokens using identical BLIP-2 backbone to provide head-to-head quantitative validation of the sentence-level advantage

2. **Cross-domain generalization**: Test the model on non-fashion CIR datasets or synthetic CIR tasks to verify that sentence-level prompts generalize beyond the fashion domain where the method shows strong performance

3. **Prompt visualization and interpretability**: Generate and visualize the actual sentence-level prompts for sample queries to qualitatively assess whether they capture the intended modifications and remove irrelevant objects as claimed in Mechanism 1