---
ver: rpa2
title: Structure Learning with Adaptive Random Neighborhood Informed MCMC
arxiv_id: '2311.00599'
source_url: https://arxiv.org/abs/2311.00599
tags:
- mcmc
- parni-dag
- proposal
- learning
- bayesian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel Markov chain Monte Carlo (MCMC) sampler,
  PARNI-DAG, for efficient sampling from the posterior distribution over directed
  acyclic graphs (DAGs) in structure learning problems. PARNI-DAG uses an adaptive
  random neighborhood informed proposal that leverages posterior edge probabilities
  to construct neighborhoods, along with a point-wise implementation and warm-start
  procedure using a skeleton graph.
---

# Structure Learning with Adaptive Random Neighborhood Informed MCMC

## Quick Facts
- arXiv ID: 2311.00599
- Source URL: https://arxiv.org/abs/2311.00599
- Reference count: 12
- Primary result: PARNI-DAG achieves faster convergence and better mixing than existing MCMC methods for structure learning, particularly in high-dimensional settings

## Executive Summary
This paper introduces PARNI-DAG, a novel Markov chain Monte Carlo sampler for Bayesian structure learning that uses an adaptive random neighborhood informed proposal. The algorithm leverages posterior edge probabilities to construct neighborhoods and employs a point-wise implementation with warm-starting to improve computational efficiency. Experiments demonstrate that PARNI-DAG outperforms existing methods like ADR and Order MCMC in terms of mixing properties, convergence speed, and structure learning accuracy on both synthetic and real-world datasets.

## Method Summary
PARNI-DAG implements a Markov chain Monte Carlo sampler for directed acyclic graphs that uses posterior edge probabilities to adaptively construct neighborhoods for proposal generation. The method employs a point-wise implementation that reduces computational complexity from O(2^d) to O(2d) by sequentially proposing DAGs through smaller sub-neighborhoods rather than full enumeration. A warm-start procedure initializes adaptive parameters using skeleton graphs from PC or GES algorithms, providing better starting points for the chain. The algorithm maintains π-reversibility through Metropolis-Hastings acceptance while adaptively updating proposal parameters during sampling.

## Key Results
- PARNI-DAG shows faster convergence and better mixing compared to ADR and Order MCMC, particularly in high-dimensional settings
- On real-world datasets, PARNI-DAG achieves superior DAG learning accuracy as measured by structural Hamming distance
- The point-wise implementation reduces computational complexity while maintaining exploration effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PARNI-DAG uses posterior edge probabilities to adaptively construct neighborhoods, improving mixing efficiency.
- Mechanism: By leveraging posterior edge probabilities (PEPs) to define the conditional distribution p(k|γ), the algorithm focuses computational effort on flipping edges that are more likely to be important, thereby avoiding wasteful proposals in low-probability regions.
- Core assumption: PEPs provide a meaningful signal about which edges to prioritize in the neighborhood construction.
- Evidence anchors:
  - [abstract]: "PARNI-DAG performs efficient sampling of DAGs via locally informed, adaptive random neighborhood proposal that results in better mixing properties."
  - [section]: "The PARNI proposal falls into the class of random neighborhood informed proposals, which are characterized by two steps: i) random sampling of a neighborhood N, and then ii) proposal of a new DAG within this neighborhood N according to a informed proposal [Zanella, 2020]."
  - [corpus]: No direct evidence; related work focuses on sampling in other domains, not DAG structure learning.
- Break condition: If PEPs are poorly estimated (e.g., due to limited data), the adaptive neighborhood may focus on irrelevant edges, degrading mixing.

### Mechanism 2
- Claim: The point-wise implementation reduces computational complexity from O(2^d) to O(2d), enabling scalability.
- Mechanism: Instead of enumerating all possible DAGs in a neighborhood, PARNI-DAG constructs a sequence of smaller neighborhoods and proposes new DAGs via a series of sub-proposals, each evaluating only a small subset of the full neighborhood.
- Core assumption: The sequence of sub-proposals can still explore the neighborhood effectively without full enumeration.
- Evidence anchors:
  - [section]: "The point-wise implementation of the algorithm, which dramatically reduces the computational complexity from O(2dk) to O(2dk)."
  - [section]: "Instead of working with the full neighborhood N(γ, k), we construct a sequence of smaller neighborhoods {N(γ(r), Kr)}R r=1 ⊂ N(γ, k), and a new DAG γ′ drawn from these neighborhoods {N(γ(r), Kr)}R r=1 is sequentially proposed according to the sub-proposals qg,Kr(γ(r − 1), ·) at each time r."
  - [corpus]: No direct evidence; related works focus on different MCMC methods, not point-wise implementations.
- Break condition: If the sequence of sub-proposals fails to explore the neighborhood adequately, the algorithm may miss important DAGs.

### Mechanism 3
- Claim: Warm-starting the adaptive parameters using a skeleton graph improves initial mixing and convergence.
- Mechanism: Before running the main MCMC, PARNI-DAG approximates PEPs using a skeleton graph (e.g., from PC or GES algorithms) and uses these approximations to initialize the adaptive parameters η, providing a better starting point for the chain.
- Core assumption: The skeleton graph provides a reasonable approximation of the true edge posterior probabilities.
- Evidence anchors:
  - [section]: "We argue that the iterative procedure used to restrict the initial search space in Kuipers et al. [2022] can be particularly useful in our case to efficiently approximate the PEPs before running the chain."
  - [section]: "Our solution then is to approximate the PEPs ˜πij before actually running the PARNI-DAG’s MCMC, and use this approximation to warm-start the chain."
  - [corpus]: No direct evidence; related works focus on different aspects of MCMC, not warm-starting via skeleton graphs.
- Break condition: If the skeleton graph is too sparse or dense, the warm-start may mislead the algorithm, causing poor initial mixing.

## Foundational Learning

- Concept: Bayesian Network Factorization
  - Why needed here: Understanding how BNs factorize joint distributions into conditional probabilities is crucial for grasping the structure learning problem.
  - Quick check question: In a BN with nodes X1, X2, X3, how would you express p(X1, X2, X3) in terms of conditional probabilities?
- Concept: Directed Acyclic Graph (DAG) Representation
  - Why needed here: The paper operates in the space of DAGs, so understanding their structure and acyclicity constraints is fundamental.
  - Quick check question: Why must a BN’s underlying graph be acyclic, and what would happen if cycles were allowed?
- Concept: Markov Chain Monte Carlo (MCMC) Sampling
  - Why needed here: PARNI-DAG is an MCMC sampler, so understanding MH acceptance, proposal kernels, and convergence is essential.
  - Quick check question: In MH sampling, what condition must the proposal kernel satisfy to ensure the chain converges to the target distribution?

## Architecture Onboarding

- Component map:
  Neighborhood construction -> Point-wise proposal -> MH acceptance -> Adaptive parameter update
- Critical path:
  1. Preprocess data and compute skeleton graph
  2. Approximate PEPs using skeleton graph
  3. Initialize adaptive parameters η with PEPs
  4. For each iteration:
     - Sample neighborhood indicator k
     - Construct sequence of sub-neighborhoods
     - Propose new DAG via sub-proposals
     - Accept/reject using MH criterion
- Design tradeoffs:
  - Adaptive neighborhood vs. random walk: Better mixing but higher computational cost per iteration
  - Point-wise implementation vs. full enumeration: Scalable but may miss some DAGs
  - Warm-start vs. cold start: Faster initial convergence but depends on quality of skeleton graph
- Failure signatures:
  - Poor mixing: Trace plots show slow exploration or getting stuck in local modes
  - High computational cost: Neighborhood thinning parameter ω is too high, leading to excessive likelihood evaluations
  - Incorrect PEPs: Skeleton graph is too sparse/dense, leading to poor warm-start
- First 3 experiments:
  1. Run PARNI-DAG on small synthetic DAG with known structure to verify convergence
  2. Compare mixing efficiency (trace plots, ESS) against ADR on protein dataset
  3. Test scalability on larger synthetic graphs with varying numbers of nodes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the theoretical mixing time bounds for the PARNI-DAG proposal in Bayesian structure learning, and how do they compare to existing bounds for random walk proposals?
- Basis in paper: [explicit] The paper mentions that "similar results for the class of locally informed proposals are relatively under-developed" and discusses the need to study "PARNI's theoretical mixing time bounds" as future research.
- Why unresolved: The paper acknowledges that while mixing time bounds exist for random walk proposals, analogous results for locally informed proposals like PARNI-DAG have not been formally proven, particularly in the context of Bayesian structure learning.
- What evidence would resolve it: A formal mathematical proof establishing the mixing time bounds for PARNI-DAG and comparing them to existing bounds for random walk proposals in structure learning contexts.

### Open Question 2
- Question: How does the performance of PARNI-DAG change when applied to non-linear causal discovery models, such as Additive Noise Models (ANMs), where closed-form marginal likelihoods do not exist?
- Basis in paper: [inferred] The paper discusses the current limitation of PARNI-DAG being constrained to linear models and mentions that recent work has explored extending PARNI to non-linear models, suggesting this as a potential direction for improvement.
- Why unresolved: The paper explicitly states that PARNI-DAG is currently limited to linear functional models and suggests extending it to non-linear models as future work, indicating this has not been explored yet.
- What evidence would resolve it: Empirical results demonstrating the performance of PARNI-DAG on non-linear causal discovery models, comparing it to other methods designed for such scenarios.

### Open Question 3
- Question: How does the choice of the "neighborhood thinning" parameter ω affect the trade-off between computational efficiency and mixing performance in PARNI-DAG?
- Basis in paper: [explicit] The paper introduces an adaptive scheme for updating ω on the fly, mentioning that varying ω can control the number of posterior DAG probability evaluations, but notes that they chose a specific value based on empirical results.
- Why unresolved: While the paper describes the adaptive scheme for updating ω and provides a chosen value based on empirical findings, it does not explore the full range of possible values or provide theoretical guidance on optimal selection.
- What evidence would resolve it: A systematic study varying ω across a range of values, analyzing the resulting trade-offs between computational efficiency and mixing performance, potentially accompanied by theoretical insights into optimal selection strategies.

## Limitations

- The paper's computational efficiency claims are based on theoretical analysis but lack empirical wall-clock time comparisons with baseline methods
- The adaptive neighborhood construction relies heavily on accurate posterior edge probability estimation, which may be challenging in high-dimensional settings with limited data
- The warm-start procedure's effectiveness depends on the quality of skeleton graphs from PC or GES algorithms, which themselves have limitations in certain data scenarios

## Confidence

- **High confidence**: The mechanism of using posterior edge probabilities for neighborhood construction is well-founded theoretically, with clear mathematical derivation of the proposal kernel
- **Medium confidence**: The claim about improved mixing and convergence is supported by trace plots and ESS comparisons, but lacks comprehensive statistical tests across multiple random seeds
- **Low confidence**: The computational complexity claims (O(2d) vs O(2^d)) are theoretically sound but not empirically validated with runtime measurements

## Next Checks

1. **Runtime validation**: Implement PARNI-DAG and baseline methods to measure actual wall-clock time per iteration across different problem sizes, verifying the claimed computational efficiency gains
2. **Robustness testing**: Run multiple chains with different random seeds on each dataset to assess the consistency of mixing improvements and compute confidence intervals for SHD and MSE metrics
3. **Hyperparameter sensitivity**: Systematically vary the thinning parameter ω and neighborhood size k across a grid to understand their impact on convergence speed and mixing quality