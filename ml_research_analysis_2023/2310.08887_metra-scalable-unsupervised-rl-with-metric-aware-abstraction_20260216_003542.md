---
ver: rpa2
title: 'METRA: Scalable Unsupervised RL with Metric-Aware Abstraction'
arxiv_id: '2310.08887'
source_url: https://arxiv.org/abs/2310.08887
tags:
- learning
- metra
- state
- unsupervised
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces METRA, a scalable unsupervised RL method
  that learns diverse behaviors by covering a compact latent space connected to the
  state space via temporal distances. Unlike prior approaches that aim to cover the
  entire state space or rely on mutual information, METRA learns a compact representation
  and skill policy that together maximize coverage in terms of temporal distances,
  enabling it to scale to high-dimensional environments.
---

# METRA: Scalable Unsupervised RL with Metric-Aware Abstraction

## Quick Facts
- arXiv ID: 2310.08887
- Source URL: https://arxiv.org/abs/2310.08887
- Authors: 
- Reference count: 40
- One-line primary result: METRA achieves superior state coverage and downstream task performance compared to skill discovery, exploration, and goal-reaching baselines on five locomotion and manipulation tasks, including pixel-based Quadruped and Humanoid.

## Executive Summary
METRA introduces a scalable unsupervised RL method that learns diverse behaviors by covering a compact latent space connected to the state space via temporal distances. Unlike prior approaches that aim to cover the entire state space or rely on mutual information, METRA learns a compact representation and skill policy that together maximize coverage in terms of temporal distances, enabling it to scale to high-dimensional environments. Experiments on five locomotion and manipulation tasks show METRA discovers diverse, useful behaviors and outperforms previous unsupervised RL methods, including on pixel-based Quadruped and Humanoid—the first to do so.

## Method Summary
METRA is an unsupervised RL method that learns a compact latent representation of states connected via temporal distances, and a skill policy that maximizes coverage in this latent space. The method uses a Wasserstein dependency measure to encourage exploration of temporally spread-out manifolds, and enforces temporal distance preservation through a Lipschitz constraint. The core components are a skill policy π(a|s,z), representation function ϕ(s), and score function f(s,z), trained with dual gradient descent to maximize the Wasserstein dependency measure while preserving temporal distances in the latent space.

## Key Results
- METRA achieves superior state coverage compared to skill discovery, exploration, and goal-reaching baselines on five benchmark tasks
- METRA is the first unsupervised RL method to succeed on pixel-based Quadruped and Humanoid environments
- METRA discovers diverse, useful behaviors and achieves better downstream task performance than prior unsupervised RL methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal distance metric enables scalable unsupervised skill discovery in high-dimensional spaces
- Mechanism: METRA uses temporal distances between states as a metric for the latent space Z, which is invariant to state representation and thus scalable to pixel-based environments. This allows the method to learn diverse behaviors by maximizing coverage in a compact latent space rather than attempting to cover the entire high-dimensional state space.
- Core assumption: Temporal distances provide a meaningful measure of state dissimilarity that is invariant to state representation
- Evidence anchors:
  - [abstract]: "METRA learns a compact representation and skill policy that together maximize coverage in terms of temporal distances, enabling it to scale to high-dimensional environments."
  - [section]: "Our second main idea is therefore to use temporal distances (i.e., the number of minimum environment steps between two states) as a metric for the latent space."
  - [corpus]: Weak - the corpus doesn't contain papers that directly address temporal distance metrics for unsupervised RL
- Break condition: If temporal distances do not capture meaningful behavioral differences, or if the MDP has highly asymmetric transitions that make temporal distances less informative

### Mechanism 2
- Claim: Wasserstein dependency measure encourages exploration of temporally spread-out manifolds
- Mechanism: By using the Wasserstein dependency measure (WDM) between states and skills, METRA encourages learning behaviors that maximize temporal distances in the latent space. This forces the learned skills to span the "longest" subspaces of the state space, analogous to temporal PCA.
- Core assumption: Maximizing temporal distances in the latent space leads to diverse and useful behaviors in the original state space
- Evidence anchors:
  - [abstract]: "By learning to move in every direction in the latent space, METRA obtains a tractable set of diverse behaviors that approximately cover the state space"
  - [section]: "As a result, by maximizing coverage in the compact latent space, we can acquire diverse behaviors that approximately cover the entire state space"
  - [corpus]: Assumption: The corpus contains related work on unsupervised skill discovery but none specifically use Wasserstein measures with temporal distances
- Break condition: If the latent space capacity is insufficient to represent the diversity of behaviors needed, or if the temporal distance metric fails to capture the most important state distinctions

### Mechanism 3
- Claim: Constrained optimization with Lipschitz constraints ensures temporal distance preservation
- Mechanism: METRA enforces that the representation function ϕ preserves temporal distances through a Lipschitz constraint: ∥ϕ(s) - ϕ(s')∥₂ ≤ 1 for all adjacent state pairs. This ensures the learned latent space maintains temporal structure while being compact.
- Core assumption: The temporal distance constraint can be effectively enforced through a Lipschitz constraint in the learned representation
- Evidence anchors:
  - [section]: "s.t. ∥ϕ(s) - ϕ(s')∥₂ ≤ 1, ∀(s, s') ∈ Sadj, (where Sadj denotes the set of adjacent state pairs in the MDP)"
  - [section]: "Note that ∥ϕ∥L ≤ 1 is equivalently converted into ∥ϕ(s) - ϕ(s')∥₂ ≤ 1 under the temporal distance metric"
  - [corpus]: Weak - the corpus doesn't contain papers that directly address Lipschitz constraints for temporal distance preservation in RL
- Break condition: If the Lipschitz constraint is too restrictive and prevents learning useful representations, or if the relaxation constant ε is poorly chosen

## Foundational Learning

- Concept: Mutual Information vs. Wasserstein Dependency Measure
  - Why needed here: Understanding the difference between MI and WDM is crucial for grasping why METRA works when traditional skill discovery methods fail
  - Quick check question: Why does maximizing mutual information between states and skills often lead to limited state coverage?

- Concept: Temporal Distance as a Metric
  - Why needed here: Temporal distance is the core metric that makes METRA scalable to pixel-based environments
  - Quick check question: How does temporal distance differ from Euclidean distance in terms of invariance to state representation?

- Concept: Wasserstein Distance and Kantorovich-Rubinstein Duality
  - Why needed here: The tractable optimization of WDM relies on the Kantorovich-Rubenstein duality
  - Quick check question: What is the key insight from the Kantorovich-Rubenstein duality that makes WDM optimization tractable?

## Architecture Onboarding

- Component map:
  - Skill Policy π(a|s,z) -> Representation Function ϕ(s) -> Score Function f(s,z)
  - Lagrange Multiplier λ enforces temporal distance constraint
  - Replay Buffer D stores experience tuples for training

- Critical path:
  1. Sample skill z from prior p(z)
  2. Roll out trajectory with π(a|s,z)
  3. Update ϕ to maximize (ϕ(s') - ϕ(s))⊤z under temporal distance constraint
  4. Update π using SAC with reward r(s,z,s') = (ϕ(s') - ϕ(s))⊤z
  5. Update λ to enforce constraint

- Design tradeoffs:
  - Compact latent space vs. expressiveness: Smaller Z is more scalable but may limit behavioral diversity
  - Continuous vs. discrete skills: Continuous skills enable smoother exploration but may require more careful tuning
  - Full WDM vs. simplified objective: Full WDM is more expressive but computationally expensive

- Failure signatures:
  - Skills collapse to similar behaviors: Likely constraint λ is too weak or representation ϕ is not learning temporal structure
  - No meaningful behaviors discovered: Likely the latent space capacity is insufficient or the temporal distance metric is not capturing important state distinctions
  - Poor performance on downstream tasks: Likely the learned behaviors don't align with task-relevant state manifolds

- First 3 experiments:
  1. Verify temporal distance preservation: Check if ∥ϕ(s) - ϕ(s')∥₂ ≤ dtemp(s,s') holds for adjacent states
  2. Visualize latent space: Plot ϕ(s) trajectories to confirm the representation captures meaningful state structure
  3. Test zero-shot goal reaching: Use the procedure in Section 4.2 to reach specific goals and verify it works

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of METRA scale with increasing state space dimensionality beyond the tested environments?
- Basis in paper: [inferred] The paper mentions that pure exploration methods might struggle in complex environments with large state spaces where covering every possible transition is infeasible. METRA is proposed as a scalable solution, but the experiments only test up to 64x64x3 pixel-based environments.
- Why unresolved: The paper does not test METRA on environments with higher dimensional state spaces than those used in the experiments. It is unclear if the method would maintain its performance advantage as dimensionality increases significantly.
- What evidence would resolve it: Additional experiments testing METRA on environments with progressively higher dimensional state spaces (e.g., 128x128x3, 256x256x3, etc.) and comparing its performance to baselines would provide evidence of how well it scales.

### Open Question 2
- Question: How sensitive is METRA to the choice of the temporal distance metric, and would alternative metrics like graph-based distances improve performance?
- Basis in paper: [explicit] The paper proposes using temporal distances as the metric for the latent space and discusses connections to graph Laplacian-based methods, but does not explore alternative metrics.
- Why unresolved: The paper only tests METRA with the temporal distance metric and does not investigate how sensitive the method is to this choice or whether other metrics could yield better results.
- What evidence would resolve it: Experiments testing METRA with different distance metrics (e.g., graph-based distances, state-based metrics in structured environments) and comparing the results would reveal the sensitivity to the choice of metric.

### Open Question 3
- Question: How does the performance of METRA compare to supervised RL methods when a reward function is available?
- Basis in paper: [inferred] The paper focuses on unsupervised RL and demonstrates that METRA can discover diverse behaviors without supervision. However, it does not compare its performance to supervised RL methods when rewards are provided.
- Why unresolved: The paper does not include a comparison between METRA and supervised RL methods in a setting where rewards are available, so it is unclear how much performance is sacrificed by using an unsupervised approach.
- What evidence would resolve it: Experiments comparing the performance of METRA (fine-tuned with rewards) to directly training an RL agent with rewards from scratch would reveal the trade-off between unsupervised pre-training and supervised learning.

## Limitations
- The method relies heavily on the assumption that temporal distances capture meaningful behavioral differences across all state representations, which may not hold in environments with complex or asymmetric transition dynamics
- The scalability claims are demonstrated on a limited set of environments, and performance in more diverse or complex domains remains to be seen
- The paper does not provide extensive ablation studies on the choice of latent space dimensionality or the relaxation constant ε

## Confidence

- **High confidence**: METRA's superior state coverage and downstream task performance compared to baselines on the tested environments
- **Medium confidence**: The claim that temporal distance metrics enable scalability to pixel-based environments, as this relies on the assumption that temporal distances are truly invariant to state representation
- **Medium confidence**: The assertion that Wasserstein dependency measures provide better exploration than mutual information, as this is demonstrated empirically but not theoretically proven

## Next Checks

1. Test METRA's performance on environments with more complex transition dynamics or asymmetric MDPs to validate the temporal distance metric assumption
2. Conduct extensive ablation studies on latent space dimensionality and relaxation constant ε to understand their impact on performance and scalability
3. Compare METRA's learned representations against alternative metrics (e.g., Euclidean distance, graph-based distances) to isolate the contribution of the temporal distance metric to the method's success