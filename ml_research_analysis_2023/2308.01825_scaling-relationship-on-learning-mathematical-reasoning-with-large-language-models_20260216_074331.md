---
ver: rpa2
title: Scaling Relationship on Learning Mathematical Reasoning with Large Language
  Models
arxiv_id: '2308.01825'
source_url: https://arxiv.org/abs/2308.01825
tags:
- reasoning
- paths
- performance
- data
- pre-training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the scaling relationship between large
  language model (LLM) capacity and mathematical reasoning performance. The authors
  find that pre-training loss is a better indicator of model performance than parameter
  count, and that model performance has a log-linear relationship with supervised
  data amount.
---

# Scaling Relationship on Learning Mathematical Reasoning with Large Language Models

## Quick Facts
- arXiv ID: 2308.01825
- Source URL: https://arxiv.org/abs/2308.01825
- Authors: 
- Reference count: 33
- Key outcome: Pre-training loss is a better predictor of mathematical reasoning performance than parameter count, with log-linear scaling between supervised data and performance.

## Executive Summary
This paper investigates how large language model (LLM) capacity scales with mathematical reasoning performance. The authors find that pre-training loss is a more reliable indicator of reasoning ability than parameter count, and that supervised fine-tuning performance follows a log-linear relationship with the amount of supervised data. To address data limitations, they propose rejection sampling fine-tuning (RFT), which uses supervised models to generate correct reasoning paths as augmented training data. RFT significantly improves performance, particularly for smaller models, with LLaMA-7B achieving 49.3% accuracy on GSM8K when using combined rejection samples.

## Method Summary
The authors conduct experiments with LLaMA and LLaMA2 models of varying sizes (7B, 13B, 33B) on the GSM8K mathematical reasoning dataset. They first establish the relationship between pre-training loss and reasoning performance, then examine how supervised fine-tuning performance scales with data amount. For RFT, they use supervised models to generate 100 candidate reasoning paths per question, filter for correct ones, deduplicate by equation lists, and fine-tune on the augmented dataset. They also combine rejection samples from multiple models to further improve performance.

## Key Results
- Pre-training loss is negatively correlated with mathematical reasoning performance across model sizes
- Model performance improves logarithmically with supervised data amount
- Rejection sampling fine-tuning improves performance by providing diverse reasoning paths
- Combining rejection samples from multiple models yields the best performance (LLaMA-7B: 49.3% accuracy on GSM8K)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training loss is a better indicator of mathematical reasoning performance than parameter count.
- Mechanism: Lower pre-training loss correlates with better reasoning ability because it reflects more effective knowledge acquisition during pre-training.
- Core assumption: Pre-training loss captures the model's fundamental reasoning capabilities, independent of model size.
- Evidence anchors:
  - [abstract]: "We find that pre-training loss is a better indicator of the model's performance than the model's parameter count."
  - [section 3.1]: "We analyze the SFT and ICL performance of GPT-3, LLaMA, LLaMA2, and GPT-4. The pre-training losses of these models are observed in their paper...we find the pre-training loss is a stable performance indicator of the math reasoning ability."
  - [corpus]: Weak evidence - no direct citations about pre-training loss as performance indicator in related papers.

### Mechanism 2
- Claim: Supervised fine-tuning performance improves logarithmically with supervised data amount.
- Mechanism: Each doubling of supervised data yields diminishing returns in performance improvement, following a log-linear relationship.
- Core assumption: The relationship between data amount and performance follows a predictable mathematical pattern that can be extrapolated.
- Evidence anchors:
  - [abstract]: "We apply supervised fine-tuning (SFT) with different amounts of supervised data and empirically find a log-linear relation between data amount and model performance."
  - [section 3.2]: "We observe that the model performance has a log-linear relation versus data amount. When the data amount doubles, the performance increases by a unit."
  - [corpus]: Moderate evidence - related papers discuss scaling laws but focus on pre-training rather than SFT data scaling.

### Mechanism 3
- Claim: Rejection sampling fine-tuning (RFT) improves performance by providing diverse reasoning paths.
- Mechanism: Generating multiple reasoning paths per question and selecting correct ones creates a more diverse training dataset that improves generalization.
- Core assumption: Model performance improves with increased diversity of reasoning paths in training data, not just quantity of samples.
- Evidence anchors:
  - [abstract]: "RFT uses supervised models to generate and collect correct reasoning paths as augmented fine-tuning datasets. We find with augmented samples containing more distinct reasoning paths, RFT improves mathematical reasoning performance more for LLMs."
  - [section 3.3]: "We find the key factor influencing RFT performance is the distinct reasoning path amount which can be increased by sampling more times or combining samples from multiple models."
  - [corpus]: Strong evidence - related papers (Zhu et al., 2023; Ni et al., 2023) discuss similar rejection sampling approaches.

## Foundational Learning

- Concept: Pre-training loss as performance indicator
  - Why needed here: Understanding why pre-training loss predicts reasoning ability helps in model selection and architecture decisions.
  - Quick check question: Why might a smaller model with lower pre-training loss outperform a larger model with higher pre-training loss?

- Concept: Log-linear scaling relationships
  - Why needed here: The log-linear relationship between data amount and performance determines how much data is needed for desired improvements.
  - Quick check question: If doubling supervised data improves performance by 5%, how much improvement would quadrupling the data provide?

- Concept: Rejection sampling for data augmentation
  - Why needed here: RFT's effectiveness depends on understanding how diverse reasoning paths improve generalization.
  - Quick check question: Why does deduplicating reasoning paths based on equation lists improve RFT performance?

## Architecture Onboarding

- Component map: Pre-trained LLM -> SFT baseline -> Rejection sampling generation -> RFT training -> Performance evaluation

- Critical path: Pre-trained model → SFT baseline → Rejection sampling generation → RFT training → Performance evaluation

- Design tradeoffs:
  - Sampling temperature (0.7 used) vs. diversity vs. correctness of generated paths
  - Number of samples per question (k=100) vs. computational cost vs. path diversity
  - Deduplication strategy (equation list matching) vs. path variety vs. training efficiency

- Failure signatures:
  - RFT performs worse than SFT: likely overfitting on training paths or insufficient path diversity
  - No improvement with increased k: model has memorized training data or generation temperature too low
  - Large models show minimal RFT improvement: possible overfitting during pre-training on reasoning tasks

- First 3 experiments:
  1. Replicate SFT scaling experiment with different fractions of GSM8K to verify log-linear relationship
  2. Test RFT with k=1,3,6,12 to confirm diminishing returns pattern
  3. Combine rejection samples from two different models to verify performance boost from path diversity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the scaling relationships for mathematical reasoning differ from other reasoning tasks like logical inference or commonsense reasoning?
- Basis in paper: [explicit] The paper focuses specifically on mathematical reasoning but acknowledges this is one type of reasoning task.
- Why unresolved: The paper only examines mathematical reasoning performance. Other reasoning tasks may have different scaling relationships with model size, data amount, and pre-training loss.
- What evidence would resolve it: Experiments comparing scaling relationships across multiple reasoning tasks (math, logic, commonsense) using the same model families and datasets.

### Open Question 2
- Question: What is the optimal generation temperature for rejection sampling that balances reasoning path diversity and correctness across different model sizes?
- Basis in paper: [explicit] The paper uses a fixed temperature of 0.7 but notes this may not be optimal, especially for larger models.
- Why unresolved: The paper uses a fixed temperature without exploring the full temperature range or finding optimal temperatures for different model sizes.
- What evidence would resolve it: Systematic experiments varying temperature for each model size to find optimal settings that maximize distinct correct reasoning paths.

### Open Question 3
- Question: How does the mathematical reasoning performance scale when pre-training specifically on mathematical content rather than general text?
- Basis in paper: [explicit] The paper acknowledges this was not done and that pre-training losses from different domains may not be directly comparable.
- Why unresolved: The paper uses models pre-trained on general text corpora and does not explore math-specific pre-training.
- What evidence would resolve it: Training models from scratch on math-focused datasets and comparing their scaling relationships to general pre-trained models.

## Limitations

- Pre-training loss as performance predictor is highly sensitive to pre-training data distribution and tokenization schemes
- Study focuses primarily on LLaMA/LLaMA2 models with only one GPT model included, limiting generalizability
- GSM8K accuracy as the sole evaluation metric may not fully capture mathematical reasoning capabilities

## Confidence

**High confidence:**
- Pre-training loss correlates with reasoning performance across tested models
- Log-linear relationship between supervised data amount and model performance holds within tested range
- RFT improves performance by providing diverse reasoning paths when properly implemented

**Medium confidence:**
- Pre-training loss is a better indicator than parameter count for model selection
- Doubling supervised data consistently improves performance by a predictable unit
- Combining rejection samples from multiple models yields the best performance

**Low confidence:**
- The pre-training loss relationship extends to other mathematical reasoning tasks beyond GSM8K
- The log-linear scaling relationship holds for data amounts beyond those tested (full GSM8K)
- RFT performance improvements will scale proportionally with model size increases

## Next Checks

1. **Cross-dataset validation**: Test the pre-training loss performance correlation on multiple mathematical reasoning datasets (MATH, SVAMP, AIM) to verify if the relationship holds beyond GSM8K.

2. **Scaling relationship extrapolation**: Conduct experiments with supervised data amounts exceeding the full GSM8K dataset by combining multiple mathematical reasoning datasets to measure whether the log-linear relationship continues or breaks down.

3. **Alternative rejection sampling strategies**: Implement and compare RFT with different rejection sampling approaches (e.g., different temperature settings, beam search instead of sampling) to determine which components are most critical for performance gains.