---
ver: rpa2
title: 'Hyperparameter Adaptive Search for Surrogate Optimization: A Self-Adjusting
  Approach'
arxiv_id: '2310.07970'
source_url: https://arxiv.org/abs/2310.07970
tags:
- hyperparameter
- optimization
- hyperparameters
- hasso
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HASSO, a self-adjusting surrogate optimization
  (SO) approach that dynamically tunes hyperparameters during optimization without
  requiring additional evaluations. HASSO treats each hyperparameter as an arm in
  a multi-armed bandit problem, using Thompson sampling to adaptively update hyperparameter
  values based on their contribution to objective improvement.
---

# Hyperparameter Adaptive Search for Surrogate Optimization: A Self-Adjusting Approach

## Quick Facts
- arXiv ID: 2310.07970
- Source URL: https://arxiv.org/abs/2310.07970
- Authors: 
- Reference count: 1
- Key outcome: Introduces HASSO, a self-adjusting surrogate optimization approach that dynamically tunes hyperparameters using Thompson sampling, outperforming baseline methods on global optimization test problems while maintaining computational efficiency

## Executive Summary
This paper introduces HASSO (Hyperparameter Adaptive Search for Surrogate Optimization), a self-adjusting approach that dynamically tunes hyperparameters during optimization without requiring additional function evaluations. HASSO treats each hyperparameter as an arm in a multi-armed bandit problem, using Thompson sampling to adaptively update hyperparameter values based on their contribution to objective improvement. The method addresses the challenge of manual hyperparameter tuning in surrogate optimization algorithms, which significantly impacts their performance across different problems. Experiments on global optimization test problems demonstrate that HASSO outperforms baseline approaches including fixed configurations, random updates, rule-based updates, and grid search, achieving better convergence rates and optimization performance while maintaining computational efficiency comparable to other methods.

## Method Summary
HASSO integrates hyperparameter adaptation directly into the surrogate optimization loop by treating each hyperparameter as an independent arm in a multi-armed bandit framework. The algorithm uses Beta distributions initialized with α=β=1 for each hyperparameter, samples from these distributions in each iteration, selects the hyperparameter with the highest sampled value, updates it, and then updates the corresponding Beta distribution based on whether the update led to improvement in the primary objective. This approach allows HASSO to adaptively balance exploration and exploitation in the hyperparameter space while concurrently optimizing the primary objective function. The method is designed to work with any surrogate optimization algorithm and acquisition function, making it a generic self-adjusting SO framework that requires no additional function evaluations for hyperparameter tuning.

## Key Results
- HASSO achieves superior convergence rates and final objective values compared to baseline approaches including fixed hyperparameters, random updates, rule-based updates, and grid search
- The method demonstrates significant computational efficiency, requiring only negligible additional time compared to standard SO algorithms without hyperparameter adaptation
- HASSO shows particular effectiveness for problems requiring both exploration and exploitation, such as those with multiple local optima, outperforming baselines on challenging benchmark functions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Treating each hyperparameter as an independent arm in a multi-armed bandit problem allows parallel, adaptive tuning without global hyperparameter grid searches
- **Mechanism**: HASSO initializes a Beta distribution for each hyperparameter (α=β=1). In each iteration, it samples from these distributions, selects the hyperparameter with the highest sampled value, updates it, and then updates the corresponding Beta distribution based on whether the update led to improvement in the primary objective
- **Core assumption**: Hyperparameter improvements can be evaluated independently within the context of the primary optimization loop, and their contribution to objective improvement can be reliably measured without separate evaluations
- **Evidence anchors**:
  - [abstract]: "HASSO is not a hyperparameter tuning algorithm, but a generic self-adjusting SO algorithm that dynamically tunes its own hyperparameters while concurrently optimizing the primary objective function, without requiring additional evaluations"
  - [section]: "HASSO treats each hyperparameter as an arm with a predefined range of values and adapts them based on the probability of improving the primary objective value"
  - [corpus]: No direct evidence found for this specific mechanism; the claim is primarily supported by the paper's methodology description
- **Break condition**: If hyperparameter interactions are too strong, treating them independently could lead to suboptimal combinations being missed

### Mechanism 2
- **Claim**: Thompson sampling with Beta distributions provides an efficient balance between exploration and exploitation in hyperparameter space
- **Mechanism**: The Beta distribution's shape parameters (α for successes, β for failures) encode historical performance. Higher α/β ratios lead to more concentrated distributions favoring exploitation, while balanced ratios promote exploration. This naturally adapts the exploration-exploitation trade-off as optimization progresses
- **Core assumption**: The improvement/no-improvement binary signal provides sufficient information to guide hyperparameter selection, and the Beta distribution appropriately models this uncertainty
- **Evidence anchors**:
  - [section]: "HASSO incorporates a hyperparameter tuning step within an SO algorithm. It treats each hyperparameter of a given SO algorithm as a separate arm with an unknown reward distribution"
  - [section]: "Figure 2 demonstrates how the posterior beta distributions of two given hyperparameters in HASSO can change after 20 iterations"
  - [corpus]: No direct evidence found for this specific mechanism; the claim is primarily supported by the paper's methodology description
- **Break condition**: If objective improvements are noisy or if improvement signals are delayed, the Beta distribution may not accurately represent hyperparameter quality

### Mechanism 3
- **Claim**: The self-adjusting approach avoids the computational overhead of traditional grid or random search while maintaining or improving optimization performance
- **Mechanism**: Instead of evaluating multiple hyperparameter combinations at each iteration (as in grid search) or randomly sampling from the hyperparameter space (as in random search), HASSO selects only one hyperparameter to update per iteration based on Thompson sampling. This targeted approach reduces computational cost while focusing on promising hyperparameters
- **Core assumption**: Selecting one hyperparameter per iteration is sufficient to navigate the hyperparameter space effectively, and the sequential nature of updates doesn't miss important combinations
- **Evidence anchors**:
  - [abstract]: "HASSO achieves self-adjustment without exhaustively searching for hyperparameters at every step, resulting in significant computational resources and time savings"
  - [section]: "HASSO is not limited to surrogate model hyperparameters, making it versatile and adaptable to any SO algorithm"
  - [section]: "Incorporating HASSO into any regular SO algorithm results in only a negligible increase in computational time compared to the SO algorithm without HASSO (the SO baseline)"
- **Break condition**: If the hyperparameter space is highly interdependent and requires coordinated updates, the sequential single-parameter approach may be too slow to find optimal combinations

## Foundational Learning

- **Concept**: Thompson Sampling and Multi-Armed Bandit Problems
  - Why needed here: HASSO is fundamentally based on Thompson sampling applied to hyperparameter selection, so understanding this framework is essential for grasping the algorithm's design
  - Quick check question: How does Thompson sampling balance exploration and exploitation differently from epsilon-greedy or upper confidence bound strategies?

- **Concept**: Surrogate Optimization and Acquisition Functions
  - Why needed here: HASSO operates within surrogate optimization frameworks and must understand how different acquisition functions (EI, UCB, Wscore) interact with hyperparameter tuning
  - Quick check question: What are the key differences between expected improvement, upper confidence bound, and weighted score acquisition functions in terms of exploration-exploitation balance?

- **Concept**: Gaussian Process Kernels and Hyperparameter Tuning
  - Why needed here: The paper specifically discusses lengthscale and kernel function hyperparameters in GP models, which are common in surrogate optimization and critical for understanding the experimental setup
  - Quick check question: How do the lengthscale and amplitude parameters in a Gaussian process kernel affect the smoothness and generalization of the surrogate model?

## Architecture Onboarding

- **Component map**: Main optimization loop (Algorithm 1) -> Beta distribution managers -> Hyperparameter update mechanism -> Surrogate optimization algorithm (Algorithm 2) -> Acquisition function -> Candidate selection
- **Critical path**: (1) Sample from Beta distributions for all hyperparameters, (2) Select hyperparameter with maximum sample, (3) Update selected hyperparameter, (4) Run surrogate optimization with updated hyperparameter, (5) Evaluate improvement, (6) Update corresponding Beta distribution. This path repeats each iteration
- **Design tradeoffs**: HASSO trades off between computational efficiency (single hyperparameter update per iteration) and potentially missing optimal hyperparameter combinations that require coordinated updates. The Thompson sampling approach trades off between exploration (trying different hyperparameters) and exploitation (focusing on known good hyperparameters)
- **Failure signatures**: Poor performance on problems with strong hyperparameter interactions, slow convergence when optimal hyperparameters require coordinated changes, or sensitivity to initialization when the search space has multiple good regions
- **First 3 experiments**:
  1. Implement HASSO with a simple surrogate optimization algorithm (e.g., EI with fixed GP) on a 2D test function like Ackley, comparing to fixed hyperparameter settings
  2. Test HASSO's hyperparameter selection behavior on a problem with known hyperparameter importance (e.g., radius parameter in dynamic coordinate search)
  3. Compare HASSO against random and grid search baselines on a medium-dimensional problem (e.g., Rosenbrock-10d) to validate computational efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HASSO's performance scale with the number of hyperparameters beyond two, particularly in high-dimensional optimization problems?
- Basis in paper: [explicit] The paper states "Future research directions could explore the extension of HASSO to parallel optimization settings and investigate its performance in problems involving more than two hyperparameters"
- Why unresolved: The experimental results only demonstrate HASSO with two hyperparameters (lengthscale and radius), leaving scalability with additional hyperparameters untested
- What evidence would resolve it: Experimental results showing HASSO performance with 3+ hyperparameters across various dimensional problems and comparison to current performance with 2 hyperparameters

### Open Question 2
- Question: What is the theoretical convergence guarantee of HASSO compared to standard SO algorithms with fixed hyperparameters?
- Basis in paper: [inferred] While the paper demonstrates superior empirical performance, it does not provide theoretical analysis of convergence rates or regret bounds
- Why unresolved: The paper focuses on empirical evaluation without mathematical proof of convergence properties or regret bounds
- What evidence would resolve it: Formal theoretical analysis proving convergence rates, regret bounds, or other performance guarantees for HASSO compared to baseline SO methods

### Open Question 3
- Question: How does HASSO's hyperparameter adaptation mechanism perform in real-world optimization problems compared to synthetic test functions?
- Basis in paper: [explicit] The paper states "testing the proposed approach on real-world optimization problems would provide valuable insights into its practical applicability"
- Why unresolved: All experiments are conducted on synthetic test functions (Ackley, Rosenbrock, Rastrigin, Perm, Shubert), with no real-world problem evaluation
- What evidence would resolve it: Application of HASSO to real-world optimization problems (e.g., engineering design, drug discovery) with performance comparison to standard SO approaches

## Limitations

- Experimental validation is limited to synthetic benchmark functions and relatively low-dimensional problems, with no real-world optimization problem evaluation
- The method's performance on problems with strong hyperparameter interactions and requiring coordinated updates remains untested
- Theoretical convergence guarantees and regret bounds are not provided, limiting understanding of the algorithm's fundamental properties

## Confidence

- **High confidence**: The core algorithmic framework of treating hyperparameters as multi-armed bandit arms is clearly defined and implementable
- **Medium confidence**: Performance claims on benchmark functions are supported by experimental results, but generalizability to other problem domains remains to be validated
- **Medium confidence**: Computational efficiency claims are plausible but require more extensive benchmarking against alternative hyperparameter tuning methods

## Next Checks

1. Test HASSO on high-dimensional problems (100+ dimensions) to verify scalability claims and identify potential performance degradation
2. Compare HASSO against state-of-the-art hyperparameter optimization frameworks (e.g., HyperOpt, Optuna) on real-world engineering optimization problems
3. Conduct ablation studies to quantify the contribution of each hyperparameter being tuned and identify which ones provide the most value