---
ver: rpa2
title: Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees
arxiv_id: '2311.08384'
source_url: https://arxiv.org/abs/2311.08384
tags:
- policy
- offline
- learning
- where
- bellman
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a hybrid RL algorithm that combines on-policy
  policy gradient methods with offline data through a novel hybrid actor-critic approach.
  The key idea is to integrate an off-policy fitted policy evaluation procedure into
  the on-policy natural policy gradient framework, allowing the algorithm to benefit
  from both online and offline data.
---

# Offline Data Enhanced On-Policy Policy Gradient with Provable Guarantees

## Quick Facts
- arXiv ID: 2311.08384
- Source URL: https://arxiv.org/abs/2311.08384
- Reference count: 40
- One-line primary result: HNPG achieves best-of-both-worlds guarantees by combining on-policy NPG with offline data through hybrid actor-critic approach

## Executive Summary
This paper introduces a hybrid RL algorithm that combines on-policy policy gradient methods with offline data through a novel hybrid actor-critic approach. The key idea is to integrate an off-policy fitted policy evaluation procedure into the on-policy natural policy gradient framework, allowing the algorithm to benefit from both online and offline data. Theoretically, the method achieves best-of-both-worlds guarantees: it matches the performance of state-of-the-art offline RL methods when Bellman completeness holds, while maintaining the robustness of on-policy methods otherwise. Empirically, the proposed HNPG algorithm significantly outperforms strong baselines like PPO and RLPD on challenging rich-observation combination lock tasks, including a novel image-based variant using CIFAR100 features, demonstrating superior sample efficiency and generalization.

## Method Summary
The method combines on-policy natural policy gradient with offline data through a hybrid actor-critic framework. The algorithm maintains an online policy evaluation through least squares regression while incorporating off-policy TD learning from offline data. The hybrid policy evaluation (HPE) subroutine performs dual optimization with on-policy and off-policy objectives, weighted by parameter λ. The main algorithm collects online data, calls HPE to estimate value functions, and updates policy parameters using compatible function approximation. The method is theoretically grounded with provable guarantees that match state-of-the-art offline RL when Bellman completeness holds and maintain on-policy robustness otherwise.

## Key Results
- HNPG achieves best-of-both-worlds theoretical guarantees, matching offline RL performance when Bellman completeness holds while maintaining on-policy robustness otherwise
- On continuous combination lock tasks, HNPG demonstrates significantly better sample efficiency than TRPO and RLPD baselines
- On image-based CIFAR100 combination lock tasks, HNPG shows superior generalization performance compared to pure off-policy methods like RLPD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm achieves best-of-both-worlds performance by combining on-policy NPG with offline data through hybrid fitted policy evaluation (HPE).
- Mechanism: HPE performs off-policy TD learning on offline data while maintaining on-policy least squares regression on online data. This dual objective ensures that when Bellman completeness holds, the function approximation can accurately estimate both the value function and Bellman residuals. When Bellman completeness fails, the on-policy component still provides valid gradients for policy improvement.
- Core assumption: Bellman completeness holds approximately or offline data distribution has bounded Bellman error transfer coefficient relative to the comparator policy.
- Evidence anchors: [abstract] states the best-of-both-worlds result; [section] explains how the algorithm achieves state-of-the-art offline RL guarantees while maintaining on-policy guarantees; corpus contains related work on on/off-policy alignment.

### Mechanism 2
- Claim: The hybrid approach maintains stable training even when Bellman completeness fails, unlike pure off-policy methods.
- Mechanism: The on-policy least squares regression loss in HPE provides a stable supervised learning signal that doesn't depend on bootstrapping. This prevents the divergence that occurs in pure off-policy TD learning when function approximation error is large.
- Core assumption: Online data distribution is sufficiently rich to provide accurate policy evaluation through least squares regression.
- Evidence anchors: [abstract] shows HNPG outperforms PPO and RLPD; [section] explains the stability from supervised learning style least squares regression; corpus contains related work on on-policy sampling.

### Mechanism 3
- Claim: The algorithm generalizes better to test data in image-based environments due to reduced overfitting from off-policy updates.
- Mechanism: By using fewer off-policy updates compared to pure off-policy methods, the algorithm is less likely to overfit to the training distribution of offline data, improving test-time performance.
- Core assumption: The offline data distribution differs from the test distribution, and overfitting to offline data harms generalization.
- Evidence anchors: [abstract] demonstrates superior generalization in image-based variant; [section] shows HNPG achieves larger margin over RLPD in test environment; corpus contains related work on distribution shift.

## Foundational Learning

- Concept: Bellman completeness
  - Why needed here: This is the key assumption that determines whether the hybrid approach can achieve the superior theoretical guarantees. When Bellman completeness holds, the algorithm can benefit from offline data in a way that pure on-policy methods cannot.
  - Quick check question: What does Bellman completeness mean, and how does it relate to the function class F in this context?

- Concept: Policy gradient theorem and natural policy gradient
  - Why needed here: The algorithm builds on these foundations to understand how the hybrid approach maintains on-policy guarantees while incorporating offline data. Understanding the original NPG update rule is crucial for grasping how the hybrid modification works.
  - Quick check question: How does the natural policy gradient update differ from standard policy gradient, and why is this distinction important for the hybrid approach?

- Concept: Fitted Q-iteration and policy evaluation
  - Why needed here: The HPE subroutine uses concepts from fitted Q-iteration but modifies them with the hybrid objective. Understanding standard fitted Q-iteration helps explain why the hybrid version works better.
  - Quick check question: What is the main difference between standard fitted Q-iteration and the HPE procedure used in this algorithm?

## Architecture Onboarding

- Component map:
  - HNPG/HAC algorithm -> HPE subroutine -> Online data collection -> Offline data buffer -> Function approximator F -> Parameterized policy class Π

- Critical path:
  1. Collect online data by executing current policy
  2. Call HPE with current policy, function class, and both online/offline data
  3. HPE performs K2 iterations of dual objective optimization
  4. Extract value function estimate from HPE output
  5. Update policy parameters using compatible function approximation
  6. Repeat until convergence

- Design tradeoffs:
  - λ parameter: Controls the balance between on-policy and off-policy objectives. Higher λ emphasizes on-policy accuracy, while lower λ allows more off-policy influence.
  - K1, K2 iterations: More iterations in HPE can improve accuracy but increase computational cost.
  - Function class capacity: Larger function classes can better approximate value functions but risk overfitting.
  - Online vs offline data ratio: More online data improves on-policy accuracy but may reduce the benefit of offline data.

- Failure signatures:
  - Training instability: Could indicate poor choice of λ or insufficient online data
  - Poor performance despite convergence: May suggest Bellman completeness doesn't hold and λ is too high
  - Overfitting to offline data: Might occur with very high updates-to-data ratio in off-policy component
  - Exploration failure: Could indicate insufficient online data or poor coverage of state space

- First 3 experiments:
  1. Run HNPG on a simple gridworld with both online and offline data to verify basic functionality and compare against pure PPO
  2. Test HNPG on the continuous combination lock task with varying amounts of offline data to see how performance scales
  3. Implement a modified version with different λ values on the image-based task to study the tradeoff between on-policy and off-policy objectives

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the HNPG algorithm's performance scale with increasing Bellman completeness error εbe?
- Basis in paper: [explicit] The paper states "Our algorithm enjoys an on-policy NPG style convergence guarantee even when εbe or bilinear-rank is large" and provides bounds for both εbe ≤ 1/T and εbe > 1/T cases.
- Why unresolved: The paper provides theoretical bounds but doesn't empirically evaluate how the algorithm's performance degrades as εbe increases.
- What evidence would resolve it: Empirical results showing HNPG's performance on environments with varying degrees of Bellman completeness error.

### Open Question 2
- Question: What is the optimal value of the reweighting parameter λ in Algorithm 2 and how does it affect the trade-off between on-policy and off-policy learning?
- Basis in paper: [explicit] The paper mentions "The relative weights of the terms is decided by the parameter λ, given as an input to the algorithm and chosen via hyperparameter tuning in our experiments."
- Why unresolved: The paper doesn't provide guidance on how to choose λ optimally or analyze its impact on the algorithm's performance.
- What evidence would resolve it: Empirical results showing HNPG's performance with different λ values and theoretical analysis of the optimal λ selection.

### Open Question 3
- Question: How does the HNPG algorithm's performance compare to other hybrid RL methods in environments with continuous action spaces and complex dynamics?
- Basis in paper: [explicit] The paper compares HNPG to TRPO and RLPD on continuous combination lock tasks but doesn't evaluate it against other hybrid RL methods like SAC or DDPG.
- Why unresolved: The paper's experiments are limited to a specific type of environment and don't provide a comprehensive comparison with other hybrid RL methods.
- What evidence would resolve it: Empirical results showing HNPG's performance compared to other hybrid RL methods on a variety of environments with continuous action spaces and complex dynamics.

## Limitations
- Theoretical claims depend critically on Bellman completeness assumptions that may not hold in practical scenarios
- Empirical validation is limited to synthetic combination lock environments, limiting generalizability to real-world applications
- Ablation studies on key hyperparameters like λ and function class capacity are limited, making it unclear how robust the method is to hyperparameter choices

## Confidence
- Theoretical guarantees: Medium - proofs appear sound but rely on strong assumptions difficult to verify in practice
- Empirical performance claims: Medium - results are consistent across combination lock variants but lack diversity in task types
- Generalization to image-based tasks: Low-Medium - CIFAR100 features are abstract representations, not raw images, limiting strength of visual generalization claims

## Next Checks
1. Test HNPG on a non-synthetic benchmark like D4RL or Atari games to verify performance outside the combination lock domain
2. Conduct ablation studies systematically varying λ and function class capacity to understand robustness
3. Implement a version of the algorithm without the on-policy component to isolate the contribution of each part to overall performance