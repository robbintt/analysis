---
ver: rpa2
title: Improving the Diproche CNL through Autoformalization via Large Language Models
arxiv_id: '2303.17513'
source_url: https://arxiv.org/abs/2303.17513
tags:
- language
- which
- diproche
- such
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Diproche system, designed for teaching mathematical proofs,
  initially used a Prolog-based controlled natural language (CNL) for automated proof
  checking. This paper explores using large language models, specifically OpenAI's
  DaVinci-3, for autoformalization of proof texts.
---

# Improving the Diproche CNL through Autoformalization via Large Language Models

## Quick Facts
- arXiv ID: 2303.17513
- Source URL: https://arxiv.org/abs/2303.17513
- Reference count: 10
- Key outcome: LLM-based autoformalization achieved 100% accuracy on 33 test sentences, offering greater flexibility and error tolerance than Prolog-based CNL

## Executive Summary
This paper presents a novel approach to improving the Diproche system for teaching mathematical proofs by replacing its Prolog-based controlled natural language (CNL) with large language model (LLM) autoformalization. The authors developed a Python prototype that uses OpenAI's DaVinci-3 model to automatically convert proof texts into formal representations for logical processing. Testing against typical Diproche texts showed perfect accuracy on 33 sentences in Boolean set theory exercises, with the LLM approach demonstrating superior flexibility in handling natural language variations and minor typos compared to the rigid grammar-based system.

## Method Summary
The authors created a Python prototype that sends proof sentences to OpenAI's DaVinci-3 model with context from previous sentences. The model transforms these into formal representations using a prompt containing 71 lines of example formalization pairs. The processed output is then passed to a Prolog backend for logical verification and proof checking. The system processes sentences individually with minimal context, using explicit labels to identify the current sentence and its context. A postprocessing routine standardizes variable notation in the formalizations.

## Key Results
- 100% correct processing of 33 sentences from typical Diproche texts
- Successful resolution of anaphors and context-dependent references
- Superior error tolerance for typos and natural language variations compared to hard-coded CNL

## Why This Works (Mechanism)

### Mechanism 1
Large language models can learn controlled natural language patterns from examples rather than formal grammars. The model generalizes from prompt examples, recognizing patterns like variable extraction, sentence classification, and formula translation without explicit grammar rules. Core assumption: The model's pretraining provides sufficient semantic understanding to map natural language patterns to formal representations when given sufficient examples. Break condition: Performance degrades when prompt length exceeds 4000 tokens or when examples become too diverse.

### Mechanism 2
Context-aware processing enables correct interpretation of elliptical sentences and anaphors. The model maintains context from previous sentences to resolve references like "Suppose not" or "Thus, it is even." Core assumption: The model can track variable scope and logical dependencies across multiple sentences. Break condition: Context window overflow or ambiguous references that could map to multiple interpretations.

### Mechanism 3
Autoformalization via LLM provides greater flexibility and error tolerance than hard-coded parsers. The model handles natural language variations, minor typos, and alternative formulations that would break rigid formal grammars. Core assumption: The model's language understanding generalizes beyond training examples to handle unseen but semantically similar inputs. Break condition: Significant deviation from learned patterns or domain-specific terminology outside the model's pretraining.

## Foundational Learning

- Concept: Controlled Natural Language (CNL) design principles
  - Why needed here: Understanding how to balance expressiveness with formal precision is crucial for designing effective prompts
  - Quick check question: What are the key differences between a CNL and unrestricted natural language in the context of automated proof checking?

- Concept: Prompt engineering and few-shot learning
  - Why needed here: The success depends on crafting effective examples that teach the model the desired transformation patterns
  - Quick check question: How does the structure of prompt examples influence the model's ability to generalize to new inputs?

- Concept: Text classification and semantic parsing
  - Why needed here: The system must identify sentence functions (assumptions, claims, annotations) and convert them to formal representations
  - Quick check question: What challenges arise when mapping natural language sentences to logical formulas that preserve both content and function?

## Architecture Onboarding

- Component map: Python preprocessing layer -> OpenAI DaVinci-3 API -> Prolog backend -> Interface layer
- Critical path: 1) User input sentence received 2) Context determined based on prior sentences 3) Prompt constructed with relevant examples 4) API call to DaVinci-3 5) Result parsed and post-processed 6) Passed to Prolog for logical verification 7) Feedback generated and returned to user
- Design tradeoffs:
  - Prompt length vs. comprehensiveness: Longer prompts capture more patterns but risk exceeding token limits
  - Context window vs. performance: Including more context improves accuracy but increases API costs and latency
  - Hard-coded rules vs. learned patterns: Manual rules ensure consistency but reduce flexibility and increase maintenance
- Failure signatures:
  - "missing context" errors indicate insufficient surrounding sentences
  - "invalid" responses suggest unparseable input or pattern mismatch
  - Inconsistent formalizations reveal ambiguity in the prompt examples
  - API timeouts suggest prompt optimization needed
- First 3 experiments:
  1. Test basic sentence types (declarations, assumptions, claims) with minimal context
  2. Test anaphora resolution with controlled context windows
  3. Test error tolerance by introducing typos and variations in standard formulations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the maximum prompt length that can be used with DaVinci-3 while maintaining acceptable accuracy for autoformalization in Diproche?
- Basis in paper: [explicit] The paper mentions a 4000 token limit for prompts and discusses how this restriction affects the system's ability to handle different areas of mathematics
- Why unresolved: The paper tested with 4000 tokens but didn't explore how performance scales with different prompt lengths or what the optimal length might be
- What evidence would resolve it: Systematic testing of autoformalization accuracy across various prompt lengths (2000, 3000, 4000, 5000+ tokens) would reveal the relationship between prompt length and performance

### Open Question 2
- Question: How does the cost-effectiveness of using large language models for proof checking scale with classroom size?
- Basis in paper: [explicit] The paper provides specific cost calculations showing 8Â¢ per 4000 tokens and estimates weekly costs of $400 for a class of 250 students
- Why unresolved: The paper only considers one classroom size and doesn't explore how costs scale for different sizes or what the break-even point is compared to traditional methods
- What evidence would resolve it: Cost-benefit analysis across multiple classroom sizes, comparing per-student costs with alternative proof-checking methods

### Open Question 3
- Question: Can local deployment of smaller, specialized language models achieve similar accuracy to cloud-based models while avoiding server dependency issues?
- Basis in paper: [explicit] The paper discusses server costs, response times, and capacity limitations as practical concerns for classroom use
- Why unresolved: The paper only mentions these issues but doesn't test whether smaller, locally-run models could provide adequate performance
- What evidence would resolve it: Comparative testing of locally-deployed models (like GPT-2, DistilBERT) against cloud-based DaVinci-3 on the same benchmark tasks

## Limitations
- 4000-token prompt length limit restricts number of examples and context that can be included
- Server response times and costs present barriers to classroom deployment
- Evaluation limited to German language Boolean set theory without cross-domain testing

## Confidence
- **High Confidence**: The mechanism of using LLM for autoformalization is technically sound and the observed improvements in flexibility and error tolerance are well-supported by specific examples
- **Medium Confidence**: The claims about context-awareness and anaphora resolution are supported by test cases but would benefit from systematic evaluation across varied contexts
- **Low Confidence**: The scalability claims and practical deployment feasibility lack empirical validation beyond the prototype stage

## Next Checks
1. Conduct systematic evaluation with larger, more diverse test sets including edge cases, ambiguous references, and cross-domain examples
2. Measure performance degradation as prompt length approaches token limits and quantify the impact of context window constraints
3. Benchmark against the original Prolog-based CNL on error rates, processing speed, and maintenance overhead to quantify the claimed improvements