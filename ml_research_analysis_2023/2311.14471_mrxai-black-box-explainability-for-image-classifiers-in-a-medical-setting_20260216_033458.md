---
ver: rpa2
title: 'MRxaI: Black-Box Explainability for Image Classifiers in a Medical Setting'
arxiv_id: '2311.14471'
source_url: https://arxiv.org/abs/2311.14471
tags:
- tools
- medical
- tool
- which
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of selecting suitable black-box
  explainability tools for medical image classifiers, specifically in brain cancer
  MRI detection. The authors compare several popular black-box methods (LIME, RISE,
  SHAP, and ReX) against the white-box Grad-CAM on a brain cancer MRI dataset.
---

# MRxaI: Black-Box Explainability for Image Classifiers in a Medical Setting

## Quick Facts
- arXiv ID: 2311.14471
- Source URL: https://arxiv.org/abs/2311.14471
- Reference count: 40
- The paper compares black-box explainability tools for brain cancer MRI detection, finding that most are unsuitable for medical imaging except ReX, which matches white-box Grad-CAM performance.

## Executive Summary
This study evaluates the suitability of popular black-box explainability tools (LIME, RISE, SHAP, and ReX) for explaining medical image classifications, specifically brain cancer MRI detection. The authors compare these tools against the white-box Grad-CAM method on a brain cancer MRI dataset. They find that most black-box tools fail to provide clinically relevant explanations due to the unique characteristics of medical imaging data, such as low diversity and homogeneous backgrounds. ReX, a causal explainability-based method, performs as well as Grad-CAM, providing explanations that better align with human expectations. The study introduces a Penalised Dice Coefficient (PDC) measure to quantify the similarity between explanations and human annotations, accounting for area, distance, and number of non-contiguous regions.

## Method Summary
The study evaluates multiple black-box explainability tools on a pre-trained ResNet-50 CNN model for brain tumor detection in MRI images. The researchers used the Buda et al. brain MRI dataset from The Cancer Imaging Archive (3,929 images, 1,370 positive cases) and radiologist-provided tumor masks as ground truth. They applied LIME, RISE, SHAP, and ReX in default settings with 2000 mutants each, comparing their explanations against Grad-CAM using the newly introduced Penalised Dice Coefficient (PDC) metric. The PDC combines Dice similarity with distance, area ratio, and contiguity penalties to better capture clinical relevance.

## Key Results
- Most black-box explainability tools (LIME, RISE, SHAP) produce explanations that poorly align with human expectations in medical imaging contexts
- ReX, a causal explainability method, performs as well as white-box Grad-CAM in explaining tumor regions
- The Penalised Dice Coefficient (PDC) provides a more clinically relevant evaluation metric than standard Dice alone by accounting for explanation contiguity, distance, and size
- Black-box tools' default occlusion and perturbation strategies are inappropriate for low-diversity medical images

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReX performs better than other black-box tools because it uses causal responsibility to iteratively refine explanations, unlike other tools that rely on occlusion or gradient-based heuristics.
- Mechanism: ReX ranks pixels by their degree of responsibility in classification, constructs explanations greedily, and refines based on high-responsibility regions, stopping when no further improvement is achieved.
- Core assumption: The model's classification decision is sensitive to the presence of specific pixel regions in a way that can be meaningfully ranked by causal responsibility.
- Evidence anchors:
  - [abstract] "ReX, a causal explainability-based method, performs as well as Grad-CAM"
  - [section] "ReX constructs an approximation of a causal explanation... by first ranking the pixels... and then constructing an explanation greedily from the saliency landscape"
- Break condition: If the model does not depend causally on pixel regions, or if the causal ranking fails to capture clinically relevant features.

### Mechanism 2
- Claim: Other black-box tools fail because their occlusion/perturbation strategies are inappropriate for the low diversity of medical images.
- Mechanism: Tools like RISE and LIME use perturbations (e.g., random masks, mean pixel replacement) that assume high diversity in background and context, which does not hold in brain MRI.
- Core assumption: The perturbation strategies used in standard image classification do not generalize to homogeneous medical image datasets.
- Evidence anchors:
  - [section] "MRI images almost completely lack this diversity... Occlusion techniques and defaults developed for models trained on diverse data may be inappropriate for models trained on low diversity data."
  - [section] "LIME... uses a segmentation algorithm... The resulting segments are generally larger than pertinent to neuroanatomical features."
- Break condition: If perturbations are adapted to medical image characteristics or if the model is retrained on diverse data.

### Mechanism 3
- Claim: The Penalised Dice Coefficient (PDC) provides a better comparison metric than Dice alone because it penalizes distance, size ratio, and number of non-contiguous regions.
- Mechanism: PDC combines Dice similarity with normalized distance between explanation and ground truth, area ratio penalty, and mean over non-contiguous regions.
- Core assumption: Clinical usefulness of explanations correlates with proximity, size alignment, and continuity with human-provided annotations.
- Evidence anchors:
  - [section] "We also calculate the ratio between the HPE area and exp area... We combine the distance and area ratio with the DC... to form the summary statistic of the PDC"
  - [section] "The PDC for a single explanation mask is given as the mean of all non-contiguous areas"
- Break condition: If clinical relevance does not depend on these factors, or if alternative metrics are preferred by domain experts.

## Foundational Learning

- Concept: Causal reasoning in explainability
  - Why needed here: ReX relies on causal responsibility to rank pixels, distinguishing correlation from causation in explanations.
  - Quick check question: What is the difference between correlation-based and causation-based explanations in XAI?

- Concept: Model-agnostic vs. white-box explainability
  - Why needed here: The paper contrasts black-box tools (LIME, SHAP, RISE, ReX) with white-box Grad-CAM to evaluate trade-offs.
  - Quick check question: What are the key advantages and disadvantages of black-box explainability methods compared to white-box methods?

- Concept: Evaluation metrics for explanation quality
  - Why needed here: PDC is introduced to measure explanation quality by comparing against human-provided annotations, accounting for area, distance, and contiguity.
  - Quick check question: How does PDC differ from the standard Dice coefficient, and why is this difference important for medical image explanations?

## Architecture Onboarding

- Component map: Buda dataset (MRI images) -> ResNet-50 CNN model -> XAI tools (LIME, RISE, SHAP, ReX, Grad-CAM) -> Human-provided tumor masks (HPE) -> PDC evaluation

- Critical path:
  1. Load and preprocess MRI dataset
  2. Run model inference on positive cases
  3. Generate explanations using each XAI tool
  4. Extract binary masks from heatmaps where needed
  5. Compute PDC, Dice, and region count for each explanation
  6. Aggregate results and visualize comparisons

- Design tradeoffs:
  - Using default parameters avoids overfitting but may miss optimal performance
  - PDC penalizes non-contiguous regions, which may be clinically relevant in some cases
  - ReX stops early when no improvement is found, trading completeness for speed

- Failure signatures:
  - High count of non-contiguous regions suggests spurious explanations
  - Low Dice but high PDC indicates distance or size mismatch
  - Empty explanations indicate model insensitivity to perturbations

- First 3 experiments:
  1. Run all XAI tools with default parameters on a subset of 50 positive MRI slices and compute PDC distributions.
  2. Vary the size penalty parameters (s, b) in PDC to see sensitivity to explanation size.
  3. Compare explanation masks from ReX and Grad-CAM on a single image to analyze differences in localization strategy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different XAI tools perform on clinically validated deep learning models trained on large, multi-site datasets?
- Basis in paper: [explicit] The paper states that future work will include assessing tools with clinically validated deep learning models trained on large, multi-site datasets.
- Why unresolved: The current study used a single dataset with a model trained on relatively small, curated data. Performance on more robust, clinically validated models may differ significantly.
- What evidence would resolve it: Comparative analysis of XAI tool performance (PDC, DC, count) across multiple clinical datasets and models validated by leading cancer imaging experts.

### Open Question 2
- Question: What are the optimal parameter settings for XAI tools in medical imaging contexts?
- Basis in paper: [inferred] The paper used default settings for all tools except mutant budget, noting that parameter optimization could improve performance but was impractical to exhaustively search.
- Why unresolved: Default parameters may not be suitable for medical images, and tool-specific optimization could enhance explainability. However, finding optimal settings requires balancing performance with generalizability.
- What evidence would resolve it: Systematic evaluation of XAI tool performance across various parameter settings on multiple medical imaging datasets, demonstrating improvements over default settings without overfitting.

### Open Question 3
- Question: How does the PDC correlate with clinician assessments of XAI tool usefulness in medical imaging?
- Basis in paper: [explicit] The paper acknowledges the need for clinical validation of the PDC measure, stating plans to correlate PDC with clinicians' assessments.
- Why unresolved: The PDC is a new measure developed to quantify explanation similarity to human annotations, but its clinical relevance and usefulness remain to be established.
- What evidence would resolve it: Empirical study correlating PDC scores with clinician ratings of XAI tool explanations, establishing the measure's validity and usefulness in clinical contexts.

## Limitations

- The study focuses on a single CNN architecture (ResNet-50) and one specific medical imaging task, limiting generalizability to other model types or clinical domains
- Default parameters were used for black-box tools, which may not represent optimal configurations for medical imaging
- The causal mechanism underlying ReX's superior performance remains partially opaque
- The PDC metric introduces additional hyperparameters whose sensitivity to clinical relevance is not extensively validated

## Confidence

- Confidence in the claim that ReX matches Grad-CAM: High
- Confidence in the claim that other black-box tools are generally unsuitable for medical imaging: Medium
- Confidence in the proposed mechanism for ReX's success: Low-Medium

## Next Checks

1. Test ReX and other tools across multiple CNN architectures and medical imaging modalities to assess generalizability.
2. Conduct ablation studies on PDC parameters to determine their clinical significance and sensitivity.
3. Perform qualitative expert review of ReX explanations by radiologists to validate alignment with clinical reasoning beyond quantitative metrics.