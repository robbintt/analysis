---
ver: rpa2
title: 'Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed
  Mixture-of-Experts'
arxiv_id: '2306.04845'
source_url: https://arxiv.org/abs/2306.04845
tags:
- supernet
- training
- architecture
- search
- neuron-wise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mixture-of-Supernets improves weight-sharing supernet training
  by employing a mixture-of-experts (MoE) approach with architecture-routed routing.
  Instead of direct weight sharing, subnetworks share weights through an architecture-based
  routing mechanism, allowing customized weight generation for specific architectures.
---

# Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with Architecture-Routed Mixture-of-Experts

## Quick Facts
- arXiv ID: 2306.04845
- Source URL: https://arxiv.org/abs/2306.04845
- Authors: 
- Reference count: 40
- Key outcome: MoS improves weight-sharing supernet training by employing architecture-routed MoE, achieving state-of-the-art results in neural architecture search for machine translation and BERT models, outperforming existing methods in terms of latency-BLEU tradeoff and model efficiency.

## Executive Summary
This paper addresses the performance gap between weight-sharing supernets and standalone training in neural architecture search (NAS) for NLP tasks. The authors propose Mixture-of-Supernets (MoS), which employs a mixture-of-experts (MoE) approach with architecture-routed routing. Instead of directly extracting weights for each architecture, MoS uses a router to dynamically generate specialized weights based on architecture characteristics. This approach significantly improves the performance of discovered architectures while maintaining the efficiency benefits of weight sharing.

## Method Summary
MoS introduces an architecture-based routing mechanism that enables indirect sharing of model weights among subnetworks. The method uses multiple expert weight matrices and a router network that generates alignment scores to combine these experts based on the input architecture. Two variants are proposed: layer-wise MoS (sharing at layer level) and neuron-wise MoS (sharing at individual neuron level). The supernet is trained using sandwich training, and architecture-specific weights are generated through the MoE mechanism without requiring additional training after supernet training.

## Key Results
- MoS achieves state-of-the-art results in neural architecture search for machine translation and BERT models
- Outperforms existing methods in terms of latency-BLEU tradeoff and model efficiency
- Reduces the performance gap between supernet and standalone training in NLP tasks
- Provides customized weight generation for specific architectures through architecture-routed routing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Architecture-based routing enables specialized weight generation for different subnetworks.
- Mechanism: Instead of directly extracting subset of weights for each architecture, MoS uses a router to generate alignment scores that weight combination of expert weight matrices, creating architecture-specific weights dynamically.
- Core assumption: The router can learn to assign appropriate alignment scores that optimize performance for each architecture based on its characteristics.
- Evidence anchors: [abstract] "this method employs an architecture-based routing mechanism, enabling indirect sharing of model weights among subnetworks"

### Mechanism 2
- Claim: Neuron-wise granularity provides more flexible weight sharing than layer-wise approaches.
- Mechanism: By treating individual neurons as experts rather than entire layers, MoS can create more fine-grained weight combinations that better match each architecture's specific needs.
- Core assumption: Individual neurons can be meaningfully combined to create optimal weights for different architectures without losing coherence.
- Evidence anchors: [section 3.3] "Compared to the layer-wise MoS, the neuron-wise MoS has more flexibility (m × nouta instead of only m) to control the degree of weight sharing"

### Mechanism 3
- Claim: Mixture-of-experts framework increases model expressiveness without changing final architecture.
- Mechanism: The supernet maintains multiple expert weight matrices and uses routing to combine them dynamically, creating an ensemble effect that can represent more diverse functions than a single weight matrix.
- Core assumption: The routing mechanism can effectively learn which expert combinations work best for different architectures without requiring changes to the final subnetwork structure.
- Evidence anchors: [abstract] "mixture-of-experts (MoE) is adopted to enhance the expressive power of the supernet model"

## Foundational Learning

- Concept: Neural Architecture Search (NAS)
  - Why needed here: The entire paper addresses improving supernet training within NAS frameworks
  - Quick check question: What is the key difference between weight-sharing NAS and traditional black-box NAS?

- Concept: Mixture-of-Experts (MoE)
  - Why needed here: MoS directly builds on MoE principles to create specialized weight combinations
  - Quick check question: How does the router in MoE differ from standard attention mechanisms?

- Concept: Weight sharing in neural networks
  - Why needed here: Understanding standard weight sharing is crucial to grasp why MoS improves upon it
  - Quick check question: What are the main drawbacks of direct weight sharing between architectures of different sizes?

## Architecture Onboarding

- Component map: Architecture encoder -> Router network (MLP with softmax) -> Expert weight matrices -> Final weight generation function -> Subnetwork

- Critical path:
  1. Sample architecture a from search space
  2. Encode architecture a → Enc(a)
  3. Router produces alignment scores αa or βa
  4. Generate architecture-specific weights Wa using expert combinations
  5. Forward pass through subnetwork
  6. Compute loss and backpropagate to update all components

- Design tradeoffs:
  - More expert weights (larger m) → greater flexibility but more parameters
  - Layer-wise vs neuron-wise → coarse vs fine-grained control
  - Router complexity → better specialization vs harder training

- Failure signatures:
  - Router produces uniform alignment scores → no specialization occurs
  - Certain architectures consistently underperform → routing not learning properly
  - Training instability or divergence → MoE framework too complex

- First 3 experiments:
  1. Implement layer-wise MoS with m=2 experts on a simple CNN search space
  2. Compare performance vs standard weight-sharing supernet
  3. Visualize router alignment scores for different architectures to verify specialization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Mixture-of-Supernets (MoS) method perform when trained with a larger number of expert weights (m) and a larger training budget (e.g., ≥ 200K steps)?
- Basis in paper: [explicit] The paper mentions that increasing 'm' introduces too many parameters and might necessitate a significant increase in the training budget.
- Why unresolved: The paper only provides results for a limited number of expert weights (m = 2) and a standard training budget (40K steps for MT, 125K steps for BERT).
- What evidence would resolve it: Experiments showing the performance of MoS with varying 'm' and training budgets, comparing the results with the current findings.

### Open Question 2
- Question: How does the performance of the proposed MoS method compare to other NAS methods, such as DARTS, when applied to NLP tasks like machine translation and BERT?
- Basis in paper: [inferred] The paper compares MoS with state-of-the-art NAS methods like HAT, NAS-BERT, and AutoDistil for specific tasks (machine translation and BERT). However, it does not compare MoS with other popular NAS methods like DARTS.
- Why unresolved: The paper does not provide a direct comparison between MoS and other NAS methods like DARTS for NLP tasks.
- What evidence would resolve it: Experiments comparing MoS with DARTS and other NAS methods for machine translation and BERT tasks, evaluating their performance in terms of latency-BLEU tradeoff and model efficiency.

### Open Question 3
- Question: How does the performance of MoS vary with different choices of the router function, such as using different activation functions or architectures (e.g., RNNs, CNNs)?
- Basis in paper: [explicit] The paper mentions that the router function is an MLP with softmax and that they experiment with different numbers of hidden layers in the router function. However, it does not explore other choices of router functions.
- Why unresolved: The paper only experiments with a specific router function (MLP with softmax) and does not explore other possibilities.
- What evidence would resolve it: Experiments comparing the performance of MoS with different router functions, such as using different activation functions or architectures (e.g., RNNs, CNNs), and evaluating their impact on the overall performance.

## Limitations
- Limited empirical validation to specific NLP tasks (MT and BERT)
- Computational overhead of router and expert weight matrices not thoroughly analyzed
- Generalization to other domains like computer vision remains unproven

## Confidence
- **High Confidence**: The core mechanism of architecture-routed MoE for weight sharing is technically sound and well-motivated
- **Medium Confidence**: Performance improvements on MT and BERT tasks are demonstrated but need further validation
- **Low Confidence**: Claims about broad applicability and efficiency compared to other methods are not fully substantiated

## Next Checks
1. Apply MoS to a different domain (e.g., image classification) and compare performance against standard weight-sharing supernets and other NAS methods
2. Conduct an ablation study to quantify the contribution of the router mechanism and number of expert weight matrices to overall performance
3. Measure training time and memory usage of MoS compared to standard weight-sharing supernets and standalone training to assess practical efficiency