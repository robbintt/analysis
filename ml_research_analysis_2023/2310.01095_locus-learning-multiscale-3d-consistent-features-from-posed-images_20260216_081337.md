---
ver: rpa2
title: 'LoCUS: Learning Multiscale 3D-consistent Features from Posed Images'
arxiv_id: '2310.01095'
source_url: https://arxiv.org/abs/2310.01095
tags:
- features
- image
- retrieval
- segmentation
- patches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LoCUS learns multi-scale, 3D-consistent image features from posed
  images by optimizing a novel ranking-based objective: Vectorized-Smooth-AP. This
  objective encourages feature consistency across views of the same 3D location, while
  adaptively selecting highly distinguishable landmarks.'
---

# LoCUS: Learning Multiscale 3D-consistent Features from Posed Images

## Quick Facts
- **arXiv ID:** 2310.01095
- **Source URL:** https://arxiv.org/abs/2310.01095
- **Reference count:** 40
- **Primary result:** LoCUS learns multi-scale, 3D-consistent image features from posed images, achieving AP 0.55 on place recognition, mAP 0.54 on segmentation, and 0.92m error on relative pose estimation

## Executive Summary
LoCUS introduces a novel approach to learning 3D-consistent image features by optimizing a ranking-based objective called Vectorized-Smooth-AP. This method encourages feature consistency across views of the same 3D location while adaptively selecting highly distinguishable landmarks. A key innovation is the introduction of "don't-care" regions that balance feature distinctiveness with reusability across different environments. Trained on Matterport3D, LoCUS features demonstrate strong performance on downstream tasks including place recognition, semantic and instance segmentation, and relative pose estimation.

## Method Summary
LoCUS learns multi-scale, 3D-consistent image features from posed images by training on top of a frozen DINO backbone using a Vectorized-Smooth-AP objective. The method optimizes for 3D-location-consistent features by encouraging similarity between image patches corresponding to nearby 3D locations. The "don't-care" region mechanism excludes patches mapping to far-away locations from the retrieval set, balancing distinctiveness with reusability. The model uses a 2-stage transformer architecture with 64-dimensional output features and is trained for 20 epochs on Matterport3D with Adam optimizer and learning rate 1e-4.

## Key Results
- Place recognition: AP 0.55 vs 0.20 baseline
- Semantic and instance segmentation: mAP 0.54 vs 0.40 baseline
- Relative pose estimation: error 0.92m vs 1.02-1.08m baselines
- Features stable under significant viewpoint changes
- Enables semantic segmentation and object re-identification

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The "don't-care" region beyond a certain spatial extent allows the model to balance feature distinctiveness with reusability
- Mechanism: By excluding patches that map to far-away locations from the retrieval set, the model avoids forcing all landmarks to be unique to a particular place. This prevents overfitting to training scenes and enables the same feature to be reused for similar-looking landmarks in different environments
- Core assumption: Similar-looking landmarks in distant places should not be pushed apart unnecessarily
- Evidence anchors: [abstract]: "We find that it is possible to balance retrieval and reusability by constructing the retrieval set carefully, leaving out patches that map to far-away locations."
- Break condition: If the model needs to distinguish between semantically similar but spatially distant landmarks, the "don't-care" region may hinder this capability

### Mechanism 2
- Claim: Optimizing for (smooth) Average Precision (AP) encourages 3D-location-consistent features
- Mechanism: The ranking-based objective directly encourages the features from two image patches to be similar if they correspond to 3D locations that are at most a distance ρ apart. This ensures that the model extracts similar features for different viewpoints of the same 3D location
- Core assumption: Patches corresponding to the same 3D location should have similar features
- Evidence anchors: [abstract]: "We optimize for (smooth) Average Precision (AP), in a single unified ranking-based objective."
- Break condition: If the model encounters viewpoints with significant occlusions or lighting variations, the AP-based objective may struggle to maintain feature consistency

### Mechanism 3
- Claim: Vectorized-Smooth-AP facilitates the selection of highly-identifiable landmarks
- Mechanism: By considering all pairs of landmarks and patches as if they're part of a single query, the objective adaptively selects highly distinguishable landmarks. This is achieved by first distinguishing the easiest landmark-patch pairs from the rest, while ignoring those that are too ambiguous
- Core assumption: Highly distinguishable landmarks are more useful for downstream tasks
- Evidence anchors: [abstract]: "This objective also doubles as a criterion for choosing landmarks or keypoints, as patches with high AP."
- Break condition: If the model encounters environments with a large number of similar-looking landmarks, the vectorized-AP may struggle to identify the most distinctive ones

## Foundational Learning

- Concept: Average Precision (AP) and ranking-based metrics
  - Why needed here: The model is trained to optimize a ranking-based metric (smooth AP) that encourages the retrieval of image patches corresponding to the same 3D location
  - Quick check question: What is the difference between Average Precision (AP) and the area under the precision-recall curve (AUC-PR)?

- Concept: Metric learning and contrastive loss
  - Why needed here: The model learns to map image patches to a feature space where similar patches (corresponding to the same 3D location) are close together, while dissimilar patches are far apart
  - Quick check question: How does the contrastive loss differ from the triplet loss in metric learning?

- Concept: 3D geometry and camera pose estimation
  - Why needed here: The model relies on camera pose information to project image patches to 3D space and define the positive and negative sets for the ranking objective
  - Quick check question: What is the relationship between camera pose, intrinsic parameters, and the projection of a 3D point onto the image plane?

## Architecture Onboarding

- Component map: Frozen DINO backbone -> Two linear layers -> 64-dimensional LoCUS features -> Vectorized-Smooth-AP objective

- Critical path: Extract image patches from training images → Compute DINO features for each patch → Apply two linear layers to obtain LoCUS features → Compute pairwise scores between LoCUS features and tentative landmark embeddings → Calculate the Vectorized-Smooth-AP objective → Backpropagate gradients to update the linear layers

- Design tradeoffs:
  - Using a frozen DINO backbone limits the model's ability to learn task-specific features but reduces training complexity
  - Mapping to a lower-dimensional feature space (64 dimensions) may lose some information but improves computational efficiency
  - The Vectorized-Smooth-AP objective encourages feature consistency but may struggle with highly ambiguous landmarks

- Failure signatures:
  - Poor performance on downstream tasks (e.g., place recognition, semantic segmentation) may indicate issues with the LoCUS features
  - High AP on training data but low AP on validation data suggests overfitting
  - Inconsistent features across different viewpoints of the same 3D location may indicate problems with the ranking objective

- First 3 experiments:
  1. Evaluate the AP of LoCUS features on a validation set with unseen environments to assess generalizability
  2. Compare the performance of LoCUS features with DINO features on a semantic segmentation task to highlight the benefits of the proposed training method
  3. Analyze the impact of the "don't-care" region size (κ) on the model's ability to balance feature distinctiveness and reusability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the fully self-supervised approach be extended to larger-scale video data without camera pose supervision?
- Basis in paper: [explicit] "Nonetheless, strategies for removing the weak camera pose supervision warrant investigation, since a fully self-supervised approach would facilitate access to greater quantities of data."
- Why unresolved: The paper relies on camera pose supervision for the 3D-aware loss, but acknowledges that removing this requirement would enable training on larger datasets
- What evidence would resolve it: Demonstrating that LoCUS features can be learned effectively using Structure-from-Motion or SparsePose to provide approximate camera poses, or developing an alternative self-supervised approach that doesn't require any pose information

### Open Question 2
- Question: How does the performance of LoCUS features compare to state-of-the-art methods on fine-grained geometric matching tasks?
- Basis in paper: [explicit] "The most comparable method are the two relative pose estimation algorithms using SuperPoint keypoints, which our method outperforms." The paper acknowledges that methods more geared towards fine-grained keypoint matching would likely do better on tasks requiring precise geometric correspondences
- Why unresolved: The experiments focus on coarse landmark retrieval and relative pose estimation with limited overlap, but don't evaluate performance on tasks requiring precise keypoint matching and geometric verification
- What evidence would resolve it: Benchmarking LoCUS features against state-of-the-art feature matching methods on established datasets like HPatches or Aachen Day-Night for visual localization

### Open Question 3
- Question: What is the optimal strategy for sampling tentative landmarks in environments with varying densities of image views?
- Basis in paper: [explicit] "While ideally it would be sufficient to sample the landmark positions randomly across space... in a mini-batch with limited memory this is often not efficient." The paper proposes uniform sampling among image patches as a practical solution but doesn't explore alternative strategies
- Why unresolved: The current approach may not be optimal for environments with highly uneven sampling of views, potentially leading to poor coverage or overfitting to frequently observed locations
- What evidence would resolve it: Comparing LoCUS performance using different landmark sampling strategies (e.g., density-based sampling, adaptive sampling based on coverage metrics) on datasets with varying view densities

## Limitations
- The claim that "don't-care" regions are essential for balancing distinctiveness and reusability lacks direct empirical validation
- Reliance on a frozen DINO backbone limits the model's ability to learn task-specific features
- Performance may degrade in environments with highly repetitive visual structures due to the "don't-care" region mechanism

## Confidence
- **High confidence**: Claims about strong downstream task performance (place recognition AP 0.55, segmentation mAP 0.54, pose estimation error 0.92m) are well-supported by quantitative results
- **Medium confidence**: The mechanism by which Vectorized-Smooth-AP encourages landmark selection is theoretically sound but lacks direct ablation studies
- **Medium confidence**: The claim that the "don't-care" region approach prevents overfitting is reasonable but not empirically proven

## Next Checks
1. Conduct an ablation study comparing LoCUS with and without the "don't-care" region mechanism across different environments to quantify its impact on feature reusability
2. Test LoCUS features on a dataset with highly repetitive visual structures (like indoor office environments) to evaluate whether the "don't-care" regions hinder discrimination of similar landmarks
3. Implement a version with a trainable backbone (instead of frozen DINO) to assess whether the performance gains are primarily from the ranking objective or could be enhanced with end-to-end training