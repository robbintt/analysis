---
ver: rpa2
title: Cross-Domain Robustness of Transformer-based Keyphrase Generation
arxiv_id: '2312.10700'
source_url: https://arxiv.org/abs/2312.10700
tags:
- keyphrase
- generation
- text
- corpora
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the cross-domain robustness of transformer-based
  models for keyphrase generation. The research evaluates BART, a transformer-based
  denoising autoencoder, fine-tuned for keyphrase generation across six benchmark
  corpora spanning scientific and news domains.
---

# Cross-Domain Robustness of Transformer-based Keyphrase Generation

## Quick Facts
- arXiv ID: 2312.10700
- Source URL: https://arxiv.org/abs/2312.10700
- Reference count: 40
- BART models fine-tuned on target corpora outperform baseline methods in many cases, but zero-shot performance on out-of-domain corpora is generally lower.

## Executive Summary
This study investigates the cross-domain robustness of transformer-based models for keyphrase generation, focusing on BART (a denoising autoencoder) fine-tuned across six benchmark corpora spanning scientific and news domains. The research evaluates various transfer learning strategies, including mixing training examples from different domains and two-stage fine-tuning. Results demonstrate that BART models fine-tuned on target corpora outperform baseline methods in many cases, while preliminary fine-tuning on out-of-domain corpora improves performance in few-shot settings. The study provides insights into the effectiveness of transfer learning for keyphrase generation across diverse domains.

## Method Summary
The study fine-tunes BART-base (12 layers, 768 hidden units) for keyphrase generation across six benchmark corpora (Krapivin, Inspec, PubMed, NamedKeys, DUC-2001, KPTimes) with 70/30 train/test splits. Multiple transfer learning strategies are explored: Domaineq (equal domain examples), Domainall (all examples), Mixeq (equal mixed examples), and Mixall (all mixed examples). Models are evaluated using F1, ROUGE-1, ROUGE-L, and BERTScore metrics, comparing results with baselines (TopicRank, YAKE!, KeyBART) across zero-shot, few-shot, and full-target corpus fine-tuning scenarios.

## Key Results
- BART models fine-tuned on target corpora outperform baseline methods in many cases (Krapivin-A, Inspec, KPTimes - all metrics; Krapivin-T - F1, R1, RL; PubMed - R1, RL, BS; NamedKeys - RL, BS)
- Zero-shot performance on out-of-domain corpora is generally lower than target corpus fine-tuning
- Preliminary fine-tuning on out-of-domain corpora improves performance in few-shot settings, allowing use of fewer target data while maintaining or exceeding full-target corpus fine-tuning results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning BART on target corpus outperforms baseline methods in many cases
- Mechanism: BART learns domain-specific patterns and keyphrase distributions when fine-tuned on target corpus data
- Core assumption: The target corpus contains sufficient examples for BART to learn effective keyphrase generation patterns
- Evidence anchors:
  - [abstract] "BART models fine-tuned on target corpora outperform baseline methods in many cases"
  - [section] "The BART fine-tuned on a target corpus outperforms baselines in many cases (Krapivin-A, Inspec, and KPTimes – all metrics; Krapivin-T – F1, R1, and RL; PubMed – R1, RL, and BS; NamedKeys – RL and BS)"
  - [corpus] Corpus size varies from 308 to 20,000 examples, providing reasonable training data for fine-tuning
- Break condition: If target corpus is too small (e.g., DUC-2001 with only 308 examples), fine-tuning may underperform unsupervised baselines

### Mechanism 2
- Claim: Preliminary fine-tuning on out-of-domain corpora improves performance in few-shot settings
- Mechanism: BART learns general keyphrase generation patterns from out-of-domain data, which transfers to target domain with limited examples
- Core assumption: Keyphrase generation patterns are transferable across domains
- Evidence anchors:
  - [abstract] "preliminary fine-tuning on out-of-domain corpora can be effective under conditions of a limited number of samples"
  - [section] "The models with two-stage fine-tuning outperform the ones fine-tuned only on a target corpus on a small target sample size"
  - [corpus] Mixed training strategies (Domaineq, Domainall, Mixeq, Mixall) show varying effectiveness across corpora
- Break condition: If domains are too dissimilar (e.g., news vs biomedical), transfer learning may not be effective

### Mechanism 3
- Claim: Mixing training examples from different domains can improve model performance
- Mechanism: Combining diverse examples helps BART learn more robust keyphrase generation patterns
- Core assumption: Keyphrase generation benefits from exposure to diverse domain patterns
- Evidence anchors:
  - [abstract] "mixing training examples from different domains and two-stage fine-tuning"
  - [section] "The use of the Domaineq and Mixeq strategies led to a sharp decrease in the size of the training set and the number of targeted examples and negatively affected the model performance"
  - [corpus] Mixall strategy generally improves performance or at least does not lead to strong degradation of results
- Break condition: If mixed training significantly reduces target domain examples (Mixeq), performance may degrade

## Foundational Learning

- Concept: Transfer learning in NLP
  - Why needed here: Understanding how knowledge transfers from out-of-domain to target domain is crucial for interpreting results
  - Quick check question: What is the key difference between zero-shot and few-shot transfer learning settings?

- Concept: Keyphrase generation vs. text summarization
  - Why needed here: The study adapts summarization models for keyphrase generation, requiring understanding of both tasks
  - Quick check question: How does keyphrase generation differ from traditional text summarization?

- Concept: Domain adaptation techniques
  - Why needed here: The study explores multiple strategies for adapting models across domains
  - Quick check question: What are the main differences between Domaineq, Domainall, Mixeq, and Mixall strategies?

## Architecture Onboarding

- Component map: BART-base (12 layers, 768 hidden units) → Fine-tuning → Evaluation on multiple corpora
- Critical path: Pre-training → Domain-specific fine-tuning → Evaluation with F1, ROUGE, BERTScore metrics
- Design tradeoffs: Larger training sets improve performance but increase training time; mixing domains helps in few-shot settings but may reduce target domain focus
- Failure signatures: Poor performance on out-of-domain corpora; underperformance compared to unsupervised baselines on small corpora
- First 3 experiments:
  1. Fine-tune BART on Krapivin-A, evaluate zero-shot on all other corpora
  2. Mix all corpora in equal proportions, fine-tune BART, evaluate on each corpus
  3. Implement two-stage fine-tuning: out-of-domain → target domain with increasing sample sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of BART models for keyphrase generation vary across different scientific domains beyond computer science and biomedical fields?
- Basis in paper: [explicit] The study evaluates BART models on scientific texts from computer science and biomedical domains, but does not explore other scientific fields.
- Why unresolved: The paper focuses on a limited set of scientific domains, leaving open the question of how well BART models generalize to other scientific fields with different terminologies and writing styles.
- What evidence would resolve it: Conducting experiments on additional scientific corpora from diverse fields such as physics, chemistry, or social sciences, and comparing the results to those obtained in the current study.

### Open Question 2
- Question: What is the impact of varying the number of epochs in the two-stage fine-tuning procedure on the performance of BART models for keyphrase generation?
- Basis in paper: [inferred] The study explores two-stage fine-tuning but does not investigate the effect of changing the number of epochs in each stage on the model's performance.
- Why unresolved: The paper presents results using a fixed number of epochs in the two-stage fine-tuning process, but does not examine how varying these values might affect the model's ability to generate keyphrases across different domains.
- What evidence would resolve it: Conducting experiments with different combinations of epoch numbers in the two-stage fine-tuning process and analyzing the resulting performance on keyphrase generation tasks.

### Open Question 3
- Question: How does the size of the training corpus affect the effectiveness of transfer learning for keyphrase generation using BART models?
- Basis in paper: [explicit] The study investigates the impact of preliminary out-of-domain fine-tuning on model performance with limited training data, but does not explore the relationship between corpus size and transfer learning effectiveness.
- Why unresolved: The paper focuses on the benefits of transfer learning with small training sets, but does not examine how the size of the training corpus influences the effectiveness of this approach for keyphrase generation.
- What evidence would resolve it: Conducting experiments with varying corpus sizes and analyzing the performance of BART models with and without transfer learning to determine the relationship between corpus size and the effectiveness of transfer learning in keyphrase generation tasks.

## Limitations

- Varying effectiveness of transfer learning strategies across different corpora, with some mixed training approaches causing performance degradation
- Highly dependent on domain similarity and corpus size, without clear guidelines for determining when transfer learning will be beneficial
- Focus primarily on zero-shot and few-shot settings, leaving typical fine-tuning scenarios less explored

## Confidence

- High confidence: BART fine-tuning on target corpora generally outperforms baselines when sufficient training data is available
- Medium confidence: Two-stage fine-tuning improves performance in few-shot settings, but effectiveness varies significantly by corpus
- Low confidence: Generalizability of findings to other domain pairs not represented in the six benchmark corpora

## Next Checks

1. Conduct ablation studies to quantify the contribution of each transfer learning strategy (Domaineq, Domainall, Mixeq, Mixall) to overall performance
2. Test the proposed methods on additional domain pairs, particularly focusing on domains with varying degrees of similarity to assess the limits of cross-domain transfer
3. Implement systematic hyperparameter tuning across all corpora to determine optimal training configurations for each transfer learning strategy