---
ver: rpa2
title: Analysing the Resourcefulness of the Paragraph for Precedence Retrieval
arxiv_id: '2308.01203'
source_url: https://arxiv.org/abs/2308.01203
tags:
- similarity
- pl-f
- paragraph
- methods
- legal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper analyzes the resourcefulness of paragraph-level information
  for capturing similarity among legal judgments to improve precedence retrieval.
  It proposes a paragraph-based similarity framework that computes similarity between
  judgments by considering paragraph interactions.
---

# Analysing the Resourcefulness of the Paragraph for Precedence Retrieval

## Quick Facts
- arXiv ID: 2308.01203
- Source URL: https://arxiv.org/abs/2308.01203
- Reference count: 23
- Primary result: Paragraph-level methods with few interactions outperform document-level baselines for legal judgment similarity

## Executive Summary
This paper proposes a paragraph-based similarity framework for legal precedence retrieval that computes similarity between judgments by considering paragraph interactions. The method breaks down judgments into paragraphs, generates embeddings using TF-IDF, Word2Vec, and BOW models, and computes cosine similarity values between paragraph pairs. By aggregating the most relevant paragraph similarities, the approach captures legal issue similarity more effectively than traditional document-level methods. Experiments on two benchmark datasets demonstrate that paragraph-level methods exhibit better discriminative power and retrieval performance.

## Method Summary
The method decomposes legal judgments into paragraphs and generates embeddings for each paragraph using vector space models (TF-IDF, Word2Vec, BOW). It computes pairwise cosine similarity between paragraphs from query and target judgments, then aggregates these values using either the Mean approach (PL-M) that considers all paragraphs or the Fixed approach (PL-F) that selects only the top-k most similar paragraph pairs. The final similarity score represents the overall relationship between two judgments, enabling more precise precedence retrieval by focusing on localized legal issue interactions rather than treating entire judgments as monolithic documents.

## Key Results
- Paragraph-level methods outperform document-level baselines on FIRE IRLed 2017 and FIRE AILA 2019 datasets
- TF-IDF combined with PL-F approach exhibits the best discriminative power among all tested methods
- Selecting only top-k similar paragraph pairs (PL-F) achieves comparable performance to considering all paragraphs (PL-M)
- The method requires only a few paragraph interactions to capture judgment similarity effectively

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Paragraph-level methods capture legal issue similarity more effectively by focusing on localized interactions
- Mechanism: Breaking judgments into paragraphs allows localized similarity comparisons between individual legal issues
- Core assumption: Each paragraph typically discusses a single legal issue, enabling precise issue-level matching
- Evidence anchors:
  - [abstract] "The paragraph-level methods capture the similarity among judgments more effectively, exhibiting better discriminative power and retrieval performance."
  - [section] "We have considered a paragraph-level approach for computing similarity between the judgments and conducted extensive experiments to analyze paragraph-level methods with a document-level method as a baseline."
- Break condition: If paragraphs frequently contain multiple legal issues or topical coherence is low, paragraph-level methods may lose effectiveness

### Mechanism 2
- Claim: Using only top-k most similar paragraph pairs improves retrieval by focusing on relevant matches
- Mechanism: For each query paragraph, only the most similar target paragraph is considered, and the final score is the mean of top-k maximum similarities
- Core assumption: Few paragraph pairs with highest similarity significantly impact judgment similarity, while lower pairs add noise
- Evidence anchors:
  - [section] "The similarity of ùêΩ1 and ùêΩ2 is equal to the mean of the Top-ùëò values from MSP. This approach is based on the intuition that only a few paragraph pairs, specifically those with the highest similarity value, will impact the similarity between the judgment pair."
  - [section] "We found that the paragraph-level methods could capture the similarity among the judgments with only a few paragraph interactions and exhibit more discriminating power over the baseline document-level method."
- Break condition: If multiple paragraphs discuss the same issue with varying relevance, selecting only top-k pairs might miss important context

### Mechanism 3
- Claim: TF-IDF with paragraph-level methods exhibits better discriminative power than other vector space models
- Mechanism: TF-IDF downweights common terms while emphasizing distinctive terms within paragraphs, improving distinction between judgments
- Core assumption: Legal judgments contain specialized terminology where IDF weighting helps identify distinctive legal concepts
- Evidence anchors:
  - [section] "It can be noted that the overlap(ùê∑1, ùê∑2) score in indicates the distinguishing power between the existence and non-existence of a link. It can be observed that among all the methods and models, the O(ùê∑1, ùê∑2) score for the PL-F method is low, which implies that PL-F exhibits better discriminative power. Also, the least overlap score with a combination of TF-IDF and PL-F implies that TF-IDF with PL-F has the most discriminative power among all the methods."
  - [corpus] "Average neighbor FMR=0.403, average citations=0.0"
- Break condition: If legal judgments use highly specialized vocabulary appearing frequently across documents, IDF weighting may become less effective

## Foundational Learning

- Concept: Cosine similarity and vector space models
  - Why needed here: The method relies on computing cosine similarity between paragraph embeddings to measure similarity between legal judgments
  - Quick check question: How does cosine similarity measure the angle between two vectors, and why is this useful for text similarity?

- Concept: Legal document structure and paragraph-level analysis
  - Why needed here: Understanding that legal judgments are structured as sequences of paragraphs, with each paragraph typically addressing a single legal issue, is crucial for the method's effectiveness
  - Quick check question: Why might analyzing legal documents at the paragraph level be more effective than at the document level for finding relevant precedents?

- Concept: Citation networks and legal precedent relationships
  - Why needed here: The paper uses citation graphs as ground truth for evaluating similarity methods, assuming that closer judgments in the citation graph tend to be more similar
  - Quick check question: How does the citation structure in legal documents reflect the relationship between different judgments?

## Architecture Onboarding

- Component map: Document ‚Üí Paragraph segmentation ‚Üí Vector embedding ‚Üí Pairwise cosine similarity ‚Üí Maximum similarity pair selection ‚Üí Aggregation ‚Üí Similarity score

- Critical path: Document ‚Üí Paragraph segmentation ‚Üí Vector embedding ‚Üí Pairwise cosine similarity ‚Üí Maximum similarity pair selection ‚Üí Aggregation ‚Üí Similarity score

- Design tradeoffs:
  - Paragraph-level vs document-level granularity: Paragraph-level provides more localized analysis but increases computational complexity
  - Vector space model choice: TF-IDF provides better discriminative power but may miss semantic relationships that Word2Vec captures
  - k parameter in PL-F: Higher k values may capture more relevant information but reduce the focus on most similar pairs

- Failure signatures:
  - Low MAP and MRR scores despite high similarity scores between judgments
  - Poor performance on datasets with longer documents or more complex legal issues
  - Performance degradation when paragraphs contain multiple legal issues

- First 3 experiments:
  1. Compare paragraph-level methods with document-level baseline on a small subset of judgments to verify the core mechanism
  2. Test different values of k in the PL-F approach to find the optimal balance between precision and recall
  3. Evaluate the impact of different vector space models (TF-IDF, BOW, Word2Vec) on retrieval performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do paragraph-level similarity methods perform on legal datasets from different jurisdictions (e.g., UK, US, Australia) compared to the Indian Supreme Court dataset?
- Basis in paper: [inferred] The paper focuses solely on Indian Supreme Court judgments, leaving generalizability to other common law systems unexplored
- Why unresolved: The study does not include cross-jurisdictional datasets, so it's unclear if the findings hold universally
- What evidence would resolve it: Experiments comparing paragraph-level methods on datasets from multiple common law jurisdictions

### Open Question 2
- Question: What is the impact of paragraph segmentation quality on the performance of paragraph-level similarity methods?
- Basis in paper: [inferred] The paper assumes that paragraphs are well-defined legal issues but does not analyze how segmentation errors affect results
- Why unresolved: The paper does not address the robustness of methods to imperfect paragraph boundaries
- What evidence would resolve it: Experiments varying paragraph segmentation quality and measuring the impact on retrieval performance

### Open Question 3
- Question: How do paragraph-level methods compare to fine-tuned transformer models like Legal-BERT when trained on large-scale legal corpora?
- Basis in paper: [inferred] The paper uses pre-trained embeddings and does not explore fine-tuning legal domain models
- Why unresolved: The study does not include fine-tuned transformer baselines, leaving the relative effectiveness unexplored
- What evidence would resolve it: Direct comparison of paragraph-level methods with fine-tuned legal transformer models on the same datasets

## Limitations
- Limited to specific vector space models without exploring advanced neural approaches
- Relies on citation-based ground truth which may not perfectly align with actual legal relevance
- Focuses on Indian Supreme Court judgments, limiting generalizability to other legal systems

## Confidence
- **High confidence**: The general finding that paragraph-level methods outperform document-level baselines is well-supported by experimental results across two benchmark datasets
- **Medium confidence**: The claim about TF-IDF having superior discriminative power is supported but could benefit from comparison with more recent embedding methods
- **Medium confidence**: The effectiveness of selecting only top-k paragraph pairs is demonstrated but may not generalize to all legal domains

## Next Checks
1. Test the paragraph-level framework with transformer-based embeddings (BERT, legal-specific models) to assess whether current results extend to modern neural approaches
2. Conduct human evaluation studies to validate whether citation-based similarity truly reflects legal relevance and precedence quality
3. Apply the framework to legal judgments from different jurisdictions (e.g., US, UK, EU courts) to test generalizability across legal systems