---
ver: rpa2
title: Liveness Detection Competition -- Noncontact-based Fingerprint Algorithms and
  Systems (LivDet-2023 Noncontact Fingerprint)
arxiv_id: '2310.00659'
source_url: https://arxiv.org/abs/2310.00659
tags:
- competition
- fingerprint
- pais
- systems
- noncontact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The LivDet-2023 Noncontact Fingerprint competition benchmarked
  the state-of-the-art in presentation attack detection for contactless fingerprint
  systems. Four algorithms and three systems were tested against live fingers and
  six types of presentation attack instruments (PAIs) using both old and new smartphone
  cameras.
---

# Liveness Detection Competition -- Noncontact-based Fingerprint Algorithms and Systems (LivDet-2023 Noncontact Fingerprint)

## Quick Facts
- arXiv ID: 2310.00659
- Source URL: https://arxiv.org/abs/2310.00659
- Reference count: 24
- Key outcome: Four algorithms and three systems competed, with the winning algorithm achieving 11.35% APCER and 0.62% BPCER, while most competitors struggled with novel synthetic PAIs.

## Executive Summary
The LivDet-2023 Noncontact Fingerprint competition benchmarked state-of-the-art presentation attack detection algorithms and systems for contactless fingerprint recognition. The competition tested four algorithms and three systems against live fingers and six types of presentation attack instruments (PAIs) using both old and new smartphone cameras. The evaluation revealed that while top performers achieved reasonable accuracy on known attack types, most algorithms struggled significantly with novel synthetic PAIs, highlighting vulnerabilities to sophisticated injection attacks.

## Method Summary
The competition employed ISO/IEC 30107-3 compliant metrics (APCER, BPCER, ACER) to evaluate algorithms and systems on a dataset containing live finger photos and six PAI types (finger photo printout, ecoflex, playdoh, wood glue, latex, and synthetic fingertip photos) captured across multiple smartphone models. Four algorithms and three systems were tested, with algorithms trained on provided datasets and systems evaluated on their ability to detect attacks in real-time. The evaluation protocol included both known PAIs from training data and novel PAIs (latex and synthetic) to assess generalization capabilities.

## Key Results
- Winning algorithm (Dermalog) achieved 11.35% APCER and 0.62% BPCER
- Winning system achieved 13.04% APCER and 1.68% BPCER
- All competitors struggled with synthetic PAIs, with most registering APCER above 98%
- Performance varied across smartphone sensors, with better results on newer cameras for moderately difficult PAIs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The competition's use of both known and novel presentation attack instruments (PAIs) reveals algorithmic weaknesses in generalizing to unseen attack types.
- Mechanism: By testing algorithms against a spectrum of PAIs—including ecoflex, playdoh, wood glue, latex, printed paper, and synthetic fingertip images—the evaluation exposes failure modes when models encounter attack types not present in training data.
- Core assumption: Models trained on a limited PAI set cannot generalize well to sophisticated or synthetic attacks.
- Evidence anchors: [abstract] "Most competitors struggled with novel synthetic PAIs, highlighting vulnerabilities to sophisticated injection attacks." [section 6.1] "However, three of the competitors did not perform well against synthetic PAIs with all of them registering APCER 98% or above." [corpus] Weak: Related papers discuss generalization but do not specifically analyze synthetic PAIs or injection attacks.
- Break condition: If synthetic PAI generation techniques become widely available for training, the generalization gap would narrow.

### Mechanism 2
- Claim: Evaluation across multiple smartphone camera sensors exposes sensor-dependent performance variability in PAD systems.
- Mechanism: By testing algorithms and systems on both old (Samsung Galaxy S9) and new (Samsung Galaxy A71) smartphone cameras, the competition reveals how hardware differences impact PAD accuracy.
- Core assumption: Different camera sensors introduce varying noise profiles, resolutions, and color characteristics that affect PAD model performance.
- Evidence anchors: [abstract] "The winning system achieved an APCER of 13.04% and a BPCER of 1.68% over all smartphones tested." [section 4.3] "All the submitted systems were tested against a new Samsung Galaxy A71 smartphone model... and a few years old Samsung Galaxy S9 model..." [corpus] Weak: Related work mentions sensor variation but does not analyze multi-sensor evaluation in PAD contexts.
- Break condition: If PAD models become sensor-agnostic through domain adaptation techniques, performance variance across devices would decrease.

### Mechanism 3
- Claim: The competition's standardized evaluation protocol enables fair comparison across diverse algorithmic approaches.
- Mechanism: By enforcing ISO/IEC 30107-3 metrics (APCER, BPCER, ACER) and a common test dataset, the competition ensures that performance differences reflect true algorithmic capability rather than evaluation inconsistencies.
- Core assumption: Without standardized metrics and datasets, PAD algorithm comparisons are unreliable.
- Evidence anchors: [section 2] "LivDet-2023 Noncontact Fingerprint Algorithm and System competition employs two basic PAD metrics for evaluation which follow the recommendations of ISO/IEC 30107-3" [section 6.1] "Team Dermalog achieved the lowest APCERaverage of 11.35% and the second lowest BPCER of 0.62% among all four competitors, which helped them win the algorithm competition category." [corpus] Weak: Related papers reference ISO standards but do not demonstrate their application in competition settings.
- Break condition: If future competitions abandon standardized metrics for proprietary scoring, fair comparison would become impossible.

## Foundational Learning

- Concept: Presentation Attack Detection (PAD) fundamentals
  - Why needed here: Understanding PAD metrics (APCER, BPCER) is essential for interpreting competition results and designing robust algorithms.
  - Quick check question: What is the difference between APCER and BPCER, and why are both important for evaluating PAD systems?

- Concept: Convolutional Neural Networks (CNNs) for image-based classification
  - Why needed here: Most top-performing algorithms use CNN architectures, so understanding their strengths and limitations is crucial.
  - Quick check question: How does a CNN's receptive field size affect its ability to detect local versus global attack patterns in fingerprint images?

- Concept: Domain adaptation and generalization in machine learning
  - Why needed here: The competition reveals that models trained on limited PAI types struggle with novel attacks, highlighting the need for better generalization techniques.
  - Quick check question: What techniques can improve a model's ability to generalize from known to unknown presentation attack types?

## Architecture Onboarding

- Component map: Input image -> Preprocessing -> Feature extraction (CNN/handcrafted) -> Classification -> Threshold application -> Decision
- Critical path: 1. Input image preprocessing and normalization 2. Feature extraction using CNN or alternative method 3. Classification and score generation 4. Threshold application and final decision 5. Metric calculation and performance reporting
- Design tradeoffs: Model complexity vs. inference speed: More complex models may achieve better accuracy but slower inference; Training data diversity vs. dataset size: Balancing breadth of PAI types with sufficient samples per type; Generalization vs. specialization: Models that perform well on known attacks may struggle with novel ones
- Failure signatures: High APCER on novel PAIs indicates poor generalization; High BPCER suggests overly aggressive attack detection; Performance degradation across different smartphone sensors indicates sensor dependency
- First 3 experiments: 1. Train a baseline CNN on the provided dataset and evaluate on known PAIs to establish performance benchmarks 2. Test the trained model on synthetic PAIs to assess generalization capability 3. Evaluate model performance across both Samsung Galaxy S9 and A71 sensors to identify hardware dependencies

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can noncontact fingerprint PAD algorithms be improved to better detect synthetic presentation attack instruments (PAIs) and injection attacks?
- Basis in paper: [explicit] The paper states that most competitors struggled with novel synthetic PAIs, with the winning algorithm achieving only 99.9% APCER against this attack type. It notes that synthetic PAIs can be classified as injection attacks rather than presentation attacks.
- Why unresolved: The poor performance against synthetic PAIs indicates that current PAD algorithms are vulnerable to sophisticated injection attacks. The winning algorithm still had a high error rate, showing room for improvement.
- What evidence would resolve it: Developing and testing new PAD algorithms specifically designed to detect synthetic PAIs and injection attacks, with significantly lower APCER rates than current approaches.

### Open Question 2
- Question: What factors contribute to the difference in PAD performance between older and newer smartphone cameras for noncontact fingerprint systems?
- Basis in paper: [explicit] The paper notes that system performance was better against lower difficulty PAIs on older smartphones but better against moderately difficult PAIs on newer smartphones. It speculates this may be due to camera sensor upgrades.
- Why unresolved: The paper only speculates on the cause of the performance differences between smartphone models. The exact factors contributing to the performance differences are not identified.
- What evidence would resolve it: Controlled experiments varying specific smartphone camera parameters (resolution, sensor type, etc.) while keeping other factors constant to isolate the impact on PAD performance.

### Open Question 3
- Question: How can noncontact fingerprint PAD datasets be expanded to include a wider variety of presentation attack instruments and attack scenarios?
- Basis in paper: [explicit] The paper states that current PAD datasets do not have a large spectrum of PAIs made according to FIDO standards. It notes the absence of standard comparison of models and results across academia and industry.
- Why unresolved: The limited variety of PAIs in current datasets restricts the ability to thoroughly test and compare PAD algorithms. The lack of standard protocols makes it difficult to benchmark performance.
- What evidence would resolve it: Creation of larger, more diverse PAD datasets following FIDO standards, with standardized evaluation protocols and public availability to enable broader research and comparison of algorithms.

## Limitations

- The competition's dataset composition and training procedures remain partially opaque, limiting full reproducibility of results
- The focus on smartphone-based capture introduces sensor-specific biases that may not represent all contactless fingerprint acquisition scenarios
- Success of top-performing systems may not translate directly to real-world deployment scenarios where attack techniques evolve continuously

## Confidence

**High Confidence**: The competition's evaluation methodology using ISO/IEC 30107-3 metrics (APCER, BPCER, ACER) is well-established in the biometric community. The reported performance metrics and the relative ranking of algorithms are likely accurate given the standardized evaluation protocol.

**Medium Confidence**: The generalizability findings regarding synthetic PAIs and sensor-dependent performance are supported by the data but may not fully capture all failure modes in practical deployments. The difficulty level assessments for different PAI types appear reasonable but could benefit from additional validation.

**Low Confidence**: The exact mechanisms behind individual algorithm successes and failures cannot be fully determined from the competition results alone. The training data composition and hyperparameter settings that led to specific performance outcomes remain largely unspecified.

## Next Checks

1. **Generalization Validation**: Test the winning algorithm (Dermalog) on an independent dataset containing PAI types not included in the LivDet-2023 dataset to verify the reported generalization capabilities. This should include both synthetic PAIs and other known attack modalities to assess robustness beyond the competition scope.

2. **Sensor Independence Assessment**: Conduct cross-sensor evaluation by training the top-performing algorithm on one smartphone model (e.g., Samsung Galaxy S9) and testing exclusively on a different model (e.g., Samsung Galaxy A71) not used during training. This will validate whether the sensor-dependent performance degradation is as significant as reported.

3. **Training Data Diversity Analysis**: Replicate the training process using varying proportions of PAI types in the training set to determine the minimum diversity required for acceptable performance on unknown PAIs. This experiment would quantify the relationship between training data composition and generalization capability reported in the findings.