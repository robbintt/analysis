---
ver: rpa2
title: Gradient Surgery for One-shot Unlearning on Generative Model
arxiv_id: '2307.04550'
source_url: https://arxiv.org/abs/2307.04550
tags:
- unlearning
- generative
- data
- influence
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method for one-shot unlearning in generative
  models by manipulating gradients to remove the influence of specific data samples.
  Inspired by multi-task learning, the approach projects gradients onto the normal
  plane of gradients to be retained, effectively nullifying the impact of unwanted
  data on the model's outputs.
---

# Gradient Surgery for One-shot Unlearning on Generative Model

## Quick Facts
- arXiv ID: 2307.04550
- Source URL: https://arxiv.org/abs/2307.04550
- Reference count: 12
- One-line primary result: Gradient projection method achieves 65-70% feature ratio reduction for unlearned classes while maintaining generation quality

## Executive Summary
This paper introduces a novel approach to one-shot unlearning in generative models by manipulating gradients to remove the influence of specific data samples. Inspired by multi-task learning techniques, the method projects gradients of forget samples onto the normal plane of retain sample gradients, effectively nullifying unwanted data influence. The approach is evaluated on VAEs trained on MNIST and CelebA datasets, demonstrating significant reductions in feature ratios while maintaining reasonable generation quality. The method outperforms existing baselines in privacy preservation, offering a simple, cost-effective solution for removing data influence from pre-trained generative models.

## Method Summary
The method operates by computing the gradient of forget sample reconstruction loss and projecting it orthogonally onto the normal plane of retain sample gradients. This orthogonal projection nullifies the component of the forget gradient that conflicts with retain gradients, thereby reducing their inner product toward zero. The approach is agnostic to removal statistics and can be applied to various unlearning scenarios including single data, classes, or features. The method uses a single-step update with fixed learning rates (5e-4 for MNIST, 1e-5 for CelebA) and is theoretically grounded in influence function minimization.

## Key Results
- Achieves 65-70% reduction in feature ratios for unlearned classes or attributes
- Maintains reasonable generation quality with acceptable FID score increases
- Outperforms existing baselines in privacy preservation while being computationally efficient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projecting forget sample gradients onto the normal plane of retain sample gradients reduces the inner product between conflicting gradients, effectively diminishing the influence of forget samples on the model.
- Mechanism: The method computes the gradient of the forget sample's reconstruction loss and projects it orthogonally onto the normal plane of the retain sample gradients. This orthogonal projection nullifies the component of the forget gradient that conflicts with retain gradients, thereby reducing their inner product toward zero.
- Core assumption: The normal plane projection sufficiently captures the geometric relationship between conflicting gradients such that minimizing their inner product approximates the desired influence removal.
- Evidence anchors:
  - [abstract]: "we propose to manipulate gradients to regularize the interplay of influence among samples by projecting gradients onto the normal plane of the gradients to be retained"
  - [section]: "we project a gradient of forget sample xf ∈ Df onto normal plane of a set of retain samples xr ∈ Dr to meet Iup,loss(xf , xr) = 0"
  - [corpus]: Weak - no direct matching evidence in neighboring papers; this appears to be the unique contribution.
- Break condition: If the gradient space of retain samples does not span a sufficient normal plane to capture the conflict, or if the projection introduces numerical instability in high-dimensional spaces.

### Mechanism 2
- Claim: Minimizing the inner product between gradients of forget and retain samples approximates removing the data influence as defined by influence functions.
- Mechanism: By reducing the dot product ∇θL(z′, ˆθ)T Σz∈Df ∇θL(z, ˆθ), the method indirectly minimizes the second-order influence of forget samples on retain samples' test loss, aligning with theoretical influence function definitions.
- Core assumption: The inner product of gradients serves as a tractable proxy for the intractable influence function computation involving Hessian inverses.
- Evidence anchors:
  - [section]: "Reducing the influence of samples z ∈ Df in training data with regard to test loss is formulated as: I′up,loss(Df , z′) → 0, which is equivalent to ∇θL(z′, ˆθ)T Σz∈Df ∇θL(z, ˆθ) → 0"
  - [section]: "we could achieve this by alleviating the conflict between two gradients ∇θL(z′, ˆθ) and ∇θL(z, ˆθ), resulting in diminishing the inner product of two gradients"
  - [corpus]: Weak - neighboring papers focus on orthogonalization but do not explicitly link inner product minimization to influence function theory.
- Break condition: If the influence function's higher-order terms dominate, making the inner product an insufficient proxy for influence removal.

### Mechanism 3
- Claim: The method's agnosticism to removal statistics allows flexible application to various unlearning scenarios (single data, class, feature).
- Mechanism: By operating directly on gradient geometry rather than statistical properties of removal data, the method can handle arbitrary subsets without retraining or dataset restructuring.
- Core assumption: Gradient conflicts capture the necessary information for unlearning regardless of the semantic or statistical nature of the forget set.
- Evidence anchors:
  - [abstract]: "Our work is agnostic to statistics of the removal samples, outperforming existing baselines"
  - [section]: "Agno stic to removal statistics and thus applied to any removals such as a single data, a class, some data feature, etc."
  - [corpus]: Moderate - neighboring papers mention statistical independence but do not emphasize this agnosticism as a core mechanism.
- Break condition: If certain removal statistics encode influence in ways not captured by gradient conflicts, the agnosticism could fail.

## Foundational Learning

- Concept: Influence functions and their application to machine unlearning
  - Why needed here: The paper's theoretical foundation relies on understanding how individual data points influence model parameters and test loss, motivating the gradient-based unlearning approach.
  - Quick check question: How does the influence function of a single data point on model parameters relate to its influence on test loss of other samples?

- Concept: Gradient projection and orthogonal decomposition in high-dimensional spaces
  - Why needed here: The core mechanism involves projecting gradients onto normal planes, requiring understanding of vector projection and orthogonality in parameter spaces.
  - Quick check question: What is the mathematical operation to project a vector onto the normal plane of another vector in n-dimensional space?

- Concept: Variational Autoencoders and generative modeling
  - Why needed here: The method is applied to pre-trained VAEs, so understanding VAE architecture, loss functions, and training dynamics is essential for implementation.
  - Quick check question: What are the key components of a VAE's loss function, and how do they differ from standard autoencoders?

## Architecture Onboarding

- Component map:
  - Pre-trained VAE model (encoder, decoder, latent space)
  - Loss computation module (reconstruction loss for forget/retain samples)
  - Gradient projection module (orthogonal projection onto normal plane)
  - Parameter update module (gradient descent with projected gradients)
  - Evaluation pipeline (feature ratio measurement, FID calculation, generation quality)

- Critical path:
  1. Load pre-trained VAE and forget/retain datasets
  2. Compute gradients for forget sample reconstruction loss
  3. Compute gradients for retain sample reconstruction loss
  4. Perform orthogonal projection of forget gradients onto retain normal plane
  5. Update model parameters using projected forget gradients
  6. Evaluate unlearning efficacy (feature ratio, FID, generation quality)

- Design tradeoffs:
  - Single-step update vs. iterative refinement: The method uses one-shot unlearning for efficiency, trading potential unlearning completeness for speed.
  - Gradient projection accuracy vs. computational cost: More precise projection methods may improve unlearning but increase computation time.
  - Feature ratio focus vs. overall generation quality: Prioritizing privacy (feature ratio) may slightly degrade general generation quality (FID).

- Failure signatures:
  - High feature ratio persistence: Indicates insufficient gradient conflict resolution
  - Degraded generation quality beyond acceptable FID increase: Suggests over-aggressive gradient manipulation
  - Unstable training dynamics: May indicate numerical issues in gradient projection

- First 3 experiments:
  1. Unlearn a single class from MNIST VAE (e.g., class '1') and verify feature ratio reduction from ~0.343 to near zero
  2. Unlearn a semantic feature from CelebA VAE (e.g., 'male') and measure feature ratio decrease and FID change
  3. Compare against baseline methods (gradient ascent, Moon et al.) on both MNIST and CelebA to validate privacy-utility tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed gradient manipulation method be extended to handle unlearning of a single data sample rather than groups with shared statistics?
- Basis in paper: [inferred] The paper mentions that unlearning a single data sample is left for future work due to challenges in evaluation, but suggests the method could be applied seamlessly.
- Why unresolved: Current evaluation metrics (feature ratio) are designed for groups sharing statistics. Unlearning a truly unique sample would require new metrics to assess if the sample's influence has been sufficiently removed without affecting overall model utility.
- What evidence would resolve it: Development and validation of evaluation metrics for single-sample unlearning, along with experiments demonstrating the method's effectiveness on truly unique samples while maintaining model performance.

### Open Question 2
- Question: What is the theoretical bound on the approximation error between the unlearned model and a model retrained from scratch without the removed data?
- Basis in paper: [inferred] The paper claims to provide "theoretical analysis for the first time in unlearning a generative model" but the specific bounds on approximation error are not detailed in the main text.
- Why unresolved: While the paper mentions theoretical grounding, it doesn't provide explicit bounds or error analysis comparing the unlearned model to a retrain-from-scratch baseline. This is crucial for understanding the method's effectiveness and limitations.
- What evidence would resolve it: Rigorous mathematical proof of bounds on the difference between the unlearned model and a retrain-from-scratch model, including analysis of how these bounds vary with model size, dataset characteristics, and the extent of data removal.

### Open Question 3
- Question: How does the proposed method perform on larger, more complex generative models beyond VAEs, such as diffusion models or GANs?
- Basis in paper: [explicit] The paper acknowledges the method's potential application to other generative models but only validates it on VAEs for MNIST and CelebA datasets.
- Why unresolved: The effectiveness of gradient manipulation techniques may vary significantly across different generative model architectures. Complex models like diffusion models or GANs have different training dynamics and loss landscapes that could impact the method's performance.
- What evidence would resolve it: Experiments applying the method to state-of-the-art generative models (e.g., Stable Diffusion, StyleGAN) on diverse datasets, with thorough evaluation of privacy preservation, utility maintenance, and computational efficiency across different model types.

## Limitations
- The theoretical connection between gradient inner product minimization and influence function theory lacks rigorous empirical validation
- Implementation details for baseline methods are sparse, preventing fair comparison
- Single-step update approach's completeness in removing influence is not quantified

## Confidence
- High confidence: The core gradient projection mechanism and its application to VAEs are clearly specified and reproducible
- Medium confidence: The privacy-utility tradeoff claims are supported by quantitative results, though baseline comparisons may not be perfectly fair
- Low confidence: The theoretical connection between gradient inner product minimization and influence function theory lacks empirical validation

## Next Checks
1. Implement the influence function computation (including Hessian inverse approximation) for a subset of forget samples and compare against the gradient projection method's effectiveness
2. Reconstruct the exact implementation details for Moon et al. 2023 and gradient ascent baselines to ensure fair comparison in feature ratio and FID metrics
3. After applying the one-shot unlearning, measure the remaining influence of forget samples on retain samples' test loss using influence functions, quantifying how much influence the projection method actually removes versus what remains