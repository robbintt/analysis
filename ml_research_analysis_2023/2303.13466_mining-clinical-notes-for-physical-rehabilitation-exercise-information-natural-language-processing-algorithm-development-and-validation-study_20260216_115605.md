---
ver: rpa2
title: 'Mining Clinical Notes for Physical Rehabilitation Exercise Information: Natural
  Language Processing Algorithm Development and Validation Study'
arxiv_id: '2303.13466'
source_url: https://arxiv.org/abs/2303.13466
tags:
- were
- clinical
- concepts
- notes
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A rule-based NLP algorithm was developed to extract physical rehabilitation
  exercise information from clinical notes of post-stroke patients. The algorithm
  was compared against several machine learning models including logistic regression,
  SVM, AdaBoost, and gradient boosting.
---

# Mining Clinical Notes for Physical Rehabilitation Exercise Information: Natural Language Processing Algorithm Development and Validation Study

## Quick Facts
- arXiv ID: 2303.13466
- Source URL: https://arxiv.org/abs/2303.13466
- Reference count: 1
- Primary result: Rule-based NLP achieved highest F1 scores on 14 of 40 tested concepts for extracting physical rehabilitation exercise information from clinical notes

## Executive Summary
This study developed and validated a rule-based NLP algorithm to extract physical rehabilitation exercise information from clinical notes of post-stroke patients. The algorithm was compared against multiple machine learning models including logistic regression, SVM, AdaBoost, and gradient boosting, as well as large language models. The rule-based approach using MedTagger achieved the highest F1 scores on most concepts, particularly excelling at detecting body side and exercise type, while gradient boosting performed best on concepts with high variability like lower extremity location and passive range of motion.

## Method Summary
The research extracted therapeutic procedure sections from clinical notes of 13,605 stroke patients, creating a manually annotated gold standard dataset of 300 sections. A rule-based NLP algorithm using MedTagger with predefined regular expressions was developed and compared against several machine learning models using bag-of-words features. The performance was evaluated across 40 concepts including exercise type, body side, location, motion plane, duration, sets, and reps, using F1 scores as the primary metric.

## Key Results
- Rule-based NLP achieved highest F1 scores on 14 of 40 concepts, particularly excelling at detecting body side and exercise type
- Gradient boosting performed best on 18 concepts, notably lower extremity location and passive range of motion
- Rule-based method efficiently handled duration, sets, and reps with F1 scores up to 0.65
- Large language models showed high recall but generally had lower precision and F1 scores compared to rule-based approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Rule-based NLP with MedTagger achieves higher precision on domain-specific concepts due to explicit pattern definitions that capture clinical terminology.
- **Mechanism:** MedTagger applies predefined regular expressions that map directly to the clinical ontology concepts (e.g., "Right Side", "Lower Extremity"), reducing false positives that machine learning models might generate from noisy feature extraction.
- **Core assumption:** The clinical notes follow a predictable structure where concept mentions are explicit and can be captured with deterministic rules.
- **Evidence anchors:**
  - [abstract] "The rule-based NLP algorithm demonstrated superior performance in most areas, particularly in detecting the 'Right Side' location with an F1 score of 0.975"
  - [section] "The rule-based NLP algorithm demonstrated superior performance in most areas, particularly in detecting the 'Right Side' location with an F1 score of 0.975, outperforming Gradient Boosting by 0.063"
  - [corpus] Weak corpus signal for this specific rule-based approach; limited evidence of similar methods in related papers
- **Break condition:** If clinical notes become highly variable in terminology or structure, rule patterns would fail to generalize, causing precision to drop.

### Mechanism 2
- **Claim:** Gradient boosting outperforms rule-based methods on concepts with high variability by learning complex feature interactions from training data.
- **Mechanism:** Gradient boosting aggregates multiple weak learners to capture subtle patterns in feature space (e.g., word embeddings, n-grams) that represent nuanced distinctions like "Passive Range of Motion" versus "Active Range of Motion".
- **Core assumption:** Training data contains sufficient examples of concept variations to learn discriminative features.
- **Evidence anchors:**
  - [abstract] "Gradient boosting performed best on 18 concepts, notably lower extremity location and passive range of motion"
  - [section] "Gradient Boosting excelled in 'Lower Extremity' location detection (F1 score: 0.978), surpassing rule-based NLP by 0.023"
  - [corpus] Related paper "Precision Rehabilitation for Patients Post-Stroke based on Electronic Health Records and Machine Learning" suggests ML methods are viable for similar domains
- **Break condition:** With sparse data for rare concepts, gradient boosting would overfit or fail to learn meaningful patterns.

### Mechanism 3
- **Claim:** Large language models (ChatGPT) achieve high recall but lower precision due to their generative nature and few-shot prompting.
- **Mechanism:** LLMs use contextual understanding from prompts to identify concept mentions, leading to high sensitivity but also false positives from overgeneralization.
- **Core assumption:** Few-shot examples in prompts are sufficient to guide the model toward correct concept boundaries.
- **Evidence anchors:**
  - [abstract] "LLM-based NLP, particularly ChatGPT with few-shot prompts, achieved high recall but generally lower precision and F1 scores"
  - [section] "preliminary results show that these prompts result in high recall scores that sometimes exceed other methods. However, precision tended to be quite low"
  - [corpus] No direct corpus evidence for this specific few-shot prompting approach; assumption based on general LLM behavior
- **Break condition:** If prompts lack sufficient negative examples or domain-specific context, recall advantage diminishes and precision worsens.

## Foundational Learning

- **Concept: Named Entity Recognition (NER)**
  - Why needed here: The rule-based MedTagger approach relies on NER to identify spans of text corresponding to clinical concepts, which is fundamental for extracting structured information from unstructured notes.
  - Quick check question: What is the difference between entity recognition and entity classification in this context?

- **Concept: Sequence Classification**
  - Why needed here: The machine learning models perform binary classification on exercise description sequences to determine which concepts are mentioned, requiring understanding of how to structure input sequences and assign labels.
  - Quick check question: How does sequence classification differ from traditional document classification in this NLP task?

- **Concept: Clinical Ontology**
  - Why needed here: The ontology defines the domain concepts (e.g., body parts, motion types) that guide both rule creation and feature engineering for ML models, ensuring consistent annotation and extraction.
  - Quick check question: Why is it important to have both enumerated and binary concept types in the ontology?

## Architecture Onboarding

- **Component map:** Data preprocessing -> Section extraction -> Annotation -> Rule-based MedTagger -> ML models (SVM, LR, AdaBoost, Gradient Boosting) -> LLM prompts (ChatGPT) -> Evaluation
- **Critical path:** Data preprocessing -> Section extraction -> Rule-based MedTagger -> ML models -> Evaluation (for comparing performance)
- **Design tradeoffs:** Rule-based methods offer high precision but require manual rule engineering; ML models generalize better but need labeled data; LLMs provide quick prototyping but struggle with precision.
- **Failure signatures:** Low F1 scores indicate either data sparsity, poor feature representation, or concept ambiguity in notes; high precision but low recall suggests overly strict rules or features.
- **First 3 experiments:**
  1. Test MedTagger rule coverage by running on a small annotated sample and measuring missed entities
  2. Train and evaluate gradient boosting on binary classification for top 10 most frequent concepts
  3. Generate and test LLM prompts for 5 concepts with balanced positive/negative examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would performance change if the dataset included clinical notes from multiple medical centers beyond UPMC?
- Basis in paper: [inferred] The authors note that their data was collected only from stroke patients at UPMC and mention that clinical notes can differ between sites and times, suggesting institutional variation might affect NLP performance.
- Why unresolved: The study only tested on UPMC data, so cross-site variation remains unmeasured.
- What evidence would resolve it: Testing the same NLP models on multi-site clinical notes and comparing performance metrics across institutions.

### Open Question 2
- Question: What is the optimal prompt engineering strategy for ChatGPT/GPT-3.5-turbo to maximize both precision and F1-score for concept extraction?
- Basis in paper: [explicit] The authors found that while few-shot prompting achieved high recall (0.84), precision remained low (0.29) with F1-scores not yet exceeding the rule-based approach, indicating room for prompt optimization.
- Why unresolved: The study used only automatic template-generated prompts and had not completed comprehensive prompt engineering experiments due to API constraints.
- What evidence would resolve it: Systematic testing of different prompt structures, context windows, and instruction phrasing with full evaluation metrics across all concepts.

### Open Question 3
- Question: Would incorporating contextual information from outside the therapeutic procedures section improve concept extraction accuracy?
- Basis in paper: [explicit] The authors note that some concepts may be mentioned more often in other sections and suggest future research could focus on including information from outside exercise descriptions.
- Why unresolved: The study only extracted and analyzed therapeutic procedures sections, leaving the potential benefit of broader context unmeasured.
- What evidence would resolve it: Comparative evaluation of NLP models using only procedure sections versus models incorporating full clinical notes, measuring performance differences for sparse concepts.

## Limitations
- Rule patterns and exact implementation details of MedTagger are not fully specified, making direct replication challenging
- Corpus evidence for this specific rule-based approach is weak, with limited signals from related papers
- LLM performance claims are based on preliminary results with limited detail on prompt engineering

## Confidence

- **High Confidence:** The comparative performance ranking (rule-based > gradient boosting > other ML models > LLM) is well-supported by F1 scores and directly measured
- **Medium Confidence:** The mechanism explanations for why rule-based methods excel at explicit concepts and ML models handle variable concepts better are reasonable but not directly tested
- **Low Confidence:** The LLM performance claims are based on preliminary results with limited detail on prompt engineering, making the underlying reasons for high recall but low precision uncertain

## Next Checks

1. Test MedTagger rule coverage on a small annotated sample to measure missed entities and validate the precision advantage
2. Run gradient boosting with varying training set sizes for rare concepts to determine if performance degradation follows expected patterns
3. Design and test LLM prompts with explicit negative examples for the same 5 concepts to measure precision improvement and validate the recall-precision tradeoff hypothesis