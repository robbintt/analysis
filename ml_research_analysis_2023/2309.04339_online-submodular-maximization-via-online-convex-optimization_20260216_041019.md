---
ver: rpa2
title: Online Submodular Maximization via Online Convex Optimization
arxiv_id: '2309.04339'
source_url: https://arxiv.org/abs/2309.04339
tags:
- regret
- functions
- setting
- online
- submodular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reduction framework that converts online
  submodular maximization problems into online convex optimization (OCO) problems,
  provided the submodular functions satisfy a "sandwich" property with concave relaxations.
  The authors focus on weighted threshold potential (WTP) functions, a broad class
  of submodular functions that includes important applications like influence maximization,
  facility location, and caching.
---

# Online Submodular Maximization via Online Convex Optimization

## Quick Facts
- arXiv ID: 2309.04339
- Source URL: https://arxiv.org/abs/2309.04339
- Reference count: 40
- Primary result: Online submodular maximization under matroid constraints can be reduced to online convex optimization via the sandwich property, achieving sublinear α-regret.

## Executive Summary
This paper presents a reduction framework that transforms online submodular maximization (OSM) problems into online convex optimization (OCO) problems, provided the submodular functions satisfy a "sandwich" property with concave relaxations. The authors focus on weighted threshold potential (WTP) functions, a broad class of submodular functions that includes influence maximization, facility location, and caching. They prove that WTP functions, when optimized over matroid constraints, satisfy the sandwich property with efficiently computable concave relaxations. This allows any OCO algorithm to be used, achieving sublinear α-regret. The framework extends to dynamic regret, optimistic learning, and bandit settings, with experiments showing superior performance over existing methods.

## Method Summary
The paper introduces a reduction framework that converts online submodular maximization (OSM) problems into online convex optimization (OCO) problems. The key insight is that certain submodular functions, specifically weighted threshold potentials (WTPs), admit concave relaxations that are themselves WTP functions. By coupling these relaxations with negatively correlated rounding schemes (e.g., swap rounding), the authors show that the sandwich property holds, allowing any OCO algorithm to be transformed into an OSM algorithm with sublinear α-regret. The framework is extended to dynamic regret, optimistic learning, and bandit settings by adapting OCO algorithms accordingly.

## Key Results
- The sandwich property is proven for weighted threshold potential (WTP) functions under matroid constraints.
- Any OCO algorithm can be transformed into an OSM algorithm with sublinear α-regret via the reduction framework.
- The framework extends to dynamic regret, optimistic learning, and bandit settings with appropriate OCO algorithms.
- Experiments show the proposed algorithms outperform existing methods in terms of both regret and computational efficiency.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weighted threshold potential (WTP) functions can be reduced to online convex optimization (OCO) by exploiting concave relaxations and negative correlation rounding.
- Mechanism: WTP functions admit concave relaxations that are themselves WTP functions. These relaxations are coupled with a negatively correlated rounding scheme (e.g., swap rounding) to preserve approximation guarantees in expectation.
- Core assumption: The concave relaxation of a WTP function maintains the same functional form and satisfies Lipschitz continuity.
- Evidence anchors:
  - [abstract] "functions in this class admit a concave relaxation"
  - [section] "the relaxation of f is itself: it has the same functional form"
  - [corpus] "Submodular maximization ... can be reduced to online convex optimization" (from corpus neighbor abstracts)
- Break condition: If the concave relaxation cannot be computed efficiently or the rounding scheme fails to be negatively correlated, the reduction breaks.

### Mechanism 2
- Claim: Any OCO algorithm can be transformed into an OSM algorithm with sublinear α-regret via the "sandwich" property.
- Mechanism: Under Assumption 2, for every submodular function there exists a concave relaxation that upper bounds it and a rounding scheme that lower bounds it by α in expectation. This allows transferring OCO regret guarantees to α-regret guarantees.
- Core assumption: The existence of both an upper bound (concave relaxation) and a lower bound (rounding) that differ by at most factor α.
- Evidence anchors:
  - [abstract] "coupled with an appropriate rounding scheme, can be used to achieve sublinear regret"
  - [section] "Eqs. (6) and (7) imply that ˜f bounds f both from above and below, up to the approximation factor α"
  - [corpus] "Stochastic $k$-Submodular Bandits ... first sublinear $α$-regret bounds" (from corpus neighbor)
- Break condition: If the sandwich property fails (e.g., no good concave relaxation or rounding), the reduction fails.

### Mechanism 3
- Claim: The reduction framework extends to dynamic, optimistic, and bandit settings by adapting OCO algorithms accordingly.
- Mechanism: Dynamic regret is handled by OCO algorithms with dynamic regret guarantees; optimistic learning uses prediction information in OCO updates; bandit setting uses gradient estimation techniques (e.g., perturbation) to convert bandit feedback to OCO gradients.
- Core assumption: OCO algorithms exist with the required regret guarantees for each setting.
- Evidence anchors:
  - [abstract] "our reduction extends to many different versions of the online learning problem"
  - [section] "We show our reduction also extends to the dynamic regret and optimistic settings"
  - [corpus] "Online Two-Stage Submodular Maximization" (from corpus neighbor, suggesting extensions)
- Break condition: If the required OCO algorithms don't exist or their regret guarantees are too weak, the extension fails.

## Foundational Learning

- Concept: Submodularity and matroid constraints
  - Why needed here: The reduction relies on matroid structure for negatively correlated rounding and WTP functions are submodular by definition.
  - Quick check question: What is the defining inequality for submodularity, and how does it relate to the greedy algorithm's performance?

- Concept: Online convex optimization and regret
  - Why needed here: The reduction framework converts OSM to OCO, so understanding OCO algorithms (OGA, OMA, FTRL) and their regret guarantees is essential.
  - Quick check question: What is the difference between static regret and dynamic regret in OCO, and when would you use each?

- Concept: Concave relaxations and rounding schemes
  - Why needed here: The sandwich property depends on constructing concave relaxations and applying rounding to convert fractional solutions back to integral ones.
  - Quick check question: What does it mean for a rounding scheme to be "negatively correlated," and why is this property important for the approximation guarantee?

## Architecture Onboarding

- Component map:
  WTP function class -> concave relaxation generator -> OCO policy (OGA/OMA/FTRL) -> rounding scheme -> integral decision
  Extensions: dynamic regret (OMA with path-length regularization) -> optimistic learning (OMA with predictions) -> bandit setting (gradient estimation + OCO)

- Critical path:
  1. Receive submodular function at time t
  2. Construct concave relaxation (same functional form for WTP)
  3. Apply OCO update to get fractional solution
  4. Apply negatively correlated rounding to get integral decision
  5. Observe reward and repeat

- Design tradeoffs:
  - Using the function itself as relaxation (low complexity) vs. a different relaxation (potentially better approximation)
  - Choice of OCO algorithm: OGA (simple, sublinear regret) vs. OMA (better constants, supports dynamic/optimistic) vs. FTRL (different regret structure)
  - Rounding scheme: swap rounding (general matroids) vs. pipage rounding (specific cases)

- Failure signatures:
  - Poor approximation ratio (α close to 0) -> concave relaxation not tight enough or rounding scheme not negatively correlated
  - Linear regret -> OCO algorithm not suitable for the setting (e.g., using static OCO in dynamic setting)
  - High computational cost -> concave relaxation or rounding scheme inefficient

- First 3 experiments:
  1. Implement RAOCO with OGA on a simple WTP function (e.g., weighted coverage) over a uniform matroid, verify sublinear regret.
  2. Replace OGA with OMA, measure improvement in constants and check dynamic regret guarantee.
  3. Test the bandit variant on a partition matroid, verify gradient estimation works and regret scales as expected.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the sandwich property (Assumption 2) be generalized to hold for broader classes of submodular functions beyond weighted threshold potentials (WTPs)?
- Basis in paper: [explicit] The paper proves the sandwich property for WTPs using concave relaxations and negatively correlated rounding. The authors suggest investigating the applicability to monotone submodular functions with bounded curvature and non-monotone submodular functions.
- Why unresolved: The paper focuses on WTPs, which are a strict subset of all submodular functions. Generalizing the sandwich property to other classes like submodular functions with bounded curvature or non-monotone functions remains open.
- What evidence would resolve it: A proof showing that a broader class of submodular functions admits a concave relaxation and negatively correlated rounding satisfying the sandwich property, or a counterexample demonstrating such a property cannot hold for certain classes.

### Open Question 2
- Question: How does the choice of mirror map in Online Mirror Ascent (OMA) affect the dynamic regret bound, and can this be optimized further?
- Basis in paper: [explicit] The paper extends the fixed-share update rule to matroid polytopes and uses shifted negative entropy mirror maps to achieve sublinear dynamic regret. However, the dependence on problem parameters like the matroid rank and time horizon could potentially be improved.
- Why unresolved: While the paper provides a general framework for dynamic regret with OMA, the specific choice of mirror map and its impact on regret constants is not fully explored. Different mirror maps might yield tighter bounds.
- What evidence would resolve it: A theoretical analysis comparing different mirror maps (e.g., negative entropy, Euclidean, p-norm) for OMA in terms of their dynamic regret guarantees under various problem settings, and empirical results demonstrating the impact of mirror map choice on regret performance.

### Open Question 3
- Question: Can the reduction framework be extended to bandit settings with general submodular functions and general matroid constraints?
- Basis in paper: [explicit] The paper provides a reduction for the bandit setting, but it is restricted to partition matroids and bounded submodular monotone rewards. The authors mention that it is conjectured that no sublinear regret algorithm exists for general submodular functions under general matroid constraints in the bandit setting.
- Why unresolved: The paper's bandit reduction is limited in scope, and the conjecture about the impossibility of sublinear regret for general submodular functions under general matroid constraints in the bandit setting remains unproven.
- What evidence would resolve it: A proof establishing the impossibility of sublinear regret for general submodular functions under general matroid constraints in the bandit setting, or an algorithm with sublinear regret guarantees that applies to a broader class of submodular functions and matroid constraints.

## Limitations

- The framework is limited to matroid constraints, which enable negatively correlated rounding. For more general constraints (e.g., knapsack), the rounding scheme may not exist or may be much weaker, breaking the reduction.
- The extension to bandit settings relies on gradient estimation techniques (e.g., perturbation) that are not deeply explored. Without validation, it's uncertain if the added noise and variance compromise the α-regret guarantees.
- While the paper proves that WTP functions satisfy the sandwich property, it does not empirically verify that the resulting α-regret bounds are tight in practice. The experiments show RAOCO outperforms baselines, but without comparison to optimal offline solutions or ablation studies on the rounding schemes, it's unclear if the theoretical α factor is the practical bottleneck.

## Confidence

The core claim—that online submodular maximization under matroid constraints can be reduced to online convex optimization via the sandwich property—is well-supported theoretically, with rigorous proofs for WTP functions. The mechanism is sound: WTP functions admit concave relaxations that are themselves WTP functions, and negative correlation rounding preserves approximation guarantees. However, confidence is tempered by several factors.

- **High**: Core reduction framework for full information setting with matroid constraints
- **Medium**: Extensions to dynamic and optimistic settings (assuming OCO algorithms exist with required guarantees)
- **Low**: Bandit setting without further empirical validation

## Next Checks

1. **Empirical α Factor Validation**: Measure the actual α-regret achieved by RAOCO on a WTP function (e.g., weighted coverage) over a uniform matroid, and compare it to the theoretical bound to see if the relaxation/rounding is tight in practice.
2. **Rounding Scheme Ablation**: Test RAOCO with different rounding schemes (swap vs. pipage) on a partition matroid, measuring both approximation ratio and computational cost to quantify the tradeoff.
3. **Bandit Setting Stress Test**: Implement the bandit variant of RAOCO on a small WTP function with full feedback available, comparing the estimated gradients to the true gradients to assess the quality of the perturbation-based estimation.