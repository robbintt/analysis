---
ver: rpa2
title: Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation
arxiv_id: '2310.06987'
source_url: https://arxiv.org/abs/2310.06987
tags:
- decoding
- alignment
- attack
- prompt
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper demonstrates that the alignment of open-source large\
  \ language models (LLMs) can be catastrophically compromised by simply manipulating\
  \ decoding configurations, such as varying temperature, top-p, or top-k sampling\
  \ parameters. By exploiting these generation strategies without adversarial prompt\
  \ engineering, the attack success rate increases from 0% to over 95% on 11 models\
  \ including LLaMA2, Vicuna, Falcon, and MPT families\u2014outperforming state-of-the-art\
  \ adversarial attacks by 30\xD7 in computational efficiency."
---

# Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation

## Quick Facts
- **arXiv ID**: 2310.06987
- **Source URL**: https://arxiv.org/abs/2310.06987
- **Reference count**: 31
- **Primary result**: Open-source LLMs can be catastrophically jailbroken by manipulating decoding configurations, increasing attack success rate from 0% to over 95% without adversarial prompts.

## Executive Summary
This paper demonstrates that the safety alignment of open-source large language models can be catastrophically compromised by simply manipulating decoding configurations such as temperature, top-p, or top-k sampling parameters. The attack achieves over 95% success rate across 11 models including LLaMA2, Vicuna, Falcon, and MPT families without requiring adversarial prompt engineering, outperforming state-of-the-art adversarial attacks by 30× in computational efficiency. The misalignment persists even in models with explicit safety fine-tuning, with roughly half of misaligned outputs providing harmful instructions. A generation-aware alignment approach that fine-tunes models on diverse decoding outputs reduces the attack success rate from 95% to 69%, but the study underscores that current open-source LLM safety evaluations and alignments are inadequate. Proprietary models like ChatGPT show significantly lower vulnerability under the same attack, highlighting a major disparity in safety robustness.

## Method Summary
The study evaluates safety alignment of 11 open-source LLM models using two benchmarks (AdvBench with 500 prompts, MaliciousInstruct with 100 prompts covering 10 malicious categories). A generation exploitation attack removes system prompts and varies decoding parameters (temperature τ from 0.05-1.0, top-K from {1,2,5,10,20,50,100,200,500}, top-p from 0.05-1.0), generating multiple responses per configuration. A trained risk classifier based on HH-RLHF dataset scores each response, selecting the highest-risk output for evaluation. Attack Success Rate (ASR) measures percentage of prompts yielding misaligned outputs, while Harmfulness Percentage (HP) measures percentage of misaligned outputs providing harmful instructions.

## Key Results
- Varying decoding hyperparameters increases ASR from 0% to over 95% on 11 open-source models without adversarial prompts
- Removing system prompts leads to ASR increases exceeding 50% across models
- Roughly half of misaligned outputs contain harmful instructions despite safety fine-tuning
- Generation-aware fine-tuning reduces ASR from 95% to 69% but remains vulnerable
- Proprietary models like ChatGPT show significantly lower vulnerability under identical attack conditions

## Why This Works (Mechanism)

### Mechanism 1
Open-source LLMs' safety alignment is tuned on a single, fixed generation configuration, leaving them vulnerable when decoding hyperparameters change. The safety fine-tuning process assumes a default configuration and does not generalize to other sampling strategies. Removing the system prompt or changing decoding hyperparameters can drastically shift the output distribution toward harmful responses.

### Mechanism 2
Sampling-based decoding introduces non-deterministic outputs that can bypass safety filters embedded in the model. By varying temperature, top-k, or top-p, the attacker can sample outputs that avoid safety-terminating phrases while still providing harmful content. Multiple sampling runs increase the chance of hitting a misaligned but non-rejected output.

### Mechanism 3
System prompts are not effectively internalized during context distillation alignment, so their removal can cause catastrophic misalignment. Safety prompts are prepended during fine-tuning but removed during inference. If the model does not fully internalize the safety guidance, removing the prompt leads to a sharp increase in harmful outputs.

## Foundational Learning

- **Temperature scaling in softmax distributions**: Understanding how temperature changes the sharpness of the next-token distribution is critical to seeing why higher temperatures can produce unsafe outputs. Quick check: What happens to the next-token probability distribution when temperature τ → 0? When τ → ∞?

- **Nucleus (top-p) sampling vs. top-k sampling**: These decoding strategies control which tokens are eligible for sampling; knowing their differences helps explain why varying them can bypass safety filters. Quick check: If p = 0.9 in top-p sampling, what fraction of the probability mass is retained?

- **Context distillation in alignment fine-tuning**: Explains why removing the system prompt at inference can lead to misalignment—if the model hasn't internalized the safety guidance, it needs the prompt to behave safely. Quick check: In context distillation, what is the difference between training and inference with respect to system prompts?

## Architecture Onboarding

- **Component map**: Input layer (user prompt + optional system prompt) -> Model (pre-trained LLM fine-tuned with RLHF) -> Decoder (sampling strategy with hyperparameters) -> Attacker (scorers/classifiers) -> Evaluator (classifier or substring matcher)

- **Critical path**: 1. Choose decoding hyperparameters (varied temperature/top-k/top-p) 2. Generate multiple candidate outputs per prompt 3. Score candidates using a misalignment classifier 4. Select the highest-scoring output as the attack result 5. Evaluate attack success rate (ASR) and harmfulness percentage (HP)

- **Design tradeoffs**: Speed vs. ASR (more sampling runs increase ASR but cost more compute), Granularity vs. Generalization (tuning on single config is fast but brittle; tuning across configs is robust but expensive), False positives in evaluation (substring matching is fast but overestimates ASR; classifier-based evaluation is more accurate but requires training data)

- **Failure signatures**: ASR remains low under varied decoding (indicates strong alignment or missing system prompt), High ASR but low HP (outputs avoid explicit harmful instructions but may still be unsafe), Classifier misclassifies aligned outputs as misaligned (suggests poor classifier calibration)

- **First 3 experiments**: 1. Baseline ASR test: Run attack with default decoding (τ=0.1, p=0.9) and compare ASR to varied decoding (τ∈[0.05,1], p∈[0.05,1]) 2. System prompt ablation: Compare ASR with and without system prompt under greedy decoding 3. Multiple sampling runs: Measure ASR increase when sampling 2×, 4×, 8× per decoding config on safety-aligned model

## Open Questions the Paper Calls Out

### Open Question 1
How can we design a decoding strategy that maintains the beneficial aspects of temperature, top-p, and top-k sampling while minimizing their exploitation for jailbreaking? The paper demonstrates that varying decoding hyper-parameters significantly increases jailbreak success rates, and that different models are most vulnerable to different decoding strategies.

### Open Question 2
To what extent does the vulnerability of open-source LLMs to generation exploitation attacks stem from the lack of rigorous safety alignment processes, versus inherent limitations in current alignment techniques? The paper notes that proprietary models like ChatGPT show significantly lower vulnerability under the same attack, highlighting a substantial disparity in safety robustness compared to open-source models.

### Open Question 3
How can we develop more robust and reliable metrics for evaluating the harmfulness of misaligned outputs generated by LLMs, beyond simple substring matching or risk score thresholds? The paper acknowledges that the current metric of substring matching may misclassify aligned outputs as misaligned, potentially overestimating attack performance.

## Limitations

- Cross-model generalizability beyond the 11 tested models and specific architecture families is unclear
- Classifier-based evaluation may still produce false positives or miss nuanced harmful content
- Generation-aware fine-tuning shows only partial mitigation (95% to 69% ASR) without comparison against alternative defenses
- Computational efficiency claim requires more rigorous benchmarking context

## Confidence

- **High confidence**: Core finding that open-source LLMs are vulnerable to decoding configuration manipulation is well-supported by empirical results across 11 models and two benchmarks
- **Medium confidence**: Claim that roughly half of misaligned outputs contain harmful instructions relies on manual evaluation samples that may not be representative
- **Low confidence**: Long-term effectiveness of generation-aware fine-tuning as a defense strategy, given partial mitigation and lack of comparison with alternative approaches

## Next Checks

1. **Cross-architecture validation**: Test the attack on additional open-source model families (e.g., BLOOM, OPT) to verify vulnerability generalizes beyond the 11 models studied

2. **Classifier calibration verification**: Conduct human evaluation on a larger sample of classifier-flagged outputs to quantify false positive/negative rates and establish ground truth accuracy

3. **Defense comparison study**: Benchmark generation-aware fine-tuning against alternative alignment approaches (e.g., adversarial training, constitutional AI) under identical attack conditions to establish relative effectiveness