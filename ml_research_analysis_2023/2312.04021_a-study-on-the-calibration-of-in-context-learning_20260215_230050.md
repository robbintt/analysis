---
ver: rpa2
title: A Study on the Calibration of In-context Learning
arxiv_id: '2312.04021'
source_url: https://arxiv.org/abs/2312.04021
tags:
- calibration
- arxiv
- language
- shot
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the calibration of in-context learning (ICL)
  in large language models. It finds that ICL suffers from a trade-off between accuracy
  and calibration - as more ICL examples are used, accuracy improves but calibration
  error increases.
---

# A Study on the Calibration of In-context Learning

## Quick Facts
- arXiv ID: 2312.04021
- Source URL: https://arxiv.org/abs/2312.04021
- Reference count: 13
- This paper studies the calibration of in-context learning (ICL) in large language models, finding a trade-off between accuracy and calibration as more ICL examples are used.

## Executive Summary
This paper investigates the calibration properties of in-context learning (ICL) in large language models, revealing a fundamental trade-off between accuracy and calibration. As more ICL examples are included in prompts, accuracy improves but calibration error increases, with this effect being more pronounced in larger models and those that have undergone fine-tuning. The study demonstrates that common recalibration techniques like temperature scaling are ineffective at addressing ICL miscalibration, suggesting the need for new approaches to ensure well-calibrated predictions in ICL settings.

## Method Summary
The study evaluates ICL calibration across multiple model families (LLaMA, GPT-2, and fine-tuned variants) on NLU tasks (AGNews, TREC, CB, SST-2, DBPedia) and reasoning tasks (Strategy QA, Commonsense QA, OpenBook QA, World Tree). Researchers implement ICL by prompting models with task-specific examples across different shot settings (0-shot, 1-shot, 4-shot, 8-shot), then compute accuracy, Expected Calibration Error (ECE), entropy, confidence, and representation norm. The study employs temperature scaling for recalibration attempts and systematically investigates prompt repetition strategies to understand calibration behavior.

## Key Results
- Increasing ICL examples initially worsens calibration before improving it, creating an accuracy-calibration trade-off
- Fine-tuned models (Vicuna, Alpaca, LLaMA2-Chat) show worse calibration than their base counterparts despite higher accuracy
- Common recalibration techniques like temperature scaling are ineffective at fixing ICL miscalibration

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Increasing in-context examples initially degrades calibration before improving it, creating an accuracy-calibration trade-off
- Mechanism: As more examples are added to the prompt, the model's representations shift and entropy decreases, leading to more confident but potentially overconfident predictions. The model's learned priors from pretraining conflict with the specific task distribution in the prompt, causing miscalibration.
- Core assumption: The model's internal representations and confidence scores are directly influenced by the composition and number of in-context examples
- Evidence anchors:
  - [abstract] "with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration"
  - [section] "more ICL samples lead to smaller entropy and higher confidence in most cases"
  - [corpus] Weak - neighbor papers discuss similar calibration issues but lack detailed mechanistic explanation
- Break condition: If the model's internal representation space is sufficiently regularized or the in-context examples perfectly align with the model's learned priors

### Mechanism 2
- Claim: Fine-tuning methods like instruction-tuning and RLHF worsen calibration despite improving accuracy
- Mechanism: Fine-tuning adapts the model to specific distributions and objectives that conflict with the diverse pretraining data, creating a mismatch between the model's learned confidence calibration and the ICL setting
- Core assumption: Fine-tuning fundamentally alters the model's confidence calibration learned during pretraining
- Evidence anchors:
  - [abstract] "methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration"
  - [section] "vicuna and alpaca are both more accurate but less calibrated than their LLaMA counterpart backbones"
  - [corpus] Moderate - neighbor papers mention calibration issues with fine-tuned models but lack detailed explanation
- Break condition: If fine-tuning is specifically designed to preserve calibration or the fine-tuning data closely matches ICL distribution

### Mechanism 3
- Claim: Common recalibration techniques like temperature scaling are ineffective for ICL miscalibration
- Mechanism: Temperature scaling assumes a consistent relationship between confidence scores and accuracy that doesn't hold in ICL due to the dynamic nature of prompt composition and the model's adaptation process
- Core assumption: ICL creates a unique calibration problem that differs fundamentally from standard supervised learning
- Evidence anchors:
  - [abstract] "common recalibration techniques that are widely effective such as temperature scaling provide limited gains in calibration errors"
  - [section] "none of the above strategies achieves satisfactory calibration performance, which is in contrast to the well-studied supervised learning setting"
  - [corpus] Moderate - neighbor papers discuss calibration challenges but don't deeply explore recalibration limitations
- Break condition: If a recalibration method specifically accounts for ICL's unique properties or learns adaptive scaling per prompt

## Foundational Learning

- Concept: Expected Calibration Error (ECE)
  - Why needed here: ECE is the primary metric for quantifying the alignment between predicted confidence and actual accuracy in ICL
  - Quick check question: How is ECE calculated from binned confidence predictions and their corresponding accuracies?

- Concept: In-context Learning (ICL) mechanics
  - Why needed here: Understanding how ICL works is crucial for analyzing its calibration properties and the impact of prompt design
  - Quick check question: What happens to the model's internal representations as more examples are added to an ICL prompt?

- Concept: Temperature scaling for recalibration
  - Why needed here: Temperature scaling is a common recalibration method, and understanding why it fails in ICL provides insights into the problem's nature
  - Quick check question: Why does temperature scaling work well in standard supervised learning but not in ICL?

## Architecture Onboarding

- Component map: ICL system → Prompt construction → Model inference → Confidence extraction → Calibration evaluation → Recalibration attempt
- Critical path: Prompt → Model → Confidence scores → Accuracy metrics → Calibration analysis
- Design tradeoffs: More ICL examples improve accuracy but worsen calibration; fine-tuning improves accuracy but harms calibration; complex prompts may introduce reasoning errors
- Failure signatures: High confidence on wrong predictions, inverse scaling of ECE with model size, ineffectiveness of standard recalibration methods
- First 3 experiments:
  1. Measure ECE across different numbers of ICL examples on a simple classification task
  2. Compare calibration of base vs. fine-tuned models on the same ICL task
  3. Test various recalibration methods (temperature scaling, binning) on ICL predictions and measure their effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can recalibration techniques like temperature scaling effectively reduce calibration errors in in-context learning, or are new methods needed?
- Basis in paper: [explicit] The paper states that common recalibration techniques like temperature scaling are ineffective at fixing ICL miscalibration, suggesting new methods may be needed.
- Why unresolved: The paper only explores a few recalibration strategies and finds them ineffective, but does not provide a comprehensive evaluation of all possible recalibration techniques.
- What evidence would resolve it: Further research exploring a wider range of recalibration techniques, such as isotonic regression or Bayesian methods, applied to ICL settings would help determine if effective recalibration is possible.

### Open Question 2
- Question: How do the calibration properties of ICL change as the number of in-context examples increases, and what is the underlying mechanism?
- Basis in paper: [explicit] The paper observes that with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration, and that miscalibration tends to arise in low-shot settings.
- Why unresolved: The paper provides empirical observations but does not provide a theoretical explanation for the observed calibration-accuracy trade-off in ICL.
- What evidence would resolve it: Developing a theoretical framework that models the relationship between ICL performance, calibration, and the number of in-context examples would help explain the observed phenomena.

### Open Question 3
- Question: How does fine-tuning on specialized datasets like instruction, dialog, or RLHF data impact the calibration of ICL, and why?
- Basis in paper: [explicit] The paper finds that fine-tuning using specialized data can lead to miscalibration and unreliable natural language explanations in ICL.
- Why unresolved: The paper shows that fine-tuning degrades calibration but does not investigate the underlying reasons or explore potential mitigation strategies.
- What evidence would resolve it: Analyzing the internal representations and decision boundaries of fine-tuned models compared to their base counterparts, and experimenting with different fine-tuning objectives or regularization techniques, could shed light on the causes of miscalibration and potential solutions.

## Limitations
- The study focuses primarily on English NLU and reasoning tasks with specific model families, limiting generalizability to other domains and languages
- The analysis of why temperature scaling fails is speculative without definitive theoretical grounding
- The claim that fine-tuning inherently conflicts with ICL calibration lacks investigation into specific causal mechanisms

## Confidence

**High Confidence**: The empirical observation that increasing ICL examples initially worsens then improves calibration (creating an accuracy-calibration trade-off) is well-supported by the experimental data across multiple datasets and models. The finding that fine-tuned models show worse calibration than base models on ICL tasks is also robustly demonstrated.

**Medium Confidence**: The mechanistic explanations for why calibration degrades with more examples (representation shifts, entropy changes) are plausible but not definitively proven. The assertion that standard recalibration techniques fail requires more rigorous theoretical grounding beyond empirical observation.

**Low Confidence**: The claim that fine-tuning inherently conflicts with ICL calibration due to distribution mismatch is presented without sufficient evidence about what specific aspects of fine-tuning cause the problem.

## Next Checks

1. **Cross-domain validation**: Test the accuracy-calibration trade-off on non-NLU tasks (e.g., code generation, mathematical reasoning) and non-English languages to determine if the phenomenon is fundamental to ICL or specific to the studied domains.

2. **Fine-tuning ablation study**: Systematically vary fine-tuning objectives, dataset sizes, and training procedures to isolate which aspects (e.g., instruction distribution, objective function, dataset alignment) specifically contribute to calibration degradation in ICL settings.

3. **Theoretical framework development**: Construct a formal model linking ICL prompt composition to confidence calibration, explicitly modeling how in-context examples interact with learned representations and testing predictions against empirical observations.