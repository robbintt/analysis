---
ver: rpa2
title: Anomaly Detection via Learning-Based Sequential Controlled Sensing
arxiv_id: '2312.00088'
source_url: https://arxiv.org/abs/2312.00088
tags:
- algorithm
- time
- policy
- active
- processes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses anomaly detection among a set of binary processes
  where each process can be normal or anomalous. A decision-making agent observes
  a subset of processes at each time step, with each observation incurring a cost,
  aiming to minimize delay and total sensing cost while achieving a desired confidence
  level in detection accuracy.
---

# Anomaly Detection via Learning-Based Sequential Controlled Sensing

## Quick Facts
- arXiv ID: 2312.00088
- Source URL: https://arxiv.org/abs/2312.00088
- Reference count: 30
- Primary result: Deep RL and active inference algorithms for anomaly detection balance accuracy, delay, and sensing cost while adapting to unknown process dependencies.

## Executive Summary
This paper addresses sequential anomaly detection among binary processes using controlled sensing. A decision-making agent observes subsets of processes at each time step, aiming to minimize detection delay and total sensing cost while achieving desired confidence. The problem is formulated as a Markov decision process using posterior belief vectors, with two reward functions based on Bayesian log-likelihood ratio and entropy. Two approaches are proposed: deep reinforcement learning using dueling deep Q-learning and actor-critic algorithms, and deep active inference. Experiments demonstrate the algorithms' ability to adapt to unknown statistical dependencies among processes while balancing the trade-off between detection accuracy, delay, and sensing cost.

## Method Summary
The anomaly detection problem is formulated as an MDP where the state is the posterior belief vector π(t) representing the agent's belief about each possible process state, and the action is the choice of processes to observe. The posterior is recursively updated using Bayes' rule with each new observation. Two reward functions are proposed: a Bayesian log-likelihood ratio-based reward that emphasizes accuracy and an entropy-based reward that balances accuracy and cost. The MDP objective is to maximize expected cumulative reward. Three learning algorithms are implemented: dueling deep Q-learning, actor-critic, and deep active inference. Deep active inference uses a generative model to compute a policy that minimizes expected free energy, balancing information gain and cost. The algorithms are trained on synthetic data and tested on held-out data.

## Key Results
- The proposed algorithms adapt to unknown statistical dependencies among processes while balancing detection accuracy, delay, and sensing cost
- Active inference shows greater robustness to parameter variations and better adaptation to statistical dependence patterns compared to RL algorithms
- The algorithms outperform baseline Chernoff test in terms of total sensing cost while maintaining similar detection accuracy
- Actor-critic achieves higher detection accuracy than active inference but at the cost of longer stopping times and higher total sensing cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm adapts to unknown statistical dependencies among processes by learning posterior beliefs and updating them with noisy observations.
- Mechanism: The posterior belief vector π(t) is recursively updated using Bayes' rule with each new observation, incorporating conditional independence given the true state x. This allows the agent to capture and exploit correlations between processes when computing the likelihood of each hypothesis.
- Core assumption: Observations are conditionally independent given the true state vector x, and the observation model is known (flipping probabilities pi).
- Evidence anchors:
  - [abstract] "show that our algorithms adapt to any unknown statistical dependence pattern of the processes."
  - [section II] "Each process is parameterized by a binary random variable indicating whether the process is anomalous."
  - [section III.A] "Given the previous posterior π(t-1), the action A(t) and the observation yA(t), we can exactly compute the updated posterior belief vector π(t) using (5)."
- Break condition: If the conditional independence assumption fails or the observation model is misspecified, posterior updates become inaccurate, degrading performance.

### Mechanism 2
- Claim: The reward function balances detection accuracy, delay, and sensing cost by weighting the change in information (LLR or entropy) against the cost of observations.
- Mechanism: The instantaneous reward combines the difference in information gain (ξ(π(t)) - ξ(π(t-1))) with the sensing cost ∑k∈A(t)ck, scaled by λ. Maximizing expected cumulative reward encourages the agent to choose actions that yield high information gain quickly while minimizing cost.
- Core assumption: The reward function ξ correctly measures the information gain toward identifying the true state, and λ appropriately trades off accuracy and cost.
- Evidence anchors:
  - [abstract] "utilize both a Bayesian log-likelihood ratio-based reward and an entropy-based reward."
  - [section III.B] "The reward indicates the intrinsic desirability of choosing the subset of processes as a function of the posterior belief."
  - [section III.B] "The MDP objective is to find a policy... that maximizes the long-term reward."
- Break condition: If λ is set incorrectly (too high or too low), the agent may prioritize cost over accuracy or vice versa, leading to suboptimal performance.

### Mechanism 3
- Claim: Deep active inference uses a generative model to compute a policy that minimizes expected free energy, which balances expected information gain and cost.
- Mechanism: The agent maintains a generative model φ(A(t), yA(t)|π(t-1)) that assigns higher probability to actions and observations with high reward. The variational distribution µAI is optimized to minimize the KL divergence to the generative model, which is equivalent to minimizing the expected free energy G(A(t), π(t-1)). This leads to a policy that favors actions with high expected information gain and low cost.
- Core assumption: The generative model accurately represents the relationship between posteriors, actions, and observations, and the agent can compute the expected free energy efficiently.
- Evidence anchors:
  - [abstract] "a deep active inference-based approach."
  - [section V] "The agent constructs the generative model using the EFE, obtains the optimum policy by minimizing the EFE of all the paths into the future, and chooses an action that minimizes the EFE."
  - [section V] "The EFE can be approximated as follows [29]: G(A(t), π(t-1)) ≈ E {-r(t) + G(A(t+1), π(t))}."
- Break condition: If the generative model is misspecified or the EFE approximation is poor, the learned policy may not effectively balance information gain and cost.

## Foundational Learning

- Concept: Bayesian inference and posterior updating
  - Why needed here: The algorithm relies on recursively updating the posterior belief vector π(t) using Bayes' rule with each new observation. Understanding Bayesian inference is crucial for implementing the posterior update equation and interpreting the belief states.
  - Quick check question: How does the posterior belief vector π(t) get updated with a new observation yA(t) using the previous posterior π(t-1)?

- Concept: Markov decision processes (MDPs) and reinforcement learning
  - Why needed here: The anomaly detection problem is formulated as an MDP, where the state is the posterior belief, the action is the choice of processes to observe, and the reward balances accuracy, delay, and cost. Understanding MDPs and RL algorithms (Q-learning, actor-critic, active inference) is essential for designing and implementing the learning algorithms.
  - Quick check question: What is the role of the discount factor γ in the RL algorithms, and how does it affect the agent's behavior?

- Concept: Information theory and entropy
  - Why needed here: The reward functions use the Bayesian LLR and entropy to measure the information gain towards identifying the true state. Understanding information theory concepts like entropy, KL divergence, and the relationship between entropy and information gain is important for interpreting the reward functions and the agent's behavior.
  - Quick check question: How does the entropy-based reward function encourage the agent to identify the true state quickly?

## Architecture Onboarding

- Component map:
  - Observation model -> Posterior belief vector π(t) -> Reward function -> Learning algorithm -> Policy -> Process subset A(t)

- Critical path:
  1. Initialize posterior belief vector π(0) based on prior knowledge.
  2. At each time step t:
     a. Observe the current posterior belief π(t-1).
     b. Choose a set of processes A(t) to observe based on the learned policy.
     c. Obtain noisy observations yA(t)(t) from the chosen processes.
     d. Update the posterior belief vector π(t) using Bayes' rule.
     e. Compute the reward r(t) based on the information gain and sensing cost.
     f. Update the learning algorithm (Q-network, actor/critic networks, or policy/EFE networks) using the observed reward.
  3. Stop when the posterior belief on the true state exceeds the confidence threshold πupper.

- Design tradeoffs:
  - Reward function: The LLR-based reward gives more weight to accuracy, while the entropy-based reward balances accuracy and cost more evenly. The choice depends on the application's priorities.
  - Learning algorithm: Q-learning is simpler but may require more training episodes. Actor-critic and active inference directly learn the policy but are more complex.
  - Neural network architecture: Deeper networks may capture more complex dependencies but require more data and computation.

- Failure signatures:
  - Posterior belief vector π(t) not converging to a clear hypothesis: May indicate poor learning, insufficient exploration, or model mismatch.
  - Policy not adapting to changes in process behavior: May indicate insufficient exploration or overfitting to the training data.
  - High sensing cost or long detection delay: May indicate an inappropriate tradeoff parameter λ or suboptimal learning.

- First 3 experiments:
  1. Implement the posterior belief update equation and verify it correctly incorporates new observations and the observation model.
  2. Train the Q-learning algorithm on a simple scenario (e.g., N=2 processes, no correlation) and verify it learns to choose actions that balance information gain and cost.
  3. Compare the performance of the three learning algorithms (Q-learning, actor-critic, active inference) on a more complex scenario (e.g., N=3 processes, correlated) and analyze their strengths and weaknesses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed active inference algorithm compare to the actor-critic algorithm in terms of robustness to different sensing cost and flipping probability settings?
- Basis in paper: [explicit] The paper states that the active inference algorithm is more sensitive to the flipping probabilities than the cost, and it provides shorter stopping times at the price of a higher total cost. However, it does not provide a comprehensive comparison of the two algorithms in terms of robustness to different sensing cost and flipping probability settings.
- Why unresolved: The paper only provides a limited comparison of the two algorithms in terms of robustness to different sensing cost and flipping probability settings. A more comprehensive comparison would require additional experiments and analysis.
- What evidence would resolve it: Additional experiments and analysis comparing the performance of the active inference algorithm and the actor-critic algorithm in terms of robustness to different sensing cost and flipping probability settings.

### Open Question 2
- Question: How does the proposed active inference algorithm compare to the dueling deep Q-learning algorithm in terms of performance and training time?
- Basis in paper: [explicit] The paper states that the dueling deep Q-learning algorithm requires a more prolonged training phase than the active inference algorithm. However, it does not provide a comprehensive comparison of the two algorithms in terms of performance and training time.
- Why unresolved: The paper only provides a limited comparison of the two algorithms in terms of training time. A more comprehensive comparison would require additional experiments and analysis.
- What evidence would resolve it: Additional experiments and analysis comparing the performance and training time of the active inference algorithm and the dueling deep Q-learning algorithm.

### Open Question 3
- Question: How does the proposed active inference algorithm compare to the Chernoff test in terms of performance and adaptability to different correlation parameters?
- Basis in paper: [explicit] The paper states that the Chernoff test does not adapt its stopping time and total sensing cost to the correlation parameter, while the active inference algorithm does. However, it does not provide a comprehensive comparison of the two algorithms in terms of performance and adaptability to different correlation parameters.
- Why unresolved: The paper only provides a limited comparison of the two algorithms in terms of adaptability to different correlation parameters. A more comprehensive comparison would require additional experiments and analysis.
- What evidence would resolve it: Additional experiments and analysis comparing the performance and adaptability to different correlation parameters of the active inference algorithm and the Chernoff test.

## Limitations
- The paper assumes known observation models and conditionally independent observations given the true state, which may not hold in real-world scenarios
- Performance comparisons are based on synthetic data with specific parameter settings, and generalizability to different problem configurations needs further validation
- The algorithms require multiple training episodes, which may be computationally expensive for large-scale problems or real-time applications

## Confidence
- High confidence: The MDP formulation, posterior belief update equations, and reward function definitions are clearly specified and mathematically sound
- Medium confidence: The effectiveness of the algorithms in adapting to unknown statistical dependencies and balancing accuracy, delay, and cost is demonstrated through experiments, but generalizability needs further validation
- Low confidence: The relative performance of the three learning algorithms across a wider range of problem configurations and the impact of hyperparameter choices on performance are not fully characterized

## Next Checks
1. Implement and validate the posterior belief update equation with synthetic data for a simple scenario (N=2 processes, no correlation) to ensure correct incorporation of observations and the observation model
2. Compare the performance of the three learning algorithms (Q-learning, actor-critic, active inference) on a more complex scenario (N=3 processes, correlated) with different correlation structures and cost configurations to assess their robustness and adaptability
3. Analyze the impact of hyperparameter choices (e.g., learning rates, exploration parameters, tradeoff parameter λ) on the performance of each algorithm and identify optimal settings for different problem configurations