---
ver: rpa2
title: An analysis on the effects of speaker embedding choice in non auto-regressive
  TTS
arxiv_id: '2307.09898'
source_url: https://arxiv.org/abs/2307.09898
tags:
- speaker
- embedding
- speech
- embeddings
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how different speaker embedding choices affect
  the performance of non-autoregressive multi-speaker TTS systems. The authors investigate
  six types of speaker embeddings, including jointly learned embeddings, pretrained
  TitaNet embeddings, frozen embeddings, random embeddings, and individual utterance-level
  embeddings.
---

# An analysis on the effects of speaker embedding choice in non auto-regressive TTS

## Quick Facts
- arXiv ID: 2307.09898
- Source URL: https://arxiv.org/abs/2307.09898
- Reference count: 0
- Primary result: All speaker embedding types produce similar output quality in non-autoregressive TTS, with speaker leakage occurring in core network regardless of embedding choice.

## Executive Summary
This paper analyzes how different speaker embedding choices affect non-autoregressive multi-speaker TTS systems. The authors investigate six types of speaker embeddings (jointly learned, pretrained TitaNet, frozen TitaNet, random frozen, individual utterance-level, and Mara-pretrained) in FastPitch-based TTS models. Surprisingly, all embedding types produce high-quality speech with minimal differences in output, and speaker leakage occurs within the core TTS network regardless of embedding choice. The study reveals that conditioning only variance adaptors without speaker information in the encoder does not adequately disentangle speaker identity.

## Method Summary
The authors use FastPitch architecture with six different speaker embedding strategies, training on the Romanian SW ARA corpus (18 speakers, 12h 32min total) with pretraining on SW ARA2.0 (28 speakers, 60h) and Mara corpus (single speaker, 11h). They finetune pretrained models for 800k iterations with batch size 8, evaluating speaker identity preservation through cosine similarity of TitaNet-derived embeddings between synthesized and target speech. The study also investigates zero-conditioning scenarios and t-SNE visualizations of speaker representations.

## Key Results
- All six embedding types produce speech with cosine similarity scores around 0.9, showing minimal quality differences
- Speaker leakage occurs in core network modules regardless of embedding type or conditioning location
- Zero-conditioning produces similar speaker representations across inputs, failing to generate consistent speaker identities
- Conditioning only variance adaptors without encoder speaker information cannot adequately disentangle speaker identity

## Why This Works (Mechanism)

### Mechanism 1
Speaker embeddings can be swapped without significantly changing output quality because the core network learns to extract speaker identity from acoustic features regardless of embedding input. The FastPitch encoder and decoder modules adapt their weights during training to recover speaker-specific characteristics from the input mel-spectrogram, compensating for variability in the conditioning signal.

### Mechanism 2
Speaker leakage occurs in the core network because the network learns to encode speaker identity into intermediate representations even when conditioning is limited to variance adaptors. During training, the network distributes speaker information across multiple modules (encoder, decoder, predictors) to minimize reconstruction loss, even when speaker conditioning is restricted to certain components.

### Mechanism 3
Jointly learned embeddings can represent speaker identity even when randomly initialized because the training process shapes them to be useful for the task. During multi-speaker training, the embedding layer is optimized alongside the rest of the network, converging to values that help minimize the reconstruction loss regardless of initial values.

## Foundational Learning

- Concept: Speaker embedding extraction and conditioning
  - Why needed here: Understanding how speaker representations are created and used in TTS systems is fundamental to analyzing their impact on synthesis quality
  - Quick check question: What are the differences between jointly learned, pretrained, and frozen speaker embeddings?

- Concept: Non-autoregressive TTS architecture (FastPitch)
  - Why needed here: The paper analyzes how different embedding choices affect a specific architecture, so understanding its components and training process is essential
  - Quick check question: What are the main modules in FastPitch and how do they interact with speaker conditioning?

- Concept: Speaker verification and embedding quality
  - Why needed here: The paper uses TitaNet embeddings derived from speaker verification, so understanding their characteristics and limitations is important
  - Quick check question: Why might speaker verification embeddings not perfectly disentangle speaker identity from other factors?

## Architecture Onboarding

- Component map: Text encoder -> Encoder output + Speaker embeddings -> Variance adaptors (pitch, duration, energy) -> Decoder -> Mel-spectrogram -> HiFi-GAN vocoder -> Waveform

- Critical path: Text → Text Encoder → Encoder Output + Speaker Embeddings → Variance Adaptors → Decoder → Mel-spectrogram → Vocoder → Waveform

- Design tradeoffs:
  - Jointly learned vs pretrained embeddings: Flexibility vs speaker coverage
  - Frozen vs trainable embeddings: Training stability vs adaptability
  - Conditioning location: Encoder vs variance adaptors only affects how speaker information flows through the network

- Failure signatures:
  - Poor speaker identity preservation: Embedding layer not capturing speaker characteristics or core network not using them
  - Speaker leakage in zero-conditioning: Core network encodes speaker identity even without explicit conditioning
  - Random speaker identities: Network cannot properly condition on speaker embeddings

- First 3 experiments:
  1. Train FastPitch with standard trainable embeddings and evaluate speaker similarity using cosine distance between TitaNet embeddings of synthesized and target speech
  2. Replace trainable embeddings with frozen TitaNet embeddings and compare output quality and speaker preservation
  3. Train with random frozen embeddings and analyze if the core network can compensate for poor conditioning

## Open Questions the Paper Calls Out

### Open Question 1
How can speaker leakage within the core modules of non-autoregressive TTS models be effectively reduced or eliminated? The authors found that speaker leakage is inevitable in standard training procedures, even when using different types of speaker embeddings. The underlying mechanisms of how speaker identity is encoded and transferred within the network remain unclear.

### Open Question 2
Can the zero-conditioned output of a TTS model be exploited to learn an eigen-like speaker representation that enables more efficient voice cloning? The authors investigated the zero-conditioned output and found that it does not produce a consistent speaker identity across different inputs, but suggest potential for using it as a basis for voice cloning.

### Open Question 3
What is the optimal way to incorporate external speaker embeddings into TTS models to maximize their impact on output speech quality and speaker identity preservation? Despite testing various embedding types and learning strategies, the study did not identify a clear winner in terms of output quality or speaker identity preservation.

## Limitations

- Analysis is limited to a relatively small corpus (18 speakers, 12h total) with parallel prompts
- Study focuses exclusively on FastPitch architecture, limiting generalizability to other non-autoregressive TTS systems
- Evaluation relies on TitaNet embeddings for speaker similarity measurement, which may not capture all aspects of speaker identity humans perceive

## Confidence

- **High Confidence**: All embedding types produce similar output quality (cosine similarity around 0.9) is well-supported by quantitative metrics
- **Medium Confidence**: Jointly learned embeddings can represent speaker identity even when randomly initialized requires careful interpretation
- **Low Confidence**: Conditioning only variance adaptors cannot adequately disentangle speaker identity is based on limited experimental evidence

## Next Checks

1. Replicate the embedding analysis using a different non-autoregressive TTS architecture (e.g., Glow-TTS) to determine if robustness to embedding choice is architecture-specific

2. Test the same embedding strategies on a larger, more diverse multi-speaker dataset (100+ speakers, varied speaking styles) to assess whether minimal differences persist at scale

3. Evaluate the quality of jointly learned embeddings (trained from random initialization) for speaker verification and speaker adaptation tasks to determine if they capture speaker identity useful beyond the TTS task