---
ver: rpa2
title: Dynamical Isometry based Rigorous Fair Neural Architecture Search
arxiv_id: '2307.02263'
source_url: https://arxiv.org/abs/2307.02263
tags:
- layer
- search
- module
- network
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel neural architecture search (NAS)
  algorithm based on dynamical isometry to address the fairness and interpretability
  issues in weight-sharing NAS methods. The authors propose an orthogonal initialization
  method to ensure that candidate modules achieve dynamic isometry, allowing for fair
  evaluation across different layers.
---

# Dynamical Isometry based Rigorous Fair Neural Architecture Search

## Quick Facts
- arXiv ID: 2307.02263
- Source URL: https://arxiv.org/abs/2307.02263
- Reference count: 40
- This paper introduces a novel neural architecture search (NAS) algorithm based on dynamical isometry to address fairness and interpretability issues in weight-sharing NAS methods.

## Executive Summary
This paper addresses fairness and interpretability issues in weight-sharing neural architecture search (NAS) methods by introducing a novel approach based on dynamical isometry. The authors propose an orthogonal initialization method that ensures candidate modules achieve dynamic isometry, allowing for fair evaluation across different layers. By proving that their module selection strategy guarantees well-conditioned Jacobians and estimating generalization error, they demonstrate that the method achieves state-of-the-art top-1 validation accuracy on ImageNet classification with improved training performance and stability.

## Method Summary
The method uses orthogonal weight initialization to ensure each candidate module in the supernet achieves dynamical isometry, maintaining singular values of the Jacobian close to 1. Only Batch Normalization (BN) layer parameters are trained while weights remain frozen, with BN-based indicators used to evaluate module performance. A layer-based BN indicator is added to each layer to measure its importance, enabling fair comparison across layers. The approach selects modules based on combined block and layer indicators using an evolutionary algorithm, then retrains the selected subnet for final evaluation.

## Key Results
- Achieves state-of-the-art top-1 validation accuracy on ImageNet classification
- Demonstrates improved training performance and stability compared to existing methods
- Provides rigorous fairness guarantees through well-conditioned Jacobians and estimated generalization error

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal initialization ensures that each candidate module in the supernet achieves dynamical isometry, which guarantees equal signal propagation across all layers.
- Mechanism: By decomposing randomly initialized Gaussian weights into orthogonal tensors, the input-output Jacobian matrix of each module has singular values close to 1. This ensures stable signal propagation without vanishing or exploding gradients.
- Core assumption: The activation function (tanh) behaves linearly near zero, which approximates the identity matrix behavior needed for dynamical isometry.
- Evidence anchors: "We denote λi as the i-th eigenvalue of Jl,mJT l,m... We define that the condition under which the candidate module can achieve dynamic isometric is ϕ(Jl,mJT l,m) ≈ 1, φ(Jl,mJT l,m) ≈ 0."
- Break condition: If the activation function deviates significantly from linearity near zero, or if the network width is insufficient to approximate the expected singular value distribution.

### Mechanism 2
- Claim: Batch normalization layer parameters (γ) can reliably indicate module performance when dynamical isometry is achieved.
- Mechanism: With frozen weights and orthogonal initialization, the only variable affecting signal propagation is the BN layer's γ parameter. Larger γ values indicate better signal propagation and thus better module performance.
- Core assumption: The distance between outputs of different random inputs can be controlled probabilistically when modules satisfy dynamic isometry.
- Evidence anchors: "Since the network parameters are untrained, it is almost impossible to achieve the expected state of the candidate module... By adjusting δ so that the distance of change of the output tensor approximates the theoretical distance of change of the output with maximum probability..."
- Break condition: If the module initialization fails to achieve dynamical isometry, or if the BN layer parameters are not properly optimized during training.

### Mechanism 3
- Claim: Adding layer-based BN indicators solves the problem of comparing modules across different layers.
- Mechanism: Each layer has a parallel identity connection with a BN layer that serves as a layer indicator. This indicator measures the importance of the current layer in the entire network, enabling fair comparison across layers.
- Core assumption: The layer indicator can be trained independently while the block indicators are being optimized.
- Evidence anchors: "In order to facilitate the distinction, we call BNNAS's BN-indicator as BN-based block indicator... Therefore, we add a BN-based layer indicator to every layer to measure the importance of the current layer in the entire network."
- Break condition: If the layer indicator training interferes with block indicator training, or if the identity connection doesn't properly represent the layer's contribution.

## Foundational Learning

- Concept: Dynamical isometry
  - Why needed here: Ensures stable signal propagation through deep networks by maintaining singular values of the Jacobian close to 1.
  - Quick check question: What happens to signal propagation when singular values of the Jacobian deviate significantly from 1?

- Concept: Mean field theory in neural networks
  - Why needed here: Provides theoretical framework for analyzing signal propagation in randomly initialized networks and understanding when dynamical isometry can be achieved.
  - Quick check question: How does mean field theory help predict the behavior of signal variance across network layers?

- Concept: Jacobian matrix analysis
  - Why needed here: Used to quantify how signals transform through each module and determine whether dynamical isometry is achieved.
  - Quick check question: What properties of the Jacobian matrix indicate that a network has achieved dynamical isometry?

## Architecture Onboarding

- Component map: Supernet -> Candidate modules (orthogonal initialized) -> BN layers -> BN-based block indicators -> Layer indicators -> Evolutionary algorithm -> Selected subnet

- Critical path:
  1. Orthogonal initialization of all module weights
  2. Training only BN layer parameters while keeping weights frozen
  3. Collecting BN-based indicators for each module and layer
  4. Selecting modules based on combined block and layer indicators
  5. Evaluating selected subnets on validation set

- Design tradeoffs:
  - Orthogonal initialization ensures fairness but may limit expressivity compared to trained weights
  - Training only BN parameters speeds up supernet training but may miss some architectural nuances
  - Adding layer indicators improves cross-layer comparison but increases computational overhead

- Failure signatures:
  - Poor performance despite orthogonal initialization: Indicates activation function may not behave linearly near zero
  - Inconsistent BN indicator values: Suggests modules are not achieving dynamical isometry
  - Slow training convergence: May indicate improper orthogonal decomposition or BN layer optimization

- First 3 experiments:
  1. Verify orthogonal initialization produces singular values close to 1 for simple test modules
  2. Test BN indicator reliability by comparing frozen weights with trained weights on small dataset
  3. Evaluate layer indicator effectiveness by comparing subnets selected with and without layer indicators

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between the variance of the Jacobian matrix and the stability of the signal propagation in deep neural networks?
- Basis in paper: The paper discusses the importance of the variance of the Jacobian matrix in relation to the stability of the signal propagation in deep neural networks.
- Why unresolved: The paper provides a theoretical framework for understanding this relationship but does not provide a precise mathematical formula or proof.
- What evidence would resolve it: A mathematical proof or a simulation showing the exact relationship between the variance of the Jacobian matrix and the stability of the signal propagation in deep neural networks.

### Open Question 2
- Question: How does the proposed initialization method compare to other initialization methods in terms of computational efficiency and final model performance?
- Basis in paper: The paper proposes a new initialization method based on dynamical isometry and claims that it can achieve better and more stable training performance without loss of generality. However, it does not provide a direct comparison with other initialization methods.
- Why unresolved: The paper does not provide a direct comparison with other initialization methods in terms of computational efficiency and final model performance.
- What evidence would resolve it: A comprehensive study comparing the proposed initialization method with other initialization methods in terms of computational efficiency and final model performance.

### Open Question 3
- Question: How does the proposed module selection strategy affect the diversity of the selected architectures?
- Basis in paper: The paper proposes a new module selection strategy based on the value of the BN layer's parameters. However, it does not discuss how this strategy affects the diversity of the selected architectures.
- Why unresolved: The paper does not discuss the impact of the proposed module selection strategy on the diversity of the selected architectures.
- What evidence would resolve it: A study showing the impact of the proposed module selection strategy on the diversity of the selected architectures, such as a comparison of the diversity of the architectures selected by the proposed strategy with those selected by other strategies.

## Limitations

- The fairness guarantees rely heavily on theoretical assumptions about activation function behavior near zero, which may not hold for all architectures
- The approach may not generalize well to architectures with skip connections or residual blocks where signal propagation patterns differ
- The BN-based indicators assume the supernet training captures all relevant architectural differences, which may not hold for modules with fundamentally different computational graphs

## Confidence

**High confidence**: The empirical results demonstrating improved top-1 accuracy on ImageNet and the stability improvements during training. The experimental methodology is sound and the comparisons are fair.

**Medium confidence**: The theoretical claims about dynamical isometry ensuring fair evaluation. While the mathematical framework is rigorous, the practical implementation details and their impact on real-world performance have some uncertainty.

**Low confidence**: The scalability of the approach to much larger search spaces or different network architectures beyond MobileNetV2 and ShuffleNetV2. The paper provides limited evidence for generalization to other architectures.

## Next Checks

1. **Jacobian Singular Value Distribution Analysis**: Systemively measure and report the singular value distributions of Jacobian matrices across different layers and modules in the trained supernet to verify that dynamical isometry is maintained throughout training, not just at initialization.

2. **Activation Function Sensitivity Study**: Repeat the core experiments using ReLU and other common activation functions to determine whether the dynamical isometry benefits are specific to tanh or generalize to other activation functions commonly used in modern architectures.

3. **Cross-Architecture Generalization**: Apply the proposed method to search for architectures in domains outside ImageNet classification (such as object detection or semantic segmentation) to validate whether the fairness guarantees and performance improvements transfer to different computer vision tasks.