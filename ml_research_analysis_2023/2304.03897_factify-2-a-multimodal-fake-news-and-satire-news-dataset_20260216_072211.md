---
ver: rpa2
title: 'Factify 2: A Multimodal Fake News and Satire News Dataset'
arxiv_id: '2304.03897'
source_url: https://arxiv.org/abs/2304.03897
tags:
- news
- fake
- data
- dataset
- factify
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Factify 2 presents a large-scale multimodal fake news and satire\
  \ news dataset for fact verification, addressing the need for diverse, multi-source\
  \ data in fake news detection. The dataset contains 50,000 instances spanning five\
  \ categories\u2014SupportMultimodal, SupportText, InsufficientMultimodal, InsufficientText,\
  \ and Refute\u2014each labeled based on the entailment of textual and visual data."
---

# Factify 2: A Multimodal Fake News and Satire News Dataset

## Quick Facts
- arXiv ID: 2304.03897
- Source URL: https://arxiv.org/abs/2304.03897
- Reference count: 40
- Dataset contains 50,000 instances with 5-category labeling for multimodal fake news detection

## Executive Summary
Factify 2 presents a large-scale multimodal fake news and satire news dataset for fact verification, addressing the need for diverse, multi-source data in fake news detection. The dataset contains 50,000 instances spanning five categories—Support_Multimodal, Support_Text, Insufficient_Multimodal, Insufficient_Text, and Refute—each labeled based on the entailment of textual and visual data. It expands upon Factify 1 by incorporating satirical articles and additional data sources, including tweets from major news outlets and fact-checking websites. A baseline multimodal entailment model using Vision Transformer and Sentence-BERT achieved a 65% F1 score on the test set, highlighting the dataset's utility for advancing multimodal fake news detection research.

## Method Summary
The dataset was constructed using a combination of Twitter data from verified news sources, fact-checked articles from Politifact, and satirical content from The Onion. Each instance consists of a claim-document pair with both textual and visual components. The data was labeled into five categories based on whether the claim was supported or refuted by textual evidence alone, visual evidence alone, both modalities, or neither. The baseline multimodal entailment model extracts visual features using Vision Transformer and textual features using Sentence-BERT, concatenates these representations, and passes them through an MLP classifier for 5-way classification.

## Key Results
- 50,000 instances spanning five categories of multimodal fake news verification
- Baseline multimodal model achieves 65% F1 score on test set
- Multimodal approach outperforms text-only baselines by 15-20% F1
- Dataset includes satirical articles as a new data source beyond traditional fake news

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal entailment outperforms text-only baselines by 15-20% F1
- Mechanism: Combining ViT image features with SBERT text embeddings allows joint representation learning that captures both visual and textual entailment signals, improving classification over unimodal approaches.
- Core assumption: Visual and textual entailment are complementary for fake news detection.
- Evidence anchors:
  - [abstract] "A baseline multimodal entailment model using Vision Transformer and Sentence-BERT achieved a 65% F1 score on the test set"
  - [section] "The improvement on using the Vision transformer over the ResNet model signifies the importance of images for the task"
  - [corpus] Weak evidence - only baseline results mentioned, no comparison studies
- Break condition: If visual features add noise rather than signal, or if text is sufficient for detection.

### Mechanism 2
- Claim: Adding satirical articles improves dataset realism and detection robustness
- Mechanism: Satirical content mimics real news structure but contains false claims, forcing models to learn nuanced distinctions beyond simple fact patterns.
- Core assumption: Satire presents unique challenges that standard fake news doesn't, requiring models to handle subtler forms of deception.
- Evidence anchors:
  - [abstract] "improving Factify 1 by using new data sources and adding satire articles"
  - [section] "we added them to the support category. This is because... the articles support their claim despite the claim being fake in nature"
  - [corpus] No direct evidence - mechanism inferred from dataset construction description
- Break condition: If satirical examples are too similar to real news, models may overfit to surface features.

### Mechanism 3
- Claim: Balanced five-category labeling captures the full spectrum of fake news verification
- Mechanism: Explicit separation of multimodal vs text-only entailment cases forces models to learn modality-specific patterns and handle cases where only one modality provides evidence.
- Core assumption: Fake news detection requires understanding when text alone is sufficient versus when visual context is needed.
- Evidence anchors:
  - [abstract] "each labeled based on the entailment of textual and visual data"
  - [section] "we have three broad categories - support, no-evidence, and refute, with sub-categories based on the entailment of visual and textual data"
  - [corpus] Moderate evidence - labels described but effectiveness not directly measured
- Break condition: If categories are too granular, models may struggle to learn meaningful distinctions.

## Foundational Learning

- Concept: Multimodal entailment task formulation
  - Why needed here: Fact verification requires comparing claim-document pairs across both text and image modalities
  - Quick check question: Can you explain why comparing claim and document images helps determine veracity?

- Concept: Vision Transformer feature extraction
  - Why needed here: ViT provides strong visual representations by processing image patches as tokens, outperforming traditional CNNs for this task
  - Quick check question: How does ViT's patch-based approach differ from ResNet's convolutional approach?

- Concept: Sentence-BERT for text embeddings
  - Why needed here: SBERT generates semantically meaningful sentence embeddings suitable for similarity and entailment tasks
  - Quick check question: Why use SBERT instead of raw BERT for generating claim-document similarity features?

## Architecture Onboarding

- Component map: Data pipeline → ViT image encoder → SBERT text encoder → Feature concatenation → MLP classifier → 5-way classification
- Critical path: Image-text pair → Visual/textual feature extraction → Joint representation → Classification
- Design tradeoffs: ViT provides better visual features but is computationally heavier than ResNet; SBERT is efficient but may miss fine-grained semantic details
- Failure signatures: Low multimodal accuracy but high unimodal accuracy suggests modality fusion isn't working; poor overall performance may indicate feature extraction issues
- First 3 experiments:
  1. Train text-only baseline using SBERT embeddings only
  2. Train image-only baseline using ViT features only
  3. Compare multimodal model performance against unimodal baselines to quantify benefit of feature fusion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of multimodal fact verification models change when incorporating reasoning explanations for why a particular news item is fake?
- Basis in paper: [inferred] The paper mentions that enriching the dataset with reasoning could be a future research direction.
- Why unresolved: The current dataset lacks explicit reasoning annotations for fake news detection, limiting the ability to evaluate models that leverage such explanations.
- What evidence would resolve it: Performance comparisons of fact verification models with and without reasoning explanations on a dataset that includes explicit reasoning annotations.

### Open Question 2
- Question: How does the inclusion of synthetic data, matched to the general data distribution, impact the robustness and generalization of multimodal fact verification models, particularly for the "refute" category?
- Basis in paper: [explicit] The paper suggests using synthetic data to add complexity to the refute category as a future research direction.
- Why unresolved: The current dataset does not include synthetic data, and its impact on model performance is unknown.
- What evidence would resolve it: Comparative studies evaluating model performance on the original dataset versus a dataset augmented with synthetic data, focusing on the refute category.

### Open Question 3
- Question: What are the key linguistic and visual features that distinguish satirical articles from other types of fake news, and how can these features be leveraged to improve detection accuracy?
- Basis in paper: [explicit] The paper introduces satirical articles as a new data source in Factify 2, highlighting their unique presentation of fake news.
- Why unresolved: The dataset provides examples of satirical articles, but a detailed analysis of their distinguishing features is not provided.
- What evidence would resolve it: A comprehensive analysis identifying and quantifying the linguistic and visual features that characterize satirical articles, followed by experiments demonstrating improved detection accuracy using these features.

## Limitations
- The claimed 15-20% improvement over text-only baselines lacks detailed comparative studies
- Five-category labeling effectiveness is described but not empirically validated
- Dataset coverage is limited to English-language sources and specific domains

## Confidence
- Multimodal improvement claim: Medium confidence (supported by baseline results but lacking comparative studies)
- Satirical content value: Low confidence (mechanism inferred but not empirically validated)
- Five-category labeling effectiveness: Medium confidence (described in methodology but not directly measured)

## Next Checks
1. **Comparative Baseline Validation**: Implement and evaluate multiple text-only baselines (using different embedding approaches) alongside the multimodal model to quantify the actual performance gap and validate the claimed 15-20% improvement.

2. **Category Distinction Analysis**: Conduct ablation studies removing the multimodal vs text-only category distinctions to determine if the finer granularity actually improves model performance or if a simpler three-category scheme would suffice.

3. **Cross-domain Generalization Test**: Evaluate model performance when trained on Factify 2 data but tested on external fake news datasets covering different domains or languages to assess the dataset's coverage limitations.