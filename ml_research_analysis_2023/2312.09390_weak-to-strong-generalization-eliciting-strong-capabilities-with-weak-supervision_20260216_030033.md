---
ver: rpa2
title: 'Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision'
arxiv_id: '2312.09390'
source_url: https://arxiv.org/abs/2312.09390
tags:
- weak
- strong
- page
- labels
- cited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We study whether strong models can generalize beyond weak supervision,
  which is critical for aligning future superhuman models. We finetune large language
  models with labels from weaker models across NLP tasks, chess puzzles, and reward
  modeling.
---

# Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision

## Quick Facts
- **arXiv ID**: 2312.09390
- **Source URL**: https://arxiv.org/abs/2312.09390
- **Reference count**: 40
- **Key outcome**: Strong models can consistently outperform their weak supervisors when trained on weak supervision, a phenomenon called weak-to-strong generalization.

## Executive Summary
This paper investigates whether strong models can generalize beyond weak supervision, which is critical for aligning future superhuman models. Through experiments across NLP tasks, chess puzzles, and reward modeling, the authors demonstrate that strong pretrained models consistently outperform their weak supervisors. However, naive finetuning alone is insufficient to recover full capabilities. The paper shows that simple methods like confidence losses and bootstrapping can significantly improve generalizationâ€”for example, GPT-4 finetuned with GPT-2 supervision and a confidence loss recovers near GPT-3.5-level performance on NLP tasks. These results suggest that empirical progress on aligning superhuman models is feasible today.

## Method Summary
The paper studies weak-to-strong generalization by finetuning large language models with labels from weaker models across three domains: 22 NLP benchmark datasets converted to balanced binary classification, chess puzzles with Elo-scored positions, and reward modeling using ChatGPT comparisons. The core approach involves generating weak supervision from smaller models, then training stronger models on these labels and measuring performance against ground truth. The paper introduces the performance gap recovered (PGR) metric to quantify how much of the strong model's potential is realized under weak supervision. Key improvements include adding confidence losses that encourage models to disagree with weak supervisors when appropriate, and bootstrapping through intermediate models to bridge large capability gaps.

## Key Results
- Strong models consistently outperform their weak supervisors across all three task domains (NLP, chess, reward modeling)
- Naive finetuning alone recovers only 10-20% of the potential performance gap
- Confidence losses and bootstrapping methods can recover 40-80% of the performance gap
- GPT-4 with GPT-2 supervision plus confidence loss achieves near GPT-3.5-level performance on NLP tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Strong pretrained models naturally generalize beyond weak supervision.
- **Mechanism**: The strong model already has good internal representations of alignment-relevant tasks from pretraining, so weak supervision can elicit existing knowledge rather than teaching new capabilities.
- **Core assumption**: Strong models have internally represented the alignment-relevant concepts even if they haven't been directly trained on them.
- **Evidence anchors**:
  - [abstract] "strong models consistently outperform their weak supervisors, a phenomenon we call weak-to-strong generalization"
  - [section] "strong pretrained models should already have good representations of the alignment-relevant tasks we care about"
- **Break condition**: If the strong model lacks internal representations of the target concepts, it cannot generalize beyond weak supervision.

### Mechanism 2
- **Claim**: Confidence losses help strong models disagree with weak supervision when it's wrong.
- **Mechanism**: By adding an auxiliary loss that encourages confident predictions, strong models learn to trust their own internal representations over potentially incorrect weak labels.
- **Core assumption**: Strong models can reliably identify when their own predictions differ from weak labels.
- **Evidence anchors**:
  - [abstract] "simple methods like confidence losses and bootstrapping can significantly improve generalization"
  - [section] "we want the student to learn the intent of the supervisor, but not to imitate its mistakes"
- **Break condition**: If the strong model cannot distinguish its correct predictions from incorrect ones, confidence losses may amplify errors.

### Mechanism 3
- **Claim**: Generative pretraining on task-relevant data makes concepts more salient to the model.
- **Mechanism**: Unsupervised finetuning on data related to the target task helps the model develop stronger internal representations of that task.
- **Core assumption**: Task-relevant concepts become more linearly separable in the model's representation space after generative pretraining.
- **Evidence anchors**:
  - [abstract] "when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance"
  - [section] "one way to increase the salience of a task without needing ground truth labels is to perform unsupervised finetuning with the language modeling objective on data relevant to that task"
- **Break condition**: If the task concepts are not naturally represented in the pretraining data, generative pretraining won't help.

## Foundational Learning

- **Concept**: Weak-to-strong generalization
  - Why needed here: This is the core phenomenon being studied - whether strong models can outperform weak supervisors
  - Quick check question: If a strong model is trained on labels from a weak model, what should we expect its performance to be relative to the weak model?

- **Concept**: Pretraining representations
  - Why needed here: Understanding how pretraining creates internal representations is crucial for understanding why weak-to-strong generalization works
  - Quick check question: Why might a pretrained model already know how to solve a task it hasn't been directly trained on?

- **Concept**: Confidence calibration
  - Why needed here: The confidence loss mechanism relies on the model being able to calibrate its confidence in predictions
  - Quick check question: How can we tell if a model is making confident but incorrect predictions?

## Architecture Onboarding

- **Component map**: GPT-4 family models -> Weak supervisor models (GPT-2 level and up) -> Classification heads (linear layers) -> Auxiliary confidence loss components -> Bootstrapping pipeline (intermediate models)

- **Critical path**: 
  1. Generate weak labels using smaller models
  2. Train strong model on weak labels
  3. Evaluate performance vs. ground truth
  4. Apply improvements (confidence loss, bootstrapping)
  5. Re-evaluate performance

- **Design tradeoffs**:
  - Model size vs. computational cost
  - Number of epochs vs. overfitting to weak labels
  - Strength of confidence loss vs. ignoring weak supervision entirely
  - Bootstrapping steps vs. diminishing returns

- **Failure signatures**:
  - Strong model performs worse than weak supervisor (indicates overfitting to weak errors)
  - No improvement with larger strong models (indicates ceiling in weak supervision quality)
  - Degradation with confidence loss (indicates poor calibration)
  - Bootstrapping fails to improve (indicates too large gaps between model sizes)

- **First 3 experiments**:
  1. Baseline weak-to-strong generalization: Train GPT-4 on labels from GPT-2 on NLP tasks
  2. Confidence loss ablation: Compare performance with and without auxiliary confidence loss
  3. Bootstrapping test: Train GPT-4 via intermediate models from GPT-2 to GPT-3.5

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the precise mechanism by which the auxiliary confidence loss improves weak-to-strong generalization?
- **Basis in paper**: [explicit] The paper observes that the auxiliary confidence loss reduces student-supervisor agreement and imitation of supervisor mistakes, but does not fully explain why this occurs.
- **Why unresolved**: The paper does not provide a detailed analysis of how the confidence loss interacts with the model's representations or learning dynamics to achieve this effect.
- **What evidence would resolve it**: Further experiments analyzing the model's representations before and after training with the confidence loss, and ablation studies isolating the impact of different components of the loss.

### Open Question 2
- **Question**: To what extent can weak-to-strong generalization be improved by methods beyond those explored in the paper (e.g., consistency training, data augmentation, or more advanced debiasing techniques)?
- **Basis in paper**: [inferred] The paper tests several methods but notes that none work universally, and suggests that methods from semi-supervised learning or robust finetuning literature could be promising.
- **Why unresolved**: The paper only explores a limited set of methods and does not comprehensively investigate the full space of potential approaches.
- **What evidence would resolve it**: Systematic experiments testing a wider range of methods and comparing their effectiveness across different tasks and model sizes.

### Open Question 3
- **Question**: How does the error structure of the weak supervisor impact weak-to-strong generalization, and what are the implications for aligning superhuman models?
- **Basis in paper**: [explicit] The paper shows that different types of weak supervisor errors lead to different generalization behaviors, but does not fully characterize the relationship between error structure and generalization performance.
- **Why unresolved**: The paper provides initial evidence but does not conduct a comprehensive analysis of how different error structures affect generalization, particularly in the context of aligning superhuman models.
- **What evidence would resolve it**: Controlled experiments systematically varying the error structure of the weak supervisor and measuring the impact on generalization performance, along with theoretical analysis of the underlying mechanisms.

## Limitations

- The exact architecture specifications and training hyperparameters for the GPT-4 family models are unspecified, making precise reproduction difficult.
- The theoretical underpinnings of why pretrained models can generalize beyond weak supervision are not fully explained.
- Evaluation primarily focuses on binary classification tasks, with unclear applicability to more complex, multi-class, or open-ended tasks.

## Confidence

- **High confidence**: The core empirical finding that strong models can outperform weak supervisors across multiple task types.
- **Medium confidence**: The proposed mechanisms (confidence losses, bootstrapping) for improving weak-to-strong generalization.
- **Low confidence**: The scalability of weak-to-strong generalization to superhuman levels and its applicability to alignment-critical tasks beyond the studied domains.

## Next Checks

1. **Architecture sensitivity test**: Reproduce the main results with different strong model architectures (not just GPT-4 variants) to verify that weak-to-strong generalization is architecture-agnostic.

2. **Task complexity scaling**: Test weak-to-strong generalization on increasingly complex tasks (e.g., multi-class classification, generation tasks) to identify performance ceilings.

3. **Distribution shift robustness**: Evaluate how weak-to-strong generalization performs when the test distribution differs from the training distribution, particularly for alignment-critical scenarios.