---
ver: rpa2
title: Fine-tuning pre-trained extractive QA models for clinical document parsing
arxiv_id: '2312.02314'
source_url: https://arxiv.org/abs/2312.02314
tags:
- dataset
- clinical
- system
- pre-trained
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A system was developed to automate extraction of ejection fraction
  (EF) values from echocardiogram reports for a heart failure remote patient monitoring
  program. The approach used fine-tuning of a pre-trained DistilBERT QA model on custom-labeled
  clinical text data.
---

# Fine-tuning pre-trained extractive QA models for clinical document parsing

## Quick Facts
- arXiv ID: 2312.02314
- Source URL: https://arxiv.org/abs/2312.02314
- Reference count: 6
- A system was developed to automate extraction of ejection fraction (EF) values from echocardiogram reports for a heart failure remote patient monitoring program

## Executive Summary
This paper presents a system for automating the extraction of ejection fraction values from echocardiogram reports using fine-tuned pre-trained extractive QA models. The approach leverages a DistilBERT QA model fine-tuned on custom-labeled clinical text data to answer prompts like "What is the EF percentage?" applied to OCR-converted report text. Experiments on MIMIC-IV-Note discharge summaries demonstrated significant performance improvements over pre-trained models, with exact match accuracy improving by 33-88 percentage points and F1 score by 35-91 percentage points. The deployed system reduced clinician review time by an estimated 1500 hours over 12 months.

## Method Summary
The system uses a pre-trained DistilBERT extractive QA model fine-tuned on custom-labeled clinical text data to extract ejection fraction values from echocardiogram reports. Reports are first converted to text using OCR (AWS Textract), then processed by the fine-tuned QA model which identifies text spans containing EF values. The system also performs PHI redaction before model inference and maps span predictions back to original document coordinates for highlighting. The model is trained to answer specific prompts about EF values and can handle multiple valid answer formats.

## Key Results
- Fine-tuning improved exact match accuracy by 33-88 percentage points compared to pre-trained model
- Fine-tuning improved F1 score by 35-91 percentage points compared to pre-trained model
- Deployed system reduced clinician review time by an estimated 1500 hours over 12 months

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning pre-trained QA models on domain-specific data significantly improves performance on clinical text extraction tasks.
- Mechanism: Pre-trained models like DistilBERT have general language understanding capabilities, but lack specific knowledge about clinical terminology and document structures. Fine-tuning adapts the model's parameters to the specific patterns and vocabulary of clinical text.
- Core assumption: The model can effectively transfer general language understanding skills to the specialized domain when provided with appropriate training data.
- Evidence anchors:
  - [abstract] "Experiments on MIMIC-IV-Note discharge summaries showed that fine-tuning improved exact match accuracy by 33-88 percentage points and F1 score by 35-91 percentage points compared to the pre-trained model."
  - [section] "We chose to skip the evaluation of pre-trained clinical QA models because Soni et al. (Soni and Roberts, 2020) showed in their evaluation that models fine-tuned on an open-domain dataset like SQuAD perform better on new clinical QA tasks than models fine-tuned on medical-domain datasets like CliCR (Šuster and Daelemans, 2018) and emrQA (Pampari et al., 2018)."

### Mechanism 2
- Claim: Using extractive QA format allows precise identification of relevant information spans in unstructured clinical text.
- Mechanism: The QA model architecture is designed to predict start and end positions of answer spans within the context, which maps naturally to finding specific values like EF percentages in free text.
- Core assumption: The target information (EF values) appears as explicit text spans that can be identified through span prediction rather than requiring complex reasoning or generation.
- Evidence anchors:
  - [abstract] "The model was trained to answer prompts like 'What is the EF percentage?' and then applied to OCR-converted report text."
  - [section] "At the heart of this system is a pre-trained extractive QA transformer model that is fine-tuned on custom-labeled data."

### Mechanism 3
- Claim: Combining OCR output with QA model predictions enables extraction from scanned document images.
- Mechanism: OCR converts image-based clinical reports to text, which the QA model can process. Span position predictions from the QA model are mapped back to the original document coordinates for highlighting.
- Core assumption: OCR accuracy is sufficient to preserve the information needed for the QA model to correctly identify the answer spans.
- Evidence anchors:
  - [section] "Echocardiogram reports are fetched from the EHR and then converted to text using an Optical Character Recognition (OCR) API like AWS Textract 3."
  - [section] "Since QA models output the text span containing the required EF value, index information from the span output is combined with pixel-level bounding box output from the OCR API to highlight the pixels in the original PDF."

## Foundational Learning

- Concept: Transfer learning and fine-tuning
  - Why needed here: Pre-trained models have general language understanding but need adaptation to clinical domain specifics. Fine-tuning efficiently adapts these models to specialized tasks without training from scratch.
  - Quick check question: What is the difference between training from scratch and fine-tuning a pre-trained model?

- Concept: Question Answering model architecture
  - Why needed here: Understanding how span-based QA models work is crucial for designing appropriate prompts and interpreting model outputs for clinical information extraction.
  - Quick check question: How do extractive QA models predict the start and end positions of answer spans?

- Concept: Optical Character Recognition (OCR)
  - Why needed here: Clinical documents are often scanned images rather than digital text, requiring OCR to make them processable by text-based models.
  - Quick check question: What are the main challenges of using OCR on clinical documents compared to regular text documents?

## Architecture Onboarding

- Component map: EHR -> Document retrieval -> OCR -> PHI Redaction -> QA Model -> Span-to-coordinate mapping -> Document highlighting -> Clinician review
- Critical path: Document retrieval -> OCR -> QA Model -> Answer extraction
- Design tradeoffs: Using OCR + QA vs. end-to-end visual QA models; pre-trained vs. domain-adapted models; extractive vs. generative approaches
- Failure signatures: High OCR error rates causing QA failures; model confusion on similar prompts; inability to handle document layout variations
- First 3 experiments:
  1. Test pre-trained QA model on sample clinical text to establish baseline performance
  2. Fine-tune model on small labeled dataset and compare performance metrics
  3. Integrate OCR pipeline and test end-to-end extraction from sample scanned documents

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of using a multi-modal generative model like GPT-4 for end-to-end visual QA on echocardiogram report parsing accuracy compared to the current pipeline?
- Basis in paper: [explicit] The paper states "An end-to-end visual QA model(Antol et al., 2015) or a multi-modal generative model like GPT-4(OpenAI, 2023) might address that problem, and we leave the exploration of such models for future work."
- Why unresolved: The authors did not conduct experiments with multi-modal generative models and only speculated on their potential benefits.
- What evidence would resolve it: Direct comparison of accuracy metrics between the current pipeline and a GPT-4 based approach on the same dataset.

### Open Question 2
- Question: Does domain adaptation of the base model with masked language modeling (MLM) improve performance on clinical QA tasks?
- Basis in paper: [explicit] The authors state "Also, our experiments leveraged dataset annotations by non-clinicians only. We were encouraged by the findings from Xie et al. (2022), where they found no difference in the correctness of the non-clinician and clinician annotators."
- Why unresolved: The authors chose to skip MLM domain adaptation based on prior work, but did not empirically test its impact on their specific task.
- What evidence would resolve it: Comparative experiments measuring QA performance with and without MLM domain adaptation on clinical text.

### Open Question 3
- Question: How does the performance of fine-tuned clinical QA models compare to models fine-tuned on open-domain datasets like SQuAD when evaluated on new clinical QA tasks?
- Basis in paper: [explicit] The authors cite Soni and Roberts (2020) who showed "models fine-tuned on an open-domain dataset like SQuAD perform better on new clinical QA tasks than models fine-tuned on medical-domain datasets like CliCR (Šuster and Daelemans, 2018) and emrQA (Pampari et al., 2018)."
- Why unresolved: While the authors cite prior work, they did not directly compare fine-tuned clinical vs. open-domain models in their own experiments.
- What evidence would resolve it: Head-to-head comparison of clinical and open-domain fine-tuned models on the same clinical QA benchmark.

## Limitations

- The evaluation results are based on MIMIC-IV-Note discharge summaries rather than actual echocardiogram reports, which may not fully represent real-world performance
- The paper doesn't provide quantitative metrics on OCR accuracy or how OCR errors might affect downstream QA model performance
- The labeling process for the custom dataset lacks details about inter-annotator agreement and quality control procedures

## Confidence

- High Confidence: The mechanism of fine-tuning pre-trained QA models on domain-specific data improving performance is well-supported by the experimental results on MIMIC-IV-Note and aligns with established transfer learning principles in NLP.
- Medium Confidence: The claim about the deployed system reducing clinician review time by 1500 hours over 12 months is reasonable given the performance improvements shown in experiments, but the specific implementation details and real-world validation data are not fully disclosed.
- Medium Confidence: The effectiveness of using extractive QA format for clinical information extraction is supported by the experimental results, but the paper doesn't explore alternative approaches or compare against other clinical document parsing methods.

## Next Checks

1. Test OCR Quality Impact: Run the complete pipeline (OCR → QA model) on a sample of actual echocardiogram PDFs with known ground truth EF values to measure the end-to-end accuracy and isolate the contribution of OCR errors to overall system performance.

2. Validate Domain Transfer: Evaluate the fine-tuned model on a small held-out sample of actual echocardiogram reports (not MIMIC-IV-Note) to verify that the performance improvements observed on discharge summaries generalize to the target document type.

3. Assess Annotation Quality: Review a sample of the labeled training data for potential inconsistencies or ambiguities in how EF values were annotated, particularly in cases where the same clinical information might be expressed differently across reports.