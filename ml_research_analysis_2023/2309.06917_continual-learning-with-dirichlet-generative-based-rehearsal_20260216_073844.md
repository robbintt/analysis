---
ver: rpa2
title: Continual Learning with Dirichlet Generative-based Rehearsal
arxiv_id: '2309.06917'
source_url: https://arxiv.org/abs/2309.06917
tags:
- learning
- task
- samples
- knowledge
- pseudo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses catastrophic forgetting in continual learning
  for task-oriented dialogue systems. It proposes a novel generative-based rehearsal
  method called Dirichlet Continual Learning (DCL) that uses a Dirichlet distribution
  to model the latent variable in a Conditional Variational Autoencoder, replacing
  the traditional Gaussian approach.
---

# Continual Learning with Dirichlet Generative-based Rehearsal

## Quick Facts
- arXiv ID: 2309.06917
- Source URL: https://arxiv.org/abs/2309.06917
- Reference count: 21
- One-line primary result: Novel Dirichlet-based generative rehearsal method significantly outperforms state-of-the-art baselines in continual learning for task-oriented dialogue systems.

## Executive Summary
This paper addresses catastrophic forgetting in continual learning for task-oriented dialogue systems by proposing Dirichlet Continual Learning (DCL), a novel generative-based rehearsal method. DCL replaces the traditional Gaussian latent variable in Conditional Variational Autoencoders with a Dirichlet distribution, enabling more flexible modeling of task-specific distributions. Combined with Jensen-Shannon Knowledge Distillation (JSKD) for knowledge transfer, DCL demonstrates superior performance in both intent detection and slot filling tasks across multiple datasets, effectively mitigating catastrophic forgetting while maintaining computational efficiency.

## Method Summary
DCL employs a Dirichlet-guided CVAE for pseudo-sample generation, where the latent variable follows a Dirichlet distribution instead of Gaussian. The model uses reject sampling for reparametrization and incorporates JSKD for knowledge transfer between tasks. During training, pseudo samples generated from previous tasks are mixed with current task data to update the Language Model, allowing the system to retain knowledge while learning new tasks. The approach is evaluated on multiple benchmark datasets for both intent detection and slot filling tasks.

## Key Results
- Achieves 93.73% accuracy and 93.04% LCA in intent detection, outperforming state-of-the-art baselines
- Attains 77.37% F1 and 74.49% LCA in slot filling tasks
- Demonstrates significant reduction in catastrophic forgetting across six different task learning orders
- Shows consistent performance improvements over PCLL baseline across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dirichlet distribution models latent variables more flexibly than Gaussian, capturing task-specific distributions better.
- Mechanism: Replaces symmetric Gaussian latent prior in CVAE with Dirichlet, which can model concave, convex, symmetrical, or asymmetrical distributions matching discrete task semantics.
- Core assumption: Task-specific utterances follow distributions that are not well-captured by symmetric Gaussian priors.
- Evidence anchors:
  - [abstract]: "Unlike the traditionally used Gaussian latent variable in the Conditional Variational Autoencoder (CVAE), DCL leverages the flexibility and versatility of the Dirichlet distribution to model the latent prior variable."
  - [section 3.3]: "we introduce the Dirichlet distribution, which uses a more flexible structure to approximate the prior distribution of z. The versatile forms of the Dirichlet distribution, which can be concave, convex, symmetrical, or asymmetrical, make it an appealing choice for our model."
  - [corpus]: No direct supporting citations found; mechanism is novel to this work.

### Mechanism 2
- Claim: Jensen-Shannon Knowledge Distillation (JSKD) provides more robust cross-task knowledge transfer than KL-based distillation.
- Mechanism: Uses symmetric JS divergence instead of asymmetric KL to measure similarity between teacher and student logits, avoiding infinite divergence when distributions have non-overlapping support.
- Core assumption: Knowledge transfer benefits from symmetric, bounded divergence measures that remain stable across task boundaries.
- Evidence anchors:
  - [abstract]: "We introduce Jensen-Shannon Knowledge Distillation (JSKD), a robust logit-based knowledge distillation method that enhances knowledge transfer during pseudo sample generation."
  - [section 3.5]: "JS divergence offers advantages over KL divergence: a) Its symmetry ensures consistent values regardless of comparison order... b) JS provides bounded value in [0, 1], while KL divergence spans [0, +âˆž])."
  - [corpus]: No direct supporting citations found; comparison to KL is explained but not benchmarked against existing JSKD methods.

### Mechanism 3
- Claim: Pseudo-rehearsal with task-conditioned CVAE mitigates catastrophic forgetting by generating diverse, representative samples from previous tasks.
- Mechanism: CVAE conditioned on task ID generates pseudo-samples that are mixed with current task data, allowing rehearsal without storing real examples.
- Core assumption: Generated pseudo-samples sufficiently approximate the true data distribution to maintain performance on previous tasks.
- Evidence anchors:
  - [abstract]: "This enables it to efficiently capture sentence-level features of previous tasks and effectively guide the generation of pseudo samples."
  - [section 3.2]: "The key of the DCL is to generate pseudo samples. In the proposed DCL model, we replace the Gaussian latent with the Dirichlet latent and employ reject sampling to reparametrize the Dirichlet latent variable."
  - [corpus]: No direct supporting citations; rehearsal literature exists but specific Dirichlet-CVAE approach is novel.

## Foundational Learning

- Variational Autoencoders (VAEs)
  - Why needed here: CVAE forms the generative backbone for pseudo-sample creation; understanding ELBO and KL-vanishing is critical.
  - Quick check question: What is the ELBO objective and why does KL-vanishing occur in VAEs?

- Dirichlet Distribution
  - Why needed here: Used to model the latent prior in CVAE; understanding its properties over multinomial distributions is essential.
  - Quick check question: How does the Dirichlet distribution differ from a Gaussian in terms of shape flexibility and conjugacy?

- Knowledge Distillation
  - Why needed here: JSKD is used for cross-task transfer; understanding temperature scaling and divergence measures is necessary.
  - Quick check question: Why is JS divergence preferred over KL divergence in this context?

## Architecture Onboarding

- Component map: Input (Task ID + utterance) -> CVAE Encoder (GPT-2-based) -> Latent sampling (Reject sampling from Dirichlet) -> CVAE Decoder (GPT-2-based) -> LM module (GPT-2 decoder) -> Knowledge distillation (JSKD)

- Critical path: CVAE pretrain -> pseudo-sample generation -> LM training with JSKD

- Design tradeoffs:
  - Dirichlet vs Gaussian latent: More flexible modeling vs higher computational cost
  - JSKD vs KL distillation: Bounded, symmetric vs unbounded, asymmetric
  - Pseudo-sample ratio: More samples improve retention but increase training time

- Failure signatures:
  - KL-vanishing in CVAE -> generic pseudo-samples
  - Over-specialization in JSKD -> poor generalization
  - Too few pseudo-samples -> catastrophic forgetting

- First 3 experiments:
  1. Ablation: CVAE with Dirichlet vs Gaussian latent (reproduce Table 2)
  2. Distillation: JSKD vs KL distillation performance (reproduce Table 3)
  3. Sample ratio: Vary pseudo-sample ratio and measure LCA/accuracy (reproduce Table 4)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Dirichlet latent variable approach perform in more complex dialogue domains with longer sequences or more varied intents?
- Basis in paper: [inferred] The paper shows DCL outperforms baselines on six dialogue datasets but does not test on more complex or diverse domains.
- Why unresolved: The experiments focus on relatively standard datasets (HWU, BANKING, CLINC, SNIPS, AITS, TOP), which may not fully capture the complexity of real-world dialogue systems.
- What evidence would resolve it: Testing DCL on larger, more diverse dialogue datasets with longer sequences and more complex intent structures would demonstrate its scalability and robustness.

### Open Question 2
- Question: What is the optimal number of pseudo samples to generate for different task complexities?
- Basis in paper: [explicit] The paper shows that increasing pseudo sample numbers improves performance, but does not investigate the optimal ratio for different task complexities.
- Why unresolved: The experiments only test a limited range of pseudo sample ratios (0.1, 0.2, 0.4, 0.5) and do not explore how this might vary with task complexity or domain.
- What evidence would resolve it: Conducting experiments with varying pseudo sample ratios across different task complexities and domains would reveal the optimal generation strategy.

### Open Question 3
- Question: How does the DCL model's performance compare to other generative models like GANs or diffusion models for pseudo sample generation?
- Basis in paper: [explicit] The paper mentions VAE and GAN as commonly used generative models but only implements a VAE-based approach.
- Why unresolved: The paper does not provide a direct comparison between DCL and other generative models for pseudo sample generation.
- What evidence would resolve it: Implementing and comparing DCL with other generative models like GANs or diffusion models on the same datasets would reveal the relative strengths and weaknesses of each approach.

## Limitations
- Limited theoretical justification for why Dirichlet and JSKD specifically outperform alternatives
- Underspecified implementation details including hyperparameters and reject sampling procedure
- Evaluation focused on limited set of dialogue datasets, may not generalize to more diverse domains

## Confidence
- High Confidence: Core problem formulation and general approach are well-established and correctly implemented
- Medium Confidence: Comparative results against baselines appear robust with clear improvements in key metrics
- Low Confidence: Specific advantages of Dirichlet over Gaussian and JSKD over KL lack comprehensive ablation studies and theoretical justification

## Next Checks
1. Conduct ablation study replacing Dirichlet latent variable with Gaussian and JSKD with standard KL distillation to quantify marginal benefits
2. Analyze learned Dirichlet parameters across tasks to verify they capture meaningful task-specific characteristics
3. Evaluate diversity and quality of generated pseudo-samples using metrics like Frechet Distance or human evaluation