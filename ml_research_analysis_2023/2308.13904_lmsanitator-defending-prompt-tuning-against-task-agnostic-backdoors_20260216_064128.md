---
ver: rpa2
title: 'LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors'
arxiv_id: '2308.13904'
source_url: https://arxiv.org/abs/2308.13904
tags:
- lmsanitator
- backdoor
- attack
- trigger
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Task-agnostic backdoors in pretrained models can compromise prompt-tuning
  downstream tasks. This work proposes LMSanitator, which detects and removes such
  backdoors by inverting the backdoor-induced feature outputs instead of triggers,
  leveraging prompt-tuning's frozen model property for efficient inference-time defense.
---

# LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors

## Quick Facts
- arXiv ID: 2308.13904
- Source URL: https://arxiv.org/abs/2308.13904
- Authors: 
- Reference count: 40
- Key outcome: Task-agnostic backdoors in pretrained models can compromise prompt-tuning downstream tasks. This work proposes LMSanitator, which detects and removes such backdoors by inverting the backdoor-induced feature outputs instead of triggers, leveraging prompt-tuning's frozen model property for efficient inference-time defense. LMSanitator achieves 92.8% detection accuracy on 960 models and reduces attack success rate to under 1% in most cases without modifying model parameters.

## Executive Summary
This paper addresses the vulnerability of prompt-tuning to task-agnostic backdoors in pretrained language models. Task-agnostic backdoors can compromise downstream tasks by causing the model to output predefined attack vectors when triggered, regardless of the specific task. LMSanitator is proposed as a defense mechanism that detects and removes these backdoors by inverting the backdoor-induced feature outputs instead of the triggers themselves. The method leverages the frozen model property of prompt-tuning to efficiently defend against backdoors during inference without modifying model parameters.

## Method Summary
LMSanitator is a three-step defense mechanism designed to detect and remove task-agnostic backdoors in pretrained models used for prompt-tuning. The first step, PV mining, involves inverting the predefined attack vectors (PVs) of the backdoors using an iterative approach with distance loss and diversity loss. The second step, PV filtering, removes illegal PVs based on thresholds related to out-of-range soft prompts and high diversity loss. The final step, PV monitoring, detects and removes triggers during inference using a sign-based method that compares the model's output to the PV set. LMSanitator achieves this without modifying the model parameters, making it efficient for inference-time defense.

## Key Results
- LMSanitator achieves 92.8% detection accuracy on 960 models with various backdoors.
- The method reduces attack success rate to under 1% in most cases after defense.
- LMSanitator maintains F1 scores and classification accuracy for downstream tasks, demonstrating minimal impact on normal functionality.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-agnostic backdoors cause the pretrained model's feature outputs to act as outliers in the feature space when triggered.
- Mechanism: The backdoor causes the model's output to converge to a predefined attack vector (PV) when a trigger is present, making it distinct from normal outputs.
- Core assumption: The attacker's PV is an outlier in the feature space and can be distinguished from normal outputs.
- Evidence anchors:
  - [abstract]: "Instead of directly inverting the triggers, LMSanitator aims to invert the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors..."
  - [section 4.2]: "Observation I: Let f (·; θ∗) denote a backdoored language model, x denotes a clean sentence, and t denotes a trigger... disθ∗ (x ⊕ wi,x ⊕t) ≫ disθ∗ (x ⊕ wi,x ⊕ w j)..."
  - [corpus]: Weak - corpus does not contain direct evidence for this mechanism.
- Break condition: If the attacker's PV is not an outlier in the feature space or if the defender cannot effectively distinguish between normal and triggered outputs.

### Mechanism 2
- Claim: LMSanitator can detect and remove triggers by monitoring the similarity between the pretrained model's output and the found PVs.
- Mechanism: LMSanitator uses a sign-based trigger detection method that counts the number of matching signs between the feature vector output and the PV sign set.
- Core assumption: The sign distribution of backdoored outputs is consistent with the PV, while clean outputs have a random sign distribution.
- Evidence anchors:
  - [section 4.4]: "We propose a more efficient sign-based trigger detection method... If Nmatch exceeds a specific value Tmatch... the monitor considers that the input contains a trigger."
  - [section 4.4]: "We find that in the prompt-tuning model, placing the monitor on the output side is also effective. Immuning to catastrophic forgetting, language models in prompt-tuning models output very close to PVs when the input contains a trigger."
  - [corpus]: Weak - corpus does not contain direct evidence for this mechanism.
- Break condition: If the attacker can design triggers that do not cause the model's output to match the PV sign set or if the defender cannot effectively distinguish between normal and triggered outputs.

### Mechanism 3
- Claim: LMSanitator's fuzz training and adaptive learning rate mechanisms improve the efficiency of PV inversion.
- Mechanism: Fuzz training uses different random seeds and a test dataset to enhance inverse training, while adaptive learning rate adjusts the learning rate based on gradients.
- Core assumption: Fuzz training and adaptive learning rate can effectively prevent the model from converging to already found PVs and improve convergence speed.
- Evidence anchors:
  - [section 4.2]: "To find as many PVs as possible instead of converging to the same PV all the time, we add an extra loss term— path loss... Path loss increases as the output of the target model moves closer to c."
  - [section 4.2]: "We adjust the learning rate according to gradients... When the gradient of one parameter is larger than a threshold Tgrad, we reset lr to 0.01lr0."
  - [section 5.5]: "We observe that removing path loss increases the number of convergences needed to find three unique PVs... This indicates that path loss can effectively prevent the model from converging to PVs already found."
- Break condition: If the attacker can design triggers that do not cause the model's output to match the PV sign set or if the defender cannot effectively distinguish between normal and triggered outputs.

## Foundational Learning

- Concept: Task-agnostic backdoors
  - Why needed here: Understanding task-agnostic backdoors is crucial for comprehending the problem LMSanitator aims to solve and the mechanism it employs.
  - Quick check question: What is the main difference between task-agnostic backdoors and task-specific backdoors?
- Concept: Prompt-tuning
  - Why needed here: Prompt-tuning is the target application of LMSanitator, and understanding its properties is essential for appreciating the solution's design choices.
  - Quick check question: How does prompt-tuning differ from fine-tuning in terms of parameter updates and storage requirements?
- Concept: Backdoor detection and removal techniques
  - Why needed here: LMSanitator is a defense mechanism against backdoors, and familiarity with existing detection and removal techniques is necessary to understand its novelty and effectiveness.
  - Quick check question: What are the main challenges in detecting and removing backdoors in NLP models compared to computer vision models?

## Architecture Onboarding

- Component map: PV mining -> PV filtering -> PV monitoring
- Critical path: PV mining -> PV filtering -> PV monitoring
- Design tradeoffs:
  - LMSanitator trades off some computational overhead for the ability to detect and remove task-agnostic backdoors without modifying model parameters.
  - The choice of using sign-based trigger detection instead of direct output comparison trades off some detection accuracy for efficiency.
- Failure signatures:
  - If LMSanitator cannot find any PVs after PV mining and filtering, it indicates that the target model is likely clean.
  - If the ASR remains high after applying LMSanitator's defense, it may indicate that the attack is adaptive or that the PV set is incomplete.
- First 3 experiments:
  1. Test LMSanitator's backdoor detection accuracy on a set of clean and backdoored models.
  2. Evaluate LMSanitator's PV searching capability on models with known PVs.
  3. Assess LMSanitator's end-to-end defense effectiveness by measuring the ASR before and after applying the defense.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LMSanitator's performance scale with increasing model size beyond the tested range of RoBERTa and BERT variants?
- Basis in paper: [inferred] The paper demonstrates LMSanitator on RoBERTa-base/large and BERT-base/large models, but does not test on the largest available models (e.g., GPT-3, PaLM) which contain significantly more parameters.
- Why unresolved: The authors note that larger models take more time for backdoor detection and PV searching, but do not provide empirical results for models with more than 24 attention layers or hidden sizes exceeding 1024.
- What evidence would resolve it: Empirical evaluation of LMSanitator on models with 50+ attention layers and hidden sizes of 4096+ parameters, including runtime measurements and detection accuracy.

### Open Question 2
- Question: What is the theoretical upper bound on the number of PVs that can be simultaneously injected into a single model before LMSanitator's detection capabilities degrade?
- Basis in paper: [explicit] The paper tests models with 6 PVs per model but does not explore scenarios with 10+, 20+, or 50+ PVs. The adaptive attack section mentions "fewer triggers" but does not systematically test increasing PV counts.
- Why unresolved: The paper shows that LMSanitator can find 239 out of 252 PVs in 42 models, but does not establish when the method would fail if attackers injected substantially more PVs.
- What evidence would resolve it: Systematic testing of LMSanitator's detection accuracy and PV searching completeness as the number of injected PVs increases from 6 to 50+ in various model architectures.

### Open Question 3
- Question: How does LMSanitator perform against task-agnostic backdoors that target intermediate layers rather than just the output layer?
- Basis in paper: [inferred] The paper focuses on backdoors that map inputs to predefined vectors at the output layer, but does not address backdoors that manipulate features at intermediate attention layers.
- Why unresolved: All experiments and threat models assume the attacker targets the final output representation, while recent research has shown that intermediate layer manipulation can also create effective backdoors.
- What evidence would resolve it: Evaluation of LMSanitator against backdoors that manipulate features at specific attention layers, measuring detection accuracy and trigger removal effectiveness.

### Open Question 4
- Question: What is the minimum clean dataset size required for LMSanitator to maintain detection accuracy above 90%?
- Basis in paper: [explicit] The paper uses 2000 clean sentences for all experiments but does not systematically vary this number to determine the minimum required for reliable detection.
- Why unresolved: While the paper states that "a small clean dataset" is sufficient, it does not establish how small this dataset can be before performance degrades significantly.
- What evidence would resolve it: Empirical testing of LMSanitator's detection accuracy and PV searching completeness across clean dataset sizes ranging from 100 to 10,000 sentences.

## Limitations
- The method's effectiveness may vary depending on the specific characteristics of the task and model architecture, as the paper primarily evaluates on sentence classification and NER tasks using BERT and RoBERTa models.
- The computational overhead introduced by LMSanitator, particularly during the PV mining phase, may pose challenges for real-world applications with limited resources.
- The method's reliance on specific properties of task-agnostic backdoors may be exploited by attackers to develop more sophisticated attack techniques that evade the defense.

## Confidence
- **Low** on the general applicability of LMSanitator across different NLP tasks and model architectures.
- **Medium** in the scalability of LMSanitator for large-scale deployment.
- **Medium** in the robustness of LMSanitator against adaptive attacks.

## Next Checks
1. Evaluate LMSanitator on a broader range of NLP tasks and model architectures, including machine translation, text summarization, and question answering, using various model architectures like GPT, T5, and ELECTRA.
2. Assess the scalability of LMSanitator on larger datasets and models, such as GLUE, SuperGLUE, and BERT-large, RoBERTa-large, to evaluate computational overhead and memory requirements.
3. Investigate the robustness of LMSanitator against adaptive attacks by designing and implementing attack strategies that aim to evade the defense mechanisms, such as manipulating the model's output distribution or introducing noise to the trigger patterns.