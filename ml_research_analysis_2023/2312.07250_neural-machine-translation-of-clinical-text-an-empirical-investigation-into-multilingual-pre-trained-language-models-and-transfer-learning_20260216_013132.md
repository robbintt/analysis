---
ver: rpa2
title: 'Neural Machine Translation of Clinical Text: An Empirical Investigation into
  Multilingual Pre-Trained Language Models and Transfer-Learning'
arxiv_id: '2312.07250'
source_url: https://arxiv.org/abs/2312.07250
tags:
- clinical
- translation
- features
- language
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares small- and extra-large multilingual pre-trained
  language models for clinical domain neural machine translation. Marian (small, 7.6M
  params) was fine-tuned on medical data, while NLLB (54B) and WMT21fb (4.7B) served
  as large models, with WMT21fb also tested for zero-shot Spanish via transfer learning.
---

# Neural Machine Translation of Clinical Text: An Empirical Investigation into Multilingual Pre-Trained Language Models and Transfer-Learning

## Quick Facts
- arXiv ID: 2312.07250
- Source URL: https://arxiv.org/abs/2312.07250
- Reference count: 4
- Small model (Marian, 7.6M params) outperformed large models (NLLB, 54B; WMT21fb, 4.7B) in clinical domain translation, and transfer learning enabled WMT21fb to translate previously unseen Spanish.

## Executive Summary
This paper compares small- and extra-large multilingual pre-trained language models for clinical domain neural machine translation. Marian (small, 7.6M params) was fine-tuned on medical data, while NLLB (54B) and WMT21fb (4.7B) served as large models, with WMT21fb also tested for zero-shot Spanish via transfer learning. Automatic metrics and expert-based human evaluation showed that the small model outperformed the large ones in clinical translation, contradicting the assumption that larger models always perform better. Transfer learning on WMT21fb successfully handled Spanish, previously unseen in its pre-training. These findings highlight the importance of domain-specific fine-tuning and high-quality data over model size, and demonstrate the potential of transfer learning for low-resource languages in healthcare NLP.

## Method Summary
The study fine-tuned three multilingual pre-trained language models—Marian (small, 7.6M params), NLLB (54B), and WMT21fb (4.7B)—on a cleaned clinical corpus (250K segments) and evaluated them on ClinSpEn-2022 test sets. Automatic metrics (BLEU, METEOR, ROUGE, SACRE BLEU, COMET) and expert-based human evaluation using the HOPE framework were used. WMT21fb was also tested for zero-shot Spanish translation via transfer learning, despite Spanish not being in its pre-training pairs. The models were compared to submissions from Optum and Huawei in the ClinSpEn-2022 shared task.

## Key Results
- Fine-tuning the small Marian model on high-quality clinical data yielded superior translation quality compared to larger models.
- Transfer learning enabled WMT21fb to translate Spanish, a language pair not seen during pre-training.
- Expert-based human evaluation confirmed that the small Marian model significantly outperformed both large models in clinical translation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning with domain-specific, high-quality data can outperform larger models that are fine-tuned on noisier, less curated datasets.
- Mechanism: Domain-specific fine-tuning enables the model to adapt to specialized terminology, context, and phrasing unique to clinical text, whereas large generic models retain general-language biases.
- Core assumption: The fine-tuning dataset is sufficiently large and representative of the target domain, and the smaller model's architecture is flexible enough to learn these domain-specific patterns.
- Evidence anchors:
  - [abstract] "our expert-based human evaluations demonstrate that the small-sized pre-trained language model (PLM) won over the other two extra-large language models by a large margin, in the clinical domain fine-tuning"
  - [section 4.1] "We carried out data cleaning and extracted around 250K pairs of segments on this language pair for domain fine-tuning of the three models."
- Break condition: If the fine-tuning dataset is too small, noisy, or not representative of the clinical domain, the performance advantage of the smaller model diminishes or reverses.

### Mechanism 2
- Claim: Transfer learning can enable a model to generate competent translations in a new language pair, even if that pair was not seen during pre-training, provided there is sufficient fine-tuning data.
- Mechanism: The model leverages shared multilingual representations and alignment learned during pre-training, allowing it to generalize to unseen language pairs through fine-tuning on a smaller, targeted dataset.
- Core assumption: The new language pair shares enough structural or lexical similarity with pre-trained pairs, and the fine-tuning corpus is large and clean enough to bootstrap the translation capability.
- Evidence anchors:
  - [abstract] "the transfer learning method works well in our experimental setting using the WMT21fb model to accommodate a new language space Spanish that was not seen at the pre-training stage within WMT21fb itself"
  - [section 3.2] "the WMT21fb model does not include Spanish in its pre-trained language pairs, while NLLB includes Spanish as a high-resource language"
- Break condition: If the new language pair is too dissimilar from pre-trained pairs or the fine-tuning corpus is too small, transfer learning fails to produce usable translations.

### Mechanism 3
- Claim: Human evaluation is a more reliable and actionable measure of translation quality than automatic metrics, especially in specialized domains.
- Mechanism: Expert evaluators can detect subtle errors, domain-specific terminology misuse, and fluency issues that automatic metrics miss or misjudge, leading to more accurate model comparison.
- Core assumption: Evaluators are trained in both the source and target languages, familiar with the clinical domain, and apply consistent evaluation criteria.
- Evidence anchors:
  - [section 5.2] "the human evaluation result clearly shows which model is the best with large score gap in-between, i.e. the Clinical-Marian with score 0.801625, followed by Clinical-NLLB and Clinical-WMT21fb with scores 0.768125 and 0.692429 respectively"
  - [section 5.2] "Results of human evaluation fully confirm initial hypothesis about the quality of outputs of different engines, which is based on initial holistic spot-check human evaluation"
- Break condition: If evaluators lack domain expertise or evaluation criteria are inconsistent, human evaluation may be biased or unreliable.

## Foundational Learning

- Concept: Multilingual pre-trained language models and their architecture (e.g., Transformer, conditional MoE)
  - Why needed here: Understanding how these models learn cross-lingual representations and how fine-tuning modifies them is essential to interpret results and design experiments.
  - Quick check question: What is the difference between standard MoE and Conditional MoE, and why is it relevant for multilingual translation?

- Concept: Transfer learning and its limitations
  - Why needed here: The paper's key contribution is demonstrating successful transfer to a new language pair; understanding the conditions under which transfer learning works (or fails) is critical.
  - Quick check question: Under what conditions can a model successfully translate a language pair it never saw during pre-training?

- Concept: Evaluation metrics for machine translation (automatic vs. human)
  - Why needed here: The paper contrasts automatic metrics with expert-based human evaluation; understanding their strengths and weaknesses is key to interpreting results.
  - Quick check question: Why might automatic metrics like BLEU or COMET fail to capture translation quality in specialized domains?

## Architecture Onboarding

- Component map:
  - Data pipeline: Corpus extraction and cleaning (250K segments), tokenization, fine-tuning
  - Models: Marian (small, 7.6M params), NLLB (large, 1.3B distilled), WMT21fb (4.7B, no Spanish pre-training)
  - Training: Fine-tuning on clinical data, transfer learning for WMT21fb
  - Evaluation: Automatic metrics (BLEU, METEOR, ROUGE, COMET) and expert-based human evaluation (HOPE)

- Critical path: Clean and prepare clinical data → Fine-tune models → Evaluate with both automatic and human metrics → Compare results

- Design tradeoffs:
  - Model size vs. domain adaptation: Larger models may be more general but less adapted to domain-specific language.
  - Data quantity vs. quality: More data may help, but noisy data can hurt performance; curated data is better.
  - Automatic vs. human evaluation: Automatic metrics are fast but unreliable in specialized domains; human evaluation is slower but more accurate.

- Failure signatures:
  - Poor performance despite large model size: Indicates domain mismatch or insufficient fine-tuning data.
  - Transfer learning failure: Suggests the new language pair is too dissimilar or fine-tuning data is insufficient.
  - Discrepancy between automatic and human evaluation: Indicates automatic metrics are not reliable for the domain.

- First 3 experiments:
  1. Fine-tune Marian on the 250K clinical dataset and evaluate on ClinSpEn-2022 test set.
  2. Fine-tune NLLB on the same dataset and compare results with Marian.
  3. Fine-tune WMT21fb (transfer learning) on the dataset and compare with both models and NLLB (Spanish version).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mechanism by which the Conditional MoE Routing layer in NLLB improves routing efficiency compared to the original MoE in WMT21fb?
- Basis in paper: [explicit] The paper describes that NLLB uses Conditional MoE Routing to assign weights to dense FNN or MoE Gating based on token capacity demanding or routing efficiency, unlike WMT21fb's random dropout.
- Why unresolved: The paper mentions the concept but does not provide detailed technical implementation or quantitative comparisons of efficiency gains.
- What evidence would resolve it: A detailed technical breakdown of the Conditional MoE Routing algorithm and experimental results showing performance and efficiency differences between Conditional MoE and original MoE.

### Open Question 2
- Question: How would the performance of the small-sized Marian model compare if it were trained with a similar amount of parameters as the NLLB model, while maintaining the same fine-tuning data and methodology?
- Basis in paper: [inferred] The paper shows that the small Marian model outperforms the much larger NLLB model, suggesting that model size is not the primary factor in clinical domain performance.
- Why unresolved: The study only compares models of different sizes without exploring how scaling the smaller model would affect performance.
- What evidence would resolve it: Training a scaled-up version of the Marian model with comparable parameters to NLLB and evaluating its performance on the same clinical translation tasks.

### Open Question 3
- Question: What specific aspects of the clinical data cleaning and fine-tuning process contributed most significantly to the superior performance of the Marian model over the larger models?
- Basis in paper: [explicit] The paper emphasizes the importance of data cleaning and fine-tuning over model size, with the Marian model achieving better results despite being much smaller.
- Why unresolved: While the paper highlights the importance of data quality, it does not detail which specific cleaning or fine-tuning techniques were most impactful.
- What evidence would resolve it: An ablation study or detailed analysis of different data cleaning and fine-tuning strategies to identify which steps most improve translation quality in clinical domains.

## Limitations

- The study only compared three models and one clinical dataset, limiting generalizability to other domains or language pairs.
- The human evaluation process lacks detail on evaluator selection, training, and inter-rater reliability.
- The paper does not address potential model bias or ethical considerations in deploying clinical translation models.

## Confidence

- High confidence: The finding that domain-specific fine-tuning improves translation quality over generic large models in clinical text is well-supported by both automatic metrics and expert human evaluation.
- Medium confidence: The success of transfer learning for a previously unseen language pair is demonstrated, but the generalizability to other unseen language pairs is not established.
- Low confidence: The claim that smaller models can consistently outperform larger models in all clinical translation tasks is overstated, given the limited scope of the experiments.

## Next Checks

1. Test the models on additional clinical datasets from different medical specialties (e.g., radiology, pathology) to assess whether the observed performance trends hold across diverse clinical domains.

2. Conduct experiments ablating model size, depth, and capacity (e.g., comparing Marian-6, Marian-12, and Marian-30) to isolate the effect of model size from domain adaptation and data quality.

3. Systematically vary the similarity between the new language pair and those seen during pre-training, and test transfer learning success on a wider range of unseen language pairs (e.g., Arabic, Chinese) to determine the limits of cross-lingual transfer.