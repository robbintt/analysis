---
ver: rpa2
title: 'Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden
  Attacks'
arxiv_id: '2310.10077'
source_url: https://arxiv.org/abs/2310.10077
tags:
- harmful
- prompts
- prompt
- llms
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel Compositional Instruction Attack
  (CIA) framework that reveals large language models' vulnerability to hidden malicious
  intentions within seemingly harmless instructions. The authors propose two transformation
  methods, T-CIA and W-CIA, which automatically disguise harmful prompts as talking
  or writing tasks by leveraging psychological principles and in-context learning.
---

# Prompt Packer: Deceiving LLMs through Compositional Instruction with Hidden Attacks

## Quick Facts
- arXiv ID: 2310.10077
- Source URL: https://arxiv.org/abs/2310.10077
- Reference count: 40
- Primary result: CIA framework achieves over 95% attack success rate on safety assessment datasets

## Executive Summary
This paper introduces a novel Compositional Instruction Attack (CIA) framework that reveals large language models' vulnerability to hidden malicious intentions within seemingly harmless instructions. The authors propose two transformation methods, T-CIA and W-CIA, which automatically disguise harmful prompts as talking or writing tasks by leveraging psychological principles and in-context learning. Extensive experiments on three leading LLMs (GPT-4, ChatGPT, and ChatGLM2) demonstrate that CIA achieves over 95% attack success rate on safety assessment datasets and 83-91% on harmful prompt datasets, significantly outperforming baseline approaches.

## Method Summary
The CIA framework employs two transformation methods to disguise harmful prompts. T-CIA (Talking-CIA) disguises harmful prompts as talking tasks by inferring adversarial personas from the harmful content and commanding the LLM to respond under that persona, exploiting the similarity-attraction principle from psychology. W-CIA (Writing-CIA) disguises harmful prompts as writing tasks by constructing disguised writing prompts and instructing the LLM to continue writing a novel based on them, leveraging the observation that LLMs apply different safety standards to fictional content. The framework evaluates attack effectiveness using attack success rate (ASR) and non-rejection rate (NRR) metrics across multiple safety and harmful prompt datasets.

## Key Results
- CIA achieves over 95% attack success rate on safety assessment datasets (Safety-Prompts, Harmless Prompts)
- Attack success rates on harmful prompt datasets range from 83-91% (Forbidden Question Set, AdvBench)
- CIA outperforms baseline approaches significantly, with repeated attacks increasing success rates further
- T-CIA is more effective for harmful prompts involving interaction or instruction-following, while W-CIA works better for standalone harmful content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs are vulnerable to compositional instructions because they can recognize individual intents but fail to synthesize them into a unified threat assessment.
- Mechanism: The model sees the harmless outer instruction (e.g., "write a dialogue") and processes it normally, but the inner harmful intent is carried along and executed because the model doesn't check for malicious intent encapsulation.
- Core assumption: The LLM's safety alignment is applied only to explicitly harmful prompts, not to prompts containing embedded harmful instructions within benign contexts.
- Evidence anchors:
  - [abstract] "CIA hides harmful prompts within instructions of harmless intentions, making it impossible for the model to identify underlying malicious intentions."
  - [section 3.1] "CIA achieve this by employing the successfully answered innocuous prompt ð‘ð‘– to encapsulate the harmful prompt ð‘â„Ž, to induce LLMs to generate a harmful response to ð‘â„Ž"
- Break condition: If the model is trained to decompose instructions into atomic intents and evaluate each for safety before execution, the encapsulation would fail.

### Mechanism 2
- Claim: T-CIA works by exploiting the similarity-attraction principle - LLMs are more likely to follow harmful instructions when they adopt a persona that matches the harmful intent.
- Mechanism: The system first infers a negative persona from the harmful prompt, then instructs the LLM to respond under that persona, making it more receptive to harmful content.
- Core assumption: The LLM's alignment is persona-dependent, and it will bypass safety checks when operating under a persona that aligns with harmful intent.
- Evidence anchors:
  - [abstract] "T-CIA analyzed why LLM rejected harmful prompts from a psychological perspective and gave corresponding solutions"
  - [section 3.2] "According to the similarity-attraction principle [39] in psychological science, people are more inclined to interact with individuals who share similar personalities."
- Break condition: If the model is trained to maintain ethical boundaries regardless of persona, or if persona-switching is detected and flagged as suspicious.

### Mechanism 3
- Claim: W-CIA exploits the LLM's lack of safety judgment on virtual content - harmful actions described in fictional contexts are not flagged as dangerous.
- Mechanism: Harmful prompts are rewritten as unfinished novel plots, and the LLM is instructed to complete the story, treating the harmful content as creative writing rather than real-world instruction.
- Core assumption: The model distinguishes between real-world safety and fictional content, applying fewer restrictions to the latter.
- Evidence anchors:
  - [abstract] "W-CIA applies in-context learning to combine harmful prompts with writing tasks and then disguise them as writing instructions for completing unfinished novels"
  - [section 3.3] "The inspiration for W-CIA comes from the fact that LLMs' judgment of harmful behaviors is often limited to real-world behaviors and lacks safety judgments on virtual works such as novels."
- Break condition: If the model is trained to apply consistent safety standards across both real and fictional content, or if it can identify harmful patterns regardless of context.

## Foundational Learning

- Concept: Intent recognition and synthesis
  - Why needed here: The attack succeeds because the LLM can't recognize that multiple instructions combine to form a harmful intent
  - Quick check question: Can you design a test case where an LLM must identify whether two seemingly harmless instructions combine to create a harmful outcome?

- Concept: Persona-based behavior modification
  - Why needed here: T-CIA relies on the LLM changing its behavior when adopting different personas
  - Quick check question: How would you test whether an LLM maintains safety constraints when operating under different personas?

- Concept: Context-dependent safety evaluation
  - Why needed here: W-CIA exploits the difference between real-world and fictional content safety standards
  - Quick check question: Can you create a prompt that distinguishes between harmful content in a real context vs. a fictional one, and test how the LLM responds?

## Architecture Onboarding

- Component map: CIA Framework -> T-CIA (APE -> RUAP) or W-CIA (DWPC -> SDWP) -> LLM Interaction -> Safety Evaluation
- Critical path: For T-CIA: Harmful prompt â†’ Persona inference â†’ Persona-based response generation â†’ Safety evaluation. For W-CIA: Harmful prompt â†’ Novel context generation â†’ Story completion â†’ Safety evaluation.
- Design tradeoffs: The system trades off between attack effectiveness (higher success rates) and detection risk (more sophisticated attacks might be easier to detect). The use of random factors in LLM generation helps avoid detection but also introduces unpredictability.
- Failure signatures: Low attack success rates might indicate that the LLM has improved intent recognition, persona detection, or context-aware safety evaluation. High rejection rates in repeated attacks might indicate that the LLM is learning to identify and block compositional attacks.
- First 3 experiments:
  1. Test the base LLM's rejection rate on harmful prompts vs. the same prompts wrapped in harmless contexts to establish the baseline vulnerability.
  2. Test the effectiveness of T-CIA by varying the persona inference accuracy and measuring attack success rates.
  3. Test the effectiveness of W-CIA by varying the degree of fictional context and measuring whether the LLM treats harmful content differently based on context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically evaluate and improve large language models' intent recognition capabilities to better detect and resist compositional instruction attacks?
- Basis in paper: explicit
- Why unresolved: The paper highlights the vulnerability of LLMs to hidden malicious intentions within seemingly harmless instructions, but does not provide a comprehensive framework for evaluating or improving intent recognition capabilities.
- What evidence would resolve it: A systematic evaluation framework for LLM intent recognition capabilities, including benchmarks, metrics, and improvement strategies, would provide insights into how to better defend against compositional instruction attacks.

### Open Question 2
- Question: What are the potential long-term consequences of compositional instruction attacks on the trustworthiness and reliability of large language models in real-world applications?
- Basis in paper: explicit
- Why unresolved: The paper demonstrates the effectiveness of compositional instruction attacks but does not explore the potential long-term impacts on LLM trustworthiness and reliability in various applications.
- What evidence would resolve it: Long-term studies on the effects of compositional instruction attacks on LLM performance, user trust, and real-world application outcomes would provide valuable insights into the potential consequences.

### Open Question 3
- Question: How can we develop more robust decoding mechanisms for large language models to reduce the risk of being exploited by compositional instruction attacks?
- Basis in paper: inferred
- Why unresolved: The paper suggests that the random factors in the decoding mechanism increase the diversity of responses but also the risk of being attacked, but does not provide concrete solutions for developing more robust decoding mechanisms.
- What evidence would resolve it: The development and evaluation of new decoding mechanisms that balance response diversity with security against compositional instruction attacks would help address this issue.

## Limitations

- The attack framework relies on three specific LLM implementations without testing generalization to other model architectures or training approaches
- The persona inference component lacks detailed specification of how adversarial personas are extracted from harmful prompts
- The safety evaluation methodology using ChatGPT as an oracle introduces potential circularity in the testing process
- The paper does not address whether repeated exposure to compositional attacks might train the model to recognize and block such patterns

## Confidence

**High Confidence**: The core finding that LLMs struggle with compositional instruction attacks is well-supported by experimental results showing 95%+ success rates across multiple datasets. The experimental methodology using standardized safety datasets is sound, and the comparative results against baseline approaches are statistically significant.

**Medium Confidence**: The psychological mechanism explanation (similarity-attraction principle for T-CIA) is plausible but not rigorously tested. The paper presents the psychological framework as the explanation for why persona-based attacks work, but does not experimentally isolate this mechanism from other potential factors like attention manipulation or instruction-following patterns.

**Low Confidence**: The claim that W-CIA exploits a fundamental limitation in LLMs' treatment of fictional vs. real content is speculative. The paper provides limited evidence that models consistently apply different safety standards to fictional contexts, and this mechanism could vary significantly across different model architectures or fine-tuning approaches.

## Next Checks

1. **Cross-model generalization test**: Apply the CIA framework to at least three additional LLM architectures (including open-source models with different training approaches) to verify whether the attack success rates remain consistent or if certain model types show resistance to compositional attacks.

2. **Persona mechanism isolation**: Design an ablation study that tests T-CIA effectiveness with varying degrees of persona alignment (perfectly aligned, partially aligned, misaligned) while controlling for other variables to determine whether the similarity-attraction principle is the primary driver of success or if other factors are more important.

3. **Adaptive defense evaluation**: Implement a simple defense mechanism that decomposes composite instructions into atomic components and applies safety checks to each component before execution, then measure whether this reduces CIA attack success rates by at least 50% compared to baseline models without this capability.