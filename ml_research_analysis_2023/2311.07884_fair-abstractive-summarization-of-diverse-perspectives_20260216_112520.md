---
ver: rpa2
title: Fair Abstractive Summarization of Diverse Perspectives
arxiv_id: '2311.07884'
source_url: https://arxiv.org/abs/2311.07884
tags:
- fairness
- summarization
- summary
- source
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework for defining, measuring, and
  improving fairness in abstractive summarization. The authors propose four automatic
  metrics that evaluate whether summaries fairly represent diverse perspectives across
  demographic groups by comparing value distributions in the source and target texts.
---

# Fair Abstractive Summarization of Diverse Perspectives

## Quick Facts
- arXiv ID: 2311.07884
- Source URL: https://arxiv.org/abs/2311.07884
- Reference count: 28
- Both model-generated and human-written reference summaries exhibit fairness issues

## Executive Summary
This paper introduces a framework for defining, measuring, and improving fairness in abstractive summarization. The authors propose four automatic metrics that evaluate whether summaries fairly represent diverse perspectives across demographic groups by comparing value distributions in source and target texts. They curate a benchmark dataset covering six domains and evaluate nine LLMs, finding that both model-generated and human-written reference summaries exhibit fairness issues. The paper identifies temperature and summary length as influential factors and proposes three methods—adjusting temperature, controlling summary length, and instruction prompting—to improve fairness. Experiments show these methods can significantly reduce unfair summarization rates.

## Method Summary
The framework defines fairness in summarization as maintaining proportional representation of different social groups from source to summary. It proposes four reference-free automatic metrics (BUR, UER, AUC, SOF) that measure differences between target and source perspective distributions. The authors curate PERSPECTIVE SUMM, a benchmark covering six domains with social attributes like gender, party, sentiment, and rating. They evaluate nine LLMs using zero-shot prompting and propose three improvement methods: temperature adjustment, summary length control, and instruction prompting. The framework computes value distributions using token attribution methods like n-gram matching and neural similarity scoring.

## Key Results
- Both model-generated and human-written reference summaries exhibit fairness issues across multiple domains
- Higher temperature values (0.7-1.0) significantly reduce unfair summarization rates
- 3-sentence summaries demonstrate optimal fairness performance across datasets
- Instruction prompting that explicitly defines fairness requirements can significantly improve fairness outcomes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fairness can be measured by comparing the distribution of tokens associated with different social groups in source and summary texts.
- **Mechanism:** The authors propose computing value distributions (px and py) by counting tokens associated with each group value, then measuring underrepresentation through metrics like BUR and UER.
- **Core assumption:** Token-level attribution from source to summary can be reliably computed using n-gram matching or semantic similarity scores.
- **Evidence anchors:**
  - [abstract] "We propose four reference-free automatic metrics by measuring the differences between target and source perspectives."
  - [section 3.2] "We propose two algorithms for calculating the target distribution py: N-gram Matching and Neural Matching."
  - [corpus] Weak evidence - no direct evaluation of attribution accuracy.
- **Break condition:** If token attribution is unreliable due to abstractive generation creating novel words not in source, or if semantic matching fails to capture true provenance.

### Mechanism 2
- **Claim:** Temperature and summary length are key hyperparameters that influence fairness in generated summaries.
- **Mechanism:** Higher temperature increases diversity in token selection, potentially improving fairness by reducing bias toward overrepresented groups. Shorter summaries may struggle to maintain balanced representation.
- **Core assumption:** The sampling mechanism in LLMs can be manipulated to produce more balanced outputs through temperature adjustment.
- **Evidence anchors:**
  - [section 6.1] "Figure 4 shows the BUR and UER scores decrease significantly when the temperature rises."
  - [section 6.1] "Figure 5 shows the BUR and UER scores for different lengths. Among them, 3 sentences demonstrate the best performance on fairness."
  - [corpus] Weak evidence - only tested on one dataset (Claritin) with one model (gpt-turbo-3.5).
- **Break condition:** If temperature increase degrades summary quality (readability, coherence) beyond acceptable thresholds, or if optimal length varies significantly across domains.

### Mechanism 3
- **Claim:** Instruction prompting that explicitly defines fairness requirements can improve the fairness of generated summaries.
- **Mechanism:** By adding instructions about fairness to the prompt (e.g., "ensure the length of male review in summary is still X% of total length"), models can be guided to produce more balanced outputs.
- **Core assumption:** LLMs can interpret and follow abstract fairness instructions in prompts to modify their generation behavior.
- **Evidence anchors:**
  - [section 6.2] "Adding the definition of fairness summaries can significantly improve fairness."
  - [abstract] "Experiments show these methods can significantly reduce unfair summarization rates."
  - [corpus] Weak evidence - only tested on two datasets (Claritin, Yelp) with one model (gpt-turbo-3.5).
- **Break condition:** If models misinterpret fairness instructions or if the improvement in fairness comes at the cost of summary quality or other important dimensions.

## Foundational Learning

- **Concept:** Value Distribution Computation
  - **Why needed here:** Understanding how to compute px and py is fundamental to applying the proposed fairness metrics.
  - **Quick check question:** Given a source text with 60% male and 40% female tokens, and a summary with 50% male and 50% female tokens, what would px and py be?
- **Concept:** Token Attribution Methods
  - **Why needed here:** Different methods (n-gram matching vs. semantic similarity) have different trade-offs in accuracy and applicability.
  - **Quick check question:** When would n-gram matching fail to properly attribute tokens in an abstractive summary?
- **Concept:** Fairness Threshold Selection
  - **Why needed here:** The tolerance hyperparameter τ determines what constitutes underrepresentation and affects metric outcomes.
  - **Quick check question:** If τ is set to 0.9, what percentage of the source distribution must the summary maintain for each group to be considered fair?

## Architecture Onboarding

- **Component map:** Dataset preprocessing with social attribute annotations → source/target value distribution computation → fairness metric calculation (BUR, UER, AUC, SOF) → fairness improvement methods (temperature adjustment, length control, instruction prompting)
- **Critical path:** For evaluating a new summary, the critical path is: compute source distribution → compute target distribution using RuleScore/BERTScore/BARTScore → calculate BUR and UER → determine fairness.
- **Design tradeoffs:** RuleScore is token-level and works on long texts but may miss semantic shifts; BERTScore/BARTScore capture semantics but have length limitations; BUR gives binary fairness classification while UER provides graded severity.
- **Failure signatures:** High BUR with low UER suggests binary underrepresentation; low BUR with high UER suggests subtle but widespread unfairness; inconsistent results across metrics may indicate attribution errors.
- **First 3 experiments:**
  1. Replicate the Claritin dataset experiment with gpt-turbo-3.5 to verify the basic framework works.
  2. Test different temperature values (0, 0.3, 0.7, 1.0) on a single dataset to observe fairness-quality tradeoff.
  3. Compare RuleScore, BERTScore, and BARTScore on a dataset with mixed-length summaries to understand their relative strengths.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different social attributes (e.g., gender, party, sentiment, rating, speaker) affect the fairness of abstractive summarization across different domains?
- Basis in paper: [explicit] The paper discusses fairness in abstractive summarization across various domains (healthcare, politics, restaurant, product, law, policy) and attributes (gender, party, sentiment, rating, speaker).
- Why unresolved: While the paper provides a comprehensive benchmark and evaluation across these domains and attributes, it does not explicitly analyze how the choice of social attribute influences fairness outcomes in different domains.
- What evidence would resolve it: Comparative analysis showing how fairness metrics (BUR, UER, AUC, SOF) vary when summarizing with different social attributes within the same domain or across domains.

### Open Question 2
- Question: What is the optimal balance between fairness and summary quality (e.g., readability, informativeness) when adjusting hyperparameters like temperature and summary length?
- Basis in paper: [explicit] The paper investigates the influence of temperature and summary length on fairness, finding that higher temperatures and specific lengths can improve fairness but may impact quality.
- Why unresolved: The paper identifies a trade-off between fairness and quality but does not quantify or optimize this balance, leaving open questions about the ideal settings for practical applications.
- What evidence would resolve it: Empirical studies measuring both fairness metrics and quality metrics (e.g., ROUGE, BERTScore) across a range of hyperparameter values to identify Pareto-optimal settings.

### Open Question 3
- Question: How do different reference-free automatic metrics (RuleScore, BERTScore, BARTScore) compare in terms of reliability and correlation with human judgments of fairness?
- Basis in paper: [explicit] The paper proposes and evaluates three automatic metrics for measuring fairness but does not provide a detailed comparison of their performance or reliability.
- Why unresolved: While the paper shows that these metrics can capture fairness to some extent, it does not establish which metric is most reliable or how they correlate with human evaluations across different domains and attributes.
- What evidence would resolve it: Systematic comparison of the three metrics on the same datasets, including correlation analysis with human judgments and error analysis to identify strengths and weaknesses of each approach.

## Limitations

- Token attribution mechanisms may fail when abstractive generation creates novel words or paraphrases that don't directly map to source tokens
- Evaluation primarily focuses on binary gender attributes, limiting generalizability to more complex demographic dimensions
- Temperature and length optimization experiments were conducted on only two datasets with a single model, raising questions about robustness across diverse domains

## Confidence

**High Confidence:** The identification of fairness issues in both model-generated and human-written summaries. This claim is supported by extensive experimental evidence across six diverse datasets and nine different LLMs, with consistent patterns of underrepresentation across multiple fairness metrics.

**Medium Confidence:** The effectiveness of instruction prompting as a fairness improvement method. While experiments show significant improvements, the evaluation was limited to two datasets and one model. The generalizability of prompt engineering across different domains, languages, and model families remains uncertain.

**Low Confidence:** The generalizability of temperature and length optimization findings. The experiments were conducted on a narrow subset of the overall evaluation space (two datasets, one model), and the optimal settings may vary significantly across different summarization tasks, domains, and model architectures.

## Next Checks

1. **Cross-Model Validation:** Replicate the temperature and length optimization experiments across all nine evaluated LLMs on the Claritin dataset to determine if the observed patterns hold consistently or are model-specific.

2. **Attribution Method Comparison:** Conduct a controlled experiment comparing RuleScore, BERTScore, and BARTScore on a subset of summaries where ground-truth token provenance can be manually verified, to quantify the accuracy trade-offs between token-level and semantic matching approaches.

3. **Multi-Dimensional Fairness:** Extend the evaluation framework to handle multi-attribute fairness (e.g., simultaneously considering gender and sentiment) and test whether the proposed improvement methods maintain effectiveness when fairness definitions become more complex.