---
ver: rpa2
title: 'USTHB at NADI 2023 shared task: Exploring Preprocessing and Feature Engineering
  Strategies for Arabic Dialect Identification'
arxiv_id: '2312.10536'
source_url: https://arxiv.org/abs/2312.10536
tags:
- arabic
- dialect
- identification
- preprocessing
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates key factors influencing Arabic Dialect
  Identification for the NADI 2023 task. The authors explore the effects of surface
  preprocessing, morphological preprocessing, FastText vector models, and weighted
  concatenation of TF-IDF features.
---

# USTHB at NADI 2023 shared task: Exploring Preprocessing and Feature Engineering Strategies for Arabic Dialect Identification

## Quick Facts
- arXiv ID: 2312.10536
- Source URL: https://arxiv.org/abs/2312.10536
- Reference count: 12
- This study investigates key factors influencing Arabic Dialect Identification for the NADI 2023 task, achieving an F1 score of 62.51% for country-level dialect identification.

## Executive Summary
This study investigates key factors influencing Arabic Dialect Identification for the NADI 2023 task. The authors explore the effects of surface preprocessing, morphological preprocessing, FastText vector models, and weighted concatenation of TF-IDF features. Using Linear Support Vector Classification (LSVC), the system achieves an F1 score of 62.51% for country-level dialect identification. This performance aligns closely with the average F1 score of 72.91% obtained by other systems submitted for the same subtask. The findings suggest that combining FastText features with weighted TF-IDF concatenation is effective for improving dialect identification accuracy.

## Method Summary
The study employs a Linear Support Vector Classification (LSVC) model with various preprocessing and feature engineering strategies on a dataset of 23.4K tweets from 18 Arab countries. The method involves surface preprocessing (Arabic Letter Normalization, punctuation/emoji/stopword removal, diacritics elimination, non-Arabic content removal), morphological preprocessing (lemmatization and stemming), feature extraction using FastText models (supervised and unsupervised) and TF-IDF vectorizer with Word, Char, and Char_wb analyzers, and weighted feature fusion. The system achieves an F1 score of 62.51% for country-level dialect identification.

## Key Results
- The system achieves an F1 score of 62.51% for country-level dialect identification using Linear Support Vector Classification.
- Combining FastText features with weighted TF-IDF concatenation is effective for improving dialect identification accuracy.
- Morphological preprocessing and surface preprocessing steps were found to harm performance, reducing F1 score from 62.51% to 53.10%.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining FastText features with weighted TF-IDF concatenation improves Arabic dialect identification accuracy.
- Mechanism: FastText captures subword information and semantic relationships, while TF-IDF captures term importance; their weighted combination leverages both semantic and statistical features.
- Core assumption: FastText vectors encode dialect-specific semantic patterns that complement statistical TF-IDF features.
- Evidence anchors:
  - [abstract] "the findings suggest that combining FastText features with weighted TF-IDF concatenation is effective for improving dialect identification accuracy."
  - [section] "The findings underscore the significance of feature engineering and the fusion of weighted TF-IDF matrices as a robust foundation for this classification task. Furthermore, the integration of FastText, with its capacity to capture semantic relationships and nuances, enhances the overall accuracy and effectiveness of our dialect identification system."
  - [corpus] Weak: No direct evidence of FastText capturing dialect-specific patterns in this paper's results; corpus only shows this is a known method in related work.
- Break condition: If FastText vectors do not encode dialect-specific semantic patterns or if the dialectal variation is primarily lexical rather than semantic.

### Mechanism 2
- Claim: Weighted feature fusion outperforms simple union by assigning appropriate importance to different feature types.
- Mechanism: The weighted union assigns different importance weights to word, char, and char_wb analyzers, allowing the model to prioritize more discriminative features.
- Core assumption: Different n-gram analyzers capture different aspects of dialectal variation, and these aspects have varying importance for classification.
- Evidence anchors:
  - [section] "Experiment 4 (Lichouri et al., 2021b): In this instance, we solely executed the fourth phase, applying a weighted union of TF-IDF features for feature extraction."
  - [section] "The remarkable alignment between our system's performance, yielding an F 1 score of 62.51% for closed country-level dialect identification, and the average scores achieved by other submitted systems (72.91% for the first subtask), underscores the competitiveness and efficacy of our approach."
  - [corpus] Weak: No specific evidence in this paper about the effectiveness of weighted vs simple union; corpus shows this is a known approach in related work.
- Break condition: If the feature types are equally important or if the dialectal differences are not captured differently by different n-gram analyzers.

### Mechanism 3
- Claim: Removing preprocessing steps preserves important dialectal information that would otherwise be lost.
- Mechanism: Morphological preprocessing (lemmatization, stemming) and surface preprocessing (stop word removal, punctuation removal) can remove dialect-specific morphological markers and stylistic features.
- Core assumption: Dialectal variation includes morphological and stylistic features that are important for identification but are removed by standard preprocessing.
- Evidence anchors:
  - [section] "However, an intriguing observation occurred when we introduced preprocessing (SP and MP) into the process. To our surprise, the F1 score experienced a significant drop, reaching only 53.10% compared to using vectorization alone (TF-IDF)."
  - [section] "This decline in performance can be attributed to potential information loss, alterations in word forms through lemmatization, as well as the removal of punctuation."
  - [corpus] Weak: This paper provides direct evidence, but corpus doesn't show whether this is a general finding or specific to this dataset.
- Break condition: If dialectal variation is primarily in vocabulary rather than morphology or style, or if the preprocessing steps are carefully tuned to preserve dialect-specific features.

## Foundational Learning

- Concept: Feature engineering for dialect identification
  - Why needed here: The paper shows that different feature combinations (FastText, TF-IDF with different analyzers) have varying effectiveness for dialect classification.
  - Quick check question: What types of features (surface, morphological, semantic) are most important for distinguishing between similar Arabic dialects?

- Concept: Weighted feature fusion
  - Why needed here: The paper demonstrates that weighted combination of features outperforms simple concatenation, suggesting importance of feature weighting.
  - Quick check question: How do you determine optimal weights for combining different feature types in a classification task?

- Concept: Impact of preprocessing on dialect identification
  - Why needed here: The paper shows that preprocessing can hurt performance by removing dialect-specific information, contrary to typical NLP practice.
  - Quick check question: When might standard preprocessing steps (lemmatization, stop word removal) actually harm performance in dialect identification tasks?

## Architecture Onboarding

- Component map: Preprocessing (SP and MP) -> Feature extraction (FastText and TF-IDF with different analyzers) -> Weighted feature fusion -> Linear SVC classification
- Critical path: Feature extraction → Weighted feature fusion → Linear SVC classification. The feature extraction and fusion steps are most critical for performance.
- Design tradeoffs: Simpler preprocessing preserves dialectal information but may introduce noise; complex preprocessing cleans data but risks removing important features.
- Failure signatures: Performance drops when preprocessing is applied (F1 score drops from ~62% to ~53%); indicates preprocessing is removing important dialectal features.
- First 3 experiments:
  1. Compare FastText supervised vs unsupervised feature extraction on the same dataset to verify FastText's contribution.
  2. Test different weight combinations in the weighted TF-IDF fusion to find optimal weights.
  3. Compare performance with and without morphological preprocessing to quantify its impact on dialect identification accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the removal of morphological preprocessing steps affect the accuracy of Arabic dialect identification in NADI tasks?
- Basis in paper: [explicit] The paper notes that introducing morphological preprocessing (MP) led to a significant drop in F1 score from 61.24% to 53.10%.
- Why unresolved: The specific reasons for the drop, such as information loss or class imbalance, are not fully explored or quantified.
- What evidence would resolve it: Controlled experiments isolating the effects of MP steps (lemmatization, stemming) on different dialect datasets and measuring changes in feature space and classification metrics.

### Open Question 2
- Question: What is the optimal n-gram range for TF-IDF features in Arabic dialect identification?
- Basis in paper: [explicit] The authors experimented with n-gram ranges from 1 to 10 but did not identify an optimal range.
- Why unresolved: The paper does not provide a detailed analysis of how different n-gram ranges affect performance or which range maximizes accuracy.
- What evidence would resolve it: Systematic testing of various n-gram ranges with detailed performance metrics to determine the optimal configuration for dialect identification.

### Open Question 3
- Question: How do supervised and unsupervised FastText models compare in feature extraction for Arabic dialects?
- Basis in paper: [explicit] The paper uses both supervised and unsupervised FastText models but does not provide a direct comparison of their effectiveness.
- Why unresolved: The relative performance and suitability of each model type for Arabic dialect identification are not clearly established.
- What evidence would resolve it: Comparative experiments using both FastText models on the same datasets, with detailed analysis of feature quality and classification accuracy.

## Limitations

- The study's finding that preprocessing harms performance contradicts standard NLP practice and requires further investigation to determine if this is dataset-specific.
- The paper lacks detailed parameter tuning information for both preprocessing and feature extraction phases, making faithful reproduction challenging.
- The focus on country-level rather than finer-grained dialectal variation limits generalizability to other Arabic dialect identification tasks.

## Confidence

- High: The overall approach of combining semantic (FastText) and statistical (TF-IDF) features is methodologically sound
- Medium: The specific effectiveness of weighted feature fusion for this task
- Low: The generalizability of preprocessing findings to other Arabic dialect datasets

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of FastText vectors, different TF-IDF analyzers, and their weighted combination
2. Test the preprocessing findings on multiple Arabic dialect datasets to verify if preprocessing consistently harms dialect identification performance
3. Perform parameter sensitivity analysis for both feature extraction (n-gram ranges, max_features) and classification (SVM regularization parameters)