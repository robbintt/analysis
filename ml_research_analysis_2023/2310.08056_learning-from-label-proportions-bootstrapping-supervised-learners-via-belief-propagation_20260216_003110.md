---
ver: rpa2
title: 'Learning from Label Proportions: Bootstrapping Supervised Learners via Belief
  Propagation'
arxiv_id: '2310.08056'
source_url: https://arxiv.org/abs/2310.08056
tags:
- labels
- label
- learning
- information
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an iterative algorithm for learning from label
  proportions (LLP) that combines belief propagation (BP) with deep learning. The
  key idea is to use BP to obtain pseudo-labels for instances by incorporating both
  bag-level constraints and covariate similarity information, then train a deep neural
  network using these pseudo-labels and bag-level labels to learn better embeddings.
---

# Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation

## Quick Facts
- **arXiv ID**: 2310.08056
- **Source URL**: https://arxiv.org/abs/2310.08056
- **Reference count**: 40
- **Primary result**: Iterative BP-deep learning algorithm achieves up to 15% improvement in LLP classification over state-of-the-art baselines

## Executive Summary
This paper introduces an iterative algorithm for Learning from Label Proportions (LLP) that combines belief propagation (BP) with deep learning. The method uses BP to generate pseudo-labels by exploiting bag-level constraints and nearest-neighbor similarity, then trains a deep neural network with a dual-head architecture to refine embeddings. This process iterates until convergence, yielding strong performance improvements on both tabular and image datasets.

## Method Summary
The algorithm operates in two main steps: Pseudo Labeling via Belief Propagation and Embedding Refinement. First, it constructs a Gibbs distribution over instance labels incorporating bag-level constraints and K-nearest neighbor similarity, then uses BP to marginalize and obtain soft pseudo-labels. Second, it trains a deep neural network with instance and bag-level heads using these pseudo-labels and bag constraints to produce refined embeddings. The process iterates twice, with the second iteration using the refined embeddings as new covariates.

## Key Results
- Achieves up to 15% improvement in test AUROC over state-of-the-art LLP baselines
- Shows consistent gains across diverse datasets including Adult Income, Bank Marketing, Criteo, and CIFAR-10 variants
- The 1-nearest neighbor approximation captures most performance gains while reducing computational complexity
- Two iterations typically suffice for convergence

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Belief Propagation can recover pseudo-labels by exploiting bag-level label proportions and nearest-neighbor similarity constraints
- **Mechanism**: Defines a Gibbs distribution with least-squares penalty for bag-level deviations and similarity penalties for neighbors, then uses BP to marginalize and obtain soft pseudo-labels
- **Core assumption**: True instance labels exhibit smoothness over covariate space and bag-level counts provide sufficient information for bootstrapping label recovery
- **Evidence anchors**: Abstract mentions incorporating bag-level aggregated labels and covariate information through similarity constraints
- **Break condition**: Weak nearest-neighbor similarity or insufficient bag sizes lead to underdetermined Gibbs distribution and poor BP marginals

### Mechanism 2
- **Claim**: Iterative refinement using pseudo-labels and bag-level supervision improves both instance and bag prediction accuracy
- **Mechanism**: Trains dual-head deep network with one head predicting instance labels from pseudo-labels and another predicting bag-level proportions from average pooling
- **Core assumption**: Pseudo-labels provide meaningful supervision to improve embedding space, and dual-head architecture enforces consistency
- **Evidence anchors**: Abstract describes using pseudo-labels to provide supervision for a learner yielding better embeddings
- **Break condition**: Noisy pseudo-labels or weak bag constraints cause degenerate solutions or failure to improve embeddings

### Mechanism 3
- **Claim**: 1-nearest neighbor approximation captures most performance gain while reducing computational complexity
- **Mechanism**: Uses single nearest neighbor instead of k-NN for similarity constraints, simplifying factor graph and reducing message-passing complexity
- **Core assumption**: Nearest neighbor relationship provides sufficient redundancy in similarity constraints, with higher-order k-NN adding minimal additional information
- **Evidence anchors**: Shows 1-NN is empirically close to best k-NN performance and reduces time complexity without greatly affecting performance
- **Break condition**: Complex data manifold structure where higher-order neighbors provide crucial information not captured by single nearest neighbors

## Foundational Learning

- **Concept**: Belief Propagation on factor graphs
  - Why needed here: BP marginalizes Gibbs distribution over instance labels to obtain soft pseudo-labels respecting bag-level and similarity constraints
  - Quick check question: What are the two main types of potentials in the Ising model formulation used for pseudo-labeling?

- **Concept**: Dual-head neural network architecture
  - Why needed here: Simultaneously learns instance-level predictions from pseudo-labels and bag-level predictions from aggregated representations, enforcing consistency
  - Quick check question: How does the aggregate embedding loss combine instance-level and bag-level supervision?

- **Concept**: Nearest neighbor graph construction
  - Why needed here: k-NN graph provides similarity constraints in Gibbs distribution, connecting instances that should have similar labels
  - Quick check question: How does choice of distance metric (cosine vs L2) affect quality of neighbor graph and downstream performance?

## Architecture Onboarding

- **Component map**: Covariates → Gibbs distribution → BP → pseudo-labels → Dual-head training → Refined embeddings → Repeat → Final classifier

- **Critical path**: Covariates → Gibbs distribution → BP → pseudo-labels → Dual-head training → Refined embeddings → Repeat → Final classifier

- **Design tradeoffs**:
  - Using k-NN vs 1-NN: Higher k provides more constraints but increases computational complexity; 1-NN is sufficient for most gains
  - Choice of kernel (RBF vs Matern): Matern kernel provides slightly better performance but RBF is simpler
  - Number of BP iterations: More iterations improve convergence but increase computation; 100 iterations typically sufficient

- **Failure signatures**:
  - Poor pseudo-label quality: Check BP marginals against true labels (should show reasonable ordering)
  - Degenerate embeddings: Monitor instance-level and bag-level losses during training
  - Slow convergence: Check sparsity of factor graph and message-passing efficiency

- **First 3 experiments**:
  1. Run BP with 1-NN constraints on a small dataset and visualize the pseudo-labels against true labels
  2. Train the dual-head network with only the instance-level loss (no bag-level loss) and compare performance
  3. Vary the number of BP iterations (T=50, 100, 200) and measure impact on downstream classification accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the choice of distance metric (cosine vs L2) impact the quality of the k-NN graph and subsequent performance of the belief propagation step?
- **Basis in paper**: [explicit] Authors state they experimented with both cosine and L2 distance, finding cosine led to better downstream performance, but do not provide detailed analysis of why
- **Why unresolved**: Paper only shows comparison of kNN scores for different metrics but doesn't delve into underlying reasons for observed performance difference
- **What evidence would resolve it**: Detailed study comparing structural properties of kNN graphs formed using different distance metrics and their correlation with BP performance

### Open Question 2
- **Question**: What is the theoretical justification for the effectiveness of belief propagation in this setting, particularly with the sparse factor graph induced by k-NN constraints?
- **Basis in paper**: [inferred] Paper mentions BP is known to converge on trees and sparse graphs, but doesn't provide rigorous theoretical analysis of its performance in this specific problem
- **Why unresolved**: Authors acknowledge this as limitation and suggest it as future work direction, indicating current lack of complete theoretical understanding
- **What evidence would resolve it**: Theoretical analysis proving convergence and performance guarantees of BP on specific factor graph structure used in this work

### Open Question 3
- **Question**: How sensitive is the algorithm's performance to choice of hyperparameters, particularly regularization parameters (λs, λb) and number of BP iterations (T)?
- **Basis in paper**: [explicit] Authors mention tuning these hyperparameters using Vizier but do not provide sensitivity analysis of performance to their values
- **Why unresolved**: Paper focuses on demonstrating overall effectiveness but doesn't delve into robustness of results to hyperparameter choices
- **What evidence would resolve it**: Systematic study varying hyperparameters within reasonable range and analyzing impact on performance

## Limitations
- Lacks rigorous theoretical analysis of convergence and generalization bounds for the iterative BP-deep learning procedure
- Doesn't thoroughly explore sensitivity to hyperparameters or provide robust guidelines for hyperparameter selection
- K-NN graph construction and BP message passing scale poorly with large datasets, though 1-NN approximation helps

## Confidence
- **High confidence**: Overall iterative framework combining BP and deep learning is sound with consistent empirical improvements over baselines
- **Medium confidence**: Specific implementation details (MLP architecture, exact loss formulations, hyperparameter values) are partially specified and may require significant tuning
- **Low confidence**: Theoretical properties of the method including convergence guarantees and sensitivity to initialization are not well-established

## Next Checks
1. Track pseudo-label quality and classification accuracy across iterations (beyond 2 iterations reported) to understand convergence behavior and potential overfitting
2. Systematically vary K-NN parameters, distance metrics, and kernel choices to quantify their impact on performance and identify most critical design choices
3. Attempt to prove convergence guarantees or derive generalization bounds for the iterative BP-deep learning procedure, building on existing work in semi-supervised learning and message-passing algorithms