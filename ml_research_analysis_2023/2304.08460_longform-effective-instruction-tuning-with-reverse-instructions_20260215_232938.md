---
ver: rpa2
title: 'LongForm: Effective Instruction Tuning with Reverse Instructions'
arxiv_id: '2304.08460'
source_url: https://arxiv.org/abs/2304.08460
tags:
- instruction
- generation
- longform
- language
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LongForm, a novel dataset and method for
  instruction tuning of language models on long text generation. The LongForm dataset
  is created by leveraging human-written documents from existing corpora and generating
  instructions via LLMs.
---

# LongForm: Effective Instruction Tuning with Reverse Instructions

## Quick Facts
- arXiv ID: 2304.08460
- Source URL: https://arxiv.org/abs/2304.08460
- Reference count: 40
- Key outcome: LongForm models outperform larger language models without instruction tuning on long text generation tasks and surpass prior instruction-tuned models like FLAN-T5 and Alpaca

## Executive Summary
This paper introduces LongForm, a novel approach to instruction tuning for long text generation that reverses the typical paradigm. Instead of providing instructions and inputs to generate outputs, the authors leverage human-written documents as outputs and use LLMs to generate corresponding instructions. This approach creates a cleaner, more cost-effective instruction-tuning dataset. The authors fine-tune various language models (T5, OPT, LLaMA) on this dataset and demonstrate that even smaller LongForm models outperform much larger language models without instruction tuning on tasks like story/recipe generation and long-form question answering. The models also show strong multilingual capabilities.

## Method Summary
The LongForm approach involves creating a dataset by leveraging human-written documents from existing corpora (C4, Wikipedia, Stack Exchange, WikiHow) as outputs, then using LLMs with zero-shot prompting to generate relevant instructions for these documents. The authors fine-tune encoder-decoder models (T5-XL) and autoregressive models (OPT-2.7B, OPT-6.7B, LLaMA-7B) on this dataset using DeepSpeed optimizations. The fine-tuned models are evaluated on their ability to follow instructions and generate high-quality long-form text across various domains and languages.

## Key Results
- LongForm models outperform 10x larger language models without instruction tuning on story/recipe generation and long-form question answering tasks
- LongForm models achieve significantly better performance than prior instruction-tuned models (FLAN-T5, Alpaca) on the same tasks
- Models demonstrate effective multilingual instruction following and text generation in German, Spanish, French, and Russian

## Why This Works (Mechanism)

### Mechanism 1
Leveraging existing human-written documents as outputs and generating instructions via LLMs provides a cleaner and more cost-effective instruction-tuning dataset compared to traditional approaches. By reversing the typical instruction-tuning paradigm, the authors ensure natural, diverse, and high-quality outputs while significantly reducing the cost and effort associated with human annotation. The core assumption is that LLMs can generate relevant and diverse instructions for given human-written documents with sufficient accuracy.

### Mechanism 2
Fine-tuning various language models (T5, OPT, LLaMA) on the LongForm dataset improves their instruction-following capabilities for long text generation tasks. The diverse nature of the dataset, containing instructions and human-written outputs across various domains, enables the models to better understand and follow instructions for generating long-form text. The core assumption is that the LongForm dataset is diverse and representative enough to improve the generalization capabilities of the fine-tuned models across different long text generation tasks.

### Mechanism 3
The LongForm models outperform prior instruction-tuned models and raw LLMs on long text generation tasks and demonstrate multilingual capabilities. By fine-tuning on the LongForm dataset, the models learn to better follow instructions and generate high-quality long-form text. The diverse nature of the dataset, including multilingual examples, enables the models to handle multilingual instructions and generate text in different languages. The core assumption is that the LongForm dataset's diversity and quality are sufficient to improve the models' performance on long text generation tasks and enable multilingual capabilities.

## Foundational Learning

- **Instruction Tuning**: The process of training language models to follow natural language instructions. Why needed: This is the core technique used to improve the models' ability to follow instructions and generate long-form text.
  - Quick check: What is the primary goal of instruction tuning in the context of language models?

- **Zero-shot Prompting**: Using LLMs to generate outputs without providing any examples. Why needed: The authors use zero-shot prompting to generate instructions from LLMs for the corpus examples.
  - Quick check: How does zero-shot prompting differ from few-shot or fine-tuning approaches when generating instructions with LLMs?

- **Long Text Generation**: The task of generating coherent, contextually appropriate text spanning multiple paragraphs or documents. Why needed: The paper focuses on improving language models' capabilities for long text generation tasks.
  - Quick check: What are some of the key challenges in evaluating long text generation models, and how do they differ from evaluating short text generation models?

## Architecture Onboarding

- **Component map**: Corpus selection and preprocessing -> Instruction generation via LLMs using zero-shot prompting -> Fine-tuning various language models (T5, OPT, LLaMA) on the LongForm dataset -> Evaluation on long text generation tasks and multilingual capabilities

- **Critical path**: The critical path involves selecting and preprocessing the corpus examples, generating instructions via LLMs, fine-tuning the language models on the resulting dataset, and evaluating the models' performance on long text generation tasks and multilingual capabilities.

- **Design tradeoffs**: The authors trade off the cost and effort of human annotation for the potential noisiness of LLM-generated instructions. They also choose to focus on long text generation tasks, which may limit the dataset's applicability to other NLP tasks.

- **Failure signatures**: Potential failure modes include: 1) Low quality or relevance of generated instructions, 2) Insufficient diversity in the corpus examples or generated instructions, 3) Overfitting to the LongForm dataset, leading to poor generalization on unseen tasks, and 4) Struggles with multilingual text generation due to limited multilingual examples in the dataset.

- **First 3 experiments**:
  1. Evaluate the relevance and diversity of generated instructions for a subset of corpus examples through human evaluation.
  2. Fine-tune a smaller language model (e.g., T5-base) on the LongForm dataset and compare its performance to a baseline model on a simple long text generation task (e.g., story generation).
  3. Evaluate the multilingual capabilities of the LongForm models by generating text in different languages for a set of multilingual instructions and assessing the quality of the generated text.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unaddressed based on the limitations section and experimental scope.

## Limitations
- Dataset Quality Evaluation: Limited direct evaluation of instruction quality, relying on downstream task performance as a proxy
- Generalization Boundaries: Evaluation focuses primarily on story, recipe, and news generation tasks, with unclear performance on other long-form text generation domains
- Reproducibility Concerns: Lack of specific implementation details for fine-tuning process, including exact hyperparameters and DeepSpeed optimization configuration

## Confidence

**High Confidence**: The claim that LongForm models outperform prior instruction-tuned models (FLAN-T5, Alpaca) and raw LLMs on tested tasks is supported by empirical results in the paper.

**Medium Confidence**: The claim that the reverse instruction generation approach provides cleaner and more cost-effective instruction-tuning data is plausible based on the methodology described, but lacks direct evidence comparing dataset quality to traditional approaches.

**Low Confidence**: The claim that even smaller LongForm models can outperform much larger language models without instruction tuning is supported only for specific tasks (story/recipe generation and long-form question answering), not as a general principle across all domains.

## Next Checks
1. **Instruction Quality Audit**: Conduct a systematic human evaluation of 200 randomly sampled instruction-output pairs from the LongForm dataset to assess instruction relevance, clarity, and diversity. Compare these ratings against instructions from FLAN-T5 and Alpaca datasets.

2. **Out-of-Domain Generalization Test**: Evaluate LongForm models on at least two additional long text generation domains not represented in the training data (e.g., technical documentation, academic abstracts) to assess true generalization capabilities beyond the tested tasks.

3. **Scale Sensitivity Analysis**: Systematically compare LongForm model performance across different model scales (from 1B to 30B parameters) to determine whether the claimed performance advantages hold consistently across different model sizes or are specific to particular scale ranges.