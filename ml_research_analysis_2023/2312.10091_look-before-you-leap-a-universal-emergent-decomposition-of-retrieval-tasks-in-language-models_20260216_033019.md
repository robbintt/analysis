---
ver: rpa2
title: 'Look Before You Leap: A Universal Emergent Decomposition of Retrieval Tasks
  in Language Models'
arxiv_id: '2312.10091'
source_url: https://arxiv.org/abs/2312.10091
tags:
- task
- token
- input
- tasks
- request-patching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a study of how language models solve retrieval
  tasks, introducing ORION, a collection of structured datasets spanning six domains.
  Using causal analysis on 18 models, the authors discover a universal modular decomposition
  where middle layers process the request and late layers retrieve the context.
---

# Look Before You Leap: A Universal Emergent Decomposition of Retrieval Tasks in Language Models

## Quick Facts
- arXiv ID: 2312.10091
- Source URL: https://arxiv.org/abs/2312.10091
- Reference count: 35
- Primary result: Universal modular decomposition discovered in language models solving retrieval tasks, with middle layers processing requests and late layers retrieving context, enabling scalable internal oversight applications.

## Executive Summary
This paper investigates how language models solve retrieval tasks by analyzing their internal mechanisms using causal interventions. The authors introduce ORION, a structured dataset spanning six domains, and apply residual stream patching across 18 models to discover a universal modular decomposition where middle layers process requests and late layers retrieve context. After enforcing this decomposition via request-patching, models preserve 70% of correct token probability in 98/106 model-task pairs. A fine-grained case study on Pythia-2.8b reveals that late MLPs, not just attention heads, are critical for retrieval. The authors demonstrate a proof of concept applying this understanding to scalable internal oversight, improving accuracy from 15.5% to 97.5% on prompt injection tasks while requiring human supervision on only a single input.

## Method Summary
The authors created ORION, a structured dataset with 15 retrieval tasks across six domains (question answering, translation, factual recall, variable binding, induction pattern matching, and coding). They applied residual stream patching, a causal intervention technique, to 18 open-source language models ranging from 125 million to 70 billion parameters. The method involves patching the residual stream at intermediate layers to artificially enforce the modular decomposition between request processing and context retrieval. They measured normalized token probability and accuracy for label tokens, identified limit layers where models transition between processing stages, and performed path patching and attention pattern analysis on Pythia-2.8b for fine-grained understanding. The internal oversight application used request-patching from trusted inputs to untrusted inputs in a prompt injection setting.

## Key Results
- Universal modular decomposition discovered: middle layers process requests, late layers retrieve context across 18 models and 15 tasks
- Request-patching preserves 70% of correct token probability in 98/106 model-task pairs
- Fine-grained case study reveals late MLPs are critical for retrieval, not just attention heads
- Internal oversight proof of concept improves prompt injection accuracy from 15.5% to 97.5% with minimal human supervision

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language models internally decompose retrieval tasks into request processing and context retrieval stages.
- Mechanism: Middle layers at the last token position process the request, while late layers retrieve the correct entity from the context.
- Core assumption: The model has learned a modular decomposition of the retrieval task that is preserved across domains and model scales.
- Evidence anchors:
  - [abstract] "We find that LMs internally decompose retrieval tasks in a modular way: middle layers at the last token position process the request, while late layers retrieve the correct entity from the context."
  - [section] "We discover that language models handle retrieval tasks by cleanly separating the layers at which they process the request and the context at the last token position."
- Break condition: If the request processing and context retrieval stages are not separable, or if the model does not preserve the modular decomposition across domains and scales.

### Mechanism 2
- Claim: Request-patching can enforce the modular decomposition of retrieval tasks and preserve model performance.
- Mechanism: By patching the residual stream at intermediate layers, we can artificially separate the request processing and context retrieval stages, causing the model to output a modular combination of the request from one input and the context from another.
- Core assumption: The model's internal representation of the request is separable from the context, and patching the residual stream at the right layer preserves the modular decomposition.
- Evidence anchors:
  - [abstract] "After causally enforcing this decomposition, models are still able to solve the original task, preserving 70% of the original correct token probability in 98 of the 106 studied model-task pairs."
  - [section] "We design an interchange intervention called request-patching that artificially forces this division as illustrated in Figure 1."
- Break condition: If patching the residual stream does not preserve the modular decomposition, or if the model's performance degrades significantly after request-patching.

### Mechanism 3
- Claim: The modular decomposition of retrieval tasks can be leveraged for scalable internal oversight of language models.
- Mechanism: By enforcing the request processing stage using a trusted input, we can make the model more robust against prompt injection and detect anomalous mechanisms.
- Core assumption: The model's internal representation of the request is separable from the context, and enforcing the request processing stage using a trusted input preserves the modular decomposition and makes the model more robust.
- Evidence anchors:
  - [abstract] "Building on our high-level understanding, we demonstrate a proof of concept application for scalable internal oversight of LMs to mitigate prompt-injection while requiring human supervision on only a single input."
  - [section] "We designed an application to partially supervise the internal processes of models in a simple prompt-injection setting."
- Break condition: If enforcing the request processing stage using a trusted input does not make the model more robust against prompt injection, or if the model's performance degrades significantly.

## Foundational Learning

- Concept: Causal interventions
  - Why needed here: To understand the internal mechanisms of language models solving retrieval tasks and enforce the modular decomposition.
  - Quick check question: What is a causal intervention, and how is it different from a behavioral intervention?

- Concept: Interchange interventions
  - Why needed here: To artificially force the modular decomposition of retrieval tasks by patching the residual stream at intermediate layers.
  - Quick check question: What is an interchange intervention, and how is it different from a regular causal intervention?

- Concept: Residual stream patching
  - Why needed here: To implement request-patching and enforce the modular decomposition of retrieval tasks.
  - Quick check question: What is residual stream patching, and how does it work in the context of request-patching?

## Architecture Onboarding

- Component map: Input tokens → Embedding layer → Residual stream → Attention heads → MLP layers → Logits
- Critical path: Input tokens → Embedding layer → Residual stream → Attention heads (request processing) → MLP layers (context retrieval) → Logits
- Design tradeoffs: Coarse-grained vs. fine-grained analysis of model internals; Generalizability vs. specificity of the modular decomposition; Computational efficiency vs. interpretability of the causal interventions
- Failure signatures: Model performance degradation after request-patching; Inability to separate request processing and context retrieval stages; Lack of generalizability of the modular decomposition across domains and scales
- First 3 experiments:
  1. Apply residual stream patching at different layers to see if it enforces the modular decomposition of retrieval tasks.
  2. Measure the model's performance after request-patching to see if it preserves the modular decomposition and maintains accuracy.
  3. Apply request-patching using a trusted input to see if it makes the model more robust against prompt injection and detects anomalous mechanisms.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the specific mechanism by which large models like Llama 2 70b exhibit a narrow range of optimal layers for request-patching compared to other models?
- Basis in paper: [inferred] The paper notes that Llama 2 70b has a distinct pattern where all limit layers (L1, L2, L3) are concentrated in a very narrow span of layers (39-43) across all tasks, unlike other models which show more spread.
- Why unresolved: The paper only speculates that this could be due to the larger scale of the model or peculiarities of the architecture, but does not provide concrete evidence or a definitive explanation.
- What evidence would resolve it: A detailed comparative analysis of the architectural differences between Llama 2 70b and other models, particularly focusing on the attention heads and MLP configurations in the layers 39-43. Additionally, experiments that vary the model scale while keeping the architecture constant could help isolate the effect of size.

### Open Question 2
- Question: How does the request-patching phenomenon generalize to tasks beyond retrieval, such as reasoning or generation tasks?
- Basis in paper: [inferred] The paper demonstrates request-patching's effectiveness on retrieval tasks but does not explore its applicability to other task types. The authors suggest that the modularity observed could be a universal motif in model internals, but this is not empirically tested.
- Why unresolved: The scope of the paper is limited to retrieval tasks, and extending the analysis to other domains would require significant additional experimentation and dataset creation.
- What evidence would resolve it: Applying request-patching to a diverse set of non-retrieval tasks (e.g., mathematical reasoning, creative writing, code generation) and measuring its effectiveness in preserving task performance and interpretability.

### Open Question 3
- Question: What is the precise role of MLPs in the retrieval mechanism, and how do they interact with attention heads during request-patching?
- Basis in paper: [explicit] The case study on Pythia-2.8b reveals that MLPs at later layers contribute significantly to the retrieval mechanism, and their direct effect is preserved after request-patching. However, the exact nature of their contribution and interaction with attention heads is not fully elucidated.
- Why unresolved: The paper provides preliminary observations but does not conduct a comprehensive analysis of MLP mechanisms or their interplay with attention heads.
- What evidence would resolve it: A detailed mechanistic study of MLPs in the context of retrieval tasks, including path patching experiments to isolate their direct effects, attention pattern analysis to understand their interaction with attention heads, and ablation studies to determine their necessity for task performance.

## Limitations
- The generalizability of the modular decomposition beyond retrieval tasks to more complex reasoning or generation tasks remains uncertain
- The theoretical foundation for why this specific layer-wise decomposition emerges across models is not fully explained
- The internal oversight application represents only a proof of concept on a simplified prompt injection setting

## Confidence
- Universal modular decomposition claim: Medium
- Request-patching effectiveness: High (empirical results)
- Theoretical explanation of mechanisms: Low
- Internal oversight applications: Medium (proof of concept)

## Next Checks
- Replicate the ORION dataset creation process to verify the structured format and semi-automatic generation with ChatGPT
- Implement residual stream patching on a subset of models and tasks to validate the 70% accuracy preservation claim
- Test the internal oversight application on additional prompt injection scenarios beyond the simplified proof of concept