---
ver: rpa2
title: Learning to Follow Object-Centric Image Editing Instructions Faithfully
arxiv_id: '2310.19145'
source_url: https://arxiv.org/abs/2310.19145
tags:
- image
- edit
- instruction
- images
- editing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses challenges in natural language-based image
  editing, focusing on underspecification, grounding, and faithfulness. The authors
  improve training data quality by leveraging Chain-of-Thought prompting, object detection,
  segmentation, and visual question answering to filter noise and ensure context-appropriate
  edits.
---

# Learning to Follow Object-Centric Image Editing Instructions Faithfully

## Quick Facts
- arXiv ID: 2310.19145
- Source URL: https://arxiv.org/abs/2310.19145
- Reference count: 11
- Outperforms state-of-the-art baselines with TIFA score of 65.84 vs 62.24

## Executive Summary
This paper addresses fundamental challenges in natural language-based image editing, specifically underspecification, grounding, and faithfulness. The authors improve training data quality by leveraging Chain-of-Thought prompting with LLMs to filter nonsensical edit instructions, object detection and segmentation for precise entity localization, and VQA techniques to ensure edited images remain faithful to the original. Their fine-tuned model achieves superior performance compared to state-of-the-art baselines in both automated TIFA scores and human evaluation, demonstrating strong generalization to unseen domains including visual metaphors.

## Method Summary
The approach involves a multi-stage data curation pipeline that filters and improves synthetic edit instructions from InstructPix2Pix. First, Chain-of-Thought prompting with ChatGPT evaluates instruction plausibility and identifies the entity to transform. GroundingDINO then generates bounding boxes around these entities, followed by SAM segmentation for precise masks. The edited images are generated using Stable Diffusion Inpainting and filtered for faithfulness using Vicuna-13B to generate questions about unmodified elements and BLIP-2 to answer them. The final model is fine-tuned on this curated dataset with additional supervision from bounding boxes and segmentation masks.

## Key Results
- Achieves TIFA score of 65.84, outperforming state-of-the-art baseline (62.24)
- 69.8% instruction satisfiability on in-domain evaluation
- 65.1% instruction satisfiability on out-of-domain evaluation
- Successfully generalizes to visual metaphors domain

## Why This Works (Mechanism)

### Mechanism 1: Chain-of-Thought Instruction Filtering
- Large language models analyze image context and proposed edits to filter nonsensical instructions
- LLM predicts whether transformation is logically possible and identifies the entity to transform
- Core assumption: LLMs possess sufficient commonsense knowledge to evaluate edit plausibility
- Break condition: LLM fails to correctly identify nonsensical instructions or misidentifies entities

### Mechanism 2: Object Detection and Segmentation Supervision
- GroundingDINO generates bounding boxes around entities identified by LLM
- SAM segments the entity for precise inpainting guidance
- Core assumption: Detection and segmentation models accurately localize identified entities
- Break condition: Models fail to accurately localize or segment entities, causing incorrect inpainting

### Mechanism 3: VQA-Based Faithfulness Evaluation
- Vicuna-13B generates questions about unmodified elements in edited images
- BLIP-2 answers questions to ensure faithfulness to original image
- Core assumption: VQA models accurately answer questions about entity presence/absence
- Break condition: VQA models fail to accurately evaluate faithfulness or questions don't capture all aspects

## Foundational Learning

- **Chain-of-Thought prompting**: Enables LLM reasoning about edit plausibility and entity identification; quick check: How does this technique help generate more accurate and relevant responses for instruction-based editing?

- **Object detection and segmentation**: Provides precise localization and segmentation for grounded editing; quick check: What are key differences between detection and segmentation, and how do they contribute to grounding?

- **Visual Question Answering (VQA)**: Ensures edited images remain faithful by evaluating unmodified elements; quick check: How can VQA evaluate faithfulness, and what challenges exist in generating effective questions?

## Architecture Onboarding

- **Component map**: LLM (ChatGPT) → Object detection (GroundingDINO) → Segmentation (SAM) → Image inpainting (Stable Diffusion) → Faithfulness evaluation (VQA)

- **Critical path**: Instruction filtering (LLM) → Entity identification (LLM) → Object detection (GroundingDINO) → Segmentation (SAM) → Image inpainting (Stable Diffusion) → Faithfulness evaluation (VQA)

- **Design tradeoffs**: More powerful LLM improves accuracy but increases computational costs; additional detection/segmentation models improve grounding but increase complexity; sophisticated VQA models improve evaluation but increase costs

- **Failure signatures**: Inaccurate instruction filtering due to LLM limitations; incorrect localization/segmentation due to model limitations; inaccurate faithfulness evaluation; low-quality edited images

- **First 3 experiments**: 
  1. Evaluate LLM instruction filtering accuracy on held-out test set
  2. Assess object detection and segmentation accuracy for entity localization
  3. Measure VQA faithfulness evaluation effectiveness against human judgments

## Open Questions the Paper Calls Out

- How does Chain-of-Thought prompting compare to other natural language understanding approaches for entity identification?
- How does the model perform on style transfer or global editing tasks beyond object-centric edits?
- How does the model handle instructions in languages other than English, and what are the limitations?
- What safeguards are needed to prevent misuse for creating fake or harmful content?

## Limitations

- Heavy reliance on black-box LLM-based instruction filtering with unclear precision/recall
- VQA-based faithfulness evaluation depends on quality of generated questions and may not capture all aspects
- Limited analysis of cross-domain generalization beyond the three domains tested
- Potential computational costs from multi-stage pipeline with multiple model components

## Confidence

- **High Confidence**: Improved TIFA scores and human evaluation results showing better instruction satisfiability
- **Medium Confidence**: Chain-of-Thought prompting effectiveness for instruction filtering
- **Low Confidence**: VQA-based faithfulness evaluation comprehensiveness and potential biases

## Next Checks

1. Conduct detailed analysis of Chain-of-Thought prompting accuracy by manually evaluating filtered instructions to determine precision and recall
2. Perform ablation studies on VQA-based faithfulness evaluation with different question generation strategies
3. Systematically evaluate model performance across broader range of unseen domains beyond three mentioned domains