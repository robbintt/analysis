---
ver: rpa2
title: An Evaluation of Zero-Cost Proxies -- from Neural Architecture Performance
  to Model Robustness
arxiv_id: '2307.09365'
source_url: https://arxiv.org/abs/2307.09365
tags:
- linf
- clean
- fgsm
- proxies
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how well zero-cost proxies (ZCPs) can predict
  both clean and robust accuracy of neural architectures, focusing on the popular
  NAS-Bench-201 search space. ZCPs are cheap to compute and have been widely used
  for neural architecture search (NAS) to predict the performance of architectures.
---

# An Evaluation of Zero-Cost Proxies -- from Neural Architecture Performance to Model Robustness

## Quick Facts
- arXiv ID: 2307.09365
- Source URL: https://arxiv.org/abs/2307.09365
- Reference count: 40
- One-line primary result: Random forest regression combining 15 zero-cost proxies can predict both clean and robust accuracy of neural architectures, with Jacobian-based proxies being most important features.

## Executive Summary
This paper evaluates how well zero-cost proxies (ZCPs) can predict both clean and robust accuracy of neural architectures, focusing on the NAS-Bench-201 search space. ZCPs are cheap to compute and have been widely used for neural architecture search (NAS) to predict the performance of architectures. However, previous work has focused mainly on clean accuracy, whereas robustness is equally important. The authors collect 15 different ZCPs for 6,466 architectures and analyze their correlation with clean and robust accuracy. They use random forest regression to predict clean and robust accuracy using the ZCPs as features.

## Method Summary
The authors evaluate 15 different zero-cost proxies (ZCPs) on 6,466 architectures from NAS-Bench-201, measuring their ability to predict both clean accuracy and robustness against various adversarial attacks (FGSM, PGD, APGD, Square) at different epsilon values. They use random forest regression with 100 trees and mean squared error criterion to combine ZCPs into predictive models for both single-objective (clean or robust accuracy) and multi-objective targets. Feature importance is assessed using Gini importance and permutation importance, while Kendall tau rank correlation measures the relationship between individual ZCPs and accuracy targets.

## Key Results
- Individual ZCPs show low correlation with robustness, but random forest regression achieves good prediction accuracy for both clean and robust accuracy
- Jacobian-based ZCPs (jacov, nwot) are the most important features for predicting both clean and robust accuracy
- Predicting robustness is harder than clean accuracy and requires combining multiple ZCPs, with R² scores typically lower for robustness targets

## Why This Works (Mechanism)

### Mechanism 1
Zero-cost proxies (ZCPs) can predict neural architecture performance without training by leveraging structural properties of untrained networks. ZCPs compute scalar metrics on untrained networks using operations like Jacobian analysis, pruning sensitivity, or architectural complexity measures. These metrics correlate with final trained accuracy because certain structural features (e.g., linear separability, gradient flow, parameter sensitivity) influence how well the architecture will train. The core assumption is that the untrained network's structural properties are sufficiently predictive of the trained network's behavior, despite lacking learned weights.

### Mechanism 2
Random forest regression combining multiple ZCPs can predict both clean and robust accuracy more accurately than individual ZCPs. Each ZCP provides a feature dimension; random forest regression learns nonlinear combinations of these features to predict accuracy targets. The ensemble approach captures complementary information that individual ZCPs miss. The core assumption is that different ZCPs capture different aspects of architecture quality, and their combination improves prediction accuracy.

### Mechanism 3
Jacobian-based ZCPs are particularly important for predicting both clean and robust accuracy. Jacobian-based measures capture how the network's output changes with respect to input perturbations, which is directly related to both classification ability and robustness to adversarial attacks. The core assumption is that the sensitivity of the untrained network's output to input changes is indicative of both its learning capacity and robustness properties.

## Foundational Learning

- **Concept**: Kendall tau rank correlation
  - Why needed here: To evaluate how well individual ZCPs rank architectures compared to their actual performance, providing a measure of correlation that is robust to outliers and non-linear relationships
  - Quick check question: What does a Kendall tau correlation of 0.6 between a ZCP and clean accuracy indicate about their relationship?

- **Concept**: Random forest regression and feature importance
  - Why needed here: To combine multiple ZCPs into a single predictive model and identify which ZCPs contribute most to the predictions, enabling understanding of which architectural properties matter most
  - Quick check question: How does random forest calculate feature importance, and why is this useful for understanding ZCP contributions?

- **Concept**: Adversarial robustness evaluation (FGSM, PGD, APGD, Square attacks)
  - Why needed here: To understand the different attack types and their strengths (epsilon values) when evaluating how well ZCPs predict robust accuracy, as different attacks test different aspects of robustness
  - Quick check question: What is the key difference between white-box attacks (FGSM, PGD, APGD) and black-box attacks (Square) in terms of their evaluation methodology?

## Architecture Onboarding

- **Component map**: ZCP computation module -> Dataset integration -> Random forest regression model -> Evaluation framework
- **Critical path**: For predicting robust accuracy, the critical path is: load untrained architecture -> compute all 15 ZCPs -> feed ZCP vector into trained random forest model -> output predicted robust accuracy. The most time-consuming step is ZCP computation, which should be optimized for efficiency.
- **Design tradeoffs**: Using random forest enables capturing non-linear relationships between ZCPs and accuracy but sacrifices interpretability compared to linear models. Including all 15 ZCPs provides comprehensive information but may introduce redundancy; feature selection could improve efficiency. Evaluating robustness requires computing multiple attack strengths, increasing computational cost but providing more comprehensive assessment.
- **Failure signatures**: Poor prediction accuracy may indicate that ZCPs are not capturing the relevant architectural properties for robustness, suggesting a need for new proxy types. If feature importance is evenly distributed across ZCPs, it may indicate that no single proxy type is particularly informative, requiring architectural modifications. If correlation drops significantly for stronger attacks, it suggests ZCPs are better at predicting resistance to weak attacks only.
- **First 3 experiments**:
  1. Compute Kendall tau correlation between each individual ZCP and clean accuracy on a subset of architectures to identify which proxies show the strongest correlations
  2. Train a random forest regressor using all ZCPs to predict clean accuracy, then evaluate R² score on held-out data to establish baseline performance
  3. Repeat experiment 2 but predict robust accuracy (using FGSM at epsilon=1/255) to compare the difficulty of predicting robustness versus clean accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do zero-cost proxies generalize to other NAS search spaces beyond NAS-Bench-201?
- Basis in paper: The paper states "we leverage the recently published robustness dataset which allows for easily accessible robustness evaluations on an established NAS search space [6]" and evaluates 15 ZCPs on NAS-Bench-201 specifically.
- Why unresolved: The paper only evaluates ZCPs on one search space, and it's unclear if findings would hold for other architectures or search spaces with different properties.
- What evidence would resolve it: Systematic evaluation of the same ZCPs across multiple NAS search spaces with varying characteristics (e.g., different cell sizes, operation sets, or macro-architecture constraints).

### Open Question 2
- Question: What is the impact of dataset-specific characteristics on the effectiveness of ZCPs for predicting robustness?
- Basis in paper: The paper evaluates ZCPs on three different image datasets (CIFAR-10, CIFAR-100, ImageNet16-120) and notes that "the prediction of robustness is a more difficult task" but doesn't isolate dataset effects from architectural effects.
- Why unresolved: The analysis conflates architectural properties with dataset-specific properties, making it unclear whether observed patterns are due to the architectures themselves or the specific datasets used.
- What evidence would resolve it: Cross-dataset evaluation where the same architectures are tested across multiple domains, or synthetic dataset experiments that systematically vary properties like class overlap or feature distribution.

### Open Question 3
- Question: How do zero-cost proxies perform when predicting robustness against adaptive adversarial attacks?
- Basis in paper: The paper evaluates against standard white-box and black-box attacks (FGSM, PGD, APGD, Square) but notes that "high-performing architecture is not necessarily robust" without addressing whether attackers can exploit the proxy-based prediction itself.
- Why unresolved: The evaluation uses standard attack settings but doesn't consider whether an adversary could design attacks specifically to fool the ZCP-based robustness prediction system.
- What evidence would resolve it: Evaluation against adaptive attacks that explicitly target the ZCP features or the random forest prediction model, measuring whether attackers can create architectures that score highly on ZCPs but are actually vulnerable.

## Limitations
- The study is limited to NAS-Bench-201 search space, which may not generalize to other architectures or tasks
- The robustness evaluation uses specific attack types and parameters that may not capture the full spectrum of robustness
- Feature importance analysis depends on random forest implementation details, which can vary across libraries

## Confidence
- **High Confidence**: The core finding that random forest regression outperforms individual ZCPs for both clean and robust accuracy prediction is well-supported by the experimental results and methodology
- **Medium Confidence**: The claim about Jacobian-based ZCPs being the most important features is supported by feature importance metrics but may be dataset-specific and requires validation on other search spaces
- **Low Confidence**: The generalizability of these findings beyond NAS-Bench-201 to real-world NAS applications remains uncertain without additional validation on diverse architectures and tasks

## Next Checks
1. **Cross-dataset validation**: Test the random forest model trained on NAS-Bench-201 ZCPs on a completely different search space (e.g., NAS-Bench-301) to assess generalizability of both the approach and feature importance rankings
2. **Ablation study on ZCP selection**: Systematically remove subsets of ZCPs (particularly the Jacobian-based ones) to quantify their individual contributions and test whether the random forest can compensate for missing proxy types
3. **Robustness transferability evaluation**: Train the model to predict robustness against one attack type and evaluate its performance on predicting robustness against different attack types to assess whether ZCPs capture attack-specific or attack-agnostic robustness properties