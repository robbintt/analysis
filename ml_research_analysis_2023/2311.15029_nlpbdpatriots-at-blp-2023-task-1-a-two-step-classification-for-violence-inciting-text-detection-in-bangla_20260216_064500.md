---
ver: rpa2
title: 'nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence Inciting
  Text Detection in Bangla'
arxiv_id: '2311.15029'
source_url: https://arxiv.org/abs/2311.15029
tags:
- task
- dataset
- violence
- bangla
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the nlpBDpatriots team's approach to the Violence
  Inciting Text Detection (VITD) shared task at BLP 2023. The task focuses on identifying
  and classifying violent threats in Bangla text that provoke unlawful violent acts.
---

# nlpBDpatriots at BLP-2023 Task 1: A Two-Step Classification for Violence Inciting Text Detection in Bangla

## Quick Facts
- arXiv ID: 2311.15029
- Source URL: https://arxiv.org/abs/2311.15029
- Reference count: 5
- Ranked 6th out of 27 teams with macro F1 score of 0.74

## Executive Summary
This paper presents the nlpBDpatriots team's approach to the Violence Inciting Text Detection (VITD) shared task at BLP 2023, which focuses on identifying and classifying violent threats in Bangla text. The team explored various models including statistical ML classifiers, transformer-based models, few-shot prompting, and task-specific fine-tuning. They found that a two-step classification approach combining back translation and multilinguality achieved the best performance. This method ranked 6th out of 27 teams with a macro F1 score of 0.74.

## Method Summary
The approach involves augmenting the dataset by translating it into multiple languages and using two separate fine-tuned models to first classify text as violent or non-violent, then further classify violent text as direct or passive violence. The two-step classification pipeline uses XLM-RoBERTa for binary classification followed by MuRIL for multi-class classification of violent instances. Data augmentation includes back translation using diverse languages (Zulu, Pashto, Azerbaijani) and translation into related languages (Hindi, Urdu, Tamil) to create structurally varied versions of the same content.

## Key Results
- Two-step classification approach achieved 6th place out of 27 teams
- Macro F1 score of 0.74 on the VITD task
- Perfect classification scores for very short texts (1-50 words)
- Challenges with longer texts (>500 words) showing reduced performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining back translation with multilingual data augmentation improves model robustness by exposing it to diverse linguistic variations of the same content.
- Mechanism: The approach translates Bangla text into several unrelated languages (Zulu, Pashto, Azerbaijani) and back, generating structurally varied versions of the same sentences. This forces the model to learn more abstract representations of violent content that are not tied to specific language patterns.
- Core assumption: Structural and grammatical changes from back translation preserve the core meaning while creating sufficiently diverse training examples.
- Evidence anchors:
  - [abstract] "The team also analyzed model performance across different text lengths, finding perfect scores for very short texts but challenges with longer ones."
  - [section] "We use Zulu, Azerbaijan, and Pashto - 3 very diverse languages from Bangla for back translation. So, we also get the size of our train set three times higher than the original Bangla one with significantly different translations for each instance."
  - [corpus] Weak - no direct evidence of back translation effectiveness from neighboring papers.
- Break condition: If back translation introduces semantic drift or loses critical context markers for violence detection.

### Mechanism 2
- Claim: Two-step classification with separate models for binary violence detection and multi-class violence type classification outperforms single-step approaches.
- Mechanism: First model (XLM-RoBERTa) performs binary classification (violent vs non-violent), then second model (MuRIL) classifies only the violent subset into direct vs passive violence. This specialization allows each model to focus on a simpler task.
- Core assumption: Binary classification is easier than multi-class, and separating tasks reduces confusion between similar violence types.
- Evidence anchors:
  - [abstract] "This method ranked 6th out of 27 teams with a macro F1 score of 0.74."
  - [section] "We use the finetuned xlm-roBERTa to label the whole dataset as either violent or non-violent data. We then separate all the data from the test set that are labeled as 'violent' by the finetuned xlm-roBERTa model and use the fine-tuned MuRIL model to predict the 'active violence' and 'passive violence' labels."
  - [corpus] Weak - neighboring papers don't discuss two-step approaches specifically.
- Break condition: If the first model's errors cascade and the second model cannot recover from misclassifications.

### Mechanism 3
- Claim: Multilingual training data augmentation improves performance by leveraging linguistic similarities between Bangla and related languages.
- Mechanism: Translating training data into Hindi, Urdu, and Tamil (languages with cultural interaction and similar morphosyntactic features) creates additional training examples that preserve structural integrity while expanding the dataset.
- Core assumption: Related languages share enough structural patterns that models can transfer knowledge across them.
- Evidence anchors:
  - [section] "We combine these new synthetic datasets with the original train dataset and finetune the multilingual transformer models on them."
  - [section] "Bangla, Hindi, Urdu belong to Indo-Aryan language branch and Tamil from Dravidian language branch, though, all of these languages have cultural interaction in south-east asian region."
  - [corpus] Moderate - neighboring papers mention transformer models but not multilingual augmentation specifically.
- Break condition: If the additional languages introduce noise or the model overfits to language-specific patterns rather than violence-related features.

## Foundational Learning

- Concept: Transformer architecture fundamentals (attention mechanisms, positional encoding)
  - Why needed here: All evaluated models are transformer-based, understanding their core mechanics helps explain why multilingual models perform better.
  - Quick check question: How does self-attention allow transformers to capture relationships between words regardless of their distance in the input sequence?

- Concept: Fine-tuning vs. pre-training distinction
  - Why needed here: The paper uses pre-trained models (BERT, XLM-RoBERTa, MuRIL) and fine-tunes them on the VITD dataset.
  - Quick check question: What is the key difference between pre-training (learning general language patterns) and fine-tuning (adapting to a specific task)?

- Concept: Macro F1 score calculation and interpretation
  - Why needed here: The primary evaluation metric is macro F1 score, which is used to compare different approaches and determine the best-performing model.
  - Quick check question: How does macro F1 differ from micro F1, and why is it more appropriate for evaluating performance on imbalanced datasets?

## Architecture Onboarding

- Component map: Data augmentation → XLM-RoBERTa binary classifier → MuRIL multi-class classifier → Ensemble output
- Critical path: Data augmentation → Binary classification → Violent text filtering → Multi-class classification → Final prediction
- Design tradeoffs: Two-step approach adds complexity but improves accuracy; multilingual augmentation increases data diversity but may introduce noise
- Failure signatures: Stage 1 errors propagate to Stage 2; multilingual data introduces semantic drift; back translation creates ungrammatical sentences
- First 3 experiments:
  1. Compare single-step vs two-step classification using the same models to quantify the performance gain from specialization
  2. Test different combinations of languages for back translation to find the optimal balance between diversity and semantic preservation
  3. Evaluate the impact of dataset size by training with different fractions of the augmented data to determine the optimal augmentation level

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the two-step classification approach compare to a single-step multi-class classification model when using the same data augmentation techniques?
- Basis in paper: [inferred] The paper focuses on the success of the two-step classification approach but doesn't compare it directly to single-step multi-class classification models under the same data augmentation conditions.
- Why unresolved: The authors only present results for the two-step classification approach with data augmentation, not for single-step classification with the same augmentation.
- What evidence would resolve it: A direct comparison of the two-step approach vs. single-step multi-class classification models (e.g., fine-tuned XLM-R, mBERT, or MuRIL) using the same augmented datasets (multilingual and back-translated data) would provide clarity.

### Open Question 2
- Question: Does the two-step classification approach maintain its performance advantage when tested on a larger, more diverse dataset with different class distributions?
- Basis in paper: [explicit] The authors note that the dataset is relatively small and has imbalanced labels, which may have affected model performance.
- Why unresolved: The current results are based on a small, imbalanced dataset, and it's unclear if the approach would generalize to larger, more balanced datasets.
- What evidence would resolve it: Testing the two-step classification approach on a larger, more balanced dataset with different class distributions would demonstrate its scalability and robustness.

### Open Question 3
- Question: What is the impact of different intermediary languages used in back translation on the model's performance?
- Basis in paper: [inferred] The authors use Zulu, Azerbaijani, and Pashto for back translation but don't analyze the individual impact of each language on performance.
- Why unresolved: The paper doesn't provide a breakdown of how each intermediary language affects the model's performance in the back translation process.
- What evidence would resolve it: Analyzing the model's performance when using each intermediary language individually for back translation would reveal which languages contribute most to the improvement.

## Limitations

- No detailed hyperparameter configuration provided for the fine-tuning process
- Limited analysis of why the approach underperforms on longer texts (>500 words)
- No comparison to simpler baselines like single-step classification or monolingual approaches

## Confidence

- Two-step classification with multilingual augmentation: Medium
- Back translation effectiveness: Medium
- Generalization to larger datasets: Low

## Next Checks

1. Conduct an ablation study comparing single-step vs two-step classification using identical models and hyperparameters to isolate the performance gain from the approach design
2. Test different language combinations for back translation (varying relatedness to Bangla) to identify which languages provide the most beneficial augmentation without semantic drift
3. Analyze the distribution of errors across text length ranges to determine if specific modeling approaches (e.g., hierarchical transformers or text summarization before classification) could address the poor performance on longer texts