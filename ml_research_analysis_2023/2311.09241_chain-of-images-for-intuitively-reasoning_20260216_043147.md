---
ver: rpa2
title: Chain of Images for Intuitively Reasoning
arxiv_id: '2311.09241'
source_url: https://arxiv.org/abs/2311.09241
tags:
- image
- images
- reasoning
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Chain of Images (CoI), a method that uses image
  generation as intermediate steps to help language models solve complex reasoning
  problems. The authors observe that current large language models lack the ability
  to intuitively reason with visual information.
---

# Chain of Images for Intuitively Reasoning

## Quick Facts
- arXiv ID: 2311.09241
- Source URL: https://arxiv.org/abs/2311.09241
- Reference count: 10
- One-line primary result: Chain of Images improves reasoning accuracy from 27.75% to 64.25% on geometry tasks

## Executive Summary
This paper introduces Chain of Images (CoI), a method that uses generated images as intermediate steps to help language models solve complex reasoning problems. The authors observe that current LLMs struggle with intuitive reasoning involving visual information. They propose SyMLLM, a multimodal model that can generate SVG images from language instructions and accept both text and images as input. Experiments on geometry, chess, and commonsense tasks from a new CoI evaluation dataset show significant improvements over pure-language Chain of Thoughts baselines, with accuracy gains of up to 64.25% on geometry tasks.

## Method Summary
The method involves generating SVG images step-by-step during reasoning, using these images as intermediate representations that are encoded and fed back into the language model. The SyMLLM framework combines a symbolic multimodal LLM with SVG-based symbol-to-image generation and a CLIP ViT-L/14 image encoder. The model is fine-tuned using LoRA on task-specific datasets including 50k geometry training samples, chess positions, and commonsense reasoning examples. The approach converts complex reasoning problems into visual pattern recognition tasks where the model can leverage image encoders' capabilities.

## Key Results
- Geometry task accuracy improved from 27.75% to 64.25% using CoI
- Chess checkmate detection accuracy increased by 9.1% with CoI
- Commonsense reasoning accuracy improved by 27.1% with CoI
- Visual representations enabled the model to find anti-commonsense points that were missed in pure text

## Why This Works (Mechanism)

### Mechanism 1
- Converting abstract spatial/textual reasoning into concrete visual patterns enables pattern recognition instead of symbolic computation
- Visual pattern recognition is computationally simpler than multi-step symbolic geometric computation for LLMs
- Break condition: When image generation quality degrades significantly or reasoning task cannot be decomposed into visual pattern recognition

### Mechanism 2
- Visual representations provide better information density for tracking complex state evolution
- Images encode complete state information more efficiently than textual descriptions for Markovian tasks
- Break condition: When visual representation becomes too complex to decode efficiently

### Mechanism 3
- Images capture commonsense knowledge that is difficult to express in text
- Visual representations encode implicit spatial and physical knowledge
- Break condition: When visual commonsense knowledge is incomplete or misleading

## Foundational Learning

- Concept: Symbolic representation and SVG generation
  - Why needed here: SyMLLM needs to generate accurate, rule-based images without noise of generative models
  - Quick check question: Can you write an SVG representation for a line segment from (0,0) to (1,1) with red stroke?

- Concept: Multimodal model architecture (text + image encoder)
  - Why needed here: Model needs to accept both text instructions and image embeddings for CoI reasoning
  - Quick check question: What happens to performance if you remove image encoder and only use text?

- Concept: Pattern recognition in images vs symbolic computation
  - Why needed here: Understanding when visual pattern recognition is more efficient than symbolic computation
  - Quick check question: For counting intersection points of 5 shapes, would you prefer symbolic computation or visual pattern recognition? Why?

## Architecture Onboarding

- Component map: LLM backbone -> Symbol-to-image decoder (SVG) -> Image encoder (CLIP) -> Text encoder (LLM) -> Fusion mechanism (concatenation)
- Critical path: Text instruction → LLM generates SVG → SVG converted to bitmap → Image encoder produces embeddings → Concatenated with text embeddings → Next token prediction
- Design tradeoffs:
  - SVG vs raster: SVG provides precise control but requires symbolic reasoning; raster is easier but may introduce noise
  - Image encoder choice: CLIP provides general-purpose embeddings but may miss domain-specific features
  - Token budget: Images add computational cost and token usage; need to balance with reasoning benefits
- Failure signatures:
  - SVG generation failures → noisy or incorrect images
  - Image encoder misinterpretation → wrong pattern recognition
  - Token length issues → truncation or inefficiency
  - Domain mismatch → images don't capture relevant features
- First 3 experiments:
  1. Test SVG generation accuracy on simple geometric shapes (circles, lines, polygons)
  2. Compare intersection counting accuracy with and without generated images on simple 2-3 shape problems
  3. Test chess state representation accuracy by generating board positions from PGN moves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of CoI vary across different types of visual representations (e.g., sketches, diagrams, flowcharts) and reasoning tasks?
- Basis in paper: [inferred] The paper discusses using images for reasoning but doesn't compare different visual representation styles
- Why unresolved: Paper focuses on general concept of using images but doesn't delve into specific visual representation styles
- What evidence would resolve it: Empirical studies comparing effectiveness of various visual representation styles across reasoning tasks

### Open Question 2
- Question: What is the optimal level of detail for generated images to maximize reasoning performance without introducing noise?
- Basis in paper: [explicit] Paper mentions introducing noisy images could worsen performance
- Why unresolved: While paper acknowledges importance of image quality, doesn't specify ideal level of detail
- What evidence would resolve it: Controlled experiments varying image detail and measuring impact on reasoning accuracy

### Open Question 3
- Question: How does effectiveness of CoI compare to other intermediate reasoning aids, such as code or tables?
- Basis in paper: [explicit] Paper briefly mentions structured prompts like code can be helpful for tasks with typical workflows
- Why unresolved: Paper focuses on benefits of CoI but doesn't directly compare to other reasoning aids
- What evidence would resolve it: Comparative studies evaluating performance of CoI against other reasoning aids

## Limitations
- Dataset quality and generalization concerns due to GPT-4 filtering introducing potential selection bias
- Training data composition details unclear, particularly balance between visual and textual reasoning patterns
- Critical implementation details like SVG generation templates and fusion mechanisms not fully specified

## Confidence

**High Confidence**:
- Visual pattern recognition is computationally simpler than multi-step symbolic computation for certain geometric tasks
- Images can encode state information more efficiently than text for Markovian tasks like chess
- The basic framework of using generated images as intermediate reasoning steps is technically sound

**Medium Confidence**:
- The 64.25% accuracy on geometry tasks represents a significant improvement over baselines
- The 27.1% improvement on commonsense reasoning tasks is directly attributable to CoI
- The chess performance improvements generalize to more complex positions

**Low Confidence**:
- CoI will achieve human-level reasoning ability across diverse domains
- The improvements scale proportionally with task complexity
- The approach will work equally well for non-visual reasoning tasks

## Next Checks

**Check 1: Cross-Dataset Validation**
Replicate geometry intersection task using different dataset (e.g., synthetic geometric problems or standardized math tests) to verify that 64.25% accuracy is not specific to filtered CoIEval dataset. Compare performance using both generated images and ground-truth diagrams.

**Check 2: Ablation Study on Image Quality**
Systematically degrade image quality (resolution, precision of SVG generation, noise injection) and measure impact on reasoning accuracy. Determine sensitivity of CoI to visual representation quality and identify minimum quality thresholds.

**Check 3: Real-Time Performance Analysis**
Measure inference time and computational overhead of CoI compared to CoT baselines across different reasoning tasks. Calculate cost-benefit ratio in terms of accuracy improvement per additional computation time to determine practical deployment viability.