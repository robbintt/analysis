---
ver: rpa2
title: 'Llamas Know What GPTs Don''t Show: Surrogate Models for Confidence Estimation'
arxiv_id: '2311.08877'
source_url: https://arxiv.org/abs/2311.08877
tags:
- confidence
- linguistic
- confidences
- gpt-4
- probabilities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies confidence estimation for large language models
  that do not provide access to their internal probabilities, such as GPT-4 and Claude-v1.3.
  The authors first examine eliciting linguistic confidence scores by prompting the
  model to assess its own confidence, which performs reasonably (80.5% AUC) but leaves
  room for improvement.
---

# Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation

## Quick Facts
- arXiv ID: 2311.08877
- Source URL: https://arxiv.org/abs/2311.08877
- Authors: 
- Reference count: 26
- This paper proposes using surrogate models to estimate confidence for LLMs that don't provide access to internal probabilities, achieving state-of-the-art results with 84.6% average AUC on GPT-4.

## Executive Summary
This paper addresses the challenge of confidence estimation for large language models like GPT-4 and Claude-v1.3 that do not provide access to their internal probabilities. The authors explore two approaches: eliciting linguistic confidence scores through prompting and using surrogate models (like Llama 2) that do provide probabilities. Surprisingly, surrogate model probabilities outperform linguistic confidences on 9 out of 12 datasets, and combining both approaches achieves state-of-the-art performance with 84.6% average AUC across 12 datasets.

## Method Summary
The paper explores two main approaches for confidence estimation: (1) eliciting linguistic confidence scores by prompting the LLM to assess its own confidence, and (2) using a surrogate model with accessible probabilities to provide confidence estimates for the original model's answers. The best method combines linguistic confidence and surrogate probabilities with a mixing parameter α. For evaluation, the authors use AUC (area under coverage-accuracy curve) and AUROC metrics across 12 standard question-answering datasets, including TruthfulQA, CommonsenseQA, OpenbookQA, MedQA, and 8 MMLU subsets.

## Key Results
- Linguistic confidence elicitation from GPT-4 achieves 80.5% AUC, reasonable but leaves room for improvement
- Surrogate model probabilities outperform linguistic confidences on 9 out of 12 datasets
- Combining linguistic and surrogate confidences achieves state-of-the-art 84.6% average AUC on GPT-4
- Adding just 0.1% surrogate probability to linguistic confidence allows tie-breaking for identical confidence values

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linguistic confidence scores are limited by coarse granularity, causing repeated values across examples.
- Mechanism: GPT-4 outputs identical confidence scores (e.g., 0.9) for 50% of examples across tasks, preventing effective separation of correct vs. incorrect answers.
- Core assumption: Training data contains frequent occurrences of "nice" probability numbers like 90% or 100%, biasing model outputs.
- Evidence anchors:
  - [section] "GPT-4 outputs the exact same confidence (0.9) on 50% of examples"
  - [section] "model probabilities is quite varied (1456 unique values for Llama 2 70B across 12 datasets), while the distribution of linguistic confidences is quite clustered (only 8 unique values for GPT-4 across 12 datasets)"
  - [corpus] Weak - no direct evidence for training data bias in corpus

### Mechanism 2
- Claim: Surrogate model probabilities transfer well because they share similar error patterns with the main model.
- Mechanism: Questions GPT-4 answers incorrectly are more correlated with those Llama 2 70B answers incorrectly (Pearson correlation 0.39) than with Llama 2 13B (correlation 0.19), indicating shared semantic difficulty patterns.
- Core assumption: Both models have higher entropy probability distributions over answer choices for more difficult questions.
- Evidence anchors:
  - [section] "the questions that are challenging for one model transfer over to a different model"
  - [section] "GPT-4 and Llama 2 70B tend to make mistakes on more of the same questions"
  - [section] "better surrogate models S and their corresponding main models M may struggle with semantically related concepts"

### Mechanism 3
- Claim: Mixing linguistic and surrogate confidences enables tie-breaking for identical confidence values.
- Mechanism: Adding even 0.1% surrogate probability to linguistic confidence allows separation of previously indistinguishable examples through composite scores.
- Core assumption: Surrogate probabilities provide unique ordering information even at minimal mixing weights.
- Evidence anchors:
  - [section] "Adding only 0.1% of a surrogate model's probabilities to a model's linguistic confidences performs better than using either the linguistic confidences or surrogate probabilities alone"
  - [section] "composing just a small fraction of them with linguistic confidence scores...can allow answers which previously had the same linguistic confidence to now be separable"
  - [section] "epsilon is all you need" (referring to small mixing weights)

## Foundational Learning

- Concept: Selective classification (classification with reject option)
  - Why needed here: The paper's goal is to output confidence scores that are higher on inputs where the model is correct than inputs where the model is incorrect.
  - Quick check question: What is the key difference between standard classification and selective classification?
  - Answer: Selective classification allows the model to abstain on low-confidence examples rather than always making a prediction.

- Concept: AUC (Area Under the Curve) for selective classification
  - Why needed here: The paper uses AUC as the primary metric to evaluate how well confidence scores separate correct from incorrect answers.
  - Quick check question: What does an AUC of 0.5 indicate for selective classification?
  - Answer: It indicates performance equivalent to random guessing (no better than chance at separating correct/incorrect examples).

- Concept: Confidence elicitation vs. model probabilities
  - Why needed here: The paper distinguishes between prompting models to output their confidence (linguistic) versus using internal model probabilities.
  - Quick check question: Why might linguistic confidence elicitation produce less accurate results than using model probabilities?
  - Answer: Because linguistic confidences are generated through prompting and can be coarse, repetitive, or biased by training data patterns, while model probabilities are direct outputs from the model's softmax layer.

## Architecture Onboarding

- Component map: Input question -> Main model -> Answer, Input question -> Main model -> Linguistic confidence, Input question -> Surrogate model -> Surrogate probability, Combine linguistic confidence and surrogate probability, Evaluate confidence quality via AUC/AUROC

- Critical path:
  1. Input question → Main model → Answer
  2. Input question → Main model → Linguistic confidence
  3. Input question → Surrogate model → Surrogate probability
  4. Combine linguistic confidence and surrogate probability
  5. Evaluate confidence quality via AUC/AUROC

- Design tradeoffs:
  - Single model vs. multi-model approach: Using only linguistic confidences is simpler but less accurate; adding surrogate model increases complexity but improves performance
  - Surrogate model selection: Larger, more capable models provide better surrogate confidences but may be less accessible; smaller models are more accessible but potentially less correlated with main model errors
  - Mixing parameter α: Higher values give more weight to surrogate probabilities (better separation) but may dilute main model's domain knowledge; lower values preserve main model's signal but may not fully exploit surrogate benefits

- Failure signatures:
  - Surrogate model makes uncorrelated errors: AUC improvement disappears
  - Linguistic confidences are already granular: Minimal benefit from mixing
  - Surrogate model probabilities are poorly calibrated: Can degrade overall confidence quality
  - Mixing parameter α is poorly chosen: Can overemphasize noisy signals

- First 3 experiments:
  1. Compare AUC of GPT-4 linguistic confidences vs. GPT-4 linguistic confidences mixed with 0.001% Llama 2 probabilities
  2. Test multiple surrogate models (Llama 2 13B, text-davinci-003) to measure correlation with main model errors
  3. Vary mixing parameter α from 0 to 1 in 0.1 increments to find optimal balance for GPT-4

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do linguistic confidence scores from stronger models like GPT-4 perform worse than model probabilities from weaker models like Llama 2?
- Basis in paper: [explicit] The authors find that linguistic confidences are worse than model probabilities when these probabilities are available, for both weaker models (text-davinci-003 and Llama 2) and stronger models (GPT-4).
- Why unresolved: The paper provides some intuitions about the clustering of linguistic confidence values and their repetitiveness, but a deeper understanding of the underlying reasons for this phenomenon is still lacking.
- What evidence would resolve it: Further experiments investigating the relationship between model scale, confidence elicitation methods, and the quality of confidence estimates could provide insights into why linguistic confidences are worse than model probabilities.

### Open Question 2
- Question: What is the mechanism behind the transferability of confidence scores between models in the surrogate model approach?
- Basis in paper: [explicit] The authors find that using a surrogate model (e.g., Llama 2) to provide confidence estimates for a stronger model (e.g., GPT-4) leads to higher AUC than linguistic confidences, even though the surrogate model is often weaker.
- Why unresolved: The paper provides some intuitions about the correlation between the questions answered correctly by the main and surrogate models, but a deeper understanding of the underlying mechanisms is still lacking.
- What evidence would resolve it: Further experiments investigating the relationship between the main and surrogate models, such as analyzing the semantic similarity of their mistakes or the entropy of their probability distributions, could provide insights into the transferability of confidence scores.

### Open Question 3
- Question: How can the composition of confidence signals from different models be optimized to further improve confidence estimation?
- Basis in paper: [explicit] The authors find that mixing linguistic confidences and surrogate model probabilities leads to the best confidence estimates, but the optimal mixing ratio varies depending on the models used.
- Why unresolved: The paper provides some initial findings on the effectiveness of mixing confidence signals, but a systematic approach to optimizing the composition of these signals is still lacking.
- What evidence would resolve it: Further experiments investigating different mixing strategies, such as learning-based approaches or more sophisticated composition methods, could provide insights into how to optimize the combination of confidence signals from different models.

## Limitations

- Linguistic confidence elicitation quality is uncertain without direct validation of whether elicited confidences reflect true internal uncertainty
- Surrogate model transferability mechanism is not fully understood, with limited evidence for why correlation in errors translates to confidence transfer
- Mixing parameter sensitivity is not systematically explored across all dataset-model pairs, potentially overstating robustness of "epsilon is all you need" claim

## Confidence

**High Confidence Claims:**
- Linguistic confidences are more coarse-grained than model probabilities (strong empirical evidence with 8 unique values for GPT-4 vs 1456 for Llama 2)
- Surrogate model probabilities transfer better than linguistic confidences alone (9 out of 12 datasets show improvement)
- Mixing linguistic and surrogate confidences improves performance (state-of-the-art 84.6% average AUC)

**Medium Confidence Claims:**
- Coarse granularity of linguistic confidences is due to training data bias (plausible but not directly verified)
- Surrogate models work because they share error patterns with main models (correlation data supports but mechanism is incomplete)
- Small mixing weights (ε) are sufficient for tie-breaking (shown but optimal mixing weights not fully explored)

**Low Confidence Claims:**
- This approach will generalize to all LLM pairs (limited to tested models)
- The mechanism applies equally to non-QA tasks (only tested on QA datasets)

## Next Checks

1. **Direct measurement of entropy distributions**: For a subset of questions, measure and compare the entropy of GPT-4's and Llama 2's probability distributions over answer choices to verify the claimed correlation between model difficulty and entropy.

2. **Ablation study on mixing parameter**: Systematically vary the mixing parameter α from 0 to 1 in smaller increments (0.01) across all dataset-model pairs to identify whether the "epsilon is all you need" claim holds universally or if optimal mixing weights vary by task.

3. **Corpus analysis for training data bias**: Examine GPT-4's training corpus (or a representative sample) to verify whether "nice" probability numbers like 0.9 appear with sufficient frequency to explain the clustering of linguistic confidence scores. This would validate or refute the proposed mechanism for coarse granularity.