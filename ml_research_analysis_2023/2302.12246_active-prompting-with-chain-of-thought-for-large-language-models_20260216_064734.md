---
ver: rpa2
title: Active Prompting with Chain-of-Thought for Large Language Models
arxiv_id: '2302.12246'
source_url: https://arxiv.org/abs/2302.12246
tags:
- answer
- arxiv
- reasoning
- step
- uncertainty
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Active-Prompt, a method to improve large language
  model performance on reasoning tasks by actively selecting task-specific questions
  for human annotation with chain-of-thought reasoning. Instead of using arbitrary
  exemplars, Active-Prompt measures uncertainty in model predictions using metrics
  like disagreement, entropy, and variance, then selects the most uncertain questions
  for annotation.
---

# Active Prompting with Chain-of-Thought for Large Language Models

## Quick Facts
- arXiv ID: 2302.12246
- Source URL: https://arxiv.org/abs/2302.12246
- Reference count: 28
- Key outcome: Uncertainty-based exemplar selection improves chain-of-thought reasoning performance on eight reasoning datasets

## Executive Summary
This paper addresses the limitation of current chain-of-thought (CoT) methods that rely on fixed sets of human-annotated exemplars, which may not be optimal for different tasks. The authors propose Active-Prompt, a method that actively selects task-specific questions for human annotation based on uncertainty metrics. By estimating uncertainty through disagreement, entropy, and variance across multiple predictions, the method identifies the most informative examples that benefit from human reasoning annotations. Experiments demonstrate that Active-Prompt significantly outperforms competitive baselines, achieving state-of-the-art results across eight reasoning datasets while reducing the need for extensive human engineering.

## Method Summary
Active-Prompt operates by first generating multiple predictions (k) for each question in a training pool using few-shot CoT prompting. Uncertainty is then estimated using three metrics: disagreement (variance in predicted answers), entropy (uncertainty in answer distribution), and variance (variance in answer probabilities). The method selects the top-n most uncertain questions for human annotation with chain-of-thought reasoning. These annotated exemplars are then used in few-shot prompting with self-consistency decoding to improve model performance on test questions. The approach iteratively reduces model uncertainty while improving accuracy through targeted human intervention.

## Key Results
- Active-Prompt outperforms competitive baselines (CoT, SC, Auto-CoT, Random-CoT) on eight reasoning datasets
- State-of-the-art results achieved with significantly fewer annotated exemplars
- Highly negative correlation observed between uncertainty and accuracy across tasks
- Performance improves with iterative selection and annotation of uncertain examples

## Why This Works (Mechanism)

### Mechanism 1
Uncertainty-based selection improves CoT performance by choosing examples where the model is uncertain. The method estimates uncertainty using disagreement, entropy, or variance across multiple model predictions, then selects the most uncertain examples for annotation. This reduces the chance of including redundant or easy examples. Core assumption: High uncertainty correlates with examples that are both challenging and informative for improving model reasoning.

### Mechanism 2
Human-annotated chain-of-thought reasoning on selected examples improves model generalization. Annotators provide step-by-step reasoning for selected questions, creating exemplars that guide the model through the reasoning process. These exemplars are then used in few-shot prompting. Core assumption: Structured reasoning steps help the model learn the decomposition of complex problems into solvable parts.

### Mechanism 3
Reducing model uncertainty leads to better few-shot prompting performance. By iteratively selecting and annotating uncertain examples, the model's uncertainty decreases, which improves its ability to handle test questions accurately. Core assumption: Lower uncertainty in predictions correlates with higher accuracy in reasoning tasks.

## Foundational Learning

- Concept: Uncertainty estimation in machine learning
  - Why needed here: To select the most informative examples for annotation, we need to quantify how uncertain the model is about its predictions.
  - Quick check question: What are three common ways to measure uncertainty in model predictions?

- Concept: Chain-of-thought reasoning
  - Why needed here: CoT is the prompting technique being improved; understanding its mechanics is essential to grasp why selecting better exemplars matters.
  - Quick check question: How does CoT differ from standard prompting in terms of intermediate steps?

- Concept: Active learning principles
  - Why needed here: The method borrows from active learning to efficiently select examples that maximize learning gain.
  - Quick check question: What is the main goal of uncertainty-based active learning?

## Architecture Onboarding

- Component map: Uncertainty estimation module -> Selection module -> Annotation module -> Inference module
- Critical path: Uncertainty estimation → Selection → Annotation → Inference
- Design tradeoffs:
  - k (number of predictions) vs. computational cost: Higher k gives better uncertainty estimates but increases latency.
  - Number of annotated examples (n) vs. performance gain: More annotations improve performance but increase cost.
  - Uncertainty metric choice (disagreement, entropy, variance) vs. task specificity: Some metrics may work better for certain tasks.
- Failure signatures:
  - Low accuracy improvement: Could indicate poor uncertainty-metric correlation or low-quality annotations.
  - High variance in results: May suggest instability in uncertainty estimation or selection.
  - Computational bottleneck: Likely due to high k or large pool size.
- First 3 experiments:
  1. Baseline: Run CoT with random exemplars on GSM8K; record accuracy.
  2. Uncertainty estimation: Generate k=5 predictions per question; compute disagreement and entropy.
  3. Selection and annotation: Select top-8 uncertain questions; annotate with CoT; run inference with new exemplars.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Active-Prompt scale with larger language models beyond the ones tested in this study? The paper primarily tests Active-Prompt with code-davinci-002 and text-davinci-002, but does not explore larger or different types of language models. Conducting experiments with larger language models and comparing their performance with Active-Prompt would determine if the benefits scale with model size.

### Open Question 2
Can the combination of diversity and uncertainty-based selection methods improve the selection of informative questions for annotation? The paper mentions that both diversity and uncertainty are useful for selecting informative questions and suggests combining them as a future direction. Implementing a hybrid selection method that incorporates both diversity and uncertainty metrics and evaluating its performance would provide evidence.

### Open Question 3
How does the quality of human annotation affect the performance of Active-Prompt, and can it be standardized across different annotators? The paper discusses the effects of different annotators and notes variations in performance, but does not explore standardization methods. Developing a standardized annotation protocol and comparing the performance of Active-Prompt using annotations from multiple annotators following the protocol would provide evidence.

## Limitations
- Evaluation limited to two model variants (text-davinci-002 and code-davinci-002) across eight reasoning datasets
- Human annotation process introduces potential variability in reasoning quality and consistency
- Computational overhead of generating multiple predictions per question may become prohibitive at scale

## Confidence
- High Confidence: The core mechanism of uncertainty-based exemplar selection is well-grounded in active learning principles and empirical results show consistent improvements over baselines.
- Medium Confidence: The specific uncertainty metrics are shown to work well, but optimal choice may depend on task characteristics not fully explored.
- Medium Confidence: The correlation between uncertainty and accuracy is demonstrated, but causal relationship stability across different model scales needs further validation.

## Next Checks
1. Test the method on non-arithmetic reasoning tasks (e.g., commonsense QA, multi-hop reasoning) to assess domain generalization and identify any task-specific limitations.
2. Conduct an ablation study varying k (number of predictions for uncertainty estimation) to determine the optimal tradeoff between computational cost and selection quality.
3. Compare Active-Prompt performance against ensemble-based approaches to isolate whether improvements come from better exemplar selection versus inherent benefits of ensembling.