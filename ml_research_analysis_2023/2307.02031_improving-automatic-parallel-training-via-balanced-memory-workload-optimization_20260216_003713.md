---
ver: rpa2
title: Improving Automatic Parallel Training via Balanced Memory Workload Optimization
arxiv_id: '2307.02031'
source_url: https://arxiv.org/abs/2307.02031
tags:
- memory
- parallelism
- galvatron-bmw
- training
- galvatron
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Galvatron-BMW, a novel system framework for
  automatic parallel training of Transformer models across multiple GPUs. The key
  innovation is a decision-tree-based search space decomposition that efficiently
  explores a large space of parallelism strategies including data parallelism, model
  parallelism, pipeline parallelism, and activation checkpointing.
---

# Improving Automatic Parallel Training via Balanced Memory Workload Optimization

## Quick Facts
- arXiv ID: 2307.02031
- Source URL: https://arxiv.org/abs/2307.02031
- Reference count: 40
- Key outcome: Up to 530% and 242% speedups over pure and hybrid parallelism baselines respectively for Transformer training

## Executive Summary
This paper introduces Galvatron-BMW, a system framework that automatically optimizes parallel training strategies for Transformer models across multiple GPUs. The key innovation is a decision-tree-based search space decomposition that efficiently explores hybrid parallelism strategies combining data parallelism, tensor parallelism, pipeline parallelism, and activation checkpointing. The system uses dynamic programming to find optimal strategies and bi-objective optimization to balance memory and computation workloads across pipeline stages, achieving significant throughput improvements over existing methods.

## Method Summary
Galvatron-BMW constructs a decision tree to decompose the large search space of hybrid parallelism strategies, leveraging hierarchical ordering and equal group size constraints to prune redundant combinations. A dynamic programming search algorithm then efficiently finds optimal parallelism strategies for each pipeline stage while respecting memory constraints. The system employs bi-objective optimization to iteratively balance memory and computation workloads across pipeline stages, maximizing system throughput. Cost estimation is used throughout to predict performance and guide strategy selection.

## Key Results
- Achieved up to 530% speedup compared to state-of-the-art pure parallelism methods
- Demonstrated up to 242% improvement over existing hybrid parallelism approaches
- Consistently outperformed baselines across various Transformer models (BERT, T5, ViT, Swin) and GPU memory constraints

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decision-tree-based decomposition reduces search space from hundreds to 44 strategies per layer
- **Mechanism:** The decision tree captures permutations of DP, TP, PP, SDP and CKPT in a non-overlapping way, enforcing equal group sizes and device-island PP placement
- **Core assumption:** PP should be applied first across high-bandwidth device islands, and equal-sized groups yield better load balance
- **Evidence anchors:** [abstract] decision tree approach for decomposition and pruning; [section] hierarchical search space exploration; [corpus] weak - no direct evidence in neighbors

### Mechanism 2
- **Claim:** Dynamic programming achieves O(LE|S|) complexity instead of O(LE²)
- **Mechanism:** Forward memory Efwd is optimized first, then backward memory is checked for feasibility, avoiding quadratic complexity from max operation
- **Core assumption:** Forward and backward memory can be optimized separately without losing optimality
- **Evidence anchors:** [abstract] dynamic programming search algorithm; [section] decouple forward memory from Eall; [corpus] weak - no explicit dynamic programming in neighbors

### Mechanism 3
- **Claim:** Bi-objective optimization balances pipeline time and memory workloads
- **Mechanism:** Iterative adjustment from memory-balanced to time-balanced partitions greedily reduces slowest stage's workload while maintaining memory feasibility
- **Core assumption:** Memory-balanced and time-balanced partitions lie on opposite ends of a Pareto frontier
- **Evidence anchors:** [abstract] bi-objective optimization workflow; [section] workload balance across pipeline stages; [corpus] weak - no direct bi-objective optimization in neighbors

## Foundational Learning

- **Concept:** Transformer model components (self-attention, feed-forward) and memory hierarchy
  - Why needed here: Understanding memory consumption patterns is essential for applying CKPT and partitioning strategies effectively
  - Quick check question: What is the difference between forward activations, boundary activations, and intermediate activations in a Transformer layer?

- **Concept:** Distributed training paradigms (DP, TP, PP, SDP) and their communication patterns
  - Why needed here: Choosing and combining parallelism strategies directly impacts memory and compute efficiency
  - Quick check question: Why does sharded data parallelism have 1.5× the communication overhead of pure data parallelism?

- **Concept:** Dynamic programming and optimal substructure in algorithm design
  - Why needed here: Used to efficiently search the large hybrid parallelism space under memory constraints
  - Quick check question: Why does the max operation in memory calculation cause quadratic complexity if not decoupled?

## Architecture Onboarding

- **Component map:** Search Space Construction -> Dynamic Programming Search -> Bi-objective Optimization -> Cost Estimator -> Execution Plan Generator
- **Critical path:** Model + GPU specs -> Decision-tree construction -> Dynamic programming search (with cost estimation) -> Iterative partition adjustment (if using BMW) -> Final strategy selection
- **Design tradeoffs:** More parallelism dimensions -> larger search space -> need for pruning (decision tree). Simpler models -> less benefit from CKPT
- **Failure signatures:** Out-of-memory errors despite feasible batch size -> memory estimation errors; low throughput vs expected -> suboptimal parallelism choice; long search time -> too many strategies or layers
- **First 3 experiments:**
  1. Run Galvatron-Base on a small BERT model (32 layers) with 8 GPUs, 16GB each; verify search time < 5 minutes and throughput > baseline
  2. Test Galvatron-BMW on T5-512/4 (imbalanced) with same setup; check that bi-objective adjustment improves throughput vs memory-balanced only
  3. Compare cost estimator accuracy: run real training with predicted strategy, measure actual vs predicted time/memory, aim for <10% error

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the decision-tree-based search space decomposition scale to models with thousands of layers or massive parameter counts?
- Basis in paper: [explicit] The paper mentions that for extreme cases with thousands of layers or huge memory capacity, coarse-grained explorations like fusing multiple layers or using large memory granularity can further reduce complexity
- Why unresolved: The paper does not provide experimental results or analysis of the system's performance in such extreme scenarios
- What evidence would resolve it: Experiments demonstrating the system's performance on models with thousands of layers or massive parameter counts, along with analysis of the trade-offs between search space granularity and optimization quality

### Open Question 2
- Question: What is the impact of non-homogeneous GPU clusters on the performance of Galvatron-BMW's automatic parallelism strategy?
- Basis in paper: [inferred] The paper assumes homogeneous GPUs and focuses on optimizing resource utilization within this assumption
- Why unresolved: The paper does not discuss or provide experimental results on heterogeneous GPU clusters
- What evidence would resolve it: Experiments comparing the performance of Galvatron-BMW on homogeneous and heterogeneous GPU clusters, along with analysis of the system's adaptability to different hardware configurations

### Open Question 3
- Question: How does Galvatron-BMW handle Transformer models with complex and dynamic structures, such as those with varying layer dimensions or conditional computation?
- Basis in paper: [explicit] The paper mentions that Galvatron-BMW hopes to facilitate future research directions on large DL models with complex and dynamic structures
- Why unresolved: The paper does not provide experimental results or analysis of the system's performance on Transformer models with complex and dynamic structures
- What evidence would resolve it: Experiments demonstrating the system's performance on Transformer models with varying layer dimensions or conditional computation, along with analysis of the system's ability to adapt to such models

## Limitations

- The effectiveness depends heavily on assumptions about equal-sized GPU groups and fixed pipeline parallelism ordering, which may not hold for heterogeneous clusters
- Memory estimation accuracy is critical for system performance, and errors can lead to out-of-memory failures despite feasible batch sizes
- The approach may add unnecessary complexity for workloads with minimal imbalance or uniform constraints

## Confidence

- **High Confidence:** Superior throughput compared to pure parallelism baselines is well-supported by experimental results
- **Medium Confidence:** Complexity reduction through decoupled memory optimization is theoretically sound but lacks rigorous proof
- **Low Confidence:** Performance claims may be highly dependent on specific model architectures and GPU configurations tested

## Next Checks

1. **Robustness to Device Heterogeneity:** Test Galvatron-BMW on heterogeneous GPU clusters where device groups have different memory capacities and communication bandwidths to validate the decision-tree's equal-grouping assumption

2. **Memory Estimation Accuracy:** Implement a controlled experiment comparing predicted vs. actual memory consumption across all pipeline stages when using the automatically selected parallelism strategy, targeting <10% estimation error

3. **Scalability Boundary Analysis:** Systematically vary the number of GPUs and model sizes to identify the point where the search space decomposition becomes ineffective or the dynamic programming search time becomes prohibitive