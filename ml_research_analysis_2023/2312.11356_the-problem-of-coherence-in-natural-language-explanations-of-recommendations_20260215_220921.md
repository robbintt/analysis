---
ver: rpa2
title: The Problem of Coherence in Natural Language Explanations of Recommendations
arxiv_id: '2312.11356'
source_url: https://arxiv.org/abs/2312.11356
tags:
- explanations
- coherence
- explanation
- rating
- generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper highlights a critical issue in explainable recommendation
  systems: the lack of coherence between generated textual explanations and predicted
  ratings. Despite comprehensive evaluation of explanation quality using various metrics,
  this coherence has been overlooked.'
---

# The Problem of Coherence in Natural Language Explanations of Recommendations

## Quick Facts
- arXiv ID: 2312.11356
- Source URL: https://arxiv.org/abs/2312.11356
- Reference count: 31
- Key outcome: CER improves explanation coherence by up to 6.5 percentage points compared to state-of-the-art methods

## Executive Summary
This paper addresses a critical gap in explainable recommendation systems: the lack of coherence between generated textual explanations and predicted ratings. Through manual verification of explanations from state-of-the-art methods, the authors discovered that up to 40% of explanations were incoherent with their corresponding ratings. To solve this, they propose a new method called Coherent Explainable Recommender (CER) that extends the transformer-based PETER+ architecture. CER introduces an additional intermediary task of explanation-based rating estimation, providing an extra training signal that encourages the generation of coherent explanations.

## Method Summary
The Coherent Explainable Recommender (CER) extends the PETER+ architecture by adding an explanation-based rating estimation task. The core of CER is a transformer module that processes user-item interactions, item features, and generates explanations. An auxiliary MLP network predicts the rating solely from the generated explanation's representation, and the model is trained to minimize the difference between this auxiliary prediction and the main rating prediction. This additional training signal encourages more coherent explanations. The method also includes a trainable coherence classifier based on BERT for automatic evaluation of explanation-rating coherence.

## Key Results
- CER achieved up to 6.5 percentage points higher coherence scores compared to PETER+ on various datasets
- Manual verification confirmed that up to 40% of explanations from state-of-the-art methods were incoherent with their ratings
- CER maintained comparable recommendation performance (MAE/RMSE) while significantly improving coherence

## Why This Works (Mechanism)

### Mechanism 1
The coherence classifier improves explanation-rating consistency by learning to evaluate whether generated explanations align with predicted ratings. The classifier uses a transformer-based language model (BERT) to assess if a generated explanation semantically matches the predicted rating by filling in sentence templates and checking if the result sounds correct. If the language model cannot learn the semantic mapping between rating values and appropriate explanation polarity, the classifier will fail to distinguish coherent from incoherent pairs.

### Mechanism 2
The explanation-based rating estimation task provides an additional training signal that encourages the generation of coherent explanations. An auxiliary MLP network predicts the rating solely from the generated explanation's representation, and the model is trained to minimize the difference between this auxiliary prediction and the main rating prediction. If the auxiliary network cannot learn to predict ratings from explanations effectively, the training signal will be weak or misleading.

### Mechanism 3
The transformer architecture with self-attention mechanisms enables the model to generate personalized explanations that reference relevant item features. The transformer layer processes a sequence containing user, item, item features, and explanation tokens, using self-attention to create contextual representations that inform both rating prediction and explanation generation. If the transformer cannot effectively model the complex relationships between users, items, and features, the generated explanations will lack personalization and coherence.

## Foundational Learning

- **Natural Language Generation (NLG)**: Why needed here - The system generates natural language explanations for recommendations, requiring understanding of text generation techniques. Quick check question: How does a transformer-based model differ from a sequence-to-sequence model for text generation?

- **Attention Mechanisms**: Why needed here - The transformer uses self-attention to capture relationships between different parts of the input sequence, which is crucial for generating coherent explanations. Quick check question: What is the role of the masking matrix in transformer self-attention for this application?

- **Transfer Learning**: Why needed here - The coherence classifier leverages pre-trained BERT, demonstrating how transfer learning can be applied to new tasks with limited data. Quick check question: Why is using a pre-trained language model beneficial for the coherence classification task?

## Architecture Onboarding

- **Component map**: User embeddings + Item embeddings + Item features → Transformer layers → Rating prediction + Explanation generation → Context prediction + Explanation-based rating estimation → Coherence evaluation
- **Critical path**: User and item embeddings → Transformer layers → Rating prediction and explanation generation → Coherence evaluation
- **Design tradeoffs**: Using pre-trained BERT improves coherence classification but adds complexity; multiple auxiliary tasks improve coherence but increase training time; greedy decoding is simple but may produce less diverse explanations
- **Failure signatures**: Low coherence scores indicate explanations don't match ratings; poor BLEU/ROUGE scores indicate low text quality; high FMR but low coherence suggests feature mentions don't translate to coherent explanations
- **First 3 experiments**: 1) Train CER on a small dataset and manually evaluate 50 explanation-rating pairs for coherence; 2) Compare CER's automatic coherence metric scores against PETER+ on the same dataset; 3) Test CER's recommendation performance (MAE/RMSE) to ensure it matches PETER+

## Open Questions the Paper Calls Out

### Open Question 1
How can we effectively measure and improve the factuality of natural language explanations in recommendation systems beyond coherence with predicted ratings? The authors acknowledge that explanations provided by CER may lack factuality, semantic coherence, or causality, indicating these as important open issues. Development and validation of metrics that can assess explanation factuality, along with experimental results showing improved factuality scores in generated explanations, would resolve this question.

### Open Question 2
What are the most effective methods for constructing high-quality benchmarking datasets for explainable recommendation systems that minimize noise and inconsistency? The authors note that up to 30% of reference explanations in commonly used datasets are incoherent, highlighting the need for better benchmarking datasets. Comparative studies showing reduced noise levels in newly constructed datasets versus existing ones, along with improved model performance when trained on cleaner data, would resolve this question.

### Open Question 3
How can we extend the explanation-based rating estimation task to handle more complex recommendation scenarios, such as multi-aspect ratings or diverse user preferences? The paper introduces explanation-based rating estimation as a key innovation for improving coherence, but only demonstrates it for single rating predictions. Experimental results showing successful extension of the method to handle multi-dimensional ratings or more nuanced user preference modeling would resolve this question.

## Limitations

- The evaluation relies on datasets with relatively small reference explanations (Amazon: 24,281, TripAdvisor: 21,425, Yelp: 50,000), which may limit generalizability to larger-scale systems
- The coherence classifier's reliance on BERT introduces potential bias toward the language patterns present in the pre-training corpus, which may not fully align with recommendation domain language
- The explanation-based rating estimation task could potentially lead to overly conservative explanations that prioritize coherence over informativeness

## Confidence

- Coherence improvement claims: Medium confidence - well-supported by both automatic and manual evaluations, but limited by small dataset sizes and manual annotation sample
- Methodology and evaluation framework: High confidence - comprehensive and well-designed approach with appropriate metrics
- Real-world applicability: Low confidence - paper doesn't explore long-term effects on user satisfaction or trust, and scalability challenges are not addressed

## Next Checks

1. Implement CER and verify that the automatic coherence metric correlates with manual annotation results on a small test set
2. Test CER's scalability by training on progressively larger subsets of the Yelp dataset and measuring coherence improvements
3. Conduct a user study to evaluate whether the improved coherence translates to better user understanding and trust in the recommendations