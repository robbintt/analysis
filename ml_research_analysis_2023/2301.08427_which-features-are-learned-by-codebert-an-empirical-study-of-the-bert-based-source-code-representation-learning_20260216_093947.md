---
ver: rpa2
title: 'Which Features are Learned by CodeBert: An Empirical Study of the BERT-based
  Source Code Representation Learning'
arxiv_id: '2301.08427'
source_url: https://arxiv.org/abs/2301.08427
tags:
- code
- language
- learning
- names
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the learning capabilities of BERT-based
  models for source code representation, specifically CodeBERT and GraphCodeBERT.
  The authors hypothesize that these models rely too heavily on literal features (variable
  and function names) rather than understanding code logic.
---

# Which Features are Learned by CodeBert: An Empirical Study of the BERT-based Source Code Representation Learning

## Quick Facts
- arXiv ID: 2301.08427
- Source URL: https://arxiv.org/abs/2301.08427
- Reference count: 1
- Key outcome: BERT-based models like CodeBERT and GraphCodeBERT rely heavily on literal features (variable and function names) rather than code logic, showing significant performance drops when names are anonymized.

## Executive Summary
This study investigates whether BERT-based models for source code representation, specifically CodeBERT and GraphCodeBERT, learn code logic or rely on surface-level literal features. Through experiments that anonymize user-defined names in code using random and meaningful strings, the authors demonstrate that these models show significant performance degradation on code search and clone detection tasks when variable, method, or function names are replaced. The results indicate that current BERT-based models are not effective at learning deeper code semantics, which has implications for applications requiring sophisticated program analysis like vulnerability detection and code patching.

## Method Summary
The authors conduct experiments using the original dataset from Guo et al. (2020) containing Java and Python source code. They create anonymized variants by replacing variable names, method definition names, and method invocation names using two strategies: random string generation and meaningful but misleading string replacement. The CodeBERT and GraphCodeBERT models are then retrained on these anonymized datasets and evaluated on code search and clone detection tasks. Performance is measured using accuracy metrics and compared against models trained on the original, unanonymized data.

## Key Results
- CodeBERT and GraphCodeBERT show significant performance drops when variable, method, or function names are anonymized in code search and clone detection tasks.
- "Meaningfully-generated" anonymization strings (contextually relevant but misleading) perform worse than randomly-generated strings, suggesting models overfit to lexical patterns.
- The models demonstrate heavy reliance on literal features rather than understanding underlying code logic or structure.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERT-based models like CodeBERT and GraphCodeBERT learn representations heavily dependent on literal features such as variable, method, and function names rather than code logic.
- Mechanism: The models tokenize user-defined names as part of the input sequence and rely on these tokens for downstream task performance. When names are replaced with meaningless strings, performance drops significantly.
- Core assumption: Literal features (names) are more salient in the training data and easier for the model to associate with task outcomes than semantic code structure.
- Evidence anchors:
  - [abstract] "Results on code search and clone detection tasks show significant performance drops when variable, method, or function names are anonymized, indicating the models' heavy reliance on literal features."
  - [section 3] "The results show that the anonymization of the variable names, method definition names, and method invocation names will result in a huge downgrade in model performance..."
- Break condition: If the model could learn robust structural embeddings independent of names, anonymization would not cause such large performance drops.

### Mechanism 2
- Claim: The "meaningfully-generated" anonymization strategy (replacing names with contextually relevant but misleading terms) performs worse than "randomly-generated" strings.
- Mechanism: Models overfit to surface-level lexical cues; meaningful-sounding replacements create false associations that mislead the model more than random strings.
- Core assumption: The model does not perform deep semantic parsing but instead matches lexical patterns.
- Evidence anchors:
  - [section 3.1] "on average the dataset with meaningfully-generated strings shows worse result then the dataset with randomly-generated strings, which indicates that 'meaningfully-generated' strings could misleading the models."
- Break condition: If the model truly understood code semantics, both anonymization strategies would degrade performance equally.

### Mechanism 3
- Claim: The tokenizer and training objective of BERT-based models prioritize lexical matching over structural reasoning.
- Mechanism: The masked language modeling objective encourages prediction of tokens based on surrounding context, which includes names. Structural features like data flow or control flow are not explicitly modeled or weighted equally.
- Core assumption: The pre-training tasks and tokenization strategy are not designed to capture deeper program semantics.
- Evidence anchors:
  - [section 3] "Currently, GraphCodeBert takes code pieces of functions or class methods as data samples. It tokenizes keywords, operators, and user-defined names from the code pieces."
- Break condition: If the model could attend to structural dependencies beyond token sequences, anonymization would have less impact.

## Foundational Learning

- Concept: Bidirectional Encoder Representations from Transformers (BERT)
  - Why needed here: The paper critiques BERT-based models; understanding their architecture is essential to grasp the limitations.
  - Quick check question: What is the key architectural difference between BERT and autoregressive models like GPT?

- Concept: Tokenization and subword units in NLP
  - Why needed here: The model's reliance on user-defined names stems from how code is tokenized into input sequences.
  - Quick check question: How does the BERT tokenizer handle out-of-vocabulary words in code?

- Concept: Masked Language Modeling (MLM) objective
  - Why needed here: MLM is the primary pre-training task; its focus on predicting masked tokens explains the model's lexical bias.
  - Quick check question: What type of information does MLM encourage the model to learn?

## Architecture Onboarding

- Component map: Input tokenizer → Transformer encoder → Task-specific head (e.g., classification, similarity)
- Critical path: Tokenization → Embedding lookup → Multi-head attention → Feed-forward → Output representation
- Design tradeoffs: Trade-off between learning lexical patterns (easy, surface-level) vs. structural semantics (hard, deeper)
- Failure signatures: Large performance drops on anonymization datasets; high sensitivity to variable/method naming
- First 3 experiments:
  1. Anonymize variable names with randomly-generated strings and evaluate on code search.
  2. Anonymize method definition names with meaningfully-generated strings and evaluate on clone detection.
  3. Anonymize all three name categories and compare performance drop to individual anonymizations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop models that effectively learn and utilize logic features in source code rather than relying heavily on literal features?
- Basis in paper: [explicit] The paper demonstrates that current BERT-based models (CodeBERT and GraphCodeBERT) heavily rely on literal features (variable and function names) and struggle to learn logic features in source code.
- Why unresolved: The paper identifies the problem but does not propose specific solutions for developing models that can better understand code logic.
- What evidence would resolve it: A new model architecture or training methodology that shows significantly better performance on code analysis tasks when variable and function names are anonymized, approaching or matching the performance on unanonymized code.

### Open Question 2
- Question: What specific aspects of code logic are most critical for various program analysis tasks, and how can models be designed to capture these aspects?
- Basis in paper: [inferred] The paper mentions that logical analysis is more important for sophisticated program analysis tasks like vulnerability analysis and patching generation, but does not specify which aspects of code logic are most crucial.
- Why unresolved: While the paper highlights the importance of logic features, it does not delve into which specific elements of code logic are most valuable for different analysis tasks.
- What evidence would resolve it: A comprehensive study analyzing the performance of various program analysis models on tasks with anonymized code, identifying which aspects of code logic (e.g., control flow, data dependencies, algorithmic patterns) contribute most to successful analysis.

### Open Question 3
- Question: How can we evaluate and improve the robustness of code representation models against adversarial naming strategies?
- Basis in paper: [explicit] The paper shows that meaningfully-generated strings (which have literal meaning but do not reflect the actual code logic) perform worse than randomly-generated strings, indicating models can be misled by such naming strategies.
- Why unresolved: The paper demonstrates the vulnerability to naming-based attacks but does not explore methods to make models more robust against such strategies.
- What evidence would resolve it: A new training approach or model architecture that maintains high performance across a wide range of naming strategies (random, meaningful but misleading, etc.) in various program analysis tasks.

## Limitations

- The study's findings are limited by the specific choice of anonymization strategies, which may not capture all ways literal features could be replaced while preserving code semantics.
- Evaluation is restricted to code search and clone detection tasks, leaving open whether findings generalize to other downstream applications like bug detection or code summarization.
- The study does not investigate whether fine-tuning or architectural modifications could mitigate the observed limitations, focusing instead on default behavior of pre-trained models.

## Confidence

**High Confidence**: The claim that CodeBERT and GraphCodeBERT models show significant performance drops when user-defined names are anonymized is well-supported by empirical results across multiple experiments and datasets.

**Medium Confidence**: The assertion that models rely "too heavily" on literal features is somewhat subjective and depends on specific task requirements.

**Low Confidence**: The broader implications about current BERT-based models being "not effective at learning deeper code logic" extend beyond the specific experimental evidence.

## Next Checks

1. **Generalization Test**: Evaluate the same anonymization strategies on additional downstream tasks (e.g., method name prediction, code summarization) to determine whether the observed reliance on literal features is task-specific or a general limitation of the architecture.

2. **Alternative Anonymization**: Implement and test additional anonymization strategies that preserve semantic relationships (e.g., consistent renaming within a codebase) to determine whether performance drops are due to name information loss or the specific replacement approach used.

3. **Architectural Intervention**: Modify the training procedure to explicitly encourage learning of structural features (e.g., by incorporating data flow or control flow information) and evaluate whether this reduces sensitivity to name anonymization while maintaining or improving performance on original datasets.