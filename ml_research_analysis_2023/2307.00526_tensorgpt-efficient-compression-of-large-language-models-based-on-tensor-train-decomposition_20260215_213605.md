---
ver: rpa2
title: 'TensorGPT: Efficient Compression of Large Language Models based on Tensor-Train
  Decomposition'
arxiv_id: '2307.00526'
source_url: https://arxiv.org/abs/2307.00526
tags:
- embedding
- token
- tensor
- matrix
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TensorGPT, a training-free approach to compress
  embedding layers in large language models (LLMs) using Tensor-Train Decomposition
  (TTD). The method treats each token embedding as a Matrix Product State (MPS), enabling
  efficient storage and computation while preserving model performance.
---

# TensorGPT: Efficient Compression of Large Language Models based on Tensor-Train Decomposition

## Quick Facts
- arXiv ID: 2307.00526
- Source URL: https://arxiv.org/abs/2307.00526
- Reference count: 15
- Achieves up to 38.40× compression of embedding layer with 3.31× compression maintaining or improving text generation performance

## Executive Summary
This paper introduces TensorGPT, a training-free approach to compress embedding layers in large language models (LLMs) using Tensor-Train Decomposition (TTD). The method treats each token embedding as a Matrix Product State (MPS), enabling efficient storage and computation while preserving model performance. Evaluated on GPT-2, TensorGPT achieves up to 38.40× compression of the embedding layer with a 3.31× compression rate maintaining or improving text generation performance compared to the original model. The approach demonstrates that approximately half of the parameters in embedding layers are redundant, offering significant storage savings for deployment on resource-constrained devices.

## Method Summary
TensorGPT compresses embedding layers by factorizing each token embedding using Tensor-Train Decomposition (TTD) into a sequence of smaller 3rd-order tensors (core tensors). The method converts each token embedding into a Matrix Product State (MPS) format, where the original high-dimensional vector is approximated by a product of smaller tensors with shared dimensions (TT-ranks). Each token embedding is processed individually by padding to power-of-2 dimensions (1024), reshaping into an order-11 tensor, and applying TT-SVD decomposition. This enables flexible vocabulary updates since new tokens can be decomposed separately without recomputing the full decomposition. The approach is training-free and requires no fine-tuning of the original model.

## Key Results
- Achieves up to 38.40× compression of the embedding layer with minimal performance degradation
- Maintains or improves text generation performance with 3.31× compression rate
- Demonstrates approximately half of embedding layer parameters are redundant
- Enables flexible vocabulary updates through individual token decomposition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tensor-Train Decomposition (TTD) enables efficient compression of high-dimensional embeddings by factorizing them into a sequence of smaller 3rd-order tensors (core tensors)
- Mechanism: The TTD algorithm converts each token embedding into a Matrix Product State (MPS), where the original high-dimensional vector is approximated by a product of smaller tensors with shared dimensions (TT-ranks). This reduces the parameter count from exponential to linear in the tensor order.
- Core assumption: The original token embeddings contain redundant information that can be captured by a low-rank tensor representation.
- Evidence anchors:
  - [abstract] "treats each pre-trained token embedding as a Matrix Product State (MPS), enabling efficient storage and computation while preserving model performance"
  - [section 4.1] "the Matrix Product State (MPS or TT-MPS), introduced in the quantum physics community [11], which applies the Tensor-Train Singular Value Decomposition (TT-SVD) algorithm"
- Break condition: When the TT-rank is set too low, the approximation error becomes significant and model performance degrades substantially.

### Mechanism 2
- Claim: Decomposing each token embedding individually (rather than the entire embedding matrix) enables flexible vocabulary updates without recomputing the full decomposition.
- Mechanism: By treating each token embedding as an independent MPS, new tokens can be added to the vocabulary by computing their decomposition separately, avoiding the computational cost of re-decomposing the entire embedding matrix.
- Core assumption: Token embeddings are largely independent in their semantic content, allowing for individual decomposition without significant loss of cross-token relationships.
- Evidence anchors:
  - [section 4.2] "we propose an approach that decomposes and stores each row of the embedding matrix separately. This reduces the computation costs significantly, as the decomposition can be performed locally on every new token"
  - [abstract] "achieving up to 38.40× compression of the embedding layer with a 3.31× compression rate maintaining or improving text generation performance"
- Break condition: When vocabulary changes are frequent and the computational overhead of individual decompositions outweighs the benefits of avoiding full matrix re-computation.

### Mechanism 3
- Claim: The over-parameterization of original LLMs creates redundant parameters that can be eliminated through low-rank approximation without performance loss.
- Mechanism: The original GPT-2 embedding layer contains approximately 31% of the model's parameters, many of which are redundant. The TTD approach identifies and removes this redundancy while preserving essential semantic information.
- Core assumption: Modern LLMs are deliberately over-parameterized, creating opportunities for compression without functional degradation.
- Evidence anchors:
  - [abstract] "demonstrates that approximately half of the parameters in embedding layers are redundant"
  - [section 1] "the embedding layer can be compressed by a factor of up to 38.40 times, and when the compression factor is 3.31 didn't sacrifice model performance"
  - [section 5.4] "we found that with the reconstructed embedding layer from the stored MPS, the overall performance of the GPT-2 even improved under certain TT rank settings, this is likely due to the over-parameterization of the original model"
- Break condition: When the model is not over-parameterized (e.g., smaller models or when embedding dimensions are already minimal), the compression may lead to performance degradation.

## Foundational Learning

- Tensor-Train Decomposition (TTD):
  - Why needed here: TTD is the mathematical foundation that enables the compression of high-dimensional token embeddings into a low-rank format while preserving semantic information.
  - Quick check question: What is the computational complexity reduction when converting from a D-dimensional vector to an N-dimensional tensor with TT-rank R using TTD?

- Matrix Product State (MPS):
  - Why needed here: MPS is the specific form of TTD used to represent each token embedding as a product of smaller tensors, enabling efficient computation and storage.
  - Quick check question: How does the element-wise representation xi1,i2,...,iN = Gp1q:,i1,: · ... · GpN q:,iN,: in MPS relate to the original token embedding vector?

- Tensor operations and matricization:
  - Why needed here: Understanding tensor operations like unfolding and contraction is essential for implementing the TTD algorithm and working with the compressed embeddings.
  - Quick check question: What is the relationship between mode-n matricization and the reconstruction of the original tensor from its TT cores?

## Architecture Onboarding

- Component map:
  Input tokenizer (BPE) -> Token embedding layer (original GPT-2) -> TTD compression -> MPS-formatted embeddings -> Reconstructed embeddings -> GPT-2 model blocks

- Critical path:
  1. Tokenization and embedding extraction
  2. Tensorization of each token embedding (padding to power-of-2 dimensions)
  3. TT-SVD decomposition to obtain core tensors
  4. Storage of MPS cores instead of original embeddings
  5. Reconstruction during inference by contracting the MPS cores

- Design tradeoffs:
  - Compression ratio vs. reconstruction fidelity: Higher compression (lower TT-ranks) reduces storage but increases approximation error
  - Computational overhead: Individual decomposition of tokens adds preprocessing cost but enables dynamic vocabulary updates
  - Padding requirements: Zero-padding to power-of-2 dimensions increases storage slightly but enables optimal tensor factorization

- Failure signatures:
  - Generation loss increases significantly beyond baseline when TT-ranks are too low
  - Reconstruction error (MAE/norm-MAE) spikes for specific vocabulary regions
  - Inference latency increases due to additional tensor contractions during embedding reconstruction

- First 3 experiments:
  1. Baseline measurement: Run GPT-2 with original embeddings on GLUE/MRPC dataset and record generation loss, MAE, and norm-MAE
  2. Compression validation: Apply TTD with varying TT-ranks (1,2,4,8,16) and measure compression ratio, reconstruction error, and generation loss for each setting
  3. Dynamic update test: Simulate vocabulary expansion by adding new tokens and measure decomposition time for individual tokens vs. full matrix re-computation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the TensorGPT approach perform on larger language models beyond GPT-2, such as GPT-3 or GPT-4, in terms of compression ratio and task performance?
- Basis in paper: [inferred] The paper only evaluated TensorGPT on GPT-2 and CerebrasGPT, but did not test on larger models like GPT-3 or GPT-4.
- Why unresolved: The authors did not extend their experiments to larger language models, so the performance on these models is unknown.
- What evidence would resolve it: Experiments on larger models showing compression ratios and task performance metrics would provide this evidence.

### Open Question 2
- Question: What is the impact of TensorGPT compression on downstream tasks like question answering, summarization, or translation, beyond the text reconstruction task evaluated in the paper?
- Basis in paper: [inferred] The paper only evaluated TensorGPT on a text reconstruction task, but did not test its impact on other downstream NLP tasks.
- Why unresolved: The authors did not conduct experiments on other downstream tasks, so the performance impact is unknown.
- What evidence would resolve it: Experiments on various downstream tasks showing performance metrics would provide this evidence.

### Open Question 3
- Question: How does the TensorGPT approach compare to other model compression techniques, such as pruning or quantization, in terms of compression ratio, task performance, and computational efficiency?
- Basis in paper: [inferred] The paper does not compare TensorGPT to other compression techniques, so its relative performance is unknown.
- Why unresolved: The authors did not conduct comparative experiments with other compression techniques.
- What evidence would resolve it: Comparative experiments with other compression techniques showing metrics like compression ratio, task performance, and computational efficiency would provide this evidence.

### Open Question 4
- Question: What is the theoretical limit of compression achievable with TensorGPT, and how does it relate to the intrinsic dimensionality of the embedding space?
- Basis in paper: [explicit] The authors state that about half of the parameters in the embedding layers are redundant, but do not explore the theoretical limits of compression.
- Why unresolved: The authors did not investigate the theoretical limits of compression or its relation to the intrinsic dimensionality of the embedding space.
- What evidence would resolve it: Theoretical analysis or empirical experiments exploring the limits of compression and its relation to the intrinsic dimensionality of the embedding space would provide this evidence.

## Limitations
- The method requires padding token embeddings to power-of-2 dimensions (1024), which introduces minimal but non-negligible storage overhead
- The compression is currently demonstrated only on GPT-2 architecture, limiting generalizability to other LLM architectures
- The computational overhead of reconstructing embeddings during inference via tensor contractions is not fully characterized
- The approach assumes the embedding layer is the primary target for compression, but other components (attention matrices, feed-forward networks) may offer different trade-offs

## Confidence
- Core TTD mechanism: High confidence
- Vocabulary flexibility claims: Medium confidence
- Performance improvement claims: Medium confidence

## Next Checks
1. **Cross-architecture validation**: Apply TensorGPT to BERT and RoBERTa embedding layers to verify compression effectiveness generalizes beyond GPT-style models
2. **Inference latency characterization**: Measure the end-to-end impact on inference speed, including the tensor contraction overhead for embedding reconstruction
3. **Dynamic vocabulary stress test**: Evaluate the computational cost and memory overhead when simulating frequent vocabulary updates with hundreds of new tokens added incrementally