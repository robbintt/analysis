---
ver: rpa2
title: How Do Large Language Models Capture the Ever-changing World Knowledge? A Review
  of Recent Advances
arxiv_id: '2310.07343'
source_url: https://arxiv.org/abs/2310.07343
tags:
- knowledge
- language
- methods
- learning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys recent advances in aligning large language models
  (LLMs) with ever-changing world knowledge without full retraining. The authors categorize
  methods into implicit (modifying model parameters) and explicit (using external
  resources).
---

# How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances

## Quick Facts
- arXiv ID: 2310.07343
- Source URL: https://arxiv.org/abs/2310.07343
- Reference count: 40
- Key outcome: Categorizes methods for updating LLMs with new knowledge into implicit (parameter modification) and explicit (external resources) approaches, highlighting trade-offs and key challenges.

## Executive Summary
This paper surveys recent advances in aligning large language models with ever-changing world knowledge without requiring full retraining. The authors categorize approaches into implicit methods that modify model parameters (like knowledge editing and continual learning) and explicit methods that leverage external resources (like retrieval-augmented generation). The paper provides a comprehensive taxonomy and comparison of these approaches, highlighting their trade-offs in terms of scalability, side effects, and efficiency. Key challenges identified include robust knowledge editing, efficient continual learning, resolving knowledge conflicts, and comprehensive evaluation benchmarks.

## Method Summary
The paper reviews and categorizes methods for updating LLMs with new knowledge. Implicit methods modify model parameters directly through techniques like knowledge editing (ROME, MEMIT) that target specific knowledge neurons, or continual learning that gradually updates models with new data. Explicit methods use external resources without modifying the base model, including retrieval-augmented generation that incorporates current information through in-context learning, memory-enhanced systems, and internet-based approaches. The paper compares these methods across multiple dimensions and identifies open challenges in the field.

## Key Results
- Knowledge editing techniques can modify specific factual associations in LLMs by targeting knowledge neurons without affecting general capabilities
- Retrieval-augmented generation can ground LLM outputs in current information without modifying the base model
- Both implicit and explicit methods face challenges with knowledge conflicts when updating models with new information
- Explicit methods are increasingly popular for updating LLMs while keeping the base model unchanged

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models (LLMs) implicitly store world knowledge in their parameters during pre-training on massive corpora.
- Mechanism: The self-attention and feed-forward layers in transformer architectures learn to encode relational and factual information into weight matrices through exposure to diverse text sources.
- Core assumption: The knowledge is stored in a distributed, implicit manner rather than as explicit facts.
- Evidence anchors:
  - [abstract] "Although large language models (LLMs) are impressive in solving various tasks, they can quickly be outdated after deployment."
  - [section] "Previous studies have shown that LLMs can implicitly memorize knowledge in their large number of parameters after being pre-trained on massive corpora (Petroni et al., 2019; Roberts et al., 2020; Jiang et al., 2020; Singhal et al., 2022)."
  - [corpus] Weak - the corpus provides related papers but doesn't directly anchor this claim about implicit storage.
- Break condition: If the model is trained on insufficient or biased data, the implicit knowledge representation becomes unreliable or incomplete.

### Mechanism 2
- Claim: Knowledge editing techniques can modify specific factual associations in LLMs without affecting general capabilities.
- Mechanism: By identifying and updating the weights or activations associated with specific knowledge neurons, the model's output for particular facts can be changed while preserving other knowledge.
- Core assumption: Knowledge is localized in specific neurons or weight subspaces that can be isolated and modified.
- Evidence anchors:
  - [section] "Dai et al. (2022) introduce the knowledge neurons concept and propose a gradient-based knowledge attribution method to identify these knowledge neurons in FFNs."
  - [section] "Meng et al. (2022a) conduct casual tracing analysis on GPT-2 and hypothesize that the Transformer MLP can be viewed as a linear associative memory."
  - [corpus] Weak - the corpus doesn't directly address knowledge localization evidence.
- Break condition: If knowledge is not truly localized but distributed across the network, editing specific neurons may have unintended ripple effects.

### Mechanism 3
- Claim: Retrieval-augmented generation can ground LLM outputs in current information without modifying the base model.
- Mechanism: An external retriever finds relevant documents from knowledge sources (web, memory, databases), and the LLM uses in-context learning to incorporate this information during generation.
- Core assumption: LLMs can effectively use retrieved context through prompting without additional training.
- Evidence anchors:
  - [section] "Leveraging an off-the-shelf retriever and the in-context learning ability of LLMs (Brown et al., 2020), this line of work designs better retrieval strategies to incorporate world knowledge into a fixed LLM through prompting."
  - [section] "Press et al. (2023); Jiang et al. (2023) interleave reasoning with web search."
  - [corpus] Weak - the corpus doesn't provide direct evidence about retrieval-augmented generation effectiveness.
- Break condition: If the retrieved information is noisy, contradictory, or exceeds context limits, the LLM may ignore it or produce degraded outputs.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: Understanding how LLMs process and store information is essential for grasping why implicit knowledge storage works and how knowledge editing targets specific components.
  - Quick check question: How do self-attention weights contribute to the implicit storage of knowledge in transformer models?

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Continual learning approaches must address this phenomenon to maintain previously learned knowledge while acquiring new information.
  - Quick check question: What regularization techniques can prevent catastrophic forgetting when fine-tuning LLMs on new data?

- Concept: In-context learning capabilities
  - Why needed here: Retrieval-augmented methods rely on LLMs' ability to use external information provided through prompts without additional training.
  - Quick check question: How does the number and quality of examples in a prompt affect an LLM's ability to perform in-context learning?

## Architecture Onboarding

- Component map: LLM (parameter store) -> Knowledge editing modules (parameter modifiers) -> External retrieval systems (knowledge sources)
- Critical path: For knowledge editing: identify target knowledge → locate relevant parameters/neurons → compute parameter updates → apply edits → validate generalization. For retrieval-augmented methods: receive query → retrieve relevant documents → generate grounded response → validate factual accuracy.
- Design tradeoffs: Knowledge editing offers fine-grained control but risks side effects and requires understanding of model internals. Retrieval methods avoid modifying the base model but add latency and dependency on external sources. Continual learning updates at scale but requires careful management of forgetting and conflicts.
- Failure signatures: Knowledge editing failures manifest as degraded performance on unrelated tasks or inconsistent edits across semantically equivalent inputs. Retrieval failures show as irrelevant or ignored context, while continual learning failures appear as catastrophic forgetting of previously learned knowledge.
- First 3 experiments:
  1. Test knowledge editing on a small, well-defined fact in a public LLM (like GPT-2) and verify the edit generalizes to semantically equivalent questions.
  2. Implement a simple kNN-LM retriever with a pre-trained model and measure performance improvement on questions requiring recent information.
  3. Set up a continual learning scenario where a model learns new domain knowledge while retaining performance on original tasks, measuring forgetting through benchmark comparisons.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the most effective method for updating factual knowledge in large language models while minimizing interference with unrelated knowledge?
- Basis in paper: [explicit] The paper compares implicit and explicit methods for aligning LLMs with world knowledge, noting that implicit methods modify model parameters while explicit methods use external resources. It discusses the trade-offs between scalability, side effects, and efficiency of different approaches.
- Why unresolved: The paper provides a comprehensive review and comparison of various methods, but does not offer a definitive answer on which approach is most effective overall. The effectiveness of different methods depends on specific use cases and requirements, and there is a lack of quantitative comparison between methods of different categories.
- What evidence would resolve it: A large-scale empirical study comparing the performance of different methods (implicit vs. explicit, knowledge editing vs. continual learning, etc.) on a diverse set of benchmarks and real-world scenarios would help determine the most effective approach for updating factual knowledge in LLMs.

### Open Question 2
- Question: How can knowledge conflicts be effectively resolved when updating large language models with new information?
- Basis in paper: [explicit] The paper discusses the challenge of knowledge conflicts when updating LLMs, noting that both implicit and explicit methods can cause conflicts. It mentions that for implicit methods, the side effects on general skills are not well understood, while for retrieval-based methods, the retrieved context can contradict the knowledge memorized inside LLMs.
- Why unresolved: The paper acknowledges the problem of knowledge conflicts but does not provide a comprehensive solution. Existing attempts to address this issue are limited, and there is no clear understanding of how to effectively resolve conflicts between new and existing knowledge in LLMs.
- What evidence would resolve it: Developing and evaluating novel methods for detecting and resolving knowledge conflicts in LLMs, along with rigorous empirical studies demonstrating their effectiveness across various scenarios and knowledge domains, would help address this open question.

### Open Question 3
- Question: What are the most efficient and scalable approaches for continually updating large language models with new information over time?
- Basis in paper: [inferred] The paper discusses the importance of keeping LLMs up-to-date with ever-changing world knowledge and mentions the challenges of efficiently updating models at scale. It notes that knowledge editing methods are limited in their ability to update large amounts of knowledge, while continual learning methods can update more knowledge but are more computationally expensive.
- Why unresolved: While the paper provides an overview of different approaches for updating LLMs, it does not identify the most efficient and scalable methods for continual updating. The trade-offs between efficiency, scalability, and effectiveness are not fully explored, and there is a lack of research on how to effectively update LLMs at scale over time.
- What evidence would resolve it: Conducting empirical studies comparing the efficiency and scalability of different approaches (e.g., knowledge editing, continual learning, retrieval-based methods) for updating LLMs with new information over extended periods, along with developing novel techniques to improve the efficiency and scalability of these methods, would help answer this open question.

## Limitations

- Most evaluated methods focus on GPT-2 scale models, with uncertain performance on larger architectures
- Limited empirical evidence for knowledge localization claims across different model families
- Lack of comprehensive benchmarks for evaluating knowledge updates, especially for real-world scenarios
- Potential conflicts between multiple knowledge updates not well-studied

## Confidence

High confidence in identifying the two main categories of approaches (implicit and explicit) for updating LLMs with new knowledge, as this is well-supported by the cited literature and aligns with established research patterns.

Medium confidence around specific mechanism claims, particularly regarding knowledge localization in neurons and the effectiveness of knowledge editing without side effects. While the review cites relevant work, the evidence for these mechanisms being universally applicable across model scales and architectures remains limited.

Low confidence in the scalability claims for many methods beyond the GPT-2 scale, as the review notes that most evaluations focus on smaller models and the behavior of these approaches on frontier LLMs remains largely untested.

## Next Checks

1. **Knowledge editing generalization test**: Apply ROME or MEMIT to a small, well-defined fact in a public LLM (like GPT-2) and verify the edit generalizes to semantically equivalent questions across at least 5 paraphrase variations.

2. **Retrieval-augmented scaling evaluation**: Implement a simple kNN-LM retriever with a pre-trained model and measure performance degradation as context window size increases from 4K to 32K tokens, identifying the point where retrieval becomes ineffective.

3. **Continual learning conflict resolution**: Set up a multi-domain continual learning scenario where a model sequentially learns knowledge from three distinct domains, then measure knowledge interference and conflict resolution by testing cross-domain generalization.