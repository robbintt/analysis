---
ver: rpa2
title: Generalized Activation via Multivariate Projection
arxiv_id: '2309.17194'
source_url: https://arxiv.org/abs/2309.17194
tags:
- relu
- function
- projection
- activation
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new activation function called Multivariate
  Projection Unit (MPU) by generalizing the Rectified Linear Unit (ReLU) from a single-input
  single-output (SISO) function to a multi-input multi-output (MIMO) function. The
  key idea is to view ReLU as a projection onto a nonnegative half-line and extend
  it to projections onto more complex convex cones, such as the second-order cone.
---

# Generalized Activation via Multivariate Projection

## Quick Facts
- arXiv ID: 2309.17194
- Source URL: https://arxiv.org/abs/2309.17194
- Reference count: 40
- Primary result: Introduces Multivariate Projection Unit (MPU) activation function that generalizes ReLU through convex cone projections, showing improved performance on CIFAR10 and ImageNet-1k while maintaining similar computational complexity to ReLU

## Executive Summary
This paper introduces the Multivariate Projection Unit (MPU), a new activation function that generalizes ReLU by replacing scalar projection onto the nonnegative half-line with projection onto more complex convex cones like second-order cones. The authors theoretically prove that FNNs using MPU have greater expressive power than those using ReLU, as MPU can faithfully represent iterations of projected gradient descent for more general cone programming problems. Experimental results demonstrate that MPU outperforms ReLU on multidimensional function fitting and achieves comparable performance to other activation functions on CIFAR10 and ImageNet-1k datasets while maintaining similar computational complexity.

## Method Summary
MPU operates by projecting input vectors onto convex cones rather than applying elementwise ReLU. The activation partitions input tensors into chunks of size m, applies cone projection ΠC(m)α with learnable angle α, and reshapes back to original dimensions. The authors also introduce a Leaky MPU variant using Moreau envelopes that combines 0.99*MPU + 0.01*identity. The method is evaluated on ResNet18 and DeiT-tiny architectures trained on CIFAR10 and ImageNet-1k datasets with standard hyperparameters including SGD optimizer, learning rate 0.1 with cosine annealing, and 200 epochs.

## Key Results
- MPU achieves 95.71% test accuracy on CIFAR10 compared to 95.53% for Leaky ReLU and 95.53% for ReLU
- On ImageNet-1k, MPU reaches 79.1% top-1 accuracy on DeiT-tiny, comparable to other activation functions
- MPU maintains similar MACs complexity to ReLU while showing improved performance on synthetic function fitting tasks
- Theoretical proof shows MPU has strictly greater expressive power than ReLU for shallow networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MPU generalizes ReLU by replacing scalar projection with cone projection, increasing expressive power
- Mechanism: MPU operates on vectors and projects them onto convex cones (e.g., second-order cones), allowing multidimensional nonlinearity instead of elementwise ReLU. This enables representation of more complex decision boundaries.
- Core assumption: Projection onto a cone is strictly more expressive than scalar ReLU for shallow networks.
- Evidence anchors:
  - [abstract]: "extend ReLU by substituting it with a generalized projection operator onto a convex cone... naturally extending it to a Multivariate Projection Unit (MPU)"
  - [section 2.1]: "we consider ReLU as a projection from R onto the nonnegative half-line R+... we extend ReLU by substituting it with a generalized projection operator onto a convex cone"
  - [corpus]: Weak. No direct citations match cone-based activations.

### Mechanism 2
- Claim: MPU allows shallow FNNs to represent iterations of PGD for SOCP/SDP, which ReLU cannot.
- Mechanism: A single FNN layer mirrors one PGD iteration: linear step followed by projection. ReLU corresponds to projection onto R^n_+ (nonnegative orthant), but MPU extends this to more general convex cones (SOCP, SDP), capturing richer constrained optimization problems.
- Core assumption: Structural similarity between PGD and FNN layer is exact and lossless when using MPU.
- Evidence anchors:
  - [section 2.1]: "a single layer of the FNN activated by ReLU can replicate a single iteration of the PGD process for linearly constrained QP problems... However, we theoretically prove that any shallow FNN utilizing ReLU cannot faithfully represent a single iteration of PGD for more generalized cone programming problems"
  - [section 2.2]: Theorem 2 proves ReLU cannot represent cone projection, MPU can.
  - [corpus]: Weak. No explicit SOCP/SDP references.

### Mechanism 3
- Claim: Leaky MPU inherits Moreau envelope properties, enabling smoother gradients and better optimization.
- Mechanism: By connecting MPU to proximal operators, leaky variants can be derived via Moreau envelopes, ensuring differentiability while preserving convexity. This smooths the activation landscape and helps optimization.
- Core assumption: Moreau envelope construction preserves proximal operator structure and leads to improved learning dynamics.
- Evidence anchors:
  - [section 3]: "the Leaky version of these proximal functions are also proximal operators leveraging the notion of Moreau’s envelope"
  - [section 4.1]: "Leaky MPU (ΠC(2)α) 95.53 ± 0.03 (+ 0.18)" shows performance gain over plain MPU.
  - [corpus]: Weak. No explicit Moreau envelope references in neighbors.

## Foundational Learning

- Concept: Convex cone projections (e.g., second-order cone projection).
  - Why needed here: MPU relies on projecting inputs onto cones rather than clamping at zero. Understanding cone geometry is essential to implement and tune MPU.
  - Quick check question: Given a point outside a second-order cone, how do you compute its projection onto the cone?

- Concept: Projected Gradient Descent (PGD) algorithm.
  - Why needed here: The paper draws an exact analogy between PGD iterations and FNN layers. Knowing PGD helps understand why ReLU corresponds to projection onto R^n_+ and why MPU extends it.
  - Quick check question: Write the update rule for one iteration of PGD for minimizing a quadratic objective subject to x ≥ 0.

- Concept: Proximal operators and Moreau envelopes.
  - Why needed here: Many activation functions (ReLU, sigmoid, tanh, softmax) are proximal operators. MPU extends this view; leaky variants come from Moreau envelopes.
  - Quick check question: What is the proximal operator of the indicator function of a set S?

## Architecture Onboarding

- Component map: Input tensor -> Reshape to 2D -> Partition into m-sized chunks -> Apply cone projection ΠC(m)α -> Optional Leaky combination -> Reshape back to original dimensions
- Critical path: 1. Reshape input to 2D (batch × features) 2. Partition into m-sized groups 3. Compute projection (requires norm, dot products) 4. Apply scaling and reshape back
- Design tradeoffs:
  - Width vs expressiveness: Wider layers allow larger m, more expressive cones, but cost more compute
  - Fixed vs learnable α: Fixed is simpler; learnable may adapt cone shape to data
  - Zero-padding vs ReLU for leftovers: Padding + cone projection marginally better
- Failure signatures:
  - NaN or Inf in projection due to extreme α or ill-conditioned input
  - Gradient vanishing if projection collapses to zero
  - Over-regularization if α is too small (cone too narrow)
- First 3 experiments:
  1. Compare MPU vs ReLU on a synthetic 2D function fitting task (as in section 4.1).
  2. Replace ReLU in ResNet18 with MPU on CIFAR10; measure accuracy and MACs.
  3. Test Leaky MPU on a small transformer (e.g., DeiT-tiny) on ImageNet; compare to ReLU and plain MPU.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational complexity of MPU scale with increasing dimensions of the input space, and what are the practical limits to this scaling?
- Basis in paper: [explicit] The paper mentions that the computational complexity of MPU is comparable to ReLU and Leaky ReLU in terms of MACS, but does not provide a detailed analysis of how this complexity scales with input dimension.
- Why unresolved: The paper provides a general comparison of computational complexity but does not delve into the specifics of how the complexity scales with input dimension, which is crucial for understanding the practical limits of MPU in high-dimensional spaces.
- What evidence would resolve it: A detailed analysis of the computational complexity of MPU as a function of input dimension, including theoretical analysis and empirical benchmarks on high-dimensional datasets.

### Open Question 2
- Question: What is the impact of using different types of convex cones (e.g., polyhedral cones vs. second-order cones) as the basis for MPU, and how does this choice affect the performance of neural networks?
- Basis in paper: [inferred] The paper introduces MPU based on second-order cones and provides theoretical and empirical evidence of its effectiveness. However, it does not explore the use of other types of convex cones as the basis for MPU.
- Why unresolved: The choice of convex cone can significantly impact the properties of MPU, such as its expressiveness and computational efficiency. Exploring different types of cones could lead to new insights and potentially better activation functions.
- What evidence would resolve it: Comparative studies of neural networks using MPU based on different types of convex cones, including theoretical analysis of their properties and empirical evaluation on benchmark datasets.

### Open Question 3
- Question: How does the Leaky MPU variant, which combines the MPU with a small linear component, compare to other activation functions in terms of training stability and generalization performance across different network architectures and tasks?
- Basis in paper: [explicit] The paper introduces Leaky MPU as a variant of MPU and shows that it achieves improved performance on CIFAR10. However, it does not provide a comprehensive comparison with other activation functions across different tasks and architectures.
- Why unresolved: While Leaky MPU shows promise on CIFAR10, its performance on other tasks and architectures is not well understood. A comprehensive comparison is needed to determine its strengths and limitations.
- What evidence would resolve it: Extensive experiments comparing Leaky MPU to other activation functions across a wide range of tasks (e.g., object detection, semantic segmentation) and architectures (e.g., CNNs, transformers), including analysis of training stability and generalization performance.

## Limitations
- Theoretical claims about expressive power are asymptotic and may not translate to practical benefits for finite-width networks
- Experimental validation is limited to specific architectures and datasets, with most comparisons showing marginal improvements over ReLU
- Strong analogies between neural network layers and projected gradient descent iterations may not hold in practice due to numerical instability during backpropagation through cone projections

## Confidence
- **High confidence**: MPU can be implemented and will run without fundamental errors
- **Medium confidence**: MPU provides measurable improvements on CIFAR10/Imagenet tasks compared to ReLU
- **Low confidence**: The claimed expressive power gains translate to meaningful practical advantages in realistic settings

## Next Checks
1. **Numerical stability test**: Implement MPU with various cone angles (α values) and measure projection stability across diverse input distributions to identify failure thresholds
2. **Gradient flow analysis**: Compare gradient magnitudes and vanishing/exploding patterns between ReLU and MPU networks during training on CIFAR10
3. **Ablation study on cone geometry**: Systematically vary m (partition size) and α (cone angle) to determine optimal configurations and identify sensitivity to hyperparameters