---
ver: rpa2
title: 'From Language Modeling to Instruction Following: Understanding the Behavior
  Shift in LLMs after Instruction Tuning'
arxiv_id: '2310.00492'
source_url: https://arxiv.org/abs/2310.00492
tags:
- instruction
- words
- arxiv
- preprint
- word
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops a suite of explanation methods to study how
  instruction fine-tuning transforms pre-trained models into instruction followers.
  Using gradient-based input-output attribution and global interpretation techniques,
  the authors analyze three key shifts: 1) instruction fine-tuned models better recognize
  instruction words and use them to guide generation, 2) knowledge in feed-forward
  layers shifts toward user-oriented tasks while preserving linguistic levels, and
  3) self-attention heads in lower and middle layers learn more word-word relations
  involving instruction verbs.'
---

# From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning

## Quick Facts
- arXiv ID: 2310.00492
- Source URL: https://arxiv.org/abs/2310.00492
- Reference count: 40
- This paper develops explanation methods to study how instruction fine-tuning transforms pre-trained models into instruction followers through three key behavioral shifts.

## Executive Summary
This paper investigates how instruction fine-tuning transforms pre-trained language models into effective instruction followers. Using gradient-based input-output attribution and global interpretation techniques, the authors analyze three key shifts: instruction fine-tuned models better recognize instruction words and use them to guide generation, knowledge in feed-forward layers shifts toward user-oriented tasks while preserving linguistic levels, and self-attention heads in lower and middle layers learn more word-word relations involving instruction verbs. These insights clarify the internal mechanisms behind improved instruction following and lay groundwork for future LLM interpretability and optimization.

## Method Summary
The study develops three interpretation methods to compare pre-trained and instruction-tuned LLaMA models. First, a gradient-based importance scoring function measures input token contributions to each output token, creating normalized importance matrices and density scores. Second, automated concept extraction from feed-forward weights uses PCA to identify interpretable concepts and track their evolution. Third, self-attention head interpretation analyzes word-pair relationships through activated neuron pairs. The authors apply these tools to responses from LLaMA-7B/13B and Alpaca-7B/13B/Vicuna-13B models on user-oriented prompting datasets.

## Key Results
- Instruction fine-tuned models show higher attribution density for instruction words compared to pre-trained models
- Feed-forward networks shift concepts toward user-oriented tasks (writing, coding, math) while preserving linguistic level distributions
- Self-attention heads in lower (1-8) and middle (8-24) layers encode more word-word relations with instruction verbs after fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction fine-tuning empowers LLMs to recognize instruction words from user prompts and use them to guide response generation.
- Mechanism: Gradient-based input-output attribution identifies which input tokens influence each output token. Instruction fine-tuned models show higher attribution density for instruction words compared to pre-trained models.
- Core assumption: Models that identify and use instruction words produce better instruction-following responses.
- Evidence anchors:
  - [abstract]: "It empowers LLMs to recognize the instruction parts of user prompts, and promotes the response generation constantly conditioned on the instructions."
  - [section 3.2.3]: "Table 1 underscores that instruction fine-tuned models excel in identifying and harnessing instruction words from user prompts for response generation."
  - [corpus]: Weak - related papers discuss instruction-following but don't specifically address input attribution mechanisms.
- Break condition: If input attribution scores don't correlate with instruction-following quality, or if pre-trained models show similar attribution patterns.

### Mechanism 2
- Claim: Instruction fine-tuning aligns knowledge in feed-forward networks with user-oriented tasks while preserving linguistic levels.
- Mechanism: Principal component analysis of feed-forward weight patterns reveals concept-level knowledge. Fine-tuning shifts concepts toward user tasks (writing, coding, math) without changing linguistic level distribution.
- Core assumption: Feed-forward networks store task-relevant concepts that can be interpreted through principal components.
- Evidence anchors:
  - [abstract]: "It encourages the feed-forward networks to rotate their pre-trained knowledge toward user-oriented tasks."
  - [section 4.3]: "Instruction fine-tuning shifts the principal vectors of feed-forward network neurons toward user-oriented tasks with minimal shifts across linguistic levels."
  - [corpus]: Weak - corpus neighbors don't provide evidence for feed-forward knowledge evolution mechanisms.
- Break condition: If principal component analysis fails to reveal interpretable concepts, or if linguistic level distributions change significantly.

### Mechanism 3
- Claim: Instruction fine-tuning encourages lower and middle layer self-attention heads to learn word-word relations involving instruction verbs.
- Mechanism: Self-attention heads capture word-pair relationships. Fine-tuning increases the proportion of heads encoding instruction verb relations compared to general verbs.
- Core assumption: Self-attention heads in lower layers learn word-word relations that can be interpreted through activated neuron pairs.
- Evidence anchors:
  - [abstract]: "It facilitates the learning of word-word relations with instruction verbs through the self-attention mechanism, particularly in the lower and middle layers."
  - [section 5.3]: "Table 4 shows that more self-attention heads from lower (1-8) and middle (8-24) layers encode word-word relations with instruction verbs after instruction fine-tuning."
  - [corpus]: Weak - related papers don't specifically address self-attention head evolution for instruction verbs.
- Break condition: If self-attention head word-pair analysis fails to show instruction verb emphasis, or if change rates are similar for instruction and general verbs.

## Foundational Learning

- Concept: Input-output attribution for generation models
  - Why needed here: The paper develops a gradient-based method to measure input token importance for each output token, which is essential for understanding instruction-following behavior.
  - Quick check question: How would you modify standard input attribution methods designed for classification to work with text generation?

- Concept: Principal component analysis for neural network interpretation
  - Why needed here: The paper uses PCA on feed-forward weight matrices to identify interpretable concepts encoded in the model.
  - Quick check question: What information does the covariance matrix of weight patterns capture about the encoded knowledge?

- Concept: Self-attention head analysis through activated word pairs
  - Why needed here: The paper interprets self-attention heads by analyzing word pairs activated by neuron pairs, revealing how they capture word relationships.
  - Quick check question: How does the cosine similarity threshold affect the selection of word pairs for self-attention interpretation?

## Architecture Onboarding

- Component map: Gradient-based attribution -> Input importance analysis -> PCA-based feed-forward interpretation -> Self-attention word-pair analysis -> Model comparison
- Critical path: 1) Generate model outputs for input prompts, 2) Compute input attributions using gradients, 3) Analyze feed-forward weights with PCA, 4) Extract self-attention word pairs, 5) Compare pre-trained vs fine-tuned models.
- Design tradeoffs: The gradient-based method trades computational cost for precise token-level attribution. PCA interpretation balances interpretability with information loss. Word-pair analysis trades granularity for scalability across all self-attention heads.
- Failure signatures: 1) Input attributions show uniform importance across all tokens, 2) PCA reveals no interpretable concepts, 3) Self-attention analysis produces random or meaningless word pairs.
- First 3 experiments:
  1. Run the gradient-based attribution on a simple instruction-following example and visualize the input-output importance heatmap.
  2. Apply PCA to a single feed-forward layer and manually interpret the top principal components using word lists.
  3. Extract word pairs from a self-attention head and verify they capture meaningful word relationships through manual inspection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the importance density score be effectively used as a training objective for instruction fine-tuning to better capture instruction-following capability?
- Basis in paper: Explicit - The authors propose using importance density as an inherent metric to assess instruction-following and suggest it could potentially serve as a training objective.
- Why unresolved: The paper only explores importance density as an evaluation metric, not as a training objective. No experiments or theoretical analysis are provided on how to integrate it into the training process.
- What evidence would resolve it: Empirical results showing improved instruction-following performance when using importance density as a training loss compared to standard instruction-tuning approaches, along with ablation studies on different density formulations.

### Open Question 2
- Question: Do transformer-based LLMs actually simulate a prefix-tree data structure in their upper layers for efficient text generation?
- Basis in paper: Inferred - The authors observe that morphological knowledge increases in the last few layers while semantic knowledge decreases, and they conjecture this pattern might indicate prefix-tree simulation.
- Why unresolved: This is presented as a hypothesis based on observed patterns but not empirically validated. The authors explicitly state they leave this as future work.
- What evidence would resolve it: Architectural analysis showing the layer outputs align with prefix-tree properties, or controlled experiments demonstrating improved generation efficiency when this pattern is enforced.

### Open Question 3
- Question: How do self-attention modules and feed-forward networks collaboratively generate responses that effectively follow user instructions?
- Basis in paper: Explicit - The authors note this as an open question in their discussion, acknowledging they haven't fully explored the interaction between these components.
- Why unresolved: The paper analyzes these components separately and shows they contribute differently to instruction-following, but doesn't investigate their synergistic effects.
- What evidence would resolve it: Mechanistic interpretability studies showing how information flows between self-attention and feed-forward components during instruction-following, or ablation experiments showing the impact of modifying one component on the other's effectiveness.

## Limitations
- Analysis focuses exclusively on LLaMA-based models (7B and 13B variants) fine-tuned on limited datasets (Alpaca, Vicuna), which may not generalize to other model families.
- Interpretation methods rely on automated concept extraction and manual verification of a small sample, introducing potential bias.
- Study examines static snapshots rather than dynamic training processes, missing insights about how these behavioral shifts emerge during fine-tuning.

## Confidence

- **High Confidence**: Instruction fine-tuned models show increased input attribution density for instruction words - directly measurable through gradient-based importance scoring with clear quantitative comparisons.
- **Medium Confidence**: Feed-forward networks shift toward user-oriented tasks while preserving linguistic levels - supported by PCA analysis but dependent on automated concept extraction quality.
- **Medium Confidence**: Self-attention heads in lower/middle layers learn more instruction verb relations - based on word-pair analysis but limited by the interpretation method's granularity.

## Next Checks
1. **Cross-model validation**: Apply the same attribution and interpretation methods to GPT-3.5/4 and other instruction-tuned models to verify whether observed shifts generalize beyond LLaMA-family architectures.
2. **Temporal analysis**: Track how input attribution patterns, feed-forward concept distributions, and self-attention head behaviors evolve across different fine-tuning checkpoints to identify critical transition points.
3. **Ablation study**: Systematically remove or modify instruction words in prompts and measure the corresponding changes in model outputs and internal representations to establish causal links between instruction recognition and improved following.