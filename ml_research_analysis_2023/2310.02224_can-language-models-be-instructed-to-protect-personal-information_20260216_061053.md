---
ver: rpa2
title: Can Language Models be Instructed to Protect Personal Information?
arxiv_id: '2310.02224'
source_url: https://arxiv.org/abs/2310.02224
tags:
- protected
- information
- language
- response
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PRIV QA, a multimodal benchmark designed
  to assess the trade-off between privacy protection and model utility when instructing
  language models to protect specific categories of personal information. The authors
  propose a self-moderation technique that iteratively guides models to evaluate and
  authorize their responses, significantly improving privacy protection.
---

# Can Language Models be Instructed to Protect Personal Information?

## Quick Facts
- arXiv ID: 2310.02224
- Source URL: https://arxiv.org/abs/2310.02224
- Reference count: 40
- Key outcome: Access control instructions can improve privacy protection but remain vulnerable to adversarial attacks and show demographic biases

## Executive Summary
This paper introduces PRIV QA, a multimodal benchmark to evaluate whether language models can follow instructions to protect personal information while maintaining utility. The authors propose a self-moderation technique that iteratively guides models to evaluate and authorize their responses, significantly improving privacy protection scores from 62% to 74%. However, red-teaming experiments reveal that adversaries can easily circumvent these protections using jailbreaking methods through both textual and image inputs. The study also uncovers systematic biases where less popular or minority individuals receive substantially less privacy safeguarding compared to high-profile figures.

## Method Summary
The authors create PRIV QA, a benchmark containing 4,678 textual and 2,000 visual question-answering examples across eight protected populations (citizenship, age, occupation, position) and protected information categories (location, profession, education, relations, affiliation, nationality). They implement a self-moderation technique with three components: question answering, moderation, and authorization. Models first generate responses, then iteratively moderate and authorize their own decisions through self-reflection. The system is evaluated using protection scores (harmonic mean of sensitivity and specificity) and response F1 scores, with red-teaming experiments testing robustness against adversarial attacks.

## Key Results
- Self-moderation improves protection scores from 62% to 74% within three iterations
- Red-teaming attacks achieve 30-40% success rates against even strong models like GPT-4
- Protection effectiveness shows significant demographic biases, with less popular individuals receiving 10-15% lower protection scores
- Visual language models exhibit additional concerns when using physical characteristics for population membership determination

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Access control instructions can guide models to selectively protect personal information by abstaining from responses in defined protected groups.
- Mechanism: The model uses natural language instructions to identify queries belonging to protected groups and abstains from answering them, while answering queries in the control group normally.
- Core assumption: The model can accurately determine whether a query or its answer belongs to the protected group based on the provided instruction.
- Evidence anchors:
  - [abstract]: "We introduce PRIV QA, a multimodal benchmark to assess this privacy/utility trade-off when a model is instructed to protect specific categories of personal information in a simulated scenario."
  - [section 3.1]: "The goal of this task is to develop a response-generation model F : X → Y, which takes a user query (x ∈ X) as input and produces a response ( y ∈ Y ) that both protects personal information of individuals and ensures responsive answering."
- Break condition: The model fails to correctly identify queries belonging to the protected group, leading to either over-protection (abstaining from answering control group queries) or under-protection (answering protected group queries).

### Mechanism 2
- Claim: Self-moderation iteratively improves privacy protection by having the model evaluate and authorize its own responses.
- Mechanism: The model generates an answer, moderates the question and answer based on the protected group instruction, and then authorizes the decision through self-reflection steps.
- Core assumption: The model can effectively self-reflect and re-evaluate its moderation decisions to improve accuracy.
- Evidence anchors:
  - [section 4]: "Motivated by the recent progress of using language models to refine their own response using external feedback to improve coding (Madaan et al., 2023), we develop a Self-Moderation technique with three components (QA, Moderation, Authorization)."
  - [section 4]: "We show an upward trend in the protection score for protected populations, improving from 62% to 74% within three steps."
- Break condition: The model's self-reflection is ineffective, leading to inconsistent or incorrect authorization decisions.

### Mechanism 3
- Claim: Red teaming exposes vulnerabilities in access control instructions by simulating adversarial attacks.
- Mechanism: Adversaries use jailbreaking prompts and multi-hop question templates to bypass access control instructions and extract protected information.
- Core assumption: The model's access control instructions are susceptible to common jailbreaking techniques and can be circumvented through iterative questioning.
- Evidence anchors:
  - [abstract]: "However, through a series of red-teaming experiments, we find that adversaries can also easily circumvent these protections with simple jailbreaking methods through textual and/or image inputs."
  - [section 5.2]: "We adopt the Attack Success Rate (ASR) metric used in current state-of-the-art adversarial attack literature (Zou et al., 2023) to measure the extent to which the attack can induce the LLM to misclassify a query about protected information or a protected group as benign."
- Break condition: The model's access control instructions are robust against adversarial attacks, maintaining high protection scores even when faced with jailbreaking attempts.

## Foundational Learning

- Concept: Access control instructions
  - Why needed here: To guide the model in selectively protecting personal information based on predefined protected groups.
  - Quick check question: How does the model determine whether a query belongs to the protected group or control group?

- Concept: Self-moderation
  - Why needed here: To iteratively improve the model's ability to follow access control instructions by having it evaluate and authorize its own responses.
  - Quick check question: What are the three components of the Self-Moderation technique?

- Concept: Red teaming
  - Why needed here: To assess the robustness of access control instructions against adversarial attacks and identify potential vulnerabilities.
  - Quick check question: What is the Attack Success Rate (ASR) metric used for in the context of red teaming?

## Architecture Onboarding

- Component map: Response Generation -> Moderation -> Authorization -> Final Response
- Critical path: Generate answer → Moderate question/answer → Authorize decision → Output final response
- Design tradeoffs: The system trades off between privacy protection and model utility. More stringent access control instructions may lead to higher privacy protection but lower utility, as the model may abstain from answering more queries.
- Failure signatures: Low protection scores (under-protection), low response F1 scores for control group (over-protection), high attack success rates (adversarial vulnerabilities)
- First 3 experiments:
  1. Evaluate baseline response generation model performance on PRIV QA without access control instructions
  2. Implement and evaluate Self-Moderation technique with single iteration of self-authorization
  3. Conduct red teaming experiments using common jailbreaking prompts to assess robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of access control instructions vary across different demographic groups in terms of privacy protection?
- Basis in paper: Explicit - The paper mentions that models exhibit biases based on popularity and race, leading to inconsistent protection across demographic groups.
- Why unresolved: While the paper notes this bias, it does not provide a detailed analysis of how privacy protection effectiveness varies across specific demographic groups.
- What evidence would resolve it: A detailed analysis comparing the protection score across different demographic groups, such as race, gender, and socioeconomic status, would provide insights into the extent of bias in privacy protection.

### Open Question 2
- Question: Can the proposed self-moderation technique be adapted to mitigate biases in privacy protection for visual language models?
- Basis in paper: Inferred - The paper highlights the use of physical visual characteristics for determining population membership in visual language models, which raises concerns about bias.
- Why unresolved: The paper does not explore potential modifications to the self-moderation technique to address biases in visual language models.
- What evidence would resolve it: Experimental results demonstrating the effectiveness of an adapted self-moderation technique in reducing biases in privacy protection for visual language models would provide insights into potential solutions.

### Open Question 3
- Question: How do the privacy-utility trade-offs of access control instructions compare to other privacy-preserving methods, such as differential privacy or model editing?
- Basis in paper: Explicit - The paper mentions that access control instructions levy an alignment tax, similar to other privacy-preserving methods, but does not provide a detailed comparison.
- Why unresolved: While the paper briefly mentions other privacy-preserving methods, it does not provide a comprehensive comparison of their privacy-utility trade-offs with access control instructions.
- What evidence would resolve it: A comparative study evaluating the privacy-utility trade-offs of access control instructions against other privacy-preserving methods would provide insights into the relative effectiveness of these approaches.

## Limitations
- Access control instructions remain vulnerable to simple jailbreaking attacks, with 30-40% success rates
- Protection effectiveness shows systematic demographic biases, with less popular individuals receiving 10-15% lower protection scores
- The benchmark focuses on eight specific demographic and informational categories, limiting generalizability to real-world scenarios

## Confidence

- High confidence: The core finding that access control instructions can be implemented and evaluated using PRIV QA benchmark
- Medium confidence: The effectiveness of self-moderation in improving protection scores from 62% to 74%
- Medium confidence: The existence of demographic biases in protection effectiveness
- Low confidence: The generalizability of findings to real-world deployment scenarios beyond the controlled benchmark

## Next Checks

1. Test the self-moderation technique on a more diverse set of protected groups and personal information categories beyond the eight currently covered
2. Evaluate model performance on entities with varying levels of public information availability to better understand the demographic bias patterns
3. Develop and test more sophisticated red teaming approaches that combine multiple attack vectors to assess the true robustness limits of access control mechanisms