---
ver: rpa2
title: "Do SSL Models Have D\xE9j\xE0 Vu? A Case of Unintended Memorization in Self-supervised\
  \ Learning"
arxiv_id: '2304.13850'
source_url: https://arxiv.org/abs/2304.13850
tags:
- memorization
- training
- crop
- learning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces the concept of \"d\xE9j\xE0 vu memorization\"\
  \ in self-supervised learning (SSL) models, where models inadvertently memorize\
  \ specific details of individual training images beyond general correlations. The\
  \ authors propose a novel testing methodology to quantify this phenomenon by splitting\
  \ the dataset into target, reference, and public sets, then using label inference\
  \ and visual reconstruction techniques to measure memorization."
---

# Do SSL Models Have Déjà Vu? A Case of Unintended Memorization in Self-supervised Learning

## Quick Facts
- **arXiv ID**: 2304.13850
- **Source URL**: https://arxiv.org/abs/2304.13850
- **Reference count**: 24
- **Key outcome**: This work introduces the concept of "déjà vu memorization" in self-supervised learning (SSL) models, where models inadvertently memorize specific details of individual training images beyond general correlations.

## Executive Summary
This paper introduces "déjà vu memorization" in self-supervised learning models, demonstrating that SSL models trained on datasets like ImageNet memorize specific details of individual training images beyond general correlations. The authors propose a novel testing methodology using label inference and visual reconstruction techniques to quantify this phenomenon. They find that SSL models, particularly VICReg, exhibit significant déjà vu memorization that worsens with more training epochs and is present even with large datasets. This memorization is stronger than in supervised models and persists in the backbone representations, suggesting that withholding the projector layer may not be sufficient mitigation.

## Method Summary
The authors split the dataset into target, reference, and public sets, then train separate SSL models on the target and reference sets. They extract embeddings of background crops from target set images and find their K-nearest neighbors in the public set embeddings using L2 distance. These neighbors are used for label inference (KNN classification) and visual reconstruction (conditional generative model). The difference in accuracy between target and reference models quantifies déjà vu memorization. Experiments are repeated across different architectures, hyperparameters, and training set sizes.

## Key Results
- SSL models exhibit significant déjà vu memorization, able to infer foreground objects from background crop embeddings
- Déjà vu memorization worsens with more training epochs and larger model capacity
- The projector head amplifies memorization compared to backbone representations
- Even with large datasets, déjà vu memorization persists and can be exploited

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SSL models memorize specific foreground objects from training images even when given only background crop embeddings.
- **Mechanism**: The joint-embedding SSL objective forces representations of different augmented views (including background crops) to be close. During training, these background crops often co-occur with specific foreground objects, creating associations that persist in the learned embedding space.
- **Core assumption**: The model learns associations beyond general correlations - specifically memorizing which foreground object appears with which specific background patch.
- **Evidence anchors**:
  - [abstract] "we show that given the trained model and a crop of a training image containing only the background... it is possible to infer the foreground object with high accuracy"
  - [section] "SSL models suffer from the unintended memorization of specific parts of images in their training data"
  - [corpus] "Memorization in Self-Supervised Learning Improves Downstream Generalization" - shows memorization exists in SSL

### Mechanism 2
- **Claim**: Déjà vu memorization worsens with more training epochs and larger model capacity.
- **Mechanism**: Extended training allows the model to encode more specific details about training samples. Larger models have more capacity to store these specific associations rather than just general patterns.
- **Core assumption**: The SSL training objective continues to refine associations even after general correlations are learned.
- **Evidence anchors**:
  - [section] "déjà vu memorization is exacerbated by the atypically large number of training epochs often recommended in SSL training"
  - [section] "we observe that increasing the number of parameters of the model leads to higher degree of déjà vu memorization"
  - [corpus] "Localizing Memorization in SSL Vision Encoders" - suggests memorization can be localized in model architecture

### Mechanism 3
- **Claim**: The projector head amplifies déjà vu memorization compared to backbone representations.
- **Mechanism**: The projector is trained to make embeddings of different views (including background crops) maximally similar. This creates a compressed representation that can retain specific object associations even when the backbone has more general features.
- **Core assumption**: The projector's compression creates a bottleneck that preserves specific training sample details while discarding other information.
- **Evidence anchors**:
  - [section] "Interestingly, for VICReg, there is a drastically lesser degree of detectable déjà vu memorization" when using backbone
  - [section] "déjà vu is significantly reduced in the backbone representation" versus projector
  - [corpus] "Training Large ASR Encoders with Differential Privacy" - suggests privacy risks in SSL representations

## Foundational Learning

- **Concept**: Joint-embedding SSL architecture
  - Why needed here: Understanding how SSL models learn representations from augmented views is fundamental to understanding why they memorize specific training samples
  - Quick check question: How does the contrastive loss in SimCLR encourage background crops to encode information about their corresponding foreground objects?

- **Concept**: Differential privacy and memorization
  - Why needed here: The paper's definition of déjà vu memorization is closely related to differential privacy - memorization means the model retains information specific to individual training samples
  - Quick check question: What is the key difference between correlation (general patterns) and memorization (specific sample details) in the context of privacy?

- **Concept**: KNN-based reconstruction techniques
  - Why needed here: The paper uses KNN classification and RCDM reconstruction to quantify and visualize memorization, requiring understanding of how to extract information from embeddings
  - Quick check question: How does averaging KNN embeddings and feeding them to RCDM allow reconstruction of the original image?

## Architecture Onboarding

- **Component map**: Backbone (ResNet variants, Vision Transformers) -> Projector head (fully connected layers) -> SSL loss function (contrastive, variance-invariance-covariance) -> Augmentation pipeline (random crops, flips, color transforms) -> KNN search infrastructure -> RCDM generative model

- **Critical path**: 
  1. Train SSL model on ImageNet
  2. Extract periphery crops from training images
  3. Build KNN using public set embeddings
  4. Perform label inference or reconstruction
  5. Compare target vs reference model performance

- **Design tradeoffs**:
  - Backbone vs projector: Backbones retain less memorization but more general features; projectors maximize memorization but may be removable
  - K value in KNN: Larger K gives more stable results but may blur specific associations
  - Training epochs: More epochs increase memorization but also improve general representation quality

- **Failure signatures**:
  - Low KNN confidence scores across all samples
  - Reference model performance matching target model (no memorization detected)
  - Reconstruction results that are generic rather than specific to training samples

- **First 3 experiments**:
  1. Run label inference test on VICReg with K=100 to establish baseline memorization
  2. Compare backbone vs projector representations for the same model
  3. Test effect of training epochs on memorization by training VICReg for different epoch counts

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the degree of déjà vu memorization scale with dataset size in real-world, large-scale SSL models trained on millions or billions of images?
- **Basis in paper**: [inferred] The paper shows déjà vu memorization persists even with large datasets (up to 500K samples), but does not test the effect of much larger datasets common in real-world SSL applications.
- **Why unresolved**: The paper's largest tested dataset (500K) is still orders of magnitude smaller than datasets used in industrial-scale SSL (e.g., 1.2B images in CLIP). It's unclear if the observed trends continue or plateau at larger scales.
- **What evidence would resolve it**: Testing déjà vu memorization on SSL models trained with industrial-scale datasets, comparing memorization metrics across dataset sizes spanning several orders of magnitude.

### Open Question 2
- **Question**: What specific architectural or algorithmic modifications to SSL models can effectively mitigate déjà vu memorization while preserving representation quality?
- **Basis in paper**: [explicit] The paper suggests several potential mitigation strategies including hyperparameter tuning, removing the projector head, data curation, and differential privacy, but does not provide comprehensive empirical evaluation of these approaches.
- **Why unresolved**: The paper only provides preliminary evidence that certain strategies (like guillotine regularization) may help, but doesn't systematically evaluate or compare different mitigation approaches or provide implementation details.
- **What evidence would resolve it**: Controlled experiments comparing multiple mitigation strategies on the same SSL models, measuring both memorization reduction and impact on downstream task performance, with ablation studies to identify which components are most effective.

### Open Question 3
- **Question**: How does déjà vu memorization manifest in SSL models trained on privacy-sensitive data types beyond ImageNet, such as medical images or personal photographs?
- **Basis in paper**: [inferred] The paper discusses potential privacy implications but only tests on ImageNet, which contains relatively low-risk content. The severity and nature of memorization may differ for more sensitive data types.
- **Why unresolved**: The experiments were conducted on a general-purpose image dataset. Different data domains may exhibit different correlation structures and memorization patterns that could affect the severity of privacy risks.
- **What evidence would resolve it**: Systematic evaluation of déjà vu memorization on SSL models trained on various sensitive data domains, comparing memorization metrics and reconstruction quality across different data types, with privacy impact assessments for each domain.

## Limitations

- The exact method for background crop extraction is underspecified, particularly how to reliably exclude foreground objects without ground truth masks for all images
- The paper does not provide exact hyperparameter values for SSL training, making precise reproduction difficult
- The choice of K in KNN is presented as 100 but the sensitivity to this hyperparameter is not thoroughly explored

## Confidence

- **High Confidence**: The fundamental claim that SSL models exhibit unintended memorization beyond general correlations is well-supported by multiple experimental results across different architectures (VICReg, SimCLR)
- **Medium Confidence**: The mechanism explanation linking joint-embedding objectives to memorization is plausible but could benefit from more ablation studies showing what aspects of the architecture are most responsible
- **Medium Confidence**: The claim that projector amplification is the primary driver of memorization is supported but the backbone results show significant memorization still exists, suggesting the story is more nuanced

## Next Checks

1. **Cross-architecture validation**: Test the label inference and reconstruction methodology on additional SSL architectures (BYOL, Barlow Twins) to verify the phenomenon generalizes beyond the studied models
2. **Hyperparameter sensitivity**: Systematically vary K in KNN and the number of training epochs to establish the robustness of the memorization detection methodology
3. **Downstream impact study**: Evaluate whether the identified memorization actually harms downstream task performance or if it correlates with improved generalization in some cases, as suggested by the cited "Memorization in Self-Supervised Learning Improves Downstream Generalization" paper