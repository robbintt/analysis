---
ver: rpa2
title: Spatio-Temporal Anomaly Detection with Graph Networks for Data Quality Monitoring
  of the Hadron Calorimeter
arxiv_id: '2311.04190'
source_url: https://arxiv.org/abs/2311.04190
tags:
- data
- channels
- anomaly
- university
- digi-occupancy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses data quality monitoring in the CMS experiment
  at the Large Hadron Collider by developing a semi-supervised spatio-temporal anomaly
  detection system for the Hadron Calorimeter using deep learning models. The GraphSTAD
  system integrates convolutional, graph, and recurrent neural networks to capture
  both Euclidean and non-Euclidean spatial features and temporal evolution of digi-occupancy
  maps.
---

# Spatio-Temporal Anomaly Detection with Graph Networks for Data Quality Monitoring of the Hadron Calorimeter

## Quick Facts
- arXiv ID: 2311.04190
- Source URL: https://arxiv.org/abs/2311.04190
- Reference count: 0
- Primary result: GraphSTAD achieves production-level anomaly detection for the CMS Hadron Calorimeter with precision of 0.999 and false positive rates as low as 10^-7 for extreme anomalies

## Executive Summary
This paper presents GraphSTAD, a semi-supervised deep learning system for detecting anomalies in the CMS Hadron Calorimeter data quality monitoring system. The model integrates convolutional, graph, and recurrent neural networks to capture both Euclidean and non-Euclidean spatial features and temporal evolution of digi-occupancy maps. Trained exclusively on healthy data, the system uses reconstruction error to identify anomalous channels across spatial and temporal dimensions. Evaluation on both simulated and real LHC data demonstrates high detection accuracy, with the system achieving production-level performance and being integrated into the CMS core monitoring infrastructure.

## Method Summary
GraphSTAD processes 3D digi-occupancy maps (64×72×7) from consecutive lumisections using a hybrid architecture combining CNN for local spatial features, GNN for non-Euclidean relationships between channels, and RNN for temporal modeling. The model employs a weighted mean squared error loss function with variational autoencoder regularization and is trained on 10,000 healthy maps using Adam optimizer with one-cycle learning rate scheduling. Data normalization through luminosity and event count regression enables robust generalization across different experimental conditions. The system detects anomalies by thresholding normalized reconstruction error values.

## Key Results
- Achieves precision of 0.999 for extreme anomaly detection (dead and hot channels)
- False positive rates as low as 10^-6 to 10^-7 for extreme anomalies
- Competitive performance for subtle anomalies with 80% degradation detection rate of approximately 0.88
- Successfully integrated into CMS core production system for real-time monitoring

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GraphSTAD effectively detects anomalies by leveraging reconstruction error from a semi-supervised autoencoder trained only on healthy data.
- Mechanism: The autoencoder learns to reconstruct normal digi-occupancy patterns. When anomalous data is presented, the reconstruction error increases significantly, providing a detection signal.
- Core assumption: The healthy data distribution is sufficiently distinct from anomalous patterns to allow reliable reconstruction error thresholding.
- Evidence anchors:
  - [abstract] "The model is trained on healthy data and uses reconstruction error to detect anomalies."
  - [section] "We present an ST reconstruction AE to detect abnormality in the HCAL channels using reconstruction deviation scores on ST digi-occupancy maps from consecutive lumisections."

### Mechanism 2
- Claim: Integration of CNN, GNN, and RNN architectures enables capture of both local spatial patterns and global connectivity-induced behaviors.
- Mechanism: CNNs extract local spatial features from particle hit patterns, GNNs capture non-Euclidean spatial relationships due to shared backend circuits, and RNNs model temporal evolution across lumisections.
- Core assumption: The spatial structure of calorimeter channels has both Euclidean (local particle hits) and non-Euclidean (shared circuit connections) components that can be effectively modeled separately and combined.
- Evidence anchors:
  - [abstract] "GraphSTAD system integrates convolutional, graph, and recurrent neural networks to capture both Euclidean and non-Euclidean spatial features and temporal evolution"
  - [section] "We employ CNN and GNN with a pooling mechanism to extract relevant features from high dimensional spatial data followed by RNN to capture temporal characteristics of the extracted features"

### Mechanism 3
- Claim: Data normalization through luminosity and event count regression enables robust generalization across different experimental conditions.
- Mechanism: A regression model normalizes digi-occupancy values based on received luminosity and number of events, creating consistent quantity interpretation across varying run settings.
- Core assumption: The relationship between luminosity, event count, and digi-occupancy can be effectively modeled as a regression problem that captures the underlying physical dependencies.
- Evidence anchors:
  - [section] "We devise a renormalization of the γ through a regression model R to have a consistent quantity interpretation of the γ and build a model that robustly generalizes previously unseen run settings"
  - [section] "The model R is trained to minimize the MSE cost function, E[(γs − ¯γs)2], where γs is calculated as the sum of digi-occupancy across all channels"

## Foundational Learning

- Concept: Spatio-temporal anomaly detection fundamentals
  - Why needed here: The HCAL data has both spatial dependencies (channel relationships) and temporal dependencies (evolution across lumisections) that must be jointly modeled
  - Quick check question: What distinguishes spatio-temporal anomalies from purely spatial or purely temporal anomalies?

- Concept: Graph neural networks for non-Euclidean spatial relationships
  - Why needed here: Adjacent calorimeter channels may share backend circuits but not be spatially close, creating irregular spatial patterns that standard CNNs cannot capture effectively
  - Quick check question: How does a graph convolution differ from a standard convolution in terms of neighborhood definition?

- Concept: Autoencoder-based anomaly detection principles
  - Why needed here: The semi-supervised approach requires training only on healthy data, relying on reconstruction error as the anomaly detection signal
  - Quick check question: Why does an autoencoder trained only on normal data produce high reconstruction error for anomalies?

## Architecture Onboarding

- Component map:
  - Input: 3D digi-occupancy maps [iη=64 × iϕ=72 × depth=7] from consecutive lumisections
  - CNN Encoder: 4-layer 3D convolutional network with batch normalization and max pooling
  - GNN Encoder: 4-layer graph convolutional network with global attention pooling
  - RNN: 2-layer LSTM network processing concatenated CNN and GNN features
  - Variational Layer: Adds stochastic regularization to latent space
  - Decoder: 2-layer LSTM followed by 4-layer 3D deconvolutional network with unpooling
  - Output: Reconstructed 3D digi-occupancy maps for reconstruction error calculation

- Critical path:
  1. Normalize input data using luminosity/event regression model
  2. Extract spatial features using CNN (local patterns) and GNN (shared circuit relationships)
  3. Process temporal sequence of spatial features using RNN
  4. Generate latent representation with variational regularization
  5. Reconstruct original input through decoder
  6. Calculate reconstruction error and normalize per-channel
  7. Apply threshold to detect anomalies

- Design tradeoffs:
  - Spatial vs temporal resolution: Using T=5 lumisections balances anomaly detection capability with computational cost
  - Graph vs CNN: Both spatial models are retained to capture different aspects of spatial relationships
  - Supervised vs semi-supervised: Semi-supervised approach avoids expensive anomaly labeling but may miss rare anomaly types

- Failure signatures:
  - High false positive rate on low-occupancy channels indicates reconstruction accuracy issues
  - Poor detection of 80% degradation suggests difficulty with subtle anomalies
  - Inconsistent performance across different luminosity ranges suggests normalization model inadequacy

- First 3 experiments:
  1. Test reconstruction accuracy on held-out healthy data to verify model capability
  2. Inject synthetic dead channels and verify detection sensitivity
  3. Evaluate detection performance on simulated 80% degraded channels to assess subtle anomaly detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal time window length for balancing anomaly detection performance and computational efficiency in the GraphSTAD system?
- Basis in paper: [explicit] The paper mentions using a time window T=5 for evaluation and discusses computational cost, but doesn't explore optimal window sizes
- Why unresolved: The paper only uses T=5 as a fixed parameter without investigating how different window sizes affect performance
- What evidence would resolve it: Systematic evaluation of detection performance and computational cost across different time window sizes (e.g., T=1, 3, 5, 10, 15)

### Open Question 2
- Question: How does the GraphSTAD system perform on data from future LHC runs with different luminosity profiles and collision rates?
- Basis in paper: [inferred] The paper evaluates on 2018 Run-2 data and discusses data normalization challenges, but doesn't test generalization to different experimental conditions
- Why unresolved: The model is trained and tested on a specific dataset from a specific year, and LHC conditions vary significantly between runs
- What evidence would resolve it: Testing and validation on data from multiple LHC runs with varying collision rates, luminosity profiles, and detector configurations

### Open Question 3
- Question: What is the relationship between reconstruction error thresholds and false positive rates across different types of channel anomalies?
- Basis in paper: [explicit] The paper uses a fixed threshold α for anomaly detection but doesn't systematically explore the tradeoff between detection sensitivity and false positives
- Why unresolved: The paper sets a single threshold value but doesn't investigate how different threshold values affect the precision-recall tradeoff for various anomaly types
- What evidence would resolve it: Detailed analysis of precision-recall curves and ROC curves for different threshold values across dead, hot, and degrading channel anomalies

## Limitations

- Performance degradation for subtle anomalies (80% degradation) with detection rates dropping to approximately 0.88
- Reliance on reconstruction error thresholding may miss anomalies that share statistical properties with healthy data
- Assumes both Euclidean and non-Euclidean spatial relationships are equally important for anomaly detection

## Confidence

- High: Extreme anomaly detection (dead and hot channels) with precision of 0.999 and FPR as low as 10^-7
- Medium: Subtle anomaly detection (80% degradation) with detection rates around 0.88
- Medium: Generalization to different LHC run conditions and luminosity profiles

## Next Checks

1. Test the system's performance on a held-out validation set of healthy data to verify that reconstruction accuracy is consistently high across all channels, particularly those with low occupancy levels where false positives are most likely to occur.

2. Evaluate the model's robustness to different luminosity ranges and run conditions by testing on data from multiple LHC runs to ensure the normalization model generalizes effectively beyond the training data distribution.

3. Conduct ablation studies to determine the relative contribution of each component (CNN, GNN, RNN) to overall performance, particularly testing whether the GNN component provides significant benefit for non-Euclidean spatial relationships in HCAL data.