---
ver: rpa2
title: Bi-Level Offline Policy Optimization with Limited Exploration
arxiv_id: '2310.06268'
source_url: https://arxiv.org/abs/2310.06268
tags:
- policy
- proof
- lemma
- function
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of offline reinforcement learning,
  where an agent learns from a fixed dataset without further exploration. The authors
  propose a bi-level policy optimization framework that models a hierarchical interaction
  between the policy (upper-level) and the value function (lower-level).
---

# Bi-Level Offline Policy Optimization with Limited Exploration

## Quick Facts
- **arXiv ID**: 2310.06268
- **Source URL**: https://arxiv.org/abs/2310.06268
- **Reference count**: 40
- **Primary result**: A bi-level policy optimization framework that achieves regret guarantees under only realizability, without requiring data coverage or completeness assumptions, and shows competitive performance on benchmark offline RL datasets.

## Executive Summary
This paper addresses offline reinforcement learning by proposing a bi-level policy optimization framework that learns from fixed datasets without further exploration. The approach constructs confidence sets of value estimates with small weighted average Bellman errors while controlling distribution mismatch uncertainty. The method preserves model extrapolation power and doesn't require strong data-coverage or completeness assumptions, making it theoretically and practically appealing for real-world offline RL applications.

## Method Summary
The method employs a bi-level optimization framework where the lower level constructs a confidence set of value estimates with sufficiently small weighted average Bellman errors while controlling uncertainty from distribution mismatch through L2(µ) constraints on importance weights. The upper level then maximizes a conservative value estimate from this confidence set. A penalized adversarial estimation procedure is developed to solve this bi-level problem efficiently, handling both non-linear and linear function approximation. The algorithm is evaluated on synthetic, benchmark, and real-world datasets, showing competitive performance with state-of-the-art methods.

## Key Results
- Achieves regret guarantees under only realizability assumptions, without requiring data coverage or completeness-type assumptions
- Constructs confidence intervals that account for both approximation error and sampling uncertainty
- Shows competitive performance with state-of-the-art methods on D4RL benchmark datasets
- Preserves model extrapolation power while maintaining robust performance on covered policies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The lower level constructs a confidence set of value estimates with small weighted average Bellman errors while controlling uncertainty from distribution mismatch.
- **Mechanism**: The lower level optimization enforces that for any value function in the confidence set, the weighted average Bellman error (where weights come from the importance-weight class) must be below a threshold εn. Simultaneously, it controls the uncertainty in the importance weights via an L2(µ) constraint. This creates a conservative value interval that accounts for both model misspecification and sampling uncertainty.
- **Core assumption**: The importance-weight class Ω is rich enough to approximate the true density ratio between any policy's visitation distribution and the behavior distribution, or at least provide a good enough approximation to bound the distributional shift.
- **Evidence anchors**:
  - [abstract]: "The lower level focuses on constructing a confidence set of value estimates that maintain sufficiently small weighted average Bellman errors, while controlling uncertainty arising from distribution mismatch."
  - [section 3]: "At the lower level, one component is to construct a confidence set with consistent value estimates regarding the appropriately small weighted average Bellman error, effectively preventing overly pessimistic evaluation. Meanwhile, the second component, which deals with uncertainty control, implicitly enhances the power of model extrapolation."
- **Break condition**: If the importance-weight class Ω cannot approximate the true density ratio well enough, the confidence set will either be too conservative (missing good policies) or too loose (including poor estimates).

### Mechanism 2
- **Claim**: The upper level maximizes a conservative value estimate from the confidence set formed at the lower level, preserving model extrapolation power.
- **Mechanism**: The upper level takes the lower bound of the confidence interval as the value estimate for each policy and maximizes this over the policy class Π. Because the confidence interval is constructed using weighted average Bellman errors rather than squared errors, it doesn't require Bellman completeness - only realizability. This allows the model to extrapolate beyond the support of the data.
- **Core assumption**: The function class Q contains a good approximation to the true q-function for any policy in Π, i.e., realizability holds.
- **Evidence anchors**:
  - [abstract]: "This novel formulation preserves the maximum flexibility of the implicitly induced exploratory data distribution, enabling the power of model extrapolation."
  - [section 4]: "Notably, to the best of our knowledge, Theorem 4.1 is the first result of regret guarantee under only realizability without requiring any data coverage or completeness-type assumptions."
- **Break condition**: If realizability fails (Q cannot approximate qπ well), the confidence set will contain poor estimates, and the upper level will optimize based on incorrect values.

### Mechanism 3
- **Claim**: The bi-level structure allows the algorithm to compete with any policy covered by the data without requiring global coverage or Bellman completeness.
- **Mechanism**: By controlling uncertainty through L2(µ) constraints on importance weights rather than L∞ constraints, the algorithm can implicitly define an exploratory distribution that is close to µ on the support but flexible off-support. This allows minimizing extrapolation error while maintaining good on-support estimation. The regret bound shows competition with any policy in the U2-covered class Π(U2).
- **Core assumption**: The mixture density ratio concentrability coefficient sups,a{dπ(s,a)1µ(s,a)>0/µ(s,a)}L2(µ) is bounded for policies we want to compete with.
- **Evidence anchors**:
  - [abstract]: "Our theoretical regret guarantees do not rely on any data-coverage and completeness-type assumptions, only requiring realizability."
  - [section 4]: "Definition 4.1 (U2-covered policy class). Let Π(U2) denote the U2-covered policy class of µ for U2 ≥ 1..."
- **Break condition**: If the policy we want to compete with has unbounded mixture density ratio concentrability, the algorithm cannot guarantee performance relative to that policy.

## Foundational Learning

- **Concept**: Importance sampling and density ratios in offline RL
  - Why needed here: The algorithm uses importance weights to reweight the offline data when evaluating policies. Understanding how density ratios measure distributional shift is crucial for grasping the lower level's uncertainty control.
  - Quick check question: What is the difference between L2(µ) and L∞ concentrability coefficients, and why does using L2 help with model extrapolation?

- **Concept**: Bellman equations and Bellman completeness
  - Why needed here: The algorithm relaxes Bellman completeness to realizability by using weighted average Bellman errors. Understanding the difference between these conditions is key to appreciating the algorithm's theoretical contribution.
  - Quick check question: Why does using weighted average Bellman errors (rather than squared errors) avoid the double sampling issue that requires Bellman completeness?

- **Concept**: Confidence intervals and statistical learning theory
  - Why needed here: The algorithm constructs confidence intervals for value estimates that account for both approximation error and sampling uncertainty. Understanding covering numbers and uniform convergence is essential for the theoretical analysis.
  - Quick check question: How does the algorithm use covering numbers of the function classes to bound the statistical error in the confidence intervals?

## Architecture Onboarding

- **Component map**: Data → Lower level optimization (construct Qεn) → Upper level optimization (find best policy in Qεn) → Policy output

- **Critical path**: The algorithm processes data through a bi-level optimization where the lower level constructs a confidence set of value functions, and the upper level selects the policy with the highest conservative value estimate from this set.

- **Design tradeoffs**:
  - Conservative vs. optimistic: The algorithm chooses pessimism to handle distribution shift, trading off potential performance for robustness
  - Complexity vs. accuracy: Richer function classes Ω and Q give better approximations but increase statistical error
  - On-support vs. off-support: L2 constraint allows flexibility off-support but may increase on-support bias

- **Failure signatures**:
  - Poor performance on policies not covered by data: Check if mixture density ratio is unbounded
  - Overly conservative policies: Check if εn is too small or Ω is too restrictive
  - High variance in estimates: Check if sample size n is sufficient for the function class complexity

- **First 3 experiments**:
  1. Synthetic CartPole with varying exploration levels (α = 0.1, 0.5, 1) to test robustness to distribution shift
  2. D4RL medium-replay datasets to compare against baselines like CQL, BCQ, BEAR
  3. Linear MDP case study with controlled concentrability to validate theoretical bounds

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text. However, based on the theoretical framework and assumptions made, several important open questions arise:

- How does the choice of the strongly convex function D in the detection function impact the algorithm's performance and theoretical guarantees?
- How does the algorithm perform in environments with partial observability or hidden confounders?
- Can the algorithm be extended to handle continuous action spaces more efficiently?

## Limitations

- The algorithm assumes access to a generative model or simulator for policy evaluation, which may not be available in many offline RL settings
- The bi-level optimization framework requires solving a non-convex problem that could be computationally expensive for high-dimensional state-action spaces
- Performance critically depends on the choice of function approximation classes Q, Ω, and Π - if these are misspecified, the confidence intervals may be invalid

## Confidence

- **Theoretical regret bounds**: High (proof structure is sound, though constants may be loose)
- **Practical algorithm performance**: Medium (competitive results but limited ablation studies)
- **Realizability assumptions**: Low (difficult to verify in practice)

## Next Checks

1. Conduct ablation studies removing the L2 constraint on importance weights to quantify its impact on model extrapolation
2. Test algorithm performance when realizability fails by using intentionally misspecified function classes
3. Evaluate computational scalability on high-dimensional continuous control tasks (e.g., D4RL locomotion tasks with >10 state dimensions)