---
ver: rpa2
title: Using DUCK-Net for Polyp Image Segmentation
arxiv_id: '2311.02239'
source_url: https://arxiv.org/abs/2311.02239
tags:
- segmentation
- image
- https
- block
- polyp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DUCK-Net introduces a novel CNN architecture for polyp segmentation
  that achieves state-of-the-art performance on four benchmark datasets (Kvasir-SEG,
  CVC-ClinicDB, CVC-ColonDB, ETIS-LARIBPOLYPDB). The core innovation is the DUCK block,
  which combines multiple parallel convolutional kernels (Residual, Midscope, Widescope,
  and Separated blocks) to capture multi-scale features effectively.
---

# Using DUCK-Net for Polyp Image Segmentation

## Quick Facts
- arXiv ID: 2311.02239
- Source URL: https://arxiv.org/abs/2311.02239
- Authors: Not specified in provided content
- Reference count: 40
- Primary result: State-of-the-art polyp segmentation with Dice coefficient up to 0.9502 on Kvasir-SEG

## Executive Summary
DUCK-Net introduces a novel CNN architecture for polyp segmentation that achieves state-of-the-art performance on four benchmark datasets (Kvasir-SEG, CVC-ClinicDB, CVC-ColonDB, ETIS-LARIBPOLYPDB). The core innovation is the DUCK block, which combines multiple parallel convolutional kernels to capture multi-scale features effectively. The architecture also employs residual downsampling to preserve low-level details and uses addition instead of concatenation for feature fusion. Trained without pre-training on limited data, DUCK-Net significantly outperforms existing methods including PraNet and HarDNet-DFUS.

## Method Summary
DUCK-Net is a CNN architecture specifically designed for polyp segmentation. The core innovation is the DUCK block, which combines four parallel convolutional paths (Residual, Midscope, Widescope, and Separated blocks) to capture multi-scale features. The architecture uses residual downsampling where the original input image is added back at each downsampling stage to preserve spatial detail, and addition-based feature fusion instead of concatenation. The model is trained from scratch without pre-training using RMSprop optimizer with learning rate 0.0001, batch size 4, for 600 epochs on 352x352 RGB images with data augmentation.

## Key Results
- Achieves Dice coefficient up to 0.9502 and Jaccard index up to 0.9051 on Kvasir-SEG dataset
- Significantly outperforms existing methods including PraNet, HarDNet-DFUS, and Transformer-based approaches
- Ablation studies confirm DUCK block's superior performance over standard convolutions
- Code is publicly available on GitHub

## Why This Works (Mechanism)

### Mechanism 1: Multi-scale Feature Learning
The DUCK block enables multi-scale feature learning by combining six parallel convolutional kernels with different receptive fields. Multiple convolutional paths (Residual, Midscope, Widescope, Separated) are run in parallel and their outputs are summed, allowing the network to learn which kernel size/shape is most useful at each spatial location. The core assumption is that the network can adaptively choose the best kernel path at each spatial position through gradient learning. Break condition: If gradients vanish or the network fails to specialize paths, all parallel kernels will learn the same features and the advantage disappears.

### Mechanism 2: Residual Downsampling
Residual downsampling preserves low-level spatial detail that would otherwise be lost in standard downsampling. Instead of only processing the image through convolutional downsampling, the original high-resolution input is added back at each downsampling step, ensuring that fine-grained details are always available. The core assumption is that addition of unprocessed input to processed feature maps retains spatial precision without disrupting learned feature extraction. Break condition: If the unprocessed signal dominates the learned features, the network may fail to learn meaningful high-level abstractions.

### Mechanism 3: Addition-based Feature Fusion
Addition-based feature fusion outperforms concatenation in this task by reducing parameter count and forcing efficient feature reuse. Instead of concatenating encoder and decoder feature maps (which doubles channel depth), the two are summed element-wise, reducing memory usage and encouraging the network to merge complementary features rather than stack redundant ones. The core assumption is that the feature distributions from encoder and decoder are compatible for direct summation without losing representational power. Break condition: If the encoder and decoder features have different statistical distributions, addition may produce destructive interference.

## Foundational Learning

- **Multi-scale feature extraction in CNNs**: Why needed here - Polyps vary widely in size and texture; capturing both fine edges and large contextual regions is critical for accurate segmentation. Quick check question: Why might a single 3x3 kernel be insufficient for detecting both small and large polyps in the same image?

- **Residual connections and skip connections**: Why needed here - Residual paths prevent vanishing gradients in deep networks and preserve spatial resolution across downsampling stages. Quick check question: How does adding the input image at each downsampling step differ from a standard skip connection in U-Net?

- **Loss functions for imbalanced segmentation (Dice loss)**: Why needed here - Polyp pixels are a small fraction of the image; standard cross-entropy would be dominated by background class. Quick check question: What property of the Dice coefficient makes it suitable for handling class imbalance in medical segmentation?

## Architecture Onboarding

- **Component map**: Input → Lanczos-rescaled 352x352 RGB image → Encoder (5 downsampling stages with DUCK blocks) → Residual downsampling → Decoder (4 upsampling stages with addition-based fusion) → Output binary segmentation map

- **Critical path**: Encoder → DUCK block → residual downsampling → decoder fusion → output

- **Design tradeoffs**: DUCK block increases parameter count and compute but improves feature diversity; Addition fusion saves memory but assumes compatible feature distributions; No pre-training forces model to learn from scratch, improving data efficiency claims

- **Failure signatures**: Vanishing gradients in deep DUCK blocks; Loss of spatial precision despite residual downsampling; Overfitting on small datasets due to high model capacity

- **First 3 experiments**:
  1. Replace DUCK block with single 3x3 conv and compare Dice scores on Kvasir-SEG
  2. Swap addition fusion for concatenation and measure memory vs accuracy trade-off
  3. Train with and without residual downsampling to quantify low-level detail preservation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the DUCK block's performance compare when applied to non-medical image segmentation tasks versus polyp segmentation?
- **Basis in paper**: The paper states the architecture is "versatile and applicable to various segmentation tasks" but only demonstrates results on polyp segmentation
- **Why unresolved**: The paper only evaluates DUCK-Net on polyp segmentation datasets, leaving its generalizability to other domains untested
- **What evidence would resolve it**: Comparative experiments applying DUCK-Net to benchmark datasets in natural image segmentation (e.g., Cityscapes, COCO) with performance metrics

### Open Question 2
- **Question**: What is the impact of different filter sizes on the DUCK block's ability to capture features at various scales?
- **Basis in paper**: The paper mentions testing 17 and 34 filter sizes but doesn't provide detailed analysis of how filter size affects feature capture
- **Why unresolved**: The ablation study only compares two filter sizes without exploring the relationship between filter count and feature representation capability
- **What evidence would resolve it**: Systematic experiments varying filter sizes (e.g., 8, 17, 34, 64) with corresponding feature visualization and performance metrics

### Open Question 3
- **Question**: How does the computational complexity of DUCK-Net scale with image resolution and why was 352x352 chosen as the standard size?
- **Basis in paper**: The paper mentions using 352x352 resolution for computational cost reasons but doesn't provide complexity analysis or justification for this specific size
- **Why unresolved**: No computational complexity analysis or comparison of different resolution sizes is provided in the paper
- **What evidence would resolve it**: Detailed FLOPs calculation across different resolutions, memory usage analysis, and performance trade-off curves showing accuracy vs. resolution

### Open Question 4
- **Question**: What specific architectural modifications could improve DUCK-Net's performance on polyps with colors similar to the background?
- **Basis in paper**: The discussion section mentions this as a limitation but doesn't propose specific solutions
- **Why unresolved**: The paper identifies the limitation but doesn't explore potential architectural or training modifications to address it
- **What evidence would resolve it**: Implementation and evaluation of specific modifications (e.g., additional attention mechanisms, different loss functions, or specialized training strategies) on challenging cases

## Limitations
- Architecture lacks pre-training, which may limit performance on extremely small datasets
- DUCK block increases computational complexity compared to standard convolutional blocks
- Study only evaluates on four polyp segmentation datasets, limiting generalizability to other medical imaging tasks
- Model's performance on extremely small or large polyps beyond dataset distributions remains unverified

## Confidence
- **High**: Reported benchmark results with clearly specified methodology and evaluation metrics
- **Medium**: DUCK block's superiority supported by ablation studies but lacking comparison to alternative multi-scale architectures
- **Low-Medium**: Mechanism explanations provide theoretical justification but limited empirical validation

## Next Checks
1. **Ablation on Feature Fusion**: Systematically compare addition vs concatenation across multiple datasets to quantify the claimed memory-accuracy trade-off
2. **Multi-scale Kernel Analysis**: Visualize and analyze the learned weights in each parallel kernel path of the DUCK block to verify adaptive specialization
3. **Cross-domain Generalization**: Test DUCK-Net on non-polyp medical segmentation tasks (e.g., lung nodules, retinal vessels) to assess architectural generalizability