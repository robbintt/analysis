---
ver: rpa2
title: Retrieval Augmented Generation using Engineering Design Knowledge
arxiv_id: '2307.06985'
source_url: https://arxiv.org/abs/2307.06985
tags:
- patent
- patents
- https
- google
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method to extract explicit engineering design
  facts from patent documents to support Retrieval Augmented Generation (RAG) in design
  processes. The method uses token classification and sequence-to-sequence learning
  to identify head entities, relationships, and tail entities from sentences.
---

# Retrieval Augmented Generation using Engineering Design Knowledge

## Quick Facts
- arXiv ID: 2307.06985
- Source URL: https://arxiv.org/abs/2307.06985
- Reference count: 40
- One-line primary result: Method extracts over 2.93 million engineering design facts from fan system patents with up to 99.7% accuracy to support knowledge retrieval in design processes.

## Executive Summary
This paper presents a novel approach for extracting explicit engineering design facts from patent documents to enhance knowledge retrieval in design processes. The method employs a two-step token classification approach using spaCy taggers to identify head entities, relationships, and tail entities from sentences. A comprehensive dataset of 375,084 examples was created and used to fine-tune language models, achieving high accuracy in relationship identification. The extracted facts form a knowledge base that guides Large Language Models in generating technical and cohesive responses for design tasks.

## Method Summary
The method uses token classification and sequence-to-sequence learning to identify head entities, relationships, and tail entities from sentences in patent documents. First, a tagger identifies all entity and relationship tokens in a sentence. Then, given a pair of entities marked with unique markers, a second tagger identifies only the relationship tokens between them. This avoids the complexity of pairwise comparisons or graph edge classification. The approach was applied to 4,870 fan system patents, extracting over 2.93 million facts to form a knowledge base that supports knowledge retrieval tasks in design processes.

## Key Results
- Method achieves up to 99.7% accuracy in relationship identification
- Extracted over 2.93 million facts from 4,870 fan system patents
- Generated technical and cohesive responses for knowledge retrieval tasks in design processes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token classification with unique entity markers (HEAD ~ … and TAIL ~ …) enables accurate extraction of relationships without needing contextual embeddings for each entity pair.
- Mechanism: The tagger first identifies all entity and relationship tokens in a sentence. Then, given a pair of entities marked with unique markers, a second tagger identifies only the relationship tokens between them. This avoids the complexity of pairwise comparisons or graph edge classification.
- Core assumption: Marking entities uniquely in the sentence provides sufficient context for the relationship extraction step, eliminating the need for additional context vectors or pairwise feature concatenation.
- Evidence anchors:
  - [abstract] "Given a pair of entities thus identified, we train another tagger to identify the relationship tokens that specifically denote the relationship between the pair."
  - [section] "We mark these in the sentence using {HEAD ~ …} and {TAIL ~ …} and intend to identify the tokens … that represent the relationship between the pair."
  - [corpus] Weak - no related work directly compares this marker-based approach.
- Break condition: If the sentence contains multiple entity pairs with overlapping tokens, the marker system may not disambiguate which tokens belong to which relationship.

### Mechanism 2
- Claim: Using a transformer fine-tuned for masked language modeling provides better token embeddings than using pre-trained embeddings directly.
- Mechanism: The BERT model is fine-tuned on the 44,227 sentences from the dataset to learn domain-specific token representations. These embeddings are then reduced in dimensionality (128D) and used in benchmark models.
- Core assumption: Fine-tuning captures domain-specific semantics that pre-trained models miss, leading to more accurate link prediction in benchmarks.
- Evidence anchors:
  - [abstract] "we fine-tune BERT for masked-language modelling … and obtain contextualised embeddings for tokens in these sentences."
  - [section] "While the fine-tuned model correctly predicted [MASK 1] as ‘compositions’ with the highest weight (0.836), the pre-trained model only predicts some terms that could be semantically similar to ‘compositions’."
  - [corpus] Weak - no comparative study of fine-tuned vs pre-trained embeddings in similar contexts.
- Break condition: If the domain vocabulary is too small or too specialized, fine-tuning may overfit or fail to generalize.

### Mechanism 3
- Claim: The two-step tagger approach (entity/relationship identification → relationship extraction for marked pairs) is significantly more accurate than pairwise or graph-based alternatives.
- Mechanism: Step 1 identifies tokens as ENT, REL, or OTH. Step 2, given a marked entity pair, identifies REL tokens between them. This is simpler and more accurate than predicting links among all token pairs or graph edges.
- Core assumption: The accuracy of the first tagger (0.933) and the second tagger (0.994) is sufficient to ensure overall fact extraction accuracy.
- Evidence anchors:
  - [abstract] "The token classification approach achieves up to 99.7 % accuracy."
  - [section] "An accuracy of 99.41% is objectively satisfactory and does not indicate the need to improve performance."
  - [corpus] Weak - no external validation of this two-step approach on other datasets.
- Break condition: If the sentence structure is highly complex or the relationship is implicit, the taggers may fail to capture the correct tokens.

## Foundational Learning

- Concept: Token classification vs. sequence labeling
  - Why needed here: The method needs to label each token in a sentence as entity, relationship, or other, which is a token classification task, not sequence labeling.
  - Quick check question: What is the difference between token classification and sequence labeling, and why is token classification more appropriate for identifying entities and relationships in this context?

- Concept: Masked language modeling
  - Why needed here: Fine-tuning BERT with masked language modeling on the dataset provides domain-specific token embeddings that improve downstream classification tasks.
  - Quick check question: How does masked language modeling help BERT learn better token representations for a specific domain?

- Concept: Graph Neural Networks (GNNs) for edge classification
  - Why needed here: The benchmark II approach uses GNNs to predict edges (relationships) between nodes (entities/relationships) in a sentence graph.
  - Quick check question: How do GNNs aggregate neighborhood information to predict edge labels, and why might this be less effective than the two-step tagger approach?

## Architecture Onboarding

- Component map: spaCy NLP pipeline -> BERT fine-tuning -> Two-step tagger -> Benchmark models (MLP, GNNs) -> Knowledge base population
- Critical path:
  1. Preprocess patent sentences (cleaning, formatting)
  2. Identify tokens as ENT, REL, or OTH using first tagger
  3. For each entity pair, mark with {HEAD ~ …} and {TAIL ~ …}
  4. Identify relationship tokens using second tagger
  5. Combine tokens to form facts
  6. Populate knowledge base with facts
- Design tradeoffs:
  - Accuracy vs. complexity: The two-step tagger approach is more accurate but requires careful handling of entity markers.
  - Pre-trained vs. fine-tuned embeddings: Fine-tuning provides better domain-specific embeddings but requires more data and computation.
  - Graph-based vs. token-based: Graph-based approaches (benchmarks) are more complex but may capture more context.
- Failure signatures:
  - Low accuracy in first tagger: May lead to incorrect entity/relationship identification
  - Incorrect entity markers: May cause second tagger to identify wrong relationship tokens
  - Overfitting in fine-tuned BERT: May lead to poor generalization to new sentences
- First 3 experiments:
  1. Test the two-step tagger approach on a small set of sentences outside the dataset to verify accuracy.
  2. Compare the performance of fine-tuned vs pre-trained BERT embeddings on a benchmark task.
  3. Implement and test the MLP classifiers and GNNs from the benchmarks to understand their limitations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed method for extracting engineering design facts from patent documents compare to human expert annotation?
- Basis in paper: [explicit] The paper states that the proposed method achieves up to 99.7% accuracy in relationship identification and 99.41% accuracy in extracting facts from sentences. However, it does not provide a comparison to human expert annotation.
- Why unresolved: The paper does not include a comparison of the method's performance to human expert annotation, which would provide a benchmark for evaluating the accuracy and reliability of the automated approach.
- What evidence would resolve it: A study comparing the accuracy and reliability of the proposed method to human expert annotation on the same set of patent documents would provide evidence to resolve this question.

### Open Question 2
- Question: How does the generalizability of the extracted design knowledge from patent documents vary across different engineering domains?
- Basis in paper: [inferred] The paper demonstrates the application of the method to a domain of 4,870 fan system patents, but does not investigate how the generalizability of the extracted knowledge varies across different engineering domains.
- Why unresolved: The paper focuses on a specific engineering domain (fan systems) and does not explore the performance of the method in extracting generalizable knowledge from patent documents in other engineering domains.
- What evidence would resolve it: A study applying the method to patent documents from various engineering domains and comparing the generalizability of the extracted knowledge would provide evidence to resolve this question.

### Open Question 3
- Question: How can the extracted design knowledge from patent documents be effectively integrated into existing design environments and workflows?
- Basis in paper: [explicit] The paper discusses the potential of the extracted knowledge to support knowledge-intensive tasks in the design process and demonstrates a comparative discussion against ChatGPT. However, it does not provide specific guidance on how to effectively integrate the knowledge into existing design environments and workflows.
- Why unresolved: The paper highlights the potential benefits of the extracted knowledge but does not address the practical challenges of integrating it into existing design environments and workflows.
- What evidence would resolve it: A study investigating the practical implementation and integration of the extracted design knowledge into existing design environments and workflows, including user feedback and performance evaluations, would provide evidence to resolve this question.

## Limitations
- Dataset creation process relies on manual annotation without detailed documentation of annotation guidelines
- Cleaning pipeline for patent text lacks specific rules for handling edge cases like figure references and complex chemical formulas
- Uncertainty about generalizability to other technical domains or sentences with implicit relationships

## Confidence
**High confidence** in the technical feasibility of the two-step tagger approach and its demonstrated accuracy on the created dataset. The methodology is sound and the implementation details are sufficiently specified for reproduction.

**Medium confidence** in the claim that this approach is superior to pairwise or graph-based alternatives, as the benchmarks are internally generated and lack comparison to established methods on standard datasets.

**Low confidence** in the generalizability of the method to other technical domains or to patent sentences with significantly different structures or vocabulary, given the limited discussion of edge cases and the specialized nature of the fan system patent corpus.

## Next Checks
1. **External dataset validation**: Apply the trained taggers to a separate corpus of engineering patents (e.g., from a different technical domain like automotive or medical devices) and measure accuracy drop to assess generalizability.

2. **Edge case stress test**: Create a test set of complex patent sentences containing nested relationships, implicit relationships, and technical jargon not well-represented in the original dataset, then measure tagger performance degradation.

3. **Benchmark comparison on standard data**: Replicate the benchmark experiments (MLP classifiers and GNNs) on an established relationship extraction dataset like TACRED or DocRED to compare against published results and validate the claimed superiority.