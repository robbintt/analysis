---
ver: rpa2
title: 'Mavericks at BLP-2023 Task 1: Ensemble-based Approach Using Language Models
  for Violence Inciting Text Detection'
arxiv_id: '2311.18778'
source_url: https://arxiv.org/abs/2311.18778
tags:
- language
- bangla
- dataset
- task
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an ensemble-based approach for detecting violence-inciting
  texts in Bangla, using multiple transformer-based language models including BERT
  and ELECTRA variants. The authors experimented with several pre-trained models (BanglaBERT,
  BanglishBERT, MuRIL, XLM-RoBERTa, BengaliBERT) and used a hard voting ensemble to
  combine their predictions.
---

# Mavericks at BLP-2023 Task 1: Ensemble-based Approach Using Language Models for Violence Inciting Text Detection

## Quick Facts
- arXiv ID: 2311.18778
- Source URL: https://arxiv.org/abs/2311.18778
- Reference count: 5
- Ranked 10th in shared task with macro F1 score of 0.737 on test set

## Executive Summary
This paper presents an ensemble-based approach for detecting violence-inciting texts in Bangla using multiple transformer-based language models. The authors experimented with five pre-trained models including BanglaBERT, BanglishBERT, MuRIL, XLM-RoBERTa, and BengaliBERT, combining their predictions through hard voting ensemble. Their approach achieved a macro F1 score of 0.737, ranking 10th in the shared task competition. Post-evaluation experiments with weighted ensembling further improved performance to 0.745, demonstrating the effectiveness of ensemble methods for this low-resource language classification task.

## Method Summary
The approach fine-tuned five transformer-based language models on the Vio-Lens dataset for violence incitement detection. Models were trained for 10 epochs using AdamW optimizer with learning rate 1e-5, batch size 16, and Cross-Entropy Loss. The ensemble combined predictions from BanglaBERT, BanglishBERT, MuRIL, XLM-RoBERTa, and BengaliBERT using hard voting, with post-evaluation experiments exploring weighted ensemble techniques.

## Key Results
- Ranked 10th in shared task with macro F1 score of 0.737
- ELECTRA-based models (BanglaBERT, BanglishBERT) outperformed MLM-based models
- Post-evaluation weighted ensemble improved performance to 0.745 macro F1
- BanglaBERT achieved highest individual model performance (0.733 macro F1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ELECTRA-based models outperform MLM-based models for this low-resource language task.
- Mechanism: ELECTRA's Replaced Token Detection (RTD) pre-training objective provides better fine-tuning performance than Masked Language Modeling (MLM) while using less compute, especially beneficial for low-resource languages like Bangla.
- Core assumption: The RTD objective creates more challenging and informative pre-training signals than MLM for low-resource languages.
- Evidence anchors:
  - [abstract] States that BanglaBERT (ELECTRA-based) achieved the best macro F1 score of 0.733 among individual models
  - [section 4.1] Explicitly compares ELECTRA vs MLM approaches and their computational efficiency
  - [corpus] Shows related work using BERT-based models for similar tasks, supporting the general approach
- Break condition: If sufficient Bangla training data becomes available, MLM-based models might catch up or surpass ELECTRA's performance as the data scarcity advantage diminishes.

### Mechanism 2
- Claim: Ensemble methods improve performance by reducing model variance and leveraging complementary strengths.
- Mechanism: Hard voting ensemble combines predictions from diverse models, with each model contributing unique perspectives based on its pre-training and architecture, resulting in more robust and generalizable predictions.
- Core assumption: Different models make uncorrelated errors, so combining them reduces overall error rate.
- Evidence anchors:
  - [abstract] Reports ensemble achieved macro F1 score of 0.737, ranking 10th in the shared task
  - [section 5] Explains hard voting ensemble mechanism and its benefits
  - [corpus] Provides evidence from other shared tasks showing ensemble approaches work well for text classification
- Break condition: If models become too similar in their predictions or if computational constraints make ensembling impractical for deployment.

### Mechanism 3
- Claim: Language-specific pre-training data significantly improves model performance for low-resource languages.
- Mechanism: Models pre-trained on Bangla-specific corpora (BanglaBERT, BanglishBERT) outperform multilingual models (MuRIL, XLM-RoBERTa) because they capture language-specific linguistic patterns and nuances better.
- Core assumption: Domain-specific pre-training data provides more relevant linguistic features than general multilingual corpora for low-resource languages.
- Evidence anchors:
  - [abstract] Shows BanglaBERT achieving best performance among individual models
  - [section 4.1] Explains that BanglaBERT is trained on carefully curated Bangla dataset
  - [corpus] Shows related work emphasizing importance of language-specific models for low-resource languages
- Break condition: If multilingual models receive significantly larger and more diverse training data, or if language-specific data becomes insufficient for meaningful pre-training.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: All models used (BERT, ELECTRA, XLM-RoBERTa) are transformer-based, so understanding attention mechanisms is crucial for model selection and optimization
  - Quick check question: How does multi-head self-attention allow transformers to capture different linguistic relationships simultaneously?

- Concept: Fine-tuning vs pre-training distinction
  - Why needed here: The paper uses pre-trained models and fine-tunes them for the specific task, requiring understanding of both processes and their differences
  - Quick check question: What are the key differences between pre-training objectives (MLM vs RTD) and how do they affect downstream task performance?

- Concept: Ensemble methods and voting strategies
  - Why needed here: The final submission uses ensemble methods, requiring understanding of different ensembling techniques and their tradeoffs
  - Quick check question: How does hard voting differ from weighted voting, and when would each be preferable?

## Architecture Onboarding

- Component map:
  - Input: Bangla text (up to 600 words)
  - Processing: 5 transformer models (BanglaBERT, BanglishBERT, MuRIL, XLM-RoBERTa, BengaliBERT)
  - Ensemble layer: Hard voting and weighted ensemble
  - Output: Violence category (Non-Violence, Passive Violence, Direct Violence)
  - Metrics: Macro F1 score

- Critical path:
  1. Text preprocessing and tokenization
  2. Model inference (5 parallel paths)
  3. Ensemble voting
  4. Final classification output
  5. Performance evaluation

- Design tradeoffs:
  - Computational cost vs performance: Using 5 models provides good performance but increases inference time and resource requirements
  - Model diversity vs complexity: More diverse models might improve ensemble performance but increase system complexity
  - Fine-tuning vs using pre-trained weights: Fine-tuning provides task-specific adaptation but requires additional training data and computation

- Failure signatures:
  - Low ensemble agreement: Indicates model disagreement, suggesting need for model selection or training data quality issues
  - Consistent misclassification of specific categories: Points to class imbalance or model bias toward certain violence types
  - Performance degradation on longer texts: May indicate tokenization or context window limitations

- First 3 experiments:
  1. Baseline: Single model (BanglaBERT) fine-tuned on training data, evaluate macro F1
  2. Ensemble comparison: Hard voting ensemble vs weighted ensemble, compare macro F1 scores
  3. Model selection impact: Test different subsets of models (e.g., only ELECTRA-based vs only MLM-based) to understand contribution of each model type

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would incorporating domain-specific features beyond transformer embeddings improve performance for violence-inciting text detection in Bangla?
- Basis in paper: [inferred] The paper mentions that "features that combine main words and the distance between those in the sentence attain the best results" from prior work (Hammer 2014), suggesting potential for feature engineering beyond pure transformer approaches.
- Why unresolved: The authors only experimented with transformer-based models and ensemble methods, without incorporating any handcrafted features or domain knowledge features specific to violence detection.
- What evidence would resolve it: Comparative experiments using models that incorporate violence-specific features (keywords, patterns, linguistic markers) alongside or instead of pure transformer embeddings.

### Open Question 2
- Question: How would larger pre-training datasets impact the performance of Bangla language models for violence detection?
- Basis in paper: [explicit] The authors state in the limitations section that "Larger pre-training datasets are required for better low-resource models" and note that current models are limited by the quantity and quality of Bangla text used in pre-training.
- Why unresolved: The current experiments used existing pre-trained models without exploring the impact of training on larger or more diverse Bangla corpora.
- What evidence would resolve it: Training experiments comparing models trained on different sizes of Bangla corpora, or models trained on additional specialized datasets related to violence/incitement.

### Open Question 3
- Question: Would alternative ensemble techniques beyond hard voting and weighted voting provide better performance?
- Basis in paper: [explicit] The authors mention in the conclusion that "More sophisticated ensembling techniques can better utilize the performance of individual models and need to be researched further" after experimenting only with hard voting and weighted voting.
- Why unresolved: The paper only explored two basic ensemble methods (hard voting and weighted voting) without investigating more advanced ensemble techniques.
- What evidence would resolve it: Comparative experiments using different ensemble methods such as stacking, boosting, or dynamic weighting based on input characteristics.

## Limitations
- Computational overhead of using five transformer models simultaneously makes deployment challenging
- Hard voting ensemble assumes model independence without correlation analysis verification
- Post-evaluation improvement to 0.745 using weighted ensembling was not validated under competition conditions

## Confidence

**High Confidence:**
- ELECTRA-based models outperform MLM-based models for this low-resource Bangla task
- Ensemble methods improve performance compared to individual models

**Medium Confidence:**
- Language-specific pre-training data provides significant advantages
- Superiority of ELECTRA's RTD objective over MLM for low-resource languages

**Low Confidence:**
- Weighted ensemble's superiority over hard voting
- Generalizability of findings to other low-resource languages or violence detection tasks

## Next Checks
1. **Model Correlation Analysis**: Conduct correlation analysis of individual model predictions to verify independence assumption underlying hard voting ensemble
2. **Computational Efficiency Benchmark**: Measure inference time and resource utilization of five-model ensemble versus individual models
3. **Weighted Ensemble Validation**: Replicate weighted ensemble approach using cross-validation on development set to verify post-competition improvement to 0.745