---
ver: rpa2
title: Algorithm Selection for Deep Active Learning with Imbalanced Datasets
arxiv_id: '2302.07317'
source_url: https://arxiv.org/abs/2302.07317
tags:
- algs
- learning
- algorithm
- active
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TAILOR, the first adaptive algorithm selection
  strategy for deep active learning, particularly effective for imbalanced datasets.
  TAILOR iteratively and adaptively chooses among candidate active learning algorithms
  using novel reward functions that encourage class-balanced example collection.
---

# Algorithm Selection for Deep Active Learning with Imbalanced Datasets

## Quick Facts
- arXiv ID: 2302.07317
- Source URL: https://arxiv.org/abs/2302.07317
- Reference count: 40
- One-line primary result: Introduces TAILOR, an adaptive algorithm selection strategy for deep active learning that achieves accuracy comparable or better than the best candidate algorithm while maintaining class balance on imbalanced datasets

## Executive Summary
This paper introduces TAILOR, the first adaptive algorithm selection strategy for deep active learning, particularly effective for imbalanced datasets. TAILOR iteratively and adaptively chooses among candidate active learning algorithms using novel reward functions that encourage class-balanced example collection. The method employs Thompson Sampling in a novel bandit setting to maximize cumulative rewards. Extensive experiments on multi-class and multi-label datasets show TAILOR achieves accuracy comparable or better than the best candidate algorithm, while maintaining class balance. The algorithm is especially valuable when label budgets are limited, as it can select from hundreds of candidates with only 10-20 rounds of interaction.

## Method Summary
TAILOR addresses the challenge of selecting the best active learning algorithm from a large pool of candidates for a specific dataset and label budget. The method frames algorithm selection as a multi-armed bandit problem where each candidate algorithm is an "arm" and rewards are based on class balance. Using Thompson Sampling, TAILOR maintains posterior distributions over algorithm reward parameters and selects algorithms probabilistically to balance exploration and exploitation. The reward functions are designed to encourage collection of examples from underrepresented classes by inversely weighting classes by their current representation. TAILOR updates these posteriors based on observed labels after each batch, enabling rapid adaptation without requiring expensive retraining of all candidate algorithms.

## Key Results
- TAILOR achieves accuracy comparable or better than the best candidate algorithm across multiple datasets including CelebA, COCO, and CIFAR variants
- Maintains class balance effectively, with the rarest class receiving 4-10x more labels compared to baseline algorithms
- Outperforms baseline meta algorithms (Random Meta, ALBL Meta) by 2-4% in accuracy on average across datasets
- Demonstrates effectiveness with limited rounds (10-20) and large batch sizes (500-10,000 examples per batch)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TAILOR uses Thompson Sampling to adaptively choose among candidate active learning algorithms based on observed reward distributions
- Mechanism: At each round, TAILOR samples from posterior distributions over algorithm reward parameters and selects the algorithm with highest estimated reward given the current weighting vector. This balances exploration and exploitation.
- Core assumption: Reward distributions for each algorithm are stationary and can be estimated from past observations
- Evidence anchors:
  - [abstract]: "TAILOR employs Thompson Sampling in a novel bandit setting to maximize cumulative rewards"
  - [section]: "The key idea is to maintain posterior distributions for θ1,..., θM"
- Break condition: If reward distributions are highly non-stationary or the posterior updates are too slow relative to distribution shifts

### Mechanism 2
- Claim: TAILOR's reward functions encourage class-balanced example collection by inversely weighting classes by their current representation
- Mechanism: The diversity reward rt^div(x,y) = 1/K ∑[1/(1 ∨ COUNTt(i))y:i] up-weights classes with fewer labeled examples, driving the meta-algorithm to select more examples from underrepresented classes
- Core assumption: Class balance correlates with downstream model performance, especially under imbalanced data distributions
- Evidence anchors:
  - [section]: "Our reward function works well even under practical scenarios such as limited number of rounds and large batch sizes"
  - [section]: "we focus on the class-balance of the collected labels"
- Break condition: If class balance does not improve downstream performance (e.g., accuracy vs. class diversity trade-off)

### Mechanism 3
- Claim: TAILOR can select from hundreds of candidate algorithms with limited rounds by maintaining efficient posterior updates
- Mechanism: TAILOR updates Beta/Dirichlet posteriors using only the labels from the current batch, avoiding expensive retraining and enabling rapid adaptation to new datasets
- Core assumption: Limited rounds (10-20) are sufficient to identify the best subset of algorithms for a given dataset
- Evidence anchors:
  - [abstract]: "TAILOR can select from hundreds of candidates with only 10-20 rounds of interaction"
  - [section]: "In practice, one may design more task-specific priors"
- Break condition: If dataset complexity requires many more rounds to identify good algorithms, or if posterior updates are too slow

## Foundational Learning

- Concept: Multi-armed bandit problem
  - Why needed here: TAILOR reduces the algorithm selection problem to a bandit setting where each candidate algorithm is an "arm" and rewards are based on class balance
  - Quick check question: How does Thompson Sampling differ from epsilon-greedy or UCB in balancing exploration vs. exploitation?

- Concept: Thompson Sampling
  - Why needed here: TAILOR uses Thompson Sampling to maintain and update posterior distributions over algorithm reward parameters, enabling probabilistic selection
  - Quick check question: What distributions are used for posteriors in TAILOR for multi-label vs. multi-class classification?

- Concept: Active learning algorithm decomposition
  - Why needed here: TAILOR assumes any batch active learning algorithm can be decomposed into iterative single-example selection procedures, enabling sequential selection within batches
  - Quick check question: Why is the iterative decomposition assumption important for TAILOR's framework?

## Architecture Onboarding

- Component map: Meta-algorithm (TAILOR) -> Candidate algorithms pool -> Reward functions -> Model trainer -> Posterior distributions
- Critical path:
  1. Initialize posteriors for all candidate algorithms
  2. For each round: observe weighting vector, select algorithms via Thompson Sampling
  3. Sequentially collect examples from selected algorithms
  4. Observe labels and compute rewards
  5. Update posteriors based on observed labels
  6. Retrain model on new batch

- Design tradeoffs:
  - Posterior update frequency vs. computational cost (batch vs. online updates)
  - Exploration rate (number of algorithms tried) vs. exploitation of known good algorithms
  - Reward function design (class balance vs. accuracy vs. domain-specific objectives)

- Failure signatures:
  - Poor class diversity despite high accuracy (overfitting to majority classes)
  - Slow convergence to good algorithms (posterior updates too conservative)
  - Inconsistent performance across datasets (reward functions not generalizable)

- First 3 experiments:
  1. Run TAILOR on a balanced dataset with 10-20 rounds to verify basic functionality and convergence speed
  2. Test class diversity reward on an imbalanced dataset to verify class balance improvement
  3. Compare TAILOR against random meta and ALBL sampling on multiple datasets to verify relative performance gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TAILOR's performance degrade when the number of candidate algorithms M increases exponentially?
- Basis in paper: [explicit] The paper mentions that there are exponential number of possible AL algorithms, which could easily surpass labeling budget and overwhelm the meta selection algorithm. It states "one should not include too many algorithms" and suggests future work on leveraging extra information to choose proper candidate sets.
- Why unresolved: The paper only provides theoretical regret bounds that scale with M, but doesn't empirically test performance degradation with very large M (e.g., thousands of candidates). The experiments use M ranging from 7 to 515, which is relatively modest.
- What evidence would resolve it: Experiments varying M from hundreds to thousands of candidates while keeping other parameters fixed, measuring both regret and computational cost, would show the practical limits of TAILOR's scalability.

### Open Question 2
- Question: Can alternative reward functions that incorporate accuracy or uncertainty measures improve TAILOR's performance on balanced datasets?
- Basis in paper: [explicit] The paper notes "TAILOR's effectiveness on balanced datasets warrants future study through further experiments and alternative reward design" and mentions "designing better rewards functions that may incorporate accuracy or uncertainty measures" as future work.
- Why unresolved: The current implementation only uses class diversity rewards. The authors observe a potential negative correlation between class diversity and accuracy on Caltech256, suggesting the current reward might not be optimal for all scenarios.
- What evidence would resolve it: Empirical comparisons of TAILOR using various reward functions (accuracy-based, uncertainty-based, or hybrid) on both balanced and imbalanced datasets would show whether incorporating accuracy signals improves overall performance.

### Open Question 3
- Question: How does non-stationarity in label distributions {Pθi} affect TAILOR's performance, and what are effective strategies to handle it?
- Basis in paper: [explicit] The authors note that while their analysis assumes stationary distributions, "these distributions could be dynamically changing" and mention "studying non-stationarity in label distributions {Pθi} an interesting next step" as future work. They implement a simple discounting trick but acknowledge this as preliminary.
- Why unresolved: The theoretical analysis assumes stationarity, but practical implementation uses a crude discounting mechanism. The effectiveness of this approach versus more sophisticated methods is unknown.
- What evidence would resolve it: Experiments comparing TAILOR with different strategies for handling non-stationarity (e.g., sliding window updates, change-point detection, adaptive learning rates) on datasets with known distribution shifts would reveal the best approaches.

## Limitations

- Performance may degrade with exponentially large candidate pools (M >> 500) due to exploration-exploitation trade-offs and computational overhead
- Current reward functions focus on class balance rather than accuracy, potentially suboptimal for balanced datasets
- Assumes stationary reward distributions, though practical implementation uses only simple discounting for non-stationarity

## Confidence

- TAILOR achieves accuracy comparable or better than the best candidate algorithm: Medium
- TAILOR effectively improves class balance in collected examples: High
- TAILOR generalizes well across different dataset types and imbalance ratios: Medium

## Next Checks

1. Test TAILOR on non-image datasets (e.g., text, tabular) to assess domain generalizability
2. Evaluate performance when label budgets are extremely constrained (fewer than 10 rounds) to stress-test exploration-exploitation trade-offs
3. Implement the suggested posterior discounting factor of 0.9 to measure robustness to non-stationary reward distributions