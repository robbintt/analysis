---
ver: rpa2
title: Offline Pseudo Relevance Feedback for Efficient and Effective Single-pass Dense
  Retrieval
arxiv_id: '2308.10191'
source_url: https://arxiv.org/abs/2308.10191
tags:
- retrieval
- pseudo-queries
- dense
- online
- offline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of applying pseudo relevance
  feedback (PRF) to dense retrieval while maintaining online efficiency. The authors
  propose shifting the PRF process offline by pre-generating pseudo-queries for each
  document and pre-computing relevance scores between these pseudo-queries and documents.
---

# Offline Pseudo Relevance Feedback for Efficient and Effective Single-pass Dense Retrieval

## Quick Facts
- arXiv ID: 2308.10191
- Source URL: https://arxiv.org/abs/2308.10191
- Authors: 
- Reference count: 40
- Key outcome: Shifts PRF process offline by pre-generating pseudo-queries and pre-computing relevance scores, achieving competitive effectiveness with lower online latency compared to state-of-the-art dense retrieval models

## Executive Summary
This paper proposes a novel approach to address the efficiency challenges of applying pseudo relevance feedback (PRF) to dense retrieval. The key insight is to shift the computationally expensive PRF process offline by pre-generating pseudo-queries for each document and pre-computing relevance scores between these pseudo-queries and documents. During online retrieval, a simple BM25-based lexical matching is used to identify relevant pseudo-queries, and their pre-computed results are aggregated to produce the final ranking. This approach significantly reduces online retrieval to a single matching operation, improving efficiency while maintaining competitive effectiveness compared to state-of-the-art dense retrieval models.

## Method Summary
The proposed method consists of an offline preparation stage and an online retrieval stage. In the offline stage, pseudo-queries are generated for each document using a seq2seq model, and dense retrieval with PRF is performed to compute relevance scores between these pseudo-queries and documents. These scores are stored for later use. During online retrieval, BM25 is used to match user queries with relevant pseudo-queries, and the pre-computed relevance scores are aggregated using softmax weighting to produce the final ranking. This approach effectively shifts the computational burden of PRF and dense retrieval to the offline stage, allowing online retrieval to be performed with a single-pass sparse matching operation.

## Key Results
- Achieves competitive effectiveness compared to state-of-the-art dense retrieval models on TREC DL and HARD datasets
- Demonstrates significant improvements in nDCG@10, MAP, and R@1k metrics
- Maintains lower online latency compared to traditional dense retrieval with PRF
- Reduces online retrieval to a single matching operation with pseudo-queries

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-generating pseudo-queries offline and computing relevance scores in advance eliminates the need for expensive dense retrieval and PRF computations during online queries.
- Mechanism: The system generates pseudo-queries for each document offline using a seq2seq model, then pre-computes relevance scores between these pseudo-queries and documents using dense retrieval models with PRF. During online retrieval, a simple BM25 search identifies relevant pseudo-queries, and their pre-computed scores are aggregated.
- Core assumption: The semantic coverage of pre-generated pseudo-queries is sufficient to represent the search intents that user queries will express, and that lexical matching via BM25 can effectively identify relevant pseudo-queries.
- Evidence anchors:
  - [abstract] "During online retrieval, a simple BM25-based lexical matching is used to identify relevant pseudo-queries, and their pre-computed results are aggregated"
  - [section 3.3] "Given a query q for online retrieval, BM25 [10] is first utilized to fetch the top-s pseudo-queries"
  - [corpus] Weak evidence - no direct citations found supporting this specific offline pre-computation approach.
- Break condition: If the pseudo-queries fail to capture important query intents, or if BM25 cannot reliably match user queries to appropriate pseudo-queries, the effectiveness will degrade significantly.

### Mechanism 2
- Claim: Shifting the computational burden of PRF and dense retrieval to the offline stage allows online retrieval to be performed with a single-pass sparse matching operation.
- Mechanism: By computing the heavy-weight relevance scores offline using dense retrieval models enhanced with PRF, the online stage only needs to perform lightweight lexical matching between user queries and short pseudo-queries, followed by aggregation of pre-computed results.
- Core assumption: The offline pre-computed relevance scores remain valid and useful for online queries, and that the aggregation of these scores produces meaningful final rankings.
- Evidence anchors:
  - [abstract] "This approach reduces online retrieval to a single matching with pseudo-queries, significantly improving efficiency"
  - [section 3.3] "online retrieval mainly involves a simple lexical matching process"
  - [corpus] No direct citations found supporting this specific offline-to-online shift strategy.
- Break condition: If the pre-computed scores become stale or if the aggregation function fails to properly combine scores from multiple pseudo-queries, the system's effectiveness will suffer.

### Mechanism 3
- Claim: The use of softmax normalization on pseudo-query similarity scores and weighted aggregation of relevance scores produces a more robust final ranking.
- Mechanism: The system applies softmax to the similarity scores between the original query and pseudo-queries to create weights, then uses these weights to combine the normalized relevance scores between pseudo-queries and documents.
- Core assumption: The softmax-weighted combination of pseudo-query relevance scores provides a better estimate of document relevance than using a single dense retrieval pass.
- Evidence anchors:
  - [section 3.3] "we first apply a softmax operation to the relevance scores between the original query and the pseudo-queries, transforming sim (q, q) into weighted terms"
  - [section 3.3] "rank documents d ∈ R q by a weighted sum of scores of different pseudo-queries"
  - [corpus] No direct citations found supporting this specific softmax weighting approach for PRF aggregation.
- Break condition: If the softmax weighting overemphasizes certain pseudo-queries or if the normalization step introduces artifacts, the final rankings may become less accurate.

## Foundational Learning

- Concept: Dense retrieval with pre-trained language models
  - Why needed here: The paper builds on dense retrieval as the foundation, using PLMs to generate document and query representations for efficient semantic matching.
  - Quick check question: What is the key architectural difference between cross-encoder and bi-encoder dense retrieval models?

- Concept: Pseudo-relevance feedback (PRF) techniques
  - Why needed here: PRF is the core mechanism being optimized - the paper aims to maintain PRF effectiveness while reducing its computational cost during online retrieval.
  - Quick check question: How does traditional PRF work in dense retrieval, and what is its computational bottleneck?

- Concept: Lexical matching algorithms (BM25)
  - Why needed here: BM25 is used as the lightweight online matching mechanism to identify relevant pseudo-queries from user queries.
  - Quick check question: What are the key components of the BM25 scoring function, and how does it differ from dense retrieval?

## Architecture Onboarding

- Component map:
  - Offline pipeline: Pseudo-query generation (seq2seq model) → Dense retrieval with PRF → Relevance score computation and storage
  - Online pipeline: BM25 matching → Pseudo-query identification → Aggregation of pre-computed scores → Final ranking
  - Storage: Inverted index of pseudo-queries, pre-computed relevance scores, document IDs

- Critical path: During online retrieval, the critical path is: user query → BM25 matching to pseudo-queries → aggregation of pre-computed relevance scores → final ranking. The offline preparation is a one-time cost that enables this fast online path.

- Design tradeoffs: The system trades offline storage and computation for reduced online latency. Generating more pseudo-queries per document improves coverage but increases storage costs. Retrieving more documents per pseudo-query improves recall but increases aggregation time.

- Failure signatures: Significant drop in effectiveness metrics (nDCG@10, MAP) despite maintaining low latency suggests the pseudo-queries are not capturing user intent well. High online latency despite the single-pass design indicates issues with the BM25 matching or aggregation steps.

- First 3 experiments:
  1. Test the pseudo-query generation quality by sampling pseudo-queries and evaluating their semantic coverage against a diverse query set.
  2. Benchmark the online latency and effectiveness with different numbers of pseudo-queries returned online (s parameter) to find the optimal tradeoff.
  3. Compare the effectiveness of different dense retrieval models in the offline stage to determine which provides the best balance of accuracy and computational efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed approach handle cases where pseudo-queries fail to capture diverse search intents, leading to semantic drift from the original query?
- Basis in paper: [inferred] The authors acknowledge potential semantic shifts caused by pseudo-queries and propose fusing results with the original query to mitigate this issue.
- Why unresolved: The paper only briefly mentions this problem and its mitigation strategy without providing a detailed analysis of its impact on retrieval effectiveness or exploring alternative solutions.
- What evidence would resolve it: A comprehensive study comparing retrieval effectiveness with and without query fusion across various query types and evaluating the extent of semantic drift caused by pseudo-queries.

### Open Question 2
- Question: What is the impact of the number of documents retrieved per pseudo-query (k) on both effectiveness and storage costs, and how can this tradeoff be optimized?
- Basis in paper: [explicit] The authors discuss the influence of k on MAP and R@1k metrics, showing initial improvements followed by plateauing, while also noting increased latency with larger k values.
- Why unresolved: While the paper provides some insights into this relationship, it does not offer a detailed analysis of the optimal k value or explore strategies to balance effectiveness and storage costs.
- What evidence would resolve it: An in-depth study examining the effectiveness-storage cost tradeoff across different k values and proposing optimization strategies, such as adaptive k selection based on query characteristics.

### Open Question 3
- Question: How does the proposed approach scale with increasing collection sizes, and what are the potential bottlenecks in terms of online latency and storage requirements?
- Basis in paper: [inferred] The authors claim that their approach maintains low online latency regardless of collection size, unlike dense retrieval methods. However, they do not provide a detailed analysis of scalability issues.
- Why unresolved: The paper does not present empirical evidence or theoretical analysis to support the scalability claims, nor does it discuss potential bottlenecks in terms of online latency and storage requirements.
- What evidence would resolve it: A scalability study evaluating the online latency and storage requirements of the proposed approach across different collection sizes, identifying potential bottlenecks, and proposing solutions to address them.

## Limitations
- Heavy offline storage requirements for pseudo-queries and pre-computed relevance scores across entire document collection
- Effectiveness heavily depends on quality and coverage of pre-generated pseudo-queries
- Assumes pre-computed relevance scores remain valid for online queries, which may not hold for rapidly evolving document collections

## Confidence

**High confidence claims:**
- The core mechanism of shifting PRF computation offline to improve online efficiency is technically sound and experimentally validated on TREC DL and HARD datasets
- The effectiveness improvements in nDCG@10, MAP, and R@1k metrics are supported by empirical results
- The fundamental tradeoff between offline storage/computation and online latency reduction is well-established

**Medium confidence claims:**
- The softmax-weighted aggregation of pseudo-query relevance scores provides optimal effectiveness
- The specific choice of 80 pseudo-queries per document represents the ideal balance between coverage and storage costs
- BM25-based lexical matching provides sufficient precision for pseudo-query identification

**Low confidence claims:**
- The generalizability of results to other domains beyond passage retrieval
- Long-term effectiveness given document collection changes without re-computation
- Performance under varying query distributions and search intents

## Next Checks
1. **Coverage Validation**: Systematically evaluate the semantic coverage of pre-generated pseudo-queries by measuring their ability to capture diverse query intents from multiple domains, identifying potential gaps in representation.

2. **Storage Scalability Analysis**: Measure the storage requirements and computational overhead as the document collection size scales, determining practical limits for real-world deployment scenarios.

3. **Dynamic Collection Testing**: Evaluate system performance when applied to document collections with temporal dynamics or concept drift, assessing the need for periodic offline re-computation to maintain effectiveness.