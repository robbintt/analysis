---
ver: rpa2
title: Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent
  Space
arxiv_id: '2307.13390'
source_url: https://arxiv.org/abs/2307.13390
tags:
- space
- latent
- counterfactual
- sample
- classifier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of generating counterfactual explanations
  (CEs) for binary classifiers that are both computationally efficient and realistic.
  The core method involves shaping the latent space of an autoencoder to follow a
  Gaussian mixture distribution, then generating CEs through linear interpolation
  between the query sample and the centroid of the target class in this latent space.
---

# Counterfactual Explanation via Search in Gaussian Mixture Distributed Latent Space

## Quick Facts
- arXiv ID: 2307.13390
- Source URL: https://arxiv.org/abs/2307.13390
- Reference count: 13
- Method generates counterfactual explanations through search in Gaussian mixture distributed latent space with adversarial disentanglement

## Executive Summary
This paper presents a method for generating counterfactual explanations (CEs) for binary classifiers that addresses key challenges in computational efficiency and realism. The approach shapes the latent space of an autoencoder to follow a Gaussian mixture distribution, then generates CEs through linear interpolation between the query sample and the target class centroid. By incorporating an adversarial classifier to disentangle label-relevant and label-irrelevant features, the method maintains query characteristics while efficiently finding valid counterfactuals. Experiments on MNIST, Adult income, and Lending Club datasets demonstrate superior performance compared to state-of-the-art approaches across multiple metrics.

## Method Summary
The method involves training an autoencoder with a latent space shaped as a Gaussian mixture distribution, where samples of the same class cluster around their respective centroids. An adversarial classifier ensures disentanglement between label-relevant and label-irrelevant features in the latent space. To generate a counterfactual explanation, the query sample is encoded to obtain both label-relevant and label-irrelevant latent representations. Linear interpolation is performed in the label-relevant latent space toward the target class centroid, while the label-irrelevant component is preserved. The interpolated latent representation is then decoded to produce the counterfactual, which is validated by checking if it crosses the decision boundary of the pre-trained classifier.

## Key Results
- Counterfactual generation times of 0.007-0.008 seconds, significantly faster than baseline methods
- Validity rates of 89.5-92.1%, demonstrating high success rates in generating meaningful counterfactuals
- Reconstruction losses of 0.012-0.132, indicating counterfactuals stay close to the original data distribution
- Outperforms three state-of-the-art approaches across all evaluated metrics including proximity and sparsity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear interpolation in the Gaussian mixture latent space preserves label-relevant characteristics while efficiently crossing the decision boundary
- Mechanism: By shaping the latent space to follow a Gaussian mixture distribution, samples of the same class cluster around their respective centroids. Linear interpolation between a query point and the target class centroid ensures monotonic movement toward the target class while maintaining proximity to the data manifold
- Core assumption: The latent space captures class-separating structure and the Gaussian mixture approximation adequately represents the true data distribution
- Evidence anchors:
  - [abstract] "CEs are then generated in latent space by linear interpolation between the query sample and the centroid of the target class"
  - [section] "If the latent space follows a Gaussian mixture distribution, the conditional probability distribution of a latent embedding z given its class label c can be expressed..."

### Mechanism 2
- Claim: Disentanglement of label-relevant and label-irrelevant features maintains query characteristics during counterfactual generation
- Mechanism: The adversarial classifier trains Encoderu to produce embeddings zu that are independent of the classification label. This creates a label-irrelevant latent space that preserves non-predictive features of the input, which are then combined with the interpolated label-relevant latent space to reconstruct realistic counterfactuals
- Core assumption: The adversarial training successfully separates label-relevant from label-irrelevant information without losing important input characteristics
- Evidence anchors:
  - [section] "Inspired by a Two-Step Disentanglement Method [Hadad et al., 2020], we introduce an Adversarial Classifier to ensure that the embedding zu captured by the Encoder u is classification label-irrelevant"
  - [section] "With the Adversarial Classifier, we could guarantee that the zu and ŷ are disentangled and independent, which prepares for the interpolation in the Generating Step"

### Mechanism 3
- Claim: Gaussian mixture loss with likelihood constraint prevents outliers and ensures CEs stay close to the data manifold
- Mechanism: The likelihood term in the GM loss (Llkd) measures how well training data fits the Gaussian mixture distribution. This constraint prevents latent embeddings from being far from their class centroids while still being correctly classified, addressing the outlier problem that pure classification loss might create
- Core assumption: The likelihood constraint effectively regularizes the latent space without overly constraining the model's ability to represent the data
- Evidence anchors:
  - [section] "Applying the classification loss only cannot reach our goal of forcing the latent space to be a Gaussian mixture distribution. There will be situations where a zi can be far away from the corresponding target class centroid µc and still be correctly classified..."
  - [section] "To fix this problem, we then use a likelihood to measure the extent to of the training data fits the Gaussian mixture distribution"

## Foundational Learning

- Concept: Autoencoder architecture and training
  - Why needed here: The autoencoder learns to compress and reconstruct input data, creating a latent space that captures the data distribution. This is fundamental to the method's ability to generate realistic counterfactuals
  - Quick check question: What happens to the reconstruction loss if the latent dimension is set too low?

- Concept: Gaussian mixture models and EM algorithm
  - Why needed here: The method explicitly shapes the latent space to follow a Gaussian mixture distribution using class labels from the pre-trained classifier. Understanding GMMs is crucial for grasping how the latent space is structured
  - Quick check question: How does the EM algorithm relate to the GM loss formulation in this paper?

- Concept: Adversarial training and GAN principles
  - Why needed here: The adversarial classifier component uses principles similar to GANs to achieve disentanglement between label-relevant and label-irrelevant features in the latent space
  - Quick check question: What is the role of the discriminator in a standard GAN, and how does that relate to the adversarial classifier here?

## Architecture Onboarding

- Component map:
  - Pre-trained binary classifier (f) -> provides class labels for training the autoencoder
  - Encoder -> maps input to label-relevant latent space (z) following Gaussian mixture distribution
  - Encoderu -> maps input to label-irrelevant latent space (zu) through adversarial training
  - Decoder -> reconstructs input from combined latent representations
  - Adversarial Classifier -> ensures zu is label-irrelevant
  - Training pipeline -> jointly optimizes all components with GM loss, reconstruction loss, and adversarial loss

- Critical path:
  1. Training phase: Learn autoencoder with Gaussian mixture latent space and disentangled representations
  2. Generation phase: For each query, encode to get zq and zu,q, interpolate zq toward target centroid, combine with zu,q, decode to get counterfactual
  3. Validation: Check if generated counterfactual crosses decision boundary within tolerance

- Design tradeoffs:
  - Gaussian mixture vs. other latent space distributions - GMM provides interpretable centroids but may not capture complex data structure
  - Disentanglement vs. reconstruction quality - adversarial training may reduce reconstruction accuracy to achieve better feature separation
  - Interpolation step size vs. computational efficiency - smaller steps increase accuracy but require more computation

- Failure signatures:
  - High reconstruction loss on test data indicates poor autoencoder training
  - Low validity rate suggests the latent space doesn't capture class structure well
  - Unrealistic counterfactuals indicate failed disentanglement
  - Slow generation time suggests computational bottleneck in interpolation or decoding

- First 3 experiments:
  1. Train the autoencoder on MNIST with λlkd = 0.1, λadv = 0.05 and visualize the latent space with PCA to verify Gaussian mixture structure
  2. Generate counterfactuals for a few query samples from class 1 to class 7 and visualize the interpolation path with classification scores
  3. Compare reconstruction loss of original samples vs. counterfactuals to verify they stay close to data manifold

## Open Questions the Paper Calls Out
- How would the method perform with multi-class classification problems beyond binary classification?
- What is the impact of different choices for the latent space dimensionality on the quality of counterfactual explanations?
- How does the method handle datasets with significant class imbalance in practice?

## Limitations
- Performance on highly imbalanced datasets or multi-class problems beyond tested binary cases is unclear
- Computational overhead of training the autoencoder with Gaussian mixture constraints versus simpler approaches is not thoroughly analyzed
- Robustness to adversarial attacks or distribution shifts is not evaluated

## Confidence
- The effectiveness of the Gaussian mixture latent space approach: High
- The validity and efficiency improvements over baseline methods: High
- The generalizability to complex, high-dimensional real-world data: Medium
- The scalability to large datasets and real-time applications: Medium

## Next Checks
1. Test the method on multi-class classification problems (e.g., MNIST with all digits) to evaluate scalability beyond binary cases
2. Conduct ablation studies removing the Gaussian mixture constraint or adversarial disentanglement to quantify their individual contributions
3. Evaluate the method's robustness by introducing noise or adversarial examples into the query samples and measuring CE validity rates